{"article_publication_date": "06-05-2010", "fulltext": "\n Evaluating the Accuracy of Java Pro.lers Todd Mytkowicz Amer Diwan Matthias Hauswirth Peter F. Sweeney \nUniversityofColorado at Boulder University ofLugano IBM Research {mytkowit,diwan}@colorado.edu Matthias.Hauswirth@unisi.ch \npfs@us.ibm.com Abstract Performance analysts pro.le their programs to .nd methods that are worth optimizing: \nthe hot methods. This paper shows that four commonly-used Java pro.lers (xprof, hprof, jpro.le, and yourkit)often \ndisagree on the identity of the hot methods. If two pro.lers disagree, at least one must be incorrect. \nThus, there is a good chance that a pro.ler will mislead a performance analyst into wastingtime optimizinga \ncold method withlittle or noperfor\u00admanceimprovement. This paper uses causality analysis to evaluate pro.lers \nand to gain insight into the source of their incorrectness. It shows that thesepro.lers all violate afundamental \nrequirementfor sampling\u00adbasedpro.lers:tobecorrect,a sampling-basedpro.lermust collect samples randomly. \nWe show that a proof-of-concept pro.ler, which collects sam\u00adples randomly, does not suffer from the above \nproblems. Specif\u00adically, we show, using a number of case studies, that our pro.ler correctlyidenti.es \nmethodsthat areimportantto optimize;in some cases otherpro.lers reportthatthese methods are cold andthus \nnot worth optimizing. Categories and Subject Descriptors C.4 [Measurement tech\u00adniques] General Terms \nExperimentation,Performance Keywords Bias,Pro.ling,Observer effect 1. Introduction Performance analysts \nuse pro.lers to identify methods that con\u00adtributethe mostto overallprogram executiontime(hot methods) \nand are therefore worth optimizing.If apro.leisincorrect,it may mislead the performance analyst into \noptimizing cold methods instead of hot methods. This paper shows that four commonly\u00adused Java pro.lers \n(xprof [24], hprof [23], jpro.le [11], and yourkit [26])oftenproduceincorrectpro.les.Speci.cally,it shows \nthat these pro.lers often disagree on both the identity of the hot methods and the time spent in methods; \nif two pro.lers disagree, they cannotbothbe correct. A pro.ler may produce incorrect pro.les due to two \nreasons. First, a pro.ler may be biased toward some methods in favor of other methods.For example, apro.ler \nthatignores native methods (i.e., biasedaway from native methods) may indicate that the hot Permission \nto make digital or hard copies of all or part of this work for personal or classroomuseisgranted withoutfeeprovided \nthatcopiesarenot madeordistributed forpro.tor commercial advantage andthat copiesbearthis notice andthefull \ncitation onthe .rstpage.Tocopy otherwise,torepublish,topost onserversortoredistribute tolists, requiresprior \nspeci.cpermission and/or afee. PLDI 10 June5 10,2010,Toronto,Ontario,Canada. Copyright c &#38;#169; 2010 \nACM 978-1-4503-0019-3/10/06. . . $10.00 methods areinbytecode whenin reality they are allin native code. \nSecond, a pro.ler may perturb the program being optimized and thus changeitspro.le(observer effect).Thispaper \nshowsthatboth of these effects contributetoincorrectpro.les. Determining if a pro.le is correct is impossible \nin general be\u00adcausethereisno perfect pro.le: allpro.lesexhibitsomebiasand observer effect.1 Thus, weintroducethe \nnotion ofactionable to ap\u00adproximate the correctness of a pro.le. A pro.le is actionable if acting on \nthe pro.le yields the expected outcome. For example, if apro.le of an applicationidenti.es method Mashot, \nthen we ex\u00adpect thatoptimizing Mwill signi.cantly speed up the application. Actionabledoes notimply correctness: \ne.g., apro.le may attribute 15% of overall execution time to M but a hypothetical correct pro.le may \nattribute 20%. Both of these pro.les are actionable (i.e.,bothwillguidetheperformance analysttowards \nMsinceboth 15%and20% are signi.cant)but only oneis correct. To evaluate if a pro.ler is actionable, we \nuse causality analy\u00adsis[21]. Causality analysis works by intervention: we change our system(theintervention) \nand then checkif theinterventionyields the predicted performance. If the prediction holds, then causal\u00adity \nanalysis gives us con.dence that the pro.ler is actionable; if the prediction does not hold, causality \nanalysis indicates that the pro.ler is not actionable. To ensure our results are not an arti\u00adfact of \na particular JVM, we show that pro.lers often produce non-actionablepro.les on twodifferentproductionJVMs(Sun \ns HotspotandIBM sJ9). The contributions ofthispaper are asfollows: 1. We showthatcommonly-usedpro.lersoftendisagreewitheach \nother and thus are oftenincorrect. 2. We use causalityanalysistodetermine whether or not apro.ler is \nproducing an actionable pro.le. This approach enables us to evaluate a pro.ler without knowing the correct \npro.le; prior work on evaluating the accuracy ofpro.lersassumesthe existence of a correct pro.le (Section \n8.1). For example, Buytaert et al. use apro.le obtained usingHPMs and frequent sampling to evaluate the \naccuracy of otherpro.lers [5]. 3. We show,using causality analysis,that commonly-usedpro.l\u00aders oftendo \nnotproduce actionablepro.les. 4. We showthatthe observer effectbiases ourpro.les.Inpartic\u00adular, dynamic \noptimizations interact with a pro.ler s sampling mechanism toproducepro.lerdisagreement. 5. We introduce \na proof-of-concept pro.ler that addresses the above mentioned source ofpro.lerbias.As a consequence our \npro.lerproduces actionablepro.les.  1Wedonot considerpro.lersthatuseexternalhardwareprobesorsimula\u00adtion,both \nof which canavoid someformsofincorrectness.Thosemethods introduce their own challenges(e.g., simulator \naccuracy) and are outside the scope of thispaper. percent of overall execution JavaParser.jj_scan_token \n20 NodeIterator.getPositionFromParent DefaultNameStep.evaluate 15 10 5 0 hprof jprofile xprof yourkit \n   Figure1. Disagreementinthehottestmethodforbenchmark pmd acrossfourpopularJavapro.lers. This paper \nis organized as follows: Section 2 presents a mo\u00adtivating example. Section 3 presents our experimental \nmethodol\u00adogy. Section 4 illustrates how pro.ler disagreement can be used to demonstrate that pro.les \nare incorrect. Section 5 uses causal\u00adity analysis to determine if a pro.ler is actionable. Section 6 \nex\u00adplores why pro.lers often produce non-actionable data. Section 7 introduces aproof-of-conceptpro.ler \nthat addresses thebiasprob\u00adlems with existing pro.lers and produces actionable pro.les. Fi\u00adnally,Section8discusses \nrelated work andSection9 concludes. 2. Motivation Figure 1 illustrates the amount of time that four popular \nJava pro.lers (hprof, jpro.le, xprof, and yourkit) attribute to three methods from the pmd DaCapobenchmark[3]. \nThere arethree bars for each pro.ler, and each bar gives data for one of the three methods: jj scan token, \ngetPositionFromParent, and evaluate. These arethe methodsthatone ofthefourpro.lersidenti.ed asthe hottestmethod.For \nagivenpro.ler, P, and method, M,theheight of the bar is the percentage of overall execution time spent \nin M accordingto P.The errorbars(which aretight enoughtobe nearly invisible)denote95% con.denceinterval \nof the mean of30 runs. Figure 1 illustrates that the fourpro.lers disagree dramatically about which methodis \nthehottest method.For example, two ofthe pro.lers,hprof and yourkit,identify the jj scan token method \nas the hottest method; however, the other two pro.lers indicate that this method is irrelevant to performance \nas they attribute 0% of execution time toit. Figure 1 also illustrates that even when two pro.lers agree \non the hottest method, they disagree in the percentage of time spentin the method.For example, hprof \nattributes6.2% of overall execution time to the jj scan token method and yourkit attributes 8.5% of overall \nexecution time to this method. Clearly, when two pro.lers disagree, they cannot both be cor\u00adrect.Thus,if \naperformance analyst uses apro.ler, she may or may notget a correctpro.le;in the case of anincorrectpro.le, \ntheper\u00adformance analyst may waste her time optimizing a cold method that will not improve performance. \nThis paper demonstrates that the aboveinaccuracies are not corner casesbut occurforthe major\u00adity of commonly \nstudiedbenchmarks. 3. Experimental methodology This sectiondescribespro.lers we useinthis study,thebenchmark \nprograms we usein our experiments,the metrics we useto evaluate pro.lers, andour experimental setup. \n B.mark Description Time [sec.] Overhead hprof xprof jprof. y.kit antlr parser generator 21.02 1.1x \n1.2x 1.2x 1.2x bloat bytecode optimizer 74.26 1.1x 1.3x 1.0x 1.2x chart plotand render PDF 75.70 1.1x \n1.1x 1.1x 1.1x fop print formatter 27.68 1.5x 1.1x 1.0x 1.8x jython python interpreter 68.12 1.1x 1.3x \n1.1x 1.7x luindex text indexing tool 85.98 1.1x 1.2x 1.0x 1.1x pmd source analyzer 62.75 1.9x 1.3x 1.0x \n2.2x  Table1. Overheadforthefourpro.lers.Wecalculate Overhead as the total execution time with the pro.ler \ndivided by execution time withoutthepro.ler 3.1 Pro.lers We study four state-of-the-art Java pro.lers \nthat they are widely usedinboth academia andindustry: hprof: is an open-source pro.ler that ships with \nSun s Hotspot andIBM sJ9. xprof:is theinternalpro.lerinSun sHotspotJVM. jpro.le:is an award-winning2 \ncommercialproductfromEJ tech\u00adnologies. yourkit:isan award-winning3 commercialproductfromYourKit. To \ncollect data with minimal overhead, all four pro.lers use sampling.Sampling approximates thetimespentinan \napplication s methodsbyperiodically stopping aprogram and recordingthe cur\u00adrently executing method(a \nsample ).Thesepro.lersall assume that the number of samplesfor a methodisproportionalto thetime spent \nin the method. We used a sampling rate of 10ms for the ex\u00adperimentsin thispaper(thisis thedefaultratefor \nmostpro.lers). 3.2 Benchmarks We evaluated the pro.lers using the single-threaded DaCapo Java benchmarks[3](Table1)with \ntheirdefaultinputs. Wedid not usethemulti-threadedbenchmarks(eclipse, luse\u00adarch, xalan, and hsqldb), \nbecause each pro.ler handles threads differently, whichcomplicate comparisons acrosspro.lers. The Overhead \ncolumns in Table 1 give the overhead of each pro.ler. Speci.cally, theygive the end-to-end execution \ntime with pro.ling divided by the end-to-end execution time without pro.l\u00ading. We see that pro.ler overhead \nis relatively low, usually 1.2 or better for all pro.lers except yourkit, which has more overhead than \notherpro.lersbecauseit alsoinjectsbytecodesinto classes to count the number of calls to each method,in \naddition to sampling 3.3 Howtoevaluatepro.lers If weknewthe correct pro.leforaprogramrun,wecould eval\u00aduatethepro.ler \nwith respecttothiscorrectpro.le.Unfortunately, there is no correct pro.le most of the time and thus we \ncannot de.nitivelydetermineifapro.lerisproducing correct results. For this reason, we relax the notion \nof correctness into ac\u00adtionable . By saying that a pro.le is actionable we mean that we do not know if \nthe pro.le is correct ; however, acting on the pro.le yields the expected outcome. For example, optimizing \nthe hot methodsidenti.edby thepro.le willyield a measurablebene\u00ad.t.Thus, unlike correctness whichisanabsolutecharacterization \n(apro.leis either correct orincorrect),actionableis necessarilya fuzzy characterization. 2Java Developer \ns Journal Readers Choice Award for Best Java Pro.ling (2005-2007). 3JavaDeveloper sJournalEditorsChoiceAward.(2005). \n Section 7.3 uses the notion of actionable to evaluate pro.lers. However,thisapproachisnoteasy:evenifweknowwhichmethods \narehot, we may notbe able to optimize them.Thus, we use adual of this approach to evaluate pro.lers: \nrather than speeding up hot methods, we slowdownhot methods(Section5).If a methodis hot, then slowing \nit down further should only make ithotterin the pro.le.Ifitdoes not, then thepro.le(before or after slowingit \ndown)was not actionable.  3.4 Platform We use anIntelCore2Quadprocessor(2.4GHz runningUbuntu Linux version \n2.6.28-13) workstation with 4GB of RAM for our experiments. To increase the generality of our results, \nwe use two production JVMs: Sun Hotspot version 1.6.0 12 and IBM J9 ver-sion60.Unless we explicitly say \nso, all thedata thatispresentedis fortheSunJVM.InbothJVMs, we always usethedefaultcon.gu\u00adrationthat ships \nwiththeJVM(e.g.heap size, clientJIT compiler).  3.5 Measurements We usedbest experimentalpractices: \n To reducetheimpact of otherprocesses onourresults,werun all experiments on an unloaded machine(i.e., \nwekilled all non\u00adessentialprocesses) and use onlylocaldisks.  To ensure that thepro.lershave enough \nsamples[13] and to avoid start-up effects, we iterate each benchmark 20 times in the same JVM invocation \nusing the DaCapo test harness. With this methodology, execution times rangefrom 21 to 86 seconds which \ntranslatesinto 2, 100 to 8, 600 samplesper run.  Witheachinvocation oftheJVM,theJIT compilerplaces com\u00adpiledmethods \natdifferentlocations.Prior workhas shown code placementimpactsperformance[14,19]and so, unless other\u00adwise \nindicated, we repeat each experiment 30 times with each experiment restarting theJVM.Wepicked 30 runsbecause \nthe meanis usually normallydistributed after 30 trials , which en\u00adables subsequent statistical analysis(e.g., \ncon.denceintervals) ofthedata[16]. Whereappropriate,wealsoincludethe 95% con.denceintervals of this mean. \n 4. Extent of theproblem Section 2 demonstrated that at least for one program, four pro.l\u00aders identify \nthree different hottest methods. Worse, the hottest method due to one pro.ler is often cold according \nto another pro\u00ad.ler.Thus, at least some of thepro.lers areincorrect.This section explores thefollowingquestions:(i) \nhowfrequentlydopro.lers disagree,(ii) whatisthe magnitude oftheirdisagreement,(iii) is theirdisagreement \neasily explained and thusinnocuous(e.g., one pro.ler labels the calling method as hot while another labels \nthe callee method ashot), and(vi)ispro.lerdisagreement speci.cto a particularJVM. 4.1 Metricsforquantifyingpro.leragreement \nWe use two metrics toquantifypro.ler agreement: Unionn is the cardinality of the set obtained by unioning \nthe hottest n methods from each pro.ler. More formally, if the set of n hottest methods according to \nfour pro.lers are H1 , H2 , H3, and H4 respectively,then Unionn is|H1 .H2 .H3 .H4|. Inthebestcase, Unionn \nisn indicatingthatthepro.lers agreed with each otherin theidentity(but not necessarily order) of the \nhottestn methods.Inthe worst case, Unionn isn *m, where m isthe number ofpro.lers andn*m indicatestotaldisagreement \nbetween them pro.lers. UNION 1 4 3 2 1 0 antlr bloat chart fop jython luindex pmd benchmark Figure2. \nUnion1 across thefourpro.lers. HOTNESSpm isthepercentage of overall executiontime spent executing a method, \nm according to pro.ler p. HOTNESSpm tells the performance analyst how much of a bene.t they can expect \nwhenthey optimize m.For example,ifthe HOTNESSpm is5%, the maximum speedup we expect from optimizing m \nis about5%.4 We picked the above two metrics because they capture two of the most common ways in which \nperformance analysts interpret pro.leinformation.Speci.cally,performance analysts oftenfocus onthefewhottestmethodsespeciallyifthesemethodseach \naccount for more than a certainpercentage ofprogram execution time. To ensure statistically signi.cant \nresults, we always run our experiments multiple times. Consequently, when we report the above two metrics, \nwe report the average behavior across multiple runs.For Unionn we order the methodsfor eachpro.lerbased \non the averagefrom multiple runs and then compute the union. 4.2 Howfrequentlydopro.lersdisagree? If \ndisagreement between pro.lers happens only for a few corner cases, we may be justi.ed in ignoring it. \nOn the other hand, if pro.lersdisagreefrequently,then we cannotignoreit.This section shows thatpro.lerdisagreementis \ncommon. 4.2.1 Pro.lersdisagreeonthehottest method In this section, we demonstrate that pro.lers often \ndisagree as to which methodisthehottest.Wepicked thehottest methodbecause itistheonethatperformance analysts \nofteninvestigateforpotential bottlenecks. Figure 2 gives the Union1 metric for four pro.lers (i.e., we \nconsider onlythe singlehottest method).Eachbargivesthe Union1 metric for one benchmark. Recall that Union1 \nwill be 1 if all pro.lers agree on the hottest method and 4 if the four pro.lers totallydisagree on thehottest \nmethod. From the .gure we see pro.ler disagreement for four of the sevenbenchmarks(i.e., thebars arehigher \nthan1).In other words, if we use apro.ler topick thehottest method we may end up with a method thatis \nnot really thehottest. These results are surprising: the most common usage scenario forpro.lersisto usethemtoidentifythehottestmethod;our \nresults show thatpro.lers oftenfail evenin thisbasic task. 4This back-of-the-envelope calculation ignores \nindirect costs of the method; e.g.,memory system costs.Nonetheless,becauseperformance an\u00adalystsoftendosuch \ncalculations,weusethismetricinourpaper.  antlrbloatchartfopjythonluindexpmd agreement across 6 pairs \n4 3 2 1 UNION n 0 top n methods Figure3. Unionn for ourfourpro.lers with n from1 to10.  4.2.2 Pro.lersdisagreeonthehottest \nn methods One explanation for the pro.ler disagreements in Figure 2 is that there are other methodsthat \nare ashot or nearly ashot asthehottest method.Inthis case,twopro.lers maydisagree with each otherbut \nbothcouldbe(almost) correct.For example,if theprogram spends 20% of its time in the hottest method and \n19.9% in the second\u00adhottest method, then some pro.lers may identify the .rst method as the hottest while \nsome may identify the second method; both arecloseenough tobeing hottest thatthedisagreementdoesnot really \nmatter. To determine whether or not thisis the case, Figure 3 presents Unionn forn rangingfrom1to10.Apoint \n(x, y) saysthat Unionx isy.Eachpointisthe mean across the7benchmarks and errorbars denote95% con.denceinterval \nof the mean. The line y =1 * n gives thebestpossible scenarioforpro.ler agreement: with full agreement, \nthe number of unique methods acrossthetop-n hottestmethods ofthefourpro.lers(i.e., Unionn) willbe n. \nThe line 4 * n givestheworstcase scenarioforpro.ler agreement: there is no agreement among the 4 pro.lers \nas to what are the top-n hottestmethods. FromFigure3 we seethat as we consider more methods(i.e., increasethen \nvalue), Unionn increases.In other words, evenifwe lookbeyondthehottestmethod anddisregardthe orderingbetween \nthehottestfew methods, we stillgetpro.lerdisagreement.  4.3 Byhowmuchdopro.lersdisagree? The Unionn \nmetric for quantifying disagreement ignores the per\u00adcentage of time the program spends in the hot methods. \nThis per\u00adcentage determines whether or not the performance analyst will even bother to optimize the method; \ne.g., if thehottest method ac\u00adcountsfor only onepercent of execution time,it may notbe worth optimizing \neven thoughitis thehottest method. Figure 4 presents the percentage of overall execution time for four \nhot methods in each benchmark. We picked four methods because a client of a pro.le usually only cares \nabout the .rst few hot methods in the pro.le. For each benchmark, we picked the four methods asfollows:(i) \nfor each method weattributedtoit the maximum percentage of execution time according to the four pro.lers(e.g.,if \nthefourpro.lers assign5%,10%,7%, and3% to the method, we assigned10%tothe method);(ii) wepickedthe four \nmethods withthehighestassigned value.For eachbenchmark, Figure4 hasfourbars, onefor eachpro.ler.For eachpro.ler, \np,a bar representsthesumofHOTNESSpm forthefourhottestmethods. Ifpro.lers agreeperfectlyinthepercentage \noftime spentinthe hot methods, then we expect all bars for a method to be the same height.Instead, we \nsee thatthebarsfor a methodcandiffer widely. For example, for the .rst method of luindex, the yourkit \nbar is at Figure 5. When a pair of pro.lers disagree, how is that disagree\u00admentdistributed? 55% while \nthe hprof and jpro.lebarsareat about15%.Inmany cases(e.g.,forthe .rstmethod of jython)weseethatonepro.ler \n.nds that the method consumes tens of percent of execution time while another pro.ler .nds that the method \nconsumes little or no executiontime. We also see no consistent agreement between pro.lers: e.g., hprof \nsometimes agrees and sometimesdisagrees with jpro.le.In otherwords,wecannotjustthrowout onepro.lerand \nexpectthat the remainingpro.lers will all agree with each other. To summarize, pro.lers disagree and \nattribute vastly different amountsoftimetothe samemethod.Becauseperformance analysts use the time spent \nin a method to decide whether or not they should optimizethe method,thisdisagreement can easily mislead \na performance analyst and resultin theperformance analyst wasting time optimizing a method that will \nhave little or no impact on performance. 4.4 Ispro.lerdisagreementinnocuous? InSection4.2.2wedemonstrated \nthatpro.lers oftendisagree as to which method is the hottest. In this section, we discuss how two pro.lers \ndisagree. In particular, if two pro.lers identify different methodsasthe hottest butthetwomethodsareinacaller-callee \nrelationship then the disagreement may not be problematic: it is often dif.cult to understand a method \ns performance without also considering its callers and callees and thus a performance analyst willprobably \nend uplooking atboth methods withbothpro.lers. Figure 5 categorizes pro.ler disagreement to determine \nif the caller-callee relationship accounts for most of the disagreement betweenpro.lers.Agivenpair ofpro.lers \nmay(a) agree onthe hottest method( agree );(b) disagree on thehottest methodbut thehottest methoddue \nto onepro.leris a transitive caller or callee of thehottest methodduetothe otherpro.ler( caller/callee \n); or (c)disagree on thehottestmethod and thehottest method returned by one pro.ler does not call (directly \nor transitively) the hottest method returnedby the otherpro.ler( disagree ).Eachbar cat\u00adegorizes the \nagreement for all pro.ler pairs for one benchmark; ` 4 \u00b4 because there are six( 2 )possiblepairings \noffourpro.lers, each bargoes to6. From Figure 2, we know that all four pro.lers agree on the hottest \nmethod for benchmarks bloat, chart, and fop, and thus all pro.ler pairs for these benchmarks fall in \nthe agree cate\u00adgory. However, for the four benchmarks where the pro.lers dis\u00adagree, only onebenchmark, \nluindex has a caller/callee relationship between the hottest methods identi.ed by the different pro.lers. \nThree out of the four times when a pair of pro.lers disagree their hottestmethods are notin a(transitive) \ncaller/callee relationship. % of total time in method 70 60 50 40 30 20 10 0   antlrantlrantlrantlrbloatbloatbloatbloatchartchartchartchartfopfopfopfopjythonjythonjythonjythonluindexluindexluindexluindexpmdpmdpmdpmd \nFigure4. HOTNESSpm forp beingone of thefourpro.lers and m beingone of thehottestfour methods. In summary, \npro.ler disagreement is not innocuous: when two pro.lersdisagree one, orbothof them maybe totallyincorrect. \n 4.5 IstheJVM thecauseofpro.lerdisagreement? In Figure 2, we quanti.ed pro.ler agreement on Sun s Hotspot \nproduction JVM. We also repeated the same set of experiments using IBM s J9 production JVM. Because J9 \ndoes not ship with xprof, we used threepro.lersforJ9instead offour. For J9 we also found the same kinds \nof pro.ler disagreement as withHotspot.For example, acrossthesevenbenchmarks,there were only two benchmarks \n(fop and luindex) where the three pro.lers agreedon thehottest method.Thus,pro.lerdisagreement is not \nan artifact of a particular JVM we have encountered it on twoproductionJVMs.  4.6 Summary Wehave shownthatpro.lerdisagreementis(i) \nsigni.cant four state of the artJavapro.lers oftendisagree with each other and(ii) pervasive occurringformanyofoursevenbenchmarksandintwo \nproduction Java virtual machines. Because pro.ler disagreement implies incorrect pro.les, this problem \nis serious: a performance analyst may waste time and effort optimizing a cold method that haslittle or \nnoimpact on overallprogramperformance. 5. Causality analysis In the previous section, we used pro.ler \ndisagreement to identify that at least one of the pro.lers generates incorrect pro.les. How\u00adever, pro.ler \ndisagreement does not tell us if any of the pro.lers produce actionablepro.les.We use causality analysis[21]tode\u00adtermineif \napro.leis actionable. Causality analysis,in our context,proceedsin three steps: Intervene: We transform \na method, M, to change the time spent in M. The transformation may take the form of code changes or changes \nto some parameters that affect the performance of M.For example, we may change the algorithmin a method \nor askit to use adifferent seedfor a random numbergenerator. Pro.le: We measure the change in execution \ntime for M and for theentireprogram.We useapro.lertodeterminethetime spent in M before and after the \nintervention. We use a lightweight approach(e.g.,the time UNIXcommand)todeterminethetime spentin the \noriginal andintervenedprogram. Validate: If the pro.les are actionable, then the change in the executiontimefor \nMshould equal the changein the execution time of theprogram. There are two signi.cant dif.culties with \nthis approach. First, the most obvious intervention is to optimize a hot method. How\u00adever,itis not always \neasy to speed up a method;it may be that the method already uses the most clever approach that we can \nimag\u00adine.This section exploits a keyinsight toget around thisproblem: slowingdown a methodis often easier \nthan speeding up a method. Ifthepro.les are actionable,theyshould attributethe slowdownin theprogram \nto the slowdownin the method. Second,thegoalof ourinterventionisto affecttheperformance of aparticular \nmethod;howeverdueto memory system effects, our interventionmayalso affecttheperformance of othermethods[19]. \nWetaketwoprecautionsto avoidthese unintended effects:(i) In this section, we limit interventions to changes \nin parameters; thus the memory layout for the method s code before and after the intervention is the \nsame. This ensures that our intervention is not impacting performance due to a change in the program \ns memory layout,a change wedidnotintend.(ii)We useinterventionsthat are simple(e.g., affect only computation \nand not memory operations). This ensures thatourinterventiondoes notinteract with otherparts of theprogramin \na way wedid notintend. 5.1 Theinterveneandpro.lesteps In this section, we use automatic interventions \ndesigned to slow down a program; in Section 7.3 we explore manual interventions designedto speed up aprogram. \nWe use a Java agent that uses BCEL (a bytecode re-writing library) to inject the intervention code into \nthe program. For the data in this section, we insert a while loop that calculates the sum of the .rst \nf Fibonacci numbers, where f is a parameter we specify in a con.guration .le. We use a Fibonacci computation \nfor two reasons.First,Fibonaccihas a small memoryfootprint and does notdo anyheap allocation.This simpli.es \nthe validation step because wedo not need to concern ourselves with memory system effects. Second, we \ncan easily change the amount of slow down weinduce(i.e.,theintervention)by altering the value of f . \nThus, we can change the execution time of a program and see how that affectstheprogram spro.le;speci.cally, \nwe can check whetherthe pro.ler reportsthe changeinprogram executiontimetothe method containing the Fibonacci \ncode. Section 7 explores the effects of injectinga memory-bound computationin the code. For each benchmark, \nwe randomly picked two hot methods from the top-10 hottest methods and inject the Fibonacci code. The \nsecond column in Table 2 identi.es the two hot methods we used.For each experiment, we usethe methodologyfromSection3 \nwith the exception that we conducted .ve runs per experiment instead of 30(which we use everywhere else \nin thepaper) tokeep experimentation time manageable. time in method (sec) 300 250 200 150 100 50  overall \nruntime (sec) (a) ByteBuffer.append inchart 0 20 40 60 80 100 120 overall runtime (sec) (b) PyFrame.setlocal \ninjython Figure 6. Does application slow down match slow down of the method containing theFibonacciloop. \nWe couldalsohave repeated this experiment using sleep topre\u00adcisely controlhow much slowdown weintroduceintotheprogram. \nHowever, sleep does not work in our context: when a program is sleeping,pro.lersdo not attributethe sleeping \ntime to theprogram becauseitis notactually running.  5.2 The validate step By injecting Fibonacci code \ninto a method M we slow down a programbya .xed amount.Anactionablepro.lershould attribute most,if not \nall, of that slowdownto method M.Inthis section, we demonstrate thatpro.lers rarelyproduce actionablepro.les. \nFigure 6 gives the results of our experimentsfor two methods: the top graphgives data for the ByteBuffer.append \nmethod from the chart benchmark and the bottom graph gives data for the PyFrame.setlocal method from \nthe jython benchmark. There is one set of points for each pro.ler; the line through the points is a linear \n.t of thepoints.Wewereunabletoconductthisexperiment for theyourkit pro.lerbecauseitdoesits ownbytecode \nre-writing which con.icts with our methodforinjecting theFibonacci code. The leftmostpointisfor f = 100;each \nsubsequentpointadds 200 to f .Apoint (x, y) on alineforpro.ler, P, saysthat whenthe overall execution \ntime of theprogramis x seconds, P attributed y seconds of execution time to the method with theFibonacci \ncode. Intheperfectcase, we expect eachpro.ler slinetobe a straight line with a slope of 1.0: i.e., we \nexpect the increase in execution timefor theprogram to exactly match theincreasein the execution time \nfor the method containing the Fibonacci code. The farther a pro.ler s slope is from 1.0 the less actionable \nis the pro.le. To Benchmark Method slope hprof xprof jpro.le antlr CharBuffer..ll 0.45 0.24 -0.05 CharQueue.elementAt \n0.04 0.00 0.11 bloat PrintWriter.print 0.00 0.13 0.00 PrintWriter.write 0.42 0.23 0.39 chart ByteBuffer.append \n0.94 0.93 0.65 ByteBuffer.append i 0.00 0.00 0.00 fop PropertyList..ndMaker 0.00 0.00 0.00 PropertyList..ndProperty \n0.00 0.00 0.01 jython PyType.fromClass 0.22 0.52 0.55 PyFrame.setlocal 0.00 0.00 0.00 luindex jjCheckNAddTwoStates \n0.97 1.20 0.99 StandardTokenizer.next 0.00 0.00 0.00 pmd NodeIterator.getFirstChild 0.66 0.90 0.82 JavaParser.jj \nscan token 0.00 0.00 0.00 mean across methods 0.26 0.28 0.23 Table2. Slopefrom thelinear regressionforFibonacciinjection \n make it easy to see this, we have included an actionable line which has a slope of 1.0. In addition, \nthe numbers in the legend givethe slope ofeachline obtained using alinear regression on the data. For \ntheByteBuffer.append methodfrom the chart benchmark in the topgraph, the hprof and xprof pro.ler lines \nhave slopes of 0.94 and 0.93, respectively, thus,for this method, hprof and xprof perform reasonablywell.However, \njpro.lehas a slope of0.65 and thus,itdoes notperform as well. For the PyFrame.setlocal methodfrom the \njython benchmark in the bottom graph, all three pro.ler lines have a slope close to 0, indicating that \nthe pro.lers do not detect any change in the execution time of the method containing the Fibonacci as \nwe change n.Therefore, none ofthe threepro.lersproduce actionable datafor this method. Table 2 gives \nthe slopes for all benchmarks and pro.ler pairs. Thelastrowgivesthe averagefor eachpro.ler.From this \ntable we see that the slopes are rarely 1.0; in other words, except for rare cases, none of the threepro.lersproduce \nactionabledata. 6. Understanding thecauseofpro.ler disagreement Section 4 demonstrates that four state-of-the-art \nJava pro.lers of\u00adten disagree with each other and Section 5 demonstrates that the four state-of-the-art \nJava pro.lers rarely produce actionable data. This section explores the reason why pro.lers are producing \nnon\u00adactionablepro.les and whypro.lersdisagree. 6.1 Theassumptionbehind sampling The four state-of-the-art \nJava pro.lers explored in this paper all use sampling to collect pro.les. Pro.lers commonly use sampling \nto collectdatabecause ofitslow overhead.However,for sampling toproduce resultsthat are comparableto afull(unsampled)pro.le, \nthefollowingtwo conditions musthold. First,we musthavealargenumber ofsamplestoget statistically signi.cant \nresults. For example, if a pro.ler collects only a single sampleintheentireprogramrun,thepro.ler will \nassign100% of theprogram execution time to the codein whichit tookits sample and 0% to everything else. \nTo ensure that we were not suffering from an inadequate number of samples, we made sure that all of our \nbenchmarks were long running; the shortest benchmark ran for 21.02 seconds(Table1), which at a sampling \ninterval of 10ms (whichwe used), weget about 2, 100 samples. Second, the pro.ler should sample all points \nin a program run with equal probability. If a pro.ler does not do so, it will end up  Figure7. Autocorrelationforjython \nusing random sampling. withbiasinitspro.le.For example,let ssupposeourpro.lercan only sample methods \nthat contain calls. Thispro.ler will attribute no execution time to methods thatdo not contain calls \neven though they may accountfor much of theprogram s execution time.  6.2 Doourpro.lerspick samplesrandomly? \nBecause we were carefulto satisfythe .rstcondition(by usinglong runs)we suspectedthatthepro.lerswereproducingnon-actionable \npro.lesbecause theydidnot satisfy the second condition. One statically sound method for collecting random \nsamples is to collect a sample at every t + r milliseconds, where t is the de\u00adsired samplinginterval \nand r is a random numberbetween -t and t. One might think that sampling every t secondsis enough(i.e., \ndrop the r component) but it is not: speci.cally, if a pro.ler sam\u00adples everyt seconds,the sampling rate \nwouldbe synchronized with any program or system activity that occurs at regular time inter\u00advals[17].For \nexample,if the thread scheduler switchesbetween threads every 10ms and our samplinginterval was also \n10ms, then we may always take samples immediately after a thread switch. Because performance is often \ndifferent immediately after a thread switchthan at otherpoints(e.g.,dueto cache andTLB warm-up ef\u00adfects)we \nwouldgetbiaseddata.The random component, r,guards against such situations. In order to investigate whether \nr impacts our pro.les, we used a debug version of Hotspot to record a timestamp whenever the JVM services \na sample on the behalf of a pro.ler. This results in a time-series of timestamps thatdenote when apro.ler \nsamples an application. Figure 7 givestheautocorrelation[15]graphforwhen wetake samples with a random \ncomponent r. Intuitively, autocorrelation determinesifthereis a correlationbetween a sequence ofsampling \nintervalsat onepointintheexecutionand anotherpointintheex\u00adecution.More concretely,if theprogram runproduces \na sequence, (x1,x2 , ..., xn), of sampling intervals, Figure 7 plots the correla\u00adtion ofthe sequence(x1,x2 \n, ..., xn-k ) with (xk ,xk+1, ..., xn),for different values of k (k is often called the lag ). Because \ncorre\u00adlation produces a value in the range [-1, 1], the autocorrelation graphsalsorangefrom -1 to1(wehavetruncated \nthey-axisrange to make thepatterns more obvious). As expected, the autocorrelation graph in Figure 7, \nwhen we take samples randomly from all points in the program run, looks random. In contrast, consider \nthe correlation graph for hprof (Figure8).5 Itexhibits a systematicpatternimplyingthat sampling 5The \nautocorrelationgraphsfortheotherpro.lers aresimilar and thuswe omitthemforspace considerations. 0 500 \n1000 1500 2000 lag of series Figure8. Autocorrelationforjython using hprof. intervals at onepointin \ntheprogram runpartiallypredict sampling intervals atalaterpoint;thusthe samples are not randomlypicked. \nIn summary,the autocorrelationgraphfor ourpro.lerslookdif\u00adferentfromthe autocorrelationgraphfor randomlypickedsamples. \nThus, our pro.lers are not using random samples, which is a re\u00adquirementforgettingcorrect resultsfrom \nsampling.The remainder of this section explores the cause of this samplingbias. 6.3 What makes the samples \nnot random? To understand why our pro.lers were not randomly picking sam\u00adples from the program run, we \ntook a closer look at their imple\u00admentation.We determined that all fourpro.lers take samples only atyieldpoints[1].More \nspeci.cally, when apro.ler wishestotake a sample,it waitsfortheprogram s executionto reach ayieldpoint. \nYield points are a mechanism for supporting quasi-preemptive thread scheduling; they areprogramlocationswhereitis \nsafe to run agarbage collector(e.g., all theGC tables arein a consistent state[7]).Becauseyieldpoints \nare notfree, compilers often opti\u00admizetheirplacement.Forexample,aslongas applicationcodedoes not allocate \nmemory anddoes not runfor an unbounded amount of time, the JVM can delay garbage collection until after \nthe appli\u00adcation code .nishes; thus, a compiler may omit yield points from a loop if it can establish \nthat the loop will not do any allocation and will not runinde.nitely.This clearly con.icts with thegoal \nof pro.lers; in the worst case, thepro.ler may wish to take a sample in a hot loop, but because that \nloop does not have a yield point, thepro.ler actually takes a sample sometime afterthe execution of the \nloop. Thus, the sample may be incorrectly attributed to some method other than the one containingtheloop. \nListing1demonstratesthisproblem.Thehot method accounts for most of the execution time of thisprogram \nand cold accounts for almostnone ofthe executiontime.6 Becausehot does nothave anydynamic allocation \nand runsfor abounded amount oftime,the compilerdoes notput ayieldpointinit.Thereis,however, ayield pointincold,becauseit \ncontains a call(compilers conservatively assume that a call may eventually lead to memory allocation \nor recursion).Thus, the cold methodincorrectlygets all the samples meant for the hot method, resulting \nin a non-actionable pro.le. Indeed, thexprof pro.ler attributes99.8%of the execution timeto the cold \nmethod. In the above example a yield point-based pro.ler incorrectly attributes a callee s sample to \na caller. The problem is actually much worse: JIT compilers aggressively optimize the placement 6The \nkey thing about the hot method is that it is expensive compared to cold and does not have calls or loops. \nWe included this code so you can try it out yourself! We created this example from a similar situation \nwe encounteredin antlr. stati c int [] array = new int [1024]; public st ati c void hot (int i) { int \nii =(i +10 * 100) % array .length; int jj =(ii +i/33)%array .length; if (ii < 0)ii = -ii; if (jj < 0)jj \n= -jj ; array[ ii] = array[jj] + 1; } public st ati c void cold () { for (int i =0; i < Integer .MAXVALUE; \ni++) hot( i); } } Listing 1. Code that demonstrates the problem with using yield pointsfor sampling 70 \n hprof jprofile60 yourkit Figure 9 illustrates how turning on different pro.lers changes xprof spro.le \nof aprogram. Thegraph in Figure 9 has one set of bars for each benchmark and each set has one bar for \neach of the hprof, jpro.le, and yourkit pro.lers(avg of30 runs).Theheight of the bar quanti.es the pro.ler \ns effect on xprof s pro.le for the hottestmethod, M.If xprof attributes x% of execution time to M whenno \notherpro.lerisrunning andy% of executiontimeto M when a pro.ler, P, is also running, then P s bar will \nhave height abs(x -y), where abs computes the absolute value. From this graph we see that pro.lers signi.cantly \nand differ\u00adently affect the time spent in the hottest method (according to xprof).The observer effect \ncausedbythedifferentpro.lersin.u\u00adences where the JIT places yield points. To quantify the observer effect, \nwe used a debug build of Hotspot to count the number of yieldpointstheJITplacesin a method.For example, \nwhen wepro\u00ad.le with xprof, the JIT placed 9 yield points per method for the hottest10 methods of antlr, \non average. When we used hprof, the JITplaced7yieldpointsper method. Although the data in Figure 9 illustrates \nhow xprof s pro.les change when other pro.lers simultaneously collect pro.les, we see similar behavior \nwhen xprof is replaced by one of the other pro.lers. abs(x - y) 50 40 30 20 10 0 In summary, the observer \neffect due to pro.lers affects opti\u00ad mization decisions, which affects the placement of yield points, \nwhichin turn resultsindifferentbiasesfordifferentpro.lers. 7. Testing ourhypotheses Theprevious sectionshypothesizedthat \nourpro.lersproduce non\u00ad antlrbloatchartfopjythonluindexpmdmean Figure9. The observer effectdue topro.lers \nof yield points and unrelated optimizations (e.g., inlining) may also affect the placement of yield points. \nConsequently, a pro.ler may attribute a method s samples to another seemingly unrelated method.  6.4 \nBut whydopro.lersdisagree? Whiletheabovediscussionexplainswhyourpro.lersproducenon\u00adactionablepro.les,itdoes \nnot explain whytheydisagree with each other.If thepro.lers all useyieldpointsfor sampling, they should \nall be biased in the same way and thus produce the same non\u00adactionable data. This section shows that \ndifferentpro.lersinteract differently with dynamic optimizations, which results in pro.ler disagreement. \nAny pro.ler, by its mere presence (e.g. due to its effect on memory layout, or because it launches some \nbackground threads), changes thebehavior oftheprogram(observer effect).Becausedif\u00adferent pro.lers have \ndifferent memory requirements and may per\u00adformdifferent background activities, the effect onprogrambehav\u00adiordiffersbetweenpro.lers.Becauseprogrambehavior \naffects the virtual machine sdynamic optimizationdecisions, using adifferent pro.ler canleadtodifferencesin \nthe compiled code. These differences relate to pro.ler disagreement in two ways: (i)directly, because \nthe presence of different pro.lers causes dif\u00adferentlyoptimized code, and(ii)indirectly,becausethepresence \nof differentpro.lerscausesdifferentlyplacedyieldpoints. While(i) directlyaffects theperformance of theprogram,(ii)does \nnot affect program performance, but it affects the location of the probes that measureperformance.Our \nresultsinSection7 suggest that(ii) contributes more signi.cantly todisagreement than(i). actionablepro.lesbecause(i) \nthey sample atyieldpoints which biasestheirpro.les and(ii) theyinteract with compiler optimiza\u00adtionswhichaffectsbothprogramperformance \nandtheplacement of yieldpoints.Thissectionpresentsresultsfrom aproof-of-concept pro.ler that does not \nuse yield points and shows that this pro.ler produces actionablepro.les. 7.1 Implementation Our proof-of-concept \npro.ler, tprof, collects samples randomly using a t of 10ms and r being uniform random numbers between \n-3ms and 3ms (Section 6.2). tprof has two components: (i) a sampling threadthat sleepsforthe samplinginterval(determined \nbyadding t and a random number, r,for each sample) andthen uses standardUNIXsignalstopausetheJava applicationthreadandtake \nasampleof thecurrent executing method; and(ii) aJVMTI agent that builds a map of an x86 code address \nto Java methods so that tprof can map the samplesback toJava code. We encountered three challengesinimplementing \ntprof. First, the JIT may recompile methods and discard previously compiled versions of a method, therefore \na single map from Java code to x86 instructions is not enough. Instead, we have different maps at differentpoints \nin theprogram execution and the samples alsohave a timestamp so tprof knows which map to use. Second, \ntprof operates outside of the JVM and therefore it does not know which method is executing when it samples \nan interpreted method.As a consequence, tprof attributes all samples of interpreted code into a special \ninterpreted method . This is a source of inaccuracy in tprof; we believe this inaccuracy will be insigni.cantexceptfor \nshort-runningprogramsthat spend much of their timein theinterpreter. Third,SunHotspotdoes notaccurately \nreport methodlocations when inlining is turned on. Thus, we cannot reliably use tprof if inliningis enabled. \nThe latter two limitations are implementation artifacts and not a limitation of pro.lers that use random \nsampling. Indeed, tprof is not meant to be a production pro.ler; its purpose is to support Table 3. Slope \nfrom the linear regression for Fibonacci injection (noinlining) Bmark Method slope tprof hprof xprof \njprof antlr CharBuffer..ll 1.00 0.78 0.78 0.78 CharQueue.elementAt 1.00 0.00 0.00 0.00 bloat PrintWriter.print \n1.00 0.01 0.00 0.00 PrintWriter.write 0.88 0.56 0.23 0.49 chart ByteBuffer.append 0.99 0.91 0.89 0.65 \nByteBuffer.append i 0.99 0.00 0.00 0.00 fop PropertyList..ndMaker 0.97 0.00 0.00 0.00 PropertyList..ndProperty \n1.00 0.00 0.00 0.01 jython PyType.fromClass 0.99 0.00 0.00 0.00 PyFrame.setlocal 0.99 0.00 0.00 0.00 \nluindex jjCheckNAddTwoStates 0.99 0.97 0.97 0.98 StandardTokenizer.next 1.00 0.01 0.01 0.01 pmd NodeIterator.getFirstChild \n1.00 0.79 0.75 0.87 JavaParser.jj scan token 1.00 0.01 0.00 0.00 mean across methods 0.99 0.29 0.26 0.27 \n and validate our claim that a Java pro.ler can produce actionable pro.lesbyensuringits samples are taken \nrandomly.  7.2 Evaluating tprof with automatic causality analysis From Table 2 we know that hprof, xprof, \nand jpro.ledo notpro\u00adduce actionablepro.les;speci.cally,theydo not correctly attribute the increase in \nprogram execution time to the increase in the time spent computing the Fibonacci sequence. We now evaluate \ntprof using the same methodology. Table3is similartoTable2 exceptthat(i) itincludesdatafor tprof along \nwith hprof, jpro.le, and xprof;and(ii) wedisabledin\u00adliningin theJVM when collectingdatafor this table(Section7.1). \nFirst we notice that hprof, xprof, and jpro.le all perform slightly better without inlining than with \ninlining; Section 6.4 ex\u00adplains the reason for this. However, even with inlining disabled, thesepro.lers \nusuallyproduce non-actionabledata. From the tprof column we see that tprof performs nearly per\u00adfectly:itcorrectly \nattributestheincreaseinprogram executiontime to an increase in the time spent in the method that calculates \nthe Fibonacci sequence. To increase thegenerality of our results, we repeated the above experiment, this \ntime injecting a different computation. Speci.\u00adcally, weinjected code that allocates an array of1024integers \nand loops over a computationthataddstwo randomly selected elements of the array. The motivation behind \nthis computation rather than Fibonacci is to demonstrate that our results are not speci.c to oneparticulartype \nofinjected computation.By allocating memory, ourinjected codehas side effects that mayimpact other aspects \nof theruntimesystem(e.g.garbagecollection,compilation, ...) andin turn may affect whether apro.leris \nactionable.Despite these side effects, once again wefound that otherpro.lersdidpoorly(with slopes ranging \nfrom 0.18 to 0.37) while tprof performed nearly perfectly(withslope of1.02)7. We had posed three hypotheses \nfor explaining non-actionable datafromthepro.lers:(i)reliance onyieldpoints whichledtobias (Section6.3),(ii)interactions \nwithoptimizations whichdirectly af\u00adfectedpro.les(Section6.4), and(iii) interactions with optimiza\u00adtions \nwhich affected the placement of yield points and thus bias (Section 6.4). Our results indicate thattprof, \nwhich addresses(i) and(iii)(but not(ii)),performs nearlyperfectly. 7Wehave omitted thefull set of resultsdue \nto spacelimitations.  7.3 Evaluatingtprof with real case studies The previous section used causality \nanalysis with synthetic inter\u00adventions to evaluate the bene.t of tprof s sampling strategy com\u00adpared \nto hprof s, xprof s, and jpro.le s sampling strategy. This section uses realistic interventions instead \nof the synthetic inter\u00adventionsto make the same comparison. 7.3.1 Speeding up pmd by52% tprof reported \nthat java.util.HashMap.transfer was the hottest method accounting for about 20% of overall execution \ntime of pmd.In contrast, xprof reportedthatthe methodtook up no execu\u00adtion time and the otherpro.lers(hprof, \njpro.le, yourkit)reported that three other methods werehotter than this method. Oninvestigation, wefoundthat \npmd creates a HashMap using HashMap s default constructor and then adds many elements to the HashMap. \nThese addition cause the HashMap to repeatedly resize its internal table, each time transferring the \ncontents from the smaller table to the larger table. Based on tprof s report, we changed the HashMap \nallocation to use the non-default construc\u00adtor whichpre-allocated100K entriesfor the table, thusdecreasing \nthe number of timesithas to resize the table. This one line code change sped up the program by 52% with \ninlining(i.e., thedefaultcon.gurationfortheJVM) and47% with\u00adout inlining8. These performance improvements \nactually exceed tprof spredictionof20%.We expectthisisbecause reducing the resizings also reduced the \namount of memory allocation. This in turn translated tobetter memory system andgarbage collectorper\u00adformance. \n 7.3.2 Speeding up bloat by50% tprof reported that java.util.AbstractMap.containsValue was the hottest \nmethod accounting for 45% of program execution time in bloat.The otherpro.lersreportedthat AbstractMap.containsValue \ntook up 22% of program execution time and reported that other methods werehotter. On investigation we \nfound that bloat frequently calls Ab\u00adstractMap.containsValue aspartof an assertion. AbstractMap.\u00adcontainsValue \ndoes a linear search through the values of a map and thus takes time proportional to the number of values \nin that map.We removed this callby commenting outthe assert statement (thisdoes not affectbehavior oftheprogram,justthe \nchecking of programinvariants atrun time). As a result of this change, bloat sped up by 50% with inlining \nand 47% without inlining. tprof immediately directed us to this method as the slowest method and even \npredicted the speedup wegot(within2%). If wehadfollowedthe advice oftheother pro.lers, we would still \nhave found this methodbut notbefore we hadlookedat several othermethods .rst.  7.4 Summary Using a combination \nof synthetic and real causality analysis, we have demonstrated that a proof-of-concept pro.ler, tprof, \nwhich uses random sampling,produces actionabledata. 8. Related work We now reviewprior workon evaluatingpro.lers \nand the common approaches used toimplementpro.lers. 8We report the performance improvements with both \nthe default JVM con.guration and the one with inlining disabled because we collected the pro.les with \ninlining disabled; recall that tprof cannot currently handle inlining. 8.1 Evaluatingpro.lers If weknow \nthe correctpro.le, we canprecisely evaluate the accu\u00adracy of apro.ler.For example, callfrequenciesfor \nadeterministic program are apropertyoftheprogram anditsinputs and should not depend on thepro.ler.Arnold \nandGrove[2] exploitthisinsight to evaluatetheirlightweight(but notperfectly accurate) method for measuring \ncall frequencies. Other papers use similar insights to evaluatetheirpro.lers(e.g.,[9,18]).Unfortunately,fortiming \ndata(which wefocuson) thereisno correct pro.le; thisiswhy we need to resort to actionable. If we have \na particular use for a pro.ler in mind, then we can evaluate apro.ler with respecttohowthepro.ler supportsthat \nuse. For example,Rubinet al. [22]evaluate apro.lerby the amount of speedup theyget fromdatalayout optimizations.Thisis \naform of causality analysis, whichis one of the techniques that we also use. In work concurrent to ours \nChen et al. [6] .nd that sampling with hardware performance monitors often produces biased pro\u00ad.les.Indeed,intheirwork,they.ndthat \nrandomizingthesampling period of theirhardwarepro.lerallowsthemtoproducemoreac\u00adcurate pro.les. Like Arnold \nand Grove, Chen et al. compare the accuracy oftheir sampledpro.leto aperfectpro.le obtained using expensiveinstrumentation. \nWhileChen et al. evaluatepro.lersthat collect edge or basic block pro.les, our work focuses on pro.lers \nthatproduce timingdata. Finally, Whaley evaluates a pro.ler based on whether or not different runs of \nthe samepro.ler agree with each other[25].This approach determines whether or not a pro.ler is consistent \nwith itself, but does not say anything about pro.ler accuracy: e.g., a pro.ler that consistentlyproducesincorrect \nresults will score high according to Whaley s criteria. 8.2 Implementationofpro.lers Broadly speaking, \npro.lers work by either instrumenting the code orby sampling. For example,Dmitriev spro.ler[8], as well \nas atleast some versions of theNetbeans andEclipseTPTCpro.lers[20,10],in\u00adstrumenttheprogram.This approachtypicallyyieldshugeprogram \nslow down; in our experience, we get 1000x slow down if we in\u00adstrument all methods. For this reason, \nthese pro.lers only pro.le methods that the user explicitly speci.es, thus reducing their over\u00adheadto \na more reasonablelevel.In ourexperience,thesetechniques suffer signi.cantly from the observer effect \nand thus we did not consider thesepro.lersin our study. For example,gprof[13]uses samplingforC andC++programs. \nMoreover, unlike the Java pro.lers we considered, gprof uses an OS timer togenerate samples every 10ms \nand thusdoes not suffer from the biased samples that we get when we use yield points.9 Implementing a \ntimer-basedpro.leris much easierforC andC++ than it is for Java; as we discussed in Section 7.1, Java \nintroduces many challenges(e.g.,dynamicallygenerated code) that makesit dif.cult to map samples back \nto user code. Perhaps this is the reason why all the Java pro.lers we know of use yield points for sampling. \nNevertheless, this paper shows how we can collect random samples evenforJavaprograms.  8.3 Improving \nthestate-of-the-artinperformanceanalysis There has been much work on improving experimental methodol\u00adogyin \nvarious areas of computer science. Blackburn et al. [4] .nd that the prevalent methodology for evaluating \ngarbage collectors is misleading. Speci.cally, using a singleheap size often biases the evaluation of \nagarbage collector; 9Gprof does not randomize its sampling interval which could bias its results. thusBlackburn \net al. suggest using manyheap sizesinstead ofjust one. Georges et al. [12]demonstrate the importance \nof using statis\u00adtical methods for interpreting performance results. Mytkowicz et al. [19]show how seeminglyinnocuousfactors(e.g., \nenvironment variables) canbiasperformance results. Buytaert et al. [5] is the closest paper to ours. \nIt makes the point that yield points are a poor choice for sampling in pro.ler\u00adguided optimizers(which \nare commoninJava virtual machines). The reasons theygivefor this are consistent to ours.Buytaertet al. \nalso compare the accuracy of their HPM-based pro.ler to a base\u00adlinepro.ler which uses thehardwareperformance \nmonitor(HPM) to trigger sampling at a high rate. Our paper complements Buy\u00adtaert spaperby(i) we show \nthatyieldpoints severely compromise theresultsproducedby commonly-usedJavapro.lersintwopro\u00adduction virtual \nmachines; Buytaert et al. use JikesRVM for their experimentation anddo not experiment withdifferentpro.lers;(ii) \nwe use causality analysis todirectly evaluate the correctness of the pro.lers; Buytaert et al. s evaluation \nof pro.ler accuracy assumes that a pro.ler with a high-sampling rate using HPM is correct , which may \nor may notbethe case; and(iii) we showthatthe ob\u00adserver effect due topro.lersleads todisagreement betweenpro.l\u00aders. \nIn summary,theabovepapersallattempttoimproveexperimen\u00adtal methodology in computer science. This paper \nshares the same goalsby tackling adifferentpro.lingproblemthanthe earlierpa\u00adpers. 9. Conclusion What \ndo we do when our program has a performance problem? We use a pro.ler to .nd the hot methods in the program \nand then optimize these methods to speed up the program. If the program does not speed up as predicted \nby the pro.le, we typically blame it on a poor interaction with the memory system or our lack of understanding \nof the underlyinghardware,but we neverblame the pro.ler. In this paper, we surprisingly demonstrate that \nfour state\u00adof-the-artJavapro.lers(xprof, hprof, jpro.le, and yourkit)often produceincorrectpro.les. We \nuse causality analysis to determine two reasons for why the four pro.lers produce incorrect pro.les. \nFirst, the pro.lers only sample at yield points, a JVM mechanism for supporting maintenance operations \nsuch as garbage collection. Only taking samples at yield points introduces bias into a pro.le. Second, \nthe pro.lersperturbtheprogrambeing optimized(i.e. observer effect) and thus changehow thedynamic compiler \noptimizes theprogram andplacesyieldpointsin the optimized code. Our results aredisturbingbecause theyindicate \nthatpro.lerin\u00adcorrectness is pervasive occurring in most of our seven bench\u00admarks and in two production \nJVM -and signi.cant all four of the state-of-the-art pro.lers produce incorrect pro.les. Incorrect pro.les \ncan easily cause aperformance analyst to spend time opti\u00admizing cold methodsthat willhave minimal effect \nonperformance. We show that a proof-of-concept pro.ler that does not use yield pointsfor samplingdoes \nnotsufferfrom the aboveproblems. Acknowledgments Thanks to Devin Coughlin, Michael Hind, Robert Hundt, \nTipp Mosely,RhondaHoenigman,DickSites, andManishVachharajani for thoughtfuldiscussions andfeedback on \nthis work.This workis supported by NSF grant NSF CSE-0509521. Any opinions, .nd\u00adings and conclusions \nor recommendations expressed in this ma\u00adterial are the authors and do not necessarily re.ect those of \nthe sponsors. References [1] B.Alpern,C.R.Attanasio,J.J.Barton,M.G.Burke,P.Cheng,J.-D. Choi, A. Cocchi, \nS. J. Fink, D. Grove, M. Hind, S. F. Hummel, D. Lieber, V. Litvinov, M. F. Mergen, T. Ngo, J. R. Russell, \nV.Sarkar,M.J.Serrano,J.C.Shepherd,S.E.Smith,V.C.Sreedhar, H. Srinivasan, and J. Whaley. The Jalape \n no virtual machine. IBM Systems Journal,39(1):211 238, February2000. [2] M. Arnold and D. Grove. Collecting \nand exploiting high-accuracy callgraphpro.lesin virtual machines. In Proc. of Int l Symposium on Code \nGeneration and Optimization,pages 51 62, Los Alamos, CA, March2005.IEEEComputerSociety. [3] S. M. Blackburn, \nR. Garner, C. Hoffman, A. M. Khan, K. S. McKinley, R. Bentzur, A. Diwan, D. Feinberg, D. Frampton, S. \nZ. Guyer, M. Hirzel, A. Hosking, M. Jump, H. Lee, J. E. B. Moss, A. Phansalkar, D. Stefanovi\u00b4c, T. VanDrunen, \nD. von Dincklage, and B. Wiedermann. The DaCapo benchmarks: Java benchmarking development and analysis. \nIn Proc. of ACM SIGPLAN Conf. on Object-Oriented Programing, Systems, Languages, and Applications, pages169 \n190,Portland,OR,Oct.2006.ACM. [4] S.M. Blackburn, P. Cheng, and K.S. Mckinley. Myths and realities: The \nperformance impact of garbage collection. In Proc. of ACM SIMETRICS Conf. on Measurement and Modeling \nComputer Systems, pages25 36,NewYork,NY,Jan.2004.ACM. [5] D. Buytaert, A. Georges, M. Hind, M. Arnold, \nL. Eeckhout, and K. De Bosschere. Using HPM-sampling to drive dynamic compilation. In Proc. of ACM SIGPLAN \nConf. on Object-Oriented Programing, Systems, Languages, and Applications,pages553 568, Montreal,Canada,Oct.2007.ACM. \n[6] DehaoChen,NeilVachharajani, andRobertHundt.Taminghardware event samplesforfdo compilation. International \nSymposium on Code Generation and Optimization (CGO),2010. [7] A. Diwan, E. Moss, and R. Hudson. Compiler \nsupport for garbage collection in a statically typedlanguage. SIGPLAN Not.,27(7):273 282,1992. [8] M.Dmitriev. \nSelectivepro.ling ofJava applications using dynamic bytecode instrumentation. In Proc. of IEEE Int l \nSymposium on Performance Analysis of Systems and Software, pages 141 150, Washington,DC,March2004.IEEE. \n[9] E. Duesterwald and V. Bala. Software pro.ling for hot path prediction:lessis more. SIGPLAN Not.,35(11):202 \n211, 2000. [10] Eclipse:Opensourcejavapro.lerv4.6.1.http://www.eclipse.org/tptp/. [11] Ej technologies: \nCommercial java pro.ler. http://www.ej\u00adtechnologies.com/products/jpro.ler/overview.html. [12] A.Georges,D.Buytaert, \nandL.Eeckhout.StatisticallyrigorousJava performance evaluation. InProc. of ACM SIGPLAN Conf. on Object\u00adoriented \nProgramming, Systems, Languages and Applications,pages 57 76,Montreal,Canada,Oct.2007.ACM. [13] S.L. \nGraham, P.B. Kessler, and M.K. Mckusick. Gprof: A call graph executionpro.ler. In Proc. of ACM SIGPLAN \nSymposium on Compiler Construction,pages120 126,Boston,Mass.,1982.ACM. [14] D. Gu, C. Verbrugge, and \nE. Gagnon. Code layout as a source of noise in JVM performance. Studia Informatica Universalis, pages \n83 99,2004. [15] R. Hegger, H. Kantz, and T. Schreiber. Practical implementation of nonlinear time series \nmethods: The TISEAN package. Chaos, 9(2):413 435,1999. [16] Sam Kash Kachigan. Statistical Analysis: \nAn Interdisciplinary Introduction to Univariate &#38; Multivariate Methods. Radius Press, 1986. [17] \nS. Mccanne and C. Torek. A randomized sampling clock for CPU utilization estimation and code pro.ling. \nIn Proc. of the Winter USENIX Conf.,pages387 394,SanDiego,CA,Jan.1993. [18] T.Moseley,A.Shye,V.J.Reddi,D.Grunwald, \nandR.Peri. Shadow pro.ling: Hiding instrumentation costs with parallelism. In Proc. of Int l Symposium \non Code Generation and Optimization, pages 198 208,Washington,DC,March2007.IEEEComputerSociety. [19] \nT.Mytkowicz,A.Diwan,M.Hauswirth, andP.F.Sweeney.Producing wrongdatawithoutdoing anything obviously wrong!In \nProc. of Int l Conf. on Architectural Support for Programming Languages and Operating Systems, pages \n265 276, Washington, DC, March 2009. ACM. [20] Netbeans:Opensourcejavapro.ler.v6.7.http://pro.ler.netbeans.org/. \n[21] J. Pearl. Causality: Models, Reasoning, and Inference. Cambridge UniversityPress,1st edition,2000. \n[22] S. Rubin, R. Bod\u00b4ik, and T. Chilimbi. An ef.cient pro.le-analysis frameworkfordata-layout optimizations. \nSIGPLAN Not.,37(1):140 153,2002. [23] hprof: anopensourcejavapro.ler. http://java.sun.com/developer/ \ntechnicalArticles/Programming/HPROF.html [24] xprof: Internal pro.ler for hotspot. http://java.sun.com/docs/books/ \nperformance/1st edition/html/JPAppHotspot.fm.html. [25] J. Whaley. A portable sampling-based pro.ler \nfor java virtual machines. In Proc. of Conf. on Java Grande, pages 78 87, New York,NY,2000.ACM. [26] \nYourkit,llc:Commercialjavapro.ler.http://www.yourkit.com/.    \n\t\t\t", "proc_id": "1806596", "abstract": "<p>Performance analysts profile their programs to find methods that are worth optimizing: the \"hot\" methods. This paper shows that four commonly-used Java profilers (<i>xprof , hprof , jprofile, and yourkit</i>) often disagree on the identity of the hot methods. If two profilers disagree, at least one must be incorrect. Thus, there is a good chance that a profiler will mislead a performance analyst into wasting time optimizing a cold method with little or no performance improvement.</p> <p>This paper uses causality analysis to evaluate profilers and to gain insight into the source of their incorrectness. It shows that these profilers all violate a fundamental requirement for sampling based profilers: to be correct, a sampling-based profilermust collect samples randomly.</p> <p>We show that a proof-of-concept profiler, which collects samples randomly, does not suffer from the above problems. Specifically, we show, using a number of case studies, that our profiler correctly identifies methods that are important to optimize; in some cases other profilers report that these methods are cold and thus not worth optimizing.</p>", "authors": [{"name": "Todd Mytkowicz", "author_profile_id": "81100300026", "affiliation": "University of Colorado, Boulder, CO, USA", "person_id": "P2184542", "email_address": "", "orcid_id": ""}, {"name": "Amer Diwan", "author_profile_id": "81100202872", "affiliation": "University of Colorado, Boulder, CO, USA", "person_id": "P2184543", "email_address": "", "orcid_id": ""}, {"name": "Matthias Hauswirth", "author_profile_id": "81332503330", "affiliation": "University of Lugano, Lugano, Switzerland", "person_id": "P2184544", "email_address": "", "orcid_id": ""}, {"name": "Peter F. Sweeney", "author_profile_id": "81332530743", "affiliation": "IBM Research, Hawthorne, NY, USA", "person_id": "P2184545", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1806596.1806618", "year": "2010", "article_id": "1806618", "conference": "PLDI", "title": "Evaluating the accuracy of Java profilers", "url": "http://dl.acm.org/citation.cfm?id=1806618"}