{"article_publication_date": "06-05-2010", "fulltext": "\n Printing Floating-Point Numbers Quickly and Accurately with Integers Florian Loitsch Inria Sophia Antipolis \n2004 Route des Lucioles -BP 93 -06902 Sophia Antipolis Cedex forian.loitsch@inria.fr Abstract We present \nalgorithms for accurately converting .oating-point numbers to decimal representation. They are fast (up \nto 4 times faster than commonly used algorithms that use high-precision in\u00adtegers) and correct: any printed \nnumber will evaluate to the same number, when read again. Our algorithms are fast, because they require \nonly .xed-size integer arithmetic. The sole requirement for the integer type is that it has at least \ntwo more bits than the signi.cand of the .oating-point number. Hence, for IEEE 754 double-precision numbers \n(having a 53-bit signi.cand) an integer type with 55 bits is suf.cient. Moreover we show how to exploit \nadditional bits to improve the generated output. We present three algorithms with different properties: \nthe .rst algorithm is the most basic one, and does not take advantage of any extra bits. It simply shows \nhow to perform the binary-to-decimal transformation with the minimal number of bits. Our second al\u00adgorithm \nimproves on the .rst one by using the additional bits to produce a shorter (often the shortest) result. \nFinally we propose a third version that can be used when the shortest output is a requirement. The last \nalgorithm either pro\u00adduces optimal decimal representations (with respect to shortness and rounding) or \nrejects its input. For IEEE 754 double-precision numbers and 64-bit integers roughly 99.4% of all numbers \ncan be processed ef.ciently. The remaining 0.6% are rejected and need to be printed by a slower complete \nalgorithm. Categories and Subject Descriptors I.m [Computing Methodolo\u00adgies]: Miscellaneous General Terms \nAlgorithms Keywords .oating-point printing, dtoa 1. Introduction Printing .oating-point numbers has always \nbeen a challenge. The naive approach is not precise enough and yields incorrect results in many cases. \nThroughout the 1970s and 1980s many language libraries and in particular the printf function of most \nC libraries were known to produce wrong decimal representations. Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 10, June 5 10, 2010, Toronto, Ontario, \nCanada. Copyright&#38;#169; 2010 ACM 978-1-4503-0019-3/10/06... $10.00. In the early 1980s Coonen published \na paper [Coonen(1980)] and a thesis [Coonen(1984)] containing algorithms for accurate yet economical \nbinary-decimal conversions, but his work went largely unnoticed (at least with respect to the printing \nalgorithms). Steele and White s paper [Steele Jr. and White(1990)]1 had a much bigger impact. Correct \nprinting become part of the speci.ca\u00adtion of many languages and furthermore all major C libraries (and \nas a consequence all programs relying on the printf functions) adapted accurate algorithms and print \ncorrect results now. Steele and White s algorithm, Dragon4 , relies on high preci\u00adsion arithmetic (also \nknown as bignums ) and even though two other papers ([Gay(1990)] and [Burger and Dybvig(1996)]) pro\u00ad \nposed improvements and optimizations to the algorithm this re\u00adquirement remained. It is natural to wonder \nif limited-precision arithmetic could suf.ce. Indeed, according to Steele and White s retrospective of \n2003 [Steele Jr. and White(2004)] [d]uring the 1980s, White investigated the question of whether one \ncould use limited-precision arithmetic [...] rather than bignums. He had ear\u00adlier proved by exhaustive \ntesting that just 7 extra bits suf.ce for correctly printing 36-bit PDP-10 .oating-point numbers, if \npow\u00aders of ten used for prescaling are precomputed using bignums and rounded just once . The document \ncontinues by asking whether [one could] derive, without exhaustive testing, the necessary amount of extra \nprecision solely as a function of the precision and exponent range of a .oating-point format . In this \npaper we will present a new algorithm Grisu, which allows us to answer this question. Grisu requires \nonly two extra bits and a cache of precomputed powers-of-ten whose size depends on the exponent range. \nHowever, Grisu does not supersede Dragon4 and its optimized descendants. While accurate and fast (up \nto 4 times faster than previous approaches) it produces suboptimal results. For instance the IEEE 754 \ndouble-precision number representing 0.3 is printed as 29999999999999998e-17. When read, both numbers \nwill be approximated to the same .oating-point number. They are hence both accurate representations of \nthe corresponding .oating-point number, but the shorter 0.3 is clearly more desirable. With just two \nextra bits it is dif.cult to do better than in our example, but often there exists an integer type with \nmore bits. For IEEE 754 .oating-point numbers, which have a signi.cand size of 53, one can use 64 bit \nintegers, providing 11 extra bits. We have developed an algorithm Grisu2 that uses these extra bits to \nshorten the output. However, even 11 extra bits may not be suf.cient in every case. There are still boundary \nconditions under which Grisu2 will not be able to produce the shortest representation. Since this property \nis often a requirement (see [Steele Jr. and White(2004)] 1 A draft of this article had existed long before \nand had already been mentioned in Knuth Volume 2 [Knuth(1981)] in 1981.  for some examples) we propose \na variant, Grisu3, that detects (and aborts) when its output may not be the shortest. As a consequence \nGrisu3 is incomplete and will fail for some percentage of its input. Given 11 extra bits roughly 99.5% \nare processed correctly and are thus guaranteed to be optimal (with respect to shortness and rounding). \nThe remaining 0.5% are rejected and need to be printed by another printing algorithm (like Dragon4). \nAll presented algorithms come with code snippets in C that show how they can be ef.ciently implemented. \nWe use C99, as this version provides the user with a platform independent means of using 64-bit data \ntypes. In this paper we will concentrate exclusively on IEEE 754 double-precision .oating-point numbers. \nThey are the de facto standard today and while our work applies to other .oating-point representations \nit would unnecessarily complicate the descriptions. We will now discuss some basics in Section 2. In \nSection 3 we present a custom .oating-point data-type which will be used in all remaining sections. Section \n4 details the requirements on the cache of powers-of-ten. In Section 5 we introduce Grisu, and in Section \n6 we present its evolutions Grisu2 and Grisu3. In Section 7 we interpret experimental results. Section \n8 discusses related work, and we .nally conclude in Section 9. 2. Floating-Point Numbers In this section \nwe will give a short introduction on .oating-point numbers. Interested readers may want to consult [Goldberg(1991)] \nfor a thorough discussion of this subject. For simplicity we will consider only positive .oating-point \nnumbers. It is trivial to extend the text to handle signs. Section 2.3 contains examples for all notions \nwe introduce in this section. Readers might want to have a look at this section whenever a de.nition \nis unclear. A .oating point number, as the name suggests, has a radix point that can .oat . Concretely \na .oating-point number v in base b (usually 2) with precision p is built out of an integer signi.cand \n(also known as mantissa or fraction) fv of at most p digits and an exponent ev, such that v = fvxbev \n. Unless otherwise stated, we use the convention that the signi.\u00adcand of a .oating-point number is named \nf with the variable s name as subscript. Similarly the exponent is written as e with the same subscript. \nFor instance a .oating-point number w is assumed to be composed of fw and ew. p-1 Any signi.cand f satis.es \nf =i dixbi , 0 < di < b where the integers di are called the digits of f . We call a number normal\u00adized \nif the most-signi.cant digit dp-1 is non-zero. If the exponent has unlimited range any non-zero number \ncan be normalized by shifting the signi.cand to the left while adjusting the exponent accordingly. When \nthe exponent is size-limited then some numbers can not be normalized. We call non-normalized numbers \nthat have the minimal exponent denormals . Note. Floating-point numbers may allow different representations \nfor the same value (for example 12x101 and 1.2x102). The rep\u00adresentation is however unique when all numbers \nare either normal\u00adized or denormal. 2.1 Rounding and Errors Floating point numbers have only a limited \nsize and thus a limited precision. Real numbers must hence be rounded in order to .t into this .nite \nrepresentation. In this section we will discuss the rounding mechanisms that are used in this document \nand introduce a mechanism to quantify the error they introduce. The most natural way of rounding is to \nchose the nearest avail\u00adable .oating-point number. This rounded-to-nearest approach is straightforward \nexcept for half-way cases (in the decimal system numbers ending with 5). In this paper we will use the \nfollowing strategies for half-way cases: up: picks the number closer to +in.nity. We will use the nota\u00adtion \n[x]t when rounding x by this strategy.  even: picks the number that is even: [x]D . For instance, in \nthe decimal system, 1.5 would round to 2, whereas 0.5 would round to 0. This is the default strategy \nused by IEEE.  Whenever the half-way rounding strategy has no importance we will use a star to make \nthis fact explicit: [x]* . We will use the notation x =[x]sp to indicate that the .oating\u00adpoint number \nx contains a normalized signi.cand of size p which has been computed by rounding-to-nearest using strategy \ns (up, even, or any). We can quantify x s error Ix -xI as follows: x is of form f xbe and since f has \nbeen rounded to nearest Ix -xI< 0.5xbe, or, in other words, by half a unit in the last place (of the \nsigni.cand). Following established conventions we will use the shorthand ulp to describe these units. \nA ulp needs to be given with respect to a certain .oating-point number. In almost all cases the associated \n.oating-point number is clear from context. In the remaining cases we add the associated number as subscript \nas so: 1 ulpx. During the remainder of this document we will use the tilde\u00adnotation to indicate that \na number has been rounded-to-nearest. In most cases its error will be 0.5 ulp, but this is not always \nthe case. 2.2 Neighbors and Boundaries For .oating-point number types where the value of each encoded \nnumber is unique we can de.ne predecessors and successors. Let v = fvxbev be a strictly positive .oating-point \nnumber. The - predecessor vof v is the next smallest number. If v is mini\u00admal, then we de.ne 0 to be \nits predecessor. Similarly v+ desig\u00adnates the successor of v. For the maximal v we de.ne v+ to be () \n+ -+ v:= v + v -v. That is for this particular v the successor v + is at the same distance than the predecessor \nv-. We call v-and vneighbors of v. The boundary between two adjacent numbers v1 and v2 is v1 +v2 simply \ntheir arithmetic mean: m := 2 . By de.nition bound\u00adaries can not be expressed in the given .oating-point \nnumber type, since its value lies between two adjacent .oating-point num\u00adbers. Every .oating-point number \nv has two associated boundaries: -+v +m-:= v2 and m+ := v+2 v. Clearly, any real number w, such - that \nm< w < m+, will round to v. Should w be equal to one of the boundaries then we assume that w is rounded \nto even (the IEEE 754 default). That is, the rounding algorithm will chose the .oating-point number with \nan even signi.cand. We conclude this section with a de.nition we will use frequently in the remainder \nof this document. De.nition 2.1. A printed representation R of a .oating-point num\u00adber v satis.es the \ninternal identity requirement iff R would convert to v when read again. For IEEE 754 double-precision \nnumbers (where half-way cases are rounded to even) this implies [R]D= v. In other words: p + m-< V < \nmwhen fv is even, and + m-< V < mwhen fv is odd.  2.3 Examples In this section we show some examples \nfor the previously de.ned notions. For simplicity we will work in a decimal system. The signi.cand s \nsize p is set to 3, and any exponent is in range 0 to 10. All numbers are either normalized or denormals. \n In this con.guration the extreme values are min := 1x10 and max := 999x101 . The smallest normalized \nnumber equals 100x10 . Non-normalized representations like 3x104 are not valid. The signi.cand must either \nhave three digits or the expo\u00adnent must be zero. Let v := 1234 be a real number that should be stored \ninside the .oating-point number type. Since it contains four digits the number will not .t exactly into \nthe representation and it must be rounded. When rounded to the nearest representation then xv := [v] \n* 3 := 123x101 is the only possible representation. The rounding error is equal to 4=0.4 ulp. Contrary \nto v the real number w := 1245 lies exactly between to possible representations. Indeed, 124x101 and \n125x101 are both at distance 5. The chosen representation depends on the rounding mechanism. If rounded \nup then the signi.cand 125 is chosen. If rounded to even then 124 is chosen. For w = 1235 both rounding \nmechanisms would have chosen 124 as signi.cand. The neighbors of w are w -:= 123x101 and w+ := 125x101 \n. - Its respective boundaries are therefore m := 123.5x101 and m+ := 124.5x101 . In this case the neighbors \nwere both at the same distance. This is not true for r := 100x103, with neighbors -+ \u00ad r := 999x102 and \nr:= 101x103. Clearly r is closer to r than is r+ . For the sake of completeness we now show the boundaries \nfor the extreme values and the smallest normalized number. The number min has its lower (resp. upper) \nboundary at 0.5x101 (resp. 1.5x101). For max, the boundaries are 998.5x101 and 999.5x101 . The boundaries \nfor the smallest normalized number are special: even though its signi.cand is equal to 100 the distance \nto its lower neighbor (99x10 ) is equal to 1 ulp and not just 0.5 ulp. Therefore its boundaries are 99.5x10 \nand 100.5x10 .  2.4 IEEE 754 Double-Precision An IEEE 754 double-precision .oating-point number, or \nsimply double , is de.ned as a base 2 data type consisting of 64 bits. The .rst bit is the number sign, \nfollowed by 11 bits reserved for the exponent eIEEE, and 52 bits for the signi.cand f IEEE . For the \npurpose of this paper the sign-bit is irrelevant and we will assume to work with positive numbers. With \nthe exception of some special cases (which will be dis\u00adcussed shortly) all numbers are normalized which \nin base 2 implies a starting 1 bit. For space-ef.ciency this initial bit is not included in the encoded \nsigni.cant. IEEE 754 numbers have hence effec\u00adtively a 53 bit signi.cand where the .rst 1 bit is hidden \n(with value hidden =252). The encoded exponent eIEEE is an unsigned positive integer which is biased \nby bias = 1075. Decoding an eIEEE con\u00adsist of subtracting 1075. Combining this information, the value \nv of any normalized double can be computed as fv := hidden + f IEEE, ev := eIEEE -bias and hence v = \nfv. x2ev Note. This choice of decoding is not unique. Often the signi.cand is decoded as fraction with \na decimal separator after the hidden bit. IEEE 754 reserves some con.gurations for special values: when \neIEEE = 0x7FF (its maximum) and f IEEE =0 then the double is in\u00ad.nity (or minus in.nity, if the bit-sign \nis set). When eIEEE = 0x7FF 0 then the double represents NaN (Not a Number). = and f IEEE The exponent \neIEEE =0 is reserved for denormals and zero. Denormals do not have a hidden bit. Their value can be computed \nas follows: f IEEE x21-bias . Throughout this paper we will assume that positive and nega\u00adtive in.nity, \npositive and negative zero, as well as NaN have al\u00adready been handled. Developers should be careful when \ntesting for negative zero, though. Following the IEEE 754 speci.cation -0.0= +0.0 and -0.0< +0.0. One \nshould thus use the sign-bit to ef.ciently determine a number s sign. In the remainder of this paper \na .oating-point number will designate only a non-special number or a strictly positive denormal. It does \nnot include zero, NaN or in.nities. Note. Any value representable by doubles (except for NaNs) has a \nunique representation. Note. For any non-special strictly positive IEEE double v with 0 the upper and \nlower boundaries m+ -are at dis\u00ad= and m f IEEEtance 2ve -1. When f IEEE =0 then m+ is still at distance \n2ve -1 but --2 2 the lower boundary only satis.es v -m < 2ve . 3. Handmade Floating-Point 1: typedef \nstruct diy fp { 2: uint64 t f; 3: int e; 4: } diy fp; Figure 1: The diy fp type. Grisu and its variants \nonly require .xed-size integers, but these integers are used to emulate .oating-point numbers. In general \nreimplementing a .oating-point number type is a non-trivial task, but in our context only few operations \nwith severe limitations are needed. In this section we will present our implementation, diy fp, of such \na .oating-point number type. As can be seen in Figure 1 it consists of a limited precision integer (of \nhigher precision than the input .oating-point number), and one integer exponent. For the sake of simplicity \nwe will use the 64 bit long uint64 t in the accompanying code samples. The text itself is, however, size-agnostic \nand uses q for the signi.cand s precision. De.nition 3.1 (diy fp). A diy fp x is composed of an unsigned \nq-bit integer fx (the signi.cand) and a signed integer ex (the ex\u00adponent) of unlimited range. The value \nof x can be computed as x = fxx2ex . The unlimited range of diy fp s exponent simpli.es proofs. In practice \nthe exponent type must only have a slightly greater range than the input exponent. Input numbers are \nsystematically normalized, and a denormal will therefore require more bits than the original data-type. \nWe furthermore need some extra space to avoid over.ows. For IEEE doubles which reserves 11 bits for the \nexponent, a 32-bit signed integer is by far big enough. 3.1 Operations Grisu extracts the signi.cand \nof its diy fps in an early stage and diy fps are only used for two operations: subtraction and multi\u00adplication. \nThe implementation of the diy fp type is furthermore simpli.ed by restricting the input and by relaxing \nthe output. For instance, both operations are not required to return normalized re\u00adsults (even if the \noperands were normalized). Figure 2 shows the C implementation of the two operations. The operands of \nthe subtraction must have the same exponent and the result of subtracting both signi.cands must .t into \nthe signi.cand-type. Under these conditions the operation clearly does not introduce any imprecision. \nThe result might not be normalized. The multiplication returns a diy fp xr containing the rounded result \nof multiplying the two given diy fps x and y. The result might not be normalized. In order to distinguish \nthis imprecise from the precise multiplication we will use the rounded symbol for this operation: xr \n:= x0y. 2 The inequality is only needed for eIEEE =1 where the predecessor is a denormal.  1: diy fp \nminus(diy fp x, diy fp y) { 2: assert(x.e == y.e &#38;&#38; x.f >= y.f); 3: diy fp r= {.f = x.f -y.f, \n.e = x.e}; 4: return r; 5: } (a) Subtraction 1: diy fp multiply(diy fp x, diy fp y) { 2: uint64 t a,b,c,d,ac,bc,ad,bd,tmp; \n3: diy fp r; uint64 t M32 = 0xFFFFFFFF; 4: a = x.f >> 32; b = x.f &#38; M32; 5: c = y.f >> 32; d = y.f \n&#38; M32; 6: ac = a*c; bc = b*c; ad = a*d; bd = b*d; 7: tmp = (bd>>32) + (ad&#38;M32) + (bc&#38;M32); \n8: tmp += 1U \u00ab\u00ab 31; // Round 9: r.f = ac + (ad>>32) + (bc>>32) + (tmp>>32); 10: r.e = x.e + y.e + 64; \n11: return r; 12: } (b) Multiplication Figure 2: diy fp operations De.nition 3.2. Let x and y be two \ndiy fps. Then []t f xf xy x2ex +ey+q x0y := 2q The C implementation emulates in a portable way a partial \n64\u00adbit multiplication. Since the 64 least signi.cant bits of the multi\u00adplication fxxfy are only used \nfor rounding the procedure does not compute the complete 128-bit result. Note that the rounding can be \nimplemented using a simple addition (line 8). Since the result is rounded to 64 bits a diy fp multiplication \nintroduces some error. Lemma 3.3. Let x and y be two diy fps. Then the error of x0y is less than or equal \nto .5 ulp: Ixxy -x0yI< .5 ulp fxXfy x2ex+ey+q Proof. We can write xxy as 2q . Furthermore, by [ r t fx \nXfy x2ex+ey+q de.nition x0y = 2q and the rounding only in\u00ad [rt f Xff Xf xy xy troduces an error of .5:2q \n-2q < .5. Since, for x0y, 2q+ex +ey 1 ulp = we can conclude that the error is bounded by .5x2q+ex+ey \nIxxy -x0yI< .5 ulp = . Lemma 3.4. Let x and xy be two diy fps, and y a real such that Iy -xyI< uy ulp. \nIn other words xy is the approximated diy fp of y and has a maximal error of uy ulp. Then the errors \nadd up and the result is bounded by (.5+ uy) ulp. Vy, Iy -xyI< uy ulp ==Ixxy -x0xyI < (uy + .5) ulp Proof. \nBy Lemma 3.3 we have Ixxxy -x0xyI< 0.5x2q+ex +ey and by hypothesis Iy -xyI< uy ulp = uyx2ey . Clearly \nIxxy -xxxyI< xx(uyx2ey ) < uyx2q+ex+ey and thus, by summing the inequalities Ixxy -x0xyI < (.5+ uy)2q+ex+ey \n. Lemma 3.5. Let x be a normalized diy fp, xy be a diy fp, 2q-1 and y a real such that Iy -xyI< uy ulp. \nIf x = (the minimal signi.cand) then x0xy undershoots by at most u2 y ulp compared to xxy. uy 2q-1 Iy \n-xyI< uy ulp 1 fx === xxy -x0xy < ulp 2 [ r t fy fy Proof. By de.nition x0y = 2 x2ex+ey +q. Since 2 is \neither ex\u00ad [r y x2ex +ey+q ? f x2ex+ey+q-1 act or a half-way case we have f 2 t y and hence x0xy ? xxxy. \nAlso Ixxy -xxxyI< uyx2q+ex +ey -1 and thus xxy -x0xy < u2 y ulp. 4. Cached Powers Similar to White s \napproach (see the introduction) Grisu needs a cache of precomputed powers-of-ten. The cache must be precom\u00adputed \nusing high-precision integer arithmetic. It consists of normal\u00adized diy fp values cxk := [ck] * q where \nck := 10k. Note that, since all ck are normalized Vi, 3 < ecei < 4. -ecei-1 The size of the cache (k \ns range) depends on the used algorithm as well as the input s and diy fp s precision. We will see in \nSec\u00adtion 5 how to compute the needed range. For IEEE doubles and 64 bit diy fps a typical cache must \nhold roughly 635 precom\u00adputed values. Without further optimizations the cache thus takes about 8KB of \nmemory. In [Coonen(1984)] Coonen discusses ef.\u00ad cient ways to reduce the size of this cache. The corresponding \nC procedure has the following signature: diy fp cached power(int k); 4.1 k Computation Grisu (and its \nevolutions) need to .nd an integer k such that its [* cached power cxk = f ck x2eck = 10k]q satis.es \na < eck + e < , for a given e, a and ,. We impose , ? a +3, since otherwise a solution is not always \npossible. We now show how to compute the sought k. All cached powers are normalized and any f ck thus \nsatis.es 2q-1 ck < 2eck +q < f ck < 2q. Hence, 2eck +q-1 < x. Suppose that all cached powers are exact \n(i.e. have no rounding errors). Then k (and its associated cxk) can be found by computing the smallest \npower of ten 10k that veri.es 10k ? 2u-e+q-1 . !1 2u-e+q-11 k := log1 =(a -e + q -1)x log210 1: #define \nD 1 LOG2 10 0.30102999566398114 // 1/lg(10) 2: int k comp(int e, int alpha, int gamma) { 3: return ceil((alpha-e+63) \n* D 1 LOG2 10); 4: } Figure 3: k computation C procedure Figure 3 presents a C implementation (specialized \nfor q = 64) of this computation. In theory the result of the procedure could be wrong since cxk is rounded, \nand the computation itself is approxi\u00admated (using IEEE .oating-point operations). In practice, however, \nthis simple function is suf.cient. We have exhaustively tested all exponents in the range -10000 to 10000 \nand the procedure returns the correct result for all values. 5. Grisu In this section we will discuss \nGrisu, a fast intuitive printing algo\u00adrithm. We will .rst present its idea, followed by a formal descrip\u00adtion \nof the algorithm. We then prove its correctness, and .nally show a C implementation. Grisu is very similar \nto Coonen s algorithm (presented in [Coonen(1980)]). By replacing the extended types (.oating-point numbers \nwith higher precision) of the latter algorithm with diy fp types, Coonen s algorithm becomes a special \ncase of Grisu.  5.1 Idea Printing a .oating-point number is dif.cult because its signi.cant and exponent \ncannot be processed independently. Dragon4 and its variants therefore combine the two components and \nwork with high-precision rationals instead. We will now show how one can print .oating-point numbers \nwithout bignums. Assume, without loss of generality, that a .oating-point number fv v has a negative \nexponent. Then v can be expressed as 2-ev . The decimal digits of v can be computed by .nding a decimal \nexponent t fv X1 t such that 1 < 2-ev < 10. The .rst digit is the integer result of this fraction. Subsequent \ndigits are generated by repeatedly taking the remainder of the fraction, multiplying the numerator by \n10 and by computing the integer result of the newly obtained fraction. The idea behind Grisu is to cache \napproximated values of 1 2ett . The expensive bignum operations disappear and are replaced by operations \non .xed-size integer types. A cache for all possible values of t and et would be expensive and Grisu \ntherefore simpli.es its cache requirement by only storing normalized .oating-point approximations of \nall relevant powers of [* ten: cxk := 10k](where q is the precision of the cached numbers). q By construction \nthe digit generation process uses a power of ten with an exponent ecet close to ev. Even though ecet \nand ev do not can\u00adcel each other out anymore, the difference between the two expo\u00adnents will be small \nand can be easily integrated in the computation of v s digits. In fact, Grisu does not use the power \nof ten cxk that yields the smallest remaining power of two, but selects the power-of-ten so that the \ndifference lies in a certain range. We will later see that different ranges yield different digit-generation \nroutines and that the smallest difference is not always the most ef.cient choice.  5.2 Algorithm In \nthis section we present a formalized version of Grisu. As ex\u00adplained in the previous section, Grisu uses \na precomputed cache of powers-of-ten to avoid bignum operations. The cached numbers cancel out most of \nv s exponent so that only a small exponent re\u00admains. We have also hinted that Grisu chooses its power-of-ten \nde\u00adpending on the sought remaining exponent. In the following algo\u00adrithm we parametrize the remaining \nexponent by the variables a and ,. We impose , ? a +3 and later show interesting choices for these parameters. \nFor the initial discussion we assume a := 0 and , := 3. Algorithm Grisu Input: positive .oating-point \nnumber v of precision p Preconditions: diy fp s precision q satis.es q ? p +2, and the powers-of-ten \ncache contains precomputed normalized rounded [* diy fp values cxk = 10k]. We will determine k s necessary \nrange q shortly. Output: a string representation in base 10 of V such that [V]D p = v. That is, V would \nround to v when read again. Procedure: 1. Conversion: determine the normalized diy fp w such that w = \nv. 2. Cached power: .nd the normalized cx-k = fcx2ec such that a < ec + ew + q < , 3. Product: let \nDx= fDx2eD := w0cx-k. 4. Output: de.ne V := Dx10k. Produce the decimal presentation  x of Dxfollowed \nby the string e and the decimal representation of k. Since the signi.cand of the diy fp is bigger than \nthe one of the input number the conversion of step 1 produces an exact result. By de.nition diy fps have \nan in.nite exponent range and w s exponent is hence big enough for normalization. Note that the exponent \new satis.es ew < ev -(q -p). With the exception of denormals we actually have ew = ev -(q -p). The sought \ncx-k of step 2 must exist. It is easy to show that Vi, 0 < x-< 4 and since the cache is unbounded the \nre\u00ad eci ecxi-1 quired cx-k has to be in the cache. This is the reason for the initial requirement , ? \na +3. An in.nite cache is of course not necessary. k s range depends only on the input .oating-point \nnumber type (its exponent range), the diy fp s precision q and the pair a, ,. By way of example we will \nnow show how to compute kmin and kmax for IEEE doubles, q = 64, and a =0, , =3. Once IEEE doubles have \nbeen normalized (which requires them to be stored in a different data-type) the exponent is in range \n-1126 to +971 (this range includes denormals but not 0). Stored as diy fps the double s exponent decreases \nby the difference in precision (accounting for the normalization), thus yielding a range of -1137 to \n+960. Invoking k comp from Section 4.1 with these values yields: kmin := k comp(960+64) = -289, and \n kmax := k comp(-1137 + 64) = 342.  In step 3 w is multiplied with cx-k. The goal of this operation \nis to obtain a diy fp Dxthat has an exponent eD such that a < eD < , . Some con.gurations make the next \nstep (output) easy. Suppose, for instance, that eD becomes zero. Then Dx= fD and the decimal digits of \nDxcan be computed by printing the signi.cand fD (a q-bit integer). With an exponent eD =0 the digit-generation \nbecomes slightly more dif.cult, but since eD s value is bounded by , the computation is still straightforward. \nGrisu s result is a string containing Dx s decimal representation followed by the character e and k s \ndigit. As such it represents the number V := Dx10k. We claim that V yields v when rounded x to .oating-point \nnumber of precision p. Theorem 5.1. Grisu s result V satis.es the internal identity re\u00adquirement: [V]D \np = v. Proof. In the best case V = v and the proof is trivial. Now, suppose V > v. This can only happen \nif cx-k > c-k. We will ig\u00adnore V s parity and simply show the stronger strict inequality V < m+. Since \nc-k is positive we can reformulate our requirement () as (V -v)xc-k < m+ -vxc-k. Using the equalities \nv = w, V = w0cx-kx10k , 10k xc-k =1, and m+ -v =2ev -1 this expands to w0cx-k -wxc-k < 2ev -1 xc-k. Since, \nby hypothesis, ev ? ew +2 it is hence suf.cient to show that -wxc-k < 2ew+1 w0cx-k xc-k. We have two \ncases: fc > 2q-1. By hypothesis cx-k s error is bounded by .5 ulp and thus c-k ? 2(q-1)+ec . It suf.ces \nto show that w0cx-k -wxc-k is strictly less than 2ew+q+ec which is guaranteed by Lemma 3.4.  fc =2q-1. \nSince the next lower diy fp is only at distance 2ec -1  and c-k is rounded to nearest, c-k s error is \nbounded by 1 (2q-11 )x2ec ? .x2(q-1)+ec 4 ulp. Clearly c-k ?-4 for any .x2(ew+1)+(q-1)+ec q ? 2. The \ninequality w0cx-k -wxc-k < is (due to the smaller error of cx-k) guaranteed by Lemma 3.4. We have proved \nthe theorem for V ? v. The remaining case V < v can only happen when cx-k < c-k. Now suppose: - fv > \n2p-1 and therefore v -m =2ev -1. The proof for this case is similar to the previous cases.  2p-1 2ev \nfv = and therefore v -m -= -2. Since fv is even we - only need to show m < V. Using similar steps as \nbefore it suf.ces to show that 2ew+(q-1)+ec < w0cx-k -wxc-k which is guaranteed by Lemma 3.5.  5.3 C \nImplementation We can now present a C implementation of Grisu. This implemen\u00adtation uses 64 bit integers, \nbut a proof of concept version, using only 55 bits, can be found on the author s homepage. 1: #define \nTEN7 10000000 2: void cut(diy fp D, uint32 t parts[3]) { 3: parts[2] = (D.f % (TEN7 >> D.e)) \u00ab\u00ab D.e; \n4: uint64 t tmp = D.f / (TEN7 >> D.e); 5: parts[1] = tmp % TEN7; 6: parts[0] = tmp / TEN7; 7: } 8: void \ngrisu(double v, char* buffer) { 9: diy fp w; uint32 t ps[3]; 10: int q = 64, alpha = 0, gamma = 3; 11: \nw = normalize diy fp(double2diy fp(v)); 12: int mk = k comp(w.e + q, alpha, gamma); 13: diy fp c mk = \ncached power(mk); 14: diy fp D = multiply(w, c mk); 15: cut(D, ps); 16: sprintf(buffer, \"%u%07u%07ue%d\", \n17: ps[0], ps[1], ps[2], -mk); 18: } Figure 4: C implementation of Grisu with a,, =0,3. In Figure 4, \nline 8 we show the core grisu procedure special\u00adized for a := 0 and , := 3. It accepts a non-special \npositive dou\u00adble and .lls the given buffer with its decimal representation. Up to line 15 the code is \na direct translation from the pseudo-algorithm to C. In this line starts step 4 (output). By construction \nD.e is in the range 0 -3. With a suf.ciently big data-type one could simply shift D.f, the signi.cand, \nand dump its decimal digits into the given buffer. Lacking such a type (we as\u00adsume that uint64 t is the \nbiggest native type), Grisu cuts D into three smaller parts (stored in the array ps) such that the concatena\u00adtion \nof their decimal digits gives D s decimal digits (line 15). Note that 26. = 147573952589676412928 has \n21 digit. Three 7-digit integers will therefore always be suf.cient to hold all deci\u00admal digits of D. \nIn line 16 ps digits and the decimal exponent are dumped into the buffer. For simplicity we have used \nthe stdlib s sprintf procedure. A specialized procedure would be signi.cantly faster, but would unnecessarily \ncomplicate the code. Another bene.t of cutting D s signi.cand into smaller pieces is that the used data-type \n(uint32 t) can be processed much more ef.ciently. In our specialized printing procedure (replacing the \ncall to sprintf) we have noticed tremendous speed improvements due to this choice. Indeed, current processors \nare much faster when dividing uint32 ts than uint64 ts. Furthermore the digits for each part can be computed \nindependently which removes pipeline stalls.  5.4 Interesting target exponents We will now discuss some \ninteresting choices for a and ,. The most obvious choice a,, := 0,3 has already been presented in the \nprevious section. Its digit-generation technique (cutting D into three parts of 7 digits each) can be \neasily extended to work for target exponents in the range a := 0 to , := 9. One simply has to cut D into \nthree uint32 ts of 9 decimal digits each. As a consequence x D s decimal representation might need up \nto 27 digits. On the one hand the bigger , increases the output size (without increasing its precision), \nbut on the other hand the extended range provides more room to .nd a suitable cached power-of-ten. The \nincreased clearance can, for instance, be used to reduce the number of cached powers-of-ten. It is possible \nto remove two thirds of the cache while still being able to .nd the required cx-k of step 2. Indeed, \ntwo cached powers-of-ten cxi and ci+3x will always satisfy -ee< 10. eci+3e ci Another technique uses \nthe increased liberty to choose the best cached power-of-ten among all that satisfy the requirement. \nFor example, a heuristic could prefer exact cached numbers over in\u00adexact ones. Without additional changes \nto the core algorithm there is however little bene.t in using such a heuristic. Despite the added optimization \nopportunities the basic digit\u00adgeneration technique still stays the same, though. We therefore move on \nto the next interesting exponent range: a,, := -63,-60. 1: int digit gen no div(diy fp D, char* buffer) \n{ 2: int i = 0, q = 64; diy fp one; 3: one.f = ((uint64 t) 1) \u00ab\u00ab -D.e; one.e = D.e; 4: buffer[i++] = \n0 + (D.f >> -one.e); //division 5: uint64 t f = D.f &#38; (one.f -1); // modulo 6: buffer[i++] = . ; \n7: while (-one.e > q -5) { 8: uint64 t tmp = (f \u00ab\u00ab 2) &#38; (one.f -1); 9: int d=f >> (-one.e -3); 10: \nd &#38;= 6; f = f + tmp; d += f >> (-one.e -1); 11: buffer[i++] = 0 + d; 12: one.e++; one.f >>= 1; 13: \nf &#38;= one.f -1; 14: } 15: while (i \u00ab 19) { 16: f *= 10; 17: buffer[i++] = 0 + (f >> -one.e); 18: f \n&#38;= one.f -1; 19: } 20: return i; 21: } Figure 5: Digit generation for a = -63 and , = -60. The beauty \nof this exponent range lies in the fact that the normalized diy fp one, representing the number 1, is \ncomposed of f one =263 and eone = -63. Usually expensive operations, such as division and modulo, can \nbe implemented very ef.ciently for this signi.cand. The C implementation in Figure 5 dispenses of division \nand modulo operators entirely and uses only inexpensive operations such as shifts and additions. With \nthe exception of the exponent (which has at most 3 digits) Grisu manages to produce a decimal representation \nof an input IEEE .oating-point number without any division at all. The price for this feat is the complicated \ncode of Figure 5. Its complexity is necessary to avoid over.ows. For simplicity we will start by describing \nthe algorithm without caring for data-type sizes. Algorithm digit-gen-no-div Input: a diy fp D with exponent \n-63 < eD <-60. Output: a decimal representation of D. Procedure: 1. One: determine the diy fp one with \nf one =2 -eD and eone = eD. l D J 2. Digit0: compute d0 := one and D1 := D mod one  2b. Ten: If d0 \n? 10 emit the digit 1 followed by the character representing d0 -10. Otherwise emit the character represent\u00ad \ning the digit d0. 3. Comma: emit . , the decimal separator. 4. Digits: generate and emit digits di as \nfollows  lJ 1 XDi di := one  emit the character representing the digit di Di+1 := 10xDi mod one 5. \nStop: stop at the smallest positive integer n such that Dn =0. We will now show that the algorithm computes \na decimal rep\u00adresentation of D. Let Ri be the number that is obtained by reading the emitted characters \nup to and including di. In step 2b d0 is printed. Since d0 consists of at most 4 binary digits it cannot \nexceed 15, and therefore (after this step) R0 evalu\u00adates to d0. We declare the following invariant for \nthe loop of step 4: Di+1 D = Ri + 1 -i . Clearly the invariant holds for i =0, and the invari\u00adant is \nstill valid after the execution of the loop-body. We can hence conclude that D = Rn-1. The C implementation \nof this algorithm is more involved as it has to deal with over.ows. When multiplying Di by ten (step \n4) the result might not .t into a uint64 t. The code has therefore been split into two parts, one that \ndeals with potential over.ows, and an\u00adother where the product safely .ts in the data-type. The test in \nline 7 checks if the result .ts into a uint64 t. Indeed, Di < one for any 1 < i < n and with 4 additional \nbits the multiplication will not over.ow. The easy, fast case is then handled in line 15. This loop corresponds \nto the loop of step 4. Note that digit gen no div produces at most 18 digits. We will discuss this choice \nshortly. Should 10xDi not .t into a uint64 t the more complicated loop of line 7 is used. As to avoid \nover.ows the code combines the multiplication by ten with the division/modulo by one. By construction \neD = eone and f one =2 -eone . The division by one can f Di 4Xf Di +f Di thus be written as DiX1 = X1 \n= . From this equation one 2-eone-. f one it is then only a small step to the implementation in Figure \n5. In order to escape from this slow case digit gen no div introduces an implicit common denominator. \nIn line 12 one is divided by this denominator. This way one s exponent decreases at each iteration and \nafter at most 5 iterations the procedure switches to the lightweight loop. Our implementation takes some \nshortcuts compared to the de\u00adscribed algorithm: it skips step 2b and prints at most 18 digits. The .rst \nshortcut is only possible when Grisu uses the smallest cached power-of-ten that satis.es the range-requirement, \nsince in that case d0 < 10. The 18 digit shortcut relies on the high precision (64 bits) used in the \nimplementation. An implementation featuring only two extra-bits (55 bits for IEEE doubles) is forced \nto continue iterating until Di =0. Since each iteration clears only one bit one could end up with 55 \ndecimal digits. 1: int digit gen mix(diy fp D, char* buffer) { 2: diy fp one; 3: one.f = ((uint64 t)1)\u00ab\u00ab-D.e; \none.e = D.e; 4: uint32 t part1 = D.f >> -one.e; 5: uint64 t f = D.f &#38; (one.f -1); 6: int i = sprintf(buffer, \n\"%u\", part1); 7: buffer[i++] = . ; 8: while (i \u00ab 19) { 9: f *= 10; 10: buffer[i++] = 0 + (f >> -one.e); \n11: f &#38;= one.f -1; 12: } 13: return i; 14: } Figure 6: Digit generation for a = -59 and , = -32. \nFinally one can mix both digit-generation techniques. The pro\u00adcedure in Figure 6 can be used for a,, \n:= -59,-32. It combines the advantages of the previous approaches. It cuts the input num\u00adber D into two \nparts: one that .ts into a 32 bit integer and one part that can be processed without divisions. By construction \nit does not need to worry about over.ows and therefore features relatively straightforward code. Among \nthe presented digit-generation proce\u00addures it also accepts the greatest range of exponents. Compared \nto the con.guration a,, =0,3 this version needs only a ninth of the cached powers. For completeness sake \nwe now present its pseudo\u00adalgorithm: Algorithm digit-gen-mix Input: a diy fp D with exponent -59 < eD \n<-32. Output: a decimal representation of D. Procedure: 1. One: determine the diy fp one with f one =2 \n-eD and eone = eD. lJ 2. Parts: compute part1 := D and part2 := D mod one one 3. Integral: print the \ndigits of part1. 4. Comma: emit . , the decimal comma separator. 5. Fractional: let D0 := part2. Generate \nand emit digits di (for i ? 0) as follows  lJ 1 XDi di := one emit the character representing the digit \ndi Di+1 := 10xDi mod one 6. Stop: stop at the smallest positive integer n such that Dn =0. The C implementation \ntakes the same shortcut as for the no\u00addivision technique: it stops after 18 digits. The reason is the \nsame as before. Note that the mixed approach can be easily extended to accept exponents in the range \na,, := -59,0 by cutting the input number into four (instead of two) parts. This last version would require \n64 bit divisions and would therefore execute slower than the shown one. However it would require the \nleast amount of cached powers\u00adof-ten. We will base future evolutions of Grisu on digit-get-mix with a,, \n= -59,-32. This con.guration contains the core ideas of all presented techniques without the obfuscating \nover.ow-handling operations. All improvements could be easily adapted to other ranges. 6. Evolutions \nIn this section we will present evolutions of Grisu: Grisu2 and Grisu3. Both algorithms are designed \nto produce shorter outputs. Grisu may be fast, but its output is clearly suboptimal. For example, the \nnumber 1.0 is printed as 10000000000000000000e-19. The optimal solution (printed by Grisu2 and Grisu3) \navoids the trailing 0 digits. Grisu2 and Grisu3 use the extra capacity of the used integer type to shorten \nthe produced output. That is, if the diy fp integer type has more than two extra bits, then these bits \ncan be used to create shorter results. The more bits are available the more often the produced result \nwill be optimal. For 64-bit integers and IEEE doubles (with a 53-bit signi.cand) more than 99% of all \ninput\u00adnumbers can be converted to their shortest decimal representation. Grisu2 and Grisu3 differ in \nthe way they handle the non-optimal solutions. Grisu2 simply generates the best solution that is possible \nwith the given integer type, whereas Grisu3 rejects numbers for which it cannot prove that the computed \nsolution is optimal. For demonstration purposes we include rounding as an optimal\u00adity requirement for \nGrisu3. It is simple to adapt Grisu2 so it rounds its outputs, too. Finally we render Grisu2 and Grisu3 \nmore .exible compared to Grisu. There are different ways to format a .oating-point num\u00adber. For instance \nthe number 1.23 could be formatted as 1.23, 123e-2, or 0.123e1. For genericity it is best to leave the \nfor\u00admatting to a specialized procedure. Contrary to Grisu, Grisu2 and Grisu3 do not produce a complete \ndecimal representation but sim\u00adply produce its digits ( 123 ) and the corresponding exponent (-2). The \nformatting procedure then needs to combine this data to pro\u00adduce a representation in the required format. \n 6.1 Idea We will .rst present the general idea of Grisu2 and Grisu3, and then discuss each algorithm \nseparately. Both algorithms try to produce optimal output (with respect to shortness) for a given input-number \nv. The optimal output of input v represents a number V with the smallest leading length that still satis.es \nthe internal identity requirement for v.3 The leading length of V is its digit length once it has been \nstripped of any unnecessary leading and trailing 0 digits. De.nition 6.1. Let v be a positive real number \nand n, l and s be integers, such that l ? 1, 10l-1 < s < 10l , v = sx10n-l and l as small as possible. \nThen the l decimal digits of s are v s leading digits and l is v s leading length. In the following we \ndemonstrate how the optimal V can be com\u00ad - puted. Let v be a .oating-point number with precision p and \nlet m , m+ be its boundaries (as described in Section 2.2). Assume, with\u00ad out loss of generality, that \nits signi.cand fv is even. The optimal output consist of a number V such that m -< V < m+ and such that \nV s signi.cant length is minimal. The current state of art [Burger and Dybvig(1996)] computes V by generating \nthe input number v s digits from left to right and by stopping once the produced decimal representation \nwould evaluate to v when read again. Basically the algorithm tests for two termination conditions tc1 \nand tc2 after each generated digit di: tc1 is true when the produced number (consisting of digits - d0...di) \nis greater than m , and tc2 is true when the rounded up number (consisting of digits d0...(di + 1)) is \nless than m+ . In the .rst case a rounded down number (of v) would be re\u00adturned, whereas in the second \ncase the result would be rounded up. Since these two tests are slow and cumbersome to write we have developed \nanother technique that needs only one. The basic approach is similar: one produces the decimal digits \nfrom left to right, but instead of using v to compute the digits the faster approach generates the digits \nof m+. By construction any rounded up number of the generated digits will be greater than m+ and thus \nnot satisfy the internal identity requirement anymore. Therefore the second termination condition will \nalways fail and can hence be discarded. We can show that this technique generates the shortest possible \nnumber. Theorem 6.2. Let x and y two real numbers, such that x < y. Let k be the greatest integer, such \nthat y mod 10k < y -x. Then l y J V := k x10k satis.es x < V < y. Furthermore V s leading 1 length l \nis the the smallest of all possible numbers in this interval: any number V such that x < V < y has a \nleading length l ? l. Proof. We start by showing x < V < y: since y = V + y mod 10k we know that V < \ny. Also y mod 10k < y -x and therefore V ? x. For the sake of contradiction assume that there exists \na V with leading length l , such that x < V < y and l < l. 3 The shortest output may not be unique. There \nare many numbers that verify the internal identity requirement for a given .oating-point number, and \nseveral of them might have the same leading length. The number V has a leading length of l and by de.nition \nthere exists hence an s , and n such that 10l -1 < s < 10l and V = s x10n -l . There are three cases \nto consider: l y J 1. s > n : impossible since this implies V > y. 1 lJ 2. s = yn : contradiction, since \nthis implies V = V . 1 l y J 3. s < n : we .rst prove the case for n > k. 1 By hypothesis k is maximal \nand hence y mod 10n > y -x. l y J Given that y mod 10n = y -n x10n we can conclude 1 that y -sx10n > \ny -x and thus V < x. Contradiction. Suppose now that n < k. By de.nition of leading length we know that \nV ? 10l-1 x10k and V < 10l x10n . Since l < l we have V < 10l-1+k < V. Also x < V and V < y and therefore \nx < 10l-1+k < y. Clearly y -10l-1+k < y -x and thus, using the same equality as before, y mod 10l-1+k \n< y -x which contradicts the minimality property of k.  6.2 Grisu2 In this section we will present Grisu2. \nAs described above it will use extra bits to produce shorter outputs. As an evolution of Grisu, Grisu2 \nwill not work with exact numbers (requiring bignum rep\u00adresentations) either, but compute approximations \nof m -and m+ , instead. In order to avoid wrong results (outputs that do not satisfy the internal identity \nrequirement) we add a safety margin around the approximated boundaries. As a consequence Grisu2 sometimes \nfails to return the shortest optimal representation which could lie outside the conservative approximation. \nAlso this safety-margin re\u00adquires us to change the precondition. Indeed, using only 2 extra bits, the \ncomputation is so imprecise that Grisu2 could end up with an empty interval. In that case Grisu2 could \nsimply fall back to Grisu, but this would unnecessarily complicate the following algorithm. We thus opted \nto give Grisu2 an extra bit: q ? p +3. Algorithm Grisu2 Input: same as for Grisu. Preconditions: diy \nfp s precision q satis.es q ? p +3, and the powers-of-ten cache contains precomputed normalized rounded \n[* diy fp values cxk = 10k]. q Output: decimal digits di for i < 0 < n and an integer K such that the \nreal V := d0...dnx10K veri.es [V]D p = v. Procedure: 1. Boundaries: compute v s boundaries m -and m+ \n. 2. Conversion: determine the normalized diy fp w+ such that w+ = m+. Determine the diy fp w -such \nthat w -= m -and  + that e -= w . we 3. Cached Power: .nd the normalized cx-k = fcx2ec such that + a \n< ec + ew + q < , (with a and , as discussed for Grisu). x-x+ 4. Product: compute M-:= w 0cx-k, M+ := \nw0cx-k, and let M-xx := M-+1 ulp, M+:= M+ -1 ulp, 5 := M+-M-. tt tt 5. Digit Length: .nd the greatest \n\" such that M+< 5 lJt mod 10\" M+ 0 and de.ne P := 1 t . 6. Output: de.ne V := Px10k+\" . The decimal digits \ndi and n are obtained by producing the decimal representation of P (an integer). Set K := k + \", and \nreturn it with the n digits di. We will show ef.cient implementations combining step 5 and 6 later, but \n.rst, we prove Grisu2 correct. As a preparation we start by showing that M-t . < M+ t  Lemma 6.3. The \nvariables M-and M+ as described in step 4 tt verify M-< M+ . tt Proof. By de.nition M-x t = M-+1 ulp \n= w -0cx-k +1 ulp ([ rt ) fw -Xfc x2ew+ec +q = +1 2q C) f Xfc w -x2ew+ec+q < +1.5 2q Cf )w+ Xfc x2ew+ec+q \nand similarly M+ = 2q -1.5. t Since fw+ ? fw -+2q-p-1 +2q-p-2 it suf.ces to show f Xf (f +2q-p-. +2q-p-2 \n)Xfc w - cw \u00ad 2q 2q +1.5 < -1.5 or f X(2q-p-. +2q-p-2 )3 < c 2q ? 2q-1 Using the inequalities fc and \nq -p ? 3 it is suf.cient to 2q-. X(22 +2. )show 3 < 2q . Theorem 6.4. Grisu2 s result V = d0...dnx10K \nsatis.es the inter\u00adnal identity requirement: [V]D p = v. - + Proof. We will show that m < M-x10k < V \n< M+ x10k < m tt (with m -and m+ v s boundaries). The inner inequality, M-x10k < V < M+ x10k, is a conse\u00ad \ntt - < M\u00ad quence of Theorem 6.2. Remains to show that m t x10k and + M+ x10k < m. t xx By Lemma 3.4 M-and \nM+ have an error of strictly less than - 1 ulp, and therefore m < M-x10k and M+ x10k < m+. As a tt consequence \nm -< V < m+ . Grisu2 does not give any guarantees on the shortness of its result. Its result is the shortest \npossible number in the interval M-x10k to M+ x10k (boundaries included), where M-and M+ tt tt are dependent \non diy fp s precision q. The higher q, the closer - M-and M+ are to the actual boundaries m and m+. For \nq = 64 tt and p = 53 (as in our code samples) Grisu2 produces the shortest number for approximately 99.9% \nof its input. The C implementation of Grisu2 is again cut into two parts: a core routine, independent \nof the chosen a/,, and a digit-generation procedure that needs to be tuned for the chosen target exponents. \nThe core procedure is straightforward and we will therefore omit its C implementation. In Figure 7 we \npresent a version of the digit-generation routine tuned for a,, = -59,-32. The input variables Mp, and \ndelta correspond to M+ and 5 respectively. The len and K are used as t return values (with obvious meanings). \nWe assume that K has been initialized with k. We hence only need to add the missing \". The proposed implementation \ncombines step 5 and 6. While trying all possible \"s (starting from the top ) it generates the lJ M+ digits \nof 1 0 . There are two digit-generation loops. One for the t l Mp J most-signi.cant digits one , stored \nin p1, and one for the least\u00adsigni.cant digits Mp mod one, stored in p2. Let R be the number that is \nobtained by reading the generated digits (R := 0 if no digit has been generated yet). Then the following \ninvariants holds for l Mp J both loops (line 9 and line 17): R = 1 kappa . For the .rst loop we can show \nthat p1xone + p2 = Mp mod 10kappa . The equation in line 13 thus tests if M+ mod 10 \" < 5. t The following \ninvariant holds for the second loop (line 17): Mp p2 = 1 kappa mod one. 1: #define TEN9 1000000000 2: \nvoid digit gen(diy fp Mp, diy fp delta, 3: char* buffer, int* len, int* K) { 4: uint32 t div; int d,kappa; \ndiy fp one; 5: one.f = ((uint64 t) 1) \u00ab\u00ab -Mp.e; one.e = Mp.e; 6: uint32 t p1 = Mp.f >> -one.e; 7: uint64 \nt p2 = Mp.f &#38; (one.f -1); 8: *len = 0; kappa = 10; div = TEN9; 9: while (kappa > 0) { 10: d = p1 \n/ div; 11: if (d || *len) buffer[(*len)++] = 0 + d; 12: p1 %= div; kappa--; div /= 10; 13: if ((((uint64 \nt)p1)\u00ab\u00ab-one.e)+p2 \u00ab= delta.f) { 14: *K += kappa; return; 15: } 16: } 17: do { 18: p2 *= 10; 19: d = p2 \n>> -one.e; 20: if (d || *len) buffer[(*len)++] = 0 + d; 21: p2 &#38;= one.f -1; kappa--; delta.f *= 10; \n22: } while (p2 > delta.f); 23: *K += kappa; 24: } Figure 7: Grisu2 s digit generation routine (for a,, \n= -59,-32).  6.3 Grisu3 Given enough extra precision, Grisu2 computes the best result (still with respect \nto shortness) for a signi.cant percentage of its input. However there are some numbers where the optimal \nresult lies outside the conservative approximated boundaries. In this section we present Grisu3, an alternative \nto Grisu2. It will not be able to produce optimal results for these numbers either, but it reports failure \nwhen it detects that a shorter number lies in the uncertain region. We denote with uncertain region the \ninterval around the approximated boundaries that might, or might not be inside the boundaries. That is, \nit represents the error introduced by Grisu3 s imprecision. Until now, optimality was de.ned with respect \nto the leading length (and of course accuracy) of the generated number V. For Grisu3 we add closeness \nas additional desired property: when\u00adever there are several different numbers that are optimal with re\u00adspect \nto shortness, Grisu3 should chose the one that is closest to v. Instead of generating a valid number \nand then verifying if it is the shortest possible, Grisu3 will produce the shortest number inside the \nenlarged interval and verify if it is valid. Whereas Grisu2 used a conservative approximation of m -and \nm+, Grisu3 uses a liberal approximation and then, at the end, veri.es if its result lies in the conservative \ninterval, too. Algorithm Grisu3 Input and preconditions: same as for Grisu2. Output: failure, or decimal \ndigits di for i < 0 < n and an integer K such that the integer V := d0...dnx10K veri.es [V]D p = v. V \nhas the shortest leading length of all numbers verify\u00ading this property. If more than one number has \nthe shortest leading length, then V is the closest to v. Procedure: 1-2. same as for Grisu2. 2b. Conversion: \ndetermine the normalized diy fp w such that w = v. 3-4. same as for Grisu2. Mx- x 4b. Product2: let M-:= \n-1 ulp, M+ := M+ +1 ulp, and t t -M- A := M+ tt .  5. Digit Length: .nd the greatest \" such that M+ \nmod 10 \" < A. t xx 6. Round: compute W := w0cx-k, and let Wt := W -1 ulp, and lJ M+ x Wt := W +1 ulp. \nSet Pi := t-i for i ? 0. Let m be 1 t the greatest integer that veri.es Pmx10 \" > M-. t Let u, 0 < u \n< m the smallest integer such that IPux10 \" -Wt I is minimal. Similarly let d, 0 < d < m the largest \ninteger such that IPdx10 \" -Wt I is minimal. If u = d return failure, else set P := Pu. 7. Weed: if not \nM-< Px10 \" < M+ return failure. tt 8. Output: de.ne V := Px10k+\" . The decimal digits di and n are obtained \nby producing the decimal representation of P (an integer). Set K := k + \", and return it with the n digits \ndi. Grisu3 uses the liberal boundary approximations (M-and M+ ) tt instead of the conservative ones (M-and \nM+). These values are tt - guaranteed to lie outside the actual interval m to m+. The test in step 5 \ntherefore features a strict inequality. This way M-is t excluded from consideration. There is however \nno mechanism to exclude M+. In the rare event when M+ mod 10 \" =0 then Grisu3 tt will, at this point \nof the algorithm, wrongly assume that M+ is t a potentially valid representation of v. Since M+ lies \noutside the t conservative region Grisu3 would the return failure in step 7. This case is rare and counter-measures \nare expensive, so we decided to accept this .aw. Rounding is performed in step 6. Grisu3 simply tries \nall possible numbers with the same leading length and picks the one that is closest to v. At this stage \nGrisu3 works with approximated values and W, the approximation of v, may have an error of up to 1 ulp. \nx The closest representation Px10 \" must not only be the closest to xx W but to all possible values in \nW s margin of error. Grisu3 .rst .nds the closest Pu to the upper boundary, and Pd for the lower boundary. \nIf they are not the same, then the precision is not enough to determine the optimal rounding and Grisu3 \naborts. Finally, just before outputting the computed representation, Grisu3 veri.es if the best pick \nis within the conservative bound\u00adaries. If it is not, then the optimal solution lies in the uncertain \nregion and Grisu3 returns failure. 1: bool round weed(char* buffer, int len, 2: uint64 t wp W, uint64 \nt Delta, 3: uint64 t rest, uint64 t ten kappa, 4: uint64 t ulp) { 5: uint64 t wpWup = wpW -ulp; 6: uint64 \nt wpWdown = wpW + ulp; 7: while (rest \u00ab wp Wup &#38;&#38; 8: Delta -rest >= ten kappa &#38;&#38; 9: (rest \n+ ten kappa \u00ab wp Wup || 10: wp Wup-rest >= rest+ten kappa -wp Wup)) 11: { 12: buffer[len-1]--; rest += \nten kappa; 13: } 14: if (rest \u00ab wp Wdown &#38;&#38; 15: Delta -rest >= ten kappa &#38;&#38; 16: (rest \n+ ten kappa \u00ab wp Wdown || 17: wp Wdown-rest > rest+ten kappa -wp Wdown)) 18: return false; 19: return \n2*ulp \u00ab= rest &#38;&#38; rest \u00ab= Delta -4*ulp; 20: } Figure 8: Grisu3 s round-and-weed procedure. The \ndigit-generation routine of Grisu2 has to be modi.ed to take the larger liberal boundary interval into \naccount, but these changes are minor and obvious. The interesting difference can be summa\u00adrized in the \nround weed procedure shown in Figure 8 which com\u00ad bines step 6 and 7. The function is invoked with the \nparameters set to the following values: buffer := d0...dlen-1 where d0...dlen-1 lJ W+ t xx are the decimal \ndigits of 1 t , wp W := W+ -W, Delta := A, rest := W+ mod 10 \" , ten kappa := 10 \" , and ulp the value \nt of 1 ulp relative to all passed diy fps. Let T := d0...dlen-1x10 \" . By construction T lies within \nthe - unsafe interval: W< T < W+. Furthermore, among all possible tt values in this interval, it has \nthe shortest leading length len. If there are other possible values with the same leading length and \nin the same interval, then they are smaller than T. The loop in line 7 iteratively tests all possible \nalternatives to .nd the closest to Wt . The .rst test, rest < wpW -ulp, ensures that T is not already \nless than Wt (in which case the current T must be the closest). Then follows a veri.cation that the next \nlower - number with the same leading length is still in the interval Wt to W+. In line 9 the procedure \ntests if the alternative would be t closer to Wt . If all tests succeed, then the number d0...dlen-1x10 \n\" - is guaranteed to lie inside the interval Wto W+ and is furthermore tt closer to Wt than the current \nT. The body of the loop thus replaces T (physically modifying the buffer) with its smaller alternative. \nThe if in line 14 then veri.es the chosen T is also closest to - Wt . If this check fails then there \nare at least two candidates that could be the closest to Wxand Grisu3 returns failure. Now that the buffer \nhas been correctly rounded a .nal weeding - test in line 19 veri.es that W< T < W+. That is, that the \nchosen tt T is inside the safe conservative interval. 7. Benchmarks Algorithm R R/PP S S/PP sprintf %c \n2.60 - - - sprintf %g 22.83 - 24.17 - sprintf %.17e 36.03 - 36.17 - burger/dybvig 61.53 61.49 28.73 28.66 \ngrisu Fig4 9.17 - 9.98 - grisu 0,3 2.85 - 3.26 - grisu -63,-59 3.36 - 3.77 - grisu -35,-32 2.66 - 3.04 \n- grisu -59,-56 2.50 - 2.96 - grisu2 -59,-56 3.80 4.88 3.07 4.04 grisu2b -59,-56 5.38 6.42 4.40 5.48 \ngrisu3 -59,-56 4.47 5.61 3.49 4.55 Figure 9: Speed of sprintf, Burger/Dybvig and Grisu. Algorithm optimal \nshortest grisu2 -59,-56 0 99.92 grisu2b -59,-56 99.85 99.92 grisu3 -59,-56 99.49 99.49 Figure 10: Optimality \nof Grisu2 and Grisu3. In this section we present some experimental results. In Fig\u00adure 9 we compare different \nvariants of Grisu against sprintf and Burger/Dybvig s algorithm (the code has been taken from their website). \nIn order to measure the parsing-overhead of sprintf we added one sprintf benchmark where the double is \nconverted to a char, and then printed (.rst row). Also we included the un\u00adoptimized algorithm of Figure \n4. Grisu2b ( grisu2b -35,-32 ) is a variant of Grisu2 where the result is rounded.  Input numbers are \nrandom IEEE doubles. Speed is measured in seconds per thousand numbers . All benchmarks have been executed \non a Intel(R) Xeon(R) CPU 5120 @ 1.86GHz quad-core system, Linux 2.6.31, glibc 2.11. The .rst column \n(R) gives the speed of processing random doubles. The next column shows the time for the same numbers, \nbut with a pretty-printing pass for the algorithms that only returned the digits and the exponent K. \nColumn S measures the processing speed for short doubles. That is, doubles that have at most 6 leading \ndigits. Algorithms that stop once the leading digits have been found are clearly faster for these numbers. \nThe next column (S/PP) adds again a pretty-printing pass. In Figure 10 we show the percentage of numbers \nthat are op\u00ad timal (shortest and rounded) or just shortest. We have excluded sprintf (0 in both columns), \nBurger/Dybvig (100 in both columns) and Grisu (0 in both columns). 99.92% of Grisu2 s pre\u00adsentations \nare the shortest, and once a rounding-phase has been added (Grisu2b) 99.87% of the numbers are optimal. \nGrisu3 pro\u00adduces optimal results for 99.49% of its input and rejects the rest. 8. Related Work In 1980 \nCoonen was the .rst to publish a binary-decimal conver\u00adsion algorithm [Coonen(1980)]. A more detailed \ndiscussion of con\u00ad version algorithms appeared in his thesis [Coonen(1984)] in 1984. Coonen proposes \ntwo algorithms: one using high-precision integers (bignums) and one using extended types (a .oating-point \nnumber type speci.ed in IEEE 754 [P754(1985)]). By replacing the ex\u00ad tended types with diy fp s the latter \nalgorithm can be transformed into a special case of Grisu. In his thesis Coonen furthermore describes \nspace ef.cient al\u00adgorithms to store the powers-of-ten, and presents a very fast logarithm-estimator for \nthe k-estimation (closely related to the k\u00adcomputation of Section 4.1). In 1990 Steele and White published \ntheir printing-algorithm, Dragon, [Steele Jr. and White(1990)]. A draft of this paper had ex\u00ad isted for \nmany years and had already been cited in Knuth, Volume II [Knuth(1981)] (1981). Dragon4 is an exact algorithm \nand re\u00ad quires bignums. Dragon4 generates its digits from left to right and stops once the result lies \nwithin a certain precision. This approach differs from Coonen s bignum algorithm where all relevant digits \nare produced before the result is rounded. The rounding process might lead to changing a trailing sequence \nof 9s to 0s thus shorten\u00ading the generated sequence. Conceptually the simplest form of Grisu2 and Grisu3 \n(presented in Section 6) can be seen as a combination of Coonen s .oating\u00ad point algorithm and Dragon. \nIn the same year (1990) Gay improved Dragon, by proposing a faster k-estimator and by indicating some \nshortcuts [Gay(1990)]. Burger and Dybvig published their improvements in 1996 [Burger and Dybvig(1996)]. \nThis paper features a new output for\u00ad mat where insigni.cant digits are replaced by # marks. They had \nalso rediscovered the fast logarithm-estimator that had been pub\u00adlished in Coonen s thesis. 9. Conclusion \nWe have presented three new algorithms to print .oating-point numbers: Grisu, Grisu2 and Grisu3. Given \nan integer type with at least two more bits than the input s signi.cand, then Grisu prints its input \ncorrectly. Grisu2 and Grisu3 are designed to bene.t from integer types that have more than just two extra \nbits. Grisu2 may be used where an optimal decimal representation is desired but not required. Given a \n64 bit unsigned integer Grisu2 computes the optimal decimal representation 99.8% of the times for IEEE \ndoubles. Grisu3 should be used when the optimal representation is re\u00adquired. In this case Grisu3 will \nef.ciently produce the optimal re\u00adsult for 99.5% of its input (with doubles and 64-bit integers), and \nreject the remaining numbers. The rejected numbers must then be printed by a more accurate (but slower) \nalgorithm. Since Grisu3 is about 4 times faster than these alternative algorithms the average speed-up \nis still signi.cant. Grisu (and its evolutions) are furthermore straightforward to im\u00adplement and do \nnot feature many special cases. This is in stark contrast to ef.cient printing-algorithms that are based \non bignums. Indeed, the major contributions to Dragon4 (one of the .rst pub\u00adlished bignum-algorithms) \nhave been the identi.cation of special cases that could be handled more ef.ciently. We hope that Grisu3 \nrenders this special cases uneconomic, thus simplifying the com\u00adplete development of .oating-point printing \nalgorithms. References [Burger and Dybvig(1996)] R. G. Burger and R. K. Dybvig. Printing Floating-Point \nNumbers Quickly and Accurately. In Proceedings of the ACM SIGPLAN 1996 Conference on Programming Language \nDesign and Implementation, PLDI 1996, pages 108 116, New York, NY, USA, June 1996. ACM. doi: 10.1145/249069.231397. \n[Coonen(1980)] J. T. Coonen. An implementation guide to a proposed standard for .oating-point arithmetic. \nComputer, 13(1):68 79, 1980. ISSN 0018-9162. doi: 10.1109/MC.1980.1653344. [Coonen(1984)] J. T. Coonen. \nContributions to a Proposed Standard for Binary Floating-Point Arithmetic. PhD thesis, University of \nCalifornia, Berkeley, June 1984. [Gay(1990)] D. M. Gay. Correctly rounded binary-decimal and decimal\u00adbinary \nconversions. Technical Report 90-10, AT&#38;T Bell Laboraties, Murray Hill, NJ, USA, Nov. 1990. [Goldberg(1991)] \nD. Goldberg. What every computer scientist should know about .oating-point arithmetic. ACM Computing \nSurveys, 23(1): 5 48, 1991. ISSN 0360-0300. doi: 10.1145/103162.103163. [Knuth(1981)] D. E. Knuth. The \nArt of Computer Programming, Volume II: Seminumerical Algorithms, 2nd Edition. Addison-Wesley, 1981. \nISBN 0-201-03822-6. [P754(1985)] I. T. P754. ANSI/IEEE 754-1985, Standard for Binary Floating-Point Arithmetic. \nIEEE, New York, Aug. 12 1985. [Steele Jr. and White(1990)] G. L. Steele Jr. and J. L. White. How to print \n.oating-point numbers accurately. In Proceedings of the ACM SIGPLAN 1994 Conference on Programming Language \nDesign and Implementation, PLDI 1994, pages 112 126, New York, NY, USA, 1990. ACM. ISBN 0-89791-364-7. \ndoi: 10.1145/93542.93559. [Steele Jr. and White(2004)] G. L. Steele Jr. and J. L. White. How to print \n.oating-point numbers accurately (retrospective). In 20 Years of the ACM SIGPLAN Conference on Programming \nLanguage Design and Implementation 1979-1999, A Selection, pages 372 374. ACM, 2004. ISBN 1-58113-623-4. \ndoi: 10.1145/989393.989431.    \n\t\t\t", "proc_id": "1806596", "abstract": "<p>We present algorithms for accurately converting floating-point numbers to decimal representation. They are fast (up to 4 times faster than commonly used algorithms that use high-precision integers) and correct: any printed number will evaluate to the same number, when read again.</p> <p>Our algorithms are fast, because they require only fixed-size integer arithmetic. The sole requirement for the integer type is that it has at least two more bits than the significand of the floating-point number. Hence, for IEEE 754 double-precision numbers (having a 53-bit significand) an integer type with 55 bits is sufficient. Moreover we show how to exploit additional bits to improve the generated output.</p> <p>We present three algorithms with different properties: the first algorithm is the most basic one, and does not take advantage of any extra bits. It simply shows how to perform the binary-to-decimal transformation with the minimal number of bits. Our second algorithm improves on the first one by using the additional bits to produce a shorter (often the shortest) result.</p> <p>Finally we propose a third version that can be used when the shortest output is a requirement. The last algorithm either produces optimal decimal representations (with respect to shortness and rounding) or rejects its input. For IEEE 754 double-precision numbers and 64-bit integers roughly 99.4% of all numbers can be processed efficiently. The remaining 0.6% are rejected and need to be printed by a slower complete algorithm.</p>", "authors": [{"name": "Florian Loitsch", "author_profile_id": "81464645567", "affiliation": "Inria, Sophia Antipolis, France", "person_id": "P2184554", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1806596.1806623", "year": "2010", "article_id": "1806623", "conference": "PLDI", "title": "Printing floating-point numbers quickly and accurately with integers", "url": "http://dl.acm.org/citation.cfm?id=1806623"}