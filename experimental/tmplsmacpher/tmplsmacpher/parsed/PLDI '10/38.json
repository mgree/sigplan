{"article_publication_date": "06-05-2010", "fulltext": "\n Evaluating Iterative Optimization Across 1000 Data Sets Yang Chen Yuanjie Huang LCSA *, ICT, CAS, China \nand Graduate School, CAS, China {chenyang,huangyuanjie}@ict.ac.cn Liang Peng LCSA, ICT, CAS, China and \nGraduate School, CAS, China pengliang@ict.ac.cn Lieven Eeckhout Grigori Fursin Ghent University, Belgium \nINRIA, Saclay, France Lieven.Eeckhout@elis.UGent.be grigori.fursin@inria.fr Olivier Temam Chengyong Wu \nINRIA, Saclay, France LCSA, ICT, CAS, China olivier.temam@inria.fr cwu@ict.ac.cn Abstract While iterative \noptimization has become a popular compiler opti\u00admization approach, it is based on a premise which has \nnever been truly evaluated: that it is possible to learn the best compiler opti\u00admizations across data \nsets. Up to now, most iterative optimization studies .nd the best optimizations through repeated runs \non the same data set. Only a handful of studies have attempted to exer\u00adcise iterative optimization on \na few tens of data sets. In this paper, we truly put iterative compilation to the test for the .rst time \nby evaluating its effectiveness across a large number of data sets.We therefore compose KDataSets,adata \nset suite with 1000datasetsfor32 programs,whichwe releasetothepublic.We characterize the diversity of \nKDataSets, and subsequently use it to evaluate iterativeoptimization.Wedemonstrate thatitis possibleto \nderive a robust iterative optimization strategy across data sets: for all 32 programs, we .nd that there \nexists at least one combination of compiler optimizations that achieves 86% or more of the best possible \nspeedup across all data sets using Intel s ICC (83% for GNU s GCC). This optimal combination is program-speci.c \nand yields speedupsupto1.71onICCand2.23onGCCoverthehigh\u00adest optimization level(-fast and -O3, respectively). \nThis .nding makes the task of optimizing programs across data sets much eas\u00adier than previously anticipated, \nand it paves the way for the practi\u00adcal and reliable usage of iterative optimization. Finally, we derive \npre-shipping and post-shipping optimization strategies for software vendors. Categories and Subject Descriptors \nD.3.4.[Software: Program\u00adming Languages]: Processors Compilers, Optimization General Terms Design, Experimentation, \nMeasurement, Perfor\u00admance * KeyLaboratory of Computer System and Architecture Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page.To copyotherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 10, June 5 10, 2010,Toronto, Ontario, \nCanada. Copyright c &#38;#169; 2010 ACM 978-1-4503-0019-3/10/06. . . $10.00 Keywords Compiler optimization, \nIterative optimization, Bench\u00admarking 1. Introduction Iterative optimization has become a popular optimization \nstrat\u00adegy [4, 10, 12, 22, 29] because of its simplicity while yielding substantial performance gains. \nIt is essentially based on repeat\u00adedly trying out a large number of compiler optimizations until the \nbest combination of compiler optimizations is found for a partic\u00adular program (one iteration is the evaluation \nof one combination of compiler optimizations). From the start though, there has been a nagging issue: \nhow data set dependent is iterative optimization? More precisely,if one selectsa combinationof optimizations \nbased on runs over one or a few data sets, will that combination still be the best for other data sets? \nAnswering this question is key in order to con.dently use it\u00aderative optimization in practice. At the \nsame time, this question is dif.cult to answer because there is no benchmark suite with a large enough \nnumber of data sets to emulate the two most likely appli\u00adcation scenarios. Either a program is tuned \nthrough iterative opti\u00admization before shippingbythe softwarevendorsothatit performs well across a broad \nrange of data sets; or a program is tuned after shipping, across a large number of production runs performed \nby users. Several researchers have investigated how program behavior and performance varies across data \nsets. Zhong et al. [32] present two techniques to predict how programlocality is affected across data \nsets; they consider6 programs using up to6 data sets each. Hsu et al. [20] compare the pro.les generated \nusingthe3data sets of some SPECint2000 programs, and observe signi.cant program behavior variability \nacross data sets for some programs. Several studies focus on the impact of data sets on compiler optimization \nparameterization and selection. In particular, Berube et al. [6] col\u00ad lect17 inputsper programonaveragefor7SPECint2000 \nprograms to study inlining. Haneda et al. [16] use the3 SPECint2000 data sets to evaluate compiler optimizations. \nMao et al. [24] collect be\u00ad tween 8 and 976 inputs per program for 10 Java programs (150 inputs onaverage \nper program,4programs with more than 100 in\u00adputs) to investigate the impact of inputs on the selection \nof the best garbage collector. Fursin et al. [14] collect 20 data sets for each of the MiBench benchmarks \n the same benchmark suite used in this paper and also evaluate the data set sensitivity of com\u00adpiler \noptimizations. These iterative optimization studies and oth\u00aders[4,10,12,22,29] underscorethefactthata \nsigni.cant number of iterations (tens or hundreds) are required to .nd the best combi\u00adnation of compiler \noptimizations. However,to our knowledge, there is no benchmark suite available with multiple hundreds \nof distinct data sets per program. As a result, not only are researchers forced to evaluate iterative \noptimization using an unrealistic experimental setting, they are unable to answer the aforementioned \nfundamental question about whether iterative optimization is effective across a broad range of data sets. \n In order to address thiskeyquestion, we collect 1000 data sets for 32 programs, mostly derived from \nthe MiBench benchmark suite [15]. Our results show that, for each benchmark, there is at least one combination \nof compiler optimizations that achieves 86% or more of the maximum speedup (i.e., the speedup obtained \nwith the best possible combination per data set) across all data sets. This result has a signi.cant implication: \nit means that, in practice, iterative optimization may often be largely data set insensitive. It also \nmeans that a program can be optimized by a software vendor on a collection of data sets before shipping, \nand will retain near\u00adoptimal performance for most other data sets. So the problem of .nding the best \noptimization for a particular program may be signi.cantly less complex than previously anticipated. This \npaper is organized as follows. We present KDataSets in Section 2, and characterize its diversity and \ncoverage in Section 3; we also compare KDataSets against the previously proposed Mi-DataSets, which comes \nwith 20 data sets per benchmark, and show that a large population of data sets is indeed necessary to \ncapture a suf.cientlywide rangeof programbehaviors.In Section4,wethen emulate realistic iterative optimization \nusing the 1000 data sets in KDataSets, devise a strategy for selecting the best possible combi\u00adnation \nof optimizations, and evaluate the impact of an unrealistic experimental context on iterative optimization \nstrategies.We also discuss the scope and the general applicability of the results ob\u00adtainedinthispaper(Section5).Finally,we \nsummarizerelatedwork (Section 6) and conclude (Section 7). 2. KDataSets:A1000-Data Set Suite As mentioned \nin the introduction, we have collected 1000 data sets for each of our 32 benchmarks. Most of these benchmarks \ncome from the MiBench benchmark suite. MiBench [15] is an embed\u00ad ded benchmark suite covering a broad \nspectrum of applications, ranging from simple embedded signal-processing tasks to smart\u00adphone and desktop \ntasks. It was developed with the idea that desk\u00adtops and sophisticated embedded devices are on a collision \ncourse (for both applications and architectures, e.g., the x86 Intel Atom processoris increasingly used \nfor embeddeddevices), calling fora broad enough benchmark suite.We usea modi.edversionof the MiBench \nsuite, which was evaluated across different data sets and architectures [2, 14].We also added bzip2 (both \nthe compressor and the decompressor) to our set of benchmarks, and we plan to add and disseminate more \nprograms over time. The benchmarks are listedinTable1; the numberof source lines ranges from 130 lines \nforkernels, e.g., crc32, to 99,869 lines for large programs, e.g., ghostscript. Table1 summarizes thevarious \ndata setsin KDataSets;it de\u00ad scribes the range in .le size along with a description of how these data \nsets were obtained. The data sets vary from simple num\u00adbers and arrays, to text .les, postscript .les, \npictures and audio .les in different formats. Some data sets, such as the numbers for bitcount as well \nas the numbers in the array for qsort, are randomly generated. For other programs, such as dijkstra and \npatricia, webuilt data sets thatexhibit distinct characteristicsin terms of how the workload exercises \ndifferent control .ow paths and deals with different working set sizes this was done based on studying \nthe benchmarks source code and our target domain knowledge. Program (# source lines) Data set .le size \nData set description bitcount (460) - Numbers: randomly generated in\u00adtegers qsort1 (154) 32K-1.8M 3D \ncoordinates: randomly gener\u00adated integers dijkstra (163) 0.06k-4.3M Adjacency matrix: varied matrix size, \ncontent, percentage of discon\u00adnected vertices (random) patricia (290) 0.6K-1.9M IP and mask pairs: varied \nmask range to control insertion rate (ran\u00addom) jpeg d(13501) 3.6K-1.5M JPEG image: varied size, scenery, \ncompression ratio, color depth jpeg c (14014) 16k-137M PPM image:output of jpeg c (con\u00adverted) tiff 2bw \n(15477), 2rgba (15424), dither (15399), median (15870) 9K-137M TIFF image: from JPEG images by ImageMagick \nconverter (con\u00adverted) susan c, e, s (each 1376) 12K-46M PGM image: from jpeg images by ImageMagick converter \n(con\u00adverted) mad (2358) 28K-27M MP3 audio: varied length, styles (ringtone, speech, music) lame (14491), \nad\u00adpcm c (210) 167K-36M WAVE audio: output of mad (con\u00adverted) adpcm d(211) 21K-8.8M ADPCM audio: output \nof adpcm c (converted) gsm (3806) 83K-18M Sun/NeXT audio: from MP3 au\u00addios by mad (converted) ghostscript \n(99869) 11K-43M Postscript .le: varied page num\u00adber, contents (slides, notes, papers, magazines, manuals, \netc.) ispell (6522), rsynth (4111), stringsearch1 (338) 0.1K-42M Text .le: varied size, contents (novel, \nprose, poem, technical writings, etc.) blow.sh e (863) 0.6K-35M Any.le: a mix of text, image, au\u00addio, \ngenerated .les blow.sh d(863) 0.6K-35M Encrypted .le: output of blow\u00ad.sh e pgp e (19575) 0.6K-35M Any.le: \na mix of text, image, au\u00addio, generated .les pgp d(19575) 0.4K-18M Encrypted .le: output of pgp e rijndael \ne (952) 0.6K-35M Any.le: a mix of text, image, au\u00addio, generated .les rijndael d(952) 0.7K-35M Encrypted \n.le: output of rijndael d sha (197) 0.6K-35M Any.le: a mix of text, image, au\u00addio, generated .les CRC32 \n(130) 0.6K-35M Any.le: a mix of text, image, au\u00addio, generated .les bzip2e (5125) 0.7K-57M Any .le: a \nmix of above text, image, audio, generated .les, and other .les like program binary, source code bzip2d \n(5125) 0.2K-25M Compressed .le: output of bzip2e Table 1. KDataSets description. The text .les are \ncollected from the Gutenbergproject (gutenberg.org)andpython.org. Postscript .les are collected from \nvarious web sites: somethingconstructive.ne, oss.net, ocw.mit.edu, samandal.org, pythonpapers.org, etc., \nand we converted some of the collected PDF .les into PS format. Images are collected from public-domain-image.com \nand converted into the different required formats (TIFF, JPEG, PGM, PPM). Au\u00addio .les are collected from \nfreesoundfiles.tintagel.net, jamendo.com, ejunto.com and converted again into the appro\u00adpriate formats(WAVE, \nADPCM, Sun/NeXT).For programs with miscellaneous .les as inputs (e.g., compression, encryption), we useamixofthe \naforementioned .les.Andin some cases,the output of some programs are inputs to other programs, e.g., \nmad/adpcm c. The entire data set suite corresponds to 27GB of data. All data sets within KDataSets are \nfree in the public domain or under the Creative Commons license, and will be publicly dissem\u00adinated at \nhttp://www.kdatasets.org. 3. Data Set Characterization In this section, we characterize KDataSets, analyze \nhowdistinct the data sets are with respect to each other, and how differently they react to compiler \noptimizations. 3.1 Experimental setup Before doing so, we .rst brie.y describe our experimental setup. \nWe consider 7 identical machines with 3GHz Intel Xeon dual\u00adcore processors (E3110 family), 2GB RAM, 2\u00d73MB \nL2 cache. We use the CentOS 5.3 Linux distribution based on Red Hat with kernel 2.6.18 patched to support \nhardware counters through the PAPI library [3]. Most of the results in the paper are obtained using the \nIntel ICC compiler v11.1. However, for completeness and in order to demonstrate that the results in this \npaper translate to other compil\u00aders,wehavealso conductedthe sameexperimentsusingGNUGCC v4.4.1; these \nresults are similar to the ICC results and are reported in Section 5.1. Both compilers feature a large \nnumber of optimiza\u00ad tions.We selected 53 optimization .ags for ICCand 132 .ags for GCC that control inlining, \nunrolling,vectorization, scheduling, reg\u00adister allocation, constant propogation, among manyothers.We \nuse a random optimization strategy for creating combinations of opti\u00admizations:a combinationis basedonthe \nuniform random selection of .ags; the number of .ags is itself randomly selected. Random selection of \ncompiler optimization combinations is a popular and effective approach for exploring the compiler optimization \ndesign space [8, 14, 27]. We consider 300 combinations of compiler optimizations for each program/data \nset pair throughout the paper we study the results sensitivitytothe numberof combinationsinSection5by \nconsidering 8,000 combinations. For each combination and each data set, we measure the program sexecution \ntime.We also mea\u00adsure 9 performance characteristics using hardware performance counters through the PAPI \ninterface, such as L1, L2 and TLB miss rates and branch prediction miss rates. And we also col\u00adlect 66 \narchitecture-independent characteristics using the MICA toolset [18], such as instruction mix, ILP, branch \npredictability, memory access patterns, and working set size. Finally, Table 2 summarizes the terminology \nand de.nitions used throughoutthetext.We presentthis tablehereasa reference, and we will introduce the \nde.nitions in the text itself as need be. We use harmonic mean when reporting average speedup numbers. \n 3.2 Performance characterization Figure 1 summarizes the performance characteristics for all pro\u00ad grams \nand all data sets. The violin graphs show the distribution of the execution time (in cycles), IPC as \nwell as other performance characteristics that relate to cache and branch behavior across all data sets \nper program. All graphs are shown on a logarithmic scale. The goal of this characterization is to underscore \nthe differ\u00adences among the data sets this illustrates the broad range of program behavior that these \ndata sets generate. The execution time distribution (left top graph) shows that the amount of work varies \ngreatly across data sets: while the standard deviation can be low, as for tiffmedian, it is fairly high \nfor most other benchmarks. = maxo.Omean{s,d . D}. Table 2. De.nitions and terminology. The other distributions \n(see the other graphs) show that not only doesthe amountofworkvary wildly across data sets,but alsothe \nperformance seen at the architecture level in terms of IPC and sev\u00aderal other performance characteristics \n(L1 and L2 miss rates, and branch prediction rate). 3.3 How programs react to compiler optimizations \nacross data sets We now investigate how programs react to compiler optimizations across data sets.To \nthis end, we compile each program with each of the 300 randomly generated combinations of compiler optimiza\u00adtions, \nand then run each of these 300 program versions with each of the 1000 data sets.We then record the best \npossible speedup for each data set, and we will refer to this speedup as the data set op\u00adtimal speedup, \nsee alsoTable2 fora formal de.nition. The data set optimal speedup is the speedup relative to -fast and \n-O3 for ICC and GCC, respectively. Because we have 1000 data sets per program (and hence 1000 data set \noptimal speedup numbers), we report these results asa distribution, see Figure2for the Intel ICC compiler; \nwe report similar results for GNU s GCC in Section 5.1. The violin plots thus show the distribution of \nthe data set optimal speedup for each benchmark. Figure 2(a) sorts the various bench\u00ad marks by average \ndata set optimal speedup. Figure 2(b) shows the same databut sortsthe benchmarksbythe numberof source \ncode lines; this illustrates that the impact of compiler optimizations is uncorrelated with program size. \nThe results are contrasted. For some programs, the data set optimal speedups are within \u00b11% of the average \nfor more than 98% of the data sets, i.e., there is hardly anydiscrepancyamong the data sets. However, \nat the other end of the spectrum, ten programs exhibit signi.cant variation in performance improvements, \ni.e., the deviationfromtheiraverage performancerangesfrom10%to25%. One can also note that, even for programs \nwith a relatively small average performance improvement, there are some data sets for which the performance \nimprovement is signi.cant. Figure 3 illustrates this further. Here we have sorted all data sets and all \nprograms according to the data set optimal speedup theyachieve. For 14% of data sets, iterative optimization \ncan achieve a speedup of 1.1 or higher, and for nearly half of the data sets, it can achieve a speedup \nof 1.05 or higher. The end conclusion is that iterative compilation yields substantial performance improvements \nacross programs and data sets, and the signi.cance of the improvement is sensitive to the data set. We \nalso .nd that the reactions to compiler optimizations are non-trivial, both across programs, and across \ndata sets for the same program.In Figures4and5, we plot the mean, standarddeviation, data sets:d . D, \n|D| = 1000. optimization combinations:o . O, |O| = 300. speedup of o on d:so d. data set optimal speedup \nof d: soptimal d = max{so d, o . O}. fraction of data set optimal speedup of o on d: fo d = s o d soptimal \nd . program-optimal combination:oopt: ooptwith highest mininum fraction: oopt = o/fo d = maxo.Omin{fo \nd , d . D}. ooptwith highest mininum speedup: oopt = o/so d = maxo.Omin{so d, d . D}. ooptwith highest \naverage fraction: oopt = o/fo d = maxo.Omean{fo d , d . D}. ooptwith highest average speedup: oopt = \no/so d o d   Figure 1. Distribution of (normalized) performance characteristics across data sets (on \na logscale): number of cycles, IPC, L1 instruction cache miss rate, L1 data cache miss rate, L2 cache \nmiss rate, branchmisprediction rate.  (b)sortedby numberof source code lines Figure 2. Data set optimal \nspeedups relative to -fast for KDataSets (the horizontal line shows the mean value). and maximum and \nminimum speedup for each compiler optimiza\u00adtion, sorted by increasing mean speedup; this is done for \nthe .ve programs with the highestaverage speedup across all data sets.A small arrow below the horizontal \naxis indicates the baseline com\u00adbination(-fast). The large variations for the standard deviation show \nthat the impact of each combination can vary strongly: for some programs, it is almost stable across \nall data sets, e.g., gsm (Figure 5(b)), or there are largevariations across data sets (all other examples). \n Figure 3. Distribution of the data set optimal speedups across all data sets and all programs. 3.4 \nKDataSets versus MiDataSets We now compare KDataSets (1000 data sets) against MiDataSets (20 data sets) \n[14]. The reason for doing so is to investigate whether a 1000-data set suite is really needed compared \nto a 20-data set suite. The more fundamental question is whether the 20 data sets in MiDataSets cover \nmostof the spacebuiltupby the 1000 data sets. In other words, do the 1000 data sets cover a much broader \nspectrum of the data set space to justify its usage over the 20 data sets that were previously collected? \nThis is valid question, especially given that the data set optimal speedup are comparable for KDataSets \n(Figure2) compared to MiDataSets (see Figure 6). An in-depth analysis that involves a broad set of both \nmicro\u00adarchitecture-dependent and microarchitecture-independent charac\u00adteristics reveals that the coverage \nof the program behavior space varies signi.cantly across data sets.For that purpose, wehave col\u00adlected \nthe 66 microarchitecture-independent features provided by  Figure 4. Reactions to compiler optimizations \nfor adpcm d. (a) dijkstra (b) gsm (c) jpeg c (d) pgp d   Figure5. Reactions to compiler optimizations \nfor dijkstra,gsm, jpec c, pgp d. the MICA tool v0.22 [18] and9microarchitecture-dependent fea\u00ad tures \nusing hardware performance counters as mentioned in Sec\u00adtion 3.1.For each program, we apply principal \ncomponent analysis (PCA) on the characteristics collected for all data sets, following the methodologyby \nEeckhoutetal.[11].Wethenplotthedatasets along the two most signi.cant principal components in Figure \n7 these principal components capture the most signi.cant dimen\u00adsions; we show plots for only a few representative \nprograms due to paper length constraints. MiDataSets and KDataSets are shown using crosses and dots, \nrespectively. KDataSets coversa larger part of the space than MiDataSets for 25 out of 32 programs, see \nfor example patricia, ghostscript, bitcount, pgp d. KDataSets substantially expands the space covered \ncompared to MiDataSets for4 out of 32 programs, e.g., gsm, stringsearch1. KDataSets and MiDataSets cover \nroughly the same space for three programs, such as lame and adpcm d. We have now made the case that KDataSets \nexhibits signi.\u00adcantly different behavior compared to MiDataSets. However, this does not necessarily \nimply that KDataSets will react very differ\u00adently to compiler optimizations.We therefore conduct yet \nanother  Figure 6. Data set optimal speedups relative to -fast for Mi-DataSets. analysis using PCA with \nthe features being the speedups obtained for each of the combinations, see Figure 8. The conclusion is \nes\u00ad sentially the same as before: the graphs clearly illustrate the greater diversity of reactions to \ncompiler optimizations for KDataSets rel\u00adative to MiDataSets. 4. Iterative Optimization in Practice In \nthis section, we investigate the stability of iterative optimization across data sets and revisit iterative \noptimization in a realistic ex\u00adperimental context for two application scenarios. 4.1 Program-optimal \ncombinations In order to investigate how sensitive the selection of combinations is to data sets, we \n.rst determine the data set optimal speedup, i.e., this is the highest speedup for each data set across \nall 300 combi\u00adnations. Then, for each program, we retain the combination which yields the bestaverage \nperformance across all data sets.We term this combination the program-optimal combination, seeTable2for \na formal de.nition; we will clarify the exact selection process in the next section. In Figure 9, we \nreport the performance for all data sets and for each program compiled with its program-optimal com\u00adbination. \nThe distribution of speedups (relative to -fast)across data sets is shown at the top, while the distribution \nof the fraction of the data set optimal speedup is shown at the bottom; the fraction of the data set \noptimal speedup is the ratio of the program-optimal combination speedup over the data set optimal speedup \n(see also Table2)anditisalwayslessthanorequalto1.Thekey obser\u00ad vation is that, for each program, a single \ncombination can achieve at least 86% of the data set optimal speedup for all data sets, with most programs \nstanding at a minimum of 90% of the data set opti\u00admal speedup. As mentioned in the introduction, the \nconsequences are signi.cant. This result con.rms that iterativeoptimization is ro\u00adbust across data sets: \nafter learningovera suf.cient numberof data sets, the selected best tradeoffcombination is likely to \nperform well on yet unseen data sets. 4.2 How to select the program-optimal combination We now explain \nin more detail our strategy for selecting the program-optimal combination. InTable 3(a), we show an illustra\u00ad \ntiveexample withtwodata sets D1 and D2 and three combinations comb1, comb2 and comb3 plus the baseline(-fast \nfor ICC).For each program, we evaluate every combination on every data set, deduce the data set optimal \nspeedup, and, in the next two columns, compute the fraction of the data set optimal speedup achieved \nby every combination onevery data set.We thenwantto selecta com\u00adbination that maximizes performance across \nall data sets. So we .nd the minimum fraction of the data set optimal speedup achieved byeach combination \n(rightmost column), and pick the combination with the highest minimum fraction of the data set optimal \nspeedup.  Figure 7. Data set characterization using principal component analysis: the features are a \nset of microarchitecture-dependent and microarchitecture-independent characteristics; KDataSets is shown \nusing gray dots, and MiDataSets is shown using blackcrosses. Figure 8. Data set characterization using \nprincipal component analysis: the features are the speedup numbers across the 300 combinations of compiler \noptimizations; KDataSets is shown using gray dots, and MiDataSets is shown using black crosses.  Figure \n9. Speedup (top graph) and fraction of the data set optimal speedup (bottom graph) for the program-optimal \ncombinations. (a) (b)  Speedup Fraction data set optimal speedup Comb. D1 D2 Avg. D1 D2 Min baseline \n1.00 1.00 1.00 0.77 0.91 0.77 comb1 1.20 1.10 1.15 0.92 1.00 0.92 comb2 1.30 1.07 1.19 1.00 0.97 0.97 \ncomb3 1.25 1.05 1.15 0.96 0.95 0.95 data set optimal 1.30 1.10 1.20 1.00 1.00 1.00 Speedup Fraction data \nset\u00adoptimal speedup Comb. D1 D2 Avg. D1 D2 Min baseline 1.00 1.00 1.00 0.77 0.99 0.77 comb1 1.20 1.01 \n1.11 0.92 1.00 0.92 comb2 1.30 0.98 1.14 1.00 0.97 0.97 comb3 1.25 0.99 1.12 0.96 0.98 0.96 data set \noptimal 1.30 1.01 1.16 1.00 1.00 1.00  Table 3. Illustrative examples showing (a) how to select the \nprogram-optimal combination with the highest minimum fraction of the data set optimal speedup; (b) shows \na case for which the program-optimal combination leads to slowdown relative to the baseline. Program-optimal \ncombinations may sometimes, though rarely, induce slowdowns compared to the baseline. Let us de.ne the \ndata set optimal speedup for each data set as B;and let us de.ne the fraction of the data set optimal \nspeedup that the program-optimal speedup achieves as M.Ifthe data set optimal speedupis small,it maybethecase \nthat B \u00d7 M< 1,i.e.,aslowdown comparedtothe baseline. Because this only happens when B is small, the resulting \nslowdown is small as well. Consider the example in Table 3(b): because the data set optimal speedup for \ndata set D2 is small, the fraction of the data set optimal speedup for comb2 and comb3 is high even though \nit induces a slight slowdown. As a result, comb2 gets selected as the combination with the highest minimum \nfraction of the data set optimal speedup; on average across D1 and D2, it does inducea signi.cant average \nspeedup,but thereisa slight slowdown for D2. In Figure 9, we can see that this phenomenon happens (in \ntop graph, violin part below speedup equal to one), though infrequently. For instance, programs such \nas patricia exhibit slowdowns for a few of their data sets. It is relatively more frequent for a couple \nprograms such as rijndael e; the most extreme example is CRC32 for which the program-optimal combination \nyields a slowdown on average.  4.3 Alternative waysfor selectingprogram-optimal combinations There \naredifferentwaysforhowto determinethe program-optimal combination. Depending on the user s objectives, \nit is possible to tune the selection of the program-optimal combination to minimize risk (maximize the \nminimum speedup), or to maximize average performance (maximize average speedup or maximize the average \nfraction of the data set optimal speedup). As an example, we show in Figure 10 the performance for program-optimal \ncombinations of CRC32 selected using these different criteria. While someexhibit an average slowdown, \nsome result in an average speedup. Below, we study these different selection criteria and outlineageneral \nstrategy for selecting program-optimal combinations. We consider four different criteria for selecting \nthe program\u00adoptimal combination: pick the combination that maximizes (i) the average speedup (across \nall data sets), or (ii) the minimum speedup, or(iii)the minimum fractionofthedatasetoptimal speedup,or(iv) \nthe average fraction of the best speedup. While the best criterion really depends on the user s objective, \nwe propose the compromise selection strategy that performs well under all criteria. The strategy determines \na set of program-optimal combinations that perform best under each criterion: each set for a criterion \ncontains the combinations that achieve at least P % of the optimal value for that criterion. Then, we \ndetermine the intersection of these four sets of combinations. Each combination in this intersection \nis eligible to be a compromise program-optimal combination; we randomly pick a compromise program-optimal \ncombination within that intersection. If the intersectionis empty, welower thevalueofP (from 100% downwards) \nand try again. In Figure 11, we show the fraction of programs for which we can .nd at least one combination \nthat achieves at least P % of the optimalvalue for all four criteria.For all programs,itis possible to \n.nd combinations that achieve 91% of the optimal value of all criteria; 95% of the optimal value is achieved \nfor all but one program. As a result, the end conclusion is that, independently of the user s objective, \nthere is almost always a combination that performswellforevery criterion,atleastamongthosetestedabove. \n 4.4 How many data sets are needed One remainingissueforhavingatrulyrobust program optimization approach \nis to understand how manydata sets are required in order to .nd a good program-optimal combination. \nProgram Percentage of program-optimal performance 85% 90% 95% 99% adpcm d 2 3 5 11 dijkstra 2 2 2 5 gsm \n2 2 2 7 jpeg c 1 1 2 21 pgp d 7 12 21 40 ispell 2 2 2 3 patricia 1 2 2 3 blow.sh d 1 2 6 154 blow.sh \ne 1 2 2 54 susan e 122 145 179 259 tiffmedian 2 2 2 153 bzip2e 2 2 2 467 lame 2 2 2 2 bitcount 1 1 1 \n1 tiffdither 1 1 2 2 ghostscript 2 2 2 90 qsort1 1 2 16 34 bzip2d 1 32 74 340 pgp e 2 2 2 294 rsynth \n1 2 17 43 rijndael e 2 20 130 623 mad 2 2 2 7 CRC32 2 2 138 326 rijndael d 2 4 59 845 tiff2bw 1 1 2 615 \nadpcm c 1 2 2 6 sha 8 21 41 231 jpeg d 1 2 2 8 tiff2rgba 1 1 2 856 susan c 1 1 2 7 stringsearch1 2 2 \n8 760 susan s 2 2 60 119 Figure 12. Iterative optimization usinga single data set versus us\u00ading 1000 \ndata sets; the horizontal line corresponds to the aver\u00adage speedup using the program-optimal combination \nselected over 1000 data sets.  Table 4. Minimum number of data sets required to select a combi\u00adnation \nachieving X% of the program-optimal speedup; the bench\u00admarks are sortedby their average data set optimal \nspeedups. To do so, we set up the following experiment. We randomly sample N (out of 1000) data sets, \nselect the program-optimal com\u00adbination across these N data sets, evaluate its performance on all of \nthe 1000 data sets, and deduce what percentage of the program\u00adoptimal combination performance is achieved. \nFor each N, we repeat this process 1000 times and compute the average percent\u00adage.We then report the \nminimum number of data sets that are re\u00adquired to select a combination with a performance within 85%, \n90%, 95% and 99% of the performance obtained when comput\u00ading the program-optimal combination across all \n1000 data sets, see Table 4. The resultsvary across programs.For14outofthe32 programs, we achieve 85% \nof the program-optimal performance using only a single data set; only1program can achieve 95% of the \nprogram\u00adoptimal performance with a single data set. For 18 programs, at least2datasetsareneededtoachievewithin85%ofthe \nprogram\u00adoptimal speedup. The top .ve programs still need 6 data sets on averageto achieve95%ofthe program-optimal \nperformance.Four programs require several tens, or even several hundreds, of data sets to achieve 90% \nof the program-optimal performance, and 14 programs require between 154 and 856 data sets to achieve \n99% of the program-optimal performance. In summary,about nine-tenths of the programs require from one \nto a few tens of data sets to achieve near-optimal performance, and the remaining tenth requires hundreds \nof data sets. 4.5 Evaluating iterative optimization within a realistic experimental context The twomost \ntypical use cases for iterativeoptimization are: (i) the softwarevendor optimizes the software before \nshipping, and (ii) the end user is willing to tune the software she/he is using. There is one signi.cant \ndifference between these two cases for the iterativeopti\u00admization learning process.Inthe .rst case,the \nsoftwarevendor can repeatedly run the same data sets, and thus easily compare the per\u00adformance impact \nof combinations across runs. In the second case, the user is only doing production runs, so each run \ncorresponds to a different data set. Most users are not willing to dedicate time, i.e., theydo not want \nto perform repeated runs of the same data sets, in order to tune their compiler combinations; they instead \nwant this process to happen transparently. As a result, the iterative optimiza\u00adtion process must learn \nacross different data sets, making it dif.cult to compare the performance impact of compiler combinations. \n Until now, due to the lack of a large number of data sets, itera\u00adtive optimization was evaluated in \neither scenario using unrealistic hypotheses. In the pre-shipping case, the evaluation was often con\u00adducted \nusing a single data set. In the post-shipping case, the eval\u00aduation assumed that a user would be willing \nto run the same data sets multiple times. In the sections below, we use our large num\u00adber of data sets \nto create realistic conditions for both scenarios, and to truly assess the potential performance bene.ts \nof iterative opti\u00admization in each scenario. 4.5.1 Pre-shipping scenario In this scenario, we assess \nthe performance bene.t of tuning a software using a large number of data sets rather than a single one, \nsee Figure 12.For the one-data set case, we compute the speedup as follows.For each program, we pick \ndata sets oneby one.For each data set, we run all possible combinations, select the combination with \nthe maximum speedup, and apply it to all other 999 data sets; the average data set speedup is computed \nover the 999 data sets and the training data set.Wereport the corresponding distribution in Figure 12, \nas well as the average speedup obtained with the program-optimal combination selected using 1000 data \nsets (thick horizontal bar). Thefact that thebulk of most violins is below the bar,and in most cases \nwell belowthe bar,indicates that this training also often leads to sub-optimal performance. Thefact that \nseveral violins are elongated also indicates that single-data set training is unstable/unreliable, i.e., \nperformance will vary wildly depending on which data set is chosen.  4.6 Post-shipping scenario In the \npost-shipping scenario, since we can only run each data set once, it is impossible to compute the speedup. \nThis experimental context differs in two ways from the unrealistic context used in most iterative optimization \nstudies. First, the relative ordering of combinations is imprecise: an experiment with a particular data \nset may conclude that one combination outperforms another, whereas an experiment with another data set \nmay conclude the opposite. Second, the number of combinations that can be evaluated is lim\u00adited by the \nnumber of available data sets.  In order to compare combinations, we resort to the run-time approach \nproposed by Stephenson et al. [30] and Fursin et al. [13]. We use function cloning where each clone is \ncompiled differently; this is akin to versioning: the code contains two versions of the same routines \ncompiled using two different combinations. At run\u00adtime, either version of each method is randomly used \nupon each call, and the execution time is collected for each call, allowing to compare the sum of the \nexecution times of the methods for each version. Even though this comparison is statistical, because \nof the discrepancies in numbers and durations of calls, previous research works have shown that it is \nreliable in practice and comes with low overhead [13, 30]. In order to realistically emulate iterative \noptimization in a post\u00adshipping scenario, we must evaluate the combination selection strategies on unseen \ndata sets. So we split the data sets into 900 training data sets and 100 test data sets.We randomly de.ne \none thousand such 900/100 training/test sets, and conduct the evalua\u00adtions below for all sets, and report \nthe average results. We now have to select the set of combinations to be evaluated. When optimizing one \nprogram, it is realistic to leverage the expe\u00adriencegathered from other programs. Therefore, for each \nprogram, we start with a set of high-potential combinations; these are the program-optimal combinations \nfor the other 31 programs. In or\u00ad N\u00d7(N-1) der to evaluate N = 31 combinations, we need 2 = 465 comparisons \nand therefore need that manydata sets.With 900 data sets at our disposal, we can increase the initial \nset of 31 combina\u00adtions with Nr randomly selected combinations, in order to increase the probability \nof discovering new better suited combinations. The )\u00d7(N+Nr-1) only constraint is that (N+Nr2 = 900;we \n.nd that the maximum value of Nr is 11, so we add 11 randomly selected com\u00adbinations, for a total of \n42 combinations. We now compare the realistic case where 42 combinations are evaluated using 42\u00d741 = \n861 runs versus the unrealistic case 2 where the same set of combinations are evaluated on all data \nsets. The comparison between the realistic case, see the Realistic bars, and the unrealistic case, see \nthe 42 combs, speedups bars in Figure 13, helps discriminate the impact of comparing on distinct data \nsets. While the performance obtained under the realistic experimental conditions is slightly below the \nunrealistic case, we .nd that it is often close for most programs, and barely different on average. -fast \n-.nline-limit=8 -no-vec -fast -ansi-alias -.nline -no-vec -fast -no-vec -O2 -fast -nolib-inline -fast \n-fno-builtin -no-inline-factor -fast -inline-max-total-size=3000 -no-inline-factor -fast -inline-max-size=1024 \n-ipo3 -fast -no-inline-factor -unroll8 -fast -fno-fnalias -fno-omit-frame-pointer -no-vec -fast -fno-alias \n-fpack-struct -no-vec -fast -ansi-alias -.nline-limit=512 -nolib-inline -fast -.nline-limit=512 -opt-jump-tables=large \n-O3 -ipo1 -no-opt-jump-tables -fast -no-inline-min-size -fast -.nline-limit=32 -fno-builtin -opt-malloc-options=2 \n-fast -ip-no-inlining -nolib-inline -opt-malloc-options=1 -fast -ipo2 Table 5. The Intel ICC compiler \noptimizations that affect perfor\u00admance the most, obtained after pruning the program-optimal com\u00adbinations. \nWealso comparethe realistic caseagainsta casewhereall com\u00adbinations are compared, though using distinct \ndata sets, in order to discriminate the impact of evaluating a restricted set of combina\u00ad 300\u00d7299 tions. \nSince we do not have 2 = 44850 data sets, we ap\u00adproximate this scenario by reusing data sets (i.e., for \ncomparing each pair of combinations, we randomly pick a data set), and re\u00adport the result in Figure 13, \nsee 300 combs, comparisons .We again .nd that the performance of the realistic case is very close to \nthe performance of the less realistic case. This result is signi.cant. It implies that, even under realistic \nex\u00adperimental conditions, it is possible to achievealmost the same per\u00adformance as the upper bound performance \nde.ned by less realistic experimental conditions. In other words, the potential of iterative optimization \ncan indeed be achieved in practice, even in a post\u00adshipping scenario. 4.7 Analysis:Key compiler optimizations \nWe now delve a little deeper in the composition of program\u00adoptimal combinations. These combinations may \ncontain more than 50 optimization .ags, making it dif.cult to understand which op\u00adtimizations really \nimpact performance. We therefore prune the program-optimal combinations by simply removing optimization \n.ags one by one, randomly, as long as they do not change the overall speedup.Table5 shows the top performing \ncombinations after pruning across all programs and data sets for ICC. (There are fewer pruned combinations \nthan programs: program-optimal com\u00adbinations may prune to the same combinations or the baseline.) This \nlist of top combinations underscores that, even though mod\u00adern compilers have powerful optimizations \nsuch as vectorization, inter-procedural analysis, etc., the heuristics in charge of activat\u00ading these \ndifferent optimizations and selecting their parameters may notbeeffective.Forexample, the -fast optimization \nlevel in ICC includes inlining andvectorizationby default,but both opti\u00admizations may degrade performance. \nIterative optimization does a better job at applying these optimizations to particular programs: e.g., \nonly apply inlining to programs with small functions, or turn off vectorization when it is detrimental \nto performance. 5. Discussion on scope of the results The results in this paper are obviously tied to \nthe experimental setup. In this section, we aim atgaining insight into how general the results and conclusions \nare.  5.1 Resultsfor GNU s GCC We used Intel s ICC compiler throughout the paper.We noweval\u00aduate whether \nthe conclusions also apply to other compilers. We therefore consider GNU s GCC compiler v4.4.1. Figure \n14 reports the data set optimal speedups (with respect to -O3)that can be achieved using GCC. Compared \nto Figure 9, we observe that GCC achieves higher speedups than ICC this is relativeto the -O3 and -fast \nbaseline combinations, respectively. It is also interesting to note that the best speedups are achieved \nfor different programs for the most part. More importantly, we .nd again that, for each pro\u00adgram,asingle \ncombinationcanachieve83%ofthedatasetoptimal speedup for all data sets, with most programs standing at \n90% or higher. Therefore, our conclusions are valid for two of the most widely used compilation platforms. \n 5.2 Data sets coverage There is no guarantee that our collection of 1000 data sets covers the entire \nprogram behavior space there may still be parts of the space that are unrepresentedby KDataSets.For \ninstance, our initial attempt at collecting data sets for program tiffmedian resulted in the distribution \nshown in Figure 15(a). The 20 data sets did in fact a better job at covering the program behavior space \nthan our initial 1000 data sets. This turned out to be due to the lack of very small images(< 29kB). \nThis is because these small images exhibit a signi.cantly different L1, L2 and DTLB behavior than large \nimages(29kB - 1.5MB), resulting in signi.cantly different and more spread out program behavior. After \nincreasing the mix of small and large images, the coverage of the 1000 data sets was eventually superior \nto that of the 20 data sets, as shown in Figure 15(b); the circles in Figure 15(b) correspond to the \ndata sets added to our initial set, the withdrawn redundant data sets do not show. However, even the \nnotion of covering the program behavior space is subjective because a given user may be interested in \na limited type of data sets only. 5.3 Compiler combinations There is also no guarantee that the 300 \ncombinations we explored covers the entire compiler optimization space.To see whether 300 combinations \nis too small a number, we conducted an additional experiment in which we consider 8000 combinations. \nHowever, to complete it in a reasonable amount of time, we had to run each combination on 10 randomly \nchosen data sets instead of all the 1000 data sets. The data set optimal speedups across these 8000 \n(a) initial 1000 data sets (b) including small images  Figure 16. Data set optimal speedups relative \nto -fast for 8000 combinations. combinations are reported in Figure 16. Compared to Figure 2(a), except \nfor a few data sets (e.g., in sha and adpcm d), we observe no signi.cant differenceoverall, which suggests \nthat 300 combina\u00adtions may be enough to represent the optimization space that can be explored in a reasonable \ntime window. 5.4 Platforms and benchmarks More generally, we cannot assert that our conclusions generalize \nbeyond our benchmark suite, compiler platforms (Intel ICC and GNU GCC) and target architecture. There \nare also obvious exam\u00adples where the performance of program segments is data set sen\u00adsitive; auto-tuned \nlibraries like FFTW [25] orATLAS [31] areex\u00ad amples where data set dependent tuning is useful. Nevertheless, \nour results suggest that, in general, the data set sensitivity problem may have beenoverstated.We also \nnote that our results are consistent across all benchmarks sofar. 5.5 Fine-grain iterative optimization \nFor now, we only considered whole-program optimizations, i.e., a combination of compiler optimizations \nis applied across the en\u00adtire program. However, a more .ne-grain iterative optimization ap\u00adproach may \napply different combinations to different sections of the code (e.g., functions or loops).Fine-grain \niterative optimization is more complicated becauseitintroduces optimization dependences (and possibly \nripple effects): e.g., applying a combination to func\u00adtion F1 can affect the behavior of another function \nF2, and thus the choice of its best combination. This dependence complicates the problem as well as largely \nincreases the design space.We willex\u00adplore practical ways to tackle that issue as part of our future \nwork. 5.6 Measurement bias Recent work by Mytkowicz [26] raises the issue of measurement bias, and provides \nevidence for two sources of measurement bias, namely link order and environment size.We believe that \nlink or\u00adder should not be considered measurement bias in the context of iterative optimization. Link \norder should rather be viewed as an\u00adother opportunity for iterative optimization. Environment size af\u00adfectsonly \none fourthofthe benchmark programsin[26]bya small margin only (within \u00b12% using ICC). In our study, 73% \nof pro\u00adgram/dataset pairs have a speedup that is higher than 1.02, and are thus relatively immune to \nthis source of measurement bias. 6. RelatedWork We already mentioned in the introduction that several \nstudies have investigated the impact of data sets on program behavior and per\u00adformance. And some of these \nhave even looked how data sets af\u00adfect program optimization decisions. Most of these works remain limited \nby the restricted number of data sets available in existing benchmark suites. The SPEC CPU2000 suite \ncontains3input sets per benchmark, and most benchmarks have less than 10 data sets in the SPEC CPU2006. \nThe embedded EEMBC benchmark suite [1] also contains less than 10 data sets for most benchmarks. The \nre\u00adcently introduced parallelPARSEC benchmark suite [7] contains 6data sets for each benchmark.Alarge \nnumberof data setsis not only useful for compiler research andworkload characterization re\u00adsearch (e.g., \n[21]), manyarchitecture studies rely on pro.le-based optimization techniques as well [23, 28], and may \nbene.t from hav\u00ad ing more data sets in order to study data set sensitivity. Several studies have investigated \nredundancy among data sets and how to .nd a minimal set of representative programs and inputsfor architecture \nresearch[11].Weuseseveralofthe proposed statistical techniques and feature characterization approaches \nin this article to investigate our own data set suite. Finally, we have mentioned the broad set of iterative/adaptive \ncompilation techniques which attempt to .nd the best possible compiler optimizations by stochastically \nscanning the set of all possible combinations [4, 10, 12, 17, 19, 22, 29]. They have demonstrated that \noptimizations search techniques can effectively improve performance of statically compiled programs on \nrapidly evolving architectures, thereby outperforming state-of-the-art com\u00adpilers, albeitatthe costofalarge \nnumberofexploration runs.Many of these researchworkshaveshownhow machine-learningand sta\u00adtistical techniques \n[4, 9, 12, 22, 29] can be used to select or tune program transformations based on program features. Most \nof these works also requirea large numberof training runs. For instance, Stephenson et al. [30] and Arnold \net al. [5] collect pro.le infor\u00ad mation across multiple runs of a Java program to selectively apply run-time \noptimizations. 7. Conclusions and FutureWork Using KDataSets, a collection of 1000 data sets for 32 programs, \nwe investigate a fundamental issue in iterative optimization, which could not be thoroughly evaluated \nup to now: whether it is pos\u00adsible to learn the best possible compiler optimizations across dis\u00adtinct \ndata sets.Weconclude that the issue seems signi.cantly more simple than previously anticipated, with \nthe ability to .nd a near\u00adoptimal combination of compiler optimizations across all data sets. We outline \na process for selecting the program-optimal combina\u00adtion, and we investigate the impact of performing \niterative opti\u00admization in an unrealistic context. For now, we have investigated whole-program optimizations, \nand we intend to study whether the same conclusions are sustained for .ne-grain optimizations.We also \nintend to apply our conclu\u00adsions to datacenters where, typically, a few programs are run a very large \nnumber of times, and where any execution time reduc\u00adtion translates into proportionalgainsin datacenter \nequipment and operating costs. Acknowledgments We would like to thank the anonymous reviewers for their \nvalu\u00adable feedback. Lieven Eeckhout is supported through the FWO projects G.0232.06, G.0255.08, and G.0179.10, \nand the UGent-BOF projects 01J14407 and 01Z04109. Olivier Temam is sup\u00adported by the IST FP7 HiPEAC2 \ncontract No. IST-217068. Olivier Temam and Grigori Fursin are supported by EU FP6 MILEPOST project contract \nNo. IST-35307. The rest of the authors are sup\u00adported by National Natural Science Foundation of China \nunder grant No. 60873057 and 60921002, and National Basic Research Program of China under grant No. 2005CB321602. \nReferences [1] EEMBC: The Embedded Microprocessor Benchmark Consortium. http://www.eembc.org. [2] cBench: \nCollective Benchmarks. http://www.ctuning.org/ cbench. [3]PAPI:APortable Interfaceto Hardware Performance \nCounters. http: //icl.cs.utk.edu/papi. [4] F. Agakov, E. Bonilla, J. Cavazos, B. Franke, G. Fursin, M. \nF. P. O Boyle, J. Thomson, M. Toussaint, and C. K. I. Williams. Using machine learning to focus iterative \noptimization. In Proceedings of the International Symposium on Code Generation and Optimization (CGO), \npages 295 305, March 2006. [5] M. Arnold, A.Welc, andV.T.Rajan. Improving virtual machine per\u00adformance \nusing a cross-run pro.le repository. In Proceedings of the ACM Conference on Object-Oriented Programming, \nSystems, Lan\u00adguages and Applications (OOPSLA), pages 297 311, October 2005. [6] P. Berube and J. Amaral. \nAestimo: a feedback-directed optimization evaluation tool. In Proceedings of the IEEE International Symposium \non Performance Analysis of Systems and Software (ISPASS), pages 251 260, March 2006. [7] C. Bienia,S.Kumar,J.P. \nSingh, andK. Li. ThePARSEC benchmark suite: characterization and architectural implications. In Proceedings \nof the 17th International Conference on Parallel Architectures and CompilationTechniques(PACT), pages \n72 81, October 2008. [8]J.Cavazos,G. Fursin,F.Agakov,E. Bonilla,M.F.P. O Boyle,and O. Temam. Rapidly \nselecting good compiler optimizations using performance counters. In Proceedings of the International \nSymposium on Code Generation and Optimization (CGO), pages 185 197, March 2007. [9] K. Cooper,P. Schielke, \nand D. Subramanian. Optimizing for reduced code space using genetic algorithms. In Proceedings of the \nConference on Languages, Compilers, andTools for Embedded Systems (LCTES), pages 1 9, July 1999. [10] \nK. D. Cooper, A. Grosul, T. J. Harvey, S. Reeves, D. Subramanian, L. Torczon, and T. Waterman. ACME: \nadaptive compilation made ef.cient. In Proceedings of theACM SIGPLAN/SIGBED Conference on Languages, \nCompilers, andTools for Embedded Systems (LCTES), pages 69 77, July 2005. [11] L. Eeckhout, H.Vandierendonck, \nand K. De Bosschere. Quantifying the impact of input data sets on program behavior and its applications. \nJournalof Instruction-LevelParallelism, 5:1 33, February 2003. [12] B. Franke, M. O Boyle, J. Thomson, \nand G. Fursin. Probabilistic source-level optimisation of embedded programs. In Proceedings of the ACM \nSIGPLAN/SIGBED Conference on Languages, Compilers, andTools for Embedded Systems (LCTES), pages 78 86, \nJuly 2005. [13] G. Fursin and O.Temam. Collective optimization. In Proceedings of the International Conference \non HighPerformance Embedded Archi\u00adtectures&#38;Compilers (HiPEAC), pages 34 49, January 2009. [14] G. \nFursin, J. Cavazos, M. O Boyle, and O. Temam. Midatasets: Creating the conditions for a more realistic \nevaluation of iterative optimization. In Proceedings of the International Conference on High Performance \nEmbedded Architectures&#38;Compilers (HiPEAC), pages 245 260, January 2007. [15] M. Guthaus, J. Ringenberg, \nD. Ernst, T. Austin, T. Mudge, and R. Brown. Mibench:Afree, commercially representative embedded benchmark \nsuite. In Proceedings of the IEEEFourth Annual Interna\u00adtionalWorkshop onWorkload Characterization (WWC), \npages 3 14, December 2001. [16] M. Haneda,P. Knijnenburg, and H.Wijshoff. On the impact of data input \nsets on statistical compiler tuning. In Proceedings of the 20th IEEE International Parallel and Distributed \nProcessing Symposium (IPDPS), April 2006. [17] K. Hoste and L. Eeckhout. Cole: compiler optimization \nlevel explo\u00adration. In Proceedings of the Sixth Annual IEEE/ACM International Symposium on Code Generation \nand Optimization (CGO),pages 165 174, April 2008. [18] K. Hoste and L. Eeckhout. Comparing benchmarks \nusing key microarchitecture-independent characteristics. In Proceedings of the IEEE International Symposium \non Workload Characterization (IISWC), pages 83 92, October 2006. [19] K. Hoste, A. Georges, and L. Eeckhout. \nAutomated just-in-time com\u00adpiler tuning. In Proceedings of the Eighth Annual IEEE/ACM Inter\u00adnational \nSymposium on Code Generation and Optimization (CGO), April 2010. [20]W.C. Hsu,H. Chen,P.C.Yew, and D.-Y. \nChen. On the predictability of program behavior using different input data sets. In Proceedings of the \nSixth AnnualWorkshop on Interaction between Compilers and Computer Architectures (INTERACT), pages 45 \n53, February 2002. [21] Y. Jiang, E. Z. Zhang, K. Tian, F. Mao, M. Gethers, X. Shen, and Y. Gao. Exploiting \nstatistical correlations for proactive prediction of program behaviors. In Proceedings of the International \nSymposium on Code Generation and Optimization (CGO), April 2010. [22]P.Kulkarni,S. Hines,J. Hiser,D. \nWhalley,J.Davidson, andD. Jones. Fast searches for effective optimization phase sequences. InProceed\u00adings \nof the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI), pages 171 182, \nJune 2004. [23] G. Magklis, M. L. Scott, G. Semeraro, D. H. Albonesi, and S. Dropsho. Pro.le-based dynamic \nvoltage and frequency scaling for a multiple clock domain microprocessor. In Proceedings of the 30th \nAnnual International Symposium on Computer Architecture(ISCA),pages 14 27, June 2003. [24] F. Mao, E. \nZ. Zhang, and X. Shen. In.uence of program inputs on the selection ofgarbage collectors. In Proceedings \nof theACM SIG-PLAN/SIGOPS International Conference onVirtual Execution Envi\u00adronments (VEE), pages 91 \n100, March 2009. [25] F. Matteo and S. Johnson. FFTW: An adaptive software architecture for the FFT. \nIn Proceedings of the IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), \nvolume 3, pages 1381 1384, May 1998. [26]T.Mytkowicz,A.Diwan,M. Hauswirth,andP.F. Sweeney. Producing \nwrong data without doing anything obviously wrong! In Proceeding of the 14th International Conference \non Architectural Support for Programming Languages and Operating Systems (ASPLOS), pages 265 276, February \n2009. [27]Z.PanandR. Eigenmann.Fastandeffectiveorchestrationof compiler optimizations for automatic performance \ntuning. In Proceedings of the International Symposium on Code Generation and Optimization (CGO), pages \n319 332, March 2006. [28] K. Sankaranarayanan and K. Skadron. Pro.le-based adaptation for cache decay. \nACMTransactions on Architecture and Code Optimiza-tion(TACO), 1:305 322, September 2004. [29] M. Stephenson, \nM. Martin, and U. O Reilly. Meta optimization: Improving compiler heuristics with machine learning. In \nProceedings oftheACM SIGPLAN ConferenceonProgrammingLanguageDesign and Implementation (PLDI), pages 77 \n90, June 2003. [30] M.W. Stephenson. Automating the Construction of Compiler Heuris\u00adtics Using Machine \nLearning. PhD thesis, MIT, USA, January 2006. [31] R. C. Whaley, A. Petitet, and J. Dongarra. Automated \nempirical optimization of software and the atlas project. In Parallel Computing, March 2001. [32] Y. \nZhong, X. Shen, and C. Ding. Program locality analysis using reuse distance. Transactions on Programming \nLanguages and Systems (TOPLAS), 31(6):1 39, Aug. 2009.   \n\t\t\t", "proc_id": "1806596", "abstract": "<p>While iterative optimization has become a popular compiler optimization approach, it is based on a premise which has never been truly evaluated: that it is possible to learn the best compiler optimizations across data sets. Up to now, most iterative optimization studies find the best optimizations through repeated runs on the same data set. Only a handful of studies have attempted to exercise iterative optimization on a few tens of data sets.</p> <p>In this paper, we truly put iterative compilation to the test for the first time by evaluating its effectiveness across a large number of data sets. We therefore compose KDataSets, a data set suite with 1000 data sets for 32 programs, which we release to the public. We characterize the diversity of KDataSets, and subsequently use it to evaluate iterative optimization.We demonstrate that it is possible to derive a robust iterative optimization strategy across data sets: for all 32 programs, we find that there exists at least one combination of compiler optimizations that achieves 86% or more of the best possible speedup across <i>all</i> data sets using Intel's ICC (83% for GNU's GCC). This optimal combination is program-specific and yields speedups up to 1.71 on ICC and 2.23 on GCC over the highest optimization level (-fast and -O3, respectively). This finding makes the task of optimizing programs across data sets much easier than previously anticipated, and it paves the way for the practical and reliable usage of iterative optimization. Finally, we derive pre-shipping and post-shipping optimization strategies for software vendors.</p>", "authors": [{"name": "Yang Chen", "author_profile_id": "81464667415", "affiliation": "Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China", "person_id": "P2184618", "email_address": "", "orcid_id": ""}, {"name": "Yuanjie Huang", "author_profile_id": "81464657618", "affiliation": "Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China", "person_id": "P2184619", "email_address": "", "orcid_id": ""}, {"name": "Lieven Eeckhout", "author_profile_id": "81330490198", "affiliation": "Ghent University, Gent, Belgium", "person_id": "P2184620", "email_address": "", "orcid_id": ""}, {"name": "Grigori Fursin", "author_profile_id": "81100024385", "affiliation": "National Institute for Research in Computer and Control Sciences (INRIA), Saclay, France", "person_id": "P2184621", "email_address": "", "orcid_id": ""}, {"name": "Liang Peng", "author_profile_id": "81464655578", "affiliation": "Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China", "person_id": "P2184622", "email_address": "", "orcid_id": ""}, {"name": "Olivier Temam", "author_profile_id": "81100032294", "affiliation": "National Institute for Research in Computer and Control Sciences (INRIA), Saclay, France", "person_id": "P2184623", "email_address": "", "orcid_id": ""}, {"name": "Chengyong Wu", "author_profile_id": "81464653854", "affiliation": "Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China", "person_id": "P2184624", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1806596.1806647", "year": "2010", "article_id": "1806647", "conference": "PLDI", "title": "Evaluating iterative optimization across 1000 datasets", "url": "http://dl.acm.org/citation.cfm?id=1806647"}