{"article_publication_date": "06-05-2010", "fulltext": "\n PACER: Proportional Detection of Data Races * Michael D. Bond Katherine E. Coons Kathryn S. McKinley \nDepartment of Computer Science, The University of Texas at Austin {mikebond,coonske,mckinley}@cs.utexas.edu \nAbstract Data races indicate serious concurrency bugs such as order, atom\u00adicity, and sequential consistency \nviolations. Races are dif.cult to .nd and .x, often manifesting only after deployment. The fre\u00adquency \nand unpredictability of these bugs will only increase as software adds parallelism to exploit multicore \nhardware. Unfortu\u00adnately, sound and precise race detectors slow programs by factors of eight or more \nand do not scale to large numbers of threads. This paper presents a precise, low-overhead sampling-based \ndata race detector called PACER. PACER makes a proportionality guarantee: it detects any race at a rate \nequal to the sampling rate, by .nding races whose .rst access occurs during a global sam\u00adpling period. \nDuring sampling, PACER tracks all accesses using the dynamically sound and precise FASTTRACK algorithm. \nIn non\u00adsampling periods, PACER discards sampled access information that cannot be part of a reported \nrace, and PACER simpli.es tracking of the happens-before relationship, yielding near-constant, instead \nof linear, overheads. Experimental results con.rm our theoretical guarantees. PACER reports races in \nproportion to the sampling rate. Its time and space overheads scale with the sampling rate, and sampling \nrates of 1-3% yield overheads low enough to consider in production software. The resulting system provides \na get what you pay for approach that is suitable for identifying real, hard-to\u00adreproduce races in deployed \nsystems. Categories and Subject Descriptors D.3.4 [Programming Lan\u00adguages]: Processors Debuggers, Run-time \nenvironments; D.2.5 [Software Engineering]: Testing and Debugging Debugging aids, Testing tools General \nTerms Reliability, Performance, Experimentation Keywords Concurrency, Data Races, Bugs, Sampling 1. Introduction \nSoftware must become more parallel to exploit hardware trends, which are increasing the number of processors \non each chip. Unfor\u00adtunately, correct and scalable multithreaded programming is quite * This work is \nsupported by NSF SHF-0910818, NSF CSR-0917191, NSF CCF-0811524, NSF CNS-0719966, a Microsoft Research \nPh.D. Fellow\u00adship, Intel, and Google. Any opinions, .ndings, and conclusions expressed herein are the \nauthors and do not necessarily re.ect those of the sponsors. Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 10, June 5 10, 2010, Toronto, Ontario, \nCanada. Copyright c &#38;#169; 2010 ACM 978-1-4503-0019-3/10/06. . . $10.00 challenging. It is notoriously \ndif.cult to specify program synchro\u00adnization, i.e., the ways in which threads may interleave operations \non shared data. Too much synchronization degrades performance and causes deadlock, while missing synchronization \ncauses unin\u00adtended interleavings. A data race occurs when two accesses to the same variable, one of which \nis a write, do not correctly synchro\u00adnize. While data races are not necessarily errors in and of them\u00adselves, \nthey indicate a variety of serious concurrency errors that are dif.cult to reproduce and debug such as \natomicity violations [25], order violations [24], and sequential consistency violations [26]. Because \nsome races occur only under certain inputs, environments, or thread schedules, deployed low-overhead \nrace detection is nec\u00adessary to achieve highly robust deployed software. Static techniques for detecting \nraces scale to large programs and try to limit false positives, typically by being unsound in a few lim\u00adited \nways [29; 33; 38]. Precision (no false positives) is important because both false and true data race \nreports take lots of devel\u00adoper time to understand, and thus developers are not embracing approaches \nthat report false positives. Dynamic analysis typically uses either lockset or vector clock algorithms. \nLockset algorithms reveal errors in locking disciplines, but are imprecise. Vector clock algorithms are \nprecise and now achieve about the same perfor\u00admance as lockset algorithms [14]. Vector clock-based race \ndetection is precise because it tracks the happens-before relationship. Recently FASTTRACK reduced most \nvector clock-based analysis time from O(n) to O(1), where n is the number of threads. FASTTRACK exploits \nthe observation that some reads and all writes are totally ordered in race-free programs, but it still \nslows programs down by a factor of eight on average. LITERACE reduces overhead by sampling [27]. While \nLITERACE .nds many races handily, it uses heuristics that provide no guaran\u00adtees, incurs O(n) overhead \nat synchronization operations, and has high online space overhead. This paper presents a new approach \nfor detecting data races based on sampling called PACER. PACER makes a proportional\u00adity guarantee: it \ndetects each race with a probability equal to the sampling rate, and in practice it adds time and space \nproportional to the sampling rate. PACER builds on the FASTTRACK algorithm but reduces overhead through \nsampling. FASTTRACK .nds short\u00adest races. Two racy accesses A and B are a shortest race if there is no \nintervening access that races with B. PACER reports sampled, shortest races. Two racy accesses A and \nB are sampled if A occurs in a sampling period. B may occur any time later in any subse\u00adquent sampling \nor non-sampling period. The key insights we use to reduce overhead are as follows. (1) During non-sampling \nperiods, once PACER determines a sampled access cannot be part of a short\u00adest race, it discards the metadata \nto save time and space. (2) Dur\u00ading non-sampling periods, we observe that because PACER must only determine \nif the sampled accesses happen before the current time, it is not necessary to increment vector clocks. \nWithout incre\u00adments, vector clock values converge due to redundant synchroniza\u00adtion, which PACER exploits \nto reduce almost analysis from O(n) to O(1) time during non-sampling periods. The appendices prove PACER \ns completeness and statistical soundness.  PACER provides a qualitative improvement over most prior \nwork. Its scalable performance makes it suitable for all-the-time use in production. For sampling rates \nof 1 to 3%, PACER adds over\u00adheads between 52 and 86%, which we believe could be lower with additional \nimplementation effort and may already be low enough for many deployed settings. We show that PACER avoids \nnearly all O(n) operations during non-sampling periods. Both sampling and the observer effect complicate \nrace detection evaluation be\u00adcause they change the reported races. However, our evaluation (which currently \nevaluates only frequently occurring races due to experimental resource limitations) suggests that PACER \nachieves its theoretical guarantees, .nding each dynamic data race with a probability equal to the sampling \nrate. A given data race may occur extremely infrequently or never in testing and in some deployed environments, \nbut occur period\u00adically in other deployed environments. Widely deploying PACER gives coverage across \nmany environments, with reasonable odds of .nding any given race that occurs periodically. We note that \npo\u00adtentially harmful data races occur quite frequently without causing errors in the same execution. \nThus, sampling may be attractive even in safety-critical software, to identify data races before they \nlead to errors. In less critical software such as web browsers, PACER is at\u00adtractive for identifying \ndata races that frequently lead to errors; its reports can help developers detect, diagnose, and .x the \nroot cause of previously undiagnosed nondeterministic crashes. PACER is a get what you pay for approach \nthat provides scalable performance and scalable odds of .nding any race. PACER provides a qualitative \nimprovement over prior approaches because it is suitable for all-the-time use in deployed systems, where \nit can help developers eliminate rare, tough-to-reproduce errors. 2. Background, Motivation, and Requirements \nThis section describes dynamic race detection algorithms that pre\u00adcisely track the happens-before relationship \nusing vector clocks. It .rst reviews the happens-before relationship and a GENERIC O(n) (time and space) \nvector clock algorithm. We describe how the FASTTRACK algorithm replaces most O(n) analysis, where n \nis the number of threads, with O(1) analysis without losing accu\u00adracy. Section 2.3 motivates sampling \nto reduce overhead, but argues that prior heuristics are unsatisfactory because they may miss races, \nand have unscalable time and memory overheads. 2.1 Race Detection Using Vector Clocks The happens-before \nrelationship computes a partial order over dy\u00adnamic program statements [21]. Statement A happens before \nB HB (A --. B) if any of the following is true: A executes before B in the same thread.  A and B are \noperations on the same synchronization variable such that the semantics imply a happens-before edge (e.g., \nA releases a lock, and B subsequently acquires the same lock).  HBHB A - . C and C - . B. Happens before \nis transitive. HBHB - . B and B .Two statements A and B are concurrent if A .--. A, i.e., they are not \nordered by the happens-before relationship. A data race occurs when there are two concurrent accesses \nto a variable and at least one is a write. We follow prior work by considering only happens-before data \nraces because their absence guarantees sequential consistency. Accesses to synchronization objects are \nalways ordered and never race. Synchronization objects in Java are: threads, locks, and volatile variables. \n(We focus on threads and locks to simplify Algorithm 1 Acquire [GENERIC]: thread t acquires lock m Ct \n. Ct U Cm Algorithm 2 Release [GENERIC]: thread t releases lock m Cm . Ct Ct[t] . Ct[t]+1 Algorithm \n3 Fork [GENERIC]: thread t forks thread u Cu . Ct Cu[u] . Cu[u]+1 Ct[t] . Ct[t]+1 Algorithm 4 Thread \njoin [GENERIC]: thread t joins thread u Ct . Cu U Ct Cu[u] . Cu[u]+1 Algorithm 5 Read [GENERIC]: thread \nt reads variable f check Wf . Ct {Check race with prior writes} Rf [t] . Ct[t] Algorithm 6 Write [GENERIC]: \nthread t writes variable f check Wf . Ct {Check race with prior writes and reads} check Rf . Ct Wf [t] \n. Ct[t] this presentation. Appendix C explains the differences for volatile variables.) All other program \naccesses may race, if the program synchronization does not order them. Potentially racing accesses include \nobject .elds, static .elds, and array element accesses in Java. We follow the literature: all these accesses \nare on variables, and synchronization operations are on synchronization objects. Vector clock race detection \nalgorithms soundly and precisely track the happens-before relationship [21; 28]. These algorithms perform \ndynamic analysis on all synchronization, read, and write operations. They detect concurrent variable \naccesses, and if one is a write, they report a data race. Synchronization operations. The simplest vector \nclock race de\u00adtection algorithm stores a vector clock for each synchronization ob\u00adject, each variable \nread, and each variable write. A vector clock is indexed by thread identi.er: C[1..n]. For each synchronization \nob\u00adject o, the analysis maintains a vector clock Co that maps every thread t to a clock value c. Algorithms \n1, 2, 3, and 4 show GENERIC vector clock algo\u00adrithms at lock acquires and releases, and thread forks \nand joins. Following Flanagan and Freund [14], gray shading indicates that operations take O(n) time, \nwhere n is the number of threads. The vector clock join operator U takes two vector clocks and returns \nthe maximum of each element. For example, if thread t acquires lock m, GENERIC stores the join of t and \nm s vector clocks into t s vector clock by computing Ct . Ct U Cm, which updates each el\u00adement Ct[i] \nto max(Ct[i],Cm[i]). When a thread t releases a lock m, the analysis copies the contents of t s vector \nclock to m s vector clock. It then increments the t entry in t s vector clock. Variable reads and writes. \nGENERIC tracks, for each variable, the logical time at which every thread last read and wrote it: R[1..n] \nRead vector W [1..n] Write vector Algorithms 5 and 6 show GENERIC analysis for reads and writes. At reads, \nthe analysis checks that prior writes happen before the current thread s vector clock, and then updates \nthe read vector s component for the current thread. At writes, the analysis checks for races with prior \nreads and writes, and updates the write vector.  2.2 FASTTRACK FASTTRACK is a dynamically sound and \ncomplete race detection algorithm [14]. It is nearly an order of magnitude faster than prior techniques \nbecause instead of O(n) time and space analysis, it replaces all write and many read vector clocks with \na scalar and almost always performs O(1)-time analysis on them. FASTTRACK exploits the following insights. \n(1) In a race-free program, writes to a variable are totally ordered. (2) In a race-free program, upon \na write, all previous reads must happen before the write. (3) The analysis must distinguish between multiple \nconcurrent reads since they all potentially race with a subsequent write. For each variable, FASTTRACK \nreplaces the write vector clock with an epoch c@t, which records the thread t and its clock value c that \nlast wrote the variable. This optimization reduces nearly all analysis at reads and writes from O(n) \nto O(1) time and space. When reads are ordered by the happens-before relation, FASTTRACK uses an epoch \nfor the last read. Otherwise, it uses a vector clock for reads. The function epoch(t) is shorthand for \nc@t where c = Ct[t]. For clarity of exposition, we combine the read epoch and vector clock into a read \nmap. A read map R maps zero or more threads t to clock values c. A read map with one entry is an epoch. \nA read map with zero entries is the initial-state epoch 0@0. R Read map: t . c W Write epoch: c@t FASTTRACK \nuses the same analysis at synchronization operations as GENERIC (Algorithms 1, 2, 3, and 4). Algorithms \n7 and 8 show FASTTRACK s analysis at reads and writes. Ataread,if FASTTRACK discoversthatthereadmapisasingle\u00adentry \nepoch equal to the current thread s time, epoch(t), it does nothing. Otherwise, it checks whether the \nprior write races with the current read. Finally, it either replaces the read map with an epoch (if the \nread map is an epoch already, and it happens before the current read) or updates the read map s t entry. \nAt a write, if FASTTRACK discovers the variable s write epoch is the same as the thread s epoch, it does \nnothing. Otherwise, it checks whether the current write races with the prior write. Finally, it checks \nfor races with prior reads and clears the read map. The check takes O(|Rf |) time and thus O(n) at most, \nalthough it is amortized over the prior |Rf | analysis steps that take O(1) time each. When Rf is an \nepoch, the original FASTTRACK algorithm does not clear Rf . Clearing Rf is sound since the current write \nwill race with any future access that will also race with the discarded read. We modify FASTTRACK to \nclear Rf to correspond more directly with PACER, which clears read maps and write epochs to reduce space \nand time overheads during non-sampling periods. Discussion. FASTTRACK performs signi.cantly faster than \nprior vector clock-based race detection [14]. Notably, it performs about the same as imprecise lockset-based \nrace detection, but it still slows programs by 8X on average and adds 3X space overhead, which is too \ninef.cient for most deployed applications. (While the original FASTTRACK implementation executes in pure \nJava, we estimate that an ef.cient implementation inside a JVM would still slow pro\u00adgrams by 3-4X. Our \nPACER implementation at a 100% sampling rate, while implemented in a JVM and functionally equivalent \nto FASTTRACK, incurs a 12X slowdown primarily because it is op\u00adtimized for the non-sampling case at the \nexpense of the sampling case; Section 5.4.) FASTTRACK s analysis for nearly all read and write operations \ntakes O(1) time; however, its analysis for synchro-Algorithm 7 Read [FASTTRACK]: thread t reads variable \nf if Rf = epoch(t) then {If same epoch, no action}check Wf Ct if |Rf | =1 . Rf Ct then Rf . epoch(t) \n{Overwrite read map}else Rf [t] . Ct[t] {Update read map} end if end if Algorithm 8 Write [FASTTRACK]: \nthread t writes variable f if Wf = epoch(t) then {If same epoch, no action}check Wf Ct if |Rf |= 1 then \ncheck Rf Ct Rf . empty {New: clear read map here} else check Rf Ct {O(1) amortized time} Rf . empty end \nif Wf . epoch(t) {Update write epoch}end if nization operations takes O(n) time. Although synchronization \nop\u00aderations account for only about 3% of analyzed operations, as the number of threads increases, they \npresent a scalability bottleneck.  2.3 Sampling A potential strategy for reducing overhead is to sample \nrace detec\u00adtion analysis, i.e., execute only a fraction of the analysis. On .rst glance, sampling has \ntwo serious problems. First, sampling syn\u00adchronization operations will miss happens-before edges and \nthus will report false positive races. Second, because a race involves two accesses, sampling a proportion \nr of all reads and writes will report only r 2 of races (e.g., 0.09% for r = 3%). LITERACE solves some \nof these problems [27]. To avoid miss\u00ading happens-before edges, LITERACE fully instruments all syn\u00adchronization \noperations. It then samples read and write operations with a heuristic. It applies the cold-region hypothesis: \nbugs occur disproportionately in cold code [10]. LITERACE samples at a rate inversely proportional to \nexecution frequency, down to a minimum. LITERACE thus cannot make claims on proportionality, since with \nits minimum rate of 0.1%, a race in hot code will only be reported 0.1%2 = 0.0001%, i.e., one out of \na million times. The LITERACE implementation uses of.ine race detection by recording synchronization, \nread, and write operations to a log .le [27]. Of.ine analysis performs checks for races in the log when, \nfor example, an execution fails. Of.ine race detection is imprac\u00adtical in many cases, such as long-running \nprograms. An online implementation of LITERACE requires O(n) analysis for synchro\u00adnization operations. \nFurthermore, since it samples code, rather than data, space overhead is proportional to the data, not \nthe sample rate.  2.4 Requirements While recent work offers signi.cant advances in dynamic, pre\u00adcise \nrace detection, serious drawbacks limit its applicability: anal\u00adysis that requires O(n) time and space, \nand sampling heuristics that consistently miss some races. We believe the following re\u00adquirements are \nkey for deployable race detection. First, like the approaches just described, race detection needs to \nbe precise to avoid alienating developers with false positives. Second, the time and space impact must \nbe low enough to be acceptable for produc\u00adtion software, and must scale with the number of threads. Third, \nthe\n\t\t\t", "proc_id": "1806596", "abstract": "<p>Data races indicate serious concurrency bugs such as order, atomicity, and sequential consistency violations. Races are difficult to find and fix, often manifesting only after deployment. The frequency and unpredictability of these bugs will only increase as software adds parallelism to exploit multicore hardware. Unfortunately, sound and precise race detectors slow programs by factors of eight or more and do not scale to large numbers of threads.</p> <p>This paper presents a precise, low-overhead <i>sampling-based</i> data race detector called Pacer. PACER makes a <i>proportionality</i> guarantee: it detects any race at a rate equal to the sampling rate, by finding races whose first access occurs during a global sampling period. During sampling, PACER tracks all accesses using the dynamically sound and precise FastTrack algorithm. In nonsampling periods, Pacer discards sampled access information that cannot be part of a reported race, <i>and</i> Pacer simplifies tracking of the happens-before relationship, yielding near-constant, instead of linear, overheads. Experimental results confirm our theoretical guarantees. PACER reports races in proportion to the sampling rate. Its time and space overheads scale with the sampling rate, and sampling rates of 1-3% yield overheads low enough to consider in production software. The resulting system provides a \"get what you pay for\" approach that is suitable for identifying real, hard-to-reproduce races in deployed systems.</p>", "authors": [{"name": "Michael D. Bond", "author_profile_id": "81100148693", "affiliation": "University of Texas at Austin, Austin, TX, USA", "person_id": "P2184558", "email_address": "", "orcid_id": ""}, {"name": "Katherine E. Coons", "author_profile_id": "81381599031", "affiliation": "University of Texas at Austin, Austin, TX, USA", "person_id": "P2184559", "email_address": "", "orcid_id": ""}, {"name": "Kathryn S. McKinley", "author_profile_id": "81100402805", "affiliation": "University of Texas at Austin, Austin, TX, USA", "person_id": "P2184560", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1806596.1806626", "year": "2010", "article_id": "1806626", "conference": "PLDI", "title": "PACER: proportional detection of data races", "url": "http://dl.acm.org/citation.cfm?id=1806626"}