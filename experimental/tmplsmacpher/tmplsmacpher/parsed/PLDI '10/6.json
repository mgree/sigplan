{"article_publication_date": "06-05-2010", "fulltext": "\n Cache Topology Aware Computation Mapping for Multicores * Mahmut Kandemir ,TaylanYemliha , SaiPrashanth \nMuralidhara , Shekhar Srikantaiah , Mary Jane Irwin ,Yuanrui Zhang The Pennsylvania State University, \nUniversityPark,PA. {kandemir, smuralid, srikanta, mji, yuazhang}@cse.psu.edu Syracuse University, Syracuse, \nNY. {tyemliha}@syr.edu Abstract The main contribution of this paper is a compiler based, cache topology \naware code optimization scheme for emerging multicore systems. This scheme distributes the iterations \nof a loop to be executed in parallel across the cores of a target multicore machine and schedules the \niterations assigned to each core. Our goal is to improve the utilization of the on-chip multi-layer cache \nhierarchy and to maximizeoverall application performance.Weevaluate our cache topology aware approach \nusing a set of twelve applications and three different commercial multicore machines. In addition, to \nstudy some of our experimental parameters in detail and to explore future multicore machines (with higher \ncore counts and deeper on-chip cache hierarchies), we also conduct a simulation based study. The results \ncollected from our experiments with three Intel multicore machines show that the proposed compiler-based \napproach is very effective in enhancing performance. In addition, our simulation results indicate that \noptimizing for the on-chip cache hierarchy will be even more important in future multicores with increasing \nnumbers of cores and cache levels. Categories and Subject Descriptors B.3.2[Memory Structures]: Design \nStyles Cache memory; C.4[Performance of Systems]: Design Studies General Terms Management, Design, Performance, \nExperimen\u00adtation, Algorithms Keywords Cache, Multi-level, Multicore,Topology-aware, Com\u00adpiler 1. Introduction \nRunning into the power wall has forced processor designers to look to other than clock-frequency scaling \nto continue the scaling in processor performance that we have come to expect over the last few decades. \nSince Moore Law continues to provide a doubling in the number of transistors in every technology generation \n(every two to three years), one way to double the performance without * This research is supported in \npart by NSF grants CNS #0720645, CCF #0811687, OCI #0821527, CCF #0702519, and CNS #0720749, and a grant \nfrom Microsoft Corporation. Permission to make digital or hard copies of all or part of this work for \npersonal or classroomuseisgrantedwithout feeprovidedthat copies arenot madeordistributed forpro.torcommercialadvantage \nandthatcopiesbearthisnoticeandthefullcitation onthe .rstpage.To copy otherwise,to republish,topostonserversorto \nredistribute tolists, requirespriorspeci.cpermission and/ora fee. PLDI 10, June5 10,2010,Toronto,Ontario, \nCanada. Copyright c &#38;#169; 2010 ACM 978-1-4503-0019-3/10/06... $10.00 increasing the clock frequency \nis to double the number of proces\u00adsors (cores) on the chip. The .rst (non-embedded) dual-core ar\u00adchitecture \nwas delivered in 2001 and now quad-core architectures are becoming increasingly common. As the microprocessor \nworld experiences this transition from out-of-order execution and clock\u00adfrequencyscaling to scaling through \nthe placement of multiple pro\u00adcessor cores on one chip, programming these multicore machines and optimizing \nsoftware for them is emerging as a very challenging task. It is clear that, without proper application \nsoftware and system software support that can take advantage of multiple cores on the same chip, we will \nnot be able to extract the expected performance bene.ts from these machines. Unfortunately, developing \nreusable and portable software for emerging multicore architectures is not easy as the architectures \nof these machines (even those produced by the same chip manufacturer) can be very different from one \nan\u00adother. Consider for example three sample multicore architectures, shown in Figure 1, from Intel: Harpertown, \nNehalem, and Dun\u00adnington. Each of these architectures has two sockets; Harpertown and Nehalem have 8 \ncores and Dunnington has 12 cores. We see that these three architectures have very different on-chip \ncache hi\u00aderarchies. For example, while Harpertown has only two levels of on-chip caches (L1 and L2), \nthe other two machines have three levels of on-chip caches (L1, L2, and L3). Also, in Nehalem, each corehasaprivateL2, \nwhereasin DunningtoneachL2is sharedbya pair of cores.Considering thefact that the on-chip cache hierarchy \nplays a very important role in determining the overall performance of a multi-threaded application [8, \n11, 21, 32, 33, 38], one can expect that a given parallel code can have dramatically different behaviors \non these three machines. As a result, if a multi-threaded code written and optimized for one multicore \narchitecture is to be ported to another, either one must accept the resulting perfor\u00admance degradation \nor one must perform extensive code re-tuning and re-optimization by hand to customize the code for the \nnew tar\u00adget architecture. Clearly, neither of these options is very desirable from a programmer s viewpoint. \nIn this work, we explore a third option enlisting the com\u00adpiler s help in customizing a program for \na target multicore on-chip cache hierarchy.Traditionally, optimizing compilershave three lay\u00aders: front-end \n(which implements scanning, parsing and interme\u00addiate representation construction), middle-end (which \nmostly im\u00adplements architecture-independent optimizations such as code re\u00adstructuring and data layout \nreorganization), and back-end (which implements architecture-dependent optimizations such as instruc\u00adtion \nscheduling and register allocation).1 However, we believe that, with multicore architectures, the middle-end \nlayer of an optimizing 1We should mention that a few optimizing compilers that target uniproces\u00adsor systems \nconsider cache capacity during the optimizations in the middle layer.  (a) (b) (c) Figure 1. Multicore \ncache architectures: (a) Harpertown (b) Ne\u00adhalem (c) Dunnington. In each .gure, ovals denote processors \n(cores) and L1, L2 and L3 represent on-chip caches. Typically, L2 has higher data access latency than \nL1, and L3 has higher access latency than L2. Each of these architectures has two sockets (de\u00adnoted by \nrectangles), each holding half of the cores and half of the cache components. compiler should consider \nthe on-chip cache hierarchy as well; that is, the cache hierarchyof the target multicore architecture \nshould be exposed to the middle-end layer of the compiler. While we present a detailed experimental analysis \nlater in the paper, Figure 2 illus\u00adtrates the normalized parallel execution times for a multi-threaded \napplication (named galgel, which is a .uid dynamics code that im\u00adplements an analysis of oscillatory \ninstability) on our three Intel multicore machines, shown in Figure 1. The three bars in each group correspond, \nfrom left to right, to the code versions generated while targeting the on-chip hierarchies of Harpertown, \nNehalem and Dunnington, respectively.Forexample, the .rst barin the sec\u00adond group gives the execution \ntime of the Harpertown version of the code when run on Nehalem. The results for each group of bars are \nnormalized with respect to the best-performing version, and the Dunningtonversionisexecuted using8 threads(1 \nthread per core) when ported to the other machines. We observe from these results that, on each multicore \nmachine, the version that has been special\u00adized for that machine (by considering the on-chip cache hierarchy) \ngenerates the best result. Exploiting the on-chip cache hierarchy is critical for achieving the best \nperformance on a given multicore machine. As an example, if one were to run code optimized for Harpertown \non Nehalem, one would see a performance hit of 26%. In the remainder of this paper, we present a compiler \nbased, cache topology aware code optimization strategy that customizes the data access pattern of an \napplication for the on-chip cache hierarchy of a target multicore architecture. Our target application \ndomain is array/loop intensive programs. These programs cover at least two important classes of applica\u00adtions. \nFirst, many scienti.c and engineering applications operate on large data arrays using nested loops. Examples \ninclude appli\u00adcations for climate modeling, astrophysics, computational chem\u00adistry, bioinformatics, and \nnuclear structure exploration [20]. Sec\u00adond, many data-intensive embedded applications such as multime\u00addia \ncodes operate on large data arrays that represent signals us\u00ading structured loop nests [14]. The important \npoint is that these array/loop intensive programs (from both the scienti.c/engineering domain and the \nembedded world) can potentially take advantage of emerging multicore systems very well, as their parallelization \nhas been thoroughly studied and manyloop level code/data restruc\u00adturing techniques (e.g., loop reordering, \niteration space tiling, data Normalized Execution Time 1.4 1.2 1 0.8 0.6 0.4 0.2 0 Execution on Execution \non Execution on Harpertown Nehalem Dunnington  Figure 2. Normalized parallel execution times for a \nmulti-threaded application(galgel)on different Intel multicore architectures. layout optimizations) have \nbeen developed to maximize their par\u00adallelism.2 What is missing, however, is the data locality aspect, \ni.e., how one can maximize data locality for a given on-chip cache hi\u00aderarchy. Unfortunately, existing \ndata locality (cache performance) optimization techniques in the context of both single processor machines \nand discrete multi-processor machines do not speci.\u00adcally address shared on-chip caches that are now \nalmost ubiquitous across the spectrum of multicore machines. We make three main contributions in this \npaper:  We present a cache topology aware loop iteration distribution scheme for multicore systems. \nThis strategy uses polyhedral arith\u00admetic to distribute the iterations of a parallel loop across multiple \ncores such that the performance of the on-chip cache hierarchy is maximized.We describe our compiler \nalgorithm andgive anexam\u00adple to illustrate how it operates in practice.  We explain how this strategy \ncan be extended to handle data dependencies (i.e., if we want to execute in parallel the iterations of \na loop nest that have loop-carried dependencies) and to exploit intra-core data locality.  We implemented \nin a compiler and evaluated our proposed scheme using a set of twelve applications and three commercial \nmulticore machines. In addition, to study some of our experimental parameters in detail and to explore \nfuture multicores (with larger numbers of cores and deeper on-chip cache hierarchies), we also present \nresults from a simulation based study.  The results collected from our experiments with three Intel \nmul\u00adticore machines show that the proposed compiler-based approach is very effective in enhancing the \nperformance of on-chip cache hierarchies of multicores, and as a result, signi.cant overall perfor\u00admance \nimprovements are possible.Forexample, our loop distribu\u00adtion scheme generated 16%, 17% and 21% improvements \nin exe\u00adcution cycles, on average, over a state-of-the-art data locality op\u00adtimization strategy, when \ntargeting the Harpertown, Nehalem and Dunnington systems, respectively. Our results also indicate that \noptimizing for the on-chip cache hierarchy will be even more im\u00adportant in future multicores with increasing \nnumbers of cores and cache levels. The next section explains the problem of optimizing data lo\u00adcality \nin shared on-chip caches of multicores. The details of our proposed iteration distribution and scheduling \nschemes are pre\u00adsented in Section 3. Section 4 presents an evaluation of the pro\u00adposed schemes using \nboth real multicore machines and a simula\u00adtion framework. Section 5 discusses related work and the paper \nis concludedin Section6witha summaryof our major observations. 2The main parallelization strategy for \nthese applications is loop-level par\u00adallelism in which iterations of a given (parallel) loop are distributed \nacross processors.  Figure 3. Motivation for distributing loop iterations and reorganiz\u00adingloop iterations.A,BandCdenotedifferentdata \nblocks.(a)The case with iterations that do not share data. (b) The case with itera\u00adtions that share data. \n(c) Impact of local scheduling. In the original schedule (left), core 0 accesses .rst A and then B, and \ncore 1 ac\u00adcesses .rstB and then C. It is assumed thatA andB con.ict in the shared cache, and therefore, \naccess to B by core 0 will result in a cache miss. The revised schedule on the right .xes this problem. \n 2. The Problem of Enhancing the Performance of On-Chip Hierarchy In multicore machines, the cores on \nthe same chip can share an on-chip cache space. How they share this cache space can make a difference \nin runtime performance as cache sharing across different cores can be constructive or destructive [13]. \nIn the former case, concurrently scheduled threads share data through the cache and/or their access patterns \nare such that theydo not displace each other s data from the cache. In the latter case, however, accesses \nfrom concurrently scheduled threads displace each other s data. This occurs frequently when multiple \napplications are executed at the same time on the same multicore machine (as different applications do \nnot share memory-resident data), but it can also happen when data accesses from the threads that belong \nto the same application (and share data between them) con.ict in the shared cache [37]. We now focus \non distributing and scheduling the iterations of a parallel loop across multiple cores. We say that two \ncores have af.nityat cacheL if bothhave access to that cache.Forexample, in the Dunnington architecture \nin Figure 1(c), cores 0 and 1 have af.nity at the .rst (leftmost) L2 cache as both are connected to it \n(as well as the L3 cache they share). Consider two cores that have af.nity at cache L. The chances for \nexperiencing destructive interactions in the cache space are higher if these cores do not share data, \nas this will typically increase the number of distinct data elements that compete for the limited cache \nspace. Therefore, if two iterations (that belong to a parallel loop) do not share data, it is better \nto assign those iterations to cores that do not have af.nity at any cache (if possible). Figure 3(a) \nillustrates this scenario (A and B represent two different data blocks). The left .gure in (a) shows \nthe case when the iterations that accessAandB are assigned to two cores with no af.nity, whereas the \nright one illustrates a potential con.ict if the iterations that do not share data are assigned to cores \nwith cache af.nity. On the other hand, if two iterations do share data, it is better to assign them to \ncores that have af.nity at some cache (see the right part of Figure 3(b)). If we do not do so (see the \nleft part of Figure 3(b)), this leads to data replication across multiple on-chip caches at the same \nlayer, which in turn reduces the effective on-chip cache capacity, and ultimately hurts overall performance. \nThe two scenarios illustrated in Figures 3(a) and (b) provide motivation for distributing loop iterations \nacross available cores carefully by considering on-chip cache topology. Figure 3(c), on the other hand, \nprovides motivation for reorganizing the loop itera\u00adtions assigned to a core, i.e., scheduling them. \nThis reorganization (scheduling) has two goals: (i) improving the shared cache local\u00adity beyond what \ncan be achieved by loop distribution alone and (ii) improving L1 cache locality. In this particular case, \nif two data ac\u00adcesses from two cores con.ict in the shared cache (e.g., when the access sequence is B[core \n1], A[core 0], B[core 0] and C[core 1] andA andB con.ict in the shared cache space), the reuse ofBby \ncore0will not be converted into locality because the second access toBwill resultinamiss(seetheleftpartofFigure3(c)).One \nsolu\u00adtion is to change the order of data accesses for this core, as shown on the right part of the .gure. \nThat is, the goal should be to ensure that when two cores do share data, they access that data at simi\u00adlar \ntimes to increase the chances for exploiting the resulting reuse while the shared data is still in the \nshared cache space. In addition, for a given core, the successively scheduled computations should reuse \ndata as much as possible to improve private (L1) cache local\u00adity. To summarize, there are two complimentary \nproblems. The .rst is the distribution of loop iterations across cores, that is, the loop iteration-to-core \nassignment problem. This problem should be attacked considering the data sharing across loop iterations \nas well as the cache topology of the target multicore system. The cores with af.nity should be given \niterations that share data (to minimize con.icts in the cache), whereas the cores without af.nity should \nbe given iterations that do not share data (to minimize data replication within the on-chip cache space). \nIn this paper, our main goal is to address this assignment problem. Once, this problem is handled, one \ncan also be interested in a second problem, namely, that of scheduling iterations assigned to a core \nto further improve data locality. Clearly, in some cases, a compiler/user can decide to run the iterations \nof a loop with loop-carried dependencies in parallel and use proper synchronization to ensure correct \nsemantics. In this paper, we also discuss the necessary enhancements to our base approach to handle such \ncases that involve data dependencies (see Section 3.5.2). 3. Cache Hierarchy Aware Loop Iteration Distribution \nand Scheduling 3.1 Assumptions The input to our approach is a set of loop iterations that are to be distributedoveravailable \ncores for parallelexecution.For now, we restrict ourselves to fully-parallel loops, i.e., the loops in \nwhich there are no loop-carried dependencies across iterations. As a re\u00adsult, anydistribution of the \niterations of such a loop is legal (i.e., se\u00admantically correct). However, as will be demonstrated later, \ndiffer\u00adent distributions can have signi.cantly different data locality (on\u00adchip cache performance) behavior. \nThe reason why we .rst focus on fully-parallel loops is that, in practice, many commercial com\u00adpilers \nexecute only such loops in parallel. While there are certainly cases where bene.ts are possible when \nthe iterations of a loop with dependencies are distributed across available cores and the correct\u00adness \nis ensured through inter-core synchronization, such cases are rare in practice. In fact, in the parallel \nbenchmarks that we have, only 14% of the loops that are executed in parallel have some sort of dependencies \nacross iterations; the rest are fully parallel. Still, we do discuss in this paper how our base approach \ncan be enhanced to handle data dependencies. 3.2 Representing Data Elements, Loop Iterations, and Array \nAccesses In our compiler framework, we use sets to represent array elements, loop iterations, and mappings \nbetween them. Consider for example the code fragment in Figure 4, written in a C-like pseudo-language. \n int A[0..D1-1][0..D2-1] int B[0..m-1] ... ... for(i1=0;i1<Q1;i1++) for(j=2k;j<m-2k+1;j++) for(i2=2;i2<Q2+2;i2++) \nB[j] = B[j] + B[2k+j] ...A[i1+1][i2-1]... + B[j-2k] Figure 4. Example code frag-Figure 5. Example code \nfrag\u00adment. ment. The iteration space Kof this loop nest can be expressed as follows: K = {(i1,i2)|(0 \n= i1 = Q1 -1) .(2 = i2 = Q2 + 1)}, where .denotes the and operator.We use I.= (i1 i2)T to denote the \niteration vector for this loop nest. The set of values I.can assume, i.e., iteration space K, is the \nset of all combinations of the values of the loop indices. Similarly, the set D = {(d1,d2)|(0 = d1 = \nD1 -1) .(0 = d2 = D2 -1)} de.nes the data elements declared in the code fragment. Further, the array \nreference in the loop body can be expressed as: R = {(i1,i2). (d1,d2)|(i1,i2).K.(d1,d2).D. (d1 = i1 + \n1) .(d2 = i2 -1)}, where . represents a mapping from the iteration space to the data space. Note that, \nwhile the .rst line of this expression enforces the data and loop indices to be within the array and \nloop bounds, respectively, the second line captures the relationship between these indices. We use R(I.) \nto indicate the array element accessed by iteration I using reference R (the fragment in Figure 4 has \nonly one reference, which is A[i1 + 1][i2 -1]). These sets can be viewed as a polyhedral model in which \nthe ob\u00adjects of interest (loop iterations and data elements) are represented as integer valued points \nin various regions of different spaces and the mappings (such as R)are used to connect these spaces.Two \nof the widely used polyhedral tools are the Omega Library [23] and Polylib [3]. In this work, we use \nthe Omega Library to capture the iteration and data sets and the mappings between them as well as to \ngenerate output code. However, its choice is orthogonal to the main focus of this work, i.e., if desired, \nit can be replaced by another polyhedral tool.  3.3 Tagging Iterations We assume that data manipulated \nby an application are partitioned into equal-sized blocks, and use \u00df0, \u00df1, \u00df2, \u00b7\u00b7\u00b7 \u00dfn-1 to denote these \nblocks, assuming a total of n data blocks. We want to em\u00adphasize that (i) this partitioning is a logical \none, i.e., the data are not physically divided into blocks, (ii) blocks do not cross array boundaries, \ni.e., each array starts with a new block, (iii) blocks are numbered sequentially in some order (though \nthe exact ordering scheme used is not very important, one would expect two consecu\u00adtive blocks of an \narray to be assigned consecutive numbers, and the numberofthe .rst blockofthenext arrayis goingtobelargerby1 \nthan the last block of the current array), and (iv) all blocks collec\u00adtively cover all the data elements \naccessed by the loop nest being optimized. Similar to data arrays, iterations of a loop nest are partitioned \ninto groups. We use notation to indicate an iteration group .. . with tag .. This tag is determined based \non the data block access pattern exhibited by the iteration group. Speci.cally, if . is d0d1 \u00b7\u00b7\u00b7 dn-2dn-1, \nfor each j between 0 and n - 1, dj is 1 if all iterations I.. .. access a data element d.that belongs \nto \u00dfj ; otherwise, dj is set to 0. Therefore, the tag associated with an iteration group captures the \nset of data blocks accessed by the iterations in that group as well as those that are not accessed. For \n4, .1100 instance, assuming n = represents an iteration group that accesses the .rst two data blocks \nand does not access the last two data blocks. As another example, .1000 represents an iteration group \nthat accesses only the elements of the .rst data block. Note that .1100 and .1000 do not share anyiteration \nbetween themselves, as all the iterations in the former one access the second data block, whereas none \nof the iterations in the latter one accesses the second data block. Based on this discussion, we can \nsay that two different iteration groups do not share any iterations between them, and all iteration groups \ncollectively cover the entire iteration set of the loop nest being distributed, that is, [ . K = .. . \n 3.4 Going from Data Blocks to Iteration Groups We now discuss how we obtain iteration groups from data \nblocks and generate code that enumerates the iterations in an iteration group. This code generation capability \nis important because, once we distribute loop iterations across available processor cores and reorganize \n(schedule) them, we need to generate code for each 3 core. Let us focus, without loss of generality, \non .., where . = d0d1 \u00b7\u00b7\u00b7 di-1di di+1 \u00b7\u00b7\u00b7 dn-2dn-1 = 11 \u00b7\u00b7\u00b7 100 \u00b7\u00b7\u00b7 00, that is, the .rst ibits are1 \nand the rest are 0. Then, assuming that there are R references in the loop nest(R0,\u00b7\u00b7\u00b7 ,RR-1), we can \nwrite: . {.(. .= I|.q,0 = q = (i-1)[.r, 0 = r = R-1s. t. Rr I). \u00dfq ] and \u00ac.q ' ,i = q ' = (n -1)[.r, \n0 = r = R-1s. t. Rr (I.). \u00dfq ' ]}. The .rst line of this expression captures the iterations that access \nall data blocks \u00dfq where 0 = q = (i - 1), while the second line indicates that none of these iterations \naccess any data block \u00dfq ' where i = q ' = (n - 1). In our approach, the iteration distribu\u00adtion across \nthe cores is carried out at an iteration group granularity. That is, each core is assigned a number of \niteration groups. There\u00adfore, once we determine the set of iteration groups assigned to a core, we need \nto generate code that enumerates the iterations in those groups. Note that, for a given .., the Omega \nLibrary can be used for generating code for it. Speci.cally, the codegen(.)utility provided by the Omega \nLibrary helps us generate the code (typi\u00adcally in form of a set of nested loops) that enumerates the \nitera\u00adtions in .. (and we repeat this for all iteration groups based on the scheduling determined for \nthe core). 3.5 Algorithms In this section, we .rst describe our iteration distribution scheme for the \nfully parallel case, i.e., when the loops to be executed in parallel do not have any loop-carried dependencies. \nWe then discuss the case with data dependencies, and explain how data locality can be further improved \nthrough intra-core scheduling of iteration blocks. 3.5.1 Iteration Distribution Algorithm Our loop distribution \nalgorithm is shown Figure 6. This algorithm takes the set of iterations to be distributed and the cache \ntopology of the target multicore machine as the input, and produces itera\u00adtion groups/clusters to be \nscheduled on each core such that cache sharing among the iteration groups is maximized. In the initialization \nstep, the iterations of the loop nest are grouped into iteration groups based on the similarity of their \ntags. Recall that a tag of an iteration is a signature of the data blocks ac\u00adcessed by that iteration, \nand that a set of iterations are clustered into 3Note that, the iteration groups allocated to a core \nand their scheduling de.nes the thread that will be executed on that core. an iteration group if theyhave \nthe same tag. After this initialization, webuild a graph with these iteration groups being nodes. An \nedge between two nodes of this graph has a weight equal to the number of common 1 s between the tags \nof the two nodes. Therefore, the weight of an edge between any two nodes is an indicator of the de\u00adgree \nof data block sharing (sort of af.nity) between the iterations of the two nodes. We then cluster the \nnodes (iteration groups) hierarchically ac\u00adcording to the cache hierarchy tree. The cache hierarchy tree \nis a tree representation of the on-chip cache hierarchywith the last level cache as the root. By considering \nthe cache hierarchy tree in clus\u00adtering the iteration groups, our scheme customizes the clustering for \nthe cache topology of the target multicore architecture.We start at the root and cluster the iteration \ngroups, level by level, until we reach the leaf level.To emphasize, the graph mentioned above cap\u00adtures \nthe degree of data sharing between iteration groups, whereas the cache hierarchy tree represents the \ntarget cache topology. In clustering for a given level in the cache hierarchy, we con\u00adsider the bitwise \nsum of the tags of all the nodes in the cluster as the tag of the cluster. We compute the dot product \n(denoted using  in the algorithm) of the tags of two clusters as the qualitative measure of af.nity \nand use that as a measure in our clustering algo\u00adrithm. The dot product measures the degree of data block \nsharing between any two clusters and hence is used as a qualitative mea\u00adsure for clustering. The number \nof clusters formed at each level is the same as the number of child nodes in the cache hierarchy tree. \nIf the number of clusters formed is less than the number of child nodes, we go on splitting the clusters \n(i.e., form smaller clusters) until the number of clusters is equal to the number of child nodes. After \nthe clustering step, we are left with the required number of iteration group clusters for the current \nlevel of the cache hierarchy. Our goal in the next step is load balancing, i.e., balancing the sizes \nof the iteration group clusters. In this context, size of a cluster is the total number of iterations \npresent in the iteration group cluster the sumofthe sizesofallthe iteration groups presentinthe cluster.For \nthe load balancing step, we use a greedy approach to balance the number of iterations4 across the clusters. \nUsing a balance threshold (which is the maximum tolerable imbalance across the iteration counts of different \ncores), we compute an upper and a lower limit for the size of each cluster. Note that the balance threshold \nis a tunable parameter.Weevict iteration groups from the largest sized cluster to the smallest in each \nstep. While doing so, we ensure that the dot product of the tag of the evicted iteration group and that \nof the recipient cluster is maximized. Also, we only evict if, after the eviction, the donor cluster \nsize does not drop below the lower limit of the cluster size and the recipient cluster size does not \ngo above the upper limit. If no such eligible iteration group is found for eviction, we break an iteration \ngroup according to the balance threshold requirements andevict that iteration group.We repeat this step \nuntil all the iteration group clusters are within the limits of the balance threshold. Our algorithm \nrepeats the clustering and load balancing steps at each level of the cache hierarchy tree. After all \nthe levels of the cache hierarchytree are considered, the number of clusters we have is equal to the \nnumber of cores in the target multicore architecture. Note that, by performing the clustering and load \nbalancing at each level of the cache hierarchy, our approach considers the data block sharing at each \nlevel of the on-chip cache and optimizing the same at each cache level. Input : Loop Iteration Set, IS \n= {.0,.1,....m} IS isthesetofalliterationsintheloopnest Data Block Set, DS = {\u00df0,\u00df1,...\u00dfr-1} DS isthesetofall \nequalsizeddata elementblocks accessedby theloop nest Architecture Description, A = {T,N} T isthe cachehierarchy \ntreewith thelastlevel cache as the root node and N is the number of cores /*Offchipmemoryistreated asthe \nrootif there are more thanonelastlevel caches*/ BalanceT hreshold = Maximumtolerableimbalanceiniteration \ncounts Output : Iteration Group Set, CS = {c0,c1,...ck} ci = {.j,.l,....z}, N is the number of cores \n Algorithm Initialization : Initialize tags: Assign a tag .j = d0d1 ...dr-1 toiteration .i where, dk \n=1if .i accessesdatablock \u00dfk IterationGroup .. = {.k, such that .k has tag .} /*As there arer datablocks,there \nare 2r tags and consequently, 2r possibleiterationgroups*/ Size of theiterationgroup, .. , S(..)= |..| \nBuildGraph: Build agraph G = {V, E}, V = {..1,..2,.....2r } E = {(..i,..j)such that .(..i,..j)= number \nof 1 bits in .i ..j}  Hierarchical Iteration Distribution: HierLevel = root of the cachehierarchy tree, \nT NumClusters =degreeof nodes atlevel HierLevel Cluster Set, CS = {cs}, cs = {{c}.c . V } W hile HierLevel \nleaf level: = New cluster set, NCS = {} ForEach cluster csi . CS: CS = CS -csi T otaliterations = total \nnumber ofiterationsin csi Clustering: While(|csi|> NumClusters): For each clusterc csi, ap . c ap = \n{..a ,..b,.....c } ap = BitwiseSum(.a, .b,... .c) S(c ap)= |..a |+ |..b|+ ... + |..c | Select and merge \nc ap and c aq in to a new cluster c anew such that, ap aq(dotproduct)is maximized . c = cc anew ap . \naq If(|csi|< NumClusters): //ifcurrent number ofclusters < requirednumber atthislevel While(|csi| NumClusters) \n=: Select c aq . csi, such that S(c aq)is maximum Break c aq into two clusters  Load balancing: After \nclustering, csi = {c a1 ...c aNumClusters} /*Usegreedyapproachtobalance clustersizes*/ Totaliterations \nUpLimit =+ BalanceT hreshold NumClusters Totaliterations LowLimit = -BalanceT hreshold NumCusters While \n.c ap . csi,such that S(c ap)> UpLimit: Select c csi such that, aq . S(c aq)< LowLimit Evict some..a \nfrom cap to caq such that, LowLimit < S(c ap)< UpLimit LowLimit < S(c aq)< UpLimit and, .a aq is maximum \nIf no such node exists, split ..a such that S(c ap)and S(c aq)are within limits andevict asdescribedabove \nForEachc csi: ap . NCS = NCS + {{..a }...a . c ap} CS = NCS Hierlevel = Hierlevel +1, Update NumClusters \ntothedegreeof nodes at Hierlevel Afterh = log2N hierarchicallevels,CS = {c0,c1,...ck} where, N is the \nnumber ofcores  Return CS 4Although iteration count is not the ideal metric for load balancing, it \nperforms much better than not balancing at all, and is implementable within Figure 6. Iteration distribution \nalgorithm. a compiler.   Input : Iteration Group Set, CS. = {..a ,..b,.....c }.0 < i < ncores DG =(V, \nE)-Iterationgroupdependencegraph //for..a ,..b . V , ..b depends on..a ife(..a ,..b). E a = Shared cache \nreusefactor \u00df =Level1 cache reusefactor Output : Scheduled Iteration Group Set, SCS. = {..p,..m ,.....q}.i \n Algorithm : ForEachshared cache S atthe .rst shared cachelevel n =Number ofcores sharing the cache S \nj = First core under the shared cache S ACS = {CSj, CSj+1, . . . CSj+k} SCSi = {}, for all j<i<j + n \nsi =0, for all j<i<j + n //num ofiterationsinSCSi AllGrps = CSj .CSj+1 ... .CSj+n While.CSk, such thatCSk \n= \u00d8 PrevSched = SCSj . SCSj+1 ... .SCSj+n CurrRnd = {} UnSched = AllGrps - P revSched ForEach i from \nj to j + n If(CSi == \u00d8)continue; If(i == j and SCSi == \u00d8) SCSi = SCSi + ..a , CSi = CSi -..a , and, \nsi = si + S(..a ), such that, ..a . CSi, and, .a hastheleast numberof 1 bits CurrRnd = CurrRnd + ..a \nElse If(i>j and SCSi == \u00d8) SCSi = SCSi + ..a , CSi = CSi -..a , and, si = si + S(..a ), such that, ..a \n. CSi, ...k . CurrRnd, with e(..k,..a ). E and, a \u00d7 (.a .x)is maximum where, ..x =Last element added \nto SCSi-1 CurrRnd = CurrRnd + ..a Else If(i == j and SCSi = \u00d8) While si <sj+n SCSi = SCSi + ..a , CSi \n= CSi -..a , and, si = si + S(..a ), such that, ...k . UnSched, with e(..k,..a ). E and, ..a . CSi, and, \n\u00df \u00d7 (.a .y)is maximum where, ..y =Last element added to SCSi CurrRnd = CurrRnd + ..a Else While si <si-1 \nSCSi = SCSi + ..a , CSi = CSi -..a , and, si = si + S(..a ), such that, ..a . CSi, ...k . CurrRnd, with \ne(..k,..a ). E, ...k . UnSched, with e(..k,..a ). E and, a \u00d7 (.a .x)+ \u00df \u00d7 (.a .y)is maximum where, ..x \n=Last element added to SCSi-1 and, ..y =Last element added to SCSi CurrRnd = CurrRnd + ..a add barrier \nsynchronization Return SCSi.0 < i < ncores Figure 7. Dependence-aware local iteration scheduling algorithm. \nThis algorithm is invoked after the one in Figure 6.  3.5.2 Handling Data Dependencies In the cache \ntopology aware loop iteration distribution algorithm described in the previous section, we restricted \nourselves to fully\u00adparallel loops, i.e., loops in which there are no loop-carried depen\u00addencies across \niterations. In cases where distributing the loop iter\u00adations with dependencies could bring high bene.ts, \nour algorithm can be extended by distributing the loop iterations with dependen\u00adcies across available \ncores and ensuring correctness through inter\u00adcore synchronization. There are at least two ways of extending \nour approach to handle loops with dependencies. First, we can ensure that the clustering al- C0 C1 C2 \nC3 ..a ..p ..m ..x ..b ..n ..y   Figure 8. Scheduling order of iteration groups for a 4-core ma- \nFigure 9. Target multicore architecture. gorithm clusters all iteration groups with loop carried dependencies \ntogether in a single cluster. This can be achieved by associating an in.nite edge weight between iteration \ngroups that have dependen\u00adcies between them. This ensures correctness without the need for inter-core \nsynchronization. However, conservatively clustering all dependent iteration groups together may reduce \nthe bene.ts of par\u00adallelism in iteration group execution, and therefore, this approach may not be very \neffective when we have large number of depen\u00addencies. Alternatively, the clustering algorithm can treat \nloop car\u00adried dependencies between iteration groups as normal data block sharing. Therefore, in the presence \nof dependencies across loop it\u00aderations, the data sharing resulting from these dependencies is ac\u00adcounted \nfor by the edge weights used to quantify the sharing of data between the iteration groups containing \nthe respective itera\u00adtions (provided they are different). Therefore, we can use the same global cache \nhierarchy aware loop iteration distribution algorithm described above (given in Figure 6) to improve \ndata sharing. How\u00adever, to ensure correctness, the dependencies can be detected during the local reorganization \nstep (explained shortly) and corresponding inter-core synchronization directives can be inserted to enforce \nthe dependencies.We now elaborate on this second option. Note that so far we did not discuss how the \niteration groups assigned to a core by our iteration distribution algorithm are sched\u00aduled (i.e., their \norder of execution). The important point is that if there are no loop carried dependencies, any scheduling \nof itera\u00adtions groups is legal. However, if we have data dependencies, we need to determine a legal schedule \nthat respects all dependencies. In our extension, the cache hierarchy aware loop distribution algo\u00adrithm \nclusters the iteration groups as in the case of the dependence\u00adfree case. Once the iteration group clusters \nare computed, a local reorganization algorithm schedules the iteration groups assigned to cores such \nthat the iteration group dependencies are preserved. In order to do this, when the iteration groups are \nscheduled in time, proper synchronization constructs are inserted. Figure 7 describes this algorithm \nin detail. The dependence-aware local reorganization algorithm takes the iteration group set computed \nby the hierarchical clustering algo-rithm(CS)and the iteration group dependence graph(DG)as in\u00adputs. \nThe iteration group dependence graph(DG) represents de\u00adpendencies between the iteration groups. It contains \nan edge from  . j =0 to k-1 101010000000 . j =k to 2k-1 010101000000 . j =2k to 3k-1 001010100000 \n. j =3k to 4k-1 000101010000 . j =4k to 5k-1 000010101000 . j =5k to 6k-1 000001010100 . j =6k to \n7k-1 000000101010 . j =7k to 8k-1 000000010101   . . . . L2 . . . . L2  (b) After clustering \nat the .rst level of the (c) After clustering at the second level of (a) Initial graph and iteration \ngroups with their cache hierarchy. the cache hierarchy. tags. Figure 10. Example application of our \nscheme. node (iteration group), ..a to ..b, if at least an iteration in ..a depends on an iteration in \n..b. Note that an edge in DG can go from an iteration group assigned to one core to an iteration group \nassigned to another core. Note also that DG can potentially be a cyclic graph, since some iterations \npresent in ..a can depend on iterations in ..b, while other iterations present in ..a can be de\u00adpendent \non iterations in ..b. We remove all the cycles in the de\u00adpendence graph by merging the involved nodes \nand consequently convert the graph to an acyclic graph before proceeding. The algo\u00adrithm then computes \na schedule of iteration groups for each core with inserted synchronization constructs.We schedule the \niteration groups starting with the .rst core. For the .rst core, we pick any iteration group that belongs \nto the cluster assigned to the .rst core and does not depend on anyiteration group (i.e., it is scheduleable). \nWe then consider all the iteration groups assigned to the second core and pick the iteration group which \nis not dependent on the last scheduled group on core one.We repeat this process for the second core till \nthe total number of iterations assigned to the second core is at least as many as that assigned to the \n.rst core. When that hap\u00adpens, we move on to the next core and repeat the process. This way, we try to \nbalance the iteration counts across the cores. After one round of scheduling for all the cores, we insert \na barrier synchro\u00adnization construct. Note that, not balancing the iteration counts in the abovefashion \ncan hurt performance in presence of barrier syn\u00adchronization. We then start the second round of scheduling \nfrom the .rst core. In the second and later scheduling rounds, we also ensure that the scheduled iteration \ngroup does not depend on any iteration group yet to be scheduled. These scheduling rounds are repeated \nuntil all the iteration groups are scheduled.With this pro\u00adcess, not only are there no dependencies between \niteration groups scheduled in a round, but also iteration groups scheduled at any particular round can \nonly be dependent on the iteration groups of the previous rounds. The dependencies between iteration \ngroups are enforced by the inserted barrier synchronization construct.  3.5.3 Improving Cache Block \nReuse While the dependence-aware scheduling described in Section 3.5.2 schedulesthe iteration groupson \ncoresina loop-carried dependence\u00adaware manner, it does not consider the data block sharing at dif\u00adferent \ncache levels. The algorithm described in Figure 7 not only tries to perform dependence-aware scheduling, \nit also tries to im\u00adprove both local level one cache reuse and the .rst shared level cache reuse. This \nlocal reorganization can be used with both the dependence and dependence-free cases. The cache block \nreuse can be improved both at the local level one cache or at the .rst shared cache level. The local \nlevel one cache reuse can be improved by scheduling the iteration groups assigned to a core such that \nthe tags of the contiguously sched\u00aduled iteration groups have the least possible Hamming Distance between \nthem. On the other hand, shared level cache reuse can be improved by scheduling iteration groups with \nminimum Ham\u00adming Distance on the cores sharing the cache such that those it\u00aderation groups are executed \nsimultaneously on the cores, thereby improving cache performance. Our algorithm exploits data local\u00adity \nin two directions: horizontal and vertical. Maximizing the dot product with the last scheduled iteration \ngroup on the previous core improves data block reuse at the .rst level of shared cache (hori\u00adzontal). \nIn comparison, maximizing the dot product with the last scheduled iteration group on the same core improves \ndata block reuse on the local level one cache (vertical). We weigh these dot products with tunable parameters, \na and \u00df, so that the cache level at which the sharing needs to be improved can be customized by assigning \nsuitable values to aand \u00df. In Figure 8, in the third round of scheduling for core 1, we pick ..r because \nit maximizes the value a \u00d7 (.c .r )+ \u00df \u00d7 (.q .c). Therefore, to schedule ..r (circled) on core 1, we \nconsider its left and upper neighbors (dotted circles) as indicated by the arrows in Figure 8. In this \nway, our approach takes care of the data reuse in both horizontal and vertical directions. 3.5.4 Example \nConsider the example loop shown in the Figure 5. We consider a dependence-free case here for simplicity. \nThis loop body has four references with each iteration accessing three elements of the same array. For \nillustrative purposes, we assume a data block size of k elements. We also assume that the total number \nof data blocks is twelve, i.e., m/k = 12. The cache hierarchyof the target multicore architecture is \ndepicted in Figure 9. The iterations of the loop can be divided into eight iteration groups based on \nthe data blocks accessed. The iteration groups and the initial graph are shown in Figure 10(a). We now \ngo over our hierarchical loop iteration distribution scheme. The .rst step is to cluster the loop iteration \ngroups for the L2 cache (as L3 is shared by all cores and is considered the root of the cache hierarchy \ntree). The graph and the assignment after the .rst level of clustering and loop iteration distribution \nare shown in Figure 10(b). Next, the loop iteration distribution is applied to each of the two clusters \nformed in the previous step. After this second and .nal level of clustering and load balancing, the iteration \n     ( ) Core 0 .M2,.M Core 1 .M6,.M Core 2 .M1,.M3 Core 3 .M5,.M7 Figure 11. Final assignments \nand schedule. Figure 12. Architectures with complex on-chip cache hierarchies: (a) Arch-I and (b) Arch-II. \n  Harpertown Nehalem  Dunnington Number of Cores 8 cores (2 sockets) 8cores (2 sockets) 12cores (2 \nsockets)  Clock Frequency 3.2GHz 2.9GHz 2.4GHz L1 32KB,8-way,64-byteline,3 cyclelatency 32KB,8-way,64-byteline,4cyclelatency \n32KB,8-way,64-byteline,4 cyclelatency L2 6MB,24-way,64-byteline,15 cyclelatency 256KB,8-way,64-byteline,10cyclelatency \n3MB,12-way,64-byteline,10 cyclelatency L3 - 8MB,16-way,64-byteline,30-40cyclelatency 12MB,16-way,64-byteline,32-40cyclelatency \n Off-Chip Latency ~100 ns ~60ns ~50 ns  Table 1. Important parameters for our three multicore machines. \nclusters are assigned to the cores as shown in Figure 10(c). Finally, the iteration groups assigned to \neach core are scheduled using the local iteration group scheduling algorithm given in Figure 7. The .nal \niteration group assignments and the corresponding schedule for each core is depicted in Figure 11.  \n  4. Experimental Evaluation 4.1 Setup In our experimental evaluation, we used both commercial multi\u00adcore \nmachines and a simulation framework (for sensitivity experi\u00adments).Table1givesthe important characteristicsofthe \nthree Intel machines we used: Harpertown, Nehalem, and Dunnington (all il\u00adlustrated in Figure 1). The \nsetof applications usedin this studyisgiveninTable2.We experimented with two types of applications, namely, \nsequential and parallel. For the sequential benchmarks, we executed a paral\u00adlelism extraction phase before \nour scheme could be applied. Since the selection of the loop parallelization strategy used is orthogonal \nto our scheme (which performs iteration distribution across cores), here we only summarize it. This strategy, \nwhich is similar to the one discussed in Anderson s thesis [1], identi.es (for each loop nest) the outermost \nloop that does not have anyloop-carried dependence and parallelizes it. This tends to exploit coarse \ngrain parallelism and minimize inter-core synchronization. For the parallel bench\u00admarks on the other \nhand, our strategy is slightly different. Since the loops to execute parallel are already identi.ed, \nwhat we need to do is to apply our scheme to distribute (redistribute if necessary) itera\u00adtions to cores. \nOur parallel benchmarks are from three sources: ap\u00adplu, galgel and equake are from SpecOMP [16]; cg and \nsp are from NAS [4]; and bodytrack, facesim and freqmine are from the Par\u00adsec benchmark suite [7].Two \nof our sequential benchmarks, namd and povray, are from the Spec2006 suite [18]. The remaining two benchmarks, \nmesa and H.264, are two serial applications we main\u00adtain locally. The data set sizes of these applications \nvaried from 4.6MB to 2.8GB. Unless otherwise stated, the default data block size used in our experiments \nis 2KB. Our iteration distribution al\u00adgorithm operates with a given data block size. However, we use \na strategy to determine the block size (and consequently the number of blocks). This strategy tries to \nensure that the total size of data manipulated by even the most aggressive iteration group does not exceed \nL1 cache capacity. An iteration group becomes most aggressive when it has all 1 s in its tag.We .rst \npro.le the applica\u00adtion and determine the size of the total data it manipulates and then, assuming a \ntag size of k bits (unknown), we determine the amount of data accessed by the most aggressive iteration \ngroup. This value Table 2. Our applications. The last column gives the execution time when the original \napplication is executed on a single core of the Dunnington machine.  is in terms of k and we solve for \nk such that this value does not exceed L1 capacity. Please note that this sets an upper bound, and anylower \nvalue would be good as well. In this work, we compare our scheme against two base cases. The .rst of \nthese, called Base, is the original application code with\u00adout anymodi.cation (except for parallelization \nwhen the input ap\u00adplication is sequential). The second base case, referred to as Base+, represents the \nstate-of-the-art in data locality (cache performance) enhancement. It improves data locality, for each \ncore, by applying a set of well-known locality optimizations, which include loop per\u00admutation (changing \nthe order in which loop iterations are executed) and iteration space tiling (also known as blocking, \nwhich imple\u00adments a blocked version of the code to improve temporal reuse in outer loop positions). Please \nnote that Base+ contains a com\u00adprehensive set of well-established locality optimizations including linear \ntransformations and tiling. Infact, the linear transformations we used were very similar to those discussed \nin [43]. However, we found that non-unimodular transformations (such as scaling) did not bring additional \nbene.ts over unimodular ones in our bench\u00admarks.To approximate the ideal tile size (blockingfactor), \nweex\u00adperimented with different tile sizes and selected the one that per\u00adformed the best. The important \npoint to emphasize is that the set of iterations assigned to each core is the same in both Base and Base+; \nthe only difference is the order used to execute these iter\u00adations. Therefore, in a sense, Base+ can \nbe viewed as intra-core locality optimization, or an extension of single core locality opti\u00admization \nto the multicore case (i.e., we apply conventional locality optimization to each core separately). In \nthe rest of this section, we refer to our proposed loop iteration distribution strategy as Topol\u00adogyAware \n. Unlessexplicitly stated,TopologyAware does not in\u00adclude the loop scheduling step, which is part of \nthe algorithm in Section 3.5.3. Instead, once the iteration distribution is carried out, the iteration \ngroups assigned to each core are scheduled consider\u00ading only data dependencies. In order to have a better \nunderstanding of their behavior, we study loop distribution and loop scheduling in isolation as well \nas when theyare combined. Note that, in Base, Base+, and Topology Aware, the set of iterations executed \nin par\u00adallel is the same; the only difference is in the way in which these iterations are partitioned \nacross the cores and scheduled within a core.  In our experiments, the load balance threshold mentioned \nin Section 3.5.1 is set to 10%, and the aand \u00dfparameters discussed in Section 3.5.3 are both set to 0.5 \n(i.e., equal weights). When we use Intel machines, the results are obtained using all available cores \n(8 in the case of Harpertown and Nehalem and 12 in the case of Dun\u00adnington). All the versions used in \nthis work have been implemented using Microsoft s Phoenix infrastructure [31]. Speci.cally, after the \ninput codeis analyzedby Phoenix, webuild the polyhedral frame\u00adwork and pass it to the Omega Library [23]. \nThe code returned from the Omega Library enumerates the iterations in the iteration groups assigned to \nthe cores and is passed back to the Phoenix in\u00adtermediate format. In our experiments with the Intel machines, \nwe used the Intel compiler (with the most powerful optimization .ag available) as our back-end compiler. \nIn our simulations on the other hand, the back-end compiler we used is gcc (again, with the highest optimization \n.ag). Before moving to the discussion of our experimental analysis, we want to mention that the increase \nin compilation times due to our scheme varied between 65% and 94% (depending on the appli\u00adcation being \ncompiled) over the compilation that includes a paral\u00adlelization step (but does not include anydata locality \noptimization). We also observed that the our approach did not have a signi.cant impact on the instruction \ncache miss rates (less than 1% increase in all the application codes we tested). Figure 15. In.uence \nof local iteration reorganization (scheduling). 4.2 Results Goals. We have four main goals in our experimental \nevaluation. First, we want to demonstrate that, for a given multicore archi\u00adtecture, our approach generates \nbetter results than both Base and Base+. Second, we want to show that a version customized for a speci.c \nmulticore architecture may not perform well when ex\u00adecuted on another multicore architecture. Third, \nwe want to see whether considering the entire cache hierarchyis a must for achiev\u00ading maximum savings. \nFourth, we want to illustrate that our ap\u00adproach is expected to perform well in future multicore systems \nwith larger numbers of cores and deeper on-chip cache hierarchies. All the results presented in this \nsubsection are normalized with re\u00adspect to Base. Results on Commercial Machines. Figure 13 gives the \nexecution cycles for the benchmarks in our three multicore machines, nor\u00admalized with respect to Base \n(remember that all different versions use the same back-end compiler). Our main observation from these \nresults is that, for all three multicores and all application programs, our approach generates better \nresults than both Base andBase+, in\u00addicating the importance of cache topology aware loop iteration as\u00adsignment.We \nalso observe that the difference between our scheme andBase+is higherin Dunnington. Thisis because Dunnington \nhas a more complex on-chip cache topology, which makes optimizing for data locality even more important. \nThe average performance improvements our topology aware approach brings over Base and Base+ are about \n28% and 16%, respectively, in the case of Harper\u00adtown. The corresponding savings are 29% and 17% for \nNehalem and 30% and 21% for Dunnington. Since Topology Aware, Base, andBase+haveexactlythe samesetofloop \niterations executedin parallel and only differ in how these iterations are distributed and scheduled, \nthis difference across execution times is due to entirely on-chip cache behavior. For example, we observed \nthat, in Dun\u00adnington, our approach reduced the L1, L2 and L3 cache misses on average by 18%, 39%, 47%, \nrespectively, over the Base version. The corresponding cache miss reductions over Base+ were 16%, 31% \nand 37%. Cross-Machine Results. Our next set of results quantify the per\u00adformance of a multi-threaded \ncode generated for a speci.c multi\u00adcore when run on another multicore, and are presented in Figure 14. \nThe .rst group of bars in this graph corresponds to the execution of the Nehalem version on Harpertown, \nand the second group repre\u00adsents the execution of the Dunnington version on Harpertown. The remaining \ngroups are interpreted similarly. These results clearly underline the importance of customizing loop \niteration distribution to the speci.c on-chip cache topology. The important point to note here is that, \nin the Harpertown machine, using the multi-threaded versions generated for Nehalem and Dunnington results, \non av\u00aderage, in about 17% and 31% worse performance, respectively, than the version customized for Harpertown. \nSimilarly, using the Harpertown and Nehalem versions in Dunnington leads to 24% and 21% average degradation, \nand using the Harpertown and Dun\u00adnington versions in Nehalem results in 25% and 19% performance degradation, \nrespectively, on average. These results combined with those in Figure 13 help us to conclude that, if \none is to map an application to a target multicore architecture, it is not a good idea to use a conventional \ndata locality optimization strategy or to sim\u00adply use the code optimized originally with a different \nmulticore in mind. Instead, the best results are achieved by customizing loop iteration distribution \nconsidering the underlying cache topology.  Impact of Intra-Core Scheduling. Recall that Section 3.5.3 \npre\u00adsented a local iteration reorganization (scheduling) strategy, which can be applied after the global \niteration distribution scheme. Fig\u00adure 15 plots the normalized results, for Dunnington, that summarize \nthe in.uence of this algorithm in improving overall performance. In this bar-chart, for each application, \nwe have three bars: global loop distribution alone (Topology Aware), local iteration reorgani\u00adzation \nalone (Local), and combined (when the local reorganization is applied after the global distribution scheme). \nNote that in Lo\u00adcal the iteration distribution across cores is either the default one indicated by the \noriginal parallel code or random (in the case of sequential codes parallelizedby our loop parallelization \nstep).Two trends can be observed from this plot. First, Local generates similar (slight better) results \nthan Base+ (see Figure 13 for the Base+ re\u00adsults). This is not very surprising as both Local and Base+ \nimprove the locality behavior from an individual core s perspective. While Base+ tries to do that using \nconventional restructuring techniques, Local takes a more data centric approach and attempts to cluster \niterations with similar data block access patterns. In addition, as ex\u00adplained earlier, Local also considers \naf.nity with other cores when performing scheduling for a given core. Second, the best results are achieved \nwhen both these schemes are used together. Speci.\u00adcally, this combined scheme obtains an average performance \nim\u00adprovement of around 37%. Although not presented here, we also performed experiments with different \nvalues for a and \u00df (recall Figure 18. Impact of the on-chip cache hierarchy (Default repre\u00adsents the \ncon.guration in the commercial Dunnington machine). that the defaultvalue used sofaris 0.5 for both).We \nobserved that giving them equal values generated the best results. Speci.cally, if \u00df is too big, the \npotential locality in the shared caches is missed, and if ais too big, L1 locality starts to suffer. \nSensitivity Experiments. In the rest of our experiments, we con\u00adducta sensitivity study withTopologyAware.In \ntheseexperiments, our starting point is the Dunnington architecture (see Figure 1(c)). We .rst present \nthe sensitivity of our savings to the data block size. Recall that the default block size used in our \nexperiments was 2KB. One can observe from Figure 16 that, as expected, smaller data block sizes are better \nas theylead to smaller iteration groups which in turn result in a .ner granular clustering by our algorithm \n(Fig\u00adure 6). While this result motivates small block sizes, a smaller block size increases overall compilation \ntime.Forexample, we observed that, as we move from 2KB to 256 bytes, the compilation time in\u00adcreased \nby more than 80%. For our experiments described below, we used Simics [28], a simulator that can simulate \nmultiprocessor architectures. We also used GEMS [29], which is a set of modules for Simics that enables \ndetailed (cycle accurate) simulation of different types of multipro\u00adcessor systems, including multicores. \nIn Figure 17, we give the nor\u00admalized results obtained by Topology Aware and Base+ when the number of \ncores is increased. Recall that, in Dunnington, the total number of cores is 12.5 We see from these results \nthat the effective\u00adness of our approach increases as we increase the number of cores (for each experiment, \nwe added a six more cores to the architec\u00adture shown in Figure 1(c)). For example, the average performance \nimprovement our approach brings over Base jumps from 29% to 46% as we move from 12 cores (in our default \ncon.guration) to 24 cores. The main reason for this is that a higher number of cores typ\u00adically lead \nto more sparse data access patterns from the perspective of a single core, and this in turn causes the \nperformance of Base to suffer. Since the results in Figure 17 are normalized with respect to the Base \nscheme, we see an improvement. These results are im\u00adportant mainly because future multicore machines \nare expected to accommodate large core counts [9]. Figure 18 illustrates the results from our simulations \nwith deeper on-chip cache topologies shown in Figure 12. It needs to be emphasized that Arch-I is more \ncomplex than our default Dun\u00adnington architecture (see Figure 1(c)) and Arch-II is more com\u00adplex than \nArch-I. One can observe from Figure 18 that Topology Aware performs better as we have deeper cache hierarchies \n(the best improvements are obtained with Arch-II). Again, this result is very important, because (considering \ncurrent trends) one can ex\u00ad 5It needs to be noted that the 8 core results are slightly different from \nthose presented for Dunnington earlier. This difference is because while the former results are obtained \non real machines, the latter results are obtained using simulation.  pect future multicores to have \nincreasingly deeper on-chip cache hierarchies [9]. Our next set of experiments were designed to explore \nwhat hap\u00adpens when the values of the parameter total data set size/cumulative on-chip cache space is \nincreased. Since increasing data set sizes of our applications is not trivial (as it requires understanding \nthe details of the underlying algorithms implemented by the applica\u00adtion), we instead changed the on-chip \ncache capacities. Speci.cally, maintaining the original Dunnington topology, we cut the capacity of each \ncache component (L1, L2 and L3) by half. We see from Figure 19 that our scheme performs better with smaller \ncache ca\u00adpacities. More speci.cally, the average performance improvements broughtby Base+ andTopologyAware \nare approximately 21% and 33%, respectively. These savings jumped to 29% and 41%, respec\u00adtively, when \nloop distribution was combined with loop scheduling (detailed results are omitted due to space concerns). \nResults with Different Versions and Comparison with Optimal Savings. Focusing now on the architecture \nin Figure 12(a), we try to answer the question of whether it is really important to consider the entire \ncache hierarchy. In Figure 20, L1+L2 and L1+L2+L3 re\u00adfertotwodifferentversionsof our scheme(TopologyAware) \nwhere only L1 and L2, and only L1, L2 and L3 are considered, respec\u00adtively. We see that, on average, \nconsidering all levels in the hier\u00adarchy (L1+L2+L3+L4) bring 21.8% and 12.7% improvement over the L1+L2 \nand L1+L2+L3 versions, respectively. In other words, for the best results, it is necessary to consider \nthe entire cache hi\u00aderarchy in distributing loop iterations among cores. Figure 20 also illustrates the \nresults from an optimal scheme. To obtain these re\u00adsults, we determined the ideal iteration group-to-core \nmapping (for each parallel loop nest) using integer linear programming (which took up to 23 hours in \nsome cases). We see that the performance difference between the mapping generated by our scheme and the \noptimal mapping is around 7.6%.  5. Related Work There have been several architectural level schemes \nto optimize shared cache management [44] [10] [5] [19]. Zhang et al propose a scheme that employs a small \nvictim cache to improve the per\u00adformance of the shared level 2 cache in multicores [44]. Chang and Sohi \npropose to combine the advantages of both shared and private caches using a uni.ed, cooperative caching \nscheme [10]. Beckmann et al propose that, by monitoring the workload behavior and using controlled selective \ncache block replication, performance can be signi.cantly improved [5]. Hsu et al discuss various policies \nthat can be employed in multicore shared cache management [19]. There have also been schemes that partition \nshared caches of mul\u00adticores in order to improve the overall throughput in most cases and fairness, QoS \nin a few cases. These schemes include dynamic cache way partitioning [38] andfairnessaware partitioning \n[25]. Liu et al discuss different organizations for the last level of cache [27].Younetal also recognizethe \nimportanceofa combined approach [40]. Their scheme employs a private L2 cache architec\u00adture, while emulating \na shared L2 cache through evictions into peer L2 private caches. Speight et al present simple architectural \nexten\u00adsions for effective management of L2 and L3 caches in multicores [36]. Kim et al state that large \ncache designs will be limited by growing wire delays and propose a nonuniform cache architecture to placate \nthe effects of growing wire delays [24]. Beckmann and Wood propose a cache management scheme that incorporates \nvari\u00adous latencymanagement techniques [6]. Theyalso show that block migration is less effective in commercial \nworkloads than expected. Prior parallelization, scheduling, and mapping strategies out\u00adside the multicore \ndomain include [15, 17, 26, 35]. There have also been compiler based schemes aimed at cache management \nfor multicores. Sarkar andTullsen proposea data-cache aware compi\u00adlation to .nd a layout for data objects \nwhich minimizes inter-object con.ict misses [34]. Anderson et al propose using sharing rules to avoid \ndata races [2]. Chen et al use a compiler directed approach to increase the idle periods of communication \nchannels by reusing the same set of channels for as manycommunication messages as pos\u00adsible, there by \nreducing the power requirement in network-on-chip based multicores [12]. Kandemir et al [22] discuss \na multicore map\u00adping strategy that does not customize mapping based on target on\u00adchip cache hierarchy. \nZhang et al [45] evaluate the impact of cache sharing on modern parallel workloads. Zhang et al study \nreference af.nity and further present a heuristic model for data locality [41]. Markatos et al propose \na loop iteration distribution scheme that bal\u00adances the workload, minimizes synchronization overhead \nand also co-locates the iterations with the data they access [42]. Li et al pro\u00adpose to not only balance \nthe workload but also take data locality in to account [43]. In contrast to the above two schemes ([42] \nand [43]), our proposed scheme not only balances theworkloadbut also takes the cache hierarchy of the \nunderlying multicore architecture into account while deciding the iteration scheduling. Although not \npresented here due to space concerns, our initial experience with dynamic scheduling schemes like [42] \ndid not generate good re\u00adsults on the Harpertown and Dunnington machines, mostly due to the cost of dynamic \niteration distribution. Our approach differs from the above efforts in that it tries to improve the performance \nof shared on-chip caches using loop dis\u00adtribution and loop scheduling. In doing so, it takes the target \ncache topology as input. It is also complementary to many of these prior studies. For example, in a setting \nwhere multiple multi-threaded applications exercise the same multicore machine, an OS based scheme can \npartition shared caches across different applications, and our scheme can optimize the performance of \neach application individually.  6. Concluding Remarks In order to take advantage of emerging multicore \narchitectures, we need compilers that can optimize a given application for the tar\u00adget architecture. \nUnfortunately, current multicore architectures are very different from each other and we expect this \ndissimilarity to grow wider in future generations of multicores. This makes it very dif.cult to develop \narchitecture agnostic high level code and data optimizations. The two main contributions of this paper \nare an iter\u00adation distribution algorithm and a loop scheduling algorithm target\u00ading at improving the \nperformance of on-chip cache hierarchies of emerging multicore systems.We implemented our approach using \na compiler infrastructure, and performed experiments on real archi\u00adtectures, and also conducted a simulation \nbased study. Our results are encouraging and show that considering on-chip cache topology makes signi.cant \ndifference in performance. Our results also show that the code optimized using our proposed scheme remains \nwithin 8% of the schedule generated by an optimal strategy.  References [1] J. M. Anderson. Automatic \nComputation and Data Decomposition for Multiprocessors. Ph.D Thesis, Stanford University, March 1997. \n[2] Z. R. Anderson et al. Lightweight annotations for controlling sharing in concurrent data structures. \nSIGPLAN Not., 44(6):98 109, 2009. [3] R. Bagnara et al. The PARMA polyhedra library: Toward a com\u00adplete \nset of numerical abstractions for the analysis and veri.cation of hardware and software systems. Sci. \nComput. Program., 72(1-2):3 21, 2008. [4] D. Bailey et al. TheNASParallel Benchmarks 2.0,NASA. Technical \nReport, 1995. [5] B. M. Beckmann et al. ASR: Adaptive selective replication for CMP caches. In Proc. \nMICRO, 2006. [6] B.M. Beckmann andD.A.Wood. Managing wire delayin largechip\u00admultiprocessor caches. In \nProc. MICRO, 2004. [7] C. Bienia et al. The PARSEC benchmark suite: characterization and architectural \nimplications. In Proc. PACT, 2008. [8] R. Bitirgen et al. Coordinated management of multiple interacting \nresources in chip multiprocessors: A machine learning approach. In Proc. MICRO, 2008. [9] S. Borkar et \nal. Platform 2015: Intel processor and platform evolution for the next decade. Technical Report, Intel \nCorporation, 2005. [10] J. Chang and G. S. Sohi. Cooperative caching for chip multiprocessors. In Proc. \nISCA, 2006. [11] J. Chang and G. S. Sohi. Cooperative cache partitioning for chip multiprocessors. In \nProc. ICS, 2007. [12] G. Chen et al. Compiler-directed channel allocation for saving power in on-chip \nnetworks. In In Proc. POPL, 2006. [13] S. Chen et al. Scheduling threads for constructive cache sharing \non CMPs. In Proc. SPAA, 2007. [14] F. Catthoor et al. Data Access and Storage Management for Embed\u00added \nProgrammable Processors. Kluwer Academic Publishers, Boston, 2002. [15] A. Darte et al. Scheduling the \nComputations of a loop nestwith respect to a given mapping. In Proc. Europar, 2000. [16] SPEC OMP V3.2. \nhttp://www.spec.org/omp/ [17] P. Feautrier. Scalable and structured scheduling. Int. J. Parallel Pro\u00adgram. \n34, 5, 2006. [18] J. L. Henning. SPEC CPU2006 benchmark descriptions. SIGARCH Comput. Archit. News, 34(4):1 \n17, 2006. [19] L. R. Hsu et al. Communist, utilitarian, and capitalist cache policies on CMPs: caches \nas a shared resource. In Proc. PACT, 2006. [20] INCITE Leadership Computing. Technical Report, http://www.er.doe.gov/ \nascr/incite/. [21] R. Iyer et al. QoS policies and architecture for cache/memory in CMP platforms. SIGMETRICS \nPerform. Eval. Rev., 35(1):25 36, 2007. [22] M. Kandemir et al. Optimizing shared cache behavior of chip \nmulti\u00adprocessors. In Proc. MICRO, 2009. [23] W.Kelly et al. The Omega Library interface guide. Technical \nReport, University of Maryland, 1995. [24]C.Kimetal.Anadaptive, non-uniform cachestructurefor wire-delay \ndominated on-chip caches. SIGPLAN Not., 37(10):211 222, 2002. [25] S. Kimet al.Fair cachesharing and \npartitioningina chipmultiproces\u00adsor architecture. In Proc. PACT, 2004. [26] A. Legrand et al. Mapping \nand Load-Balancing Iterative Computa\u00adtions. IEEE TPDS, 2004. [27] C. Liu et al. Organizing the last line \nof defense before hitting the memory wall for CMPs. In Proc. HPCA, 2004. [28]P.S.Magnussonetal. Simics:Afullsystemsimulationplatform. \nIEEE Computer, 35(2):50 58, 2002. [29] M. M. K. Martin et al. Multifacet s general execution-driven multipro\u00adcessor \nsimulator (GEMS) toolset. SIGARCH Comput. Archit. News, 33(4):92 99, 2005. [30] The OPENMP API speci.cation \nfor parallel programming. http://openmp.org/wp/ [31] Phoenix Compiler Infrastructure. Technical Report, \nMicrosoft. https://connect.microsoft.com/Phoenix. [32] M. K. Qureshi andY. N.Patt. Utility-based cache \npartitioning:Alow\u00adoverhead, high-performance, runtime mechanism to partition shared caches. In Proc. \nMICRO, 2006. [33] N. Ra.que et al. Architectural support for operating system-driven CMP cache management. \nIn Proc. PACT, 2006. [34] S. Sarkar and D. M. Tullsen. Compiler techniques for reducing data cache miss \nrate on a multithreaded architecture. In Proc. HiPEAC, 2008. [35] S.K. Singhai and K.S. McKinley. A Parameterized \nLoop Fusion Al\u00adgorithm for ImprovingParallelism and Cache Locality. The Computer Journal, vol. 40, no. \n6, 1997. [36] E. Speight et al. Adaptive mechanisms and policies for managing cache hierarchies in chip \nmultiprocessors. SIGARCH Comput. Archit. News, 33(2):346 356, 2005. [37] S. Srikantaiah et al. Adaptive \nset pinning: managing shared caches in chip multiprocessors. In Proc. ASPLOS, 2008. [38] G. E. Suh et \nal. Dynamic partitioning of shared cache memory. Journal of Supercomputing, 28(1):7 26, 2004. [39]P.Vianaetal. \nCon.gurable cachesubsettingforfastcache tuning.In Proc. DAC, 2006. [40] S.Younetal.A reusability-aware \ncache memorysharingtechniquefor high-performance low-power CMPs with private l2 caches. In Proc. ISLPED, \n2007. [41] C. Zhang et al.A hierarchical model of data locality. In Proc. POPL, 2006. [42] E. P. Markatos \nand T. J. LeBlanc. Using Processor Af.nity in Loop Scheduling on Shared-Memory Multiprocessors. In Proc. \nIPDPS, 1994. [43] H. Li et al. Locality and Loop Scheduling on NUMA Multiprocessors. In Proc. ICPP, 1993. \n[44] M. Zhang and K. Asanovic. Victim replication: Maximizing capacity while hiding wire delay in tiled \nchip multiprocessors. In Proc. ISCA, 2005. [45] E. Zhang et al. Does cache sharing on modern CMP matter \nto the performance of contemporary multithreaded programs?. In In Proc. PPOPP, 2010.  \n\t\t\t", "proc_id": "1806596", "abstract": "<p>The main contribution of this paper is a compiler based, cache topology aware code optimization scheme for emerging multicore systems. This scheme distributes the iterations of a loop to be executed in parallel across the cores of a target multicore machine and schedules the iterations assigned to each core. Our goal is to improve the utilization of the on-chip multi-layer cache hierarchy and to maximize overall application performance. We evaluate our cache topology aware approach using a set of twelve applications and three different commercial multicore machines. In addition, to study some of our experimental parameters in detail and to explore future multicore machines (with higher core counts and deeper on-chip cache hierarchies), we also conduct a simulation based study. The results collected from our experiments with three Intel multicore machines show that the proposed compiler-based approach is very effective in enhancing performance. In addition, our simulation results indicate that optimizing for the on-chip cache hierarchy will be even more important in future multicores with increasing numbers of cores and cache levels.</p>", "authors": [{"name": "Mahmut Kandemir", "author_profile_id": "81100186744", "affiliation": "The Pennsylvania State University, University Park, PA, USA", "person_id": "P2184506", "email_address": "", "orcid_id": ""}, {"name": "Taylan Yemliha", "author_profile_id": "81317502793", "affiliation": "Syracuse University, Syracuse, NY, USA", "person_id": "P2184507", "email_address": "", "orcid_id": ""}, {"name": "SaiPrashanth Muralidhara", "author_profile_id": "81381605570", "affiliation": "The Pennsylvania State University, University Park, PA, USA", "person_id": "P2184508", "email_address": "", "orcid_id": ""}, {"name": "Shekhar Srikantaiah", "author_profile_id": "81350585673", "affiliation": "The Pennsylvania State University, University Park, PA, USA", "person_id": "P2184509", "email_address": "", "orcid_id": ""}, {"name": "Mary Jane Irwin", "author_profile_id": "81350580816", "affiliation": "The Pennsylvania State University, University Park, PA, USA", "person_id": "P2184510", "email_address": "", "orcid_id": ""}, {"name": "Yuanrui Zhnag", "author_profile_id": "81464673040", "affiliation": "The Pennsylvania State University, University Park, PA, USA", "person_id": "P2184511", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1806596.1806605", "year": "2010", "article_id": "1806605", "conference": "PLDI", "title": "Cache topology aware computation mapping for multicores", "url": "http://dl.acm.org/citation.cfm?id=1806605"}