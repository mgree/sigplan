{"article_publication_date": "06-05-2010", "fulltext": "\n Bamboo:A Data-Centric, Object-Oriented Approach to Many-core Software Jin Zhou BrianDemsky Department \nof Electrical Engineering and Computer Science University of California, Irvine Irvine, CA 92697 {jzhou1,bdemsky}@uci.edu \nAbstract Traditional data-oriented programming languages such as data.ow languages and stream languages \nprovide a natural abstraction for parallel programming. In these languages, a developer focuses on the \n.ow of data through the computation and these systems free the developer from the complexities of low-level, \nthread-oriented concurrencyprimitives. This simpli.cation comes at a cost tra\u00additional data-oriented \napproaches restrict the mutation of state and, in practice, the types of data structures a program can \neffectively use. Bamboo borrowsfromworkin typestateand software transac\u00adtions to relax the traditional \nrestrictions of data-oriented program\u00adming models to support mutation of arbitrary data structures. We \nhave implemented a compiler for Bamboo which generates code for the TILEPro64 many-core processor.Wehaveevaluated \nthis implementation on six benchmarks:Tracking,a feature track\u00ading algorithm from computer vision; KMeans, \na K-means cluster\u00ading algorithm; MonteCarlo, a Monte Carlo simulation; FilterBank, a multi-channel .lter \nbank; Fractal, a Mandelbrot set computation; and Series,aFourier series computation.We found that our \ncom\u00adpiler generated implementations that obtained speedups ranging from 26.2\u00d7 to 61.6\u00d7 when executed \non 62 cores. Categories and Subject Descriptors D.1.3 [Concurrent Pro\u00adgramming]: Parallel programming; \nD.3.2 [Language Classi.\u00adcations]: Data-.ow languages; G.1.6 [Optimization]: Gradient Methods General \nTerms Algorithms, Languages Keywords Many-core Programming, Data-Centric Languages 1. Introduction With \nthe wide-scale deployment of multi-core processors and the impending arrival of many-core processors, \nsoftware developers must write parallelsoftware to realize the bene.ts of continued im\u00adprovements in \nmicroprocessors. Developing parallel software using today s development tools can be challenging. These \ntools require Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page.To copyotherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI \n10, June 5 10, 2010,Toronto, Ontario, Canada. Copyright c &#38;#169; 2010 ACM 978-1-4503-0019-3/10/06. \n. . $10.00 developers to expose parallelism as threads and then control con\u00adcurrent access to data with \nlocks. Reasoning about the correctness and performance of these systems has proven to be dif.cult. In \nthe past, mainstream processors have presented software de\u00adveloperswitha relatively static programmingtargetatthe \nlanguage abstractionlevel.Weexpectthiswillnolongerbe true inthefu\u00adture asfabrication technologies advance, \nthe number and types of cores in each successive microprocessor generation will change. Adapting thread-based \napplications to these changes may require signi.cant refactoring efforts. Developing software for many-core \nprocessors will clearly bene.t from both new tools and languages. This paper presents a data-oriented \nextension to Java. In this approach, the developer focuses on how data .ows through the application. \nOur approach borrows inspiration from data.ow and stream-based languages and extends the basic data-oriented \nap\u00adproachfor usein imperative, object-oriented languages.Traditional data-oriented programming approaches \nplace severe restrictions on howprograms mutate data structures; Bamboo relaxesthese restric\u00adtions to \nallow programs to freely mutate data structures. Bamboo programs are composed of a set of tasks that \nimple\u00adment the program s operations. Objects have abstract states asso\u00adciate with them and tasks contain \nparameter guards that specify the abstract states of the parameter objects that they can operate on.Taskshave \ndata-orientedinvocation semantics: when thereex\u00adist objects with abstract states that satisfy a task \ns guards, the run\u00adtime invokes the task on the objects.Tasks can in turn modify the abstract states of \nthe objects theyoperate on. Bamboo s abstract object states are similar to typestates [31]. Like typestates, \noperations can change the abstract states of objects and an object s abstract state determines which \noperations can be invoked on the object. However, Bamboo s abstract object states differ from typestates \nin that abstract object states are dynamically computed and used to determine which operation to invoke \nnext, while typestates are used to statically detect errors in object usage. Conceptually, abstract states \nserve to control access to data at the task level. For example, an email client may have message objects \nwith both an edit state and a sent state. The send task might transition a message object into the sent \nstate, preventing anyfurther edits to the message. Bamboo supports traditional method calls from both \ntasks and other methods. Bamboo restricts the usage of global variables tasks (and methods) can only \nread their parameter objects or objects reachable from the parameters. Tasks do not maintain persistent \nstate between invocations therefore, a Bamboo implementation can safely create multiple parallel instantiations \nof a task if these instantiations operate on disjoint parameter objects.  The Bamboo compiler is staged \nas follows: Dependence Analysis: The compiler performs a static depen\u00addence analysis to determine (1) \nthe set of abstract states that objects can reach and (2) how tasks cause objects to transition through \nthese abstract states. The analysis generates as output a .nite state machine for each class in which \nthe states model the abstract object states and the transitions model the effects of tasks on these abstract \nstates.  Disjointness Analysis: Thecompiler performsastatic disjoint\u00adness analysis on the Java-like \nimperative code inside Bamboo tasksand methodsto determine whetheratask introduces alias\u00ading between \nparameter objects. Bamboo uses the disjointness analysis to automatically generate .ne-grained locking \ncode that ensures transactional task semantics. Bamboo transactions are light-weight atinvocation,a task \nsimply locksits param\u00adeter objects. If the runtime cannot lock all of a task s parame\u00adters, the runtime \nreleases the locks andexecutesadifferent task. Tasks never abort and theyincur no extra overheads.  \nImplementation Generation Algorithm: The implementation generation algorithm takes as input the static \nanalyses results andpro.le information.The algorithm usesasetof transforma\u00adtions to generate many candidate \nmany-core implementations of the application to serve as starting points for optimization.  Implementation \nOptimization: The evaluation stage uses high-level abstract simulation to evaluate the relative perfor\u00admanceofthe \nimplementations.The compiler performsacritical path analysis on the simulated execution to identify opportuni\u00adtiestoimprovethe \nimplementationand usesthe resultsto direct a simulated annealing-based search. The compiler generates \ncode for the best implementation.  1.1 Relation with Stream, Data.ow, andTuple-space Languages Bamboo \nis closely related to both stream and data.ow languages. Stream and data.ow languages traditionally impose \nsevere restric\u00adtions on how an application can mutate data and often forbidding it. Theyoften require \napplications to access data structures in deter\u00administic patterns. These restrictions are imposed because \nthese lan\u00adguages typically do not contain concurrencyprimitives that restrict access to shared state \nand instead must make the state immutable. Bamboo combines transactional task semantics together with \nabstract object states to control access to shared data structures. This enables Bamboo to support mutation \nof structurally complex shared data structuresinadata-oriented programming model. Bam\u00adboo s task parameter \nobjects are intended to be the roots of disjoint heap data structures. The Bamboo compiler includes a \nstatic dis\u00adjointness analysis that detects violations of this disjointness prop\u00aderty and generates a \nlocking strategy that guarantees transactional semantics. The disjointness analysis enables the Bamboo \nimple\u00admentation to ef.ciently provide transactional task semantics. The combination of abstract object \nstates and task dispatch allows Bamboo to support applications that must access shared data structures \nat non-deterministic times in the computation. Abstract object states serve to control these accesses. \nTuple-space languages contain similar constructs to Bamboo s global object space. However, threads in \ntuple-space applications can contain internal state and can manipulate the tuple-space in arbitrary ways. \nThese differences make it dif.cult for a compiler to automatically understand the role of a thread in \na tuple-space language and frustrate efforts to automatically parallelize the thread by creating multiple \ninstantiations. 1.2 Contributions This paper makes the following contributions: Hybrid Data-Oriented \nApproach: It presents the Bamboo language, which implements a data-oriented programming model in the \ncontext of an imperative, object-oriented program\u00adming language.  Data Structure Mutation: It presents \na data.ow-based pro\u00adgramming model that supports data structure mutation. Many algorithms are most naturally \nexpressed in terms of state mu\u00adtations. This approach allows developers to easily code these algorithms \nwhile still bene.ting from high-level language sup\u00adport for data.ow-type constructs.  Automatic Implementation \nOptimization: It presents an al\u00adgorithm that automatically generates many candidate many\u00adcore implementations \nof the application. This algorithm com\u00adbines pro.le information with a simulation-based implementa\u00adtion \nevaluation algorithm to generate many-core implementa\u00adtions that are optimized for the target processor. \n Evaluation: It presents an evaluation of Bamboo on a 64-core TILEPro64 microprocessor [2] for several \nbenchmarks. The TILEPro64 processorisamany-coreCPUand representativeof the many-core microprocessors \nthat willbecome commonplace inthe future.We foundthatitwas relatively simpletodevelop Bamboo implementations \nof the benchmarks and that the im\u00adplementations madeeffective useofavailable cores. Theexper\u00adimental \nresults show that Bamboo is able to achieve speedups between 26.2\u00d7 and 61.6\u00d7 for our benchmarks. Moreover, \nwe found that Bamboo successfully generated a sophisticated het\u00aderogeneous implementation of the MonteCarlo \nbenchmark that used pipeliningtooverlapthe simulationand aggregation tasks.  The remainder of the paper \nis structured as follows. Section2 presents an example that we use to illustrate our approach. Sec\u00adtion3presents \nthe Bamboo language. Section4presents the imple\u00admentation synthesis algorithm. Section 5 presents our \nevaluation of the approach on several benchmark applications. Section6dis\u00adcusses related work; we conclude \nin Section 7. 2. Example We presentakeyword countingexampleto illustrate Bamboo. 2.1 Classes Figure1presents \npartof the Text class declaration for the exam\u00adple.Thekeyword counter uses instancesofthe Text class \nto divide the input text into sections and count occurrences of each word. Class declarations contain \ndeclarations for the class s abstract states. An abstract state is declared with the keyword flag fol\u00adlowedbyaname. \nBamboo sabstract states support orthogonal clas\u00adsi.cationsofobjects:anobjectmay simultaneouslybeinmorethan \none abstract state. The runtime uses the abstract state of an object to determine which tasks toinvokeonthegiven \nobject.Whenatask exits, it can change the values of the .ags of its parameter objects. 1 class Text {2 \nflag process; 3 flag submit; 4 ... 5 } Figure 1. Text Class Declaration  The Text class in the example \ncontains two abstract state dec\u00adlarations: the process .ag, which indicates that the Text object is ready \nto be processed, and the submit .ag, which indicates that the Text object can submit its result. 2.2 \nTasks Bamboo applications are structured as a collection of tasks. The keydifference between tasks and \nmethods is that tasks have data\u00adorientedinvocation semantics:the runtimeinvokesa task when the heap contains \nobjects with the appropriate abstract state to serve as the task s parameters. Note that while the runtime \ncontrols task invocation, tasks can call methods. The runtime uses a task s spec\u00adi.cation to determine \nwhich objects serve as the task s parameters and when to invoke the task. Each task declaration consists \nof the keyword task, the task s name, the task s parameters, and the body of the task. Figure 2 presents \nthe task declarations for the example. The .rst task declaration declares the startup task. The guard \nin initialstate declares that the StartupObject object must have its initialstate .ag set before the \nruntime can invoke this task. The runtime invokes the task when there exist parameter objects in the \nheap that satisfy the parameters guard expressions. Before exiting, the taskexit statement in the startup \ntask re\u00adsets the initialstate .ag in the StartupObject tofalse to prevent the runtime from repeatedly \ninvoking the startup task. The abstract state-based programming modelisapowerful con\u00adstruct for specifying \nwhich objectsatask operates on.For manyap\u00adplications, it is possible to specify these dependencies with \ngraphi\u00adcaltask dependence diagrams.Bamboo supportstheuseofexternal graphical toolsin whichadeveloperwoulddrawa \ndependencedia\u00adgram from which the tool automatically generates task declarations. 1 task startup(StartupObject \ns in initialstate) { 2 Partitioner p = new Partitioner(s.args[0]); 3 while (p. morePartitions()) { 4 \nString section =p.nextpartition(); 5 Text tp = new Text(section) { process:= true} ; 6 } 7 Results rp \n= 8 new Results(p. sectionNum()) { finished:= false}; 9 taskexit (s: initialstate:= false ); 10 } 11 \ntask processText(Text tp in process) { 12 tp .process(); 13 taskexit (tp: process:= false , submit:= \ntrue ); 14 } 15 task mergeIntermediateResult( 16 Results rp in !finished , Text tp in submit) { 17 boolean \nallprocessed = rp .mergeResult(tp); 18 if (allprocessed) 19 taskexit (rp: finished:= true ; 20 tp: submit:= \nfalse ); 21 taskexit (tp: submit:= false ); 22 } Figure 2. Flag Speci.cations forTasks  2.3 Execution \nWe next describe the execution of the example. It performs the following operations (although not necessarily \nin this order): Startup: The runtime creates a StartupObject object in the initialstate abstract state. \nThis causes the runtime to invoke the startup task. This task creates a Partitioner object to partition \nthe text stream into sections.For each sec\u00adtion, the task creates a new Text object in the process ab\u00adstract \nstate, which indicates that the object is ready for process\u00ading. The task then creates a Results object \nto merge the in\u00adtermediate results. Finally, it transitions the StartupObject object out of the initialstate \nabstract state to prevent the runtime from repeatedly invoking the startup task. Processing a Text Section: \nWhen the runtime identi.es a Text object in the process abstract state, it invokes the processText task \non that object to process the text section and stores the intermediate result in the object. Upon exiting, \nthe objectis transitioned from the process abstract state to the submit abstract state to indicate that \nthe intermediate re\u00adsult can be merged into the .nal result and to prevent repeated invocations of processText \ntask on this object.  Merging Results: The mergeIntermediateResult task merges the intermediate results \nfrom the Text objects into the Results object. It transitions the Text object out of the submit abstract \nstate to prevent merging the same object again. If it has merged all of the intermediate results, it \ntransi\u00adtions the Results object to the finished abstract state.   2.4 Scheduling The Bamboo compiler \nuses pro.le information anda processor de\u00adscription to generate a binary that is optimized for both the \nap\u00adplication s runtime behavior and the target processor. The Bam\u00adboo compiler contains an automatic \nimplementation synthesis and evaluation-based optimizationframework that generates andevalu\u00adates manypossible \ncandidate implementations of the application to synthesize an optimized many-core implementation. The \nBamboo compiler generates a combined state transition graph (CSTG) to reason about the possible behaviors \nof the ap\u00adplication. Figure3presents the CSTG for theexample. The nodes in this graph model the abstract \nobject states for the classes that serve as task parameters.Two concentric ellipses indicate that the \nobject can be allocated with this abstract state. Solid edges model the state transitions causedby theinvocationofa \ntask on an object. Dashed edges model the creation of new objects a new object edge points from the \ntask that creates an object to the abstract state node that abstracts the state of the newly created \nobject. Figure 3. CSTG for theKeyword Counting Example The compiler associates pro.le information with \nthe nodes and edges in the CSTG. The CSTG combined with the pro.le informa\u00adtion forms a Markov model \n[26] of the program s execution. The solid edges contain labels with the name of a task followed by a \ncolonandatuplethat containstheexpectedtimethetasktakesto execute if it makes this transition and the \nprobability that the task will take this transition. The dashed edges are labeled with a tuple thatgivestheexpected \nnumberofnewly created objects.Forexam\u00adple, the edgein Figure3from the startup task to the process state \nis labeled 4 indicating that the task may generate four new Text objects in the process state. Each node \ncontains an ab\u00adstract object state followed by a colon and a lower bound estimate of the time it takes \nto complete processing an object in this state.  The compiler transforms the CSTG to optimize the application \ns implementation foragiven processor.Forexample, the algorithm might begin by generating multiple instantiations \nof the tasks that process Text objects. The compiler uses these transformations to generate candidate \nlayouts (as presented in Figure 4) to serve as starting points for the directed-simulated annealing optimization \nal\u00adgorithm.The algorithm .rst runsahigh-level simulationtoevaluate these layouts using the pro.le statistics. \nThen it computes the crit\u00adical path subject to scheduling constraints of the simulated execu\u00adtion and \nuses this critical path along with dependence information to identify a set of tasks that can potentially \nbe migrated to differ\u00adent cores to reduce the length of the critical path. It then generates a set of \ncandidate layouts that implement these transformations as the starting point for an iterative optimization \nprocedure. When it reaches the point of diminishing returns, the compiler selects the best candidate \nlayout and generates the corresponding executable. Figure4 presentsa candidate layoutof thekeyword counting \nexample for a quad core processor. This implementation deploys all tasks on core0while deploying only \nthe processText task on the other three cores. Theexecution distributes the Text objects to all4coresina \nround-robinfashion. 3. Bamboo Language Bamboo applications are composedofasetof tasks that implement \nhigh-level operations and a set of task declarations that describe whentoexecute these tasks. Bamboo \ntasks are writteninan object\u00adoriented, type-safe subset of Java. The task declaration language describes \nthe data dependencies between tasks. Tasks are blocks of code that encapsulate individual conceptual \noperations. Each task contains a task declaration that the runtime uses to determine (1) when to execute \nthe task, (2) what data the task needs, and (3) how the task changes the role this data plays in the \ncomputation. Thus the set of task declarations describes the dependencies between the tasks. Bamboo associates \nabstract object states with objects. The abstract object states are used to determine which tasks should \nbe invoked on an object. Figure 5 presents the grammar for Bamboo s task extensions to Java. Each task \ncontains a set of guards that specify when the runtime should invoke the task. The guards contain a set \nof predicates on the abstract states of the parameter objects. Only objects whose abstract state satisfy \nthe task s guards can serve as a parameter object to the task. Tasks can in turn modify an object s abstract \nstate when (1) the object is allocated or (2) at the completion of its execution. An abstract object \nstate is declared in a class declaration by using the flag keyword followed by the state s name. Developers \ncan use abstract object states to capture orthogonal aspects of an object s current role in the computation: \ntherefore an object can simultaneously be in more than one abstract object state. The runtime uses an \nobject s abstract state to determine which tasks to invoke on that object. It uses a task s declaration \nto deter\u00admine which objects can serve as the task s parameters and when to invoke the task. When the \nheap contains parameter objects with the speci.ed abstract states to serve as a task s parameters, the \nruntime invokes that task on those objects. Bamboo provides a tag construct to support reuse of blocks \nof tasks and to group related objects. Consider a Bamboo task\u00adbased library routine1 that takes as input \nan Image object in the uncompressed state, compresses the image, and then transitions 1Bamboo also supports \nstandard method-based libraries. Figure 4. Scheduling Layout on a Quad Core Processor .agdecl := flag \n.agname; tagdecl := tagtype tagname; taskdecl := task name(taskparamlist) taskparamlist := taskparamlist, \ntaskparam | taskparam taskparam := type name in .agexp with tagexp |type name in .agexp .agexp := .agexp \nand .agexp | .agexp or .agexp | !.agexp | (.agexp) | .agname | true | false tagexp := tagexp and tagtype \ntagname | tagtype tagname statements := ... | taskexit(.agactionlist) | tag tagname = new tag(tagtype) \n| new name(params){.agortagactions} .agactionlist := .agactionlist; name : .agortagactions | name : .agortagactions \nparams := ... | tag tagname .agortagactions := .agortagactions, .agortagaction |.agortagaction .agortagaction \n:= .agaction | tagaction .agaction := .agname := boolliteral tagaction := add tagname | clear tagname \nFigure 5. Task Grammar the Image object to the compressed state.A startsave task for a graphics editing \napplication might take as input a Drawing object and create an uncompressed Image object. A block of \ntasks in the library would then perform a set of task invoca\u00adtions thateventually transition the uncompressed \nImage object to the compressed state. A second finishsave task would then take as input the Drawing object \nand the compressed Image object. It is important that the finishsave task re\u00adceives the compressed Image \nfor the speci.c Image object the startsave task created and not some other Image ob-ject.Tags solve this \nproblem. The startsave task creates a tag and tags both the Image object and the Drawing object. The \nfinishsave task declaration then speci.es that the Drawing object and the Image object bothhave the same \ntag, ensuring that it receives the correct Image object. Tags serve a second purpose. In addition to \ndisambiguating different uses of the same object, they can disambiguate differ\u00adent instances of the same \nuse. Suppose that two Drawing ob\u00adjects were simultaneously saved. Without tags, it is possible for a \ncompressed Image object to be associated with the wrong Drawing object. Tags are allocated using the \nnew tag state\u00adment. Methods can declare tag parameters and tag instances can be passed into a method \ncall.  Bamboo provides a modi.ed new object allocation statement. This statement takes as input the \ninitial abstract state for objects allocated at this site and a list of tag variables whose tag instances \nshould be bound to the newly allocated objects. Bamboo provides the taskexit statement that can change \nthe abstract states and tag bindings of the task s parameter objects and then exits the task. The current \nimplementation of Bamboo is data-oriented at the top-level Bamboo does not contain threads and Bamboo \nappli\u00adcations are started by the creation of a StartupObject object. It is straightforward to implement \nBamboo as a strict extension to Java thread-based code would then use data-oriented compo\u00adnents through \nan asynchronous calling interface. 4. Implementation Synthesis We next present the Bamboo compiler s \nalgorithm for generating an optimized implementation. The Bamboo compiler uses the fol\u00adlowing staged \nstrategy to synthesize application implementations: 1. Dependence Analysis: The dependence analysis processes \nthe task declarations to characterize the tasks data dependences. 2. Disjointness Analysis: Disjointness \nanalysis processes the im\u00adperativecode inside of Bamboo tasks and methods to determine whether it introduces \nsharing between different task parameter objects. The compiler uses this information to generate locks \nthat guarantee transactional semantics for task invocation. 3. Candidate Implementation Generation: \nCandidate imple\u00admentation generation synthesizes non-isomorphic candidate implementations. Several implementations \nare randomly gen\u00aderated to serve as a starting point for further optimization. 4. Simulation-based Evaluation: \nThe evaluation phase uses pro\u00ad.le information along with an architectural speci.cation to per\u00adform a \nhigh-level simulation of the candidate implementation. If the pro.le indicates that the application terminates, \nthe sim\u00adulation computes an estimated execution time. Otherwise, it es\u00adtimates the percentage of the \ntime spent doing useful work. 5. Optimization: Based on the evaluation results, the .nal stage uses \ndirected-simulated annealing to iteratively optimize the candidate implementations to improve their performance. \n We next present each of these stages in more detail. 4.1 Dependence Analysis The dependence analysis \noperates on abstract state transition graphs (ASTGs) [17]. An ASTG is associated with an object type \nand abstracts the possible state transitions of instances of that type. An ASTG is composed of abstract \nstate nodes and edges between these nodes. An abstract state node represents the abstract state and tag \ncomponents of an object s state it contains the states of all the object s abstract states and a 1-limited \ncount (0, 1, or at least 1) of the tag instances of each type that are bound to the object. If an object \nin the computation can reach a given abstract state, the abstract state transition graph for that object \ns class contains the corresponding abstract state node. The edges in the ASTG abstract the actions of \ntasks on objects. If a task can transition an object from one abstract state to a second abstract state, \nthen there is an edge labeled with that task from the abstract state node that cor\u00adresponds to the .rst \nabstract state to the abstract state node that corresponds to the second abstract state. 4.2 Disjointness \nAnalysis Disjointness analysis determines whether the parts of the heap reachable from distinct task \nparameter objects are disjoint [23]. Disjointness analysis differs from pointer analysis in that it can \nde\u00adtermine that two objects represented by the same static node are distinct if the parameter objects \nthey are reachable from are dis\u00adtinct. The analysis reasons about static reachability graphs, which characterize \nthe reachability of each object in the heap from a se\u00adlect set of root objects. Nodes in reachability \ngraphs represent ob\u00adjects and edges represent heap references. The graphs are annotated with sets of \nreachability states that describe which objects can reach other objects. The analysis uses the reachability \nstates to determine if a task introduces sharing between the parts of the heap reachable from twodifferent \nparameter objects. If the analysis determines that a task may create sharing between the disjoint heap \nregions asso\u00adciated with two different parameter objects, the compiler generates code that adds a shared \nlock for the two parameter objects. 4.3 Candidate Implementation Generation This stage in the compilation \nprocess generates several candidate implementations. These candidates serveasastarting point for later \nevaluation and optimization stages. The implementation generation process is structured as three steps: \n(1) generate a combined state transition graph that characterizes the dependences between parts of the \napplication, (2) transform this graph to expose parallelism, and (3) search for mappings of the transformed \ngraph to the target processor.We next discuss these stepsin detail. 4.3.1 Characterizing theApplication \nThe Bamboo compiler combines the ASTGs for the individual classes into a combined state transition graph \n(CSTG) to charac\u00adterize the entire application. It then annotates the nodes and edges in the CSTG with \nruntime pro.le information. The Bamboo compiler uses pro.le data to obtain the analog of a developer \ns intuition about the behavior of applications. Bamboo supports generating single or many-core pro.ling \nversions of ap\u00adplications. Single-core pro.ling is used to bootstrap the application synthesis process.Apro.le \nincludescycle countsfor taskinvoca\u00adtions,thetaskexittakenbyeachtaskinvocation,anda countofthe numberof \nparameter objectsagiven taskinvocation allocated. The Bamboo compiler processes the pro.le data to compute \nstatistics including the average execution time a task takes for a given exit, the probability that the \ntask takes the exit, and the average number of new objects allocated when the task takes the exit. Figure3from \nSection 2.4 presents anexample CSTG.Asolid rectangle in the graph represents a core group all tasks \nin a core group will be mapped onto the same core. It also describes the possible state transitions of \nthe objects on that core. Therefore, a CSTG represents a possible implementation of a computation on \na many-core processor. The compiler next performs a series of transformations on the CSTG to optimize \nthe implementation.  4.3.2 Preprocessing A tree transformation phase preprocesses the CSTG to prepare \nit for subsequent parallelization phases. We note that core groups may have more than one incident new \nobject edge. These edges represent disjoint sourcesofworkforthe core groupand presentan opportunity for \nparallelism the compiler can replicate the core group for each source of work. This phase transforms \nthe CSTG intoatreeof strongly connected components (SCCs)byduplicating SCCs as necessary. The algorithm \nbegins by computing the SCCs in the CSTG. For the purpose of computing SCCs in the CSTG, the compiler \nconceptually inserts a task node on each task transition edge that serves as the source of all the new \nobject edges associated with that task transition. It then duplicates SCCs which have more than one incident \nedge originating from different SCCs. This process continues until each core group (except the StartupObject \nclass core group) has exactly one incident new-object edge.  4.3.3 Parallelizing the Implementation \nThe Bamboo language is implicitly parallel. This phase transforms the CSTG to make the parallelism inherent \nin the application ex\u00adplicit. It is structured as a set of rules, where each rule transforms the CSTG \nto address an opportunity to improve performance. The compiler implements the following transformation \nrules: Data Locality Rule: The default rule maximizes data locality by placing tasks on the same core \nunless other rules apply. This minimizes communications to coordinate the task invocation. Moreover, \nthis optimization is likely to improve performance due to caching.  DataParallelization Rule: If a task \nin one core group creates objectsofa class thatis processedbya second core group, task invocations on \nthese new objects can potentially be processed in parallel with task invocations in the .rst core group. \n Pro.le information for allocation sites contains the expected number m of objects a given task invocation \nwill allocate. The compiler then replicates the destination core group to generate m - 1 new copies. \n Rate Matching Rule: ShortcyclesinaCSTGthat producenew objects can overwhelm a consumer core group s \nability to pro\u00adcess these objects.We introducea rule that replicates the con\u00adsumer core group as necessary \nto match the object consumption rate with the creation rate.We apply this rule onlyif the source coregroupisinadifferentSCCthanthe \ndestinationcoregroup. Given the expected number m of allocated objects on the allo\u00adcation site from the \npro.le, the peak new object creation rate is  m and the object consumption rate for n copies of the \ncon\u00ad tcycle sumer core group is n . tprocess Fora taskAthat allocates new objects, lettrecycle be the \ntime of the shortest path from the destination ofAto the source of A. tcycle = tA + trecycle is the shortest \ntime to complete the cycle. tprocess is the estimated object processing time for the consumer core group \ncontaining the new object created by A. Matching these rates we get n = I mtprocess l. The compiler compares \nn to tcycle m. If m is greater, it applies the data parallelization rule. If n is greater,it appliesthecycle \nrate matchingruleto generate n - 1 new copies of the destination core group.  4.3.4 Mapping to the Processor \nWe next discuss how the compiler maps an optimized CSTG to layouts for a physical processor. The mapping \nprocess uses a backtracking-based search algorithm to generate non-isomorphic mappings of the SCCs of \ncore groups to the cores. We have ex\u00adtended the standard enumeration algorithm to randomly skip sub\u00adsets \nof the search space. Thus, the extended algorithm generates a random set of non-isomorphic mappings. \nFor each mapping, the compiler generates a candidate layout. Figure4from Section 2.4 presents anexample \nlayout. The layout speci.es for each core: (1) which tasks are on the core and (2) a table that lists \nfor each destination abstract object state that may be generated by tasks on the core where the core \nshould send the object. If there are multiple destinations for the same abstract object state, the runtime \ndistributes the objectsina round-robinfashion. An issue arises if the generated layout includes more \nthan one instantiation of a task that operates on multiple parameter objects. Sucha task canfailto triggerevenif \nobjects areavailableto serve as all of its parameter objects because the parameter objects could be enqueued \nin different instantiations of the task. If the task decla\u00adration requires that all parameter objects \nare tagged with the same tag, Bamboo hashes the tag instances bound to an object to deter\u00admine which \ncore to send the object to. Otherwise, it only generates one instantiation of the task and statically \nchooses one core group to process this instantiation. The host core can be speci.ed by the developer \nor randomly chosenby the compiler.  4.4 Performance Estimation The candidate implementation generation \nstage creates many can\u00addidate layouts for an application.Ahigh-level discrete-event simu\u00adlation estimates \nthe relative performance of these candidate layouts using pro.le information. The simulation strategy \nwas designed to support future extensions that would allow the synthesis process to use detailed speci.cationsofanindividual \ncore s capabilitiesanda processor s on-chip network to optimize the executable. Note that the simulator \ndoes not actually execute the application it instead uses pro.le data to estimate for a given layout \nhow long the ap\u00adplication will likely take to execute. We evaluate in Section 5.4, whether the .nal layout \ngeneralizes well to other inputs. The simulationbeginsbyinjectingastartupobjectintotheheap. The simulator \nthen checks if there is a task that can be executed on any core. When a task can be executed, the simulator \nuses a Markov modelbuiltfromthe pro.leto estimate:(1)the destination state of the task, (2) the time \ntaken to execute the task, and (3) a count of each type of new parameter object that the task allocates. \nThe simulator maintains a count for each possible destination state of a task, which it increments when \nthe simulated task takes the given transition. For each task invocation, the simulator chooses the destination \nstate that minimizes the difference between these counts and the counts predicted by the task s recorded \nstatistics. The simulator can accept developer hints that specify for a given task whether the counts \nare maintained on a per object basis or per task basis. It estimates the task execution time using an \naverage of the execution times for a given exit point of the task. The simulator then skips ahead to \nthe .nish time of the currently executing task that will .nish .rst. The simulator state is updated to \nre.ect the completed task. If new objects were created, the simulator generates new object events for \nthe destination cores.  4.5 Optimizing with Directed-Simulated Annealing For realworld applications, \nthe synthesis stage can potentially gen\u00aderate several million or more non-isomorphic candidate layouts. \nAn exhaustive search of these layouts is infeasible for most applica\u00adtions. One possible solution is \nto randomly generate candidate im\u00adplementationstoevaluate. Section5.3 presentsexperimental results that \nshow that implementations with good performance are rare for our benchmark applications and therefore \nrandomly generating im\u00adplementations is unlikely to yield well-optimized implementations. Instead, Bamboo \ncombines random generation of several initial candidate layouts with a directed-simulated annealing based \nop\u00adtimization algorithm. The directed-simulated annealing algorithm improves the candidate layouts to \ngenerate an optimized layout. The design mirrors actions taken by real developers a developer executes \nan application, analyzes the execution to identify possible opportunities for optimization, implements \nthe optimizations, and then repeats the process until she obtains the desired performance. Our directed-simulated \nannealing algorithm operates in an iter\u00adativefashion in each iterationit identi.es performance problems \nin the candidate layouts and then generates several new layouts de\u00adsigned to correct those performance \nproblems. Each iteration be\u00adgins by running the simulation on the set of candidate layouts. The algorithm \nthen prunes the set of candidate layouts using a proba\u00adbilistic strategy: it keeps the best layouts with \na high probability and poor layouts witha small probability.Acritical path analysis for each execution \nidenti.es possible opportunities to further im\u00adprove the candidate layout (see Section 4.5.1). The compiler \nuses the results of this analysis to generate a new set of candidate lay\u00adouts that have been modi.ed \nto attempt to remove the bottleneck  Figure 6. ExecutionTrace for theKeyword Counting Example in the \nprevious candidate (see Section 4.5.2). This iterative pro\u00adcess is repeated until the current best layout \nhas the same or bet\u00adter performance than the candidate layouts from the previous iter\u00adation. Because \nthe optimization process may have simply reached a local maxima, the algorithm probabilisticallydecides \nwhether to continue searching or not, with a high probability to continue. The primary difference between \nour directed-simulated anneal\u00ading algorithm and the standard simulated annealing algorithm is that we \nuse the results of the critical path analysis to direct the gen\u00aderation of future optimized candidate \nlayouts. Section 5.3 presents our evaluation of the directed-simulated annealing algorithm. 4.5.1 CriticalPath \nAnalysis We next describe the critical path analysis used to direct the gener\u00adation of new candidate \nlayouts. The critical path analysis processes execution traces generatedfromthe simulatedexecutionofthelay\u00adouts. \nFigure6presents anexecution trace for thekeyword counting example. Nodes represent events in the simulated \nexecution on the cores. Node labels describe when the events happened. Edges rep\u00adresent eithertaskinvocationsordata \ntransfers betweencores.There are edges between (1) the nodes corresponding to the start and end ofagiventaskinvocation,(2)theendnodeofonetaskandthe \nstart node of the next task on the same core if the invocation of the sec\u00adond task had to wait for the \ncompletion of the .rst task, and (3) the end node of one task and the start node of a second task if \nthe in\u00advocation of the second task had to wait for data from the .rst task. Weights are associated with \neach edge to indicate how long it takes to execute the task instance or transfer the data. Edge labels \ngive the name of the task or the transferred data and the weight. The dashed edges in Figure6 indicate \nthe critical path of this graph. It is the path with the largest weight from the start of the execution \nto the end of the execution. Note that this critical path accounts for both resource and scheduling limitations. \n 4.5.2 Optimizing Implementations For each task instance on the critical path, the optimization algo\u00adrithm \ncomputes the time when its data dependencies are resolved, which is the earliest point when all the parameter \nobjects of the task instance are ready. The algorithm sorts task instances by the data dependence resolution \ntime. Task invocations whose data depen\u00addencies are resolved at the same time compete with each other \nfor computational resources. The algorithm groups such task instances together. Next, it randomly selects \na group to attempt to optimize. A difference between the time at which a task instance s data dependences \nare resolved and when the task is executed implies that the task instance was delayed because of a resource \ncon.ict. If there are spare cores during the interval between a task instance s expected start time and \nits actual start time, the optimization algo\u00adrithm attempts to shorten the critical path by generating \na set of new layouts in which the task instance is migrated to a spare core. When spare cores are not \navailable, moving tasks to other cores can still be desirable. In this case, the optimization algorithm \niden\u00adti.es a set of key task instances tasks on the critical path that produce data that the next taskon \nthe critical path consumes. Care\u00adfully schedulingkeytask instances is likely to be more important than \nother tasks asthere are tasks that depend on the data thatkey tasks produce. The algorithm identi.es \nsituations in which a non\u00adkeytask instance on the critical path delays theinvocationofakey task instance. \nIt attempts to movethe non-keytask instance to other cores to eliminate the resource con.ict. The algorithm \nextends the core search algorithm described in Section 4.3.4 to generate the newcandidate layouts which \nredeploy the chosen task invocations on selected cores. It then iteratively repeats the simulation, evaluation, \nand optimization process until several iterationsfail to yield anyimprovements.  4.6 Comparison to \nDynamic Scheduling An alternative to our approach is to dynamically schedule tasks usinga centralized \nscheduler. Our approach has severalkeyadvan\u00adtages.The.rstadvantageisthatasthe numberof cores increases,a \ncentralized scheduler will quickly become the performance bottle\u00adneck. Our approach generates implementations \nthat distribute the work of scheduling tasks across all cores. The second advantage is that our approach \ncan generate sophisticated implementations that account for future data dependencies.Forexample, we have \nobserved that our approach generates implementations of the Mon\u00adteCarlo benchmark that use pipelining \nto overlap the simulation and aggregation components of the computation. Moreover, it is straightforward \nto extend our basic approach to optimize for data locality, heterogeneous cores, and new network topologies \nby sim\u00adplyextendingthe simulationto model thesefactors.  4.7 Runtime System In general Bamboo uses a \nsimilar runtime strategy as earlier work on Bristlecone [17]. The primary differences are that the scheduler \nis distributed across all cores, the .nite state machines generatedby the static analysis are used to \noptimize task dispatch, Bamboo does not support task rollback or recovery, and the static analysis is \nused to resolve manyinter-core scheduling decisions. Each processor core runs a lightweight Bamboo runtime \nthat schedules tasks for that core. Each task onacore hasaparameter set for each parameter objects that \nmay satisfy the task s parameter guard are placed in the corresponding parameter set. For each combination \nof task and parameter object, the com\u00adbined state transition graph shows the set of tasks that can be \nin\u00advoked next on that parameter object. The compiler generates cus\u00adtomized code for each task that sends \na message directly to the coresthatexecutethenexttaskstoaddtheobjecttothe appropriate parameter sets. \nWhen a new object is added to a parameter set, the runtime enqueues new task invocations (assignments \nof parameter objects to parameters) that the newobject makes possible. The run\u00adtime contains optimizations \nto ef.ciently task dispatch with tags constraints. Each tag instance contains backward references to \nall objects it is bound to. The runtime uses these references to ef.\u00adciently prune task invocations that \ncontain tag constraints. The runtime selects task invocations to execute from the queue oftaskinvocations. \nBeforeexecutingataskinvocation,the runtime locks all of the parameter objects. If it is unable to acquire \na lock, it releases all locks and tries a different task invocation.  5. Evaluation We have developed \na Bamboo implementation, whichcontains ap\u00adproximately 120,000 linesofJava andCcode for the compiler and \nruntime system. The compiler generates C code that runs on the TILEPro64 many-core processor. The TILEPro64 \nprocessor con\u00adtains 64 cores interconnected with an on-chip network. The source code for our compiler \nis available at http://demsky.eecs. uci.edu/compiler.php.We executed our benchmarks on a 700MHz TILEPro64 \nprocessor. We used 62 cores as 2 cores are dedicated to the PCIbus. For each benchmark, we generated \nthree versions: a single-core C version, a single-core Bamboo version, and a 62-core Bamboo version.Weexecuted \nthese threeversions on the TILEPro64 pro\u00adcessorand recordedhowmanyclockcycles weretakenforeachex\u00adecution. \nFigure7presents the results. The reportedexecution times are averaged over .ve executions. Benchmark \nClock Cycles(108cyc) Speedup to 1-Core Bamboo Speedup to 1-Core C Overhead of Bamboo 1-Core C 1-Core \nBamboo 62-Core Bamboo Tracking 405.2 406.4 15.5 26.2 26.1 0.3% KMeans 1124.6 1243.8 32.0 38.9 35.1 10.6% \nMonteCarlo 44.4 47.0 1.3 36.2 34.2 5.9% FilterBank 554.6 554.9 14.8 37.5 37.5 0.1% Fractal 162.5 172.6 \n2.8 61.6 58.0 6.2% Series 1774.7 1885.7 30.8 61.2 57.6 6.3% Figure 7. Speedup of the Benchmarks on 62 \ncores 5.1 Results We report our results on six benchmarks: Tracking The tracking benchmarkextracts motion \ninformation from a sequence of images. It was ported from the San Diego Vision benchmark suite [32]. \nFigure8shows the task .owof the Bambooversion. Theverti\u00adcal lines divide thetask .ow into the three major \ncomputation phases: image processing, feature extraction, and feature track\u00ading. Each node represents \na task. Edges show how data .ows between the tasks. Dashed boxes group related tasks together. The benchmark \nutilizes data parallelism: the image is divided into multiple pieces and each piece is wrapped with a \ntask parameter object. The computations in the dashed boxes work on pieces and then later aggregate these \nresults for these pieces. The speedup of the 62-core Bamboo version is 26.2\u00d7 relative to the single-core \nBamboo version and is 26.1\u00d7 relative to the single-coreC version. KMeans: The KMeans benchmark groups \nobjects in an N\u00addimensional space into K clusters. The algorithm is used to partition data items into \nrelated subsets.We ported it from the STAMP benchmark suite [9]. Our implement differs from the originalversioninthatitdoesnotuse \ntransactionstoupdatethe shared data structures. Instead, one core runs a task to update this data structure, \nand the other cores send partial results to that core.The speedupofthe 62-core Bambooversionis38.9\u00d7 relativetothe \nsingle-core Bambooversionandis35.1\u00d7 relative to the single-coreC version.  MonteCarlo: The MonteCarlo \nsimulation benchmark was ported from the Java Grande benchmark suite [29]. It imple\u00adments a Monte Carlo \nsimulation. The speedup of the 62-core Bamboo version is 36.2\u00d7 relative to the single-core Bamboo version \nand is 34.2\u00d7 relativetothe single-coreCversion.  Figure 8. Task Flowof theTracking Benchmark We were \nsurprised to .nd that for larger workloads Bamboo generated a sophisticated heterogeneous implementation \nthat used pipelining to improve performance by overlapping sim\u00adulation and aggregation.We further discuss \nthisin Section 5.4. FilterBank: FilterBankisa multi-channel .lterbankfor multi\u00adrate signal processing. \nWe ported this benchmark from the StreamIt benchmark suite [20]. It performs a down-sample fol\u00adlowed \nby an up-sample on each channel and then combines the results for all channels. The speedup of the 62-core \nBamboo version is 37.5\u00d7 relativetothe single-core Bambooversionand is 37.5\u00d7 relativetothe single-coreC \nversion.  Fractal: Fractal computes a Mandelbrot set. We observed a 61.6\u00d7 speedup of the 62-core Bamboo \nversion relative to the single-core Bamboo version and a 58.0\u00d7 speedup relative to the single-coreC version. \n Series: Series computesFourier coef.cients.We portedit from the Java Grande benchmark suite [29]. The \nspeedup ofthe 62\u00adcore Bamboo version is 61.2\u00d7 relative to the single-core Bam\u00adboo version and 57.6\u00d7 relativetothe \nsingle-coreCversion.  The directed-simulated annealing algorithm took 1.3 minutes to optimize theTracking \nbenchmark, 10 seconds for the KMeans benchmark, and less than 0.2 seconds for the other benchmarks. The \nBamboo compilerwasexecuted ona quad-core 2.00 GHz Intel Xeon running 64-bit Linux version 2.6.18.  5.2 \nAccuracy of Scheduling Simulator The accuracyof the high-level scheduling simulator is important as the \n.nal implementation is selected based on the scheduling simu\u00adlation results.Toevaluatethe accuracyofthe \nscheduling simulator, we compared the estimated execution time for the 62-core Bam\u00adboo implementation \nstrategy chosen by the scheduling simulator with the real execution time of the corresponding 62-core \nbinary for each of our benchmarks.  Benchmark 1-Core Bamboo Version 62-Core Bamboo version Clock Cycles(108cyc) \nError Clock Cycles(108cyc) Error Estimation Real Estimation Real Tracking 405.9 406.4 -0.1% 14.9 15.5 \n-3.9% KMeans 1265.1 1243.8 1.7% 31.9 32.0 -0.3% MonteCarlo 47.1 47.0 0.2% 1.2 1.3 -7.7% FilterBank 554.8 \n554.9 -0.02% 14.1 14.8 -4.7% Fractal 170.7 172.6 -1.1% 2.8 2.8 0.0% Series 1856.7 1885.7 -1.5% 29.9 30.8 \n-2.9% Benchmark Pro.leoriginal , Inputdouble Pro.ledouble , Inputdouble Clock Cycles (108cyc) Speedup \nClock Cycles (108cyc) Speedup 1-Core 62-Core 62-Core Tracking 1594.0 44.8 35.6 44.7 35.7 KMeans 5147.9 \n125.8 40.9 125.5 41.0 MonteCarlo 94.1 2.6 36.2 1.8 52.3 FilterBank 1109.6 19.9 55.8 19.9 55.8 Fractal \n289.8 5.8 50.0 5.1 56.8 Series 3785.4 61.3 61.8 63.6 59.5 Figure 11. Generality of Synthesized Implementations \n Figure 9. Accuracyof Scheduling Simulator Figure9 presents the results. The simulation s predictions \nare closetotherealexecution time.For MonteCarlo,the estimationfor the 62-core Bamboo version is 7.7% \nless than the real execution time.A closerexaminationof the pro.ling data shows that when executing on \n62 cores, the execution of individual tasks slowed down. The 4.7% difference for FilterBank has a similar \ncause. 5.3 Ef.ciency of Directed-Simulated Annealing We next discuss our evaluation of the directed-simulated \nannealing used to ef.ciently optimize the many-core implementations. In this experiment, we exhaustively \ngenerated all candidate implementa\u00adtions. We did the evaluation on 16 cores instead of 62 cores be\u00adcause \nan exhaustive search of all candidate implementations for 62 coresis prohibitivelyexpensive.For each \nbenchmark,we selected an input and collected the corresponding pro.ling data.For most benchmarks, we \n.rst generated all possible candidate implementa\u00adtions and used the scheduling simulator toevaluate them.We \ndid not perform thisexperiment for theTracking benchmark as anex\u00adhaustive search for even 16 cores is \nprohibitively expensive. The empty bars in Figure 10 present result of this experiment which show the \nprobability distributions for the candidate implementa\u00adtions. The x-axis is the estimated execution time \nof the candidate implementation. The y-axis is the relative percentage of the partic\u00adular estimated execution \ntime. Candidate implementations with the smallest estimated execution times are the best. The graphs \nshow that for most benchmarks there is a very small chance of randomly generating thefastest implementation. \nWe next show that directed-simulated annealing greatly in\u00adcreases the probability of synthesizing thefastest \nimplementation. For each benchmark, we randomly chose 1,000 candidate imple\u00admentations as starting points \nfor the directed-simulated anneal\u00ading algorithm. For each starting point, we executed the directed\u00adsimulated \nannealing and recorded the execution time of the best implementation generated by the directed-simulated \nannealing al\u00adgorithm. The solid bars in Figure 10 present the probability distri\u00adbutions for thisexperiment.We \nfound that with directed-simulated annealing, the probability of generating the best candidate imple\u00admentation \nfrom a random starting point is larger than 98% for all benchmarks.For KMeans, the probability reaches \n100%. 5.4 Generality of Synthesized Implementation We used pro.ling data to generate an implementation \nthat is op\u00adtimized for the target many-core processor.Weexpect thatif the pro.le data exposes suf.cient \nparallelism in the application, the optimized implementation willwork well for inputslarger than the \ninput for the pro.led execution. We next discuss our evaluation of how well our optimized im\u00adplementation \ngeneralize to other inputs.For each benchmark, we de.ne the original input as Inputoriginal and de.ned \na second input Inputdouble, which contains a workload that is twice as large. We then collected pro.ling \ndata Pro.ledouble for Inputdouble and gener\u00adateda new 62-core Bambooversion using the Pro.ledouble.Weex-ecuted \nthe new version as well as the single-core Bamboo version and the 62-core Bambooversion generated with \nPro.leoriginal on the new Inputdouble. The results are presented in Figure 11. Formost benchmarks,thespeedupofboth62-coreBamboover\u00adsions \nare similar indicating that the synthesized binaries general\u00adizetodifferent inputs.For MonteCarlo,the \n62-coreversion gener\u00adated using Pro.ledouble performs much better on Inputdouble than the 62-core version \ngenerated using Pro.leoriginal. After examining the two versions, we were surprised to discover that \nour search-based synthesis algorithm generatedaheterogeneous implementation that utilized pipelining \nto overlap the aggregation and simulation com\u00adputations. The smaller input size does not contain enough \nwork to bene.t from the pipelining strategy, and therefore did not yield a pipelined implementation.Weexamined \nFractal and Series and discovered that the differences between the speedups are due to differences in \nthe workloads on the cores, which occurs because object are distributed in different orders for the two \nversions.  5.5 Overhead of Bamboo To characterizetheoverheadofthe Bamboo languageand runtime, we compared \nthe performance of the single-core C version and the single-core Bamboo version for each benchmark. Results \nare listed in Figure 7. Bamboo optionally supports array bounds checks for non-performance critical applications.We \nturned offthe array bounds check option for these benchmarks so as to be comparable with the C versions. \nFor Tracking, KMeans, MonteCarlo, Filter-Bank, Fractal, and Series, we found that the overheads of Bamboo \nare 0.3%, 10.6%, 5.9%, 0.1%, 6.2%, and 6.3%, respectively.  5.6 Discussion We evaluated several aspects \nof the Bamboo implementation syn\u00adthesis tool. We found that the parallel implementations that the Bamboo \ncompiler generated not only achieved signi.cant speedups when compared to the single-core Bamboo/C implementations, \nbut they also generalized to other sized inputs. We found that the scheduling simulator generated accurate \nand useful estimations for the evaluation of candidate implementation strategies, and the directed-simulated \nannealing algorithm greatly helps to ef.ciently generate optimized implementations. Moreover,we found \nthat the implementation synthesis tool gen\u00aderated a sophisticated implementation for the Monte Carlo \nsimula\u00adtion.We were surprisedby the synthesized implementation as we had not realized that the benchmark \ns performance could be im\u00adprovedbyoverlapping the simulation and aggregation components. We foundthatthe \nruntimeoverheadsof Bamboo relativetoC were relatively small. We also found porting the applications to \nBamboo to be straightforward and freed us from manyof the low\u00adlevel concerns of writing parallel code \nin C. The porting process simply involved structuring the program as a set of tasks and writ\u00ading a few \nshort task declarations to describe the dependencies be\u00adtween the tasks.  (a) Tracking (b) KMeans \n(c) MonteCarlo (d) FilterBank (e) Fractal (f) Series 6. RelatedWork Researchers have developed parallelizing \ncompilers, domain spe\u00adci.c languages, explicitly parallel languages [5, 11, 12], work\u00adstealing based \nmulti-threaded systems [18], nested data-parallel languages [8], array-based programming languages [10], \nand other external tools fordevelopingef.cient parallel software.We survey related work in languages \nand automatic parallelization. Akeycomponent of Bamboo is decoupling unrelated concep\u00adtual operations \nand tracking data dependencies between these oper\u00adations. Data.owcomputations alsokeep trackof data dependencies \nbetween operations so that the operations can be parallelized [24]. Bamboo borrows ideas from data.ow \nand integrates them within the context of a standard imperative language to ease adoption by developers. \nBamboo relaxeskeyrestrictionsinthe data.ow model to permit .exible mutation of data structures and construction \nof structurally complex data structures. Furthermore, Bamboo sup\u00adports applications that non-deterministically \naccess data. Course-grained or macro-data.ow languages [14, 22] compose several sequential operations \ntogether to construct larger granular\u00adity code segments for data.ow execution. Relative to these lan\u00adguages, \nBamboo provides a safe mechanism to support arbitrary mutation of structurally complex data structures. \nBamboo also stat\u00adically generates distributed runtime schedulers that are optimized for a program s typical \nruntime behavior. Tuple-space languages, such as Linda [19], decouple computa\u00adtions to enable parallelization. \nThe threads of execution communi\u00adcate through primitives that manipulate a global tuple space. Be\u00adcause \nthese threads can contain state, the compiler cannot automat\u00adically create multiple instantiations to \nutilize additional cores. The orchestration language Orc [13] speci.es how work .ows between tasks. Note \nthatif an operationfails, any work (and any corresponding data) .owing through the task may be lost. \nAnother language, Oz, is a concurrent, functional language that organizes computationsasasetoftasks[30].Tasksare \ncreatedanddestroyed bythe program.Task reducibilityis monotonic onceataskisre\u00adducible it is always reducible. \nAda [1] also has tasks. However,Ada tasks have state and therefore are not straightforward to parallelize. \nBamboo s computational model is similar to actors. Actors communicate through messages [3, 21]. The parallelism \nin an actor-based program is limited by the number of actors the de\u00adveloper created.We note that because \nactors contain state, an indi\u00advidual actor is not straightforward to parallelize. Plaidextends object-oriented \nlanguages with support for typestate\u00adoriented programming [4]. While the formulations and goals of Plaid \ndiffer from Bamboo, both projects leverage the power of making high-level state constraints visible to \nthe compiler infras\u00adtructure. The two approaches differ in how they handle the prob\u00adlem of tracking typestate \nin the presence of aliasing: Plaid uses a type system to ensure reference uniqueness while Bamboo uses \ndata.ow-like dispatch model. Previous languages have use the concept of object states or views to express \ncorrectness constraints on concurrent accesses to objects [15, 16]. Constraints associated with object \nstates can provide a natural expression of read-write locks. Bamboo s use of object states differs from \nthis work in that Bamboo uses abstract object states to coordinate task invocation. Jade provides annotations \nthat developers use to specify how to decompose methods in a serial program into a set of tasks [28]. \nThese annotations describethe data thata Jade task reads from or writes to along with commutativity properties. \nJade uses these spec\u00adi.cations to parallelize applications at runtime. The approaches are complementary \n Bamboo can be used to extract less structured parallelism while Jade can be used to extract more structured \npar\u00adallelism from the imperative code inside of Bamboo tasks. Streaming languages [20, 25] are designed \nto support computa\u00adtions that can be structured as streams. While Bamboo shares sim\u00adilar constructs with \nstream-based languages, Bamboo s task dis\u00adpatch is considerably more expressive and eliminates key weak\u00adnessesof \nstream languages.Forexample,instream languagesitis dif.culttoexpress computationsin which several different \npartsof the computation access a shared data structure in an irregular pat\u00adtern. Bamboo s task dispatch \nsupports irregular dispatch patterns on shared objects the developer simply creates an instance of the \nshared object type and then uses the task speci.cations to spec\u00adify the shared object. Bamboo also adds \nsupports for sharing struc\u00adturally complex data structures and mutating shared objects. Spiral uses search \nto optimize DSP algorithms [27]. While both approaches use search, Bamboo targets general computation \nrather than Spiral s more speci.c focus on linear DSP algorithms. Bamboo borrows constructs from the \nBristlecone language for creatingrobustsoftware systems [17]. Our previouswork only sup\u00adports single-threaded \nexecution and contains language constructs speci.c to automated recovery. Bamboo shares language ideas \nwith Bristlecone,butextends these ideasto support parallelexecution. CellSs dynamically schedules function \ninvocation when a func\u00adtion s operands are available [7]. Bamboo differs in that it supports linked data \nstructures, which are not supportedby CellSs. Bamboo also differs in that we use static analysis to eliminate \nscheduling overheadsandto parallelizethe runtime scheduler.Weexpect that our distributed schedulers would \nscale to much larger processors than CellSs s centralized runtime scheduler. Manyofthe recenteffortson \nsoftware synthesisfor parallel ma\u00adchines have focused on fully automatic approaches to paralleliza\u00adtion \n(e.g., [6]). When this approach is effective, it is ideal because it maximizes programmer productivity. \nBamboo is largely comple\u00admentary to this work. It is conceptually straightforward to leverage parallelizing \ncompilers to extract .ne-grained parallelism by auto\u00admatically parallelizing individual Bamboo tasks. \n7. Conclusion We have successfully implemented several parallel applications in Bamboo. Bamboo applications \nconsist of a set of interacting tasks with each task implementing one of the conceptual operations in \nthe application. The developer speci.es how these tasks interact using task declarations. Bamboo extracts \ndata dependence infor\u00admation from the declarations and combines this information with pro.le data to \nautomatically synthesize parallel implementations that are optimized for a target many-core processor. \nBamboo gen\u00aderated implementations of our benchmark applications that scaled successfully to 62 cores. \nThe implementations generalized to other sized inputs. Moreover, Bamboo generated sophisticated imple\u00admentation \nthat used pipelining to overlap the computation and data aggregation phases of the benchmark applications. \nOur current implement performs optimization at compile time. However, the basic technique is more generally \napplicable. It is straightforward to modify the basic approach to support executa\u00adbles that periodically \nre-optimize themselves for the workloads theyencounter in the .eld or for new processor layouts. The \nbasic idea is to separate layout information from code in the application executable. An executable would \nperiodically pro.le itself and re\u00adportthe resultstoasystemlibrarythat implementsour optimization strategy. \nThe librarywould then rerun the optimizations, generatea new layout, and update the executable s layout \ninformation. Acknowledgments This research was supportedby the National ScienceFoundation under grants \nCCF-0846195 and CCF-0725350.Wewould like to thank the anonymous reviewers for their helpful comments. \n References [1] Ada Reference Manual. http://www.adaic.org/ standards/05rm/html/RM-TTL.html, 2005. [2] \nTilera. http://www.tilera.com/. [3]G.Agha,I.A. Mason,S.F. Smith,andC.L.Talcott.Afoundationfor actor computation. \nJournal of Functional Programming, 7(1):1 72, 1997. [4] J. Aldrich, J. Sunshine, D. Saini, and Z. Sparks. \nState-oriented pro\u00adgramming. In Proceedings of Onward!, 2009. [5] E. Allen, D. Chase, J. Hallett,V. Luchangco, \nJ.-W. Messen, S. Ryu, G.L. Steele, andS.Tobin-Hochstadt. TheFortress Language Speci.\u00adcation. Sun Microsystems, \nInc., September 2006. [6]S.P. Amarasinghe,J.-A.M. Anderson,M.S.Lam,andA.W.Lim.An overview of a compiler \nfor scalable parallel machines. In Proceedings of the 6th InternationalWorkshop on Languages and Compilers \nfor Parallel Computing, 1994. [7] P. Bellens, J. M. Perez, R. M. Badia, and J. Labarta. CellSs: A programming \nmodel for the Cell BE architecture. In Proceedings of theACM/IEEESC 2006 Conference on Supercomputing, \n2006. [8] G. E. Blelloch, S. Chatterjee, J. C. Hardwick, J. Sipelstein, and M. Za\u00adgha. Implementationofaportable \nnested data-parallel language. Jour\u00adnalofParallel and Distributed Computing, 21(1):4 14, Apr. 1994. [9] \nC. Cao Minh, J. Chung, C. Kozyrakis, and K. Olukotun. STAMP: Stanford transactional applications for \nmulti-processing. In Proceed\u00adingsofthe IEEE International SymposiumonWorkload Characteriza\u00adtion, September \n2008. [10] B. L. Chamberlain, S.-E. Choi, E. C. Lewis, C. Lin, L. Snyder, and W. D.Weathersby. The case \nfor high level parallel programming in ZPL. IEEE Computational Science and Engineering, pages 76 86, \nJuly September 1998. [11] B.L. Chamberlain,D. Callahan, andH.P. Zima.Parallel programma\u00adbility and the \nChapel language. InternationalJournalof HighPerfor\u00admance Computing Applications, 2007. [12] P. Charles, \nC. Grothoff, V. Saraswat, C. Donawa, A. Kielstra, K. Ebcioglu, C. von Praun, and V. Sarkar. X10: an object-oriented \napproach to non-uniform cluster computing. In Proceedings of the ACM SIGPLAN Conference on Object Oriented \nProgramming, Sys\u00adtems, Languages, and Applications, 2005. [13]W.R. Cook,S.Patwardhan,andJ. Misra.Work.ow \npatternsin Orc. In Proceedings of the 2006 International Conference on Coordination Models and Languages, \n2006. [14] K. Dai. Code parallelization for the LGDG large-grain data.ow computation. In CONPAR 90/VAPP \nIV: Proceedings of the Joint International Conference on Vector and Parallel Processing, pages 243 252, \nLondon, UK, 1990. Springer-Verlag. [15] F. Damiani, E. Giachino, P. Giannini, N. Cameron, and S. Drossopoulou. \nA state abstraction for coordination in Java\u00adlike languages. In Electronic Proceedings of the 2006Workshop \non FormalTechniques forJava-likePrograms, 2006. [16] B. DemskyandP. Lam.Views: Object-inspired concurrencycontrol. \nIn Proceedings of the 2010 International Conference on Software Engineering, 2010. [17] B. Demsky and \nS. Sundaramurthy. Bristlecone: Language support for robust software applications. IEEE Transactions on \nSoftware Engineering. [18] M. Frigo, C. Leiserson, and K. Randall. The implementation of the Cilk-5 multithreaded \nlanguage. In International Conference on Programming Language Design and Implementation, 1998. [19] D. \nGelernter. Generative communication in Linda. ACMTransactions on Programming Languages and Systems, 7(1):80 \n112, 1985. [20] M. Gordon,W. Thies, M. Karczmarek, J. Lin, A. S. Meli, C. Leger, A.A.Lamb,J.Wong,H.Hoffman,D.Z.Maze,andS. \nAmarasinghe. Astream compiler for communication-exposed architectures. InInter\u00adnational Conference on \nArchitectural Support for Programming Lan\u00adguages and Operating Systems, October, 2002. [21] C. Hewitt \nand H. G. Baker. Actors and continuous functionals. Tech\u00adnical report, Massachusetts InstituteofTechnology, \n1978. [22] C. Huang andL.V. Kale. Charisma: orchestrating migratable parallel objects. In Proceedingsofthe \n2007ACM International Symposiumon HighPerformance Distributed Computing, pages 75 84, 2007. [23] J. C. \nJenista and B. Demsky. Disjointness analysis for Java-like languages. Technical Report UCI-ISR-09-1, \n2009. [24]W.M. Johnston,J.R.P. Hanna,andR.J. Millar.Advancesindata.ow programming languages. ACM Computing \nSurveys, 36(1), 2004. [25] M. Kudlur and S. Mahlke. Orchestrating the execution of stream programs on \nmulticore platforms. In Proceedings of the Conference on Programming Language Design and Implementation, \n2008. [26] H. J. Larson and B. O. Shubert. Probabilistic Models in Engineering Sciences. Wiley, 1979. \n[27] M. P\u00a8uschel, J. M. F. Moura, J. Johnson, D. Padua, M. Veloso, B. Singer, J. Xiong,F. Franchetti, \nA. Gacic,Y.Voronenko, K. Chen, R.W. Johnson, and N. Rizzolo. SPIRAL: Code generation for DSP transforms. \nProceedings of the IEEE, special issue on Program Gen\u00aderation, Optimization, and Adaptation , 93(2):232 \n275, 2005. [28] M. C. Rinard. The Design, Implementation and Evaluation ofJade, aPortable, ImplicitlyParallel \nProgramming Language. PhD thesis, Stanford University, September 1994. [29] L. A. Smith, J. M. Bull, \nand J. Obdrzalek. A parallel Java Grande benchmark suite. In Proceedings of SC2001, 2001. [30] G. Smolka. \nThe Oz programming model. In Proceedings of the European Workshop on Logics in Arti.cial Intelligence, \npage 251, London, UK, 1996. Springer-Verlag. [31] R. E. Strom and S. Yemini. Typestate: A programming \nlanguage concept for enhancing software reliability. IEEE Transactions on Software Engineering, January \n1986. [32] S. K.Venkata, I. Ahn, D. Jeon, A. Gupta, C. Louie, S. Garcia, S. Be-longie,andM.B.Taylor. \nSD-VBS:TheSanDiegoVision Benchmark Suite. In Proceedingsof the IEEE International Symposium onWork\u00adload \nCharacterization, October 2009.    \n\t\t\t", "proc_id": "1806596", "abstract": "<p>Traditional data-oriented programming languages such as dataflow languages and stream languages provide a natural abstraction for parallel programming. In these languages, a developer focuses on the flow of data through the computation and these systems free the developer from the complexities of low-level, thread-oriented concurrency primitives. This simplification comes at a cost --- traditional data-oriented approaches restrict the mutation of state and, in practice, the types of data structures a program can effectively use. Bamboo borrows from work in typestate and software transactions to relax the traditional restrictions of data-oriented programming models to support mutation of arbitrary data structures.</p> <p>We have implemented a compiler for Bamboo which generates code for the TILEPro64 many-core processor. We have evaluated this implementation on six benchmarks: Tracking, a feature tracking algorithm from computer vision; KMeans, a K-means clustering algorithm; MonteCarlo, a Monte Carlo simulation; FilterBank, a multi-channel filter bank; Fractal, a Mandelbrot set computation; and Series, a Fourier series computation. We found that our compiler generated implementations that obtained speedups ranging from 26.2x to 61.6x when executed on 62 cores.</p>", "authors": [{"name": "Jin Zhou", "author_profile_id": "81537293756", "affiliation": "University of California, Irvine, Irvine, CA, USA", "person_id": "P2184599", "email_address": "", "orcid_id": ""}, {"name": "Brian Demsky", "author_profile_id": "81100338144", "affiliation": "University of California, Irvine, Irvine, CA, USA", "person_id": "P2184600", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1806596.1806640", "year": "2010", "article_id": "1806640", "conference": "PLDI", "title": "Bamboo: a data-centric, object-oriented approach to many-core software", "url": "http://dl.acm.org/citation.cfm?id=1806640"}