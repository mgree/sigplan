{"article_publication_date": "06-05-2010", "fulltext": "\n Inferable Object-Oriented Typed Assembly Language Ross Tate Juan Chen Chris Hawblitzel University of \nCalifornia, San Diego Microsoft Research Microsoft Research rtate@cs.ucsd.edu juanchen@microsoft.com \nchrishaw@microsoft.com Abstract A certifying compiler preserves type information through compi\u00adlation \nto assembly language programs, producing typed assembly language (TAL) programs that can be veri.ed for \nsafety indepen\u00addently so that the compiler does not need to be trusted. There are two challenges for \nadopting certifying compilation in prac\u00adtice. First, requiring every compiler transformation and optimiza\u00adtion \nto preserve types is a large burden on compilers, especially when adopting certifying compilation into \nexisting optimizing non-certifying compilers. Second, type annotations signi.cantly increase the size \nof assembly language programs. This paper proposes an alternative to traditional certifying com\u00adpilers. \nIt presents iTalX, the .rst inferable TAL type system that supports existential types, arrays, interfaces, \nand stacks. We have proved our inference algorithm is complete, meaning if an assem\u00adbly language program \nis typeable with iTalX then our algorithm will infer an iTalX typing for that program. Furthermore, our \nalgo\u00adrithmis guaranteedto terminateevenifthe assemblylanguage pro\u00adgram is untypeable.We demonstrate that \nit is practical to infer such anexpressiveTALby showinga prototype implementationof type inference for \ncode compiled by Bartok, an optimizing C# compiler. Our prototype implementation infers complete type \nannotations for 98% of functions in a suite of realistic C# benchmarks. The type\u00adinference time is about \n8% of the compilation time. We needed to change only 2.5% of the compiler code, mostly adding new code \nfor de.ning types and for writing types to object .les. Most trans\u00adformations are untouched.Type-annotation \nsize is only 17% of the size of pure code and data, reducing type annotations in our pre\u00advious certifying \ncompiler [4] by 60%. The compiler needs to pre\u00adserve only essential type information such as method signatures, \nobject-layout information, and types for static data and external la\u00adbels. Even non-certifying compilers \nhave most of this information available. Categories and Subject Descriptors D.3.1 [Programming Lan\u00adguages]: \nFormal De.nitions and Theory; D.3.4 [Programming Languages]: Processors Compilers General Terms Languages, \nTheory,Veri.cation Keywords Type inference, Typed assembly language (TAL), Object-oriented compiler, \nExistential quanti.cation, Certifying compiler Permission to make digital or hard copies of all or part \nof this work for personal or classroom use is granted without fee provided that copies are not made or \ndistributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page.To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. PLDI 10, June 5 10, 2010,Toronto, Ontario, Canada. Copyright \nc &#38;#169; 2010 ACM 978-1-4503-0019-3/10/06... $10.00 1. Introduction Internet users regularly download \nand execute safe, untrusted code, in the form of Java applets, JavaScript code, Flash scripts, and Silverlight \nprograms. In the past, browsers have interpreted much of this code, but the desire for better performance \nhas recently made just-in-time compilation more common, even for scripting languages. However, compilation \nposes a security risk: users must trust the compiler, since a buggy compiler could translate safe source \ncode into unsafe assembly language code. Therefore, Nec\u00adula and Lee [17] and Morrisett et al. [15] introduced \nproof-carrying code and typed assembly language (TAL). These technologies an\u00adnotate assembly language \nwith proofs or types that demonstrate the safety of the assembly language, so that the user trusts a \nsmall proof veri.er or type checker, rather than trusting an entire compiler. Before a veri.er can check \nthe annotated assembly code, some\u00adone must produce the annotations. Morrisett et al. [15] proposed that \na compiler generate these annotations: the compiler preserves enough typing information at each stage \nof the compilation to gen\u00aderate types or proofs in the .nal compiler output. The types may evolve from \nstage to stage; for example, local variable types may change to virtual register types, and then to physical \nregister types and activation record types [15] or stack types [16]. Nevertheless, each stage derives \nits types from the previous stage s types, and all compiler stages must participate in producing types. \nFurthermore, typical typed intermediate languages include pseudo-instructions, such as pack and unpack, \nthat coerce one type to another. The com\u00adpiler stages must also preserve these pseudo-instructions. Implementing \ntype preservation in a compiler may require sub\u00adstantial effort. In our previous work, we modi.ed about \n19,000 linesofa 200,000-line compilerto implementtype preservation[4]. Even a 10% modi.cation may pose \nan obstacle to developers trying to retro.t large legacycompilers with type preservation, especially \nwhen these modi.cations require developers to interact with the complex type systems that are typical \nin typed compiler interme\u00addiate languages [4, 10, 14 16]. As a result, programmers currently face a trade-off: \nuse a popular existing compiler that does not cer\u00adtify the safety of its output, or use one of the few \navailable exper\u00adimental type-preserving compilers, which are less optimizing, less documented, and less \nsupported. This paper proposes another approach to building certifying compilers a type inference system \ncalled iTalX. Instead of hav\u00ading each stage explicitly track the type of each variable/register, our \ntype inference algorithm will infer the types of the assembly lan\u00adguage after all stages .nish. This \neases the implementation of both new certifying compilers and those retro.tted from legacycompil\u00aders, \nand vastly reduces the number of type annotations that com\u00adpilers pass from stage to stage. iTalX requires \nonly two kinds of annotation for instructions: the types of null-pointer literals and the length of jump \ntables. Null literals and jump tables appear only oc\u00adcasionally in code anyway. All other required type \nannotations are coarser-grained metadata: function signatures, object-layout infor\u00admation, and types \nof static data and external labels. Furthermore, there are no special pseudo-instructions, such as pack \nand unpack.  This paper makes the following contributions. First, we de.ne a practical type system, \niTalX, for assembly language, supporting classes, arrays, interfaces, casts, stacks, structs, by-reference \nargu\u00adments, and function pointers. We believe this to be the most ex\u00adtensive type system for object-oriented \nassembly language code at present. Second, we prove that inference for iTalX is decidable and complete: \nif a fully annotated program is well-typed, we can erase the type annotations on basic blocks, and the \ninference algorithm can still infer valid types for the basic blocks. It is very dif.cult to infer a \ntype system with general existential quanti.cation. First-class quanti.cation easily leads to undecidable \ntype inference [22, 23]. To make type inference decidable, we re\u00adstrict quanti.cationto classvariablesandintegervariables.Never\u00adtheless, \nour type system still supports expressive subclassing and integer constraints, making it suitable for \ntyping realistic compiled object-oriented code. Third, we implement type inference for the x86 assembly \nlan\u00adguage, and show that our implementation can completely infer types for 98% of functions compiled \nfrom real, large C# bench\u00admarks by the Bartok optimizing compiler. As far as we know, no other systems \nare able to infer types for real-world x86 benchmarks at a similar scale. Furthermore, omitting basic-block \ntypes reduces the size of type annotations in the generated TAL .les by 60%. Type inference takes about \n8% of the compilation time. 2. Language iTal We .rst present iTal, a small inferable typed assembly language \nwhich is a stripped down version of iTalX. Although iTal is too small to be directly usable on real-world \ncode, it illustrates the main features of our inference system, such as the treatment of typevariables, \nsubtyping, and joins. Section3 showshow the full\u00ad.edged type system iTalX supports real-world features. \nFor manyclass-based, object-oriented languages without quan\u00adti.ed types, type inference is straightforward. \nFor example, Java bytecode omits type annotations from local variables, and uses a forward data.ow analysis \nto infer these types [12]. The analysis must be able to join types that .ow into merge points in the \nbyte\u00adcode: if one branch assigns a value of type t1 to variable x, and another branch assigns a value \nof type t2 to variable x, then where the two paths merge, x will have type t1 .t2, where t1 .t2 is the \nleast common supertype of t1 and t2, known as the join (which, even in Java s simple type system, is \nsubtle to de.ne properly [6]). Like Java bytecode, our inference algorithm uses a forward data.ow analysis,but \nunlike Java bytecode, it supports quanti.ca\u00adtion over type variables. Such type variables allow us to \ncheck indi\u00advidual assembly language instructions for method invocation, array accesses, casts, and other \noperations (in contrast to Java bytecode, which treats each of these operations as single, high-level \nbytecode instructions). Consider a class Point with three .elds and a virtual method Color that takes \nan instance of class RGB and returns void. The following code invokes incorrectly the Color method on \np1: class Point { int x; int y; RGB c; virtual void Color(RGB c); } void Unsafe(Point p1, Point p2) \n{ vt = p1.vtable; // fetch p1 s vtable m = vt.Color; // fetch p1 s Color method m(p2, p1.c); // call \nwith p2 as \"this\" } class Point3D : Point { int z; } This code above unsafely passes the wrong this \npointer to p1 s Color method: if p1 is an instance of the subclass Point3D, Color may refer to .elds \nde.ned in Point3D, which p2 does not neces\u00adsarily contain, since p2 may not be an instance of Point3D. \nMost type systems for object-oriented typed assembly languages use type variables to distinguish between \nsafe method invocations and un\u00adsafe code [3, 4, 10]. LILC [3], for example, describes p1 s type and p2 \ns type by existentially quantifying over a class variable a: both p1 and p2 have type .a\u00abPoint.a, which \nsays that p1 and p2 are each instances of some class that is a subclass of Point. The vir\u00adtual methods \nof the existentially quanti.ed dynamic class a of p1 require the this pointer to be an instance of a. \nThe type system conservatively assumes that the a for p1 may differ from the a for p2, ensuring that \nonly p1 can be passed to p1 s methods, and only p2 can be passed to p2 s methods. The rest of this section \nbuilds ideas from LILC into a small class-based object-oriented typed assembly languageiTal(inferable \nTal), which supports type inference with existential types: type in\u00adference can infer all the types inside \neach function of any typeable iTal program without needing any annotations within the function bodies. \nIt requires no type annotations on basic blocks, no annota\u00adtions on instructions, and no type-coercion \ninstructions. We have proved that iTal is sound and that type inference is decidable and complete. The \nproofs and complete formalization of the semantics of a slight variant of iTal and its join algorithm \ncan be found in our technical report [21]. The purpose of iTal is to shed some light on the more realis\u00adtic \niTalX described in the next section. Both systems use the same mechanisms for subtyping, joining, and \ninferring existential types, and both systems follow similar restrictions to keep inference de\u00adcidable. \nTo make the key ideas stand out, iTal focuses on only core object-oriented features such as classes, \nsingle inheritance, object layout, .eld fetch, and virtual-method invocation. iTalX, however, applies \nto more expressive languages with arrays, casts, interfaces, stacks, by-reference parameters, and structs. \nThe .rst three features require more signi.cant changes to the type system. Others are mostly straightforward. \nThe extensions are discussed in Section 3. 2.1 Syntax iTal borrows ideas from LILC, a low-level object-oriented \ntyped intermediate language [3]. LILC preserves classes, objects, and subclassing, instead of compiling \nthem away as in most previous object encodings. iTal is even lower level than LILC in that iTal is at \nthe assembly language level. iTal uses the type Ins(C) to represent only instances of the exact class \nC, not C s subclasses, unlike most source languages. An existential type .a\u00abC. Ins(a)represents instances \nof C and C s subclasses where class variable aindicates the dynamic classes of the instances. The subclassing \nbound C on a means that the dynamic class a is a subclass of the static class C. The source language \ntype C is translated to the above existential type. An instance of a subclass of C can be packed to have \ntype .a\u00abC. Ins(a). A value with type .a\u00abC. Ins(a) can be unpacked to have type Ins(\u00df), where \u00df is a fresh \nclass variable (distinct from any existing class variables) indicating the dynamic class of the instance, \nand the constraint \u00df \u00ab C records thefact that the instance s dynamic class inherits C. The separation \nbetween static and dynamic classes guarantees the soundness of dynamic dispatch (Section 2.3 explains \na virtual\u00admethod invocation example). A type system without such separa\u00adtion cannot detect the error \nin the previous unsafe example. Class variables in iTal have subclassing bounds and are instan\u00adtiated \nwith only class names. The bounds cannot be arbitrary types. This simpli.es both type inference and type \nchecking in iTal.  The syntax of iTal is shown below. class type . ::= a |C regtype tReg ::= Int |Code(F. \nF ' )|Ins(.)|Vtable(.) term type t ::= tReg |.a\u00abC. Ins(a) value v ::= n |l operand o ::= v |r |[r + n] \ninstr . ::= bop r,o |mov r,o |mov [r1 +n],r2 |call o binary op bop ::= add |sub |mul |div instrs .s ::= \njmp l |bz o,lt,lf |ret |.;.s func prec F ::= {ri : ti}ni=1 - -. state type ...G speci.es a constraint \nenvironment . and a regis\u00adter bank type G. iTal automatically unpacks a register when it is assigned \na value with an existential type .a\u00abC. Ins(a): the exis\u00adtentially quanti.ed class variable is lifted \nto the constraint environ\u00adment of the state type corresponding to the current machine state, and the \nregister is given an instance type. In a state type ...G, . records the type variables for the unpacked \nregisters so far, and G never maps a register to an existential type .a\u00abC. Ins(a). This convention eliminates \nthe explicit unpack and makes type inference and type checking easier. Rules corresponding to the tra\u00additional \npack operation will be explained later in the section. function f ::= (F. F ' ){ l : .s} A class type \n. is either a class variable (ranged over by a, \u00df, and .)or a class name (ranged over by B and C). A \nspecial class named Object is a superclass of anyother class type. iTal supports the primitive type Int \nand the code-pointer type Code(F . F ' ) where F . F ' describes function signatures with precondition \nF and postcondition F ' . The other types are object-oriented: the type Ins(.)describes instances of \nexact ., Subclassing. iTal preserves source-level subclassing between class names. Judgment T;. . .1 \n\u00ab .2 means that under the class declarations T and the constraint environment ., class type .1 is a subclass \nof .2. A class C is a subclass of B if C declares soin its declaration (rule sc-class).A classvariable \na is a subclass of a class name C if C is a s bound (rule sc-var). Additionally, subclassing is re.exive \nand transitive. and the type Vtable(.)represents the virtual-method table of class .. All these types \ncan be used to type registers in basic-block preconditions, and are called register types. Type .a\u00abC. \nIns(a) represents objects of C and C s subclasses. The existential type can be used to type .elds in \nobjects and registers in function signatures, but not registers in basic-block preconditions. Both register \ntypes and the above existential types are called term types. Values in iTal include integers n and heap \nlabels l. Operands include values, registers r, and memory words [r +n](the value at memory address r \n+ n). All values are word-sized. Instructions in iTal are standard. Instruction bop r,o computes r bop \no and assigns the result to r. Instruction mov r,o moves the value of o to register r. Instruction mov \n[r1 + n],r2 stores the value in r2 into the memory word at address r1 + n. Instruction call o calls a \nfunction o. Instruction sequences .s consist of a sequence of instructions ended with a control-transfer \ninstruction. Instruction jmp ljumps to a block labeled l. Instruction bz o,lt,lf branches to lt if o \nequals 0and to lf otherwise. Instruction ret returns to the caller. T(C)= C : B{...} a \u00ab C . . sc-class \nsc-var T;. . C \u00ab B T;. . a \u00ab C Subtyping between State Types. Subtyping between two state typesisusedto \ncheck controltransfer.Itisthekeytotype inference in iTal, allowing subtyping between two state types \nwithout .rst unpacking one type and then packing to the second type. No type coercion is necessary. The \njudgment T . S1 = S2 means that under class declarations T, state type S1 is a subtype of S2 . . : dom(. \n' ). (dom(.).dom(T)) .r . dom(G ' ).G(r)=G ' (r)[.] .a \u00ab C . . ' . T;. . .(a)\u00ab C st-sub T . (...G) = \n(.. ' .G ' ) A state type ...G is a subtype of .. ' .G ' if a substitution . maps each class variable \nin . ' to either a class variable in . or a constant class name in T, such that G ' (r)after substitution \nis the same as G(r)for all registers r in dom(G ' ). The substitution must preserve subclassing in . \n' : for each constraint a \u00ab C in . ' , .(a) - -. ){l : .s}speci.es its signature F . F ' must be a subclass \nof C under .. The substitution is computed A function (F . F ' and a sequence of basic blocks, each of \nwhich has a label l and a during type inference and made ready to use by the type checker. . body .s. \nThe notation -a means a sequence of items in a. We can derive the following from st-sub, one case of \nthe tra\u00additional pack rule for existential types, using a substitution that 2.2 Subclassing and Subtyping \n maps a to C: We describe selected static semantics of iTal. The static semantics T ....(G,r : Ins(C))=.(.,a \n\u00ab C).(G,r : Ins(a)) uses the following environments: . ----- Subtyping between state types is re.exive \nand transitive, as implied . C : B{-t class decls T ::= |T, cdecl substitution.Transitivity canbe provedby \ncomposing substitutions. constr env . ::= |.,a \u00ab C No Subtyping between Term Types. Although iTal includes \nsub\u00ad regbank type G ::= |G,r : tReg typing between state types S, it omits subtyping between term state \ntype S ::= ...G types t, instead using a weaker notion of assignability. Omitting the F . F ' } class \ndecl cdecl ::= , by the st-sub rule. Re.exivity can be proved by using the identity . ----- ' .. Class \ndeclaration C : B{-t superclass - --- , subtyping relation t = t avoids issues of covariant and contravari\u00ad \n . F . F ' }introduces a class C with B, .elds with types -t , and methods with signatures ant subtyping \nwithin code-pointer types, which makes it easier to join types. Our larger language iTalX allows subtyping, \nrestricting function arguments to be contravariant to guarantee soundness and nearly invariant to guarantee \ndecidability of inference. F . F ' . It speci.es all .elds and methods of C, including those from superclasses. \nMethod bodies are translated to functions in the heap. Therefore, only method signatures are included \nin class dec\u00ad larations, not method bodies. Tis a sequence of class declarations, which the compiler \npreserves to iTal. The constraint environment . is a sequence of type variables and their bounds. Each \ntype variable has a superclass bound. The register bank type Gis a partial map from registers to register \ntypes. iTal uses state types S, another form of existential types, to rep\u00adresent machine states, including \npreconditions of basic blocks. A Assignability. Assignability decides if the value in a register can \nbe assigned to a memory location or a formal of a method, both of which can have existential types. Assignability \nallows a value of type t to be assigned to a location of type t. More importantly, it handles packing \nsubclass instances to superclass instances (with existential types) by allowing a value of type Ins(.)to \nbe assigned to a location of type .a\u00ab. ' . Ins(a)whenever . \u00ab . ' can be inferred from the constraint \nenvironment. iTal uses assignability to avoid confusion with subtyping between state types.  2.3 Type \nInference and Type Checking Type inference computes the precondition for each basic block in a function \nfrom the function signature. The precondition of the entry block is the function signature with all registers \nunpacked. Type inference then uses a forward data.ow analysis, starting from the entry block. For each \nbasic block, if type inference .nds a precondition, it then type checks the instruction sequence in the \nblock, untilit reaches the control-transfer instruction.Ifthe control\u00adtransfer instruction is ret , the \nblock is done. Otherwise ( jmp or bz ), type inference propagates the current state type to the target(s). \nIf a target has no precondition, the current state type will be the new precondition for the target. \nOtherwise, type inference computes the join of the current state type and the precondition of the target, \nand uses the result as the new precondition for the target. If the precondition of the target changes, \ntype inference goes through the target again to propagate the changes further. Type inference continues \nuntil it .nds a .xed point. When joining two state types, the result is a supertype of both state types. \nThe type system does not have in.nite supertype chains for any given state type, which guarantees termination \nof type inference. We use the following code segment to explain type inference and type checking. The \nexample is contrived to show various as\u00adpects of type inference. Most compilers would generate better \nop\u00adtimized assembly code. The function Foo takes an instance of the previous class Point (in r1), an \ninstance of Point3D (in r2), and an integer (in r3). Block L0 branches to L1 if r3 equals 0 and to L2 \notherwise. L1 and L2 merge at L3, which calls the Color method (at offset 4 of the vtable) on either \nthe instance of Point or the instance of Point3D, depending on the value of the integer. Instructions \n3), 4), and 7) are added for the purpose of showing joining of types. //void Foo(r1: Point, r2: Point3D, \nr3: int) L0: 1) bz r3, L1, L2 // branch on r3 equals 0 L1: 2) mov r4, r1 // true branch 3) mov r5, r1 \n 4) mov r6, r2 5) jmp L3 L2: 6) mov r4, r2 // false branch 7) mov r5, r2 8) jmp L3 L3: 9) mov r6, \n[r4+12] // get the RGB field 10) mov r7, [r4+0] // get vtable from r4 11) mov r7, [r7+4] // get the \nColor method 12) call r7 // call the Color method The signatureofFoois representediniTalas FFoo .{}, \nwhere FFoo is {r1: .a\u00abPoint. Ins(a),r2: .a\u00abPoint3D. Ins(a),r3: Int}. The precondition of block L0, S0, \nis then .a1 \u00ab Point, a2 \u00ab Point3D.{r1: Ins(a1),r2: Ins(a2),r3: Int}, by un\u00adpacking r1 and r2 in FFoo \nand lifting the two fresh (and distinct) class variables a1 and a2 to the constraint environment. Block \nL0 has precondition S0. It has only one control-transfer instruction 1). Type inference checks that r3 \nhas type Int and propagates the state type S0 to L1 and L2 since instruction 1) does not change the machine \nstate. Block L1 now has precondition S0. Checking instruction 2) adds the mapping r4: Ins(a1)to the current \nstate type because r4 now contains a value of type Ins(a1), and checking 3) and 4) is similar. Now we \nreach instruction 5) with state type S ' 1 = .a1 \u00ab Point,a2 \u00ab Point3D.{r1: Ins(a1),r2: Ins(a2), r3: Int,r4: \nIns(a1),r5: Ins(a1),r6: Ins(a2 )}, which becomes the precondition of the successor L3. Similarly, checking \nblock L2 produces post condition S ' 2 = .a1 \u00ab Point,a2 \u00ab Point3D.{r1: Ins(a1),r2: Ins(a2), r3: Int,r4: \nIns(a2),r5: Ins(a2)}. The successor L3 has its precondition already, so we need to compute the join of \nand S ' 1 S ' 2. The result will be the new precondition of L3. Join. Computing the join (least upper \nbound) of two state types is the most important task during type inference.We use S1 .S2 to represent \nthe join of S1 and S2, and a to represent variables created during the joining process (called generalization \nvariables). Generalization variables are not different from other class vari\u00adables.We use the special \nnotation to ease the presentation. The join operation is performed in two steps. The .rst step generalizes \nthe two register bank types in S1 and S2 to a common register bank supertype.To generalize G1 and G2, \nfor each register r that appears in both G1 and G2, it generalizes G1 (r)and G2(r) to a common supertype \nand maps r to the supertype in the result register bank type. The generalization omits registers that \nappear in G1 or G2 but not both. Generalization recursively goes through the structure of types. Generalizing \nInt and Int returns Int. Generalizing Ins(C) and Ins(C)returns Ins(C). Otherwise, generalizing Ins(.1)and \nIns(.2 ) returns Ins( a), where a is a fresh class variable (generalization variable). Generalization \nalso records two mappings a . .1 and a . .2 to track where the new variable is from. The mappings will \nbe used to construct substitutions for the subtyping rule st-sub. The bound of a will be computed in \nthe second step of join. Generalizing .a\u00ab.1. Ins(a) and .\u00df\u00ab.2. Ins(\u00df) returns ..\u00ab a . and a . Generalizing \na. Ins(.), where .1 .2. Vtable(.1)and Vtable(.2)returns Vtable( a)with similar maps. Code-pointer types \nCode(F1 .F ' 1 )and Code(F2 .F ' 2)are generalized to Code(F.F ' )if F1 and F2 are generalized to F and \nF ' 1 and F2 ' to F ' , provided no registers are dropped in either case. Our treatment of arguments \nis sound because iTal does not have subtyping on term types, as explained earlier. Two types with different \nstructures, such as a code-pointer type and a vtable type, cannot be generalized. If a register has differently \nstructured types in the two register bank types to join, the join result will not contain the register. \nFor our example, r4 has type Ins(a1)in S ' 1 and type Ins(a2) in S ' 2. Generalization creates a fresh \nvariable a and two mappings a . a1 and a . a2 for S ' 1 and S2 ' respectively.For r5, it creates another \nfresh variable \u00df , different from a , and mappings \u00df . a1 and \u00df . a2. Generalization also creates new \ngeneralization variables for r1and r2,but for simplicity of presentation, wekeep the types of r1and r2unchanged \nafter generalization, since S ' 1 and S ' 2 agree on their types and a1 (a2)means the same dynamic type \nof r1 (r2)in both S ' 1 and S2' . The result of generalization is then {r1: Ins(a1),r2: Ins(a2), r3: \nInt,r4: Ins( a),r5: Ins(\u00df )}. Register r6is omitted because it does not exist in S ' 2 . The second step \nof the join operation is factorization. The goal is to compute the constraint environment of the result \nstate type, using the least number of generalization variables. This step is done by unifying equivalent \ngeneralization variables. We de.ne an equivalence relation on generalization variables: two variables \nare equivalent iff they are results of generalizing the same two types. For example, in our example, \na and \u00df are equivalent because both come from generalizing a1 and a2. Factorization then creates the \n.nal join result ...G, where . collects arbitrarily chosen representatives of equivalence classes for \ngeneralization variables, and G is the register bank type resulting from generalization after substituting \neach generalization variable with the representative of its equivalence class. The bound of a representative \na in . is the least common superclass name for .1 and .2, where a comes from generalizing .1 and .2. \n For our example, the join of S1 ' and S2 ' is S3 = .a1 \u00ab Point,a2 \u00ab Point3D,a \u00ab Point.{r1: Ins(a1),r2: \nIns(a2),r3: Int,r4: Ins( a),r5: Ins( a)}. a is chosen for the equivalence class { \u00df}and given bound Point \nbecause Point is the least common a, superclass name for Point(a1 s bound) and Point3D(a2 s bound). S3 \nis a supertype of S ' 1 and S2' , using the st-sub rule. The joining process generates two maps, a . \na1 and a . a2. Using the former map as the substitution in st-sub, we get S ' 1 = S3. The latter map \nevidences that S ' 2 = S3 holds.We have proved that the join process always computes the least upper \nbound [21]. Continuing our example, we now check block L3 with the new precondition S3 . Instruction \n9) loads into r6 a .eld of r4 (an instance of a ), using the object-layout information provided by the \ncompiler. This is one of the few places where type inference needs hints (metadata here) generated by \nthe compiler. The layout information maps a .eld offset to the .eld type. The class variable a is nota \nconcrete class, and thusits layoutis statically unknown. From thefact that a is a subclass of Point, \nthe checker knows that an instance of a has at least those .elds declared in Point, and at the same offsets \nas in Point. It looks up the offset 12 (for the .eld c) in the layout information of Point and .nds an \nexistential type .. ' \u00abRGB. Ins(. ' ). When a register is assigned a .eld with an existential type, it \nis unpacked automatically. r6 is given type Ins(.), where . is fresh and . \u00ab RGB is added to the constraints. \nInstruction 10) loads into r7 the .rst word the vtable of r4. The type system gives type Vtable( a) to \nr7, indicating that r7 points to the vtable of r4 s dynamic type. Instruction 11) loads into r7 the method \npointer for Color. the checker uses the layout information of Point to .nd the code\u00adpointer type Code({r4: \nIns( a),r6: ..\u00abRGB. Ins(.)} .{}). The .rst parameter is the this pointer, whose type is the same as the \ndynamic type of the object from which the method pointer is fetched. The second parameter is an instance \nof RGB. The two parameters are consumed by the method, returning nothing. Instruction 12) calls the method \nin r7. The checker checks if the precondition of the callee is satis.ed by the current machine state. \nRegister r4 has the required type. Register r6 has type Ins(. ' ) in the current state, and the callee \nrequires an existential type ..\u00abRGB. Ins(.). We use assignability rules: register r6 can be assigned \nto the parameter because r6 has type Ins(.) and . \u00ab RGB can be inferred from the constraint environment, \nso r6can be packed to the existential type. After the call, the state type contains the remaining registers \nand type inference continues. This example illustrates how the various features of iTal work together \nto produce an inferable typed assembly language capable of verifying a simplistic object-oriented language. \nMany of the strategies we use in iTal can be extended to more expressive type systems. The use of existential \nquanti.cation of class variables, the separation of assignability and subtyping, and the creation of \ngeneralization variables with mappings in order to join types are all more broadly applicable, as we \ndemonstrate in the next section. 3. The iTalX Type System This section describes how our more realistic \ntype system iTalX expresses common language features, such as interior pointers, covariant arrays, type \ncasts, interfaces, and the stack. iTalX is robust with respect to the common optimizations most compilers \nhave, meaning these optimizations can be applied to any typeable program and the resulting program will \nstill be typeable. iTalX uses the same substitution-based subtyping rules for ex\u00adistentially quanti.ed \nstate types as iTal, and extends iTal s type\u00adinference algorithm. iTalX introduces singleton integer \ntypes and integer variables with constraints to handle array-bounds checking. iTalX treats integer variables \nwith constraints the same way as class variables with constraints with respect to subtyping and joining, \nwhich demonstrates the .exibility of our type system (Section 3.3). iTalX uses the same existentially \nquanti.ed state types for pre\u00adconditions of basic blocks as iTal does. In fact, this is the only form \nof existential quanti.cation in iTalX. iTalX uses a type SubInsPtr(C) to represent the iTal type .a \u00ab \nC.Ins(a). iTalX disallows existential quanti.cation in term types those used to type registers, stack \nslots, .elds in objects, etc. Such quanti.cation in term typesis not needediniTalX.We removed nestedexistential \nquanti.cation from iTalX because having two layers of quanti.ca\u00adtion causes complications with the join \nprocess. These complica\u00adtions do not arise in iTal because iTal does not have subtyping at the term-type \nlevel. Many of the extensions in iTalX require complex constraints for class variables, whereas iTal \nneeds only one simple subclass\u00ading constraint (an upper bound) for each class variable. Covariant arrays \nmay need bounds that are arrays themselves. Type casting needs class variables with lower bounds and \nwith other class vari\u00adables as bounds. Interfaces have multiple inheritance, which means that class variables \nmay have multiple bounds. iTalX separates class variables from their bounds to allow complex constraints \non class variables. One environment collects all class variables (with\u00adout constraints), and another \ncollects constraints on class variables. iTal uses a single environment .for both purposes because a \nclass variable is constrained only by a single upper bound. We use con\u00adstraints and bounds interchangeably \nin this section. 3.1 Requirements for Inference It is challenging to make a type system as expressive \nas iTalX inferable. For example, if class variables can be bound by other variables, the join of the \ntuples (ArrayList, IList, IList)and (Hashtable, Hashtable, IDictionary)is .a\u00ab\u00df\u00ab..(a,\u00df,.) (where ArrayList \nimplements IList and Hashtable imple\u00adments IDictionary). Even though both types being joined use only \ntwo classes each, three variables are required to describe the join. The join also has to recognize the \nleft-to-right inheritance hi\u00aderarchy. Furthermore, as illustrated in Section 3.8, adding a simple feature \nsuch as null pointers can break type inference. Fortunately, iTalX uses the results of an abstract framework \nbased on category theory (described in our technical report [21]) to guarantee inferability. The framework \ndescribes how to construct joins in any existential type system that satis.es three properties, stated \ninformally as follows: (1) if t1 = t2, every free class variable in t2 occurs in t1; (2) term types have \njoins disregarding existentially quanti.ed variables; (3) bounds and substitutions have afactorization \nstructure [1], the metatheory behind thefactorization process used in constructing joins for iTal. By \ncomputing joins, our type inference algorithm is made complete, meaning our inference algorithm will \nalways infer a typing for anytypeable program. For decidability of inference, the main requirements are \nas fol\u00adlows: (1) instruction typing is monotonic, that is, if t1 = t2 , the postcondition when checking \nan instruction with t1 as the precon\u00addition should be a subtype of the postcondition when checking the \nsame instruction with t2 as the precondition; (2) subtyping is well\u00adfounded: there are no in.nite chains \nof strict supertypes. We have translated both iTal and iTalX into the category\u00adtheoretic framework, and \nproven that theysatisfy the requirements above. This proves that they are both inferable. The rest of \nthis section explains majorextensions in iTalX.We introduce the constructs of iTalX as needed.  3.2 \nInterior Pointers and Records iTal uses only a simple model of memory: registers may point to objects, \nwhich in turn may have .elds pointing to other objects. Languages like C# support a more complex memory \nmodel, for example, a method may pass a reference to a .eld of an object as an argument to another method. \nThe compiler represents this reference as an interior pointer into the middle of the object, and iTalX \nmust be able to type such pointers. Furthermore, a C# reference may point to not just a single word in \nthe object, but to an entire struct of multiple .elds embedded directly in the object. In addition, even \nif the language does not require interior pointers, an optimizing compiler may introduce such pointers \n(e.g. when iterating through array elements).  To model this, iTalX breaks iTal s named reference type \nIns(.) into separate pointer types and name types, and adds a distinct record type where individual .eld \ntypes are made explicit: name ::= INS(.)| VTABLE(.)| ... tHeap ::= int |HeapPtr(name)|HeapPtrN(name)| \nSubInsPtr(.)| ... rw ::= R |RW recslot ::= | (rw : tHeap) rec ::= {...,recslot, ...} tReg ::= HeapRecPtr(name \n: rec + n)| HeapRecPtrN(name : rec + n)| RecPtr(rec + n)| ... Name types include INS(.) to name a class \ns instance type and VTABLE(.)to name a class s vtable type. The HeapPtr(name)and HeapPtrN(name)represent \npointers to objects on the heap. The subscript N denotes that the pointer may be null.For example, the \niTalX type HeapPtrN(INS(.))cor\u00adresponds to the iTal type Ins(.). There is also SubInsPtr(.), which is \nequivalent to .a \u00ab ..HeapPtrN(INS(a)), but it has more re\u00adstrictive subtypings than general existential \nquanti.cation. Intu\u00aditively, SubInsPtr(.)should be a subtype of SubInsPtr(. ' )when\u00ad . ' ever . inherits \n; however, this breaks the rule that all vari\u00adables in a supertype are present in the subtype. We can \nstill allow HeapPtrN(INS(.))to be a subtype of SubInsPtr(.), though. Records. Record types rec are used \nto represent object layouts. Arecord type contains zero or more record slots, each with atHeap type. \nEach record slot is either read-write or read-only. The separation of name types and record types avoids \nthe dif.\u00adcultyof recursive types, such as whena class C has a .eld with type C. Slots of record types \nin iTalX do not contain other records or pointers to other record types theyonly contain pointers to \nname types. When the program assigns a pointer-to-name type into a reg\u00adister or stack slot, iTalX automatically \nopens the name to a record type describing the corresponding layout. For example, when the type HeapPtr(name)is \nassigned into a register, it is converted into the type HeapRecPtr(name : rec + 0), where rec is the \nrecord type describing the layout of name. HeapRecPtr(name : rec+n) represents an interior pointer, offset \nby n, to a record of type rec that is in the heap. A HeapRecPtrN(INS(.): rec +0) can be as\u00adsigned to \na SubInsPtr(. ' ).eld of a record provided . inherits . ' . Note that this is not subtyping,but simply \ntype-checking an assign\u00adment, so it does not break our framework. This reuses the concept of assignability \nthat we used in iTal. Whereas a HeapRecPtr points to records in the heap, a RecPtr can also refer to \nthe stack of the caller. A HeapRecPtrN can for\u00adget its heap structure and become a RecPtr. Subtyping \nfor record pointers is primarily inherited from pre.x subtyping of their record types, but it can be \nmore .exible to also incorporate offsets. HeapRecPtr(name : rec + n) and RecPtr(rec + n) represent interior \npointers when the offset n is positive. When opening a name to a record type, we may not statically know \nthe layout of the entire record, such as when the name refers to the dynamic type a of an object. Opening \na name INS(a) (representing an instance of a), where a is a subclass of C, results in a record type that \ncontains C s .elds, except the vtable .eld has name VTABLE(a)to guarantee soundness of dynamic dispatch. \n 3.3 Arrays Adding support for arrays goes as follows. First, we introduce class types for array classes \nand adjust how to join existential types ac\u00adcordingly. Second,weaddexistential quanti.cationof integersand \nsimple arithmetic expressions in order to do array-bounds check\u00ading. Third, we add ordering constraints \non integers.Fourth, we in\u00adtroduce extended records to encode records (array headers) with a subrecord \nwhich repeats an unknown number of times (array ele\u00adments). Lastly, we allow names to open to existentially \nquanti.ed extended-record types. Array Classes. In iTal, a class type is either a class variable or a \nclass constant. Now we add a class constructor Array: . ::= ... |Array(.) Array(.)always extends the \nArray class, per C# s array classes. When constructing the join we have to infer when to use array classes. \nFor this, we again use the two maps constructed during generalization in iTal s join algorithm (we refer \nto them as R1 and R2). If R1 maps a generalization variable a to Array(.1)and R2 maps a to Array(.2), \nwe introduce a fresh generalization variable \u00df mapping to .1 and .2 and substitute a with Array(\u00df ). \nIf \u00df also maps to two array types, we repeat this process recursively. Existentially Quantifying Integers. \nTo verify array accesses, we add existentially quanti.ed integer variables. By integers, we mean mathematical \nintegers, not 32-bit integers. Existential quanti.ca\u00adtion may introduce integer variables i with simple \narithmetic ex\u00adpressions of the form I = i * a + b as constraints, where a and b are (mathematical) integer \nconstants. The type Int(I)is a singleton integer type representing the single integer value I: tReg ::= \n... |Int(I) Joining existentially quanti.ed simple arithmetic expressions poses an interesting challenge. \nThe join of the states {r : Int(4)}and {r : Int(10)}is .i.{r : Int(i * 6 + 4)}or equivalently .i.{r : \nInt(i* 6+10)}. The join of the states: .i.{r : Int(i* 4+4),r ' : Int(i* 8)}  .j.{r : Int(j* 6+2),r \n' : Int(j* 12-4)}  is .k.{r : Int(k*2+4),r ' : Int(k*4)}, with substitutions k . i*2 and k . j * 3 - \n1. More broadly, the join generalizes integer types by introducing variables to represent them, similar \nto iTal s generalization of class types: Int(I1)and Int(I2)are generalized to Int( i)where Int( i)is \na fresh generalization variable and i . I1 and i . I2. As with iTal s join in section 2.3, the join for \ninteger types de\u00ad.nes an equivalence relation on generalization variables. Consider, for simplicity, \nequivalences for types of the form I = i * a where a = 1. Then the equivalence i = jholds if: R1( i)= \nk1 * a1 and R1( j)= k1 * b1  R2( i)= k2 * a2 and R2( j)= k2 * b2  a1 b1 a1 b1 = (so that = ) a2 b2 \ngcd(a1 ,a2 ) gcd(b1 ,b2 ) The join then designates a fresh variable k for each equivalence class of generalization \nvariables, then substitutes each generaliza\u00adtion variable with an appropriate expression in terms of \nk. Sup\u00adpose we have a generalization variable i with R1( i)= k1 * a1 , R2( j)= k2 * a2, and k is the \nfresh variable designated for i s equivalence class; then the join substitutes i with k * gcd(a1,a2) \nwhere gcd is the greatest common divisor. The mappings k .  a1 a2 k1 * and k . k2 * serve as evidence \nthat gcd(a1 ,a2 ) gcd(a1 ,a2 ) this construction forms a common supertype of the two types be\u00ading joined. \nThe use of gcd is necessary since a coef.cient for k smaller than gcd(a1,a2 )would fail to yield the \nbest common su\u00adpertype, while a coef.cient larger than gcd(a1,a2)would fail to yield a common supertype \nat all. The generalization of the join beyond I = i * a to all forms of I is straightforward (see Granger \n[7] for a thorough discussion of joining integer equalities). Ordering Integer Expressions. To check \narray bounds, our exis\u00adtential quanti.cations also need to include ordering constraints of the form I1 \n<32+ I2. This constraint means that, viewed as un\u00adsigned 32-bit integers, I1 is strictly less than I2. \nThus, we view the machine as capable of manipulating mathematical integers,but the comparisons are limited \nto a 32-bit perspective on these math\u00adematical integers. Dereferencing also has a 32-bit perspective, \nso we can rely on 32-bit ordering constraints to verify array accesses. The reasons for this unusual \nperspective are contained within our abstract framework. In short, complications arise because i * 4 \nis not an injective operation on 32-bit integers. iTalX restricts which ordering constraints can be present \nin an existential type. In particular, iTalX permits the ordering constraint I1 <32+ I2 to be present \nin an existential type if both I1 and I2 oc\u00adcur in the body of the existential type. iTalX also allows \nthe above ordering constraint if I1 is a constant and I2 occurs in the body. A constraint is not permitted \nif it does not satisfy either of these con\u00additions. This restriction bounds the number of constraints \npresent in iTalX s existential types, essentially discarding all constraints that are irrelevant to type \nchecking the program. We found this bound to be important to achieving an ef.cient implementation. At \npresent, iTalX does not use any arithmetic inference rules in its subtyping rules, only that <32+ is \ntransitive. This prevents anycomplications with arithmeticover.ow,but also preventsiTalX from handling \narray-bounds-check elimination. The above restric\u00adtion on constraints, however, would allow us to extend \niTalX with arithmetic inference rules that are sound even in the presence of arithmetic over.ow, while \nstill keeping the type system well\u00adfounded as required by our inference algorithm. Such an extension \nis considered future work. Extended Records. An extended-record type is used to represent array layouts. \nIt consists of a record type (array header), a .xed\u00adlength name (the elements), and an integer expression \n(the number of elements): rec . nameI. Although in general a name can actually describe an extended-record \ntype, a .xed-length name must describe a record type of a predetermined length. This allows us to identify \nwhich index of an array and which .eld within that index that an interior pointer is referencing. Arrays \nhave statically indeterminable lengths. To refer to the length of an array, we allow names to open to \nexistentially quan\u00adti.ed record types, although any existentially quanti.ed vari\u00adables and constraints \nare immediately pulled into the outer ex\u00adistential bound upon opening the name. For example, the name \nINS(Array(a))opens to the existentially quanti.ed record type . . . R : HeapPtr(VTABLE(Array(a))) . R \n: HeapPtr(RUNTIME(a)) .l. . . R : Int(l) . SUBINSPTRREC(a)l where lis an existentially quanti.ed integer \nvariable indicating the length of the array, and lwould be pulled into the environment.We represent thefact \nthat the third .eld of the header is also the length of the array by using l in both positions. The second \n.eld is used to type check covariant arrays using the techniques described next. SUBINSPTRREC(a)is the \n.xed-length name for the record type {RW : SubInsPtr(a)}, representing the elements of the array. Thus, \niTalX type checks arrays and array accesses by combining our earlier conceptsofexistential quanti.cation, \nrecords, and names.  3.4 Type Casts and Runtime Types Type checking downward type casts requires adding \nclass variables with lower bounds and variables bounded by other variables. Downward type casts test \nat run time whether an object is an instance of a class. Each class has a unique identi.er, called its \nruntime type. Two runtime types are equal if and only if the corresponding classes are the same. The \nruntime type of a class points to the runtime type of its immediate superclass, and such pointers form \na runtime type chain. A typical implementation of downward type casts walks up the chain to .nd if a \nsuperclass matches the class to which we want to cast. iTalX uses the name RUNTIME(.)to represent the \nruntime type of a class .: name ::= ... | RUNTIME(.) To represent the pointer to the runtime type of \nthe superclass, we reuse the concept of existentially quanti.ed records that we intro\u00adduced for arrays. \nRUNTIME(.)opens to the following existentially quanti.ed record type: .\u00df \u00bb ..{..., R : HeapPtrN(RUNTIME(\u00df)),...} \nNote that if . were a class variable a, this would introduce a constraint a \u00ab \u00df between two classvariables,justifying \nthe need for more complex constraints in iTalX. When walking up the runtime type chain to cast an object \nof class a to a class C, if a (possibly null) runtime type with name RUNTIME(.) matches the (non-null) \nruntime type of our target class C, the type inference concludes that . = C, and uses that to check the \ninstructions that follows. In particular, the quanti.cation used abovewould inform us that ainherits \n., so the equality . = C informs us that ainherits C, indicating that the object can be safely treated \nas an instance of a subclass of C.  3.5 Interfaces To support interfaces, iTalX distinguishes class \nvariables that can be instantiated with classes from those instantiated with interfaces, usinga subscript \nC or I onavariable respectively.Forexample, iTalX uses the constraint aC to indicate that the variable \nacan only be instantiated with classes. Furthermore, variables can have more than one bound because classes \ncan inherit multiple interfaces. To compute the join of class variables with such constraints, we again \nuse the two generalization maps R1 and R2. A gener\u00adalization variable a will inherit all of the (possibly \nimplicit) con\u00adstraints on R1( a) and R2( a). For example, if both R1( a) and R2( a)are constrained to \nbe classes, the generalization variable a will be as well. Similarly, given two generalization variables \na and \u00df , if R1( a) \u00ab R1(\u00df ) and R2 ( a) \u00ab R2(\u00df ) hold, we add the constraint a \u00ab \u00df . We also have to \ninfer inheritance constraints with respect to class and interface names such as ArrayList and IList.For \nupper bounds this poses no problem since any class or interfaceonly inheritsa .nite numberof classesand \ninterfaces.For lower bounds, however, due to multiple inheritance there may be an in.nite number of classes \nand interfaces which inherit both R1( a) and R2( a).To address this issue, we also allow class variables \nto be bounded below by a tensor .of class and interface names provided there exists a class or interface \nthat inherits all those in the tensor. This restriction on valid tensors bounds their size in a given \npro\u00adgram, whichkeeps our type system well-founded. Finally,ifa class or interface inherits all the tensored \nclasses and interfaces bound\u00ading a, then that class implicitly inherits aas well. Thus, the join of ISerializable \nand IList is .aI \u00bb ISerializable.IList.a  and we can infer that ArrayList inherits a since ArrayList \nim\u00adplements both ISerializable and IList. These techniques grant us an inferable type system capable \nof casting and even interface\u00admethod lookup in a multiple inheritance context (the latter process is \ndescribed in detail in the Appendix).  3.6 Generics To support generics, we can reuse manyof the same \ntechniques we used to support array classes. If generic class types C(.1,...,.n) and C(.1' ,...,. ' )are \ngeneralized to a generalization variable na, we introduce n fresh generalization variables a i mapping \nto .i and . ' i and substitute a with C( a1,..., a n). If anyof the a i also map to two similar generics, \nwe repeat this process recursively. We also have to extend the constraint environment to include constraints \nof the form a \u00ab C(..) or C(..) \u00ab a, where .. may also contain class variables. In fact, the lower bound \non an interface variable a may need to be a tensor of generics (with their arguments supplied). However, \nwe never need constraints of the form C(..)\u00ab D(..' );because of how inheritance can be speci.ed in C#, \nconstraints of this form can always be simpli.ed. Even with these more complex constraints, type inference \nis still decidable. 3.7 Using the Stack iTal has no concept of a stack, an obvious shortcoming since \nthe stack plays such an important role at the assembly level. Here we make simple extensions to iTalX \nto let it use the stack intraproce\u00addurally and interprocedurally.We represent the stack essentially as \na partial map from non-negative integers, marking stack slots in the current stack frame, to register \ntypes. We use StackPtr(n)to ac\u00adcess and manipulate the stack. By using only non-negative integers in \nthe stack, we prevent a callee from changing the caller s stack. These simple extensions allow iTalX \nto use the stackintraprocedu\u00adrally, but we need to re.ne code-pointer types in order to use the stack \ninterprocedurally. Code Pointers. A code-pointer type in iTalX, as in iTal, is spec\u00adi.ed simply as a \nrequired input state type and a produced output state type. These state types are a stack type and a \nregister bank type; however, they cannot refer to register types tReg (de.ned in Section 3.2, along with \nheap types tHeap ). The output state type can only refer to heap types and stack-pointer types. The input \nstate type can only refer to heap types, minus code-pointer types, and stack-pointer types along with \ntwo additional types. The .rst type, ReturnAddress, tells the caller where the return address should \nbe stored. ReturnAddress is also a register type, used to type check the ret instruction. The second \ntype is ParamPtr(name), a new pointer type which can be used only as an input type. Unlike the other \ninput pointer types HeapPtr and SubInsPtr, ParamPtr need not refertotheheap.Inthe callee,aParamPtr(name)will \nbe trans\u00adlated into a RecPtr of the record type that name opens to; thus, the caller can pass any pointer \nwhose referenced space will look like the appropriate record type for the duration of the call. This \nallows the caller to pass even a stack pointer, provided the referenced por\u00adtion of the stack is appropriately \ntyped at the time of the call and does notoverlap with the callee s stack frame.ParamPtrs allow us to \npass references to local variables, .elds, and array indices per the pass-by-reference semantics of C# \ns ref keyword. Callee-Save Registers Acommon calling convention requires the callee to ensure that the \nvalues of certain registers upon entering the function are the same upon exiting the function. This convention \nis known as callee-save registers. We incorporate this into iTalX by having each code-pointer type declare \nthe set of registers whose values will be preserved. Subtyping of code-pointer types allows this set \nto be smaller in supertypes. We type check callee-save registers in the usual manner: each callee-save \nregister is given its own type variable at the beginning of the function body and must have that same \ntype variable upon returning from the function. 3.8 Null Pointers The extensions above capture most \nfeatures of C# except one seem\u00adingly unremarkable feature: null pointers. Although iTalX has nul\u00adlable \nheap pointers, it does not have an explicit null type. We do this for a very good reason: null pointers \nbreak joins in the pres\u00adence of existential quanti.cation, breaking the inference process. Our framework \neven suggests this, since the a in the simple rule null = HeapPtr(INS(a))is not used in the subtype null. \nWith\u00adout changing null to already refer to a, there is no way to re\u00adsolve this problem.Fortunately, we \ncan illustrate the problem con\u00adcretely and concisely using some shorthand. Take the two exis\u00adtentially \nquanti.ed triples ta := .a.(null,a, null)and t\u00df := .\u00df,\u00df ' .(\u00df,null,\u00df ' ). The join of ta and t\u00df cannot \ncontain null. The two existentially quanti.ed triples t. := ..,. ' .(.,.,. ' )and td := .d,d ' .(d,d \n' ,d ' )are both supertypes of both ta and t\u00df. Their only common subtype without null is t. := ...(., \n.,.),but t. is not a supertype of t\u00df. Thus, ta and t\u00df have no join. This example illustrates how some \nof the most intuitive types can break an infer\u00adence algorithm. Although C# has null, it always occurs \nwhere the class that it is a nullpointer of can be easily discerned. In the lower\u00ading stage, when we \nreplace null with 0, we include an annotation indicating that that occurrence of0has type NullPtr(.), \nwhere . is the class or interface associated with that use of null. NullPtr(.) is a subtype of both HeapPtrN(Ins(.))and \nInt(0).  3.9 Theorems We have proven the following properties for inference of iTalX: Decidability. \nThe inference algorithm described in Section 2.3 extended to iTalX halts. Completeness. If an iTalX function \nis typeable, the inference algorithm described in Section 2.3 extended to iTalX infers a valid typing \nof that function. These theorems result primarily from our categorical framework for existential types \ndescribed in our technical report [21]. The proof strategy extends the strategies used for iTal. The \nproofs for a slight variant of iTal can also be found in our technical report. 4. Implementation Here \nwe present our prototype implementation of a type inference engine for iTalX on the output of a large-scale \nobject-oriented opti\u00admizing C# compiler called Bartok.We show that: (1) it is practical to infer types \nfor an expressive TAL such as iTalX; (2) type infer\u00adence needs much less effort from the compiler and \nmuch fewer type annotations, compared to traditional certifying compilation. Our base compiler Bartok \ncompiles Microsoft .NET bytecode programs to standalone x86 executables. It is not a just-in-time compiler. \nThe compiler has about 200,000 lines of code, mostly written in C#, and is fully self-hosting. Performance \nof Bartok s generated code is comparable to performance under the Microsoft Common Language Runtime (CLR). \nAccording to the benchmarks tested, programs compiled by Bartok are 0.94 to 4.13 times faster than the \nCLR versions, with a geometric mean of 1.66. Throughout this evaluation we compare against our previous \nwork [4] in which webuilta traditional certifying compiler, also based on Bartok,by making every compilation \nphase preserve types. For our benchmarks, about 98%of methods are inferable.Asfar as we know, no other \nsystems are able to infer types for real-world x86 benchmarks at a similar scale. We changed about 2.5% \nof the compiler code, about 5,000 lines of code out of 200,000 lines. Among the 5,000 lines, about 4,500 \nlines are for adding new code to de.ne iTalX types and to write metadata such as the class hierarchy, \nrecord layouts, and func\u00adtion signatures into the object .les in terms of iTalX s type sys\u00adtem. The compiler \ntransformations and optimizations are mostly untouched. Our previous traditional certifying compiler \nchanged about 10% (19,000 lines) of the compiler code. It required every transformation and optimization \nto preserve types, and therefore modi.ed many more compilation phases. Although our experience showed \nthat most optimizations can be made to preserve types eas\u00adily, changing 19,000 linesofcodeis stillalargeburdenonthe \ncom\u00adpiler writers, especially .guring out where changes are needed and what types in the complexTAL type \nsystem to use. Compiler writ\u00aders who build certifying compilers from scratch also have to think about \nmaintaining the right type information in every optimization if theyfollow the traditional type-preserving \napproach.  Our type inference engine mainly consists of the iTalX de.ni\u00adtions (5,000 lines of C# code), \nan x86 disassembler (4,700 lines), and the type inference (about 4,100 lines). The main differences between \nour type inference engine and our previous traditional cer\u00adtifying compiler s type checker are the de.nitionsof \nstate types and the computation of joins, which add up to about 1,300 lines of code. We chose to increase \nthe trusted computing base slightly to relieve the compiler from full-blown certifying compilation. In \norder to re\u00adduce the trusted computing base, we could separate type inference and type checking into \ntwo phases so that only the type checking phase would be trusted. Type annotation needed by our type \ninference implementation is about 60% less than that required by our previous traditional certifying \ncompiler. Size of type annotations required by inference is only about 17% of the size of pure code and \ndata in object .les, compared with 36% for the previous certifying compiler. It indicates that type-annotation \nsize may no longer be a big obstacle for adopting certifying compilation. Our implementation supports \nallocating C# structs on the stack, without annotations specifying the struct type during allocation. \nType inference supports initializing a struct .eld by .eld and then using a pointer to the .rst .eld \nas a pointer to the whole struct. It even supports passing structs as parameters on the stack. We are \nunaware of anyother systems with similarly .exible stack support. The implementation also supports jump \ntables (a more ef.cient way to compile switch statements) by disassembling the data sec\u00adtion where the \njump tables are stored to .gure out the jump tar\u00adgets. The compiler only needs to annotate the jump instruction \nwith the length of the jump table. We also extend our permitted integer constraints to include expressions \nbounded above by constants less than or equal to the length of the largest jump table in the function. \nThis way we can ensure the assembly code is accessing the jump table correctly, while stillkeeping our \ntype system inferable. The implementation extends iTalX slightly to address features such as type arguments \nfor polymorphic methods.For polymorphic method calls, we infer the type arguments instead of relying \non type annotations. Type inference for type arguments of polymorphic functions is in general undecidable \n[19], but currently we support only polymorphic methods for type casts and memory allocation, where inferring \ntype arguments is simple in these special cases. Our implementation does not support exceptions or delegates. \nThose are considered future work. Our framework can handle generics,but our prototype does not include \nit because Bartok fully instantiates generics before code generation.Wehave notyet added type annotations \nfor null literals, as discussed in Section 3.8. This would only cause our type inference to report null-related \ntype er\u00adrors when it should not. We expect that the missing annotations would have little impact on type-annotation \nsize. Measurement. Here we describe measurement of type-annotation size and type-inference time on our \nbenchmarks. We chose the  Benchmarks Succ. Total Succ./Total (%) ahcbench 67 67 100.0 asmlc 15,820 16,462 \n96.1 bartok 7,037 7,222 97.4 lcscbench 5,718 5,860 97.6 mandelform 12 12 100.0 sat solver 274 274 100.0 \nzinger 1,125 1,189 94.6 Geomean 97.9 Table 2. Number of Successfully Inferred Methods Benchmarks Infer. \n TAL Infer./TAL (%) ahcbench 3,320 5,936 55.9 asmlc 644,937 2,745,130 23.5 bartok 334,590 1,448,867 \n  23.1 lcscbench 256,116 911,308 28.1 mandelform 3,732 4,716 79.1 sat solver 13,560 22,828 59.4 zinger \n45,411 114,084 39.8 Geomean 39.8 Table 3. Type-Annotation Size (in bytes) seven major benchmarks used \nin our previous work, which range from 54KB to 21MB in object .le size not including libraries (see Table \n1). We compile the benchmarks separately from the libraries, to focus on the user programs. The object \n.les include type annotations for type inference. We compile with all of Bar\u00adtok s standard optimizations \n(more than 40 of them) turned on, except for three optimizations that our inference cannot yet handle: \narray-bounds-check elimination, redundant-type-test elimination, and inlined memory allocation. About \n98% of methods in the benchmarks are inferable (seeTa\u00adble 2). All methods in the small benchmarks (ahcbench, \nsat solver, and mandelform) are inferable. For the large benchmarks (asmlc, bartok, lcscbench, and zinger), \ntype inferencefails ona small num\u00adber of methods because the methods use unsupported language fea\u00adtures, \nsuch as delegates, or interact with unsafe code, say by using Platform Invocation Services (PInvoke). \nTable 3 compares the type-annotation sizes: type inference needs about 23%-79% of the type annotation \nrequired by our pre\u00advious certifying compiler, with a geometric mean of 40%, which is abouta 60% reduction \non the type-annotation size.We see more size reduction on large benchmarks than on small ones because \nsmall benchmarks do not have manyannotations to begin with and types for static data and function signatures \nare more dominating than those in large benchmarks.   Benchmarks Infer. Comp. Infer./Comp. (%) \nahcbench 0.1 4.8 1.9 asmlc 21.6 135.2 16.0 bartok 24.2 69.6 34.7 lcscbench 8.5 61.3 13.9 mandelform 0.1 \n10.5 1.3 sat solver 0.6 6.7 9.3 zinger 2.1 15.2 13.7 Geomean 8.2 Table 4. Type-InferenceTime vs. CompilationTime \n(in seconds) Table4 shows the type-inference time compared with compila\u00adtion time (including writing \nmetadata to .le). The numbers were measured ona PC runningWindowsVista witha 3GHz quad core CPU and 4GB \nof memory. Type inference in our current imple\u00admentation is slower than type checking in the previous \ncertifying compiler: type inference takes about 1%-35% of compilation time, with a geometric mean of \n8%, whereas type checking in the previ\u00adous certifying compiler takes less than 3% of the compilation \ntime. The difference is mainly because type inference is more sensi\u00adtive to the control .ow structures \nof methods. Straight-line code is easy to infer; type inference scans code only once and thus can be \nas ef.cient as type checking. For methods with complex loops, type inference sometimes takes much longer \nto reach a .xed point for preconditions of basic blocks without the guidance of type anno\u00adtations. The \ntype checker with full annotations needs to scan code only once no matter how complex the code structure \nis, because a basic block at each control merge point is annotated with its pre\u00adcondition. The Bartok \nbenchmark is an outlier for type-inference time. It has more than 7,000 methods. The largest 23 methods \nin the benchmark have complex control .ow graphs and together take about half of the total type-inference \ntime. One approach to getting more ef.cient type inference even with complex control .ow structures is \nto ask for slightly more type annotation from the compiler, such as loop invariants, so that the type \ninference engine can reach the .xed pointfaster.We consider this approach future work. One lesson we \nlearned from our implementation experience is that memoizing large types does not pay off when we do \nnot com\u00adpare those types for equality often. State types in iTalX are large and complicated because they \nmodel machine states. Our .rst im\u00adplementation memoized state types, which required substitutions (because \nstate types are quanti.ed types) and structural equality, and the type-inference time took about 3%-323% \nof the compila\u00adtiontime(witha geometric meanof36%).Withno memoizationof state typesandafew other .ne-tunings, \nour current implementation is much more ef.cient. 5. Related Work Hindley-Milner type inference [13] \nis used by the ML and Haskell programming languages. The Hindley-Milner algorithm discovers omitted types \nby using uni.cation to solve systems of equations be\u00adtween types. For a simple enough type system, this \nalgorithm can infer all types in a program without relying on any programmer\u00adsupplied type annotations \n(unlike our forward data.ow analysis, it does not require a method type signature as a starting point). \nUn\u00adfortunately, this remarkable result does not extend to all type sys\u00adtems. In particular, .rst-class \nquanti.ed types are known to make type inference undecidable [23]. Extensions to the Hindley-Milner approach \nsupporting .rst-class quanti.ed types require some type annotations [9, 11] or pack/unpack annotations \n[8]. Alternatives to the Hindley-Milner approach, such as local type inference [20], also require some \ntype annotations. Although these extended and alternative algorithms [8, 9, 11, 20] were developed for \nfunctional languages, theycould be applied to a typed assembly language like iTalX by treating each basic \nblock as a (recursive) function. Unfor\u00adtunately, this would force a compiler to provide type annotations \non some of the basic blocks, which our approach avoids. Much of the dif.culty in inferring .rst-class \nquanti.ed types stems from the broad range of types that type variables can repre\u00adsent. In the type .a. \na, many type systems allow a to represent any type in the type system, including quanti.ed types like \n.a. a itself. In order to accomplish type inference, iTalX restricts what quanti.ed variables may represent. \nIn this respect, our work is most similar to the Pizza language s internal type system [18], whose existential \ntypes quantify over named classes rather than over all types. Like iTalX, Pizza s internal type system \nde.nes a join opera\u00adtion over existential types. However, to the best of our understand\u00ading, the operation \ncomputes the join of .a.IList(a)with itself as a type of the form .a \u00ab .C is a set of classes not C.a, \nwhere .containing a. Regardless of the contents of C., this type cannot be equivalent to .a.IList(a), \nand therefore cannot be the join. This complication is simply a demonstration of how challenging infer\u00adence \nof existential types can be. SpecialJ [5] is a certifying compiler for Java that uses a proof generator \nto create proofs of safety for assembly language. How\u00adever, SpecialJ s proof generation relies on compiler-generated \nloop invariants, whereas iTalX infers loop invariants automatically. With respect to inference in assembly \nlanguage, our work is most similar to Coolaid [2], which performs a forward data.ow analysis to infer \nvalues and types of registers for code compiled from a Java-like language. Coolaid s inference introduces \nsym\u00adbolic values to represent unknown values, corresponding to ex\u00adistentially quanti.ed variables in \niTalX s state types. Coolaid is more specialized towards a particular source language and a partic\u00adular \ncompilation strategy than most typed assembly languages are, whereas iTalX encodes objects and classes \nusing more standard general-purpose types (namely existential quanti.cation). This makes us optimistic \nthat our framework will more easily grow to incorporate more advanced programming language features, \nsuch as generics with bounded quanti.cation. Chang et al. [2] state that We might hope to recover some \ngenerality, yet maintain some sim\u00adplicity,by moving towards an object-oriented TAL .We envision iTalX \nasexactly such an object-orientedTAL. 6. Conclusions We have formalized and implemented type inference \nfor iTalX, a typed assembly language capable of verifying optimized assembly code compiled from object-oriented \nlanguages like C# and Java. Currently, the implementation completely infers the types for about 98% of \nfunctions in our benchmark suite. Inferring most of the re\u00admaining 2% appears to be a matter of engineering \nthe inference implementation to recognize idioms such as Bartok s implementa\u00adtionof delegates.Itmay also \nrequire modi.cationstothe compiler, such as propagating types of null-pointer literals, but such mod\u00adi.cations \nare minor compared to the effort of implementing type preservation throughout a large compiler. Based \non this, it appears feasible to use inference as the primary mechanism for generating TAL types from \na large optimizing compiler, only rarely disabling optimizationsorfallingbacktoa smaller type-preserving \ncompiler. Although our type system is not yet able to support all optimiza\u00adtions (e.g. array-bounds-check \nelimination), it supports the com\u00admon optimizations essential to generating good code from object\u00adoriented \nlanguages. Only 3 out of more than 40 optimizations in Bartok are not supported. Based on the abstract \nframework under\u00adlying our type system, we believe that inference can readily be ad\u00adjusted to accommodate \nnew language features. We are currently investigating adding null-dereference checking and more powerful \narray-bounds checking directly to our type system by expanding the capabilities of our existential bounds. \nAs languages like Java and C# evolve, so will our inferable typed assembly language.  Acknowledgements. \nWe would like to thank Francesco Logozzo for discussions about numerical abstract domains, as well as \nour anonymous reviewers for their insightful feedback. References [1] J. Ad\u00b4 Abstract and Concrete amek, \nH. Herrlich, and G. E. Strecker. Categories.Wiley-Interscience, NewYork, NY, USA, 1990. [2] B. E. Chang, \nA. Chlipala, G. C. Necula, and R. R. Schneck. Type\u00adbased veri.cation of assembly language for compiler \ndebugging. In TLDI, pages 91 102, 2005. [3] J. Chen and D. Tarditi. A simple typed intermediate language \nfor object-oriented languages. In POPL, pages 38 49, 2005. [4] J. Chen, C. Hawblitzel,F. Perry, M. Emmi, \nJ. Condit, D. Coetzee, and P. Pratikaki. Type-preserving compilation for large-scale optimizing object-oriented \ncompilers. In PLDI, pages 183 192, 2008. [5] C. Colby, P. Lee, G. C. Necula, F. Blau, K. Cline, and M. \nPlesko. A certifying compiler for Java. In PLDI, pages 95 107, 2000. [6] A. Goldberg.Aspeci.cationofjava \nloading and bytecodeveri.cation. In Computer and Communications Security, pages 49 58, 1998. [7] P. Granger. \nStatic analysis of linear congruence equalities among variables of a program. In TAPSOFT, volume 1, pages \n169 192, 1991. [8] M.P. Jones. First-class polymorphism with type inference. In POPL, pages 483 496, \n1997. [9] D. Le Botlan and D.R\u00b4emy. MLF: Raising ML to the power of System F. In ICFP, pages 27 38, 2003. \n[10] C. League, Z. Shao, andV.Trifonov. Type-preserving compilation of Featherweight Java. TOPLAS, 24(2):112 \n152, 2002. [11] D. Leijen. HMF: Simple type inference for .rst-class polymorphism. In ICFP, pages 283 \n294, 2008. [12] T. Lindholm and F. Yellin. TheJava Virtual Machine Speci.cation. Sun Microsystems, 2nd \nedition, 1999. [13] R. Milner. A theory of type polymorphism in programming. Journal of Computer and \nSystem Sciences, 17:348 375, 1978. [14] G. Morrisett, K. Crary, N. Glew, D. Grossman, R. Samuels, F. \nSmith, D. Walker, S. Weirich, and S. Zdancewic. TALx86: A realistic typed assembly language. In ACM Workshop \non Compiler Support for System Software, pages 25 35, 1999. [15] G. Morrisett, D. Walker, K. Crary, and \nN. Glew. From System F to typed assembly language. TOPLAS, 21(3):527 568, 1999. [16] G. Morrisett, K. \nCrary, N. Glew, and D. Walker. Stack-based typed assembly language. JFP, 13(5):957 959, 2003. [17] G. \nC. Necula and P. Lee. Safe kernel extensions without run-time checking. In OSDI, pages 229 243, 1996. \n[18] M. Odersky and P. Wadler. Pizza into java: translating theory into practice. In POPL, pages 146 \n159, 1997. [19] F. Pfenning. On the undecidability of partial polymorphic type recon\u00adstruction. Fundamenta \nInformaticae, 19(1,2):185 199, 1993. [20] B. C. Pierce and D. N.Turner. Local type inference. In POPL, \npages 252 265, 1998. [21] R. Tate, J. Chen, and C. Hawblitzel. A framework for type inference with existential \nquanti.cation. Technical report, http://research.microsoft.com/pubs/78684/tr.pdf, 2008. [22] S. Wehr \nand P. Thiemann. On the decidability of subtyping with bounded existential types. In APLAS, pages 111 \n127, 2009. [23] J. B. Wells. Typability and type checking in System F are equivalent and undecidable. \nAnnals of Pure and Applied Logic, 98:111 156, 1998. Appendix. Interface-Method Lookup in Detail Here \nwe describe in detail how we type check the common but surprisingly challenging process of looking up \nan interface method implementation. Suppose we have an instance of some class a implementing the interface \nIList, and we want to invoke the method getCount declared in IList. A typical implementation .rst loads \nthe vtable for a from the instance. Then it must load a s interface table from the vtable. This table \nis an array of all the interfaces implemented by a. Each entry in the interface table corresponds to \nan interface implemented by a and consists of two pieces of information about that interface: the runtime \ntype as well as the offset of the appropriate interface-method table from the beginning of a s vtable. \nSo, the program has to go through each index of a s interface table until it .nds an interface matching \nIList. It then adds the offset at that index to the vtable to get a s interface-method table for IList. \nFrom that we .nally retrieve a s implementation of getCount. This is all accomplished by the assembly \ncode in Figure 1 (for simplicity we assume that the important .elds for this example are always at offset \n0). Now we wish to type check this assembly code. First, we start at the beginning of the block. There \nis an instance of IList at stack offset -4, corresponding to the .rst stack slot, so we give that slot \nthe type HeapPtr(INS(a)), where a is an existentially quanti.ed class variable. We use the variable a \nto refer to the exact class of this instance. We know a is a class, so we have the constraint aC. Furthermore, \nwe know the instance implements IList, so we also have the constraint a \u00ab IList. Since we have a HeapPtr \nin a stack slot, we automatically open the name INS(a)to the record type {R : HeapPtr(VTABLE(a)),... \n}(using the metainformation provided by the compiler). Thus, the .rst stack slot is given the following \ntype: HeapRecPtr(INS(a): {R : HeapPtr(VTABLE(a)),... }+0) This way we may both use the instance as an \nobject in the heap and have access to the .elds of its record. Now, instruction 1) simply copies this \ntype into register EAX. Instruction 2) replaces the type in EAX with the type of the .rst .eld of the \ninstance s record: HeapPtr(VTABLE(a)). Once again, we automatically open the name type to the following \nrecord type: {R : HeapPtr(ITABLE(a)),... } ITABLE(a)is the name for a s interface table. Instruction \n3) saves the vtable for later use by copying EAX to the third slot on the stack. Instruction 4) replaces \nthe type in EAX with the type of the .rst .eld of a s vtable: HeapPtr(ITABLE(a)). Once again, we automatically \nopen the name type ITABLE(a) to a record type. Because ITABLE(a)represents an array, this actually opens \nto the following existentially quanti.ed extended-record type: .l.{R : Int(l)}. ITABLEENTRY(a)l The existentially \nquanti.ed variable l (the length of the array) is automatically pulled into the environment. ITABLEENTRY(a) \nis a .xed-length name representing the length-2-record type compris\u00ading each entry of the interface table, \nwhich we will examine in more detail later. Instruction 5) loads the .rst .eld of a s interface table. \nThis .eld is the singletoninteger type representing the length of the array. Comparing with this value \nwill enable us to ensure that the assembly code is accessing the array with a valid index. Instruc\u00adtion \n6) is an optimization which adds 4 to EAX so that EAX points to the .rst element of the interface table \ns array.We can handle this optimization since our HeapRecPtr and RecPtr types have an offset. Speci.cally, \nEAX will have the following type: HeapRecPtr(ITABLE(a): {R : Int(l)}.ITABLEENTRY(a)l +4) Note the offset \n+4, and that l is now in the environment.  EAX . HeapRecPtr(ITABLE(a): {R : Int(l)}.ITABLEENTRY(a)l[last \n: i . \u00df]+ 4) Variables Constraints ist ryEBX . Int(l) . a, \u00df i,l : aC \u00dfI a \u00ab IList a \u00ab \u00df i <32+ l . RegECXEDXStack1st2nd3rd \n. Int(i) . HeapRecPtr(RUNTIME(\u00df): {. . . }+ 0) . HeapRecPtr(INS(a): {R : HeapPtr(VTABLE(a)), . . . }+ \n0) . HeapRecPtr(RUNTIME(IList): {. . . }+ 0) . HeapRecPtr(VTABLE(a): {R : HeapPtr(ITABLE(a)),. . . }+ \n0) CCHeapRecPtr(RUNTIME(IList): {. . . }+ 0) with HeapRecPtr(RUNTIME(\u00df): {. . . }+ 0) Figure 2. An existentially \nquanti.ed state type inferred during interface-method lookup // stack offset -4 contains an instance \nof IList // stack offset -8 contains runtime type of IList 1) mov EAX, [ESP-4] // load instance into \nreg 2) mov EAX, [EAX] // load vtable 3) mov [ESP-12], EAX // store vtable on stack 4) mov EAX, [EAX] \n// load interface table 5) mov EBX, [EAX] // load table s length 6) addi EAX, #4 // move to head of array \n7) movi ECX, #0 // start at index 0 L0:8) cmp EBX, ECX // compare length and index 9) jbe L1 // break \nif length <= index 10) mov EDX, [EAX+ECX*8] // retrieve runtime type 11) cmp EDX, [ESP-8] // compare \nwith IList 12) je L2 // break from loop if equal 13) addi ECX, #1 // increment index 14) jmp L0 // continue \nlooping L1:15) throw exception // not an instance of IList L2:16) mov EDX, [EAX+ECX*8+4]// load offset \nfrom table 17) add EDX, [ESP-12] // add offset to vtable 18) mov EAX, [EDX] // load getCount // stack \noffset -4 contains an instance of IList // EAX contains getCount implementation for the instance Figure \n1. The assembly code for looking up an interface method Instruction 7) loads the constant 0 into register \nECX (which tracks the index into the interface table), so that ECX has the sin\u00adgleton integer type Int(0). \nThe block starting at L0 iterates over each interface-table entry until it .nds the one (if any) correspond\u00ading \nto IList. Notice that instruction 14) jumps back to L0 for the next iteration. Normally, we would proceed \nto type the instructions with ECX having type Int(0)until we get to instruction 14), at which point we \nwould join that type with the current type and repeat the whole process. In the interest of saving time, \nwe will simply replace the type of ECX with Int(i), where i is a fresh existentially quanti\u00ad.ed integer \nvariable representing the index of the current iteration. Instruction 8) compares EBX, which has type \nInt(l), with ECX, which has type Int(i). The state type is augmented to note that the condition code \nresults from comparing Int(l) with Int(i). If l =32+ i holds, instruction 9) jumps to instruction 15) \nto throw a runtime exception as there is no IList entry. Otherwise, we can only proceed to instruction \n10) if i<32+ l holds, so we add this constraint to the environment when type checking instruction 10). \nInstruction 10) accesses the interface array. Since 4 was added to the address of the interface table \nearlier, EAX already points to the .rst element of the array. Instruction 10) accesses this array at \noffset ECX * 8. Since ECX has type Int(i), we know the value of this offset is i * 8. Since the name \nITABLEENTRY(a)always refers to a record with 2 .elds, amounting to 8 bytes of data total, we can deduce \nthat offset i* 8is accessing the .rst .eld of the ith index of the array. The constraint i<32+ l is in \nthe environment due to the earlier comparison, so we can ensure that this is a safe access into the array. \nIn order to load the .rst .eld of the ith index, we must open the name ITABLEENTRY(a), which results \nin the following existentially quanti.ed record type: R : HeapPtr(RUNTIME(\u00df)) .\u00df : \u00dfI,a \u00ab \u00df. R : IMTableOffset(a,\u00df) \n\u00df represents the interface corresponding to that index, and the vari\u00adable is pulled out into the environment. \nThe constraint \u00dfI indicates that \u00df is an interface, and the constraint a \u00ab \u00df indicates that a implements \n\u00df. The .rst .eld has type HeapPtr(RUNTIME(\u00df)), indicating it is the runtime type of \u00df. Instruction 10) \ncopies this type into EDX and opens the name RUNTIME(\u00df)to a record type as usual. Furthermore, we tag \nthe extended-record type for the inter\u00adface table with[last : i . \u00df], indicating that the last index \naccessed was i and \u00df is the interface corresponding to that index. This is a feature of iTalX (not mentioned \nearlier) used speci.cally for any extended-record types whose .xed-length name opens to an exis\u00adtentially \nquanti.ed type, the above case being the most important example. The purpose of this feature will be \ndemonstrated later. Instruction 11) compares the runtime type of IList with the runtime type in EDX corresponding \nto interface \u00df. The state type is augmented to note that the condition code results from comparing these \ntwo types. Figure 2 shows the entire existentially quanti.ed state type after instruction 11). Instruction \n12) breaks from the loop if the two runtime types are the same. Otherwise, we proceed to instruction \n13). No useful information isgained from knowing that the runtime types differ, so the state type stays \nthe same except that the condition code is forgotten. Instruction 13) adds 1 to ECX, so ECX is given \nthe type Int(i +1). Instruction 14) jumps back to instruction 8), so we simply check that whether the \ncurrent type is a subtype of the type before instruction 8), which is the case due to our earlier shortcut. \nInstruction 12) could also break from the loop and proceed to instruction 16). This can only happen if \nthe runtime type of IList matches the runtime type of \u00df. From this, iTalX infers that these two class \ntypes are equal and merges them, essentially substituting all uses of \u00df in the state type with IList. \nIn partic\u00adular, the tag we added earlier to the interface table s extended\u00adrecord type becomes [last \n: i . IList], so that we know that index i corresponds to IList. Thus, when instruction 16) loads the \nsecond .eld of index i into EDX, the type of that .eld is IMTableOffset(a,IList). This type represents \nthe integer offset which, when added to HeapRecPtr(VTABLE(a): {... } + 0), results in HeapPtr(IMTABLE(a,IList)), \nwhich is precisely the effect of instruction 17). IMTABLE(a,IList)is the name of the record containing \na s implementations of IList s methods, all of which expect an instance of a subclass of a as the this \npointer. Instruction 18) fetches the .rst .eld from this record, which our metainformation informs us \ncorresponds to getCount. Thus, after all this effort, iTalX is .nally able to type check a call to getCount \nusing the original instance as the this pointer.   \n\t\t\t", "proc_id": "1806596", "abstract": "<p>A certifying compiler preserves type information through compilation to assembly language programs, producing typed assembly language (TAL) programs that can be verified for safety independently so that the compiler does not need to be trusted. There are two challenges for adopting certifying compilation in practice. First, requiring every compiler transformation and optimization to preserve types is a large burden on compilers, especially when adopting certifying compilation into existing optimizing non-certifying compilers. Second, type annotations significantly increase the size of assembly language programs.</p> <p>This paper proposes an alternative to traditional certifying compilers. It presents iTalX, the first inferable TAL type system that supports existential types, arrays, interfaces, and stacks. We have proved our inference algorithm is complete, meaning if an assembly language program is typeable with iTalX then our algorithm will infer an iTalX typing for that program. Furthermore, our algorithm is guaranteed to terminate even if the assembly language program is untypeable. We demonstrate that it is practical to infer such an expressive TAL by showing a prototype implementation of type inference for code compiled by Bartok, an optimizing C# compiler. Our prototype implementation infers complete type annotations for 98% of functions in a suite of realistic C# benchmarks. The type-inference time is about 8% of the compilation time. We needed to change only 2.5% of the compiler code, mostly adding new code for defining types and for writing types to object files. Most transformations are untouched. Type-annotation size is only 17% of the size of pure code and data, reducing type annotations in our previous certifying compiler [4] by 60%. The compiler needs to preserve only essential type information such as method signatures, object-layout information, and types for static data and external labels. Even non-certifying compilers have most of this information available.</p>", "authors": [{"name": "Ross Tate", "author_profile_id": "81392610098", "affiliation": "University of California, San Diego, San Diego, CA, USA", "person_id": "P2184611", "email_address": "", "orcid_id": ""}, {"name": "Juan Chen", "author_profile_id": "81100119052", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P2184612", "email_address": "", "orcid_id": ""}, {"name": "Chris Hawblitzel", "author_profile_id": "81100064145", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P2184613", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1806596.1806644", "year": "2010", "article_id": "1806644", "conference": "PLDI", "title": "Inferable object-oriented typed assembly language", "url": "http://dl.acm.org/citation.cfm?id=1806644"}