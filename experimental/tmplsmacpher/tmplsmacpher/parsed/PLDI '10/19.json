{"article_publication_date": "06-05-2010", "fulltext": "\n A Context-free Markup Language for Semi-structured Text Qian Xi Princeton University qxi@CS.Princeton.EDU \nAbstract An ad hoc data format is any nonstandard, semi-structured data for\u00admat for which robust data \nprocessing tools are not easily available. In this paper, we present ANNE, a new kind of markup language \ndesigned to help users generate documentation and data process\u00ading tools for ad hoc text data. More speci.cally, \ngiven a new ad hoc data source, an ANNE programmer edits the document to add a number of simple annotations, \nwhich serve to specify its syntac\u00adtic structure. Annotations include elements that specify constants, \noptional data, alternatives, enumerations, sequences, tabular data, and recursive patterns. The ANNE \nsystem uses a combination of user annotations and the raw data itself to extract a context-free grammar \nfrom the document. This context-free grammar can then be used to parse the data and transform it into \nan XML parse tree, which may be viewed through a browser for analysis or debugging purposes. In addition, \nthe ANNE system generates a PADS/ML de\u00adscription [19], which may be saved as lasting documentation of \nthe data format or compiled into a host of useful data processing tools. In addition to designing and \nimplementing ANNE, we have de\u00advised a semantic theory for the core elements of the language. This semantic \ntheory describes the editing process, which translates a raw, unannotated text document into an annotated \ndocument, and the grammar extraction process, which generates a context-free grammar from an annotated \ndocument. We also present an alter\u00adnative characterization of system behavior by drawing upon ideas from \nthe .eld of relevance logic. This secondary characterization, which we call relevance analysis, speci.es \na direct relationship be\u00adtween unannotated documents and the context-free grammars that our system can \ngenerate from them. Relevance analysis allows us to prove important theorems concerning the expressiveness \nand utility of our system. Categories and Subject Descriptors D.3.m [Programming lan\u00adguages]: Miscellaneous \nGeneral Terms Languages, Algorithms Keywords Domain-speci.c Languages, Tool Generation, Ad Hoc Data, \nPADS, ANNE 1. Introduction The world is full of ad hoc data formats those nonstandard, semi-structured \ndata formats for which robust data processing tools are not easily available. Examples of ad hoc data \nformats include the billions of log .les that are generated by web servers, .le Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. PLDI 10, June 5 10, 2010, Toronto, Ontario, \nCanada. Copyright c . 2010 ACM 978-1-4503-0019-3/10/06... $10.00. David Walker Princeton University dpw@CS.Princeton.EDU \nservers, billing systems, network monitors, content distribution sys\u00adtems, and other applications that \nrequire monitoring, debugging or supervision. The data analysts and programmers who .nd them\u00adselves working \nwith ad hoc data formats waste signi.cant amounts of time on various low-level chores like parsing and \nformat trans\u00adlation to extract the valuable information they need from their data. Making these tasks \nmore dif.cult is the fact that many ad hoc data sets have limited or out-of-date documentation. Moreover, \nthese data formats evolve, so documentation that is up-to-date one month may be deprecated the next. \nIn the past, two starkly different research communities, the programming languages (PL) community and \nthe machine learning (ML) community, have attempted to apply their technologies to help solve the problem \nof using ad hoc data .les productively. PL Solutions. In the programming languages community, work has \ncentered on the development of a variety of domain-speci.c languages that allow data analysts to both \ndocument and program with their ad hoc data. Examples of such languages include DEME-TER [18], PACKETTYPES \n[21], DATASCRIPT [3], PADS [9, 19] and BINPAC [25]. When used for documentation purposes, these lan\u00adguages \nprovide a means to write clear, concise and declarative spec\u00adi.cations of a data source s syntax and \nimportant semantic proper\u00adties. Moreover, the fact that the documentation produced is exe\u00adcutable (i.e., \nthere exist tools for checking that ad hoc data sources adhere to the format speci.cation given) means \nthat there is an au\u00adtomatic way to check whether documentation is up to date or falling behind. When \nused for programming support, these languages and their associated compilers provide a means to generate \na variety of useful programming libraries for manipulating ad hoc data includ\u00ading parsers, printers, \nand end-to-end data processing tools. While these language-based solutions have many useful, even essential \nfeatures, there is still room for improvement. In particular, producing descriptions of unknown data \nsources is still a somewhat tedious, time-consuming and error-prone process. For instance, ex\u00adperiments \nwith the PADS system1 suggest that expert users can cre\u00adate descriptions for many simple line-based system \nlogs in roughly one to two hours, on average, and sometimes less than that. Be\u00adginners take substantially \nlonger often a day or two to read rele\u00advant parts of the manual, .gure out the syntax, grasp the meaning \nof various error messages and complete a robust description. For more complicated data sources, and especially \nfor data sources of massive size, the process of creating descriptions becomes substan\u00adtially more dif.cult, \neven for experts. Kathleen Fisher reported that she struggled off-and-on for three weeks in her attempts \nto describe one particularly massive data .le at AT&#38;T that had the unfortunate property of switching \nformats after a million and a half lines.2 ML Solutions. On the other end of the spectrum, the machine \nlearning community has sought to tame ad hoc data sources by developing algorithms for analyzing complex \ndata sources and 1See table 2, page 10 of earlier work on PADS [11] for anecdotal evidence regarding \ncreation of descriptions for a variety of simple system log for\u00admats. 2 Personal communication, 2008. \n either automatically extracting key bits of information from the data sources in question [28, 16, \n2, 5] or inferring a grammar that describes them [13, 6, 23, 29, 15, 12, 22, 26, 11]. Whereas the programming \nlanguages approaches incur some signi.cant start-up cost, the machine learning approaches usually require \nless initial work by the programmer. For example, in su\u00adpervised learning approaches, users must label \nsome subset of their data to indicate the content of interest. Then, various machine learn\u00ading algorithms \ncan be used to learn the features of the labelled data in order to be able to extract it from its context.Naturally, \nif a lot of labelling is required of a machine learning approach, then it too has a substantial start-up \ntime, perhaps even more than that of a PL approach. A great deal depends upon the domain in which each \napproach is used and the speci.cs of the approach itself, but once a machine learning model has been \nset up in one domain, it can reduce the start-up time of other learning tasks in the same domain to some \ndegree. Even better, unsupervised approaches re\u00adquire no initial user input. They merely analyze a given \ndataset, uncover patterns and produce a synthesized grammar. In principal, perfect grammatical inference \nis impossible [13] but, nevertheless, researchers such as Stolke and Omohundro [29] have shown em\u00adpirically \nthat one can sometimes synthesize useful grammars using statistical techniques and heuristic search. \nWhile fully automated approaches involving machine learning are usually easy to try, they often suffer \nfrom the joint problems of producing unreliable results and having those results hard to un\u00adderstand \nor analyze. By unreliable results, we do not mean unsound results rather we mean that the grammars produced \nmay not be particularly compact or well-organized. Moreover, even when an automated system performs perfectly \nin a structural sense, it will generate a description teaming with machine-generated names for data subcomponents \nsuch as Union 237 or Enum 99. Such descriptions are naturally dif.cult for people to use and require \na human post-processing pass to add semantically meaningful iden\u00adti.ers. Yet another dif.culty with fully \nautomatic grammar induction is that it appears dif.cult to design a single system that operates well \nover a broad range of domains. For example, experience with the LEARNPADS system [11] suggests that though \nit works well for the sorts of systems log .les on which it has been tuned, it can eas\u00adily be thrown \noff when it encounters data outside its domain. In this latter case, it often generates far more complex, \ndif.cult-to-read and dif.cult-to-use descriptions than a human would. This problem commonly occurs when \nthe data in question depends on some new basic format element a new sort of date representation, a different \nway of formatting phone numbers, etc. Humans draw upon their worldly experience to identify, modularize, \nand especially, name the new element effectively whereas the LEARNPADS algorithms are often unable to \ntease apart the details of the new element from the rest of the description and they certainly cannot \nchoose a rea\u00adsonable name for it. Hence, even though LEARNPADS, and other systems like it, can certainly \nbe improved, the overall approach has some fundamental limitations. 1.1 ANNE: A New Approach Given the \nchallenges faced by both traditional ML approaches and traditional PL approaches, we have developed a \nnew system, called ANNE, to help improve the productivity of programmers who need to understand, document, \nanalyze and transform ad hoc text data. In particular, we have focused on text data organized in line-by\u00adline \nor tabular formats, as this is the most common sort of layout in systems log .les and a variety of other \ndomains. However, in principle, our techniques are suf.ciently general to handle any data format that \ncan be described as a context-free grammar. Rather than requiring programmers to write complete data \nde\u00adscriptions, as in the conventional PL approach, or simply accepting the unvarnished results of a fully \nautomatic, heuristic algorithm, as in the conventional ML approach, ANNE combines ideas from both communities \nin search of the best of all worlds. To be more speci.c, the process of generating a description for \na text document begins by having the user edit the text itself to add annotations that help describe \nit. These annotations, and the surrounding unanno\u00adtated text, are used to generate a human-readable PADS \ndescription. The PADS description may then be fed through the PADS compiler, generating a host of useful \nartifacts ranging from programming li\u00adbraries for parsing, printing and traversal to end-to-end tools \nfor format-conversion, querying, and simple statistical analysis. In ad\u00addition to generating a PADS description, \nthe system will translate the text data into a structured XML parse tree. The XML parse tree can be viewed \nthrough a browser, analyzed and used for debugging purposes. In a word, with help of programmer annotations, \nANNE will translate the original text data into a set of data description end products. The annotations \nthat constitute the ANNE language perform a number of different roles including each of the following: \n associating user-friendly names with bits of text or descriptions generated from sub-documents  de.ning \natomic abstractions such dates, ip addresses, times, and urls using regular expressions,  identifying \nsequences, constants and enumerations,  delimiting tabular data and its headers,  relating different \nvariants of a .eld to one another, and  introducing recursive descriptions.  Together this set of annotations \nis both convenient and powerful, and overall, the bene.ts of this new approach are numerous. First, as \nin the PL approaches, ANNE provides the user with great control over the resulting description, when \nthey want it. The user can introduce meaningful, human-readable names, identify the correct atomic abstractions, \nand shape key parts of the grammar however they desire. Second, again as in the PL approaches, ANNE is \nextremely powerful. For example, ANNE easily supports tables and recursive grammars even though identifying \ntables in text data is a dif.cult machine learning challenge [22, 26, 17] and learning context-free grammars \nis even harder than the already-hard challenge of learn\u00ading regular expressions. LEARNPADS supports neither \nof these features. Third, as in the ML approaches, less work is required of the pro\u00adgrammer. Importantly, \nunannotated text in the surrounding context is used to .ll in the blanks left in a description using \nvarious de\u00adfault mechanisms. This means that the programmer does not have to, and is not encouraged to, \nwrite the entire description. Hence, in somerespects, ANNE resemblesasupervisedlearningapproachex\u00adcept \nthat rather than using simple labels to identify important data, ANNE uses more powerful, higher-level \ncommands. Fourth, the annotation language has small number of constructs in it and, perhaps more subjectively, \nwe .nd it is relatively easy to use. Ease of use comes from the fact that programmers can stare directly \nat the text they are interested in and directly wrap an annotation around it to capture it. There is \nno counting of .elds or the possibility of off-by-one errors. In this way, the system supports a what-you-annotate-is-what-you-get \nstyle of interaction. The XML-generation tool provides immediate feedback and facilitates debugging. \nIn addition to designing and implementing ANNE, we have developed an elegant theory to explain its semantics. \nThis theory is based around IDEALIZED ANNE (IA for short), an idealized  207.136.97.49 --[15/Oct/1997:18:46:51 \n-0700] \"GET /turkey/amnty1.gif HTTP/1.0\" 200 3013 207.136.97.49 --[15/Oct/1997:18:46:51 -0700] \"GET /turkey/clear.gif \nHTTP/1.0\" 200 76 polux.entelchile.net --[15/Oct/1997:21:02:07 -0700] \"GET /latinam/spoeadp.html HTTP/1.0\" \n200 8540 152.163.207.138 --[15/Oct/1997:19:06:03 -0700] \"GET /images/spot5.gif HTTP/1.0\" 304 \u00adip160.ridgewood.nj.pub-ip.psi.net \n--[15/Oct/1997:23:45:48 -0700] \"GET /whatsnew.html HTTP/1.0\" 404 168 ppp31.igc.org -amnesty [16/Oct/1997:08:40:11 \n-0700] \"GET /members/afreport.html HTTP/1.0\" 200 450 ... Figure 1. Excerpt from the web server log ai.3000. \ncore annotation calculus. The semantics of the IA programming process is given by a relation between \nannotated and unannotated documents and the semantics of IA itself is given by a function that generates \ncontext-free grammars from annotated documents. In order to understand the capabilities of IA in greater \ndepth, we prove theorems that characterize the kinds of grammars that can be generated by our system. \nIn doing so, we introduce an interesting new set of relations, inspired by relevance logic [1], that \nmore precisely de.ne the relationship between generated grammars and the data they describe. We use these \nrelations to prove important theorems concerning the expressiveness of our system. Contributions. To \nsummarize, this paper makes a number of ma\u00adjor contributions: We introduce a highly practical, new technique \nfor generation of format speci.cations from text data. We illustrate its use on a number of examples \nand evaluate its effectiveness.  We develop an idealized, core annotation calculus that captures the \nkey elements of our design. We give a semantics to the calculus to describe how ANNE programming and \ngrammar extraction works.  We introduce a secondary characterization of ANNE based on concepts drawn \nfrom relevance logic. We use this secondary characterization to analyze the expressive power of our system. \n We have implemented the system and combined it with the PADS language and compiler, allowing users \nof our system to easily generate useable documentation along with a suite of programming libraries and \nend-to-end data processing tools.  In the following section of the paper, we explain our language design \nand how to use it in more detail. In section 3, we develop the syntax and semantics IDEALIZED ANNE. In \nsection 4, we intro\u00adduce our relevance analysis and use it to prove key theorems about the expressiveness \nof IDEALIZED ANNE. In Section 5, we com\u00adment further on our experiences using ANNE to generate format \nspeci.cations and evaluate its effectiveness relative to both manual construction of PADS formats and \nthe grammar induction system developed in earlier work [11]. Section 6 describes related work and Section \n7 concludes. 2. ANNE by Example ANNE is a language and system for deriving grammatical speci.\u00adcations \nand text processing tools directly from example text .les. In this section, we will illustrate the basic \nfunctionality of the lan\u00adguage through a number of examples. 2.1 A Web Server Log Our .rst example involves \nthe problem of processing a web server log. We will be highlighting text added to the .le using a grey \nback\u00adground. The log itself is presented in Figure 1. System administra\u00adtors query, transform and analyze \nlogs just like this (and hundreds of variants thereof) as part of their day-to-day job of assessing the \nhealth and security of the systems they oversee. !# #include \"systems.config\" !# This step adds the \npreamble de.ned by the .le systems.config, which is presented in Figure 2. A con.g .le such as this is \ncom\u00adposed of a series of lines with one regular expression de.nition per line. Each line begins with \neither def or exp and is followed by a name and a regular expression. Those lines beginning with exp \nwill export the named regular expression so it can be used in describing formats. Those lines beginning \nwith def provide a local de.nition for the name. A local de.nition can be used in sub\u00adsequent defs or \nexps but is not in scope in the rest of the .le. Comment lines begin with a # symbol. The systems.config \n.le has been specially designed for system administrators dealing with log .les. Each new domain can \ncreate its own set of common, reusable data de.nitions to speed up data format construction. Introducing \nNonterminals. The next step is to identify, describe and give names to elements of interest in the .le. \nFor instance, a sysadmin might start with the .rst line after the preamble and begin to edit it as follows \n(though the annotation process can start at any place in the .le that happens to be convenient). To format \nlines within the boundaries of the narrow sigplanconf style, we will break lines where necessary with \na slash and continue them indented two spaces on the next line. {Record: 207.136.97.49 --\\ [15/Oct/1997:18:46:51 \n-0700] \\ \"GET /turkey/amnty1.gif HTTP/1.0\" 200 3013 } Intuitively, the simple annotation {Name: ...} \nbegins the pro\u00adcess of de.ning a scannerless context-free grammar. Note that if braces { and } already \nappear in the .le, a command line switch can alter the bracketing syntax. In this case, the portion of \nthe grammar so-de.ned involves a single nonterminal named Record. Moreover, since there are no other \nannotations to guide grammar generation, the system uses a simple default rule to gen\u00aderate the right-hand \nside it assumes the desired right-hand side is a simple concatenation of basic tokens derived by running \na default lexer over the data enclosed in braces. Record ::= Num . Num WS - WS - WS [ ... In order to \nmaintain predictability and ease-of-use, the set of default tokens has been kept to the barest minimum. \nIt includes numbers (Num integer or .oating point), punctuation symbols (e.g., [ or . or ] , etc.), \nwords (Word), and whitespace (WS). The default tokenization scheme can be overridden by extending the \npreamble with new programmer-de.ned tokens expressed as regular expres\u00adsions. However, doing so changes \nthe tokenization globally for the entire .le, which is not particularly useful here. Using the Preamble. \nInstead of overriding the preamble, we will take advantage of some of the regular expression de.nitions \nin systems.config to further re.ne the grammar for the Record nonterminal: {Record: {IP<: 207.136.97.49 \n} --\\ The Preamble. The .rst step in processing any log like this is to[ {Date<: 15/Oct/1997 } : {Time<: \n18:46:51 -0700 } ]\\ edit the .le at the top to add the following lines. \"GET /turkey/amnty1.gif HTTP/1.0\" \n200 3013} # Name Regular Expression def trip [0-9][0-9][0-9]\\|[0-9][0-9]\\|[0-9] def db [0-9][0-9] def \nzone [+-][0-1][0-9]00 def ampm am\\|AM\\|pm\\|PM ... exp Time {db}:{db}:{db}\\([ ]*{ampm}\\)?\\([ \\t]+{zone}\\)? \nexp IP {trip}\\.{trip}\\.{trip}\\.{trip} ... Figure 2. Excerpt from systems.config Above, we used several \nannotations with the form {Name<: ... } to introduce regular expressions named Name. For instance, we \nidenti.ed an ip address (IP), a date (Date) and a time (Time). All of these named regular expressions \nwere introduced in the pream\u00adble (by including their de.nitions from systems.config). Af\u00adter this re.nement, \nour generated grammar has the following form. IP ::= ... Date ::= ... Time ::= ... Record ::= IP WS - \nWS - WS [ Date : Time ] ... The right-hand sides of IP, Date and Time will be regular ex\u00adpressions de.ned \nby the preamble. Annotations for Termination Symbols. The next re.nement of the grammar involves dealing \nwith the string \"GET /turkey/ amnty1.gif HTTP/1.0\". In many applications, the internal structure of this \nstring might be irrelevant. If this is the case, one could simply wrap the contents of the string with \nan annotation of the form {Name>: ...}. In this case, Name introduces another nonterminal into the grammar \nand the greater-than sign indicates that the extent of nonterminal s reach is de.ned by a terminating \ncharacter the character that follows the close brace. Here is the annotation used in context: {Record:{IP<:207.136.97.49} \n--\\ [{Date<:15/Oct/1997}:{Time<:18:46:51 -0700}] \\ \" {Message>: GET /turkey/amnty1.gif HTTP/1.0 } \"\\ \n200 3013} ... In the text above, the > annotation introduces the Message non\u00adterminal and its extent \nis terminated by a quotation symbol. Such a token can easily be de.ned by a regular expression, but experience \nwith the PADS data description language [9] con.rms that this id\u00adiom is extremely common in all kinds \nof log .les. Building in this shorthand is a nice programmer convenience. Generating XML and Debugging \nResults. At this point, the programming burden has been minimal. It consists of including the preamble \nin the data source and writing .ve simple annota\u00adtions, which mainly involve naming key parts of the \ndata. All-in-all the job of describing the data may have taken a minute or two. To debug the work, one \ncan invoke the ANNE compiler, which will generate a number of artifacts, including a PADS description \nand an XML parse tree of the data. Viewing the XML through a browser, as shown in the screen shot in \nFigure 3, reveals that the grammar generated so far only covers a subset of the data in the .le colored \nlines indicate lines covered by the generated grammar and greyed out lines indicate lines that are uncovered. \nA quick examination of the .rst greyed out line indicates that there is more variation in the data .le \nthan had been apparent at .rst glance. Fortunately, generating a complete cover is relatively easy with \njust a few more annotations. Introducing Alternatives. Alternatives can be introduced into the grammar \nin several ways. The simplest way is merely to use a par\u00adticular nonterminal name repeatedly. We illustrate \nthis technique below by using the nonterminal Size twice, once around an in\u00adteger (which represents the \nnormal case the number of bytes re\u00adturned by the server is reported properly) and once around \"-\" (which \nrepresents the nonstandard case of no data available). {Record:{IP<:207.136.97.49} --\\ [{Date<:15/Oct/1997}:{Time<:18:46:51 \n-0700}] \\ \"{Message>:GET /turkey/amnty1.gif HTTP/1.0}\" 200 \\ {Size: 3013 } } ... 152.163.207.138 --\\ \n[15/Oct/1997:19:06:03 -0700] \\ \"GET /images/spot5.gif HTTP/1.0\" 304 {Size: -} Such annotations extend \nthe grammar with a union of two or more options: Size ::= Num + - Record ::= IP WS - WS - WS ... Size \n An alternative technique is to use a collection of annotations of the form {Name/Name1: ...} and {Name/Name2: \n...} and {Name/Name3: ...}, etc. as follows. {Record:{IP<:207.136.97.49} --\\ [{Date<:15/Oct/1997}:{Time<:18:46:51 \n-0700}] \\ \"{Message>:GET /turkey/amnty1.gif HTTP/1.0}\" 200 \\ {Size/S: 3013 } } ... 152.163.207.138 --\\ \n[15/Oct/1997:19:06:03 -0700] \\ \"GET /images/spot5.gif HTTP/1.0\" 304 {Size/Dash: -} This technique names \nthe alternatives and generates the following equivalent grammar. S ::= Num Dash ::= - Size ::= S + Dash \nRecord ::= IP WS - WS - WS ... Size One reason to use the more verbose form with named alternatives \nis that it will generate a nicer PADS/ML description for the user one that is an ML-style description \nand uses data type descriptions with well-named constructors (See Section 2.3). There is one other detail \nto consider when it comes to alter\u00adnatives: the most concise grammar is sometimes one in which al\u00adternatives \noverlap. PADS, and many other systems, use prioritized choice to disambiguate between overlapping alternatives. \nIn ANNE, priorities may be speci.ed as integers using a syntax with the form {Name1/Name2[priority]: \n...}. Finishing up the Web Log Example. With just a few more anno\u00adtations, the web log annotation job \nis complete. In total, it was nec\u00adessary to add the preamble and annotate four lines of text. Three of \nthe four lines only required annotating one bit of data. The whole process might have taken .ve minutes. \nThe resulting generated grammar is presented in Figure 4. Notice that by default, the top\u00adlevel nonterminal \nsymbol is Source and that the top-level gram\u00admatical rule is as follows. Source ::= Record (NL Record)* \nIn the line above, NL is the newline character and the asterisk is the familiar Kleene star. In other \nwords, the entire source is a sequence of Records separated by newline characters. In general, a programmer \ncan create annotations for any number of top-level items, which may be line-by-line descriptions or tables, \nand ANNE will produce a top-level grammar with the form Source ::= (Item1+...+Itemk) (NL (Item1+...+Itemk))* \n 2.2 Additional Language Features The web log document discussed in the previous subsection is one example \nof the sort of ad hoc data source that ANNE was designed  Figure 3. View of generated XML after partial \ndata description. S ::= Num Dash ::= - Size ::= S + Dash Sender ::= IP + Hostname ID ::= - + Word Record \n::= Sender WS - WS ID WS [ Date : Time ] WS \\\" Message \\\" WS Num WS Size Source ::= Record (NL Record)* \nFigure 4. Generated Grammar. Regular expression de.nitions of IP, Hostname, Date, Time, and Message are \nomitted. to service. It used a good number of different kinds of annotations, but there are a number \nof other features of the language, which we describe more brie.y in this section. Repetition. The web \nlog had an implicit, repeated structure at the top-level, but no internal repetition. Many other ad hoc \ndata .les do. To generate a grammar with a repeated sequence of items, one may use a starred annotation \nas in the following pipe-separated number sequence, which is drawn from one of our data sources: {Record*[|]: \n9152271|9152271|1|0|0|0|0|... } In the annotation above, Record names a part of a grammar involving a \nsequence of items in which each item is separated by a | symbol. By default, if there are no further \nannotations, the record element structure will be any character sequence not including the separator. \nNoBar ::= ... Record ::= (NoBar ( | NoBar)*)? Alternatively, the record elements can be speci.ed exactly \nusing the syntax {Name1/Name2*[sep]: ...}, as in the following example. {Record/Elem*[|]: 9152271| {Elem: \n9152271 } |1| \\ 0|0|0|0|... } The separator (de.ned in square brackets prior to the colon) is op\u00adtional \nand, if desired, the programmer can add an optional termina\u00adtor string. Optional data. Optional data \noccurs often. The annotation {Name?: ...} de.nes Name to either be formatted as the grammar gener\u00adated \nby ... or the empty string. Constants and Enumerations. In order to specify a nonterminal that has a \nconstant value, as opposed to generating a more liberal grammar, one can uses an equality annotation \n{Name=: ...} or its unnamed variant {=: ...}. Sometimes the nonterminal may contain a small number of \nconstant values instead of a single one, and then, to generate a grammar involving the list of constants \nthat {E#h:Name GP Goals Assists Points +/-Jason Blake, 78 25 38 63 -2 Alexei Ponikarovsky, 82 23 38 61 \n6 ...} Name GP Goals Assists Points +/-Alexander Ovechkin, 79 56 54 110 10 Nicklas Backstrom, 82 22 66 \n88 3 ... Figure 5. Fragment of an annotated document containing NHL player statistics from the 2008-2009 \nseason, one table per team. ptype IP = Pstring_ME(...) ptype Hostname = Pstring_ME(...) ... ptype Size \n= S of Num | Dash of - ptype Sender = IP of IP | Hostname of Hostname ptype ID = - | Num ptype Record \n= Sender * WS * - * WS * ID * WS * [ * Date * : * Time * ] * WS * \\\" * Message * *** \\\" WS Int Size ptype \nSource = Record plist(No_sep, No_term) Figure 6. PADS/ML description generated from annotated web log. \nRegular expression de.nitions of IP, Hostname, etc. are omitted. actually appear in the .le, one can \nuse an enumeration annotation. An enumeration is written as {Name//enum>:...}. It generates an initial \ngrammar in the same way that our termination symbol speci.cation generates a grammar (by looking for \na terminating symbol). That initial grammar is used to parse the document at hand and collect all strings \nthat match the spec in the document. The .nal grammar is one de.ned using the instances that match. Tables. \nThe last important feature of ANNE involves tables. Even though tables can be speci.ed using concatenation \nand Kleene star, it is worthwhile building special support for them as they appear frequently. Identifying \ntables is a useful programmer convenience and also makes it easier to generate a good query interface \nfor the data. Figure 5 shows a small portion of a document containing a se\u00adries of tables describing \nNHL player statistics, with one table per NHL team. Hockey a.cionados use such data regularly to compute \nplayer values and argue important points such as Is Crosby bet\u00adter than Ovechkin? or Was John Ferguson \nJunior the worst Leafs GM since the early 80s? Tables such as the ones displayed here often have a header \nrow followed by some number of rows with a .xed number of columns. Using ANNE, deriving a grammar for \nsuch a table simply involves using one of the hash annotations, ei\u00adther {Name#: ...} or {Name#h: ...}. \nThe h in the second variant indicates that the table has a header row that varies in struc\u00adture from \nthe table data. The number and structure of the columns is determined by counting the number of each \nsort of token in ev\u00adery line. If some token t appears k times in every line then there are k +1 columns \nand t serves as the separator between columns. If more than one token satis.es this property, one such \ntoken is se\u00adlected heuristically (tokens that serve frequently as separators such as tab, comma, and \nvertical bar are prioritized). However, the pro\u00adgrammer is free to specify the separator in question \nexplicitly using square braces as in the Kleene star annotations.  Assertions. In a number of situations, \nand particularly when data is recursive, it is useful for a programmer to be able to assert that some \npart of the data satis.es a nonterminal de.nition without go\u00ading to the trouble of annotating all its \nsubparts. We allow such as\u00adsertions through annotations with the form {Name!:...}. For ex\u00adample, given \na simple string of parentheses such as (((()))) , the simplest way to annotate the data is as follows. \n {Parens?: ( {Parens!: ((())) } ) } An annotation associates the data enclosed in braces with a non\u00adterminal \nname, but it doesn t generate a grammar rule for the non\u00adterminal. Thus, the above annotation scheme \nwill give rise to the following grammar. Parens ::= ( ( Parens ) )?  2.3 Generating PADS Descriptions \nIn the previous subsection, we explained the semantics of the ANNE language by presenting the context-free \ngrammars that are generated from each annotation scheme. These context-free gram\u00admars are used to parse \nthe data source and generate an XML parse tree that can be viewed through a browser or processed using \nany one of a number of XML-based tools, languages or libraries. In addition to generating structured \nXML, an ANNE mark-up will also generate a PADS description [9, 10, 19]. The PADS de\u00adscription language \nuses augmented type declarations to describe the syntactic structure of a document as well as the programming \nlanguage data structures one generates by parsing the document. Figure 6 shows the PADS description generated \nfrom the annotated web log presented in Figure 4. A PADS description such as the one in Figure 6 can \nserve as per\u00admanent executable documentation for the data source. It can also be used to generate a variety \nof libraries such as parsers, print\u00aders, and traversal functions for processing other data sources with \nthe same format. Finally, the PADS compiler can link generated li\u00adbraries against various generic tools \nincluding a query engine [8], data synchronization engine [7], and various format translators. Consequently, \nwhile using ANNE is a quick and simple process, the result of this minimal bit of labour is an enduring \npiece of human\u00adreadable documentation (the PADS description) and a valuable col\u00adlection of reusable tools. \n3. IDEALIZED ANNE The previous section introduced ANNE through a series of exam\u00adples, but did not answer \nany general questions about the principles involved in the language design: What do these annotations \nmean? What grammars do they generate? When do we have suf.cient data to generate a particular grammar? \nIn this section, we make some initial headway towards answering these more general questions by de.ning \nthe syntax and semantics of IDEALIZED ANNE (IA), a simpli.ed variant of the full ANNE language that encapsulates \nits Regular Expressions: b ::= E | c | b.b | \u00b7 \u00b7\u00b7 Annotated Documents: ad ::= v | ad1ad2...adn |{ad}|{[b]: \nv}|{A : ad}|{A/inl : ad}|{A/inr : ad}|{A/Aelem * : ad}|{A/Aelem *0 :}|{A!: ad} Figure 7. IDEALIZED ANNE \ndocuments. Nonterminal Clauses: s ::= b | A | s1 \u00b7 s2 \u00b7 ... \u00b7 sn Nonterminal Right-hand Sides: r ::= \ns | s1 + s2 | ?+ s2 | s1 +? | A*| A* 0 Nonterminal De.nitions: G ::= [] | G[A = r] Grammars: gram ::= \n(A, G) Figure 8. Grammar Syntax. essential features. IA doesn t re.ect features of optional data, con\u00adstants \nand enumerations, tables, while most of these features can be represented by essential features in IA. \n 3.1 IDEALIZED ANNE Syntax and Programming In the following formal work, we will let c range over characters \nwhile v and w range over strings (our unannotated documents). We let denote the empty string and v1v2 \ndenote the concatenation of two strings. Meta-variable A ranges over nonterminal names and b ranges over \nregular expressions. We write L(b) to denote the language of regular expression b. Regular expressions \nwith an empty language are prohibited. Syntax. The syntax of annotated documents is de.ned in Fig\u00adure \n7. An annotated document may either be unannotated (v) or a sequence of annotated documents (ad1ad2...adn). \nOther annota\u00adtions include the following. {ad} identi.es a sub-document  {[b]: v} identi.es the data \nv as inhabiting the language of regular expression b.  {A : ad} assigns a nonterminal A to the format \ninferred from annotated sub-document ad.  {A/inl : ad} and {A/inr : ad} introduce the left-and right\u00adhand \nelements of a union respectively  {A/Aelem* : ad} introduces a repetition named A with ele\u00adments named \nAelem. Sub-document ad is used to infer Aelem. {A/Aelem*0 :} is a related annotation, added to the calculus \nto simplify certain inductive proofs. It need not be used by pro\u00adgrammers. It s semantically equivalent \nto E.  {A!: ad} claims that the data ad has the format given by A without checking. Sub-document ad \nmay be used to infer other parts of the grammar.  The programming process. In order to use IDEALIZED \nANNE, a programmer need simply apply some collection of annotations to their data. This programming process \nis formalized by a judgement written v . ad, which relates an unannotated document v to any  v . ad \n(a-none) v . v vi . adi i =1..n (a-con) v1v2...vn . ad1ad2...adn v . ad v .L(b) (a-group)(a-re) v .{ad} \nv .{[b]: v} v . ad (a-name) v .{A : ad} v . ad v . ad (a-inl)(a-inr) v .{A/inl : ad} v .{A/inr : ad} \nv . ad (a-rep) v .{A/Aelem* : ad} (a-rep-empty) .{A/Aelem *0 :} v . ad (a-assert) v .{A!: ad} Figure \n9. Document annotation. one of its annotated variants ad. Figure 9 presents the annotation rules. For \ninstance, rule (a-none) says that annotating a document can involve doing nothing. Rule (a-con) says \nthat annotating a document can involve subdividing the document into arbitrarily many subpieces, each \nof which is recursively annotated. All of the other rules simply wrap one of the particular annotation \nforms around a sub-document (usually after recursively annotating the sub-document, except for repetitions \nand assertions).  3.2 Grammars Syntax. The purpose of IDEALIZED ANNE is to generate gram\u00admars of the \nform given in Figure 8. Reading from the bottom of the .gure towards the top, one sees that a grammar \nis a pair of a start nonterminal A and .nite partial map G from nonterminal names to right-hand sides. \nA right-hand side may be a clause s, a union of clauses (s1 + s2) or a repetition of some nonterminal \nA*. A right\u00adhand side may also be one of three partial right-hand sides: ( ?+ s) or (s +?) or A* 0 (other \nright-hand sides are called complete). Intu\u00aditively, the ? symbol represents a missing part of the grammar, \nand both ? and * 0 symbols indicate that no underlying data is recog\u00adnized by that part of the grammar. \nPartial right-hand sides appear during the course of constructing a grammar (or inductively in the midst \nof our proofs), but should not appear in any .nal result. A clause (s) is either a regular expression \n(b), a nonterminal (A), ora sequence of clauses s1 ...sn. Semantics. The semantics of grammars is de.ned \nby the judge\u00adment f v . gram, which depends upon judgements G f v . r and G fc v . s. Intuitively, the \nlatter two may be read string v is in the language of r (or s) when nonterminals are de.ned by G. The \nrules de.ning this judgement are presented in Figure 10. Many of these rules are self-explanatory. For \ninstance, rule g-name states G fc v . s v .L(b) G(A)= r G f v . r (g-re)(g-name) G fc v . b G fc v . \nA G fc vi . si i =1..n (g-con) G fc v1v2...vn . s1 \u00b7 s2 \u00b7 ... \u00b7 sn G f v . r G fc v . s (g-clause) G \nf v . s G fc G fc v1 . s1 v2 . s2 (g-sum1)(g-sum2) G f v1 . s1+? G f v2 . ?+ s2 G fc G fc v1 . s1 v2 \n. s2 (g-sum3)(g-sum4) G f v1 . s1 + s2 G f v2 . s1 + s2 G fc vi . Ai =1..n (g-rep)(g-rep-emp) G f v1v2...vn \n. A* G f . A*0 f v . gram G f v . A (g-gram) f v . (A, G) Figure 10. Semantics of Grammars. that a string \nis in the language of A provided it is in the language of its de.ning right-hand side. In rule g-rep, \na sequence of strings is recognized. In a slight abuse of notation, we allow n to be 0, in which case \nwe interpret the rule to say that the repetition recognizes the empty string. The only unusual rules \nare the rules for the partial right-hand sides. The rules for partial unions s +? and ?+ s state a value \nis in their language provided it is in the known alternative s. The rule for partial repetitions A*0 \nstates that the empty string is in its language.  3.3 Grammar Extraction Once a document has been annotated, \nthe IDEALIZED ANNE run time system can extract a grammar from it. This extraction process is implemented \nby recursively traversing the annotated document and extracting partial grammars from the subpieces. \nA .nal gram\u00admar results from fusing (i.e., combining in a special way) collec\u00adtions of partial grammars. \nWe will de.ne the fusion relation (written G1 . G2) in a mo\u00adment, but .rst we will direct the reader \ns attention to Figure 11, which presents the grammar extraction function itself. This func\u00adtion, written \nad . (s, G), analyzes annotated document ad and generates a clause s as well as partial grammar G to \ndescribe it. The .rst rule in the extraction de.nition (p-none) explains how unannotated data will generate \na description. This occurs by .nd\u00ading a sequence of regular expressions that matches the data. These \nregular expressions are drawn from the default set D. The default set for our implementation contains \nbasic tokens such as numbers, words, whitespace and punctuation symbols. The choice of defaults is unimportant \nin the theory. Like Lexer, this tokenization pro\u00adcedure is deterministic, which means, for any string, \nIDEALIZED  ad (s, G) vi .L(bi) bi .D i =1..n (p-none) v1v2...vn (b1 \u00b7 b2 \u00b7 ... \u00b7 bn, []) adi (si, Gi) \ni =1..n (p-con) ad1ad2...adn (s1 \u00b7 s2 \u00b7 ... \u00b7 sn, G1 . G2 . ... . Gn) ad (s, G) v .L(b) (p-group)(p-re) \n{ad} (s, G) {[b]: v} (b, []) ad (s, G) (p-name) {A : ad} (A, G . [A = s]) ad1 (s1, G1) (p-inl) {A/inl \n: ad1} (A, G1 . [A = s1+ ?]) ad2 (s2, G2) (p-inr) {A/inr : ad2} (A, G2 . [A =?+ s2]) ad (s, G) (p-rep) \n{A/Aelem * : ad} (A, G . [A = Aelem *]) (p-rep-emp) {A/Aelem * 0 :} (A, [A = Aelem * 0]) ad (s, G) (p-assert) \n{A!: ad} (A, G) Figure 11. Grammar Extraction. ANNE will produce exactly 1 concatenation of regular expressions \nb1, ..., bk. The next rule (p-con) explains how to handle a sequence of annotated sub-documents. In this \ncase, each sub-document is an\u00adalyzed recursively, producing a clause and a right-hand side. The result \nis a concatenation of clauses and a grammar formed by fus\u00ading together the generated subgrammars. Many \nof the other rules should now be relatively self-explanatory. However, the reader should take note of \nrules (p-inl) and (p-inr), as these rules are primary points where partial grammars are gen\u00aderated. Notice \nin particular that rule (p-inl) infers the shape of the left-hand side of a union from its sub-document, \nbut has no infor\u00admation about the right-hand side and hence leaves ? in its place. Rule (p-inr) behaves \nin a complementary fashion. In addition, rules (p-rep) and (p-assert) are other 2 rules that should raise \nthe reader s attention. Rule (p-rep) infers the top\u00adlevel structure of a repetition and fuses A = Aelem \n* with partial grammar G, whereas Aelem can be de.ned by any annotation anywhere, inside its sub-document \nor from other documents. The fusion of A = Aelem* with G or other grammar parts de.ned elsewhere will \nbring the de.nition of A together with the de.nition of Aelem to create a .nal grammar of the repetition. \nFor instance, recall the example in Section 2.2, Aelem, in this case called Elem, is de.ned relatively \ndeeply within the string that makes up the array. Rule (p-assert) discards the right-hand side generated \nfrom its sub-document and instead uses the speci.ed grammar symbol A. It doesn t generate any grammar \nrule, which means, the rule for nonterminal A is generated from elsewhere. Grammar fusion. Intuitively, \nfusing two right-hand sides to\u00adgether involves eliminating the ? symbols and replacing them with real \ngrammar parts. For instance, fusing (s1+ ?) with (? + s2) results in (s1 + s2). Fusing two grammars together \ninvolves tak\u00ading the union of the disjoint grammar parts and fusing together the right-hand sides of \nthe overlapping grammar parts. More for\u00admally, the right-hand side fusion relation r1 . r2 is de.ned \nas the symmetric closure of the following rules. r . r = r (s1 + ?) . (?+ s2)= s1 + s2 (s1 + s2) . (s1 \n+?) = s1 + s2 (s1 + s2) . (?+ s2)= s1 + s2 A *. A*0 = A* We have chosen the weakest possible fusion operation \n basing it upon syntactic equality of grammars. A stronger fusion operation could be based upon semantic \nequality of grammars, but this is an undecidable problem for context-free grammars. We chose the weak \nfusion operation because of its simplicity, predictability, ease of understanding and implementation, \nand, importantly, because it is effective in practice. Given the right-hand side fusion, we de.ne the \nfusion of two grammars G1 . G2 as follows. D(G) denotes the domain of gram\u00admar G (i.e., the set of de.ned \nnonterminals). 8 <G1(A) if A . D(G1) and A .. D(G2) G1.G2(A)= G2(A) if A . D(G2) and A .. D(G1) : G1(A) \n. G2(A) if A . D(G1) and A . D(G2) Finally, the fusion of two grammars with the same start symbol, (A, \nG1) . (A, G2), is de.ned to be (A, G1 . G2). Formalism vs. Implementation. The observant reader will \nnotice that the formal system requires a few more annotations be made ex\u00adplicit than the implemented \nsystem. In other words, for the sake of convenience and brevity, the implemented system performs some \nsimple annotation inference for the user in various situations. For example, consider the following text \nfragment. {Foo: 123 } {Foo: cat } In formal system, inconsistent right-hand sides, number vs. word, are \nrejected by the de.nition of fusion operator. In the implemented system, however, rather than fail and \nask the programmer to add more annotations, we infer inl and inr union annotations as follows: {Foo/inl: \n123 } {Foo/inr: cat } With these additional annotations in place, the formal system (and the implementation) \nwill successfully extract a union grammar. Another difference between formal system and implementation \nis that the implementation contains many complex annotations. However, these additional complex annotations \ncan be compiled into the lower-level annotations presented the formal system. As an example, consider \nthe repetition operator used in Section 2.2: {Record/Elem*[|]: 9152271| {Elem: 9152271 } |1| \\ 0|0|0|0|... \n} This idiom may be compiled into the following collection of anno\u00adtations drawn from the formal system: \n {NewRecord:{Record/NewElem*: 9152271| {NewElem:{Elem: \\ 9152271 } | } 1|0|0|0|0|...| }{Elem!: 0 }} \nThe extraction process will give rise to this grammar: Elem ::= Num NewElem ::= Elem | Record ::= NewElem* \nNewRecord ::= Record Elem 4. IDEALIZED ANNE Properties Now that we have de.ned the semantics of IDEALIZED \nANNE, we can answer some important questions about its properties and ex\u00adpressive power. For instance, \nsuppose one has some data v that inhabits the language of a grammar gram, is it always the case that \none can annotate v in such a way as to extract gram? Un\u00adfortunately, the answer to this question is no. \nThe simplest counter\u00adexample involves choosing the empty string as the data and a gram\u00admar (A, [A = E+Num]) \nas the target to extract in this example, there is no way to annotate the empty string to enable generation \nof the right side of the union. However, we can extract (A, [A = E]) an approximation of the grammar \nwe might have wanted. Intuitively, we can extract [A = E] but not [A = E + Num], because in the former \ncase all branches of the grammar are used during a parse of the empty string whereas in the latter case, \nsome branches (the Num branch) go unused during the parse. Hence, in order to better understand the grammars \nthat can be extracted from data, we need a theory that captures those parts of the grammar that are used \nduring recognition of a string. That theory is closely connected to the substructural logic known as \nrelevance logic. 4.1 Relevance Analysis Relevance Logic [1] is a simple logic requires every hypothesis \nbe used at least once during the course of a proof. If we think of gram\u00admar rules as hypotheses in a \nproof, we can develop an analogous theory in which each grammar rule, and all of its subparts, must be \nused at least once in the derivation that a string belongs to the grammar. Based on this intuition, we \nhave developed a relevance analysis that directly relates grammars to the values that can generate them. \nThe central judgements for this analysis have the form G frel v . r and G fc v . s. These judgements \naf.rm that all elements of rel G are used during the course of proving that v is an element of r and \ns respectively. A third judgement, frel v . gram, af.rms that all elements of gram are used during the \ncourse of proving v is in gram. Figure 12 presents the inference rules for these judgements. Rule (e-re) \nprovides an example of how these rules work. It states that v is recognized by b provided it is in L(b). \nMoreover, this rule uses no parts of a grammar. Hence, the grammar to the left of the turnstile must \nbe empty. Rule (e-name) states that if G is used in recognizing that v belongs to r then G . [A = r] \nis used in recognizing that v belongs to A. Rule (e-con) states that if G1 through Gn are used in recognizing \ns1 to sn then the grammar fusion is used to recognize the concatenation of clauses. It is also important \nto observe how the unions work. In particu\u00adlar, there are rules (e-sum1) and (e-sum2) to explain what \nthe partial right-hand sides ?+ s and s +? use, but there are no rules for the complete right-hand side \ns1 + s2. This is because no derivation can use both the left-hand side and the right-hand side of a union \nsimultaneously. The rules for repetitions are also interesting. Notice that the rule (e-rep) is constrained \nso that i is greater than 0. This guarantees that the underlying element grammar is used. The rule (e-rep-empty) \nis for the situation in which the empty string matches an iteration. The entire reason for including \nthe right-hand side A* 0 is to distinguish this case in which the underlying element type is not used. \nOur relevance analysis may be viewed as a relevance logic primarily because the structural rules for \nexchange and contraction are admissible but weakening is not.  4.2 Relevant Properties The key property \nof relevance analysis stems from the following essential property: if a grammar is relevant to a string \nthen a pro\u00adgrammer can use IDEALIZED ANNE to extract it from the string. In particular, the programmer \ndoes not need to annotate with asser\u00adtions. In other words, assertions are not essential features and \nthey are only introduced for annotation convenience. Theorem 1 (Relevance implies grammar extraction.) \ni. If G fc v . s, then there exists ad such that v . ad, rel ad (s, G) and the rule (a-assert) has never \nbeen used in the derivation of ad; ii. If G frel v . r, then for any A, there exists ad such that v . \nad, ad (A, G . [A = r]) and the rule (a-assert) has never been used in the derivation of ad; iii. If \nfrel v . gram, then there exists ad such that v . ad and ad gram. The theorem above states properties \nof a single string, but IDE-ALIZED ANNE can sometimes do more for us when there is more than one string \nto annotate. To make this idea precise, we .rst de\u00ad.ne what it means to extract a grammar from a collection \nof strings. De.nition 2 (Collective Extraction.) For grammar gram =(A, G) and data v1,v2, ..., vk, v1,v2, \n..., vk gram iff there exists ad1, ..., adk such that vi . adi, for all i =1, ..., k;  adi (si, Gi) \nfor all i =1, ..., k;  G1 . G2 . ... . Gk = G.  Next, we present the following theorem, which states \nthat no matter what data one has in hand, one can extract an approximation of any grammar for that data, \nwhere approximate grammars are de.ned as follows: (A, G1) is an approximation of (A, G) provided that \nthere exists G2 such that G = G1 . G2. We write (A, G1) = (A, G) when (A, G1) is an approximation of \n(A, G). Theorem 3 (Sound collective extraction.) Given some data v1,v2, ..., vk, if f vi . gram for all \ni, then there ' exists gram' such that v1,v2, ..., vk gram' and f vi . gramand gram' = gram. We proved \nTheorem 3 by showing that relevance analysis is a sound approximation of ordinary grammar recognition \nand then constructing the grammar gram' from the grammars gram1, ..., gramn that are relevant for each \ndatum v1,...,vn. To summarize, given a grammar gram and some data v, our theoretical analysis has told \nus two useful facts: (1) if gram is a grammar for v then IDEALIZED ANNE can extract some approxi\u00admation \nto gram, and (2) if gram is relevant for v then IDEALIZED ANNE can extract gram exactly. Given the second \npoint, one might say relevance analysis is a sound characterization of IDEALIZED ANNE. However, it is \nnot a complete characterization. There exist some unusual grammars that are not relevant for any data, \nbut can be extracted by IDEALIZED ANNE. In particular, IDEALIZED ANNE can extract grammars with disconnected \nnonterminals. One simple example is the grammar (A, [A = int, B = int]). This grammar is not relevant \nfor any data, but can be extracted from two example documents that each contain a single integer.  G \nfc rel v . s v .L(b) G frel v . r (e-re)(e-name) [] fc G . [A = r] fc rel v . b rel v . A Gi fc i =1..n \n rel vi . si (e-con) G1 . G2 . ... . Gn fcrel v1v2...vn . s1 \u00b7 s2 \u00b7 ... \u00b7 sn G frel v . r G fc rel v \n. s (e-clause) G frel v . s G fc G fc rel v1 . s1 rel v2 . s2 (e-sum1)(e-sum2) G frel v1 . s1+? G frel \nv2 . ?+ s2 Gi fc i =1..n n> 0 rel vi . A (e-rep) G1 . G2 . ... . Gn frel v1v2...vn . A* Table 1. Number \nof annotations, lines touched and time taken to (e-rep-empty) construct descriptions using ANNE. [] frel \n . A*0 Data Source # Annots # Lines Time(min) 1967Transactions 6 1 5 ai.3000 14 4 10 yum.txt 6 1 15 rpmpkgs \n2 1 1 railroad.txt 10 4 10 dibbler.1000 6 3 5 asl.log 7 2 5 scrollkeeper.log 4 1 3 page log 5 1 5 MER \nT01 01.csv 1 1 1 crashreporter.log 4 1 3 ls-l 4 2 5 windowserver last 5 1 10 netstat-an 10 3 10 boot.txt \n7 1 5 quarterlyincome 3 2 5 corald.log.head 3 2 5 irvpiv1.sel 7 1 15 latitude.txt 10 3 15 frel v . gram \nG frel v . A (e-gram) frel v . (A, G) Figure 12. Relevance analysis. 5. Evaluation We conducted a series \nof experiments to compare using ANNE against the process of writing PADS descriptions by hand and against \nthe process of learning descriptions automatically using LEARNPADS [11]. When comparing ANNE with hand-written \nde\u00adscriptions, we focused on the time and efforts users dedicated to creating descriptions; when comparing \nANNE with the LEARN-PADS system, we focus on the readability and compactness of descriptions. Our benchmark \nformats include 19 different ad hoc data sources, drawn mainly from various different kinds of system \nlogs. The same benchmarks have been used previously to evalu\u00adate the effectiveness of PADS and its variants \n[11]. Those readers interested in the speci.cs can .nd the benchmarks on the web [24]. Comparison with \nhand-written descriptions. In the .rst set of experiments, we measured the time and effort spent constructing \ndescriptions using ANNE. For each benchmark, Table 1 shows the total number of annotations the programmer \nneeded to construct the description (# annots), the total number of lines that were annotated (# lines) \nand the approximate time in minutes for the user to complete the description. The number of annotations \ndid not include the preamble or the regular expressions de.ned therein. The table shows that for most \nof our benchmarks, the user needed to insert anywhere from 1 to 14 annotations (with the median being \n5). On average, the user was required to annotate 3 or 4 lines of data. The time taken varied between \n5 and 15 minutes. In contrast, a previous study [11] of the time taken to write the same descriptions \nby hand showed users with some experience spent anywhere from 1/2 an hour to an hour or two. Part of \nthe reason users would take longer to write descriptions by hand is that they can add additional information \nin the form of constraints something that is not supported by ANNE right now. However, from Table 2. \nANNE (A) vs. LEARNPADS (L): Type complexity in bits and description size in lines. Asterisks indicate \nmeaningful qualitative differences in the performance of the two systems. Data source Type Complexity \nDesc. Size A L A L 1967Transactions 52 175 13 26 ai.3000 328 437 56 47 yum.txt* 84 640 17 74 rpmpkgs* \n7 314 4 70 railroad.txt * 89 975 28 150 dibbler.1000 76 85 21 25 asl.log 551 1545 78 102 scrollkeeper.log \n44 372 8 14 page log 206 729 23 22 MER T01 01.csv 96 211 22 12 crashreporter.log * 105 973 16 63 ls-l* \n195 721 25 80 windowserver last 148 85 24 11 netstat-an 822 1324 57 138 boot.txt* 98 944 19 123 quarterlyincome \n520 579 86 87 corald.log.head 793 1094 106 71 irvpiv1.sel* 284 1334 44 130 latitude.txt* 140 500 11 77 \n the experience of several PADS and ANNE programmers, the main reason is simply that ANNE is easier to \nuse. Comparison with LEARNPADS. Before comparing LEARN-PADS with ANNE in detail, we would like to review \nthe basics of how LEARNPADS works. LEARNPADS uses a multi-stage al\u00adgorithm to automatically discover \nthe structural information in the input data source according to which the format speci.cation, pre\u00adsented \nas a PADS description, is generated. During the .rst stage, LEARNPADS tokenizes the text data using a \n.xed set of base tokens. During the second stage, it analyzes the distribution of tokens found within \nthe data and infers a candidate grammar. Dur\u00ading the last stage, the system applies a set of rewriting \nrules to optimize the candidate structure according to the minimum de\u00ad  total 275528 drwxr-xr-x 3 dpw \nfac 4096 Jan 21 2005 as9 drwxr-xr-x 4 dpw fac 4096 Jan 21 2005 as8 -rw-r--r--1 dpw fac 15878 Jan 23 2002 \nasynch.txt drwxr-xr-x 2 dpw fac 4096 Jan 2 13:44 cv ... Figure 13. Excerpt from ls-l scription length \nprinciple, which balances speci.city of a grammar against compactness. Because of the nature of the second \nstage of the algorithm, LEARNPADS is incapable of inferring context-free grammars. Table 2 presents a \ndetailed comparison between the two sys\u00adtems. This table presents two metrics: the type complexity of \nthe resulting description and the number of lines of the resulting de\u00adscription when printed. The type \ncomplexity measures the number of bits it would take to encode the syntax of the PADS description. It \nis one of the metrics that the LEARNPADS system optimizes for. The number of lines of the resulting description \nis simply the number of lines of output from the respective pretty printers. Dif\u00adferences of 20% or so \nare usually meaningless in this table. On the other hand, differences on the order of a factor of 5 or \n10 are quite meaningful we placed asterisks in Table 2 to indicate those formats for which the differences \nbetween the results produced by ANNE and those by LEARNPADS were very signi.cant. Signi.cant differences \noccur for several reasons, but perhaps the most pervasive is that the performance of LEARNPADS is quite \nsensitive to the set of basic tokens (de.nitions of times, dates, ip addresses, etc.) that it starts \nout with. Unfortunately, while the LEARNPADS designers would like to create the perfect tok\u00adenizer, doing \nso for a broad set of formats is extremely dif.cult. As one begins to add more and more token de.nitions \nto the token set, the token de.nitions become mutually ambiguous with no ob\u00advious way to resolve the \nambiguities. For instance, the proper de.\u00adnition of URLs is incredibly broad and is ambiguous with just \nabout anything. Some date formats are ambiguous with URLs, .le paths, phone numbers, .oating point numbers \nor IP addresses. As a re\u00adsult, the LEARNPADS strategy has been to use a relatively simple default tokenizer. \nHowever, the consequence is that unanticipated token types will show up in data .les with some frequency \nand when this happens, LEARNPADS often produces overly complex grammars. ANNE does not suffer from this \nproblem because users can override the default tokenizer with local annotations whenever they need to. \nLEARNPADS will also do a suboptimal job learning some for\u00admats because the rewriting heuristics it uses \nfail. As an example, consider the ls-l data source presented in Figure 13. This data was obtained by \nexecuting Unix command ls -l. The dif.culty with this data .le is that it contains an insuf.ciently diverse \nset of ex\u00adamples from which to learn a accurate format. In particular, when LEARNPADS is applied to such \na data source, it fails to properly generalize in the following ways: The access control strings are \nturned in to an enumeration of four possibilities instead of a more general string description.  The \nowner and group .elds are turned in to constants dpw and fac.  There is a switch on the constant 4096 \nbecause that .le size happens to show up uncharacteristically often in the .le.  If the example data \nused was more varied, the learning system would work as expected. More generally, since LEARNPADS is \ndriven by a statistical analysis and heuristic rewriting rules, its results can be unpredictable, a problem \nthat ANNE does not have. In addition to sometimes undergeneralizing, LEARNPADS will sometimes overgeneralize. \nFor instance, Table 2 indicates that the LEARNPADS system produced a much smaller description than ANNE \nwhen applied to the windowserver last benchmark. This occurs because LEARNPADS uses a heuristic to simplify \ngrammars, and in this case, it over-simpli.ed, eliminating some useful information about the format. \nOf course, if the simple de\u00adscription was the desired one, it would have been possible for the ANNE programmer \nto generate it. 6. Related Work ANNE was designed to improve the productivity of data analysts by providing \na quick, simple way to generate documentation and data processing tools for an ad hoc data source given \nthe availability of example data. Many of its commands are directly inspired by the design of domain-speci.c \nlanguages and language extensions such as PADS [9, 10, 19], DATASCRIPT [3], PACKETTYPES [21], Demeter \n[18], BINPAC [25] and Erlang binaries [30, 14]. ANNE is not designed to work in the binary domain -it \nwill only work well when a human can stare at a data source, uncover it s structure, and add annotations \nin place. When it comes to the domain of semi-structured text data, ANNE provides an alternative to writing \nformat speci.cations (like PADS speci.cations) by hand. The main advantage of ANNE comes in its ease-of-use \nand ability to .ll in details such as separators, terminating characters, and members of an enumeration \nautomatically. Having a machine .ll in such details is both more convenient and less error-prone than \nmanually constructing the description. One limitation of ANNE right now is that it does not support the \nfull range of PADS features. In particular, it is missing dependency and constraints. We believe the \noverall ANNE framework can support these features; we are currently working on extending our theory and \nimplementation to include them. Potter s Wheel [27] is another system with some similarities to to ANNE \nin that it supports an interactive process to manage, clean and transform data. Unlike ANNE, it uses \na spreadsheet-style interface to represent classical relational data and its goal is to help users detect \nerrors and transform data to make it ready to load into a commercial database. Whereas Potter s Wheel \nis limited to managing relational tables, ANNE is designed for a broader range of context-free grammars; \nwhereas Potter s Wheel is an on-the-.y interactive transformation system, ANNE is a descriptive system \nthat produces documentation and programming tools for later use. Whereas Potter s Wheel operates over \nrelational data, many other data cleaning and transformation systems operate over XML. For example, SchemaScope \n[4] is a powerful new tool developed by Bex, Neven and Vansummeren to infer DTDs and XML Schemas from \nunknown XML documents and to visualize and edit exist\u00ading schema. The inference mechanisms used in SchemaScope \nare highly effective as they are tuned to common properties of DTDs [20]. Unfortunately, the grammar \ninference problem for ad hoc data sources is substantially different from the schema in\u00adference problem \nfor XML in part because the basic tokenization problem for ad hoc data is so ambiguous there is no standard \ntag-based syntax to delineate different parts of an ad hoc docu\u00adment. On the contrary, ANNE was created \nto provide a means for programmers to delineate and disambiguate elements of their data sources. The \nmachine learning community has developed a number of tools that perform wrapper induction, where a wrapper \nis a pro\u00adgram that can extract information from designated slots in a doc\u00adument or set of documents. \nTwo examples of such work are Kush\u00admerick s HLRT induction system [16] and Soderland s Whisk sys\u00adtem \n[28]. One high-level difference between a system like Whisk and one like ANNE or PADS is that Whisk is \ndesigned to work on data with very little regular structure. For example, the working example in Soderland \ns paper involved extraction of features such as price and location from Craigslist apartment advertisements. \nSuch advertisements are pseudo-English blurbs and have much less structure than web logs, for instance. \nHence, while Whisk and sim\u00adilar systems can be effective at solving the information extraction problem, \nthey are not designed to produce the kind of documenta\u00adtion or programming tools that ANNE is.  7. Conclusions \nIn this paper, we have presented the design and implementation of ANNE, a new kind of markup language \nfor text data. This markup language allows users to specify the syntactic structure of documents by adding \nannotations that indicate the presence of constants, enumerations, repetitions, optional data, tables, \nand recursive data. The markup language also allows users to import from libraries of pre-de.ned regular \nexpressions and to name parts of their data as they choose. A markup can be used to generate a context-free \ngrammar, an XML parse tree and a PADS description. The XML parse tree facilitates debugging and the PADS \ndescription serves as useful documentation that may be compiled into many more useful tools. Experience \nwith ANNE suggests that compact, human-readable descriptions can be constructed more quickly than by \nwriting PADS descriptions by hand and more reliably than using LEARNPADS. In addition, we have de.ned \nand analyzed the semantics of ANNE. In the process of doing so, we have uncovered a connec\u00adtion to relevance \nlogic, which we have used to prove illuminating theorems concerning the expressiveness of our system. \nAcknowledgments We would like to thank Kathleen Fisher, Nate Foster and Kenny Zhu for many fruitful conversations \non this topic. This material is based upon work supported by the NSF under grants 0612147 and 0615062 \nand by a gift from Google. Any opinions, .ndings, and conclusions or recommendations expressed in this \nmaterial are those of the authors and do not necessarily re.ect the views of the NSF or Google. References \n[1] A. R. Anderson, N. Belnap, and J. Dunn. Entailment: The Logic of Relevance and Necessity. Princeton \nUniversity Press, Princeton, NJ, 1975. [2] A. Arasu and H. Garcia-Molina. Extracting structured data \nfrom web pages. In SIGMOD, pages 337 348, 2003. [3] G. Back. DataScript -A speci.cation and scripting \nlanguage for binary data. In GPCE, volume 2487, pages 66 77. Lecture Notes in Computer Science, 2002. \n[4] G. J. Bex, F. Neven, and S. Vansummeren. SchemaScope: a system for inferring and cleaning xml schemas. \nIn SIGMOD, pages 1259 1262, 2008. [5] V. Crescenzi, G. Mecca, and P. Merialdo. Roadrunner: Towards automatic \ndata extraction from large web sites. In VLDB, pages 109 118, San Francisco, CA, USA, 2001. [6] F. Denis, \nA. Lemay, and A. Terlutte. Learning regular languages using rfsas. Theor. Comput. Sci., 313(2):267 294, \n2004. [7] M. Fernandez, K. Fisher, J. Foster, M. Greenberg, and Y. Mandel\u00adbaum. A generic programming \ntoolkit for PADS/ML: First-class upgrades for third-party developers. In PADL, pages 133 149, 2008. [8] \nM. F. Fern\u00b4andez, K. Fisher, R. Gruber, and Y. Mandelbaum. PADX: Querying large-scale ad hoc data with \nxquery. In PLANX, Jan. 2006. [9] K. Fisher and R. Gruber. PADS: A domain speci.c language for processing \nad hoc data. In PLDI, pages 295 304, 2005. [10] K. Fisher, Y. Mandelbaum, and D. Walker. The next 700 \ndata description languages. In POPL, 2006. [11] K. Fisher, D. Walker, K. Q. Zhu, and P. White. From dirt \nto shovels: Fully automatic tool generation from ad hoc data. In POPL, pages 421 434, Jan. 2008. [12] \nP. Garc\u00b4ia and E. Vidal. Inference of k-testable languages in the strict sense and application to syntactic \npattern recognition. IEEE Trans. Pattern Anal. Mach. Intell., 12(9):920 925, 1990. [13] E. M. Gold. Language \nidenti.cation in the limit. Information and Control, 10(5):447 474, 1967. [14] P. Gustafsson and K. Sagonas. \nAdaptive pattern matching on binary data. In ESOP, pages 124 139. Springer, Mar. 2004. [15] T. W. Hong \nand K. L. Clark. Using grammatical inference to automate information extraction from the Web. Lecture \nNotes in Computer Science, 2168:216+, 2001. [16] N. Kushmerick. Wrapper induction for information extraction. \nPhD thesis, University of Washington, 1997. Department of Computer Science and Engineering. [17] K. Lerman, \nL. Getoor, S. Minton, and C. Knoblock. Using the structure of web sites for automatic segmentation of \ntables. pages 119 130, New York, NY, USA, 2004. [18] K. J. Lieberherr and A. J. Riel. Demeter: A CASE \nstudy of software growth through parameterized classes. 1(3):8 22, August 1988. [19] Y. Mandelbaum, K. \nFisher, D. Walker, M. Fernandez, and A. Gleyzer. PADS/ML: A functional data description language. In \nPOPL, 2007. [20] W. Martens, F. Neven, T. Schwentick, and G. J. Bex. Expressiveness and complexity of \nXML schema. ACM Trans. Database Syst., 31(3):770 813, 2006. [21] P. McCann and S. Chandra. PacketTypes: \nAbstract speci.cation of network protocol messages. In SIGCOM, pages 321 333. ACM Press, August 2000. \n[22] H. T. Ng, C. Y. Lim, and J. L. T. Koo. Learning to recognize tables in free text. pages 443 450, \nMorristown, NJ, USA, 1999. [23] J. Oncina and P. Garcia. Inferring regular languages in polynomial updated \ntime. Machine Perception and Arti.cial Intelligence, 1:29 61, 1992. [24] PADS project learning demo. \nhttp://www.padsproj.org/ learning-demo.cgi, 2007. [25] R. Pang, V. Paxson, R. Sommer, and L. Peterson. \nbinpac: a yacc for writing application protocol parsers. In IMC 06: Proceedings of the 6th ACM SIGCOMM \nconference on Internet measurement, pages 289 300, New York, NY, USA, 2006. ACM. [26] D. Pinto, A. McCallum, \nX. Wei, and W. B. Croft. Table extraction using conditional random .elds. In SIGIR, pages 235 242, New \nYork, NY, USA, 2003. [27] V. Raman and J. M. Hellerstein. Potter s wheel: An interactive data cleaning \nsystem. In VLDB, pages 381 390, 2001. [28] S. Soderland. Learning information extraction rules for semi\u00adstructured \nand free text. Machine Learning, 34(1-3):233 272, 1999. [29] A. Stolcke and S. Omohundro. Inducing probabilistic \ngrammars by bayesian model merging. In ICGI, 1994. [30] C. Wikstr\u00a8om and T. Rogvall. Protocol programming \nin Erlang using binaries. In Fifth International Erlang/OTP User Conference, Oct. 1999.    \n\t\t\t", "proc_id": "1806596", "abstract": "<p>An <i>ad hoc data format</i> is any nonstandard, semi-structured data format for which robust data processing tools are not easily available. In this paper, we present ANNE, a new kind of markup language designed to help users generate documentation and data processing tools for ad hoc text data. More specifically, given a new ad hoc data source, an ANNE programmer edits the document to add a number of simple annotations, which serve to specify its syntactic structure. Annotations include elements that specify constants, optional data, alternatives, enumerations, sequences, tabular data, and recursive patterns. The ANNE system uses a combination of user annotations and the raw data itself to extract a context-free grammar from the document. This context-free grammar can then be used to parse the data and transform it into an XML parse tree, which may be viewed through a browser for analysis or debugging purposes. In addition, the ANNE system generates a PADS/ML description, which may be saved as lasting documentation of the data format or compiled into a host of useful data processing tools.</p> <p>In addition to designing and implementing ANNE, we have devised a semantic theory for the core elements of the language. This semantic theory describes the editing process, which translates a raw, unannotated text document into an annotated document, and the grammar extraction process, which generates a context-free grammar from an annotated document. We also present an alternative characterization of system behavior by drawing upon ideas from the field of relevance logic. This secondary characterization, which we call <i>relevance analysis</i>, specifies a direct relationship between unannotated documents and the context-free grammars that our system can generate from them. Relevance analysis allows us to prove important theorems concerning the expressiveness and utility of our system.</p>", "authors": [{"name": "Qian Xi", "author_profile_id": "81413603551", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P2184552", "email_address": "", "orcid_id": ""}, {"name": "David Walker", "author_profile_id": "81100426485", "affiliation": "Princeton University, Princeton, NJ, USA", "person_id": "P2184553", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1806596.1806622", "year": "2010", "article_id": "1806622", "conference": "PLDI", "title": "A context-free markup language for semi-structured text", "url": "http://dl.acm.org/citation.cfm?id=1806622"}