{"article_publication_date": "06-05-2010", "fulltext": "\n A GPGPU Compiler for Memory Optimization and Parallelism Management Yi Yang Ping Xiang Jingfei Kong \nHuiyang Zhou Dept. of ECE School of EECS School of EECS, UCF Dept. of ECE North Carolina State University \nUniv. of Central Florida Univ. of Central Florida North Carolina State University yyang14@ncsu.edu xp@knights.ucf.edu \n jfkong@cs.ucf.edu hzhou@ncsu.edu Abstract This paper presents a novel optimizing compiler for general \npur\u00adpose computation on graphics processing units (GPGPU). It ad\u00addresses two major challenges of developing \nhigh performance GPGPU programs: effective utilization of GPU memory hierarchy and judicious management \nof parallelism. The input to our compiler is a na\u00efve GPU kernel function, which is functionally correct \nbut without any consideration for performance optimization. The compiler analyzes the code, identi\u00adfies \nits memory access patterns, and generates both the optimized kernel and the kernel invocation parameters. \nOur optimization process includes vectorization and memory coalescing for mem\u00adory bandwidth enhancement, \ntiling and unrolling for data reuse and parallelism management, and thread block remapping or ad\u00address-offset \ninsertion for partition-camping elimination. The ex\u00adperiments on a set of scientific and media processing \nalgorithms show that our optimized code achieves very high performance, either superior or very close \nto the highly fine-tuned library, NVIDIA CUBLAS 2.2, and up to 128 times speedups over the naive versions. \nAnother distinguishing feature of our compiler is the understandability of the optimized code, which \nis useful for performance analysis and algorithm refinement. Categories and Subject Descriptors D.3.4 \n[Programming Languages]: Processors Compilers, Optimization General Terms Performance, Experimentation, \nLanguages Keywords GPGPU; Compiler; 1. Introduction The high computational power and affordability of \nstate-of-art graphics processing units (GPU) have made them the first widely accessible parallel computers \nwith teraflops capability. To fully realize the power of general purpose computation on graphics processing \nunits (GPGPU), two key issues need to be considered carefully: (1) how to parallelize an application \ninto concurrent work items and distribute the workloads in a hierarchy of thread blocks and threads; \nand (2) how to efficiently utilize the GPU memory hierarchy, given its dominant impact on performance. \nAs these two issues usually coupled together and finding an optimal Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for profit or commercial advantage and that copies bear this notice \nand the full cita\u00adtion on the first page. To copy otherwise, or republish, to post on servers or to redis\u00adtribute \nto lists, requires prior specific permission and/or a fee. PLDI 10 June 5 10, 2010, Toronto, Ontario, \nCanada. tradeoff between different levels of parallelism and memory op\u00adtimizations requires detailed \nunderstanding of GPU hardware, developing high performance GPGPU programs remains chal\u00adlenging for application \ndevelopers. Furthermore, GPU hardware architectures are evolving rapidly, which makes the code devel\u00adoped \nand tuned for one generation (e.g., NVIDIA GTX 8800) less optimal for the next one (e.g., NVIDIA GTX280). \nOur envisioned solution to these problems is to let application developers identify fine-grain thread-level \nparallelism and/or data-level parallelism and to use an optimizing compiler to perform memory and paral\u00adlelism \noptimizations. This way, we leverage the algorithm-level expertise of application developers and at the \nsame time relieve them of low-level hardware-specific performance optimizations. Our compiler works as \nfollows. The input is a na\u00efve GPU ker\u00adnel function, which is functionally correct but does not include \nany device-specific performance optimizations. Such a kernel function represents the user-identified \nfine-grain work item that can run concurrently. A typical example of a fine-grain work item is the computation \nof a single data element in the output domain. The compiler analyzes the na\u00efve kernel, checks the off-chip \nmem\u00adory access patterns, and optimizes the memory accesses through vectorization and coalescing to achieve \nhigh data access band\u00adwidth. Then the compiler analyzes data dependencies and identi\u00adfies possible data \nsharing across threads and thread blocks. Based on data sharing patterns, the compiler intelligently \nmerges threads and/or thread-blocks to improve memory reuse through the regis\u00adter file and the on-chip \nshared memory. These merges provide a novel way to achieve loop tiling and unrolling by aggregating fine-grain \nwork items into threads and thread blocks. Additionally, the compiler schedules the code to enable data \nprefetching so as to overlap computation with memory access latencies. To avoid partition camping [12] \n(i.e., to distribute memory traffic evenly across memory partitions), thread blocks are checked for their \nmemory accesses and depending on the thread block dimensions, either an address offset is inserted or \nthe block identifiers (ids) are remapped, if necessary. The compiler also performs hardware\u00adspecific \ntuning based on hardware parameters such as the register file size, the shared memory size, and the number \nof cores in the target GPU. Besides the aggressive compiler optimizations, another distin\u00adguishing feature \nof our compiler is that the optimized code is rea\u00adson-ably understandable compared to the code generated \nusing algebraic frameworks such as polyhedral models [11]. As a result, it is relatively easy to reason \nabout the optimized code generated by our compiler, which facilitates algorithm-level exploration. In \nour experiments, we used the compiler to optimize 10 scien\u00adtific and image processing functions. The \nexperimental results on NVIDIA 8800 GTX and NVIDIA GTX 280 GPUs show that our Copyright &#38;#169; 2010 \nACM 978-1-4503-0019-3/10/06 $10.00.  optimized code can achieve very high performance, either supe\u00adrior \nor very close to the NVIDIA CUBLAS 2.2 library and up to 128X over the na\u00efve implementation. In summary, \nour work makes the following contributions. (1) We propose a compiler for GPGPU programming that enables \nthe application developers to focus on algorithm-level issues rather than low-level hardware-specific \nperformance optimizations. (2) We propose a set of new compiler optimization techniques to improve memory \naccess bandwidth, to effectively leverage on\u00adchip memory resource (register file and shared memory) for \ndata sharing, and to eliminate partition conflicts. (3) We show that the proposed optimizing compiler \nis highly effective and the pro\u00adgrams optimized by our compiler achieve very high performance, often \nsuperior to manually optimized codes. The remainder of the paper is organized as follows. In Section \n2, we present a brief background on the NVIDIA CUDA pro\u00adgramming model [19] and highlight key requirements \nfor high performance GPU computation. In Section 3, we present our pro\u00adposed optimizing compiler in detail. \nSection 4 explores the design space of our propose optimizations. A case study of matrix multi\u00adplication \nis presented in the Section 5 to illustrate the compilation process. The experimental methodology and \nresults are presented in the Section 6. In Section 7, we highlight the limitations of the proposed compiler. \nRelated work is discussed in Section 8. Fi\u00adnally, Section 9 concludes our paper and discusses future \nwork.  2. Background State-of-the-art GPUs employ many-core architectures. The on\u00adchip processors cores \nare organized in a hierarchical manner. In the NVIDIA G80/GT200 architecture, a GPU has a number of streaming \nmultiprocessors (SMs) (16 SMs in an NVIDIA GTX 8800 and 30 SMs in an NVIDIA GTX 280) and each SM contains \n8 streaming processors (SPs). The on-chip memory resource in\u00adcludes register files (32kB per SM in GTX \n8800 and 64kB per SM in GTX 280), shared memory (16kB per SM), and caches with undisclosed sizes for \ndifferent memory regions. To hide the long off-chip memory access latency, a high number of threads are \nsupported to run concurrently. These threads follow the single\u00adprogram multiple-data (SPMD) program execution \nmodel. They are grouped in 32-thread warps with each warp being executed in the single-instruction multiple-data \n(SIMD) manner. According to the CUDA programming guide [19], each warp contains threads of consecutive, \nincreasing thread ids. In a typical 2D/3D execu\u00adtion domain, the threads in a warp (if not at the boundary) \nhave increasing thread ids along the X direction, and the same thread ids along the Y and Z directions. \nIn the CUDA programming model, the code to be executed by GPUs is the kernel functions. All the threads \nwill run the same kernel code with different thread ids to determine their workloads. The software architecture \nalso defines the concept of a thread block as an aggregation of threads which must be executed in the \nsame SM and the threads in the same thread block can communi\u00adcate with each other through the shared \nmemory on the SM. Next, we summarize the key aspects for high performance GPGPU code as they are the \nmain focus of our proposed compiler optimizations. a) Off-chip memory access bandwidth. To utilize the \noff-chip memory bandwidth efficiently, memory accesses need to be coalesced and each data item may need \nto be a vector type, depending on specific GPU hardware. Memory coalescing re\u00ad fers to the requirement \nthat the accesses from 16 consecutive threads in a warp (i.e., a half warp) can be coalesced into a single \ncontiguous, aligned memory access [19]. In this paper, we refer to such a coalesced contiguous, aligned \nregion as a coalesced segment. If each memory access is of the type float , each segment starts from \nan address which is a multi\u00adple of 64 bytes, and has the size of 64 bytes. The memory bandwidth utilization \nmay be significantly improved when each of coalesced memory accesses is of a vector data type, such as \nfloat2 (a vector of two float numbers) and float4 (a vector of four float numbers). For ATI/AMD HD 5870, \nthe sustained bandwidth reaches 71GB/s, 98GB/s, and 101GB/s when accessing 128MB data using the float, \nfloat2, and float4 data types, respectively. In comparison, for the same data transmission on NVIDIA \nGTX 280, the sustained bandwidth is 98GB/s, 101GB/s, and 79GB/s using the float, float2, and float4 data \ntypes, respectively. b) Shared memory. The common usage of shared memory is a software-managed cache \nfor memory reuse. Although it has low access latencies, shared memory is slower than register files and \nhas certain overheads beyond access latency. First it needs to be synchronized to ensure proper access \norder among the threads in a thread block. Second, the shared memory in NVIDIA GPUs has 16 banks, and \nbank conflicts can impair the performance. c) Balanced resource usage. As multiple threads in the same \nthread block and multiple thread blocks compete for limited resources in an SM, including the register \nfile, the shared memory, and the number of the thread contexts being sup\u00adported in hardware, we need \nto carefully balance parallelism and memory optimizations. d) Off-chip memory partitions. In current \nGPUs, off-chip mem\u00adory is divided into multiple partitions. There are 6 and 8 parti\u00adtions in GTX8800 \nand GTX280, respectively, and the partition width is 256 bytes. To use the partitions effectively, the \nmem\u00adory traffic should be evenly distributed among all the parti\u00adtions. Otherwise, the requests may be \nqueued up at some partitions while others are idle. This is referred to as partition camping [12] or \npartition conflicts, which are similar to bank conflicts at shared memory but incur much higher perform\u00adance \npenalties. Since concurrent memory requests are issued on a per half-warp basis from all active thread \nblocks, partition conflicts happen across different thread blocks. Note that the key performance issues \nlisted above are not unique to current GPUs. Future many-core architectures will probably use similar \napproaches to achieve high memory band\u00adwidth (i.e., coalescing, multiple memory partitions) and to reduce \nmemory access latency (i.e., on-chip software managed cache or shared memory). So, the proposed compiler \noptimizations are expected to be relevant beyond the scope of GPGPU.  3. An Optimizing GPGPU Compiler \nOur proposed compiler framework is shown in Figure 1. The in\u00adput to our compiler is a na\u00efve GPU kernel \nfunction, which is func\u00adtionally correct, but does not include any device-specific performance optimizations. \nFor many scientific computing and media processing functions, the na\u00efve version is simply the code to \ncompute one element/pixel in the output matrix/image. Typi\u00adcally such code is straightforward to extract \nfrom the sequential CPU code. One common example is the loop body from a heavily executed loop. In Figures \n2a and 2b, we show the sample na\u00efve kernel functions for the matrix multiplication (mm) and matrix\u00advector \nmultiplication (mv) algorithms, respectively. Each com\u00adputes one element at the position (idx, idy). \nIn Figure 2, idx and idy are the position/coordinate of the element in the output matrix. In the CUDA \nprogramming model, idy can be viewed as the absolute thread id along the Y direction, which is equal \nto (blockIdx.y*blockDimy + threadIdx.y) in the CUDA code. Correspondingly, idx is the absolute thread \nid along the X direction, which equal to (blockIdx.x*blockDimx + threadIdx.x). In comparison, the CUDA \npredefined threadIdx.x and threadIdx.y are the relative thread position/coordinate within a thread block \nand we refer to them as tidx and tidy for short. Both tidx and tidy are independent of the thread block \nids.  As can be seen from the two examples, the na\u00efve kernel func\u00adtions don't have any shared memory \nusage and do not require thread block partition. In other words, we may simply assume every block only \nhas one thread. All the arrays are initially in the off-chip global memory. For applications which require \nsynchronization among com\u00adputing different output pixels, e.g., reduction operations, a global sync function \nis supported in the na\u00efve kernel.  Figure 1. The framework of the proposed compiler. To facilitate compiler \noptimizations, the following (optional) information can be conveyed using the #pragma interface: the \nsize of the input and output dimensions, and the output variable names. The latter can be used to eliminate \nglobal memory writes to temporary variables when they are moved to shared memory. Given the na\u00efve kernel \nfunction, the compiler takes the follow\u00ading steps to generate the optimized kernel code. First, depending \non the targeted GPUs, the compiler attempts to group memory accesses into vector data accesses. Second, \nthe off-chip memory accesses are checked to see whether they satisfy the requirements for memory coalescing. \nIf not, the code will be converted to coa\u00adlesced memory accesses using shared memory as temporary stor\u00adage. \nThird, the compiler analyzes data dependencies and sharing patterns to determine how the data are shared \namong the neighboring thread blocks. Based on data sharing patterns, the compiler merges both threads \n(i.e., combining several threads in different thread blocks into one) to enable the data reuse through \nregisters and thread blocks (i.e., combining several blocks into one) to increase data reuse through \nshared memory. The data re\u00aduse information is also used to disable certain memory coalescing transformations \nwhen there is little or no data reuse. After thread/thread-block merge, the compiler schedules the code \nto perform data prefetching. Then, the compiler checks the memory accesses from different thread blocks \nfor partition camping and either inserts address offsets or remaps thread block ids, if neces\u00adsary. Finally, \nthe compiler generates the optimized kernel and the parameters (i.e., the thread grid &#38; block dimensions) \nto invoke the kernel function. The optimization process described above can also be used as a generic \nmethodology to guide manual optimizations of GPGPU programs. As a result, our optimized code is reasonably \nunder\u00adstandable, as will be seen in the remainder of Section 3. 3.1 Vectorization of Memory Accesses \nAs discussed in Section 2, the data type of memory accesses may have significant impact on bandwidth \nutilization. Therefore, the compiler first checks data accesses inside the kernel function to see whether \nthey can be grouped in a vector type data access. Since different GPUs feature significantly different \nrequirements on vector types for bandwidth utilization, the compiler follows different rules to adjust \nthe aggressiveness of vectorization. In this paper, we focus on CUDA and NVIDIA GPUs, in which a vector \nof two floats (i.e. float2) is the preferred data type but the band\u00adwidth improvement over the float \ntype is less than 3%. Therefore, we use the following strict rule: if there is a pair of accesses to \nthe same array with the indices: 2*idx+N and 2*idx+N+1, where N is an even number, the compiler generates \na float2 variable f2 with array offset as idx+N/2 and replaces the original array accesses with f2.x \nand f2.y. This rule is essentially designed for applica\u00adtions using complex numbers when the real part \nis stored next to the imaginary part of each data element. Note that this vectoriza\u00adtion of data accesses \nis simpler than classical vectorization. For AMD/ATI GPUs, due to the much more profound impact on bandwidth, \nthe compiler is more aggressive and also groups data accesses from neighboring threads along the X direction \ninto float2/float4 data types. The tradeoff of vectorization of data ac\u00adcesses is that if the vector \ndata accesses are not coalesced (Section 3.2) and the compiler converts them into coalesced ones (Section \n3.3) through shared memory, there may be bank conflicts. For AMD/ATI GPUs, the benefits of vectorization \nfar overweigh the penalties of shared memory bank conflicts. For NVIDIA GPUs, however, the benefits from \nvectorization are limited. Therefore, the compiler skips these additional steps to vectorize data ac\u00adcesses. \n (a) A na\u00efve kernel for matrix multiplication float sum = 0; for (int i=0; i<w; i++) sum+=a[idx][i]*b[i]; \nc[idx] = sum; (b) A na\u00efve kernel for matrix-vector multiplication Figure 2. Examples of naive kernel \nfunctions.  3.2 Checking Memory Coalescing As discussed in Section 2, GPGPU employs the SPMD model and \nthe threads in a single warp execute the kernel function in the SIMD mode. Therefore, in order to determine \nwhether off-chip memory accesses can be coalesced, we need to compute the ad\u00addresses of each memory access \nin the kernel function for different threads. As arrays are the most common data structure in scien\u00adtific \nand media processing, we consider four types of array indices and affine transformations of these indices: \n 1. Constant index: the constant value is used in an array index, for example, the constant integer \n5 in a[idy][i+5] . 2. Predefined index: the predefined numbers, such as absolute thread ids, idx, idy, \nand relative thread ids, tidx (i.e., threa\u00addIdx.x), tidy (i.e., threadIdx.y), are used as an array index. \nFor example, idy in a[idy][i+5] . 3. Loop index: a loop iterator variable is used as an array index, \nfor example, i in b[i][idx] in Figure 2a. 4. Unresolved index: an array index is used, which is not \none of the first three types. For example, an indirect access a[x] where x is a value loaded from memory. \nAs our compiler cannot determine the addresses of such indices, we simply skip them without checking \nwhether they can be coalesced. Among the four types of indices, the addresses corresponding  to the \nfirst two are fixed for a given thread. For the third, however, we need to check different values of \nthe loop iterator. Assuming that a loop index starts from S with increment Incr, then we need to check \nthe index addresses from the first 16 iterations: S, S+Incr, S+2*Incr, to S+15*Incr. The reason is that \nthe same behavior repeats for remaining iterations in terms of whether the access can be coalesced as \nthe difference in addresses is a multiple of 16. After determining the types of array indices in the \nkernel func\u00adtion, for each memory access instruction, the compiler computes the addresses from the 16 \nconsecutive threads in the same warp (i.e., a half warp) to see whether they can be coalesced. As dis\u00adcussed \nin Section 2, if we assume the array type of float , the coalesced accesses will form a coalesced segment, \nwhich starts from an address, whose value is a multiple of 64, and has the size of 64 bytes. Among the \naddresses from the 16 threads, we refer to the smallest one as the base address . The differences between \nthe base address and the addresses from the subsequent 15 threads are referred to as offsets . To satisfy \nthe coalescing requirement, the base address needs to be a multiple of 64 and offsets need to be 1 to \n15 words. The following two rules are used to handle common array accesses. For an index to a multi-dimensional \narray, e.g., A[z][y][x] , the index to the higher-order dimensions, e.g., the y and z di\u00admensions, should \nremain the same for all the 16 threads in the half warp. Otherwise, for example, if the predefined index \nidx (the thread id along the x direction) is used in an index to the y dimension in a multi-dimension \narray A[][idx][0] , the accesses from the 16 threads will be A[][0][0] , A[][1][0] , A[][2][0] , etc., \nand are not coalesced. When a loop index is used in the kernel function, the compiler computes the base \naddress and the offsets for each possible value of the loop iterator. For example, for the address a[idy][i] \nin Figure 2a, the base address is &#38;a[idy][0] when the iterator i is 0; &#38;a[idy][1] when i is 1, \netc. The offsets are all zeros as the addresses do not change for different threads in the same half \nwarp. As both the base addresses and the offsets do not meet the condition, the array access a[idy][i] \nis not coalesced. For the array access b[i][idx] in Figure 2a, the base address is &#38;b[0][0] when \ni is 0; &#38;b[1][0] when i is 1, etc.. The offsets are from 1 word to 15 words. Thus, the array access \nb[i][idx] is coalesced as long as each row of array b is aligned to the multiple of 16 words. For the \narray access b[idx+i] , although the offsets satisfy the condition for every possible i , it is not a \ncoalesced access since the base address is not always a multiple of 16 words, e.g., b[1] when i is 1. \n 3.3 Converting Non-Coalesced Accesses into Coalesced Ones After the compiler analyzes every array access \nin the kernel code, the compiler converts the non-coalesced accesses into coalesced ones through shared \nmemory. The observation here is that for each non-coalesced memory access instruction, the compiler can \ndetermine the coalesced segments that contain the data required by the non-coalesced memory accesses \nfrom the half warp. The compiler then introduces shared-memory array variables, inserts statements (coalesced \nmemory accesses) to initialize the shared memory variables, and replaces the original global memory ac\u00adcesses \nwith shared memory accesses. The thread block size is also set to 16 so that each thread block contains \none half warp. The syncthreads function is also inserted to ensure the proper access order. (S0)for (i=0; \ni<w; i=(i+16)) { (S1) __shared__ float shared0[16]; (S2) shared0[(0+tidx)]=a[idy][((i+tidx)+0)]; (S3) \n__syncthreads(); (S4) for (int k=0; k<16; k=(k+1)) { (S5) sum+=shared0[(0+k)]*b[(i+k)][idx]); (S6) } \n(S7) __syncthreads(); (S8)} (S9)c[idy][idx] = sum; (a) The coalesced mm kernel (S0) for (i=0; i<w; i=(i+16)) \n{ (S1) __shared__ float shared2[16]; (S2) __shared__ float shared1[16][17]; (S3) shared2[(0+tidx)]=b[i+tidx]; \n(S4) for (l=0; l<16; l=(l+1)) (S5) shared1[(0+l)][tidx]= a[((idx-tidx)+l)][(i+tidx)]; (S6) __syncthreads(); \n(S7) for (int k=0; k<16; k=(k+1)){ (S8) sum+=(shared1[tidx][k]*shared2[k]); (S9) } (S10) __syncthreads(); \n(S11) } (S12) c [idx] = sum; (b) The coalesced mv kernel Figure 3. Coalesced kernels generated by the \ncompiler. For array accesses using constant or predefined indices, the process is typically straightforward. \nFor example, the non\u00adcoalesced access, A[idy][0] , the coalesced segment is A[idy][0:15] . The compiler \ninserts a shared-memory array vari\u00adable sA[0:15] and initializes the sA[0:15] with A[idy][tidx] , where \ntidx is relative thread id within the warp. In the case when idx is used in an index to a multi-dimensional \narray, the com\u00adpiler may introduce a loop to load the required data for a half warp. For example, for \nan array access A[idx][0] , the required data for a half warp is A[(idx-tidx)+(0:15)][0] , where (idx\u00adtidx) \nprovides the start address of each thread block, which is the same as the start address of the half warp \nas each thread block only contains a half warp at this time. The coalesced segments that contains the \nrequired data are A[(idx-tidx)+(0:15)][0:15] . In the introduced loop of 16 iterations, a shared memory \narray is initialized with A[(idx-tidx)+l][tidx] , where l is the iterator of the newly introduced loop. \nFrom these examples, it can be seen that not all the data loaded in the shared memory are useful, the \ncompiler will perform data reuse analysis (Section 3.4) to deter\u00admine whether this transformation is \nbeneficial or not. If it is not, the compiler will skip coalescing transformation on this access. In \nthe special case where an array access involves both idx and idy , such as A[idx][idy] , the compiler \nanalyzes the feasibility to exchange idx and idy to make it coalesced. This transforma\u00adtion is equivalent \nto loop interchange on the CPU code.  For array accesses using a loop index, A[m*i+n] , where i is the \nloop iterator and m and n are constants, the compiler unrolls the loop for 16/(GCD(m,16)) times if m \nis less than or equal to 8. If m is greater than 8, the coalesced access has little benefit due to limited \nreuse across different iterations. Then, the compiler groups the accesses from unrolled loops into coalesced \nones. For example, for the array access A[idy][i] where i is the loop iterator, the segment A[idy][0:15] \ncontains all the required data for the first 16 iterations. The compiler unrolls the loop for 16 times, \nintro\u00adduces shared memory variable sA[0:15] which are initialized with A[idy][tidx+i] (coalesced as the \nincrement of i is 16 after unroll\u00ading), and replaces A[idy][i] with sA[i] . For the na\u00efve kernels in \nFigure 2, the coalesced versions are shown in Figure 3. The inner loop with the iterator k is a result \nof unrolling the outer loop with the iterator i . In the na\u00efve kernel in Figure 2a, the access a[idy][i] \nis not coalesced, which results in loop unrolling as described above. b[i][idx] is coalesced and it transforms \nto b[(i+k)][idx] due to unrolling for a[idy][i] . In the mv kernel in Figure 2b, both accesses a[idx][i] \nand b[i] are not coalesced. Converting the access b[i] into coalesced accesses involves a loop unrolling \nof 16 (=16/GCD(1,16)) times and it becomes b[i+tidx] in Figure 3b. For the access a[idx][i] the loop \nwith the iterator l is introduced and the access is trans\u00adformed to a[(idx-tidx)+l][i+tidx] . In addition, \nthe compiler may add padding to the shared memory arrays to avoid bank conflicts and padding to input \ndata arrays to ensure that the row size of each array is a multiple of 16 words so as to meet the requirement \nof memory coalescing. After memory coalescing, the kernel code generated by our compiler has the following \ncharacteristics: 1. Each thread block has 16 consecutive threads (i.e., only a half warp) along the X \ndirection, because 16 threads are needed by hardware to coalesce memory accesses and they communicate \nwith each other through shared memory. The number of threads in each thread block will be expanded during \nthe next optimization phase (Section 3.5) to make sure there are enough threads in each thread block. \n 2. There are two types of global memory load statements: (a) Global memory to shared memory (G2S): the \nstatements read data from global memory and store them into the shared mem\u00adory, such as (S2) in Figure \n3a. (b) Global memory to register (G2R): the statements read data from global memory and save them to \nregisters. For example, in (S5) in Figure 3a, the global memory access b[(i+k)[idx] loads the data into \nregisters. 3.4 Data Dependencies and Data Sharing  In this step, the compiler detects data dependency \nand data shar\u00ading. Such analysis is similar to those used in analyzing affine ar\u00adray accesses for locality \noptimization and parallelization [1]. As our compiler has already enforced memory coalescing by associ\u00adating \ncoalesced segments with each global memory access, the compiler can detect data sharing by comparing \nwhether the ad\u00address ranges of the segments have overlaps. In the applications that we studied, we found \nthat data sharing happens most fre\u00adquently among neighboring blocks along the X or Y direction. Therefore, \nour current compiler implementation mainly focuses on checking data sharing among neighboring thread \nblocks and also the thread blocks with a fixed stride along the X or Y direc\u00adtion. Thread block before \nmerge  Thread block before merge  Shared Data Segment     Thread Thread block after thread-block \nmerge Figure 4. Improve memory reuse by merging neighboring thread blocks. int i = 0; float sum = 0; \nfor (i=0; i<w; i=(i+16)) { __shared__ float shared0[16]; if (tidx<16) { /*inserted due to block merge \nto remove redundant loads */ shared0[(0+tidx)]=a[idy][((i+tidx)+0)]; } __syncthreads(); int k; for \n(k=0; k<16; k=(k+1)) { sum+=shared0[(0+k)]*b[(i+k)][idx]); } __syncthreads(); } c[idy][idx] = sum; \nFigure 5. The kernel function for matrix multiplication, after merging blocks along the X direction. \nThe data sharing/reuse information is also used to determine whether the code conversion for memory coalescing \nis beneficial. As described in Section 3.3, shared memory is used as temporary storage to achieve memory \ncoalescing. The data in the shared memory, however, may not be useful as they are simply loaded from \noff-chip memory to satisfy the coalescing requirement. For example, the compiler loads A[idy][0:15] in \norder to convert the access A[idy][0] into a coalesced one. Currently, our complier employs a simple \nrule to check whether an access needs to be converted: if the loaded data in shared memory have no reuse, \nit is not converted. A more crafted heuristic may further rank code conversions for different accesses \nby comparing their shared memory usage and number of data reuses, and then select the most beneficial \nones if shared memory is used up. We left such investigation as our future work to refine our compiler \nframework. 3.5 Thread/Thread-Block Merge to Enhance Memory Reuse After detecting that there exists data \nsharing among thread blocks (mainly neighboring blocks), we propose two new techniques to enhance data \nsharing so as to reduce the number of global mem\u00adory accesses: merging thread blocks and merging threads. \nThread\u00adblock merge determines the workload for each thread block while thread merge decides the workload \nfor each thread. These two techniques combined are essentially a way to achieve loop tiling and unrolling \nby aggregating the fine-grain work items into threads and thread blocks. We first present the two techniques \nand then discuss how compiler prioritizes one over the other.  Shared Data Segment Thread Shared Register \nFigure 6. Improve memory reuse by merging threads from neighboring thread blocks. 3.5.1 Thread-block \nmerge When our compiler determines that multiple thread blocks share some common data, it may choose \nto merge them into one thread block, as shown in Figure 4. To illustrate the procedure to merge thread \nblocks, we show how our compiler combines two neighboring blocks along the X direction into one. First, \nthe compiler re-computes the thread id information within the thread block (i.e., tid). As two thread \nblocks along the X direction are merged, idx, idy and tidy remain the same while tidx is re-computed \nas (idx%(N*blockDim.x)), where N is 2 for Figure 4. Second, for the statements that result in data sharing, \nwe add control flow to ensure that the global mem\u00adory data are loaded only once. For the matrix multiplication \nex\u00adample in Figure 3a, the statement S2 in threads from two neighboring thread blocks accesses the same \nsegment. Therefore, we add an if (tidx < blockDim.x) statement to eliminate redun\u00addant global memory \naccesses, as shown in Figure 5. Third, the thread block dimension is resized (blockDim.x = 2*blockDim.x). \nAs thread-block merge determines the workload for each thread block and all threads in the same thread \nblock reuse data in shared memory, it essentially achieves loop tiling for locality and parallelism optimizations. \n3.5.2 Thread merge The other approach to enhance data sharing is to merge threads from different thread \nblocks, which combines several threads workloads into one, as shown in Figure 6. Compared to thread\u00adblock \nmerge, after these threads are combined into one, they can share not only shared memory, but also the \nregisters in the register file. Furthermore, some control flow statements and address com\u00adputation can \nbe reused, thereby further reducing the overall in\u00adstruction count. The limitation is that an increased \nworkload typically requires a higher number of registers, which may reduce the number of active threads \nthat can fit in the hardware. From the discussion, it can be seen that thread merge achieves the effects \nof loop unrolling. Note that thread merge also combines multiple thread blocks into one but it does not \nincrease the number of threads in each thread block. int i = 0; float sum_0 = 0; float sum_31 = 0; for \n(i=0; i<w; i=(i+16)) { __shared__ float shared0_0[16]; __shared__ float shared0_31[16]; if (tidx<16) \n{ /* 32 is the number of the threads to be merged */ shared0_0[(0+tidx)]= a[idy*32+0][((i+tidx)+0)]; \n shared0_31[(0+tidx)]= a[idy*32+31][((i+tidx)+0)]; } syncthreads(); int k; for (k=0; k<16; k=(k+1)) \n{ float r0 = b[(i+k)][idx]) sum_0+=shared0[(0+k)]*r0;  sum_31+=shared0_31[0+k]*r0; } __syncthreads(); \n} c[idy*32+0][idx] = sum_0;  c[idy*32+31][idx] = sum_31; Figure 7. The matrix multiplication kernel \nafter merging 32 threads in 32 adjacent blocks along the Y direction. To illustrate the procedure to \nmerge threads, we show how our compiler combines N neighboring blocks along the Y direction into one. \nFirst, the compiler re-computes the thread id information. As we merge threads from two thread blocks \nalong the Y direc\u00adtion, the absolute thread ID along the X direction idx remains the same while the thread \nID along the Y direction idy will be changed to idy*N, idy*N+1, idy*N+2 , idy*N+(N-1) for the N replicated \nstatements. The thread id information within a thread block remains the same. Second, for the statement \nthat results in data sharing, we need only one copy. Third, for the control flow statement such as loops, \nwe also only need one copy. Fourth, for the remaining statements including data declaration, ALU compu\u00adtation \nstatement and other memory access statements, we replicate them for N times. For the matrix multiplication \nexample in Figure 5, the array access b[(i+k)][idx] results in the shared data among the thread blocks \nalong the Y direction (as the access address is not dependent on idy ). The compiler merges 32 neighboring \nblocks along the Y direction using thread merge, as shown in Figure 7. 3.5.3 Selection between thread \nmerge and thread-block merge As discussed in Section 3.3, the code generated by the compiler after memory \ncoalescing has two types of global memory accesses: global to shared memory (G2S) and global to register \n(G2R). If data sharing among neighboring blocks is due to a G2S access, the compiler prefers thread-block \nmerge to better utilize the shared memory. When data sharing is from a G2R access, the compiler prefers \nto merge threads from neighboring blocks due to the reuse of registers. If there are many G2R accesses, \nwhich lead to data sharing among different thread blocks, the register file is not large enough to hold \nall of the reused data. In this case, thread block merge is used and shared memory variables are introduced \nto hold the shared data. In addition, if a block does not have enough threads, thread-block merge instead \nof thread merge is also used to increase the number of threads in a block even if there is no data sharing. \n for (i=0; i<w; i=(i+16)){ __shared__ float shared0[16]; shared0[(0+tidx)]=a[idy][((i+tidx)+0)]; \n__syncthreads(); int k; for (k=0; k<16; k=(k+1)) { sum+=(shared0[(0+k)]*b[(i+k)][idx]); } __syncthreads(); \n} (a) Before inserting perfetching /* temp variable */ float tmp = a[idy][((0+tidx)+0)]; for (i=0; i<w; \ni=(i+16)) { __shared__ float shared0[16]; shared0[(0+tidx)]=tmp; __syncthreads(); if (i+16<w) //bound \ncheck tmp = a[idy][(((i+16)+tidx)+0)]; int k; for (k=0; k<16; k=(k+1)) { sum+=(shared0[(0+k)]*b[(i+k)][idx]); \n} __syncthreads(); } (b) After inserting perfetching Figure 8. A code example to illustrate data prefetching. \n3.6 Data Prefetching Data prefetching is a well-known technique to overlap memory access latency with \ncomputation. To do so, the compiler analyzes the memory accesses in a loop and uses a temporary variable \nto prefetch data for the next iteration before the computation in the current loop. The process is illustrated \nin Figure 8. The code be\u00adfore insertion of prefetching is in Figure 8a and Figure 8b shows the code after \ninsertion. Besides the temporary variable, additional checking is added to ensure that the prefetching \naccess does not generate unnecessary memory accesses. The overhead of data prefetching code is the increased \nregister usage due to the temporary variables. If the register can be used for data reuse (e.g., as a \nresult of thread merge), the compiler skips this optimization. 3.7 Eliminating Partition Camping In this \nstep, the compiler reuses the address access patterns ob\u00adtained for thread/thread-block merge to see \nwhether they lead to partition camping. As neighboring thread blocks along the X di\u00adrection are likely \nto be active at the same time, the compiler fo\u00adcuses on the addresses that involve blockIdx.x or bidx \nin short. Those accesses without involving bidx either access the same line in the same partition (e.g., \nA[0]) or access the same partition at different times (e.g., A[bidy][0] based on the assumption that \nthread blocks with different bidy will execute at different times). The following rules are followed \nby our compiler. Partition Camping Detection: If an array access involves bidx, the compiler checks the \naddress stride between the two accesses from the two neighboring blocks (i.e., one with block id bidx \nand the other with bidx+1). The compiler detects partition camping if the stride is a multiple of (partition \nsize * number of partitions). For example, for an array access A[idx], it is equivalent to A[bidx*blockDimx+tidx]. \nThe stride between two neighboring blocks is blockDimx, whose value then decides whether there are partition \nconflicts (i.e., two concurrent accesses to the same parti\u00adtion). Partition Camping Elimination: If an \naccess results in partition conflicts, depending on how thread blocks are organized, we use two ways \nto eliminate partition conflicts: 1. If thread blocks are arranged in one dimension, we add a fixed offset, \n(the partition width * bidx), to the access and update the loop bounds to accommodate the change. For \nexample, in mv, the output is a vector. So the thread blocks are organized in one dimension. The accesses \nA[idx][i] (or the coalesced version A[((idx-tidx)+l)][(i+tidx)]), from neighboring thread blocks result \nin partition camping if the width of A is a multi\u00adple of (partition size * number of partitions), as \nshown in Figure 9a. With the added offset, the access pattern is changed to Figure 9b, eliminating partition \ncamping. 2. If thread blocks are organized in two or more dimensions, we apply the diagonal block reordering \nproposed in [12], which essentially changes the workload (or tile) that each thread block is assigned \nto. The diagonal mapping rule is newbidy = bidx and newbidx = (bidx+bidy)%gridDim.x.  Array A Array \nA TB0 TB0 TB1 TB1 TB2 TB2  TB: Thread block P0 P1 (a) P: Partition (b) Figure 9. Eliminating partition \ncamping. (a) Accesses to array A resulting in conflicts at partition 0. (b) Adding an offset as (parti\u00adtion \nsize * bidx) eliminates the conflicts. The dark regions repre\u00adsent the memory footprint of A[idx][0] \nfrom different thread blocks. 4. Design Space Exploration 4.1 The Number of Threads in A Thread Block \nIn our compiler algorithm, the number of threads in a thread block is determined by thread/thread-block \nmerge. The CUDA pro\u00adgramming guide suggests that one SM should have at least 192 active threads to hide \nthe latency of register read-after-write de\u00adpendencies. Because our compiler tries to use a number of \nre\u00adsources (the shared memory due to thread-block merge and the register file due to thread merge) for \nbetter memory reuse, it is possible that the code after thread/thread-block merge requires a large amount \nof shared memory and registers so that one SM can only support a limited number of thread blocks. To \nbalance the thread-level parallelism and memory reuse, our compiler tries to put 128, 256, or 512 threads \ninto one thread block (equivalent to merging of 8, 16, and 32 blocks), if possible. Also, the compiler \nvaries the degrees of thread merge (i.e., how many threads to be merged into one) across 4, 8, 16, or \n32 so as to balance register\u00adbased data reuse and thread-level parallelism. As such, the combi\u00adnation \nof these design parameters creates a design space to ex\u00adplore. As discussed in Section 3.5, merging threads/thread \nblocks is one way to achieve loop tiling and unrolling. So, exploring such a design space is similar \nto finding the best tile size and unrolling factors for parallelization and locality enhancement. Due \nto the non-linear performance effect of those parameters on GPU per\u00adformance, the compiler generates \nmultiple versions of code and resorts to an empirical search by test running each version to se\u00adlect \nthe one with the best performance. Another way is to use an analytical performance model [7], [15] to \npredict the performance of each version, but this requires higher accuracy than current models. Moreover, \nbased on our experiments, the optimal version may be dependent upon the size of the input arrays, which \nimplies that unless the compiler knows detailed information of the in\u00adtended inputs, it is almost inevitable \nthat the compiler must run multiple versions of code in order to find the optimal one.  throughput of \n110 GFLOPS when computing the product of two 2kx2k matrices on GTX 8800. In CUBLAS 2.2, a more optimized \nversion is implemented based on the work of Vasily et. al. [18], which can reach 187 GFLOPS. In comparison, \nthe CUDA SDK version has a throughput of 81 GFLOPS. In this section, we use matrix multiplication as \nan example to illustrate our compilation process. The na\u00efve kernel, i.e., the input to our compiler, \nis shown in Figure 2a. In the kernel, there are two input arrays, a and b, from the global memory. The \ncompiler converts the accesses to array a into coalesced ones, as shown in Figure 3a. Based on detected \ndata sharing, the compiler determines that neighboring thread blocks along the X direction can be merged \nto improve reuse of array a and neighboring thread blocks along the Y direction can be merged to improve \nmemory reuse of array b. As the access to array a is a R2S (read-to-shared memory), the compiler chooses \nto perform thread-block merge. As the access to array b is R2R (read-to-register), the compiler chooses \nthread merge as discussed in Section 3.5. The next question is then how many thread blocks should be \nmerged along either direction? As discussed in Section 4, the heuristic is to put at least 128 threads \nin each thread block and to generate different versions of kernel functions depending on the number of \nthreads/thread blocks to be merged. Figure 10 shows the performance effect on GTX 280 of the number of \nmerged threads/thread blocks in either direction. It can be seen that the optimal performance for different \nsizes of input matrices is achieved with merging 16 thread blocks along the X direction 4.2 Hardware \nSpecification GPU hardware is evolving rapidly. Although different generations of GPU hardware may share \nsimilar architecture, e.g., NVIDIA GTX8800 and GTX280, there are significant changes, e.g., the register \nfile size, which may have a strong impact on performance. When there are more registers, more threads \ncan be put into one block or one thread can use more registers for temporary data. As a result, an optimized \ncode tuned for one GPU generation may not be optimal for the next. To solve this problem, our compiler \ngen\u00aderates different versions of optimized code based on different machine descriptions so that they \ncan be deployed on different GPU platforms. 5. Case Study: Matrix Multiplication Matrix multiplication \n(mm) is a commonly used algorithm and there has been continuing effort to improve its performance [18]. \nThe fine tuned implementation in NVIDIA CUBLAS 1.0 has a and 16 threads along the Y direction. 6. Experiments \n6.1 Experimental Methodology We implemented the proposed compiler framework in Cetus, a source-to-source \ncompiler infrastructure for C programs [8]. The CUDA language support in Cetus is ported from MCUDA [16]. \nThe compiler optimizes the na\u00efve kernel functions of the algo\u00adrithms listed in Table 1, all of which \ncompute a single element at the position (idx, idy). The numbers of lines of code (LOC) of these na\u00efve \nkernel functions are included in Table 1 to illustrate their programming complexity/simplicity. Among \nthe kernels, #pragma is used in the reduction kernel to convey the informa\u00adtion of input vector length \nand the actual output to the compiler. The output of our compiler, i.e., the optimized kernel, is compiled \nby the CUDA compiler, nvcc, to generate the GPU executable file. In our experiments, we used both NVIDIA \nGTX8800 and NVI\u00adDIA GTX280 GPUs with CUDA SDK 2.2 and a 32-bit CentOS  5.2 operating system. Our compiler \ncode, the na\u00efve kernels, and the optimized kernels are available at [20]. Algorithm The size of input \nmatrices/vectors Num. of LOC in the na\u00efve kernel transpose matrix vector multiplication (tmv) 1kx1k \nto 4kx4k (1k to 4k vec.) 11 matrix mul. (mm) 1kx1k to 4kx4k 10 matrix-vector mul. (mv) 1kx1k to 4kx4k \n 11 vector-vector mul. (vv) 1k to 4k 3 reduction (rd) 1-16 million 9 matrix equation solver (strsm) 1kx1k \nto4kx4k 18 convolution (conv) 4kx4k image, 32x32 kernel 12 matrix transpose (tp) 1kx1k to 8kx8k 11 Reconstruct \nimage (de\u00admosaicing) 1kx1k to 4kx4k 27 find the regional maxima (imregionmax) 1kx1k to 4kx4k 26 Table \n1. A list of the algorithms optimized with our compiler. 6.2 Experimental Results In our first experiment, \nwe examine the effectiveness of our com\u00adpiler optimizations. Figure 11 shows the kernel speedups of the \noptimized kernels over the na\u00efve ones running on both GTX8800 and GTX 280 GPUs. From the figure, it can \nbe seen that the com\u00adpiler significantly improves the performance using the proposed optimizations (15.1 \ntimes and 7.9 times on average using the geometric mean). To better understand the achieved performance, \nwe dissect the effect of each step of our compilation process and the results are shown in Figure 12. \nThe performance improvement achieved in each step is an average of all applications using the geometric \nmean. Since data vectorization is designed to handle complex numbers and all the inputs in this experiment \nare scalar numbers, this step has no effect. From Figure 12, it can be seen that thread/thread-block \nmerge has the largest impact on performance, which is expected as tiling and unrolling achieved with \nthis opti\u00admization is critical for locality and parallelism optimizations. Between the two GPUs, GTX280 \nbenefits less from the optimiza\u00adtions due to its improved baseline performance (i.e., na\u00efve kernels) \nas the hardware features more cores and higher memory band\u00adwidth. Prefetching shows little impact in \nour results. The reason is that after thread/thread-block merge, the kernel consumes many registers. \nWhen allocating registers for prefetch, either the degree of thread merge must be reduced or the off-chip \nlocal memory may have to be used, resulting in degraded performance. There\u00adfore, when registers are used \nup before prefetching, the prefetch\u00ading step is skipped by our compiler. Elimination of partition camping \nshows larger impact on GTX280 than GTX8800. One reason is due to the input data sizes used in our experiments. \nFor example, there is significant partition camping on GTX280 when transposing a 4kx4k matrix as it has \n8 partitions and the partition size is 256 bytes. For the same input on GTX8800 which has 6 partitions, \nthe accesses become more evenly distributed and eli\u00adminating partition camping has little effect. When \ntransposing a 3kx3k matrix on GTX8800, however, eliminating partition camp\u00ading results in a 21.5% performance \nimprovement. Among the algorithms in Table 1, six are implemented in the CUDA CUBLAS library. In the \nnext experiment, we compare our optimized kernel with the highly tuned CUBLAS v2.2 on GTX 280. Figure \n13 shows the performance comparison of the algo\u00adrithms with different input sizes. From Figure 13, we \ncan see that the kernel optimized by our compiler achieves consistently better performance than CUBLAS \n2.2 for transpose matrix vector multi\u00adplication (tmv), matrix vector multiplication (mv), vector vector \nmultiplication (vv), and matrix equation solver (strsm) for differ\u00adent input sizes. For matrix multiplication \n(mm) and reduction (rd), the performance of our optimized code is very close to CUBLAS 2.2 (within 2% \ndifference). On average (based on the geometric mean), our performance improvement over CUBLAS varies \nfrom 26% to 33% for different input sizes.  To study the effect of data vectorization, we chose the \nreduc\u00adtion (rd) algorithm since rd is the only algorithm in our study that has a corresponding version \nfor complex numbers (CublasScasum) in CUBLAS. We changed the na\u00efve kernel of rd to process com\u00adplex numbers \nby using two float-type variables to read the real (A[2*idx]) and imaginary (A[2*idx+1]) parts of a complex \nnum\u00adber instead of a single float2 variable. Then, we optimized this na\u00efve kernel with and without the \ndata vectorization step. For different input sizes, we compared the performance of the two optimized \nkernels (labeled optimized_wo_vec and optimized , respectively) and the results are show in Figure 14. \n From Figure 14, we can see that data vectorization signifi\u00adcantly improves the performance. One reason \nis the improved memory bandwidth due to the use of float2 data types as dis\u00adcussed in Section 2. Another \nreason is the side effect of memory coalescing. Without data vectorization, the compiler recognized that \nthe array accesses to both real and imaginary parts (A[2*idx] and A[2*idx+1]) are not coalesced. So, \nit uses shared memory as temporary storage to generate coalesced memory accesses as dis\u00adcussed in Section \n3.3. In comparison, the accesses in the kernel after data vectorization, A[idx], is coalesced. As a result, \nthe data are directly loaded into registers for computation. Although the compiler uses the shared memory \nto improve memory reuse for both vectorized and un-vectorized versions, there are more shared memory \naccesses in the un-vectorized kernel optimized_wo_vec due to code transformation for coalescing. These \nextra shared memory accesses contribute to the performance differences be\u00adtween the optimized_wo_vec \nand optimized kernels. Among all the kernels, transpose (tp) and matrix-vector multi\u00adplications (mv) \nexhibit the partition camping problem. Ruetsch and Micikevicius [12] proposed diagonal block reordering \nto ad\u00address the issue with transpose and their implementation is in\u00adcluded in the latest CUDA SDK. In \nFigure 15, we compare the performance of our optimized kernel (labeled optimized ) with theirs (labeled \nSDK new ) and we also include the previous CU-DA SDK version for reference (labeled SDK prev ). Since \ntp does not have any floating point operations, the effective band\u00adwidth is used. From Figure 15, it \ncan be seen that although our compiler uses the same approach to eliminate partition camping, the remaining \noptimizations taken by our compiler result in better performance than the version in the latest SDK. \nIn mv, the thread blocks are in one dimension. Therefore, di\u00adagonal block reordering cannot be applied. \nOur compiler uses the address offset approach described in Section 3.7 and the results are shown in Figure \n16. It can be seen that for different input sizes, even without partition camping elimination, our optimized \nkernel (labeled Opti_PC ) already achieves better performance than CUBLAS and eliminating partition camping \n(labeled optimized ) further improves the performance. In summary, our experimental results show that \nour optimizing compiler generates very high quality code and often achieves superior performance even \ncompared to the manually optimized code in CUDA CUBLAS and SDK. Figure 16. Performance of mv using the \nna\u00efve kernel, the opti\u00admized kernel without partition camping elimination (labeled Op-ti_PC ), the optimized \nkernel, and CUBLAS. 7. Limitations Although the proposed compiler can dramatically improve the performance \nover na\u00efve kernel functions, the fundamental limita\u00adtion is that it cannot change the algorithm structure. \nInstead, our compiler can be used to facilitate algorithm-level exploration. The reasons are two-fold. \nFirst, developers can leverage our aggressive compiler optimizations so that they do not need to optimize \ntheir implementations of each candidate algorithm. Second, the rela\u00adtively good understandability of \nour optimized code may give a hint of what algorithms are better suited. Taking 1D fast Fourier transform \n(FFT) as an example, when the na\u00efve kernel (50 lines of code) simply uses 2-point FFT in each step of \nthe Cooley Tukey algorithm [4], the throughput is 24 GLOPS for computing the FFT of 220 complex numbers \non GTX280. Our compiler optimizes the na\u00efve kernel by merging threads and the resulting kernel computes \n8-point FFT in each step, which delivers a throughput of 41 GFLOPS, significantly better than CUFFT 2.2 \n(26GFLOPS). The compiler generated 8-point FFT version, however, is not as good as a na\u00efve implementation \nof 8-point FFT (113 lines of code with a throughput of 44 GFLOPS). The reason is that the compiler generated \nversion uses multiple 2-point FFT calculations for an 8\u00adpoint FFT. On the other hand, as our compiler \ngenerated code is reasonably understandable, it serves as a good guideline for algo\u00adrithm exploration: \nchanging the na\u00efve kernel from 2-point FFT to 8-point FFT, for which the compiler can further optimize \nthe per\u00adformance to achieve 59 GFLOPS. More elaborate algorithm-level development by Govindaraju et. \nal. [6] as well as the one used in CUFFT2.3 achieves even higher performance (89 GFLOPS), indicating \nthat our compiler facilitates but cannot replace intelli\u00adgent algorithm-level exploration.  8. Related \nWork CUDA [19] provides a relatively simple programming model to application developers. However, many \nhardware details are ex\u00adposed since it is critical to utilize the hardware resources effi\u00adciently in \norder to achieve high performance. Given the non-linear optimization space, optimizing GPGPU programs \nhas been shown to be highly challenging [14]. To relieve this task from developers, there has been some \nrecent work on compiler support for GPGPU optimization. Ryoo et. al. [13] defined performance metrics \nto prune the optimization spaces. G-ADAPT [10] is a compiler framework to search and predict the best \nconfiguration for differ\u00adent input sizes for GPGPU programs. Compared to our proposed approach, this \ncompiler takes the optimized code and aims to adapt the code to different input sizes, while ours optimizes \nthe na\u00efve kernel functions. One closely related work to ours is the optimizing compiler framework for \naffine loops by Baskaran et. al. [2][3]. Their com\u00adpiler uses a polyhedral model to empirically search \nfor best loop transformation parameters, including the loop tiling sizes and unrolling factors. It is \nreported that their compiler achieves similar performance to CUBLAS1.0 for matrix multiplication and \nbetter performance for other kernels. In comparison, our proposed com\u00adpiler also uses empirical search \nto determine the best parameters to merge threads/thread blocks. The difference is that we propose a \nnovel way to achieve the effect of loop tiling and loop unrolling. In our proposed approach, we start \nfrom the finest-grain work item and aggregate work items together to exploit data reuse through registers \nand share memory. This approach fits particu\u00adlarly well with GPGPU programming models where work items \nare typically defined in a 2D/3D grid and aggregating work items usually bears a clear physical meaning \nin terms of the workload of each thread and each thread block. In addition, we propose ex\u00adplicit rules \nto check memory coalescing and approaches to covert non-coalesced accesses into coalesced ones. For the \napplications that we studied, including matrix multiplication, our compiler achieves much better performance \n(superior or close to CUBLAS 2.2, which is significantly improved over CUBLAS 1.0). In addi\u00adtion, the \nloop transformed code generated based on polyhedral models is often quite complex [11] while our optimized \ncode has relatively good understandability. Our compiler shares a common goal with CUDA-lite [17]: the \nuser provides a kernel function which only uses the global mem\u00adory and the compiler optimizes its memory \nusage. In CUDA lite, the compiler uses the programmer provided annotation to improve memory coalescing. \nIt also performs loop tiling to utilize shared memory. In comparison, our compiler does not require user \nanno\u00adtation. More importantly, our compiler does not only improve memory coalescing but also effectively \nachieves data sharing with the proposed thread/thread-block merge techniques. In addition, our compiler \ndistinguishes memory reads based on their target, the register or the shared memory, to make best use \nof either type of resource for data reuse. One interesting way to automatically generate GPGPU pro\u00adgrams \nis to translate OpenMP programs to CUDA programs [19]. Our proposed compiler is complementary to this \nwork as it can be used to further optimize the CUDA kernel functions generated from OpenMP programs. \n9. Conclusions In this paper, we present a compiler framework to optimize GPGPU programs. A set of novel \ncompiler techniques is proposed to improve GPU memory usage and distribute workload in threads and thread \nblocks. Our experimental results show that the opti\u00admized code achieves very high performance, often \nsuperior to manually optimized programs. In our future work, we plan to extend our compiler to support \nOpenCL programs so that a single na\u00efve kernel can be optimized for different GPUs from both NVIDIA and \nAMD/ATI. We are also investigating detailed analytical performance models to sim\u00adply the effort in design \nspace exploration. Acknowledgements We thank the anonymous reviewers and Professor Vivek Sarkar for their \nvaluable comments to improve our paper. This work is supported by an NSF CAREER award CCF-0968667. References \n[1] A. V. Aho, Ravi Sethi, and J. D. Ullman. Compilers, Principles, Techniques, &#38; Tools, Pearson \nEducation, 2007. [2] M. M. Baskaran, U. Bondhugula, S. Krishnamoorthy, J. Ramanujam, A. Rountev, and \nP. Sadayappan. A Compiler Framework for Optimi\u00adzation of Affine Loop Nests for GPGPUs. In Proc. International \nConference on Supercomputing, 2008. [3] M. Baskaran, U. Bondhugula, S. Krishnamoorthy, J. Ramanujam, \nA. Rountev, and P. Sadayappan. Automatic Data Movement and Com\u00adputation Mapping for Multi-level Parallel \nArchitectures with Explic\u00aditly Managed Memories. In Proc. ACM SIGPLAN Symposium on Principles and Practice \nof Parallel Programming, 2008. [4] J. Cooley and J. W. Tukey. An algorithm for the machine calculation \nof complex Fourier series, In Math. Comput, 1965. [5] N. Fujimoto. Fast Matrix-Vector Multiplication \non GeForce 8800 GTX. In Proc. IEEE International Parallel &#38; Distributed Process\u00ading Symposium, 2008 \n[6] N. Govindaraju, B. Lloyd, Y. Dotsenko, B. Smith, and J. Manfer\u00addelli. High performance discrete Fourier \ntransforms on graphics processors. In Proc. Supercomputing, 2008. [7] S. Hong and H. Kim. An analytical \nmodel for GPU architecture with memory-level and thread-level parallelism awareness. In Proc. In\u00adternational \nSymposium on Computer Architecture, 2009. [8] S.-I. Lee, T. Johnson, and R. Eigenmann. Cetus an extensible \ncompiler infrastructure for source-to-source transformation. In Proc. Workshops on Languages and Compilers \nfor Parallel Computing, 2003 [9] S. Lee, S.-J. Min, and R. Eigenmann. OpenMP to GPGPU: A com\u00adpiler framework \nfor automatic translation and optimization. In Proc. ACM SIGPLAN Symposium on Principles and Practice \nof Parallel Programming, 2009  [10] Y. Liu, E. Z. Zhang, amd X. Shen. A Cross-Input Adaptive Frame\u00adwork \nfor GPU Programs Optimization. In Proc. IEEE International Parallel &#38; Distributed Processing Symposium, \n2009. [11] L.-N. Pouchet, C. Bastoul, A. Cohen, and N. Vasilache. Iterative optimization in the polyhedral \nmode: part I, on dimensional time. In Proc. International Symposium on Code Generation and Optimiza\u00adtion, \n2007 [12] G. Ruetsch and P. Micikevicius. Optimize matrix transpose in CU-DA. NVIDIA, 2009. [13] S. Ryoo, \nC. I. Rodrigues, S. S. Stone, S. S. Baghsorkhi, S. Ueng, J. A. Stratton, and W. W. Hwu. Optimization \nspace pruning for a mul\u00adtithreaded GPU. In Proc. International Symposium on Code Genera\u00adtion and Optimization, \n2008. [14] S. Ryoo, C. I. Rodrigues, S. S. Baghsorkhi, S. S. Stone, D. B. Kirk, and W.W. Hwu. Optimization \nprinciples and application perform\u00adance evaluation of a multithreaded GPU using CUDA. In Proc. ACM SIGPLAN \nSymposium on Principles and Practice of Parallel Pro\u00adgramming, 2008. [15] S. S. Baghsorkhi, M. Delahaye, \nS. J. Patel, W. D. Gropp, and W. W. Hwu. An adaptive performance modling tool for GPU architectures. \nIn Proc. ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, 2010. [16] J. A. \nStratton, S. S. Stone, and W. W. Hwu. MCUDA:An efficient implementation of CUDA kernels on multicores. \nIMPACT Technical Report IMPACT-08-01, UIUC, Feb. 2008. [17] S. Ueng, M. Lathara, S. S. Baghsorkhi, and \nW. W. Hwu. CUDA-lite: Reducing GPU programming Complexity, In Proc. Workshops on Languages and Compilers \nfor Parallel Computing, 2008 [18] V. Volkov and J. W. Demmel. Benchmarking GPUs to tune dense linear \nalgebra. In Proc. Supercomputing, 2008. [19] NVIDIA CUDA Programming Guide, Version 2.1, 2008 [20] http://code.google.com/p/gpgpucompiler/ \n   \n\t\t\t", "proc_id": "1806596", "abstract": "<p>This paper presents a novel optimizing compiler for general purpose computation on graphics processing units (GPGPU). It addresses two major challenges of developing high performance GPGPU programs: effective utilization of GPU memory hierarchy and judicious management of parallelism.</p> <p>The input to our compiler is a na&#239;ve GPU kernel function, which is functionally correct but without any consideration for performance optimization. The compiler analyzes the code, identifies its memory access patterns, and generates both the optimized kernel and the kernel invocation parameters. Our optimization process includes vectorization and memory coalescing for memory bandwidth enhancement, tiling and unrolling for data reuse and parallelism management, and thread block remapping or address-offset insertion for partition-camping elimination. The experiments on a set of scientific and media processing algorithms show that our optimized code achieves very high performance, either superior or very close to the highly fine-tuned library, NVIDIA CUBLAS 2.2, and up to 128 times speedups over the naive versions. Another distinguishing feature of our compiler is the understandability of the optimized code, which is useful for performance analysis and algorithm refinement.</p>", "authors": [{"name": "Yi Yang", "author_profile_id": "81453634868", "affiliation": "North Carolina State University, Raleigh, NC, USA", "person_id": "P2184512", "email_address": "", "orcid_id": ""}, {"name": "Ping Xiang", "author_profile_id": "81453649362", "affiliation": "University of Central Florida, Orlando, FL, USA", "person_id": "P2184513", "email_address": "", "orcid_id": ""}, {"name": "Jingfei Kong", "author_profile_id": "81384613112", "affiliation": "University of Central Florida, Orlando, FL, USA", "person_id": "P2184514", "email_address": "", "orcid_id": ""}, {"name": "Huiyang Zhou", "author_profile_id": "81100265515", "affiliation": "North Carolina State University, Raleigh, NC, USA", "person_id": "P2184515", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1806596.1806606", "year": "2010", "article_id": "1806606", "conference": "PLDI", "title": "A GPGPU compiler for memory optimization and parallelism management", "url": "http://dl.acm.org/citation.cfm?id=1806606"}