{"article_publication_date": "06-05-2010", "fulltext": "\n Software Data Spreading: Leveraging Distributed Caches to Improve Single ThreadPerformance Md Kamruzzaman \nStevenSwanson DeanM.Tullsen Computer Science and Engineering University of California, San Diego {mkamruzz,swanson,tullsen}@cs.ucsd.edu \nAbstract Single thread performance remains an important consideration even for multicore, multiprocessor \nsystems.Asa result, techniques for improving single thread performance using multiple cores have received \nconsiderable attention. This work describes a technique, software data spreading, that leverages the \ncache capacity of ex\u00adtra cores and extra sockets rather than their computational re\u00adsources. Software \ndata spreading is a software-only technique that uses compiler-directed thread migration to aggregate \ncache capac\u00adity across cores and chips and improveperformance. Thispaper de\u00adscribes an automated scheme \nthat applies data spreading to various types of loops. Experiments with a set of SPEC2000, SPEC2006, \nNAS, and microbenchmarkworkloads showthat data spreading can provide speedupofover2,averaging17%forthe \nSPECandNAS applications on two systems. In addition, despite using more cores for the same computation, \ndata spreading actually saves power since it reduces access to DRAM. Categories and Subject Descriptors \nD.3.4[Programming Lan\u00adguages]: Processors Compilers General Terms Languages, Performance Keywords chip \nmultiprocessors, compilers, single-thread perfor\u00admance 1. Introduction Hardware parallelism has become \npervasive. Every high\u00adperformance processor is now multicore, and multi-socket con.g\u00adurations are common \neven on personal machines. Current main\u00adstreamofferings contain4to16execution coresoneach processor chip \n[1, 27], and there is no sign of the trend toward higher core counts slowing. This move allows users \nto bene.t from the availability of in\u00adcreasing transistor counts in the presence of thread-level paral\u00adlelism.However,itwouldbeamistaketo \nassume that single-thread or few-threads performance no longer matters in this new era. In fact, just \nthe opposite is true, and in some cases its importance will increase: Amdahl s law dictates that as architectures \nbecome more parallel, the inherently serial portions of applications will eventu\u00adallylimitthe performanceofallbutthemost \nembarrassinglyparal\u00adlel codes. Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage.To copyotherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. PLDI 10, June 5 10, 2010,Toronto, Ontario, Canada. Copyright c &#38;#169; 2010 \nACM 978-1-4503-0019-3/10/06. . . $10.00 As available hardware parallelism continues to increase, we \nan\u00adticipate the following three phenomena: (1) single thread perfor\u00admance will remain important, (2) \nin many machines some (if not most) of the cores will be idle, and (3) manual parallelization for most \nprogramswill remaindif.cult.Asa resultweneed automatic, generally applicable techniques for accelerating \nsingle threads on many-core systems. Automatic compiler-generated parallelism is one solution,butis still \nnoteffective fora large numberof appli\u00adcations. Therefore, we also need to pursue non-traditional paral\u00adlelization \ntechniques those which provide parallel speedup (many cores run an applicationfaster thana single core) \nwithout actually of.oading parallel computation to cores techniques like helper threading, speculative \nmultithreading, etc. These techniques exploit the computational ability of other cores to accelerate \nthe original thread. In this research, we intro\u00adduce the concept of software data spreading which exploits \nthe ca\u00adpacity of remote caches to accelerate a single thread. By migrating the thread among multiple \ncores with distinct caches, we can utilize the combined cache space of all ofthose cores. Aggregating \ncache capacity is of growing importance: Although total on-chip cache capacity continues to grow with \nMoore s law, the per-core cache capacity is notkeeping pace (e.g., 4MB total for the Intel Core2 Duo \nat introduction vs. 8MB total for a quad-core Nehalem chip). Previous work [4, 10, 22, 25] has attempted \nto aggregate cache space through specialized hardware support. Migrating a thread among multiple cores \nwhile it accesses large data structures provides three primary advantages. First, when the thread repeats \na memory access pattern (e.g., during multiple in\u00adstances of a loop), we force the thread to periodically \nmigrate be\u00adtween caches in the same pattern each time. As a result, the thread tends to access the same \nportion of the data when it is running on a speci.c core, resulting in lower miss rates. Second, even \nwhen the computation moves completely unpredictably through the data structures, periodic migrations \nresult in more of the data structure residinginthe combined caches.Asaresult,manyDRAM accesses becomefaster \n(and morepoweref.cient) cache-to-cache transfers. Finally, judicious migration while accessing very large \ndata struc\u00adtures (that tend to completely over-write the cache or caches) can, in some cases, shield \nother data and allow it to remain in another cache. We have developed a compiler-based, software-only \ndata spreading system that identi.es loops which have large data foot\u00adprints and suitable sharing patterns \n(e.g., high sharing between in\u00adstances of the same loop) and spreads those loops and the data they access \nacross multiple cores, both within a chip multiprocessor or across multiple dies or sockets. Data spreading \ncan be applied to any system, multicore or multiprocessor, with private L2 or L3 caches. Our experiments \nwith multiple Intel and AMD multipro\u00adcessor systems show that data spreading can speed up a range of \napplications by an average of 17%. Most impressive, data spread\u00ading achieves this speedup without any \nextra power consumption. Infact,inthe best case,it signi.cantly reducespowerbyavoiding DRAM accesses. \nFinally, data spreading requires no new hardware support and, since it relies on the system sdefault \ncaching behavior, does not threaten correctness. This paper is organized as follows. Section2discusses \nrelated research. Section3describes the motivation and basic data spread\u00ading approach. Details of our \nexperimental methodology are pre\u00adsented in Section 4. Section5describes the actual data spreading algorithm, \nevaluating several design options and presenting initial results. Section 6 examines software data spreading \nresults more closely acrossdifferent systemsandworkingset sizes.Italsoexam\u00adines its power ef.ciencyand \napplicability to multicores. Section7 concludes. 2. RelatedWork Anumber of compiler and architecture \ntechniques have been pro\u00adposed to leverage available parallelism to speed up a single thread, targeting \neither multithreaded processors or chip multiprocessors. This includes speculative multithreading architectures \n[17, 26, 30, 31], in which separate portions of the serial execution stream are executed (speculatively) \nin parallel, and it is only discovered later whetherthe parallelexecutionwas successful(i.e.,itdidnot \nviolate anydata or control dependences). More relevant to current multi\u00adcore and multithreaded processors, \nhowever, is the work on helper threading and speculative multithreading. Helper threads[5 7,13,14,21,23,37]executein \nparallel with the original thread, enabling that thread to run faster. Typically, those threads precompute \nload addresses to prefetch data into a shared level of the cache hierarchybefore the main thread issues \nthe load instruction. In manycases, the prefetching thread is distilled from instructionsinthe original \nthread[7,37].KimandYeung[14] present algorithms for generating helper thread code statically in the compiler, \nand Zhang, et al., [36] describe the generation of helper threads in a dynamic compilation system. Event-driven \ncompilation systems [34 36] use idle cores to invoke a compiler at run-time to perform optimizations \nin response to events detected by hardware performance monitors. An event\u00addriven compiler could apply \ndata spreading transparently at run time. Chakraborty,etal.,[16] presentatechnique called Computation \nSpreading, which also tries to leverage other caches via migration. Theymigrate threads so that some \ncores execute exclusively oper\u00adating systemcode,and othersexecuteexclusively user code.Inthis way, they \nget separation of data that is not typically shared over short time frames, and co-location of data more \nlikely to be shared. However, they do not achieve the spreading effect that is the pri\u00admary contributor \nto our speedups. Cooperative Caching [4] is an architectural technique with the same goal as data spreading \n using caches from neighboring cores to support the execution of a single thread. Theydo this by allow\u00ading \ncaches to store data that have been evicted from other private caches. Thus, caches that are lightly \nused or idle can act as large victim caches [12]. However, this can only transform misses into cache-to-cache \ntransfers. In manycases, data spreading effectively transforms misses into local hits. Other work [10, \n22, 25] has sug\u00adgested blurring the distinction between private and shared caches even further.However, \neachof these techniques require changesto the architecture. Further, none of these hardware techniques \nwork across multiple chips. There is a tremendous body of work on detecting parallelism in sequentially-programmed \ncode. In some ways our work is similar in that we also detect the independence or lack of independence \nof data touched by different segments of code, in the process of applying our optimization. However, \nbecause the normal caching system ensures correctness,wecan observethisbehavioratamuch coarser level \nand make decisions based on trends and tendencies rather than guarantees. There have also been several \nprojects that attempt to share re\u00adsources across multiple cores in a single CMP to speed up a sin\u00adgle \nthread. The work in [29] exploits migration to aggregate cache space,but again relies on hardware mechanisms. \nOur approach is software only and works on current systems. Core fusion proces\u00adsors [11] take a different \napproach and allow multiple cores to be dynamically combined into a single larger core. Conjoined core \ndesigns [18] allow two cores to share even the lowest-level (L1) caches. These approaches require signi.cant \nchanges to the hard\u00adware and do not involve thread migration for sharing. Cache blocking[20,28]isacompilationtechniquethat \nreorders computation on a single core to increase locality and reduce cache misses. Data spreading could \nactually be complementary to tech\u00adniques such as this, because they can reduce the reuse distance for \nonly a subset of the accesses, and must still incur some capacity misses on anystructure that does not \n.t in the cache. 3. Software data spreading Software data spreading allows a single thread of computation \nto bene.t from the private cache capacity of idle cores in the system, whether on the same processor \nor on other, idle sockets. As the thread executes, it moves from core to core, spreading its accesses \nacross the caches. If we time the migrations correctly, the thread can either avoid misses in the private \ncache it happens to be using or have its misses serviced out of another core rather than from main memory. \nThis results in reduced execution time and energy consumption, since accesses to main memory are both \nslow and power-hungry. Data spreading works best in systems with large private caches (typically L2 or \nL3), spread across multiple cores, dies, or sockets. As shared cachesface scaling limitations, weexpectprivate \ncaches (or caches shared among a subset of cores) to become more com\u00admon. It also applies to any machine \nwith multiple processors on separatedies,sinceeachdie includesaprivate(relativetotheother dies or sockets) \ncache. In the next three subsections, we give some examples of how software data spreading applies in \ndifferent scenarios, describe our implementation of the data spreading mechanism, and then discuss its \npotential impacts on performance and power ef.ciency. 3.1 Examples Algorithm1containsa pairof loops thatare \ngood candidates for spreading. It accesses two arrays, a and b, and we assume that the combined size \nof both arrays is roughly eight times the capacity of asingle cache.Toillustratedata spreading,wewill \nassume(tokeep the example simple) a CMP with only private caches. Algorithm1 Simple example code Data \nspreading can acceler\u00adate this code if the working set does not .t in a single L2 cache. fori =1 to 100 \ndo // Loop0 forj =1 to 1000 do // Loop1 aj = aj-1 + aj+1 endfor forj =1 to 2000 do // Loop2 bj = bj-1 \n+ bj+1 endfor endfor If this code executes on a single core, the cache miss rate will be very high, \nsince Loops 1 and 2 will destructively interfere.  Figure 1. One iterationof the outer loopin Algorithm1with \ndata spreading across8 cores. However, if we have an 8-core CMP, the aggregate capacity of the cores \nprivate caches is enough to hold both arrays, and there will be no capacity misses. Our spreading technique \nallows us to perform this distribution without any hardware support. Figure1 illustrates the distribution \nof data across the private caches in the system. Data spreading may provide bene.ts even if the data \nstructures are too large to .t in the entire on-chip cache space. It will still col\u00adlect a larger portion \nof the data into the private caches. Alternately, we could spread as much of b as will .t across allbut \none of the caches, andisolate the rest of it to a single cache. Accesses to the spread out portion willbefast, \nwhile the remainder willbe slower. If b isvery large, then we can isolateexecutionof Loop2ina sin\u00adgle \ncore, while spreadingLoop1totakeadvantageofthe remaining caches.Loop2will thrash initscache(thisisunavoidable,since \nb is large),but Loop1will remain largely unaffected. Our compi\u00adlation system does not currently support \nthis last option. Algorithm2 Data spreading can reduce the cost of misses for irregular access patterns. \nfori =1 to 100 do p = list while p =.null do p = p . next end while Shuf.e(list) endfor Software data \nspreading can also speed up irregular access pat\u00adterns. Algorithm2 traversesalinkedlistthatistoolargeto.tin \na single cache, then a second function shuf.es the list. On a single core,mostofthe accesseswouldmissinthecache.With \nspreading, the number of misses to private cache remains mostly unchanged, but nearly all of them (assuming \nthe working set .ts in the com\u00adbined caches) will be satis.ed via cache-to-cache transfers, saving an \nexpensive off-chip access. Current multicores do not typically support fast cache-to-cache transfers, \nso our experiments do not showlargegainsin this case;however,weexpect thatto change in future chips. \nStill, if the shuf.e does not completely random\u00adize the ordering, we will seegainseven withoutfast cache-to-cache \ntransfers. 3.2 Implementing data spreading Theonlysupportour implementationrequiresisfromthe operating \nsystem: The OS must provide the means to pin a thread to a particular (new) core, and a mechanism to \ndetermine how many cores areavailabletotheapplication.With this support, migrating from one coreto another \nrequiresa single system call. This support already exists in most operating systems running on multicores \nor multiprocessors. The main challenge in software data spreading is determining when to migrate. Our \ncompiler pro.les applications to identify the data-intensive loops it will spread. Then it adds code \nto count loop iterations and call the migration function periodically. Algorithm3 shows the code from \nAlgorithm1with theextra code for spreading across eight caches. We discuss the loop selection process \nand spreading policies in Section 5. Choosing the loop s period requires balancing two opposing forces: \nSpreading data across as manycores as possible is desirable, since it will spread cache pressure out \nevenly and avoid spurious cache con.icts. However, a shorter period means additional thread migrations, \nwhich can be expensive. Algorithm3 Data spreading in action The codein Algorithm1 after the data spreading \ntransformation. fori =1 to 100 do forcpu =0 to7 do MigrateTo(cpu) forj = 125 \u00d7 cpu to 125 \u00d7 (cpu + 1) \ndo aj = aj-1 + aj+1 endfor endfor forcpu =0 to7 do MigrateTo(cpu) forj = 250 \u00d7 cpu to 250 \u00d7 (cpu + 1) \ndo bj = bj-1 + bj+1 endfor endfor endfor  3.3 The cost of data spreading Like most optimizations, data \nspreading is not free. There are three potential costs that we must manage to make the technique pro.table: \nits impact on the availability of other cores, the cost of thread migration, and its impact on power \nand energy consumption. Performance impact on other threads Since data spreading in\u00adcreasesthe numberof \ncoresathreadis using,it could potentiallyin\u00adterfere with other threads performance. However, when idle \ncores are unavailable, or better used for other purposes, we can forgo data spreading so the opportunity \ncost of using other cores is very low.We assumethemain threadqueriestheOSto.ndthe number of available \ncores. If it returns 0, the code runs without spreading enabled, and the only sources of overhead are \nthe quick (it sim\u00adply returns null in this case) and infrequent calls to the Migrate To function. Thread \nmigration cost This is the primary cost for implementing data spreading in the systems we tested. GNU/Linux \n(starting with Linux kernel 2.6) provides an API to pin threads to processors, but it is an expensive \noperation. Linux 2.6.18 on an Intel Nehalem processor takes about 14\u00b5s toperform one migration. If the \nother core is in a sleep state, the cost could be even higher. The high cost of migration in current \nsystems restricts our ability to employ data spreading successfully on a single CMP, as explored further \nin Section 6.6. Proposals for hardware migration support such System Information Intel Pentium-4 Intel \nCore2Quad Intel Nehalem AMD Opteron CPU Model Northwood Harpertown Gainestown Opteron 2427 #of Socket\u00d7#of \nDie\u00d7#of core 4\u00d71\u00d71 2\u00d72\u00d72 2\u00d71\u00d74 2\u00d71\u00d76 Last Level Cache 2M 6M (per die) 8M 6M Cache to cache transfer latency \n300-500 cycles 150-250 cycles 120-170 cycles 210-215 cycles Memory access latency 400-500 cycles 300-350 \ncycles 200-240 cycles 230-260 cycles Migration cost 14\u00b5s 10\u00b5s 14\u00b5s 9\u00b5s Linux Kernel 2.6.9 2.6.28 2.6.18 \n2.6.29 Table 1. The multiprocessor system con.gurations used to test data spreading. Benchmark Resident \nMemory Benchmark Resident Memory Art T 3MB BT A 298 MB Applu T 20 MB CG A 55 MB Equake T 12 MB LU A 45 \nMB Mcf T 45 MB MG A 437 MB Swim T 56 MB SP A 79 MB Art R 4MB BT B 1200 MB Applu R 180 MB CG B 399 MB \nEquake R 49 MB LU B 173 MB Mcf R 154 MB MG B 437 MB Swim R 191 MB SP B 314 MB Libq R 64 MB Table 2. \nResident memory requirements for benchmarks. as[3] could reduce this signi.cantly.Amigration that requiresOS \nintervention can be made to cost on the order of 2\u00b5s ona3GHz processor withOS changesbut no hardware \nsupport [32]. The other costof migration, besidestheoverheadof transferring the thread context itself, \nis cold start effects the cost of moving frequently accessed data into the new cache, the loss of branch \npre\u00addictor state, BTB state, etc.Typically, cache stateis themostex\u00adpensive to move. Data spreading, \nwhen done correctly, minimizes this costbymovinga threadtoa locationwhere future accesses are already \npresent in the cache (and away from a core where they are not present). Power cost In contrast to traditional \nparallelization and most of the non-traditional parallelization techniques described in Sec\u00adtion 2, data \nspreading can have a positive effect on both power and energy consumption. Other techniques achieve speedups \nby exe\u00adcuting instructions on otherwise unused cores, and those instruc\u00adtions can consume extra power \nand energy. The only extra instruc\u00adtions thatdata spreadingexecutes arein the migration function that \nmoves threads between cores. More importantly, only one core is actively executing at anytime. Most current \nmulti-core processors lack the ability to power\u00adgate or even voltage-scale individual cores. In that \ncase, one core being active implies theyare all powered (and therefore dissipating leakage power). An \nidle core uses less power than a running core, but that is true whether the same core is always idle \nor whether activity (and, therefore, inactivity) shifts from core to core. Recent processors, such as \nNehalem [1], are able to voltage-scale individ\u00adual cores, or even power-gate cores. This will lower the \npower con\u00adsumption of idle cores even further, strengtheningthe power argu\u00adment for data spreading. Powergating \nwill increase the migration latencysomewhat,butwhendata spreadingiseffective,theidlepe\u00adriodsforeach corearefarlongerthanthetime \nrequiredtowakethe core from sleep [15, 19].We can reduce this cost furtherby pre\u00addictively waking the \ncore several loop iterations before we plan to migrate. Section 6.4 quanti.es the power bene.ts of data \nspreading.  3.4 Which cores to use The largestgains from data spreading typically come from aggre\u00adgating \nthe largest caches.Forexample, ona multi-socket Nehalem architecture, we gain more from aggregating L3 \ncaches across sockets than from aggregating L2 caches on-chip. This will depend on the application if \ntheworking set .tsin fourL2 caches,but not one, it will onlygain from spreading at that level. Because \nof the large working sets of the applications we study, we focus on spreadingatthesocketlevel,exceptfor \nSection6.6.Wealso.nd we typicallygain from using the minimum number of cores to ag\u00adgregate the caches, \nbecause this reduces the frequency of migra\u00adtion. So for example, with two Nehalem sockets, our best \ngains usually come from data spreading across two cores (one on each socket), rather than across all \neight cores.For the Core2Quad, we use one coreper die(two per socket), and on the Opteron, we use one \nper socket.Tuning data spreading for individual loops and to work across multiple levels of the memory \nhierarchyis the subject of future work. 4. Methodology To evaluate our approach to data spreading, we \nimplement it un\u00adder Linux 2.6 on several real systems withIntel and AMD IA32 processors. The system con.gurations \nare given in Table 1. We compute the latencyinformation for our machines by running mi\u00adcrobenchmarks. \nThe migration cost shown here is the latencyto call the function sched setaf.nity that changes the CPU \naf.nity mask, and causes thread migration. All experiments run under Linux 2.6. We use gcc 4.1.2 with \noptimization .ag -O3 for all of our compila\u00adtions, PIN 2.6 for pro.ling and analysis, and hardware performance \ncounters to measure cache miss rates. Our benchmark applications are a set of memory intensive ap\u00adplications \nfrom Spec2000 [8], and the serial version of NAS [2]. We also pick one Spec2006 [9] integer benchmark \nLibquantum, sinceitiseasytovaryitsworkingsetsize.To identifythe memory intensive benchmarks we use the \ncycle accurate simulator, SMT-SIM [33] (con.gured to roughly match one of our real experimen\u00adtal machines) \nto identify those workloads that achieve at least 75% speedup with a perfect L1 data cache. Our rationale \nfor selecting thissetofworkloadsisthatifaworkloadshows little bene.tfroma perfect memory hierarchy, there \nis no reason to expect data spread\u00ading to offer any bene.t. Furthermore, since it is a software-only \ntechnique, there is no danger of it penalizing workloads if it is not useful for a particular application \nit should not be applied. Our system currentlyworks onC code.We were able to convertFor\u00adtran77codeusinganf2cconverter,butwe \nwerenotabletoconvert Fortran90 code, which excludes some of the SPEC benchmarks. For the SPEC benchmarks, \nwe pro.le using the train input, and experiment with the reference input for some experiments we also \nrun the train inputs, just to get more variation in working set size. When train performs better than \nref, it is typically because the workingsetfallsintheoptimal range, ratherthandueto increased pro.le \naccuracy.ForNAS, we pro.le using the W input, and ex\u00adperiment with both the Aand Binputs.Wedo all theexperiments \n\t\t\t", "proc_id": "1806596", "abstract": "<p>Single thread performance remains an important consideration even for multicore, multiprocessor systems. As a result, techniques for improving single thread performance using multiple cores have received considerable attention. This work describes a technique, <i>software data spreading</i>, that leverages the cache capacity of extra cores and extra sockets rather than their computational resources. Software data spreading is a software-only technique that uses compiler-directed thread migration to aggregate cache capacity across cores and chips and improve performance. This paper describes an automated scheme that applies data spreading to various types of loops. Experiments with a set of SPEC2000, SPEC2006, NAS, and microbenchmark workloads show that data spreading can provide speedup of over 2, averaging 17% for the SPEC and NAS applications on two systems. In addition, despite using more cores for the same computation, data spreading actually saves power since it reduces access to DRAM.</p>", "authors": [{"name": "Md Kamruzzaman", "author_profile_id": "81464675955", "affiliation": "University of California - San Diego, San Diego, CA, USA", "person_id": "P2184625", "email_address": "", "orcid_id": ""}, {"name": "Steven Swanson", "author_profile_id": "81100070924", "affiliation": "University of California - San Diego, San Diego, CA, USA", "person_id": "P2184626", "email_address": "", "orcid_id": ""}, {"name": "Dean M. Tullsen", "author_profile_id": "81100552297", "affiliation": "University of California - San Diego, San Diego, CA, USA", "person_id": "P2184627", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1806596.1806648", "year": "2010", "article_id": "1806648", "conference": "PLDI", "title": "Software data spreading: leveraging distributed caches to improve single thread performance", "url": "http://dl.acm.org/citation.cfm?id=1806648"}