{"article_publication_date": "06-05-2010", "fulltext": "\n FlumeJava: Easy, Ef.cient Data-Parallel Pipelines Craig Chambers, Ashish Raniwala, Frances Perry, Stephen \nAdams, Robert R. Henry, Robert Bradshaw, Nathan Weizenbaum Google, Inc. {chambers,raniwala,fjp,sra,rrh,robertwb,nweiz}@google.com \nAbstract MapReduce and similar systems signi.cantly ease the task of writ\u00ading data-parallel code. However, \nmany real-world computations re\u00adquire a pipeline of MapReduces, and programming and managing such pipelines \ncan be dif.cult. We present FlumeJava, a Java li\u00adbrary that makes it easy to develop, test, and run ef.cient \ndata\u00adparallel pipelines. At the core of the FlumeJava library are a cou\u00adple of classes that represent \nimmutable parallel collections, each supporting a modest number of operations for processing them in \nparallel. Parallel collections and their operations present a simple, high-level, uniform abstraction \nover different data representations and execution strategies. To enable parallel operations to run ef.\u00adciently, \nFlumeJava defers their evaluation, instead internally con\u00adstructing an execution plan data.ow graph. \nWhen the .nal results of the parallel operations are eventually needed, FlumeJava .rst op\u00adtimizes the \nexecution plan, and then executes the optimized opera\u00adtions on appropriate underlying primitives (e.g., \nMapReduces). The combination of high-level abstractions for parallel data and compu\u00adtation, deferred \nevaluation and optimization, and ef.cient parallel primitives yields an easy-to-use system that approaches \nthe ef.\u00adciency of hand-optimized pipelines. FlumeJava is in active use by hundreds of pipeline developers \nwithin Google. Categories and Subject Descriptors D.1.3 [Concurrent Pro\u00adgramming]: Parallel Programming \nGeneral Terms Algorithms, Languages, Performance Keywords data-parallel programming, MapReduce, Java \n1. Introduction Building programs to process massive amounts of data in parallel can be very hard. MapReduce \n[6 8] greatly eased this task for data\u00adparallel computations. It presented a simple abstraction to users \nfor how to think about their computation, and it managed many of the dif.cult low-level tasks, such as \ndistributing and coordinating the parallel work across many machines, and coping robustly with failures \nof machines, networks, and data. It has been used very successfully in practice by many developers. MapReduce \ns success in this domain inspired the development of a number of related systems, including Hadoop [2], \nLINQ/Dryad [20], and Pig [3]. Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for \npro.t or commercial advantage and that copies bear this notice and the full citation on the .rst page. \nTo copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. PLDI 10, June 5 10, 2010, Toronto, Ontario, Canada Copyright c &#38;#169; 2010 \nACM 978-1-4503-0019-3/10/06. . . $10.00 MapReduce works well for computations that can be broken down \ninto a map step, a shuf.e step, and a reduce step, but for many real-world computations, a chain of MapReduce \nstages is required. Such data-parallel pipelines require additional coordination code to chain together \nthe separate MapReduce stages, and require addi\u00adtional work to manage the creation and later deletion \nof the inter\u00admediate results between pipeline stages. The logical computation can become obscured by \nall these low-level coordination details, making it dif.cult for new developers to understand the computa\u00adtion. \nMoreover, the division of the pipeline into particular stages becomes baked in to the code and dif.cult \nto change later if the logical computation needs to evolve. In this paper we present FlumeJava, a new \nsystem that aims to support the development of data-parallel pipelines. FlumeJava is a Java library centered \naround a few classes that represent parallel collections. Parallel collections support a modest number \nof par\u00adallel operations which are composed to implement data-parallel computations. An entire pipeline, \nor even multiple pipelines, can be implemented in a single Java program using the FlumeJava ab\u00adstractions; \nthere is no need to break up the logical computation into separate programs for each stage. FlumeJava \ns parallel collections abstract away the details of how data is represented, including whether the data \nis represented as an in-memory data structure, as one or more .les, or as an ex\u00adternal storage service \nsuch as a MySql database or a Bigtable [5]. Similarly, FlumeJava s parallel operations abstract away \ntheir im\u00adplementation strategy, such as whether an operation is implemented as a local sequential loop, \nor as a remote parallel MapReduce invo\u00adcation, or (in the future) as a query on a database or as a streaming \ncomputation. These abstractions enable an entire pipeline to be ini\u00adtially developed and tested on small \nin-memory test data, running in a single process, and debugged using standard Java IDEs and de\u00adbuggers, \nand then run completely unchanged over large production data. They also confer a degree of adaptability \nof the logical Flume-Java computations as new data storage mechanisms and execution services are developed. \nTo achieve good performance, FlumeJava internally implements parallel operations using deferred evaluation. \nThe invocation of a parallel operation does not actually run the operation, but instead simply records \nthe operation and its arguments in an internal exe\u00adcution plan graph structure. Once the execution plan \nfor the whole computation has been constructed, FlumeJava optimizes the exe\u00adcution plan, for example \nfusing chains of parallel operations to\u00adgether into a small number of MapReduce operations. FlumeJava \nthen runs the optimized execution plan. When running the exe\u00adcution plan, FlumeJava chooses which strategy \nto use to imple\u00adment each operation (e.g., local sequential loop vs. remote parallel MapReduce, based \nin part on the size of the data being processed), places remote computations near the data they operate \non, and per\u00adforms independent operations in parallel. FlumeJava also manages the creation and clean-up \nof any intermediate .les needed within the computation. The optimized execution plan is typically sev\u00aderal \ntimes faster than a MapReduce pipeline with the same logical structure, and approaches the performance \nachievable by an expe\u00adrienced MapReduce programmer writing a hand-optimized chain of MapReduces, but \nwith signi.cantly less effort. The FlumeJava program is also easier to understand and change than the \nhand\u00adoptimized chain of MapReduces. As of March 2010, FlumeJava has been in use at Google for nearly \na year, with 175 different users in the last month and many pipelines running in production. Anecdotal \nreports are that users .nd FlumeJava signi.cantly easier to work with than MapReduce. Our main contributions \nare the following: We have developed a Java library, based on a small set of composable primitives, \nthat is both expressive and convenient.  We show how this API can be automatically transformed into \nan ef.cient execution plan, using deferred evaluation and opti\u00admizations such as fusion.  We have developed \na run-time system for executing optimized plans that selects either local or parallel execution automatically \nand which manages many of the low-level details of running a pipeline.  We demonstrate through benchmarking \nthat our system is ef\u00adfective at transforming logical computations into ef.cient pro\u00adgrams.  Our system \nis in active use by many developers, and has pro\u00adcessed petabytes of data.  The next section of this \npaper gives some background on MapReduce. Section 3 presents the FlumeJava library from the user s point \nof view. Section 4 describes the FlumeJava optimizer, and Section 5 describes the FlumeJava executor. \nSection 6 assesses our work, using both usage statistics and benchmark performance results. Section 7 \ncompares our work to related systems. Section 8 concludes. 2. Background on MapReduce FlumeJava builds \non the concepts and abstractions for data-parallel programming introduced by MapReduce. A MapReduce has \nthree phases: 1. The Map phase starts by reading a collection of values or key/value pairs from an input \nsource, such as a text .le, binary record-oriented .le, Bigtable, or MySql database. Large data sets \nare often represented by multiple, even thousands, of .les (called shards), and multiple .le shards can \nbe read as a single logical input source. The Map phase then invokes a user-de.ned function, the Mapper, \non each element, independently and in parallel. For each input element, the user-de.ned function emits \nzero or more key/value pairs, which are the outputs of the Map phase. Most MapReduces have a single (possibly \nsharded) input source and a single Mapper, but in general a single MapReduce can have multiple input \nsources and associated Mappers. 2. The Shuf.e phase takes the key/value pairs emitted by the Mappers \nand groups together all the key/value pairs with the same key. It then outputs each distinct key and \na stream of all the values with that key to the next phase. 3. The Reduce phase takes the key-grouped \ndata emitted by the Shuf.e phase and invokes a user-de.ned function, the Reducer, on each distinct key-and-values \ngroup, independently and in parallel. Each Reducer invocation is passed a key and an iterator over all \nthe values associated with that key, and emits zero  or more replacement values to associate with the \ninput key. Oftentimes, the Reducer performs some kind of aggregation over all the values with a given \nkey. For other MapReduces, the Reducer is just the identity function. The key/value pairs emitted from \nall the Reducer calls are then written to an output sink, e.g., a sharded .le, Bigtable, or database. \nFor Reducers that .rst combine all the values with a given key using an associative, commutative operation, \na separate user\u00adde.ned Combiner function can be speci.ed to perform partial combining of values associated \nwith a given key during the Map phase. Each Map worker will keep a cache of key/value pairs that have \nbeen emitted from the Mapper, and strive to combine locally as much as possible before sending the com\u00adbined \nkey/value pairs on to the Shuf.e phase. The Reducer will typically complete the combining step, combining \nvalues from different Map workers. By default, the Shuf.e phase sends each key-and-values group to a \ndeterministically but randomly chosen Reduce worker ma\u00adchine; this choice determines which output .le \nshard will hold that key s results. Alternatively, a user-de.ned Sharder func\u00adtion can be speci.ed that \nselects which Reduce worker machine should receive the group for a given key. A user-de.ned Sharder can \nbe used to aid in load balancing. It also can be used to sort the output keys into Reduce buckets, with \nall the keys of the ith Reduce worker being ordered before all the keys of the i+1st Reduce worker. Since \neach Reduce worker processes keys in lexicographic order, this kind of Sharder can be used to produce \nsorted output. Many physical machines can be used in parallel in each of these three phases. MapReduce \nautomatically handles the low-level issues of se\u00adlecting appropriate parallel worker machines, distributing \nto them the program to run, managing the temporary storage and .ow of intermediate data between the three \nphases, and synchronizing the overall sequencing of the phases. MapReduce also automatically copes with \ntransient failures of machines, networks, and software, which can be a huge and common challenge for \ndistributed pro\u00adgrams run over hundreds of machines. The core of MapReduce is implemented in C++, but \nlibraries exist that allow MapReduce to be invoked from other languages. For example, a Java version \nof MapReduce is implemented as a JNI veneer on top of the C++ version of MapReduce. MapReduce provides \na framework into which parallel computa\u00adtions are mapped. The Map phase supports embarrassingly parallel, \nelement-wise computations. The Shuf.e and Reduce phases sup\u00adport cross-element computations, such as \naggregations and group\u00ading. The art of programming using MapReduce mainly involves mapping the logical \nparallel computation into these basic opera\u00adtions. Many computations can be expressed as a MapReduce, \nbut many others require a sequence or graph of MapReduces. As the complexity of the logical computation \ngrows, the challenge of map\u00adping it into a physical sequence of MapReduces increases. Higher\u00adlevel concepts \nsuch as count the number of occurrences or join tables by key must be hand-compiled into lower-level \nMapReduce operations. In addition, the user takes on the additional burdens of writing a driver program \nto invoke the MapReduces in the proper sequence, managing the creation and deletion of intermediate .les \nholding the data passed between MapReduces, and handling fail\u00adures across MapReduces. 3. The FlumeJava \nLibrary In this section we present the interface to the FlumeJava library, as seen by the FlumeJava user. \nThe FlumeJava library aims to offer constructs that are close to those found in the user s logical computation, \nand abstract away from the lower-level physical details of the different kinds of input and output storage \nformats and the appropriate partitioning of the logical computation into a graph of MapReduces. 3.1 \nCore Abstractions The central class of the FlumeJava library is PCollection<T>, a (possibly huge) immutable \nbag of elements of type T.A PCollection can either have a well-de.ned order (called a se\u00adquence), or \nthe elements can be unordered (called a collection). Because they are less constrained, collections are \nmore ef.cient to generate and process than sequences. A PCollection<T> can be created from an in-memory \nJava Collection<T>.A PCollection<T> can also be created by reading a .le in one of several possible formats. \nFor example, a text .le can be read as a PCollection<String>, and a binary record-oriented .le can be \nread as a PCollection<T>, given a speci.cation of how to decode each binary record into a Java object \nof type T. Data sets repre\u00adsented by multiple .le shards can be read in as a single logical PCollection. \nFor example:1 PCollection<String> lines = readTextFileCollection(\"/gfs/data/shakes/hamlet.txt\"); PCollection<DocInfo> \ndocInfos = readRecordFileCollection(\"/gfs/webdocinfo/part-*\", recordsOf(DocInfo.class)); In this code, \nrecordsOf(...) speci.es a particular way in which a DocInfo instance is encoded as a binary record. Other \npre\u00adde.ned encoding speci.ers are strings() for UTF-8-encoded text, ints() for a variable-length encoding \nof 32-bit integers, and pairsOf(e1,e2 ) for an encoding of pairs derived from the en\u00adcodings of the components. \nUsers can specify their own custom encodings. A second core class is PTable<K,V>, which represents a \n(possibly huge) immutable multi-map with keys of type K and values of type V. PTable<K,V> is a subclass \nof PCollection<Pair<K,V>>, and indeed is just an unordered bag of pairs. Some FlumeJava operations apply \nonly to PCollections of pairs, and in Java we choose to de.ne a subclass to capture this abstraction; \nin another language, PTable<K,V> might better be de\u00ad.ned as a type synonym of PCollection<Pair<K,V>>. \nThe main way to manipulate a PCollection is to invoke a data-parallel operation on it. The FlumeJava \nlibrary de.nes only a few primitive data-parallel operations; other operations are im\u00adplemented in terms \nof these primitives. The core data-parallel primitive is parallelDo(), which supports elementwise compu\u00adtation \nover an input PCollection<T> to produce a new output PCollection<S>. This operation takes as its main \nargument a DoFn<T, S>, a function-like object de.ning how to map each value in the input PCollection<T> \ninto zero or more values to appear in the output PCollection<S>. It also takes an indication of the kind \nof PCollection or PTable to produce as a result. For example: PCollection<String> words = lines.parallelDo(new \nDoFn<String,String>() { void process(String line, EmitFn<String> emitFn) { for (String word : splitIntoWords(line)) \n{ emitFn.emit(word); } } }, collectionOf(strings())); In this code, collectionOf(strings()) speci.es \nthat the parallelDo() operation should produce an unordered PCollection whose String elements should \nbe encoded using UTF-8. Other options include sequenceOf(elemEncoding ) 1 Some of these examples have \nbeen simpli.ed in minor ways from the real versions, for clarity and compactness. for ordered PCollections \nand tableOf(keyEncoding, valueEncoding ) for PTables. emitFn is a call-back function FlumeJava passes \nto the user s process(...) method, which should invoke emitFn.emit(outElem ) for each outElem that should \nbe added to the output PCollection. FlumeJava includes subclasses of DoFn, e.g., MapFn and FilterFn, \nthat provide simpler interfaces in special cases. There is also a version of parallelDo() that allows \nmultiple output PCollections to be produced simultaneously from a single traversal of the input PCollection. \nparallelDo() can be used to express both the map and reduce parts of MapReduce. Since they will potentially \nbe distributed remotely and run in parallel, DoFn functions should not access any global mutable state \nof the enclosing Java program. Ideally, they should be pure functions of their inputs. It is also legal \nfor DoFn objects to maintain local instance variable state, but users should be aware that there may \nbe multiple DoFn replicas operating concurrently with no shared state. These restrictions are shared \nby MapReduce as well. A second primitive, groupByKey(), converts a multi-map of type PTable<K,V> (which \ncan have many key/value pairs with the same key) into a uni-map of type PTable<K, Collection<V>> where \neach key maps to an unordered, plain Java Collection of all the values with that key. For example, the \nfollowing computes a table mapping URLs to the collection of documents that link to them: PTable<URL,DocInfo> \nbacklinks = docInfos.parallelDo(new DoFn<DocInfo, Pair<URL,DocInfo>>() { void process(DocInfo docInfo, \nEmitFn<Pair<URL,DocInfo>> emitFn) { for (URL targetUrl : docInfo.getLinks()) { emitFn.emit(Pair.of(targetUrl, \ndocInfo)); } } }, tableOf(recordsOf(URL.class), recordsOf(DocInfo.class))); PTable<URL,Collection<DocInfo>> \nreferringDocInfos = backlinks.groupByKey(); groupByKey() captures the essence of the shuf.e step of MapRe\u00adduce. \nThere is also a variant that allows specifying a sorting order for the collection of values for each \nkey. A third primitive, combineValues(), takes an input PTable<K, Collection<V>> and an associative combining \nfunction on Vs, and returns a PTable<K, V> where each input collection of values has been combined into \na single output value. For example: PTable<String,Integer> wordsWithOnes = words.parallelDo( new DoFn<String, \nPair<String,Integer>>() { void process(String word, EmitFn<Pair<String,Integer>> emitFn) { emitFn.emit(Pair.of(word, \n1)); } }, tableOf(strings(), ints())); PTable<String,Collection<Integer>> groupedWordsWithOnes = wordsWithOnes.groupByKey(); \nPTable<String,Integer> wordCounts = groupedWordsWithOnes.combineValues(SUM_INTS); combineValues() is \nsemantically just a special case of parallelDo(), but the associativity of the combining function al\u00adlows \nit to be implemented via a combination of a MapReduce com\u00adbiner (which runs as part of each mapper) and \na MapReduce re\u00adducer (to .nish the combining), which is more ef.cient than doing all the combining in \nthe reducer. A fourth primitive, flatten(), takes a list of PCollection<T>s and returns a single PCollection<T> \nthat contains all the elements of the input PCollections. flatten() does not actually copy the inputs, \nbut rather creates a view of them as one logical PCollection. A pipeline typically concludes with operations \nthat write the .nal result PCollections to external storage. For example: wordCounts.writeToRecordFileTable( \n\"/gfs/data/shakes/hamlet-counts.records\"); Because PCollections are regular Java objects, they can be \nmanipulated like other Java objects. In particular, they can be passed into and returned from regular \nJava methods, and they can be stored in other Java data structures (although they can\u00adnot be stored in \nother PCollections). Also, regular Java con\u00adtrol .ow constructs can be used to de.ne computations involving \nPCollections, including functions, conditionals, and loops. For example: Collection<PCollection<T2>> \npcs = new Collection<...>(); for (Task task : tasks) { PCollection<T1> p1 = ...; PCollection<T2> p2; \nif (isFirstKind(task)) { p2 = doSomeWork(p1); } else { p2 = doSomeOtherWork(p1); } pcs.add(p2); }  3.2 \nDerived Operations The FlumeJava library includes a number of other operations on PCollections, but these \nothers are derived operations, imple\u00admented in terms of these primitives, and no different than helper \nfunctions the user could write. For example, the count() function takes a PCollection<T> and returns \na PTable<T, Integer> mapping each distinct element of the input PCollection to the number of times it \noccurs. This function is implemented in terms of parallelDo(), groupByKey(), and combineValues(), using \nthe same pattern as was used to compute wordCounts above. That code could thus be simpli.ed to the following: \nPTable<String,Integer> wordCounts = words.count(); Another library function, join(), implements a kind \nof join over two or more PTables sharing a common key type. When applied to a multi-map PTable<K, V1> \nand a multi\u00admap PTable<K, V2>, join() returns a uni-map PTable<K, Tuple2<Collection<V1>, Collection<V2>>> \nthat maps each key in either of the input tables to the collection of all values with that key in the \n.rst table, and the collection of all values with that key in the second table. This resulting table \ncan be processed fur\u00adther to compute a traditional inner-or outer-join, but oftentimes it is more ef.cient \nto be able to manipulate the value collections directly without computing their cross-product. join() \nis imple\u00admented roughly as follows: 1. Apply parallelDo() to each input PTable<K, Vi> to convert it into \na common format of type PTable<K, TaggedUnion2<V1,V2>>. 2. Combine the tables using flatten(). 3. Apply \ngroupByKey() to the .attened table to produce a PTable<K, Collection<TaggedUnion2<V1,V2>>>. 4. Apply \nparallelDo() to the key-grouped table, converting each Collection<TaggedUnion2<V1,V2>> into a Tuple2 \nof a Collection<V1> and a Collection<V2>.  Another useful derived operation is top(), which takes a \ncom\u00adparison function and a count N and returns the greatest N ele\u00adments of its receiver PCollection according \nto the comparison Figure 1. Initial execution plan for the SiteData pipeline. function. This operation \nis implemented on top of parallelDo(), groupByKey(), and combineValues(). The operations mentioned above \nto read multiple .le shards as a single PCollection are derived operations too, implemented using flatten() \nand the single-.le read primitives. 3.3 Deferred Evaluation In order to enable optimization as described \nin the next section, FlumeJava s parallel operations are executed lazily using deferred evaluation. Each \nPCollection object is represented internally ei\u00adther in deferred (not yet computed) or materialized (computed) \nstate. A deferred PCollection holds a pointer to the deferred operation that computes it. A deferred \noperation, in turn, holds references to the PCollections that are its arguments (which may themselves \nbe deferred or materialized) and the deferred PCollections that are its results. When a FlumeJava operation \nlike parallelDo() is called, it just creates a ParallelDo de\u00adferred operation object and returns a new \ndeferred PCollection that points to it. The result of executing a series of FlumeJava op\u00aderations is \nthus a directed acyclic graph of deferred PCollections and operations; we call this graph the execution \nplan. Figure 1 shows a simpli.ed version of the execution plan con\u00adstructed for the SiteData example \nused in Section 4.5 when dis\u00adcussing optimizations and in Section 6 as a benchmark. This pipeline takes \nfour different input sources and writes two outputs. (For simplicity, we usually elide PCollections from \nexecution plan diagrams.) Input1 is processed by parallelDo() A.  Input2 is processed by parallelDo() \nB, and Input3 is pro\u00adcessed by parallelDo() C. The results of these two operations are flatten()ed together \nand fed into parallelDo() D.  Input4 is counted using the count() derived operation, and the result \nis further processed by parallelDo() E.  The results of parallelDo()s A, D, and E are joined together \nusing the join() derived operation. Its result is processed further by parallelDo() F.   Finally, the \nresults of parallelDo()s A and F are written to output .les. To actually trigger evaluation of a series \nof parallel operations, the user follows them with a call to FlumeJava.run(). This .rst optimizes the \nexecution plan and then visits each of the deferred operations in the optimized plan, in forward topological \norder, and evaluates them. When a deferred operation is evaluated, it converts its result PCollection \ninto a materialized state, e.g., as an in\u00admemory data structure or as a reference to a temporary intermediate \n.le. FlumeJava automatically deletes any temporary intermediate .les it creates when they are no longer \nneeded by later operations in the execution plan. Section 4 gives details on the optimizer, and Section \n5 explains how the optimized execution plan is executed.  3.4 PObjects To support inspection of the \ncontents of PCollections during and after the execution of a pipeline, FlumeJava includes a class PObject<T>, \nwhich is a container for a single Java object of type T. Like PCollections, PObjects can be either deferred \nor materialized, allowing them to be computed as results of deferred operations in pipelines. After a \npipeline has run, the contents of a now-materialized PObject can be extracted using getValue(). PObject \nthus acts much like a future [10]. For example, the asSequentialCollection() operation ap\u00adplied to a \nPCollection<T> yields a PObject<Collection<T>>, which can be inspected after the pipeline has run to \nread out all the elements of the computed PCollection as a regular Java in\u00admemory Collection:2 PTable<String,Integer> \nwordCounts = ...; PObject<Collection<Pair<String,Integer>>> result = wordCounts.asSequentialCollection(); \n... FlumeJava.run(); for (Pair<String,Integer> count : result.getValue()) { System.out.print(count.first \n+ \": \" + count.second); } As another example, the combine() operation applied to a PCollection<T> and \na combining function over Ts yields a PObject<T> representing the fully combined result. Global sums \nand maximums can be computed this way. These features can be used to express a computation that needs \nto iterate until the computed data converges: PCollection<Data> results = computeInitialApproximation(); \nfor (;;) { results = computeNextApproximation(results); PCollection<Boolean> haveConverged = results.parallelDo(checkIfConvergedFn(), \ncollectionOf(booleans())); PObject<Boolean> allHaveConverged = haveConverged.combine(AND_BOOLS); FlumeJava.run(); \nif (allHaveConverged.getValue()) break; } ... continue working with converged results ... The contents \nof PObjects also can be examined within the ex\u00adecution of a pipeline. One way is using the operate() \nFlume-Java primitive, which takes a list of argument PObjects and an OperateFn, and returns a list of \nresult PObjects. When evaluated, operate() will extract the contents of its now-materialized argu\u00adment \nPObjects, and pass them in to the argument OperateFn. The 2 Of course, asSequentialCollection() should \nbe invoked only on rel\u00adatively small PCollections that can .t into memory. FlumeJava includes additional \noperations such as asIterable() that can be used to inspect parts of larger PCollections. OperateFn \nshould return a list of Java objects, which operate() wraps inside of PObjects and returns as its results. \nUsing this primitive, arbitrary computations can be embedded within a Flume-Java pipeline and executed \nin deferred fashion. For example, con\u00adsider embedding a call to an external service that reads and writes \n.les: // Compute the URLs to crawl: PCollection<URL> urlsToCrawl = ...; // Crawl them, via an external \nservice: PObject<String> fileOfUrlsToCrawl = urlsToCrawl.viewAsFile(TEXT); PObject<String> fileOfCrawledDocs \n= operate(fileOfUrlsToCrawl, new OperateFn() { String operate(String fileOfUrlsToCrawl) { return crawlUrls(fileOfUrlsToCrawl); \n} }); PCollection<DocInfo> docInfos = readRecordFileCollection(fileOfCrawledDocs, recordsOf(DocInfo.class)); \n// Use the crawled documents. This example uses operations for converting between PCollections and PObjects \ncontaining .le names. The viewAsFile() operation applied to a PCollection and a .le format choice yields \na PObject<String> containing the name of a temporary sharded .le of the chosen format where the PCollection \ns contents may be found during execution of the pipeline. File-reading operations such as readRecordFileCollection() \nare overloaded to allow reading .les whose names are contained in PObjects. In much the same way, the \ncontents of PObjects can also be examined inside a DoFn by passing them in as side inputs to parallelDo(). \nWhen the pipeline is run and the parallelDo() operation is eventually evaluated, the contents of any \nnow\u00admaterialized PObject side inputs are extracted and provided to the user s DoFn, and then the DoFn \nis invoked on each element of the input PCollection. For example: PCollection<Integer> values = ...; \nPObject<Integer> pMaxValue = values.combine(MAX_INTS); PCollection<DocInfo> docInfos = ...; PCollection<Strings> \nresults = docInfos.parallelDo( pMaxValue, new DoFn<DocInfo,String>() { private int maxValue; void setSideInputs(Integer \nmaxValue) { this.maxValue = maxValue; } void process(DocInfo docInfo, EmitFn<String> emitFn) { ... use \ndocInfo and maxValue ... } }, collectionOf(strings())); 4. Optimizer The FlumeJava optimizer transforms \na user-constructed, modular FlumeJava execution plan into one that can be executed ef.ciently. The optimizer \nis written as a series of independent graph transfor\u00admations. 4.1 ParallelDo Fusion One of the simplest \nand most intuitive optimizations is ParallelDo producer-consumer fusion, which is essentially func\u00adtion \ncomposition or loop fusion. If one ParallelDo opera\u00adtion performs function f, and its result is consumed \nby an\u00adother ParallelDo operation that performs function g, the two ParallelDo operations are replaced \nby a single multi-output ParallelDo that computes both f and g . f. If the result of the f  .  Figure \n2. ParallelDo Producer-Consumer and Sibling Fusion. ParallelDo is not needed by other operations in the \ngraph, fusion has rendered it unnecessary, and the code to produce it is removed as dead. ParallelDo \nsibling fusion applies when two or more ParallelDo operations read the same input PCollection. They are \nfused into a single multi-output ParallelDo operation that computes the results of all the fused operations \nin a single pass over the input. Both producer-consumer and sibling fusion can apply to ar\u00adbitrary trees \nof multi-output ParallelDo operations. Figure 2 shows an example execution plan fragment where ParallelDo \noperations A, B, C, and D can be fused into a single ParallelDo A+B+C+D. The new ParallelDo creates all \nthe leaf outputs from the original graph, plus output A.1, since it is needed by some other non-ParallelDo \noperation Op. Intermediate output A.0 is no longer needed and is fused away. As mentioned earlier, CombineValues \noperations are special cases of ParallelDo operations that can be repeatedly applied to partially computed \nresults. As such, ParallelDo fusion also applies to CombineValues operations.  4.2 The MapShuf.eCombineReduce \n(MSCR) Operation The core of the FlumeJava optimizer transforms combinations of ParallelDo, GroupByKey, \nCombineValues, and Flatten op\u00aderations into single MapReduces. To help bridge the gap be\u00adtween these \ntwo abstraction levels, the FlumeJava optimizer in\u00adcludes an intermediate-level operation, the MapShuf.eCombineRe\u00adduce \n(MSCR) operation. An MSCR operation has M input chan\u00adnels (each performing a map operation) and R output \nchannels (each optionally performing a shuf.e, an optional combine, and a reduce). Each input channel \nm takes a PCollection<Tm> as input and performs an R-output ParallelDo map operation (which defaults \nto the identity operation) on that input to pro\u00adduce R outputs of type PTable<Kr,Vr>s; the input channel \ncan choose to emit only to one or a few of its possible output chan\u00adnels. Each output channel r Flattens \nits M inputs and then either (a) performs a GroupByKey shuf.e , an optional CombineValues combine , and \na Or-output ParallelDo reduce (which de-Figure 3. A MapShuf.eCombineReduce (MSCR) operation with 3 input \nchannels, 2 grouping output channels, and 1 pass-through output channel.  faults to the identity operation), \nand then writes the results to Or output PCollections, or (b) writes its input directly as its output. \nThe former kind of output channel is called a grouping channel, while the latter kind of output channel \nis called a pass-through channel; a pass-through channel allows the output of a mapper to be a result \nof an MSCR operation. MSCR generalizes MapReduce by allowing multiple reducers and combiners, by allowing \neach reducer to produce multiple out\u00adputs, by removing the requirement that the reducer must produce \noutputs with the same key as the reducer input, and by allowing pass-through outputs, thereby making \nit a better target for our op\u00adtimizer. Despite its apparent greater expressiveness, each MSCR op\u00aderation \nis implemented using a single MapReduce. Figure 3 shows an MSCR operation with 3 input channels per\u00adforming \nParallelDos M1, M2, and M3 respectively, two grouping output channels, each with a GroupByKey, CombineValues, \nand reducing ParallelDo, and one pass-through output channel. 4.3 MSCR Fusion An MSCR operation is produced \nfrom a set of related GroupByKey operations. GroupByKey operations are considered related if they consume \n(possibly via Flatten operations) the same input or inputs created by the same (fused) ParallelDo operations. \nThe MSCR s input and output channels are derived from the re\u00adlated GroupByKey operations and the adjacent \noperations in the execution plan. Each ParallelDo operation with at least one out\u00adput consumed by one \nof the GroupByKey operations (possibly via Flatten operations) is fused into the MSCR, forming a new \ninput channel. Any other inputs to the GroupByKeys also form new input channels with identity mappers. \nEach of the related GroupByKey operations starts an output channel. If a GroupByKey s result is consumed \nsolely by a CombineValues operation, that opera\u00adtion is fused into the corresponding output channel. \nSimilarly, if the GroupByKey s or fused CombineValues s result is consumed soleby by a ParallelDo operation, \nthat operation is also fused into the output channel, if it cannot be fused into a different MSCR s input \nchannel. All the PCollections internal to the fused ParallelDo,  .  Figure 4. An example of MSCR \nfusion seeded by three GroupByKey operations. Only the starred PCollections are needed by later operations. \nGroupByKey, and CombineValues operations are now unneces\u00adsary and are deleted. Finally, each output of \na mapper ParallelDo that .ows to an operation or output other than one of the related GroupByKeys generates \nits own pass-through output channel. Figure 4 shows how an example execution plan is fused into an MSCR \noperation. In this example, all three GroupByKey operations are related, and hence seed a single MSCR \noperation. GBK1 is related to GBK2 because they both consume outputs of ParallelDo M2. GBK2 is related \nto GBK3 because they both consume PCollection M4.0. The ParallelDos M2, M3, and M4 are incorporated as \nMSCR input channels. Each of the GroupByKey operations becomes a grouping output channel. GBK2 s output \nchannel incorporates the CV2 CombineValues operation. The R2 and R3 ParallelDos are also incorporated \ninto output channels. An additional identity in\u00adput channel is created for the input to GBK1 from non-ParallelDo \nOp1. Two additional pass-through output channels (shown as edges from mappers to outputs) are created \nfor the M2.0 and M4.1 PCollections that are used after the MSCR. The resulting MSCR operation has 4 input \nchannels and 5 output channels. After all GroupByKey operations have been transformed into MSCR operations, \nany remaining ParallelDo operations are also transformed into trivial MSCR operations with a single input \nchan\u00adnel containing the ParallelDo and a single pass-through output channel. The .nal optimized execution \nplan contains only MSCR, Flatten, and Operate operations.  4.4 Overall Optimizer Strategy The optimizer \nperforms a series of passes over the execution plan, with the overall goal to produce the fewest, most \nef.cient MSCR operations in the .nal optimized plan: 1. Sink Flattens. A Flatten operation can be pushed \ndown through consuming ParallelDo operations by duplicating the ParallelDo before each input to the Flatten. \nIn symbols, h(f(a)+ g(b)) is transformed to h(f(a)) + h(g(b)). This transformation creates opportunities \nfor ParallelDo fusion, e.g., (h . f)(a)+(h . g)(b). 2. Lift CombineValues operations. If a CombineValues \nop\u00aderation immediately follows a GroupByKey operation, the  GroupByKey records that fact. The original \nCombineValues is left in place, and is henceforth treated as a normal ParallelDo operation and subject \nto ParallelDo fusion. 3. Insert fusion blocks. If two GroupByKey operations are connected by a producer-consumer \nchain of one or more ParallelDo operations, the optimizer must choose which ParallelDos should fuse up \ninto the output channel of the earlier GroupByKey, and which should fuse down into the in\u00adput channel \nof the later GroupByKey. The optimizer estimates the size of the intermediate PCollections along the \nchain of ParallelDos, identi.es one with minimal expected size, and marks it as boundary blocking ParallelDo \nfusion. 4. Fuse ParallelDos. 5. Fuse MSCRs. Create MSCR operations. Convert any remaining unfused ParallelDo \noperations into trivial MSCRs.  4.5 Example: SiteData In this section, we show how the optimizer works \non the SiteData pipeline introduced in Section 3.3. Figure 5 shows the execution plan initially and after \neach major optimization phase. 1. Initially. The initial execution plan is constructed from calls to \nprimitives like parallelDo() and flatten() and derived op\u00aderations like count() and join() which are \nthemselves imple\u00admented by calls to lower-level operations. In this example, the count() call expands \ninto ParallelDo C:Map, GroupByKey C:GBK, and CombineValues C:CV, and the join() call ex\u00adpands into ParallelDo \noperations J:TagN to tag each of the N input collections, Flatten J:Fltn, GroupByKey J:GBK, and ParallelDo \nJ:Untag to process the results. 2. After sinking Flattens and lifting CombineValues. Flatten operation \nFltn is pushed down through con\u00adsuming ParallelDo operations D and JTag:2. A copy of CombineValues operation \nC:CV is associated with C:GBK. 3. After ParallelDo fusion. Both producer-consumer and sibling fusion \nare applied to adjacent ParallelDo operations. Due to fusion blocks, CombineValues operation C:CV is \nnot fused with ParallelDo operation E+J:Tag3.    Figure 5. Optimizations applied to the SiteData \npipeline to go from 16 original data-parallel operations down to 2 MSCR operations. 4. After MSCR fusion. \nGroupByKey operation C:GBK and surround\u00ading ParallelDo operations are fused into a .rst MSCR opera\u00adtion. \nGroupByKey operations iGBK and J:GBK become the core operations of a second MSCR operation, which includes \nthe re\u00admaining ParallelDo operations. The original execution plan had 16 data-parallel operations (ParallelDos, \nGroupByKeys, and CombineValues). The .nal, optimized plan has two MSCR operations.  4.6 Optimizer Limitations \nand Future Work The optimizer does no analysis of the code within user-written functions (e.g., the DoFn \narguments to parallelDo() operations). It bases its optimization decisions on the structure of the execution \nplan, plus a few optional hints that users can provide giving some information about the behavior of \ncertain operations, such as an estimate of the size of a DoFn s output data relative to the size of its \ninput data. Static analysis of user code might enable better optimization and/or less manual user guidance. \nSimilarly, the optimizer does not modify any user code as part of its optimizations. For example, it \nrepresents the result of fused DoFns via a simple AST-like data structure that explains how to run the \nuser s code. Better performance could be achieved by gen\u00aderating new code to represent the appropriate \ncomposition of the user s functions, and then applying traditional optimizations such as inlining to \nthe resulting code. Users .nd it so easy to write FlumeJava pipelines that they often write large and \nsometimes inef.cient programs, contain\u00ading duplicate and/or unnecessary operations. The optimizer could \nbe augmented with additional common-subexpression elimina\u00adtion to avoid duplications. Additionally, users \ntend to include groupByKey() operations more often than necessary, simply be\u00adcause it makes logical sense \nto them to keep their data grouped by key. The optimizer should be extended to identify and remove un\u00adnecessary \ngroupByKey() operations, such as when the result of one groupByKey() is fed into another (perhaps in \nthe guise of a join() operation). 5. Executor Once the execution plan is optimized, the FlumeJava library \nruns it. Currently, FlumeJava supports batch execution: FlumeJava tra\u00adverses the operations in the plan \nin forward topological order, and executes each one in turn. Independent operations are executed si\u00admultaneously, \nsupporting a kind of task parallelism that comple\u00adments the data parallelism within operations. The most \ninteresting operation to execute is MSCR. FlumeJava .rst decides whether the operation should be run \nlocally and se\u00adquentially, or as a remote, parallel MapReduce. Since there is over\u00adhead in launching \na remote, parallel job, local evaluation is pre\u00adferred for modest-size inputs where the gain from parallel \nprocess\u00ading is outweighed by the start-up overheads. Modest-size data sets are common during development \nand testing, and by using local, in-process evaluation for these data sets, FlumeJava facilities the \nuse of regular IDEs, debuggers, pro.lers, and related tools, greatly easing the task of developing programs \nthat include data-parallel computations. If the input data set appears large, FlumeJava chooses to launch \na remote, parallel MapReduce. It uses observations of the input data sizes and estimates of the output \ndata sizes to automatically choose a reasonable number of parallel worker machines. Users can assist \nin estimating output data sizes, for example by augmenting a DoFn with a method that returns the expected \nratio of output data size to input data size, based on the computation represented by that DoFn. In the \nfuture, we would like to re.ne these estimates through dynamic monitoring and feedback of observed output \ndata sizes, and also to allocate relatively more parallel workers to jobs that have a higher ratio of \nCPU to I/O. FlumeJava automatically creates temporary .les to hold the outputs of each operation it \nexecutes. It automatically deletes these temporary .les as soon as they are no longer needed by some \nunevaluated operation later in the pipeline. FlumeJava strives to make building and running pipelines \nfeel as similar as possible to running a regular Java program. Using local, sequential evaluation for \nmodest-sized inputs is one way. Another way is by automatically routing any output to System.out or System.err \nfrom within a user s DoFn, such as debugging print statements, from the corresponding remote MapReduce \nworker to the main FlumeJava program s output streams. Likewise, any exceptions thrown within a DoFn \nrunning on a remote MapReduce worker are captured, sent to the main FlumeJava program, and rethrown. \nWhen developing a large pipeline, it can be time-consuming to .nd a bug in a late pipeline stage, .x \nthe program, and then reexecute the revised pipeline from scratch, particularly when it is not possible \nto debug the pipeline on small-size data sets. To aid in this cyclic process, the FlumeJava library supports \na cached execution mode. In this mode, rather than recompute an operation, FlumeJava .rst attempts to \nreuse the result of that operation from the previous run, if it was saved in a (internal or user-visible) \n.le and if FlumeJava determines that the operation s result has not changed. An operation s result is \nconsidered to be unchanged if (a) the operation s inputs have not changed, and (b) the operation s code \nand captured state have not changed. FlumeJava performs an automatic, conservative analysis to identify \nwhen reuse of previous results is guaranteed to be safe; the user can direct additional previous results \nto be reused. Caching can lead to quick edit\u00adcompile-run-debug cycles, even for pipelines that would \nnormally take hours to run. FlumeJava currently implements a batch evaluation strategy, for a single \npipeline at a time. In the future, it would be interesting to experiment with a more incremental, streaming, \nor continuous execution of pipelines, where incrementally added input leads to quick, incremental update \nof outputs. It also would be interesting to investigate optimization across pipelines run by multiple \nusers over common data sources. 6. Evaluation We have implemented the FlumeJava library, optimizer, and \nexecu\u00adtor, building on MapReduce and other lower-level services avail\u00adable at Google. In this section, \nwe present information about how FlumeJava has been used in practice, and demonstrate experimentally \nthat the FlumeJava optimizer and executor make modular, clear Flume-Java programs run nearly as well \nas their hand-optimized raw\u00adMapReduce-based equivalents. 6.1 User Adoption and Experience One measure \nof the utility of the FlumeJava system is the extent to which real developers .nd it worth converting \nto from systems they already know and are using. This is the principal way in which we evaluate the FlumeJava \nprogramming abstractions and API. Since its initial release in May 2009, FlumeJava has seen sig\u00adni.cant \nuser adoption and production use within Google. To mea\u00adsure usage, we instrumented the FlumeJava library \nto log a usage record every time a FlumeJava program is run. The following table presents some statistics \nderived from these logs, as of mid-March 2010:3 3 The FlumeJava usage logs themselves are processed using \na FlumeJava program. 1-day active users 62 7-day active users 106 30-day active users 176 Total users \n319 The N-day active users numbers give the number of distinct user ids that ran a FlumeJava program \n(excluding canned tutorial pro\u00adgrams) in the previous N days. Hundreds of FlumeJava programs have been \nwritten and checked in to Google s internal source-code repository. Individual FlumeJava programs have \nbeen run successfully on thousands of machines over petabytes of data. In general, users seem to be very \nhappy with the FlumeJava abstractions. They are not always as happy with some aspects of Java or FlumeJava \ns use of Java. In particular, Java provides poor support for simple anonymous functions and heterogeneous \ntuples, which leads to verbosity and some loss of static type safety. Also, FlumeJava s PCollection-based \ndata-parallel model hides many of the details of the individual parallel worker machines and the subtle \ndifferences between Mappers and Reducers, which makes it dif.cult to express certain low-level parallel-programming \ntechniques used by some advanced MapReduce users. FlumeJava is now slated to become the primary Java-based \nAPI for data-parallel computation at Google.  6.2 Optimizer Effectiveness In order to study the effectiveness \nof the FlumeJava optimizer at reducing the number of parallel MapReduce stages, we instru\u00admented the \nFlumeJava system so that it logs the structure of the user s pipeline, before and after optimization. \nThe scatterplot below shows the results extracted from these logs. Each point in the plot depicts one \nor more user pipelines with the corresponding number of stages. To aid the readability of the plot, we \nremoved data on about 10 larger pipelines with more than 120 unoptimized stages.  6.3 Execution Performance \nThe goal of FlumeJava is to allow a programmer to express his or her data-parallel computation in a clear, \nmodular way, while simultaneously executing it with performance approaching that of the best possible \nhand-optimized programs written directly against MapReduce APIs. While high optimizer compression is \ngood, the real goal is small execution time. To assess how well FlumeJava achieves this goal, we .rst \ncon\u00adstructed several benchmark programs, based on real pipelines writ\u00adten by FlumeJava users. These benchmarks \nperformed different computational tasks, including analyzing ads logs (Ads Logs), ex\u00adtracting and joining \ndata about websites from various sources (Site-Data and IndexStats), and computing usage statistics from \nlogs dumped by internal build tools (Build Logs). We wrote each benchmark in three different ways: in \na modular style using FlumeJava,  in a modular style using Java MapReduce, and  in a hand-optimized \nstyle using Java MapReduce.  For two of the benchmarks, we also wrote in a fourth way: in a hand-optimized \nstyle using Sawzall [17], a domain-speci.c logs-processing language implemented on top of MapReduce. \n The modular Java MapReduce style mirrors the logical structure found in the FlumeJava program, but it \nis not the normal way such computations would be expressed in MapReduce. The hand\u00adoptimized style represents \nan ef.cient execution strategy for the computation, and as such is much more common in practice than \nthe modular version, but as a result of being hand-optimized and represented directly in terms of MapReduces, \nthe logical com\u00adputation can become obscured and hard to change. The hand\u00adoptimized Sawzall version likewise \nintermixes logical computation with lower-level implementation details, in an effort to get better performance. \nThe following table shows the number of lines of source it took to write each version of each benchmark: \n Looking at the sizes of the user pipelines both before and after optimization, it is evident that FlumeJava \nhas been used for writing small as well as fairly large pipelines. In fact, the largest of the pipeline \nso far (not plotted) had 820 unoptimized stages and 149 optimized stages. This data further underscores \nthe usability of the FlumeJava API. Looking at the optimizer s compression ratio (the ratio of number \nof stages before and after the optimization), the optimizer appears to achieve on average a 5x reduction \nin the number of stages, and some pipelines had compression ratios over 30x. One pipeline (not plotted) \nhad 207 unoptimized stages which were fused into a single optimized stage. The FlumeJava optimizer itself \nruns quickly, especially com\u00adpared to the actual execution that follows optimization. For pipelines having \nup to dozens of operations, the optimizer takes less than a second or two. Benchmark FlumeJava MapReduce \n(Modular) MapReduce (Hand-Opt) Sawzall (Hand-Opt) Ads Logs 320 465 399 158 IndexStats 176 296 336 - Build \nLogs 276 476 355 - SiteData 465 653 625 261 For each case, the FlumeJava version is more concise than \nthe equivalent version written using raw Java MapReduce. Sawzall is more concise than Java. The following \ntable presents, for the FlumeJava version, the number of FlumeJava operations in the pipeline (both before \nand after optimization), and for the Java MapReduce and Sawzall ver\u00adsions, the number of MapReduce stages: \nBenchmark FlumeJava MapReduce (Modular) MapReduce (Hand-Opt) Sawzall (Hand-Opt) Ads Logs 14 . 1 4 1 4 \nIndexStats 16 . 2 3 2 - Build Logs 7 . 1 3 1 - SiteData 12 . 2 5 2 6 For each benchmark, the number \nof automatically optimized opera\u00adtions in the FlumeJava version matches the number of MapReduce stages \nin the corresponding hand-optimized MapReduce-based ver\u00adsion. As a higher-level, domain-speci.c language, \nSawzall does not provide the programmer suf.cient low-level access to enable them to hand-optimize their \nprograms into this minimum number of MapReduce stages, nor does it include an automatic optimizer. The \nfollowing table shows, for each benchmark, the size of the input data set and the number of worker machines \nwe used to run it: Benchmark Input Size Number of Machines Ads Logs 550 MB 4 IndexStats 3.3 TB 200 Build \nLogs 34 GB 15 SiteData 1.3 TB 200 We compared the run-time performance of the different versions of \neach benchmark. We ensured that each version used equivalent numbers of machines and other resources. \nWe measured the total elapsed wall-clock time spent when MapReduce workers were run\u00adning; we excluded \nthe coordination time of starting up the main controller program, distributing compiled binaries to worker \nma\u00adchines, and cleaning up temporary .les. Since execution times can vary signi.cantly across runs, we \nran each benchmark version .ve times, and took the minimum measured time as an approximation of the true \ntime undisturbed by unrelated effects of running on a shared cluster of machines. The chart below shows \nthe elapsed time for each version of each benchmark, relative to the elapsed time for the FlumeJava version \n(shorter bars are better): Comparing the two MapReduce columns and the Sawzall column shows the importance \nof optimizing. Without optimizations, the set-up overheads for the workers, the extra I/O in order to \nstore the intermediate data, extra data encoding and decoding time, and other similar factors increase \nthe overall work required to pro\u00adduce the output. Comparing the FlumeJava and the hand-optimized MapReduce \ncolumns demonstrates that a modular program written in FlumeJava runs at close to the performance of \na hand-optimized version using the lower-level MapReduce APIs. 7. Related Work In this section we brie.y \ndescribe related work, and compare FlumeJava to that work. Language and library support for data-parallel \nprogramming has a long history. Early work includes *Lisp [13], C* [18], C** [12], and pH [15]. MapReduce \n[6 8] combines simple abstractions for data\u00adparallel processing with an ef.cient, highly scalable, fault-tolerant \nimplementation. MapReduce s abstractions directly support com\u00adputations that can be expressed as a map \nstep, a shuf.e step, and a reduce step. MapReduces can be programmed in several languages, including \nC++ and Java. FlumeJava builds on Java MapReduce, offering higher-level, more-composable abstractions, \nand an opti\u00admizer for recovering good performance from those abstractions. FlumeJava builds in support \nfor managing pipelines of MapRe\u00adduces. FlumeJava also offers additional conveniences that help make developing \na FlumeJava program similar to developing a reg\u00adular single-process Java program. Sawzall [17] is a \ndomain-speci.c logs-processing language that is implemented as a layer over MapReduce. A Sawzall program \ncan .exibly specify the mapper part of a MapReduce, as long as the mappers are pure functions. Sawzall \nincludes a library of a dozen standard reducers; users cannot specify their own reducers. This limits \nthe Sawzall user s ability to express ef.cient execution plans for some computations, such as joins. \nLike MapReduce, Sawzall does not provide help for multi-stage pipelines. Hadoop [2] is an open-source \nJava-based re-implementation of MapReduce, together with a job scheduler and distributed .le system akin \nto the Google File System [9]. As such, Hadoop has similar limitations as MapReduce when developing multi-stage \npipelines. Cascading [1] is a Java library built on top of Hadoop. Like FlumeJava, Cascading aims to \nease the challenge of programming data-parallel pipelines, and provides abstractions similar to those \nof FlumeJava. Unlike FlumeJava, a Cascading program explicitly constructs a data.ow graph. In addition, \nthe values .owing through a Cascading pipeline are special untyped tuple values, and Cas\u00adcading operations \nfocus on transforms over tuples; in contrast, a FlumeJava pipeline computes over arbitrary Java objects \nusing ar\u00adbitrary Java computations. Cascading performs some optimizations of its data.ow graphs prior \nto running them. Somewhat akin to FlumeJava s executor, the Cascading evaluator breaks the data.ow graph \ninto pieces, and, if possible, runs those in parallel, using the underlying Hadoop job scheduler. There \nis a mechanism for elid\u00ading computation if input data is unchanged, akin to FlumeJava s caching mechanism. \nPig [3] compiles a special domain-speci.c language called Pig Latin [16] into code that is run on Hadoop. \nA Pig Latin program combines high-level declarative operators similar to those in SQL, together with \nnamed intermediate variables representing edges in the data.ow graph between operators. The language \nallows for user-de.ned transformation and extraction functions, and provides support for co-grouping \nand joins. The Pig system has a novel debugging mechanism, wherein it can generate sample data sets that \nillustrate what the various operations do. The Pig system has an optimizer that tries to minimize the \namount of data materialized between Hadoop jobs, and is sensitive to the size of the input data sets. \nThe Dryad [11] system implements a general-purpose data\u00adparallel execution engine. Dryad programs are \nwritten in C++ us\u00ading overloaded operators to specify an arbitrary acyclic data.ow graph, somewhat akin \nto Cascading s model of explicit graph con\u00adstruction. Like MapReduce, Dryad handles the details of commu\u00adnication, \npartitioning, placement, concurrency and fault tolerance. Unlike stock MapReduce but similar to the FlumeJava \noptimizer s MSCR primitive, computation nodes can have multiple input and output edge channels. Unlike \nFlumeJava, Dryad does not have an optimizer to combine or rearrange nodes in the data.ow graph, since \nthe nodes are computational black boxes, but Dryad does in\u00adclude a notion of run-time graph re.nement \nthrough which users can perform some kinds of optimizations. The LINQ [14] extension of C# 3.0 adds a \nSQL-like construct to C#. This construct is syntactic sugar for a series of library calls, which can \nbe implemented differently over different kinds of data being queried. The SQL-like construct can be \nused to express queries over traditional relational data (and shipped out to remote database servers), \nover XML data, and over in-memory C# ob\u00adjects. It can also be used to express parallel computations and \nexe\u00adcuted on Dryad [20]. The manner in which the SQL-like construct is desugared into calls that construct \nan internal representation of the original query is similar to how FlumeJava s parallel operations implicitly \nconstruct an internal execution plan. DryadLINQ also includes optimizations akin to those in FlumeJava. \nThe C# lan\u00adguage was signi.cantly extended in order to support LINQ; in con\u00adtrast, FlumeJava is implemented \nas a pure Java library, with no lan\u00adguage changes. DryadLINQ requires a pipeline to be expressed via \na single SQL-like statement. In contrast, calls to FlumeJava opera\u00adtions can be intermixed with other \nJava code, organized into func\u00adtions, and managed with traditional Java control-.ow operations; deferred \nevaluation enables all these calls to be coalesced dynam\u00adically into a single pipeline, which is then \noptimized and executed as a unit. SCOPE [4] is a declarative scripting language built on top of Dryad. \nPrograms are written in a variant of SQL, with extensions to call out to custom extractors, .lters, and \nprocessors that are written in C#. The C# extensions are intermixed with the SQL code. As with Pig Latin, \nSQL queries are broken down into a series of distinct steps, with variables naming intermediate streams. \nThe SQL framework provides named data .elds, but there appears to be little support for those names in \nthe extension code. The optimizer transforms SQL expressions using traditional rules for query optimization, \ntogether with new rules that take into account data and communication locality. Map-Reduce-Merge [19] \nextends the MapReduce model by adding an additional Merge step, making it possible to express ad\u00additional \ntypes of computations, such as relational algebra, in a sin\u00adgle execution. FlumeJava supports more general \npipelines. FlumeJava s optimizer shares many concepts with tradi\u00adtional compiler optimizations, such \nas loop fusion and common\u00adsubexpression elimination. FlumeJava s optimizer also bears some resemblance \nto a database query optimizer: they both produce an optimized execution plan from a higher-level decription \nof a logical computation, and both can optimize programs that perform joins. However, a database query \noptimizer typically uses run-time infor\u00admation about input tables in a relational database, such as their \nsizes and available indices, to choose an ef.cient execution plan, such as which of several possible \nalgorithms to use to compute joins. In contrast, FlumeJava provides no built-in support for joins. Instead, \njoin() is a derived library operation that implements a particu\u00adlar join algorithm, hash-merge-join, \nwhich works even for simple, .le-based data sets lacking indices and which can be implemented using MapReduce. \nOther join algorithms could be implemented by other derived library operations. FlumeJava s optimizer \nworks at a lower level than a typical database query optimizer, applying fu\u00adsion and other simple transformations \nto the primitives underlying the join() library operation. It chooses how to optimize without reference \nto the sizes or other representational properties of its in\u00adputs. Indeed, in the context of a join embedded \nin a large pipeline, such information may not become available until after the opti\u00admized pipeline has \nbeen partly run. FlumeJava s approach allows the operations implementing the join to be optimized in \nthe con\u00adtext of the surrounding pipeline; in many cases the joining opera\u00adtions are completely fused \ninto the rest of the computation (and vice versa). This was illustrated by the SiteData example in section \n4.5. Before FlumeJava, we were developing a system based on similar abstractions, but made available \nto users in the context of a new programming language, named Lumberjack. Lumber\u00adjack was designed to \nbe particularly good for expressing data\u00adparallel pipelines, and included features such as an implicitly \npar\u00adallel, mostly functional programming model, a sophisticated poly\u00admorphic type system, local type \ninference, lightweight tuples and records, and .rst-class anonymous functions. Lumberjack was sup\u00adported \nby a powerful optimizer that included both traditional op\u00adtimizations such as inlining and value .ow \nanalysis, and non\u00adtraditional optimizations such as fusion of parallel loops. Lum\u00adberjack programs were \ntransformed into a low-level intermediate representation, which in our implementation was interpreted \nbut which we planned to eventually dynamically translate into Java bytecode or native machine code. Lumberjack \ns parallel run-time system shared many of the characteristics of FlumeJava s run-time system. While \nthe Lumberjack-based version of Flume offered a number of bene.ts for programmers, it suffered from several \nimportant disadvantages relative to the FlumeJava version: Since Lumberjack was specially designed for \nthe task, Lumber\u00adjack programs were signi.cantly more concise than the equiv\u00adalent FlumeJava programs. \nHowever, the implicitly parallel, mostly functional programming model was not natural for many of its \nintended users. FlumeJava s explicitly parallel model, which distinguishes Collection from PCollection \nand iterator() from parallelDo(), coupled with its mostly imperative model that disallows mutable shared \nstate only across DoFn boundaries, is much more natural for most of these programmers.  Lumberjack s \noptimizer was a traditional static optimizer, which performed its optimization over the program s internal \nrepresentation before executing any of it. Since FlumeJava is a pure library, it cannot use a traditional \nstatic optimization ap\u00adproach. Instead, we adopted a more dynamic approach to op\u00adtimization, where the \nrunning user program .rst constructs an execution plan (via deferred evaluation), and then optimizes \nthe plan before executing it. FlumeJava does no static analysis of the source program nor dynamic code \ngeneration, which im\u00adposes some costs in run-time performance; those costs have turned out to be relatively \nmodest. On the other hand, being able to simply run the FlumeJava program to construct the fully expanded \nexecution plan has turned out to be a tremendous ad\u00advantage. The ability of Lumberjack s optimizer to \ndeduce the program s execution plan was always limited by the strength of its static analysis, but FlumeJava \ns dynamic optimizer has no such limits. Indeed, FlumeJava programmers routinely use complex control structures \nand Collections and Maps storing PCollections in their code expressing their pipeline compu\u00adtation. These \ncoding patterns would defeat any static analysis we could reasonably develop, but the FlumeJava dynamic \nopti\u00admizer is unaffected by this complexity. Later, we can augment FlumeJava with a dynamic code generator, \nif we wish the re\u00adduce the remaining overheads.  Building an ef.cient, complete, usable Lumberjack-based \nsys\u00adtem is much more dif.cult and time-consuming than building an equivalently ef.cient, complete, and \nusable FlumeJava system. Indeed, we had built only a prototype Lumberjack-based sys\u00adtem after more than \na year s effort, but we were able to change directions and build a useful FlumeJava system in only a \ncouple of months.  Novelty is an obstacle to adoption. By being embedded in a well-known programming \nlanguage, FlumeJava focuses the potential adopter s attention on a few new features, namely the Flume \nabstractions and the handful of Java classes and methods implementing them. Potential adopters are not \ndistracted by a new syntax or a new type system or a new evaluation model. Their normal development tools \nand practices continue to work. All the standard libraries they have learned and rely on are still available. \nThey need not fear that Java will go away and leave their project in the lurch. By comparison, Lumberjack \nsuffered greatly along these dimensions. The advantages of its specially designed syntax and type system \nwere insuf.cient to overcome these real-world obstacles.   8. Conclusion FlumeJava is a pure Java library \nthat provides a few simple abstrac\u00adtions for programming data-parallel computations. These abstrac\u00adtions \nare higher-level than those provided by MapReduce, and pro\u00advide better support for pipelines. FlumeJava \ns internal use of a form of deferred evaluation enables the pipeline to be optimized prior to execution, \nachieving performance close to that of hand-optimized MapReduces. FlumeJava s run-time executor can select \namong al\u00adternative implementation strategies, allowing the same program to execute completely locally \nwhen run on small test inputs and using many parallel machines when run on large inputs. FlumeJava is \nin active, production use at Google. Its adoption has been facilitated by being a mere library in the \ncontext of an existing, well-known, expressive language. References [1] Cascading. http://www.cascading.org. \n[2] Hadoop. http://hadoop.apache.org. [3] Pig. http://hadoop.apache.org/pig. [4] R. Chaiken, B. Jenkins, \nP.-\u00b0Larson, B. Ramsey, D. Shakib, A. S. Weaver, and J. Zhou. SCOPE: Easy and ef.cient parallel processing \nof massive data sets. Proceedings of the VLDB Endowment (PVLDB), 1(2), 2008. [5] F. Chang, J. Dean, S. \nGhemawat, W. C. Hsieh, D. A. Wallach, M. Bur\u00adrows, T. Chandra, A. Fikes, and R. E. Gruber. Bigtable: \nA distributed storage system for structured data. In USENIX Symposium on Operat\u00ading Systems Design and \nImplementation (OSDI), 2006. [6] J. Dean. Experiences with MapReduce, an abstraction for large-scale \ncomputation. In Parallel Architectures and Compilation Techniques (PACT), 2006. [7] J. Dean and S. Ghemawat. \nMapReduce: Simpli.ed data processing on large clusters. Communications of the ACM, 51, no. 1, 2008. [8] \nJ. Dean and S. Ghemawat. MapReduce: Simpli.ed data processing on large clusters. In USENIX Symposium \non Operating Systems Design and Implementation (OSDI), 2004. [9] S. Ghemawat, H. Gobioff, and S.-T. Leung. \nThe Google .le system. In ACM Symposium on Operating Systems Principles (SOSP), 2003. [10] R. H. Halstead \nJr. New ideas in parallel Lisp: Language design, implementation, and programming tools. In Workshop on \nParallel Lisp, 1989. [11] M. Isard, M. Budiu, Y. Yu, A. Birrell, and D. Fetterly. Dryad: Dis\u00adtributed \ndata-parallel programs from sequential building blocks. In EuroSys, 2007. [12] J. R. Larus. C**: A large-grain, \nobject-oriented, data-parallel pro\u00adgramming language. In Languages and Compilers for Parallel Com\u00adputing \n(LCPC), 1992. [13] C. Lasser and S. M. Omohundro. The essential Star-lisp manual. Technical Report 86.15, \nThinking Machines, Inc., 1986. [14] E. Meijer, B. Beckman, and G. Bierman. LINQ: reconciling objects, \nrelations and XML in the .NET framework. In ACM SIGMOD Inter\u00adnational Conference on Management of Data, \n2006. [15] R. S. Nikhil and Arvind. Implicit Parallel Programming in pH. Academic Press, 2001. [16] C. \nOlston, B. Reed, U. Srivastava, R. Kumar, and A. Tomkins. Pig Latin: A not-so-foreign language for data \nprocessing. In ACM SIG-MOD International Conference on Management of Data, 2008. [17] R. Pike, S. Dorward, \nR. Griesemer, and S. Quinlan. Interpreting the data: Parallel analysis with Sawzall. Scienti.c Programming, \n13(4), 2005. [18] J. R. Rose and G. L. Steele Jr. C*: An extended C language. In C++ Workshop, 1987. \n[19] H.-c. Yang, A. Dasdan, R.-L. Hsiao, and D. S. Parker. Map-reduce\u00admerge: simpli.ed relational data \nprocessing on large clusters. In ACM SIGMOD International Conference on Management of Data, 2007. \u00b4 \n[20] Y. Yu, M. Isard, D. Fetterly, M. Budiu, U. Erlingsson, P. K. Gunda, and J. Currey. DryadLINQ: A \nsystem for general-purpose distributed data\u00adparallel computing using a high-level language. In USENIX \nSympo\u00adsium on Operating Systems Design and Implementation (OSDI), 2008.   \n\t\t\t", "proc_id": "1806596", "abstract": "<p>MapReduce and similar systems significantly ease the task of writing data-parallel code. However, many real-world computations require a pipeline of MapReduces, and programming and managing such pipelines can be difficult. We present FlumeJava, a Java library that makes it easy to develop, test, and run efficient data-parallel pipelines. At the core of the FlumeJava library are a couple of classes that represent immutable parallel collections, each supporting a modest number of operations for processing them in parallel. Parallel collections and their operations present a simple, high-level, uniform abstraction over different data representations and execution strategies. To enable parallel operations to run efficiently, FlumeJava defers their evaluation, instead internally constructing an execution plan dataflow graph. When the final results of the parallel operations are eventually needed, FlumeJava first optimizes the execution plan, and then executes the optimized operations on appropriate underlying primitives (e.g., MapReduces). The combination of high-level abstractions for parallel data and computation, deferred evaluation and optimization, and efficient parallel primitives yields an easy-to-use system that approaches the efficiency of hand-optimized pipelines. FlumeJava is in active use by hundreds of pipeline developers within Google.</p>", "authors": [{"name": "Craig Chambers", "author_profile_id": "81464648653", "affiliation": "Google, Seattle, WA, USA", "person_id": "P2184589", "email_address": "", "orcid_id": ""}, {"name": "Ashish Raniwala", "author_profile_id": "81100326865", "affiliation": "Google, Seattle, WA, USA", "person_id": "P2184590", "email_address": "", "orcid_id": ""}, {"name": "Frances Perry", "author_profile_id": "81333490569", "affiliation": "Google, Seattle, WA, USA", "person_id": "P2184591", "email_address": "", "orcid_id": ""}, {"name": "Stephen Adams", "author_profile_id": "81100312823", "affiliation": "Google, Seattle, WA, USA", "person_id": "P2184592", "email_address": "", "orcid_id": ""}, {"name": "Robert R. Henry", "author_profile_id": "81100018281", "affiliation": "Google, Seattle, WA, USA", "person_id": "P2184593", "email_address": "", "orcid_id": ""}, {"name": "Robert Bradshaw", "author_profile_id": "81540986656", "affiliation": "Google, Seattle, WA, USA", "person_id": "P2184594", "email_address": "", "orcid_id": ""}, {"name": "Nathan Weizenbaum", "author_profile_id": "81464671380", "affiliation": "Google, Seattle, WA, USA", "person_id": "P2184595", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1806596.1806638", "year": "2010", "article_id": "1806638", "conference": "PLDI", "title": "FlumeJava: easy, efficient data-parallel pipelines", "url": "http://dl.acm.org/citation.cfm?id=1806638"}