{"article_publication_date": "06-05-2010", "fulltext": "\n Complete Functional Synthesis Viktor Kuncak Mika\u00a8el Mayer Ruzica Piskac Philippe Suter * School of Computer \nand Communication Sciences (I&#38;C) -Swiss Federal Institute of Technology (EPFL), Switzerland .rstname.lastname@ep..ch \nAbstract Synthesis of program fragments from speci.cations can make programs easier to write and easier \nto reason about. To inte\u00adgrate synthesis into programming languages, synthesis algorithms should behave \nin a predictable way they should succeed for a well-de.ned class of speci.cations. They should also support \nun\u00adbounded data types such as numbers and data structures. We pro\u00adpose to generalize decision procedures \ninto predictable and com\u00adplete synthesis procedures. Such procedures are guaranteed to .nd code that \nsatis.es the speci.cation if such code exists. Moreover, we identify conditions under which synthesis \nwill statically decide whether the solution is guaranteed to exist, and whether it is unique. We demonstrate \nour approach by starting from decision procedures for linear arithmetic and data structures and transforming \nthem into synthesis procedures. We establish results on the size and the ef.\u00adciency of the synthesized \ncode. We show that such procedures are useful as a language extension with implicit value de.nitions, \nand we show how to extend a compiler to support such de.nitions. Our constructs provide the bene.ts of \nsynthesis to programmers, with\u00adout requiring them to learn new concepts or give up a deterministic execution \nmodel. Categories and Subject Descriptors D.2.4 [Software Engineer\u00ading]: Software/Program Veri.cation; \nF.3.1 [Logics and Meaning of Programs]: Specifying and Verifying and Reasoning about Pro\u00adgrams General \nTerms Algorithms, Languages, Veri.cation 1. Introduction Synthesis of software from speci.cations [MW71, \nMW80] promises to make programmers more productive. Despite substan\u00adtial recent progress [SLTB+06, SLJB08, \nVYY09, SGF10], synthe\u00adsis is limited to small pieces of code. We expect that this will con\u00adtinue to be \nthe case for some time in the future, for two reasons: 1) synthesis is algorithmically a dif.cult problem, \nand 2) synthesis * The author list has been sorted according to the alphabetical order; this should not \nbe used to determine the extent of authors contributions. Ruzica Piskac was supported by the EPFL School \nof Computer and Com\u00admunication Sciences and in part by the Swiss National Foundation Grant SCOPES IZ73Z0 \n127979. Philippe Suter was supported by the Swiss Na\u00adtional Science Foundation Grant 200021 120433. Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI 10, June 5 \n10, 2010, Toronto, Ontario, Canada. Copyright c . 2010 ACM 978-1-4503-0019-3/10/06. . . $10.00  requires \ndetailed speci.cations, which for large programs become dif.cult to write. We therefore expect that practical \napplications of synthesis lie in its integration into the compilers of general-purpose programming languages. \nTo make this integration feasible, we aim to identify well-de.ned classes of expressions and synthesis \nalgorithms guar\u00adanteed to succeed for these classes of expressions, just like a com\u00adpilation attempt \nsucceeds for any well-formed program. Our start\u00ading point for such synthesis algorithms are decision \nprocedures. A decision procedure for satis.ability of a class of formulas ac\u00adcepts a formula in its class \nand checks whether the formula has a solution. On top of this basic functionality, many decision proce\u00addure \nimplementations provide the additional feature of generating a satisfying assignment (a model) whenever \nthe given formula is satis.able. Such a model-generation functionality has many uses, including better \nerror reporting in veri.cation [Mos09] and test\u00adcase generation [AGT08]. Model generation could also \nbe used as an advanced computa\u00adtion mechanism given a set of values for some of the variables, a constraint \nsolver can at run-time .nd the values of the remaining variables such that a given constraint holds. \nA recent example of integrating such a mechanism into a programming language are the quotations of the \nF # language [SGC07] used to interface to the Z3 satis.ability modulo theories (SMT) solver [B08]. Such \nmech\u00adanisms promise to bring the algorithmic improvements of SMT solvers to declarative paradigms such \nas Constraint Logic Pro\u00adgramming [JM94]. However, they involve a possibly unpredictable search at run-time, \nand require the deployment of the entire deci\u00adsion procedure as a component of the run-time system. Our \ngoal is to provide the bene.ts of the declarative approach in a more controlled way: we aim to run a \ndecision procedure at compile time and use it to generate code. The generated code then computes the \ndesired values of variables at run-time. It is thus speci.c to the desired constraint, and can be more \nef.cient. It does not require the decision procedure to be present at run-time, and gives the developer \nstatic feedback by checking the conditions under which the generated solution will exist and be unique. \nWe use the term synthesis for our approach because it starts from an implicit speci.cation, and involves \ncompile-time precomputation. Because it computes a function that satis.es a given input/output relation, \nwe call our synthesis functional, in contrast to reactive synthesis approaches [PR89] (another term for \nthe general direction of our approach is AE-paradigm or Skolem paradigm [PR89]). Finally, we call our \napproach complete because it is guaranteed to work for all speci.cation expressions from a well-speci.ed \nclass. We demonstrate this approach by describing our synthesis al\u00adgorithms for the domains of linear \narithmetic and collections of objects. We have implemented these synthesis algorithms and de\u00adployed them \nas a compiler extension of the Scala programming lan\u00adguage [OSV08]. We have found that using such constraints \nwe were able to express a number of program fragments in a more natural way, stating the invariants that \nthe program should satisfy as op\u00adposed to the computation details of establishing these invariants. \n In the area of integer arithmetic, we obtain a language extension that can implicitly de.ne integer \nvariables to satisfy given con\u00adstraints. The applications of integer arithmetic synthesis include conversions \nof quantities expressed in terms of multiple units, as well as a substantially more general notion of \npattern matching on integers, going well beyond matching on constants or (n + k)\u00adpatterns of Haskell. \nIn the area of data structures, we describe a synthesis procedure that can compute sets of elements subject \nto constraints expressed in terms of basic set operations (union, intersection, set difference, subset, \nequality) as well as linear constraints on sizes of sets. We have found these constraints to be useful \nfor manipulating sets of objects in high-level descriptions of algorithms, from simple oper\u00adations such \nas choosing an element from a set or a fresh element, or splitting sets subject to size constraints. \nSuch constructs arise in pseudo code notations, and they provide a useful addition to the transformations \npreviously developed for the SETL program\u00adming language [Dew79, Sha82]. Regarding data structures, this \npa\u00adper focuses on sets, but the approach applies to other constraints for which decision procedures are \navailable [KPSW10], including mul\u00adtisets [PK08a, PK08b, YPK10] and algebraic data types [SDK10]. Contributions. \nThis paper makes the following contributions. 1. We describe an approach for deploying algorithms for \nsynthe\u00adsis within programming languages. Our approach introduces a higher-order library function choose \nof type (a . bool) . a, which takes as an argument a speci.cation, given as an expres\u00adsion .x.F of type \na . bool. Our compiler extension rewrites calls to choose into ef.cient code that .nds a value x of type \na such that F is true. The generated code computes x as a func\u00adtion of the free variables (parameters) \nof the expression F . This deployment is easy to understand by programmers because it has the same semantics \nas invoking a constraint solver at run\u00adtime. It does not impact the semantics or ef.ciency of existing \nprogramming language constructs, because the execution out\u00adside choose remains unchanged. 2. Building \non the choose primitive, we show how to support pat\u00adtern matching expressions that are substantially \nmore expres\u00adsive than the existing ones, using the full expressive power of the term language of a decidable \ntheory. 3. We describe a methodology to convert decision procedures for a class of formulas into synthesis \nprocedures that can rewrite the corresponding class of expressions into ef.cient executable code. Most \nexisting procedures based on quanti.er elimination are directly amenable to our approach. 4. As a .rst \nillustrative example, we describe synthesis procedures for propositional logic and rational arithmetic. \nWe show that, compared to invocations of constraint solvers at run-time, the synthesized code can have \nbetter worst-case complexity in the number of variables. This is because our synthesis procedure converts \n(at compile time) a given constraint into a solved form that can be executed, avoiding most of the run-time \nsearch. The synthesized code is guaranteed to be correct by construction. 5. As our core implemented \nexample, we present synthesis for linear arithmetic over unbounded integers. Given an integer linear \narithmetic formula and a separation of variables into output variables and parameters, our procedure \nconstructs 1) a program that computes the values of outputs given the values of inputs, and 2) the weakest \npossible precondition on inputs that guarantees the existence of outputs.  6. We show that the synthesis \nfor integer arithmetic can be ex\u00adtended to the non-linear case where coef.cients multiplying output variables \nare expressions over parameters that are known only at run-time. We have implemented this extension and \nhave found that it increases the range of supported speci.cations. It shows that we can have complete \nfunctional synthesis for speci\u00ad.cations for which the satis.ability over the space of all param\u00adeters \nis undecidable. 7. We also present an implemented synthesis procedure for Boolean Algebra with Presburger \nArithmetic (BAPA), a logic of constraints on sets and their sizes. This algorithm illustrates that complete \nfunctional synthesis applies not only to numeri\u00adcal computations, but also to the very important domain \nof data structure manipulations. This result also illustrates the idea of the composition of synthesis \nprocedures. While the implemen\u00adtations of BAPA decision procedures work by reduction to in\u00adteger arithmetic \ndecision procedures [KNR06, KR07], we here show how to build a synthesis procedure for BAPA on top of \nour synthesis procedure for integer linear arithmetic. 8. We describe our experience in using synthesis \nas a plugin for the Scala compiler. Our implementation is publicly available at http://lara.epfl.ch/dokuwiki/comfusy \nand can be used as a starting point for the development of further synthesis approaches.   2. Example \nWe .rst illustrate the use of a synthesis procedure for integer linear arithmetic. Consider the following \nexample to break down a given number of seconds (stored in the variable totsec) into hours, minutes, \nand leftover seconds. val (hours, minutes, seconds) = choose((h: Int, m: Int, s: Int) . h * 3600 + m \n* 60 + s == totsec &#38;&#38; 0 = m&#38;&#38; m = 60 &#38;&#38; 0 = s&#38;&#38; s = 60) Our synthesizer \nsucceeds, because the constraint is in integer linear arithmetic. However, the synthesizer emits the \nfollowing warning: Synthesis predicate has multiple solutions for variable assignment: totsec = 0 Solution \n1: h= 0, m=0, s =0 Solution 2: h= -1, m= 59, s= 60 The reason for this warning is that the bounds on \nm and s are not strict. After correcting the error in the speci.cation, replac\u00ading m = 60 with m < 60 \nand s = 60 with s < 60, the synthesizer emits no warnings and generates code corresponding to the follow\u00ading: \nval (hours, minutes, seconds) = {val loc1 = totsec div 3600 val num2 = totsec + ((-3600) * loc1) val \nloc2 = min(num2 div 60, 59) val loc3 = totsec + ((-3600) * loc1) + (-60 * loc2) (loc1, loc2, loc3) } \nThe absence of warnings guarantees that the solution al\u00adways exists and that it is unique. By writing \nthe code in this style, the developer directly ensures that the condition h * 3600 + m * 60 + s == totsec \nwill be satis.ed, making pro\u00adgram understanding easier. Note that, if the developer imposes the constraint \nval (hours, minutes, seconds) = choose((h: Int, m: Int, s: Int) . h * 3600 + m * 60 + s == totsec &#38;&#38; \n0 = h < 24 &#38;&#38; 0 = m&#38;&#38; m < 60 &#38;&#38; 0 = s&#38;&#38; s < 60)  our system emits the \nfollowing warning: Synthesis predicate is not satisfiable for variable assignment: totsec = 86400 pointing \nto the fact that the constraint has no solutions when the totsec parameter is too large. In addition \nto the choose function, programmers can use syn\u00adthesis for more .exible pattern matching on integers. \nIn existing deterministic programming languages, matching on integers either tests on constant types, \nor, in the case of Haskell s (n + k) patterns, on some very special forms of patterns. Our approach supports \na much richer set of patterns, as illustrated by the following fast ex\u00adponentiation code that does case \nanalysis on whether the argument is even or odd: def pow(base : Int, p : Int) = { def fp(m :Int,b:Int, \ni : Int)=i match { case 0 . m case 2*j . fp(m, b*b, j) case 2*j+1 . fp(m*b, b*b, j) } fp(1,base,p) } \nThe correctness of the function follows from the observation that fp(m, b, i)= mbi, which we can prove \nby induction. Indeed, if we consider the case 2 * j +1, we observe: fp(m, b, i)= fp(m, b, 2j +1) = fp(mb, \nb2,j) (by induct. hypothesis) = mb(b2)j = mb2j+1 = mbi Note how the pattern matching on integer arithmetic \nexpressions exposes the equations that make the inductive proof simpler. The pattern matching compiler \ngenerates the code that decomposes i into the appropriate new exponent j. Moreover, it checks that the \npattern matching is exhaustive. The construct supports arbitrary ex\u00adpressions of linear integer arithmetic, \nand can prove for example that the set of patterns 2 * k, 3 * k, 6 * k - 1, 6 * k + 1 is exhaus\u00adtive. \nThe system also accepts implicit de.nitions, such as val 42 * x+5 * y=z The system ensures that the above \nde.nition matches every integer z, and emits the code to compute x and y from z. Our approach and implementation \nalso work for parameterized integer arithmetic formulas, which become linear only once the pa\u00adrameters \nare known. For example, our synthesizer accepts the fol\u00adlowing speci.cation that decomposes an offset \nof a linear represen\u00adtation of a three-dimensional array with statically unknown dimen\u00adsions into indices \nfor each coordinate: val (x1, y1,z1) =choose((x:Int, y:Int,z:Int) . o.set == x + dimX * y+ dimX * dimY \n* z&#38;&#38; 0 = x&#38;&#38;x < dimX &#38;&#38; 0 = y&#38;&#38;y < dimY &#38;&#38; 0 = z&#38;&#38;z \n< dimZ) Here dimX, dimY, dimZ are variables whose value is unknown until runtime. Note that the satis.ability \nof constraints that con\u00adtain multiplications of variables is in general undecidable. In such parameterized \ncase our synthesizer is complete in the sense that it generates code that 1) always terminates, 2) detects \nat run-time whether a solution exists for current parameter values, and 3) com\u00adputes one solution whenever \na solution exists. In addition to integer arithmetic, other theories are amenable to synthesis and provide \nsimilar bene.ts. Consider the problem of splitting a set collection in a balanced way. The following \ncode attempts to do that: val (a1,a2) = choose((a1:Set[O],a2:Set[O]) . a1 union a2 == s &#38;&#38; a1 \nintersect a2 == empty &#38;&#38; a1.size == a2.size) It turns out that for the above code our synthesizer \nemits a warning indicating that there are cases where the constraint has no solutions. Indeed, there \nare no solutions when the set s is of odd size. If we weaken the speci.cation to val (a1,a2) = choose((a1:Set[O],a2:Set[O]) \n. a1 union a2 == s &#38;&#38; a1 intersect a2 == empty &#38;&#38; a1.size - a2.size = 1&#38;&#38; a2.size \n- a1.size = 1) then our synthesizer can prove that the code has a solution for all possible input sets \ns. The synthesizer emits code that, for each input, computes one such solution. The nature of constraints \non sets is that if there is one solution, then there are many solutions. Our synthesizer resolves these \nchoices at compile time, which means that the generated code is deterministic.  3. From Decision to \nSynthesis Procedures We next de.ne precisely the notion of a synthesis procedure and describe a methodology \nfor deriving synthesis procedures from decision procedures. Preliminaries. Each of our algorithms works \nwith a set of for\u00admulas, Formulas, de.ned in terms of terms, Terms.Formulas denote truth values, whereas \nterms and variables denote values from the domain (e.g. integers). We denote the set of variables by \nVars. FV(q) denotes the set of free variables in a formula or aterm q.If xx =(x1,...,xn) then xxs denotes \nthe set of vari\u00adables {x1,...,xn}.If q is a term or formula, xx =(x1,...,xn) a vector of variables and \nxt =(t1,...,tn) a vector of terms, then q[xx := xt] denotes the result of substituting in q the free \nvariables x1,...,xn with terms t1,...,tn, respectively. Given a substitution s : FV(F ) . Terms, we write \nFs for the result of substituting each x . FV(F ) with s(x). Formulas are interpreted over ele\u00adments \nof a .rst-order structure D with a countable domain D.We assume that for each e . D there exists a ground \nterm ce whose interpretation in D is e;let C = {ce | e . D}. We further assume that if F . Formulas then \nalso F [x := ce] . Formulas (the class of formulas is closed under partial grounding with constants). \nThe choose programming language construct. We integrate into a programming language a construct of the \nform xr = choose(xx . F ) (1) Here F is a formula (typically represented as a boolean-valued programming \nlanguage expressions) and xx . F denotes an anonymous function from xx to the value of F (that is, .xx.F \n). Two kinds of variables can appear within F : output variables xx and parameters xa. The parameters \nxa are program variables that are in scope at the point where choose occurs; their values will be known \nwhen the statement is executed. Output variables xx denote values that need to be computed so that F \nbecomes true, and they will be assigned to xr as a result of the invocation of choose. We can translate \nthe above choose construct into the following sequence of commands in a guarded command language [Dij76]: \nassert (.xx.F ); havoc (xr); assume (F [xx := xr]); The simplicity of the above translation indicates \nthat it is natu\u00adral to represent choose within existing veri.cation systems (e.g. [FLL+02, ZKR08]) The \nuse of choose can help veri.cation be\u00adcause the desired property F is explicitly assumed and can aid \nin proving the subsequent program assertions. Model-generating decision procedures. As a starting point \nfor our synthesis algorithms for choose invocations we consider a model-generating decision procedure. \nGiven F . Formulas we expect this decision procedure to produce either  a) a substitution s : FV(F ) \n. C such that Fs is a true, or b) a special value unsat indicating that the formula is unsatis.able. \nWe assume that the decision procedure is deterministic and behaves as a function. We write Z(F )=s or \nZ(F )=unsat to denote the result of applying the decision procedure to F . Baseline: invoking a decision \nprocedure at run-time. Just like an interpreter can be considered as a baseline implementation for a \ncompiler, deploying a decision procedure at run-time can be considered as a baseline for our approach. \nIn this scenario, we replace the invocation of (1) with F = makeFormulaTree(makeVars(x ), makeGroundTerms(xa)); \nxr =(Z(F ) match { case s . (s(x1),...,s(xn)) case unsat . throw new Exception( No solution exists ) \n}) The dynamic invocation approach is .exible and useful. However, there are important performance and \npredictability advantages of an alternative compilation approach. Synthesis based on decision procedures. \nOur goal is therefore to explore a compilation approach where a modi.ed decision pro\u00adcedure is invoked \nat compile time, converting the formula into a solved form. DEFINITION 1 (Synthesis Procedure). We denote \nan invocation of x a synthesis procedure by [xx, F ] =(pre, .). A synthesis procedure takes as input \na formula F and a vector of variables x and outputs a pair of 1. a precondition formula pre with FV(pre) \n. FV(F ) \\ x s 2. a tuple of terms x.) . FV(F ) \\ x s  . with FV(x such that the following two implications \nare valid: (.xx.F ) . pre pre . F [x := x .] OBSERVATION 2. Because another implication always holds: \nF [x := xx.F .] ..x the above de.nition implies that the three formulas are all equiv\u00ad x alent: (.xx.F \n), pre,F [x := .]. Consequently, if we can de.ne x a function witn where for witn(xx, F )= .) . . we \nhave FV(xFV(F ) \\ x s and .xx.F implies F [x := .], then we can de.ne a synthesis procedure by [xx, F \n] =(F [x := witn(xx, F )], witn(xx, F )) The reason we use the translation that computes pre in addition \nto witn(xx, F ) is that the synthesizer performs simpli.cations when generating pre, which can produce \na formula faster to evaluate than F [x := witn(xx, F )]. The synthesizer emits the terms . in compiler \nintermediate x representation; the standard compiler then processes them along with the rest of the code. \nWe identify the syntax tree of x . with its meaning as a function from the parameters xa to the output \nvariables x . The overall compile-time processing of the choose statement (1) involves the following: \n1. emit a non-feasibility warning if the formula \u00acpre is satis.able, reporting the counterexample for \nwhich the synthesis problem has no solutions; 2. emit a non-uniqueness warning if the formula F . F [x \n:= xy] . x = yx is satis.able, reporting the values of all free variables as a counterexample showing \nthat there are at least two solutions; 3. as the compiled code, emit the code that behaves as x assert(pre); \nxr = . The existence of a model-generating decision procedure implies the existence of a trivial synthesis \nprocedure, which satis.es De.nition 1 but simply invokes the decision procedure at run-time. The usefulness \nof the notion of synthesis procedure comes from the fact that we can often create compiled code that \navoids this trivial solution. Among the potential advantages of the compilation approach are: improved \nrun-time ef.ciency, because part of the reasoning is done at compile-time;  improved error reporting: \nthe existence and uniqueness of solu\u00adtions can be checked at compile time;  simpler deployment: the \nemitted code can be compiled to any of the targets of the compiler, and requires no additional run-time \nsupport.  This paper therefore pursues the compilation approach. As for the processing of more traditional \nprogramming language constructs, we do believe that there is space in the future for mixed approaches, \nsuch as just-in-time synthesis and pro.ling-guided synthesis . Ef.ciency of synthesis. We introduce the \nfollowing measures to quantify the behavior of synthesis procedures: time to synthesize the code, as \na function of F ;  size of the synthesized code, as a function of F ;  running time of the synthesized \ncode as a function of F and a measure of the run-time values of xa.  When using F as the argument of \nthe above measures, we often consider not only the size of F , but also the dimension of the variable \nvector x and the parameter vector xa in F . From quanti.er elimination to synthesis. The precondition \npre can be viewed as a result of applying quanti.er elimination (see e.g. [Nip08]) to remove x from F \n, with the following differences. 1. Synthesis procedures strengthen quanti.er elimination proce\u00addures \nby identifying not only pre but also emitting the code x . that ef.ciently computes a witness for x . \n2. Quanti.er elimination is typically applied to arbitrary quanti\u00ad.ed formulas of .rst-order logic and \naims to successively elim\u00adinate all variables. To enable recursive application of variable elimination, \npre must be in the same language of formulas as F . This condition is not required in our case. 3. Worst-case \nbounds on quanti.er elimination algorithms mea\u00adsure the size of the generated formula and the time needed \nto xx generate it, but not the size of . or the time to evaluate .. For some domains, it can be computationally \nmore dif.cult to compute (or even print ) the solution than to simply check the existence of a solution. \nDespite the differences, we have found that we can naturally ex\u00adtend existing quanti.er elimination procedures \nwith explicit com\u00adputation of witnesses that constitute the program x ..  4. Selected Generic Techniques \nWe next describe some basic observations and techniques for syn\u00adthesis that are independent of a particular \ntheory. 4.1 Synthesis for Multiple Variables Suppose that we have a function witn(x, F ) that corresponds \nto constructive quanti.er elimination step for one variable and pro\u00adduces a term . such that F [x := \n.] holds iff .x.F holds. We can then lift witn(x, F ) to synthesis for any number of variables, using \nthe following translation scheme with non-tail recursion: S` \u00b4 [ , ] : Varsn \u00d7 Formulas . Formulas \u00d7 \nTermsn n [(),F ] =(F, ()) [(x1,...,xn),F ] = let .n = witn(xn,F ) F . = simplify(F [xn := .n]) (pre, \n(.1,..., .n-1)) = [(x1,...,xn-1),F '] .. n =.n[x1 := .1,...,xn-1 := .n-1] in (pre, (.1,..., .n-1, .. \n)) n The above translation includes the base case in which there are no variables to eliminate, so F \nbecomes the precondition, and the recursive case that applies the witn function. In implementation we \ncan use local variable de.nitions instead of substitutions. Given (1), we generate as x . a Scala code \nblock 89 >> val x1 =.1 >> >> <... = val xn-1 =.n-1 >> >val xn =.n > >> :; x where the variables in .n \ndirectly refer to variables computed in .1,..., .n-1 and where FV(.i) . FV(F ) \\{xi,...,xn}.A consequence \nof this recursive translation pattern is that the synthe\u00adsized code computes values in reverse order \ncompared to the steps of a quanti.er elimination procedure. This observation can be help\u00adful in understanding \nthe output of our synthesis procedures. 4.2 One-Point Rule Synthesis If x/. FV(t) we can de.ne witn(x, \nx = t . F )= t If the formula does not have the form x = t . F , we can often rewrite it into this form \nusing theory-speci.c transformations. 4.3 Output-Independent Preconditions Whenever FV(F1) n x s = \u00d8, \nwe can apply the following synthesis rule: xx [xx, F1 . F2] = let (pre, .) = [xx, F2] in (pre . F1, .) \nwhich moves a constant conjunct of the speci.cation into the precondition. We assume that this rule is \napplied whenever possible and do not explicitly mention it in the sequel. 4.4 Propositional Connectives \nin First-Order Theories Consider a quanti.er-free formula in some .rst-order theory. Con\u00adsider the tasks \nof checking formula satis.ability or applying elim\u00adination of a variable. For both tasks, we can .rst \nrewrite the for\u00admula into disjunctive normal form and then process each disjunct independently. This \nallows us to focus on handling conjunctions of literals as opposed to arbitrary propositional combination. \nWe next show that we can similarly use disjunctive normal form in synthesis. Consider a formula D1 . \n... . Dn in disjunctive nor\u00admal form. We can apply synthesis to each Di yielding a precondi\u00adtion prei \nand the solved form x .i. We can then synthesize code with conditionals that select the .rst x .i that \napplies: [xx, D1 . ... . Dn] = let (pre1, .1)= [xx, D1] x ... (pren, .n)= [xx, Dn] x in 89 1 0 x > \nif (pre1).1 > >> >> B >C > x >else if (pre2).2 > B> >C n <= B_ C ... BC prei, BC x >else if (pre) > B> \n.n >C i=1 > n> @> >A >else > >> :; throw new Exception( No solution ) Although the disjunctive normal \nform can be exponentially larger than the original formula, the transformation to disjunctive normal \nform is used in practice [Pug92] and has advantages in terms of the quality of synthesized code generated \nfor individual disjuncts. What further justi.es this approach is that we expect a small number of disjuncts \nin our speci.cations, and may need dif\u00adferent synthesized values for variables in different disjuncts. \nOther methods can have better worst-case quanti.er elimination complexity [Coo72, FR79, Wei97, Nip08] \nthan disjunctive normal form approaches. We discuss these alternative approaches in the sequel as well, \nbut it is the above disjunctive normal form approach that we currently use in our implementation.  4.5 \nSynthesis for Propositional Logic Our paper focuses on synthesis for formulas over unbounded do\u00admains. \nNonetheless, to illustrate the potential asymptotic gain of precomputation in synthesis, we illustrate \nsynthesis for the case when F is a propositional formula (see e.g. [KS00] for a more so\u00adphisticated approach \nto this problem). Suppose that x are output variables and xa are the remaining propositional variables \n(parame\u00adters) in F . To synthesize a function from xa to x , build an ordered binary decision diagram \n(OBDD) [Bry86] for F , treating both xa and x as variables for OBDD construction, and using a variable \nordering that puts all parameters xa before all output variables x . Then split the OBDD graph at the \npoint where all the decisions on xa have been made. That is, consider the set of nodes that terminate \non some paths on which all decisions on xa have been made and no decisions on x have been made. For each \nof these OBDD nodes, we precompute whether this node reaches the true sink node. As the result of synthesis, \nwe emit the code that consists of nested if\u00adthen-else tests encoding the decisions on xa, followed by \nthe code that, for each node that reaches true emits those values of x that trace one path to the true \nsink node. Consider the code generated using the method above. Note that, although the size of the code \nis bounded by a single exponential, the code executes in time linear in the total number of variables \nxa and x . This is in contrast to NP-hardness of .nding a satisfying assignment for a propositional formula \nF , which would occur in the baseline approach of invoking a SAT solver at run-time. In summary, for \npropositional logic synthesis (and, more generally, for NP-hard constraints over bounded domains) we \ncan precompute solutions and generate code that computes unknown propositional values in deterministic \npolynomial time in the size of inputs and outputs. In the next several sections, we describe synthesis \nprocedures for several useful decidable logics over in.nite domains (numbers and data structures) and \ndiscuss the ef.ciency improvements due to synthesis.   5. Synthesis for Linear Rational Arithmetic \nWe next consider synthesis for quanti.er-free formulas of linear arithmetic over rationals. In this theory, \nvariables range over ratio\u00adnal numbers, terms are linear expressions c0 + c1x1 + ... + cnxn, and the \nrelations in the language are < and =. Synthesis for this theory can be used to synthesize exact fractional \narithmetic compu\u00adtations (or .oating-point computations if we are willing to ignore the rounding errors). \nIt also serves as an introduction to the more complex problem of integer arithmetic synthesis that we \ndescribe in the following sections. Given a quanti.er-free formula, we can ef.ciently transform it to \nnegation-normal form. Furthermore, we observe that \u00ac(t1 <t2) is equivalent to (t2 <t1) . (t1 = t2) and \nthat \u00ac(t1 = t2) is equivalent to (t1 <t2) . (t2 <t1). Therefore, there is no need to consider negations \nin the formula. We can also normalize the equalities to the form t =0 and the inequalities to the form \n0 <t. 5.1 Solving Conjunctions of Literals Given the observations in Section 4.4, we consider conjunctions \nof literals. The method follows Fourier-Motzkin elimination [Sch98]. Consider the elimination of a variable \nx. Equalities. If x occurs in an equality constraint t =0, then solve the constraint for x and rewrite \nit as x = t',where t' does not contain x. Then simply apply the one-point rule synthesis (Sec\u00adtion 4.2). \nThis step amounts to Gaussian elimination. We follow this step whenever possible, so we .rst eliminate \nthose variables that occur in some equalities and only then proceed to inequalities. Inequalities. Next, \nsuppose that x occurs only in strict inequali\u00adties 0 <t. Depending on the sign of x in t, we can rewrite \nthese inequalities into ap <x or x<bq for some terms ap,bq . Consider the more general case when there \nis both at least one lower bound ap and at least one upper bound bq . We can then de.ne: witn(x, F )=(max \n{ap} +min {bq })/2 pq As one would expect from quanti.er elimination, the pre corre\u00adsponding to this \ncase results from F by replacing the conjunction of all inequalities containing x with the conjunction \n^ ap <bq p,q In case there are no lower bounds ap,we de.ne witn(x, F )= minq{bq }- 1; if there are no \nupper bounds bq,we de.ne witn(x, F )=maxp{ap} +1. Complexity of synthesis for conjunctions. We next examine \nthe size of the generated code for linear rational arithmetic. The elim\u00adination of input variables using \nequalities is a polynomial-time transformation. Suppose that after this elimination we are left with \nN inequalities and V remaining input variables. The above inequal\u00adity elimination step for one variable \nreplaces N inequalities with (N/2)2 inequalities in the worst case. After eliminating all out\u00ad put variables, \nan upper bound on the formula increase is (N/2)2V . Therefore, the generated formula can be in the worst \ncase doubly exponential in the number of output variables V .However, for a .xed V , the generated code \nsize is a (possibly high-degree) poly\u00adnomial of the size of the input formula. Also, if there are 4 or \nfewer inequalities in the original formula, the .nal size is polynomial, regardless of V . Finally, note \nthat the synthesis time and the exe\u00adcution time of synthesized code are polynomial in the size of the \ngenerated formula. 5.2 Disjunctions for Linear Rational Arithmetic We next consider linear arithmetic \nconstraints with disjunctions, which are constraints for which the satis.ability is NP-complete. One \nway to lift synthesis for rational arithmetic from conjunctions of literals to arbitrary propositional \ncombinations is to apply the disjunctive normal form method of Section 4.4. We then obtain a complexity \nthat is one exponential higher in formula size than the complexity of synthesis for conjunctions. In \nthe rest of this section we consider an alternative to disjunc\u00adtive normal form. This alternative synthesizes \ncode that can execute exponentially faster (even though it is not smaller) compared to the disjunctive \nnormal form approach of Section 4.4. The starting point of this method are quanti.er elimination techniques \nthat avoid disjunctive normal form transformation, e.g. [FR79], [Nip08], [BM07, Section 7.3]. To remove \na variable from negation normal form, this method .nds relevant lower bounds ap and upper bounds bq in \nthe formula, then computes the values mpq =(ap + bq)/2 and replaces a variable xi with the values from \nthe set {mpq }p,q extended with suf.ciently small and suf\u00ad.ciently large values [Nip08]. This quanti.er \nelimination method gives us a way to compute pre. We next present how to extend this quanti.er elimination \nmethod to synthesis, namely to the computation of witn(x, F ). Consider a substitution in quanti.er elimination \nstep that replaces variable xi with the term m. We then extend this step to also at\u00adtach to each literal \na special substitution syntactic form (xi . m). When using this process to eliminate one variable, the \nsize of the formula can increase quadratically. After eliminating all output variables, we obtain a formula \npre with additional annotations; the 2O(V ) size of this formula is bounded by n where n is the original \nformula size. (Again, although it is doubly exponential in V ,it is not exponential in n.) We can therefore \nbuild a decision tree that evaluates the values 2O(V ) of all n literals in pre. On each complete path \nof this tree, we can, at synthesis time, determine whether the truth values of literals imply that pre \nis true. Indeed, such computation reduces to evaluating the truth value of a propositional formula in \na given assignment to all variables. In the cases when the literals imply that pre holds, we use the \nattached substitution (xi . m) in true literals to recover the synthesized values of variables xi. Such \ndecision tree 2O(V ) has the depth n , because it tests the values of all literals in the result of quanti.er \nelimination. For a constant number of variables V , this tree represents a synthesized program whose \nrunning time is polynomial in n. Thus, we have shown that using basic methods of quanti.er elimination \n(without relying on detailed geometric facts about the theory of linear rational arithmetic) we can synthesize \nfor each speci.cation formula a polynomial-time function that maps the parameters to the desired values \nof output variables.  6. Synthesis for Linear Integer Arithmetic We next describe our main algorithm, \nwhich performs synthesis for quanti.er-free formulas of Presburger arithmetic (integer linear arithmetic). \nIn this theory variables range over integers. Terms are linear expressions of the form c0 + c1x1 + ... \n+ cnxn, n = 0, ci is an integer constant and xi is an integer variable. Atoms are built using the relations \n=, = and |. The atom c|t is interpreted as true iff the integer constant c divides term t.We use a<b \nas a shorthand for a = b.\u00ac(a = b). We describe a synthesis algorithm that works for conjunction of literals. \nPre-processing. We .rst apply the following pre-processing steps to eliminate negations and divisibility \nconstraints. We remove nega\u00adtions by transforming a formula into its negation-normal form and translating \nnegative literals into equivalent positive ones: \u00ac(t1 = t2) is equivalent to t2 = t1 +1 and \u00ac(t1 = t2) \nis equivalent to (t1 = t2 +1) . (t2 = t1 +1). We also normalize equalities into the form t =0 and inequalities \ninto the form t = 0.  We transform divisibility constraints of a form c|t into equalities by adding \na fresh variable q. The value obtained for the fresh variable q is ignored in the .nal synthesized program: \n[xx, (c|t) . F ] = let (pre, (xx,q),t = cq . F ] ., .n+1)) = [(x in (pre, .) x The negation of divisibility \n\u00ac(c|t) can be handled in a similar way by introducing two fresh variables q and r: [xx, \u00ac(c|t) . F ] \n= let F ' = t + r = cq . 1 = r = c - 1 . F (pre, (x= x, q, r),F ' ] ., .n+1, .n+2)) [(x x in (pre, .) \nIn the rest of this section we assume the input formula F to have no negation or divisibility constraints \n(these constructs can, however, appear in the generated code and precondition). 6.1 Solving Equality \nConstraints for Synthesis Because equality constraints are suitable for deterministic elimina\u00adtion of \noutput variables, our procedure groups all equalities from a conjunction and solves them .rst, one by \none. Let E be one such equation, so the entire formula is of the form E . F .Let xy be the output variables \nthat appear in E. Given an output variable y1 and E of the form cy1 + t =0 for c =0, a simple way to \nsolve it would be to impose the precondition c|t, use the witness y1 = -t/c in synthesized code, and \nsubstitute -t/c instead of y1 in the remaining formula. However, to keep the equations within linear \ninteger arithmetic, this would require multiplying the remaining equations and disequations in F by c, \npotentially increasing the sizes of coef.cients substantially. We instead perform synthesis based on \none of the improved algorithms for solving integer equations. This algorithm avoids the multiplication \nof the remaining constraints by simultaneously replacing all n output variables xy in E with n - 1 fresh \noutput variables x.. Using this algorithm we obtain the synthesis procedure in Figure 1. An invocation \nof eqSyn(xy,F ] but y, F ) is similar to [xxx returns a triple (pre, .,.), which in addition to the precondition \npre and the witness term tuple x.. . also has the fresh variables x 6.1.1 The eqSyn Synthesis Algorithm \nConsider the application of eqSyn in Figure 1 to the equation Smi=1\u00dfibi +Snj=1.jyj =0. If there is only \none output variable, y1, we directly eliminate it from the equation. Assume therefore n> 1.Let d =gcd(\u00df1,...,\u00dfm,.1,...,.n).If \nd> 1 we can divide all coef.cients by d, so assume d =1. Our goal is to derive an alternative de.nition \nof the set K = Sm {xy | i=1\u00dfibi +Sjn =1.jyj =0} which will allow a simple and effective computation of \nelements in K. Note that the set K describes the set of all solutions of a Presburger arithmetic formula. \nRecall that a semilinear set [GS64] is a .nite union of linear sets. Given an integer vector xb and a \n.nite set of integer vectors S, a linear set is a set {x | x = xb + xs1 + ... + xsn; si . S; n = 0}. \nGinsburg and Spanier [GS64, GS66] showed that the set of all solutions of a Presburger arithmetic formula \nis always a semilinear set, which implies that K is semilinear. However, we cannot apply this result \ndirectly because the values of parameter variables are not known until run-time. Instead, we proceed \nin the following steps, as shown in Figure 1: S` \u00b4 [ , ] : Varsn \u00d7 Formulas . Formulas \u00d7 Termsn n [(x= \ny, x ),E . F ] let (preY ,.)= eqSyn(x xx .Y , y,E) F ' = simplify(F [x.Y ]) y := x (pre, (x.X )) = \n[(x' ] x .., .,x ),F preY 0 = preY [x..,x.X ] . := xx := x xx .Y 0 = . := xx := x .Y [x..,x.X ] in \n.Y 0,x (preY 0 . pre, (x.X )) S eqSyn: Varsn\u00d7Formulas . Formulas\u00d7Termsn\u00d7Varsn-1 n eqSyn(y1,t + .1y1 =0) \n= ((.1|t), -t/.1, ()) eqSyn(y1,...,yn,t +Snj=1.jyj =0) = (for t =Smi=1\u00dfibi) let d =gcd(\u00df1,...,\u00dfm,.1,...,.n) \nif (d> 1) eqSyn(y1,...,yn,t/d +Snj=1(.j/d)yj=0) else let (xs1,..., xsn-1)= linearSet(.1,...,.n) (w1,...,wn)= \nparticularSol(t, .1,...,.n) pre =(gcd(.1,...,.n)|t) .1,...,.n-1 - fresh variable names x .=(w1,...,wn)+ \n.1xs1 + ... + .n-1xsn-1 x x in (pre, .,.) Figure 1. Algorithm for Synthesis Based on Integer Equations \n1. obtain a linear set representation of the set n X SH = {xy | .jyj =0} j=1 of solutions for the homogeneous \npart using the function linearSet (de.ned in Section 6.1.2 to compute xs1,... , xsn-1 such that n-1 X \nSH = {yx|..1,...,.n-1 . Z.xy = .ixsi}i=1 2. .nd one particular solution, that is, use the function particularSol \n(de.ned in Section 6.1.3) to .nd a vector of terms P wx(containing the parameters bi) such that t + nj=1 \n.jwj =0 for all values of parameters bi. n-1 X 3. return as the solution wx+ .ixsi i=1 To see that the \nalgorithm is correct, .x the values of parameters and let x. =(.1,...,.n). From linearity we have t + \nx. \u00b7 (wx+ P P j .jxsj )= t - t +0=0, which means that each wx+ j .jxsj is a solution. Conversely, if \nxy is a solution of the equation then P n x.(yx- wx)=0,so yx- wx. SH , which means xy - wx= .ixsi P i=1 \nn for some .i. Therefore, the set of all solutions of t+ j=1 .j wj = P n-1 0 is the set {wx+ .ixsi | \n.i . Z}. It remains to de.ne i=1 linearSet to .nd xsi and particularSol to .nd wx.  6.1.2 Computing \na Linear Set for a Homogeneous Equation This section describes our version of the algorithm linearSet(.1,...,.n) \nthat computes the set of solutions of an equation Sni=1.iyi =0. A related algorithm is a component of \nthe Omega test [Pug92]. We de.ne  linearSet(.1,...,.n)=(xs1, ..., xsn-1) where xsj =(K1j ,...,Knj ) \nand the integers Kij are computed as follows: if i<j, Kij =0 (the matrix K is lower triangular) gcd((.k)k=j+1) \nKjj = gcd((.k)k=j ) for each index j, 1 = j = n - 1, we compute Kij as follows. Consider the equation \nn X .jKjj + .iuij =0 i=j+1 and .nd any solution. That is, compute (K(j+1)j ,...,Knj )= particularSol(-.jKjj \n,.j+1,...,.n) where particularSol is giveninSection6.1.3. Let SH = {xi=1.iyi =0} and let y | Sn SL = \n{.1xs1 + ... + .nxsn | .1,...,.n . Z} = 8 0 1 0 1. 9 K11 K1(n-1) . > > < . = B . C B . C. .1 + ... + \n.n-1 .i . Z @ . A@ . A. .. . >> :;. Kn1 Kn(n-1) We claim SH = SL. First we show that each vector xsj \nbelongs to SH . Indeed, by P nde.nition of Kij we have .jKjj + .iKij =0. This means i=j+1 precisely that \nxsj . SH , by de.nition of xsj and SH . Next, observe that SH is closed under linear combinations. Because \nSL is the set of linear combinations of vectors xsj ,we have SL . SH . To prove that the converse also \nholds, let xy . SH . We will P n-1 show that the triangular system of equations i=1 .ixsi = xy has some \nsolution .1,...,.n-1. We start by showing that we can .nd .1.Let G1 =gcd((.k)k=1).From yx. SH we have \nSni=1.iyi =0,thatis, G1(Sni=1\u00dfiyi)=0 for \u00dfi = .i/G1. This implies \u00df1y1 +Sni=2\u00dfiyi =0 and gcd((\u00dfk)k=1)=1. \nLet G2 =gcd((\u00dfk)k=2).From \u00df1y1 +Sni=2\u00dfiyi =0 we then obtain \u00df1y1 + G2(Sni=2\u00dfi . yi)=0 for \u00dfi ' = \u00dfi/G2. \nTherefore y1 = i=2\u00dfi . yi)/\u00df1. Because gcd(\u00df1,G2)=1 we have -G2(Sn \u00df1|Sin =2\u00dfi . yi so we can de.ne the \ninteger .1 = -Sin =2\u00dfi . yi/\u00df1 and we have y1 = .1G2. Moreover, note that G2 =gcd((\u00dfk)k=2)=gcd((.k)k=2)/G1 \n= K11 Therefore, y1 = .1K11, which ensures that the .rst equation is satis.ed. Consider now a new vector \nxz = yx-.1xs1. Because yx. SH and and xs1 . SH also xz . SH . Moreover, note that the .rst component \nof xz is 0. We repeat the described procedure on xz and xs2.This way we derive the value for an integer \na2 and a new vector that has 0 as the .rst two components. We continue with the described procedure until \nwe obtain a vector xu . SH that has all components set to 0 except for the last two. From xu . SH we \nhave .n-1un-1 + .nun =0. Letting \u00dfn-1 = .n-1/ gcd(.n-1,.n) and \u00dfn = .n/ gcd(.n-1,.n) we conclude that \n\u00dfn-1un-1 + \u00dfnun =0,so un-1/\u00dfn is an integer and we let .n-1 = un-1/\u00dfn. By de.nitions of \u00dfi it follows \n.n-1 = un-1 \u00b7 gcd(.n-1,.n)/.n. Next, observe that xsn-1 has the form (0,..., 0,.n/ gcd(.n-1,.n), -.n-1/ \ngcd(.n-1,.n)). It is then easy to verify that xu = .n-1xsn-1. This procedure shows that every element \nof SH can be repre\u00adsented as a linear combination of vectors xsj,which shows SH . SL and concludes the \nproof.  6.1.3 Finding a Particular Solution of an Equation We .nally describe the particularSol function \nto .nd a solution (as a vector of terms) for an equation t +Sni=1.iui =0.We use the Extended Euclidean \nalgorithm [CLRS01, Figure 31.1] that, given the integers a1 and a2, .nds their greatest common divisor \nd and two integers w1 and w2 such that a1w1 +a2w2 = d. Our algorithm generalizes the Extended Euclidean \nAlgorithm to arbitrary number of variables and uses it to .nd a solution of an equation with parameters. \nWe chose the algorithm presented here because of its simplicity. Other algorithms for .nding a solution \nof an equation t+Sni=1.iui =0 can be found in [Ban88, FH96]. They also run in polynomial time. [Ban88] \nadditionally allows bounded inequality constraints, whereas [FH96] guarantees that the returned numbers \nare no larger than the largest of the input coef.cients divided by 2. The equation t +Sni=1.iui =0 has \na solution iff gcd((.k)k=1)|t, and the result of particularSol is guaranteed to be correct under this \ncondition. Our synthesis procedure ensures that when the results of this algorithm are used, the condition \ngcd((.k)k=1)|t is satis.ed. We start with the base case where there are only two variables, t + .1u1 \n+ .2u2 =0. By the Extended Euclidean Algorithm let v1 and v2 be integers such that .1v1 + .2v2 =gcd(.1,.2).If \nd =gcd(.1,.2) and r = t/d one solution is the pair of terms (-v1r, -v2r): particularSol(t, .1,.2)= let \n(d, v1,v2)= ExtendedEuclid(.1,.2) r = t/d in (-v1r, -v2r) If there are more than two variables, we observe \nthat Sni=2.iui is a multiple of gcd((.k)k=2). We introduce the new variable u ' and .nd a solution of \nthe equation t + .1u1 +gcd((.k)k=2) \u00b7u ' =0 as described above. This way we obtain terms (w1,w ' ) for \n(u1,w ' ). To derive values of u2,...,un we solve the equation Sni=2.iui = gcd((.k)k=2) \u00b7 w ' . Given \nthat the initial equation was assumed to have a solution, the new equation can also be showed to have \na solution. Moreover, it has one variable less, so we can solve it recursively: particularSol(t, .1,...,.n)= \nlet (w1,w ' )= particularSol(t, .1, gcd((.k)k=2)) (w2,...,wn)= particularSol(- gcd((.k)k=2)w ' ,.2,...,.n) \nin (w1,...,wn) Example. We demonstrate the process of eliminating equations on an example. Consider the \ntranslation [(x, y, z), 2a - b +3x +4y +8z =0 . 5x +4z = y - b] To eliminate an equation from the formula \nand to reduce a number of output variables, we .rst invoke eqSyn((x, y, z), 2a - b +3x + 4y +8z =0). \nIt works in two phases. In the .rst phase, it computes the linear set describing a set of solutions of \nthe homogeneous equality 3x +4y +8z =0. Using the algorithm described in Section 6.1.3, it returns: 89 \n. 01 01 . < 40 = . SL = .1 @ -3 A + .2 @ 2 A. .1,.2 . Z . :; 0 -1 . The second phase computes a witness \nvector wxand a precondition formula. Applying the procedure described in Section 6.1.1 results in the \nvector wx=(2a - b, b - 2a, 0) and the formula 1|2a - b. Finally, we compute the output of eqSyn applied \nto 2a - b +3x + 4y +8z =0: it is a triple consisting of 1. a precondition 1|2a - b 2. a list of terms \ndenoting witnesses for (x, y, z):  .1 =2a - b +4.1 .2 = b - 2a - 3.1 +2.2 .3 = -.2 3. a list of fresh \nvariables (.1,.2). We then replace each occurrence of x, y and z by the corresponding terms in the rest \nof the formula. This results in a new formula 7a - 3b +13.1 = 4.2. It has the same input variables, but \nthe output variables are now .1 and .2. To .nd a solution for the initial problem, we let (preX , (F1, \nF2)) = [(.1,.2), 7a - 3b +13.1 = 4.2] Since 1|2a - b is a valid formula, we do not add it to the .nal \nprecondition. Therefore, the .nal result has the form (preX , (2a - b +4F1,b - 2a - 3F1 +2F2, -F2)) \n 6.2 Solving Inequality Constraints for Synthesis In the following, we assume that all equalities are \nalready processed and that a formula is a conjunction of inequalities. Dealing with inequalities in the \ninteger case is similar to the case of rational arithmetic: we process variables one by one and proceed \nfurther with the resulting formula. Let x be an output variable that we are processing. Every con\u00adjunct \ncan be rewritten in one of the two following forms: [Lower Bound] Ai = aix [Upper Bound] \u00dfj x = Bj As \nfor rational arithmetic, x should be a value which is greater than all lower bounds and smaller than \nall upper bounds. However, this time we also need to enforce that x must be an integer. Let a =maxi Ai/ail \nand b =minj LBj/\u00dfj J.If b is de.ned (i.e. at least one upper bound exists), we use b as the witness for \nx, otherwise we use a. The corresponding formula with which we proceed is a con\u00adjunction stating that \neach lower bound is smaller than every upper bound: ^ Ai/ail=LBj /\u00dfj J (2) i,j Because of the division, \n.oor, and ceiling operators, the above formula is not in integer linear arithmetic. However, in the absence \nof output variables, it can be evaluated using standard programming language constructs. On the other \nhand, if the terms Ai and Bj contain output variables, we convert the formula into an equivalent linear \ninteger arithmetic formula as follows. With lcm we denote the least common multiple. Let L = lcmi,j (ai,\u00dfj \n). We introduce new integer linear arithmetic terms A ' i = L Ai and Bj ' = L Bj. Using these terms we \nderive an ai \u00dfj equivalent integer linear arithmetic formula: \u00b0. \u00a8. Ai/ail=LBj/\u00dfj J. Ai' /L = Bj' /L \n. A ' i Bj ' - Bj ' mod L =. Bj ' mod L = Bj ' - A ' i LL ' '' . Bj = L \u00b7 lj + kj . kj = Bj - Ai Formula \n(2) is then equivalent to ^^ ' '' (Bj = L \u00b7 lj + kj . (kj = Bj - Ai)) ji We still cannot simply apply \nthe synthesizer on that formula. Let {1,...,J} be a range of j indices. The newly derived formula contains \nJ equalities and 2 \u00b7 J new variables. The process of eliminating equalities as described in Section 6.1 \nwill at the end result in a new formula which contains J new output variables and this way we cannot \nassure termination. Therefore, this is not a suitable approach. However, we observe that the value of \nkj is always bounded: kj .{0,...,L - 1}. Thus, if the value of kj were known, we would have a formula \nwith only J new variables and J additional equations. The equation elimination procedure described before \nwould then result in a formula that has one variable less than the original starting formula, and that \nwould guarantee termination of the approach. Since the value of each kj variable is always bounded, there \nare .nitely many (J \u00b7 L) possible instantiations of kj variables. There\u00adfore, we need to check for each \ninstantiation of all kj variables whether it leads to a solution. As soon as a solution is found, we \nstop and proceed with the obtained values of output variables. If no solution is found, we raise an exception, \nbecause the original formula has no integer solution. This leads to a translation schema that contains \nJ \u00b7 L conditional expression. In our implementation we generate this code as a loop with constant bounds. \nWe .nish the description of the synthesizer with an example that illustrates the above algorithm. Example. \nConsider the formula 2y - b = 3x + a . 2x - a = 4y + b where x and y are output variables and a and b \nare input variables. If the resulting formula 2y - b - a/3l= L4y + a + b/2J has a solution, then the \nsynthesizer emits the value of x to be L4y + a + b/2J. This newly derived formula has only one output \nvariable y, but it is not an integer linear arithmetic formula. It is converted to an equivalent integer \nlinear arithmetic formula (4y + a + b) \u00b7 3=6l + k . k = 8y +5a +5b, which has three variables: y, k and \nl.The value of k is bounded: 0 = k = 5, so we treat it as a parameter. We start with elimination of the \nequality: it results in the precondition 6|3a +3b - k, the list of terms l =(3a +3b - k)/6+2a, y = a \nand a new variable: a. Using this, the inequality becomes k -5a -5b = 8a. Because a is the only output \nvariable, we can compute it as (k - 5a - 5b)/8l. The synthesizer .nally outputs the following code, which \ncomputes values of the initial output variables x and y: val kFound = false for k= 0to5 do { val v1 = \n3 * a+3 * b - k if (v1mod 6== 0) { val alpha = ((k - 5 * a - 5 * b)/8).ceiling val l= (v1 / 6) + 2 * \nalpha val y = alpha val kFound = true break }} if (kFound) val x = ((4 * y+ a + b)/2)..oor else throw \nnew Exception( No solution exists ) The precondition formula is .k. 0 = k = 5 . 6|3a +3b - k, which our \nsynthesizer emits as a loop that checks 6|3a +3b - k for k .{0,..., 5} and throws an exception if the \nprecondition is false.  6.3 Disjunctions in Presburger Arithmetic We can again lift synthesis for conjunctions \nto synthesis for arbi\u00adtrary propositional combinations by applying the method of Sec\u00adtion 4.4. We also \nobtain a complexity that is one exponential higher than the complexity of synthesis from the previous \nsection. Ap\u00adproaches that avoid disjunctive normal form can be used in this case as well [Nip08, FR79, \nWei97].  6.4 Optimizations used in the Implementation In this section we describe some optimizations \nand heuristics that we use in our implementation. Using some of them, we obtained a speedup of several \norders of magnitude. Merging inequalities. Whenever two inequalities t1 = t2 and t2 = t1 appear in a \nconjunction, we substitute them with an equality t1 = t2. This makes the process of variable elimination \nmore ef.cient. Heuristic for choosing the right equality for elimination. When there are several equalities \nin a formula, we choose to eliminate an equality for which the least common multiple of all the coef.\u00adcients \nis the smallest. We observed that this reduces the number of integers to iterate over. Some optimizations \non modulo operations. When processing in\u00adequalities, as described in Section 6.2, as soon as we introduce \nthe modulo operator, we face a potentially longer processing time. This is because .nding the suitable \nvalue of the remainder in equation Bj ' mod L = Bj ' - A ' i requires invoking a loop. While searching \nfor a witness, we might need to test all possible L values. Therefore, we try not to introduce the modulo \noperator in the .rst place. This is possible in several cases. One of them is when either ai =1 or bj \n=1. In that case, if for example ai =1, an equivalent integer arithmetic formula is easily derived: Ai/ail=LBj/\u00dfj \nJ. Ai =LBj /\u00dfj J. \u00dfj Ai = Bj Another example where we do not introduce the modulo operator is when A \n' i -Bj ' evaluates to a number N such that N>L.In that case, it is clear that Bj ' mod L = Bj ' - Ai \n' is a valid formula and thus the returned formula is true. Finally, we describe an optimization that \nleads to a reduction in the number of loop executions. This is possible when there exists an integer \nN such that Bj ' = N \u00b7 Tj and L = N \u00b7 L1.(Unless L = \u00dfj , this is almost always the case.) In the case \nwhere N exists, then kj also has to be a multiple of N. Putting this together, an equivalent formula \nof Bj' mod L = Bj ' - Ai ' is the formula Tjmod L1 = kj . N \u00b7 kj = Bj ' - Ai' . This reduces the number \nof loop iterations by at least a factor of N.  7. Synthesis Algorithm for Parameterized Presburger \nArithmetic In addition to handling the case when the speci.cation formula is an integer linear arithmetic \nformula of both parameters and output variables, we have generalized our synthesizer to the case when \nthe coef.cients of the output variables are not only integers, but can be any arithmetic expression over \nthe input variables. This extension allows us to write e.g. the offset decomposition program from Section \n2 with statically unknown dimensions dimX, dimY, dimZ. As a slightly simpler example, consider the following \ninvocation: val (valueX, valueY) = choose((x: Int, y: Int) . (o.set == x + dim * y&#38;&#38;0 = x&#38;&#38;x \n< dim )) Here o.set and dim are input variables, whereas x and y are output variables. Note that dim*y \nis not a linear term. However, at run\u00adtime we know the exact value of dim, so the term will become linear. \nOur synthesizer can handle such cases as well through a generalization of the algorithm in Section 6. \nGiven the problem above, we .rst eliminate the equality o.set = x + dim * y and we obtain the new problem \nconsisting of two inequalities: dim*t = o.set .o.set -dim+1 = dim*t.The variable t is a freshly introduced \ninteger variable and it is also the only output variable. At this point, the synthesizer needs to divide \na term by the variable dim. In general it thus needs to generate code F A B ::= ::= ::= A | F1 . F2 | \nF1 . F2 |\u00acF B1 = B2 | B1 . B2 | T1 = T2 | T1 <T2 | (K|T ) x |\u00d8 |U | B1 . B2 | B1 n B2 | Bc T ::= k | \nK | T1 + T2 | K \u00b7 T | |B| K ::= ... -2 |-1 | 0 | 1 | 2 ... Figure 2. A Logic of Sets and Size Constraints \n(BAPA) that distinguishes the cases when dim is positive, negative, or zero. In this particular example, \ndue to the constraint 0 = x< dim, only one case applies. The synthesizer returns the following precondi\u00adtion: \npre = (offset - dim +1)/diml=Loffset/dimJ It can easily be veri.ed that this is a valid formula for all \npositive values of dim. The synthesizer also returns the code that computes the values for x and y val \nt= (o.set/dim)..oor val valueY = t val valueX = o.set - dim * t Our general algorithm for handling parametrized \nPresburger arithmetic follows the algorithm described in Section 6. The main difference is that instead \nof manipulating known integer coef.\u00adcients, it manipulates arbitrary arithmetic expressions as coef.\u00adcients. \nIt therefore needs to postpone to run-time certain decisions that involve coef.cients. The key observation \nthat makes this al\u00adgorithm possible is that many compile-time decisions depend not on the particular \nvalues of the coef.cients, but only on their sign (positive, negative, or zero). In the presence of a \ncoef.cient that de\u00adpends on a parameter, the synthesizer therefore generates code with multiple branches \nthat cover the different cases of the sign. The coef.cients of the invocation of the Extended Euclidean \nalgorithm generally also become known only at run-time only, so the generated code invokes this algorithm \nas a library function. The situation is analogous for the gcd function. Finally, note that the running \ntime of the programs in this case is not uniform with respect to the values of all parameters. In par\u00adticular, \nthe upper bounds of the generated for loops in Section 6.2 can now be a function of parameters. Nevertheless, \nfor each value of the parameter, the generated code terminates.  8. Synthesis for Sets with Size Constraints \nIn this section we de.ne a logic of sets with cardinality constraints and describe a synthesis procedure \nfor it. The logic we consider is BAPA (Boolean Algebra with Presburger Arithmetic). It supports the standard \noperators union, intersection, complement, subset, and equality. In addition, it supports the size operator \non sets, as well as integer linear arithmetic constraints over these sizes. Its syntax is shown in Figure \n2. Decision procedures for BAPA were considered in a number of scenarios [FV59, Zar04, Zar05, KNR06, \nKR07]. As in the previous sections, we consider the problem (1) xr = choose(x . F (xx,xa)) where the \ncomponents of vectors xa, xx, xr are either set or integer variables and F is a BAPA formula. Figure \n3 describes our BAPA synthesis procedure that returns a precondition predicate pre(xa) and a solved form \nx .. The proce\u00addure is based on the quanti.er elimination algorithm presented in [KNR06], which reduces \na BAPA formula to an equisatis.able in\u00adteger linear arithmetic formula. The algorithm eliminates set \nvari\u00adables in two phases. In the .rst phase all set expressions are rewrit\u00ad  INPUT: aformula X,xk,xin \nlogic F ( xY,xl) the de\u00ad.ned in Figure 2 with input variables X1, ..., Xn, k1, ..., km and output variables \nY1,...,Ys,l1,...,lt,where Xi and Yj are set variables, ki and lj are integer variables OUTPUT: code that \ncomputes values for the output vari\u00adables from the input variables 1. Apply the .rst steps towards a \nPresburger arithmetic formula: (a) Replace each atom S1 = S2 with S1 . S2 . S2 . S1 (b) Replace each \natom S1 . S2 with |S1 n S2c| =0  2. Introduce the Venn regions of sets Xi s and Yj s: let u be a binary \nword of the length n+m. The set variable Ru represents a Venn region where each 1 stands for a set and \n0 stands for a complement. To illustrate, if n =2, m =1 and u = 001, then R001 = X1 c n X2 c n Y1. Rewrite \neach set expression as a disjoint union of corresponding Venn regions. 3. Create a Presburger arithmetic \nformula: an integer variable hu denotes the cardinality of the Venn region Ru. Use the fact that |S1 \n. S2| = |S1| + |S2| iff S1 and S2 are disjoint to rewrite the whole formula as the Presburger arithmetic \nformula. We denote  x the resulting formula by F1(hxu, k,xl). 4. Create a Presburger arithmetic formula \nthat corresponds to quanti.er elimination: let v be a binary word of length n.A set variable Pv denotes \na Venn region of input set variables, which means that |Pv | is a known value. Create a formula that \nexpresses each |Pv| as a sum of corresponding hu s. De.ne the formula F2(hxu, |Px v|) as the conjunction \nof all those formulas. 5. Create code that computes values of output vectors. First in\u00advoke the linear \narithmetic synthesizer described in Section 6 to generate the code corresponding to:  xxx val (hxun, \nlx n)= choose((hxu, l) . F1(hxu, k, l) .F2(hxu, |Px v|)) Invoking the synthesizer returns code that computes \nexpres\u00adsions for the integer output variables xln and for the variables hxun . For each set output variable \nYi, do the following: let Si be a set containing already known or de.ned set variables, let Tj be a Vennregionof \nSi . Yi that is contained in Yi. Each Tj re\u00adgion is contained in the bigger Venn region Uj which is a \nVenn region of sets in Yi. For each Tj do: take all Ru that belong to Tj and let dj be the sum of all \ncorresponding hun. Based on the value of dj , output the following code: if Tj .nS.Si Sc and dj > 0, \noutput the assignment Kj = fresh(dj )  if dj =0, output the assignment Kj = \u00d8  if dj = |Uj |, output \nthe assignment Kj = Uj  otherwise output the assignment Kj = take(dj , Uj ) Finally, construct Yi as \na union of all Kj sets: Yi = .jKj Figure 3. Algorithm for synthesizing a function . such that F [x :=.(xa)] \nholds, where F has the syntax of Figure 2 ten as unions of disjoint Venn regions. The second phase introduces \na fresh integer variable for the cardinality of each Venn region. It thus reduces the entire formula \nto an integer linear arithmetic for\u00admula. The input variables in this integer arithmetic formula are \nthe integer input variables from the original formula, as well as fresh integer variables denoting cardinalities \nof Venn regions of the in\u00adput set variables. Note that all values of those input variables are known \nfrom the program. The output variables are the original in\u00adteger output variables and freshly introduced \ninteger variables de\u00adnoting cardinalities of Venn regions that are contained in the output set variables. \nWe can therefore build a synthesizer for BAPA on top of the synthesizer for integer linear arithmetic \ndescribed in Section 6. The integer arithmetic synthesizer outputs the precondition predicate pre and \nemits the code for computing values of the new output variables. The generated code can use the returned \ninteger values to reconstruct a model for the original formula. Notice that the precondition predicate \npre will be a Presburger arithmetic formula with the terms built using the original integer input variables \nand the cardinalities of Venn regions of the original input set variables. As an example, if i is an \ninteger input variable and a and b are set input variables then the precondition predicate might be the \nfollowing formula pre(i, a, b)= |a n b| <i .|a|=|b|. In the last step of the BAPA synthesis algorithm, \nwhen out\u00adputting code, we use functions fresh and take. The function take takes as arguments an integer \nk and a set S, and returns a subset of S of size k. The function fresh(k) is invoked when k fresh elements \nneed to be generated. These functions are used only in the code that computes output values of set variables \n(the linear integer arithmetic synthesizer already produces the code to com\u00adpute the values of integer \noutput variables). The set-valued output variables are computed one by one. Given an output set variable \nYi, the code that effectively computes the value of Yi is emitted in several steps. With Si we denote \na set containing set variables occurring in the original formula whose values are already known. Initially, \nSi contains only the input set variables. Our goal is to de\u00adscribe the construction of Yi in terms of \nsets that are already in Si. We start by computing the Venn regions for Yi and all the sets in Si in \norder to de.ne Yi as a union of those Venn regions. There\u00adfore we are interested only in those Venn regions \nthat are subset of Yi.Let Tj be one such a Venn region. It can be represented as Tj = Yi n Uj where Uj \nhas a form Uj = nS.Si S(c) and S(c) denotes either S or Sc. On the other hand, Tj can also be represented \nas a disjoint union of the original Ru Venn regions. Those Ru are Venn regions that were constructed \nin the beginning of the algorithm for all input and output set variables. As the lin\u00adear integer arithmetic \nsynthesizer outputs the code that computes the values hu,where hu = |Ru|, we can effectively compute \nthe size of each Tj.If Tj = Ru1 . ... . Ruk , then the size of Tj is Pk |Tj| = dj = hul . Note that dj \nis easily computed from the l=1 linear integer arithmetic synthesizer and based on the value of dj we \nde.ne a set Kj as Kj = take(dj , Uj ). Finally, we emit the code that de.nes Yi as a .nite union of Kj \ns: Yi = .j Kj . Based of the values of dj, we can introduce further simpli.ca\u00adtions. If dj =0, none of \nelements of Uj contributes to Yi and thus Kj = \u00d8. On the other hand, if dj = |Uj |, applying a simple \nrule S = take(|S|,S) results in Kj = Uj . A special case is when Uj = nS.Si Sc. If in this case it also \nholds that dj > 0, we need to take dj elements that are not contained in any of the already known sets, \ni.e. we need to generate fresh dj elements. For this purpose we invoke the command fresh. Partitioning \na Set. We illustrate the BAPA synthesis algorithm through an example. Consider the following invocation \nof the choose function that generalizes the example in Section 2. val (setA, setB) = choose((a: Set[O], \nb: Set[O]) . (-maxDi. = a.size - b.size &#38;&#38; a.size - b.size = maxDi. &#38;&#38; a union b == bigSet \n&#38;&#38; a intersect b == empty )) This example combines integer and set variables. Given a set bigSet, \nthe goal is to divide it into two partition. The previously de.ned integer variable maxDiff speci.es \nthe maximum amount by which the sizes of the two partitions may differ. We apply the algorithm from Figure \n3 step-by-step to illustrate how it works. Af\u00adter completing Step 3, we obtain the formula  F1(xhu) \n= h100 = h110 = h010 = h001 = h111 =0 . -maxDiff = h101 - h011 . h101 - h011 = maxDiff We simplify the \nformula obtained in Step 4 using the constraints from Step 3 and obtain the formula F2(xhu) =|bigSet| \n= h101 + h011 .|bigSetc| = h000 Now we call the linear arithmetic synthesizer on the formula F1(xhu) \n. F2(xhu). The only two variables whose values we need to .nd are h101 and h011. The synthesizer .rst \neliminates the equation |bigSet| = h101 + h011: a fresh new integer variable k is introduced such that \nh101 = k and h011 = |bigSet|- k. This way there is only one output variable: k.Variable k has to be a \nsolution of the following two inequalities: |bigSet|-maxDiff = 2k . 2k =|bigSet| + maxDiff. We next check \nwhether |bigSet|- maxDiff/2l= L|bigSet| + maxDiff/2J holds. This is a precondition formula pre. Note \nthat pre is de.ned entirely in terms of the input variables and can be easily checked at run-time. The \nsynthesizer outputs the following code, which computes values for the output variables: val k = ((bigSet.size \n+ maxDi.)/2)..oor val h101 = k val h011 = bigSet.size - k val setA = take(h101, bigSet) val setB = take(h011, \nbigSet -- setA) In the code above, -- denotes the set difference operator. The synthesized code .rst \ncomputes the size k of one of the partitions, as approximately one half of the size of bigSet. It then \nselects k elements from bigSet to form setA, and selects bigSet.size -k of the remaining elements for \nsetB. 9. Implementation We have implemented our synthesis procedures as a Scala com\u00adpiler extension.1 \nWe chose Scala because it supports higher-order functions that make the concept of a choose function \nnatural, and extensible pattern matching in the form of extractors [EOW07]. Moreover, the compiler supports \nplugins that work as additional compilation phases. We used an off-the-shelf decision procedure [B08] \nto handle the compile-time checks (we could, in principle, also use our synthesis procedure for compile-time \nchecks because synthesis subsumes satis.ability checking). Our plugin supports the synthesis of integer \nvalues through the choose function constrained by linear arithmetic predicates (in\u00adcluding predicates \nin parameterized linear arithmetic), as well as the synthesis of set values constrained by predicates \nof the logic described in Section 8. Additionally, it can synthesize code for pattern-matching expressions \non integers such as the ones pre\u00adsented in Section 2. Figure 4 shows the compile times for a set of benchmarks, \nwith and without our plugin. Without the plugin, the code is of no use (the choose function, when not \nrewritten, just throws an excep\u00adtion), but the difference between the timings indicates how much time \nis spent generating the synthesized code. We also measure how much time is used for the compile-time \nchecks for satis.abil\u00adity and uniqueness. The examples SecondsToTime, FastExponenti\u00adation, SplitBalanced \nand Coordinates were presented in Section 2. ScaleWeights computes solutions to a puzzle, PrimeHeuristic \ncon\u00adtains a long pattern-matching expression where every pattern is 1 Our implementation source code \nand jar .le are available from the URL http://lara.epfl.ch/dokuwiki/comfusy scalac w/ plugin w/ checks \nSecondsToTime 3.05 3.23.25 FastExponentiation 3.13.15 3.25 ScaleWeights 3.13.43.5 PrimeHeuristic 3.13.13.1 \nSetConstraints 3.33.53.5 SplitBalanced 3.33.94.0 Coordinates 3.24.2 -- All 5.75 6.35 6.75 Figure 4. Measurement \nof compile times: without applying syn\u00adthesis (scalac), with synthesis but with no call to Z3 (w/ plu\u00adgin) \nand with both synthesis and compile-time checks activated (w/ checks). All times are in seconds. checked \nfor reachability, and SetConstraints is a variant of Split-Balanced. There is no measurement for Coordinates \nwith compile\u00adtime checks, because the formulas to check are in an undecidable fragment, as the original \nformula is in parameterized linear arith\u00admetic. We also measured the times with all benchmarks placed \nin a single .le, as an attempt to balance out the time taken by the Scala compiler to start up. Our numbers \nshow that the additional time re\u00adquired for the code synthesis is minimal. Moreover, note that the code \nwe tested contained almost exclusively calls to the synthe\u00adsizer. The increase in compilation time in \npractice would thus be lower for code that mixes standard Scala with selected choose con\u00adstruct invocations. \n10. Related Work Early work on synthesis [MW71, MW80] focused on synthesis us\u00ading expressive and undecidable \nlogics, such as .rst-order logic and logic containing the induction principle. Consequently, while it \ncan synthesize interesting programs containing recursion, it cannot pro\u00advide completeness and termination \nguarantees as synthesis based on decision procedures. Recent work on synthesis [SGF10] resolves some \nof these dif\u00ad.culties by decoupling the problem of inferring program control structure and the problem \nof synthesizing the computation along the control edges. Furthermore, the work leverages veri.cation \ntechniques that use both approximation and lattice theoretic search along with decision procedures. This \nwork is more ambitious and aims to synthesize entire algorithms. By nature, it cannot be both terminating \nand complete over the space of all programs that sat\u00adisfy an input/output speci.cation (thus the approach \nof specifying program resource bounds). In contrast, we focus on synthesis of program fragments with \nvery speci.c control structure dictated by the nature of the decidable logical fragment. Our work further \ndiffers from the past ones in 1) using decision procedures to guarantee the computation of synthesized \nfunctions whenever a synthesized function exists, 2) bounds on the running times of the synthesis algorithm \nand the synthesized code size and running time, and 3) deployment of synthesis in well-delimited pieces \nof code of a general-purpose programming language. Program sketching has demonstrated the practicality \nof program synthesis by focusing its use on particular domains [SLTB+06, SLAT+07, SLJB08]. The algorithms \nemployed in sketching are typically focused on appropriately guided search over the syntax tree of the \nsynthesized program. Search techniques have also been applied to automatically derived concurrent garbage \ncollection al\u00adgorithms [VYBR07]. In contrast, our synthesis uses the mathemat\u00adical structure of a decidable \ntheory to explore the space of all func\u00adtions that satisfy the speci.cation. This enables our approach \nto achieve completeness without putting any a priori bound on the syntax tree size. Indeed, some of the \nalgorithms we describe can generate fairly large yet ef.cient programs. We expect that our tech\u00adniques \ncould be fruitfully integrated into search-based frameworks.  Synthesis of reactive systems generates \nprograms that run for\u00adever and interact with the environment. However, known complete algorithms for \nreactive synthesis work with .nite-state systems [PR89] or timed systems [AMP95]. Such techniques have \nappli\u00adcations to control the behavior of hardware and embedded systems or concurrent programs [VYY09]. \nThese techniques usually take speci.cations in a fragment of temporal logic [PPS06] and have resulted \nin tools that can synthesize useful hardware components [JGWB07, JB06]. Our work examines non-reactive \nprograms, but supports in.nite data without any approximation, and incorporates the algorithms into a \ncompiler for a general-purpose programming language. Computing optimal bounds on the size and running \ntime of the synthesized code for Presburger Arithmetic is beyond the scope of this paper. Relevant results \nin the area of decision procedures are automata-based decision procedures [BJW05, Kla03], the bounds \non quanti.er elimination [Wei97] and results on integer program\u00adming in .xed dimensions [ES08]. Automata-based \ndecision procedures, such as those imple\u00admented in the MONA tool [KM01] could be used to synthesize ef.cient \n(even if large) code from expressive speci.cations. The work on graph types [KS93] proposes to synthesize \n.elds given by de.nitions in monadic second-order logic. Automata have also been applied to the synthesis \nof ef.cient code for pattern-matching expressions [SRR95]. Our approach can be viewed as sharing some \nof the goals of partial evaluation [JGS93]. However, we do not need to employ general-purpose partial \nevaluation techniques (which typically pro\u00advide linear speedup), because we have the knowledge of a particular \ndecision procedure. We use this knowledge to devise a synthesis al\u00adgorithm that, given formula F , generates \nthe code corresponding to the invocation of this particular decision procedure. This synthesis process \nchecks the uniqueness and the existence of the solutions, emitting appropriate warnings. Moreover, the \nsynthesized code can have reduced complexity compared to invoking the decision proce\u00addure at run time, \nespecially when the number of variables to syn\u00adthesize is bounded. 11. Conclusions We have presented \nthe general idea of turning decision procedures into synthesis procedures. We have explored in greater \ndetail how to do this transformation for theories admitting quanti.er elimina\u00adtion, in particular linear \narithmetic. Important complexity questions arise in synthesis, such as the best possible size of synthesized \ncode, time to perform synthesis, and the worst-case running time of the synthesized code over all inputs. \nWe have also illustrated that syn\u00adthesis procedures can be built even for cases for which the under\u00adlying \nparameterized satis.ability problem is undecidable (such as integer multiplication), as long as the problem \nbecomes decidable by the time the parameters are .xed. We have also transformed a BAPA decision procedure \ninto a synthesis procedure, illustrating in the process how to layer multiple synthesis procedures one \non top of the other. We believe that integer arithmetic and constraints on sets al\u00adready make our approach \ninteresting to programmers. The useful\u00adness of the proposed approach can be further supported by incor\u00adporating \nsynthesis procedures based on additional decidable con\u00adstraints. For example, more control over the desired \nsolutions for sets could be provided using decision procedures for ordered col\u00adlections that we have \nrecently identi.ed [PSK10]. In the exam\u00adple of partitioning a set, such support would allow us to specify \nthat all elements of one partition are smaller than all elements of the second partition. Another useful \nclass of data structures are algebraic data types; synthesis based on algebraic data general\u00adizes pattern \nmatching on algebraic data types with equality and inequality constraints. The starting point for such \nextensions are decision procedures for algebraic data types and their extensions [Opp78, BST07, SDK10]. \nOur approach can also be applied to im\u00adperative data structures [KS93]. This idea would bene.t from re\u00adcent \nadvances from more ef.cient decision procedures based on local theory extensions [Jac10], including [WPK09, \nMN05]. Given the range of logics for which we can obtain synthesis procedures, it is important to realize \nthat we can also combine synthesis procedures similarly to the way in which we can combine decision procedures. \nWe gave one example of such combination in this paper, by describing our BAPA synthesis procedure built \non top of a synthesis procedure for integer arithmetic. Other combination approaches are possible building \non the body of work in decision procedure combinations [GHN+04, WPK09]. We have pointed out that synthesis \ncan be viewed as a powerful programming language extension. Such an extension can be seam\u00adlessly introduced \ninto popular programming languages as a new kind of expression and a new pattern matching construct. \nIt is our hope that the availability of synthesis constructs will shift the way we think about program \ndevelopment. Program properties and as\u00adsertions can stop being part of the dreaded annotation overhead \n, but rather become a cost-effective way to build programs with the desired functionality.   Acknowledgments \nWe thank the anonymous reviewers of PLDI 2010 for useful feed\u00adback, Rastislav Bodik for useful discussions, \nBarbara Jobstmann and Nir Piterman for discussions on reactive and automata-based synthesis, Martin Odersky \nfor useful discussions on pattern match\u00ading in Scala, and Friedrich Eisenbrand for pointers to integer \nlinear programming in .xed dimensions.  References [AGT08] Saswat Anand, Patrice Godefroid, and Nikolai \nTillmann. Demand-driven compositional symbolic execution. In Tools and Al\u00adgorithms for the Construction \nand Analysis of Systems, 2008. [AMP95] Eugene Asarin, Oded Maler, and Amir Pnueli. Symbolic con\u00adtroller \nsynthesis for discrete and timed systems. In Hybrid Systems II, pages 1 20, 1995. [Ban88] Utpal K. Banerjee. \nDependence Analysis for Supercomputing. Kluwer Academic Publishers, Norwell, MA, USA, 1988. [BJW05] Bernard \nBoigelot, S\u00b4ebastien Jodogne, and Pierre Wolper. An effective decision procedure for linear arithmetic \nover the integers and reals. ACM Trans. Comput. Logic, 6(3):614 633, 2005. [BM07] Aaron R. Bradley and \nZohar Manna. The Calculus of Computa\u00adtion. Springer, 2007. [Bry86] R. E. Bryant. Graph-based algorithms \nfor boolean function manip\u00adulation. IEEE Transactions on Computers, C-35(8):677 691, Au\u00adgust 1986. [BST07] \nClark Barrett, Igor Shikanian, and Cesare Tinelli. An ab\u00adstract decision procedure for satis.ability \nin the theory of recur\u00adsive data types. Electronic Notes in Theoretical Computer Science, 174(8):23 37, \n2007. [CLRS01] Thomas H. Cormen, Charles E. Leiserson, Ronald L. Rivest, and Cliff Stein. Introduction \nto Algorithms (Second Edition).MIT Press and McGraw-Hill, 2001. [Coo72] D. C. Cooper. Theorem proving \nin arithmetic without multipli\u00adcation. In B. Meltzer and D. Michie, editors, Machine Intelligence, volume \n7, pages 91 100. Edinburgh University Press, 1972. [Dew79] Robert K. Dewar. Programming by re.nement, \nas exempli.ed by the SETL representation sublanguage. ACM TOPLAS, July 1979.  [Dij76] Edsger W. Dijkstra. \nA Discipline of Programming. Prentice-Hall, Inc., 1976. [EOW07] Burak Emir, Martin Odersky, and John \nWilliams. Matching objects with patterns. In ECOOP, 2007. [ES08] Friedrich Eisenbrand and Gennady Shmonin. \nParametric integer programming in .xed dimension. Mathematics of Operations Re\u00adsearch, 33(4):839 850, \n2008. [FH96] David Ford and George Havas. A new algorithm and re.ned bounds for extended gcd computation. \nIn ANTS, pages 145 150, 1996. [FLL+02] Cormac Flanagan, K. Rustan M. Leino, Mark Lilibridge, Greg Nelson, \nJames B. Saxe, and Raymie Stata. Extended Static Checking for Java. In PLDI, 2002. [FR79] Jeanne Ferrante \nand Charles W. Rackoff. The Computational Com\u00adplexity of Logical Theories, volume 718 of Lecture Notes \nin Mathe\u00admatics. Springer-Verlag, 1979. [FV59] S. Feferman and R. L. Vaught. The .rst order properties \nof products of algebraic systems. Fundamenta Mathematicae, 47:57 103, 1959. [GHN+04] Harald Ganzinger, \nGeorge Hagen, Robert Nieuwenhuis, Albert Oliveras, and Cesare Tinelli. DPLL(T): Fast decision procedures. \nIn CAV, pages 175 188, 2004. [GS64] S. Ginsburg and E. Spanier. Bounded algol-like languages. Trans\u00adactions \nof the American Mathematical Society, 113(2):333 368, 1964. [GS66] S. Ginsburg and E. Spanier. Semigroups, \nPresburger formulas and languages. Paci.c Journal of Mathematics, 16(2):285 296, 1966. [Jac10] Swen Jacobs. \nHierarchic Decision Procedures for Veri.cation. PhD thesis, Universit\u00a8at des Saarlandes, 2010. [JB06] \nBarbara Jobstmann and Roderick Bloem. Optimizations for LTL synthesis. In FMCAD, 2006. [JGS93] Neil D. \nJones, Carsten K. Gomard, and Peter Sestoft. Partial Evaluation and Automatic Program Generation. (freely \navailable), 1993. [JGWB07] Barbara Jobstmann, Stefan Galler, Martin Weiglhofer, and Rod\u00aderick Bloem. \nAnzu: A tool for property synthesis. In CAV, volume 4590 of LNCS, 2007. [JM94] Joxan Jaffar and Michael \nJ. Maher. Constraint logic programming: Asurvey. J. Log. Program., 19/20:503 581, 1994. [Kla03] Felix \nKlaedtke. On the automata size for presburger arithmetic. Technical Report 186, Institute of Computer \nScience at Freiburg University, 2003. [KM01] Nils Klarlund and Anders M\u00f8ller. MONA Version 1.4 User Man\u00adual. \nBRICS Notes Series NS-01-1, Department of Computer Sci\u00adence, University of Aarhus, January 2001. [KNR06] \nViktor Kuncak, Hai Huu Nguyen, and Martin Rinard. Deciding Boolean Algebra with Presburger Arithmetic. \nJ. of Automated Rea\u00adsoning, 2006. [KPSW10] Viktor Kuncak, Ruzica Piskac, Philippe Suter, and Thomas Wies. \nBuilding a calculus of data structures. In VMCAI, volume 5944 of LNCS, 2010. [KR07] Viktor Kuncak and \nMartin Rinard. Towards ef.cient satis.abil\u00adity checking for Boolean Algebra with Presburger Arithmetic. \nIn CADE-21, volume 4603 of LNCS, 2007. [KS93] Nils Klarlund and Michael I. Schwartzbach. Graph types. \nIn POPL, Charleston, SC, 1993. [KS00] James H. Kukula and Thomas R. Shiple. Building circuits from relations. \nIn CAV, 2000. [MN05] Scott McPeak and George C. Necula. Data structure speci.cations via local equality \naxioms. In CAV, pages 476 490, 2005. [Mos09] Michal Moskal. Satis.ability Modulo Software. PhD thesis, \nUniversity of Wroclaw, 2009. [B08] Leonardo de Moura and Nikolaj Bj\u00f8rner. Z3: An ef.cient SMT solver. \nIn TACAS, 2008. [MW71] Zohar Manna and Richard J. Waldinger. Toward automatic pro\u00adgram synthesis. Commun. \nACM, 14(3):151 165, 1971. [MW80] Zohar Manna and Richard Waldinger. A deductive approach to program synthesis. \nACM Trans. Program. Lang. Syst., 2(1):90 121, 1980. [Nip08] Tobias Nipkow. Linear quanti.er elimination. \nIn IJCAR, 2008. [Opp78] Derek C. Oppen. Reasoning about recursively de.ned data struc\u00adtures. In POPL, \npages 151 157, 1978. [OSV08] Martin Odersky, Lex Spoon, and Bill Venners. Programming in Scala: a comprehensive \nstep-by-step guide. Artima Press, 2008. [PK08a] Ruzica Piskac and Viktor Kuncak. Decision procedures \nfor multi\u00adsets with cardinality constraints. In VMCAI, volume 4905 of LNCS, 2008. [PK08b] Ruzica Piskac \nand Viktor Kuncak. Linear arithmetic with stars. In CAV, volume 5123 of LNCS, 2008. [PPS06] Nir Piterman, \nAmir Pnueli, and Yaniv Sa ar. Synthesis of reac\u00adtive(1) designs. In VMCAI, 2006. [PR89] Amir Pnueli and \nRoni Rosner. On the synthesis of a reactive module. In POPL, 1989. [PSK10] Ruzica Piskac, Philippe Suter, \nand Viktor Kuncak. On deci\u00adsion procedures for ordered collections. Technical Report LARA\u00adREPORT-2010-001, \nEPFL, 2010. [Pug92] William Pugh. A practical algorithm for exact array dependence analysis. Commun. \nACM, 35(8):102 114, 1992. [Sch98] Alexander Schrijver. Theory of Linear and Integer Programming. John \nWiley &#38; Sons, 1998. [SDK10] Philippe Suter, Mirco Dotta, and Viktor Kuncak. Decision proce\u00addures \nfor algebraic data types with abstractions. In POPL, 2010. [SGC07] Don Syme, Adam Granicz, and Antonio \nCisternino. Expert F#. Apress, 2007. [SGF10] Saurabh Srivastava, Sumit Gulwani, and Jeffrey S. Foster. \nFrom program veri.cation to program synthesis. In POPL, 2010. [Sha82] Micha Sharir. Some observations \nconcerning formal differenti\u00adation of set theoretic expressions. Transactions on Programming Languages \nand Systems, 4(2), April 1982. [SLAT+07] Armando Solar-Lezama, Gilad Arnold, Liviu Tancau, Rastislav \nBod\u00b4ik, Vijay A. Saraswat, and Sanjit A. Seshia. Sketching stencils. In PLDI, 2007. [SLJB08] Armando \nSolar-Lezama, Christopher Grant Jones, and Rastislav Bod\u00b4ik. Sketching concurrent data structures. In \nPLDI, 2008. [SLTB+06] Armando Solar-Lezama, Liviu Tancau, Rastislav Bod\u00b4ik, San\u00adjit A. Seshia, and Vijay \nA. Saraswat. Combinatorial sketching for .nite programs. In ASPLOS, 2006. [SRR95] R.C. Sekar, R. Ramesh, \nand I.V. Ramakrishnan. Adaptive pattern matching. SIAM Journal on Computing, 24:1207 1234, December 1995. \n[VYBR07] Martin T. Vechev, Eran Yahav, David F. Bacon, and Noam Rinetzky. Cgcexplorer: a semi-automated \nsearch procedure for prov\u00adably correct concurrent collectors. In PLDI, pages 456 467, 2007. [VYY09] Martin \nT. Vechev, Eran Yahav, and Greta Yorsh. Inferring syn\u00adchronization under limited observability. In TACAS, \n2009. [Wei97] Volker Weispfenning. Complexity and uniformity of elimination in presburger arithmetic. \nIn Proc. International Symposium on Sym\u00adbolic and Algebraic Computation, pages 48 53, 1997. [WPK09] Thomas \nWies, Ruzica Piskac, and Viktor Kuncak. Combining theories with shared set operations. In FroCoS: Frontiers \nin Com\u00adbining Systems, 2009. [YPK10] Kuat Yessenov, Ruzica Piskac, and Viktor Kuncak. Collections, cardinalities, \nand relations. In VMCAI, volume 5944 of LNCS, 2010. [Zar04] Calogero G. Zarba. A quanti.er elimination \nalgorithm for a frag\u00adment of set theory involving the cardinality operator. In 18th Inter\u00adnational Workshop \non Uni.cation, 2004. [Zar05] Calogero G. Zarba. Combining sets with cardinals. J. of Automated Reasoning, \n34(1), 2005. [ZKR08] Karen Zee, Viktor Kuncak, and Martin Rinard. Full functional veri.cation of linked \ndata structures. In PLDI, 2008.  \n\t\t\t", "proc_id": "1806596", "abstract": "<p>Synthesis of program fragments from specifications can make programs easier to write and easier to reason about. To integrate synthesis into programming languages, synthesis algorithms should behave in a predictable way - they should succeed for a well-defined class of specifications. They should also support unbounded data types such as numbers and data structures. We propose to generalize decision procedures into predictable and complete synthesis procedures. Such procedures are guaranteed to find code that satisfies the specification if such code exists. Moreover, we identify conditions under which synthesis will statically decide whether the solution is guaranteed to exist, and whether it is unique. We demonstrate our approach by starting from decision procedures for linear arithmetic and data structures and transforming them into synthesis procedures. We establish results on the size and the efficiency of the synthesized code. We show that such procedures are useful as a language extension with implicit value definitions, and we show how to extend a compiler to support such definitions. Our constructs provide the benefits of synthesis to programmers, without requiring them to learn new concepts or give up a deterministic execution model.</p>", "authors": [{"name": "Viktor Kuncak", "author_profile_id": "81100277693", "affiliation": "Swiss Federal Institute of Technology, Lausanne, Switzerland", "person_id": "P2184571", "email_address": "", "orcid_id": ""}, {"name": "Mika&#235;l Mayer", "author_profile_id": "81464657813", "affiliation": "Swiss Federal Institute of Technology, Lausanne, Switzerland", "person_id": "P2184572", "email_address": "", "orcid_id": ""}, {"name": "Ruzica Piskac", "author_profile_id": "81384606465", "affiliation": "Swiss Federal Institute of Technology, Lausanne, Switzerland", "person_id": "P2184573", "email_address": "", "orcid_id": ""}, {"name": "Philippe Suter", "author_profile_id": "81453631112", "affiliation": "Swiss Federal Institute of Technology, Lausanne, Switzerland", "person_id": "P2184574", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1806596.1806632", "year": "2010", "article_id": "1806632", "conference": "PLDI", "title": "Complete functional synthesis", "url": "http://dl.acm.org/citation.cfm?id=1806632"}