{"article_publication_date": "06-05-2010", "fulltext": "\n Resolving and Exploiting the k-CFA Paradox Illuminating Functional vs. Object-Oriented Program Analysis \nMatthew Might Yannis Smaragdakis David Van Horn University of Utah University of Massachusetts Northeastern \nUniversity might@cs.utah.edu yannis@cs.umass.edu dvanhorn@ccs.neu.edu Abstract Low-level program analysis \nis a fundamental problem, taking the shape of .ow analysis in functional languages and points-to analysis \nin imperative and object-oriented languages. Despite the similarities, the vocabulary and results in \nthe two communities remain largely distinct, with limited cross-understanding. One of the few links is \nShivers s k-CFA work, which has advanced the concept of context-sensitive analysis and is widely known \nin both communities. Recent results indicate that the relationship between the func\u00adtional and object-oriented \nincarnations of k-CFA is not as well understood as thought. Van Horn and Mairson proved k-CFA for k = \n1 to be EXPTIME-complete; hence, no polynomial-time al\u00adgorithm can exist. Yet, there are several polynomial-time \nformula\u00adtions of context-sensitive points-to analyses in object-oriented lan\u00adguages. Thus, it seems that \nfunctional k-CFA may actually be a pro\u00adfoundly different analysis from object-oriented k-CFA. We resolve \nthis paradox by showing that the exact same speci.cation of k-CFA is polynomial-time for object-oriented \nlanguages yet exponential\u00adtime for functional ones: objects and closures are subtly different, in a way \nthat interacts crucially with context-sensitivity and com\u00adplexity. This illumination leads to an immediate \npayoff: by pro\u00adjecting the object-oriented treatment of objects onto closures, we derive a polynomial-time \nhierarchy of context-sensitive CFAs for functional programs. Categories and Subject Descriptors F.3.2 \n[Logics and Meanings of Programs]: Semantics of Programming Languages Program Analysis General Terms \nAlgorithms, Languages, Theory Keywords static analysis, control-.ow analysis, pointer analysis, functional, \nobject-oriented, k-CFA, m-CFA 1. Introduction One of the most fundamental problems in program analysis \nis determining the entities to which an expression may refer at run\u00adtime. In imperative and object-oriented \n(OO) languages, this is commonly phrased as a points-to (or pointer) analysis: to which objects can a \nvariable point? In functional languages, the problem is called .ow analysis [11]: to which expressions \ncan a value .ow? Permission to make digital or hard copies of all or part of this work for personal or \nclassroom use is granted without fee provided that copies are not made or distributed for pro.t or commercial \nadvantage and that copies bear this notice and the full citation on the .rst page. To copy otherwise, \nto republish, to post on servers or to redistribute to lists, requires prior speci.c permission and/or \na fee. Both points-to and .ow analysis acquire a degree of complexity for higher-order languages: functional \nlanguages have .rst-class functions and object-oriented languages have dynamic dispatch; these features \nconspire to make call-target resolution depend on the .ow of values, even as the .ow of values depends \non what targets are possible for a call. That is, data-.ow depends on control-.ow, yet control-.ow depends \non data-.ow. Appropriately, this problem is commonly called control-.ow analysis (CFA). Shivers s k-CFA \n[17] is a well-known family of control-.ow analysis algorithms, widely recognized in both the functional \nand the object-oriented world. k-CFA popularized the idea of context\u00adsensitive .ow analysis.1 Nevertheless, \nthere have always been an\u00adnoying discrepancies between the experiences in the application of k-CFA in \nthe functional and the OO world. Shivers himself notes in his Best of PLDI retrospective that the basic \nanalysis, for any k> 0 [is] intractably slow for large programs [16]. This contra\u00addicts common experience \nin the OO setting, where a 1-and 2-CFA analysis is considered heavy but certainly possible [2, 10]. To \nmake matters formally worse, Van Horn and Mairson [19] recently proved k-CFA for k = 1 to be EXPTIME-complete, \ni.e., non-polynomial. Yet the OO formulations of k-CFA have provably polynomial complexity (e.g., Bravenboer \nand Smaragdakis [2] ex\u00adpress the algorithm in Datalog, which is a language that can only express polynomial-time \nalgorithms). This paradox seems hard to resolve. Is k-CFA misunderstood? Has inaccuracy crept into the \ntransition from functional to OO? In this paper we resolve the paradox and illuminate the deep differences \nbetween functional and OO context-sensitive program analyses. We show that the exact same formulation \nof k-CFA is exponential-time for functional programs yet polynomial-time for OO programs. To ensure .delity, \nour proof appeals directly to Shivers s original de.nition of k-CFA and applies it to the most common \nformal model of Java, Featherweight Java. As might be expected, our .nding hinges on the fundamental \ndifference between typical functional and OO languages: the for\u00admer create implicit closures when lambda \nexpressions are created, while the latter require the programmer to explicitly close (i.e., pass to a \nconstructor) the data that a newly created object can ref\u00aderence. At an intuitive level, this difference \nalso explains why the 1 Although the k-CFA work is often used as a synonym for k-context\u00adsensitive in \nthe OO world, k-CFA is more correctly an algorithm that packages context-sensitivity together with several \nother design decisions. In the terminology of OO points-to analysis, k-CFA is a k-call-site-sensitive, \n.eld-sensitive points-to analysis algorithm with a context-sensitive heap and with on-the-.y call-graph \nconstruction. (Lhot\u00b4ak [9] and Lhot\u00b4ak and Hendren [10] are good references for the classi.cation of \npoints-to analysis algorithms.) In this paper we use the term k-CFA with this more precise meaning, as \nis common in the functional programming world, and not just PLDI 10, June 5 10, 2010, Toronto, Ontario, \nCanada. as a synonym for k-context-sensitive . Although this classi.cation is more Copyright cprecise, \nit still allows for a range of algorithms, as we discuss later. &#38;#169; 2010 ACM 978-1-4503-0019-3/10/06. \n. . $10.00  exact same k-CFA analysis will not yield the same results if a func\u00adtional program is automatically \nrewritten into an OO program: the call-site context-sensitivity of the analysis leads to loss of preci\u00adsion \nwhen the values are explicitly copied the analysis merges the information for all paths with the same \nk-calling-context into the same entry for the copied data. Beyond its conceptual signi.cance, our .nding \npays immediate dividends: By emulating the behavior of OO k-CFA, we derive a hierarchy, m-CFA, of polynomial \nCFA analyses for functional pro\u00adgrams. In technical terms, k-CFA corresponds to an abstract inter\u00adpretation \nover shared-environment closures, while m-CFA corre\u00adsponds to an abstract interpretation over .at-environment \nclosures. m-CFA turns out to be an important instantiation in the space of analyses described by Jagannathan \nand Weeks [8]. 2. Background and Illustration Although we prove our claims formally in later sections, \nwe .rst illustrate the behavior of k-CFA for OO and functional programs informally, so that the reader \nhas an intuitive understanding of the essence of our argument. 2.1 Background: What is CFA? k-CFA was \ndeveloped to solve the higher-order control-.ow prob\u00adlem in .-calculus-based programming languages. Functional \nlan\u00adguages are explicitly vulnerable to the higher-order control-.ow problem, because closures are passed \naround as .rst-class values. Object-oriented languages like Java are implicitly higher-order, because \nmethod invocation is resolved dynamically the invoked method depends on the type of the object that makes \nit to the invo\u00adcation point. In practice, CFAs must compute much more than just control\u00ad.ow information. \nCFAs are also data-.ow analyses, computing the values that .ow to any program expression. In the object\u00adoriented \nsetting, CFA is usually termed a points-to analysis and the interplay between control-and data-.ow is \ncalled on-the-.y call-graph construction [9]. Both the functional community and the pointer-analysis \ncom\u00admunity have assigned a meaning to the term k-CFA. Informally, k-CFA refers to a hierarchy of global \nstatic analyses whose context\u00adsensitivity is a function of the last k call sites visited. In its func\u00adtional \nformulation, k-CFA uses this context-sensitivity for every value and variable thus, in pointer analysis \nterms, k-CFA is a k\u00adcall-site-sensitive analysis with a k-context-sensitive heap.  2.2 Insight and Example \nThe paradox prompted by the Van Horn and Mairson proofs seems to imply that k-CFA actually refers to \ntwo different analyses: one for functional programs, and one for object-oriented/imperative programs. \nThe surprising .nding of our work is that k-CFA means the same thing for both programming paradigms, \nbut that its behav\u00adior is different for the object-oriented case. k-CFA was de.ned by abstract interpretation \nof the .-calculus semantics for an abstract domain collapsing data values to static abstractions quali.ed \nby k calling contexts. Functional implemen\u00adtations of the algorithm are often heavily in.uenced by this \nabstract interpretation approach. The essence of the exponential complexity of k-CFA (for k = 1) is that, \nalthough each variable can appear with at most O(n k) calling contexts, the number of variable envi\u00adronments \nis exponential, because an environment can combine vari\u00adables from distinct calling contexts. Consider \nthe following term: (. (z)(zx1 ...xn)) . This expression has n free variables. In 1-CFA, each variable \nis mapped to the call-site in which it was bound. By binding each of the xi in multiple call-sites, we \ncan induce an exponential number of environments to close this .-term: ((. (f1)(f1 0)(f1 1)) (. (x1) \n\u00b7\u00b7\u00b7 ((. (fn)(fn 0)(fn 1)) (. (xn) (. (z)(zx1 ...xn)))) \u00b7\u00b7\u00b7 )) . Notice that each xi is bound to 0 and \n1, thus there are 2n environ\u00adments closing the inner .-term. The same behavior is not possible in the \nobject-oriented setting because creating closures has to be explicit (a fundamental differ\u00adence of the \ntwo paradigms2) and the site of closure creation be\u00adcomes the common calling context for all closed variables. \nFigures 1 and 2 demonstrate this behavior for a 1-CFA analy\u00adsis. (This is the shortest, in terms of calling \ndepth, example that can demonstrate the difference.) Figure 1 presents the program in OO form, with explicit \nclosures i.e., objects that are initialized to capture the variables that need to be used later. Figure \n2 shows the same program in functional form. We use a .ctional (for Java) construct lambda that creates \na closure out of the current environ\u00adment. The bottom parts of both .gures show the information that \nthe analysis computes. (We have grouped the information in a way that is more re.ective of OO k-CFA implementations, \nbut this is just a matter of presentation.) The essential question is in how many environments does func\u00adtion \nbaz get analyzed? The exact same, abstract-interpretation\u00adbased, 1-CFA algorithm produces O(N + M) environments \nfor the object-oriented program and O(NM) environments for the functional program. The reason has to \ndo with how the context\u00adsensitivity of the analysis interacts with the explicit closure. Since closures \nare explicit in the OO program, all (heap-)accessible vari\u00adables were closed simultaneously. One can \nsee this in terms of vari\u00adables x and y: both are closed by copying their values to the x and y .elds \nof an object in the expression new ClosureXY(x,y) . This copying collapses all the different values for \nx that have the same 1-call-site context. Put differently, x and y inside the OO version of baz are not \nthe original variables but, rather, copies of them. The act of copying, however, results in less precision \nbecause of the .\u00adnite context-sensitivity of the analysis. In contrast, the functional program makes \nimplicit closures in which the values of x and y are closed at different times and maintain their original \ncontext. The abstract interpretation results in computing all O(NM) combina\u00adtions of environments with \ndifferent contexts for x and y. (If the example is extended to more levels, the number of environments \nbecomes exponential in the length of the program.) The above observations immediately bring to mind a \nwell\u00adknown result in the compilation of functional languages: the choice between shared environments \nand .at environments [1, page 142]. In a .at environment, the values of all free variables are copied \ninto a new environment upon allocation. In a .at-environment scenario, it is suf.cient to know only the \nbase address of an environment to look up the value of a variable. To de.ne the meaning of a program, \nit clearly makes no difference which environment representation a formal semantics models. However, in \ncompilation there are trade\u00adoffs: shared environments make closure-creation fast and variable look-up \nslow, while .at environments make closure-creation slow and variable look-up fast. The choice of environment \nrepresentation also makes a profound difference during abstract interpretation. 2 It is, of course, impossible \nto strictly classify languages by paradigm ( what is JavaScript? ) so our statements re.ect typical, \nrather than uni\u00adversal, practice.  Figure 1. An example OO program, analyzed under 1-CFA. Parts that \nare orthogonal to the analysis (e.g., return types, the class containing foo, the body of baz) are elided. \nThe bottom part shows the (points-to) results of the analysis in the form context: var -> abstractObject \n. Conventions: we use [ox1], ..., [oxN], [oy1], ..., [oyM] to mean the abstract objects pointed to by \nthe corresponding environment variables. (We only care that these objects be distinct.) method var names \na local variable, var inside a method. method::Type..eld refers to a .eld of the object of type Type \nallocated inside method. (This example allocates a single object per method, so no numeric distinction \nof allocation Figure 2. The same program in functional form (implicit closures). The lambda expressions \nare drawn outside their lexical environment to illustrate the analogy with the OO code. The number of \nenvironments out of the abstract interpretation is now O(NM) because variables x and y in the rightmost \nlambda were not closed together and have different contexts. \u00a3 lam . Lam ::= (. (v1 ...vn) call)v . \nVar \u00a3 call . Call ::= (fe1 ...en)f, e . Exp = Var + Lam J . Lab is a set of labels Figure 3. Grammar \nfor CPS 3. Shivers s original k-CFA Because one possible resolution to the paradox is that k-CFA for \nobject-oriented programs and k-CFA for the .-calculus is just a case of using the same name for two different \nconcepts, we need to be con.dent that the analysis we are working with is really k-CFA. To achieve that \ncon.dence, we return to the source of k-CFA Shivers s dissertation [17], which formally and precisely \npins down its meaning. We take only cosmetic liberties in reformulating Shiv\u00aders s k-CFA we convert from \na tail-recursive denotational seman\u00adtics to a small-step operational semantics, and we rename contours \nto times. Though equivalent, Shivers s original formulation of k-CFA differs signi.cantly from later \nones; readers familiar with only modern CFA theory may even .nd it unusual. Once we have refor\u00admulated \nk-CFA, our goal will be to adapt it as literally as possible to Featherweight Java. 3.1 A grammar for \nCPS A minimal grammar for CPS (Figure 3) contains two expression forms .-terms and variables and one \ncall form. The body of every .-term is a call site, which ensures the CPS constraint that functions cannot \ndirectly return to their callers. We also attach a unique label to every .-term and call site. 3.2 Concrete \nsemantics for CPS We model the semantics for CPS as a small-step state machine. Each state in this machine \ncontains the current call site, a binding environment in which to evaluate that call, a store and a time-stamp: \n. . S= Call \u00d7 BEnv \u00d7 Store \u00d7 Time \u00df . BEnv = Var . Addr s . Store = Addr . D d . D = Clo clo . Clo = \nLam \u00d7 BEnv a . Addr is an in.nite set of addresses t . Time is an in.nite set of time-stamps. Environments \nin this state-space are factored; instead of mapping a variable directly to a value, a binding environment \nmaps a vari\u00adable to an address, and then the store maps addresses to values. The speci.c structure of \nboth time-stamps and addresses will be determined later. Any in.nite set will work for either addresses \nor time-stamps for the purpose of de.ning the meaning of the concrete semantics. (Speci.c choices for \nthese sets can simplify proofs of soundness, which is why they are left un.xed for the moment.) To inject \na call site call into an initial state, we pair it with an empty environment, an empty store and a distinguished \ninitial time: .0 =(call, [], [],t0). The concrete semantics are composed of an evaluator for ex\u00adpressions \nand a transition relation on states: E : Exp \u00d7 BEnv \u00d7 Store . D (.) . S \u00d7 S. The evaluator looks up variables, \nand creates closures over .-terms: E(v, \u00df, s)= s(\u00df(v)) E(lam, \u00df, s)=(lam,\u00df). In CPS, there is only one \nrule to transition from one state to another; \u00a3 when call =[ (fe1 ...en)] : ' (call, \u00df, s, t) . (call',\u00df'',s',t), \nwhere (lam,\u00df')= E(f, \u00df, s) di = E(ei, \u00df, s) \u00a3' ' lam =[ (. (v1 ...vn) call')] t= tick(call,t) ') \u00df'' \nai = alloc(vi,t= \u00df'[vi . ai] s' = s[ai . di]. There are two external parameters to this semantics, a \nfunction for incrementing the current time-stamp and a function for allocating fresh addresses for bindings: \ntick : Call \u00d7 Time . Time alloc : Var \u00d7 Time . Addr It is possible to de.ne a semantics in which the \ntick function does not have access to the current call site, but providing access to the call site will \nend up simplifying the proof of soundness for k-CFA. Naturally, we expect that new time-stamps and addresses \nare always unique; formally: t < tick(call,t). (1) ' If v ', then alloc(v, t)= alloc(v,t). (2) = v ' \nIf t ', then alloc(v, t)= alloc(v,t'). (3) = t For the sake of understanding the concrete semantics, \nthe obvious solution to these constraints is to use the natural numbers for time: Time = N Addr = Var \n\u00d7 Time, so that the tick function merely has to increment: tick( ,t)= t +1 alloc(v, t)=(v, t). 3.3 Executing \nthe concrete semantics The concrete semantics .nds the set of states reachable from the initial state. \nThe system-space for this process is a set of states: . . .= P (S). The system-space exploration function \nis f :. . ., which maps a set of states to their successors plus the initial state: .\u00af f(.)= .' : . . \n. and . . .' .{.0} , Because the function is monotonic, there exists a .xed point 8 G S = fn(\u00d8), n=0 \nwhich is the (possibly in.nite) set of reachable states.  3.4 Abstract semantics for CPS: k-CFA The \ndevelopment of the abstract semantics parallels the construc\u00adtion of the concrete semantics. The abstract \nstate-space is struc\u00adturally similar to the concrete semantics: S= Call \u00d7 rStore \u00d7 r . . BEnv \u00d7 ATime \n\u00df . rAddrBEnv = Var . A s . A= Addr . D Store A d d . D = P Clo c clo . dBEnvClo = Lam \u00d7 r a . A Addr \nis a .nite set of addresses t . r Time is a .nite set of time-stamps. There are three major distinctions \nwith the concrete state-space: (1) the set of time-stamps is .nite; (2) the set of addresses is .nite; \nand (3) the store can return a set of values. We assume the natural partial order (.) on this state-space \nand its components, along with the associated meaning for least-upper bound (U). For example: s u s ' \na.( a) . s '( a)). = . s(  A state-wise abstraction map a :S . S formally relates the concrete state-space \nto the abstract state-space: a(call, \u00df, s, t)=(call,a(\u00df),a(s),a(t)) a(\u00df)= .v.a(\u00df(v)) G a(s)= . a(s(a)) \na. a(a)= a a(lam,\u00df)= {(lam,a(\u00df))} a(a) is .xed by allocAa(t) is .xed by d tick. We cannot choose an abstraction \nfor addresses and time-stamps until we have chosen the sets Time, Time, Addr and M TAddr. The initial \nabstract state for a program call is the direct abstrac\u00adtion of the initial concrete state: . 0 = a(.0)=(call, \n., .,a(t0)). The abstract semantics has an expression evaluator: E : Exp \u00d7 rStore . BEnv \u00d7 AD E(v, \u00df, \ns ) = s (\u00df (v)) E(lam, \u00df, s ) = {(lam,\u00df)}. The abstract transition relation (r) . S \u00d7 S mimics its concrete \ncounterpart as well; when call =[ (fe1 ...en)\u00a3] : ' '' '' (call, \u00df, t) r (call ,\u00df s,t ), wheres, , (lam,\u00df \n' ) . \u00df, di = E(ei, \u00df, s ) E(f, s) '' )\u00a3 ' d lam =[ (. (v1 ...vn) call ] t = tick(call,t) A ' ) \u00df '' \na i = alloc(vi,t = \u00df ' [vi . a i] s ' = s u [ ai . d i]. Notable differences are the fact that this rule \nis non-deterministic (it branches to every abstract closure to which the function f eval\u00aduates), and \nthat every abstract address could represent several con\u00adcrete addresses, which means that additions to \nthe store must be performed with a join operation (U) rather than an extension. There are also external \nparameters for the abstract semantics correspond\u00ading to the external parameters of the concrete semantics: \nd tick : Call \u00d7 rTime Time . r ATime . A alloc : Var \u00d7 rAddr The d tick function allocates an abstract \ntime, which is allowed to be an abstract time which has been allocated previously; the allo\u00adcator M alloc \nis similarly allowed to re-allocate previously-allocated addresses.  3.5 Constraints from soundness \nThe standard soundness theorem requires that the abstract seman\u00adtics simulate the concrete semantics; \nthe key inductive step shows simulation across a single transition: Theorem 3.1. If . . . ' and a(.) \n. , then there must exist an ' ''' abstract state . such that: . . . and a(. ) . . The proof reduces \nto two lemmas which must be proved for every choice of the sets Time, Time, Addr and M TAddr: d Lemma \n3.2. If a(t) t , then a(tick(call,t)) tick(call,t ). M Lemma 3.3. If a(t) t, then a(alloc(v, t)) alloc(v, \nt ). 3.5.1 The k-CFA solution k-CFA represents one solution to the Simulation Lemmas 3.2 and 3.3. In \nk-CFA, a concrete time-stamp is the sequence of call sites traversed since the start of the program; \nan abstract time-stamp is the last k call sites. An address is a variable plus its binding time: r Time \n= Call * Time = Callk A Addr = Var \u00d7 Time = Var \u00d7 r Addr Time. In theory, k-CFA is able to distinguish \nup to |Call|k instances (variants) of each variable one for each invocation context. Of course, in practice, \neach variable tends to be bound in only a small fraction of all possible invocation contexts. Under this \nallocation regime, the external parameters are easily .xed: d tick(call,t)= call : t tick(call,t)= .rstk(call \n: t ) A alloc(v, t)=(v, t) alloc(v, t ) = (v, t ), which leaves only one possible choice for the abstraction \nmaps: a(t)= .rstk(t) a(v, t)=(v, a(t)). In technical terms, d tick determines the context-sensitivity \nof the analysis, and M alloc determines its polyvariance.  3.6 Computing k-CFA na\u00a8ively k-CFA can be \ncomputed na\u00a8ively by .nding the set of reachable states. The system-space for this approach is a set \nof states:  . . . = P S . The transfer function for this system-space is f : . . . : ' f (. ) = {. : \n. . . and . . . ' }.{. 0} . The size of the state-space bounds the complexity of na\u00a8ive k-CFA:3 |M Store||BEnvT| \nz }| { |TTime| z }| { z}|{ |Var|\u00d7|Call|k |Call|\u00d7 |Call|k\u00d7|Var| \u00d7 2|Lam|\u00d7|Call|k\u00d7|Var| \u00d7|Call|k Even for \nk =0, this method is deeply exponential, rather than the expected cubic time more commonly associated \nwith 0CFA.  3.7 Computing k-CFA with a single-threaded store Shivers s technique for making k-CFA more \nef.cient uses one store to represent all stores. Any set of stores may be conservatively ap\u00adproximated \nby their least-upper-bound. Under this approximation, the system-space needs only one store: \u00d7 A .= \nP Call \u00d7 rTime BEnv \u00d7 rStore. Over this system-space, the transfer function becomes: ' f ( s)=( Cs ' \n) C, C . , no '' S ' = . : c . C and ( c, s ) . . no C ' = c : ( c, s ) . S ' G s ' = s . ( c,s ).S ' \n[This formulation of the transfer function assumes that the store grows monotonically across s, r transition, \ni.e., that (..., t) (..., s ' ,t ' ) implies s s '.] To compute the complexity of this analysis, note \nthe isomor\u00adphism in the system-space: . ~rAd = Call .P Time \u00d7 Addr .P Clo , BEnv \u00d7 r Because the function \nf is monotonic, the height of the lattice . : |BEnvT||T Time| z }| { z}|{ |Call|\u00d7 |Call|k\u00d7|Var| \u00d7|Call|k \nClo| |AddrM||d z}|{z }| { + |Var|\u00d7|Call|k \u00d7|Lam|\u00d7|Call|k\u00d7|Var| , 3 Because allocA(v, t)=(v, t), we could \nencode every binding environ\u00adment with a map from variables to just times, so that, effectively, |r BEnv| \n= |Var Time| = |r= |Call|k\u00d7|Var| r Time||Var| .  . . S= Stmt \u00d7 BEnv \u00d7 Store \u00d7 KontPtr \u00d7 Time \u00df . BEnv \n= Var Addr s . Store = Addr D d . D = Val val . Val = Obj + Kont o . Obj = ClassName \u00d7 BEnv . . Kont \n= Var \u00d7 Stmt \u00d7 BEnv \u00d7 KontPtr a . Addr is a set of addresses . p . KontPtr . Addr t . Time is a set \nof time-stamps. Figure 4. Concrete state-space for A-Normal Featherweight Java. bounds the maximum number \nof times we may have to apply the abstract transfer function. For k =0, the height of the lattice is \nquadratic in the size of the program (with the cost of applying the transfer function linear in the size \nof the program). For k = 1, however, the algorithm has a genuinely exponential system-space. 4. Shivers \ns k-CFA for Java Having formulated a small-step k-CFA for CPS, it is straight\u00adforward to formulate a \nsmall-step, abstract interpretive k-CFA for Java. To simplify the presentation, we utilize Featherweight \nJava [7] in A-Normal form. A-Normal Featherweight Java is identical to ordinary Featherweight Java, except \nthat arguments to a function call must be atomically evaluable, as they are in A-Normal Form .-calculus. \nFor example, the body return f.foo(b.bar()); becomes the sequence of statements B b1= b.bar(); F f1 = \nf.foo(b1); return f1;. This shift does not change the expressive power of the language or the nature \nof the analysis, but it does simplify the semantics by eliminating semantic expression contexts. The \nfollowing grammar describes A-Normal Featherweight Java; note the (re-)introduction of statements: ----. \n- . ' '' Class ::= class C extends C {Cf ; KM} . ---------- --.--. ' '' ''' K . Konst ::= C (Cf ){super(f \n) ; this.f = f ;} --. ---. M . Method ::= Cm (Cv ) { Cv ; Ss } \u00a3 s . Stmt ::= v = e ;\u00a3 | return v ; -. \n- . e . Exp ::= v | v.f | v.m(v ) | new C (v ) | (C)v f . FieldName = Var C . ClassName is a set of class \nnames m . MethodCall is a set of method invocation sites J . Lab is a set of labels The set Var contains \nboth variable and .eld names. Every statement has a label. The function succ : Lab -Stmt yields the subsequent \nstatement for a statement s label. 4.1 Concrete semantics for Featherweight Java Figure 4 contains the \nconcrete state-space for the small-step Feath\u00aderweight Java machine, and Figure 6 contains the concrete \nseman\u00adtics.4 The state-space closely resembles the concrete state-space for CPS. One difference is the \nneed to explicitly allocate continuations (from the set Kont) at a semantic level. These same continuations \nexist in CPS, but they re hidden in plain sight the CPS transform converts semantic continuations into \nsyntactic continuations. 4 Note that the (+) operation represents right-biased functional union, and \nthat wherever a vector Sx is in scope, its components are implicitly in scope: Sx = (x0,...,xlength( \nx) . C : ClassName . (FieldName * \u00d7 Ructor) .elds arguments.eld values record z}|{ z}|{z}|{z}|{ K. Ructor \n= Addr * \u00d7 D * . ( Store \u00d7 BEnv) M : D \u00d7 MethodCall Method Figure 5. Helper functions for the concrete \nsemantics. It is important to note the encoding of objects: objects are a class plus a record of their \n.elds, and the record component is encoded as a binding environment that maps .eld names to their addresses. \nThis encoding is congruent to k-CFA s encoding of closures, but it is probably not the way one would \nencode the record component of an object if starting from scratch. The natural encoding would reduce \nan object to a class plus a single base address, i.e., Obj = ClassName \u00d7 Addr, since .elds are accessible \nas offsets from the base address. Then, given an object (C, a), the address of .eld f would be (f, a). \nIn fact, under our semantics, given an object (C, \u00df), it is effectively the case that \u00df(f)=(f, a). We \nare choosing the functional representation of records to maintain the closest possible correspondence \nwith CPS. When investigating the complexity of k-CFA for Java, we will exploit this observation: the \nfact that objects can be represented with just a base address causes the collapse in complexity. The \nconcrete semantics are encoded as a small-step transition relation (.) . S \u00d7 S. Each expression type \ngets a transition rule. Object allocation creates a new binding environment \u00df ', which shares no structure \nwith the previous environment \u00df; contrast this with CPS. These rules use the helper functions described \nin Fig\u00adure 5. The constructor-lookup function C yields the .eld names and the constructor associated \nwith a class name. A constructor K takes newly allocated addresses to use for .elds and a vector of argu\u00adments; \nit returns the change to the store plus the record component of the object that results from running \nthe constructor. The method\u00adlookup function M takes a method invocation point and an object to determine \nwhich method is actually being called at that point. 4.2 Abstract semantics: k-CFA for Featherweight \nJava Figure 7 contains the abstract state-space for the small-step Feath\u00aderweight Java machine, i.e., \nOO k-CFA. As was the case for CPS, the abstract semantics closely mirror the concrete semantics. We assume \nthe natural partial order for the components of the abstract state-space. The abstract semantics are \nencoded as a small-step transition relation (r) . S \u00d7 S , shown in Figure 9. There is one abstract transition \nrule for each expression type, plus an additional transition rule to account for return. These rules \nmake use of the helper functions described in Figure 8. The constructor-lookup function C yields the \n.eld names and the abstract constructor associated with a class name. An abstract constructor K takes \nabstract addresses to use for .elds and a vector of arguments; it returns the change to the store plus \nthe record component of the object that results from running the constructor. The abstract method-lookup \nfunction M takes a method invocation point and an object to determine which methods could be called at \nthat point.  4.3 The k-CFA solution As in the original k-CFA for CPS, we factored out time-stamp and \naddress allocation functions and even the structure of time-stamps and addresses. The equivalent to call \nsites in Java are statements. So, a concrete time-stamp is the sequence of labels traversed since the \nprogram began execution. Addresses pair either a variable/.eld name or a method with a time. Method names \nare allowed, so that continuations can have a binding point for each method at each  Variable reference \n' \u00a3 ' ([[v = v ; ] , \u00df, s, p . ,t) . (succ(J), \u00df, s ' ,p . ,t ), where ' t = tick(J, t) s ' = s[\u00df(v) \n. s(\u00df(v ' ))]. Return .. ' \u00a3 '' ' ([[return v ; ] ,\u00df,s,p ,t) . (s,\u00df ,s ,p ,t ), where . ' . ' t = tick(J, \nt)(v ' , s, \u00df ' ,p )= s(p ) d = s(\u00df(v)) s ' = s[\u00df ' (v ' ) . d]. Field reference ' \u00a3 '' ([[v = v .f ; \n] , \u00df, s, p . ,t) . (succ(J),\u00df,s ,p . ,t ), where ' ''' ' t = tick(J,t)(C, \u00df )= s(\u00df(v )) s = s[\u00df(v) . \ns(\u00df (f))]. Method invocation . -.. ' ' \u00a3 ''' ' ([[v = v0.m( v );] ,\u00df,s,p ,t) . (s0,\u00df ,s ,p ,t ), where \n---. - ---. '' '''' M =[ Cm (Cv ) {Cv ; ss}] = M(d0,m) ' d0 = s(\u00df(v0)) di = s(\u00df(vi)) . ' t = tick(J, \nt) . =(v, succ(J), \u00df, p ) ' '' p . ' = alloc.(M, t ' ) a = alloc(vi ,t ' ) i '' ''' aj = alloc(vj ,t \n' ) \u00df ' = [[[this] . \u00df(v0)] \u00df '' '' '''' '' s ' = \u00df ' [vi . ai,v . aj ]= s[p . ' . ., a ' i . di]. j \nObject allocation -. .. ' \u00a3 '' ([[v = new C ( v );] ,\u00df,s,p ,t) . (succ(J),\u00df,s ,p ,t ), where '' t = tick(J, \nt) di = s(\u00df(vi)) (f, SK)= C(C) ai = alloc(fi,t ' ) (.s, \u00df ' )= K(Sa, dS) d ' =(C, \u00df ' ) s ' = s +.s +[\u00df(v) \n. d ' ]. Casting '' '' ([[v = (C ) v ] , \u00df, s, p . ,t) . (succ(J),\u00df,s ,p . ,t ), where ' t = tick(J, \nt) s ' = s[\u00df(v) . s(\u00df(v ' ))]. Figure 6. Concrete semantics for A-Normal Featherweight Java. r A . . \nBEnv \u00d7 AKontPtr \u00d7 r S= Stmt \u00d7 rStore \u00d7 Time BEnv \u00df . r= Var Addr s . AA Store = Addr . Db d d . D \n= P Val cd val . d= Kont Val Obj + A o . d= ClassName \u00d7 r Obj BEnv Kont BEnv \u00d7 . . Ar = Var \u00d7 Stmt \u00d7 \nrKontPtr a . A Addr is a .nite set of addresses . p . KontPtrr. AddrA t . r Time is a .nite set of time-stamps. \nFigure 7. Abstract state-space for A-Normal Featherweight Java. r C : ClassName . (FieldName * \u00d7 Ructor) \n* rA\u00d7 D * . ( A K. Ructor = Addr BEnv) Store \u00d7 r M : Db\u00d7 MethodCall .P (Method) Figure 8. Helper functions \nfor the abstract semantics. Variable reference ' \u00a3 '' ([[v = v ; ] , \u00df, . ,t) r (succ(J), s ,p ,t s, \np \u00df, . ), where ' ds ' s( t = tick(J, t ) = s u [\u00df (v) . \u00df(v ' ))]. Return . ' \u00a3 '' ' ([[return v ; \n] , s,p ,t) r (s,\u00df , ,p . ,t ), where \u00df, s . ' '' t = tickd(J, t ) (v ,s, \u00df ' ,p ) . s (p . ) d = s (\u00df \n(v)) s ' = s u [\u00df ' (v ' ) . d ]. Field reference ' \u00a3 . ' ' . ([[v = v .f ; ] , \u00df, ,t) r (succ(J), \u00df, \n,p ,t ), where s,p s ' t dt) \u00df(vs u [ s( = tick(J, (C, \u00df ' ) . s ( ' )) s ' = \u00df(v) . \u00df ' (f))]. Method \ninvocation - ' . .. ' \u00a3 \u00df '' s '' ([[v = v0.m( v );] , s,p , , ,p ,t \u00df, t) r (s0, ), where ---. - ---. \n' ''' M =[ Cm (Cv '' ) {Cv ; ss}] .M(d 0,m) d 0 = s (\u00df (v0)) d i = s (\u00df (vi' )) ' d . t = tick(J, t \n) . =(v, succ(J), \u00df, p ) . ' ' '' p = allocA. (M, t ' ) a = allocA(vi ,t ' ) i '' A''' ' ) \u00df ' a j = \nalloc(vj ,t = [[[this] . \u00df (v0)] \u00df '' = \u00df ' [vi'' . a ' i,v ''' . a '' j ] s ' = s u [p . ' .{. } ,a \n' i . d i]. j Object allocation -. ' \u00a3 '' ([[v = new C ( v );] , s,p . \u00df, ,p . ), \u00df, ,t) r (succ(J),s \n,t where ' d' t = tick(J, t ) d i = s (\u00df (vi)) ( SA ' ) f, K ) = C (C) a i = alloc(fi,t (. s, K(Sa, dS) \nd ' \u00df ' ) \u00df ' )= =(C, s ' = s u . s u [\u00df (v) . d ' ]. Casting . '' '' ([[v = (C ) v ] , \u00df, ,t) r (succ(J), \n\u00df, ,p ,t ) s, p . s ' ds ' t = tick(J, t ) = s u [\u00df (v) . s (\u00df (v ' ))]. Figure 9. Abstract semantics \nfor A-Normal Featherweight Java.  time. (Were method names not allowed, then all procedures would return \nto the same continuations in 0 CFA.) r Time = Lab * Time = Labk A Addr = O.set \u00d7 Time = O.set \u00d7 r Addr \nTime O.set = Var + Method. The time-stamp function prepends the most recent label. The variable/.eld-allocation \nfunction pairs the variable/.eld with the current time, while the continuation-allocation function pairs \nthe method being invoked with the current time: d tick(J, t)= J : t tick(J, t ) = .rstk(J : t ) A alloc(v, \nt)=(v, t) alloc(v, t ) = (v, t ) alloc.(M, t)=(M, t) allocA. (M, t ) = (M, t ).  4.4 Computing k-CFA \nfor Featherweight Java When we apply the single-threaded store optimization for k-CFA over Java, the \nstate-space appears to be genuinely exponential for k = 1. This is because the analysis affords more \nprecision and control over individual .elds than is normally expected of a pointer analysis. Under k-CFA, \nthe address of every .eld is the .eld name paired with the abstract time from its moment of allocation; \nthe same is true of every procedure parameter. However, these .elds are still stored within maps, and \nthese maps are the source of the apparent complexity explosion. Fortunately, by inspecting the semantics, \nwe see that every ad\u00address in the range of a binding environment shares the same time. Thus, binding \nenvironments (T BEnv) may be replaced directly by the time of allocation with no loss of precision. In \neffect, T= BEnv ~ T Time for object-oriented programs. Simplifying the semantics un\u00adder this assumption \nleads to an abstract system-space with a poly\u00adnomial number of bits to (monotonically) .ip for a .xed \nk: |Stmt|\u00b7 |r Time|3 \u00b7|Method| + |Method + Var|\u00b7|Time| \u00b7 (|Class|\u00b7 |Time| + |Var|\u00b7|Stmt|\u00b7 |Time|\u00b7|Method|\u00b7 \n|Time|) By constructing Shivers s k-CFA for Java, and noting the subtle difference between the semantics \nhandling of closures and objects, we have exposed the root cause of the discrepancy in complexity. In \nthe next section, we pro.t from this observation by construct\u00ading a semantics in which closures behave \nlike objects, resulting in a polynomial-time, context-sensitive hierarchy of CFAs for func\u00adtional programs. \n 4.5 Variations The above form of k-CFA is not exactly what would be usually called a k-CFA points-to \nanalysis in OO languages. Speci.cally, OO k-CFAs would typically not change the context for each state\u00adment \nbut only for method invocation statements. An OO k-CFA is a call-site-sensitive points-to analysis: the \nonly context maintained is call-sites. That is, abstract time would not tick , except in the method invocation \nrule of Figure 9. Furthermore, the caller s con\u00adtext would be restored on a method return, instead of \njust advancing the abstract time to its next step. (This choice is discussed exten\u00adsively in the next \nsections.) These variations, however, are orthog\u00adonal to our main point: The algorithm is polynomial \nbecause of the simultaneous closing of all .elds of an object. 5. m-CFA: Context-sensitive CFA in PTIME \nk-CFA for object-oriented programs is polynomial-time because it collapses the records inside objects \ninto base addresses. It is possible to re-engineer the semantics of the .-calculus so that we achieve \na similar collapse with the environments inside closures. In fact, the re-engineering corresponds to \na well-known compiler optimization technique for functional languages: .at-environment closures [1, 3]. \nIn .at-environment closures, the values of all free variables are copied directly into the new environment. \nAs a result, one needs to keep track of only the base address of the environment: any free variable is \naccessed as an offset. This .at-environment re-engineering leads to the desired poly\u00adnomiality, an outcome \n.rst noted in the universal framework of Ja\u00adgannathan and Weeks [8] (here JW for brevity). Some caution \nmust be taken in the use of .at environments; if used in conjunction with Shivers s k-CFA-style last-k-call-sites \ncontour-allocation strategy, .at environments achieve weak context-sensitivity in prac\u00adtice (Section \n6). Jagannathan and Weeks suggest several contour abstractions for control-.ow analyses, including using \nthe last k call sites and the top m frames of the stack. Section 6 argues quan\u00adtitatively and qualitatively \nthat the top-m-frames approach is the right abstraction for .at environments. To distinguish this approach \nfrom other possible instantiations of the JW framework, we term the resulting hierarchy m-CFA. Additionally, \nwe note that it is im\u00adportant to specify m-CFA explicitly, as we do below, since its form does not straightforwardly \nfollow from past results. Speci.cally, Jagannathan and Weeks do specify the abstract domains necessary \nfor a stack-based polynomial k-CFA but do not give an explicit abstract semantics that would produce \nthe results of their exam\u00adples. This is signi.cant because simply adapting the JW concrete semantics \nto the abstract domains would not produce m-CFA (or any other reasonable static analysis). The analysis \ncannot just pop stack frames when a .nite pre.x of the call-stack is kept. For in\u00adstance, when the current \ncontext abstraction consists of call-sites (f, g), popping the last call-site will result in a one-element \nstack. What our analysis needs to do instead (on a function return) is re\u00adstore the abstract environment \nof the current caller. 5.1 A concrete semantics with .at closures In the new state-space, an environment \nis a base address: . . S= Call \u00d7 Env \u00d7 Store s . Store = Addr D d . D = Clo clo . Clo = Lam \u00d7 Env a . \nAddr = Var \u00d7 Env . . Env is a set of base environment addresses. The expression-evaluator E : Exp \u00d7 Env \n\u00d7 Store -D creates a closure over the current environment: E(v, ., s)= s(v, .) E(lam, ., s)=(lam,.). \nThere is only one transition rule; when call =[ (fe1 ...en)] : ''' ' (call, ., s) . (call ,. ,s ), where \n(lam,. ' )= E(f, ., s) di = E(ei, ., s) ' . '' lam =[ (. (v1 ...vn) call )] = new(call,.) {x1,...,xm} \n= free(lam) avi =(vi,. '' ) axj =(xj ,. '' ) d ' = s(xj ,. ' ) j s ' = s[avi . di][axj . d ' j ].  \n5.2 Abstract semantics: m-CFA The abstract state-space is similar to the concrete: . . Env \u00d7 A S= Call \n\u00d7 dStore Ab d s . A= Addr D Store d . D = P Clo c clo . dEnv Clo = Lam \u00d7 d a . A= Var \u00d7 d Addr Env \n. . d Env is a set of base environments addresses. The abstract evaluator E : Exp \u00d7 dStore . b Env \u00d7 \nMD also mirrors the concrete semantics: E(v, s)= .) ., .)} . ., s(v, E(lam, s)= {(lam, There is only \none transition rule; when call =[ (fe1 ...en)] : ''' ' (call, ., s ) . (call ,. ,s ), where (lam,. ' \n) . ., d i = E (ei, s) E(f, s) ., ' . '' ) lam =[ (. (v1 ...vn) call )] a xj =(xj , . '' . '' ) = new(call, \n. ' ) a vi =(vi, d., lam, {x1,...,xm} = free(lam) d ' = s (xj ,. ' ) j s ' = s U [ avi . d i] U [ axj \n. d ' j ]. 5.3 Context-sensitivity The parameter which must be .xed for m-CFA is the new envi\u00adronment \nallocator. To construct the right kind of context-sensitive analysis, we will work backward from the \nabstract to the con\u00adcrete. We would like it to be the case that when a procedure is invoked, bindings \nto its parameters are separated from other bind\u00adings based on calling context. In addition, we need it \nto be the case that procedures return to the calling context in which they were invoked. (Bear in mind \nthat returning in CPS means calling the continuation argument.) Directly allocating the last k call sites, \nas in k-CFA, does not achieve the desired effect, because variables get repeatedly rebound during the \nevaluation of a procedure with each invocation of an internal continuation. This causes variables from \nseparate invocations to merge once they are k calls into in the procedure. Counterintuitively, we solve \nthis problem by allocating fewer abstract environments. We want to allocate a new environ\u00adment when a \ntrue procedure is invoked, and we want to restore an old environment when a continuation is invoked. \nAs a result, m-CFA is sensitive to the top m stack frames, whereas k-CFA is sensitive to the last k calls.5 \nIn this case, environments will be a function of context, so we have environments play the role of time-stamps \nin k-CFA: m dA Env = Call , m-CFA assumes and exploits the well-known partitioning of the CPS grammar \nfrom .CFA [12] which syntactically distinguishes ordinary procedures from continuations: ( .rst(call \n: .) lam is a procedure m d., lam, new(call, . ' )= . ' lam is a continuation. From this it is clear \nthat [m = 0]CFA and [k = 0]CFA are actually the same context-insensitive analysis. 5 Consider a program \nwhich calls a, calls b and then returns from b.[k = 1]CFA will consider the context to be the call to \nb, while [m =1]CFA will consider the context to be the call to a. By setting Env = N\u00d7 Call *, it is straightforward \nto construct a concrete allocator that the abstract allocator simulates: ' S' S new(call, (n, call), \nlam, (n, call )) = ( S (n +1, call : call) lam is a procedure ' S (n +1, call ) lam is a continuation. \n 5.4 Computing m-CFA Consider the single-threaded system-space for m-CFA: Call \u00d7 d\u00d7 StoreA .= P Env \n d ~Call .P Envd\u00d7 Addr .P Clo . = Theorem 5.1. Computing m-CFA is complete for PTIME. Proof. Computing \nm-CFA is a monotonic ascent through a lattice whose height is polynomial in program size: |Call|\u00d7|Call|m \n\u00d7|Var|\u00d7|Call|m \u00d7|Lam|\u00d7|Call|m . Clearly, for any choice of m = 0, m-CFA is computable in polynomial time. \nHardness follows from the fact that [m = 0]CFA and [k = 0]CFA are the same analysis, which is known to \nbe PTIME-hard [18]. 6. Comparisons to related analyses This work draws heavily on the Cousots abstract \ninterpretation [4, 5] and upon Shivers s original formulation of k-CFA [17]. m-CFA (assuming suitable \nwidening) can be viewed as an instance of the universal framework of Jagannathan and Weeks [8], but for \ncontinuation-passing style. If one naively uses the framework of Ja\u00adgannathan and Weeks [8] with Shivers \ns k-CFA contour-allocation strategy, the result is a polynomial CFA algorithm that uses a last\u00adk-call-sites \ncontext abstraction, unlike our m-CFA, which uses a top-m-frames abstraction. In the rest of this section, \nnaive poly\u00adnomial k-CFA refers to a .at-environment CFA with a last-k-call\u00adsites abstraction. We will \nargue next, both qualitatively and quantitatively, why the top-m-frame abstraction is better than the \nlast-k-call abstraction for the case of .at-environment CFAs. The distinction between these policies \nis subtle yet important. Using the last k call sites forces environments within a function s scope to \nmerge after the kth (direct or indirect) call made by a function. Any recursive function will appear \nto make at least k calls during an analysis, leaving only leaf procedures with boosted context-sensitivity; \nsince leaf procedures do not invoke higher-order functions, the extra context-sensitivity offers no bene.t \nto control-.ow analysis. Consider, for example, the invocation of a simple function: (identity 3) If \nthe de.nition of the identity function is: (define (identity x) x) then both naive polynomial 1CFA and \n[m = 1]CFA return the same .ow analysis as [k = 1]CFA for the program: (id 3) (id 4) That is, all agree \nthe return value is 4. If, however, we add a seem\u00adingly innocuous function call to the body of the identity \nfunction: (define (identity x) (do-something) x)  then polynomial 1CFA would say that the program returns \n3 or 4, whereas [m = 1]CFA and [k = 1]CFA still agree that the return value is just 4. To understand \nwhy naive polynomial 1CFA degenerates into the behavior of 0CFA with the addition of the function call \nto do-something, consider what the last k =1 call sites are at the return point x. Without the intervening \ncall to (do-something), the last call site at this point was (id 3) in the .rst case, and (id 4) in the \nsecond case. Thus, polynomial 1CFA keeps the bindings to x distinct. With the intervening call to (do-something), \nthe last call site becomes (do-something) in both cases, causing the .ow sets for x to merge together. \nIf, however, we allocate the top m stack frames for the environment, then the intervening call to do-something \nhas no effect, because the top of the stack at the return point x is still the call to (id 3) or (id \n4), which keeps the bindings distinct. Several papers have investigated polyvariant .ow analyses with \npolynomial complexity bounds in the setting of type-based anal\u00adysis, as compared with the abstract interpretation \napproach em\u00adployed in this paper. Mossin [14] presents a .ow analysis based on polymorphic subtyping \nincluding polymorphic recursion for a simply-typed (i.e. monomorphically typed) .-calculus. Mossin s \nalgorithm operates in O(n 8)-time and both Rehof and F\u00a8ahndrich [15] and Gustavsson and Svenningsson \n[6] developed alternative algorithms that operate in O(n 3), where n is the size of the explic\u00aditly typed \nprogram (and in the worst case, types may be exponen\u00adtially larger than the programs they annotate). \nm-CFA does not im\u00adpose typability assumptions and is polynomial in the program size without type annotations. \nAs a consequence of the abstract inter\u00adpretation approach taken in m-CFA, unreachable parts of the pro\u00adgram \nare never analyzed, in contrast to most type based approaches. Another difference concerns the space \nof abstract values: m-CFA includes closure approximations, while polymorphic recursive .ow types relate \nprogram text and do not predict run-time environment structure. 6.1 Benchmark-driven comparisons We have \nimplemented k-CFA, m-CFA and polynomial k-CFA for R5RS Scheme (with support for some of R6RS). Making \na fair comparison of unrelated CFAs (e.g., m-CFA and polynomial k-CFA) is not straightforward. CFAs are \nnot totally ordered by either speed or precision for all programs. In fact, even within the same program, \ntwo CFAs may each be locally more precise at different points in the program. That is, given the output \nof two CFAs, it might not always be possible to say one is more precise than another. To compare CFAs \non an apples-to-apples basis requires careful benchmark construction; we discuss the results on such \nbenchmarks below. 6.1.1 Comparing speed with precision held constant The constructive content of Van \nHorn and Mairson s proof offers a way to generate benchmarks that exercise the worst-case behavior of \na CFA by constructing a program that forces the CFA to the top of the lattice (because the most precise \npossible answer is the top). Using this insight, we constructed a series of successively larger worst-case \nbenchmarks and recorded how long it took each CFA to reach the top of the lattice on a 2 Core, 2 GHz \nOS X machine: E indicates that the analysis returned in less than one second; 8 indicates the analysis \ntook longer than one hour. Terms k = 1 m = 1 poly.,k=1 k=0 69 E E E E 123 E E E E 231 46 s E 2 s E 447 \n8 3 s 5 s 2 s 879 8 48 s 1 m 8 s 15 s 1743 8 51 m 8 3 m 48 s As can be seen, m-CFA is not just faster \nthan k-CFA but also consistently faster than naive polynomial k-CFA. The difference in scalability between \nm-CFA and k-CFA is large and matches the theoretical expectations well. From these numbers we can in\u00adfer \nthat, in the worst case, the feasible range of context-sensitive analysis of functional programs has \nbeen increased by two-to-three orders of magnitude.  6.2 Comparing speed and precision On the following \nbenchmarks, we measured both the run-time of the analyses and the number of inlinings supported by the \nresults. We are using the number of inlinings supported as a crude but immediately practical metric of \nthe precision of the analysis. Prog/ Terms k = 1 m = 1 poly.,k=1 k=0 eta 49 E 7 E 7 E 3 E 3 map 157 E \n8 E 8 E 8 E 6 sat 223 8 - E 12 1s 12 E 12 regex 1015 4s 25 3s 25 14s 25 2s 25 scm2java 2318 5s 86 3s \n86 3s 79 4s 79 interp 4289 5s 123 4s 123 9s 123 5s 123 scm2c 6219 179s 136 143s 136 157s 131 55s 131 \n The .rst two benchmarks test common functional idioms; sat is a back-tracking SAT-solver; regex is a \nregular expression matcher based on derivatives; scm2java is a Scheme compiler that targets Java; interp \nis a meta-circular Scheme interpreter; scm2c is a Scheme compiler that targets C. From these experiments, \nm-CFA appears to be as precise as k-CFA in practice, but at a fraction of the cost. Compared to naive \npolynomial 1CFA, [m = 1]CFA is always equally fast or faster and equally or more precise. These experiments \nalso suggest that naive polynomial 1CFA is little better than 0CFA in practice, and, in fact, it even \nincurs a higher running time than k-CFA in some cases. 7. Conclusion Our investigation began with the \nk-CFA paradox: the apparent con\u00adtradiction between (1) Van Horn and Mairson s proof that k-CFA is EXPTIME-complete \nfor functional languages and (2) the ex\u00adistence of provably polynomial-time implementations of k-CFA \nfor object-oriented languages. We resolved the paradox by show\u00ading that the same abstraction manifests \nitself differently for func\u00adtional and object-oriented languages. To do so, we faithfully recon\u00adstructed \nShivers s k-CFA for Featherweight Java, and then found that the mechanism used to represent closures \nis degenerate for the semantics of Java. This degeneracy is what causes the collapse into polynomial \ntime. With respect to standard practice in k-CFA, the bindings inside closures may be introduced over \ntime in several contexts, whereas the .elds inside an object are all allocated in the same context. This \nallows objects to be represented as a class name plus the initial context, whereas the environments inside \nclosures must be a true map from variables to binding contexts; this map causes the exponential blow-up \nin complexity for functional k-CFA. Armed with this insight, we constructed a concrete semantics for \nthe .\u00adcalculus which uses .at environments environments in which free variables are accessed as offsets \nfrom a base pointer, rather than through a chain of environments. In fact, this environment policy corresponds \nto well-known implementation techniques from the .eld of functional program compilation.  Under abstraction, \n.at environments exhibit the same degen\u00aderacy as objects, and the end result is a polynomial hierarchy \nof context-sensitive control-.ow analyses for functional languages. Our empirical investigation found \nthat coupling .at environments with a last-k-call-sites policy for context-allocation offers negligi\u00adble \nbene.ts for precision compared with 0CFA. To solve this prob\u00adlem, we constructed a polynomial CFA hierarchy \nwhich allocates the top m stack frames as its context: m-CFA. According to our empirical evaluation, \nm-CFA matches k-CFA in precision, but with faster performance. 8. Future work Our intent with this work \nwas to build a bridge. Now built, that bridge spans the long-separated worlds of functional and object\u00adoriented \nprogram analysis. Having already pro.ted from the .rst round-trip voyage, it is worth asking what else \nmay cross. We believe that abstract garbage collection is a good candi\u00addate [13]. At the moment, it has \nonly been formulated for the func\u00adtional world. The abstract semantics for Featherweight Java make it \npossible to adapt abstract garbage collection to the static analysis of object-oriented programs. We \nhypothesize that its bene.ts for speed and precision will carry over. Going in the other direction, the \n.eld of points-to analysis for object-oriented languages has signi.cant maturity and has devel\u00adoped a \nmore practical understanding for what parameters (e.g., context depth) and approximations (e.g., maintaining \ndifferent con\u00adtexts for variables vs. closures) tend to yield fruitful precision for client analyses. \nThere is a more intense emphasis on implemen\u00adtation (e.g., using binary decision diagrams) and on evaluation, \nwhich should be possible to translate to the functional setting. Also, what the object-oriented community \ncalls shape analysis appears togo by environment analysis in the functional community. Peer\u00ading across \nfrom the functional side of the bridge, shape analyses seem far ahead of environment analyses in their \nsophistication. We hypothesize that these shape-analytic techniques will be pro.table for environment \nanalysis. Acknowledgments: We are grateful to Jan Midtgaard for com\u00adments and relevant references to \nthe literature. We thank Ond.rej Lhot\u00b4 ak for valuable discussions. This work was funded by the Na\u00adtional \nScience Foundation under grant 0937060 to the Comput\u00ading Research Association for the CIFellow Project, \nwhich sup\u00adports David Van Horn, as well as grants CCF-0917774 and CCF\u00ad0934631. References [1] Andrew \nW. Appel. Compiling with Continuations. Cambridge Uni\u00adversity Press, November 1991. ISBN 0-521-41695-7. \n[2] Martin Bravenboer and Yannis Smaragdakis. Strictly declarative spec\u00adi.cation of sophisticated points-to \nanalyses. In OOPSLA 09: 24th annual ACM SIGPLAN conference on Object Oriented Programming, Systems, Languages, \nand Applications, 2009. [3] Luca Cardelli. Compiling a functional language. In LISP and Func\u00adtional Programming, \npages 208 217, 1984. [4] Patrick Cousot and Radhia Cousot. Abstract interpretation: A uni.ed lattice \nmodel for static analysis of programs by construction or approx\u00adimation of .xpoints. In Conference Record \nof the Fourth ACM Sympo\u00adsium on Principles of Programming Languages, pages 238 252. ACM Press, 1977. \n[5] Patrick Cousot and Radhia Cousot. Systematic design of program analysis frameworks. In POPL 79: Proceedings \nof the 6th ACM SIGACT-SIGPLAN Symposium on Principles of Programming Lan\u00adguages, pages 269 282. ACM Press, \n1979. [6] J\u00a8 orgen Gustavsson and Josef Svenningsson. Constraint abstractions. In PADO 01: Proceedings \nof the Second Symposium on Programs as Data Objects, pages 63 83. Springer-Verlag, 2001. ISBN 3-540\u00ad42068-1. \n[7] Atsushi Igarashi, Benjamin C. Pierce, and Philip Wadler. Feather\u00adweight Java: a minimal core calculus \nfor Java and GJ. ACM Trans. Program. Lang. Syst., 23(3):396 450, 2001. ISSN 0164-0925. [8] Suresh Jagannathan \nand Stephen Weeks. A uni.ed treatment of .ow analysis in higher-order languages. In POPL 95: Proceedings \nof the 22nd ACM SIGPLAN-SIGACT Symposium on Principles of Program\u00adming Languages, pages 393 407. ACM, \n1995. ISBN 0-89791-692-1. [9] Ond.rej Lhot\u00b4ak. Program Analysis using Binary Decision Diagrams. PhD thesis, \nMcGill University, January 2006. [10] Ond.rej Lhot\u00b4ak and Laurie Hendren. Evaluating the bene.ts of context\u00adsensitive \npoints-to analysis using a BDD-based implementation. ACM Trans. Softw. Eng. Methodol., 18(1):1 53, 2008. \nISSN 1049-331X. [11] Jan Midtgaard. Control-.ow analysis of functional programs. Tech\u00adnical Report BRICS \nRS-07-18, DAIMI, Department of Computer Sci\u00adence, University of Aarhus, December 2007. To appear in revised \nform in ACM Computing Surveys. [12] Matthew Might and Olin Shivers. Environment analysis via .-CFA. In \nPOPL 06: Conference record of the 33rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, \npages 127 140. ACM, 2006. ISBN 1-59593-027-2. [13] Matthew Might and Olin Shivers. Improving .ow analyses \nvia GCFA: Abstract garbage collection and counting. In ICFP 06: Proceedings of the Eleventh ACM SIGPLAN \nInternational Conference on Functional Programming, pages 13 25. ACM, 2006. ISBN 1-59593-309-3. [14] \nChristian Mossin. Flow Analysis of Typed Higher-Order Programs. PhD thesis, DIKU, University of Copenhagen, \nJanuary 1997. [15] Jakob Rehof and Manuel F\u00a8ahndrich. Type-base .ow analysis: from polymorphic subtyping \nto CFL-reachability. In POPL 01: Proceed\u00adings of the 28th ACM SIGPLAN-SIGACT Symposium on Principles \nof Programming Languages, pages 54 66. ACM, 2001. ISBN 1-58113\u00ad336-7. [16] Olin Shivers. Higher-order \ncontrol-.ow analysis in retrospect: lessons learned, lessons abandoned. In Kathryn S. McKinley, editor, \nBest of PLDI 1988, volume 39, pages 257 269. ACM, 2004. [17] Olin G. Shivers. Control-Flow Analysis of \nHigher-Order Languages. PhD thesis, Carnegie Mellon University, 1991. [18] David Van Horn and Harry G. \nMairson. Relating complexity and precision in control .ow analysis. In ICFP 07: Proceedings of the 12th \nACM SIGPLAN International Conference on Functional Pro\u00adgramming, pages 85 96. ACM, 2007. ISBN 9781-59593-815-2. \n[19] David Van Horn and Harry G. Mairson. Deciding kCFA is complete for EXPTIME. In ICFP 08: Proceeding \nof the 13th ACM SIGPLAN International Conference on Functional Programming, pages 275 282. ACM, 2008. \nISBN 9781-595-9391-9-7.     \n\t\t\t", "proc_id": "1806596", "abstract": "<p>Low-level program analysis is a fundamental problem, taking the shape of \"flow analysis\" in functional languages and \"points-to\" analysis in imperative and object-oriented languages. Despite the similarities, the vocabulary and results in the two communities remain largely distinct, with limited cross-understanding. One of the few links is Shivers's <i>k</i>-CFA work, which has advanced the concept of \"context-sensitive analysis\" and is widely known in both communities.</p> <p>Recent results indicate that the relationship between the functional and object-oriented incarnations of <i>k</i>-CFA is not as well understood as thought. Van Horn and Mairson proved <i>k</i>-CFA for <i>k</i> &#8805; 1 to be EXPTIME-complete; hence, no polynomial-time algorithm can exist. Yet, there are several polynomial-time formulations of context-sensitive points-to analyses in object-oriented languages. Thus, it seems that functional <i>k</i>-CFA may actually be a profoundly different analysis from object-oriented <i>k</i>-CFA. We resolve this paradox by showing that the exact same specification of <i>k</i>-CFA is polynomial-time for object-oriented languages yet exponential-time for functional ones: objects and closures are subtly different, in a way that interacts crucially with context-sensitivity and complexity. This illumination leads to an immediate payoff: by projecting the object-oriented treatment of objects onto closures, we derive a polynomial-time hierarchy of context-sensitive CFAs for functional programs.</p>", "authors": [{"name": "Matthew Might", "author_profile_id": "81309498719", "affiliation": "University of Utah, Salt Lake City, UT, USA", "person_id": "P2184568", "email_address": "", "orcid_id": ""}, {"name": "Yannis Smaragdakis", "author_profile_id": "81100614708", "affiliation": "University of Massachusetts, Amherst, MA, USA", "person_id": "P2184569", "email_address": "", "orcid_id": ""}, {"name": "David Van Horn", "author_profile_id": "81337494657", "affiliation": "Northeastern University, Boston, MA, USA", "person_id": "P2184570", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1806596.1806631", "year": "2010", "article_id": "1806631", "conference": "PLDI", "title": "Resolving and exploiting the <i>k</i>-CFA paradox: illuminating functional vs. object-oriented program analysis", "url": "http://dl.acm.org/citation.cfm?id=1806631"}