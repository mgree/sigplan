{"article_publication_date": "06-05-2010", "fulltext": "\n Composing Parallel Software Ef.ciently with Lithe Heidi Pan Benjamin Hindman Krste Asanovi\u00b4c Massachusetts \nInstitute of Technology UC Berkeley UC Berkeley xoxo@csail.mit.edu benh@eecs.berkeley.edu krste@eecs.berkeley.edu \n Abstract Applications composed of multiple parallel libraries perform poorly when those libraries interfere \nwith one another by oblivi\u00adously using the same physical cores, leading to destructive resource oversubscription. \nThis paper presents the design and implementa\u00adtion of Lithe, a low-level substrate that provides the \nbasic primitives and a standard interface for composing parallel codes ef.ciently. Lithe can be inserted \nunderneath the runtimes of legacy parallel li\u00adbraries to provide bolt-on composability without needing \nto change existing application code. Lithe can also serve as the foundation for building new parallel \nabstractions and libraries that automatically interoperate with one another. In this paper, we show versions \nof Threading Building Blocks (TBB) and OpenMP perform competitively with their original im\u00adplementations \nwhen ported to Lithe. Furthermore, for two applica\u00adtions composed of multiple parallel libraries, we \nshow that lever\u00adaging our substrate outperforms their original, even expertly tuned, implementations. \nCategories and Subject Descriptors D.3.4 [Programming Lan\u00adguages]: Processors Run-time Environments; \nD.4.1 [Operating Systems]: Process Management; D.2.12 [Software Engineering]: Interoperability General \nTerms Algorithms, Design, Languages, Performance Keywords Composability, Cooperative Scheduling, Hierarchical \nScheduling, Oversubscription, Parallelism, Resource Management, User-Level Scheduling 1. Introduction \nWith the widespread adoption of multicore microprocessors, many software libraries are now becoming parallelized. \nThese libraries exploit decades of parallel programming innovation in novel ab\u00adstractions, performance \noptimizations, and correctness. For produc\u00adtivity, programmers would like to reuse these parallel libraries \nby composing them together to build applications, as is standard prac\u00adtice with sequential libraries. \nUnfortunately, composing parallel li\u00adbraries ef.ciently is hard. The parallel libraries may interfere \nwith one another, sometimes delivering performance that is much worse than a sequential implementation. \nThe root of this problem is poor resource management across parallel libraries. Each parallel library \ncreates its own set of op- Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed erating \nsystem threads to represent the processor cores in the ma\u00adchine. While threads provide a natural programming \nabstraction for some types of computation (e.g. [40]), they are a poor re\u00adsource abstraction for parallel \nprogramming, since they are mul\u00adtiplexed onto the same physical cores (Figure 1(a)). When too many threads \nare active, each thread runs at infrequent and unpre\u00addictable times, interfering with synchronization \nmechanisms [31] and custom scheduling policies [23]. Unpredictable multiplexing also leads to cache interference \nbetween threads, hindering opti\u00admizations such as prefetching [18] and cache-blocking [43]. Current software \nlibraries supply ad hoc solutions to this prob\u00adlem. A glaring example is Intel s Math Kernel Library \n(MKL), which instructs its clients to call the sequential version of the li\u00adbrary whenever it might be \nrunning in parallel with another part of the application [19]. Such solutions destroy any separation \nbetween interface and implementation, and place a dif.cult burden on pro\u00adgrammers who have to manually \nchoose between different library implementations depending on the calling environment. Worse still, a \nprogrammer writing a new parallel library that encapsulates MKL will just de.ect the problem to its clients. \nA thriving parallel soft\u00adware industry will only be possible if programmers can arbitrarily compose parallel \nlibraries without sacri.cing performance. One possible solution is to require all parallelism to be ex\u00adpressed \nusing a universal high-level abstraction. While attractive in principle, this goal has proven elusive \nin practice. First, there has been no agreement on the best parallel practices, as evidenced by the proliferation \nof new parallel languages and abstractions over the years. Second, it is unlikely that a competitive \nuniversal abstraction even exists, as they have been repeatedly outperformed by opti\u00admizations that leverage \ndomain or application-speci.c knowledge (e.g. [6, 37]). Our solution, Lithe, is a low-level substrate \nthat provides the basic primitives for parallel execution and a standard interface for composing arbitrary \nparallel libraries ef.ciently. Lithe replaces the virtualized thread abstraction with an unvirtualized \nhardware thread primitive, or hart, to represent a processing resource (Figure 1(b)). Whereas threads \nprovide the false illusion of unlimited processing resources, harts must be explicitly allocated and \nshared amongst the different libraries. Libraries within an application are given complete control over \nhow to manage the harts they have been allocated, including how to allocate harts to parallel libraries \nthey invoke. The runtimes of existing languages and abstractions can be eas\u00adily ported to Lithe. This \nallows, for example, an application written in a high-level language like Haskell to ef.ciently interoperate \nwith low-level libraries that use OpenMP. In addition, Lithe serves as a basis for building a wide range \nof new parallel abstractions. We have ported Intel s Threading Building Blocks (TBB) [34] for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. PLDI 10, June 5 10, 2010, Toronto, Ontario, Canada. Copyright c &#38;#169; 2010 ACM 978-1-4503-0019-3/10/06. \n. . $10.00 and GNU s OpenMP [5] libraries to run with Lithe. We have also implemented a new parallel \nlibrary, libprocess [17], which exports an Erlang-like actor-based programming model. As a baseline, \nwe show that the standalone execution of the ported libraries performs  (a) (b) Figure 1. In conventional \nsystems (a), each library is only aware of its own set of virtualized threads, which the operating system \nmultiplexes onto available physical processors. Lithe, (b), provides unvirtualized processing resources, \nor harts, which are shared cooperatively by the application libraries, preventing resource oversubscription. \ncompetitively with their original implementations. We also show signi.cant performance improvements when \ncomposing these li\u00adbraries in two application case studies. Our .rst case study, an ap\u00adplication server \nmodeled after Flickr.com, uses the popular Graph\u00adicsMagick [16] library to resize images uploaded by \nclients, and achieves roughly 2\u00d7 latency improvement for the same throughput when leveraging Lithe. Our \nsecond case study, a third-party sparse QR application [7], achieves up to a 1.5\u00d7 speedup over the origi\u00adnal \nnaive implementation when leveraging Lithe, and even outper\u00adforms the original implementation with expert \nmanual tuning of resource allocation. In addition, we show how parallel composition can cause synchronization \nand scheduling to interact poorly, and use a simple barrier case study to illustrate how Lithe can improve \nthe situation.  2. Cooperative Hierarchical Resource Manage\u00adment Applications are built by composing \nlibraries hierarchically.For example, an image processing application may use the popular GraphicsMagick \nlibrary [16], which in turn uses the network graph\u00adics library libpng (amongst others), which in turn \nuses the com\u00adpression library zlib. Programmers tend to think of this hierarchy in terms of control: \na caller (libpng) invokes a callee (zlib). In sequential computing, this explicit transfer of control \nis ac\u00adcompanied with an implicit allocation of processing resources (e.g., processor, cache) for the \ncallee to use. In fact, the caller only con\u00adtinues after the callee has returned, i.e., yielded back \nits processing resources to the caller. Callers want to cooperatively provide their callees with these \nresources because their callees are doing some\u00adthing on their behalf. Likewise, callees cooperatively \nreturn their processing resources when they are done performing their compu\u00adtation. In parallel computing, \na single processing resource is still im\u00adplicitly allocated when a caller invokes a callee, but callees \nmust obtain subsequent processing resources via other means, often by creating additional operating system \nthreads. Callers, however, would often like to manage the allocation of resources to each of their callees. \nThis enables a caller with local knowledge (e.g., about a critical path) to optimize their execution. \nIn this work, we enable the management of resources to be cou\u00adpled with the hierarchical transfer of \ncontrol between libraries. Fur\u00adthermore, just as with control, processing resources are allocated and \nyielded cooperatively: libraries allocate resources to libraries they call, and libraries return resources \nallocated to them when they are .nished computing. This is essentially the con.uence of two lines of \nresearch: hi\u00aderarchical scheduling (e.g. [13, 41]) and cooperative multitasking (e.g. [1, 30]). The hierarchical \nscheduling schemes use hierarchies for modular resource management, but often between untrusted en\u00adtities \n(typically processes). The cooperative multitasking schemes allow each task to execute without being \ninterrupted, but tasks often cannot control who runs next. Our cooperative hierarchical scheme combines \nthe two, enabling local control of resource allocation and allowing a library to make effective use of \nthe resources allocated to it without worrying about possible interference.  3. Lithe Primitives Lithe \nprovides two basic primitives, harts and contexts, to enable library runtimes to perform parallel execution. \nThe hart primitive prevents uncontrolled oversubscription of the machine, while the context primitive \nprevents undersubscription of the machine. A majority of programmers will be blissfully unaware of Lithe. \nOnly low-level runtime programmers, such as the implementors of TBB and OpenMP, will use Lithe primitives \ndirectly. Many pro\u00adgrammers, such as the implementors of GraphicsMagick and MKL, will use high-level \nparallel constructs and abstractions whose run\u00adtimes will be implemented using the Lithe primtives. The \nremain\u00adder of programmers will simply invoke library routines directly, such as those in GraphicsMagick, \nwithout knowing whether they are even implemented or executed sequentially or in parallel. In the remainder \nof this section, we describe each of our primi\u00adtives in more detail. In the following section, we describe \nhow par\u00adallel runtimes use these primitives and the rest of our substrate to share resources in a cooperative \nand hierarchical manner. 3.1 Harts A hart, short for hardware thread, represents a processing re\u00adsource. \nThere is typically one hardware thread per physical core, except for multithreaded machines like SMTs \n[39].  Scheduler Callback Interface Lithe Runtime Interface  Figure 2. Software stack showing the \ncallback and runtime inter\u00adfaces. The hart primitive prevents processing resource oversubscrip\u00adtion in \ntwo ways. First, within an application, there is a .xed one\u00adto-one mapping between harts and the physical \nhardware thread contexts. Thus, two libraries running on two different harts will not interfere with \neach other. Second, a hart must be allocated to a run\u00adtime before that runtime can execute code. This \nis in contrast to threads, which may be created by a runtime on-demand to execute code.  3.2 Contexts \nEach hart always has an associated context, which acts as the execution vessel for the computation running \non the hart. The context primitive allows one to suspend the current computation by blocking its context. \nA blocked context stores the continuation of the computation, including the stack that the computation \nis using, and any context-local state. Note that these are not .rst-class continuations, but can only \nbe used once. Contexts allow runtimes to interoperate with libraries that may need to block the current \ncomputation. For example, a library that provides a mutual exclusion lock may want to block some computation \nuntil a lock is released. Similarly, some computation calling into a network I/O library may need to \nbe blocked until a packet arrives. A library can avoid undersubscribing the machine by blocking the current \ncontext while having the underlying hart continue beating and running other contexts.  4. Lithe Interface \nMuch like how an application binary interface (ABI) enables inter\u00adoperability of codes by de.ning standard \nmechanisms for invoking functions and passing arguments, Lithe enables the ef.cient com\u00adposition of parallel \ncodes by de.ning standard mechanisms for ex\u00adchanging harts. As shown in Figure 2, the Lithe substrate \nde.nes two standard interfaces: 1. A runtime interface that parallel runtimes use to share harts and \nmanipulate contexts. 2. A scheduler callback interface that each parallel runtime must implement to \nmanage its own harts and interoperate with others.  For the remainder of this paper, we refer to the \nportion of each parallel runtime that implements the callback interface as a scheduler. At any point \nin time, a hart is managed by exactly one scheduler, and each scheduler knows exactly which harts are \nunder its control. We call the scheduler that is currently managing a hart the current scheduler. The \nLithe runtime keeps track of the current scheduler for each hart, as well as the hierarchy of schedulers. \nTable 1. The Lithe runtime functions and their corresponding scheduler callbacks.  Lithe Runtime Interface \nScheduler Callback Interface  sched register(sched) register(child)  sched unregister() unregister(child) \n sched request(nharts) request(child, nharts) sched enter(child) enter()  sched yield() yield(child) \n sched reenter() enter()   ctx init(ctx, stack) N/A ctx fini(ctx) N/A ctx run(ctx, fn) N/A \n ctx pause(fn) N/A ctx resume(ctx) N/A ctx block(ctx) block(ctx)  ctx unblock(ctx) unblock(ctx) \n Table 1 lists all of the runtime functions and their corresponding scheduler callbacks. We discuss \neach of them in detail below. 4.1 Sharing Harts 4.1.1 Runtime Functions and Callbacks Register A new \nscheduler registers itself with the Lithe runtime using the sched register function. This function performs \nfour actions: (1) it records the scheduler as the child of the cur\u00adrent scheduler, (2) invokes the current \nscheduler s (i.e., parent s) register callback, (3) updates the current scheduler of the hart to be the \nnew scheduler, and (4) inserts the new scheduler into the scheduler hierarchy. Invoking the register \ncallback informs the parent that the new child scheduler is taking over a hart. With this callback, a \nparent scheduler can update any necessary state before returning the hart to the child. Unregister Once \na child scheduler completes its com\u00adputation and no longer needs to manage harts, it calls sched unregister. \nThis function performs three actions: (1) it invokes the parent scheduler s unregister callback, (2) \nreverts the current scheduler of the hart to be the parent scheduler, and (3) removes the unregistering \nscheduler from the hierarchy. The sched unregister function does not return until all of the child s \nharts have returned back to the parent. Invoking the unregister callback informs the parent that this \nchild scheduler will no longer want more harts. It also gives the parent a chance to clean up any state \nit may have associated with the child. Request To request additional harts, a scheduler invokes sched \nrequest, passing the number of harts desired. This func\u00adtion simply invokes the parent scheduler s request \ncallback, passing both the child that has made the request and the num\u00adber of harts requested. If the \nparent chooses to, it can itself in\u00advoke sched request, and propagate the resource request up the scheduler \nhierarchy. Enter/Reenter A parent scheduler can allocate a hart to a child scheduler by invoking the \nsched enter function and passing the child as an argument. This function performs two actions: (1) it \nupdates the current scheduler to be the child, and (2) invokes the child scheduler s enter callback. \nWith this callback, the child scheduler can use the hart to execute some computation, or grant the hart \nto one of its children by invoking sched enter.Once a hart completes a computation it can return to the \ncurrent scheduler to obtain the next task by invoking sched reenter, which will invoke the current scheduler \ns enter callback. Yield Whenever a scheduler is done with a hart, it can invoke sched yield.The sched \nyield function performs the fol\u00ad       Hart 0 Hart 1 Hart 2 enter yield Time (Parent Scheduler, \nChild Scheduler ) Figure 3. An example of how multiple harts might .ow between a parent and child scheduler. \nlowing actions: (1) it updates the current scheduler from the child to the parent, and (2) invokes the \nparent scheduler s yield callback, specifying the child that yielded this hart. With this callback, the \nparent can use the hart to execute some other computation, grant the hart to another child using sched \nenter, or return the hart back to its parent using sched yield.  4.1.2 Example Figure 3 shows an example \nof how a parent and child scheduler use Lithe to cooperatively share harts. A parent library calls a \nchild library s foo function to do some computation. The implementa\u00adtion of foo then instantiates a scheduler \nto manage the parallelism within that function. The scheduler uses the initial hart to register itself \nwith its parent scheduler, who is managing the parallelism for the caller library, then requests additional \nharts from that parent scheduler. It then begins to execute the computation of foo using that hart. After \nthe parent scheduler has received the request, it may de\u00adcide to grant additional harts to the new scheduler \nby invoking sched enter on each of these harts. Each sched enter,in turn, will invoke the new scheduler \ns enter callback, which uses its extra hart to help with executing the computation. When the scheduler \nis done with the computation, it yields all of the ad\u00additionally granted harts explicitly by invoking \nsched yield.It will also unregister itself with its parent scheduler, then return from foo. 4.1.3 Transitioning \nBetween Schedulers The Lithe runtime provides a special transition context for ev\u00adery hart, each including \na small preallocated stack. When a hart transitions between schedulers using the sched enter and sched \nyield routines, it uses its transition context to execute the enter and yield callbacks, respectively. \nThe transition context acts as a temporary context for a scheduler to run on before it starts or resumes \none of its own contexts. Using the transition context frees a scheduler from having to coordinate with \nother schedulers to share contexts and their associated stacks. 4.1.4 Base Scheduler The Lithe runtime \nprovides a base scheduler for the application. The base scheduler obtains harts for the application to \nuse from the operating system environment and serves as the parent of the .rst scheduler to register \nwith Lithe, i.e., the root scheduler.    Upon receiving requests from the root scheduler, the base \nsched\u00aduler simply passes available harts directly to the root scheduler via sched enter.  4.2 Contexts \nA scheduler can allocate and manage its own contexts, giving it full control over how to manage its stacks, \nenabling optimizations such as linked stacks [40], stacklets [14], or simply frames [36]. The scheduler \ncan also allocate and manage any context-local state. A scheduler sets up a context by calling the ctx \ninit function and passing a stack for the context to use. A scheduler starts the context by invoking \nctx run, and passing it a function that should be invoked by the context. Note that after the computation \nusing the context has completed, it can be reused by subsequent calls to ctx run. A scheduler cleans \nup a context by calling the runtime function ctx fini. Context Switching The current context can be paused \nusing the ctx pause function. The ctx pause function works similarly to the call/cc control operator. \nWhen ctx pause is invoked, it stores the current continuation in the current context, switches to the \ntransition context, and calls the function passed to ctx pause with the current context as an argument. \nSwitching to a different context before invoking the function passed to ctx pause al\u00adlows a programmer \nto manipulate a context without worrying about running on said context s stack. Fisher and Reppy recognized \nthis dilemma when they realized they needed to enqueue a continua\u00adtion and atomically get off its stack \nbefore the continuation was used by another processor [10]. Using ctx pause deals with this problem, \nand others like it, in an elegant and easy to reason about manner. After pausing the context, a library \ncan perform the steps nec\u00adessary to save or discard the context before switching to something else. For \nexample, an I/O or synchronization library can alert the current scheduler of the blocked context by \nusing the ctx block function, which calls the context s scheduler s block callback. After ctx block returns, \nthe library can relinquish the hart back to the current scheduler using sched reenter, which invokes \nthe current scheduler s enter callback. To signify that a blocked context is now runnable, an I/O or \nsyn\u00adchronization library can invoke the ctx unblock function, pass\u00ading the runnable context as an argument. \nThis function invokes the speci.ed context s scheduler s unblock callback. The scheduler s unblock function \nshould not resume the unblocked context using the hart used to execute the callback, but should store \nthe context to be run later.  4.3 Putting it All Together: SPMD Scheduler Example To help explain how \nto manage harts and contexts using the Lithe runtime, we describe a simple parallel library that supports \nthe Single-Program Multiple-Data (SPMD) programming model. The library provides two functions to its \nusers: spmd spawn and spmd tid. spmd spawn spawns N tasks, each invoking the same given function with \nthe same given argument. Each task can then .gure out what to do using its task id, obtained through \nspmd tid. spmd spawn returns after all its spawned tasks have completed. The crux of the SPMD library \nis shown as pseudocode in Fig\u00adure 4. First, spmd spawn instantiates a SPMD scheduler, and reg\u00adisters \nthat scheduler (including its callbacks) with the Lithe run\u00adtime using sched register (lines 2-3). The \nscheduler requests additional harts using sched request (line 4). Using the ini\u00adtial hart, the scheduler \nexecutes its computation (line 5), essentially running one SPMD task after another until they are all \ncompleted (lines 10-13). It then unregisters itself with its parent (line 6). The          \n   1 void spmd spawn(int N, void (*func)(void*), void *arg) { 2 SpmdSched *sched = new SpmdSched(N, \nfunc, arg); 3 sched register(sched); 4 sched request(N-1); 5 sched->compute(); 6 sched unregister(); \n7 delete sched; 8 } 9 10 void SpmdSched::compute() { 11 while (/* unstarted tasks */) 12 func(arg); \n13 } 14 15 void SpmdSched::enter() { 16 if (/* unblocked paused contexts */) 17 ctx resume(/* next unblocked \ncontext */); 18 else if (/* requests from children */) 19 sched enter(/* next child scheduler */); 20 \nelse if (/* unstarted tasks */) 21 ctx = new SpmdCtx(); 22 ctx run(ctx, start); 23 else sched yield(); \n 24 }25 26 void SpmdSched::start() {27 compute(); 28 ctx pause(cleanup); 29 }30 31 void SpmdSched::cleanup(ctx) \n{ 32 delete ctx; 33 if (/* not all tasks completed */) 34 sched reenter(); 35 else 36 sched yield(); \n37 } Figure 4. SMPD scheduler pseudocode example. spmd spawn function cleans up the scheduler before \nreturning back to its caller (line 7). Any additional harts granted by its parent will join the SPMD \nscheduler through its enter callback (line 15). First, the scheduler tries to resume any paused context \nthat is now runnable (lines 16\u00ad17). Otherwise, the scheduler tries to satisfy requests from its chil\u00addren \n(lines 18-19). Finally, it will try to create a new context to start a new SPMD task, by invoking the \nruntime function ctx start with the new context and the start function (lines 20-22). start will continually \nrun the next SPMD task using that context until they are all completed (line 27). It will then clean \nup that context (line 28), and either send the hart back into the main scheduling loop to do more work \n(lines 33-34), or yield the hart back to its parent (lines 35-36).  5. Implementation We have implemented \na complete Lithe runtime, as well as a user-level implementation of harts, for 64-bit Linux. Both require \nonly modest amounts of code. The implementation of Lithe is approximately 2,300 lines of C, C++, and \nx86 assembly, while the harts implementation is approximately 600 lines of C and x86 assembly. 5.1 Lithe \nThe Lithe runtime de.nes two opaque types: schedulers and con\u00adtexts. A Lithe scheduler object contains \nthe location of the function pointer table of standard callbacks, and the location of the schedul\u00ading \nstate, both of which are supplied by the corresponding user\u00adlevel scheduler upon registering with the \nruntime. A Lithe sched\u00aduler object also contains a pointer to its parent, and a pointer to a doubly linked \nlist of its children. Internally, the runtime keeps track of the scheduler hierarchy, as well as the \ncurrent scheduler object for each hart. Externally, the runtime gives the pointer to each scheduler object \nto its parent scheduler as an opaque handle for that child. The core runtime interface is in C, but we \nprovide a C++ scheduler abstract base class that delineates the required stan\u00addard callbacks.  A Lithe \ncontext object contains the hart s machine state, a pointer to its corresponding Lithe scheduler object, \nand the location of the context-local state as speci.ed by its scheduler. The runtime uses the POSIX \nucontext to represent the hart s machine state, and uses the GNU ucontext library to save and resume \ncontexts. We have made minor changes to GNU s ucontext library to eliminate unnecessary system calls \nto change the signal mask, so that saving and resuming contexts do not incur any kernel crossings. 5.2 \nHarts Our user-level implementation represents each hart using a pthread, since Linux maps each pthread \nto its own kernel thread, and uses the af.nity extension of pthreads supported by Linux to pin each pthread \nto a unique core. We create and initialize a global pool of N harts at the beginning of the application, \nwith N being the number of cores in the ma\u00adchine. One hart is used to call the application s main function. \nThe remaining harts sleep at a gate, waiting for work. When Lithe s base scheduler requests additional \nresources using hart request,the harts are released from the gate to call into the application s entry \nfunction. As harts return from the entry function or via an explicit hart yield, they go back to waiting \nat the gate. 5.3 Operating System Support We choose to implement both Lithe and harts as completely \nuser\u00adlevel libraries rather than modifying an existing operating system. The main bene.t of our approach \nis portability across existing platforms that provide the necessary POSIX constructs. The main downfall \nis that an operating system is still free to multiplex our harts with other applications running in the \nsystem. As the results in this paper show (Section 7), however, the bene.ts of reducing the number of \noperating system threads and carefully managing their use provides better behavior even if the operating \nsystem is unaware of our user-level schedulers. Using a two-level scheduling mechanism such as scheduler \nac\u00adtivations could provide user-level Lithe schedulers with even more information about their running \ncomputations, such as when con\u00adtexts block due to page faults, or when a hart has been preempted. In \naddition, the operating system could notify schedulers when their computations block on I/O. In lieu \nof such support, we can use the existing event-based interfaces in many operating system s to avoid blocking \na hart on I/O (similar to previous work such as Capric\u00adcio [40]). In fact, in Section 6.3 we present \nthe libprocess library which provides a blocking I/O interface that pauses the underly\u00ading context and \nuses event-based mechanisms from operating sys\u00adtems to perform I/O. This allows the hart to execute other \ncompu\u00adtations/contexts while the previous computation/context is waiting on I/O.  6. Interoperable \nParallel Libraries We have built a wide range of interoperable scheduling, I/O, and synchronization libraries \nthat use Lithe. We have ported some ex\u00adisting popular libraries, as well as implemented our own. In this \nsection, we describe the original baseline for each of these libraries, then show how to modify them \nto use Lithe (and thus, work with each other).   Table 2. Approximate lines of code required to port \nTBB and OpenMP.  Added Removed Modi.ed Relevant Total   TBB 180 5 70 1,500 8,000 OpenMP 220 35 150 \n1,000 6,000 6.1 Threading Building Blocks Intel s Threading Building Blocks (TBB) library [34] provides \na high-level task abstraction to help programmers achieve perfor\u00admance and scalability without having \nto manually manage tasks parallel execution. Instead, a TBB scheduler uses a dynamic work\u00adstealing [4] \napproach to automatically map tasks onto cores. The original open-source Linux implementation of the \nTBB library uses pthreads as its resource abstraction. The TBB library creates a .xed-size global pool \nof threads upon its initialization, and associates a worker with each of these threads. Each TBB scheduler \nuses N workers from the global pool to execute its tasks, with N either being the number of cores in \nthe machine or the number speci.ed by the user. Each of these workers repeatedly completes its own share \nof work, then tries to steal more work from other workers. The original TBB library tries to prevent \nresource oversubscription by sharing the global .xed-size pool of threads between all the TBB schedulers \nwithin an application. Of course, this still does not preclude TBB schedulers from interfering with other \nnon-TBB parallel libraries. In the ported implementation of TBB, each TBB scheduler manages harts rather \nthan threads. Since a TBB scheduler does not know how many harts it can use upfront, it instead lazily \ncreates its workers for each hart that it obtains through its enter callback. The core algorithm for \neach worker remains the same, since each worker can just start stealing work from existing workers as \nsoon as it is instantiated. The ported implementation also keeps track of its child schedulers and the \ncorresponding number of harts they requested. In this initial implementation, harts are granted to children \nin a round-robin fashion only after there are no tasks left to steal and/or execute. Porting TBB was \na relatively straightforward task. It took one of the authors no more than a week. The .rst row of Table \n2 shows the approximate number of lines of code that were added, removed, and modi.ed. The column labeled \nRelevant refers to the number of lines of the TBB runtime that actually do scheduling (as opposed to \nscalable memory allocation, for example) in contrast to the total number of lines (last column). 6.2 \nOpenMP OpenMP [5] is a portable parallel programming interface contain\u00ading compiler directives for C, \nC++, and Fortran. Programmers use the directives to specify parallel regions in their source code, which \nare then compiled into calls into the OpenMP runtime library. Each parallel region is executed by a team \nof workers, each with a unique id. The number of workers in a team is speci.ed by the user, but defaults \nto the number of cores in the machine. Although there are higher level directives that allow the compiler \nto automatically split work within a parallel region across workers, the programmer can also manually \nassign work to each of worker by writing SPMD\u00adstyle code. We ported the GNU Compiler Collection s (GCC) \nopen-source Linux implementation of the OpenMP runtime library (libgomp). As with the original TBB library \nimplementation, the original OpenMP library also uses threads as its resource abstraction. An implicit \nscheduler for every team maps each of its workers onto a different thread. For ef.ciency, the library \nkeeps a global pool of the available threads to reuse across multiple teams. Note that when multiple \nteams are active, a team scheduler will create addi\u00adtional threads rather than waiting for threads from \nother teams to return to the global pool. In the ported version of OpenMP, each team scheduler multi\u00adplexes \nits workers onto its available harts. An OpenMP scheduler lets each worker run on a hart until it either \ncompletes or its con\u00adtext blocks on a synchronization operation, before running the next worker thread \non that hart. In this initial implementation, a sched\u00aduler cannot take advantage of additional harts \nin the middle of exe\u00adcuting a parallel region, since it never redistributes its workers once it assigns \nthem to their respective harts. However, the implementa\u00adtion is optimized to keep a global pool of contexts \nto reuse across multiple teams for ef.ciency. Porting OpenMP was more dif.cult than TBB because it re\u00adquired \ncarefully multiplexing the worker threads, since they are ex\u00adposed to the application programmer. It \ntook one of the authors ap\u00adproximately one week to port OpenMP, and another week to pro.le and optimize \nthe implementation to preserve cache af.nity for each worker thread. The second row of Table 2 shows \nthe approximate number of lines of code that were added, removed, and modi.ed in the OpenMP library. \n 6.3 Libprocess Libprocess [17] is a library written in C/C++ that provides an actor\u00adstyle message-passing \nprogramming model. Libprocess is very similar to Erlang s process model, including basic constructs for \nsending and receiving messages. Libprocess scales to hundreds of thousands of concurrent processes which \nare scheduled using plug\u00adgable scheduling policies. In fact, scheduling policies can be im\u00adplemented \ndirectly by a client of the library, to precisely control a process s ordering, priority, etc. In addition \nto the actor model, libprocess provides a collection of blocking I/O routines for using sockets (accept, \nconnect, recv, send). Similar to Capriccio, libprocess exploits non\u00adblocking operating system interfaces. \nThis allows libprocess to run a large number of processes that are all performing socket I/O si\u00admultaneously. \nBy using Lithe, libprocess is able to simultaneously run multiple processes across multiple harts. This \nis one of libpro\u00adcesses important improvements over Capriccio. The libprocess implementation is roughly \n2,000 lines of code. 6.4 Barriers In order to experiment with how synchronization primitives inter\u00adact \nwith parallel libraries, we implemented some simple user-level barriers with and without the Lithe substrate. \nBoth implementations share the bulk of the code. A barrier is initialized with the number of threads \nthat are synchronizing with each other. Upon reaching the barrier, each task increments a counter atomically, \nthen checks to see if it was the last to arrive. If so, it signals all other tasks to pro\u00adceed. Without \nLithe, all other tasks simply spin on their respective signals until they can proceed. With Lithe, all \nother tasks spin for a very short amount of time, then pause their contexts and yield con\u00adtrol of their \nhart back to the current scheduler using ctx block; the last task to reach the barrier will alert the \nscheduler to un\u00adblock any blocked contexts using ctx unblock. The scheduler can then decide when to resume \neach of these resumable contexts.  7. Evaluation In this section, we present results from experiments \non commodity hardware that show the effectiveness of the Lithe substrate. First, we present numbers for \nthe individual libraries that show that we can implement existing abstractions with no measurable overhead. \nThen, we present two real-world case studies to illustrate how we improve the current state-of-the-art \nin parallel library composition. Lastly, we show how Lithe can improve the performance of barrier synchronization \nin the face of parallel composition.   tree sum preorder .bonacci Original TBB 1.44 1.61 1.65 Ported \nTBB 1.23 1.40 1.45 Table 3. TBB benchmark runs (in seconds). cgft is Original OMP 0.22 0.19 0.54 Ported \nOMP 0.21 0.13 0.53 Table 4. OpenMP benchmark runs (in seconds). The hardware platform used was a quad-socket \n2.3 GHz AMD Opteron with 4 cores per socket (16 cores total). The OS platform used was the 2.6.18 64-bit \nLinux kernel (which includes the NPTL fast pthread library). We also used Glibc 2.3.6, TBB version 2.1 \nupdate 2, and GNU OpenMP from the GCC 4.4 trunk. 7.1 Ported Libraries To validate that porting libraries \nto Lithe incurs negligible or no overhead, we ran a series of benchmarks for both OpenMP and TBB to compare \nthe performance between the original and ported implementations. The TBB distribution includes a series \nof examples designed to highlight the performance and scalability of TBB. We randomly chose three of \nthese examples: a parallel tree summing algorithm, a parallel preorder tree traversal, and a parallel \nimplementation of .bonacci. Table 3 shows the average runtime of ten runs of these benchmarks for both \nthe ported and original version. Across all three examples, the ported TBB performs slightly better than \nthe original. We believe that because harts are created eagerly during the application initialization \n(similar to a thread pool), we are able to overlap thread creation with computation that occurs before \nTBB is actually invoked. To measure the overheads of the OpenMP port, we chose to run three of the NAS \nparallel benchmarks [21]: conjugate gradient, fast Fourier transform, and integer sort (from NPB version \n3.3; problem size W). Table 4 shows the average runtimes for ten runs of these benchmarks for both the \nported and original versions of OpenMP. The ported OpenMP is also competitive with, if not better than, \nthe original implementation. 7.2 Case Study 1: Application Server In our .rst case study, we investigated \nthe ef.cacy of Lithe for server-like applications that tightly integrate I/O with parallel com\u00adputation. \nMost servers use thread-per-connection parallelism be\u00adcause threads provide both (a) a natural abstraction \nof the control\u00ad.ow required to process a request and prepare a response (espe\u00adcially with respect to \nblocking while reading and writing to/from the network) and (b) a natural form of parallelism for execution \non a multicore machine. Newer application servers serve dynamic content and may potentially exploit parallel \ncomputations within a single request in addition to thread-per-connection parallelism. Examples include \nvideo encoding [44], speech recognition [15], video production [3], and online gaming (rendering, physics, \nand AI) [9]. We chose to study an image processing application server, mod\u00adeled after Flickr s [11] user \nimage upload server. The software ar\u00adchitecture of our application server is straightforward: for each \nim\u00adage the server receives, it performs .ve resize operations, creating large, medium, small, thumbnail, \nand square versions of the image. Like Flickr, we use the popular GraphicsMagick [16] library, which \nis parallelized using OpenMP, to resize images. To collect our performance numbers, we ran the application \nserver using the quad-socket Opteron, and ran the client on a sep\u00adarate machine connected by an Ethernet \nlink. We measured the Figure 5. Throughput versus latency for different con.gurations of image resizing \napplication server. throughput versus service latency of each con.guration as we in\u00adcreased the rate \nat which the client made requests. Because GraphicsMagick is parallelized using OpenMP, we can explore \na spectrum of possible con.gurations. We manually tuned GraphicsMagick by con.guring OpenMP to use N \nthreads for each invocation (labeled OpenMP = N ), and show results in Figure 5. The most straightforward \napproach to implementing our application server is to create a thread-per-connection that performs each \nof the resizes sequentially ( OpenMP = 1 ). Although this version performs well at high load (point (a)), \nit has high latency when lightly loaded and many cores sit idle causing the server to be underutilized. \nManually increasing the number of threads (N> 1) allocated to each request helps reduce lightly loaded \nlatency, but also reduces the saturation throughput (e.g. point (e) to point (b)). In general, the more \nthreads used to reduce lightly loaded latency, the greater the loss of saturation throughput. Next, we \nimplemented the application server using the libpro\u00adcess library rather than threads. Only the code that \nsets up either an actor or a thread per connection was different between the two versions. Most source \ncode was unchanged, including the code that read and wrote from sockets and .les, and the code that called \ninto GraphicsMagick. While we used the same GraphicsMagick library in both versions, we linked in the \nLithe-compliant OpenMP with the libprocess version. We implemented a very basic fair-sharing scheduling \npolicy for use by libprocess, where all concurrent in\u00advocations of GraphicsMagick receive roughly the \nsame number of harts. The line labeled libprocess in Figure 5 shows the perfor\u00admance of this implementation. \nClearly, the libprocess implemen\u00adtation dominates the convex hull of all the manually tuned variants \nacross the range of offered throughputs (points (f), (c), (d)), provid\u00ading lower latency at each load \npoint until the machine saturates and each request receives only a single hart, at which point performance \nis the same as the OpenMP = 1 case. The workload used in testing our application servers was very homogeneous, \nfavoring the manual static tuning used in the non\u00adlibprocess variants. With a more heterogeneous mix \nof image sizes, image operations (not just resize), and network speeds, a far greater degree of dynamic \nscheduling across the two levels of parallelism might be required and we would expect a greater advantage \nwhen using Lithe.  7.3 Case Study 2: Sparse QR Factorization As our second case study, we used an algorithm \nfor sparse QR fac\u00adtorization (SPQR) developed by Davis [7], which is commonly  18 6017 50 16 40 30 \n15 20 . MIN(5,3)12.02 sec 10 14 LITHE 0 10.41sec 13 4 8 8 4 12 12 OpenMP 16 16 TBB Figure 6. Original \nSPQR performance across different thread allo\u00adcations to TBB and OpenMP for the deltaX input matrix (runtime \nin seconds). The performance of the Lithe version is shown as a plane to compare against all original \ncon.gurations. 40 4 35 30 258 20 15 12 10 5 16 16128 4 TBB Figure 7. Average number of active SPQR threads \nacross different thread allocations to TBB and OpenMP for the deltaX input matrix. used in the linear \nleast-squares method for solving a variety of problems arising from geodetic survey, photogrammetry, \ntomogra\u00adphy, structural analysis, surface .tting, and numerical optimization. The algorithm can be viewed \nas a task tree, where each task performs several parallel linear algebra matrix operations, and peer \ntasks can be executed in parallel. Near the root of the tree, there is generally very little task parallelism, \nbut each task operates on a large matrix that can be easily parallelized. Near the leaves of the tree, \nhowever, there is a substantial task parallelism but each task operates on a small matrix. Original Out-of-the-Box \nImplementation. Davis imple\u00admented his algorithm using TBB to create the parallel tasks, each of which \nthen calls parallel BLAS (basic linear algebra subprogram) matrix routines [24] from MKL. MKL, in turn, \nuses OpenMP to parallelize itself. Unfortunately, although the two types of paral\u00adlelism should be complementary, \nTBB and MKL compete counter\u00adproductively with each other for resources. On an N-core machine, TBB will \ntry to run up to N tasks in parallel, and each of the tasks will call MKL, which will try to operate \non N blocks of a matrix in parallel. This potentially creates N2 linear algebra operations, OpenMP x \n108 3 x 108 2.9 3.5 2.8 3 2.7 2.5 2.6 2 2.5 LITHE 2.28\u00b7108sec 4 2.4 84 2.3 8 12 12 2.2 OpenMP16 16 TBB \nFigure 8. Number of L2 data cache misses of the original SPQR for the deltaX input matrix, compared with \nthe Lithe version. 5 x 10 14 12 5 x 10 10 15 10 8 5 6 0 -5 4 4 4 LITHE 2.21\u00b7104 8 2 8 12 12 OpenMP 16 \n16 TBB Figure 9. Number of context switches of the original SPQR for the deltaX input matrix, compared \nwith the Lithe version. each of which were carefully crafted by MKL to .t perfectly in the cache, but \nare now being multiplexed by the operating system and interfering with one another. Original Manually \nTuned Implementation. To curtail re\u00adsource oversubscription, Davis manually limits the number of OS threads \nthat can be created by each library, effectively partitioning machine resources between the libraries. \nThe optimal con.guration depends on the size of the input matrix, the threading behavior of both libraries, \nthe BLAS version in MKL, and the available hard\u00adware resources. To .nd the optimal con.guration, Davis \nhad to run every combination of thread allocations for TBB and OpenMP with each input matrix on each \nof his target machines. We reproduced Davis manual tuning runs for all four original input matrices on \nour own machine (see Table 5 for a descrip\u00adtion of the input matrices). Figure 6 shows the performance \nof a representative input matrix across all the different thread con.gu\u00adrations. The out-of-the-box con.guration \n(OMP=16, TBB=16) is where each library creates the default number of threads, which is equal to the number \nof cores on the machine. Although the out-of\u00adthe-box con.guration is better than the sequential version \n(OMP=1, landmark deltaX ESOC Rucci1 Size 71,952 x 2,704 68,600 x 21,961 327,062 x 37,830 1,977,885 x \n109,900 Nonzeros 1,146,868 247,424 6,019,939 7,791,168 Domain surveying computer graphics orbit estimates \nill-conditioned least-square Table 5. SPQR matrix workload characteristics. landmark deltaX ESOC Rucci1 \nSeq OMP / Seq TBB OMP=1, TBB=1 7.8 55.1 230.4 1352.7 Par OMP / Seq TBB OMP=16, TBB=1 5.8 19.4 106.9 448.8 \nSeq OMP / Par TBB OMP=1, TBB=16 3.1 16.8 78.7 585.8 Out-of-the-Box OMP=16, TBB=16 3.2 15.4 73.4 271.7 \nManually Tuned OMP=5, TBB=3 OMP=5, TBB=16 OMP=8, TBB=11 OMP=8, TBB=16 2.9 12.0 61.1 265.6 Lithe 2.7 10.4 \n60.4 248.3 Table 6. SPQR (TBB/OpenMP) performance with and without Lithe (runtime in seconds). TBB=1), \nit is not close to optimal. Many of the con.gurations that limit resource oversubscription achieve better \nperformance than the out-of-the-box con.guration. We refer to the con.guration with the shortest run \ntime as the manually tuned version (OMP=5, TBB=3 for this particular input). The top portion of Table \n6 shows the performance of noteworthy con.gurations for all the input matrices. For all of the inputs, \nthe out-of-the-box con.guration performs much worse than the manu\u00adally tuned con.guration. The table \nincludes the special cases where one of the libraries gets all of the resources (OMP=1, TBB=16 and OMP=16, \nTBB=1). Giving OpenMP all the resources is subopti\u00admal because the task-level parallelism is much more \nscalable than matrix-level parallelism. However, giving TBB all the resources is also suboptimal because \nthere are parts of the computation with no task-level parallelism but lots of matrix-level parallelism. \nTo obtain a better understanding of the original performance, we measured active threads, cache miss, \nand context switching behav\u00adior for all input matrices. The results for the deltaX input are shown in \nFigures 7, 8, and 9. As expected, Figure 7 shows that the average number of threads for the out-of-the-box \ncon.guration was more than double the number of cores in the machine. Figure 8 shows the L2 data cache \nmisses, which increase as the number of threads given to OpenMP/TBB increase. Figure 9 shows the number \nof OS thread context switches that occur during the run. The num\u00adber of context switches increases as \nthe number of threads given to OpenMP increases. However, for any .xed OpenMP con.gura\u00adtion, increasing \nthe number of threads given to TBB decreases the number of total context switches, since the duration \nof the run dra\u00admatically decreases. We hypothesize that the performance anomaly for the OMP=2, TBB=14,15 \ncon.gurations may be because the ef\u00adfects of the OS not co-scheduling worker threads from the same OpenMP \nparallel region are more pronounced when each thread depends on exactly one other thread, and there is \na large, irregular number of threads for the OS scheduler to cycle through. Ported Implementation. We \nrelinked the SPQR application with the modi.ed TBB and OpenMP libraries to run with Lithe. We did not \nhave to change a single line of Davis original code, since the TBB and OpenMP interfaces remained the \nsame. The bottom portion of Table 6 shows the performance results of using Lithe. This implementation \neven outperforms the manually tuned con.guration, because Lithe enables the harts to be more .exibly \nshared between MKL and TBB, and adapt to the different amounts of task and matrix-level parallelism throughout \nthe computation. landmark deltaX ESOC Rucci1 Out-of-the-Box 9.53\u00d7106 3.04\u00d7108 1.36\u00d7109 5.47\u00d7109 Lithe \n8.23\u00d7106 2.28\u00d7108 1.11\u00d7109 5.19\u00d7109    Table 7. SPQR L2 Data Cache Misses. landmark deltaX ESOC Rucci1 \nOut-of-the-Box 1.03\u00d7105 1.04\u00d7106 3.40\u00d7106 6.08\u00d7106 Lithe 1.47\u00d7104 2.21\u00d7104 7.79\u00d7104 2.50\u00d7105    Table \n8. SPQR Context Switches. 5 # Units of Work in Parallel Table 7 shows that the Lithe implementation \nhas fewer L2 cache misses, and Table 8 shows that the Lithe implementation has orders of magnitude fewer \nOS thread context switches than the out-of-the\u00adbox con.guration across all input matrices.  7.4 Barrier \nSynchronization In this section, we study how synchronization and scheduling in\u00adteract in the face of \nparallel composition using a simple barrier ex\u00adample. The basic unit of work for our microbenchmark consists \nof 16 SPMD tasks synchronizing with each other 1000 times at back\u00adto-back barrier rendezvous (with no \nwork in between the barrier rendezvous). We launched between 1 and 10 of the basic units of work in parallel \nto crudely model, for instance, GraphicsMagick or MKL routines running in parallel, each using barriers \nto synchro\u00adnize internally.  We compared three implementations of barriers: the Glibc im\u00adplementation \nof the pthread barrier, and the two user-level barriers described in Section 6 (one that spins, and one \nthat cooperatively yields using Lithe). Because the implementation of the barrier de\u00adpends on the task \nmodel, the Lithe barrier runs with the Lithe im\u00adplementation of SPMD tasks (as described in Section 4.3), \nand both the pthread and spin barriers run with a pthread implementation of SPMD tasks (in which each \nSPMD task is simply a pthread). Fur\u00adthermore, there are two different versions of the pthread SPMD implementation, \none where each SPMD task is pinned to the core corresponding to its task ID, and one where the tasks \nare unpinned. The reported measurements re.ect the performance of the SPMD implementation as well as \nthe barrier implementation. First, we examine the scenario where only a single unit of work is launched. \nThe two versions of the pthread barrier perform comparably, with the unpinned version doing slightly \nbetter due to better load balancing. As expected, the unpinned spin version performs better than the \npthread versions. However, the pinned spin version performs worse than the pthread versions due to load \nimbalance. The Lithe version performs the best, because of the lower SPMD task creation cost, and because \nmost of the SPMD tasks can simply spin for a little bit instead of having to incur the state saving and \nrescheduling costs of going back into the scheduler. Next, we examine when two units of work are launched \nin parallel. The pthread versions improved because they can achieve better load balancing with more tasks \nthan cores in the machine. The spin versions slowed down by orders of magnitude because of destructive \ninterference; each of the SPMD tasks hog up the machine resources spin-waiting rather than letting the \nother tasks run. The Lithe version slowed down because it must go into the scheduler after spin-waiting \nfor a .nite amount of time; however, the Lithe version is still faster than the pthread versions since \nthe scheduling is done at user-level. Lastly, we examine the scalability of the barrier performance as \nmore and more units of work are launched in parallel. The unpinned pthread version achieves much better \nload balancing as the number of SPMD tasks increases. Lithe also achieves slightly better load balancing \nas the number of tasks increases, but only at the same slow rate of improvement as the pinned pthread \nversion (since the harts are essentially pinned pthreads). Since Lithe is a user\u00adlevel implementation, \nit is still at the mercy of the OS scheduler to schedule its harts.  8. Related Work Previous systems \nhave attempted to prevent resource oversubscrip\u00adtion when multiple applications interfere with one another. \nThese previous schemes fall into two main categories: OS spatial parti\u00adtions [8, 27, 29], and user-level \nadaptive heuristics [35, 38]. We be\u00adlieve our work will integrate well with the .rst category to form \na two-level scheduling ecosystem, where the OS uses minimal coarse-grained scheduling to mediate between \nmultiple applica\u00adtions and gives the hardware resources allocated in each spatial partition to the user-level \nschedulers of each application to man\u00adage directly. We believe explicit sharing of resources is preferable \nto the second category of heuristic approaches that require each en\u00adtity periodically query the overall \nsystem load to guess how many resources to use, as the latter does not allow prioritization between the \ndifferent entities, and can easily lead to system instability. Other systems have also attempted to prevent \nresource under\u00adutilization due to blocking of I/O (Scheduler Activations [2] and CPU Inheritance [13]) \nor synchronization (Psyche [28] and Con\u00adverse [22]). In contrast, our context primitive provides a single \nuni\u00adform mechanism to address both I/O and synchronization blocking, while not requiring changes to existing \nOS systems. This work builds on the well-known technique of expressing parallelism with continuations \n[42]. We improve on previous work by providing a transition stack, thus obviating the need for com\u00adplex \ncross-context synchronization during context-switching (e.g. [10, 12, 26]). In addition, we augment previous \nwork to support multiple parallel components, by transferring execution control to the appropriate scheduler, \nand ensuring that the components do not share stacks and trample on each other s execution state. Several \nother systems support multiple co-existing schedulers, but with varying motivations. However, all of \nthe previous work differ from us in one or more of the following ways: Converse [22] and CPU Inheritance \n[13] require that child schedulers register individual threads with a parent scheduler, effectively breaking \nmodularity and forcing the parent to man\u00adage individual threads on a child s behalf. The child only re\u00adceives \nharts implicitly when its parent decides to invoke these threads. Our parent schedulers grant harts to \na child to do with as it pleases.  Both GHC [26] and Manticore [12] are primarily motivated by the desire \nto design language primitives that enable customiz\u00adable scheduling, rather than enabling interoperability \nof mul\u00adtiple custom schedulers. Although both claim to support hier\u00adarchical nesting of schedulers, neither \ndescribe how multiple schedulers would interact. Furthermore, a global entity in Man\u00adticore [12] decides \nhow many virtual processors to allocate to each scheduler, preventing each scheduler from deciding how \nbest to allocate resources among its children.  Manticore [12] requires all schedulers use the same \nscheduling queue primitive, rather than enabling each scheduler to manage its own parallelism in a more \nef.cient code-speci.c manner.  Converse [22] does not de.ne a standard scheduler callback in\u00adterface, \nand thus does not support true plug-and-play interoper\u00adability. A child scheduler must know its parent \ns speci.c inter\u00adface in order to register its threads.  Unlike GHC [26] and Manticore [12], Lithe is \nlanguage\u00adagnostic. Unlike Psyche [28], CPU Inheritance [13], and HLS [33], we do not impose the adoption \nof a new OS.  The virtual processor abstraction of HLS [33] only interfaces between a single pair of \nparent-child schedulers. Thus, granting access to a physical processor down the hierarchy involves acti\u00advating \na different virtual processor at every generation. Further\u00admore, all scheduling activity are serialized, \nmaking HLS dif.\u00adcult to scale to many cores.  9. Future Work We are looking into extending this work \nin four main areas: Ports of Additional Language Features. In this paper, we ex\u00adplored how parallel abstractions \nfrom different languages and li\u00adbraries can be composed ef.ciently. We ported and created run\u00adtimes that \nsupport different styles of parallel abstractions, rang\u00ading from tasks to loops to actors. In the future, \nwe would also like to port managed and functional language runtimes onto Lithe, to explore how other \nlanguage features interact with scheduling and composition. For example, we imagine implementing a par\u00adallel \ngarbage collector as a child scheduler of its language runtime, which when given its own harts to manage \ncan decide how best to parallelize and execute memory reallocation operations while not interfering with \nthe main computation.  Preemptive Programming Models. The current system is de\u00adsigned to support the \nlarge set of applications that perform well with cooperative scheduling, where preemption is often expen\u00adsive \nand unnecessary [32]. For example, preemptive round-robin scheduling of threads within an application \ncan introduce gratu\u00aditous interference when the threads are at the same priority level. User-level schedulers \nshould favor ef.ciency over an arbitrary no\u00adtion of fairness. An application may, however, want to reorder \nits computation based on dynamic information from inputs, events, or performance measurements. For this \nclass of applications, we are looking into extending Lithe to enable a parent scheduler to ask for a \nhart back from its child. Kernel-Level Implementation of Primitives. Our current sys\u00adtem implements the \nhart and context primitives purely in user space. This enables parallel codes within an application to \ninter\u00adoperate ef.ciently without the requirement of adopting a new op\u00aderating system. Nonetheless, supporting \nthe hart and context ab\u00adstractions in the OS will provide additional bene.ts. While a user\u00adlevel hart \nimplementation can provide performance isolation be\u00adtween parallel codes within a single application, \na kernel-level hart implementation can provide further performance isolation between multiple applications. \nIn addition, a kernel-level context implemen\u00adtation would enable blocking I/O calls to interoperate seamlessly \nwith user-level schedulers (e.g. [2]). Management of Non-Processing Resources. Depending on the characteristics \nof the code and the machine, a library may be more memory and bandwidth-bound than compute-bound. By \npro\u00adviding a better resource abstraction for cores, we have already re\u00adduced the number of threads of \ncontrol that are obliviously mul\u00adtiplexed, thus implicitly reducing the pressure on the memory sys\u00adtem \nand network. However, to give libraries greater control over the machine, we are looking into providing \nprimitives beyond harts to represent other resources, such as on-chip cache capacity and off\u00adchip memory \nbandwidth. This may require hardware support for partitioning those resources (e.g. [20, 25]), beyond \nthat generally available in commercial machines today. 10. Conclusion In this paper, we have shown the \ndif.culties of composing parallel libraries ef.ciently. We have also argued that the management of re\u00adsources \nshould be coupled with the hierarchical transfer of control between libraries. Our solution, Lithe, is \na low-level substrate that allows libraries to cooperatively share processing resources without imposing \nany constraints on how the resources are used to imple\u00adment parallel abstractions. Using Lithe, we are \nable to implement multiple existing parallel abstractions with no measurable over\u00adhead, while enabling \nparallel libraries to be composed ef.ciently. We believe this capability is essential for parallel software \nto be\u00adcome commonplace. 11. Acknowledgements We would like to thank George Necula and the rest of Berkeley \nPar Lab for their continued feedback on this work. We d like to thank Tim Davis for providing us with \nour SPQR case study and answering all our questions about its complicated details. We would also like \nto thank Arch Robison and Greg Henry for their helpful feedback on our uses of TBB and MKL. We d like \nto thank Ali Ghodsi and Sam Larsen, as well as our anonymous reviewers, for their helpful comments for \nstrengthening our presentation. This research was supported by Microsoft (Award #024263) and Intel (Award \n#024894) funding and by matching funding by U.C. Discovery (Award #DIG07-10227). Additional support comes \nfrom Par Lab af.liates National Instruments, NEC, Nokia, NVIDIA, Samsung, and Sun Microsystems. The authors \nwould also like to acknowledge the support of the Gigascale Systems Research Focus Center, one of .ve \nresearch centers funded under the Focus Cen\u00adter Research Program, a Semiconductor Research Corporation \npro\u00adgram. Benjamin Hindman has in part been supported by a National Science Foundation Graduate Research \nFellowship. Any opinions, .ndings, conclusions, or recommendations expressed in this pub\u00adlication are \nthose of the authors and do not necessarily re.ect the views of the National Science Foundation.   \nReferences [1] Atul Adya et al. Cooperative task management without manual stack management. In USENIX, \n2002. [2] Thomas Anderson et al. Scheduler activations: Effective kernel sup\u00adport for the user-level \nmanagement of parallelism. In SOSP, 1991. [3] Animoto. http://www.animoto.com. [4] Robert Blumofe et \nal. Cilk: An ef.cient multithreaded runtime system. In PPOPP, 1995. [5] Rohit Chandra et al. Parallel \nProgramming in OpenMP. Morgan Kaufmann, 2001. [6] Jike Chong et al. Scalable hmm based inference engine \nin large vocabulary continuous speech recognition. In ICME, 2009. [7] Timothy Davis. Multifrontal multithreaded \nrank-revealing sparse QR factorization. Transactions on Mathematical Software, Submitted. [8] K. Dussa \net al. Dynamic partitioning in a Transputer environment. In SIGMETRICS, 1990. [9] EVE Online. http://www.eveonline.com. \n[10] Kathleen Fisher and John Reppy. Compiler support for lightweight concurrency. Technical report, \nBell Labs, 2002. [11] Flickr. http://www..ickr.com. [12] Matthew Fluet et al. A scheduling framework \nfor general-purpose parallel languages. In ICFP, 2008. [13] Bryan Ford and Sai Susarla. CPU inheritance \nscheduling. In OSDI, 1996. [14] Seth Copen Goldstein et al. Lazy threads: Implementing a fast parallel \ncall. Journal of Parallel and Distributed Computing, 1996. [15] Google Voice. http://voice.google.com. \n[16] GraphicsMagick. http://www.graphicsmagick.org. [17] Benjamin Hindman. Libprocess. http://www.eecs.berkeley.edu/ \nbenh/libprocess. [18] Parry Husbands and Katherine Yelick. Multithreading and one-sided communication \nin parallel lu factorization. In Supercomputing, 2007. [19] Intel. Math Kernel Library for the Linux \nOperating System: User s Guide. 2007. [20] Ravi Iyer. CQoS: A framework for enabling QoS in shared caches \nof CMP platforms. In ICS, 2004. [21] Haoqiang Ji et al. The OpenMP implementation of NAS parallel benchmarks \nand its performance. Technical report, NASA Ames Research Center, 1999. [22] Laxmikant V. Kale, Joshua \nYelon, and Timothy Knauff. Threads for interoperable parallel programming. Languages and Compilers for \nParallel Computing, 1996. [23] Jakub Kurzak et al. Scheduling linear algebra operations on multicore \nprocessors. Technical report, LAPACK, 2009. [24] C. L. Lawson et al. Basic linear algebra subprograms \nfor FORTRAN usage. Transactions on Mathematical Software, 1979. [25] Jae Lee et al. Globally-synchronized \nframes for guaranteed quality\u00adof-service in on-chip networks. In ISCA, 2008. [26] Peng Li et al. Lightweight \nconcurrency primitives. In Haskell, 2007. [27] Rose Liu et al. Tessellation: Space-time partitioning \nin a manycore client OS. In HotPar, 2009. [28] Brian Marsh et al. First-class user-level threads. OS \nReview, 1991. [29] Cathy McCann et al. A dynamic processor allocation policy for multiprogrammed shared-memory \nmultiprocessors. Transactions on Computer Systems, 1993. [30] Ana Lucia De Moura and Robert Ierusalimschy. \nRevisiting coroutines. Transactions on Programming Languages and Systems, 2009. [31] Rajesh Nishtala \nand Kathy Yelick. Optimizing collective communica\u00adtion on multicores. In HotPar, 2009.  [32] Simon Peter \net al. 30 seconds is not enough! a study of operating system timer usage. In Eurosys, 2008. [33] John \nRegehr. Using Hierarchical Scheduling to Support Soft Real-Time Applications in General-Purpose Operating \nSystems. PhD thesis, University of Virginia, 2001. [34] James Reinders. Intel Threading Building Blocks: \nOut.tting C++ for Multi-core Processor Parallelism. O Reilly, 2007. [35] Charles Severance and Richard \nEnbody. Comparing gang scheduling with dynamic space sharing on symmetric multiprocessors using au\u00adtomatic \nself-allocating threads. In IPPS, 1997. [36] Stackless Python. http://www.stackless.com. [37] Guangming \nTan et al. A parallel dynamic programming algorithm on a multi-core architecture. In SPAA, 2007. [38] \nAndrew Tucker and Anoop Gupta. Process control and scheduling issues for multiprogrammed shared-memory \nmultiprocessors. OS Review, 1989. [39] Dean Tullsen et al. Exploiting choice: Instruction fetch and issue \non an implementable simultaneous multithreading processor. In ISCA, 1996. [40] Rob von Behren et al. \nCapriccio: Scalable threads for internet services. In SOSP, 2003. [41] Carl Waldspurger and William Weihl. \nLottery scheduling: Flexible proportional-share resource management. In OSDI, 1994. [42] Mitchell Wand. \nContinuation-based multiprocessing. In LFP, 1980. [43] Samuel Williams et al. Optimization of sparse \nmatrix-vector multipli\u00adcation on emerging multicore platforms. In Supercomputing, 2007. [44] YouTube. \nhttp://www.youtube.com.  \n\t\t\t", "proc_id": "1806596", "abstract": "<p>Applications composed of multiple parallel libraries perform poorly when those libraries interfere with one another by obliviously using the same physical cores, leading to destructive resource oversubscription. This paper presents the design and implementation of <i>Lithe</i>, a low-level substrate that provides the basic primitives and a standard interface for composing parallel codes efficiently. Lithe can be inserted underneath the runtimes of legacy parallel libraries to provide <i>bolt-on</i> composability without needing to change existing application code. Lithe can also serve as the foundation for building new parallel abstractions and libraries that automatically interoperate with one another.</p> <p>In this paper, we show versions of Threading Building Blocks (TBB) and OpenMP perform competitively with their original implementations when ported to Lithe. Furthermore, for two applications composed of multiple parallel libraries, we show that leveraging our substrate outperforms their original, even expertly tuned, implementations.</p>", "authors": [{"name": "Heidi Pan", "author_profile_id": "81100484557", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA, USA", "person_id": "P2184596", "email_address": "", "orcid_id": ""}, {"name": "Benjamin Hindman", "author_profile_id": "81319493288", "affiliation": "University of California, Berkeley, Berkeley, CA, USA", "person_id": "P2184597", "email_address": "", "orcid_id": ""}, {"name": "Krste Asanovi&#263;", "author_profile_id": "81100473103", "affiliation": "University of California, Berkeley, Berkeley, CA, USA", "person_id": "P2184598", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1806596.1806639", "year": "2010", "article_id": "1806639", "conference": "PLDI", "title": "Composing parallel software efficiently with lithe", "url": "http://dl.acm.org/citation.cfm?id=1806639"}