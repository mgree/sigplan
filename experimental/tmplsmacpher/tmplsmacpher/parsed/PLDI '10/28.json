{"article_publication_date": "06-05-2010", "fulltext": "\n Line-Up: A Complete and Automatic Linearizability Checker Sebastian Burckhardt Chris Dern Madanlal \nMusuvathi Microsoft Research Microsoft Microsoft Research sburckha@microsoft.com chrisd@microsoft.com \nmadanm@microsoft.com Roy Tan Microsoft roytan@microsoft.com Abstract Modular development of concurrent \napplications requires thread\u00adsafe components that behave correctly when called concurrently by multiple \nclient threads. This paper focuses on linearizability, a spe\u00adci.c formalization of thread safety, where \nall operations of a con\u00adcurrent component appear to take effect instantaneously at some point between \ntheir call and return. The key insight of this paper is that if a component is intended to be deterministic, \nthen it is possi\u00adble to build an automatic linearizability checker by systematically enumerating the \nsequential behaviors of the component and then checking if each its concurrent behavior is equivalent \nto some se\u00adquential behavior. We develop this insight into a tool called Line-Up, the .rst com\u00adplete \nand automatic checker for deterministic linearizability. It is complete, because any reported violation \nproves that the implemen\u00adtation is not linearizable with respect to any sequential determinis\u00adtic speci.cation. \nIt is automatic, requiring no manual abstraction, no manual speci.cation of semantics or commit points, \nno manu\u00adally written test suites, no access to source code. We evaluate Line-Up by analyzing 13 classes \nwith a total of 90 methods in two versions of the .NET Framework 4.0. The viola\u00adtions of deterministic \nlinearizability reported by Line-Up exposed seven errors in the implementation that were .xed by the \ndevelop\u00adment team. Categories and Subject Descriptors D[2]: 4 General Terms Algorithms, Reliability, \nVeri.cation Keywords Linearizability, Atomicity, Thread Safety 1. Introduction Concurrent programming \nis becoming more prevalent as Moore s law pays fewer dividends for sequential applications. To achieve \nmodular development, programmers face the question of how to build and test thread-safe components, that \nis, components that function correctly in a concurrent environment without placing undue synchronization \nburden on the caller. Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. PLDI 10, June 5 10, 2010, Toronto, Ontario, Canada. Copyright c &#38;#169; 2010 ACM 978-1-4503-0019-3/10/06. \n. . $10.00 The most well-known examples of thread-safe components are concurrent data types such as \nqueues or sets, which are provided by many major concurrency libraries (java.util.concurrent, Intel Threading \nBuilding Blocks, and Microsoft .NET 4.0). Such li\u00adbraries can simplify the development of thread-safe \ncode, but are themselves dif.cult to develop and test. Also, because such li\u00adbraries cannot cover all \nscenarios, we expect that a growing num\u00adber of programmers will develop concurrent components that are \ntailored to their applications. Thread-safety is a vaguely de.ned term; more speci.c correct\u00adness conditions \ninclude data-race-freedom [11, 17, 24], serializabil\u00adity [23] (sometimes called atomicity [10, 12]), \nlinearizability [15], or sequential consistency [16]. Over the past two decades, research on concurrent \ndata types con.rmed that .ne-grained concurrency is dif.cult to get right (by discovering bugs in published \nalgorithms [6, 18]) and researchers have focused on linearizability as a stan\u00addard for correctness [14, \n26]. A concurrent component is linearizable if its operations, when called concurrently, appear to take \neffect instantaneously at some point between their call and return. As a result, one can understand the \nconcurrent behavior of a linearizable component simply by un\u00adderstanding its sequential behavior. One \nway to design a lineariz\u00adable component is to protect all instructions in an operation by a single lock \nin the component. For performance reasons, many con\u00adcurrent components, in practice, use more sophisticated \nlock-free synchronization to guarantee linearizability. For such implementations, correctness is subtle \nenough to war\u00adrant manual proofs of linearizability. Unfortunately, such proofs are labor-intensive, \ndif.cult to apply to detailed production code, and require formal training. On the other hand, promising \nexperiences with model checking [2, 27] suggest that a focus on falsi.cation and precise counterexamples \nis a viable alternative, and may reach a wider audience by virtue of better automation and a more produc\u00adtive \nuser experience. The key challenge for an automatic linearizability checker is that the linearizability \nof a concurrent component is de.ned with respect to a sequential speci.cation. Asking the user to provide \nsuch a speci.cation would depart unacceptably from our goal of automation. Not only are most users unfamiliar \nwith formal spec\u00adi.cations, but the act of writing a speci.cation is labor-intensive even for experts. \nOur insight is that if the sequential speci.cation is deterministic, it is possible to automatically \ngenerate the speci.cation by system\u00adatically enumerating all sequential behaviors of the component. We \nuse this insight to build a tool called Line-Up that can .nd viola\u00adtions of deterministic linearizability \nautomatically, without requir\u00ading knowledge of the implementation, source annotations, or even source \ncode. Line-Up uses a stateless model checker that runs the same implementation both sequentially and \nconcurrently, and checks the consistency of the observations. Line-Up is complete: Any detected violation \nof deterministic linearizability is a conclusive proof that the implementation is not linearizable with \nrespect to any deterministic speci.cation. On the other hand, Line-Up (like all dynamic checking tools) \nis sound only with respect to the inputs and the executions tested. As a second contribution, we tightened \nthe original de.nition of linearizability to additionally detect erroneous blocking of imple\u00admentations. \nThis improves the coverage of our method as it allows us to .nd liveness errors in implementations. We \nalso describe an adaptation of our algorithm that uses random sampling to .nd bugs more quickly. We evaluate \nLine-Up on production code, speci.cally the con\u00adcurrent data types that ship with Microsoft s .NET Framework \n4.0, and that were made available in two prereleases (community tech\u00adnology preview, and beta release). \nIn the course of our project, we applied Line-Up to 13 classes with a total of 90 methods. We found 12 \nroot causes for violations of deterministic linearizabil\u00adity. Of those, 7 were caused by real implementation \nerrors. The other 5 revealed behavior that was intentionally non-linearizable or nondeterministic. Considering \nthat we tested 90 methods, non\u00addeterminism was rather rare (somewhat contrary to the prominent use of \nnondeterministic speci.cations in the research literature). In some cases the developers realized that \na method is nondetermin\u00adistic only after the fact was detected by Line-Up, and updated the documentation. \nTo test our choice of linearizability as the appropriate notion for thread safety, we evaluated Line-Up \nwith other correctness check\u00aders such as data-race detection and con.ict serializability. Our ex\u00adperiments \nrevealed that these were not well suited for this appli\u00adcation: data-race detection was ineffective because \nthe code con\u00adtained only benign data races (due to a disciplined use of volatile quali.ers and interlocked \noperations), while con.ict-serializability checking produced a discouraging number of false alarms. The \nim\u00adplementations we studied contained a large variety of programming patterns that violate con.ict serializability, \nbut are nevertheless cor\u00adrect. 1.1 Example To illustrate how Line-Up operates, consider a situation \nwhere we would like to test a concurrent queue using a black-box approach (say, we do not have access \nto the source code). First, we specify a set of method calls that we would like to test. For instance, \nafter inspecting the interface of the queue, we might consider testing with two methods that add different \nvalues to the queue and one method that removes a value: {queue.Add(200), queue.Add(400), queue.TryTake()}. \nThis is the only manual step required when using Line-Up. Line-Up then automatically enumerates concurrent \nand sequential com\u00adbinations of these operations. If it .nds a violation of determinis\u00adtic linearizability, \nit reports a small concurrent test scenario such as shown in Fig. 1. In fact, this example was a violation \nof de\u00adterministic linearizability that exposed a real bug in the .NET 4.0 community technology preview \n[19]. A client of the queue imple\u00admentation would expect a TryTake to fail only when the queue is empty. \nHowever, the buggy behavior shown in Figure 1 was caused by accidentally allowing a lock acquire in TryTake \nto time out. Note that the problem in this example can be intuitively un\u00adderstood without referring to \nimplementation details, and without knowing the formal de.nition of linearizability. This is an impor-Thread \n1 Thread 2 queue.Add (200); queue.Add(400); queue.TryTake() . returns 200; queue.TryTake(); . FAILS \nFigure 1. A buggy queue implementation that violates lineariz\u00adability. In the intended behavior of a \nqueue, both TryTake opera\u00adtions should succeed, even though they can return different values depending \non the interleaving of these operations. Line-Up is able to automatically detect this violation in the \nexample above. tant advantage when trying to convince developers that their imple\u00admentation has a bug. \n 1.2 Related Work To achieve full veri.cation of concurrent objects, researchers con\u00adstruct linearizability \nproofs, using rely-guarantee reasoning [25, 26] or simulation relations (using both forward-and backward\u00adsimulations) \n[4]. These proof methods are interactive, not auto\u00admatic, and require sequential speci.cations. In addition, \nmany of these techniques require the user to specify linearization points, which are points in the implementation \nwhere the operations ap\u00adpear to instantaneously take effect. The location of these lineariza\u00adtion point \nmay depend on conditions that are not statically known, requiring nontrivial annotations [27]. Even worse, \nthose conditions may depend on future events, thus requiring the use of prophecy variables [1] or backward \nsimulation relations [4]. Line-Up avoids these annotations and instead considers all possible linearization \npoints, following the original de.nition of linearizability [15]. Model checking is more automatic and \nhas the advantage of producing counterexamples, but usually veri.es only bounded ex\u00adecutions or .nite-state \nmodels. We know of two efforts in this area, but neither achieves the same automation as Line-Up or applies \nthe method to real production code: (1) CheckFence [2] uses a two\u00adphase check similar to ours, but does \nnot check linearizability and requires the user to write a test suite manually. (2) An experience report \nwith the model checker SPIN [27] brie.y mentions an au\u00adtomatic procedure, but focuses mainly on manually \nannotated lin\u00adearization points; it also appears to operate on .nite-state abstract models rather than \non full-featured code. Prior work on runtime re.nement checking [8, 9] is closely related to linearizability \nchecking, and does scale to production code. However, it is less automatic than Line-Up as it requires \nthe user to annotate linearization points and to construct the test scenarios. Another similar approach \n[13] executes random tests to produce logs, then checks those logs for linearizability using a greedy \nsearch (thus avoiding linearization points annotations). The tests are not driven systematically by a \nmodel checker, however, and the abstract state is modelled manually. 1.3 Contributions In the remainder \nof this paper, we elaborate on our contributions in the following order: 1. We present both the original \nde.nition of linearizability as well as a tightened version that can detect erroneous blocking of implementations \n(Section 2). 2. We present an algorithm that can automatically check whether an implementation is linearizable \nwith respect to any determin\u00adistic sequential speci.cation (Section 3). 3. We describe how we implemented \nour algorithm on top of a stateless model checker, and how we incorporated random sampling (Section 4). \n  4. We demonstrate how our tool found seven real bugs in produc\u00adtion code, and compare its effectiveness \nwith dynamic race de\u00adtection and atomicity checking (Section 5). Our results show that automatic checking \nof deterministic lineariz\u00adability with Line-Up provides an effective way of catching concur\u00adrency bugs \nwithout producing large numbers of false alarms. 2. Formulation In this section, we begin with a formal \nde.nition of linearizability, closely following the original [15] with the exception of a few minor adaptations1 \n(Section 2.1). We then elaborate on how to compare implementations to speci\u00ad.cations using classic linearizability, \nand why the latter can not de\u00adtect erroneous blocking of the implementation (Section 2.2). Next, we introduce \na generalized de.nition of linearizability that can check the blocking behaviors of the implementation \nagainst the blocking behaviors of the speci.cation (Section 2.3). Finally, we formally de.ne deterministic \nlinearizability, the property that Line-Up is checking (Section 2.4). 2.1 Linearizability For the purposes \nof this paper, the system is modelled as a set T = {A, B, C, . . . } of sequential threads and a set \nO = {o,p,q ... }of objects. Formally, these sets are static, but dynamic thread and object creation can \neasily be modelled. Each object has a type that de.nes a set of primitive operations, and these operations \nare the only means of reading or modifying the state of the object. We represent the operations on an \nobject o . O by two disjoint sets Io and Ro, where Io is the set of invocations (which include the operation \nname and arguments) and Ro is the set of responses (which may include return values). For example, consider \na concurrent counter object c supporting operations to increment, decrement, set, or get the current \ncount. In this case, we de.ne the invocation and response sets as follows: Ic = {inc, dec, get}.{set(x) \n| x . N} Rc = {ok}.{ok(x) | x . N} 2.1.1 Histories An execution of the system is represented as a history, \nwhich is de.ned to be a .nite sequence of events, where an event is de.ned as a tuple (oat) where o . \nO is an object, a . Io . Ro is an invocation or response, and t . T is a thread. A subhistory of a history \nH is a subsequence of the events of H. We call an event (oat) a call or a return depending on whether \na . Io or a . Ro. '' A return (oat) is said to match a call (oat') if o = o' and t = t'. Within a history \nH, a call is pending if it is not followed by a matching return. A history H is complete if it contains \nno pending calls. For a history H, we de.ne complete(H) to be the history obtained from H by deleting \nall pending calls. A single\u00adobject history is a history all of whose events are associated with the same \nobject. For a thread t, we de.ne the thread subhistory H|t to be the subhistory of H consisting of all \nevents associated with t. A history H is serial if (1) the sequence (if not empty) starts with a call \nevent, (2) calls and returns alternate in the sequence, and (3) each return matches the immediately preceding \ncall. A history H is well\u00adformed if the thread subhistory H|t is serial for each thread t. All histories \nconsidered in this paper are assumed to be well-formed. See Fig. 2 for an example. 1 We renamed processes \nas threads , renamed sequential histories to serial histories, renamed invocation and response events \nto call and return events, and decomposed the linearizability de.nition using the notion of a serial \nwitness. H H|A H|B (c set(0) A) (c get B) (c ok A) (c inc A) (c ok(0) B) (c get B) (c ok(1) B) (c set(0) \nA) (c ok A) (c inc A) (c get B) (c ok(0) B) (c get B) (c ok(1) B)  Figure 2. An example of a well-formed \nsingle-object history H (on the left) and two of its thread subhistories (on the right.) Figure 3. Speci.cation \nautomaton for the counter object.  2.1.2 Sequential Speci.cations A set Y of histories is pre.x-closed \nif, whenever H is in Y , every pre.x of H is also in Y .A sequential speci.cation for an object is a \npre.x-closed set of single-object serial histories for that object. It can be convenient and illustrative \nto think of Y as the set of traces generated by some suitably de.ned speci.cation automaton, such as \nshown in Fig. 3. A sequential speci.cation de.nes the intended semantics be\u00adcause it speci.es (1) what \nvalues may returned by each operation, and (2) what operations may proceed. For example, consider a se\u00adquential \nspeci.cation Y for the counter object c described above. Suppose that the initial value of the counter \nis zero. Then (c inc A)(c ok A)(c get B)(c ok(1) B). Y (c inc A)(c ok A)(c get B)(c ok(0) B)./Y Suppose \nthat the decrement operation blocks if the count is already zero (like a semaphore would). Then Y does \nnot contain any history whose .rst event is (c dec A). A sequential speci.cation Y is nondeterministic \nif it contains two distinct histories H H' whose longest common pre.x ends = in a call, and deterministic \notherwise. 2.1.3 Operations Within a history H, we de.ne an operation e to be a pair consisting of an \ninvocation inv(e) and the next matching response res(e) (if present). We let ops(H) be the set of all \noperations in H. We say that an operation e is pending if inv(e) is pending, and complete otherwise. \nWe de.ne oe, ie, re, and te to be the object, invocation, response, and thread of the operation e, respectively. \nWe sometimes write down an operation as a bracketed tuple, in the form [oe ie/re te] (for complete operations) \nor [oe ie/. te] (for pending operations). We de.ne the irre.exive partial order <H on operations of H \nby requiring that e1 <H e2 if and only if res(e1) precedes inv(e2) in H. We say two operations e1,e2 \n. ops(H) are overlapping if neither e1 <H e2 nor e2 <H e1.  2.1.4 Linearizability The key idea behind \nlinearizability is to compare concurrent his\u00adtories to serial histories. We call a history S a serial \nwitness for a history H if it satis.es 1. S is serial, and 2. H|t = S|t for all threads t, and 3. <H \n. <S .  Intuitively, a serial witness of H is simply a linear arrangement S of all the operations in \nH (all but the last one of which must be complete) such that the order of two operations is preserved \nif they are performed by the same thread, or if they do not overlap. The following de.nition is equivalent \nto the original de.nition of linearizability, restricted to single-object histories which are the sole \nfocus of our attention in this paper:2 DEFINITION 1 (Linearizability). A single-object history H is lin\u00adearizable \nwith respect to a sequential speci.cation Y if H can be extended, by appending zero or more return events, \nto a history H ' such that Y contains a serial witness for complete(H ' ).  2.2 Implementation vs. Speci.cation \nLinearizability allows us to specify the desired behavior of a con\u00adcurrent data type in terms of a sequential \nspeci.cation. It thus gives us the ability to decide whether a particular implementation be\u00adhaves correctly. \nAs it is well known that the implementation of concurrent data types is dif.cult (in particular in the \npresence of performance optimizations or ambitious guarantees such as lock-or wait-freedom), a technique \nfor detecting deviations from the spec\u00adi.cation is invaluable. Let X be an implementation of some object. \nThen we de.ne the set H(X) to be the set of single-object histories exhibited by X, for all possible \nconcurrent programs that may make use of the object, and according to the semantics of the programming \nplatform used by the implementation. We consider X to be linearizable with respect to the speci.cation \nY if and only if all histories in H(X) are linearizable with respect to Y . 2.2.1 Example : Buggy Counter \n1 We now illustrate how mistakes in an implementation can manifest as linearizability failures. If called \nconcurrently, the buggy counter implementation below (on the left) may exhibit the history H (on the \nright), because the inc operation fails to acquire a lock: class Counter1 (c inc A) { (c inc B) int count \n= 0; (c ok A) void inc() { count = count + 1; } (c ok B) int get() { return count; } (c get A) ... (c \nok(1) A) } This bug can be detected by a method that checks linearizability, because the history H is \nnot linearizable with respect to the speci.\u00adcation in Fig. 3. To see why, consider Def. 1. Because H \nis already complete, any extension H ' as described in the de.nition must sat\u00adisfy H = H ' = complete(H \n' ). Any serial witness S would have to contain exactly the three operations e1 =[c inc/ok A] e2 =[c \ninc/ok B] e3 =[c get/ok(1) A] 2 Theorem 1 [15] proves that linearizability of multi-object histories \ncan be soundly reduced to linearizability of single-object histories. class Counter2 { int count = 0; \nLock lock = new Lock(); void inc() { (c inc A) lock.acquire(); (c ok A) count = count + 1; (c get A) \nlock.release(); (c ok(1) A) } (c inc B) void get() { # lock.acquire(); return count; } ... } Figure \n4. Buggy counter implementation with a stuck history. and would have to satisfy e1 <S e3 and e2 <S e3. \nHowever, no such serial history is compatible with the speci.cation: if both increment operations precede \nthe get operation, the latter has to return the value 2. 2.2.2 Example : Buggy Counter 2 We now look \nat a second example of a faulty counter implemen\u00adtation in Figure 4; however, this time the standard \nlinearizability de.nition fails to detect the bug. If called concurrently, the follow\u00ading buggy counter \nimplementation (left) may exhibit the stuck history H (right) because the get operation fails to release \nthe lock: However, the history H on the right is perfectly linearizable according to Def. 1: adding no \nreturn events, we have H ' = H, and the sequence (c inc A)(c ok A)(c get A)(c ok(1) A)is a serial witness \nfor complete(H ' ) (in fact, it is identical to complete(H ' )). All histories produced by this buggy \nimplementation are lin\u00adearizable in the sense of Def. 1. This is because Def. 1 only con\u00adsiders whether \nthe values returned by the operations are consistent with the sequential speci.cation, but not whether \nthe operations re\u00adturn in the .rst place. Knowing the speci.cation (Figure 3), we can clearly tell that \nthe implementation is never supposed to block inside the operation inc: the only time we would expect \nit to block is during calls to dec, and only if the current count is 0. We extend the de.nition of linearizability \nbelow so that it can precisely compare the blocking behaviors of the implementation and the speci.cation. \n 2.3 Generalizing Linearizability To be able to detect progress problems such as deadlocks in the implementation, \nwe introduce the notion of stuck histories. First, we formally de.ne stuck histories to be .nite sequences \nof events ending with the special symbol #. We use the same terminology for stuck histories that we de.ned \nfor histories (such as complete versus incomplete histories, pending calls, matching returns, and so \non). Also, our de.nition of determinism extends easily to sets of serial histories that contain both \nregular and stuck histories: a set of serial histories is nondeterministic if it contains two histories \nH = H ' whose longest common pre.x ends with a call. For an implementation X, we de.ne H(X) be the set \nof stuck histories that the implementation X can exhibit, that is, all histories H where (1) H has at \nleast one pending operation, and (2) none of the pending operations in H can complete due to some inability \nof the implementation to make progress (such as deadlock, livelock, or a diverging loop). Figure 4 shows \nan example of a stuck history. For a stuck history H of X to be linearizable, we expect that we can \n.nd a stuck serial witness for all incomplete operations of H. This represents the insight that all of \nthe pending operations in the stuck history need to have a justi.cation for being stuck. More formally, \nfor a stuck history H and a pending operation e of H, let H[e] be the stuck history obtained from H by \nremoving all pending calls except inv(e). Then we de.ne: DEFINITION 2 (Linearizability of Stuck Histories). \nA stuck single\u00adobject history H is linearizable with respect to a set Z of stuck serial histories if \nfor each pending operation e of H, the set Z contains a serial witness for H[e]. Given a sequential speci.cation \nY for object o, we de.ne the set Y to consist of all histories of the form H(oit)# where H . Y is complete, \ni . Io, t . T , and such that there exists no response event x satisfying H(oit)x . Y . For example, \nif Y is the speci.cation of the counter object from Section 2.1.2, then Y contains (among others) the \nstuck history (c dec A)#. Finally, we combine the previous two de.nitions to obtain the general de.nition \nof linearizability: DEFINITION 3 (General Linearizability). An implementation X is linearizable with \nrespect to a sequential speci.cation Y if all his\u00adtories in H(X) are linearizable with respect to Y and \nall stuck histories in H(X) are linearizable with respect to Y .  2.4 Deterministic Linearizability \nWe are now ready to formally de.ne the property that Line-Up is checking. DEFINITION 4 (Deterministic \nLinearizability). An implementa\u00adtion X is deterministically linearizable if there exists a determin\u00adistic \nsequential speci.cation Y such that X is linearizable with respect to Y . Checking deterministic linearizability \nis useful for .nding concurrency-related errors in the implementation. We provide more evidence for this \nclaim in Section 5. Determining whether an implementation is deterministically linearizable is undecidable \nin general (we assume implementations use a Turing-complete pro\u00adgramming language), but useful partial \nalgorithms are nevertheless possible and sensible, as demonstrated in the remainder of this paper. 3. \nLine-Up Algorithm We now present our approach to automatically check the determin\u00adistic linearizability \nof an implementation X (for some object o, which remains .xed throughout this section). For better readabil\u00adity, \nwe have moved the detailed proofs to the appendix, and include only short proof descriptions here. 3.1 \nTests A .nite test for object o is a .nite collection of invocations orga\u00adnized by thread. More formally, \nde.ne a .nite test m to be a map m : T . Io * from threads to invocation sequences, such that m(t) is \nempty for all but .nitely many threads t. We say a .nite test m is a pre.x of a .nite test m ' if m(t) \nis a pre.x of m ' (t) for all t . T . We sometimes think of .nite tests as matrices, with each thread \ncor\u00adresponding to a column, and use the corresponding notation, such as inc get m(A)= inc inc m = means \ninc set(0)m(B)= get set(0) For an object o, and a set of invocations I . Io, we de.ne the set MpI \u00d7q \nto consist of all .nite tests corresponding to matrices of dimension p \u00d7 q with entries in I. 1: procedure \nCheck(X, m) begin // Phase 1: enumerate serial executions of test m 2: A . M s(X, m) // all serial full \nhistories 3: B . Ms(X, m) // all serial stuck histories 4: if A . B is nondeterministic then 5: return \nFAIL 6: end if // Phase 2: check concurrent executions of test m // check each full history 7: for all \nH . M (X, m) do 8: if H not linearizable with respect to A then 9: return FAIL 10: end if 11: end for \n// check each stuck history 12: for all H . M(X, m) do 13: if H not linearizable with respect to B then \n14: return FAIL 15: end if 16: end for 17: return PASS 18: end Figure 5. The function Check(X, m).  \n3.2 Stateless Model Checker The signi.cance of using a .nite test is that we can employ a state\u00adless \nmodel checker to explore all thread schedules the implemen\u00adtation can exhibit for the given .nite test. \nDuring this enumera\u00adtion, we instruction the model checker to record the call and return events along \nwith the values of the arguments and return values to generate the history for each execution. We describe \nthis in more depth in Section 4. For a given test m, we call a history full if it is complete and contains \nall operations of m. For an implementation X and a .nite test m, we de.ne M (X, m) to be the set of full \nhistories of m found by the model checker, and M (X, m) to be the set of stuck histories found by the \nmodel checker. We can also instruct the model checker to explore serial sched\u00adules only, if so desired. \nWe de.ne M s(X, m) to be the set of full serial histories found by the model checker, and Ms(X, m) to \nbe the set of stuck serial histories found by the model checker. 3.3 The Two-Phase Check At the core \nof our method is the function Check(X, m) (see Fig. 5). This function checks for a given implementation \nX and .nite test m whether the executions of X for m are consistent with some (unknown) sequential deterministic \nspeci.cation. The check has two phases. Because we do not know the speci.\u00adcation, we synthesize it in \nphase 1, by recording all serial histories of the .nite test m. In phase 2, we check whether all concurrent \nex\u00adecutions are consistent with the speci.cation recorded in phase 1. The following two theorems describe \nprecisely under what cir\u00adcumstances our test fails or succeeds. The detailed proofs are in\u00adcluded in \nthe appendix. THEOREM 5 (Completeness). Let X be an implementation and let m be an arbitrary .nite test. \nIf Check(X, m) returns FAIL, then X is not deterministically linearizable. The theorem holds because \nif X is linearizable with respect to some deterministic speci.cation, phase 1 is guaranteed to synthe\u00adsize \nexactly that speci.cation with respect to m, so we can perform a precise check in phase 2. The guarantee \nmade by this theorem 1: procedure AutoCheck(X) begin 2: n . 1 3: loop 4: for all m . MIn do n\u00d7n 5: if \nCheck(X, m) returns FAIL then 6: return FAIL 7: end if 8: end for 9: n . n +1 10: end loop 11: end Figure \n6. The algorithm AutoCheck(X). is very strong: it shows that a failing check never produces false alarms, \nbut truly refutes deterministic linearizability. On the other hand, a return of PASS does not conclusively \nprove that an implementation is deterministically linearizable, because the chosen .nite test may not \nexpose the bug. However, this is the only limitation, as the following theorem demonstrates: THEOREM \n6 (Restricted Soundness). Let X be an implementation that is not deterministically linearizable. Then \nthere exists a .nite test m such that Check(X, m) returns FAIL. The theorem holds because if all tests \nm pass (note that there are in.nitely many), then we can construct a deterministic sequential speci.cation \nfor the implementation from all the sets A, B. In practice, our experience (which we discuss in more \ndepth in Section 5) shows that even this restricted soundness is quite power\u00adful. The reason is that \nif there exists a .nite test that .nds the bug, then there usually exists a relatively small one (an \nempirical obser\u00advation called the small scope hypothesis by some researchers).  3.4 Automatic Algorithm \nThe checking function Check(X, m) still requires a test case m. To achieve full automation, we can generate \nm automatically. First, determine some enumeration Io = {i1,i2,... } of the invocations of the object \no under test. This can be automatically constructed by enumerating the interface methods of the object \nand enumerating possible values for each of the methods. For n . N, de.ne the set In = {i1,...,in} containing \nthe .rst n elements of Io. Then we can apply the algorithm AutoCheck(X) as shown in Fig. 6 to check linearizability \nautomatically. Note that this algorithm does not terminate on a correct implementation.3 Completeness \n(Thm. 5) holds for AutoCheck(X) just as for Check(X, m). Soundness is more comprehensive: THEOREM 7 (Soundness). \nLet X be an implementation that is not deterministically linearizable. Then AutoCheck(X) returns FAIL. \nThis theorem holds because we know by Thm. 6 that there exists a .nite test m such that Check(X, m) returns \nFAIL. We can then choose a suf.ciently large n such that m is a pre.x of some .nite test in M In , and \napply the following lemma: n\u00d7n LEMMA 8. If test m is a pre.x of test m ' and Check(X, m) returns FAIL, \nthen Check(X, m ' ) returns FAIL also. Intuitively, the proof works by observing that all full histories \nof m appear as pre.xes in some full or stuck history of m ', and all stuck histories of m appear as stuck \nhistories of m '. Note that condition 3 in the de.nition of a serial witness (Section 2.1.4) is essential \nfor this lemma to hold. 3 This is consistent with the fact that there cannot be an algorithm for an un\u00addecidable \nproblem that is simultaneously sound, complete, and terminating. 4. Implementation Line-Up is built \non top of the stateless model checker CHESS [22], which provides us with the capability of enumerating \nthread sched\u00adules of C# code. We treat the algorithm used by CHESS (fair state\u00adless model checking [21]) \nand its optimizations and heuristics (e.g. search prioritization [5]) essentially as a black box, thus \nour algo\u00adrithm could be used with other model checkers as well. However, we rely on the ability to enumerate \nschedules exhaustively, so sim\u00adple runtime monitoring is not suf.cient. Also, support for fairness is \nimportant because many of the concurrent data types use spin\u00adloops for synchronization. 4.1 Implementing \nCheck We implement the two phases of the function Check(X, m) (Fig.5) as two separate invocations of \nCHESS. To perform phase 1, we record all complete and all stuck serial histories. All these histories \ncan be enumerated without preempting threads inside operations, so we instruct CHESS accordingly. The \nset of observed serial histories Z is recorded in a .le (called the observation .le). To perform phase \n2, we run CHESS again, this time exploring the .ne\u00adgrained thread interleavings, and for each stuck or \ncomplete history, we check whether the observation .le contains the required serial witness(es). If the \ncheck fails, we report the violating history to the user. 4.2 Observation File Format We chose an XML-based \nformat for listing a set of observations. This format groups all histories into sections, where all histories \nin a section exhibit the same operation sequences for each thread (Fig. 7, middle). This format has two \nadvantages. For one, when our algorithm is looking for a serial witness in the observation set, it is \nenough to search one group, because any serial witness must perform matching operation sequences in each \nthread. Second, this format is easier to understand and navigate manually if the histories become large. \nWhen we report a linearizability violation to the user we include the violating history (Fig. 7, bottom). \nOften, the .rst step in analyz\u00ading such a report is to examine the observation .le for a clue to why \nit does not contain a serial witness. 4.3 Random Sampling We found that a literal implementation of \nthe algorithm AutoCheck (Fig. 6) does not perform well in practice. The reason is that (1) the model \nchecker performance starts to drop dramatically when going beyond 3x3 matrices, and (2) using the invocation \nenumeration sets In may require unnaturally large values of n for the right combination of invocations \nto show up in the test. We thus developed a random sampling technique that performed quite well in practice. \nSpeci.cally, our adapted algorithm lets the user provide a list of representative invocations I, the \ndesired dimension n \u00d7 m of the matrix, and a sample size k. We then run Check on a uniform ran\u00addom sample \nof k tests from MnI \u00d7m. Like Check and AutoCheck, the function RandomCheck is complete, but we no longer \nhave a soundness guarantee (bugs may be missed). However, our empiri\u00adcal results in Section 5 show that \nrandom sampling is quite ef.cient at discovering failing testcases. Another big practical bene.t of random \nsampling is that it is embarrassingly parallel: it is very easy to distribute the various tests and let \neach core run Check independently. We also allow users to specify entire sequences of invocations to \nbe used when constructing tests, as well as initial and .nal sequences of operations to perform before \nand after each test, respectively. Any professional experience of the tester about how to construct effective \ntests can thus be easily integrated with the Figure 7. (Top) An example 2x2 test for a concurrent FIFO \nqueue implementation. Add adds an element to the queue, TryTake re\u00admoves and returns the oldest element \nor fails if the queue is empty, and Take removes and returns the oldest element or blocks if the queue \nis empty. (Middle) An example of what the observation .le looks like for this test. The histories are \ngrouped into sections (each <observation> element is one such section). All histories in a section agree \non the sequence of operations performed by each in\u00addividual thread (<thread> elements show the sequence \nof opera\u00adtions by each thread, while <op> elements show the details of each operation), but differ in \nthe precise interleaving of the operation calls and returns (<history> elements show the precise interleav\u00ading \nof each history, in the form i[ and ]i for call and return of an operation i , respectively). Blocking \noperations are marked with a letter B, and stuck histories are marked with a .nal #. (Bottom) Example \nof a linearizability violation report for this test and ob\u00adservation .le. The test failed because no \nserial witness was found: there is only one history in the matching <observation> element of the .le, \nand it does not order the call of TryTake after the return of Add(400), so it is not a serial witness \nfor this observation. Thread A Thread B Add (200) Take() Add (400) TryTake () <observationset> <observation> \n<thread id=\"A\">1 2</thread> <thread id=\"B\">3 4</thread> <op id=\"1\" name=\"Add\">value=\"200\"</op> <op id=\"2\" \nname=\"Add\">value=\"400\"</op> <op id=\"3\" name=\"Take\">result=\"200\"</op> <op id=\"4\" name=\"TryTake\">result=\"Fail\"</op> \n<history>1[ ]1 3[ ]3 4[ ]4 2[ ]2</history> </observation> <observation> <thread id=\"A\">1 2</thread> <thread \nid=\"B\">3 4</thread> <op id=\"1\" name=\"Add\">value=\"200\"</op> <op id=\"2\" name=\"Add\">value=\"400\"</op> <op \nid=\"3\" name=\"Take\">result=\"200\"</op> <op id=\"4\" name=\"TryTake\">result=\"400\"</op> <history>1[ ]1 2[ ]2 \n3[ ]3 4[ ]4</history> <history>1[ ]1 3[ ]3 2[ ]2 4[ ]4</history> </observation> <observation> <thread \nid=\"A\"></thread> <thread id=\"B\">1B</thread> <op id=\"1\" name=\"Take\" /> <history>1[ #</history> </observation> \n</observationset> Line-Up encountered a non-linearizable history: <thread id=\"A\">1 2</thread> <thread \nid=\"B\">3 4</thread> <op id=\"1\" name=\"Add\">value=\"200\"</op> <op id=\"2\" name=\"Add\">value=\"400\"</op> <op \nid=\"3\" name=\"Take\">result=\"200\"</op> <op id=\"4\" name=\"TryTake\">result=\"Fail\"</op> <history>1[ 3[ ]1 2[ \n]3 ]2 4[ ]4</history> 1: procedure RandomCheck(X, I, i, j, n) begin 2: M . random sample of size n drawn \nfrom MiI \u00d7j 3: for all m . M do 4: if Check(X, m) returns FAIL then 5: return FAIL 6: end if 7: end for \n8: return PASS 9: end Figure 8. The algorithm RandomCheck(X). automatic test generation . Also, the \nuser is always free to specify test matrices directly, a useful feature for testing very speci.c scenarios \nor for writing regression tests. To allow the model checker CHESS to complete its schedule exploration, \nwe found it necessary to use the preemption bound\u00ading heuristic in CHESS, thus further compromising on \nsoundness. However, we use no bounding during phase 1, so we can retain the important completeness guarantee \n(that any reported violation is correct). 5. Results We now present the practical experiences we gathered \nwhen apply\u00ading Line-Up to 13 classes with a total of 90 methods in the .NET Framework 4.0 concurrency \nclasses (Table 1). We used two differ\u00adent versions of these classes, a technology preview (indicated \nby Pre in our results) and the beta2 release of the library. The two ver\u00adsions are separated by more \nthan a year of development and much of the implementation and the APIs were changed between these two \nversions. Some of the bugs in the older version were originally found by (an earlier version of) Line-Up \nand have been .xed in the latest version. 5.1 Approach For each class, we run the RandomCheck procedure \nto test a uni\u00adform random sample of 100 tests of dimension 3\u00d73. To track down the observed failures, \nwe manually remove operations from failing 3x3 test matrices to obtain a failing test of minimal dimension, \nfor the sake of easier reasoning and regression testing. We then ana\u00adlyze the reduced failures further \nto determine the root cause of the failure. Table 2 summarizes our results. Each line corresponds to \na class we tested (Table 1). A class with the suf.x (Pre) indicates a version from the older release \nof the .NET Framework 4.0. 5.2 Line-Up Failures For each failure reported by Line-Up, we identi.ed 12 \nunique root causes indicated by A to L in Table 2. The table also shows the minimum dimension of a test \nthat is suf.cient to .nd each root cause. The numbers show that most failures can be found with very \nsmall tests, con.rming the small scope hypothesis. A Line-Up failure indicates that the class is not \nlinearizable with respect to any deterministic sequential speci.cation. Accordingly, the root cause can \nbe classi.ed as one of the following three categories; we describe each cateogory in a separate subsection. \n1. A Bug (7 found): The class is intended to be deterministically linearizable with respect to the set \nof methods tested. Thus the failure indicates a bug in the program. 2. Intentional Nondeterminism (3 \nfound): The tested methods are intended to be linearizable but with respect to a nondeter\u00administic speci.cation. \n  Class LOC Properties &#38; Methods checked Lazy Initialization 470 Value, ToString, IsValueCreated \nManualResetEvent 868 Set, Wait, Reset, IsSet, WaitOne SemaphoreSlim 585 CurrentCount, Release, Release(2), \nWait, Wait(0) CountdownEvent 571 IsSet, Wait, Wait(0), CurrentCount, WaitOne ConcurrentDictionary ConcurrentQueue \n1833 819 for x in (E,2) {Signal(x), AddCount(x), TryAddCount(x)}Count, IsEmpty, Clear, for x in (10,20) \n{TryAdd(x), TryRemove(x), TryGet(x), get[x], set[x], TryUpdate(x), ContainsKey(x)}Count, IsEmpty, Enqueue, \nToArray, TryDequeue, TryPeek ConcurrentStack 835 Clear, Count, Push, PushRangeTen, TryPop, TryPopRangeOne, \nTryPopRangeTwo, TryPopRangeFour, TryPeek, ToArrayOrderBy ConcurrentLinkedList Count, AddFirst,AddLast,RemoveFirst, \nRemoveFirst(out value), RemoveList, RemoveLast(out value) BlockingCollection 1808 Count, ToArray, TryAdd, \nTryAdd(1), IsCompleted, IsAddingCompleted, CompleteAdding, Add, Take, TakeWithEnum, TryTake, TryTake(1) \nConcurrentBag 1074 Count, Add(10), Add(20), TryTake, IsEmpty, TryPeek, ToArray TaskCompletionSource 392 \nException, TrySetCanceled, TrySetException, TrySetResult, SetCanceled, SetException, SetResult, Wait, \nTryResult CancellationTokenSource 946 Increment, Cancel Barrier 870 SignalAndWait, ParticipantsRemaining, \nRemoveParticipant, CancelToken, CurrentPhaseNumber, ParticipantCount, AddParticipant Table 2. Results \nof applying Line-Up. to classes from the .NET Framework 4.0. The classes from the Parallel Extensions \npreview [19] are marked with (Pre), all others are from the Beta 2 release. 3. Intentional Nonlinearizability \n(2 found): The tested methods are not intended to be linearizable. 5.2.1 Bugs Line-Up found a total \nof 7 bugs tagged A to G in Table 2. These bugs range from simple synchronization errors to .aws in the \nlogic of the algorithm. In many cases, understanding the bugs requires an intimate knowledge of the class \nimplementation. We describe one bug (A) in the ManualResetEvent class in more detail. This class implements \nan event, and as one would expect, a thread waits till the event is set by another thread. The event \ncan also be reset after being set. For the test shown in Figure 9, Line-Up produced a concurrent execution \nin which Thread 1 was never unblocked. We can be convince ourselves that this is an erroneous (or at \nleast, an unexpected) behavior even without understanding how this class is implemented. On careful inspection, \nwe identi.ed the source of the bug to an erroneous use of the compare and swap (CAS) operation shown \nbelow: int state; Wait(){ //... int localstate = state; int newstate = f(state); // compute new value \ncompare_and_swap(&#38;state, localstate, newstate); //... } Thread 1 Thread 2 mre.Wait(); mre.Set(); \nmre.Reset(); mre.Set(); Figure 9. A ManualResetEvent test. Irrespective of the interleav\u00ading between \nthe two threads, one expects Thread 1 to be eventually unblocked. It is common for concurrent algorithms \nto use a CAS operation to atomically update a shared variable. The correct usage is to read the shared \nvariable into a local copy, use this copy to compute the new value of the variable, and update the shared \nvariable only if the variable has not been modi.ed by another thread in the interim. However, the implementation \nabove contains a pernicious typographical error where the shared variable state is read the second time \nwhen computing the new value. Arguably, this bug is very hard to .nd by manual inspection. Moreover, \neven when the bug is known, it is very hard to design a test harness that exposes the bug: the value \nof state needs to change between the two reads but needs to be set to the .rst value before the CAS operation, \nwhich would otherwise fail.  5.2.2 Intentional Nondeterminism Line-Up reported 3 failures (H, I, J) \nthat resulted from nondeter\u00adminism in the speci.cation. A ConcurrentBag represents an un\u00adordered collection \nof items and the implementation is allowed to remove any one of the elements during a TryTake. For the \nBlock\u00adingCollection, Line-Up generated a test where the Count method could return 0 even when the collection \nis not empty. Similarly, it generated a test where the TryTake method can fail even when the collection \nis not empty. While these are clearly unexpected behav\u00adiors, the developers of these classes were unable \nto provide a .x that was both ef.cient and did not require global changes to the im\u00adplementation, and \ndecided instead to change the of.cial documen\u00adtation of these methods to include the potentially nondeterministic \nbehavior.  5.3 Intentional Nonlinearizability Line-Up reported 2 instances (K, L) of nonlinearizability. \nThe BlockingCollection contains a Cancel method where the effects of cancellation can take place well \nafter the method has returned. This class is linearizable for all other methods. The Barrier is a classic \nexample of a nonlinearizable class. Barriers block each thread until all threads have entered the barrier, \na behavior that is not equivalent to any serial execution. In summary, the results above show that a \nlarge variety of concurrency errors can be caught quite easily as a linearizability violation of randomly \nchosen 3x3 tests. They also show that there is some room for improvement in extending our method to support \nnondeterministic or nonlinearizable methods.  5.4 Runtime of the Two Phases The table 2 shows the runtime \ncharacteristics of the two phases. For Phase 1, the enumeration of serial histories, the table shows \nhow many histories were observed on average and on maximum. The table also shows how long it takes to \nrun phase 1 on a single 3x3 testcase (average and maximum observed). All measurements were performed \non an 8 Core 2.33 GHz Intel Xeon, with one core assigned to one testcase at a time. The numbers demonstrate \nthat the automatic enumeration of a sequential speci.cation is very cheap, which is a key fact exploited \nby the Line-Up algorithm. For Phase 2, the enumeration of concurrent histories, the table shows how many \ntestcases passed and failed. Most violations were caught by a large proportion of the sample. We also \nshow how long it takes to complete a failing/passing testcase on average. As usual, testcases fail much \nquicker than they pass. The PB column shows the preemption bound used to limit the search. We use 2 (the \nCHESS default) except where it performed unacceptably slow.  5.5 Relevance of using generalized linearizability \nBecause we are generating random tests, some tests may get stuck. For example, a random test may easily \nacquire a block\u00ading semaphore more often than release it, thus causing deadlock (in both phases). This \nis re.ected by the fact that the number of histories enumerated in phase one is sometimes less than the \ncom\u00adbinatorial number of full histories for 3x3 matrices, which is 1680. Our use of generalized linearizability \n(as opposed to classical linearizability) is signi.cant insofar 5 of the 13 classes tested ex\u00adhibited \ndeadlocking tests and could not have been tested with a methodology that can not handle them. In particular, \nwe would not be able to single out the bug in Figure 9 with a tool that checks standard (nonblocking) \nlinearizability only. 5.6 Comparisons To compare Line-Up with other dynamic checking methods, we also \nchecked for data races and atomicity violations. For data race detection, we used the happens-before \nbased dynamic race detector included with CHESS. To perform atomicity checking, we imple\u00admented the algorithm \ndescribed in [10], which checks whether a given dynamic execution is con.ict-serializable. All the data \nraces we found were benign, and it appeared that they remained in the code only because of limitations \nin the C# compiler (which does not permit arrays of volatiles, or references to volatiles). We believe \nthe surprisingly low number of data races we found shows that, for this type of application, a conservative \nuse of volatile declarations and interlocked operations is a relatively simple way to avoid data races \n(but does of course nothing to prohibit higher-level mistakes in the logic of the algorithm). Our experiments \nwith atomicity (con.ict serializability) check\u00ading resulted in hundreds of warnings. We abandoned the \neffort of classifying these warnings into real errors after inspecting the .rst ten of them which turned \nout to be false alarms. Our initial inves\u00adtigation yielded four good and common reasons why programs \n(in particular, concurrent data types) can exhibit non-serializable exe\u00adcutions and still be correct.4 \nWe brie.y list these here. 1. (ConcurrentStack, ConcurrentQueue) The code performs a CAS. A failing CAS \nleads to a retry. However, the accesses performed before the retry break serializability. 2. (SemaphoreSlim) \nThe code contains a timing optimization (similar to double-checked locking) that does not affect cor\u00adrectness, \nbut breaks serializability. 3. (CancellationTokenSource) The current state is read and com\u00adpared using \na = operator. At an abstract level, this comparison is a right-mover, but a simple serializability detector \ndoes not know that. 4. (ConcurrentBag) A thread performs lazy initialization, acquir\u00ading a global lock. \nThis work does not affect the current opera\u00adtion in any way, but breaks serializability.  It is labor-intensive \nto decide whether an atomicity violation is be\u00adnign as doing so requires a good understanding of the \ndesign prin\u00adciples of the implementation, and we did not write this code our\u00ad 4 We believe that all of \nthe observed non-serializable behaviors could be made serializable by exploiting global knowledge about \ninvariants and commutativity of operations (e.g. using atomic blocks and movers [7]), but automating \nsuch an approach would indeed be very challenging. selves. Using Line-Up was easier because the reported \nviolations of linearizability provided a suf.ciently conclusive evidence of mal\u00adfunction to convince \nthe original developers that they had to ana\u00adlyze and .x the problem.  5.7 Memory model issues The algorithm \nas presented in this paper is orthogonal to memory model issues in the sense that it is the responsibility \nof the underly\u00ading model checker to produce all histories (including histories that manifest for non-sequentially \nconsistent executions). However, the CHESS model checker does not directly enumerate the relaxed be\u00adhaviors \nof the target architecture; instead it checks for potential vi\u00adolations of sequential consistency using \na special algorithm similar to data race detection [3]. We thus used this technique, but did not .nd \nany such issues in the studied implementations. 6. Conclusion and Future Work We have made several important \ncontributions in this paper. First, we show how to improve the de.nition of linearizability so it can \ndetect erroneous blocking in implementations. Then, we present an automatic algorithm to detect linearizability \nviolations, and show how it can be practically implemented and applied using a stateless model checker. \nWe then demonstrated on real production code that our tool Line-Up can detect a variety of concurrency \nbugs automatically. Our work shows the practical value of a tool that checks imple\u00admentations without \nrequiring knowledge of their design principles, and the appeal of error reports that show a speci.c scenario \nwhere the component misbehaves in an externally observable way. As future work, we would like to take \nLine-Up beyond sim\u00adple linearizable methods and incorporate support for (1) asyn\u00adchronous methods, such \nas the cancel method, and (2) nondeter\u00administic methods, such as methods that may fail on interference. \nReferences [1] M. Abadi and L. Lamport. The existence of re.nement mappings. Theor. Comput. Sci., 82(2), \n1991. [2] S. Burckhardt, R. Alur, and M. Martin. CheckFence: Checking con\u00adsistency of concurrent data \ntypes on relaxed memory models. In Pro\u00adgramming Language Design and Impl. (PLDI), pages 12 21, 2007. \n[3] S. Burckhardt and M. Musuvathi. Effective program veri.cation for relaxed memory models. In Computer-Aided \nVeri.cation (CAV), pages 107 120, 2008. [4] R. Colvin, L. Groves, V. Luchangco, and M. Moir. Formal veri.cation \nof a lazy concurrent list-based set algorithm. In Computer-Aided Veri.cation (CAV), LNCS 4144, pages \n475 488. Springer, 2006. [5] K. Coons, M. Musuvathi, and S. Burckhardt. Gambit: Effective unit testing \nof concurrency libraries. In Principles and Practice of Parallel Programming (PPoPP), 2010. [6] S. Doherty, \nD. Detlefs, L. Grove, C. Flood, V. Luchangco, P. Martin, M. Moir, N. Shavit, and G. Steele. DCAS is not \na silver bullet for nonblocking algorithm design. In Symposium on Parallel Algorithms and Architectures \n(SPAA), pages 216 224, 2004. [7] T. Elmas, S. Qadeer, and S. Tasiran. A calculus of atomic actions. In \nPrinciples of Programming Languages (POPL), 2009. [8] T. Elmas and S. Tasiran. VyrdMC: Driving runtime \nre.nement check\u00ading with model checkers. Electr. Notes Theor. Comput. Sci., 144:41 56, 2006. [9] T. Elmas, \nS. Tasiran, and S. Qadeer. VYRD: verifying concurrent programs by runtime re.nement-violation detection. \nIn Programming Language Design and Impl. (PLDI), pages 27 37, 2005. [10] A. Farzan and P. Madhusudan. \nMonitoring atomicity in concurrent programs. In Computer-Aided Veri.cation (CAV), 2008. [11] C. Flanagan \nand S. Freund. Ef.cient and precise dynamic race detec\u00adtion. In Programming Language Design and Impl. \n(PLDI), 2009. [12] C. Flanagan, S. Freund, and J.Yi. Velodrome: A sound and complete dynamic atomicity \nchecker for multithreaded programs. In Program\u00adming Language Design and Impl. (PLDI), 2008. [13] K. Fraser. \nPractical Lock-Freedom. PhD thesis, University of Cam\u00adbridge, 2004. [14] K. Fraser and T. Harris. Concurrent \nprogramming without locks. ACM Trans. Comput. Syst., 25(2), 2007. [15] M. Herlihy and J. Wing. Linearizability: \na correctness condition for concurrent objects. ACM Trans. Program. Lang. Syst., 12(3):463 492, 1990. \n[16] L. Lamport. How to make a multiprocessor computer that correctly executes multiprocess programs. \nIEEE Trans. Comp., C-28(9):690 691, 1979. [17] D. Marino, M. Musuvathi, and S. Narayanasamy. LiteRace: \nEffective sampling for lightweight data-race detection. In Programming Lan\u00adguage Design and Impl. (PLDI), \n2009. [18] M. Michael and M. Scott. Correction of a memory management method for lock-free data structures. \nTechnical Report TR599, Uni\u00adversity of Rochester, 1995. [19] MSDN, http://blogs.msdn.com/somasegar/archive/2007/ \n11/29/parallel-extensions-to-the-net-fx-ctp.aspx. Parallel Extensions to the .NET FX CTP, November 2007. \n[20] MSDN, http://msdn.microsoft.com/en-us/library/ dd460718(VS.100).aspx. .NET Framework 4 Data Structures \nfor Parallel Programming, November 2009. [21] M. Musuvathi and S. Qadeer. Fair stateless model checking. \nIn Programming Language Design and Impl. (PLDI), 2008. [22] M. Musuvathi, S. Qadeer, T. Ball, G. Basler, \nP. Nainar, and I. Neamtiu. Finding and reproducing heisenbugs in concurrent programs. In Op\u00aderating Systems \nDesign and Impl. (OSDI), pages 267 280, 2008. [23] C. H. Papadimitriou. The serializability of concurrent \ndatabase up\u00addates. J. ACM, 4(26), October 1979. [24] S. Savage, M. Burrows, G. Nelson, P. Sobalvarro, \nand T. Anderson. Eraser: A dynamic data race detector for multithreaded programs. ACM Trans. Comp. Sys., \n15(4):391 411, 1997. [25] V. Vafeiadis. Shape-value abstraction for verifying linearizability. In Veri.cation, \nModel Checking and Abstract Interpretation (VMCAI). Springer-Verlag, 2009. [26] V. Vafeiadis, M. Herlihy, \nT. Hoare, and M. Shapiro. Proving correct\u00adness of highly-concurrent linearisable objects. In Principles \nand Prac\u00adtice of Parallel Programming (PPoPP), pages 129 136, 2006. [27] M. Vechev, E. Yahav, and G. \nYorsh. Experience with model checking linearizability. In SPIN, 2009. A. Proofs We .rst establish the \nfollowing lemma which states that the enu\u00admeration done in phase 1 provides all the necessary histories. \nFor each history H, let mH be the corresponding test (that is, we let mH (t) be the sequence of calls \nmade by thread t in H). LEMMA 9 (Speci.cation Synthesis.). Let X be a linearizable im\u00adplementation of \nsome object o with respect to a deterministic se\u00adquential speci.cation Y . 1. If H . Y is complete, then \nH . M s(X, mH ). 2. If H . Y , then H . Ms(X, mH ).  PROOF. Let H . Y or H . Y (thus H is serial). \nNow consider an execution of the test mH where the schedule is restricted in such a way that calls are \nmade in the exact sequence that they appear in H, and no call is made before the previous call returns. \nSuch an execution must be possible without getting stuck prematurely (any stuck partial execution would \nnot be consistent with the fact that X is linearizable w.r.t. Y , because that would imply that H ' # \nis in Y for some pre.x H ' of H which is impossible because H in Y and Y is deterministic) and all the \nreturns must match the ones in the sequence H (because X is linearizable w.r.t. Y and Y is deterministic). \nThus, such an execution must end up reproducing H. If H is complete, it is full for mH , and thus H \n. M s(X, mH ). On the other hand, if H . Y , then the execution can not possibly continue with a return \n(such a continuation, since linearizable, would again contradict the determinism of Y ). Thus, it must \nbe stuck, and thus H . Ms(X, mH ). D    A.1 Proof of Thm. 5 We assume there exists a deterministic \nspec Y such that X is lin\u00adearizable with respect to Y , but assume that Check(X, m) never\u00adtheless returns \nFAIL, and show that a contradiction results. Distin\u00adguish cases. Case 1: The check on line 4 returns \nFAIL. Then there must exist histories H = H ' in A . B whose maximal common pre.x ends in a call. But \nthen both of H, H ' must be in Y (or Y ) if they are full (or stuck), because the only serial witness \nfor a serial history is that exact history. But that contradicts the assumption that Y is deterministic \nor the de.nition of Y . Case 2: The check on line 8 returns FAIL for H . M (X, m). Because X is linearizable \nand complete, we know H has a serial witness S . Y . By Lemma 9, we know S . M s(X, mH ). Because H is \na full execution of m, we know mH = m, thus S . M s(X, m). But that implies that S . A after phase 1 \n(see Fig. 5) so when the check on line 8 is performed, it does not fail, contradicting our assumption. \nCase 3: The check on line 13 returns FAIL for H . M(X, m). Let e be the operation in H for which H[e] \nfails the linearizability test (in the sense of Def. 2). Because X is linearizable, there exists by Def. \n2 a serial witness S . Y for H[e]. By Lemma 9, we know S . Ms(X, mH[e]). Now, because mH[e] is a pre.x \nof m, this implies S . Ms(X, m). But that implies that S . B after phase 1, so the check on line 13 could \nnot have failed as we assumed. A.2 Proof of Thm. 6 Assume that there exists no test m such that Check(X, \nm) re\u00adturns FAIL. For a test m, let Am, Bm be the sets computed in Check(X, m), and let B ' be the set \nof histories obtained from Bm m by removing all pending calls and symbols #. Then de.ne Y to be the pre.x-closure \nof m(Am . B ' ) and observe the following: m 1. Y is deterministic. If not, it would contain distinct \nhistories whose longest common pre.x ends with a call, but which con\u00adtinue differently (with different \nreturns). Tracking these back to the tests m, m ' where they came from, we can reason that the model \nchecker would produce the same options to con\u00adtinue this same pre.x in both Check(X, m) and Check(X, \nm ' ), which implies that the nondeterminism would be detected on the lines 4, contrary to the assumption \nthat all tests pass. 2. If H . Bm, then H . Y . To see this, let H = H(oit)#; then H . Y . For the same \nreason as argued in observation 1, H can not be the pre.x of any longer history in Y , thus H in Y . \n 3. Each element of M (X, m) is linearizable with respect to Y : the check on line 8 passed, which implies \nthat Am contains a serial witness, thus Y does too. 4. Each element of M(X, m) is linearizable with \nrespect to Y : the check on line 13 passed, which implies that Bm contains a serial witness for each \nH[e]. By observation 2 this implies that Y also contains those serial witnesses.   A.3 Proof of Thm. \n7 We know by Thm. 6 that there exists a test m such that Check(X, m) returns FAIL. Now, pick an n such \nthat (1) In contains all the invo\u00adcations appearing in m, and (2) n is larger than the largest thread \nsuch that m(t) is nonempty, and (3) n is larger than length of the longest sequence m(t), for any t. \nThis then implies that MIn n\u00d7n contains a test m ' such that m is a pre.x of m '. The claim then follows \nby Lemma 8. A.4 Proof of Lemma 8 We assume that the test m is a pre.x of a test m ', that Check(X, m) \nreturns FAIL, that Check(X, m ' ) returns PASS, and show that a contradiction results. Let A = Am, B \n= Bm, A ' = Am, , and B ' = Bm, . Now we distinguish by where the check fails. Case 1: The check on line \n4 returns FAIL. Then there must exist histories H = H ' in A . B whose maximal common pre.x ends in a \ncall. Now, because m is a pre.x of m ', the model checker will explore histories of m ' that follow H \nexactly, but continue (if H is not already stuck) until they are full or stuck. Thus each of H, H ' is \neither in B ' (if it is stuck) or is a pre.x of some longer history in A ' .B '. This implies that the \ncheck on line 4 fails in Check(X, m ' ) as well, contradicting the assumption. Case 2: The check on line \n13 returns FAIL for H . M(X, m). Let e be the operation in H for which H[e] fails the linearizability \ntest (in the sense of Def. 2). Since m is a pre.x of m ' , H is also a stuck history of m ', and since \nCheck(X, m ' ) passed, B ' must contain a serial witness S for H ' [e]. But because S ' can only contain \noperations that are in m, this implies also that S ' . B which contradicts the assumption that the test \non line 13 failed. Case 3: The check on line 8 returns FAIL for H . M (X, m). Now, because m is a pre.x \nof m ', the model checker will explore histories of m ' that start with H but continue until they are \nfull or stuck. Let s distinguish these cases. (Case 3a) H is a pre.x of a full H ' . M (X, m ' ). Then \nbecause the check passed for m ', there exists a serial witness S ' . A ' for H '. Because of condition \n3 in the de.nition of a serial witness (Section 2.1.4), we know that within S ', all operations of ops(H) \nmust precede the operations of ops(H ' ) - ops(H). Thus there exists a pre.x S of S ' that contains exactly \nthe operations ops(H) and that is thus a serial witness for H. But then we would necessarily have S . \nA which contradicts the assumption that Check(X, m) returns FAIL. (Case 3b) H is a pre.x of a stuck H \n. M(X, m ' ). Then because the check passed for m ', there exists a serial witness S ' . B ' for H[e] \nfor some e. Because of condition 3 in the de.nition of a serial witness (Section 2.1.4), we know that \nwithin S ', all operations of ops(H) must precede the operations of ops(H ' ) - ops(H). Thus there exists \na pre.x S of S ' that contains exactly the operations ops(H) and that is thus a serial witness for H. \nBut then we would necessarily have S . A which contradicts the assumption that Check(X, m) returns FAIL. \n \n\t\t\t", "proc_id": "1806596", "abstract": "<p>Modular development of concurrent applications requires thread-safe components that behave correctly when called concurrently by multiple client threads. This paper focuses on linearizability, a specific formalization of thread safety, where all operations of a concurrent component appear to take effect instantaneously at some point between their call and return. The key insight of this paper is that if a component is intended to be deterministic, then it is possible to build an automatic linearizability checker by systematically enumerating the sequential behaviors of the component and then checking if each its concurrent behavior is equivalent to some sequential behavior.</p> <p>We develop this insight into a tool called Line-Up, the first complete and automatic checker for <i>deterministic linearizability</i>. It is complete, because any reported violation proves that the implementation is not linearizable with respect to <i>any</i> sequential deterministic specification. It is automatic, requiring no manual abstraction, no manual specification of semantics or commit points, no manually written test suites, no access to source code.</p> <p>We evaluate Line-Up by analyzing 13 classes with a total of 90 methods in two versions of the .NET Framework 4.0. The violations of deterministic linearizability reported by Line-Up exposed seven errors in the implementation that were fixed by the development team.</p>", "authors": [{"name": "Sebastian Burckhardt", "author_profile_id": "81350574118", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P2184576", "email_address": "", "orcid_id": ""}, {"name": "Chris Dern", "author_profile_id": "81464670708", "affiliation": "Microsoft, Redmond, WA, USA", "person_id": "P2184577", "email_address": "", "orcid_id": ""}, {"name": "Madanlal Musuvathi", "author_profile_id": "81100333862", "affiliation": "Microsoft, Redmond, WA, USA", "person_id": "P2184578", "email_address": "", "orcid_id": ""}, {"name": "Roy Tan", "author_profile_id": "81100314858", "affiliation": "Microsoft, Redmond, WA, USA", "person_id": "P2184579", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1806596.1806634", "year": "2010", "article_id": "1806634", "conference": "PLDI", "title": "Line-up: a complete and automatic linearizability checker", "url": "http://dl.acm.org/citation.cfm?id=1806634"}