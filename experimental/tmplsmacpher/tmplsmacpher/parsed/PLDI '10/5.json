{"article_publication_date": "06-05-2010", "fulltext": "\n Supporting Speculative Parallelization in the Presence of Dynamic Data Structures ChenTian, Min Feng, \nRajiv Gupta University ofCalifornia, CSE Department, Riverside, CA, 92521 {tianc, mfeng, gupta}@cs.ucr.edu \nAbstract The availability of multicore processors has led to signi.cant inter\u00adest in compiler techniques \nfor speculative parallelization of sequen\u00adtial programs. Isolation of speculative state from non-speculative \nstate forms the basis of such speculative techniques as this separa\u00adtion enables recovery from misspeculations. \nIn our prior work on CorD [35, 36] we showed that for array and scalar variable based programs copying \nof data between speculative and non-speculative memory can be highly optimized to support state separation \nthat yields signi.cant speedups on multicore machines available to\u00adday. However, we observe that in context \nof heap-intensive pro\u00adgrams that operate on linked dynamic data structures, state separa\u00adtion based speculative \nparallelization poses many challenges. The copying of data structures from non-speculative to speculative \nstate (copy-in operation) can be very expensive due to the large sizes of dynamic data structures. The \ncopying of updated data structures from speculative state to non-speculative state (copy-out operation) \nis made complex due to the changes in the shape and size of the dy\u00adnamic data structure made by the speculative \ncomputation. In ad\u00addition, we must contend with the need to translate pointers internal to dynamic data \nstructures between their non-speculative and spec\u00adulative memory addresses. In this paper we develop \nan augmented design for the representation of dynamic data structures such that all of the above operations \ncan be performed ef.ciently. Our ex\u00adperiments demonstrate signi.cant speedups on a real machine for a \nset of programs that make extensive use of heap based dynamic data structures. Categories and Subject \nDescriptors D.3.4[Processors]: Compil\u00aders General Terms Performance, Languages, Design, Experimenta\u00adtion \nKeywords SpeculativeParallelization, Multicore Processors 1. Introduction The thread level speculation \n(TLS) [7, 9, 10, 16, 21, 25, 31, 33, 37] technique has become increasingly important due to the availabil\u00adity \nof multicores. It allows the compiler to optimistically extract parallelism from sequential programs. \nIn particular, the compiler Permission to make digital or hard copies of all or part of this work for \npersonal or classroomuseisgrantedwithout feeprovidedthat copies arenot madeordistributed forpro.torcommercialadvantage \nandthatcopiesbearthisnoticeandthefullcitation onthe .rstpage.To copy otherwise,to republish,topostonserversorto \nredistribute tolists, requirespriorspeci.cpermission and/ora fee. PLDI 10, June5 10,2010,Toronto,Ontario, \nCanada. Copyright c &#38;#169; 2010 ACM 978-1-4503-0019-3/10/06. . . $10.00 creates multiple threads \nto execute different portions of a program in parallel optimistically assuming that no dependences exist \nbe\u00adtween these selected portions. TLS detects misspeculations, i.e. vi\u00adolations of these assumptions \nby detecting if dependences that were assumed to be absent manifest at runtime. To ensure the correct\u00adness \nof the execution, TLS must appropriately handle the results of speculative computations. Although considerable \nresearch work has been carried out on TLS, most of the work is hardware based and not ready for use. \nThis is due to the architectural redesigns [7, 21, 31, 37] requiring non-trivial hardware changes (e.g., \nspe\u00adcial buffers [10, 25, 26], versioning cache [9], versioning memory [8]) for detecting misspeculations \nand handling speculative results which have not been incorporated in commercial multicores. Another avenue \nof optimistically extracting parallelism from programs is based upon a purely software realization of \nTLS. While developing an ef.cient software implementation of TLS is chal\u00adlenging, the bene.ts of this \napproach are clear as it can be ap\u00adpliedtoexisting widelyavailable multicore systems.Recently, soft\u00adware \nbased TLS techniques have been proposed in [5, 17 19, 34 36]. These techniques have been shown to be \nquite effective in optimistic parallelization of streaming applications on multicores. While the work \nin [17 19] requires the programmer to provide re\u00adcovery code, the works in [5, 14, 34 36] are based upon \nrealization of state separation with no programmer help. In [5, 14] Ding et al. achieve state separation \nby creating separate processes for a non\u00adspeculative and speculative computations since each process \nhas its own address space, state separation is achieved. In CorD [34 36] we use a single process with \nmultiple threads -a nonspeculative main thread and speculative parallel threads. To achieve state sep\u00adaration, \nstorage is allocated separately for the threads and copying operations are performed to transfer data \nbetween threads. While the above software TLS techniques have been shown to achieve speedups without \nrequiring anymodi.cations to the archi\u00adtecture of existing multicores, they cannot be used for programs \nwith heap based dynamic data structures. In case of Ding et al. [5], shared variables which are assumed \nnot to be involved in depen\u00addences must be allocated on separate pages. This is because page level access \ntracking is employed to detect misspeculations. How\u00adever, in programs that employ dynamic data structures \nthat may contain millions of data items, it is not practical to allocate each of them on a separate page. \nThus, the process based approach pro\u00adposed by Ding et al. [5] is not suitable for dynamic data structures. \nThe work proposed by Kulkarni et al. [17 19] handles dynamic data structures but is not general as it \nis aimed at work list based applications. In our work on CorD [34 36], shared variables that may be modi.ed \nby speculative threads are copied into speculative threads memory space and modi.cations to these variables \nare tracked to detect misspeculations. In case of dynamic data structures inability to separate parts \nof a large dynamic data structure that are shared from those that are not shared will lead to too much \ncopying and access tracking overhead as safe assumption is consider the entire data structure as shared. \nThus, additional mechanisms are required to prevent unnecessary copying and checking in order to make \nour approach practical for programs with large dynamic data structures.  In this paper we develop mechanisms \nthat enable CorD to ef.\u00adciently supports speculative execution of programs that operate on heap based \nlinked dynamic data structures. In particular, we address the following challenges in the context of \nheap-intensive programs: What to Copy-In ? Complexities of pointer analysis makes it dif.cult to identify \nthe portion of the dynamic data structure that is referenced by the speculative computation. Conservatively \ncopying the entire data structure may not be practical when the size of the data structure is very large. \n How to Copy-Out ? The copying of updated data structure from speculative state to non-speculative state \nis made complex due to the changes in the shape and size of the dynamic data structure that may be made \nby the speculative computation.  How to handle internal pointers ? In addition, both copy-in and copy-out \noperations must contend with the need to translate pointers internal to dynamic data structures between \ntheir non\u00adspeculative and corresponding speculative memory addresses.  In this paper we address all \nof the challenges outlined above. First, we propose the copy-on-write scheme which limits the copy\u00ading \nto only those nodes in the dynamic data structure that are mod\u00adi.ed by the speculative computation. When \na speculative thread writes to a node in a dynamic data structure for the .rst time, the node is copied \ninto speculative state. If a speculative thread only reads a node in the non-speculative state, it is \nallowed to directly access the node from the non-speculative state. Second, we present the heap pre.x \naugmentation for the representation of dynamic data structures and double pointers representation for \ninternal pointers. These representations enable the translation of addresses during copy-in and copy-out \nto be ef.ciently handled, the implementation of runtime access checks preceding data structure accesses \nto be optimized, and runtime misspeculation checks preceding copy-out operations to be optimized. Our \nexperiments demonstrate signi.\u00adcant speedups on a real machine for a set of programs that make extensive \nuse of heap based dynamic data structures. 2. State Separationfor Dynamic Data Structures We begin by \nbrie.y summarizing our CorD [35] execution model and then discuss the challenges that must be overcome \nto specula\u00adtively parallelize programs with dynamic data structures. 2.1 State Separation In CorD (The \nCorD Model) As shown in Fig. 1, separate memory is al\u00adlocated to hold the non-speculative state of the \ncomputation and the state corresponding to the speculative threads. When a specu\u00adlative thread is created, \nit speculatively copies-in values of needed variables from non-speculative memory (also called D space) \nto speculative memory (also called P space), performs the specula\u00adtive computation, and if speculation \nis successful, the results pro\u00adduced are copied-out from speculative to non-speculative state. To enable \ncopying-out of values, a mapping table is used by each speculative thread. In the mapping table, each \ncopied-in variable has an entry, which have .ve .elds, namely D Address, P Address, Length, WriteFlag \nand Version. The .rst three .elds de.ne the ad\u00address mapping information. Whenavariableis modi.edinPspace, \nits WriteFlag is set which indicates it needs to be copied-out if the speculation succeeds. If misspeculation \noccurs, i.e. the speculative computation does not conform to the sequential program seman\u00adtics, the speculative \nstate is discarded and computation is repeated. To implement misspeculation detection, version numbers \nare main\u00adtained for each variable. In particular, when a thread performs a copy-in operation for a variable, \nit copies the current version num\u00adber of the variable into the Version .eld in the variable s mapping \nentry. The current version number is maintained by the main thread for each variable, and it is incremented \neach time the variable is copied out. When detecting the misspeculation, the main thread compares this \nversion with the one stored in the Version .eld for each variable. If the versions of every speculatively-read \n(copied\u00adin) variable are the same, then the speculation is successful. This is because these variables \nused by a speculative thread have not changed from the time they were read until the time at which spec\u00adulative \nresults are produced. However, if the version of any variable has been changed by an earlier speculative \nthread being executed in parallel on another core, then misspeculation occurs. The trans\u00adfer of data \nfrom (to)D space to (from)P space at the start (end) of a speculative computation must be highly optimized \nto minimize the impact of copying overhead on execution time performance. In [35, 36] we developed techniques \nfor achieving such optimizations for programs that mainly operate upon data held in global arrays and \nscalar variables. Figure 1. Separating Speculative And Non-speculative State. (An Example) Fig. 2 illustrates \nCorD using an example. Fig. 2(a) shows a sequential while loop which basically perform some computation \nusing variable key. If the computation returns FALSE, the value of key is updated. Fig. 2(b)-(d) shows \na possible situation where two consecutive iterations are executed in parallel under the CorD model. \nAs shown in Fig. 2(d), the non-speculative address of variable key is 0xA and its current version v is \nstored in the version table. From the code executed by the main thread, one can see that two parallel \nthreads are created. Before sending the start signal to each parallel thread, the main thread creates \ntwo local copies of key key and key for thread1and thread2respectively.It also addsa mapping entry into \nthe mapping table of each parallel thread. The entry shows theDaddress,Paddress, length and currentversionof \nkey. The WriteFlag is also set tofalse. When a parallel thread executes code speculatively, it uses its \nown local copy of key as shown in Fig. 2(b) and 2(c). Since the computation in thread 1 returns FALSE, \nthread 1 must update key and set the WriteFlag to true. Thread 2 simply performs the computation using \nkey and does not need to change the WriteFlag in the mapping table. Clearly, thread 2 is using a stale \nvalue of key because the previous iteration has changed the value. This misspeculation is detected by \nthe main thread as shown in Fig. 2(d). When thread1.nishes, the main thread compares theversionof key \nstored in the mapping table with the one stored in the version table. Since they are the same, copy-out \noperations are performed and  Figure 2. Separating Speculative And Non-speculative State. the version \nof key is incremented. When thread2.nishes, the main thread .nds a mismatch between two versions of key \n(i.e., v and v+1), and thus asks this parallel thread to re-execute the code using the latest value key. \nNote that the version comparison statement shown in Fig. 2(d) should be performed for all copied variables \nand a misspeculation is reported if anymismatch is found. From the above example one can see how state \nseparation is achieved using copying. Together with the version comparison based misspeculation detection, \nCorD is able to aggressively and safely exploit the parallelism in a sequential loop.  2.2 ChallengesFor \nDynamic Data Structures A dynamic data structure consists of large number of nodes such that each node \ncontains some data .elds and pointer .elds. The pointer .elds are used to link together the nodes in \nthe data structure (e.g., link lists, trees, queues etc.). Such data structures are also called dynamic \ndata structures because the shape and size of the data structure can change as the program executes. \nSize of the data structure changes as nodes are added or removed and changes in link pointers can further \nchange the shape of the data structure. The memory for each node is dynamically allocated from the heap \nwhen the node is created and freed by returning it to the heap when the node is eliminated. Dynamic data \nstructures are extensively used in wide range of applications. The applications used in our experimentation \nareC programs with following characteristics: each node is allocated and deallocated through explicit \ncalls to library functions malloc and free;and  nodes are accessed through pointers variables that point \nto the nodes such as pointer .elds that link the nodes in the data structure.  The state separation \nbased speculative parallelization for pro\u00adgrams using dynamic data structures is much more challenging \nthan for those using scalar variables or static data structures such as ar\u00adrays. In this section, we \nwill describe the challenges and develop techniques to address them. Given a parallelized computation \nwhich consists of a non\u00adspeculative main thread that spawns multiple speculative threads, in CorD model \nstate separation is achieved by performing spec\u00adulative computations in separate memory space. While \nfor array or scalar variables the separation can be simply achieved by cre\u00adating a copy of such variables \nin the speculative thread, achieving state separation for programs using dynamic data structures poses \nmany challenges. A dynamic data structure may contain millions of nodes (e.g., the program Hash in our \nexperiments creates 3 typedef struct node{ intkey; int val; struct node *next; } NODE *head; while (...) \n{ ... 1: NODE *tmp = .ndkey(head,key); 2: if (tmp!=NULL and tmp!=head) { 3: NODE *prev = get prev node(head, \ntmp); 4: prev .next = tmp .next; 5: tmp .next = head; 6: head = tmp; 7: } 8: else { 9: if (!tmp) {//update \nthe lru queue 10: //insert the new node 11: NODE *n = (NODE *)malloc(sizeof(NODE)); 12: n.key = ...; \n13: n.val = ...; 14: n.next = head; 15: head = n; //delete the least-recent used node 16: NODE *m = get \nsecond last node(head); 17: free(m.next); 18: m.next = NULL; 19: } 20: } 21: if (write.ag) 22: { 23: \nhead.val = ...;//modify data 24: } 25: ... = head.val; 26: ... } Figure 3. LeastRecent Use Buffer. million \nnodes at runtime). This leads to a large overhead due to copying operations, mapping table accesses, \nand misspeculation checks. Moreover, need for address translation arises because a node may have many \npointers pointing to it and after the node has been copied, accesses via these pointers must be handled \ncorrectly. For a program using dynamic data structure, four types of changes to the dynamic data structure \ncan be encountered: pointer .elds in some nodes are modi.ed causing the shape of the data structure \nto change;  a new node is created and linked to the dynamic data structure;  an existing node is deleted \nfrom the dynamic data structure causing the size of the data structure to change; and  values of data \n.elds in some nodes are modi.ed.  Fig. 3 shows an example where all the above changes are en\u00adcountered. \nIn this example, a link list is used to implement a least\u00adrecent-use (LRU) buffer. A LRU buffer has a \n.xed length and it buffers most recently used data. When data is requested, we .rst search for any matching \nelements in LRU (line 1). If a match is found and the element is not in the front of LRU buffer, we move \nthis element to the front by adjusting the pointers (lines 2-7). If no match is found, we create a new \nnode, insert it in the front and delete the last node (lines 8-20). After the requested data is put at \nthe front, we check if we need to modify the data (lines 21-24). Fi\u00adnally we read the data in thisbuffer \n(line 25). Let us assume that the requested data is frequently at the front of LRUbuffer and branches \nat lines 2, 8 and 21 are rarely taken. Thus, iterations in the while loop (lines 1-25) can be speculatively \nexecuted in parallel.  Whena parallel thread is created, all pointers(head, tmp, prev, m and n) will \nhave their own local storage in the corresponding speculative space. Note that head s local storage (denotedby \nhead ) will contain the content of head, as it is de.ned outside the paral\u00adlelizable region. We use *P \nto denote a node pointed to by pointer P. Next we describe the challenges via this example. Overhead \nChallenge. As we can see the function call .nd key at line 1 traverses all nodes in the LRU buffer. If \nthe original CorD modelisused,andtheLRUbuffer containsa million nodesat run\u00adtime, then the overhead of \nthis traversal will be prohibitively high. First, the copying overhead is large as all these nodes can \nbe po\u00adtentially modi.ed by speculative thread and hence will be copied into speculative state. Second, \nthe mapping table that maintains correspondence between addresses in non-speculative and specu\u00adlative \nmemory is large, because for every copied node, we need to maintain a mapping entry. A large mapping \ntable will lead to an expensive lookup and update. Last but not least, the version ta\u00adble is large. In \nthe original CorD model, a global version number of each node is stored in the version table and used \nin perform\u00ading misspeculation checks. In particular, if a node is modi.ed, its global version is compared \nwith its local version stored in the map\u00adping entry. Searching for the global version number of a node \nin a large version table requires time. Besides, the search has to be done for all modi.ed nodes. Finally, \n.nding modi.ed nodes from a large mapping table is very time-consuming. Consequently, the misspeculation \ncheck will be very time consuming. Address Translation Challenge. A node in a dynamic data struc\u00adture \nis allocated on the heap at runtime. Its address is stored in one or several pointer variables and its \naccess is performed through such pointers. This creates the address translation problem. In par\u00adticular, \nwhen a node is copied into speculative state by a copy-in operation, all pointers in speculative thread \nholding its address and being used in the computation must change their content to the ad\u00addressof the \ncopied node accordingly.Forexample,in Fig.3line5, a parallel thread will use head which is holding a \nnon-speculative state address as it is a copy of head. If the node *head has been copied during the execution \nof function .nd key, we must change the value of pointer head to the address of the head node s local \ncopy, and then assign this new value to tmp . next. This requires comparison of each pointer being accessed \n(in this example head ) with the addresses stored in the mapping table. Similarly, when a node is copied \nback to non-speculative state by a copy-out oper\u00adation, all pointers containing its current local copy \naddress need to hold its non-speculative state address now. In Fig. 3, when line 6 is executed, head \nwill point to the node *tmp, a local copy of some node in the non-speculative state. Therefore, when \nthe value of head is copied back to head, the address needs to be changed to the address of that node. \nThis can be done by consulting the map\u00adping table with the address stored in head . If the branch at \nline 8 is taken, copying out node *n is a chal\u00adlenge. At line 14, the next .eld of node *n is assigned \nwith the address of head node s local copy. However, when committing the result, the node *n is represented \nas the starting address and length. Therefore, we cannot .nd which part of *n is the starting address \nof the next pointer .eld, and thus, cannot translate the address. One solution might be to store the \naddress of next pointer in the map\u00adping table, but again, this may lead to an even larger mapping ta\u00adble \nas one node may contain multiple pointer .elds that are modi\u00ad.ed. Similarly, when line 17 is executed, \nwe also need the address translation so that the correct node in non-speculative state is deal\u00adlocated. \nIn the case of data .eld modi.cation (line 23), however, there is no need for address translation.  \n2.3 Copy-On-Write Scheme To address the overhead challenge, primarily we must .nd a way to reduce the \nnumber of nodes that are copied to speculative state. Therefore, we propose the use of copy-on-write \nscheme to limit the copying to only those that are modi.ed by the speculative thread. Anode in non-speculative \nstate is allowed to be read by speculative threads. It will be copied into a thread s speculative state \nonly when it is about to be modi.ed. The copying is implemented through an access check a block of code \ninserted by compiler to guard every node reference via a pointer. Based on the type of reference, read \nor write, access check code differs. (Write Access) Upon a write to a node, the access check will determine \nif the node is already in the speculative state. If this is the case, the execution can proceed. Otherwise, \nthe access check concludes that the address belongs to a node in non-speculative state. In this case \nthe speculative thread must determine if this is the .rst write to the node and thus the node must be \ncopied into the speculative space. However, if this is not the .rst write to the node, then the node \nhas already been copied into speculative state. Thus, the address being referenced in the non-speculative \nstate has to be translated into the corresponding address in the speculative state. This translation \nis enabled by ensuring that the mapping table is updated by creating entries for copied nodes. In other \nwords, the access checks will consult the mapping table to determine if the current pointer refers to \na node in speculative state or non\u00adspeculative state. (Read Access) Upon a read to a node, the access \ncheck allows the execution to continue if the node is in speculative state. How\u00adever, if the node is \nin non-speculative state, the access check stores the thread task ID for this node indicating when the \nnode has been read. After this step, the execution can proceed. The thread task ID is an integer maintained \nby each thread. It is initially zero and incre\u00admented by one every time the thread is assigned a task \nto perform by the main thread. As we will see shortly, this information is used during misspeculation \nchecks. It is worth noting that in this copy\u00adon-write scheme, if a node is only read by a speculative \nthread, it will never have an entry in the mapping table. In other words, all mapping entries contain \nnodes that have been modi.ed by the speculative thread. (Example) Applying this scheme to the example \nin Fig. 3, we can observe the following two advantages. First, there is no need to copy every node in \na dynamic data structure into speculative state when function .nd key is executed. Therefore, the size \nof the mapping table is reduced. Second, the need for address translation is greatly reduced. In particular, \nif node *head is never updated during execution, then we will not make a copy of this node. Con\u00adsequently, \nthe pointer tmp.next at line 5 and n.next at line 14 will get the correct non-speculative address of \nthis node without address translation. For line 17, we can also simply mark the ad\u00address stored in m.next \nas deallocated, instead of making a copyof a node *(m.next) and then translating the address.  2.4 Heap \nPre.x Although the copy-on-write scheme can reduce the size of mapping table, the access and update of \nthis table may still impose large overhead on the parallel execution. In particular, for each heap access, \nthe access check needs to consult the mapping table to see if a node has been copied or not. This requires \na walk through the entire table. Similarly, the misspeculation check needs to search the version number \nof each modi.ed node in the version table. This requires traversing the table multiple times. To ef.ciently \nperform the access checks and misspeculation checks we associate meta-data with each node that tracks \ncertain information related to accesses of the node. We call this meta-data by the name of heap pre.x. \nNext we describe the details of heap-pre.x and show how it is effective in reducing the overhead of using \nmapping table and version table.  Figure 4. Heap Pre.xFormat. For each memory chunk allocated on the \nheap, we allocate2 *n additional bytes in front of it where n is the total number of specu\u00adlative threads. \nThese bytes represent the heap pre.x which is used to store important information to assist in access \nchecks. The for\u00admat of the heap pre.x is shown in Fig. 4. The .rst n bytes im\u00admediately before the program \ns original heap data are the status bytes. The additional n bytes are meta-data bytes. In the status \nbyte, byte i represents the status for speculative thread i and it can rep\u00adresent four different possible \nstatus values. Status NOT COPIED means the heap data has not been copied into thread i s speculative \nstate. Status ALREADY COPIED means the heap data has been copied into thread i s speculative space, and \nthe index of this entry in thread i s mapping table is stored in the corresponding meta\u00addata byte. Status \nALREADY READ means the heap data has been read by thread i, and the corresponding meta-data bytes stores \nthe task ID of thread i. Status INTERNAL indicates that the node is already in the speculative state. \nTherefore, status NOT COPIED, ALREADY COPIED and ALREADY READ only appear in heap elements of non-speculative \nstate and status INTERNAL only ap\u00adpears in heap elements in speculative state. In the meta-data bytes, \nmeta-data byte i stores either a index number of the mapping table of thread i, or the task ID of thread \ni. Note that one can put the meta-data associated with each node in a different place and use hash function \nto locate it [32]. However, we tried the hash-based solution and observed that it caused over 6x slowdowns \nfor the benchmarks we used. The reason is that a hash based lookup requires the execution of a hash function, \nwhich takes more time than performing a simple offset calculation. Thus, large number of lookups make \nthe hash based solution yield visible slowdowns. 2.4.1 Implementing Access Checks With the status bytes \nand meta-data bytes in the heap pre.x, the access check for a heap node access in thread i can be implemented \nas shown in Fig. 5. Thread i s status s in the node s pre.x is examined and following actions are taken. \nIf s is NOT COPIED and the access is a read, s is updated to ALREADY READ and the task ID of thread i \nis stored in the meta-data byte i (lines 7-10). If the access is a write, s is updated to ALREADY COPIED.A \nnew local copyof the node is then created with the corresponding status byte to be set to INTERNAL. Next, \na mapping entry is added into the mapping table to re.ect this copy operation and the index of the entry \nis stored in the meta-data byte i. Finally, the pointer points to the newly created node (lines 11-18). \nIf s is ALREADY COPIED, then that means the node has been copied; thus, address translation is needed. \nFortunately, the mapping entry can be quickly located through meta-data byte i and we only need to adjust \nthe pointer to point to the address of the local copy (lines 20-23). Finally, if s is ALREADY COPIED \nand the accessisawrite,thenweperformthecopy-in operationaswhen s is NOT COPIED (lines 25-32). Otherwise, \nthe access is a read or s is INTERNAL. In both cases, no further actions are required. Fig. 6 shows an \nexample of using heap pre.x to perform ac\u00adcess checks. First, assuming the node is allocated at 0xA in \nnon\u00ad 1: type = access type, READ or WRITE; 2: p = the pointer holding the starting address of the node \nbeing accessed; 3: len = the size of the node being accessed; 4: s = thread i s status at *p; 5: m = \nthread i s meta-data byte at *p; 6: if (s == NOT COPIED) 7: if (type == READ) { 8: s = ALREAD READ; 9: \nm = task ID; 10: } 11: else {// type == WRITE 12: s = ALREADY COPIED; 13: pointerq = make copy(*p); 14: \nset thread i s status at *q to INTERNAL; 15: index = update mapping table(p, q, len); 16: set thread \ni s meta-data byte at node to index; 17: p = q; 18: } 19: } 20: else if (s == ALREADY COPIED) { 21: index \n= thread i s meta-data byte at node; 22: p = getPaddress from mapping table entry index; 23: } 24: else \nif (s == ALREADY READ) { 25: if (type == WRITE) { 26: s = ALREADY COPIED; 27: pointerq = make copy(*p); \n28: set thread i s status at *q to INTERNAL; 29: index = update mapping table(p, q, len); 30: set thread \ni s meta-data byte at node to index; 31: p = q; 32: } 33: } 34: else {//s == INTERNAL 35: ;//do nothing \n36: } Figure 5. AccessChecks. speculative state (D space), consider the execution of speculative thread \nT3. Before any reference to this node in T3, the status byte for T3 shows that the node has not been \ncopied into its speculative state (P space) yet (as shown on the left). Suppose there is a write to this \nnode during the execution, the access check will make a copy of this node as it sees the status is NOT \nCOPIED. Therefore, the following actions will be taken. A new node is allocated at 0xB in P space and \ninitialized with the original node value; a mapping entry is created in the mapping table (its index \nis x); T3 s status of the original node is changed to ALREADY COPIED indicating that this node has been \ncopied into speculative memory, and the corresponding meta-data byte of T3 stores the indexof mapping \nentry(x); T3 s status of the copied node is set to ALREADY COPIED which means this node is already in \nspeculative state. In the later execution of T3, if the original node is accessed through some other \npointers, the access checks can easily translate those pointers to point to 0xB by looking at the heap \npre.x and the x-th mapping entry. Similarly, if the node starting at 0xB is about to be accessed by a \npointer, the access check code will con.rm the access to be valid by simply looking at the pre.x of this \nnode. After committing T3 s result, the local copyof the node will be deallocated and T3 s status byte \nand meta byte in the pre.x of the original node will be reinitialized to zero (shown on the right). In \nsummary, there are two main advantages of using heap-pre.x to implement access checks. First, the status \nbyte can tell the access checks whether or not a node has been copied. Second, the meta\u00addata bytes allow \nthe speculative thread to .nd the mapping entry in O(1) time, which speeds up the process of address \ntranslation for copy-in operations.  Figure 6. An Example Of Heap StatusTransition.  2.4.2 Implementing \nMisspeculation Checks To determine if speculation is successful, misspeculation checks have to be performed. \nThe main thread maintains a version number for each variable in a version table. When a speculative thread \nuses a variable, it makes note of the variable s current version number. Whenthe resultsofa speculative \nthread aretobe committedto non\u00adspeculative state, misspeculation check is performed by the main thread. \nThe main thread ensures that the current version number of a variable is the same as it was when the \nvariable was .rst used by the speculative thread. If no mismatch is found, the speculation is considered \nas successful. Otherwise, misspeculation occurs and the result will be discarded. This is because speculative \nthread must have prematurely read the variable. This method worked effectively for array variables and \nscalar variables [35, 36]. In a program using dynamic data structures, however, the num\u00adberof nodesin \nthe structure canbevery large, and hence theversion table can become very large. Consequently, searching \na node in the version table can impose large runtime overhead. Now that we have the heap pre.x that can \ntell how the node is being used by other threads at any time, we can exploit this information to perform \nthe misspeculation check for the dynamic data structure without using a version table. The key idea of \nour approach is that when the main thread checks the result of thread i, it also checks if any other \nthread is using a node modi.ed by thread i. If so, that thread s execution will fail as it is working \non an incorrect speculatively read value. This method works because the main thread commits results of \nspeculative threads in a sequential order. 1: if (spec[i] == FAIL) 2: return FAIL; 3: for each node mapping \nentry e in mapping[i] { 4: for each thread j s status on e.addr non-spec s[j] { 5: if (s[j] == ALREADY \nCOPIED) 6: spec[j] =FAIL; 7: if (s[j] == ALREADY READ and meta-data[j] == taskID[j]) 8: spec[j] =FAIL; \n9: } 10: } 11: return SUCCESS; Figure 7. MisspeculationChecksFor Heap Objects. Fig. 7 shows our algorithm. \nFor each node mapping entry e in the mapping table of thread i, the main thread examines other thread \ns status byte of the node starting at e.addr non-spec. If an\u00adother thread s status byte is ALREADY COPIED, \nthen speculation of that threadfails (line 5-6). Note that status ALREADY COPIED means the node has been \nmodi.ed and hence has an entry in the mapping table. When the node is copied back, the status byte and \nmeta-data byte for thread i is reset to zero. If the main thread .nds that another thread j s status \nbyte is ALREADY READ, then situation may be more complex because the status ALREADY READ can be set during \nthe current work assigned to thread j or during a previously assigned work to thread j. The latter case \nhappens if in an earlier iteration, thread j only read this node. Therefore, there was no entry in the \nmapping table and hence the status byte and meta-data byte were not cleared. However, these two cases \ncan be distinguished using the task ID stored in the meta-data byte j. The main thread only needs to \ncheck if the meta-data byte j s value is equal to thread j s current task ID. If they are the same, then \nthread j sexecutionis marked asfailed. Figure 8. A Possible Data Race During Misspeculation Check. Note \nthat there exists a data race between checking thread j s status byte and setting the byte. However, \nthis data race is harmless as it does not affect correctness. This is because we require the main thread \nto commit the result before checking other threads status bytes and the speculative thread to update \nthe status byte before accessing the node. Fig. 8 shows an example where step 2 and 3 are clearly racing \nagainst each other. If step 2 reads the value after step 3, then thread j s execution will be marked \nas failed which is correct, because thread j may read the old value of x (step4 happens beforestep1).Ifstep2 \nreadsthevalue before step 3, thread j s execution will not be marked as failed. This is also correct \nbecause thread j is using the latest value of x. In summary, the advantage of using heap-pre.x in implement\u00ading \nmisspeculation checks is that status bytes is used to identify any two threads that are accessing the \nsame node. This eliminates the requirement of maintaining a version number of each node.  2.4.3 Discussion \nOn Meta-data Bytes For each thread, we choose to use one byte to store the meta-data, i.e., the index \nof the mapping entry or the task ID. Since one byte can at most hold 256 numbers, using one byte may \nimpose some limitations in certain situations and hence needs to be discussed. First, if the meta-data \nbyte of a thread is used to store mapping entry indexes, the mapping table size must have less than 256 \nentries to avoid over.ow. This means for each task, the number of modi.ed node should be less than 256. \nIn some cases, this assumption may not be true. If a mapping table over.ow occurs, the corresponding \ntask should be considered asfailed to ensure the correctness. However, having too many over.ow events \nmeans that using one byte is not enough and performance loss results.To solve this problem, we can pro.le \nthe program to .nd out how many nodes are modi.ed in each task on average and choose multiple bytes to \nstore the indexes for each thread if necessary. Second, if the task ID is stored in one byte, the number \nmay also wrap around and cause a problem in a very extreme case. Speci.cally, when thread i writes a \nnode at iteration a and thread j reads the same node at iteration b where b = a (mod 256) and it never \nuses the node after that, we may incorrectly mark thread j asfailed.However,evenif thisextreme case arises, \nthe correctness is not affected at all a false misspeculation is reported and the computation is unnecessarily \nrepeated.   2.5 DoublePointers As described earlier, the need for address translation can be reduced \nby using copy-on-write scheme. The address translation is needed only for a copied node. It can be done \nby using the status byte and meta byte during the copy-in operation. However, we need to ef.ciently perform \nthe address translation for a pointer during a copy-out operation. This is because nodes being copied-out \nmay have manypointer .elds. Figure 9. Internal Pointer. Fig. 9 shows a simple example involving two \nnodes pointed to by pointers p and p.child1. The node *p has already been copied in from non-speculative \naddress 0xA to speculative address 0xB as shown in the mapping table. Now when the assignment is about \nto execute, the node *(p.child1) will be copied from 0xC into 0xD. The value of (p.child1) will also \nchange to 0xD so that the assignment takes effect on the local copy starting at 0xD. At the time of committing \nresults to non-speculative state, the main thread must scan the mapping table to copy these two nodes \nback to the non-speculative state. However, the value in (p.child1) is still 0xD and of course it needs \nto be translated to 0xC.To do this, one way would be to locate the .eld by adding it to the mapping table, \nand then comparing the value in this pointer with all P addr in the mapping table. This process entails \nsigni.cant overhead in programs using dynamic data structures as all nodes are linked into the structure \nthrough pointers. To tackle the above problem, we present an augmented pointer representation double \npointers. For each pointer variable p, the compiler will allocate8bytes.4bytes for the non-speculative \nstate address (denoted by p.D addr) and 4 bytes for speculative state address (denoted by p.P addr). \nWhen a node is allocated by the main thread and pointed to by p, its starting address is stored in p.D \naddr. When a node is created by a speculative thread and pointed by a pointer p , the starting address \nof this node is stored in p .P addr. If the node is created as a part of the copy-in operation, we set \np .D addr to be p.D addr (assuming p is the local copy of p). Otherwise, we set p .D addr to be p .P \naddr.For anyreference of a pointer p, if it is in the main thread, then p.D addr will be used; otherwise \np.P addr willbe used.Fora pointer assignment p= q, all 8 bytes will be copied. With this scheme, we \ncan easily resolve the problem shown in Fig. 9. Consider now the illustration in Fig. 10 where A denotes \n*p andB denotes *(p.child1). As we can see in Fig. 9(a), before the assignment, A s local copy keeps \nB s non-speculative address in the D addr .eld. Its P addr .eld has been set to NULL. After the assignment, \na local copy of B has been created and pointed by the P addr .eld of pointer p.child1. When we copy these \ntwo nodes back, we do not have to go to the mapping table for address translation. Instead, we can directly \ncopythem back, asAstill holds B s address in the non-speculative state. The double pointers scheme also \nensures the correctness when the shape of a dynamic data structure changes due to the update of some \npointer .elds in the computation. Let us consider the example in Fig. 3 again where three possible pointer \nrelated changes are encountered. (Changing the Shape) If the branch at line 2 is taken, the node pointed \nto by tmp will be moved into the front of the buffer and pointed by head. Fig. 11 shows the process of \nthis shape transformation under our scheme. As shown in Fig. 11(a) Before executing line 4, the parallel \nthread has two local pointers prev and tmp , which are the copies of pointers prev and tmp respectively. \nSince the statement at line 4 updates the node pointed to by prev (node B), a local copy of node B is \ncreated through the access check, and the P addr .eld of prev points to this copy. After line 4, the \nnext pointer .eld of node B contains the address of node D. When executing line 5 as shown in Fig. 11(b), \na local copy of node C is created and pointed by the P addr .eld of pointer tmp . After this statement, \nall 8 bytes of the original pointer head are copied into the next pointer .eld in node C . Thus, the \nD addr .eld in next contains the address of node A. The statement at line 6 creates a local copy of pointer \nhead. As shown in Fig. 11(c), after copying the contents of pointer tmp , its D addr .eld has the address \nof node C and P addr has the address of node C . Finally, copy-out operations in the result-committing \nstage will change the content of pointer head and the next pointer .eld of nodeB and C. The updated pointer \nis represented by the dash line in Fig. 11(d), which re.ects the update to the LRUbuffer. (Adding A New \nNode) If the branch at line 9 is taken, a new node is allocated and the least recent used node is deallocated. \nFig. 12 shows the process of adding a new node. In Fig. 12(a), a new node N is created by a parallel \nthread and pointed by a local pointer n . After line 14, the next .eld of this new node is the same as \nthe head pointer whose D addr bytes are storing the address of node A. After line 15, the parallel thread \nchanges the content of head pointer by creating head and making its both D addr and P addr .elds store \nthe address of node N (see Fig. 12(b)). When copy-out operation, the main thread can recognize the new \nnode by checking if the two .elds of head are the same. After the copying operation, pointer head is \npointing the new nodeNwhich is now in the front of the LRUbuffer (see Fig. 12(c)).  (Deleting A Node) \nFig. 13 shows the process of executing lines 16-18, which deallocates the last node in the LRUbuffer. \nAs shown in Fig. 13 (a), after executing line 16, a local pointer m is created by a parallel thread and \npointing to the second last node B. When free is called on nodeC at line 17, the parallel thread simply \nmarks the node as deallocated in the mapping table instead of actually call the function. This is because \nnode C is in the non-speculative state and it cannot be deallocated until the speculative computation \nperformed by this parallel thread is decided to be correct. After line 18, a local copy of node B is \ncreated because its pointer .elds is speculatively set to NULL. In the result-committing stage, the main \nthread will actually deallocate the nodeC based on the mark in the mapping table and the next pointer \n.eld (8 bytes) of node B is also set to NULL due to the copy-out operation. 2.6 Techniques And Their \nBene.ts Challenges Copying Operation Overhead Copy-on-write Scheme Reduced Heap Pre.x - Double Pointers \n- Mapping Table Access Overhead Reduced Reduced - Misspeculation Check Overhead - Reduced - Address Translation \nOverhead (Copy-in) Reduced Reduced - Address Translation Overhead (Copy-out) - - Reduced Table 1. Techniques \nAnd TheirBene.ts. In this section, we introduced three techniques to address the overhead and address \ntranslation problems when speculatively par\u00adallelizing a program using dynamic data structures. The three \ntech\u00ad niques were: copy-on-write, heap pre.x, and double pointers. Ta\u00adble1 summarizes the bene.ts of \nthese techniques. 3. Other Optimizations 3.1 Eliminating Unnecessary Checks An access check precedes \neach write access that is performed to the heap in speculative state. Although implementing access checks \nvia heap pre.x can greatly reduce their overhead, the overhead of ac\u00adcess checks can be still signi.cant \ndue to the frequencywith which they are performed. Therefore, in this section, we develop addi\u00adtional \ncompile-time optimizations for eliminating access checks. 3.1.1 Locally-created Heap Objects When a node \nis created by a speculative thread, it will have a valid speculative state address. Therefore, accesses \nperformed to a locally created node do not require access checks. The algorithm is shown in Fig. 14 provides \nsimple compile-time analysis to identify accesses that are guaranteed to always access locally created \nnodes. For each basic block, we .rst identify pointers that hold an address returned from a malloc function \ncall. Then we track propagation of these pointers to other pointer variables and thus identify additional \naccesses that do not require an access check. In the analysis, each pointer assigned by malloc is placed \ninto the GEN set. If a pointer is assigned with a pointer that is not holding a local heap address, it \nis placed in the KILL set. Then we compute the IN set for every basic block in a control .ow graph based \non the equations shown in this .gure. Given the IN set of a basic block, it is easy to determine whether \nor not to introduce an access check before a pointer dereference. Initialize IN(B0 )= \u00d8; OUT(B)={IN(B)-KILL(B)}.GEN(B); \nT IN(B)= OUT(P ); P .pred(B) where GEN(B)= {p : .p = malloc(...) in B } .{p : p = q where q .IN(B)or \nGEN(B)}; KILL(B)= {p : p = r where r ./IN(B)and GEN(B)}; Figure 14. Locally Created Heap Objects. 3.1.2 \nAlready-copied Heap Objects Given a reference to a node, if we are certain that there is an earlier write \nwhich caused the node to be copied, then we do not need an access check for the reference. The analysis \nrequired for this optimization is quite similar to the analysis described in the preceding optimization. \nThe difference is that the GEN set contains pointers through which a write is performed to a node instead \nof pointers assigned by malloc.  3.1.3 Read-Only Heap Access If, following initialization, a node is \nonly read throughout the ex\u00adecution, then it is impossible for such a node to cause a misspec\u00adulation. \nTherefore, access checks are not required for such nodes at all. However, it is challenging to identify \nsuch nodes at compile time due to pointer aliasing. Speci.cally, the same memory address may be pointed \nto by two or more pointers at runtime. During com\u00adpile time, even if we identify that the access to a \nnode through one pointer is always a read, the node may still be modi.ed through another pointer at runtime. \nFortunately, there has been much research work on alias anal\u00adysis. For any two pointers, the alias analysis \nresponds with three possible answers, yes , maybe and no , indicating they do or maybe or do not point \nto the same location.We can take advantage of such analysis to conservatively identify read-only nodes. \nIn par\u00adticular, any two pointers with answer yes or maybe from alias analysis, are considered as aliases. \nNext, we can perform the anal\u00adysisshowninFig,15.Forany accesstoa node througha pointerin ReadOnlySet, \nwe do not insert any access checks, because these accesses must involve read-only nodes.  Initialize \nReadOnlySet(S)= {all pointer variables}; for each pointer p { if there is a write access to the address \nheld in p { ReadOnlySet(S)= ReadOnlySet(S)-{p}; } } Figure 15. Finding Read-Only Heap Object.  3.2 \nOptimizing Communication In our original CorD model [35, 36], the communication between the main thread \nand speculative threads was implemented through expensive system calls such as read and write to pipes. \nUse of pipes strictly prevents speculative threads from accessing the non\u00adspeculative state memory, as \nvalues required by parallel threads are passed through pipes. This mechanism works .ne in [35] because \nfew values need to be communicated at runtime. However, the same is not true in this work many more \nvalues are communicated to speculative threads even when copy-on-write is used. Thus, overhead of using \npipes to pass values is high. In this paper, we relaxed the restriction of the state separation by allowing \na speculative thread to directly read the non-speculative state memory. This results in highly ef.cient \ncommunication. We also use busy-waiting algorithms to synchronize threads because they achieve low wake-up \nlatency and hence yield good perfor\u00admance on a shared memory machine [23]. 4. Experiments 4.1 Experimental \nSetup To show the effectiveness of our techniques, we use7 benchmarks from the LLVM test suite and SPEC2000 \nthat make intensive use of heap based dynamic data structures. In Table 2 the .rst two columns give the \nname and the description of each program. The next column shows the type of dynamic data structure used \nand the last column shows the original source of the program. The rest of the programs in these benchmark \nsuites were not used due to one or more of the following reasons: theydo not contain parallelizable \nloops;  even though theycontain parallelizable loops, no speculation is required for parallelization; \nor  theycontain parallelizable loopsbut seldomly use dynamic data structures, and thus can be ef.ciently \nparallelized using other techniques [5, 34 36].  Similar to the previous works on speculative parallelization, \nwe parallelize loops for which the number of loop-carried dependen\u00adcies is below a threshold number. \nName BH MST Power Patricia Treesort Hash Mcf Description Barns-Hut Alg. Mininum Spanning Tree Power pricing \nPatricia trie Tree sorting Hashtable Vehicle scheduling Dynamic Data Structure Tree Tree, hash Graph, \nhash Tree, hash Tree List, hash List, graph Original Source Olden Olden Olden Mibench Stanford Shootout \nSpec2000 Table 2. Dynamic Data Structures Benchmarks. In our experiments, we .rst use Pin [15] instrumentation \nframe\u00adwork to pro.le loops in these programs with a smaller input. Then the runtime dependences are analyzed \nand regions that are good candidates for speculative parallelization are identi.ed. Next the LLVM [20] \ncompiler infrastructure is used to compile these pro\u00adgrams together with the analysis result and our \nparallelization tem\u00adplate, so that the sequential version can be transformed into the par\u00adallel version. \nThe parallelization template contains the implemen\u00adtation of our runtime system including thread creation, \ninteraction, mapping table, misspeculation check etc. During the transforma\u00adtion of the code, access \nchecks are inserted preceding each heap access. The pro.ling is performed for a small input and the exper\u00adimental \ndata is collected by executing parallelized programs on a large input. All our experiments were conducted \nunder Fedora 4 OS running on a dual quad-core (i.e.,8 cores) Xeon machine with 16 GB memory. Each core \nruns at 3.0 GHz.  4.2 Performance We .rst compare the execution time of a program between its se\u00adquential \nversion and parallel version. In this experiment, all the techniques and optimizations are used. We observed \nthat running these programs under the original CorD model [35] led to at least 2x slowdown of parallel \nversions over sequential versions regard\u00adless of the number of speculative threads. However, with our \npro\u00adposed techniques, signi.cant speedup is obtained. Fig. 16 shows the execution speedup of these programs \nfor varying number of speculative threads created by the main thread. Number of Parallel Threads Our \nresults show that for all programs except Patricia, the speedup continues to increase as the number of \nspeculative threads is increased from1to 7. In particular, the highest speedup of Power is 3.2, and that \nof other programs is between 1.56 and 2.5 when 7 speculative threads are used. In the case of Patricia, \nwe found that signi.cant amount work has to be done sequentially by the main thread. Also, every speculative \nthread needs to copy up to 1 MB memory during execution. When more threads are used, more memory is copied. \nThis can cause L2 cache pollution and hence dramatically affect the performance. For Patricia best speedup \nof 1.74 is achieved when we use three speculative threads. Since we have a total of 8 cores, when 8 speculative \nthreads are used in addition to the main thread, the speedup decreases. Except for MST and BH, all programs \nactually have a slowdown. The reason is due to the use of busy-wait synchronization. When 8 speculative \nthreads and one main thread are run on an 8 core machine, context switch is required. However, using \nbusy-wait constructs makes each thread to aggressively occupy a core. This leads to even worse performance. \nIt is worth noting that using pipe will not have this problem as the main thread will be descheduled \nby OS. According to our experiments, however, the use of pipe causes the parallel execution to be much \nslower than the sequential one regardless of the number of parallel threads. We also observe that using \none speculative thread is slower than the sequential version. In the case of Treesort, even using two \nthreads cannot obtain anyspeedup. This behavior can be attributed to the overhead introduced by our model \nthat more than nulli.es the limited parallelism bene.ts.  We also measured the misspeculation rate of \neach execution. The highest rate we observed is 10.2% for mcf when using 7 parallel threads. This benchmark \nalso has a large sequential portion. These two factors make the highest speedup of this program only \n1.56. For other programs, the misspeculation rate is less than 1%. Thus, misspeculations have little \nimpact on the performance.  4.3 Overhead Analysis 4.3.1 Time Overhead We classify the execution time \ninto 4 categories: communication (time spent on busy-waiting constructs), access checks, misspecu\u00adlation \ncheckfollowed by copy-out operations, and computation.For speculative threads, we measured these times \nand averaged them across the threads. The experiment was conducted for 2, 4, and 7 threads. The results \nare shown in Fig. 17. Communication Access Check Computation we can see, for all programs except for \nPatricia and Mcf, the com\u00admunication dominates the main thread s execution, which means the main thread \nis waiting for the results of speculative threads most of the time. During the rest of the time, the \nmain thread does more work on misspeculation checks and copy-out operations for Treesort and Power, and \nmore work on sequential computation for Patricia, Mcf, BH, MST and Hash. For the last 3 programs, the \nsequential computation portion becomes larger as the number of parallel threads increases from2 to7. \nThisis becausethe totalex\u00adecution time is reduced due to greater parallelism and hence the sequential \npart becomes a greater fraction of the total execution time.  4.3.2 Space overhead While the parallel \nexecution is faster, it requires more memory space as each node has extra bytes for heap pre.x and double \npoint\u00aders and each thread needs its own space. Therefore, we conducted an experiment to measure the peak \nvalue of memory consumption of the parallelized program for varying number of threads. Fig. 19 shows \nthe results. Parallel Thread Execution Time Breakdown 120% 100% 80% 60% 40% 20% 0% Number of Parallel \nThreads 247247 247 247 247 247 247 Treesort BH MST Patricia Hash Power Mcf Figure 19. Space Overhead. \nFigure 17. Time Breakdown: Spec. Threads. As we can see, the memory consumed by the parallel version \nof all programs is between 1.1x and 3.2x compared to the sequen- From the .gure, we can clearly see that \nregardless of the total number of speculative threads, each thread, on an average, spent from at least \n50%(Treesort)to nearly 100%(MST)of the time on the computation. For some benchmarks like Treesort and \nPatricia, a signi.cant amount of time is spent on access checks. The com\u00ad munication time is very low \nfor all benchmarks, i.e. these threads tial version. Note that for most benchmarks except for Treesort \nand Hash, the space overhead caused by heap pre.x and double pointers is at most 50% (when 7 threads \nare used) and often less than 20%. This is because each node in these programs takes over 60 bytes with \nabout 2 to 6 pointers. Other space overhead mostly comes from the coping operations. Thanks to the copy-on-write \ndo not spend much time on waiting for their work. scheme, only a small number of nodes need to be copied \nand hence the total overhead is not very large. For Treesort and Hash, how- Communication Misspec. Check \n&#38; Copy-out Computation 120% 100% 80% 60% 40% 20% 0% Treesort BH MST Patricia Hash Power Mcf Figure \n18. Time Breakdown: Main Thread. 18 shows the execution time breakdown for ever, the node size is only \n12 bytes. Therefore, the double pointer scheme and heap pre.x cause signi.cant space overhead, espe\u00adcially \nwhen more threads are used as shown in the .gure.  4.4 Effectiveness of Optimizations As unnecessary \naccess checks can be eliminated by the proposed analysis, we compared the number of static and dynamic \naccess checks with and without optimizations. Table 3 shows the number of eliminated checks and the total \nnumber of checks without any elimination. From this table, we .rst observe that a small number of static \naccess checks lead to millions of dynamic checks. This is because access checks are inserted inside loops \nthat have millions of iterations. We can also see that, on an average, our optimiza\u00adtion eliminates 69.5% \nof static access checks which correspond to 71.5% of dynamic access checks. So without the optimization, \neach thread may waste signi.cant number instructions at runtime on per\u00adforming unnecessary checks. the \nmain 4.5 Comparison withTransactional Memory Main Thread Execution Time Breakdown Fig. thread, which \nis responsible for assigning work to speculative Transactional memory (TM) [1, 3, 11, 12, 24, 27, 29] \nhas been threads, performing misspeculation checks followed by copy-out an active area of research. It \nis designed to enforce the atomicity operations, and executing the sequential part of the program. As \nof shared memory accesses in parallel programs and cannot be Program Name BH Checks Eliminated Static \n5/7 (71.4%) Dynamic (Million) 0.55/0.67 (82.1%) MST 7/11 (63.6%) 8.9/13.5 (65.9%) Power 55/60 (91.6%) \n9.5/10.8 (88.0%) Patricia 53/66 (80.3%) 62.4/79.7 (78.3%) Treesort 3/6 (50%) 72.7/143.2 (50.7%) Hash \n7/12 (58.3%) 312.8/463.3 (67.5%) Mcf Average 20/28 (71.4%) 69.5% 968.2/1418.9(68.2%) 71.5% Table 3. \nEffectiveness Of Eliminating Access Checks. directly used to parallelize sequential programs [22]. However, \nsince it has the capability of tracking dependences and detecting dependence violations between two transactions, \nwe conducted an experiment to see the performance of using software based TM (STM) in the speculative \nparallelzation work. In this experiment, we manually transform the program such that each task is put \ninto a transaction and every access to the potentially shared memory in a task is monitored by the TM \nsys\u00adtem. Similar to [22], we also add the explicit synchronizations into transaction functions to enforce \nthe in-order commit, This is im\u00adportant for maintaining the sequential program semantics. The TM implementation \nwe used is based on a state-of-art algorithm -Sun s Trasactional Locking2(TL2) [4].Table4shows the speedup \ncom\u00adparisons between our approach and STM-based solution when2,4 and7 parallel threads are used respectively. \nBH Programs MST Power Patricia Treesort Hash Mcf Ours 1.33 1.63 1.27 1.50 0.97 1.12 1.15 STM 0.59 2threads \n0.84 0.93 0.43 0.34 0.64 0.54 Ours 2.06 2.34 2.47 1.63 1.62 1.41 1.30 STM 0.68 4threads 0.94 0.97 0.52 \n0.41 0.73 0.58 Ours 2.25 2.61 3.20 1.40 1.78 1.92 1.56 STM 0.73 7threads 1.02 1.08 0.47 0.40 0.87 0.61 \n Table 4. Speedup Comparisons. From the table, we can see that using STM in speculative execu\u00adtion has \nslowdowns in most cases. Only for Powerand MST,a slight speedup can be achieved when7 parallel threads \nare used. Our re\u00adsults are consistent with [22] which also shows that STM typically nulli.es the performancegains \nin compiler parallelized sequential applications. There are several reasons for the performance loss. \nFirst, STM needs special mechanisms to avoid or resolve dead\u00adlock and live-lock situations. Second, STM \naims to achieve good throughput andfairness. This requires STM to consider the priori\u00adties of transactions \n[32]. Besides, STM internally uses locks to pre\u00advent data races [4] and barriers to ensure strong atomicity \n[28, 30] and in-order commit [22]. These special considerations are not nec\u00adessary for speculative parallelization. \nInstead, they result in high runtime overhead for STM while providing a convenience for pro\u00adgrammers \nwriting parallel applications. Note that Mehrara et al. [22] propose customized STM for spec\u00adulative \nparallelization. Their work assumes dependent variables can be identi.edat compiletimeandthusthey useasetof \nspecialreg\u00adisters to track such variables. However, for the programs using dy\u00adnamic data structures, \ncross-iteration dependences cannot be rec\u00adognized statically. Therefore, their work is not applicable \nfor the class of programs we consider. 5. Related work The TLS technique is useful in improving the performance \nof sequential code. While there has been much research on TLS, it is hardware based requiring redesigned \narchitectures [7, 21, 31, 37] or non-trivial modi.cations to the existing architectures such as special \nbuffers [10, 25, 26], versioning cache [9], or versioning memory [8]. However, the hardware features \nare not present in any commercial multicores and this makes software speculation attractive as it can \nbe used on existing machines. Recently, software based TLS techniques have been proposed [5, 14, 17 19, \n35]. Although all of them can be implemented purely in software, they use different schemes to handle \nspeculative exe\u00adcution. The work proposed in [5, 14, 34 36] are all based upon the realization of state \nseparation. In particular, the result of spec\u00adulative computations are stored in a separate space instead \nof the non-speculative state. If misspeculation occurs, the result is simply discarded. Otherwise, the \nresult is merged into the non-speculative state. State separation in [5, 14] is achieved by using different \nad\u00address spaces (process-based) and hence entails larger copying over\u00adhead compared to the scheme using \nthe same address space (thread\u00adbased) [35]. Although OS-assisted techniques using paging hard\u00adware [2, \n5, 14] can be used to implement copy-on-write at page level,thefalse-sharing problem leadstoexcessive \nmisspeculations. To tackle this problem, the prior work [5] proposed to allocate each potentially shared \nvariable on a separate page. However, this ap\u00adproach is impractical for heap based dynamic data structures. \nThe reason is that such data structures may contain millions of nodes and hence, it is impossible to \nallocate one memory page for every node. Our other work on CorD [34 36] was also not practical for programs \nusing dynamic data structures. In this paper we developed techniques that enable CorD to support dynamic \ndata structures. Kulkarni et al. proposed another speculative scheme [17 19] that allows the non-speculative \nstate to be overwritten by specu\u00adlative results. Once a misspeculation occurs, the original state can \nbe recovered by performing the reverse computation of the specula\u00adtive one. However, this scheme is only \napplicable for the programs using work lists. Besides, users need to use special constructors, mark all \ncommute functions, which can be executed in any order, and de.ne the reverse computation of each commute \nfunction in their programs. This places much burden on the users. In contrast, our method is pro.ling \nbased and the transformation is performed by the compiler automatically. Recently, Johnson et al. [13] \nand Du. et al. [6] presented differ\u00adent compiler algorithms on decomposing a sequential instruction stream \ninto multiple speculative threads. In other words, theyfocus on how to identify and construct parallel \nthreads from a sequential program, and hence their work is complimentary to our work. 6. Conclusion For \nprograms using heap based dynamic data structures, specula\u00adtive parallelization is challenging. This \nis because the size of the dynamic data structure can be very large, moving heap data be\u00adtween non-speculative \nstate and speculative state can be expensive, and address translation of accesses to data structure .elds \nis needed. We proposed techniques and optimizations that effectively address these challenges. Our experiments \nshow maximum speedups from 1.56 to 3.2 on a real machine for a set of programs that make ex\u00adtensive use \nof heap based dynamic data structures. Acknowledgments This work is supported by NSF grants CCF\u00ad0963996, \nCCF-0905509, CNS-0751961, and CNS-0810906 to the University ofCalifornia, Riverside. References [1] A.-R. \nAdl-Tabatabai, B. T. Lewis, V. Menon, B. R. Murphy, B. Saha, and T. Shpeisman. Compiler and runtime support \nfor ef.cient soft\u00adware transactional memory. In PLDI 06: Proceedings of the 2006 ACM SIGPLAN conference \non Programming language design andim\u00adplementation, pages 26 37, 2006. [2] D.Bovet andM. Cesati. Understanding \nthe linuxkernel.2005.  [3]P. Damron,A. Fedorova,Y.Lev,V. Luchangco,M. Moir,andD.Nuss\u00adbaum. Hybrid transactional \nmemory. In ASPLOS-XII: Proceedings of the 12th international conference on Architectural support for \npro\u00adgramming languages and operating systems, pages 336 346, 2006. [4] D. Dice, O. Shalev, and N. Shavit. \nTransactional locking ii. In Proceedings of the 20th Intl. Symp. on Distributed Computing. [5] C. Ding, \nX. Shen, K. Kelsey, C. Tice, R. Huang, and C. Zhang. Software behavior oriented parallelization. In PLDI \n07: Proceedings of the 2007 ACM SIGPLAN conference on Programming language design and implementation, \npages 223 234, 2007. [6] Z.-H. Du, C.-C. Lim, X.-F. Li, C. Yang, Q. Zhao, and T.-F. Ngai. A cost-driven \ncompilation framework for speculative parallelization of sequential programs. In PLDI 04: Proceedings \nof the 2004 ACM SIGPLAN conference on Programming language design and imple\u00admentation, pages 71 81, 2004. \n[7] M. Franklin andG.S. Sohi. Arb:Ahardware mechanism fordynamic reordering of memory references. IEEE \nTransactions on Computers, 45(5):552 571, 1996. [8] M.J. Garzar\u00b4an, M. Prvulovic,J. M. Llaber\u00b4ia,V.Vi \nnals, L. Rauch\u00adwerger, and J. Torrellas. Tradeoffs in buffering speculative memory state for thread-level \nspeculation in multiprocessors. Transactions on Architecture and Code Optimization, 2(3):247 279, 2005. \n[9] S. Gopal, T. N. Vijaykumar, J. E. Smith, and G. S. Sohi. Speculative versioning cache. In HPCA 98: \nProceedings of the 4th International Symposium on High-Performance Computer Architecture, 1998. [10] \nL. Hammond, M.Willey, and K. Olukotun. Dataspeculation support for a chip multiprocessor. In ASPLOS-VIII: \nProceedings of the eighth international conference on Architectural support for programming languages \nand operating systems, pages 58 69, 1998. [11] M. Herlihy,V. Luchangco, M. Moir, andW. N. Scherer, III. \nSoftware transactional memory for dynamic-sized data structures. In PODC 03: Proceedings of the twenty-second \nannual symposium on Princi\u00adples of distributed computing, pages 92 101, 2003. [12] M. Herlihy and J. \nE. B. Moss. Transactional memory: Architectural support for lock-free data structures. In ISCA 93: Proceedings \nof the 20th Annual International Symposium on Computer Architecture, pages 289 300, 1993. [13]T.A.Johnson,R. \nEigenmann,andT.N.Vijaykumar. Min-cut program decomposition for thread-level speculation. In PLDI 04: \nProceedings of the 2004 ACM SIGPLAN conference on Programming language design and implementation, pages \n59 70, 2004. [14] K.Kelsey,T. Bai,C. Ding, andC. Zhang.Fast track:A softwaresys\u00adtem for speculative program \noptimization. In CGO 09: Proceedings of the 2009 International Symposium on Code Generation and Opti\u00admization, \npages 157 168, 2009. [15] C. keung Luk, R. Cohn, R. Muth, H. Patil, A. Klauser, G. Lowney, S.Wallace,V.Janapa, \nandR.K. Hazelwood. Pin: Building customized program analysis tools with dynamic instrumentation. In PLDI \n05: Proceedings of the 2005ACM SIGPLAN conference on Programming language design and implementation, \npages 190 200, 2005. [16] V. Krishnan and J. Torrellas. The need for fast communication in hardware-based \nspeculative chip multiprocessors. In PACT 99: Pro\u00adceedings of the 1999 International Conference on Parallel \nArchitec\u00adtures and CompilationTechniques, pages 24 33, 1999. [17] M. Kulkarni, M. Burtscher, R. Inkulu, \nK. Pingali, and C. Casc\u00b8aval. How much parallelism is there in irregular applications? In PPoPP 09:Proceedingsofthe \n14thACM SIGPLANsymposiumonPrinciples and practice of parallel programming, pages 3 14, 2009. [18] M. \nKulkarni, K. Pingali, G. Ramanarayanan, B. Walter, K. Bala, and L. P. Chew. Optimistic parallelism bene.ts \nfrom data partitioning. In ASPLOS XIII: Proceedings of the 13th international conference on Architectural \nsupport for programming languages and operating systems, pages 233 243, 2008. [19] M. Kulkarni, K. Pingali, \nB. Walter, G. Ramanarayanan, K. Bala, and L.P. Chew. Optimistic parallelism requires abstractions.In \nPLDI 07: Proceedings of the 2007ACM SIGPLAN conference on Programming language design and implementation, \npages 211 222, 2007. [20] C. Lattner andV. Adve. LLVM:A Compilation Framework forLife\u00adlong Program Analysis \n&#38; Transformation. In CGO 04: Proceedings of the 2004 International Symposium on Code Generation and \nOpti\u00admization, pages 75 88, 2004. [21] P. Marcuello and A. Gonz\u00b4alez. Clustered speculative multithreaded \nprocessors. In ICS 99: Proceedings of the 13th international confer\u00adence on Supercomputing, pages 365 \n372, 1999. [22] M. Mehrara, J. Hao, P.-C. Hsu, and S. Mahlke. Parallelizing sequen\u00adtial applications \non commodity hardware using a low-cost software transactional memory. In PLDI 09: Proceedings of the \n2009 ACM SIGPLAN conference on Programming language design and imple\u00admentation, pages 166 176, 2009. \n[23] J. M. Mellor-Crummey and M. L. Scott. Algorithms for scalable synchronization on shared-memory multiprocessors. \nACM Trans. Comput. Syst., 9(1):21 65, 1991. [24] M. J. Moravan, J. Bobba, K. E. Moore, L. Yen, M. D. \nHill, B. Lib\u00adlit, M. M. Swift, and D. A. Wood. Supporting nested transactional memory in logtm. SIGOPS \nOper. Syst. Rev., 40(5):359 370, 2006. [25] M. Prvulovic, M.J. Garzar\u00b4an, L. Rauchwerger, andJ.Torrellas. \nRe\u00admoving architectural bottlenecks to the scalability of speculative par\u00adallelization. SIGARCH Comput. \nArchit. News, 29(2):204 215, 2001. [26] C.G.Qui nones,C. Madriles,F.J.S\u00b4anchez,P. Marcuello,A. Gonz\u00b4alez, \nandD.M.Tullsen. Mitosis compiler: an infrastructure forspeculative threading based on pre-computation \nslices. In PLDI 05: Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and \nimplementation, pages 269 279, 2005. [27] B. Saha, A.-R. Adl-Tabatabai, R. L. Hudson, C. C. Minh, and \nB. Hertzberg. Mcrt-stm: a high performance software transactional memory system for a multi-core runtime. \nIn PPoPP 06: Proceedings of the 11th ACM Symp. on Principles and Practice of Parallel Pro\u00adgramming, pages \n187 197, 2006. [28] F. T. Schneider, V. Menon, T. Shpeisman, and A.-R. Adl-Tabatabai. Dynamic optimization \nfor ef.cient strong atomicity. In OOPSLA 08: Proceedings of the 23rd ACM SIGPLAN conference on Object\u00adoriented \nprogramming systems languages and applications, 2008. [29] N. Shavit andD.Touitou. Software transactional \nmemory. Distributed Computing, 10(2):99 116, 1997. [30] T. Shpeisman, V. Menon, A.-R. Adl-Tabatabai, \nS. Balensiefer, D. Grossman, R. L. Hudson, K. F. Moore, and B. Saha. Enforcing isolation and ordering \nin stm. In PLDI 07: Proceedings of the 2007 ACM SIGPLAN conference on Programming language design andim\u00adplementation, \npages 78 88, 2007. [31] G. Sohi, S. E. Breach, andT. N.Vijaykumar. Multiscalarprocessors. In ISCA 95: \nProceedings of the 22nd annual international symposium on Computer architecture, pages 414 425, 1995. \n[32] M.F. Spear,L. Dalessandro,V.J. Marathe, andM.L. Scott.Acompre\u00adhensive strategy for contention management \nin software transactional memory. In PPoPP 09:Proceedingsof the 14thACM SIGPLANsym\u00adposium on Principles \nand practice of parallel programming, 2009. [33] J. G. Steffan, C. B. Colohan, A. Zhai, and T. C. Mowry. \nA scalable approach to thread-level speculation. In ISCA 00: Proceedings of the 27th annual international \nsymposium on Computer architecture, 2000. [34] C.Tian,M. Feng, andR. Gupta. Speculative parallelization \nusingstate separation and multiple value prediction. In ISMM 10: Proceedings of the 2010 International \nSymposium on Memory Management, 2010. [35] C. Tian, M. Feng, V. Nagarajan, and R. Gupta. Copy or discard \nexecution model for speculative parallelization on multicores. In MICRO 08: Proceedings of the 2008 41st \nIEEE/ACM International Symposium on Microarchitecture, pages 330 341, 2008. [36] C. Tian, M. Feng, V. \nNagarajan, and R. Gupta. Speculative paral\u00adlelization of sequential loops on multicores. International \nJournal of Parallel Programming, 37(5):508 535, 2009. [37] J.-Y. Tsai, J. Huang, C. Amlo, D. J. Lilja, \nand P.-C. Yew. The su\u00adperthreaded processor architecture. IEEETransactions on Computers, 48(9):881 902, \n1999.   \n\t\t\t", "proc_id": "1806596", "abstract": "<p>The availability of multicore processors has led to significant interest in compiler techniques for speculative parallelization of sequential programs. Isolation of speculative state from non-speculative state forms the basis of such speculative techniques as this separation enables recovery from misspeculations. In our prior work on CorD [35,36] we showed that for array and scalar variable based programs copying of data between speculative and non-speculative memory can be highly optimized to support state separation that yields significant speedups on multicore machines available today. However, we observe that in context of heap-intensive programs that operate on linked dynamic data structures, state separation based speculative parallelization poses many challenges. The copying of data structures from non-speculative to speculative state (copy-in operation) can be very expensive due to the large sizes of dynamic data structures. The copying of updated data structures from speculative state to non-speculative state (copy-out operation) is made complex due to the changes in the shape and size of the dynamic data structure made by the speculative computation. In addition, we must contend with the need to translate pointers internal to dynamic data structures between their non-speculative and speculative memory addresses. In this paper we develop an augmented design for the representation of dynamic data structures such that all of the above operations can be performed efficiently. Our experiments demonstrate significant speedups on a real machine for a set of programs that make extensive use of heap based dynamic data structures.</p>", "authors": [{"name": "Chen Tian", "author_profile_id": "81333491427", "affiliation": "University of California, Riverside, Riverside, CA, USA", "person_id": "P2184503", "email_address": "", "orcid_id": ""}, {"name": "Min Feng", "author_profile_id": "81365596375", "affiliation": "University of California, Riverside, Riverside, CA, USA", "person_id": "P2184504", "email_address": "", "orcid_id": ""}, {"name": "Rajiv Gupta", "author_profile_id": "81100027751", "affiliation": "University of California, Riverside, Riverside, CA, USA", "person_id": "P2184505", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1806596.1806604", "year": "2010", "article_id": "1806604", "conference": "PLDI", "title": "Supporting speculative parallelization in the presence of dynamic data structures", "url": "http://dl.acm.org/citation.cfm?id=1806604"}