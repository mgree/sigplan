{"article_publication_date": "06-05-2010", "fulltext": "\n Finding Low-Utility Data Structures GuoqingXu Nick Mitchell Matthew Arnold Ohio State University IBM \nT.J. Watson Research Center IBM T.J. Watson Research Center xug@cse.ohio-state.edu nickm@us.ibm.com marnold@us.ibm.com \n Atanas Rountev Edith Schonberg GarySevitsky Ohio State University IBM T.J. Watson Research Center IBM \nT.J. Watson Research Center rountev@cse.ohio-state.edu ediths@us.ibm.com sevitsky@us.ibm.com Abstract \nMany opportunities for easy, big-win, program optimizations are missed by compilers. Thisis especially \ntrueinhighly layered Java applications. Often at the heart of these missed optimization op\u00adportunities \nlie computations that, withgreat expense,produce data values that have little impact on the program s \n.nal output. Con\u00adstructing a newdateformatter toformat everydate, orpopulating a large setfullof expensively \nconstructed structures only to checkits size:theseinvolve coststhat are out ofline withthebene.tsgained. \nThisdisparitybetween theformation costs and accruedbene.ts of data structuresis at theheart of much runtimebloat. \nWe introduce a run-time analysis to discover these low-utility data structures. The analysis employs \ndynamic thin slicing, which naturally associates costs with value .ows rather than raw data .ows. It \nconstructs a model of the incremental, hop-to-hop, costs and bene.ts of each data structure. The analysis \nthen identi.es suspicious structures based on imbalances of its incremental costs and bene.ts. To decrease \nthe memory requirements of slicing, we introduce abstract dynamic thin slicing, which performs thin slicing \noverbounded abstractdomains.Wehave modi.ed theIBM J9 commercialJVM toimplement this approach. Wedemonstratetwoclientanalyses:onethat \n.ndsobjectsthat are expensive to construct but are not necessary for the forward execution, and second \nthat pinpoints ultimately-dead values. We have successfully applied them to large-scale and long-running \nJava applications. We show that these analyses are effective at detecting operations thathave unbalanced \ncosts andbene.ts. Categories and Subject Descriptors D.3.4 [Programming Lan\u00adguages]: Processors Memory \nmanagement, optimization, run\u00adtimeenvironments; F.3.2[Logics and Meaning of Programs]:Se\u00admantics of Programming \nLanguages Program analysis; D.2.5 [SoftwareEngineering]:TestingandDebugging Debugging aids GeneralTerms \nLanguages,Measurement,Performance Keywords Memory bloat, abstract dynamic thin slicing, cost\u00adbene.t analysis \nPermission to make digital or hard copies of all or part of this work for personal or classroomuseisgranted \nwithoutfeeprovided that copiesarenot madeordistributed forpro.tor commercial advantage andthat copiesbearthis \nnotice andthefull citation onthe .rstpage.Tocopy otherwise,torepublish,topostonserversortoredistribute \ntolists, requiresprior speci.cpermission and/or afee. PLDI 10, June5 10,2010,Toronto,Ontario,Canada. \n1. Introduction Thefamilyofdata.ow optimizationsdoes agoodjobin .nding and .xing local program inef.ciencies. \nThrough an iterative process of enabling and pruning passes, a data.ow optimizer reduces a program s \nwasted effort.Inlining, scalar replacement, useless store elimination,dead code removal, and codehoisting \ncombinein often powerful ways. However, our experience shows that a surprising number of opportunities \nof this ilk, ones that would be easy to implementfor adeveloper, remain untappedbycompilers[19,34]. Despite \nthe commendable accomplishments of the commercial JIT teams, optimizers seem to have reached somewhat \nof an im\u00adpasse[16,22].In a typical large-scale commercialapplication, we routinely.ndmissed opportunitiesthatcanyield \nsubstantialperfor\u00admance gains, in return for a small amount of developer time. This is only true, however, \nif theproblem is clearly identi.ed for them. The sea of abstractions can easilyhide what wouldbeimmediately \nobvious to adata.ow optimizer as well as ahuman expert. In this regard, developers face the very same \nimpasse that JITs have reached. We write code that interfaces with a mountain of libraries and frameworks. \nThe code we write and use has been ex\u00adplicitlyarchitected tobe overlygeneral andlatebound, tofacilitate \nreuse and extensibility. One of the primary effects of this style of development is todecrease the ef.cacy \nofinlining[29], which is a lynchpin of the data.ow optimization chain. A JIT often can not, in a provably \nsafe way, specialize across a vast web of subroutine calls; adeveloper should not start optimizing thecodeuntilde.ni\u00adtive \nevidenceisgiventhatthegenerality andpluggability costtoo dearly. However, if we wish to enlist the help \nof humans, we cannot have them focus on every tiny detail, as a compiler would. Inef.\u00adciencies arethe \naccumulation of minutiae,including alarge volume oftemporary objects[11,29],highly-stale objects[5,26], \nexces\u00adsivedata copies[34], andinappropriate collection choices[28,35]. The challenge is tohelp thedevelopers \nperformdata.ow-style op\u00adtimizationsbyproviding them with relatively smallproblems. Very often, the required \noptimizations center around theforma\u00adtion and use of data structures. Many optimizations are as simple \nashoisting an object constructor out of aloop.In other cases, code paths must be specialized to avoid \nfrequent expensive conditional checks that are always true, or to avoid the expense of computing data \nvalues hardly, if ever, used. In each of these scenarios, a de\u00adveloper, enlisted to tune theseproblems, \nneedn tbepresented with aregister-transferdata.owgraph.There may existlargeropportu\u00adnities if they focus \non high-level entities, such as how whole data structures arebeingbuilt up and used. Copyright c &#38;#169; \n2010 ACM 978-1-4503-0019-3/10/06... $10.00  Cost and bene.t A primary symptom of these missed optimiza\u00adtion \nopportunities is animbalance between costs and bene.ts.The cost of forming a data structure, of computing \nand storing its val\u00adues, are out of line with the bene.ts gained over the uses of those values. For example, \nthe DaCapo chart benchmark creates many lists andadds thousands ofdata structures tothem,forthe solepur\u00adpose \nof obtaining list sizes. The actual values stored in the list en\u00adtries are never used.These entrieshave \nnon-trivialformation costs, but contain values of zerobene.t to the endgoal of theprogram. In this paper, \nwe focus on these low-utility data structures.A data structure has low utility if the cumulative costs \nof creating its member .elds outweighs a weighted sum over the subsequent uses of those .elds. For this \npaper, we compute these based on the dynamic .ows that occur during representative runs. The cost of \na member .eld is the total number of bytecode instructions transitively required to produce it; each \ninstruction is treated as havingunit cost.Theweightedbene.t ofthis .elddependsonhow it is used; we consider \nthree cases: a .eld that reaches program output has in.nite weight; a .eld that is used to (transitively) \nproduce a new value written into another .eld has a weight equal totheworkdone(onthestack) tocreatethatnewvalue; \n.nally,a .eld that is used for neither has zero weight. We show how these simple, heuristic assignments \nof cost and bene.t are suf.cient to identify manyimportant,large chunks of suboptimal work. Abstract \nthin slicing formulation A natural way of computing cost and bene.t for a value produced by an instruction \ni is to perform dynamic slicing and calculate the numbers of instructions that can reach i (i.e., for \ncomputing cost) and that are reachable from i (i.e., for computing bene.t) in the dynamic dependence \ngraph. Conventionaldynamic slicing[33,37,38,40]does notdistin\u00adguish .ows along pointers from .ows along \nvalues. Consider an example assignment b.f = g(a.f). The cost of the value b.f shouldincludethecost of \ncomputing the .eld a.f,plusthe costs of the g logic.Adynamic slicingapproach would alsoincludethe cost \nof computing the a pointer.An approach basedon thin slicing[30] would exclude any pointer computation \ncost. When analyzing the utilityofdata structures,thisis amore useful approach.Ifthe object referencedby \na is expensive to construct,thenits costshouldbe as\u00adsociated with the objects involved in facilitating \nthat pointer com\u00adputation. For example, had there existed another assignment c.g =a, c would be the object \nto which a s cost should be attributed, not b. We therefore formulate our solution on top of dynamic \nthin slicing. By separating pointer computations from value .ow, this approach naturally assignsformation \ncosts todata structures. Dynamic thin slicing, in general, suffers from a problem com\u00admon to most client \nanalyses ofdynamic slicing[2,9,34,41]:large memory requirements. While there exists a range of strategies \nto reducethese costs[15,27,33,38,39,41],itis stillprohibitively expensive toperform whole-programdynamic \nslicing.The amount of memory consumedbythese approachesis afunction ofthe num\u00adber ofinstructioninstances(i.e.,depending \ncompletely ondynamic behavior), whichis verylarge, and also unbounded. To improve the scalability of \nthin slicing, we introduce ab\u00adstractdynamicthin slicing,atechniquethatappliesthinslicing over bounded \nabstractdomains thatdivide equivalent classes amongin\u00adstruction instances. The resulting dependence graph \ncontains ab\u00adstractions of instructions, rather than their runtime instances. The keyinsighthereis that,having \nnoknowledge of the client that will use the pro.les, traditional slicing captures every single detail \nof the execution, much of which is not needed at all by the client. By taking into account the semantics \nof the client (encoded by the bounded abstract domain) during the pro.ling, one can record onlythe abstraction \nofthe whole executionpro.lesthat satis.esthe client,leadingto signi.canttime and space overhead reduction.For \nexample, with abstract dynamic thin slicing, the amount of mem\u00adory required for the dependence graph \nis bounded by the number of abstractions. Section 2 shows that, in addition to cost-bene.t analysis, \nsuch slicing cansolvearangeof otherbackwarddynamic .owproblems that exhibitbounded-domain properties. \nRelative costs and bene.ts How far back should costs accumu\u00adlate, and how far forward should bene.ts \naccrue? A straightfor\u00adward application of dynamic (traditional or thin, concrete or ab\u00adstract) slicing \nwould suffer from the ab initio, ad .nem problem: costs would accumulate from the beginning, and bene.ts \nwould accruetothe end of theprogram s run.Valuesproducedlaterdur\u00ading theexecution arehighly likely tohavelargercoststhanvalues \nproduced earlier. In this case, there would not exist a correlation between high cost and actual performance \nproblems, and such an analysis would notbe usefulforperformance analysis.To alleviate this problem, we \nintroduce the notion of relative cost and bene.t. The relative cost of a data structure D is computed \nas the amount of work performed to construct D from other data structures that already exist in the heap. \nThe relative bene.t of D is the .ip side of this: the amount of workperformed to transformdata readfrom \nD in orderto constructotherdata structures.We showhowto com\u00adpute the relative costs and bene.ts over \nthe abstractions; these are termed the relative abstract cost and relative abstractbene.t. Implementation \nand experiments Tohelp theprogrammer diag\u00adnoseperformanceproblems, weintroduce several cost-bene.t anal\u00adyses \nthat take as input the abstractdynamic dependence graph and report information related to the underlying \ncauses. Section 3 de\u00ad.nesindetail one of these analyses, which computes relative costs andbene.ts at \nthelevel ofdata structuresby aggregating costs and bene.tsforindividualheaplocations. These analyses \nhave been implemented in the IBM J9 com\u00admercial JVM and successfully applied to large-scale and long\u00adrunning \napplications such as derby, tomcat and trade.Section 4 presents an evaluation ofanalysis expenses.A shadowheapis \nused torecordinformationabout .eldsofliveobjects[24].Thedepen\u00addence graph consumes less than 20 megabytes \nacross the applica\u00adtions andbenchmarkstested.The currentprototypeimplementation imposes an average slowdown \nof 71 times when whole-program trackingis enabled.We show thatitispossible to reduce overhead signi.cantly \nif one tracks only important phases. For example, by tracking only the steady-state portion of a server \ns run, overheads are reduced by up to 10\u00d7. This overhead, admittedly high, has nonethelessproved acceptableforperformance \ntuning. Section 4 describes six case studies. Using the tool, we found hundreds ofperformanceproblems, \nand eliminating them resulted in2% 37%performanceimprovements.Theseproblemsinclude inef.ciencies caused \nby common programming idioms, repeated work whose results need to be cached, computation of redundant \ndata, and choices of unnecessary expensive operations. Some of these .nding also provide useful insights \nfor automatic code opti\u00admizationin compilers. The contributions of this work are: Cost and bene.t pro.ling, \na methodology that identi.es run\u00adtimeinef.cienciesby understanding thecostofproducing val\u00adues and thebene.t \nof consuming them.  Abstractdynamicthin slicing, ageneraltechniquethatperforms dynamic thin slicing \nover bounded abstract domains. It pro\u00adduces much smaller and more relevant slices than traditional dynamic \nslicing and can be used for cost-bene.t analysis and for otherdynamic analyses.  Relative cost-bene.t \nanalysis which reportsdata structures that have unbalanced cost-bene.t rates.   A J9-based implementation \nand six case studies using real\u00adworld programs, demonstrating that the tool can help a pro\u00adgrammerto \n.ndopportunitiesforperformanceimprovements. 2. CostComputationUsingAbstractSlicing This section formalizes \nthe proposed technique for abstract dy\u00adnamic thin slicing, and shows several clients that can take advan\u00adtage \nof this technique. Next, the de.nition of cost is presented to\u00adgether with a run-time pro.ling technique \nthat constructs the nec\u00adessary abstractdatadependencegraph.While ourtool works onthe low-levelJVMintermediate \nrepresentation,thediscussion ofthe al\u00adgorithms uses a three-address-code representation of the program. \nInthis representation, eachstatement corresponds to abytecodein\u00adstruction(i.e.,itiseithera copy assignment \na=b or a computation a=b+c that contains only one operator). We will use terms state\u00adment and instruction \ninterchangeably, both meaning a statementin the three-address-code representation. 2.1 AbstractDynamicThinSlicing \nThecomputation of costsof run-timevalues appearstobe aprob\u00adlem which can be solved by associating a small \namount of track\u00ading data with each storage location and by updating this data as the correspondinglocationis \nwritten.A similar techniquehasbeen adopted,for example,intaint analysis[25], wherethetrackingdata is \na simple taintmark.In the setting of cost computation, the track\u00ading data associated with each location \nrecords the cumulative cost of producing the value that is written into the location. For an in\u00adstruction \ns, a simple approach of updating the trackingdatafor the left-hand-side variable is to store in it the \nsum of the tracking data for allright-hand-side variables,plus the cost of s itself. However, this simple \napproach may double-count cost. To il\u00adlustrate this, consider the example code shown in Figure 1. Using \nthetaint-like .owtracking(showninFigure 1(a)),thecosttb of producing the value in b is 8, which must \nbe incorrect as there are only 5 instructions in the program. This is because the cost of c is included \nin the costs of both d and b. As the costs of c and d are added to obtain b s cost, c s cost is considered \ntwice. While this difference is small for the example, it can quickly accumulate and become signi.cantwhen \nthe size of theprogramincreases.For ex\u00adample, as wehave observed, this cost canquicklygrow and cause \na 64-bitintegerto over.owfor even moderate-size applications.Fur\u00adthermore, such dynamic .ow tracking \ndoes not provide any addi\u00adtionalinformationaboutthedata .owanditsusefulnessforhelping diagnoseperformanceproblemsislimited. \nTo avoid double-counting in the computation of cost tb, one could record all instruction instances before \nvariable b is written and theirdependences(as shown inFigure 1(b)).Costtb can then be computed by traversing \nbackward the dependence graph from instruction 4 and counting the number of instructions. There are many \nother dynamic analysis problems that have similar charac\u00adteristics. These problems, for example, include \nnull value propa\u00adgation analysis[9],dynamic objecttype-state checking[2],event\u00adbasedexecutionfastforwarding[41],copy \nchainpro.ling[34],etc. Onecommonfeatureof thisclassofdynamic analysesisthatthey require additional traceinformation \nrecordedfor thepurpose ofdi\u00adagnosis or debugging, as opposed to simpler approaches such as taint analysis. \nBecause the solutions these analyses need to com\u00adpute arehistory-relatedand canbe obtainedbytraversingbackward \nthe recordedtraces,we willrefertothem asbackwarddynamic .ow (BDF)problems. In general, BDF problems can \nbe solved by dy\u00adnamic slicing[14,33,37,38,40].In our example ofcost computa\u00adtion,the cost of a valueproducedby \naninstructionis essentiallythe size ofthedata-dependence-based backwarddynamic slice starting from theinstruction. \n 1a\u00a0=\u00a00;ta =\u00a01; 2c\u00a0=\u00a0f(a);tc =\u00a0t+\u00a01; 3d\u00a0=\u00a0c\u00a0*\u00a03;td =\u00a0tfc +\u00a01; 4b\u00a0=\u00a0\u00a0c\u00a0+\u00a0d; tb =\u00a0tc +\u00a0td +\u00a01; 5 int\u00a0f(int\u00a0e){ \n6 return e >>\u00a02;\u00a0\u00a0 tf =\u00a0te +\u00a01;\u00a07} (a) (b) Figure 1. An example illustrating the double-counting problem: \n(a)Example code and the step-wise updating of tracking data. ti denotes the tracking data for variable \ni; (b)its dynamic data de\u00adpendence graph where instructions are represented by their line numbers. An \nedge represents a def-use relationship between two instructions. Indynamic slicing,theinstrumentedprogramis \n.rstexecutedto obtain an execution trace with control .ow and memory reference information. At a pointer \ndereference, both the data that is refer\u00adenced and thepointer value(i.e.,the address of thedata) are \ncap\u00adtured.Our technique considers onlydatadependences and the con\u00adtrolpredicates are treatedin a special \nway asdescribedlater.Based on adynamicdatadependencegraphinferredfromthetrace, a slic\u00adingalgorithmis \nexecuted.Let I bethedomain ofstaticinstructions and N be thedomain of natural numbers. DEFINITION 1. \n(Dynamic Data Dependence Graph). A dynamic data dependence graph(V, E) has node set V . I\u00d7N, where each \nnodeis a staticinstruction annotated with aninteger j, repre\u00adsenting the j-th occurrence of thisinstructioninthetrace.Anedge \nfrom aj to bk (a, b .I and j, k .N)shows that the j-th occur\u00adrence of a writes alocation thatisthen usedby \nthe k-th occurrence of b, without an intervening write to thatlocation. If aninstruction accesses a heap \nlocation through v.f, the reference value in stack location v is also considered tobe used. Thin slicing[30]is \na statictechnique thatfocuses on statements that .ow values to the seed, ignoring the uses of base pointers. \nIn this paper, the technique is re-stated for dynamic analysis. A thin data dependence graph, formed \nfrom the execution trace, has ex\u00adactly the same set of nodes as its corresponding dynamic data de\u00adpendencegraph.However,for \nan accessv.f,thebasepointer value in v isnotconsidered tobeused.Thisproperty makesthinslicing especially \nattractive for computing costs for programs that make extensive use of object-oriented data structures \nwith traditional slicing, the cost of each data element retrieved from a data struc\u00adture would include \nthe cost ofproducing the object references that form the layout of the data structure, resulting in signi.cant \nim\u00adprecision. A thin data dependence graph contains fewer edges and leads to smaller slices.Bothfor standard \nand thindynamic slicing, the amount of memory required for representing the dynamic de\u00adpendencegraph \ncannotbebounded before orduring the execution. For some BDF problems, there exists a certain pattern \nof backward traversal that can be exploited for increased ef.ciency. Among the instruction instances \nthat are traversed, equivalence classes can usually be seen. Each equivalence class is related to a certain \nproperty of an instruction from the program code, and distinguishing instruction instances in the same \nequivalence class (i.e., withthe sameproperty)does not affect the analysisprecision. Moreover, it is \nonly necessary to record one instruction instance online as the representative for that equivalence class, \nleading to signi.cant space reduction of the generated execution trace. Sev\u00aderal examples of suchproblems \nwillbediscussed shortly. To solve such BDF problems, we propose to introduce the se\u00admantics of a target \nanalysis into pro.ling, by de.ning a problem\u00ad  1\u00a0a1\u00a0=\u00a0new\u00a0A(); 1\u00a0File\u00a0f\u00a0=\u00a0new\u00a0File();1 2\u00a0f.create(); \n 3\u00a0a2\u00a0=\u00a0new\u00a0A(); 3\u00a0i\u00a0=\u00a00;2  5  o 3  3   !3 4\u00a0c=\u00a0b; 43 5 8\u00a0h\u00a0=\u00a0e\u00a0+\u00a01; 87 9 5\u00a0f.put(...); 5 23!i \n< < 0 ( g o     . . g0 1 . . !0O .0. ... . ( 5 . \u00b7 . . 2\u00a0b\u00a0=\u00a0a1.f; 21 4\u00a0if(i\u00a0<\u00a0100){ 5\u00a0a2.f\u00a0=\u00a0c; \n6 6\u00a0d\u00a0=\u00a0new\u00a0D(); 6\u00a0 7\u00a0e\u00a0=\u00a0a2.f; 7\u00a0f.put(...); 7 4 O1 8\u00a0i++;\u00a0goto\u00a04;\u00a0} 9\u00a0d.g\u00a0=\u00a0h; 9\u00a0f.close(); 10 char\u00a0b\u00a0= \nf.get(); 10 (b) (c) Figure 2. Data dependence graphs for three BDF problems. Line numbers are used to \nrepresent the corresponding instructions. Arrows with solidlines aredef-use edges.(a)Null origin tracking.Instructions \nthathandleprimitive-typed values are omitted;(b)Typestatehistory recording;arrowswithdashedlinesrepresent \nnext-event relationships.(c)Extended copypro.ling;Oi denotes the allocation site atlinei. speci.c bounded \nabstract domain D containing identi.ers that de\u00ad.ne equivalence classesin N.An unbounded subset ofelementsin \nN can be mapped to an element in D. For a particular instruction a .I, an abstraction function fa : N.D \nis used to map aj , where j .N, to an abstracted instance a d.Thisyields an abstrac\u00adtion of the dynamic \ndata dependence graph. For our purposes we areinterestedin thin slicing.The corresponding dependencegraph \nwillbe referredas an abstract thindatadependence graph. DEFINITION 2. (Abstract Thin Data Dependence \nGraph). An ab\u00adstract thin data dependence graph (V ' , E ' , F, D) has node set V ' .I\u00d7D, where each \nnode is a static instruction annotated withan element d .D,denotingthe equivalence class ofinstances \nof the instruction mapped to d. An edge from aj to bk (a, b .I and j, k .D) shows that an instance of \na mapped to aj writes a location that is used by an instance of b mapped to bk, without an intervening \nwrite to that location. If an instruction accesses the heap through v.f, the base pointer value v is \nnot considered to be used. F is afamily of abstraction functions fa, oneperinstruction a .I. For simplicity, \nwe will use dependence graph to refer to the abstract thin data dependence graph de.ned above. Note that \nfor an array element access, the index used to locate the element is still considered tobe used.The number \nof staticinstructions(i.e., the size of I)is relativelysmallevenforlarge-scaleprograms, and by carefully \nselecting domain D and abstraction functions fa,itis possible to require only a small amount of memory \nfor the graph andyetpreserve necessaryinformation neededfor atargetanalysis. ManyBDFproblems exhibitbounded-domainproperties.Their \nanalysis-speci.c dependence graphs can be obtained by de.n\u00ading the appropriate abstraction functions. \nThe following examples show afew analyses and theirformulationsin ourframework. Propagation of null values \nWhen a NullPointerException is observed in theprogram, this analysislocates theprogrampoint where the \nnull valuestartspropagating and thepropagation .ow. Compared to existing null valuetracking approaches(e.g.,[9]) \nthattrackonlythe origin ofa null value,this analysis alsoprovides information about how this value .ows \nto the point where it is dereferenced, allowing the programmer to quickly track down the bug.Here, D \ncontains two elements null and not null.Abstraction function fa(j)= null if aj produces null and not \nnull otherwise. Based on the dependence graph, the analysis traverses backward from node a null where \na .I is the instruction whose execution causes the NullPointerException. The node that is annotated with \nnull and that does not have incoming edges represents the instruction that created the null value originally. \nFigure 2 (a) shows an example ofthis analysis.Annotation nn denotes not null. ANullPointerException is \nthrown whenline4is reached. Recording typestate history ProposedinQVM[2],this analysis tracks the typestates \nof the speci.ed objects and records the his\u00adtory of state changes. When the typestate protocol of an \nobject is violated,itprovides theprogrammer with the recorded history.In\u00adstead of recording every single \nevent in the trace, a summarization approach is employed to merge these events into DFAs. We show how \nthis analysis canbeformulated as an abstract slicingproblem, and theDFAs canbe easilyderivedfrom thedependence \ngraph. Domain D is O\u00d7S, where O is a speci.ed set of allocation sites(whose objects needtobetracked)andS \nis a setofprede.ned states s0 ,s1,...,sn of the objects created by the allocation sites in O. Abstraction \nfunction fa(j)=(alloc(aj),state(aj))if in\u00adstruction instance aj invokes a method on an object .O, and \nthe method can cause the object to changeits state.Thefunctionis un\u00adde.ned otherwise(i.e., all otherinstructions \nare not tracked).Here alloc is a function that returns the allocation site of the receiver object at \naj, andfunction state returns the state of this objectim\u00admediately before aj.The state can be stored \nas a tag of the object, and updated when a methodisinvoked on this object. An exampleis showninFigure \n2(b).Considerthe objectO1 created atline1,with states u (uninitialized), oe (opened and empty), on (openedbut \nnot empty),and c (closed).Arrowswith dashedlinesdenotethe next-event relationships.Theserelation\u00adships \nare added to thegraphfor constructing theDFAdescribed in [2], andthey can be easily obtained by memorizing \nthe last event on each tracked object. When line 10 is executed, the typestate protocol is violated because \nthe .le is read after it is closed. The programmer can easilyidentify theproblem when sheinspects the \ngraph and .ndsthatline10isexecuted onaclosed .le.Whilethe example showninFigure 2(b)is not strictly adependencegraph, \nthe next-event edges can be conceptually thought of as def-use edgesbetween nodes that write and read \nthe object state tag. Extended copy pro.ling Workin[34]describeshow topro.le copy chains that represent \nthe transfer of the same data without anycomputation.Nodesinacopychainare .eldsofobjectsrepre\u00adsentedby \ntheirallocation sites(e.g., Oi.f). An edge connects two .eldnodes, abstractingawayintermediate stack \ncopies.Each stack variablehasashadowstacklocation,whichrecordstheobject .eld from which its value originated. \nA more powerful version of this analysis is to include intermediate stack nodes along copy chains, because \nthey areimportantfor understanding the methods through which the values are transferred. Domain D is \nO\u00d7P, where O is the set of allocation sites and P is the set of .eld identi.ers. A special element ..D \nshows thatthedatadoesnotcomefromany .eld(e.g.,itisaconstantor Figure3. (a)Code example.(b)CorrespondingdependencegraphGcost;nodesinboxes/circles \nwrite/readheaplocations;underlinednodes create objects.(c)Nodes in method A.foo (Node), theirfrequencies(Freq), \nandtheir abstract costs(AC);(d)Relative abstract costsi-RAC andbene.ts i-RAB for the three allocation \nsites;i is thelevel of reference edges considered.  .   b  =   .    m 0 .   03 2 1 \n     1 43 2 3  m 5   =   2 245   m m    m 031    = 01      03        \n       -_ 2 1  b = 6O33 3  3 2 7O33 8O33   4b  3 9O33 11O33 5     3 4 _   \n    = -_ 12O3314O33 16O33 17O3319O33  _      .     = O24 O32 R 2 O32e R--  R 2B 2R \n-- R O33 e a reference to a newly-created object). Abstractionfunction fa (j) = map(shadow(aj)), if \na is a copy instruction, and . otherwise. Function shadow maps aninstruction instance to the tracking \ndata (i.e.,theobject .eldo.g from whichthe value originated)contained in the shadow location of its left-hand-side \nvariable, and function map maps o.g to its static abstraction Oi.g. An example is shown in Figure 2 (c). \nFor instance, to identify the intermediate stack locations b and c in the copy chain between O1 .f and \nO3 .f, one can traverse backward the dependence graph from 5O3.f (which writesto f inthe objectcreatedbyO3 \n).Thetraversalfollows nodes that are annotatedwithO1 .f, untilit reaches the node 2O1.f which reads that \n.eld. Similarly to how a data .ow analysis or an abstract interpreter employs static abstractions, abstract \ndynamic slicing uses abstract domainsfordynamicdata .ow,recognizingthatitisonlynecessary todistinguishinstructioninstances \nthat areimportantfor the client analysis. The rest of the paper focuses on using this approach to perform \ncost-bene.t analyses that target a class ofbloatproblems. 2.2 CostComputation DEFINITION 3. (Absolute \nCost). Given a non-abstract thin data dependence graph G and an instruction instance aj (a .I,j . N)thatproduces \na value v, the absolute cost of v is the number of j nodes that can reach ain G. Absolute costs are expensive \nto compute and it does not make much sense to present them to the programmer, unless they are aggregated \nin some meaningful way across instruction instances. In our approach the instructions are abstracted \nbased on dynamic calling contexts.The contexts are represented with object sensitiv\u00adity[17], whichis \nwellsuitedfor modeling of object-orienteddata structures. Acallingcontextis representedbya chain ofstatic \nabstractions (i.e., allocation sitesOi .O)of the receiver objects for the invo\u00adcations on the callstack.Domain \nDcost contains allpossible chains of allocation sites. Abstraction function fa(j)= objCon(cs(aj)), where \nfunction cs takes a snapshot of the call stack when aj is executed, and function objCon maps this snapshot \nto the corre\u00adsponding chain of allocation sites Oi for the run-time receiver ob-jects.Dcost is not .nite \nin the presence of recursion, and even for a recursion-free program its size is exponential. We limit \nthe size ofDcost furthertobea .xednumber s (shortfor slots ),speci.ed by the user as aparameter of thepro.ling \ntool.Now thedomainis simplythe set ofintegers 0to s-1.An encodingfunction h is used to map an allocation \nsite chain to such an integer; the description of h will be presented shortly. With this approach, the \namount of memoryrequiredfor the analysisislinearinprogram size. Eachnodeinthedependencegraphis annotatedwithaninteger, \nrepresenting the execution frequency of the node. Based on these frequencies, an abstract cost for each \nnode can be computed as an approximation of the total costs of values produced by the instructioninstances \nrepresentedbythe node. DEFINITION 4. (AbstractCost).Given adependence graph Gcost , the abstract cost \nofa node n k isSaj|aj.nk freq(aj), where aj . k jk jk n if there is apath from ato n in Gcost , or a= \nn . Example Figure 3 shows a code example and its dependence graphfor cost computation.While some statements(line29)may \ncorrespond to multiple bytecode instructions, they are still consid\u00adered tohave unit costs.These statements \nare shown forillustration purposes and willbebrokeninto multiple onesbyour tool. Allnodes are annotatedwiththeir \nobject contexts(i.e., elements of Dcost ). For ease of understanding, the contexts are shown in their \noriginal forms, and the tool actually uses the encoded forms (through function h). Nodes in boxes represent \ninstructions that write heap locations. Dashed arrows represent reference edges; these edges canbeignoredfor \nnow.The table showninpart(c) lists nodesfor the execution of method A.foo (invokedbythe call site atline34), \ntheirfrequencies, and their abstract costs. The abstract cost of a node computed by this approach may \nbe larger than the exact sum of absolute costs of the values pro\u00adduced by theinstruction instances represented \nby the node. Thisis because for a node a such that a . n, there may not exist any dependences between \nsome instruction instances of a and some instruction instances of n. This difference can be large when \nthe abstract cost is computed after traversing long dependence graph paths, and the imprecision gets \nmagni.ed. More importantly, this cost represents the cumulative effort that has been made from the very \nbeginning of the executiontoproducethevalues.Itmay still not make much sense for the programmer to diagnose \nproblems using abstract costs, as it is almost certain that nodes representing instructions executedlaterhavelarger \ncoststhanthose representing instructions executed earlier.InSection 3, we address thisproblem by computing \na relative abstract cost, which measures execution bloat at the objectlevelby traversingdependencegraphpaths \ncon\u00adnecting nodes that read and write object .elds. Special nodes and edges in Gcost To measure execution \nbloat, we augment the graph with two special kinds of nodes: predicate nodes and native nodes,both representing \nthe consumption ofdata. Apredicate nodeis createdfor eachif statement, and a native node is created for \neach call site that invokes a native method. These nodes do nothave associated contexts.In addition, \nwe mark nodes that allocate objects (underlined in Figure 3 (b)), that read heap locations(nodesin circles), \nand that writeheaplocations(nodesin boxes).These nodes arelater used toidentify object structures. Reference \nedges are used to represent reference relationships. For each heap store a.f = b, a reference edge is \ncreated to con\u00adnect the node representing this store(i.e., aboxed node) and the node allocating the object \nthat .ows to a (i.e., an underlinednode). For example, there exists a reference edge from 28O32 to 24O32 \n, because 24O32 allocates the array object and 28O32 stores an inte\u00adgertothearray(whichissimilartowriting \nanobject .eld).These edgeswillbeused toaggregate costsforindividualheap locations toform costsfor objects \nanddata structures.  2.3 Construction of Gcost Selectingencodingfunctionh There aretwo stepsin mappingan \nallocation site chainto aninteger d .Dcost (i.e.,[0,...,s-1]).The .rst stepisto encode the chaininto \na probabilisticallyunique value that will accurately represent the original object context chain. An \nencodingfunctionproposedin[6]is adapted toperform this mapping: gi =3 * gi-1 + oi, where oi is the i-th \nallocation siteID in the chain and gi-1 is the probabilistic context value computed for the chain pre.x \nwith length i-1. While simple, this function exhibits very small context con.ict rate, asdemonstrated \nin[6].In the second step, this encoded value is mapped to an integer in the range [0,...,s-1]using a \nsimple mod operation. Pro.ling for constructing Gcost Instead of recording the full execution trace andbuilding \nadependencegraph of.ine, we use an online approach that combines dynamic .ow tracking and slicing. The \nkey issue is to identify the data dependences online, which canbedone using shadowlocations[23,24].For \neachlocationl, a shadow location l ' contains the address of the dependence graph node representing the \ninstruction instance that wrote the last value of l. When an instruction is executed, the node n to which \nthis instruction instance is mapped is retrieved, and all nodes mi that lastwrotethelocations readbytheinstruction \nareidenti.ed.Edges are then addedbetween n and each mi. For a local variable, its shadow location is \na new local variable on the stack. For heap locations we use a shadow heap [24]that has the same size \nas theJavaheap.To enablequick access, thereis a prede.ned distance dist between the starting addresses \nof these two heaps. For a heap location l, the address of l ' can be quickly obtained as l + dist.A tracking \nstack is maintainedinparallelwith the callstacktopassdatadependence relationships across calls.For each \ninvocation, the tracking stack also passes the receiver object chain for the caller. The next context \nis obtained by concatenating the caller s chain and the allocation site of this. Instrumentation semantics \nFigure 4 shows a list of inference rules de.ning the instrumentation semantics. Each rule is of the form \nV, E, H, S, P, T .a:i=... V ' , E ' , H ' , S ' , P ' , T ' with unprimed andprimed symbolsrepresenting \nthestatebefore and aftertheex\u00adecution of statement a.In caseswherea setdoes not change(e.g., when S = \n),itis omitted.Node domainV contains nodes of the S ' h(c) form a , where a denotes the instruction and \nh(c)denotes the encoded integer of the object context c.Edgedomain E : V \u00d7 V is ASSIGN h(c)h(c) V ' = \nV .{a } S ' = S[i .. a ] ' h(c) E = E .{a . S(k)} V, E, S .a:i=k V ' , E ' , S ' COMPUTATION V ' h(c)} \nS ' h(c) = V .{a = S[i .. a ] ' h(c) h(c) E = E .{a . S(k)}.{a . S(l)} V, E, S .a:i=k.l V ' , E ' , S \n' PREDICATE o V ' S ' = V .{a } = S E ' oo = E .{a . S(i)}.{a . S(k)} V, E, S .a:if (i>k){...} V ' , \nE ' , S ' LOAD STATIC V ' h(c)} S ' h(c) = V .{a = S[i .. a ] h(c) E ' = E .{a . S(A.f )} V, E, S .a:i=A.f \nV ' , E ' , S ' STORE STATIC V ' h(c)} S ' h(c) = V .{a = S[A.f .. a ] h(c) E ' = E .{a . S(i)} V, E, \nS .a:A.f=i V ' , E ' , S ' ALLOC h(c) h(c) V ' = V .{a } S ' = S[i .. a ] h(c) ' '' H ' = H[a .. ( ' \nU , (new X)h(c) , )] P ' = P[o .. (new X)h(c)] V, H, S, P .a:i=new X V ' , H ' , S ' , P ' LOAD FIELD \nh(c)h(c) V ' = V .{a } S ' = S[i .. a ] E ' h(c) = E .{a . S(ov.f )} h(c) ' H ' = H[a .. ( ' C , P(ov),f \n)] V, E, H, S .a:i=v.f V ' , E ' , H ' , S ' STORE FIELD h(c)h(c) V ' = V .{a } S ' = S[ov.f .. a ] h(c) \nE ' = E .{a . S(i)} h(c) ' H ' = H[a .. ( ' B , P(ov),f )] V, E, H, S .a:v.f=i V ' , E ' , H ' , S ' \nMETHOD ENTRY ' S = S[ti .. T(i)] for1 = i = n T ' =(T(n + 1) . ALLOCID(P(othis )), T(n + 1), T(n + 2),...) \nS, T .a:m(t1 ,t2 ,...,tn) S ' , T ' RETURN T ' =(S(i), T(2), T(3),...) T .a:return i T ' Figure 4. Inference \nrules de.ning the run-time effects of instru\u00admentation. l . kn a relation containingdependence relationships \noftheform a , which represents that an instance of a abstracted as a l is data de\u00adpendenton aninstance \nof k abstracted as kn.Shadow environment S : M . V maps a run-time storage location to the content in \nits corresponding shadowlocation(i.e.,toitstrackingdata).Here M is thedomain of memory locations.For \neachlocation, its shadow lo\u00adcation contains the(address of the) node thatperforms the most re\u00adcent writetothislocation. \nRules ASSIGN, COMPUTATION,PREDI-CATE,LOAD STATIC,and STORE STATIC update the environments in expected \nways. In rule PREDICATE,instructioninstances are not distinguished and the nodeis representedby a o . \n RulesALLOC,LOAD FIELDandSTORE FIELDadditionally up\u00addate heap effect environment H, which is used to \nconstruct refer\u00adence edges in Gcost . H : V . Z maps a node a l . V to a heap effect triple(type, alloc, \n.eld). domain Z of heap effects. Here, type can be ' U ' (i.e., underlined)representing the allocation \nof an '' '' object, B (i.e.,boxed)representing a .eld store,or C (i.e., cir\u00adcled) representing a .eldload. \nalloc and .eld denote the objectand ' '' the .eld onwhichtheeffect occurs.Forinstance,triple( ' U , O,) \nmeans that a node contains an allocation site O, whiletriple( ' B ' , O, f)means that a node writes to \n.eld f of an object created by allocation site O.A reference edge canbe addedbetween a(store) node with \neffect( ' B ' , O, *)and another(allocation) node with ef\u00ad ' '' fect( ' U , O, ), where * represents \nany .eld name. In order to perform this matching, we need toprovide access to the allocation siteIDfor \neach run-time object.Thisisdone usingtag environment P that maps a run-time object toits allocation siteID. \nHowever, the reference edge couldbe spuriousifthe store node and the allocation node are connected using \nonly allocation site '' ' ID O,becausethetwo effects(i.e., B and ' U )could occur on different instances \ncreated by O. To improve the precision of the client analyses, object context is used again to annotate \nallocation sites. For example, in rule ALLOC, H is updated with effect triple ' '' ( ' U , (new X)h(c) \n, ), where the allocation site new X is an\u00adnotated with the encoded context integer h(c). This triple \nmatches X)h(c) only (store) node with effect ( ' B ' , (new , *), and many spurious reference edges can \nthus be eliminated. In rule ALLOC, (new X)h(c) is usedto tag the newly-created run-time object o (by \nupdating tag environment P), andthisinformation willbe retrieved later when o is dereferenced. In rules \nLOAD FIELD and STORE FIELD, ov denotes the run-time object that variable v points to. P(ov)is usedto \nretrievethe allocation site(annotatedwiththe con\u00adtext) of ov, whichispreviously set as ov s tag upon \nits allocation. The last two rules show the instrumentation semantics at the entry and the return site \nof a method, respectively. At the entry of a method with n parameters, trackingstack T contains the tracking \ndata for the actual parameters of the call, as the n top elements T(1),..., T(n),followedbythe receiver \nobject chainforthe caller ofthe method(as element T(n +1)). In rule METHOD ENTRY,the trackingdatafor \naformalparameter ti is updatedwith the tracking dataforthe corresponding actualparameter(storedin T(i)). \nThe new object contextis computedby applying concatenation operator .to the old chain T(n +1) and the \nallocation site of the run-time receiver object othis pointed to by this (or an empty string if the current \nmethod is static). Function ALLOCID removes the context annotation from the tag of othis, leaving only \nthe allocation site ID. The stack is updated by removing the tracking data for the actuals, and storing \nthe new context on the top of the stack. This new context is available for use by all rules applied in \nthe body of the method(denotedby c in those rules). At the return site, T is updatedto removethe current \ncontext andto storethetrackingdata for the return variable i. The rule for call sites is not shown in \nFigure 4, as it requires splitting a call site into a call part and a return part, and reason\u00ading about \nboth of them. Immediately before the call, the tracking data for the actual parameters is pushed on tracking \nstack T. Im\u00admediately after the call, the tracking data for the returned value is popped from T and used \nto update the dependence graph and the shadow location for the left-hand-side variable at the call site. \nIf the method invoked at the call site is a native method, we create a node(without context) forit, and \nadd edgesbetween each node containedin the shadowlocations of the actualparameters and this node, representing \nthat the values of parameters are consumed by this native method. Implementation of P A naturalidea \nofimplementing object tag\u00adging is to save the tag in the header of each object(i.e., theheader usuallyhas \nunused space).However,in theJ9VMthat we use,this 64-bitheader cannotbe modi.ed.To solve thisproblem, \nthe corre\u00adsponding64bits ontheshadowheap areusedtostoretheobjecttag. Hence, although environments P and \nS havedifferent mathmatical meanings,both areimplemented using shadow locations. 3. RelativeObjectCost-Bene.tAnalysis \nThis section describes a novel diagnosis technique that identi.es data structures with high cost-bene.t \nrates. As discussed in Sec\u00adtion 4, this analysis effectively uncovers signi.cant optimization opportunities \nin six large real-world applications. We propose to compute a relative abstract cost for an object, which \nmeasures the effort ofconstructingthe objectfromdata already availablein .elds of other objects(ratherthanthe \ncumulative effortfromthebegin\u00adning of the execution). Similarly, we compute a relative abstract bene.t \nfor an object, which explains how thedata contained in the object is used to construct other objects. \nThese metrics can help a programmer pinpoint speci.c objects that are expensive to con\u00adstruct(e.g.,there \narelargecosts of computingthedatabeing written intothis object)butare not very useful(e.g,the only use \nof this ob\u00adjectis to make a clone ofitand theninvoke methods on the clone). We .rst develop an object \ncost-bene.t analysis that aggregates relativecostsandbene.tsforindividual .eldsof anobjectinorder tocomputethecostandbene.tforthe \nobjectitself.Next,thecost andbene.tfor a higher-leveldata structure is obtainedin a similar manner,bygathering \ncosts andbene.ts oflower-level objects/data structures accessible through reference edges. 3.1 AnalysisAlgorithm \nDEFINITION 5. (Relative Abstract Cost). Given Gcost, the heap\u00adrelative abstract cost(HRAC) of a node \nn k isS j|afreq(aj), aj.nk jk jk where aj .nk if an and there exists a path from ato n such that no \nnode on the path reads from a static or object .eld. Therelativeabstractcost(RAC)foranobject .eld representedby \nOd .f is the average HRAC of store nodes n k that write to Od .f. Considertheentire .owofapieceofdata(fromtheinput \nofthe program to its output) during the execution. This .ow consists of multiplehops ofdatatransformationsamong \nheap locations.Each hop performs the following three steps: reading values from heap locations,performing \nstack copiesand computations onthem,and writing the results to other heap locations. Consider one single \nhop with multiple sources and one target along the .ow, which reads values fromheaplocations l1,l2 ,...,ln, \ntransforms them to produce a new value, and writes it back to heap location l ' . The RAC of l ' measuresthe \namount of work needed(onthe stack) to complete thishop of transformations. The computation of HRAC for \na node n k requires a backward traversal from n k, which .nds all nodes on the paths between each heap-reading \nnode and n k, and calculates the sum of their frequencies.Forexample,theHRACfor node35o is only1(instead \nof 4007), because the node depends directly on a node(i.e., 4O33 ) that reads heap location this.t. The \nRAC for a heap location is the average HRAC of the nodes that can write this location. For example,theRACfor \nOo , whichis4005. 33 .t istheHRACfor19O33 TheRACfor OO32 .ELM (i.e.,the elements ofthe array object)is \n24 2, which equals theHRAC of node 28O32 that writes this .eld. DEFINITION 6. (RelativeAbstractBene.t).Given \nGcost,theheap\u00adrelative abstractbene.t(HRAB)of a node n k isS aj|nk.aj freq(aj), k kj kj where n .aj if \nnaand there exists a path from n to asuch thatnonodeonthepath writestoastaticorobject .eld.The   Figure \n5. (a)Relative abstractcost andbene.t. Nodes considered in computing RAC and RAB for O.g (where O is \nthe allocation site for the object referenced by z)are included in the two circles, respectively;(b) \nIllustration of n-RAC and n-RAB for the object createdby o =newO;dashedarrows are reference edges. relative \nabstract bene.t (RAB) for an object .eld represented by Od .f is the average HRABofload nodes n k that \nread from Od .f. Symmetric to thede.nition ofRACthatfocuses onhow aheap valueis produced, theRABfor l \nexplainshow aheap valueis con\u00adsumed. Consider again one single hop (but with one source and multiple \ntargets) along the .ow, which reads a value from loca\u00adtion l,transformsthis value(together with values \nreadfrom other locations), and writes the results to a set of other heap locations l ' 1 ,l 2' ,...,l \n' . The RAB of l measures the amount of work per\u00ad n formed(on the stack) to complete thishop of transformations. \nFor example, theRABfor Oo that reads 33 .t is theHRAB of node 4O33 this .eld,whichis2(becausetherelevantnodes \naj are only 4O33 and 35o).Figure5(a)illustratesthe computation ofRACandRAB. This de.nition of bene.t \ncaptures both the frequency and the complexity of data use. First, the more target heap values that the \nvalue readfrom l is usedto(transitively)produce,thelargerbene.t location l can have for the construction \nof these other objects. Second, the more effort is made to transform the value from l to otherheap values, \nthelargerbene.t l canhave.Thisisbecause the purpose of writing a value into a heap location is, intuitively, \nto keep the value so thatit canbe reused later and the(heavy) cost of re-computing it can be avoided.Whether \nto store a value in aheap location is essentially a decision involving space-time tradeoffs. If l s value \nv canbe easily converted to some other value v ' and v ' is immediatelystoredin anotherheaplocation(i.e.,little \ncomputation performed), the bene.t of keeping v in l becomes less obvious, since v and v ' may differ \nslightly and it may not be necessary to use twodifferentheaplocations to cache them.In the extreme case \nwhere v ' is simply a copy of v, the RAB for l is 1 and storing v is not desirable at all if the RAC \nfor l is large. Special treatment is applied to consumer nodes: we assign a large RAB to a heap location \nif the value it contains can .ow to a predicate or a native node. This means the value contributes to \ncontroldecision making oris usedby theJVM, and thusbene.ts the overall execution. class\u00a0ClasspathDirectory{boolean\u00a0isPackage(String\u00a0packageName){return\u00a0directoryList(packageName)\u00a0!=\u00a0null; \n} List\u00a0directoryList(String packageName){List\u00a0ret\u00a0=\u00a0new ArrayList();\u00a0/*problematic*///try to\u00a0find\u00a0all \nthe\u00a0files\u00a0in the\u00a0dir\u00a0packageName //if\u00a0nothing is\u00a0found,\u00a0set\u00a0ret\u00a0to null return ret;\u00a0 }} Figure 6. Real-world \nexample that our analysis found from eclipse. DEFINITION 7. (n-RAC and n-RAB). Consider an object refer\u00adence \ntree RT n of height n rooted at Od. The n-RACfor Od is the sumof theRACsforall .elds Oik.f, such thatboth \nOik and the ob\u00adjectOik.f points to areinRT n.Similarly, the n-RABfor Od is the sumof theRABsforall such \n.elds Oik.f. The objectreference(points-to)tree canbe constructedbyusing reference edges in the dependence \ngraph, and by removing cycles and nodes more than n reference edges away from Od . We ag\u00adgregate the \nRACs and RABs for individual .elds through the tree edgestoformtheRACsandRABsfor objects(when n =1)and \nhigh-leveldata structures(when n> 1). Figure 5(b) illustrates n-RAC and n-RAB for an object created by \no = newO. The n-RAC(RAB) for this object includes the RAC(RAB) of each .eld writtenby aboxed node(i.e.,heap \nstore) showninthe .gure.For all case studies and experiments, n = 4 was used as this is the ref\u00aderence \nchain length for the most complex container classes in the Java collectionframework(i.e.,HashSet). Table(d) \ninFigure 3 shows examples of1-and2-RACs and RABs.Boththe1-RABandthe2-RABfor OO32 are0,becausethe 24 array \nelementis never usedin the code.Objects Oo 33 have 32 and Oo large cost-bene.t rates, which indicates \nthe existence of wasteful operations. This is indeed the case in this example: for Oo 32 , there is an \nelement added but never retrieved; for Oo 33 , there is a large cost of computing the value stored in \nits .eld t, and the value is copied to another heap location (in IntList) immediately after it is calculated. \nThe creation of object Oo is not bene.cial at all 33 because this value couldhavebeen storeddirectly \nto the array. Findingbloat Several usage scenarios are intendedfor this cost\u00adbene.tanalysis.First,it \ncan .ndlong-lived objectsthat arewritten much morefrequently thanbeing read.Second,it can .nd contain\u00aders \nthat contain many more objects than they should. These con\u00adtainers are often the sources of memory leaks. \nThe analysis can .nd that theyhavelargeRAC/RAB ratesbecausefew elements are retrieved and assigned tootherheaplocations.Third,it \ncan .nd al\u00adlocation sitesthat createlarge volumes of temporary(short-lived) objects.These objects are \noften created simply to carrydata across methodinvocations.Data thatis computed and writteninto themis \nread somewhereelseand assigned tootherobject .elds.Thissim\u00adple use of the data causes these objects to \nhave large cost-bene.t rates.Thenext sectionshowsthat ourtool.ndsall threecategories ofproblemsin real-worldJava \napplications. Real-world example Figure 6 shows a real-world example that illustrates how our analysis \nworks. An object with high costs and low bene.ts is highlighted in the .gure. The code in the example \nis extractedfrom eclipse 3.1.2, apopularJavadevelopment tool. Method isPackage returns true/false based \non whether the given package name corresponds to an actualJavapackage.This method is implemented by calling \n(reusing) directoryList which in\u00advokesmany othermethodstocomputealist of .lesanddirectories under thepackage \nspeci.edby theparameter. isPackage then re\u00adturns whetherthelistcomputedbydirectoryList isnull.While the \nreference tolist ret isusedinapredicate,its .eldsarenotread anddo notparticipatein computations.Hence, \nwhen theRACs and RABs for its .elds are aggregated based on the object hierarchy, theimbalancebetween \nthe cost andbene.tforthe entire List data structure can be seen. To optimize this case, we created a \nspecial\u00adized version of directoryList, which returnsimmediately when thepackage corresponding to thegiven \nnameisfound.  3.2 ComparisontoOtherDesignChoices Cost/bene.tfor computation vscost/bene.tfor cache Note \nthat the relative cost and bene.t for an object are essentially measured in terms of the computations \nthat produce values written into the object. The goal of this analysis is to .nd objects such that they \ncontain (relatively) useless values and these values are produced by(relatively) expensive operations.Uponidentifying \nthese oper\u00adations, the user may .nd more ef.cient ways to achieve the same functionality. This is orthogonal \nto measuring the usefulness of a data structure as a cache, where the cost of the cache should in\u00adclude \nonly the instructions executed to create the data structure it\u00adself(i.e., withoutthe cost of computing \nthe valuesbeing cached) and thebene.t shouldbe(re-)de.ned as afunction of the amount of work cached andthe \nnumber oftimesthe cached values are used. Itwouldbeinterestingtoinvestigate,infuture work,howthese new \nde.nitions of cost and bene.t can be used to .nd inappropriately\u00adused caches. Single-hop cost/bene.t \nvs multi-hop cost/bene.t The analysis limits the scope of tracked data .ow to one single hop that is, \nreadingdatafromtheheap,transformingitthrough stacklocations, and writing the results back to the heap. \nWhile this design choice can produce easy-to-understand reports, it could miss problematic datastructuresbecauseofits \nshort-sightedness .Forexample,our tool may consider a piece of data that is ultimately-dead to be ap\u00adpropriately \nused, because it is indeed involved in complex com\u00adputations within the one hop seen by the analysis. \nTo alleviate this problem,wehavedeveloped an additional analysis,based on Gcost, which identi.es computations \nthat can reach ultimately-dead val\u00adues. Section 4 presents measurements of redundant computations based \non this analysis. A different way of handling this issue is to consider multiple hops when computing \ncosts and bene.ts based on graph Gcost , so thatmoredetailedinformation aboutdataproduction and consump\u00adtion \ncanbe obtained.For example, costs andbene.tsfor aninstruc\u00adtion can be recomputed by traversing multiple \nheap-to-heap hops on Gcost backward andforward,respectively, startingfromthein\u00adstruction.Of course,extendingtheinspected \nregionofthedata .ow would make the report hard to verify as theprogrammer has to in\u00adspect larger program \nscopes to understand the detected problems. Infuture work,itwouldbeinterestingto compare empiricallyprob\u00adlems \nfound using different scope lengths, and to design particular tradeoffsbetween the scope length considered \nand thedif.culty of explaining the report. Considering vs ignoring control decision making Our analysis \ndoes not consider the effort of making control decisions as part of the costs of computing values under \nthese decisions. The major reason is that by doing so we could potentially include the costs of computing \nmany values that are irrelevant to the value of inter\u00adest into the cost of that value, leading to imprecise \nand hard-to\u00adunderstand reports. However, ignoring this effort of control deci\u00adsion making could lead \nto information loss. For example, the tool may miss problematic objects due to the underestimation of \nthe cost of constructingthem.Infuture work, we will alsobeinterested in accounting for this effort, and \ninvestigating the relationship be\u00adtweenthescopeof control.owdecisionsconsidered(e.g.,theclos\u00adest n predicates \non which an instruction is control-dependent) and the usefulness of the analysis output. Other analyses \nGraph Gcost (annotatedwith other information) can be used as basis for discovering a variety of performance\u00adrelated \nprogram properties. For example, we have implemented a few clients that can answer speci.c performance-related \nqueries. These clientsinclude an analysis that computes method-level costs (i.e., the cost of producing \nthe return value of a method rela\u00adtive to its inputs), an analysis that detects locations that are re\u00adwrittenbeforebeing \nread, an analysis thatidenti.es nodesproduc\u00ading always-true or always-falsepredicate conditions, and \nan analy\u00adsis that searches for problematic collections by ranking collection objects based on their RAC/RAB \nrates. While these analyses are currentlyimplementedinside aJVM,they couldbe easily migrated toan of.ineheap \nanalysistoolthatprovides user-friendlyinterfaces andimproved operability(i.e.,theJVM only needstowrite \nGcost to external storage). 4. Evaluation We have performed a variety of studies with our technique using \nthe DaCapo benchmark set [4], which contains 11 programs in its original version (from antlr to eclipse \nin Table 1) and an additional set of7programsinits new(beta) release(from avrora to tradesoaps). We were \nable to run our tool on all these 18 large programs, including both client and server applications. 16 \nprograms (except tradesoap and tradebeans) were executed with theirlarge workloads. tradesoap and tradebeans \nwere run withtheirdefault workloads,becausethesetwobenchmarks are not stable enough and running them \nwithlarge workloads canfail even without our tool. All experiments were conducted on a 1.99GHz Dual Core \nmachine. The evaluation has several components: cost graph characteristics, evaluation of the time and \nspace overhead of thetool,themeasurementofbloatbased onnodesproducing dead values, and six case studies \nthat describe problems found by the toolin real applications. 4.1 Gcost characteristics andbloat measurement \nParts(a) and(b) inTable 1 report,fortwodifferent values of s (the number of slots for each object used \nto represent context), the numbers of nodes and edges in Gcost, as well as the space overheads andthetimeoverheads \nofthetool.Notethatallprograms can successfully execute when weincrease s to32, whilethe of.ine traversal \nof the graph (to generate statistics) can make the tool run out of memory for some large programs. The \nspace overhead does not include the size of shadow heap, which is 500Mb for all programs. Note that the \nshadow heap is not compulsory for using our technique. For example, it can be replaced by a global hash \ntable that maps each object toits tracking data(and an object entryis removed when the objectisgarbage \ncollected).The choice ofshadowheapinour workisjusttoallowquick accesstothe tracking information. When \nthe number of context slots s grows from 8 to 16, the space overhead increases while the running time \nis almost not affected. The instrumentation signi.cantly increases the running times(i.e.,71\u00d7 slowdown \non average for s =8 and 72\u00d7fors =16whenthe whole-programtrackingis enabled).This isbecause(1) Gcost is \nupdated at eachinstructioninstance and(2) the creation of Gcost nodes and edges needs to be synchronized \ntoguarantee thatthe toolis race-free.It was anintentionaldecision not tofocus ontheperformance ofthepro.ling,butinsteadtofocus \non the collected information and on demonstrating that the results are useful for .nding bloat in real-world \nprograms. One effective way of reducing overhead is to choose only relevant components Program (a)s \n=8 (b)s = 16 (c)Bloatmeasurementfor s = 16 #N(K) #E(K) M(Mb) O(\u00d7) CR(%) #N(K) #E(K) M(Mb) O(\u00d7) CR(%) \n#I(B) IPD(%) IPP(%) NLD(%) antlr 183 689 bloat 201 434 chart 288 306 fop 195 120 pmd 184 187 jython 288 \n275 xalan 168 594 hsqldb 192 110 luindex 160 177 lusearch 139 110 eclipse 525 2435 avrora 189 108 batik \n361 355 derby 308 314 sun.ow 206 152 tomcat 533 1100 tradebeans 825 1010 tradesoap 860 1370 10.2 9.8 \n13.2 8.4 8.0 12.6 8.5 8.0 6.7 5.5 28.8 7.9 15.8 13.9 8.2 25.4 38.2 41 82 78 76 45 55 28 75 88 92 48 47 \n67 85 63 92 94 89/8* 82/17* 0.066 0.089 0.068 0.067 0.075 0.065 0.066 0.072 0.073 0.079 0.072 0.086 0.086 \n0.080 0.076 0.098 0.053 0.062 355 396 567 381 365 666 407 379 315 275 1016 330 662 425 330 730 1568 1628 \n949 914 453 162 313 539 1095 132 331 223 5724 125 614 530 212 2209 1925 2536 16.1 17.4 22.6 14.0 13.6 \n26.1 18.1 13.7 11.5 11.0 53.1 11.2 24.9 22.1 10.3 48.6 58.9 63.6 77 76 76 46 96 27 74 86 86 52 53 56 \n89 57 91 92 82/8* 81/16* 0.041 0.051 0.047 0.045 0.052 0.042 0.044 0.050 0.040 0.053 0.047 0.034 0.049 \n0.049 0.040 0.063 0.036 0.040 4.9 91.2 9.4 0.2 5.6 14.6 25.5 1.3 3.5 9.1 28.6 3.3 2.4 65.2 82.5 29.1 \n15.1 41.0 3.7 96.2 17.5 26.9 69.9 19.3 8.0 91.7 30.0 28.8 60.9 30.5 7.5 92.1 27.0 13.1 81.9 26.8 17.8 \n82.0 19.4 6.4 92.4 31.0 4.6 93.0 24.6 9.3 65.2 29.1 21.0 78.3 22.0 3.2 94.8 34.5 27.1 71.1 26.7 5.0 94.0 \n23.7 32.7 43.7 31.7 24.2 72.2 23.1 14.9 80.0 22.3 24.5 59.4 20.1 Table1. Characteristics ofGcost.Reported \nare the numbers(in thousand) of nodes(N)and edges(E),the memoryoverhead(in megabytes) excludingthe size \nofthe shadowheap(M),the runningtime overhead(O),andthe context con.ict ratio(CR).Part(c)reportsthetotal \nnumber (inbillion)ofinstructioninstances(I),thepercentages ofinstructioninstances(directlyand transitively)producing \nvalues that are ultimately dead(IPD),thepercentages ofinstructioninstances(directlyortransitively)producing \nvaluesthatend up only inpredicates(IPP), andthe percentages ofGcost nodes such that all theinstructioninstances \nrepresentedby these nodesproduce ultimately-dead values(NLD). to track. For example, for the two transaction-based \napplications tradebeans and tradesoap, there is 5-10\u00d7 overhead reduction when we enabletracking onlyfortheload \nruns(i.e.,the application is not tracked for the server startup and shutdown phases). Hence, it is possible \nfor a programmer to identify suspicious program components using lightweight pro.ling tools such as a \nmethod execution timepro.ler or an object allocationpro.ler, and run our tool on the selected components \nfor detailed diagnosis. It is also possible to employvarious sampling-based or staticpre-processing techniques(e.g.,from[38]) \ntoreducethedynamic effortindata collection. A small amount of memory is required to store the graph, \nand this is achieved primarily by employing abstract domains. The space reduction resulting from abstract \nslicing can also be seen fromthe comparisonbetweenthe number ofnodesinthegraph(N) and thetotal number \nofinstructioninstances(I), as N represents the size of the abstract domain employed in the analysis while \nI represents the size of the actual concretedomainthatfullydepends ontherun-timebehavior oftheapplication. \nCRmeasuresthedegree to which distinct contexts are mapped to the same slots by our encoding function \nh. Following[34], CR-s for an instruction i is de.ned as: ( 0 max0=j=s (dc[j])=1 CR-s(i)= P max(dc[j])/ \ndc[j] otherwise where dc[j]represents the number of distinct contexts that fall into context slot j. \nCR is 0 if each context slot represents at most one distinct context; CR is 1 if all contexts fall into \nthe same slot. The table reports the average CR for all instructions in Gcost. Note that both CR-8 and \nCR-16 show very small numbers. This is because many methods in a program only have a small number ofdistinct \nobject chains throughout the execution. Columns IPD and IPP inpart(c) reportthe measurements of inef.ciency \nfor s = 16. IPD represents the percentage of instruc\u00adtioninstances thatproduce onlydead values.Suppose \nD is a setof non-consumer nodes in Gcost thatdo nothave any outgoing edges (i.e.,nootherinstructionsaredata-dependentonthem),andis \nD * a set of nodes that can lead only to nodes in D. Hence, D * con\u00adtains nodes that ultimatelyproduce \nonlydead values. IPD is calcu\u00adlated as the ratio between the sum of execution frequencies of the nodes \nin D * and the total number of instruction instances during the execution(shownin column I).Similarly, \nsupposeP * isthe set of nodes that can lead only to predicate consumer nodes, and IPP is calculated as \nthe ratio between the sum of execution frequen\u00adcies of the nodes in P * and I. Programs such as bloat, \neclipse and sunflow have large IPDs, which indicates that there may ex\u00adist large optimization opportunities. \nIn fact, these three programs are the ones for which we have achieved the largest performance improvement \nafter removing bloat (as discussed shortly in case studies). Clearly, a signi.cant portion of the set \nof instruction in\u00adstancesisexecuted toproduce only control .owconditions.While thisdoes not helpperformance \ndiagnosis directly, ahigh IPP indi\u00adcates theprogramperforms alarge amount of comparisons-related work, \nwhich may be a sign of over-protective or over-general im\u00adplementations. Column NLD inpart(c) reports \nthepercentage of nodesin D * , relative to the total number ofgraph nodes.Thehigher NLD apro\u00adgram has, \nthe easier it is for a programmer to .nd problems from Gcost. Despite the merging of a large number of \ninstruction in\u00adstancesin a singlegraph node, there are on average25.5% nodesin thegraph thathavethisproperty.Largeperformance \nopportunities may be found by inspecting the report to identify these wasteful operations. 4.2 Case \nstudies We have carefully inspected the tool reports for the following six large applications: bloat, \neclipse, sunflow, derby, tomcat, and trade. These applications have large code bases, and are representatives \nof various kinds of real-world applications, in\u00adcluding program analysis tools (bloat), Java development \ntools (eclipse), image renders (sunflow), database servers (derby), servlet containers(tomcat), and transaction-based \nenterprise ap\u00adplications(trade). Wehavefoundsigni.cant optimization oppor\u00adtunitiesforunoptimizedprograms,such \nas bloat (37%speedup). For the other .ve applications that have been well maintained and tuned, the removal \nof the bloat detected by our tool can still re\u00adsultin considerableperformanceimprovement(2%-15% speedup). \nMoreinsightful changes couldhavebeen madeif we werefamiliar with the overall design of functionality \nand data models. We use theDaCapo versions oftheseprograms,becausethe server applica\u00adtionsareconverted \ntorun .xedloads,and theperformancecanbe measured simply by using running time rather than other metrics \nsuch as throughput and the number of concurrent users. It took us about 2.5 weeks to .nd the problems \nand implement the .xes for these six applications that wehad never studiedbefore. sun.ow Becauseitisanimagerendering \ntool,much ofitsfunc\u00adtionalityisbased on matrix and vector computations, such as trans\u00adpose and scale. \nHowever, each such method in class Matrix and Vector startswithcloninga newMatrix orVectorobject and \nassigns the result of the computation to the new object. Our tool reported that these newly created(short-lived) \nobjects have extremely large unbalanced costs and bene.ts, as they serve primarily the purpose of carrying \ndata across method invocations. Another few lines of the report directed us to an int array where some \nslots of the array areused tocontain .oat values.These .oat valuesareconverted to integers using method \nFloat.floatToIntBits and assigned to the array elements. Later, the encoded integers are read from the \narray and convertedback to .oat values.Theseoperationsoccurin the most-frequently executed methodsintheprogram \nand arethere\u00adfore are expensive to perform. By eliminating unnecessary clones and bookkeeping the .oat \nvalues that need to be passed across methodboundaries(to avoidtheback-and-forth conversions), we observed9%-15% \nrunning time reduction. eclipse Some of the allocation sites that have the largest cost\u00adbene.t rates \ncreate objects of inner classes and Iterators, which implement visitor patterns to traverse the workspace. \nThese visi\u00adtor objects do not contain any data and are passed into iterators, where their visit method \nis invoked to process individual chil\u00addren elements of the workspace.However, theIterator usedhereis \na stack-based class that provides general functionality for travers\u00adingdifferenttypes ofdata structures(e.g.,graph,tree, \netc.), while the workspace has a very simple tree structure. We replaced the visitor implementation with \na worklist implementation, and this simple specialization eliminated millions of run-time objects. The \nsecond major problem found by the tool is with the hash compu\u00adtation implemented in a set of Hashtable \nclasses in the JDT plu\u00adgin. One of the most frequently used classes in this set is called HashtableOfArrayToObject, \nwhich uses arrays of objects as keys. Every time the Hashtable is expanded, its rehash method needs to \nbe invoked and the hash codes of all existing entries have to be recomputed. Because the key can be a \nbig object array, computing its hash code can trigger invocations of the hashcode methodin many other \nobjects, and can thus take considerablylarge amount of time.Wecreated anint array.eldintheHashtableclass \nto cache thehash codes of the entries, and the recorded hash codes are used when rehash is executed.To \nconclude,byremovingthese high-cost-low-bene.t operations, we have managed to reduce the running time \nby 14.5% (from 151s to 129s), and the number of objectsby2%(5.5 million). bloat Previous work[34]hasfound \nthat bloat suffers from ex\u00adcessive string creations. This .nding is con.rmed by our tool re\u00adport.46 allocation \nsites out of the top50 thathave thelargest cost\u00adbene.t rates are String and StringBuffer objects createdin \nthe set of toString methods. Most of these objects eventually .ow into methodsAssert.isTrue and db, whichprintthe \nstrings when certaindebugging-related conditions hold.However,inproduction runs where most bugs have \nbeen .xed, such conditions can rarely evaluatetotrue,andthereisnobene.tin constructingthese objects. \nAnotherproblem exposedby ourtool(but not reportedin[34])is the excessive use of objects of an inner class \nNodeComparator, which contains no data but methods to compare a pair of AST nodes. The comparison starts \nwith the given root nodes, and re\u00adcursively creates NodeComparator objects to compare children nodes. \nComparing two large trees usually requires the allocation (andgarbage collection)ofhundreds of objects,and \nsuch compar\u00adisons occur in almost all methods related to ASTs, even includ\u00ading hashcode and equals. Eliminating \nthe unnecessary String and StringBuffer objects and replacing the visitor pattern with abreadth-.rst \nsearch algorithm resultin37% reductionin running time, and68% reductionin the number of objects created. \n derby Thetoolreportshowsthat anintarrayinclass FileContainer has large cost-bene.t rates. After inspecting \nthe code, we found it isanarraycontaining theinformationof a .le-based container.Ev\u00aderytimethe(same) \ncontaineris writteninto apage, the array needs to be updated. Hence, it is written much more frequently \n(with the same data) than being read. To solve the problem, we modify the code to update this array only \nbefore it is read. Another set of objects that were found to have unbalanced cost-bene.t rates are the \nstrings representing IDs for different ContextManagers. These strings are used to retrieve the ContextManagers \nin a variety of ways, but mostly serve as HashMap keys. Because the database contexts are frequently \nswitched, clear performance improvement canbe seen when we replacedthese strings withintegerIDs.Even\u00adtually,the \nrunning time oftheprogram was reducedby6%, and the number of objects created was reducedby8.6%. tomcat \ntomcat is a well-tuned JSP and servlet container. There are only afew objects thathavelarge cost-bene.ts \naccording to the tool report. One set of such objects is arrays used in util.Mapper, representing the(sorted)list \nof existing contexts.Once a contextis added or removed from the manager, an update algorithm is exe\u00adcuted.The \nalgorithm creates a new array,inserts the new context at the right position in this new array, copies \nthe old context array to the new one, and discards the old array. To remove this bloat, we maintain only \ntwo arrays, using them back and forth as the main context list and the backing array used for the update \nalgorithm. Another problem reported by our tool pointed to string compar\u00adisons in various getContents \nand getProperty methods. These methodstake aproperty name and a Class object(representingthe type oftheproperty) \nasinput, and returnthe value corresponding to theproperty using re.ection.Todecidethetype oftheproperty,the \nimplementations of these methods .rst obtain the names of the ar\u00adgument classes and compare them with \nthe embedded names such as Integer and Boolean . Because a property can have only a few types, we remove \nsuch string comparisons and insert code to directly compare the Class objects. After the modi.cations, \nthe program couldrun3 secondsfaster(about2% reduction). tradebeans tradebeans is an EJB application that \nperforms database queries to simulate a stock trading process. One prob\u00adlem that our tool reported was \nwith the use of KeyBlock and its iterators.This class represents a range ofintegers thatwillbegiven as \nIDs for the accounts and holdings when they are requested. We found that for each ID request, the class \nneeds to perform a few redundant database queries and updates. In addition, a simple int array can suf.ce \nto represent IDs since the KeyBlock and the iterators are just wrappers over integers. By removing the \naddi\u00adtional database queries and using directly the int array, we have manged to make the application \nrun9 secondsfaster(from350s to 341s, 2.5% reduction). The number of objects created was re\u00adducedby2.3%.DaCapohas \nanotherimplementation(tradesoap) of trade, which uses the SOAP protocol to perform client-server communication \nand runs much slower than tradebeans.Aninter\u00adesting comparisonbetween these twobenchmarksis that the \nmajor high-cost-low-bene.t objects reportedfor tradesoap are thebean objects createdinthe set of convertXBean \nmethods.Aspart ofthe SOAPprotocol,these methodsperformlarge volumes ofcopiesbe\u00adtween different representations \nof the same bean data, resulting in signi.cant performance slowdown. Developers can quickly .nd such(design) \nissuesleading tothelowperformance afterinspect\u00ading the tool reports. Summary With the help of the cost-bene.t \nanalyses, we have found various performance problems in these large applications with which wedo nothave \nany experience.Theseproblemsinclude inef.ciencies caused by common programming idioms such as visitor \npatterns, repeated work whose result needs to be cached (e.g., thehashcode examplein eclipse), computation \nofdata not necessarily used(e.g.,stringsin bloat), andchoices of expensive operations(e.g., string comparisonin \ntomcat and the use ofSOAP in tradesoap). For speci.cbloatpatterns suchas the use ofinner classes, it \nis also possible for the compiler/optimizer designers to take them into account and develop optimization \ntechniques that can removethebloat while nothavingto restrictprogrammersfrom using thesepatterns. 5. \nRelatedWork Thereis avery largebody of work related todynamic slicing,dy\u00adnamic .ow analysis, and bloat \ndetection. Due to space limitations, we onlydiscuss techniques closely related tothe workpresentedin \nthispaper. Performance measurementfor optimizations Performance mea\u00adsurement is key to optimization choices \nmade in modern runtime systems. Existing approaches for performance measurement at\u00adtribute cost to some \ncoarse-grainedprogram entities, such as meth\u00adods and calling contexts[31,42].The cost(e.g.,frequency)is \nthen used to identify hot spots and guide optimizations. On the con\u00adtrary, our work computes .ner-grained \ncost at the instruction level and uses it to understand performance and detect program regions that arelikely \nto contain wasteful operations.The workfrom[12] proposes a technique that constructs models of empirical \ncompu\u00adtational complexity, which can be used to predict the execution frequency of a basic block as a \nfunction of the program s work\u00adloads.Whilethepro.lingresults can exposeperformanceproblems in general, \nthey do not provide any information about the .ow of data. In many cases, data .ow information can be \nmore useful in detectingperformancebugs than executionfrequencies[26,34]. Dynamic slicing Ageneraldescription \nofslicing technology and challenges canbefoundinTip s survey[32].Recently,the work by Zhang et al. [37, \n38, 39, 40, 41]has signi.cantly advanced the state ofthe art ofdynamic slicing.This workincludes,for \nexample, aset of cost-effectivedynamic slicing algorithms[38,40],aslice\u00adpruning analysis that computes \ncon.dence values for instructions to select those that are most related to errors[37], a technique that \nperforms online compression of the execution trace[39], and an event-based approach that reduces the \ncost by focusing on events instead of individual instruction instances [41]. Sridharan et al. proposethin \nslicing[30], atechniquethatimprovesthe relevance of the slice by focusing on the statements that compute \nand copy a value to the seed. Although this technique is originallyproposed forstaticanalysis,it .tsnaturallyinourdynamicanalysiswork. \nOur work is fundamentally different from these existing tech\u00adniques in the following ways. Orthogonal \nto the existing pro.le summarizationtechniques such as[1,3,10,39],abstractslicing achieves ef.ciency \nby introducing analysis semantics to pro.ling, establishing afoundationfor solving a range ofbackwarddynamic \n.ow problems. As long as an analysis can be formulated in our framework, the pro.led information is suf.ciently \nprecise for the analysis. Hence, although our approach falls into the general cate\u00adgory oflossy compression,it \ncanbelosslessfor the speci.c analy\u00adsisformulated.Theworkfrom[41]ismorerelatedtoour workin thattheproposed \nevent-based slicing approachisaspecial caseof abstract slicing with the domain D containing a set of \npre-de.ned events.In addition, all the existing work ondynamic slicing targets automatedprogramdebugging, \nwhereasthegoal of ourworkisto understandperformanceand .ndbottlenecks. Leak/bloat detection A body of \nwork has been devoted to man\u00adually[22]or automatically[5,7,8,11,13,18,20,21,35]detect\u00ading memory leaks \nand bloat in large-scaleJava applications. Work from[21,22]proposes metrics toprovideperformance assessment \nof use of data structures. Recent work by Shankar et al. [29]mea\u00adsures object churn and then enables \naggressive method inlining over the regions that contain excessive temporary objects. The re\u00adsearchfrom[26]proposes \nafalse-positive-free technique thatiden\u00adti.es stale objects based on data sampling. The work from [28] \npresents a collection-centric technique that makes appropriate col\u00adlection choicesbasedon adynamically \ncollectedtrace on collection behaviors.Workfrom[36]proposes a static analysis thatcan auto\u00admaticallydiscovercontainerstructuresand \n.ndinef.ciencieswith use of containers.Ourprevious work[34]proposes a copypro.ling techniquethatidenti.esbloatbypro.ling \ncopy activities.Similarly to our Gcost which containsdependence relationships,theseactiv\u00adities are recorded \nin a copy graph, which is used later for of.ine analysis. The major difference between the core techniques \nin our work and all the existingbloatdetection workliesin thedifferent symp\u00adtom de.nitions used to locate \nbloat. For example, existing ap\u00adproaches .nd bloat based on various kinds of suspicious symp\u00adtoms(e.g., \nexcessive copies[34],redundant collection space[28], large numbers of temporary objects [11, 29], and \nobject stale\u00adness[5,26]).On the contrary, wedetectbloatbycapturingdirectly the core oftheproblem, thatis,bylookingforthe \nobjects thathave high cost-bene.t rates. In fact, unbalanced cost and bene.t can be a more general indicator \nof bloat than speci.c symptoms, because itis commonfor the wasteful operations toproducehigh-cost-low\u00adbene.t \nvalues while these operations may exhibitdifferent observ\u00adable symptoms. 6. ConclusionsandFutureWork \nWhat is the cost of constructing this object? Is that really worth performingsuch aheavyweight operation?Wehearthesequestions \nall the timeduring softwaredevelopment. They represent the most natural and explicit form in which a \nprogrammer can express her concern on performance. Tuning could have been much easier if there existed \na way sothatthesequestions canbe(evenpartially) automatically answered. As a step toward achieving the \ngoal, this paperintroducesadynamicanalysisofdata .ow,giventhefactthat much functionality of a large application \nis about massaging data. Optimizationsbased onpurecontrol .owinformation(e.g.,execu\u00adtionfrequency) can \nnolonger suf.ceto capturethe redundancy that accumulatesduringdata manipulation.This workis not only \nabout the measurement of the cost ofgenerating apiece ofdata,but more importantly,itcomputes an assessment \nofthe waythisdatais used. Instead of focusing on bytes and integers, this assessment is made at the data \nstructure level as object-oriented data structures are ex\u00adtensively used and programmers often do not \nneed to be aware of theirinternalimplementationsduringdevelopment. Infuture work, weplanto extendthe \nnotions of cost andbene.t (de.nedinterms ofcomputationsinthispaper)in many other ways to help performance \ntuning. One example is to measure the effec\u00adtiveness ofthedata structures used as caches.The way of rede.ning \ncosts andbene.tsfor caches wasdiscussedinSection 3.As another example, one can adapt the cost and bene.t \nfor data presented in thispaper to measureperformance of control-.ow entities, such as methods, components, \nandplugins.Faced with alarge and complex application, a developer would need to .rst identify such coarser\u00adgrainedprogram \nconstructs thatcanpotentially causeperformance issues, in order to track down a performance bug through \nsubse\u00adquent more detailed pro.ling. In addition, it is possible in future work to consider the space \nof other design choices that are dis\u00adcussedinSection 3.Other than the work ofbloatdetection, we are alsointerestedininvestigatingfuture \nextensions and applications of abstract slicing as a general technique, so that it could potentially \nbene.t a wider range ofdynamic analyses. Acknowledgments We would like to thank the anonymous re\u00adviewersfor \ntheir valuable and thorough suggestions.We also thank PengLiuforpointing outanerrorinaninitialdraft.Thisresearch \nwas supported in part by the National Science Foundation under CAREER grant CCF-0546040 and by an IBM \nSoftware Quality Innovation FacultyAward.GuoqingXu was supported inpartby a summerinternship at theIBMT.J.WatsonResearchCenter. \nReferences [1] H. Agrawal and J. R. Horgan. Dynamic program slicing. In PLDI, pages246 256,1990. [2] \nM.Arnold,M.Vechev, andE.Yahav. QVM:Anef.cient runtimefor detecting defects in deployed systems. In OOPSLA, \npages 143 162, 2008. [3] T.Ball andJ.Larus.Ef.cientpathpro.ling.In MICRO,pages46 57, 1996. [4] S.M.Blackburn,R.Garner,C.Hoffman,A.M.Khan,K.S.McKin\u00adley, \nR. Bentzur, A. Diwan, D. Feinberg, D. Frampton, S. Z. Guyer, M.Hirzel,A.Hosking,M.Jump,H.Lee,J.E.B.Moss,A.Phansalkar, \nD.Stefanovi\u00b4c,T.VanDrunen,D.vonDincklage, andB.Wiedermann. TheDaCapobenchmarks:Javabenchmarkingdevelopment \nand anal\u00adysis. InOOPSLA,pages169 190,2006. [5] M. D. Bond and K. S. McKinley. Bell: Bit-encoding online \nmemory leakdetection. In ASPLOS,pages61 72,2006. [6] M. D. Bond and K. S. McKinley. Probabilistic calling \ncontext. In OOPSLA,pages97 112,2007. [7] M. D. Bond and K. S. McKinley. Tolerating memory leaks. In OOPSLA,pages109 \n126,2008. [8] M. D. Bond and K. S. McKinley. Leak pruning. In ASPLOS, pages 277 288,2009. [9] M. D. Bond, \nN. Nethercote, S. W. Kent, S. Z. Guyer, and K. S. McKinley. Tracking bad apples: Reporting the origin \nof null and unde.ned value errors. In OOPSLA,pages405 422,2007. [10] B.Calder,P.Feller,andA.Eustace.Valuepro.ling.InMICRO,pages \n259 269,1997. [11] B. Dufour, B. G. Ryder, and G. Sevitsky. A scalable technique for characterizing the \nusage of temporaries in framework-intensive Java applications. In FSE,pages59 70,2008. [12] S.F.Goldsmith,A.S.Aiken,andD.S.Wilkerson. \nMeasuring empir\u00adical computational complexity. In FSE,pages395 404,2007. [13] M.JumpandK.S.McKinley.Cork:Dynamicmemoryleakdetection \nforgarbage-collected languages. In POPL,pages31 38,2007. [14] B.KorelandJ.Laski.Dynamicslicing of computerprograms. \nJ.Syst. Softw.,13(3):187 195, 1990. [15] J.Larus.Wholeprogrampaths.InPLDI,pages259 269,1999. [16] J. \nLarus. Spending Moore s dividend. Commun. ACM, 52(5):62 69, 2009. [17] A. Milanova, A. Rountev, and B. \nG. Ryder. Parameterized object sensitivity forpoints-to analysisforJava. TOSEM,14(1):1 41,2005. [18] \nN. Mitchell, E. Schonberg, and G. Sevitsky. Making sense of large heaps. In ECOOP,pages77 97,2009. [19] \nN. Mitchell, E. Schonberg, and G. Sevitsky. Four trends leading to Java runtimebloat. In IEEESoftware,27(1):56 \n63, 2010. [20] N.Mitchell andG.Sevitsky. Leakbot:Anautomated andlightweight tool for diagnosing memory \nleaks in large Java applications. In ECOOP,pages351 377,2003. [21] N.Mitchell andG.Sevitsky. Thecausesofbloat,thelimits \nofhealth. OOPSLA,pages245 260,2007. [22] N. Mitchell, G. Sevitsky, and H. Srinivasan. Modeling runtime \nbe\u00adhavior in framework-based applications. In ECOOP,pages 429 451, 2006. [23] V.NagarajanandR.Gupta.Architectural \nsupportforshadowmemory in multiprocessors. In VEE,pages1 10,2009. [24] N. Nethercote and J. Seward. How \nto shadow every byte of memory usedby aprogram. In VEE,pages65 74,2007. [25] J. Newsome and D. Song. \nDynamic taint analysis for automatic detection, analysis, andsignaturegeneration of exploits on commodity \nsoftware. In NDSS,2005. [26] G. Novark, E. D. Berger, and B. G. Zorn. Ef.ciently and precisely locating \nmemoryleaksandbloat.In PLDI,pages397 407,2009. [27] F. Qin, C. Wang, Z. Li, H. Kim, Y. Zhou, and Y. Wu. \nLift: A low-overheadpracticalinformation .owtracking systemfordetecting security attacks. In MICRO,pages135 \n148,2006. [28] O.Shacham,M.Vechev,andE.Yahav.Chameleon:Adaptiveselection of collections. In PLDI,pages408 \n418,2009. [29] A. Shankar, M. Arnold, and R. Bodik. JOLT: Lightweight dynamic analysis and removal of \nobject churn. In OOPSLA, pages 127 142, 2008. [30] M.Sridharan,S.J.Fink, andR.Bodik. Thin slicing. In \nPLDI,pages 112 122,2007. [31] N. R. Tallent, J. M. Mellor-Crummey, and M. W. Fagan. Binary analysisformeasurement \nand attribution ofprogramperformance. In PLDI,pages441 452,2009. [32] F.Tip. Asurvey ofprogram slicing \ntechniques. Journal of Program\u00adming Languages,3:121 189, 1995. [33] C. Wang and A. Roychoudhury. Dynamic \nslicing on Java bytecode traces. ACM Transactions on Programming Languages and Systems, 30(2):1 49, 2008. \n[34] G.Xu,M.Arnold,N.Mitchell,A.Rountev,andG.Sevitsky. Gowith the .ow:Pro.ling copiesto .nd runtimebloat. \nIn PLDI,pages 419 430,2009. [35] G. Xu and A. Rountev. Precise memory leak detection for Java software \nusing containerpro.ling. In ICSE,pages151 160,2008. [36] G.XuandA.Rountev.Detectinginef.ciently-used \ncontainerstoavoid bloat. In PLDI,2010. [37] X. Zhang, N. Gupta, and R. Gupta. Pruning dynamic slices \nwith con.dence. In PLDI,pages169 180,2006. [38] X. Zhang and R. Gupta. Cost effective dynamic program \nslicing. In PLDI,pages94 106,2004. [39] X. Zhang and R. Gupta. Whole execution traces. In MICRO, pages \n105 116,2004. [40] X. Zhang, R. Gupta, and Y. Zhang. Precise dynamic slicing algo\u00adrithms. In ICSE,pages319 \n329,2003. [41] X. Zhang, S. Tallam, and R. Gupta. Dynamic slicing long running programs through execution \nfast forwarding. In FSE, pages 81 91, 2006. [42] X. Zhuang, M. J. Serrano, H. W. Cain, and J.-D. Choi. \nAccurate, ef.cient, and adaptive calling context pro.ling. In PLDI,pages 263 271,2006.   \n\t\t\t", "proc_id": "1806596", "abstract": "<p>Many opportunities for easy, big-win, program optimizations are missed by compilers. This is especially true in highly layered Java applications. Often at the heart of these missed optimization opportunities lie computations that, with great expense, produce data values that have little impact on the program's final output. Constructing a new date formatter to format every date, or populating a large set full of expensively constructed structures only to check its size: these involve costs that are out of line with the benefits gained. This disparity between the formation costs and accrued benefits of data structures is at the heart of much runtime bloat.</p> <p>We introduce a run-time analysis to discover these <i>low-utility</i> data structures. The analysis employs dynamic thin slicing, which naturally associates costs with value flows rather than raw data flows. It constructs a model of the incremental, hop-to-hop, costs and benefits of each data structure. The analysis then identifies suspicious structures based on imbalances of its incremental costs and benefits. To decrease the memory requirements of slicing, we introduce <i>abstract dynamic thin slicing</i>, which performs thin slicing over bounded abstract domains. We have modified the IBM J9 commercial JVM to implement this approach.</p> <p>We demonstrate two client analyses: one that finds objects that are expensive to construct but are not necessary for the forward execution, and second that pinpoints ultimately-dead values. We have successfully applied them to large-scale and long-running Java applications. We show that these analyses are effective at detecting operations that have unbalanced costs and benefits.</p>", "authors": [{"name": "Guoqing Xu", "author_profile_id": "81350590981", "affiliation": "Ohio State University, Columbus, OH, USA", "person_id": "P2184536", "email_address": "", "orcid_id": ""}, {"name": "Nick Mitchell", "author_profile_id": "81100359733", "affiliation": "IBM T. J. Watson Research Center, Hawthorne, NY, USA", "person_id": "P2184537", "email_address": "", "orcid_id": ""}, {"name": "Matthew Arnold", "author_profile_id": "81100021720", "affiliation": "IBM T. J. Watson Research Center, Hawthorne, NY, USA", "person_id": "P2184538", "email_address": "", "orcid_id": ""}, {"name": "Atanas Rountev", "author_profile_id": "81100162864", "affiliation": "Ohio State University, Columbus, OH, USA", "person_id": "P2184539", "email_address": "", "orcid_id": ""}, {"name": "Edith Schonberg", "author_profile_id": "81350571259", "affiliation": "IBM T. J. Watson Research Center, Hawthorne, NY, USA", "person_id": "P2184540", "email_address": "", "orcid_id": ""}, {"name": "Gary Sevitsky", "author_profile_id": "81100222382", "affiliation": "IBM T. J. Watson Research Center, Hawthorne, NY, USA", "person_id": "P2184541", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1806596.1806617", "year": "2010", "article_id": "1806617", "conference": "PLDI", "title": "Finding low-utility data structures", "url": "http://dl.acm.org/citation.cfm?id=1806617"}