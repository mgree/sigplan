{"article_publication_date": "06-05-2010", "fulltext": "\n Bringing Extensibility to Veri.ed Compilers * Zachary Tatlock Sorin Lerner University of California, \nSan Diego {ztatlock,lerner}@cs.ucsd.edu Abstract Veri.ed compilers, such as Leroy s CompCert, are accompanied \nby a fully checked correctness proof. Both the compiler and proof are often constructed with an interactive \nproof assistant. This technique provides a strong, end-to-end correctness guarantee on top of a small \ntrusted computing base. Unfortunately, these compilers are also challenging to extend since each additional \ntransformation must be proven correct in full formal detail. At the other end of the spectrum, techniques \nfor compiler cor\u00adrectness based on a domain-speci.c language for writing optimiza\u00adtions, such as Lerner \ns Rhodium and Cobalt, make the compiler easy to extend: the correctness of additional transformations \ncan be checked completely automatically. Unfortunately, these systems provide a weaker guarantee since \ntheir end-to-end correctness has not been proven fully formally. We present an approach for compiler \ncorrectness that provides the best of both worlds by bridging the gap between compiler veri\u00ad.cation and \ncompiler extensibility. In particular, we have extended Leroy s CompCert compiler with an execution engine \nfor optimiza\u00adtions written in a domain speci.c language and proved that this ex\u00adecution engine preserves \nprogram semantics, using the Coq proof assistant. We present our CompCert extension, XCert, including \nthe details of its execution engine and proof of correctness in Coq. Furthermore, we report on the important \nlessons learned for making the proof development manageable. Categories and Subject Descriptors D.2.4 \n[Software Engineer\u00ading]: Software/Program Veri.cation Correctness proofs; D.3.4 [Programming Languages]: \nProcessors Optimization; F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning \nabout Programs Mechanical veri.cation General Terms Languages, Veri.cation, Reliability Keywords Compiler \nOptimization, Correctness, Extensibility 1. Introduction Optimizing compilers are a foundational part \nof the infrastructure developers rely on every day. Not only are compilers expected to produce high-quality \noptimized code, but they are also expected to * Supported in part by NSF grants CCF-0644306 and CCF-0811512. \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. PLDI \n10, June 5 10, 2010, Toronto, Ontario, Canada. Copyright c &#38;#169; 2010 ACM 978-1-4503-0019-3/10/06... \n$10.00 be correct, in that they preserve the behavior of the compiled pro\u00adgrams. Even though developers \nhit bugs only occasionally when using mature optimizing compilers, getting compilers to a level of reliability \nthat is good enough for mainstream use is challeng\u00ading and extremely time consuming. Furthermore, in \nthe context of safety-critical applications, e.g. in medicine or avionics, compiler correctness can literally \nbecome a matter of life and death. Devel\u00adopers in these domains are aware of the risk presented by compiler \nbugs; imagine the care you would take in writing a compiler if a human life depended on its correctness. \nTo guard against disaster they often disable compiler optimizations, perform manual reviews of generated \nassembly, and conduct exhaustive testing, all of which are expensive precautions. One approach to ensure \ncompiler reliability is to implement the compiler within a proof assistant like Coq and formally prove \nits correctness, as done in the CompCert veri.ed compiler [9]. Us\u00ading this technique provides a strong \nend-to-end guarantee: each step of the compilation process is fully veri.ed, from the .rst AST transformation \ndown to register allocation. Unfortunately, because the proofs are not fully automated, this technique \nrequires a large amount of manual labor by developers who are both compiler ex\u00adperts and comfortable \nusing an interactive theorem prover. Fur\u00adthermore, extending such a compiler with new optimizations re\u00adquires \nproving each new transformation correct in full formal de\u00adtail, which is dif.cult and requires substantial \nexpertise [14 16]. Another approach to compiler reliability is based on using a domain-speci.c language \n(DSL) for expressing optimizations; ex\u00adamples include Rhodium [8] and PEC [7]. These systems are able \nto automatically check the correctness of optimizations expressed in their DSL. This technique provides \nsuperior extensibility: not only are correctness proofs produced without manual effort, but the DSL provides \nan excellent abstraction for implementing new opti\u00admizations. In fact, these systems are designed to \nmake compilers extensible even for non-compiler experts. Unfortunately, the DSL based approach provides \na weaker guarantee than veri.ed compil\u00aders, since the execution engine that runs the DSL optimizations \nis not proved correct. In this paper we present a hybrid approach to compiler cor\u00adrectness that achieves \nthe best of both techniques by bridging the gap between veri.ed compilers and compiler extensibility. \nOur approach is based on a DSL for expressing optimizations cou\u00adpled with both a fully automated correctness \nchecker and a ver\u00adi.ed execution engine that runs optimizations expressed in the DSL. We demonstrate \nthe feasibility of this approach by extend\u00ading CompCert with a new module XCert ( Extensible CompCert \n). XCert combines the DSL and automated correctness checker from PEC [7] with an execution engine implemented \nas a pass within CompCert and veri.ed in Coq. XCert achieves a strong correctness guarantee by proving \nthe correctness of the execution engine fully formally, but it also pro\u00advides excellent extensibility \nbecause new optimizations can be eas\u00adily expressed in the DSL and then checked for correctness fully \nautomatically. In particular, while adding only a relatively small amount to CompCert s trusted computing \nbase (TCB), our tech\u00adnique provides the following bene.t: additional optimizations that are added using \nPEC do not require any new manual proof effort, and do not add anything to the TCB. The main challenge \nin adding a PEC execution engine to Com\u00adpCert lies in verifying its correctness in Coq. The veri.cation \nis dif\u00ad.cult for several reasons. First, it introduces new constructs into the CompCert framework including \nparameterized programs, substitu\u00adtions, pattern matching, and subtle CFG-manipulation operations. These \nconstructs require careful design to make reasoning about the execution engine manageable. Second, the \nexecution engine Figure 1. Loop peeling: (a) shows the original code, and (b) shows the transformed code. \nk := 0 k := 0 while (i < a[k] += k++; } 100) k; { while (k < a[k] += k++; } a[k] += k; k++; 99) k; { \n(a) (b)  imports correctness guarantees provided by PEC into CompCert, which requires properly aligning \nthe semantics of PEC and Com\u00adpCert. Third, applying the PEC guarantee within the correctness 3 2 I := \n0 3 266666664 77777775 I := (I < E-1) {0 while proof of the engine is challenging and tedious because \nit requires knowing information outside the engine about tests performed deep within the engine. 6664 \n7775 (I < E) { S I++ while S I++ =. G } S We discuss three general techniques that we found extremely \nuseful in mitigating these dif.culties: (1) Veri.ed Validation, a tech\u00ad } I++ nique inspired by Tristan \net al, where, for certain algorithms in the PEC engine, we reduce proof effort by implementing a veri.ed \nre\u00adsult checker rather than directly verifying the algorithm; (2) Seman\u00adtics Alignment, where we factor \nout into a separate module the is\u00adsues related to aligning the semantics between PEC and CompCert, so \nthat these dif.culties do not pervade the rest of the proof; and (3) Witness Propagation, where we return \nextra information with the result of a transformation which allows us to simplify applying the PEC guarantee \nand reduce case analyses. Our contributions therefore include: XCert, an extension to CompCert based \non PEC that provides both extensibility and a strong end-to-end guarantee. We .rst review PEC and CompCert \nin Section 2, and then present our system and its correctness proof in Sections 3 and 4.  Techniques \nto mitigate the complexity of such proofs and lessons learned while developing our proof (Sections 3, \n4 and 5). These techniques and lessons are more broadly appli\u00adcable than our current system.  A quantitative \nand qualitative assessment of XCert in terms of trusted computing base, lines of code, engine complexity \nand proof complexity, and a comparison using these metrics with CompCert and PEC (Section 6).  2. Background \nIn this section, we review background material on the PEC sys\u00adtem [7] and the CompCert veri.ed compiler \n[9]. 2.1 Parameterized Equivalence Checking (PEC) PEC is a system for implementing optimizations and \nchecking their correctness automatically. PEC provides the programmer with a domain-speci.c language \nfor implementing optimizations. Once optimizations are written in this language, PEC takes advantage \nof the stylized forms of the optimizations to check their correctness automatically. Loop peeling We \nshow how PEC works through a simple exam\u00adple, loop peeling. Loop peeling is a transformation that takes \none iteration of a loop, and moves it either before or after the loop. An instance of this transformation \nis shown in Figure 1. Loop peeling can be used for a variety of purposes, including modifying loop bounds \nto enable loop unrolling or loop merging. Optimizations in PEC are expressed as guarded rewrite rules \nof the following form: G\u00a3 =. Gr where S G where NotMod(S, I) . NotMod(S, E) . StrictlyPos(E) Figure 2. \nLoop peeling expressed in PEC where G\u00a3 is a code pattern to match, Gr is the code to replace any matches \nwith, and the side condition S is a boolean formula stat\u00ading the condition under which the rewrite may \nsafely be performed. Throughout the paper we use subscript e (which stands for left ) for the original \nprogram and subscript r (which stands for right ) for the transformed program. Figure 2 shows a simple \nform of loop peeling, expressed in PEC s domain-speci.c language. The vari\u00adables S, I and E are PEC pattern \nvariables that can match against pieces of concrete syntax: S matches statements, I variables, and E \nexpressions. The semantics of a rewrite rule G\u00a3 =. Gr where S is that, for G any substitution . mapping \npattern variables to concrete syntax, if .(G\u00a3) is found somewhere in the original program (where .(G\u00a3) \ndenotes applying the substitution . to G\u00a3 to produce concrete code), then the matched code is replaced \nwith .(Gr), as long as S(.(G\u00a3),.(Gr)) holds. The side condition S is a conjunction over a .xed set of \nside condition predicates, such as NotMod and StrictlyPos. These side condition predicates have a .xed \nsemantic meaning for example, the meaning of StrictlyPos(I) is that I is greater than 0. PEC trusts \nthat the execution engine provides an implementation of these pred\u00adicates that implies their semantic \nmeaning: if the implementation of the predicate returns true, then its semantic meaning must hold. Correctness \nchecking PEC tries to show that a rewrite rule G\u00a3G=. Gr where S is correct by matching up execution states \nin G\u00a3 and Gr using a simulation relation. A simulation relation ~ is a relation over program states in \nthe original and transformed programs. Intuitively, ~ relates a given state .\u00a3 of the original pro\u00adgram \nwith its corresponding state .r in the transformed program. The key property to establish is that the \nsimulation relation is preserved throughout execution. Using . to denote small-step semantics, this property \ncan be stated as follows: .\u00a3 ~ .r . .\u00a3 . .\u00a3 ...,.\u00a3 ~ .. .r . .(1) rr r Essentially, if the original \nand transformed programs are in a pair of related states, and the original program steps, then the trans\u00adformed \nprogram will also step, in such a way that the two resulting states will be related. Furthermore, if \nthe original states of the two programs are related by ~, then the above condition guarantees  Figure \n3. Simulation relation for loop peeling through an inductive argument over program traces that the two \nprogram always executes in lock step on related states. Figure 3 shows G\u00a3 and Gr for loop peeling, and \nshows the simulation relation that PEC automatically infers for this example. G\u00a3 and Gr are shown in \nCFG form, where a node is a program point, and edges are statements. A dashed edge between G\u00a3 and Gr \nindicates that the program points being connected are related in the simulation relation. Furthermore, \neach dashed edge is labeled with a formula showing how the heaps s\u00a3 and sr (of G\u00a3 and Gr) are related \nat those program points. The entry and exit points are related with state equality (s\u00a3 = sr), which means \nthat the simulation relation shows that if G\u00a3 and Gr start in equal states, then they will end in equal \nstates (if the exit points are reached). Aside from the entry points, there are two other entries in \nthe simulation relation, labeled with formulas A and B in Figure 3 (shown below the CFGs). The notation \neval(s, e) represents the result of evaluating expression e in heap s. The PEC checker takes as input \nthe rewrite rule shown in Fig\u00adure 2, and it automatically generates the relation shown in Figure 3. After \ngenerating this relation, PEC checks that the relation satis.es the properties required for it to be \na simulation relation, namely property (1). PEC does this by enumerating the paths from each simulation \nrelation entry to other entries that are reachable. In this case, there are .ve such paths: entry to \nA, entry to B, A to A, A to B, and B to exit. While enumerating paths, PEC prunes infeasible ones. For \nexample, PEC prunes the path A to exit , because the simulation entry at A tells us that I < E - 1, which \nafter executing I++ in the original program gives I < E, which forces the original program to go back \ninto the loop. For each feasible path that PEC enumerates, PEC shows using an automated theorem prover \n(more speci.cally an SMT solver) that if the original and transformed pro\u00adgrams start executing at the \nbeginning of the path, in related heap states, then they end up in related heap states at the end of \nthe path. One important property of the simulation relation is that all loops are cut, and so there are \nno loops between entries in the simulation relation. As a result, the SMT solver only has to reason about \nshort sequences of straight line code, which SMT solvers do very well in a fully automated way. Guarantee \nprovided by PEC The PEC work [7] initially consid\u00adered the following as its correctness guarantee: starting \nwith any initial heap s, if the original program executes to its exit and yields heap s , then the transformed \nprogram will also execute to its exit and produce the same s . However, as we will show in Section 3, \nthis fails to capture the correctness guarantee that PEC in fact pro\u00advides for non-terminating computations. \nAs a result, to integrate PEC within CompCert and prove the PEC execution engine cor\u00adrect, particularly \nfor non-terminating computations, we will have to update the interface of the PEC checker so that it \nalso returns the simulation relation it discovered. The techniques we present in this paper work for \nthe Relate module from PEC, which accounts for about three quarters of the optimizations presented in \n[7]. The remaining optimizations, which include some of the more sophisticated loop optimizations like \nloop reversal, are handled by the PEC Permute module, which presents additional challenges that we leave \nfor future work. 2.2 CompCert We now give a brief overview of the CompCert [9] compiler. CompCert takes \nas input Clight, a large subset of C, and produces PowerPC or ARM assembly. The compiler is implemented \ninside the Coq proof assistant. CompCert is organized into several stages that work over a sequence of \nincreasingly detailed intermediate representations (IRs): from various C-like AST representations, through \nCFG based representations like RTL, and .nally down to abstract syntax for PowerPC assembly. CompCert \nis accompanied by a proof of correctness, also imple\u00admented in Coq. This proof provides a strong end-to-end \ncorrectness guarantee. The guarantee is strong because the entire proof is for\u00admalized in Coq, not leaving \nany parts to a paper-and-pencil proof. The guarantee is end-to-end because it covers all the steps of \ncom\u00adpilation, from the source language all the way to assembly code. The proof is organized around CompCert \ns compilation stages. For each stage, there is a proof showing that if the input program to the stage \nhas a certain behavior, then the program produced by the stage will have the same behavior. The particular \ndetails of how each proof is done depends on the particular stage and the seman\u00adtics of the input and \noutput IR for the stage. The individual proofs are then composed together to produce an end-to-end correctness \nargument. A common strategy used in CompCert for proving optimiza\u00adtions correct is to use a simulation \nrelation. For each optimization that the programmer wants to add, the programmer must carefully craft \na simulation relation for the optimization, and prove that it satis.es property (1) in Coq. Once this \nis done, CompCert has sev\u00aderal useful theorems about small-step semantics that allows the pro\u00adgrammer \nto conclude that the semantics is preserved by the opti\u00admization. In general, proving property (1) requires \na substantial amount of manual effort, and more importantly, it requires in depth knowl\u00adedge of Coq, \nCompCert s data-structures, and proof infrastructure provided by CompCert. In contrast, in the PEC system, \nonce the checker has been implemented, new optimizations can be checked for correctness fully automatically, \nwith no manual proof effort. 3. XCert: CompCert + PEC We have seen in Section 2.1 how PEC provides extensibility, \nand in Section 2.2 how CompCert provides strong guarantees. We now give an overview of how XCert extends \nCompCert with PEC to get both extensibility and a strong correctness guarantee. This section gives a \nhigh-level informal description of the approach, whereas Section 4 will describe the formalism as implemented \nin Coq. Our general approach is to implement an execution engine for PEC optimizations in CompCert, and \nprove that this execution en\u00adgine preserves semantics, given that the optimizations being exe\u00adcuted have \nsuccessfully been checked using PEC. 3.1 Execution engine To add a PEC engine to CompCert, we must decide \nwhere in CompCert s compilation pipeline the PEC engine should be added. Although there are many different \npoints in the pipeline, each using a different IR, the decision really comes down to picking between \na CFG-based IR and an AST-based IR. We decided to apply PEC optimizations to the RTL intermediate representation, \nwhich is CompCert s highest level CFG-based IR. This is also the IR over which CompCert s primary optimizations \nwork: the RTL stage in the compilation pipeline is perfectly suited for implementing general optimizations \nbecause all of the source language constructs have been compiled away, but none of the target speci.c \ndetails have yet been introduced. Although running PEC optimizations on a CFG has many bene.ts, it also \npresents several challenges. Pattern matching First, pattern matching is more dif.cult on a CFG than \nan AST. At a high-level, given a rewrite rule G\u00a3 =. Gr G where S, the PEC execution engine must .nd occurrences \nof G\u00a3 in the program being optimized. An AST pattern-matcher is quite simple to implement recursively \nusing a simultaneous traver\u00adsal over the pattern and the expression being matched. A CFG pat\u00adtern matcher, \non the other hand, is more complex, primarily because CFGs can have cycles, whereas ASTs are acyclic. \nNot only does this make the pattern matcher itself more complex, but reasoning about it formally also \nbecomes more dif.cult. Veri.ed Validation To address the challenge of reasoning about a CFG-based pattern \nmatcher, we make use of Veri.ed Validation, a technique inspired by the work of Tristan et al. on veri.ed \ntrans\u00adlation validation [14 16]. The insight is that the result checker for an algorithm is often much \nsimpler than the algorithm itself, and so proving the result checker correct is often much simpler than \nprov\u00ading the algorithm correct. In our context, Veri.ed Validation allows us to produce matches that \nare guaranteed to be correct, while only reasoning about a pattern-match result checker, rather than \nthe pat\u00adtern matcher itself. Transforming the CFG The second challenge in executing PEC optimizations \non a CFG is that a CFG is more dif.cult to transform than an AST, and this dif.culty is re.ected in the \nCoq proof of correctness. Because ASTs are trees with no cycles or sharing, one can easily perform transformations \nlocally, replacing a whole subtree with another subtree. In a CFG, however, replacing one subgraph with \nanother requires appropriately connecting incoming and outgoing edges for the region that has been replaced. \nTo make this task as easy as possible, we take advantage of the way that CFGs are represented in CompCert. \nA CFG in CompCert is a map from program points to instructions, and each instruction contains successor \nprogram points. For example, a branch instruc\u00adtion would contain two successor program points, whereas \na simple assignment would only contain one successor program point. Con\u00adsider for example the original \nCFG shown in Figure 4(a), with a matched region of the CFG that we want to transform. We graph\u00adically \ndisplay each entry in a CompCert CFG as a box that is sub\u00addivided into two parts: the left part of the \nbox contains a program point p and the right part the instruction i that the program point is mapped \nto. We use arrows from an instruction directly to its suc\u00adcessor program points. Side Conditions As noted \nin Section 2.1, PEC relies on the exe\u00adcution engine to provide correct implementations for a .xed set \nof side conditions predicates, which are used to create the side condi\u00adtions of the PEC rewrite rules. \nFor achieving a strong correctness  Figure 4. Example of CFG splicing  Figure 5. PEC rewrite rule using \nParameterized CFGs guarantee, it is crucial that the implementation of these side condi\u00adtion predicates \nbe veri.ed. To this end, we have implemented and veri.ed a handful of side condition predicates, e.g. \nNotMod and StrictlyPos from Figure 2. Parametrized CFGs Given a PEC rewrite rule G\u00a3 =. G Gr where S, \nwe represent G\u00a3 and Gr as parametrized CFGs. A parametrized CFG (PCFG) is a CompCert CFG that can contain \npattern variables like S, E, and I, which must be instantiated to get a concrete CFG. Furthermore, these \nPCFGs also use pattern vari\u00adables wherever a program point would be expected. Thus, when the PEC engine \n.nds a match for the loop-peeling rewrite from Figure 2, the resulting substitution not only states what \nS, E, and I map to, but also how the program points of G\u00a3 map to program points of the CFG being transformed. \nFor example, Figure 5 shows how the rewrite rule I++;I++ =. G I+=2 would be represented using PCFGs. \nNote that the transformed PCFG, namely Gr, contains a program point pattern variable P4 that is not bound \nin the original PCFG, namely G\u00a3. Such unbound pattern variables (of which there can be many in the transformed \nPCFG) represent fresh program points that the engine will need to generate when it applies the transformation. \nAlthough in general it s perfectly legal for two pattern variables to map to the same piece of concrete \nsyntax, these unbound program points have a special semantics, in that the engine generates a fresh (and \nthus distinct) program point for each unbound program point pattern variable. For simplicity of presentation, \nwe will assume that all parametrized program points in the domain of Gr (i.e. program points in the left \nparts of the boxes in the diagrams) must be free, in that they do not appear in G\u00a3. This makes the example \neasier to understand intuitively and slightly simpli.es the formalization in Section 4. Our actual implementation \nin Coq does not make this assumption. Connecting outgoing edges To see how we connect edges leav\u00ading \nthe transformed region, let s take a look at Figure 5 again. Note that the transformed PCFG uses the \npattern variable P3, which is bound in the original PCFG. Thus, when the PEC engine .nds a match for \nG\u00a3 in Figure 5, the resulting substitution will have an entry for P3, which essentially captures the \nfall-through of the matched region of code. When the engine applies this substitution to Gr, to produce \nthe transformed region of code, P3 will be re\u00adplaced with the fall-through of the original region. In \nthis way, the regular match-and-transform process in the PEC engine naturally connects outgoing edges \nin the transformed region, without requir\u00ading a special case. Connecting incoming edges For connecting \nedges entering the transformed region, let s go back to Figure 4(a), and suppose the pattern matcher \nhas found a sub-CFG g\u00a3 in the original CFG that matches G\u00a3, and let s assume that the resulting substitution \nis .. Furthermore, suppose that applying . to Gr produces the replace\u00adment CFG shown in Figure 4(b). \nAs mentioned previously, the en\u00adgine generates new fresh program points in the transformed CFG, which \nmeans that we can simply union the CFG from Figures 4(a) and (b) without any name clashes in the program \npoints. Further\u00admore, after this union is performed, outgoing edges of the replace\u00adment CFG are already \nconnected, as mentioned previously. As a result, we are only left with connecting the incoming edges. \nOur approach to doing this is simple yet effective. In particu\u00adlar, we take the entry program point in \nthe matched region from Figure 4(a) and update the instruction at that point with the .rst instruction \nof the replacement region from Figure 4(b). Figure 4(c) shows the result of this process. In essence, \ninstruction i has been copied to the entry of the matched region, and since i contains in\u00adside of it \nall its successor program points, the instruction at p now has successor links pointing directly into \nthe transformed region. The remainder of the original matched region is left unchanged, al\u00adthough disconnected \n(except if there are other entry points into the matched region). Any unreachable code will be removed \nby a sub\u00adsequent dead code elimination phase. Note that in our example, the program point p is also left \ndisconnected, but this does not have to be the case in general, since instructions from the transformed \nregion may point to it (for example, in the case of a loop). Witness Propagation In general, applying \nthe PEC guarantee within the Coq correctness proof of the execution engine is chal\u00adlenging and tedious \nbecause it requires knowing information out\u00adside the engine about tests performed deep within the engine. \nTo facilitate the task of applying the PEC guarantee, we use Witness Propagation, a technique in which \nfunctions are made to return ad\u00additional information that is used only for reasoning purposes. For example, \nwe make the PEC execution engine in CompCert return not only the .nal transformed CFG, but also the substitution \nthat was used to generate this transformed CFG. When executing the compiler, the substitution is not \nused outside the engine; however, in the proof it makes applying the PEC guarantee much easier, and it \nsimpli.es case analysis for code that calls the execution engine.  3.2 Correctness Recall that optimizations \nat the RTL level are proved correct in CompCert using a simulation relation, and this amounts to showing \nproperty (1) in Coq, where .\u00a3 and .\u00a3 are states in the original pro\u00adgram, and .r and .r are states in \nthe program produced by the PEC execution engine. When performing this proof in Coq, we assume that all \nthe rules executed by the engine have been checked success\u00adfully by PEC, and therefore, we know that \nthe correctness condition provided by PEC holds for those rules (outlined in Section 2.1). One of the \nchallenges that comes up in performing this proof is that the original program and the transformed program \ndon t execute in perfect synchrony anymore with respect to the small\u00adstep semantics .: given a piece \nof code that has been transformed, it may take, say, 5 steps to go through it in the original program, \nand only 2 steps in the transformed one. This misalignment in the semantics means that, strictly speaking, \nproperty (1) does not hold. Although CompCert has stuttering variations of (1) that can be used in this \ncase, using these variations makes the proof more complex, but more importantly it also con.ates issues: \nthe proof would have to deal at the same time with the misalignment of semantics, and with the complexities \nof reasoning about PEC rewrites. Semantics alignment To separate these concerns, and to modu\u00adlarize the \nproof, we introduce two new semantics for the purposes of Semantics Alignment, .\u00a3 and .r, which are meant \nto align ex\u00adactly: each step taken by .\u00a3 should correspond to precisely one step of .r, making it easier \nto show the equivalence of .\u00a3 and .r. In a separate Semantics Alignment module, we can then show the \nequivalence between . and .\u00a3 for the original program, and between .r and . for the transformed program. \nOur .rst attempt at de.ning .\u00a3 and .r unfortunately was not strong enough. In particular, we stated that \n.\u00a3 and .r act like ., but step over any regions of code transformed by PEC in the orig\u00adinal or optimized \nprograms, respectively. Although this approach works well for terminating computations, non-terminating \ncom\u00adputations introduce additional challenges. When CompCert proves that an optimization preserves behavior, \nthe de.nition of behavior includes the possibility of running forever (with a in.nite trace of externally \nvisible events, such as calls to printf). Thus, we need to prove that the PEC engine preserves non-terminating \nbehaviors (including the details of the in.nite trace). In general formally rea\u00adsoning about the preservation \nof non-termination has proven chal\u00adlenging in the context of formally veri.ed compilers. Indeed, many \nveri.ed compilers, for example the recent work of Chlipala [3], still don t have a proof that non-termination \nis preserved. The big-step problem The problem with our original de.nition of .\u00a3 and .r in regards to \nnon-termination is that they take a big step over regions that PEC has transformed, and such a big step \ndoes not provide a guarantee when the program gets into an in.nite loop inside these stepped over regions. \nThe checks that PEC performs does however guarantee that non-termination is preserved inside of the regions \nit transforms. Thus, one way to address this problem is to strengthen the original guarantee provided \nby the PEC work (stated in Section 2.1), using a similar approach to what CompCert does at the optimization \nlevel: de.ne the behavior of a region of code as either terminates or runs forever . The guarantee that \nPEC provides would then state that the behavior of a region transformed by PEC is preserved, which would \ninclude the runs forever case. While pursuing this approach, we realized that the proof was getting unwieldy. \nApplying the new PEC correctness guarantee was dif.cult because in the non-terminating case, CompCert \nrequires the proof to produce the in.nite trace in the transformed program, which in turn requires a \nlot of accounting to properly glue traces together. The complexity is in part due to the fact that different \nkinds of traces must be glued together: .nite (inductively de.ned) traces with in.nite (co-inductively \nde.ned) traces. By carefully observing the challenges in the proof, we realized that, in the end, all \nthe problems stemmed from a single mismatch in the semantics: big-step vs. small step. The CompCert RTL \ntheory works using a small-step semantics, and our step-over approach essentially introduces a big step \nover potentially non-terminating computations. Changing the PEC interface Our solution to this problem \nis an\u00adother instance of the Semantic Alignment technique, where we es\u00adsentially change the PEC interface \nso that it aligns with CompCert s small-step proofs. The key to achieving this alignment stems from the \nrealization that PEC actually performs its checking using small steps. In particular, the simulation \nrelation that PEC generates has the property that there are no loops between entries. If there is a loop, \nPEC will generate an entry in the simulation relation that cuts the loop into acyclic paths, in much \nthe same way that a loop invari\u00adant cuts loops in program veri.cation. Entry A in Figure 3 is such a \nloop-cutting entry in the simulation relation. Therefore, there is no possibility that a program will \nnot terminate between simulation relation entries. Furthermore, PEC uses a simulation relation in its \nchecking, which is precisely the technique used in CompCert too. It would therefore make sense to change \nthe interface between the two systems to take advantage of their similarities. To this end, we modify \nthe interface between PEC and Com\u00adpCert so that the PEC checker returns the simulation relation that \nit used to prove a particular optimization correct, and we import this simulation relation into CompCert. \nWhen we prove that running this optimization in CompCert using the PEC execution engine pre\u00adserves behavior, \nwe can make use of CompCert s simulation rela\u00adtion approach, by creating a simulation relation for the \nentire pro\u00adgram as follows: if we re not in a region that has been transformed, use state equality; if \nwe are in a region that has been transformed, use the simulation relation returned by the PEC checker \nfor that optimization. Furthermore, along with the PEC simulation relation, we as\u00adsume that the PEC checker \nreturns a Coq proof that the simula\u00adtion relation satis.es the simulation property, namely property (1). \nThis proof is nothing more than a Coq rei.cation of the proofs that PEC s SMT solver performed. If PEC \nused an SMT solver that re\u00adturned proofs, it could perform a translation from the SMT proofs into Coq \ns proof language. The proof returned by PEC is used in our proof to show that the simulation relation \nwe created for the entire program is preserved while inside transformed code. Function calls are handled \nin CompCert using small steps, so that a call instruction transfers execution to the CFG of the callee. \nIf a call instruction occurs inside the transformed region, we consider the call to essentially leave \nthe transformed region. As a result, inside the callee, the simulation relation we construct will simply \nuse state equality, not the PEC simulation relation. Once the call returns, execution comes back into \nthe transformed region, and the simulation relation we construct goes back to using the PEC simulation \nrelation. Left and right semantics, revisited Now that PEC returns a sim\u00adulation relation, we can give \nthe de.nitions of .\u00a3 and .r that we use in our proof: if we re not in a region that has been transformed, \n.\u00a3 and .r work the same as .; if we are in a region that has been transformed, .\u00a3 and .r simply step \nfrom one entry to another in the simulation relation returned by PEC. To illustrate how ., .\u00a3 and .r \nwork, Figure 6 shows part of an execution trace trace\u00a3 for the original program (with round circles for \nprogram states), and part of a trace tracer for the trans\u00adformed program (with crosses for program states), \nalong with the simulation relation as it unfolds throughout execution (shown as dotted edges between \nthe original and transformed traces). The simulation relation inside the transformed region is the one \nthat PEC returns. Figure 6 also shows how the three step semantics op\u00aderate on the original and transformed \nprograms: . and .\u00a3 on the original program and .r and . on the transformed program.  3.3 Proof architecture \nTo summarize, our proof is therefore organized into three steps, which we show separately: (1) if a program \np has behavior b under ., then p has behavior b under .\u00a3; (2) if a program p has behavior b under .\u00a3, \nthen the program produced by our execution engine on p has behavior b under .r; (3) if a program p has \nbehavior b under .r, then p has behavior b under .. Steps (1) and (3) are where semantics alignment issues \nare resolved, and step (2) is where we build a simulation relation for the original and transformed programs \nusing the simulation relation returned by the PEC checker. Figure 6. Traces showing how ., .\u00a3 and .r \nwork Instruction i . Instr Program point p . PP CFG g . CFG = PP . Instr Program p . P rog = String . \nCFG Program heap s . Heap Program state . . State = CFG \u00d7 PP \u00d7 Heap PEC Sim Rel . . Sim = P(State \u00d7 State) \nSubstitution . . Subst Param. Sim . . PSim Param. CFG G . PCFG Side condition S . SC = CFG \u00d7 CFG Rewrite \nrule r . Rule = PCFG \u00d7 PCFG \u00d7 SC Figure 7. Common types used in our formalism 4. Formalization In this \nsection, we make the ideas from Section 3 more precise, by presenting a formalization of the PEC engine \nand its proof. The development presented here closely mirrors our implementation in Coq. Later, in Section \n5, we describe some of the additional challenges that arose when translating these high level ideas into \nCoq code. 4.1 Basic de.nitions We start with some basic de.nitions, shown in Figure 7. An instruc\u00adtion \ni may be any one of a number of basic RTL instructions already de.ned in CompCert. A CFG g is a map from \nprogram points to in\u00adstructions, and a program is map from function names (strings) to CFGs. A program \nheap s contains the state of dynamically allo\u00adcated memory blocks. For simplicity of presentation, we \nassume the heap also contains the state of the registers and stack, even though in the implementation \nthey are kept separate. A program state . consists of a CFG (representing the current code being exe\u00adcuted), \na program point in that CFG (representing where execution has reached), and the heap (which includes \nthe stack). We project these .elds of a program state . as follows: g(.) denotes the CFG, p(.) denotes \nthe program point, and s(.) denotes the heap. A PEC simulation relation . is a relation over program \nstates that is returned by the PEC checker. Because they are generated by PEC, these simulation relations \nhave entries for related program points, and each entry is a predicate over program heaps (recall Figure \n3). Therefore, such relations have the form: .((g\u00a3,p\u00a3,s\u00a3), (gr,pr,sr)) . .P (p\u00a3,pr)(s\u00a3,sr) where .P . \n(PP\u00d7PP) -P(Heap\u00d7Heap). We use the notation p . . to denote that p is in the domain of .P (either as a \n.rst parameter or second parameter). A substitution . is a map from pattern variables to concrete pieces \nof syntax. A parametrized simulation relation . is a ver\u00adsion of a simulation relation that contains \npattern variables which must be instantiated to yield a concrete simulation relation. For ex\u00adample, the \nsimulation relation shown in Figure 3 is parametrized because syntactic values for S, E, and I must be \nprovided before the simulation relation can apply to concrete program states. Given a parametrized simulation \nrelation ., and a substitution . that maps every free pattern variable in . to concrete syntax, the result \nof ap\u00adplying . to ., denoted .(.), is a concrete simulation relation .. Similarly, a parameterized CFG \nG is a parametrized version of a CFG. A side condition is a boolean function from two concrete CFGs (here \nexpressed as a relation). A PEC rewrite rule r contains two parametrized CFGs (representing the pattern \nto match, and the replacement to perform), and a side condition.  4.2 PEC checker and guarantee PEC \ntakes a rewrite rule and attempts to construct a parameterized simulation relation. If PEC is able to \ncheck that the rewrite rule is correct, it also returns a proof that the simulation relation satis.es \nthe simulation property. Speci.cally, PEC has the type: PEC(r : Rule) : (. : PSim \u00d7 Proof[IsSimRel(r, \n.)]) .{Fail} The proof returned by PEC plays a central role in our Coq proof of the correctness of the \nexecution engine. To describe the proof returned by PEC we ll make use of a modi.ed step relation, . \nt. , which essentially steps over any program points not in .. t the PEC simulation relation .. That \nis, .. combines the sequence t of regular . steps from one entry in . to the next into a single medium \nstep. Using this de.nition, we now de.ne IsSimRel(r, .), the guar\u00adantee provided by the proof term returned \nby PEC: DEFINITION 1. We say IsSimRel((G\u00a3,Gr,S), .) holds iff: S(.(G\u00a3),.(Gr)) . IsConSimRel(.(.),.(G\u00a3),.(Gr)) \nwhere IsConSimRel(., g\u00a3,gr) holds iff: .P (Entry(g\u00a3), Entry(gr)) = HeapEq . .P (Exit(g\u00a3), Exit(gr)) = \nHeapEq . \"# g\u00a3 = g(.\u00a3) . gr = g(.r) . h ti t...r ..(.\u00a3,.r) . .r .. .r .(.\u00a3,.r) . .\u00a3 .. .\u00a3 Intuitively, \nthe above de.nition guarantees that the simulation rela\u00adtion returned by PEC: (a) relates states on entry \nand exit to G\u00a3 and Gr by heap equality HeapEq is de.ned by .s.HeapEq(s, s); and (b) satis.es the simulation \nproperty (1).  4.3 Execution engine Figure 8 shows pseudo code for the PEC execution engine in XCert. \nGiven a program p and a PEC rewrite r, TrProg applies r to each TrProg(p, r): return .s. fst(TrCFG(p(s),r)) \nTrCFG(g, r): C .\u00d8 for p . ProgPoints(g) do x . TrPoint(g, r, p) C . C .{x} return Pick(C) TrPoint(g\u00a3, \n(G\u00a3,Gr,S),p): . . Match(G\u00a3,g\u00a3,p) p if \u00ac .(G\u00a3)= g\u00a3 return (g\u00a3, .) . . Fresh(., Gr) if \u00ac S(.(G\u00a3),.(Gr)) \nreturn (g\u00a3, .) gr . g\u00a3 . .(Gr) i . gr(.(Gr.entry)) gr . gr[p . i] return (gr,.) Figure 8. PEC execution \nengine CFG in p using TrCFG. It projects the .rst element of the result of TrCFG because it contains \nboth the transformed CFG and the substitution used to produce this CFG. TrCFG iterates over all the program \npoints in the given CFG g, and for each program point it attempts to apply the rewrite starting at that \npoint by calling TrPoint. It gathers the resulting CFGs and chooses one as the transformed version of \ng. TrPoint .rst tries to match the left parameterized CFG G\u00a3 of the rewrite rule to the given concrete \nCFG g\u00a3. It then checks that any generated substitution . applied to G\u00a3 is identical to the CFG p fragment \nof g\u00a3 rooted at p; we denote this as .(G\u00a3)= g\u00a3. If this check or the pattern match fails, TrPoint simply \nreturns the original CFG and . which indicates an invalid substitution.This instance of Veri.ed Validation \nallows us to avoid reasoning about p Match directly and instead simply show that our comparison = is \ncorrect, which is a much smaller proof burden. Next TrPoint creates fresh program points for any parameterized \nprogram points that are free in Gr. Now, TrPoint checks that the rewrite rule s side condition holds \non the CFGs generated by applying . to the left and right parameterized programs, G\u00a3 and Gr. Once again, \nif the check fails, TrPoint simply returns the original CFG and .. Next TrPoint generates the transformed \nversion of the code gr by applying the substitution . to the right parameterized CFG Gr. TrPoint then \nchanges gr so that program location p points to the .rst instruction of the transformed part of the CFG. \nFinally, TrPoint returns the transformed CFG gr and the substitution .. 4.4 Correctness condition We \nde.ne the set of behaviors of a program as follows: Beh = {term(t) | t . Trace}.{forever(t) | t . Trace} \n where t represents a potentially in.nite trace of observable events, and term(t) and forever(t) respectively \ndenote executions termi\u00adnating or diverging with a trace t. We use p . b to indicate that p has behavior \nb, as de.ned below. DEFINITION 2. The relation p . b is de.ned as follows: t if .i(p) . * .f and .f . \nFinal then p . term(t) t if .i(p) . 8 then p . forever(t) t where: .i(p) is the initial state of program \np; . * is the re.exive t transitive closure of .; Final is the set of .nal program states (in\u00ad t dicating \nprogram termination); and . . 8 indicates that execution runs forever producing trace t under . when \nstarted at .. To show the correctness of our execution engine, we prove the following theorem in Coq: \nTHEOREM 1. If PEC(r) . = Fail and p . b then TrProg(p, r) . b. In the following, we describe a Coq proof \nof Theorem 1. To do this, we .x a particular rule r and assume PEC(r) = (.,.), where . is the parametrized \nsimulation relation found by PEC for r and . is a proof of IsSimRel(r, .) (which essentially guarantees \nthat IsSimRel(r, .) holds). Step left and step right To simplify applying the proof . of IsSimRel(r, \n.), we construct two new, closely related semantics that are specialized to a concrete simulation relation: \nt DEFINITION 3. We de.ne .\u00a3 .\u00a3 .\u00a3 as the smallest relation satis\u00adfying: \u00bb and a well-founded order \n< on program states such that: tt . ~1 .\u00a3 . . . . ...\u00a3,. ~1 .\u00a3 . (.\u00a3 .\u00a3 .\u00a3 . . <.) (2) Intuitively, \nthis is the same as the standard simulation property (1), except that we allow for the possibility that \n.\u00a3 does not step as long as the order is decreasing from . to . . We de.ne . ~1 .\u00a3 to hold when either: \n(a) . = .\u00a3 and either . and .\u00a3 are outside transformed code or both are at an entry in . or (b) . is \nin a transformed region, but not at an entry in ., .\u00a3 isat an t entry in ., and . .. .\u00a3. Furthermore, \nwe de.ne the < order as follows: . <. iff m(. ) <m(.) where m(.) and m(. ) are the number of steps that \n. and . have, respectively, until reaching the next entry in .. We now have to show condition (2). The \n.rst and simpler case corresponds to (a) in the de.nition of . ~1 .\u00a3. Here we show that the executions \nare in lockstep and that the successor states . and .l are equal. The second and more dif.cult case, \ncorresponding to TrCFG(g(.\u00a3),r)=(g(.r),.) (b) in the de.nition of . ~1 .\u00a3, involves accounting for the \nsteps . = .(.) =. of p s execution between entries in .. In this case: .\u00a3 is at an entry in . (because \nwe are in case (b) of the de.nition of . ~1 .\u00a3) and it does not step; . is not at an entry in . and steps \nto . ; and from 3 2 tt p(.\u00a3) .. . . p(.\u00a3) .. . . .\u00a3 . .\u00a3 . .\u00a3 .\u00a3 .\u00a3 t the de.nition of ~1 (the second \ncase) we know . .. .\u00a3. Thus . is closer than . to the next entry in . (namely the program point of .\u00a3), \nwhich allows us to show that . <.. Lemma 2 Lemma 2 is the most dif.cult aspect of our Coq proof. CompCert \ns library for small-step semantics provides a theorem which allows us to demonstrate Lemma 2 if we can \nexhibit a simulation relation ~2 between the states of p and TrProg(p, r) that satis.es the following \n(which is essentially property (1)): tt .\u00a3 ~2 .r . .\u00a3 .\u00a3 .\u00a3 ...r,.\u00a3 ~2 .r . .r .r .r (3) DEFINITION \n4. We de.ne .\u00a3 ~2 .r as the smallest relation satis\u00adfying: \u00bb 6664 7775 tt p(.\u00a3) .. . . p(.\u00a3) . . . \n.\u00a3 . .\u00a3 . .\u00a3 .\u00a3 .\u00a3 tt p(.\u00a3) . . . p(.\u00a3) . . . .\u00a3 .. .\u00a3 . .\u00a3 .\u00a3 .\u00a3 tt p(.\u00a3) . . . p(.\u00a3) .. . . .\u00a3 . .\u00a3 \n. .\u00a3 .\u00a3 .\u00a3 Note that formulas in square brackets are implicit conjunctions of t formulas, one formula \nper line. The relation .r .r .r is de.ned tanalogously to .\u00a3 by substituting .r for .\u00a3 in the right-hand \nside of the main implication above. The notation p(.\u00a3) .. . indicates that the program point of state \n. is not in a region of CFG transformed by TrCFG. This is implemented by searching . to determine if \na parameterized program point maps to p(.\u00a3) such that the parameterized program point is not one of the \nexit points from the transformed code back TrCFG(g(.\u00a3),r)=(g(.r),.) to unmodi.ed code. For briefness, \nwe may speak of a state . not =.. = .(.) being in the transformed region; this simply means p(.) .. .. \nt Intuitively, .\u00a3 captures distinct ways in which the original code can step from state .\u00a3 to .\u00a3. In \nDe.nition 3, the .rst line of the main implication s right-hand side handles situations where neither \nt .\u00a3 nor .\u00a3 are in the transformed region. In this case .\u00a3 .\u00a3 .\u00a3 holds \u00bb p(.\u00a3) .. . . p(.\u00a3)= p(.r) . \ns(.\u00a3)= s(.r) . .\u00a3 ~2 .r .(.\u00a3,.r) . .\u00a3 ~2 .r Intuitively ~2 relates states using heap equality when the \npro\u00ad t whenever .\u00a3 . .\u00a3 holds, that is whenever .\u00a3 could take a normal RTL step to .\u00a3. The second and \nfourth lines capture entering and t exiting the transformed region, which again requires .\u00a3 . .\u00a3. Note \nthat we only allow entering and exiting transformed code through program locations that are in .. The \nthird line captures the situation where the original program executes from entry to entry of . using \n... Similar to the de.nition of . (De.nition 2), we also de.ne .\u00a3 and .r, which respectively use .\u00a3 and \n.r rather than ..  4.5 Proof architecture To establish Theorem 1 for program p and rewrite rule r = \n(G\u00a3,Gr,S) where PEC(r) . = Fail, our Coq proof shows following three lemmas, which we describe in more \ndetail below: LEMMA 1. If p . b then p .\u00a3 b. LEMMA 2. If p .\u00a3 b then TrProg(p, r) .r b. LEMMA 3. If p \n.r b then p . b. Lemma 1 CompCert s library for small-step semantics allows us to demonstrate Lemma 1 \nif we can exhibit a simulation relation ~1 gram points are outside of a transformed region, and using \nthe sim\u00adulation relation returned by PEC when the program points are in\u00adside of a transformed region. \n Proving condition (3) has four main cases, which correspond to the four conjuncts in the de.nitions \nof .\u00a3 and .r. Case 1: .\u00a3 and .r are both outside of transformed regions and so are their successor states. \nThis case is straightforward. Because .\u00a3 ~2 .r we know their heaps and program points are equal and because \nthey are outside of transformed code, we know they are executing the same instruction. Thus .r will step \nto .r where p(.r)= p(.\u00a3) and s(.r)= s(.\u00a3), which implies .\u00a3 ~2 .r (using the .rst case of ~2). Case 2: \n.\u00a3 and .r are both stepping from outside the trans\u00adformed region into the transformed region. Because \nboth states start outside the transformed region, we know their heaps are equal and that they re executing \nthe same instruction. Thus .r will step to .r such that s(.\u00a3)= s(.r). Furthermore, because PEC guarantees \nthat the entries of matched code will be related in . with heap equal\u00adity (see the part of de.nition \n1 that uses HeapEq), s(.\u00a3)= s(.r) implies .(.\u00a3,.r). Thus .\u00a3 ~2 .r (using the second case of ~2). Case \n3: .\u00a3 and .r are both stepping from one entry of . to the next. We use the fact that TrCFG(g(.\u00a3),r)=(g(.r),.) \nto invoke the guarantee provided by PEC s proof of IsSimRel(r, .). Speci.cally, TrCFG(g(.\u00a3),r)= (g(.r),.) \nimplies that S(.(G\u00a3),.(Gr)) which ensures IsConSimRel(.(.),.(G\u00a3),.(Gr)) (see De.nition 1 and TrCFG in \nFigure 8) . This fact ensures that the .r will execute to .r and .(.\u00a3,.r). Thus .\u00a3 ~2 .r (using the second \ncase of ~2). Case 4: .\u00a3 and .r are both stepping from inside the transformed region to outside the transformed \nregion. Similar to Case 2 above, PEC guarantees that exits of matched code will be related in . with \nheap equality (see the part of de.nition 1 that uses HeapEq), mean\u00ading that .(.\u00a3,.r) at the exit implies \ns(.\u00a3)= s(.r). Furthermore, the way our pattern matching works ensures that p(.\u00a3)= p(.r) and that the \ninstruction at these program points are equal. Thus .r will step to .r where p(.r)= p(.\u00a3) and s(.r)= \ns(.\u00a3). From this it follows that .\u00a3 ~2 .r (using .rst case of ~2). Lemma 3 CompCert s library for small-step \nsemantics provides a theorem which allows us to demonstrate Lemma 3 if we can show: tt .r .r .r . .r \n. + .r The above follows immediately from the de.nition of .r. 5. Coping with challenges Throughout Sections \n3 and 4, we have already shown how three techniques are very useful in managing the complexity of extend\u00ading \nCompCert to support PEC rewrite rules: Veri.ed Validation, Semantics Alignment and Witness Propagation. \nIn this section we present several additional important challenges that we faced in our development and \ntheir solutions. 5.1 Termination of Coq code Functions expressed in Coq s Calculus of Inductive Constructions \nmust be shown to terminate. In most cases, Coq can prove termi\u00adnation automatically by .nding an appropriate \nmeasure on a func\u00adtion s arguments that decreases with recursive calls. However, anal\u00adyses that attempt \nto reach a .xed point or traverse cyclic structures like CFGs often pose problems for Coq s automated \ntermination\u00adproving strategy. One solution to this problem is to develop a ter\u00admination proof for such \nfunctions in Coq. In general this can be hard, and it also makes the functions more dif.cult to update, \nsince the termination proof also needs updating. Another solution to is the introduction of a timeout \nparameter that is decremented for each recursive call. If it ever reaches zero the function immediately \nreturns with a special . value. Using this approach, Coq can now show termination automatically. The \ndownside of this simplistic approach is that the algorithm is now incomplete, since in some cases it \ncan return ., and the proof of correctness needs to take this into account. However, this is not a problem \nin domains where there is a safe fallback return value that makes the proof go through. This is indeed \nthe case in the compiler domain: the safe return value is the one that leads to no transformations for \nexample a pattern matcher can always return Fail. Although a constant timeout may appear to be crude \nsolution at .rst, we have found that it presents a very good engineering trade-off, since a large timeout \noften suf.ces in practice.  5.2 Case explosion Conceptually, our intermediate semantics .\u00a3 and .r have \nonly four cases, as show in Section 4. However, such de.nitions on paper often lead to formal Coq de.nitions \nwith many cases. For example, expressing .\u00a3 and .r in terms of CompCert s small-step . leads to a total \nof 9 cases. Most of these 9 cases use . which itself has 12 cases, leading to an explosion in the number \nof cases. In the end, however, only a handful of these case are actually feasible at any one point in \nthe proof, and a paper-and-pencil proof could easily say the only feasible cases are ... . However, the \nformal proof needs to handle every case, leading to complex accounting. One approach that we have found \nvery helpful with eliminating the many infeasible cases is to thread additional information in the return \nvalues of functions. This additional information is not used by the computation itself, but rather in \nthe proof, to provide the right context in the callers to know how to prune appropriate cases. One example \nof this approach is the PEC execution engine from Figure 8, which threads the substitution found in TrPoint \nall the way back up to TrProg, even though for the purposes of applying PEC rules, this substitution \nis not needed outside of TrPoint. In other cases, we have also found that implementing specialized tactics \nin Coq s tactic languages allows us to easily handle many similar cases using few lines of proof. 5.3 \nLaw of the excluded middle The law of excluded middle occurs very naturally when working out high level \nproof sketches. Unfortunately, the constructive logic underlying Coq does not provide this luxury. As \nan example, one could be tempted in a proof sketch to split on termination: either execution returns \nfrom a given function call or it does not. How\u00adever, this intuitive fact cannot be shown in Coq, because \nit would require deciding algorithmically if the function terminates. Instead one must create an inductive \nconstruct with two constructors corre\u00adsponding to the intuitive case split. This is precisely how termina\u00adtion \nvs. non-termination is handled in CompCert, as shown in the de.nition of . (De.nition 2). Alternatively, \nin situations where it is possible, one can implement a decision procedure that correctly distinguishes \nbetween the various cases of interest. Then, within a proof, one can perform case analysis on the result \nproduced by this decision procedure. 6. Evaluation XCert extends the CompCert veri.ed compiler with an \nexecution engine that applies parameterized rewrite rules checked by PEC. Below we characterize our implementation \nof XCert by compar\u00ading it to both an untrusted prototype execution engine and to some of the manual optimizations \nfound within CompCert (Sections 6.1 and 6.2). Next, we evaluate XCert in terms of its trusted computing \nbase (Section 6.3), extensibility (Section 6.4) and correctness guar\u00adantee (Section 6.5). We conclude \nby considering the limitations of our current execution engine (Section 6.6). 6.1 Engine Complexity The \nPEC execution engine that we added to CompCert comprises approximately 1,000 lines of Coq code. Its main \ncomponents are the pattern matching and the substitution application which al\u00adlow us to easily implement \nthe transformations speci.ed by PEC rewrite rules. The PEC untrusted prototype execution engine mentioned \nin [7] was roughly 400 lines of OCaml code. Although both execution engines apply PEC rewrite rules to \nperform optimizations, they work in very different settings. The CompCert execution engine targets the \nCFG-based RTL representation in CompCert, while the prototype in [7] targets an AST-based representation \nof a C-like IR. We also compare the PEC execution engine against CompCert s two main RTL optimizations, \ncommon subexpression elimination (CSE) and constant propagation (CP). CSE is 440 lines of Coq code, and \nCP is 1,000 lines. Both of these optimizations make use of a general purpose data.ow solver, which is \nabout 1,200 lines. Structurally, the PEC execution engine is very different from the optimizations in \nCompCert. Most of the code in the PEC engine performs pattern matching and tricky CFG splicing to achieve \nthe task of replacing an entire region of the CFG with another. In\u00adstead, CSE and CP in CompCert perform \nsimple CFG rewrites (one statement to another), and instead focus their efforts on computing data.ow \ninformation.  6.2 Proof Complexity The proof of correctness for our execution engine is approximately \n3,000 lines of Coq proof code. This code de.nes (1) the intermedi\u00adate semantics .\u00a3 and .r that facilitate \napplying the PEC guaran\u00adtee, (2) Coq proof scripts demonstrating the semantic preservation of transformations \nperformed by the execution engine and (3) tac\u00adtics that make developing these proofs easier and more \nconcise. CompCert s correctness proofs for CSE and CP each span nearly 1,000 lines of proof code. Structurally, \nthe correctness proofs for these CompCert optimizations are quite different from the execution engine \ns correctness proof, because they deal with different challenges. The CSE and CP proofs are mainly devoted \nto extracting useful facts from the result of the data.ow analysis per\u00adformed by the transformation. \nThese facts are then used to estab\u00adlish suf.cient conditions for semantic preservation. In contrast, \nthe proof of the execution engine focuses on showing that the many-to\u00admany CFG rewrites that the PEC \nengine performs are correct. This typically involves splitting into two cases: cases where execution \nis not in the transformed code, which are typically straightforward; and cases where execution is in \nsome region that has been trans\u00adformed, in which case the proof effort involves either showing the case \ncannot arise or the simulation relation from PEC applies. Note that the correctness proof for the PEC \nexecution engine is three times larger than the PEC execution engine itself. However, the engineering \neffort for developing the proof was at least an order of magnitude greater than the effort for developing \nthe execution engine. This is because we re-engineered the proof several times to make it simpler, cleaner, \nand more manageable using tactics. 6.3 Trusted Computing Base The trusted computing base (TCB) consists \nof those components that are trusted to be correct. A bug in these components could invalidate any of \nthe correctness guarantees that are being provided. The TCB for the regular CompCert compiler (without \nthe PEC engine) includes CompCert s implementation of the C semantics, Coq s underlying theory (the Calculus \nof Inductive Constructions), and Coq s internal proof checker. When CompCert is extended with the PEC \nexecution engine, the TCB grows because, even though the engine is proved correct in Coq, we trust that \nthe PEC checker correctly checks any simulation relation it returns. Within the PEC implementation this \nchecker is implemented in about 100 lines of OCaml code and makes calls to an SMT solver like Simplify \n[5] or Z3 [4]. Thus, the PEC engine adds the following to CompCert s TCB: 100 lines of OCaml for the \nPEC checker, an SMT solver like Simplify or Z3, and the encoding of CompCert s RTL semantics to be used \nby the SMT solver.  6.4 Extensibility With this relatively small increase in TCB comes the following \nben\u00ade.t: additional optimizations that are added using PEC do not re\u00adquire any new manual proof effort, \nand do not add anything to the TCB. In contrast, for each new optimization added to CompCert, unless \na veri.ed validator has already been speci.cally designed for it, the new optimization would either have \nto be proved correct, or if not, it would be trusted, thus increasing the TCB. Thus, the provably correct \nPEC execution engine brings all of the expressive\u00adness and extensibility shown previously in [7] to CompCert \nwhile adding only a small amount to the TCB. To test the extensibility of our system, we implemented \nand ran all the optimizations checked by PEC s Relate module in [7]. We ran the optimizations on an array \nof CompCert C benchmarks to\u00adtaling about 10,000 lines of code. The benchmarks included cryp\u00adtographic \ncode like AES and SHA1, numeric computations such as FFT and Mandelbrot, and a raytracer. We manually \nchecked that the transformations were carried out as expected.  6.5 Correctness Guarantee While the \nsize of the TCB tells us how much needs to be trusted, it is also important to evaluate the correctness \nguarantee provided in exchange for this trust. Essentially, the CompCert compiler ex\u00adtended with our \nPEC execution engine provides the same guaran\u00adtee as the original CompCert compiler: if the compiler \nproduces an output program, then the output program will be semantically equivalent to the corresponding \ninput program. There are two ways in which this guarantee is not as strong as one may hope for. First, \nCompCert extended with our PEC execu\u00adtion engine is not guaranteed to produce an output program, even \non a valid input program, because some passes from CompCert may abort compilation. For example, during \nthe stack layout phase of CompCert, if a program spills too many variables and exceeds the available \nstack for a given function, then CompCert is forced to abort without producing an assembly output program. \nHowever, the PEC engine itself always produces an output program, and there\u00adfore is not a source of incompleteness. \nThe other weakness in the PEC engine s correctness guarantee is shared by all systems that use veri.ed \nvalidation. In particular, those parts of the system that are checked using veri.ed valida\u00adtion may still \ncontain bugs in them. For example, the initial version of our PEC execution engine did not always correctly \ninstantiate fresh nodes for the RHS of a PCFG. However, when this bug was exercised, our veri.ed validator \ndetected that the generated nodes did not have the required freshness property, and prevented the in\u00adcorrect \ntransformation from being performed. Such bugs therefore manifest themselves not as violations of the \ninput/output equiva\u00adlence guarantee, but as missed optimization opportunities. The ex\u00adistence of such \nquality-of-optimization bugs emphasizes the value of having run our PEC engine on real code, as described \nin Sec\u00adtion 6.4, and ensuring that the optimizations operate as expected. 6.6 Limitations The PEC checker \nis currently not implemented in Coq. Thus, for each PEC rewrite rule r, we must translate by hand the \nsimulation relation produced by the PEC checker for r into a Coq term and axiomatize its correctness \nproof. We intend to develop a version of PEC that directly outputs these simulation relations as Coq \nterms. Eventually, we plan to also implement all of PEC in Coq and thus eliminate the disconnect between \nthe two systems. Our current version of parameterized statements like S in Fig\u00adure 2 are only able to \nmatch .xed length sequences of arbitrary in\u00adstructions. Although this allows us to simulate parameterized \nstate\u00adments of a .xed size, we must properly implement parameterized statements to achieve the full expressiveness \nof PEC. 7. Future Work There are several directions for future work that we intend to ex\u00adplore. First, \nwe plan to systematically and thoroughly compare the quality of existing CompCert optimizations with \ntheir correspond\u00ading PEC versions. Our evaluation will consider the runtime perfor\u00admance of generated \ncode and the number of missed optimization opportunities. This comparison will enable us to .ne tune \nour PEC optimizations and execution engine which, eventually, we hope will match the optimization capabilities \ncurrently found in CompCert. More broadly, we will also evaluate the relative effort of adding optimizations \nusing XCert versus coding them directly in Coq or within other optimization frameworks. We also plan \nto explore further reductions to the TCB. When our PEC execution engine is added to CompCert, the TCB \ngrows because the PEC checker becomes trusted. However, if we reim\u00adplement the PEC checker in Coq and \nformally prove its correct\u00adness, then our PEC engine would not at all increase the size of the TCB. The \ncore of the PEC checker consists of only 100 lines of stateless OCaml code, which we anticipate will \nbe easy to imple\u00adment and reason about in Coq. However, this core checker makes queries to an SMT solver \n(like Z3) which could be challenging to integrate into Coq. Fortunately, there are several reasons to \nbe op\u00adtimistic. First, some SMT solvers have recently been re-engineered to produce proof terms, which \nwe should be able to automatically translate to Coq terms and thus integrate into a Coq proof (possibly \nusing the Coq Classical extension to accommodate for the refuta\u00adtion based proof strategies common in \nSMT solvers). Second, the PEC checker s SMT queries tend to be simple and highly stylized. Thus, it may \ninstead be possible to implement a sophisticated tactic in Coq s tactic language to discharge these obligations \ndirectly. We plan to investigate both of these approaches, with the ultimate goal of implementing a veri.ed \nPEC checker in Coq. Finally, we would also like to investigate extending XCert to support the Permute \nmodule from PEC [7]. This would allow additional loop optimizations to be easily added to CompCert, such \nas loop reversal and loop distribution. Adding such support to XCert would require formally developing \nthe general theory of loop reordering transformations found in [18], upon which the PEC checker s Permute \nmodule is based. Doing this will be challenging because it s not clear how to express the above theory \nof loop transformations in a way that meshes well with CompCert s existing support for correctness proofs \nusing simulation relations. Nonetheless, formalizing such a theory in Coq is worthwhile, as it would \nnot only enable support for Permute optimizations in XCert, but could also be broadly useful within CompCert. \n8. Related work Our work is closely related to three lines of research: veri.ed compilers, extensible \ncompilers, and translation validation. Veri.ed Compilers Veri.ed compilers are accompanied by a fully \nchecked correctness proof which ensures that the compiler pre\u00adserves the behavior of programs it compiles. \nExamples of such compilers include Leroy s CompCert compiler [9], Chlipala s com\u00adpilers within the Lambda \nTamer project [2, 3], and Nick Benton s work [1]. At a lower level, Sewell et. al. s work [13] on formalizing \nthe semantics of real-world hardware like the x86 instruction set provides a formal foundation for other \nveri.ed tools to build on. However, none of these compilers are easily extensible ex\u00adtending these compilers \nwith additional optimizations requires ei\u00adther modifying the proofs or trusting the new optimizations \nwithout proofs. The main goal of our work is to devise a mechanism to cross this extensibility barrier \nfor veri.ed compilers. Although our work was done in the context of the CompCert compiler, the general \nap\u00adproach that we took for integrating PEC into a veri.ed compiler could be applied to other veri.ed \ncompilers. Extensible Compilers There has been a long line of work on mak\u00ading optimizers extensible. \nThe Gospel language [17] allows com\u00adpiler writers to express their optimizations in a domain-speci.c \nlan\u00adguage, which can then be analyzed to determine interactions be\u00adtween optimizations. The Broadway \ncompiler [6] allows the pro\u00adgrammer to give detailed domain-speci.c annotations about library function \ncalls, which can then be optimized more effectively. None of these systems, however, are geared at proving \nguarantees about correctness. The Rhodium [8] and PEC [7] work took the exten\u00adsible compilers work in \nthe direction of correctness checking. In these systems, correctness is checked fully automatically, \nbut the execution engine is still trusted. Our current work shows how to bring a trusted execution engine \nto such systems. Translation Validation Translation validation [10 12] is a tech\u00adnique for checking \nthe correctness of a program transformation af\u00adter it has been performed. Indeed, it is often easier \nto check that a particular instance of a transformation is correct than to show that transformation will \nalways be correct. Although these techniques may increase our con.dence that a compiler is producing \ncorrect code, only a veri.ed translation validator can guarantee the correct\u00adness of the a posteriori \ncheck performed by the validator. Tristan et. al. examine such techniques for using veri.ed translation \nval\u00adidation to add more aggressive optimizations to CompCert while keeping the veri.cation burden manageable \n[14 16]. Acknowledgments We thank Xavier Leroy, Jean-Baptiste Tristan, and the rest of the CompCert team \nfor developing and releasing a well-documented and well-engineered tool. We also thank the anonymous \nreviewers for their careful reading and helpful comments. Finally, we thank the UCSD Programming Systems \ngroup for many useful conversa\u00adtions and insightful feedback during the development of XCert. References \n[1] N. Benton and N. Tabareau. Compiling functional types to relational speci.cations for low level imperative \ncode. In TLDI, 2009. [2] A. Chlipala. A certi.ed type-preserving compiler from lambda calcu\u00adlus to assembly \nlanguage. In PLDI, 2007. [3] A. Chlipala. A veri.ed compiler for an impure functional language. In POPL, \n2010. [4] L. de Moura and N. Bj\u00f8rner. Z3: An ef.cient SMT solver. In TACAS, 2008. [5] D. Detlefs, G. \nNelson, and J. B. Saxe. Simplify: a theorem prover for program checking. J. ACM, 52(3):365 473, 2005. \n[6] S. Z. Guyer and C. Lin. Broadway: A compiler for exploiting the domain-speci.c semantics of software \nlibraries. Proceedings of IEEE, 93(2), 2005. [7] S. Kundu, Z. Tatlock, and S. Lerner. Proving optimizations \ncorrect using parameterized program equivalence. In PLDI, 2009. [8] S. Lerner, T. Millstein, E. Rice, \nand C. Chambers. Automated sound\u00adness proofs for data.ow analyses and transformations via local rules. \nIn POPL, 2005. [9] X. Leroy. Formal certi.cation of a compiler back-end, or: program\u00adming a compiler \nwith a proof assistant. In POPL, 2006. [10] G. C. Necula. Translation validation for an optimizing compiler. \nIn PLDI, 2000. [11] A. Pnueli, M. Siegel, and E. Singerman. Translation validation. In TACAS, 1998. [12] \nM. Rinard and D. Marinov. Credible compilation with pointers. In Workshop on Run-Time Result Veri.cation, \n1999. [13] S. Sarkar, P. Sewell, F. Z. Nardelli, S. Owens, T. Ridge, T. Braibant, M. O. Myreen, , and \nJ. Alglave. The semantics of x86-cc multiproces\u00adsor machine code. In POPL, 2009. [14] J.-B. Tristan \nand X. Leroy. Formal veri.cation of translation valida\u00adtors: A case study on instruction scheduling optimizations. \nIn POPL, 2008. [15] J.-B. Tristan and X. Leroy. Veri.ed validation of lazy code motion. In PLDI, 2009. \n[16] J.-B. Tristan and X. Leroy. A simple, veri.ed validator for software pipelining. In POPL, 2010. \n[17] D. L. Whit.eld and M. L. Soffa. An approach for exploring code improving transformations. ACM Transactions \non Programming Lan\u00adguages and Systems, 19(6):1053 1084, Nov. 1997. [18] L. Zuck, A. Pnueli, B. Goldberg, \nC. Barrett, Y. Fang, and Y. Hu. Translation and run-time validation of loop transformations. Form. Methods \nSyst. Des., 27(3):335 360, 2005.    \n\t\t\t", "proc_id": "1806596", "abstract": "<p>Verified compilers, such as Leroy's CompCert, are accompanied by a fully checked correctness proof. Both the compiler and proof are often constructed with an interactive proof assistant. This technique provides a strong, end-to-end correctness guarantee on top of a small trusted computing base. Unfortunately, these compilers are also challenging to extend since each additional transformation must be proven correct in full formal detail.</p> <p>At the other end of the spectrum, techniques for compiler correctness based on a domain-specific language for writing optimizations, such as Lerner's Rhodium and Cobalt, make the compiler easy to extend: the correctness of additional transformations can be checked completely automatically. Unfortunately, these systems provide a weaker guarantee since their end-to-end correctness has not been proven fully formally.</p> <p>We present an approach for compiler correctness that provides the best of both worlds by bridging the gap between compiler verification and compiler extensibility. In particular, we have extended Leroy's CompCert compiler with an execution engine for optimizations written in a domain specific and proved that this execution engine preserves program semantics, using the Coq proof assistant. We present our CompCert extension, XCert, including the details of its execution engine and proof of correctness in Coq. Furthermore, we report on the important lessons learned for making the proof development manageable.</p>", "authors": [{"name": "Zachary Tatlock", "author_profile_id": "81392605383", "affiliation": "University of California, San Diego, San Diego, USA", "person_id": "P2184521", "email_address": "", "orcid_id": ""}, {"name": "Sorin Lerner", "author_profile_id": "81100399150", "affiliation": "University of California, San Diego, San Diego, USA", "person_id": "P2184522", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1806596.1806611", "year": "2010", "article_id": "1806611", "conference": "PLDI", "title": "Bringing extensibility to verified compilers", "url": "http://dl.acm.org/citation.cfm?id=1806611"}