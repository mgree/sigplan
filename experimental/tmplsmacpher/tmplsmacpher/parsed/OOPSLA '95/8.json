{"article_publication_date": "10-01-1995", "fulltext": "\n Using Objects to Manage In-memory Data Intensive Expert Systems Steven Mamey and Mamdouh Ibrahim Electronic \nData Systems Object Oriented &#38; AI Services 5555 New King Street, M/S:402 Troy, MI 48098 . 1. Introduction \nThis report discusses the experience we gained developing the Dealer Consultant Expert System (DCES), \nand the role objects play in the representation and management of the immense amount of data the system \nanalyzes. In particular, we discuss some of the innovative ideas and benefits we realized by using objects \nto: 1) manipulate the large amount of data that form the core of the expert system; 2) reason with an \nembedded inference engine on application objects constructed in C++; and 3) implement multiple representations \nfor data access, procedural manipulation and inferencing. The next section introduces an overview of \nthe project and its current status. Section 3 presents the application architecture and the methodology \nused for development. Section 4 discusses our experience in using objects for in-memory data representation \nand manipulation. The last two sections summarize lessons learned and future plans. 2. Project Overview \nProject Background The Dealer Consultant Expert System (DCES) is a PC Windows application developed by \nGeneral Motors (GM) and Electronic Data Systems (EDS) that analyzes dealership financial operations and \nidentifies opportunities for making dealerships more profitable. The system runs in two modes: interactive \nand batch. GM dealers and field representatives use the interactive version of the application that runs \non Windows 3.1 to analyze one dealership at a time. Each month, GM analyzes all the dealerships in a \nbatch cycle that runs in a cooperative processing environment of several Windows NT clients (each analyzing \none dealer at a time) coordinated by a scheduler that resides on a database server. Without the system \nthe analysis process takes over 120 hours for manual analysis of a single dealership. DCES performs a \nmore thorough analysis in less than three minutes. Each analysis currently uses up to 17 months of data \nfrom GM s dealer Financial Analysis Capabilities Through Scanning (FACTS) database, over 85,000 pieces \nof data per dealer. Examples of such pieces of data are total inventory, new car sales, etc. DCES currently \nderives another 102,000 pieces of data, totaling approximately 1.5Mb of data that are used in the analysis. \nThe analysis first compares the dealer s data to corporate guides and to corresponding averaged data \nfrom a group (or composite) of similar dealers to produce guide results. These guide results are represented \nby objects, where each object contains a slot with a symbolic value (very high, high, low, etc.) that \ndescribes how the dealer compares to its peers or Addendum to the Proceedings OOPSLA 95 corporate guidelines \nfor a given data element. Similarly, trend processing looks at the performance of each data element over \ntime and produces trend results with symbolic values (increasing, &#38;creasing, sawtooth, etc.) that \ndescribe the trend. An embedded expert system then looks for patterns in the guide and trend results \nto determine what situations exist in each of the functional areas that are managed within the dealership. \nThe system produces summary and detail level reports on the analysis results, with full explanations \nof the reasoning behind its conclusions. It also provides explanations of how any piece of derived data \nwas calculated, showing complete derivations in algebraic form along with data values and units.  Project \nStatus To validate the feasibility of the concept, an early prototype of the system was developed using \nC and ART-IM, an expert system shell from Inference Corporation. The production system described in this \nreport was developed using Visual C++, various class libraries (including Microsoft Foundation Classes \nand text formatting GUI libraries), Microsoft Access, and Rete++, an inference engine class library from \nThe Haley Enterprise, in order to take advantage of the object paradigm. The system was demonstrated \nat this year s North American Dealers Association (NADA) convention and will again be GM s feature exhibit \nat next year s NADA convention in Las Vegas. As part of an early production rollout, DCES has been deployed \nto over 500 General Motors wholesalers and dealerships throughout the United States and Canada.  3. \nApplication Architecture and Methodology To satisfy the customer request to downsize the system, a cooperative \nprocessing environment was adopted where an array of PCs, controlled by a scheduler, run the monthly \ncycle for analyzing the financial data of all dealers. To reuse the components of the system that are \ncommon between the interactive and batch versions of the system, a three-layer architecture model was \nadopted and is illustrated in Figure 1. GUI Layer GUI Manager -&#38; GUI Objects jF!!l Application Layer \nI System I DBDB Manager Database Layer Objects Figure 1. DCES Three-Layer Architecture The interaction \nbetween the three layers is controlled by three object managers: the GUI manager, the Application manager, \nand the DB manager. Each manager implements a protocol which handles the high-level interactions with \nthe other layers. This allows the interchangeability of the GUI layer in the interactive version with \na command layer in the batch version with no required changes to the application or database layers. \nWithin the application layer, the expert system component is encapsulated and invoked as a behavior of \nthe Analysis object, which is initialized by the application manager. The expert system, based on the \nRete++ inference engine, allows reasoning on the guide and trend results represented by C++ objects. \nThis provides a seamless integration between the procedural processing of the raw data and the inferencing \nthat is performed on the processed results. A modified version of the Fusion Light methodology was adopted \nto develop an object model, interface model, a set of object interaction graphs, and class descriptions \nand inheritance graphs for each layer. The modifications to Fusion focused on generating interaction \ngraphs not only for the system operations defined in the interface model, but also for every internal \ncomplex method. These interaction graphs, in particular, proved to be very valuable in achieving a sound \nobject design. 4. In-memory Data Representation and Manipulation As mentioned in the project overview, \nDCES currently analyzes over 85,000 pieces of raw data and 102,000 derived pieces of data. The amount \nof input data is expected to double and the derived data to increase six-fold. This volume of data was \na major factor in the design of the application. These data are analyzed in a two-step process. The first \nis procedural in nature, where derived data are calculated and the results are compared to predefined \nguides and trends. This generates guide and trend results in symbolic form. The process was implemented \nas methods on C++ objects. The second step looks for patterns in the guide and trend results. This tends \nto be heuristic and was therefore implemented as forward-chaining rules in a knowledge base. The system \ncurrently contains about 1,000 rules, which require about 1Mb RAM when loaded. The number of rules is \nexpected to triple in the next release of DCES, so Rete++ s ability to load and unload the knowledge \nbase on demand gives us greater flexibility in how we manage memory. The following subsections describe \nsome innovative applications of objects for data representation and manipulation in DCES. Data Representation \nSince there is no single data representation that satisfies both the procedural and pattern matching \nprocessing requirements, we adopted a two-representation approach for DCES data, both of which have been \nimplemented using objects. The first representation is geared toward efficient relational database I/O \nand for procedural algorithms that manipulate the data. There is a one-to-one mapping between the data \nclasses and the relational tables in which the data are stored. This representation is implemented as \npart of a data manager object that manages all the input and derived data. The second representation \nis based on a classification of the data into a taxonomy of classes that is well-suited for pattern matching \nby the expert system. The guide and trend class hierarchies follow this taxonomy. These classes are specializations \nof the corresponding Rete++ classes so that the inference engine s working memory is automatically updated \nas the guide and trend results are generated. The mapping between the two representations is achieved \nthrough a meta data layer that describes the taxonomy of each piece of data in the form of logical data \npaths. A Rosetta Stone object encapsulates this meta data layer. The logical data paths are used by the \ndata manager and the Rosetta Stone objects to access the data symbolically. This is further discussed \nin the next section. Data Manipulation Procedural algorithms compute derived data, identify missing \nand inconsistent data, and process guide and trend definitions to produce guide and trend results. Methods \non the data manager perform the necessary computations to produce the derived data. By encapsulating \nthese methods on the data manager, we gain the flexibility to reorganize and/or tune the data representation \nin the future with no impact on the external analysis algorithms, which access data via logical data \npaths. We anticipate this will be necessary in the future when we extend the analysis to include data \nfrom other database systems. Similarly, an object was developed to encapsulate the business rules that \nidentify the missing and inconsistent information. To overcome the problem of calculation failure due \nto uninitialized and/or unknown pieces of data, we implemented a number type class hierarchy whose full \nset of operators (e.g., arithmetic, casting, etc.), are overloaded. For example, instead of using primitive \nnumeric data types such as float, double and long, we use DCESFloat, DCESDouble and DCESLong. The Addendum \nto the Proceedings OOPSLA 95 overloaded operators check for calculations that reference uninitialized \nand/or unknown data and return appropriate results. Implementing this using objects keeps the code clean \nand has proved to be very useful during development, even though some performance overhead is incurred. \nGuide and trend processing are encapsulated in guide manager and trend manager objects, respectively. \nSending a message to the appropriate manager triggers guide or trend results to be created by iterating \nover definitions that are declaratively represented and stored as records in a database. Based on the \ntype of guide or trend definition being processed, an appropriate guide or trend result object is created. \nThe declarative representation allows users to modify the definitions through editors in the interactive \nversion of the system. Non-procedural processing is based on execution of generic rules that match on \ncommon portions of the guides and trends logical data paths and their symbolic results. When the rules \nare fired, other logical data paths can be constructed from the matched portions to allow for access \nto related pieces of data. The rules generate analysis result objects that are used to generate various \nreports and explanations.  5. Lessons Learned The following summarize the lessons learned while developing \nthe DCES application. . Applications written in C are not necessarily easy to re-implement in an object-oriented \nway even if it is developed using C++. As in our experience with the transition from the prototype (developed \nin C) to the production system (developed in C++) it was necessary to redesign the entire application \nin order to benefit from objects. This is due to the fact that object-oriented features such as inheritance, \npolymorphism, encapsulation, etc. are not available and hence were not considered in the design of the \noriginal programs written in C. . Object-oriented applications should be based on a three-layer architecture. \nIn developing DCES, the three-layer architecture proved valuable in achieving clean separation between \nthe presentation, application and database layers of the system. This was also valuable in organizing \nand managing the development team. . It is important to use an object-oriented methodology during the \nanalysis and design of an object-oriented application. As we experienced during the development of DCES, \ndevelopers who resisted using the methodology initially produced longer and more procedurally oriented \nmethods. On the other hand, those who started their analysis and design using Fusion, generated cleaner \nand more object-oriented designs. Eventually, the first group realized the benefit of using Fusion and \na decision was made to take the time to redesign their modules. In particular, the interaction graphs \nof Fusion provided a solid foundation for the design of the system. . Even if the application is data \nintensive, it is worth encapsulating the data in objects for application processing rather than using \nglobal functions and a procedural approach. In our experience, we started with partial knowledge about \nthe relationships among the pieces of data to be analyzed. We knew that as we discover more relationships \namong the current and future data, we will need to reorganize the data to accommodate these new relationships. \nBy encapsulating the data and the methods that access and manipulate these data on the Data Manager object, \nwe are able to reorganize and regroup the data with minimal impact on the rest of the application. This \nwas a dramatic improvement over the prototype, which used global functions to manipulate data in C structures \nand was very rigid and hard to modify. . Do not hesitate to introduce classes that extend the capabilities \nof the language s native data types if needed. When we encountered the problem of processing uninitialized \nand unknown data we were debating one of two solutions: either to embed explicit checks throughout the \ncode, or introduce specialized number classes with overloaded operators that perform the necessary checks. \nWe actually implemented both solutions and found that the overhead that is incurred with the specialized \nnumber classes has so far been negligible for this application. However, the development team has realized \nsignificant productivity gains by using the number classes as knowledge and calculations are added to \nthe system. . Tools such as Rete++ are a step in the right direction toward embedding expert systems \nin object oriented applications. In traditional expert system shells, there is a need to exchange data \nbetween the main application and the expert system component through extensive usage of API s. The overloaded \noperators on Rete++ classes maintain seamless mapping between working memory elements and the corresponding \nC++ domain objects. This reduces the application code necessary to interface with the rule-base. . Object-oriented \ntools and languages should be selected on a project by project basis. In our case, we had to take into \nconsideration the ease of integration with available expert system shells, runtime performance, memory \nrestrictions imposed by target user platforms, and runtime license fees. Although C++ lacks automatic \nmemory management, restricts dynamic binding, and burdens the developers with lengthy compile/link cycles, \nit was the best choice for this project. C++ provided seamless integration with the inference engine, \nenabled us to tune system performance easily and meet memory requirements. In addition, C++ does not \nrequire runtime licenses. As a result, the production system met all customer expectations. . Adequate \ntime must be allocated within the project schedule for object-oriented training. DCES started as an expert \nsystem rather than an object-oriented project. As a result, the team was originally staffed with non-object \noriented developers. As the focus shifted, it was envisioned that the addition of a mentor would be enough \nto bring team members up to speed with minimum impact on the project. This proved costly, since it took \nmuch longer for the developers to adjust to the new paradigm than expected. From this experience we learned \nthat new team members not exposed to the object paradigm must be trained first, and that adequate time \nmust be added to the project schedule to allow for proper training. . Current class-based languages \nand environments have limitations. Ideally, each piece of data that DCES analyzes should know all about \nitself, such as how to calculate itself, trend itself, etc. With the available class-based object oriented \nlanguages, each piece of DCES data would have been implemented as a single instance of the class that \ndescribes its behavior. This would have resulted in over 5,000 classes to represent the given data. These \nclasses share some, but not all, behaviors. With the limitations of current class-based languages, we \nwould have been forced to duplicate most of the class behaviors. On the other hand, frame-based and delegation-based \nlanguages could provide a solution to this problem since they allow their objects to share behaviors \nthrough delegation of messages to other objects on which the appropriate behaviors are defined.  6. \nFuture Plans The system currently contains only about a third of the knowledge required to analyze the \nFACTS data. In the near future, we will expand the knowledge base to fully analyze these data. Once this \nstep is completed, the plan is to include new data from additional sources, such as vehicle option databases. \nThis will approximately double the current amount of input data; and the number of calculations and rules \nwill increase exponentially as new knowledge is added to analyze these additional data. Addendum to \nthe Proceedings OOPSLA 95 \n\t\t\t", "proc_id": "260094", "abstract": "", "authors": [{"name": "Steven Marney", "author_profile_id": "81100356702", "affiliation": "Electronic Data Systems, Object Oriented & AI Services, 5555 New King Street, M/S:402, Troy, MI", "person_id": "P270117", "email_address": "", "orcid_id": ""}, {"name": "Mamdouh Ibrahim", "author_profile_id": "81332505897", "affiliation": "Electronic Data Systems, Object Oriented & AI Services, 5555 New King Street, M/S:402, Troy, MI", "person_id": "PP39073575", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/260094.260237", "year": "1995", "article_id": "260237", "conference": "OOPSLA", "title": "Using objects to manage in-memory data intensive expert systems", "url": "http://dl.acm.org/citation.cfm?id=260237"}