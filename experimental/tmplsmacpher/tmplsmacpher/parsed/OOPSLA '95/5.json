{"article_publication_date": "10-01-1995", "fulltext": "\n Building and Performance Tuning a Distributed Reporting Application for an OODBMS. Debbie Meadows and \nCharles Gardner. Texas Instruments, Inc. debm@scgis.sc.ti.com INTRO Distributed Reporting (DR) is an \nObject Oriented application developed by Texas Instruments Semiconductor division. It is written primarily \nin Smalltalk with some related external processes written in C++. It is a reporting tool for data stored \nin an object database. TI semiconductor manufacturing is currently supported by a mainframe application. \nDR was developed to address the need for a more robust, flexible, and less costly reporting tool than \nwas currently available on the mainframe. It was designed to allow end users to select the data they \nwant to report on and layout that information as desired via GUI interfaces. The requirements included \nthat it must have equal or better responsiveness than the mainframe application, not be restricted to \nSC manufacturing data but be able to report on any data stored in the OODB, and be easily integrated \ninto other applications. DR is divided into three major categories: data views, query and report engines, \nGUI and command line interfaces. This paper will discuss the functionality of each category and the steps \ntaken in each to attain the performance goals for the application. It describes how the query and reporting \nengines supported changes in the application schema to improve performance where native indexes were \nnot used. Query performance was improved by building a query processor which compiled code for the query. \nOther gains were achieved by using application maintained collections to short cut some common query \nterms. The break down on query terms is described. Report performance was also improved by compiling \ncode to generate the report with a report compiler. Changes in the way X was used to display the report \nimproved the users perception of the performance. Removal of unnecessary object creation and better use \nof collection classes also resulted in savings. DATA VIEWS ----_-----  ----_---me Data views are an \nintegral part of the reporting tool. They are the mechanism that ties all other pieces of the reporting \napplication together. Any application that wants to use DR to create reports against data that is stored \nin the OODB must first define these views. The first step is to consider the type of information that \nis available in the database and what kinds of reports your users are going to want. A report typically \nis focused on a particular object, with fields on various attributes of that object, and other objects \nthat are directly related to it. Once you have determined that starting object you can begin defining \nthe data view. The view itself is given a name and description and you determine what fields will be \nreportable from this particular view. This list of fields will be provided to end users on other interface \nscreens when they define their queries and reports. Having a view allows you to provide names and descriptions \nfor each field that will be meaningful to your end user, it shields the user Austin, TX October 15-19, \n1995 from needing any information about how to access that data, and you are able to provide a selection \nof fields that will be of interest to the user without the clutter of other fields that are used for \nprogramming purposes. Multiple views can be defined against a single starting object. The purpose of \nthat could be security driven. Some users might be allowed to report on information that others may not \nbe cleared for. The fields that you define for a view contain information about how to retrieve that \ndata from the starting object. The information available is not limited to a single object. We can retrieve \ninformation from any object that is accessible via the starting object and make it a reportable field \nfrom our view. A field may even be a calculation of attributes actually defined across multiple objects. \nDefining data views requires some knowledge of class hierarchies, instance variables, and accessors so \nthis is typically a programmer function that is done up front. An issue surfaces with data views whenever \nyour class hierarchy changes. The fields on the views must be revisited to ensure that they understand \nhow to access information in the new schema. The idea1 solution would be to somehow automate changes \nto a class to the views that are affected. This is hard to do without a one to one relationship between \nthe class and the view. The advantages of more flexible views has outweighed the synchronization issues \nto date. Once the data views are defined the end user will be able to define queries and reports via \nan easy to use GUI interface. The first thing that must be done for either a query or a report definition \nis to specify the starting object and select a data view defined for that starting point. These are selections \nmade from lists available on the interface.  QUERY DEFINITIONS AND QUERY ENGINE __--___------------_-----------~~- \n_________--__----_-_-------------- Defining a query involves selecting fields (terms) from a list that \nthey wish to filter data by. For each term they can select a relational operator and then input one or \nmore filtering values. The various terms are booleaned together to create complex queries. For example, \nif your starting point was a deck of playing cards you could define a query to retrieve all cards where \nthe number was greater than 5 and less than 10 and the suit was equal to Spades or Diamonds. By providing \nlists of fields, operators, and boolean selections in a point and click environment the users are able \nto create sophisticated queries without knowing anything about how data is stored or having to learn \nany kind of querying language. The user gives the query a name and description and saves it for later \nexecution. The result of a query execution is a collection of data items that met the filtering criteria. \nThis collection will be fed into the report formatting engine. The query engine is the portion of the \napplication that is responsible for executing a query definition against the data on the database. The \nperformance of the query engine was addressed from several directions. The primary point of attack was \nthe implementation of the query engine itself. The original approach had special methods on collection \nclasses to make use of system indexes. The view fields have the accessor defined that is required to \nretrieve the data from the object. The query engine then concatenated the term withoperator: to the accessor \nat runtime and tested for the existance of this method first. If found, then a system index was defined \nfor this data. Below is an example of this special code that was maintained for indexed query terms. \n1ogpointNameText: aVa1ueSet withoperator: anoperator I result theValue I result := self class new. (anoperator \n= = ) itTrue: [ 1 to: (aVa1ueSet size) do: [ :jj I theValue := aValueSet at: jj. result addAll: (self \nselect { :ii I ii.currentWipInfo.logpoint.logPoint = thevalue})  1. I* There were a number of problems \nwith this approach. First, these special methods had to be maintained and the dot notation that was used \nmeant that any schema change required updates to the methods. Second, the query terms were always processed \nin the order they were defined on the query definition. If the first term in the query did not have a \nsystem index then the brunt of the processing was non-optimized. Regardless, once the first term had \nbeen processed and a reduced collection obtained there were no more indexed searches even if indexes \nwere defined on subsequent terms. Third, there was no generic way to handle special built indexes or \nother traversal methods, Finally, all the non-indexed terms executed one perform to get the value and \na second one for the comparison operator for each entry in the collection being filtered. These performs \nwere expensive operations. Re-evaluating the implementation revealed that the process that was needed \nto determine whether an object met the filtering criteria was the same process needed for each object \nin the collection. This made a good case for using pre- compiled blocks. A pre-compiled block basically \nmeans executing code that generates code. This removes the need for performs . The generated code orders \nquery terms to provide the best optimization and handles the special traversal needs of specific terms. \nThis generated code is compiled to make an executable block that can just be executed against each object \nin the collection. We reviewed all our query terms and categorized them into types of traversal required, \nthen determined the optimization rules from fastest to slowest. This determined how to generate the most \noptimized code to execute each query. Below is an example of generated code. [snip...] [ :theSet I IfinalResultSet \ntempSet tmpSet tetmResultSet predResultSet WC 1 I finalResultSet := Set new. WC 1 := ABCD asWildCardArray. \npredResultSet := Set new. tempSet := (theSet select: ( :ele I (ele.currentWipInfo.logpoint.logPoint >= \n1000 ) &#38; (ele.opnCurrent.ID > 2000 ) } ). [snip...] 1 to: tempSet size do: [ :idx I I ele result \ncnt toll aColVal I ele := (tempset at: idx). result := false. result := ((ele deviceNameText) match0: \nWC 1). result iffrue: [predResultSet add: ele 1. I. [snip...] The first select demonstrates the concatenation \nof 2 indexed query terms which allowed both to execute with the advantage of system indexes. The second \nselect is an example of a term where the value contained a wildcard character and required execution \nof a special comparison method. In order to enhance the query performance at the specific application \nlevel, some objects within the application were modified to maintain pointers to collections which were \nassociated with that object. Some objects within the system already had those pointers to support some \naspect of the user interface on related applications. In other cases they had to be added but from the \nend user point of view would probably be necessary on the UI side anyway at some point. Austin, TX October \n15.19,199s We found that having too many system indexes defined in the database caused performance degradation \non the real-time updates. We had to reduce the overall number of system indexes defined. Some help to \nthe end user to construct better queries was required. In some cases poor queries were constructed because \nof the limitations experienced in the mainframe application. Limitations such as number of sort fields \nand selection fields would have the user confine their queries to the fields they were familiar with. \nSince the IMS based system would respond with a screen at a time they were accustomed to getting the \nreport and then pressing the enter key several times to get to the place they wanted. With the additional \nfields and capabilities (such as grouping and sorting by any field), the end user could construct a query \nwhich would be more selective and make good use of optimized fields as the first terms. REPORT LAYOUTS \nAND THE REPORT FORMATTING ENGINE _____-_____________----------------- ___---____--_____-------~------------- \n------_---- The end user defines a report layout to reflect how they want information returned by the \nquery to be formatted on the printed page. A report definition can be as simple as row and column or \nmore complex involving headers, footers, group breaks, sub and grand totals, and multiple levels of ascending \nor descending sorts. A list of fields available for reporting are provided for selection. The list of \nfields is obtained from the selected data view. These fields can be selected one by one and placed on \na report line. Each field is given a column value, a length, the option to truncate or wrap if too long, \nand a format. For example, a field that returns a DateTime object may be formatted as mm/dd/yy hh:mm:ss \nor as mmm dd yyyy , etc. The format is selected from a list of valid formats for a particular field. \nThe list of valid formats is determined by the return type of the field and the return type is defined \nin the data view for this field. Now you begin to see how the data view provides the under current for \nall the different pieces of the application. Every report layout contains a body that is comprised of \na line containing the fields that will be printed across the page. This line will repeat once for each \nobject in the collection that resulted from query execution. Each field has information about what methods \nmust be executed in order to retrieve a value from the type of object making up the collection. We used \nperforms to execute these accessors at runtime. A line usually has at least ten fields and the collection \ncould be hundreds or thousands. The perform was too expensive to execute that many times. The processing \nrequired to format a report line is identical for each object in the collection so we wrote a method \non the report line that basically generated code to process a single line. This string is then compiled \ncreating an executable block that is stored in the report line object. This compilation must be done \nonce for each line and then need only be executed for each object. All information needed to process \na report line is set up during creation of the report layout, so we added hooks to the builder to compile \nthe executable blocks when the layout was saved. Having this already done at runtime saved even a little \nmore time during the report execution. The generated code receives as input the fields defined on the \nreport line which provides the receiver and the method(s) to execute to get a return value and removes \nthe need for ever using a perform, The return value for each field is added to an array and returned \nso that they can be formatted as necessary and added to the report output string. Below is an simplified \nexample of code that generates code for a precompiled block. geneode: msgArray methodString := String \nnew. methodString add: [:receiver I ; add: Character If. methodString add: IretArrayl ; add: Character \nIf. methodstring add: retArray := Array new. ; add: Character If. 1 to : msgArray size do: [ :msgArrayIndex \nI methodstring add: retArray addlast: (receiver . methodString add: (msgArray at: msgArrayIndex) asstring; \nadd: ) . methodstring add: Character If. I. methodString add: retArray] . aBlock := (methodString -compileInContext: \nnil symbollist: System myUserProfile symbollist) -executeInContext: nil. *aB lock Below is the string \nthat is generated by the above method. [:receiver I IretArray I retArray := Array new. retArray addlast: \n(receiver messagel). retArray addlast: (receiver message2). retArray addlast: (receiver messageN). retArray]. \nThe receiver (an entry from the data collection) is input to the block at runtime. This receiver is sent \neach one of the messages and the returned value is added to an array. These would be the values that \nare printed across the page for a single report line. Once the array of values has been acquired some \nof the additional formatting required includes padding between fields. We created an array with each \nentry containing a string of blanks that was equal in length to the array index and stored that in a \nclass variable. Padding then only required referencing an entry from the array and avoided a repetitive \naddition of one blank at a time. Obviously, appending to strings was another frequent operation and we \nfound the add: method on String to be faster than the + method. Our particular version of Smalltalk allows \nstrings to be dynamically sized rather than creating new objects when an existing string needs to expand. \nWhen the user defines the report layout they specify what fields to sort the data by and those fields \nare stored in a collection. The data collection is then sorted at report runtime and we found that a \nlot of time was being consumed in the sorting algorithm. We took several steps to reduce the time spent \nhere. First, we were building a sort string and a sort block at runtime. Since the sort fields cannot \nbe changed at runtime, we were able to do this processing during report definition time and store the \nresults in the report object. Second, we were converting a Set object to a SortedCollection object via \na asSortedCollection method. We found that converting a Set of -2500 entries to a SortedCollection took \nabout 17 seconds. Converting the same set to an Array object could be done in subsecond time. Lastly, \nwe were running the sort algorithm that was provided on the SortedCollection class which wasn t always \nthe most efficient for our data collections. We wrote a new sorting algorithm on the Array class that \ndetermined the best algorithm to use based on the contents of the data. We were able to cut the processing \ntime in half in many instances with this new approach. GENERAL CODING TIPS Here are some general tips \nthat can be used in any smalltalk code to improve performance. Austin, TX October F&#38;19,1995 The first \nis that object creation is an expensive operation so review code and try to remove any object creations \nthat aren t absolutely necessary. Another is that each version of a language has peculiarities that cause \none coding construct to perform faster than another. For example, iterating over a collection is faster \nusing a to:do: method over the two methods to: and do: and over iterating directly over the collections \nwith a do: method in our version of Smalltalk. Consult your vendor for the fastest ways to accomplish \ntasks that will be repetitive in your applications. Another thing is to create objects that contain only \nthe behavior actually needed. The more sophisticated behavior of objects deeper in a hierarchy also carry \nmore overhead to implement that behavior. We found we used OrderedCollections frequently when that additional \nbehavior wasn t actually required. An Array would suffice and be faster. Lastly, whenever using Kernel \nmethods we attempted to find ones that were implemented via primitives since those typically process \nfaster.  PERFORMANCE GAINS Query execution and report formatting each run 3.5 times faster after our \nperformance tuning. Our report definitions can format data faster than a hard coded equivalent. And, \n75% of the time our application is as fast or faster than the mainframe application. Since we are required \nto bring up an xterm window the mainframe application is still able to perform faster for very short \nreports.  REPORT OUTPUT DISPLAY --_------------------ ________--_---------- The users of our application \nare accustomed to 3270 terminal type of interaction and expect to see report results immediately. What \nthey actually receive in that environment is only a portion of the results and they have to request additional \npages. In the X environment, you typically complete the entire report and then display to the screen \nin a window with a scrollbar. This increased the perceived response time in the eyes of our users. In \norder to give our users sub-second response we modified the report formatting engine to display the first \npage of report output to a screen and then finish the report generation in the background. The user is \nable to review the first page of results and interact with the display screen while additional processing \nactually completes in the background.  \n\t\t\t", "proc_id": "260094", "abstract": "", "authors": [{"name": "Debbie Meadows", "author_profile_id": "81100246566", "affiliation": "Texas Instruments, Inc.", "person_id": "P64898", "email_address": "", "orcid_id": ""}, {"name": "Charles Gardner", "author_profile_id": "81100521579", "affiliation": "Texas Instruments, Inc.", "person_id": "P43437", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/260094.260226", "year": "1995", "article_id": "260226", "conference": "OOPSLA", "title": "Building and performance tuning a distributed reporting application for an OODBMS", "url": "http://dl.acm.org/citation.cfm?id=260226"}