{"article_publication_date": "10-17-1995", "fulltext": "\n Lessons From the Battlefield Thomas P. Vayda, Ph.D. in CSCI President, Vayda Consulting, Inc. 123 Jack \nRabbit Flat, Chico, CA, 95926, (916) 345-2200 vayda@ix.netcom.com 1. Abstract The pragmatic aspects \nof deploying large scale Object Oriented (00) applications are examined. The focus is on identifying \nsome of the main obstacles that arise in typical large scale 00 projects, and offering hints about effective \nsolutions. The topics are based on a number of actual large scale projects in which the author participated \nin a significant capacity and solutions that he adopted or developed to deal with the problems encountered. \n 2. Introduction I have been a practicing computer scientist since 1967. I first started using the Object \nOriented (00) approach in 1980 when simulating network protocols using Simula. The 00 approach immediately \nimpressed me as a fundamentally obvious and aesthetically pleasing approach to software development. \nAfter having delivered a number of applications and mentored a number of graduate students in producing \nprojects (and their theses) using 00 techniques, the use and deployment of 00 techniques seemed like \na straightforward and natural process. In 1990 I was asked to serve as an educator and mentor on a large \nscale (220 man year, 90 developers) industrial project. In the academic setting, we had applied 00 techniques \nwith relative ease and a great deal of success. It was a rude awakening to observe that applying the \n00 approach in the industrial setting turned out to be a battlefield strewn with landmines in a number \nof surprising areas. Since that initial project, during the last 5 years, I have played a key role in \nover 50 medium to large scale 00 projects, mostly working for Fortune 500 companies. This has allowed \nme to learn a lot about the typical problems people encounter in practical, large scale projects. This \npaper is a distillation of some of the lessons I have learned about deploying 00 in the battlefield. \nIt categorizes the most important problem areas and offers some corresponding recommendations. Rather \nthan focusing on a specific area, I have chosen an encyclopedic approach in which I try to address most \nof the common problem areas. Due to space limitation, the paper can only address these topics in an overview \nmanner, but the careful reader should be able to apply the lessons learned to reduce risk and improve \nquality in their own 00 projects.  3. Common Characteristics of Large Scale 00 Projects While each application \nis unique, many large scale applications share a number of common characteristics. The list below describes \nthe typical setting in which many new 00 projects must succeed [ 1][3]. 1. The organization s Information \nSystem (IS) has grown in a piecemeal manner over a long period of time. There are no encompassing enterprise \nmodels to explain how the various applications, data bases and platforms, work together to provide the \nIS solution. 2. The requirements are developed by a number of groups with different understanding of \nand perspectives on the problem to be solved.  3. The application must integrate with (or at least \nprovide interfaces to) a number of other applications and data bases and the existing set of applications \nis not well documented. 4. A platform change is part of the requirements. Typical migrations include \nmoving from mainframe to distributed client/server, from terminal to workstation and from file structures \nto relational data bases.  5. Data relationships are complex and the data bases are large. 6. High \nperformance transaction processing, tight security, robust exception handling, 7 by 24 operation, audit \ntrails, complex reports and other difficult to meet requirements must be achieved. 7. Friendly user \ninterfaces and interface navigation models must be provided. 8. The solutions must be scaleable, understandable, \nextensible and provide opportunities for reuse. 9. There is a lot of inertia; some people feel threatened \nby or unsure about new technologies and want to resist change. 10. Political issues poison the organizations \nability to work on integrated solutions that must bring together the work of several groups. In addition, \nsenior management often does not provide true support (though often pays lip service) for developing \nnew, effective solutions and adopting new technologies.  Often, organizations seek out 00 solutions \nwith the hope that they will be the silver bullet which will restore order in such difficult settings. \nAny approach would have a difficult time succeeding with such a set of demanding circumstances and requirements. \nIt is easy to see why many 00 projects are doomed by such a set of challenging factors before they even \nhave a chance to get off the ground. This paper provides some guidelines to help succeed in such situations. \nThe following sections roughly parallel the phases of a typical waterfall software development life cycle. \n 4. Requirements Without quality requirements a project is doomed. Requirements typically suffer in three \nmajor areas; I) the ability to meet the users true needs, ii) clarity, conciseness, consistency, implemen-tability \nand other similar issues. and iii) stability often due to the fact that projects take too long to deliver \nand the requirements gathering phase is distributed over the life of the project. Thus requirements become \na moving target that you can never reach. We recommend the following approaches for producing stable, \nhigh quality requirements in a reasonable amount of time. 1. Meet the customer s needs -To ensure that \ncustomers needs are met, use joint application development (JAD) style meetings that involve users, developers, \nand management, early in the project. The results of the JAD meetings should be a set of desired features \nand a specification of the inputs and outputs, performance, etc. Follow the JAD meetings with some rational \nmethod such as quality function deployment (QFD) to ensure that the voice of the customer is heard loudly \nthroughout the development life cycle. The result of these steps should be an informally stated set of \nfeatures that the application must deliver. Developing prototypes, mockups or storyboards as part of \nthe requirements gathering phase is very helpful. The customer must sign off on this document before \nfurther work begins. 2. Produce high quality requirements -To produce clear, consistent, implementable \nrequirements, the informally stated requirements should be converted into a solid, clear, consistent \nand implementable requirements model. Current successful techniques includes preparing (at least a portion \nof) a use case model as well as a domain object model [7]. The official requirements may include non-traditional \ncomponents such as user interface prototypes, storyboards and similar items. The dividing line between \nrequirements modeling and early analysis is often hard to distinguish, but this causes no real problem. \nThe goal is not to define dividing lines, but rather to produce a successful system in a seamless and \nreversible manner. 3. Aim for stable requirements -Ensuring the stability of the requirements dictates \nthe use of a well-defined development process. There should be a lifecycle defined for each work product, \nincluding the system requirements. Such a lifecycle typically has phases for discovery, creation, baselining \nand then a rigid change management procedure, which defines what steps, approvals, agreements, signoffs \nand distributions must accompany a change to the work product. It is true the Object Technology is capable \nof supporting late changes in requirements, but this does not negate the need to bring stability to the \nrequirements as early as possible, and then allow changes only via a well defined and controlled set \nof mechanisms. While the steps outlined above do not involve difficult concepts, their importance to \nthe success of a project cannot be overemphasized. In my experience the most frequent source of project \nfailures is due to problems with the requirements or scheduling (a related topic). Using these guidelines \nis a necessary pre-requisite to (but cannot alone ensure) success.  5. Architecture Issues Failing to \nadequately address architecture issues can lead to well-formed, correct and aesthetically pleasing solutions \nthat result in worthless systems. I have witnessed a reengineering project in which an existing mainframe \nsystem was migrated to 00 client/server platform. The migration yielded an extensible, well documented \nsystem whose performance went from over 100 transactions/second on the mainframe to less than 5 transactions/second \non the new platform. The result was an unacceptable system which essentially had to be scrapped. The \nproblem was not with using 00, or migrating to client/server, but instead, in not paying enough attention \nto architecture issues. The architecture should address both the logical application architecture and \nthe physical system architecture. The logical application architecture depicts the decomposition of \na system into subsystems and clusters, their relationships as layers and vertical partitions and their \nlarge scale interactions. Often these aspects dictate the packaging into dynamic link libraries, OLE \nservers and other similar compilation units. The logical database schema is also addressed. The physical \narchitecture addresses the decomposition of the system into processes allocated to various processors, \nthe physical database and communication topology and protocols. For example, a single object from the \nanalysis model may become a persistent object which begins life on one processor, and after performing \nseveral steps in its dynamic model goes to sleep (on some storage device), then reawakens to live the \nrest of its life on another processor. The main areas of concern during architecture specification include: \n1. The need for performance modeling -As part of the requirements gathering phase, the size of the databases \nand the number of transactions/second for each major type of transaction, the input sources, outputs \nand other performance requirements must be specified. These requirements allow the construction of performance \nmodels or even stubbed out test programs that verify the feasibility of the logical and physical architecture \nto meet the requirements. For instance, I have seen projects select a database system based on its published \nfeatures and performance, and later realize that in their specific application, the advertised performance \nwas optimistic by an order of magnitude, thus rendering the system incapable of meeting the requirements. \nIt is essential to conduct in-depth performance studies of the proposed architecture on the target platform. \n 2. Client/Server and data base issues -Many organizations moving from mainframe to client/server architectures \ntypically do not know enough about client/server technology to make intelligent choices [l]. They should \nbe able to adequately answer questions such as: Should we use a two, three or more tier basic architecture? \n, How does model/view/controller architecture impact my project? , Should we use an Object or Relational \ndatabase? , Do I need middleware, and if yes, what kind? , What should we use for an ORB? , What kind \nof communication architecture should we use? If these types of questions seem difficult, I highly recommend \nthe use of some experienced consultants (internal or external) to deal with these fundamental issues? \n Getting the architecture right is an absolute pre-requisite to success. Seemingly perfect solutions \ncan be result in total failures without solid proof that the architecture will work. It is not necessary \nto develop the architecture prior doing analysis and design; these activities can be performed in parallel, \nsince most analysis/design models can be mapped onto a variety of architectures. 6. Analysis Modeling \nIssues The role of Analysis Models is to provide a high level, implementation independent description \nof the main elements of the target system [2][4][5][6][7]. The steps required to produce 00 Analysis \nModels are well understood and documented in the literature. Each of the popular analysis methodologies \n(e.g., OMT, Booth, Shlaer/Mellor, etc.) prescribes a notation and a set of models used to describe the \ntarget system. Still, many projects run into major problems during the well understood analysis phase. \nIn this section, some of the undocumented analysis modeling showstoppers are discussed. 1. Failure to \nisolate analysis and design activities -One of my most difficult hurdles in teaching developers to use \n00 is to get their buy-in to the notion that analysis and design are separate and individually worthwhile \nactivities. The analysis models describe the system (and reformulate the requirements) at a high level \nand in an implementation free manner. The classes in the analysis model should come from the problem \ndomain. They should be understandable to domain experts who need not be computer science literate, but \nwho have had training in the basic notions of Object Technology and the analysis notation used. Developers \noften want to include computer solution concepts into the analysis models at an early stage. A separate \nanalysis model has a number of advantages, including; the ability to discuss the system issues at a high \nlevel without being bogged down by implementation issues, the ability to verify the models with the customers, \nmanagement and other non-technical personnel, allowing impact of requirements changes to be easily analyzed \nand for the new requirements to be accommodated, and providing a platform independent system description \nwhich can be used as the basis for migration to other platforms. I recommend that a separate analysis \nphase and its products be made an official part of the software development lifecycle. 2. Misuse of \n00 Analysis modeling concepts -Probably the single largest problem is that people new to Object Modeling \nprepare object oriented models of procedural systems. This is due to the fact that they are new to 00 \nand have not had a chance to overcome the learning curve and become experienced in true object thinking \n(which typically takes a talented analyst 6-12 months to master) [4]. Typical modeling errors include; \nanalysis classes do not represent concepts from the problem domain, classes have too much coupling and \nnot enough cohesion, inheritance is misused (does not meet the IS-A criterion), inheritance hierarchies \nbecome too deep. everything is derived from a single base class object , there is too much focus on existing \ndata bases and the models are data driven rather than being well structured classes, dynamic interactions \nbetween classes come as an afterthought because of too much focus on structural modeling (static object \nmodels), the models become unnecessarily complex due to overuse of relationships between classes, unnecessary \nuse of multiple inheritance, recursive aggregation and other advanced concepts, focusing too early on \nimplementation concepts (e.g., hash tables) and other similar issues. The best way to deal with this \narea is to ensure the availability of experienced modelers who can help the modeling team overcome these \ntypes of pitfalls. 3. Model granularity and scaleability issues -Most current analysis methodologies \npay a great deal of attention to describing the properties of class/object models, while they provide \nlittle support for providing larger scale bird s eyeviews of a system. I have found overview and architectural \ndescriptions to be extremely important in describing large scale systems and have had to invent my own \nnotation. The large scale views are needed to describe how subsystems fit together, to help decide on \narchitectural and packaging issues, to analyze the impact of platform changes and other similar global \nscope issues. In large systems, I have found it useful to provide more than one level of granularity \n(e.g., subapplication, subsystem, cluster, etc.) before descending to the object modeling level covered \nby the popular methodologies. I recommend that an organization realize the importance of providing these \nhigher level system views and prescribe some notation for describing them. 4. Connecting and ensuring \nthe consistency of multiple modeling views -Most methodologies provide some structural (static) and behavioral \n(dynamic) views of a system. Typical static models include the object model and data descriptions while \ndynamic models include event trace diagrams, state diagrams, and data flows. While it is universally \nagreed that multiple views are needed to describe/understand systems, most methodologies do not provide \nmany clues on how to relate these multiple views, much less means to ensure their consistency. Since \nall the models must eventually result in an implementation, they must finally join a that level [5]. \nI recommend that an organization develop some notation and practices that ensures that the various views \nare related and consistent. One simple device is to prefix each attribute and operation in any of the \nviews with the class to which they belong. Some CASE tools (such as Paradigm Plus) provide some automated \nmeans for consistency checking among the various models. For example, in an event trace diagram, it ensures \nthat all messages sent to a class have been defined for that class. 5. Failure to use proper tools -There \nare a number of modeling tools, which provide forward and reverse engineering, drawing and maintaining \nthe models, repositories, code generation and other similar extremely productive features. Such tools \nshould be carefully evaluated and adopted to help the development process during all phases. 6. When \nto stop modeling? -This simple point has caused serious problems for many projects. Analysts new to OT \noften feel that they must have a perfect analysis model before proceeding on to design and implementation. \nThis ailment has been referred to as analysis paralysis . Portions of early analysis models will be wrong \nwhile some portions will be mature enough for implementation. Accepting these facts enables projects \nto have overlapped, parallel, iterative development life cycles, in which analysis, design, implementation \nand testing can be carried out simultaneously. This fountain model of development allows knowledge gained \nduring later lifecyle stages to impact the products of earlier lifecycle stages (e.g., an analysis error \nis often discovered during testing).  My advice is to perform analysis while good progress is being \nmade but proceed into the subsequent development phases as soon as possible, realizing that the analysis \nmodels will be revisited and revised during the whole development cycle (although hopefully with decreasing \nfrequency as a function of time). The most important lesson is to insist on a distinct analysis phase \nand to keep the analysis work products relatively free of implementation aspects, This combined with \nthe other points discussed above should allow a project to get the most fundamental development product, \nthe analysis model, into good shape. 7. Design Hurdles The design model takes the analysis model as \ninput and adds the details needed to produce an implementation that meets the specifications [2][4][5][6][7]. \nThe design models can often be expressed with the same (or extension of the) notation used for the analysis \nmodels. Most 00 methodologies have adequate models and procedures for performing analysis. Then they \ndo some handwaving and talk about the next step being to produce design models, which should be detailed \nenough to serve as blueprints for implementation , but never really explain how to perform this step. \nThe means used to transform analysis models to design models is the black hole in most extant methodologies. \nThis section discusses some of the ways to bridge this gap. 1. The main design activities -These include \ndeveloping a logical and physical architecture (addressed above), designing interfaces between software \ncomponents (e.g., subsystems, layers and clusters), designing translations (often called wrappers) between \nexternal and legacy systems (e.g., databases and communications) and the object model, deciding on data \nstructures and algorithms to carry out the work specified in the analysis models, designing state machines, \nspecifying reuse from external and internal libraries, designing the model/view/controller approach to \nbe used and mapping the specified solutions onto the logical and physical architecture. 2. Design guidelines \n-An organization should borrow and/or develop a set of design guidelines to be used by the system designers. \nThe guidelines should contain heuristics for the major software engineering issues, including; coupling, \ncohesion, modularization, inheritance (single and multiple), aggregation, association, interface design, \ndefault values (attribute and parameter), canonical classes, reference counting, recompilation avoidance \n(e.g., Cheshire cat classes), communication styles (asynchronous and synchronous), database design (e.g., \nnormalization) and other similar issues. Such guidelines assist designers with the tough decisions and \nhelp to ensure some uniformity in the software product. The consistent use of guidelines has obvious \nbenefits for maintainability and extension. 3. Design Steps -For each logical relationship in the analysis \nmodel, some container classes are needed to store the relationships and they should be chosen for performance \nand space issues, preferably from available libraries. For each persistent class, the physical data base \nschema needs to support the class and the design needs to address how and when the database needs to \nbe updated. Each analysis class needs to be mapped on to the architecture process model, including creation, \nmoving to a dormant state, waking up and destruction. Each design class should be examined for potential \nreuse by either aggregation or inheritance from the existing set of in-house and externally produced \n(commercial) library classes. A GUI framework which provides good support for the model/view/controller \napproach on the target platforms should be selected. The state machines defined during dynamic modeling \ncan be implemented using a variety of stylized techniques; automatic generation via tools (e.g., lex, \nyacc) or at least a standardized table driven approach should be considered for the more complex ones. \nInterfaces should be designed by  using a specification language such as the CORE3A IDL. Algorithms \nand data structures usually come in matched sets and should be selected using standard knowledge available \nfrom data structure and algorithm analysis. These matched sets are usually implemented in off-the-shelf \nlibraries. 4. Produce precise specifications -All class and interface properties should be precisely \nspecified. One extremely successful approach, called design by contracts recommends specifjling all interactions \nbetween objects in terms of a legal contract . This is achieved by specifying for each operation a set \nof preconditions that must be satisfied before the operation is invoked and a set of postconditions that \nare guaranteed to hold after the operation completes (assuming that the preconditions were satisfied \nprior to invocation). In addition, for each class, a set of true at all times for should be specified. \nfurther constraints Using this approach ensuring consistent, correct interfaces. 5. Adopt safe design \ninvariants, which must be every instance of the class The invariants specify on every class operation. \ngoes a long way towards well documented and  techniques -There are approaches to design that focus on \nproducing correct and robust systems. These include the notions of design by contract, the use of class \ninvariants, and the specification of operations by pre and post conditions, as well as solid ways to \ndeal with exceptions. Learn about these approaches and include safety as a key aspect of your designs. \n6. Reviews and Inspections -The design models should be reviewed by experts from a variety of specialty \nperspectives, including architecture, database, communication, algorithms and GUI. This simple (yet often \nomitted) step can save a tremendous amount of effort, when compared to discovering design errors during \nimplementation or testing. There are a number of well-proven inspection methods available, many of them \nbased on Fagin consistently use one It has been learned over from analysis to decisions) is the quality \nsystems. great advantage, performed with difficult and will most likely be inspected.  The specifics \nof good design principles is outside the scope of this paper, but is well known to many experienced designers \nand consultants. It is highly recommended that an organization with little experience with successful \n00 design enlist some expertise to assist with this crucial step. Finally it should be noted that design \nwill continue over most of the life of a project, especially when an iterative and overlapped lifecycle \nmodel is used.  8. Implementation Lessons The translation from design to implementation is relatively \nstraightforward for experienced practitioners. The main issues in this phase include [91: 1. Knowing \nlanguage not sufficient -Many and books explain language, program knowledge program design most Simply \nrecognizing this can be a because then this step will be care, it will be anticipated as so that the \ncould be inspections; choose and of them. and over, that the translation (including the architectural \ndifficult step in producing syntax and semantics is language training courses all features of an 00 \nbehavior of an arbitrary perfectly predicted. This is not sufficient to write a quality 00 with the desirable \nattributes of robustness, clarity, extensibility, reusability. It is important to know how to map design \ndecisions into properly structured 00 programs. For example, C++ allows both virtual and non-virtual \nfunctions to be redefined in derived classes. Knowledgeable practitioners know that a function that represents \nan invariant over specialization (i.e., should remain immutable throughout its inheritance hierarchy) \nshould be implemented as a non-virtual concrete function and they also know that such a function should \nnever be redefined in a derived class. Similar statements could be made regarding default parameter \nvalues, redeclaring attributes, etc. 2. The cost of various language features should be understood -Languages \noffer a variety of features, whose pros and cons are often not well understood. Consider the following \ntwo C++ examples. Many think that a call to virtual function is expensive because it is resolved at runtime, \nbut in fact, in most implementations, the cost is only one extra level of indirect addressing (an insignificant \nperformance hit). The real cost of using virtual functions is that they cannot later be inlined and that \neach object requires extra memory for the address of the virtual function table, which could be significant \nif there are millions of small objects. Another example is that many people are unaware that the use \nof templates causes a surprising expansion in code size due to the fact that each template instantiation \ncompiles to a separate set of machine code. If space is at a premium, this could be optimized using other \napproaches. 3. Polymorphism issues -There are a number of issues regarding polymorphism that are not \nwell known, yet arise on a regular basis. One typical example is described below. Suppose that a base \nclass B includes a polymorphic function foo(), which is redefined in the derived class D. One typically \nexpects to use instances of D via pointers of type B, knowing that when the pointer actually points at \nan instance of D, the proper version of foo() will be invoked. Now suppose that D introduces a new function \ngoo() which does not exist in B. How does one invoke goo() through a pointer to B? This example arises \nall the time and its solutions are not well known. The basic choices in C++ are: i) to define goo() in \nB (which is not great since B does not need goo; this is known as the fat interface solution) or ii) \nto use downcasting as follows: define and use some sort of type system or use run time type identification \nto determine that the B pointer actually points to a D, and to cast a B pointer to a D pointer which \ncan then be used to call goo() (not great since it defeats the typing system) or iii) to not need to \ncall goo() through a pointer to B, by having done a proper design and avoiding the issue altogether (the \nright answer). Details like this are encountered frequently, yet each organization has to solve the problem \nand try to disseminate to the total development organization in some manner (see programming guidelines \nbelow) Other polymorphism specific examples include; how to handle polymorphic types stored in multiple \ncontainers, how to document the behavior of polymorphic inheritance hierarchies, knowing which functions \nshould be designed as polymorphic and what to do when you change your mind. 4. Know your compiler -Since \nmost 00 languages are too immature to have guaranteed standards, and languages do not yet have test suites \nsuch as exist for Ada (although the Plum/Hall suite can be helpful in this area), each compiler will \nimplement various language features with varying degrees of efficiency (both space and time) and in some \ncases, even correctness. (For instance, many C++ compilers either do not support templates or do so erroneously.) \nEven when standards become available and supported, compiler writers will still have a lot of latitude \nin implementing specific features. Before using any language feature extensively, verify that it works \ncorrectly, and learn about the cost of using that feature by compiling and running small focused test \nprogram. 5. Do not depend on compiler specific features -Using compiler dependent features is another \ndanger area. For instance, the C++ draft standard does not specify the order of initialization for derived \nclasses, and multiple inheritance. A number of compilers initialize in the order specified in the declaration \ninitializer list. Programmers depending on this order to be used across all implementations will be disappointed \nwhen they switch compilers. Depending on compiler specific issues can cause a number of time consuming \nbugs. Thus, avoid compiler dependent features as much as possible. 6. Learn advanced features and techniques \n-The ANSI draft C++ standard specifies a number of advanced C++ features, many of which are not yet currently \navailable, but whose use would significantly improve programs. Some of these features include exception \nhandling, run-time type identification (RTTI) and name spaces. Learn about these features, and design \nprograms as if these features were available, since they soon will be. For example RTTI greatly aids \nwith the proper design of heterogeneous containers and type casting. Exception handling can lead to more \nrobust programs, especially when constructing reusable libraries. Name spaces will support the easier \nintegration of reusable libraries. Read about the upcoming features and their proper use and plan to \nuse them in your programs. 7. Learn about and avoid error prone areas - There are a number of error \nprone areas, including memory management, copy construction, the fragile base class problem, typing and \nautomatic type conversions and similar issues. Developers should learn about the potential problems and \ndevelop guidelines for avoiding them. 8. Adopt safe programming techniques -Your designs should already \nbe safety oriented. Similarly, write your programs safely by including assertions for class invariants \nand function pre and post conditions as well as including scaffolding that will make it easier to find \nfaults, should they occur. Some languages such as Eiffel have these concepts directly supported by the \nlanguage, whereas in C++ and Smalltalk, you will have to use well known techniques for simulating such \nconstructs. 9. Develop language specific programming guidelines -Capture all the knowledge your organization \nhas in a set of programming guidelines which are maintained and extended as new knowledge is gained. \nUse inspections to ensure that people understand and are using the guidelines appropriately. There are \na number of programming techniques that go well beyond simply knowing the syntax and semantics of a given \nlanguage. Developers should be trained in how to properly translate design concepts to programs, to use \nadvanced features and idioms. Programming standards should capture most of these ideas and inspections \nshould ensure their consistent use.   9. Reuse Issues; Library Develop-ment and Integration Reuse has \nbeen claimed to be one of the major advantages to be gained from adopting OOT. In the years since this \nhas been espoused, much practical experience has been gained and the problem of reuse is now known to \nbe much more difficult than originally anticipated. On the positive side, much headway has been made \non how to make large scale reuse a reality. In this section, the main hurdles to reuse are discussed \nand some solutions are suggested. The barriers to effective reuse come from two major areas; technical \nand organizational. The main technical barriers include the difficulty of producing truly general, widely \nreusable components, documenting and distributing the components, finding the right components, handling \nchanges to the libraries and handling namespace conflicts. The organizational barriers include the difficulty \nof transforming a typical software development organization into one whose mindset includes reuse as \na key factor, developing resources to champion and facilitate reuse, developing reward systems that encourage \nthe production and use of reusable components and disseminating information about modified and new components. \n 1. The difficulty of producing truly general, widely reusable components -Biggerstaff s rule of reuse \nstates that it takes a least three tries to produce a good reusable component and that a component must \nbe reused at least three times to demonstrate its viability. Don t expect much reuse from early efforts \nin 00 but start planning for establishing the needed infrastructure as soon as possible. 2. Documenting \nand distributing the components -Documenting reusable software requires extra effort. Establish guidelines \nand keep documentation current. There must be procedures for distributing well documented components \nto all relevant parties. A current trend is to use a publish/subscribe paradigm on a system which supports \ndissemination, such as a corporate network and Netscape. 3. Finding the right components -Start by reusing \noff-the-shelf successful libraries in vanilla areas such as GUI, container, persistence and communication \nclasses. Then add an extra step to your software development lifecycle called generalization in which \nthe developers elect library candidates and work to make them general and reusable. 4. Handling changes \nto the libraries -Change management becomes extra important when software components span multiple applications. \nAgain some form of publish/subscribe approach should be used, in addition to using version control and \nconfiguration management tools. Additionally, some programming techniques that avoid recompiling applications \nthat use changed library components should be applied (e.g., the Cheshire class approach). 5. Namespace \nconflicts -These often arise when integrating libraries from multiple vendors. For example, almost all \nlibraries include their own version of a string class. Investigate this potential source of conflict \nbefore committing to  library choices. A corporate repository for namespace control can help here. For \nC++ environments, the namespace feature of recent version of C++ should help significantly. Plan for \nits arrival. The organizational barriers include: 6. Transforming a typical software development organization \ninto one whose mindset includes reuse as a key factor -This is an unexpectedly difficult problem. You \nshould not expect to overcome this hurdle in the short term. It requires support from the senior management \nand a long term plan. Organizational change management experts can be very helpful in setting up an effective \nprogram. This program could be combined with other change efforts, including software development process \nimprovement and the general introduction of the use of OOT. 7. Developing resources to champion and \nfacilitate reuse -Recognize that reuse won t happen without a facilitator, whose full time job is to \nintroduce and support the reuse effort. Eventually a reuse group, whose job is to maintain, support, \nenhance and disseminate the corporate software assets will need to be funded and staffed with some of \nthe organization s most experienced 00 personnel. 8. Developing reward systems that encourage the production \nand use of reusable components -Most people don t change without some incentives. Since it takes three \nto ten times longer to produce library quality classes than ones for a given single application, people \nmust be rewarded for putting in the extra effort required to produce them. Similar arguments could be \nmade for encouraging people to actively seek out and use library components.  For example, it has been \nreported that the following inexpensive and simple scheme has paid for itself many times over; for each \nline of code that is reused, its developer gets paid a nickel and the reuser gets a penny. The company \nwhere this approach was used spent about $60K and saved $430K in the first year. 9. Disseminating information \nabout modified and new components -This requires the establishment of an organization whose job is to \nensure that this gets done. Additionally, it requires support from some repository tools. In summary, \nwhile reuse is one of the great potential benefits of OOT, it has proven extremely difficult to achieve \nand needs a concerted and multi-faceted approach to overcome the technical and organizational barriers. \n10. Dealing with Legacy Systems Most adopters of OOT face the difficult task of how to effectively manage \nthe migration from legacy non 00 systems to new 00 systems. All organizations facing this quandary must \ncome up with an effective transition plan. This problem can only be avoided by organizations that are \nstarting from scratch and using OOT from the start. It is a fact that for most organizations, legacy \nsystems will be around for a long time, and in many cases they should be kept around (e.g., when they \nare doing a good job in a cost-effective manner). The main solution approaches to dealing with legacy \nsystems are outlined below. 1. Incremental reengineering -Some legacy systems are not worth keeping. \nSuch systems should be reengineered, using OOT. The selection of reengineering targets should be based \non two main criteria [8]. First, is the software cost-effective? This should be determined using a rational \napproach based on metrics that indicate the error rate, and other costs of maintaining the software, \nas well as the cost to rebuild it. Based on this type of data, rational cost/benefit analysis can be \nperformed. Second, a reengineering target should be a fairly isolated set of software, which has relatively \nlow (or at least very clearly defined) coupling to its environment. In addition, resources and expertise \nfor performing the reengineering task must be available. 2. Database conversion -The heart of many legacy \nsystems is the underlying database. These databases are often based on old proprietary software or are \nhome-grown additions to a standard file system. In such cases, dealing with the legacy system boils down \nto creating conversion software which allows the 00 system to access information from the legacy databases \nand to view the legacy database as an object oriented database system (ODBMS). The legacy system programmers \nare invaluable for developing such data base conversions and access mechanisms. 3. Wrapper construction \n-This is an extremely effective way of dealing with legacy system. Very briefly, building a wrapper involves \nthe following steps:  i) developing a solid requirements model for the legacy system (which may be difficult, \nas many find out that they are not exactly sure of what their legacy system does), ii) designing a good \n00 solution for the requirements (while neglecting knowledge about the internals of the legacy system) \nand iii) then implementing the operations of the 00 system by making calls to functions of the legacy \nsystem. This easy to describe solution can of course be quite labor intensive, but it brings many advantages. \nWrappers support incremental reengineering and platform independence. 4. Migration Strategies -These \nare not particularly new when migrating to 00. Using standard migration strategies, one typically runs \nparallel systems until the new 00 system is proven capable and then the legacy system is phased out. \n Dealing with legacy systems is a well studied problem, since almost every organization with a large \ninvestment in software has to do it. The combination of incremental reengineering with wrappers is particularly \nattractive since it almost automatically provides a good migration strategy and allows a slow, well-planned \nmigration to 00 by systematically replacing the wrapper s calls to legacy code by calls to newly implemented \nobjects as they become available. 11. Development Process Issues It is well known that the introduction \nof new technology, such as Object Oriented Technology (00-O, can be counter-productive, unless the organization \nhas a solid, repeatable software development process (SDP) in place (or is planning to introduce one \nalong with the new technology) [3]. The Software Engineering Institute (SEI) has quantitative evidence \nto show the veracity of this statement. They have developed means to assess the development process, \nhave cataloged the main features of a quality development process and some guidelines on how to improve \nthe process. Some of the issues include version control, configuration management, metrics, inspections, \nrequirements procedures, change management, quality control, managing of customer/supplier relationships \nand other similar issues. It is clear that object technology, especially when reuse is an intended byproduct, \nrequires a number of these development process issues to be handled extremely well. Specifically, process \nissues of primal importance for OOT include the following areas: i) a consistent use of analysis, design \nas well as the proper documentation of the work products developed during these phases, ii) the change \nmanagement of requirements, and reusable components, iii) the traceability from requirements to code \nthroughout the complete development cycle spanning multiple releases. SDP improvement usually requires \npersonnel dedicated to this task. The champions of process improvement typically form a Software Engineering \nProcess Group (SEPG) consisting of representatives from a number of projects. The SEPG s main role is \nto ensure that the process improvement effort suits everyone s needs and to gain buy-in from many quarters. \nThe SEPG is then responsible for assessing the SDP(s) in use, prioritizing improvement goals for short, \nmedium and long range, forming implementation plans and monitoring to ensure that the plans are carried \nout. I strongly recommend that along with adopting OOT, an organization also embark on a process improvement \nprogram (if they don t already have one in place). 12. The Cost of Change and The Learning Curve This \nis one of the easiest to remedy, yet most problematic area for organizations migrating to OOT. Management \ntypically underestimates the cost (in time and resources) for making organizational and process changes \nand the learning curve required to successfully adopt OOT. The sweeping changes required typically include \nevaluating, buying and incorporating new tools (software and hardware), establishing and maintaining \nnew development processes, documentation approaches, development lifecycles and institutionalizing the \nnew ways of developing software. The learning curve is required for educating the development organization \nto use OOT, including new tools, practices and processes, as well as providing the time to master the \nnew way of doing business. Industry statistics show that it takes an experienced procedural developer \nsix months to a year to become an experienced object thinker . Management must realize that moving to \nOOT is a long term process with some fixed startup costs that cannot be avoided. Avoiding doing things \nthe right way from the start invariably ends up costing a lot more in the long run. Project schedules \nmust take into account the learning curve. Failure to do so usually results in disaster, either from \nhuge schedule slips or poor quality systems that are produced by people not having adequate skills, or \nboth. One common training error is to provide training long before it will be used. Experience indicates \nthat training offered more than about two months prior to its intended use will have to be repeated. \nThus, plan your training to be offered in a just-in-time manner. For example, on a given project, I prefer \nto provide training in 00 analysis just as the analysis phase is about to begin, then immediately follow \nthe training by a hands-on mentoring session where a basic analysis model is developed. Next, I return \nto offer design training and mentoring once the analysis model is nearing completion and similarly for \nthe implementation phase. In summary, it takes time and effort to change. Unless the change is managed \nproperly, with sufficient time and resources allocated for the fixed one time cost of overcoming the \nlearning curve, the results will be unsatisfactory and costly in the long run. The support for managing \nchange and overcoming the learning curve in a proactive manner must come from the highest levels in the \norganization, since they will have to approve funding and other support to continue the program over \nan extended period of time. 13. Project Management Issues There are two kinds of management skills; technical \nand organizational. Of the two, the latter is more important, because a manager with good organizational \nskills will enlist the technical talent required to ensure the success of a project. Since many of the \ntechnical management issues have already been covered, in this section we focus on the organizational \nmanagement issues required for success in 00 projects [3]. 1. Commitment from the top -Without the sanction \nand full hearted support of senior management, a project is doomed from the start. This requires that \nthe senior management be trained in the 00 development process so that they will have realistic expectations \n(e.g., regarding the learning curve, scheduling, work products, resource needs, etc.). Based on this \nknowledge their commitment must be more than lip service. When the development team needs support, senior \nmanagement must be behind them even in the face of schedule slips, allowing sufficient time for analysis \nand design before seeing code, providing the time and resources for training and tools and other issues \ninvolving the allocation of resources. This does not imply that an 00 team be given the world, but that \nall reasonable requests be considered fairly, realistically with long term goals in mind and decisions \nbe made on an educated and rational basis. Too often, when the going gets rough, senior management reverts \nto the old style way of doing business. 2. Choose the right team -Many people oppose change, while a \nfew welcome it. An 00 project team must consist of motivated, bright individuals, who value change and \nchallenges and strongly request to work on the project. It is more important for the team members to \nhave the right attitude than to possess the required technical skills (although both would obviously \nbe the best). Technical skills can always be learned, but the right attitude can never be forced. 3. \nAssure team buy-in -All the team members must have bought-in to the project, and the processes and tools \nto be used. All new decisions should involve all members of the team. While not all team members have \nto agree with the decision, the whole team should reach consensus before proceeding. Continually involving \nall team members will help to ensure continued buy-in to the project. The importance of team buy-in cannot \nbe overemphasized. 4. Choose the right project -This is another extremely important choice. Some of \nthe main criteria for choosing early 00 projects are listed next. The project should have high visibility, \nlenient schedule, long projected life with a number of subsequent planned releases, good potential for \nproducing reusable objects, its development mostly involves new code. use a GUI, the availability of \nthe right team and resources, commitment from the top and it should not be extremely mission critical. \nWhile have participated in projects that violated most of the criteria and still succeeded, these criteria \n  should be used to evaluate the potential choices for suitability as an 00 pilot project. 5. Allow \nfor the learning curve -We have seen that project personnel have to deal with new analysis/design methods, \nlanguages, processes tools and environment and each of these requires training and experience (or at \nleast time to absorb and use). The needed time and resources must be part of the project plan, with some \nslack for the typical underestimation. 6. Ensure a quality process -Great technology can be rendered \nuseless without a quality development process in place. Many feel that more can be gained from improving \nthe development process while using traditional approaches than from switching to OOT without an adequate \ndevelopment process in place (or being introduced simultaneously with OOT). Of course the best is to \ncombine OOT with a good development process. 7. Learn from the experience of others -OOT is no longer \nbleeding edge. Many have walked down this path with great success. Learn from their lessons and engage \noutside talent with a proven track record to assist you with your early projects. The resources spent \non skillful outside trainers, mentors and consultants is worth its weight in gold. On the other hand, \nhandholding (or essentially outsourcing) your early 00 projects is not recommended because developing \nin-house expertise is one of the most important byproducts of doing an 00 project. In addition, there \nis a great deal of literature available to address most of the issues discussed. Consult the best books \nand periodicals to learn the best techniques and avoid the mistakes of others.  8. Enjoy your success \n-Follow these rules, succeed on your project and know that you have helped to ensure your organization \ns long term success and as a result, you will be valuable commodity for years to come.  14. Summary \nWe have covered the highlights of the most important lessons learned from rolling out a number of successful \n00 projects and witnessing some unsuccessful ones. In-depth coverage of all the issues discussed would \nrequire a book, but if these issues are properly handled, your 00 project should be a successful one. \nFor further information about any of the topics, either consult the large number of reference materials \navailable to the 00 community, or contact the author. Good luck in your 00 project. 15. References [l] \nClient/Server Technology for Managers, Karen Watterson, Addison Wesley, 1995 [2] Designing Object Oriented \nSoftware, Rebecca Wirfs-Brock et al, Prentice Hall, 1990 [3] Managing the Software Process, Watts Humphrey, \nAddison Wesley, 1989 [4] Object Oriented Analysis and Design with Application, 2nd Ed., Grady Booth, \nBenjamin Cummings, 1994 [5] Object Oriented Modeling and Design, James Rumbaugh et al., Prentice Hall, \n1991 [6] Object Oriented Software Construction, Bertrand Meyer, Prentice Hall, 1988 [7] Object Oriented \nSoftware Engineering, A Use-Case Driven Approach, Ivar Jacobson et al., Addison Wesley, 1992 [8] Re-engineering \nof Old Systems to an Object Oriented Architecture, Ivar Jacobson and F. Lindstrom, ACM, Proceedings of \nOOPSLA, 199 1 [9] The C++ Programming Language, 2nd Ed., Bjame Stroustrop, Addison Wesley, 1991  \n\t\t\t", "proc_id": "217838", "abstract": "The pragmatic aspects of deploying large scale Object Oriented (OO) applications are examined. The focus is on identifying some of the main obstacles that arise in typical large scale OO projects, and offering hints about effective solutions. This The topics are based on a number of actual large scale projects in which the author participated in a It significant capacity and solutions that he adopted or developed to deal with the problems encountered.", "authors": [{"name": "Thomas P. Vayda", "author_profile_id": "81547712856", "affiliation": "President, Vayda Consulting, Inc., 123 Jack Rabbit Flat, Chico, CA,", "person_id": "PP308766000", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/217838.217881", "year": "1995", "article_id": "217881", "conference": "OOPSLA", "title": "Lessons from the battlefield", "url": "http://dl.acm.org/citation.cfm?id=217881"}