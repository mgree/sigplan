{"article_publication_date": "10-01-1995", "fulltext": "\n Performance of a OODB in a Online 7x24~365 Manufacturing Operation. Charles Texas Instruments, Inc. \nAbstract. A online reporting application which is used to report and schedule material on factory floors \nis described in terms of the realtime updates and scheduled maintenance. Performance issues for these \nparts of the system are described by the various parts. The online feeds are described in terms of tuning \neach part according to function. DBA type functions such as clustering of data and backups are described. \nThe best tuning came from breaking the functions into definable parts which could then be tuned according \nto need. Factors which affected each part of the system are described.  Introduction. This application \nis an online reporting application which supports manufacturing processes and scheduling of material \nand general reporting of inventory. The focus of this report is on the parts of the system which must \nrun 7x24~365 to keep the dbf up-to-date with manufacturing processes. The performance of these processes \naffects the reliability of the reports being produced from the unix based system. Routine maintenance \nsuch as garbage collection, backups and re- clustering of critical data must be performed while the OODB \nis running and must not. adversely affect report and update performance. Objects which live and die over \nlong periods of time (vs short lived objects) must also be accounted for with routine maintenance. R \nGardner kayk@sundew.sc.ti.com Performance is affected by several factors in each of the main areas. Message \nprocessing functions must be tuned to only take the memory each will actually need. Separation of message \nclasses and application classes keeps maintenance of code simplier. Care must be taken to not create \nmore objects than actually needed for processing and to release objects not required as soon as they \nare free. Garbage collection and backups can benefit by larger cache sizes but are limited by how the \nconsumption of memory affects the online processes. Tuning of large cache sizes may produce unexpected \nresults based on available memory and other system processes. Clustering of important data in a large \ndbf must happen in incremental steps so that update and report performance is unaffected by rewriting \nof dbf pages and other dbf background activities. In high performing applications, the amount of movement \nof objects from one memory space to another must also be kept to a minimum. Each of the programs and \ntask is discussed in terms of how various system and dbf parameters affected overall performance of the \nOODB. Application. One factor which will contribute the success of the production system is the ability \nto divide the processes into identifiable units which can operate independent of each other. This allows \neach process to be tuned to get the best performance. Each process can be individually monitored for \nperformance and memory usage, object creation Austin, TX October 1519,1995 and commit rates easily with \nthe clear separation. This application is broken down into several different processes which each require \ndifferent settings for cache sizes and for private object table sizes. One processes messages from a \nmainframe application, one loads messages into application objects and one processes request from the \nunix application for additional information from the mainframe. Another application processes report \nrequest from other unix applications for query and report output. This process creates relatively small \nobjects which are short lived but has a high commit rate. Commits are based on number of transactions \nreceived from the mainframe or on elapsed time since last commit. This process alone would not create \nany garbage since all information is kept stored in the transaction queue. Each message will only last \na few seconds if the gemx process is running. These messages provide a common ground between the IMS/Mainframe \nfeed and the application objects. Keeping the number of objects on each commit to a relatively small \nnumber allows the temp cache size allocated to be small and reduce the overhead of each commit. There \nis however a fixed price for each OODB commit which means some number greater than 1 object/commit and \nless than 100 objects/ commit improves performance. The transaction count and elapse time count are configurable \nper site to provide the optimum performance to timelyness of the update factor. The cache sizes must \nbe large enough to prevent premature movement of new objects to the dbf. This process was monitored for \nnumber of ejects created over several specified time periods and by type of mainframe message recieved. \nUsing the available statistics from the OODB the number of objects created and committed over a fixed \ntime interval were determined. Even with a steady commit time interval of 15 seconds, the number of new \nobjects created and committed varied greatly. From lo-20 to 400- 450 objects per reading. During those \nintervals the number of times the dbf had to move objects to dbf pages was also monitored. This allowed \nlocal cache sizes to be adjusted to take the minimun foot print but not force objects into the dbf prematurely. \nThe types of objects being committed could also be determined. The main objects which were being created \nwere Floats, Strings and DateTime objects. Each of these groups were further examined as to content. \nThe DateTime objects were found to be created in multiples of 3 which indicated some problem processing \nfeed information. When float values were examined we discovered that we created about 1900 zero value \nfloats from 2 100 total floats created. Similar information was examined for String classes of objects. \nMany string values re-occurred constantly. It was clear that reducing this creation of unneccessary reduntant \nobjects would reduce the processing load on the OODB. The process which loads the message data into application \nobjects deals with a much larger set of objects. It must read in the number of messages from the queue \nit will process in one commit, read the targeted application objects from the dbf or create new ones \nand post any request for additional information. Thus in order to keep from having permanent objects \nrolled out of cache constantly and reading new objects in this process requires a large cache (4Meg) \nfor private objects. This process does not create very many new objects but does expire many objects \ncreated in the message loading process. Garbage is created by this process by expiring the messages it \nreads from the message queue. Many of the value objects (eg strings, floats etc) are moved from being \nreferenced in the messages to placed in the target permanent object (or long lived object). This causes \nthe previous object to be replaced and it also becomes garbage. This process also has a high commit rate \nand tries to keep the commit record size small. From the same data for the smsx process 660 zero value \nfloats 450 DateTime objects, and large numbers of redundant strings were garbaged. Insome cases the same \nvalue would be replaced from a activity message. Constant updates to the OODB cause the data to become \nscattered on pages in the DB. The gemx process also takes care of trying to keep the data more closely \npacked by clustering based on some transaction count or time period. The impact of clustering had both \nnegative and positive effects which will be discussed later in the paper. The data request server creates \nonly a few new objects but they are relatively large in size 50- 300K each. Special actions must be taken \nto prevent these temp objects from reaching the dbf pages and being added to the overhead of normal dbf \npage processing. These objects are returned to the requesting application in the form of a file name \nbut the temporary string built to contain the report is retained in the dbf. Special code to remove these \nwas necessary. A large private cache is needed to contain all the application objects which the reporting \nsystem must traverse. Its cache requirements are the same as the GUI interface. The GUI interface has \nsome additional cache requirements due creation of widgets in the display of the forms. Some GUI objects \ncan be optimized by pre- creating some forms and reusing these when they are available rather than creating \na new form each time one is required. Formatting of the report was looked at closely for creation of \na large number of temporary objects (eg strings). Special features in the OODB allow dynamic sizing of \nstring objects which keeps object creation down to only formatting specific datatypes. OODB Maintenance. \nClustering of important application objects. Because all of our behavior for the application was in the \nOODB it was important to cluster static application classes and dat objects. Application classes were \nclustered according to usage. The reporting classes and forms were clustered together. This clustering \nincluded the definitions and methods. The classes associated with the message processing part of the \napplication were clustered together. Data objects which were considered static or rarely modified were \nalso loaded at application build time and clustered together. Clustering these applications objects can \nresult in a 5 -10% improvement in page management by the OODB and result in execution speed increases. \nClustering effectiveness was measured in terms of report times over a standard collection of reports \nused as a benchmark of the system. Over time, the clustering effectiveness can be measured in terms of \nOODB page fragmentation. The clustering of application data which is constantly being modified presents \nsome special problems for the application. While it is necessary to cluster data to maintain long term \napplication performance, the act of clustering and the frequency of clustering can adversely affect enduser \napplication performance. Application data tended to fall into 1 of 4 categories for clustering behavior. \nObjects which tended to be updated frequently were only clustered on demand or as fragmentation became \ncritical or affected performance of the reporting application. Objects which are typically read only \nare clustered at creation. The structure of the object header may make some decisions for you since it \nmay or may not contain pointers to the object. Another group of objects may be infrequently read are \nclustered at creation time in separate pages of the OODB. The last group are objects which are frequently \nread and only occasionally updated. These are clustered at creation and then re-clustered as the frequently \nupdated objects are clustered. Each category of objects which are clustered are placed together on the \ndbf pages. The dbf maintains these clustering via logical dbf page grouping. Clustering becomes a performance \nconsideration for two reasons. The first is Austin, TX October 1519,199s grouping of data within the \nheirarchy which is best suited for the type of application accessing it or updating it. It will be difficult \nto get the clustering best suited for all applications but if you limit your domain of usage then you \ncatch the 80% goal for most queries. On the other hand, the act of clustering causes the process doing \nthe clustering to touch numerious objects and create commit records many times larger than normal after \na clustering activity. During normal processing for gemx 100 - 200 objects per commit were being rolled \nto the dbf. After a cluster activity the numbers ranged from 20K to 25K objects. This activity manifest \nitself as reduced report performance mostly due to OODB activity to move all the necessary objects around \nand have the applications reconnect to those pages.  Garbage collection considerations. The number one \nitem in good overall long term performance is not to create garbage in the first place. Many perfectly \ngood designs had the bad habit of creating too many throw away objects. After the code is written and \nworking, it needs to be evaluated for unnecessary object creation. Unneccessary objects create overhead \nfor the syetem during creation, writing of the object, marking it dead and finally scavenging the object. \nReducing the amount of overhead needed to run an application will reduce the chances to impact the application \nperformance and increase the chances it will keep running for the long term without any unscheduled down \ntime. Transaction rates were also determined for each facility to determine the best time to run such \nmaintenance activities as backups and full mark sweep garbage collections. In the OODB we were working \nwith, we could run our applications for some specified amount of time then examine the OODB for numbers \nof items it would consider garbage. We inventoried these items and began to categorize them according \nto life span and place of origin. It was determined that many instances were simply being instantiated \nas part of the new and initialization methods (constructors) built by the programmer. Often this is done \nso that even an empty object will respond to all its expected behavior. This may be fine for l- 10 objects \nevery so often that are created and so replaced but not for long haul applications where this number \nmay accumulate to thousands or 10 s of thousands in a single day. It was necessary to examine all C++ \nconstructors for filling in objects even when they may remain empty for the objects life time. Further \nanalysis of the information revealed that the other objects were created asmany repeated values of string, \ndatetime and float objects. Once these were identified, these could be pre-loaded on the dbf and have \nall references to these values point to the same place. This reduced the object creation by 2/3 s and \nlikewise reduced the number of objects required to be scanned by the full sweep and epoch garbage collectors. \nThe OODB used in this application provides a full time Gc process which allowed some tuning of the application \nand life span of the objects to be automatically picked up without doing a full sweep of the dbf. A full \nsweep of the dbf by a regular Gc is configured to be able to pull as much of the dbf into memory as possible \nand could be scheduled at slow periods in the manufacturing such as at shift changes and lunches. The \nmore automated the facility the less slow periods happened but frequent shorter Gc could be scheduled \non these type facilities. Even with reduced garbage to collect the garbage collection facility was tuned \nto provide as little impact to the updates and reporting application by having it run more often but \nprocess less garbage each time. This type of straetgy removes the highs and lows in end user response \ntime. Backups and maintenance of hot backup systems was implemented with as little impact to the production \nsystem as possible. Cache tuning for the backup and selection of fast I/O devices for backup disk was \nan important factor. In some cases where critical down time was limited to a few minutes a complete hot \nrunning system was maintained taking feeds but not running reports. Backups could be performed on the \nhot spare.  Conclusions. Anyone of the above factors can affect the performance of the OODB in general. \nEach part of the total application must be considered when one part of the application begins to perform \npoorly. I have described the steps taken to determine for our application the parameters which most affected \nperformance and the resolution of them. In this application several factors were examined. Code was examined \nfor loop optimizations and excessive garbage creation. Factors which caused some objects to be moved \nto the OODB had to be accounted for in each applications such as large object handling. Clustering of \napplication objects was done based on frequency of updates and access patterns. Large cache sizes were \nused for processes which stepped through the entire OODB or had large collections to maintain. Other \nprocesses could be reduced and were adjusted according to OODB stats available. The clustering data and \nits effects on commit record size were also measured due to pronounced effects on processes which were \nsharing database pages. Backups showed no impact on the production systems with large adjusted local \ncaches. The large cache also allowed 10 fold increase in backup speed. Austin, TX October 1519,199s \n\t\t\t", "proc_id": "260094", "abstract": "", "authors": [{"name": "Charles R. Gardner", "author_profile_id": "81100521579", "affiliation": "Texas Instruments, Inc.", "person_id": "P43436", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/260094.260229", "year": "1995", "article_id": "260229", "conference": "OOPSLA", "title": "Performance of an OODB in an online 7&#215;24&#215;365 manufacturing operation", "url": "http://dl.acm.org/citation.cfm?id=260229"}