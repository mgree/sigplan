{"article_publication_date": "06-15-1996", "fulltext": "\n First-Class Synchronization Barriers I?ranklyn Turbak Wellesley College Computer Science Department \nftur bak@wellesley, edu Abstract Our purpose is to promote a second-class mechanism -the synchronization \nbarrier to a fist-class value. VVe in\u00adtroduce the synchr-era, a novel synchronization mechanism that \nenables the coordination of a dynamically varying set of concurrent threads that share access to a first-class \nsyn\u00adchronization token. We demonstrate how synchrons can be used to modubrly manage resources in cases \nwhere existing techniques m-e either inapplicable or non-modular. In par\u00adticular, synchronized lazy aggregates \nenable the first space\u00adefficient aggregate data decomposition of a wide range of algorithms. We also \nintroduce explicit-demand graph reduc\u00adtion, a new semantic framework that we have developed to describe \nconcurrency and explain the meaning of a synchron rendezvous. 1 Overview Significant expressive power \ncan be harnessed by capturing programming language idioms in the form of first-class val\u00adues. A value \nis said to be first-classit can be when (1) named (2) passed as an argument to a procedure (3) returned \nas a result from a procedure and (4) stored in a data struc\u00adture. First-classness augments expressive \npower by allowing previously idiosyncratic and artificially limited mechanisms to be manipulated in general \nand orthogonal ways. Clas\u00adsic examples of powerful first-class values include first-class procedures \nand continuations. In this paper, we argue for the first-class citizenship of a new kind of value: the \nsynchronization barrier @ence\u00adforth abbreviated as barrier ). A barrier is a mechanism for coordinating \nthe execution of concurrent threads. When a thread waits at a barrier, its execution is suspended un\u00adtil \nall threads participating in the barrier are also waiting at the barrier. After this rendezvous, execution \nof the sus\u00adpended threads is resumed. By constraining the order of computational events among threads, \nbarriers help to man\u00adage shared resources. For example, threads atomically de\u00adpositing money in a shared \nbank account can be forced to wait at a barrier in order to guarantee that each observes the same final \nbalance after crossing the barrier. Even in Permission to make digifahlwd copy of part or all of this \nwork for personal or daesroom use is ranted without fee provided that copies are not made or distributed \nfor pm i t or commercial advantage, the oopynght notice, the title of the publication and its date appear, \nand notkx is given that copying is by permission of ACM, Inc. To copy otherwise, to republish, to post \non servers, or to redistribute to Ik&#38;, requires prior specific permission andlor a fee. ICFP 66 5/96 \nPA, USA 01966 ACM 0-69791 -771 -5/96/0005 ...$3.50 a functional language, barriers are helpful for limiting \nthe amount of memory used by a program. Barriers serve a purpose different from that of mutual exclusion \nmechanisms like semaphores [Dij68], locks [Bir89], and monitors [Hoa74]. Whereas mutual exclusion mech\u00adanisms \nprevent threads from simultaneously accessing re\u00adsources, barriers manage resources (e.g., shared variables \nand memory) by limiting the extent to which threads can be out of step . Although mutual exclusion mechanisms \nhave been en\u00adcapsulated in fist-class values like semaphores and locks, barriers are traditionally second-class \nmechanisms. In most data parallel models, selected processors engage in an im\u00adplicit rendezvous after \nthe execution of a data parallel opera\u00adtor. The communication protocols in channel-based concur\u00adrent \nprocess models similarly involve an implicit rendezvous [Hoa85, Mi189, CM90]. Some languages (e.g., Id \n[AAS95]) permit the programmer to insert explicit barriers, but they are not manipulable as first-class \nvalues. One approach to making barriers first-class is to manip\u00adulate them via the following interface: \n barrier : integer -+ barrier. Create a new barrier value for synchronizing integer threads. A rendezvous \nwill occur at the barrier when integer threads are sus\u00adpended at the barrier.  uause : barrier -+ m \nt. Causes the thread in which the pause is called to be suspended at the given barrier until a rendezvous \noccurs, at which point the thread resumes with the return of the pause. It is an error to call pause \nafter a rendezvous has occurred.  This sort of first-class barrier captures the essence of most conventional \nbarrier constructs, which are designed so that it is easy to determine exactly how many threads will \npar\u00adticipate in the barrier. This number is either known stat\u00adically or can conveniently be determined \nat run-time when the structure implementing the barrier is created. In these cases, a barrier can be \nimplemented as a pair of a counter and a set of suspended threads. The counter is initialized to the \nnumber of participating threads. When a thread encoun\u00adters a barrier, the counter is decremented and \nthe thread is suspended and added to the set. When the counter reaches zero, the rendezvous occurs, and \nall the threads in the set are resumed. Note that all threads are treated symmetrically in this synchronization \nprotocol, in contrast with other proto\u00adcols in which some threads wait for a signal generated by others. \nAltnougn stralghttorward, the barrier/pause form of first-class barriers suffers from some important \nmodularity problems. The main limitation of this form of first-class barrier (as well as existing second-class \nbarrier mechanisms) is that it cannot handle situations in which the number of threads participating \nin the barrier cannot be predicted at the point where the barrier is created. Section 2 presents ex\u00adamples \nwhere first-class barriers having a dynamically vary\u00ading set of participating threads are usedin an essential \nway. Furthermore, the barrier/pause style of barrier requires the programmer to keep track of the number \nof threads par\u00adticipating in the barrier an error-prone process better handled by the language implementation. \nWe have invented a novel fist-class barrier, the sgnchron, that makes it possible to coordinate a set \nof threads whose size is not known when the barrier is created. Instead, a ren\u00addezvous occurs when the \nfollowing dynamically determined rendezvous condition is met: every thread that could evev wait at the \nbarrier is waiting at the barrier. All threads with access to the synchron are potentiaJ participants \nin the barrier. A thread participates in the barrier by waiting on the synchron. A thread can leave the \nset of potential partici\u00adpants by dropping access to the synchron. A thread can also prevent a rendezvous \nby maintaining access to the synchron without waiting on it. A synchron is a one-shot barrier; af\u00adter \na rendezvous, a synchron expires and cannot be used again. It is helpful to think of synchrons in terms \nof tempo\u00adral constraints. A synchron represents the (as-yet unde\u00adtermined) instant of time at which the \nrendezvous occurs. Operations can be constrained to happen before or after this instant, or left unordered \nwith respect to it. Similarly, one synchron can be constrained to represent am instant be\u00adfore, after, \nor simultaneous with that of another synchron. A language supporting synchrons is responsible for solving \nthe temporal constraints; if the constraints are inconsistent, deadlock results. The interface to synchrons \nis specified by the following three procedures: synchron : m t -+ synchron. Returns a new first\u00adclass \nsynchron value. A synchron is a first-class barrier that represents the instant of time at which the \nbarrier rendezvous occurs. c wait : synch-on + um t. Causes the thread in which the iiait is called \nto suspend until a rendezvous occurs at the given synchron, after which the thread resumes with the return \nof the wait.  simul : synchron x synchron + uru t. Constrains both argument synchrons to represent the \nsame instant of time.  All temporal constraints involving operations and synchrons are specified via \nwait and s imul. Although s imul declares an explicit temporal constraint, the temporal constraints ex\u00ad \npressed via wait are implicit in the control flow of individual threads. The design of synchrons was \ndriven by the goal of ex\u00ad pressing space-efficient algorithms as the modular composi\u00ad tion of aggregate \ndata operators. Following Hughes [Hug84], we argue in Section 2.2 that concurrency and synchroniza\u00ad tion \nare essential for preserving the space complexity of non\u00ad modular algorithms when decomposing them into \nreusable components that communicate via aggregate data. In par\u00adticular, synchrons are the first run-time \nmechanism to enable the space-efficient decomposition of algorithms that manip\u00adulate aggregates non-linearly. \nThe first-classness of synchrons raises an important ques\u00adtion with semantic and pragmatic implications: \nhow is the rendezvous condition computed? Informally, a rendezvous shouId only take place when all threads \nholding a synchron are waiting on it. Pointers to a synchron can be classified into two types: waiting \nand non-waiting. A suspended call to wait holds a waiting pointer to a synchron; all other ref\u00aderences \nare non-waiting. A non-waiting pointer prevents a rendezvous because a thread with non-waiting access \nto a synchron might later call wait on it or share it with other threads. However, when all references \nto a synchron are waiting references, it is clearly safe for a rendezvous to oc\u00adcur. After the rendezvous, \nno references to the synchron can remain; this accounts for the one-shot nature of synchrons. Thus, the \nsemantics of synchrons is intimately tied to details of automatic storage management. The semantics presented \nin Section 5 formalizes this relationship in a high-level way. The rest of the paper is organized as \nfollows: Section 2 presents examples that illustrate the utility of synchrons. Section 3 surveys related \nwork. Section 4 introduces OPERA, a concurrent version of Scheme [CR+91] that supports syn\u00adchrons. Section \n5 gives a brief overview of EDGAR, a graph\u00adrewriting framework that formalizes the semantics of syn\u00adchrons. \nSection 6 concludes with a brief description of our experiences with synchrons. 2 Synchron Examples \n2.1 An Event Scheduler To introduce the power of first-class barriers, we present a simple event scheduler. \nThe interface to the scheduler is defined by the following two Scheme procedures: c (event thunk) crest \nes an event for performing the ac\u00adtion specifed by the parameterless procedure th.mk. (->pre POSt) declares \nthat the event pre must be performed before the event post. The goal of the scheduler is to perform the \nactions of all events m an order that is consistent with the -> declarations. A subtlety of the system \nis that performing the action of an event may introduce new temporal constraints. Consider the following \nexample: (let ((a (event (lambda () (display A )))) (b (event (lambda () (display B )))) (c (event (lambda \n() (display c ))))) (let ((d (event (lambda () (begin (-> b c) (display D )))))) (-> a d))) The (-> a \nd) forces A to be displayed before the declaration (-> b c) is encountered. The possible outputs of this \nex\u00adpression are ABCD, ABDC, and ADBC. Unsolvable constraints result in deadlock; the following example \ndeadlocks after displaying A: (let ((b (event (lambda () (display B )))) (c (event (lambda () (display \nC ))))) (let ((a (event (lambda () (begin (-> c b) (display A )))))) (begin (-> a b) (-> b c)))) Figure \n1 shows a complete implementation of the sched\u00aduler in a concurrent version of Scheme, An event is repre\u00adsented \nas a pair of synchrons that specify when the action as\u00adsociated with the event starts and stops. As in \nother ccmcur\u00adrent versions of Scheme, the future construct immediately returns a placeholder and commences \nthe concurrent ~evalu\u00adation of the placeholder value [Mi187, Ha185, For91]. Within event, future forks \na thread that forces the thunk applica\u00adtion to be sandwiched between the start and stop instants. The \n-> procedure forks a thread that guarantees that the stopping instant of the first event must precede \nthe starting instant of the second event. The language implementation is responsible for dynamically \nsolving the temporal constraints introduced by the synchrons. The event scheduler makes essential use \nof the first-class nature of synchrons and of the rendezvous condition. If syn\u00adchrons were not first-class, \nit would not be possible to bun\u00addle them up into an event. Furthermore, since the scheduling constraints \ncannot in general be determined without execut\u00ading the program, a barrier mechanism requiring advanced \nknowledge of the number of participating threads would not be helpful in this situation. (clef ine (event \nthunk) (let ((start (synchron) ) (stop (synchron) )) (begin (future (begin (wait start) (thumld (wait \nstop) )) (cons start stop)))) (clef ine (event-start event) (car event)) (clef ine (event-stop event) \n(cdr event)) (define (-> pre post) (let ((pre-stop (event-stop pre)) (post-start (event-start post))) \n(f utuie (begin (wait pre-stop) (wait post-start) ))) ) Figure 1: Synchron-based event scheduler.  2.2 \nSpace-Efficient Aggregate Data Oper\u00adators A standard modular programming technique is to express monolithic \n(i. e. non-modular) programs as the composi\u00adtion of mix-and-match operators on aggregate data (lists, \nstreams, arrays, trees). One drawback of this aggregate data paradigm is that intermediate data structures \ncan cause modular programs to require more time and space than their monolithic counterparts. Numerous \nstrategies have been developed to reduce these overheads in aggregate data programs (e.g., laziness [Hug90], \nalgebraic transformations [DR76, Bir88], listlessness [Wad84], deforestation [Wad88, GLJ93], series [Wat90]). \nHowever, these strategies either unduly restrict the style of aggregate data program allowed (e.g., trees \nare not allowed; aggregates may only have a sin\u00adgle consumer), or they provide no guarantees (e.g., a \ntrans\u00adformation may increase overhead instead of decreasing it). The synchron is the first run-time mechanism \nthat en\u00adables a broad class of algorithms to be modularized into aggregate data programs that exhibit \nthe same asymptotic time and space complexities as the original algorithms. Time complexity is easy to \npreserve, but the presence of aggre\u00adgates can make it difficult to preserve space complexity. For example, \nlist-based decompositions of constant-space algo\u00adrithms can require linear space, and tree-based decomposi\u00adtions \ncan require space proportional to the number of ele\u00adments in the tree rather than the height of the tree. \nSpace\u00adefficient aggregate data programs were the primary motiva\u00adtion for inventing synchrons. We illustrate \nthis technique in the context of a simple function for averaging a sequence a numbers. Given a func\u00adtion \ng: integer + integer, a predicate p: integer + booJean, and an integer a, consider the sequence go(a), \ngl (a), gz (a), . ..79 - (a), where n is the smallest non-negative integer for which p(gn (a)) is true. \nIntuitively, a function that av\u00aderages the numbers in this sequence should run in constant space because \nit only needs to keep track of three state vari\u00adables: the current number, the running sum of the numbers, \nand the length of the sequence. Yet, as we show below, stan\u00addard approaches for expressing this problem \nin an aggregate data style require space linear in the length of the sequence. In contrast, a corresponding \nconcurrent program with syn\u00ad chrons is guaranteed to run in constant space. In the aggregate data style, \nthe averaging function ex\u00ad hibits the stmcture of the foJJowing block diagram: SUM GENERATE I n LENGTH \n@ Figure 2: Block diagram for an averaging function. GENERATE produces a sequence determined by the \nparame\u00ad ters g, p, and a. This sequence is consumed by blocks that calculate the sum and length of the \nsequence; these two numbers are divided to give the average. 1 Figure 3 shows a realization of the block \ndiagram in Scheme. The GENERAT~ SUM, and LENGTH blocks are im\u00adplemented in terms of the higher-order \nsequence operators generate and accumulate. (generate seed next done?) creates a sequence of values starting \nat seed and iteratively applies a next function until the done? predicate is true. (accumulate op init \nseq) iteratively accumulates the el\u00adements of seq with the binary operator op starting with init as the \ninitial accumulator. Definitions of generat e and accumulate appear in Fig\u00adure 4. These in turn use the \nabstractions pack and unpack 1In the diagram, thick ]ine.q designate the transmission f a e\u00adquence, while \nthin lines designate the transmission of a single nnmber. (define (average g p a) (let ((mum (generate \na g p))) (/ (accumulate + O mans) (accumulate (lambda (x y) (+ 1 y)) o mans)))) F@n-e 3: Modular implementation \nof an averaging function in terms of higher-order sequence operators. whose purpose is to abstract over \ndifferent sequence imple\u00admentations. In the implementation of sequences as strict lists (Figure 5), the \naverage function requires space linear in the length of the fde because the entire nums sequence must \nbe generated before any accumulation operations can be performed. (clef ine (generate ini.t next done?) \n(if (done? init) )() (pack init (generate (next init) next done?)))) (define (accumulate 0p init seq) \n(if (null? seq) init (unpack seq (lambda (hd tl) (accumulate op (OP hd init) tl))) )) Figure 4: Higher-order \nsequence procedures. (pack EI E2) destigars to (cons EI E2) (define (unpack 1st f) (f (car lst) (cdr \nlst))) Figure 5: Strict Implementation of sequences. Even if lazy lists are used (Figure 6), the average \nfunc\u00adtion requires linear space in a sequential language. Lazi\u00adness does permit producer/consumer coroutining \nbetween the generation of mum and the accumulation in one of the aguments to /. But without some form \nof concurrency, evaluation of one argument to / must finish before the eval\u00aduation of the other argument \ncan begin. At this juncture, the entire nums sequence must be in memory, implying a lin\u00adear space requirement. \nHughes argues in [Hug84] that any sequential evaluation strategy for average must use linear space. Concurrency \nalone does not guarantee efficient space re\u00adquirements for average. Suppose average is executed in a \nconcurrent version of Scheme in which all subexpressions of an application expression are evaluated in \nparallel but the procedure call itself is strict. In the worst case, the sequence implementation of Figure \n6 can still require linear space be\u00ad cause the evaluation of one argument to / may race ahead of the \nevaluation of the other argument, forcing the entire nums sequence to be stored in memory at one time. \n(pack EI E2) desugars to (cons EI (delay E2) ) (define (unpack 1st f) (f (car lst) (force (cdr lst)))) \nFigure 6: Lazy implementation of sequences. (pack El EZ) de.mgws to (list (synchron) El (delay EZ) ) \n(define (unpack seq f) (let ( (sync (first seq) ) (hd (second seq)) (tl (third seq))) (begin (wait sync) \n(f hd (force tl))))) Figure 7: Synchron-based implementation of sequences, Constant-space behavior for \naverage can be guaranteed when synchrons are used to augment the lazy implementa\u00adtion of sequences in \na concurrent language (Figure 7); we call this technique synchronized lazy aggregates. pack asso\u00adciates \na new synchron with each element of the sequence; unpack waits on this synchron before performing any \noper\u00adation on the element. The barrier provided by the synchron prevents the accumulation in one argument \nto / from racing ahead of the accumulation in the other amument. In fact. the synchron forces the two \naccumulations to proceeed in lock step, so that the computation only uses constant space. Intuitively, \neach synchron in a synchronized lazy aggre\u00adgate models a strict procedure call boundary in the corre\u00adsponding \nmonolithic version of an algorithm. Figure 8(a) is a stylized depiction of some of the operations performed \nby a monolithic iterative averaging fuction, The horizontal dot\u00adted lines indicate the boundary of a \nloop or tail-recursive procedure call in the execution of such a fimction. In a language with strict \nprocedure calls, all operations above a dotted line must complete before any operations below the line \nare initiated. This is true whether or not the arguments are evaluated concurrently. A strict procedure \ncall thus acts as a kind of barrier between the evaluation of the arguments and the computation of the \nbody. This barrier is critical for guaranteeing that the averaging computation can be per\u00adformed in constant \nspace. Indeed, the space consumption problems that arise in the presence of non-strict (e.g., lazy and \neager) procedure calls are typically due to the lack of such barriers. By decomposing a computation into \nblocks with local procedure calls, the aggregate data style replaces each global barrier by a collection \nof local barriers (Figure 8(b)). Data transmission provides a loose coupling betweeen the blocks that \nis generally not good enough to simulate the global barrier. Some additional mechanism is needed to constrain \ncorresponding local barriers to behave like the global bar\u00adrier of the monolithic computation. Synchrons \nare such a mechanism. As fist-class values, synchrons can be trans\u00admitted between blocks. When the same \nsynchron is shared by several blocks, it serves the role of a global barrier (Fig\u00adure 8(c) ). In this \nway, synchrons constrain the network of blocks to exhibit the same asymptotic space complexity as ---------~---1 \n----_____ 1, 11 a o 0 la, :01 [0; 1a: :0: r-i-: II ,1 ;,! +____ _ ; -; L-I 1--: 1,,1 1+ 1, +; :1+1 \n1,,1 11 1,11 II 11 :g:ll II 11 g 11 11 ----_____ ___ k -: 1-: 1-J 11 II t, +: 1+; + 1+ II ill 11 g \n[g:/[ ------------L -.::! ,_ J , 1,  R! P1 11 1,11 II 1,11 11 1,,1 11 1,11 1,11 . ___, ---, 11 1----1 \n (a) (b) (c) Figure &#38; Shared synchrons (c) allow local strict procedure call barriers (b) to simulate \nthe global strict procedure call barriers of monolithic computations (a). the corresponding monolithic \ncomputation. This approach to the lock step processing of aggregate data operators can be generalized \nto handle recursively\u00adprocessed sequences and tree-structured data. For example, alpha-renaming of abstract \nsyntax trees can be expressed as the composition of reusable tree operators. The result\u00ading modular program \nrequires intermediate space propor\u00adtional to the height of the tree, not the number of nodes. Lock step \nprocessing in the presence of recursion and tree\u00adstructured data is more complex than the case of linear \nit\u00aderation because it is necessary to synchronize at procedure return as well as procedure call. The \nmodular handling of tail-recursion in the extended system is especially tricky and requires an extension \nto the synchron interface. Space does not permit a detailed discussion of these issues; see [Tur94] for \nin-depth coverage. The average example emphasizes that concurrency and first-class barriers are important \ntools for managing resources (in this case, memory) within a modular program. The fea\u00adture of average \nthat is hard to handle is the fact that the aggregate nums is used non-linearly (i.e., more than once). \nWhen all aggregates are used linearly, simpler mechanisms, like laziness and deforestation, suffice to \nmake programs space-efficient. These mechanisms effectively match the pro\u00adducer of an element of an aggregate \nstructure with its single consumer. But when there are multiple consumers for a component of an aggregate, \nit is tricky to direct control be\u00adtween the producer and the consumers to obtain lock step processing. \nThis is why block diagrams exhibiting fan-out of aggregates are difficult to make space-efficient. In \ngeneral, some form of concurrency and synchronization are neces\u00adsary to process the producers and multiple \nconsumers in lock step. Fan-in (blocks with multiple aggregate inputs) is much more straightforward to \nhandle than fan-out. The goal is to unify corresponding barriers from multiple inputs, This is the purpose \nof the simul operator on synchrons. The s imul operator is not illustrated by the average example, but \nwould be required in a program that maps a binary procedure over two sequence arguments. Figure 9 shows \nthis map2 procedure and the associated repack2 abstrac\u00adtion. repack2 uses simul to express that the time \ninstants associated with the two input synchrons and single output synchron are all identical. (define \n(map2 f seql seq2) (if (or (null? seql) (null? seq2)) >() (repack2 seql seq2 f (lambda (restl rest2) \n(map f restl rest2))))) (define (repack2 seql seq2 f g) (let ( (syncl (first seql) ) (sync2 (first seq2) \n) (hall (second seql) ) (hd2 (second eeq2) ) (tll (third seql)) (t12 (third seq2))) (list (simul syncl \nsync2) (f hdl hd2) (delay (g (force tll) (force t12)))))) Figure 9: Binary mapping procedure that illustrates \nsimul. 3 Related Work The synchronization mechanism most closely related to the synchron is Hughes s \nsynch E construct [Hug84]. This is similar to Scheme s delay in that it creates a promise for evaluating \nE. The difference is that there are two distinct functions (let s call them f orcel and f orce2) for \nforcing the computation of E. When one of these functions is called on a promise, the thread executing \nthe call is suspended until the other function is called. After both fimctions have been called, the \nvalue of E is computed and both threads are resumed with this value as the result of the call. The value \nreturned by synch acts as a limited kind of first-class barrier. Hughes s barrier only permits a rendezvous \nbetween exactly two threads; coordinating more threads re\u00adquires a collection of such barriers. Hughes \ndescribes a mod\u00adular, constant-space averaging function, but his generators and accumulators are not \ngeneral; they include hardwired assumptions about the particular structure of the averaging problem. \nIn contrast, the generate and accumulate func\u00adtions presented above are completely general. The main \ndrawback of Hughes s approach (as well as of the barrier/pause mechanism suggested in Section 1) is that \nknowledge of the number of threads participating in a barrier must be explicit y encoded in a program, \nobstructing modularity. For example, using Hughes s mechanism, it is not possible to implement generate \nand accumulate from Section 2.2 in such a way that they maintain the same inter\u00adface and still yield \nlock-step computations. The problem is that generate doesn t know how many barriers to create, and each \ncall to accumulate doesn t know which forcing fmction to call at a barrier. Synchrons are more more mod\u00adular \nthan Hughes s barrier because the number of threads participating in a barrier is automatically determined \nby the flow of synchron values through a program, and all threads enter a rendezvous via the same wait \nprimitive. Waters s series system [Wat90, Wat91] is the only other system we know of that can execute \naggregate data pro\u00adgrams like average in constant space. A series is an abstrac\u00adtion of an iteratively \nprocessed sequence of values. Programs structured as blocks communicating via series are guaran\u00adteed \nto compile into efficient loops as long as the block di\u00adagrams satisfy certain conditions that the programmer \ncan readily verify. (Programs not satisfying these conditions are rejected by the compiler. ) The run-time \nnature of synchrons makes them more flex\u00adible than a compiler-based approach like series. The series \ncompiler requires that the data dependencies between all aggregate data operators be statically determinable, \nSyn\u00adchronized lazy aggregates are more general than series be\u00adcause they allow the operators to be configured \ndynami\u00adcally. For example, synchronized lazy aggregates can express a constant-space fimction that takes \na list of accumulators and returns the results of each accumulator on the aggregate produced by a single \ngenerator. Such a function cannot be expressed via series because the accumulators are not stati\u00adcally \nknown. Moreover, whereas series is limited to iteratively pro\u00adcessed sequences, synchronized lazy aggregates \ncan handle recursively processed sequences and tree-structured data, It would be worthwhile to adapt \nseries compilation techniques to handle these more general kinds of processing; we plan to pursue this \nline of research. Deforestation [Wad88, GLJ93] is program transforma\u00adtion technique that removes intermediate \ndata structures from programs. It is more powerful than series in the sense that it can remove tree-structured \ndata. However, unlike series and synchronized lazy aggregates, current deforesta\u00adtion technology cannot \ndeal with fan-out of aggregates. To handle cases like average, transformations that combine ac\u00adcumulators \nhave been proposed [GLJ93], but these are ad hoc and it is not clear how to generalize them to general \nblock diagrams. Barriers are common in parallel processing systems. The operators in many data parallel \nlanguages are implicitly sandwiched between global barriers that limit the computa\u00adtion and communication \nthat can occur between successive rendezvous. In other parallel systems, programmers must explicitly \ninsert barriers to ensure synchronization between processors that share memory. Such barriers are second\u00adclass \nmechanisms that coordinate a set of processes known at barrier-creation time. The Id language supplies \na barrier construct for manag\u00ading program resources and scheduling side-effects in a lan\u00adguage based \non eager evaluation [Bar92, AAS95]. Barriers are indicated by a dotted line that separates groups of \nea\u00adgerly evaluated expressions; no computation is initiated be\u00adlow the line until all computations initiated \nabove the line have terminated. Because Id barriers can be localized to a particular set of parallel \nactivities, they can express more fine-grained coordination than is possible with the global barriers \nof data parallel languages. But the inability to manipulate barriers as first-class entities restricts \ntheir ex\u00adpressiveness; the examples given in Section 2 cannot be ex\u00adpressed using Id barriers. On the \nother hand, synchrons are powerful enough to simluat e Id barriers. A barrier can be represented as a \nsingle synchron. All threads executing above a barrier can be mod\u00adified to call wait on this synchron \nas their final action, while all expressions appearing below the barrier can be prefixed with a call \nto wait on the synchron. The first-classness and variable membership of synchrons makes it easy for threads \nabove the barrier to transitively pass along the synchron to all of their descendent threads. The communication \nhandshake in channel-based concur\u00adrent languages (e.g., [Hoa85, Mi189, CM90] ) involves a form of barrier. \nNeither the sending thread nor receiving thread(s) can proceed until a rendezvous of all the participants. \nIn these languages, synchronization is not separable from com\u00admunication, and communication events are \nnot first-class. Reppy s first-class events [Rep91] really act as first-class event generators; every \ncall of his sync on a given event causes the current thread to wait for a diflerent rendezvous. Synchrons \ncan be viewed as a way of permitting the mul\u00adtiway handshake of (XP [Hoa85] to be integrated into the \naggregate data paradigm. A variant of synchrons can be implemented in Bawden s linear language [Baw92], \nin which objects can only be shared via an explicit copy operation. The copy operation for syn\u00adchrons \ncan maintain a count of non-waiting pointers; a ren\u00addezvous occurs when this number drops to zero. A \nlanguage implementing synchrons removes the need for explicit copy operations by managing this count \nautomatically. The synchron is a new member of a class of high-level programming language features whose \nsemantics are inex\u00adtricably tied to garbage collection. Other examples of such GC-dependent features \ninclude object finalization and weak pairs [Wil]. The semantics of these features are defined in terms \nof garbage collection; even if memory were infinite and there were no need for storage reclamation, implement\u00ading \nsuch features would still require some form of garbage collection. 4 Opera: A Concurrent Scheme with \nSyn\u00ad chrons Synchrons are viable in any concurrent language that sup\u00adports automatic storage managment. \nHowever, for the sake of concreteness, we will focus on one such language: OPERA, a concurrent dialect \nof Scheme. (A concurrent version of ML would have been another reasonable choice. ) The synchron examples \nfrom Section 2 are written in OPERA. A grammar for the OPERA kernel appears in Figure 10.2 Unlike Scheme, \nOPERA S application has a concurrent se\u00admantics (subexpressions of call are evaluated in parallel). But, \nas in Scheme, the application itself is strict (all argu\u00adments must be evaluated to values before the \napplication occurs). Pe Program EE Expression IE Identifier L~ Literal OE Primitive Operator .. P .. \n.. E .. L I I [ (primop 0) (lambda (Ifo,~.L*) Eb~dg) (call .&#38;t..&#38;.id*) ; keyword optional (if \nEt.., Et~~~ E~I.~) (set ! In.rn. Ebody) (delay Ebody) I (f UtUre @mdy) (exclusive E=.cl i&#38;&#38; ) \n.. L .. usual Scheme literals .. usual Scheme identifiers : ::: +l*lconslcarlcdr other Scheme primitives \nsynchron I wait I simul [ synchron? touch I excludon I excludon? Figure 10: GrammarfortheoF ERAkernel. \n The default strictness of OPERA application canbe cir\u00adcumvented with two classic forms of non-strictness \nfmndin other dialects of Lisp [Mi187, Ha185, For91]. (delay i?) sup\u00adports lazy evaluation by suspending \nevaluation of E until its value is required. (future E) supports eager evaluation by immediately returning \na placeholder for the value of E, which is evaluated concurrently with the rest of the pro\u00adgram. future \nand the default parallel argument eva~uation strategy are the two sources of concurrency in OPERA. The \nevaluations associated with the placeholders produced by delay and future can be explicitly forced by \na touch prim\u00aditive, but they are also implicitly forced by touching contexts that require the value of \nthe placeholder (e.g., the operator position of call, the test position of if). In addition to the implicit \nsynchronization performed by a strict call, OPERA supports two explicit forms of syn\u00adchronization: barrier \nsynchronization and muturd exclusion. Barrier synchronization, in the form of synchrons, is ex\u00adpressed \nvia the synchroq wait, and simul primitives. Mu\u00adtual exclusion is provided by excludons, first-class \nlocks that are used in cor@nction with the exclusive construct. The form (exclusive Elo.~ Ebodv ) evaluates \nand returns VFdUe of E&#38;-x@ while it holds exclusive access to the lock denoted by Elock . 5 Edgar: \nthe Semantics of Opera. This section sketches our new semantic framework Em phcit Demand GrAph Reduction \n(EDGAR) which we use 2Non-kernel constructs like let, begiq and cond can be defined as syntactic sugar \nfor kernel constructs. to formalize the meaning of OPERA programs, particularly the details of a synchron \nrendezvous. EDGAR is a graph\u00adrewriting system that is distinguished from other such sys\u00ad tems by its \nexplicit representation of the flow of demand through a computation. This feature simplifies the descrip\u00adtion \nof OPERA S concurrency, non-strictness, and synchro\u00adnization. 3 The EDGAR framework was largely motivated \nby the desire to express the rendezvous condition for syn\u00adchrons in a simple, high-level way. Because \nof the close tie between synchrons and automatic storage management, an important feature of the EDGAR \nsemantics for OPERA is that it formalizes garbage collection in a way that programmers can reason about. \n5.1 Edgar Overview The overall structure of the EDGAR framework follows the recipe for an operational \nsemantics [P1081]: OPERA pro\u00adgrams are compiled into an initiai snapshot (an EDGAR graph\u00adical configuration), \nand transitions are made between snap\u00adshots in a step-by-step manner according to a collection of rewrite \nruies. A sequence of snapshots encountered in con\u00adsecutive transitions is called a trace. A trace from \nan ini\u00adtial snapshot to a final snapshot (a snapshot from which no transitions are possible) is a terminating \ncomputation while an infinite trace starting with an initial snapshot is a non\u00ad terrninating computation. \nEach computation is character\u00adized by a jate: A non-terminating computation has bottom as its fate. \n A terminating computation whose final snapshot is a value snapshot has as its fate a result whose value \nis determined by the snapshot.  c A terminating computation whose final snapshot is not a value snapshot \nhas dead~ock as its fate. The behavior of an initial snapshot is the set of all com\u00adputations that begin \nwith that snapshot. A behavior often contains numerous computations because transitions may be non-deterministic \n(several transitions are possible from a given snapshot ). The outcome of art initial snapshot is the \nset of all fates for the computations in its behavior. Because OPERA supports side effects (data mutation \nand 1/0), the non-determinism of transitions can lead to art out\u00adcome containing multiple fates. The \nEDGAR semantics for full-featured OPERA is clearly not Church-Rosser, but we suspect Church-Rosser may \nhold for a functioned subset of OPERA. The key difference between EDGAR and other graphicrd frameworks \n[Tur79, Pey87, B+ 87, AA95] is its use of explicit demand tokens to encode evaluation strategies. EDGAR \nspec\u00adifies evaluation order by annotating some graph edges with a demand token that indicates where evaluation \nsteps can take place. The implicit demand propagation implied by traditional inference rules (e.g., if \nasked to evaluate a + ap\u00adplication, evaluate the left-hand argument ) can be encoded in EDGAR as explicit \ndemand propagation steps (e.g. if the + node is annotated with a demand token, propagate a de\u00ad mand token \nto the left-hand argument ). In the presence 3W= have ~ec=ntly learned [Ari96] that the semantics Of \nOPERA can be expressed in a more traditional graph rewriting system using graphical contexts similar \nto those use in [AF94], Recasting OPERA semantics in this new form is a future goal. or expuclt clemand \ntokens, a global reduce any redex rule suffices because the details of the evaluation strategy are m \n already encoded in the graph itself. EDGAR S explicit representation of demand was inspired by the demand \ntokens used in Gelernter and Jagannathan s Ideal Soft ware Machine (ISM) [GJ90], a semantic frame\u00adwork \nthat combines aspects of Petri nets [Pet77] and graph rewriting. A handful of other systems employ explicit \nrepre\u00adsentations of demand. Pingali and Arvind describe a mecha\u00adnism for simulating demand-driven evaluation \nin a data flow model; they use data tokens to represent demand [PA85, PA86]. Ashcroft describes a system \nthat combines demand flow (via entities called questons) with data flow (via entities called datons) \n[Ash86]. 5.2 Snapshots A snapshot is a graph consisting of interconnected labelled nodes. Each node \ncan be viewed as a computational device that responds to a demand for a value by computing that value. \nEvery node has a set of labelled input ports that spec\u00adify the arguments to the node and a set of labelled \noutput ports that specify the results of the node. The number of input ports and output ports is dictated \nby the label of the node. Typically, a node has several input ports and one out put port. A connection \nbetween nodes is indicated by a directed edge from an output port of the source node to an Input port \nof the target node. Intuitively, an edge is used for a two-step communication protocol between its source \nand target ports: the target port can demand the value from its source port, and the source port can \nrespond to the demand by returning a value. Every edge has a state attribute that indicates the status \nof this protocol. There are three possible edge states: inactive: No demand has yet been made on the \nedge.  demanded: A demand has been made on the edge, but no value has been returned.  b returned A \ndemand has been made on the edge, and a value has been returned. The value returne~ by w edge in the \nreturned state is defined to be the source node of the edge; typical values include constants, pro\u00adcedures, \nand data structures. The protocol fiu-ther dictates that (1) no value can be re\u00adturned to an edge until \none has been demanded and (2) once an edge is in the returned state, it cannot be used for further communication. \nAn edge, then, acts as a one-shot communication fuse that can be used for transiting a sin\u00adgle demand \nand a single value before it is used up . This protocol distinguishes EDGAR from dataflow graph models, \nin which edges carry a stream of value tokens. The demanded state is the key feature of EDGAR that dis\u00ad \n tinguishes it from traditional graph rewriting systems and makes it an explicit demand model. The returned \nstate is not essential; it is just a convenient way to designate the class of value nodes (which could \nalso be specified syntacti\u00ad cally). Figure 11 is a pictorial representation of a sample snap\u00ad shot in \nthe computation of (7 3) + ~~, where the sharing of the -node specifies that the difference between \n7 and 3 is calculated only once. A unannotated edge is in\u00ad active, an edge with an empty circle is demanded, \nand an edge with a filled circle is returned. in result + argl arg2 ?i sqrt arg result argl arg2 cons \nconst 73 ii Figure 11: A simple snapshot. In Figure 11, the sink node is a distinguished node that serves \nas the primitive source of demand in a computa\u00adtion. OPERA programs compile into initial snapshots that \nare rooted at a sink node. (The compilation process is straightforward and is not described here. ) A \nfinal snapshot is a value snapshot when the input edge to the sink node is in the returned state. In \nthis case, the source node of the edge represents the resulting value of the OPERA program. 5.3 Rewrite \nRules Allowable transitions between snapshots are specified by set of rewrite rzdes. The rewrite rules \ndictate the dynamic be\u00adhavior of nodes and the flow of demands and values through a sequence of snapshots. \nA rewrite rule has two parts: a pattern and a replace\u00adment. The pattern is a partial graph structure \nin which edges may be at t ached to pattern vu riables inst ead of ports. A pattern is said to match \na snapshot if it can be embedded in the snapshot. The replacement specifies how the snap\u00adshot structure \nmatched by the pattern should be replaced in order to construct a new snapshot. For example, Figure 12 \nshows a simple rewrite rule that propagates demand through a sqrt node. A rewrite rule that matches a \nsnapshot can be applied to the snapshot to yield a new snapshot. Removing the structure specified by \na pattern from a snapshot that it matches leaves the contezt of the match. The new snap\u00adshot is constructed \nfrom the context by filling the hole left by the deleted pattern with the structure specified by the \nreplacement. The part of the original snapshot that is not directly matched by the pattern is carried \nover unchanged into the new snapshot. Rewrite rules are required to satisfy the following conti\u00ad nuity \nconditions: All nodes in the pattern must map invectively to sim\u00adilarly labelled nodes in the replacement. \nThe replace\u00adment can introduce new nodes, but it cannot delete existing ones. This aggressive approach \nto garbage collection guarantees that a synchron rendezvous can t accidentally be blocked by replacement \n a non-waiting pointer held by an inaccessible node. This 7S6 is a semantic simplification; a practical \nimplementation of result result sqrt sqrt arg arg  + P4 b b [sqrt-reques~ Figure 12: A rule that propagates \ndemand through a sqrt node. For each edge whose source and destination ports are preserved from pattern \nto replacement, the pattern state and replacement state must be related by the communication v-elation, \nwhich formalizes the commu\u00adnication protocol sketched in Section 5.2. This rela\u00adtion specifies that the \nedge state may either (1) stay the same (2) change from inactive to demanded or (3) change from demanded \nto returned.  5.4 Garbage Collection Rule applications can result in nodes that are inaccessible from \nsink and future nodes. which serve as root nodes for evaluation. A node is inaccessible from a root node \nif there is no directed path of edges from the output port of the node to the input port of the root. \nIn order to accurately model the space required by a computation and to avoid spurious deadlocks involving \nsyn\u00adchrons, it is necessary reclaim inaccessible nodes from a snapshot. We will assume the existence \nof a garbage col\u00adlection function, gc, that maps snapshots to snapshots by removing any nodes that are \nnot accessible from the root nodes. Since rewrite rules cannot delete nodes, garbage collec\u00adtion is the \nonly mechanism in EDGAR for removing nodes from a snapshot. Why not allow some forms of garbage col\u00adlection \nto be specified in the rules themselves? There are two reasons: 1. The continuity conditions from Section \n5.3 would be harder to state if all the pattern nodes did not appear in the replacement. 2. Rules performing \ngarbage collection are tricky to write. Even if a node appears inaccessible in the replacement of a rule, \nit can t necessarily be deleted because it might be accessible from a root via edges that don t appear \nin the rule.  5.5 Transitions In order to collect garbage as soon as possible, a transition combines \na rule application and garbage collection into a single step. There is a transition between S and gc(S \n) whenever a rule allows snapshot S to be rewritten into S , synchrons need not invoke a garbage collector \nat every eval\u00aduation step! In general, it may be possible to make several different transitions from \na given snapshot. In this case, transitions that rewrite different subgraphs of a snapshot loosely corre\u00adspond \nto different threads. Because only one rewrite rule can be applied per transition, a single transition \nallows progress for only one thread. 5.6 Synchron Semantics Here we briefly discuss the EDGAR rewrite \nrules that specify the semantics of synchrons (Figure 13). Space does not per\u00admit a presentation of all \nof OPERA s rewrite rules; for more detailed coverage, see [Tur94]. The [synchron-ret urn] rule treats \nsynchron nodes as self\u00adevaluating values. In English: If the synchron node has been demanded, then it \nis returned to the demander as a value. The [simultaneous] rule ensures that all references to two unified \nsynchrons point to the same synchron. The labelled triangles match the set of ali edges leaving a node \nas long as one of those edges holds a demand token. The [rendezvous] rule formalizes the rendezvous condi\u00adtion \nfor synchrons. A synchron output edge that is in the returned state and attached to a demanded wait node \nis a waiting pointer. Any other output edge of a synchron is a non-wait ing pointer. The rendezvous rule \nis only applica\u00adble when all of the output edges of a synchron are waiting pointers. The [rendezvous] \nrule returns a constant true node to all output edges of all the wait nodes participating in the rendezvous. \nSince the synchron is necessarily inaccessible after the [rendezvous] rule, it is garbage collected by \nthe transition based on this rule. The [rendezvous] rule embod\u00adies a proof of the rendezvous condition: \nevery thread that could ever wait on the synchron is waiting on the synchron. 6 Experience To experiment \nwith synchrons, we have built a prototype version of OPERA that directly implements the EDGAR rewrite \nrules. In this system, a node maintains the edges of its in\u00adput and output ports, and an edge maintains \nits source and target ports. An application of a rewrite rule mutates the graph structure appropriately. \nRather than performing a garbage collection (GC) after every transition, a reference\u00adcounting style of \nGC is supported; when the last output edge is deleted from a node, the node is reclaimed and its input \nedges are also deleted. Due to the possibility of unreclaimed cyclic structures, it is necessary to perform \na reachability\u00adbased GC when memory is exhausted. It is also necessary to perform a reachability-based \nGC when no rewrite rules are applicable; this removes any non-waiting pointers to sYn\u00adchrons that are \nheld by inaccessible cycles and may enable a rendezvous. If no rewrite rules are enabled by this last \ncase of GC, the computation is deadlocked. In a more traditional system, synchrons could be im\u00adplemented \nin terms of object finalization. A synchron can be represented as a mutable cell holding the set of threads \naa i synchron synchron A simul synchron [simultaneous]  ?iiii?*wx [rendezvous] Figure 13: EDGAR rewrite \nrdesforsynchrons. that are waiting at the barrier. Waiting ona synchron sus-OPERA, evaluation of the \nfollowing expression deadlocks: pends the current thread, inserts it into the set of suspended (let ((a \n(cons (synchron) 17))) threads, anddrops thereference tothe synchron. When the (begin (wait (car a)) \n(cdr a))) synchron becomes inaccessible, a finalization procedure re\u00adsumes all the suspended threads \nin the set. s imul can be The variable a, which is live after the wait, holds a non\u00ad handled by unioning \nthe thread sets of two synchrons and waiting point er to the synchron that prevents a rendezvous. sharing \nthe result. It is also necessary to monitor the num-To avoid such spurious deadlocks, it is helpful to \nadopt a ber of synchrons that hold the same set; the threads in a set style of aggressively unbundling \ndata structures that contain can only be resumed when the last such synchron is final\u00adsynchrons. The \nfollowing is an alternative to the above ex\u00ad ized. Implementing synchrons in terms of object finaliza\u00adample \nwhich the OPERA semantics guarantees will not dead\u00ad tion could be prohibitively expensive: programs involving \nlock. synchronized lazy aggregates make little progress between successive rendezvous, and each such \nrendezvous requires a (let ((a (cons (synchron) 17))) full reachability-based GC. (let ((b (car a)) \n(c (cdr a))) An implementation of synchrons based on reference counts (begin (wait b) c))) seems more \nviable. As in the graph-based implementation, a reachability-based GC needs to be triggered when mem\u00adory \nis exhausted and when no threads are scheduled. In This last example underscores the somewhat disturbing \na statically-typed language supporting synchrons, reference fact that OPERA does not respect certain \nforms of substitution\u00adcounts would only be necessary for structures from which based reasoning. The rules \nfor when objects become inac\u00adsynchrons could be reached. We plan to experiment with cessible must be \nexplicit and simple enough so that a pro\u00adsuch a language in the future. grammer can use them as a basis \nfor reasoning. The OPERA Our prototype implementation includes a graphical pro-semantics adopts the Iiveness \nand tail-call optimizations de\u00adgram animator (the Dynamator) that displays the sequence scribed in [APP92]. \nThese space consumption rules describe of snapshots in a computation. The Dynamator has been in-the aggressive \nreclamation of space in a way that the pro\u00advaluable for debugging EDGAR rules and OPERA programs. grammer \ncan understand at a relatively high level. They We plan to use it as a pedagogical tool for teaching \npro-also prevent a correct implementation of OPERA from hold\u00adgramming language concepts. ing references \nto values longer than strictly necessary. For Programming with synchrons is challenging. The main example, \nit is incorrect to evaluate (begin ( iialt b) c) in difficulty is that a thread waiting on a synchron \nmay cause an environment that maintains a binding between a and the deadlock by accidentally holding \na non-waiting pointer to pair, because such a binding would lead to a spurious dead\u00adthe synchron. For \nexample, according to the semantics of lock. Because they push the envelope of our understanding of GC-dependent \nfeatures, synchrons may seem unnecessarily complex. But we believe that synchrons merely highlight semantic \ncomplexities that are intrinsic to GC-dependent features and even to garbage collection itself. For example, \nsince synchrons can be implemented in terms of object fi\u00adnalization, any complex issues in the semantics \nof synchrons will also appear in the semantics of object finalization. The main difference between the \nfeatures is that object finaliza\u00adtion is usually considered to be a rare event while a synchron rendezvous \nis a common event. Furthermore, we view these complexities not as an in\u00addictment of synchrons, but as \nevidence that new idioms and better implementation techniques are needed to use GC\u00addependent language \nmechanisms more effectively. Raw syn\u00adcbrons are powerful but dangerous objects that are not in\u00adtended \nfor use by casual programmers. But it is possible to package them into abstractions that are accessible \nto a broader audience. We have used OPERA to implement a suite of sequence and tree operators that can \nbe composed to yield computations that exhibit fine-grained operational characteristics of non-modular \nloops and recursions [Tur94]. As suggested by Section 2.2, synchrons are crucial for achiev\u00ading this \nbehavior. Synchrons and synchronized lazy aggregates are the first steps in a research program whose \npurpose is to express algo\u00adrithms in a modular way while preserving important opera\u00adtional properties \nlike asymptotic time and space complexity. Even though concurrency and synchrons seem to be essential \nfor eqmessing certain algorithms in a modular fashion, this does not imply that these features are required \nfor executing such algorithms. In fact, we suspect that a compiler similar to Waters s series compiler \n[Wat91] should be able to auto\u00admatically generate efficient sequential monolithic programs for many algorithms \nmodularly expressed via synchronized lazy aggregates. Such a compiler would remove all overhead of concurrency \nand synchronization, as well as the overhead associated with packaging and unpackaging intermediate ag\u00adgregates. \nEven when it is impossible to remove synchrons at compile time, synchrons can be replaced by the simpler \nbarrier/pause barriers when the number of references to a synchron can be determined statically. It is \nworth exploring the expressive power of these restricted forms of synchrons because they are considerably \nless expensive to implement than full-fledged synchrons. Finally, a formal system for characterizing \nwhich combinations of synchronized lazy ag\u00adgregate operators are safe and which lead to deadlock would \ngreatly simplify reasoning about such operators. Acknowledgments We thank David Espinosa, Olin Shivers, \nDavid Gifford, and the anonymous referees for their helpful comments on drafts of this paper. We also \nbenefited from discussions with Zena Ariola, Andrew Appel, and Simon Peyton Jones. References [AA95] \nZena Ariola and Arvind. Properties of a first-order functional language with sharing. Theoretical Comp\u00aduter \nScience, 146:69 108, 1995. [AAS95] Shail Aditya, Arvind, and Joseph E. Stoy. Seman\u00adtics of barriers in \na non-strict, implicitly-parallel language. Technical Report Computation Struc\u00ad tures Group Memo 367, \nMIT Laboratory for Com\u00ad puter Science, January 1995. [AF94] Zena Ariola and Matthias Felleisen. The \ncall-by\u00adneed lambda calculus. Technical Report CIS-TR\u00ad94-23, Department of Computer Science, University \nof Oregon, October 1994. [App92] Andrew W. Appel. Cornpding with Continuations. Cambridge University \nPress, 1992. [Ari96] Zena Ariola. Personal communication, January 1996. [Ash861 E. A. Ashcroft. Dataflow \nand eduction: Data\u00ad . driven and demand-driven distributed computing. In J. W. deBakker, W.-P. de Roever, \nand G. Rozen\u00adberg, editors, Current Trends in Concurrency: Overviews and Tutorials, pages 1 50. Springer-Verlag, \n1986. Lecture Notes in Computer Science, Number 224. [B+87] H.P. Barendregt et al. Toward an intermediate \nlan\u00adguage based on graph rewriting. In PA RLE: Par\u00adatiel Architectures and Languages Europe, Volume 2, \npages 159 175. Springer-Verlag, 1987. Lecture Notes in Computer Science, Number 259. [Bar92] Paul S. \nBarth. Atomic data structures for paral\u00ad lel computing. Technical Report MIT/LCS/TR\u00ad532, MIT Laboratory \nfor Computer Science, March 1992. [Baw921 Alan Bawden. Linear Graph Reduction: Con\u00adfronting the Cost \nof Naming. PhD thesis, Depart\u00adment of Electrical Engineering and Computer Sci\u00adence, Massachusetts Institute \nof Technology, June 1992. [Bir88] Richard S. Bird. Lectures on constructive functional programming. In \nManfred Broy, editor, Construc\u00adtive Methods in Computing Science (NATO ASI Se\u00adries, Vol. F55), pages \n5 42. Springer-Verlag, 1988. [Bir89] Andrew Birrel. An introduction to programming with threads. SRC \nReport 35, Digital Equipment Corporation, January 1989. [CM90] Eric Cooper and J. Gregory Morrisett. \nAdding threads to standard ML. Technical Report CMU\u00adCS-90-186, Carnegie Mellon University Computer Science \nDepartment, December 1990. [CR+, 911., William Clinger. Jonathan Rees, et al. Revised4 . report on the \nalgorithmic language Scheme. Lisp Pointers, 4(3), 1991. [Dij68] E. W. Dijkstra. Co-operating sequential \npro\u00adcesses. In F. Genuys, editor, Programming Lan\u00adguages (NATO Advanced Study Institute), pages 43-112. \nLondon: Academic Press, 1968. [DR76] John Darlington and R. M. Burstall. A system which automatically \nimproves programs. Acts Informat\u00adica, pages 41 60, 1976. [For91] Alessandro Forin. Futures. In Peter \nLee, edi\u00adtor, Topics in Advanced Language Implementation, pages 219 241. MIT Press, 1991. [GJ90] David \nGelernter and Suresh Jagannathan. Progrmn\u00adming Linguistics. MIT Press, 1990. [GLJ93] Andrew Gill, John \nLaunchbury, and Simon L. Pey\u00adton Jones. A short cut to deforestation. In Func\u00adtional Programming and \nComputer Architecture, 1993. [Ha185] Robert Halstead. Multilisp: A language for concur\u00adrent symbolic \ncomputation. ACM Transactions on Programming Languages and Systems, pages 501\u00ad528, October 1985. [Hoa74] \nC.A.R. Hoare. Monitors: An operating system structuring concept. Communications of the A CM, 17(10):549-557, \n1974. [Hoa85] C.A.R. Hoare. Communicating Sequential Pro\u00adcesses. Prentice-Hall, 1985. [Hug84] R. J. M. \nHughes. Parallel functional languages use less space. Technical report, Oxford University Pro\u00adgramming \nResearch Group, 1984. [Hug90] R. J. M. Hughes. Why functional programming matters. In David Turner, editor, \nResearch Topics in Functional Programming, pages 17 42. Addison Wesley, 1990. [Mi187] James S. Miller. \nMulti Scheme: A parallel process\u00ading system based on MIT Scheme. Technical Report MIT/LCS/TR-402, MIT \nLaboratory for Computer Science, September 1987. [Mi189] Robin Milner. Communication and Concurrency. \nPrentice-Hall, 1989. [PA85] Keshav Pingali and Arvind. Efficient demand\u00addriven evaluation (I). ACM Transactions \non Pro\u00adgramming Languages and Systems, 7(2):311-333, April 1985. [PA86] Keshav Pingali and Arvind. Efficient \ndemand. driven evaluation (II). ACM Transactions on Pro\u00adgramming Languages and Systems, 8(1):109-139, \nJanuary 1986. [Pet77] James L. Peterson. Petri nets. ACM Comput$ng Surveys, 9(3) :223-250, 1977. [Pey87] \nSimon L. Peyton Jones. The Implementation of Functional Programming Languages. Prentice-Hall, 1987. [P1081] \nGordon D. Plotkin. A structural approach to oper\u00adational semantics. Technical Report-DAIMI FN -19, Aarhus \nUniversity Computer Science Department, September 1981. [Rep91] J. H. Reppy. CML: A higher-order concurrent \nlan\u00adgua~e. In Proceedings of the SIGPLAN 91 Confer\u00adence on Programming Language Design and Imple\u00admentation., \npages 293 305, 1991. [Tur79] D. A. Turner. A new implementation for applica\u00adtive languages. Software \n Practice and Experience, 9:31-49, 1979. [Tur94] Franldyn Turbak. Slivers: Computational Mod. -,, .. \n. uiarity via $ynchronazed Lazy Aggregates. PhD thesis, Department of Electrical Engineering and Computer \nScience, Massachusetts Institute of Technology, February 1994. Accessible from http: //wwu-sui.ss. al \n.mit. edul-lynislivers. Mml. Also to appear as MIT Artificial Intelligence Labo\u00adratory AI-TR-1466. [Wad84] \nPhilip Wadler. Listlessness is better than laziness: Lazy evaluation and garbage collection at compile\u00ad \ntime. In ACM Symposium On Lisp and Functional Programming, pages 45-52, 1984. [Wad88] Philip Wadler. \nDeforestation: Transforming pro\u00ad grams to eliminate trees. In 2nd European Sympo\u00adsium on Programmmg, \npages 344 358, 1988. Lec\u00adture Notes in Computer Science, Number 300. [Wat90] Richard C. Waters. Series. \nIn Guy L. Steele Jr., editor, Common Lisp: The Language, pages 923 955. Digital Press, 1990. [Wat91] \nRichard C. Waters. Automatic transformation of series expressions into loops. ACM Tram-a ct ions on Programmmg \nLanguages and Systems, 13(1):52-98, January 1991. [Wil] Paul R. Wilson. Uniprocessor garbage collection \ntechniques. ACM Computing Surveys (to appear). \n\t\t\t", "proc_id": "232627", "abstract": "Our purpose is to promote a second-class mechanism --- the synchronization barrier --- to a first-class value. We introduce the synchron, a novel synchronization mechanism that enables the coordination of a dynamically varying set of concurrent threads that share access to a first-class synchronization token. We demonstrate how synchrons can be used to modularly manage resources in cases where existing techniques are either inapplicable or non-modular. In particular, <i>synchronized lazy aggregates</i> enable the first space-efficient aggregate data decomposition of a wide range of algorithms. We also introduce <i>explicit-demand graph reduction</i>, a new semantic framework that we have developed to describe concurrency and explain the meaning of a synchron rendezvous.", "authors": [{"name": "Franklyn Turbak", "author_profile_id": "81339533353", "affiliation": "Wellesley College Computer Science Department", "person_id": "PP39080406", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/232627.232645", "year": "1996", "article_id": "232645", "conference": "ICFP", "title": "First-class synchronization barriers", "url": "http://dl.acm.org/citation.cfm?id=232645"}