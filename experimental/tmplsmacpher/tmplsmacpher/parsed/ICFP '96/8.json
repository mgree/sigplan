{"article_publication_date": "06-15-1996", "fulltext": "\n Analysis and Caching of Dependencies Martin Abadi Butler Lampson Jean-Jacques LI%y Digital Systems Research \nCenter Microsoft INRIA Rocquencourt ma@pa.dec. com blmpson@microsoft. com Jean-Jacques. Levy@inria. f \nr Abstract We address the problem of dependency analysis and caching in the context of the ~-calculus. \nThe dependencies of a J\u00adterm are (roughly) the parts of the J-term that contribute to the result of evaluating \nit. We introduce a mechanism for keeping track of dependencies, and discuss how to use these dependencies \nin caching. 1 Introduction Suppose that we have evaluated the function application f(l, 2), and that \nits result is 7. If we cache the equality .f(l, 2) = 7, we may save ourselves the work of evaluating \n~(1, 2) in the future. Suppose further that, in the course of evaluating f(l, 2), we noticed that the \nfirst argument of f was not accessed at all. Then we can make a more general cache entry: ~(n, 2) = 7 \nfor all n. In call-by-name evalua\u00adtion, we may not even care about whether n is defined or not. Later, \nif asked about the result of j(2, 2), for example, we may match ~(2, 2) against our cache entry, and \ndeduce that f(2, 2) = 7 without having to compute ~. There are three parts in this caching scheme: (i) \nthe dependency analysis (in this case, noticing that j did not use its first argument in the course of \nthe computation); (ii) writing down dependency information, in some way, and caching it; (iii) the cache \nlookup. Each of the parts can be complex. However, the caching scheme is worthwhile if the computation \nof ~ is expensive and if we expect to encounter several similar inputs (e.g., f(l, 2), j(2, 2), . ). \nWe address the problem of dependency analysis and cach\u00ading in the context of the J-calculus. We introduce \na mecha\u00adnism for keeping track of dependencies, and show how to use these dependencies in caching. (However, \nwe stop short of considering issues of cache organization, replacement policy, etc. ) Our techniques \napply to programs with higher-order functions, and not just to trivial first-order examples like $(1, \n2). The presence of higher-order functions creates the need for sophisticated dependency propagation. \nAs an example, consider the higher-order function: f s kz.Ay.fst(z(j st(Y) )(mri(Y))) Permission to \nmake digitalh-mrd copy of pad or all of thb work for personal or olassroom use is grantad without fee \nprovided that mpiaa am not made or distributed for profit or commercial advanta e the copyright notioe, \nthe Me of the ublication and its date appear, an % noboe ISgiven that oopying is ! y permission of ACM, \nInc. To copy othatwisa, to republish, to post on esivets, or to redsbibute to lists, mquwes prior epeoilic \npermission andlor a fee. ICFP 96596 PA, USA 01996 ACM 0-69791 -771 -5/96/0005 ...$3.50 where pairs are \nencoded as usual: (a, b) 2 kz.z(a)(b) snd ~ Ap.p(Ju.Az.z) The function ~ takes two arguments z and y; \npresumably z is a function and y is a pair. The function ~ applies x to the first and second components \nof y, and then extracts the first component of the result. A priori, it may seem that f depends on z \nand on all of y. Consider now the following arguments for f: 9 s Au.xz.(z,u) r k (1,2) 9 ~ AU.AZ.(Z, \n(u, Z)) r 2 (2,2) Both functions g and g seem to depend on their respective arguments. However, all \nthese a priori expectations are too coarse. After evaluating f(g) (r) to 2, we can deduce that f(g )(r \n) also yields 2. For this we need to exprees that ~ accessesonly part of the pair that g produces, that \ng accesses only part of the pair that ~ feeds it, and that g and g look sufficiently similar. We develop \na simple way of capturing and of exploiting these fairly elaborate dependencies. Our approach is based \non a labelled A-calculus [L6v78]. Roughly, our labelled A-calculus is like a ~-calculus with names for \nsubexpressions. In the course of computation, the names propagate, and some of them end up in the result. \nIf a reduces to v, then w will contain the names of the subex\u00adpressions of a that contribute to producing \nv. Then, if we are given a that coincides with a on those subexpressions, we may deduce that a reduces \nto v. In our example, we would proceed as follows. First, when given the expression f(g) (r), we would \nlabel some of its sub expressions. The more labels we use, the more infor\u00admation we obtain. In this example, \nwhich is still relatively simple, we label only components of g and r: where eo, el, ez, and es are distinct \nlabels. We extend the reduction rules of the A-calculus to handle labels; in this case, f(j)(;) reduces \nto eO:es:2. Stripping off all the labels, we can deduce that f(g)(r) reduces to 2. Studying the la\u00adbels, \nwe may notice that el and ez do not appear in the result. As we will prove, this means that f (g*) (r \n) reduces to 2 for any expressions g* and r* of the forms: 83 Obviously, g and r match this pattern, \nand hence f(g )(r ) reduces to 2. As this small example suggests, our techniques for dependency analysis \nare effective, reasonably efficient, and hence potentially practical. In the next section we review the \nbackground for our work and some related work. In section 3, we study de\u00adpendency analysis and caching \nin the pure A-calculus. In sections 4, we extend our techniques to a more realistic lan\u00adguage; this language \nincludes records and has a weak oper\u00adational semantics based on explicit substitutions [ACCL91, Fie90]. \n2 Motivation and Related Work The motivation for this work arose in the context of a system\u00admodelling \nsystem called Vest a [LM93, HL93] roughly a re\u00adplacement for tools like make and rcs. In Vesta, the ana\u00adlogue \nof a makefile is a program written in a specialized, un\u00adtyped, higher-order, functional, lazy, interpreted \nlanguage. The functional character of the language guarantees that the results of system building are \npredictable and reproducible. In Vesta, the basic computation steps are expensive calls to functions \nlike compile and link; hence it is important to avoid unnecessaryy recomputations. The programs can be \nreasonably large; it is therefore desirable to notice cache hits for large subexpressions rather than \nfor individual calls to primitives (e.g., individual compilations). Furthermore, irrelevant changes in \nparameters are expected to be frequent; when there are such changes, a simple memoisation [Mic68, Hug85] \ndoes not suffice for avoiding recomputations, and a more savvy caching strategy is necessary. This paper, \nhowever, is not about Vesta. There has been some research on caching in Vesta [HL93], and more is cur\u00adrently \nin progress. Here we discuss techniques for the A\u00adcalcuIus; these are somewhat simpler, easier to explain, \nand perhaps of more general interest. In the J-calculus, the work that seems most closely re\u00adlated to \nours is that of Field and Teitelbaum [FT90]. They have investigated the problem of reductions of similar \nex\u00adpressions (which may not even yield the same result). Their approach is based on a A-calculus with \na new {fork)) primi\u00adtive (A) rather than on a labelled J-calculus. For example, they can represent the \ntwo similar expressions b(a) and b (a) as the single expression A (b, b )(a), with a rule for duplica\u00adtion, \nnamely A(b, b )(a) = A(b(a), b (a)). Their algorithm seems particularly appropriate for dealing with \npairs of ex\u00adpressions that differ at only one or a few predictable subex\u00adpressions, There has been much \nother work on incremental com\u00adputation, and some of it is related to ours. In particular, Pugh s dissertation \nconcerns incremental evaluation of func\u00adtional programs; it raises the issue of caching for functions \nthat do not depend on all of their arguments [Pug88, pp. 70 71]. Dependency analysis is also similar \nto traditional anal\u00adyses such as strictness analysis (e.g., [BHA86]). There is even a recent version \nof strictness analysis that relies on a labelled A-calculus [GvS95]. Strictness analysis is con\u00adcerned \nwith wh at parts of a program must be evaluated; in contrast, for doing cache lookups, we need to know \nwhat parts of a program may affect the result. Furthermore, we do not use approximate abstract interpretations, \nbut rather rely on previous, actual executions of programs similar to the one being analyzed. 3 Dependencies \nin the Pure J-calculus In this section we consider incremental computation in the context of the pure \nA-calculus. This is a minimal system, but it enables us to illustrate our ideas. First we review some \nclassical results that suggest an approach to depen\u00addency analysis; then we describe a labelled calculus, \na basic scheme for caching, some examples, and a more sophisti\u00adcated scheme for caching; finally, we \nconsider call-by-value evaluation. 3.1 The J-calculus The standard A-calculus has the following grammar \nfor ex\u00adpressions: a,b, c ::== terms variable (m E V) kz.a abstraction (z E V) lx b(a) applicationI where \nV is a set of variables. The ~ rule is, as usual: (Az.b)a -+ b{a/x) where b{a/z} is the result of replacing \nz with a in b. When C is a context (a term with a hole), we write C {a} for the result of filling C s \nhole with a (possibly with variable captures). We adopt the following congruence rule: a+b C {a} -+ C \n{b} The reduction relation -+* is the reflexive, transitive closure of ~. A computation stops when it \nreaches a normal form. We can now reformulate the problem posed in the intro\u00adduction. Suppose that a \nis a term and a ~ v. When can we say that b +* w simply by comparing a and b? In order to address this \nquestion, we recall a few known theorems. Theorem 1 (Church-Rosser) The calculus is confluent. Theorem \n2 (Normalization) If a term can be reduced to a normal form, then its lefimost outermost reduction (whzch \nreduces the leftmost outermost redex at each step) reaches this normal form. Clearly the leftmost outermost \nreduction reduces only subex\u00adpressions necessary to get to the normal form. A prefix is an expression \npossibly with several missing sub expressions: a,b, c ;;== prefixes hole variable (x c V) 1\u00ad ~ ~~a~ abstraction \n(Z E W application A prefix a is prefix of another prefix (or expression) b if a matches b except in \nsome holes; we write a < b. For instance, we have that .(z)(_) (Ay. -(y)) < y(z)(z) (Jy. _(y)). For the \npurposes of reduction, we treat _ like a free variable; for example, (}z.z(-))(a) ~ a(.). The following \nthree results concern the prefix ordering and reduction: 84 Proposition 1 (Maximality of terms) If \nb ~ d and b M a term, then b = d. Proposition 2 (Monotonicity) If a, b, and c are pre\u00adfixes, a ~ b, and \na ~ c, then there exists a prefix d such that c+ d and b~ d. Theorem 3 (Stability) If a is a term, v \nis a term in nor\u00ad mal form, and a -+* v, then there w a minimum prefix ao < a such that ao-+ v. Proof \nThe stability theorem follows from the stability of Bohm trees [Ber78]. Here we sketch a simple, alternative \nproof. First we show that if a and b are compatible (have a common upper bound) in the prefix ordering, \nand a and b reduce to a term v in normal form, then the greatest lower bound of a and b (written a A \nb) also reduces to v. The proof is by induction on the lengths of the leftmost outermost reductions to \nv, and secondarily on the sizes of a and b. We proceed by cases on the form of a. Ifa=z, then v=xandb=x, \nso(ci Ab) =v.  If a = Jz.al, then b is of the form kc.bl, with al and bl compatible. The result follows \nby induction hypothesis.  Ifa = z(al) . . . (an), then bis of the form x(bl). . .(bw) with ai and b, \ncompatible for each z E 1..n. The result follows by induction hypothesis.  The cases where a = -or a \n= -(al) (an) are impos\u00adsible, since a reduces to a term in normal form.  Finally-, if a = (Az.al)(az) \n. . . (a~), then b is of the form (Az.bl )(bz) . . . (b~). Let a = ~l{;i/z}(a3). . . (an) and b = b,{ \nb,/z}(bq) . . . (b~); a and b are compatible, and they reduce to v with shorter leftmost outermost reductions \nthan a and b. By induction hypothesis, a A b reduces to v. Since a A b reduces to a A b , we obtain that \na A b reduces to v by transitivity.  Now suppose that a and v are as indicated in the state\u00adment of \nthe theorem. The prefixes of a that reduce to v are compatible, since they have a as upper bound; their \ngreatest lower bound is the prefix ao described in the statement. 0 We can give a first solution to our \nproblem, as follows. Suppose that a -+ v and v is a term in normal form. Let ao be the minimum prefix \nof a such that ao +* w, given by Theorem 3. By Proposition 2, if ao is a prefix of b then b + v for some \nv such that v is a prefix of v ; by Proposition 1, v is v. Therefore, if ao is a prefix of b thenwe can \nreuse the computation a ** v and conclude that b +* v. It remains for us to compute ao. As we will show, \nthis computation can be performed at the same time as we eval\u00aduate a, and does not require much additional \nwork. Intu\u00aditively, we will mark every subexpression of a necessary to compute v along the leftmost outermost \nreduction. 3.2 A labelled A-calculus In order to compute minimum prefixes as discussed above, we follow \nthe underlined method of Barenckegt [Bar84], gen\u00aderalized by use of labels as in the work of Field, L6vy, \nor Maranget [Fie90, L6v78, Mar91]. Our application of this met hod gives rise to a new labelled calculus, \nwhich we de\u00adfine next. We consider a J-calculus with the following extended grammar for expressions: \na, b,c ::== terms as in section 3.1 I ~~~ Iabelled term (e c E) where E is a set of labels. There is \none new one-step reduction rule: (e: b)(a) --+ e:(b(a)) The essential purpose of this rule is to move \nlabels outwards as little as possible in order to permit ~ reduction. For ex\u00adample, (eo:(~x.z(z)))( el:y) \nreduces to eo:((~x.z(z))(el: y)) via the new rule, and then yields eo:((el :y)(el :y)) by the /3 rule. \nThere are clear correspondences between the unlabeled calculus and the Iabelled calculus. When a is a \nlabelled term, let strzp(a ) be the unlabeled term obtained by re\u00admoving every label in a . We have: \nProposition 3 (Simulation) Let a, b be terms, and let a , b be la belled terms. . If a * b , then strip(a \n) ~ strip(b ). If a = strip(a)) and a -+ b, then a +* b! for some b such that b = strip(b ). The labelled \ncalculus enjoys the same fundamental theo\u00adrems as the unlabeled calculus: confluence, normalization, \nand stability. The confluence theorem follows from Klop s dissertation work, because the labelled calculus \nis a regular combinatory reduction systems [K108o]; the labelled calcu\u00adlus is left-linear and without \ncritical pairs. The normal\u00adization theorem can also be derived from Klop s work; al\u00adternatively it can \nbe obtained from results about abstract reductions systems [GLM92], via O Donnell s notion of left systems \n[0 D77]. The proof of the stability theorem is sim\u00adilar to the one in [HL91]. 3.3 Basic caching Suppose \nthat a ~ v, where a is a term and v is its normal form. Put a different label on every sub expression \nof a, obtaining a labelled term a . By Proposition 3, a +* V for some v such that v = strip(v ). Consider \nall the labels in v ; to each of these labels corresponds a subterm of a and thus of a. Let G(a) be a \nprefix obtained from a by replacing with . each subterm whose label does not appear in v . We can prove \nthat G(a) is well-defined. In particular, the value of G(a) does not depend on the choice of a or v ; \nand if the label for a subterm of a appears in v then so do the labels for all subterms that contain \nit. When a - v, we may cache the pair (G(a), v). When we consider a new term b, it is sufficient to check \nthat G(a) < b in order to produce v as the result of b. As G(a) is the part of a sufficient to get v \n(what we called ao in section 3.1), we obtain the following theorem: Theorem 4 If a is a term, v is a \nterm in normal form, a 4* V, and G(a) ~ b, then b+ v. Theorem 4 supports a simple caching strategy. In \nthis strategy, we maintain a cache with the following invariants: the cache is a set of pairs (a., v), \nconsisting each of an unlabeled prefix CLOand an unlabeled term w in normal form;  if(ao,v)isinthe cacheand \na.~bthen b+* v.  Therefore. whenever we know that IJ is the normal form of a, we may add to the cache \nthe pair (G(a), v). Theorem 4 implies that this preserves the cache invariant. Suppose that a is a term \nwithout labels. In order to evaluate a, we do: if there is a cache entry (ao, v) such that a. ~ a, then \nreturn v; otherwise: let a be the result of adding distinct labels to a, at every subexpression; suppose \nthat, by reduction, we find that a -+* v for v in normal form; let v = strzp(v ) and ao = G(a);  optionally, \nadd the entry (CZO,v) to the cache; return v. Both cases preserve the cache invariants. In both, the \nv returned is such that a -+* v. In a refinement of this scheme, we may put labels at only some sub expressions \nof a. In this case, we replace with . a subexpression of a only if this subexpression was labelled before \nreduction. The more labels we use, the more general the prefix obtained; this results in better cache \nentries, at a moderate cost. However, in examples, we prefer to use few labels in order to enhance readability. \nAnother refinement of the scheme consists in caching pairs of labelled prefixes and results. The advantage \nof not stripping the labels is that the cache records the precise de\u00adpendencies of results on prefixes. \nWe return to this subject in section 3.5.  3.4 Examples The machinery that we have developed so far \nhandles the example of the int reduction (the term f(g)(r)). We leave the step-by-step calculation for \nthat example as an exercise to the reader. As that example illustrates, pairing behaves nicely, in the \nsense that fst(a,b) depends only on a, as one would expect. As a second example, we show that the Church \nbooleans behave nicely too. The encoding of booleans is as usual: true ~ kz.~y,z false 4 }z.Ay.y if a \nthen b else c ~ a(b)(c) In the setting of the labelled J-calculus, we obtain as a de\u00adrived rule that: \nif (e:a) then b else c ~ e:(if a then b else c) It follows from this rule that, for example, (k. if eo:z \nthen el:y else ez:z)(e~:true) +* eo:e~:el:y We obtain the unlabeled prefix: (Az. if z then y else. )(true) \nand we can deduce that any expression that matches this prefix reduces to y. Similar examples arise in \nthe context of Vesta (see sec\u00adtion 2 and [HL93] ). A simple one is the term: (if isC(Jile) then Ccompile \nelse 143compile)(jile) where is C (~) returns true whenever ~ is a C source file, and file is either \na C source file or an M3 source file. If is C(jile) returns true, then that term yields Ccompile (file). \nUsing labels, we can easily discover that this result does not depend on the value of M3compde, and hence \nthat it need not be recomputed when that value changes. In fact, even is C (file ) and the conditional \nneed not be reevaluated. In a higher-order variant of this example, the conditional compilation function \nis passed as an argument: (Ax. z(jile)) (Ay. if zsC(y) then Ccornpile(y) else M3compile(y)) Our analysis \nis not disturbed by the higher-order abstrac\u00adtion, and yields the same information. 3.5 Limit at ions \noft he basic caching scheme The basic caching scheme of section 3.3 has some limitations, illustrated \nby the following two concrete examples. Suppose that we have the cache entry: ((k.(snd(z),fst( a))) ((true, \nfalse)), (false, true)) Suppose further that we wish to evaluate the term: ~st((ku. (snd(a), ~st(z)))((true, \nfalse))) Immediately the cache entry enables us to reduce this term to fst ( (false, true))), and eventually \nwe obtain false. How\u00adever, in the course of this computation, we have not learned how the result depends \non the input. We are unable to make an interesting cache entry for the term we have evaluated. Given \nthe new, similar term fst((k.(snd(z), ~st(z)))((false, false))) we cannot immediately tell that it yields \nthe same result. As a second example, suppose that we have the cache entry: (if true then true else -, \ntrue) and that we wish to evaluate the term: not (if true then true else true) In our basic caching scheme, \nwe would initially label this term, for example as: not(if true then eo:true else el :true) Then we would \nhave to reduce this term, and as part of that task we would have to reduce the subterm (if true then \neo:true else el :true) At this point our cache entry would tell us that the sub\u00adterm yields true, modulo \nsome labels. We can complete the reduction, obtaining false, and we can make a trivial cache entry: \n(not(if true then true else true), false) However, we have lost track of which prefix of the input determines \nthe result, and we cannot make the better cache entry: (not(if true then true else.), false) The moral \nfrom these examples is that cache entries should contain dependency information that indicates how each \npart of the result depends on each part of the input. One obvious possibility is not to strip the labels \nof prefixes and results before making cache entries; after all, these la\u00adbels encode the desired dependency \ninformation. We have developed a refinement of the basic caching scheme that does precisely that, but \nwe omit its detailed description in this paper, Next we give another solution to the limitations of the \nbasic caching scheme.  3.6 A more sophisticated caching scheme In this section we describe another caching \nscheme. This scheme does not rely directly on the labelled ~-calculus, but it can be understood or even \nimplemented in terms of that calculus. With each reduction a - v of a term a, we associate a function \nd from prefixes of v to prefixes of a. Very roughly, if V. is a prefix of a then G!(vo) is the prefix \nof a that yields vo in the reduction a -* v. We write a -~ v to indicate the function d. This annotated \nreduction relation is defined by the following rules. Reflexivity y: a 4~,~ a  where id is the identity \nfunction on lprefixes.  Transitivity: a~~b b+?, C  a +d<; d) c where d ; d is the function composition \nof d and d. Congruence: Given a function d from prefixes of b to prefixes of a, we define a function \nC{d} from prefixes of C {b} to prefixes of C{a}. If co ~ C then C{d}(co) = co; otherwise, there exists \na unique b. s b such that co = C{bO}, and we let C{d}(co) := C{d(bo)}, We obtain the rule: a+~b C{a} \n&#38;{d} C{b} 6: (Xc.b)(a) +jfi b{a/z} where alp(-) = _ and, for co # _ and co < b{a/x}, d~ (co) = \n(kz.bo)(ao) where a. and 60 are the least prefixes such that a. < a, b. ~ b, and co ~ bo{ao/z}, These \nrules are an augmentation of the reduction rules of section 3.1, in the following sense: Proposition \n4 Ifa-~bthenah*b. If a~ b then a +: bfor some d. The rules may seem a little mysterious, but they can \nbe understood in terms of labels. Imagine that every subex\u00ad pression of a is labelled (with an invisible \nlabel), that a ~j v, and that V. < v; then d(vo) is the least prefix of a that contains all of the labels \nthat end up in V.. As an example, consider the term (kr.z(z)) (a), where a is arbitrary. By ~, we have \n(k.z(z))(a) +jp a(a) where dp is such that, for instance, dp (_ ) is _, dp (a(a)) is the entire (kz.z(z))(a), \nand d6(a(_)) is (kc.z(_))(a). If we had labelled the initial term (Ax.z (z))(a) before reduction, then \nthe labels that would decorate the result prefix a(-) would be all those of the initial term except for \nthe label of the argument occurrence of x; this justifies that d6 (a(.)) be (kc.z(-))(a). We obtain: \n Theorem 5 If a is a term, a +; v, and d(v) < b, then b +: V. This theorem gives rise to a new caching \nscheme. The cache entries in this scheme consist of judgments a ~j v, where a and v are terms and d is \na function from prefixes of v to prefixes of a. The representation of d can be its graph (i.e., a set \nof pairs of prefixes) or a formal expression (written in terms of id, dd, etc.); it can even be the pair \nof a Iabelling of a and a corresponding labelling of v. According to the theorem, whenever we encounter \na term b such that d(v) ~ b, we may deduce that b -+: v. This caching scheme does not suffer from the \nlimitations of the basic one. In particular, each cache entry contains dependency information for every \npart of the result, rather than for the whole result. Moreover, the rules of inference provide a way \nof combining dependency information for sub\u00adcomputations; therefore, we can make an interesting cache \nentry whenever we do an evaluation, even if we used the cache in the course of the evaluation. 3.7 Call-by-value \nSo far, we have considered only call-by-name evaluation. Here we define a call-by-value version of the \nlabelled A\u00adcalculus, showing that we can adapt our approach to call\u00adby-value evaluation. The move from \na call-by-name to a call-by-value labelled J-calculus does not affect the basic caching scheme of section \n3.3, which remains sound. The syntax of the call-by-value labelled J-calculus is that given in section \n3.2. The ~ rule is restricted to: (kz.b) w + b{v/z} where v ranges over terms of the form z, c(al ) \n. . . (an), or Ax.a; such terms are called values, As in section 3,2, we have a rule for moving labels \noutwards from the left-hand side of applications: (e:b)(a) + e:(b(a)) We have an additional rule for \nthe right-hand side of appli\u00adcations: (kn.b)(e:a) ~ e:((kz.b)(a)) One might be tempted to adopt a stronger \nrule, namely b(e:a) e e:(b(a)), but this rule creates critical pairs. With call-by-name evaluation, the \nterm (Jx .b)(a) does not depend on a if x does not occur free in b. In particular, any term that has \n(Jz.b) (-) as prefix yields the same result. With call-by-value evaluation, on the other hand, the be\u00adhavior \nof (Xc.b) (a) depends on a: it depends on whether a can be reduced to a value or not. Therefore, a term \nmay match the prefix (Az. b)(. ) but not yield the same result as (k. b)(a). The treatment of labels \nin our rules for call-by-value takes into account that (kz.b) (a) depends on a even if z does not occur \nfree in b. Suppose that a can be reduced to a value u. To see how (Xz.b) (a) depends on a, we add a label \nin front of a, obtaining (Az.b) (e:a). Labelled reduc\u00adtion yields e:((kr.b)(a)), then e:((k.b)(v)), and \nfinally e:b. The label e does not disappear, as it would in call-by-name evaluation. For instance, let \nus consider the case where a is the value y(z). From a labelled reduction we obtain the pre\u00adfix (Az.b)(_ \n(-)). Any term that matches this prefix reduces to the same result as (kz.b) (y(z)). In fact, the evaluation \nof (k. b)(g(z)) reveals that this term depends on the vag\u00adueness of y(z), but does not depend on y(z) \nin any other way. The prefix (kr. b) (-(. )) does not express this informa\u00adtion with full accuracy, but \napproximates it. 4 Dependencies in a Weak A-calculus with Records The techniques developed in the previous \nsection are not limited to the pure J-calculus. In this section, we demon\u00adstrate their applicability \nto a more realistic language, with primitive booleans, primitive records, and explicit substitu\u00adtions. \nThe operational semantics of this language is weak (so function and record closures are not reduced). \n 4.1 A weak calculus with records We consider an extended A-calculus with the grammar for terms and for \nsubstitutions given in Figure 1. In the gram\u00ad mar, L is a set of names (field names for records) and \neke is a keyword used for else clauses in records and in substi\u00ad tutions. As we show below, these else \nclauses are useful in dependency analysis. We typically think of the else clauses as corresponding to \nrun-time errors (missing fields, unbound variables). The term a~+l in an else clause can be arbitrary; \na term that represents a run-time error will do. We use the following notation for extending substitu\u00ad \ntions. Let s be Z1 = al, ....z~ = an, eke = a~+l; then (z=a)si sz=a,zl= al,..., zw=an,e ~s,e=an+lif~ \nis not among the variables zl, . . . . xn, and It 1Sx = a, Z1 = al, . . ..zl l = at l, xt+l = at+l, . \n..j x% = an, else = a.+1 if zisz~. The one-step reduction rules now use explicit substitu\u00ad tions. They \nare given in Figure 2. In particular, the ana\u00ad logue for the /3 rule is ((k. b)[s])a --+ b[(z = a) s], \nwhich extends an explicit substitution; the replacement of a for z happens gradually, through other rules \nwhich push the substitution inwards in b. An active context is a context generated by the grammar of \nFigure 3. We adopt the following congruence rule: for any act ive context C, a-.ib C{a} ~ C{b} Notice \nthat this rule allows us to compute inside substitu\u00adtions, but not under A, inside records, or in the \nterm part of closures (since kr.c, (. ... li = C , . . .), (. ... else = c), and C[s] are not listed \nas active contexts). The relation ~ is the reflexive, transitive closure of ~. The prefix ordering for \nthis language is interesting. Let s be the substitution X1 = al, . . . Xm = 1 am, e se = an+l~ let r \nbe the record (11 = al, ..., L = an, else = a~+l). We associate with s and r the following functions \nfrom variables or field names to prefixes: ifz= z, for somei [s] (z) = { ~~+1 otherwise if 1= ltfor somei \n[r] (1) = { ~~+1 otherwise Intuitively, [s](z) is the image of $ through S, and [T ](~) is the image \nof 1 through r. The prefix ordering is as before except for substitutions and records where s < s if \n[s](z) ~ [s ](z) for all z ~ V, and r < r if [r](l) < [r ](l) for all .-r le.b. According to this definition, \nthe order of the components of substitutions and records does not matter. In addition, we obtain that, \nif the else clause has a hole, then any other holes can be collapsed into it; for example, the prefixes \n(1I = a,12 = .,else = _), (zS = .,11 = a,else = -), and (11 = a, else = -) are all equivalent. This J-calculus \nenjoys the same theorems as the pure A-calculus of section 3.1 (modulo that now < is actually a pre-order, \nnot an order). These theorems should not be taken for granted, however. Their proofs are less easy, but \nthey can be done by using results on abstract reduction sys\u00ad tems [GLM92]. The stability theorem ensures \nthat there is a minimum prefix for obtaining any result; moreover, the maximality and monotonicity propositions \nare the basis for a caching mechanism. Finally, we should note that, in this calculus, closures may contain \nirrelevant bindings. For example, consider the function closure (}Y.Y) [z = Z, else = W], where z is \na vari\u00ad able and w is an arbitrary normal form. This closure reduces only to itsel~ the irrelevant substitution \ndoes not disappear. In this case, we will consider that the result depends on the substitution. We could \nadd rules for minimizing substitu\u00ad tions but, for the sake of simplicity, we do not.  4.2 A weak Iabelled \ncalculus with records Following the same approach as in section 3, we define a labelled calculus: a,b, \nc ;:== terms . . . as in section 4.1 I e:a labelled term (e ~ E) There are new one-step reduction rules \nin addition to those of section 4.1: (e: b)(a) -+ e:(b(a)) (e: b)[s] + e:(b[s])  (e:b).1 -+ e:(b.1) \nif (e:a) then b else c ~ e:(if a then b else C) The grammar for active contexts is extended with one \nclause: c ::== active contexts as in section 4.1 e~d labelled context (e ~ E) I a,b, c ::== terms variable \n(z G V) / :;; abstraction (z e V) application I a [s] closure [ $;=al,... , lm = an, else = an+l) record \n(1, E L, distinct) selection (1 G L) 1 true true I false false if a then b else c conditional s .. \nxl = al,... , Xn = an, else = an+l substitutions (W E V, distinct) Figure 1. Grammar for the weak A-calculus. \n ZIZ1 =al, . . . ,zn = an, eke= am+l] ZIZI = al, . . ..~n =an, else =an+l] + (b(a)) [s] + (( Ax.~j[;/: \n+ + ((11 =al,... , lm = an, else= an+l)j[s].i + ((n= al,... , in = an, else= an+~))[s].l + true[s] false[s] \n(if a then b else c)[s] if true then b else c if false then b else c Figure 2. One-step reduction rules \nfor ai (X= Xi) an+l (z # all z,) b[s](a[s]) b[(z = a) os] (b[s]).1 (1= 1,) a~[sl an+l [s] (1 # all 1,) \ntrue false if a[s] then b[s] else C[S] b c the weak J-calculus. c .. ., active cent exts hole ~(a) \napplication (left) b(C) apphcation (right) a[S] closure C.1 selection (1 C L) [ if C then b else c conditional \n(guard) if a then C else c conditional (then) I if a then b else C conditional (else) S ::== xl=alj. \n... x~=C~xn =an, else else =an+l substitutions I zl= al,... ,x. =am, else=C (~i c V, distinct) Figure \n3, Grammar for active contexts for the weak A-calculus. As usual, the congruence rule permits reduction \nin any ac\u00ad tive context: a~b C{a} + C{b} for any active context C.  4.3 Dependency analysis and caching \n(by example) The labelled calculus provides a basis for dependency anal\u00adysis and caching. The sequence \nof definitions and results would be much as in section 3. We do not go through it, but rather give one \ninstructive example. We consider the term: (( Jz. z.11)(11 = YI,L2 = Y.Z,else= w)) [yl zl, y2 = z2, else= \nw] This term yields Z1. We label the term, obtaining: ((h. z.11)(11 = (e~:~~), 12= (e2:y-2), else = (es:w))) \n[y, = (e4:z~), y2 = (e,:z2), else = (e~:w)] This labelled term yields el :eA:.z1, so we immediately con\u00adclude \nthat the following prefix also yields zl: ((~z.z.il)(ll = y1,12 = -, else= -)) [Yl=zl, y2=-, else=.] \nThanks to our definition of the prefix ordering, this prefix is equivalent to: (( Az. z.11)(11 = v1, \neke= _))[yI = 21, else= -.] Suppose that, in our cache, we record this prefix with the associated result \nZ1; and suppose that later we are given the term: (( Jx. z.11)(11 = Y1,13 = Y17(Y17), else = w )) [Y17= \nzl,~l = .2a,eke= w ] This term matches the prefix in the cache entry, so we im\u00admediately deduce that \nit reduces to Z1. As this example illustrates, the labelled reductions help us identify irrelevant components \nof both substitutions and records. The prefix ordering and the use of else then allow us to delete those \nirrelevant components and to add new irrelevant components. In some applications, irrelevant components \nmaybe com\u00admon. For example, in the context of Vests, a large record may bundle compiler switches, environment \nvariables, etc.; for many computations, most of these components are irrel\u00adevant. In such situations, \nthe ability to detect and to ignore irrelevant components is quite useful it means more cache hits. 5 \nConclusions We have developed techniques for caching in higher-order functional languages. Our approach \nrelies on using depen\u00addency information from previous executions in addition to the outputs of those \nexecutions. This dependency informa\u00adtion is readily available and easy to exploit (once the proper tools \nare in place); it yields results that could be difficult to obtain completely statically. The techniques \nare based on a labelled ~-calculus and, despite their pragmatic simplicity, benefit from a substantial \nbody of theory. Acknowledgements We thank Yanhong Liu and Tim Teitelbaum for discussions about related \nwork; Allan Heydon, Jim Horning, Roy Levin, and Yuan Yu for discussions about Vestq Alain Deutsch and \nSimon Peyton Jones for comments on strictness analysis; and Georges Gonthier for assorted remarks on \na draft of this paper. References [ACCL911 M. Abadi. L. Cardelli. P.-L. Curien. and J.-J. L6vy. Explicit \nsubstitutions. Journal of Func\u00adtional Programming, 1(4) :375 416, 1991. [Bar84] Henk P, Barendregt. The \nLambda Calculus. North Holland, Revised edition, 1984, [Ber78] G. Berry. Stable models of typed lambda-calculi, \nIn Proc. 5th Coil. on Automata, Languages and Programming, Lectures Notes in Computer Sci\u00adence, pages \n72 89, Springer-Verlag, 1978. [BHA86] G. L. Burn, C. Hankin, and S. Abramsky. Strict\u00adness analysis for \nhigher-order functions. Science of Computer Programming, 7:249 278, 1986. [Fie90] John Field. On laziness \nand optimality in lambda interpreters: Tools for specification and analysis. In Proceedings of the Seventeenth \nAnnual ACM Symposium on Principles of Programming Lan\u00adguages, pages 1 15, 1990. [FT90] John Field and \nTim Teitelbaum. Incremental re\u00adduction in the lambda calculus. In Proceedings of the 1990 ACM Conference \non LISP and Func\u00adtional Programming, pages 307-322. ACM, 1990. [GLM92] Georges Gonthier, Jean-Jacques \nL6vy, and Paul-Andr6 Melli?x. An abstract standardisation the\u00adorem. In Seventh Annual IEEE Symposium \non Logic in Computer Science, 1992. [GVS95] Milind Gandhe, G. Venkatesh, and Amitabha Sanyal, Labeled \nJ-calculus and a generalised no\u00adtion of strictness. In Astan Computmg Science Conference, Lecture Notes \nin Computer Science. Springer-Verlag, December 1995, [HL91] G6rard Huet and Jean-Jacques L6vy. Compu\u00adtations \nin Orthogonal Term Rewriting Systems. MIT Press, 1991. [HL93] Chris Hanna and Roy Levin. The Vesta language \nfor configuration management, Research Re\u00adport 107, Digital Equipment Corporation, Sys\u00adtems Research \nCenter, June 1993. Available from http://www,research. digital. com/SRC. [Hug85] John Hughes. Lazy memo-functions. \nIn Jean-Pierre Jouannaud, editor, Functional Program\u00admmg Languages and Computer Architecture, pages 129 \n146, September 1985. [K108O] Jan Willem Klop. Combinatory Reduction Sys\u00adtems. PhD thesis, CWI, 1980. \n 90 [L&#38;r78] Jean-Jacques L&#38;Jy. Reductions Correctes et Op\u00ad timales versity clans le Lambda Calcul. \nof Paris 7, 1978. PhD thesis, Uni\u00ad [LM93] Roy Levin and Paul R. McJones. The Vesta approach to precise \nconfiguration of large software systems. Research Report 105, Digital Equipment Corporation, Systems \nRe\u00adsearch Center, June 1993. Available from http://www.research. digital. com/SRC. [Mar91] Luc Maranget. \nOptimal derivations in weak lambda-calculi and in orthogonal term rewriting systems. In Proceedings of \nthe Eighteenth Annual ACM Symposium on Principles of Progmmming Languages, 1991. [Mic68] D. Michie. Memo \nfunctions and machine ing. Nature, 218:19 22, 1968. learn\u00ad [0 D77] Michael scribed versity, O Donnell. \nby Equations. 1977. Computing in PhD thesis, Systems de-Cornell Uni\u00ad [Pug88] William Pugh. Incremental \nComputation and the Incremental Evaluation of Functional Programs. PhD thesis, Cornell University, 1988. \n  \n\t\t\t", "proc_id": "232627", "abstract": "We address the problem of dependency analysis and caching in the context of the &amp;lambda;-calculus. The dependencies of a &amp;lambda;-term are (roughly) the parts of the &amp;lambda;-term that contribute to the result of evaluating it. We introduce a mechanism for keeping track of dependencies, and discuss how to use these dependencies in caching.", "authors": [{"name": "Mart&#237;n Abadi", "author_profile_id": "81100547147", "affiliation": "Digital Systems Research Center", "person_id": "PP39047996", "email_address": "", "orcid_id": ""}, {"name": "Butler Lampson", "author_profile_id": "81100081662", "affiliation": "Microsoft", "person_id": "PP39026301", "email_address": "", "orcid_id": ""}, {"name": "Jean-Jacques L&#233;vy", "author_profile_id": "81100154846", "affiliation": "INRIA Rocquencourt", "person_id": "PP31084429", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/232627.232638", "year": "1996", "article_id": "232638", "conference": "ICFP", "title": "Analysis and caching of dependencies", "url": "http://dl.acm.org/citation.cfm?id=232638"}