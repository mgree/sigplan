{"article_publication_date": "06-15-1996", "fulltext": "\n The Role of Lazy Evaluation in Amortized Data Structures School of 5000 Forbes Computer Avenue, (e-mail: \nChris Okasaki* Science, Carnegie Mellon Pittsburgh, Pennsylvania, cokasaki@cs.cmu.edu) University USA \n15213 Abstract Traditional techniques for designing and analyzing amor\u00adtized data structures in an imperative \nsetting are of limited use in a functional setting because they apply only to single\u00adthreaded data structures, \nyet functional data structures can be non-single-threaded. In earlier work, we showed how lazy evaluation \nsupports functional amortized data struc\u00adt ures and described a technique (the banker s method) for analyzing \nsuch data structures. In this paper, we present a new analysis technique (the physicist s met hod) and \nshow how one can sometimes derive a worst-case data structure from an amortized data structure by appropriately \nschedul\u00ading the premature execution of delayed components. We use these techniques to develop new implementations \nof FIFO queues and binomial queues. Introduction Functional programmers have long debated the relative \nmer\u00adits of strict versus lazy evaluation. Although lazy evaluation has many benefits [11], strict evaluation \nis clearly superior in at least one area: ease of reasoning about asymptotic com\u00adplexity. Because of \nthe unpredictable nature of lazy eval\u00aduation, it is notoriously difficult to reason about the com\u00adplexit \ny of algorithms in such a language. However, there are some algorithms based on lazy evaluation that \ncannot be programmed in (pure) strict languages without an in\u00adcrease in asymptotic complexity. We explore \none class of such algorithms amortized data structures and de\u00adscribe techniques for reasoning about \ntheir complexity. Several researchers have developed theoretical frameworks for analyzing the time complexity \nof lazy programs [1, 19,20, 25]. However, these frameworks are not yet mature enough to be useful in \npractice. One difficulty is that these frame\u00adworks are, in some ways, too general. In each of these sys\u00adtems, \nthe cost of a program is calculated with respect to some cent ext, which is a description of the demand \non the This research was sponsored by the Advanced Research Projects Agency CSTO under the title The \nFox ProJect Advanced Languages for Systems Software , ARPA Order No. C533, Issued by ESC/ENS under Contract \nNo. F19628-95-C-0050. Permissionto makedigitabhard00PYof partorall of thisworkfor personal or classroom \nuse is ranted without fee provided that copies are not made i or distributed for pro t or commercial \nadvanta e, the oopyright notioe, Uw titie of the publication and its date appear, an$ notkx is given \nthat copying is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, or to \nredistribute to lists, requires prior specific permission andlor a fee. ICFP 96 !3 96 PA, USA O 1996 \nACM O-69791-771-519WOO05...$505O result of the program. However, this approach is often in\u00adappropriate \nfor a methodology of program development in which data structures are designed as abstract data types \nwhose behavior, including time complexity, is specified in isolation. Instead, we develop ad hoc, but \npragmatically useful, techniques for reasoning about the time complexity of lazy amortized data structures \nwithout regard to the con\u00adtexts in which the data structures will be used. A data structure is called \npersistent [5] if, after an up\u00addate, the old version of the data structure is still accessi\u00adble. A data \nstructure that is not persistent is called e@.ern\u00aderal. In functional programming terminology, an ephemeral \ndata structure is one that must be single-threaded [21] and a persistent data structure is one that may \nbe non-single\u00ad threaded. Aside from the obvious distinction regarding as\u00ad signments, persistence is the \nfundamental difference between functional and imperative data structures. Functional data structures \nare automatically persistent, whereas imperative data structures are almost always ephemeral. Traditional \ntechniques for designing and analyzing amortized data struc\u00adtures were developed for imperative data \nstructures and ap\u00adply only in the ephemeral case. Functional (and therefore persistent) amortized data \nstructures require different tech\u00adniques. In [16], we showed how lazy evaluation can be used to support \npersist ent amortized data structures, and described the banker s method, a technique for analyzing the \ntime complexity of such data structures. In this paper, we be\u00adgin by reviewing these previous results \nin a functional set\u00adting. Then, we describe the physicist s method, an alter\u00adnative technique for analyzing \nfunctional amortized data structures. The physicist s method is less powerful than the banker s method, \nbut is usually much simpler. Next, we show how one can sometimes derive a worst-case data structure from \nan amortized data structure by appropriately scheduling the premature execution of delayed components. \nThis technique requires both strict and lazy evaluation. Fi\u00adnally, after a brief discussion of related \nwork, we conclude with advice on designing amortized data structures. To illustrate our techniques, we \nintroduce several new implement ations of common data structures. In Section 3, we describe an extremely \nsimple FIFO queue requiring O(1) amortized time per operation. In Section 4, we show that binomial queues \nimplemented with lazy evaluation support insertion in only O(1) amortized time. Finally, in Section 5, \nwe adapt this implementation of binomial queues to support insertion in 0(1) worst-case time. We present \nsome source code in Haskell [10], and some in Standard ML [15]. Since Standard ML is strict, we extend \nthe language with the following primitives for lazy evalua\u00ad tion: type a susp val delay : (unit -> a) \n-> a susp val force : a susp -> a These primitives are actually supported by several imple\u00admentations \nof Standard ML.1 For clarity, we present only the relevant fragments of the source code for some data \nstructures. The complete implementations are included in Appendix A. 2 Amortization and Lazy Evaluation \nAmortization is a method of accounting for the cost ofse\u00adquences of operations [23]. The amortized cost \nof an indi\u00advidual operation is obtained by averaging the total cost of a worst-case sequence over all \nthe operations in the sequence. Given a bound on the amortized cost of an individual oper\u00adation, one \ncan calculate a bound for the cost of a sequence of operations by simply multiplying by the length of \nthe se\u00adquence. Cost can be measured in time, space, or any other resource of int crest, but in this paper \nwe will restrict our attention to running time as the sole measure of cost. In an amortized data structure, \ncertain operations are allowed to be more expensive than the desired bound, pro\u00advided they are balanced \nby a sufficient number of inexp en\u00adsive operations. Persistent data structures are problematic in this \nregard, since they allow expensive operations to be repeated arbitrarily often, To obtain meaningful \namortized bounds for persistent data structures, we must ensure that if z is some instance of a data \nstructure on which some oper\u00adation ~ is more expensive than the desired bound, then the first application \nof ~ to z may be expensive, but subsequent applications will not be. This is impossible under both call\u00adby-value \nand call-by-name since each application of ~ to z will take exactly the same amount of time. Of the three \nmajor evaluation orders, only call-by-need (i.e., lazy eval\u00aduation) supports the desired behavior. If \nz cent ains some delayed component that is demanded by f, then the first application of j to z will force \nthe (potentially expensive) evaluation of that component and memoize the result. Sub\u00adsequent applications \nmay then access the memoized result directly. Tarjan [23] describes two techniques for analyzing ephem\u00aderal \namortized data structures: the banker s method and the physicist s method. Both of these techniques account \nfor future expensive operations by prepaying. Whenever the amortized cost of an operation is greater \nthan the ac\u00adtual cost, the excess is saved to pay for future operations. Whenever the amortized cost \nis less than the actual cost, the deficit is made up from earlier savings. The two techniques differ \nin how they keep track of these savings. In the banker s method, the savings are represented as credits \nthat are associated with individual locations in the data structure. These credits are used to pay for \nfuture accesses to these locations. In the physicist s method, the savings are represented as potential \nthat is associated with the data structure as a whole. Inexpensive operations in\u00ad crease this potential \nand expensive operations decrease it. I It is ~o~~ible to imPlement these primitives in Standard ML us\u00ad \ning references and assignments, but not with the same degree of polymorphism. Unfortunately, neither \nof these techniques is appropri\u00ad ate for analyzing persistent data structures. An ephemeral (single-threaded) \ndata structure has only a single future, but a persistent (non-single-threaded) data structure may have \nmany futures, one for each thread. Then, the whole idea of saving credits (or potential) for future use \nbreaks down because each of those futures may need the same credits. To cope with persistent data structures, \nwe base our analyses on debt, rather than savings, where debt accounts for the cost of delayed computations. \nThe intuition is that, although savings cannot be spent more than once, it does no harm to pay off a \ndebt more than once (in the same way that it does no harm to demand a memoized value more than once). \nBy allowing debt to be paid off multiple times, we avoid the Gordian knot of analyzing interthread dependencies. \nIf we can show that every individual thread pays off its own debt, then surely the first thread to force \na delayed computation will pay off the relevant debt. Subsequent threads demand\u00ading the same result may \nunnecessarily pay off the same debt, but this does no harm. 3 The Banker s Method We adapt the banker \ns method to account for lazy evalua\u00adtion and persistence by replacing credits with debits. Each debit \nrepresents a constant amount of delayed work. When we initially delay a given computation, we crest e \na num\u00adber of debits proportional to its eventual actual cost and associate each debit with a location \nin the data structure. The choice of location for each debit depends on the nature of the computation. \nIf the computation is monolithic (i.e., once begun, it runs to completion), then all debits are usu\u00adally \nassigned to the root of the result. On the other hand, if the computation is incremental (i.e., decomposable \ninto fragments that may be executed independently), then the debits may be distributed among the roots \nof the partial results. Each operation is allowed to discharge a number of debits proportional to its \namortized cost. The order in which deb\u00adits should be discharged depends on how the data structure will \nbe accessed; debits on nodes likely to be accessed soon should be discharged first. To prove an amortized \nbound, we must show that, whenever we access a location (possi\u00adbly triggering the execution of a delayed \ncomputation), all debits associated with that location have already been dis\u00adcharged (and hence the delayed \ncomputation has been paid for). Incremental functions play an important role in the bank\u00ader s met hod \nbecause they allow debits to be dispersed to different locations in a data structure. Then, each location \ncan be accessed as soon as its debits are discharged, without waiting for the debits at other locations \nto be discharged. In practice, this means that the initial partizd results of an incremental computation \ncan be paid for very quickly, and that subsequent partial results may be paid for as they are needed. \nMonolithic functions, on the other hand, are much less flexible. The programmer must anticipate when \nthe re\u00adsult of an expensive monolithic computation will be needed, and set up the computation far enough \nin advance to be able to discharge all its debits by the time its result is needed. The banker s method \nwas first adapted to a persistent setting in [16]. Example: Amortized FIFO Queues based on lazy evaluation. \nIn the traditional physicist s meth- As an example of the banker s method, we next give an im\u00adplementation \nof persistent FIFO queues that supports all standard operations in 0(1) amortized time, This imple\u00admentation \nis similar to, but simpler than, that presented in [17]. We represent a queue as a pair of lists (f, \nr), where ~ is the front segment of the queue, and r is the rear seg\u00adment of the queue in reverse order. \nElements are enqueued at the head of r and dequeued from the head of ~. We ex\u00adplicitly maintain the lengths \nof f and r, and guarantee that Ij[ > 1~1by rotating the queue whenever Ir[ = 1~1+ 1. A rotation transfers \nelements from r to ~ by replacing (f, r) with (f~reverse r, [ ]). Figure 1 gives Haskell source code \nfor this implementation. Rotations are the only non-trivial computations in this implement at ion. We \naccount for the cost of rotations using the banker s method. Every rotation creates Ij[ + 1.1 = 21fl \n+ 1 debits If! debits for the append and Irl debits for the reverse. Since the append function is incremental, \nwe disperse the first Ifl debits across the first I~1 elements of the resulting queue. However, since \nthe reverse function is monolithic, we assign all the remaining debits to the first element of the reversed \nlist (i.e., the (Ifl + 1)-st element of the resulting queue). We discharge debits at the rate of one \ndebit per enqueue and two debits per dequeue. To prove the amortized bounds, we must show that the first \nelement of the queue never has any undischarged debits. Let da be the number of debits on element i, \nand let ~; = ~~=o di. We maintain the following invariant: The 2i term guarantees that all debits on \nthe first element have been discharged (i.e., Do = do = O), and the lfl Irl term guarantees that all \ndebits in the entire queue have been discharged whenever the lists are of equal length (i.e., just before \nthe next rotation). Now, every engueue that does not cause a rotation sim\u00adply adds a new element to the \nrear list, increasing Ir I by one and decreasing I~1 Ir I by one. Discharging a single debit restores \nthe invariant. Every dequeue thatdoes not cause a rotation simply removes an element from the front list. \nThis decreases Ifl by one (and hence Ifl Irl by one), but, more importantly, it decreases i by one for \nevery remaining element, which in turn decreases 22 by two. Discharging the first two debits in the queue \nrestores the invariant. Finally, we consider an enqueue or dequeue that causes a rotation. Immediately \nprior to the operation, we know that lfl = Irl, so D~ = O. Hence, after the rotation, the only debits \nare those generated by the rotation itself. These debits are distributed such that 1 ifi<m 2+1 ifi<m \ndi c m+l ifi=m and D; = 2m+l ifi>m o ifi>m { { where m is the length of f at the beginning of the rotation. \nThis debit distribution violates the invariant at both loca\u00adtion O and location m, but discharging the \ndebit on the first element rest ores the invariant. 4 The Physicist s Method Like the banker s method, \nthe physicist s method can also be adapted to support persistent amortized data strictures od, one describes \na function @ that maps each data st mc\u00adture to a potential representing a lower bound on the total savings \ngenerated by the sequence of operations that cre\u00adated the data structure. The amortized cost of an operation \nis defined to be the actual cost of the operation plus the change in potential. To work with debt instead \nof savings, we replace @ with a function V that maps each data struc\u00adture to a potential representing \nan upper bound on the total unpaid debt of the delayed computations within that data structure. Note \nthat V is used only in the analysis of a data structure; it does not actually appear in the program text. \nIn this framework, the amortized cost of an operation is de\u00adfined to be the actual cost of the computations \ndelayed by the operation minus the change in potential. More formally, if &#38; and CP are the amortized \nand actual cost, respectively, of some operation p, and z and z are the versions of the data structure \nbefore and after the operation, respectively, then 2P = C* (w(z ) w(z)) To prove amortized bounds in \nthis framework, we must show that the entire debt of a data structure has been paid off before we force \nany part of the data structure, Because we only know the total debt of the data structure as a whole, \nand not the debt of individual locations, we cannot access certain locations early, as we can with the \nbanker s method. However, when applicable, the physicist s method tends to yield much simpler proofs \nthan the banker s method. Example: Amortized Binomial Queues Binomial queues are an elegant form of priority \nqueue in\u00advented by Vuillemin [24]. Inserting an element into a bino\u00admial queue requires O(log n) worst-case \ntime, but it is well known that imperative (i. e., ephemeral) binomial queues surmort insertion in 0(1) \namortized time [131.L We now .. ., show, using the physicist s met hod, that persist ent bino\u00admial queues \nimplemented with lazy evaluation also support insertion in 0(1) amortized time. A binomial queue is a \nforest of heap-ordered trees. The structure of this forest is governed by the binary represen\u00adt ation \nof the size of the queue; if ,the ith digit is one, then the forest contains a tree of size 2 . Two trees \nof size 2 can be combined to form a tree of size 2i+] in constant time by the link operation. To insert \nan element into the queue, we create a new singleton tree and repeatedly link trees of equal size until \nall sizes are unique. This process is analogous to adding one to a binary number. See [14] for more details \nabout binomial queues. Figure 2 gives a fragment of a Haskell implementation of binomial queues. In this \nimplementation, we use a sparse representation of binomial queues, meaning we do not ex\u00adplicitly represent \nthe zeros in the binary representation of the size of the queue. This requires that we tag each tree \nwith its size. Note that insert is monolithic because add- Unique does not return until it has performed \nall the neces\u00adsary links. In Section 5, we will also consider a non-sparse representation of binomial \nqueues for which insert is incre\u00admental. To analyze the current data structure using the physi\u00adcist s \nmethod, we first define the potential function to be V?(q) = Z(lgl), where Z(n) is the number of zeros \nin the (minimum length) binary representation of n. Next, we show that the amortized cost of inserting \nan element into data Queue a = Queue Int [a] Int [a] -- Invariant: each queue has thejorm Queue lenf \nf lenr r -\u00ad where jen~ = ]~1 A ~enr = Irl A ~eraf ~ lenr empty :: Queue a empty = Queue O [1 O [1 is~mpty \n:: Queue a -> Bool isEmpty (Queue lenf f lenr r) = (lenf == O) -\u00ad since lenj ~ lenr, lenf=O implies lenr=O \nenqueue :: a -> Queue a -> Queue a enqueue x (Queue lenf f lenr r) = makeq lenf f (lenr+l) (x:r) dequeue \n:: Queue a -> (a, Queue a) dequeue (Queue (lenf+l) (x:f) lenr r) = (x, makeq lenf f lenr r) -\u00ad auxiliary \npseucio-constructor: guarantees lenf > lenr makeq :: Int -> [a] -> Int -> [a] -> Queue a malreq lenf \nf lenr r I lenr <= lenf = Queue lenf f lenr r I lenr == lenf+l = Queue (lenf+lenr) (f ++ reverse r) O \n[1 Figure 1. AHaskell implementation ofamortized FIFO queues. data Tree a = Node a [Tree a] --chikkn \nin decreasing order of size type BinQueue a = C(Int,Tree a)] --trees in increcwing order of size insert \n:: Ord a => a -> BinQueue a -> BinQueue a insert x q = addUnique (1, Node x [1) q --auxiliargjunctions \n--adda new tree and link until all sizes are unique addUnique :: Ord a => (Int,Tree a) -> BinQueue a \n-> BinQueue a addUnique (n,t) [1 = [(n,t)] add,Unique (n,t) ((n>,t>) : q) I n < n = (n,t) : (n ,t ) : \nq In==n = addUnique (n+n , link t t ) q --make the tree with the larger root a child of the tree with \nthe smaller root link :: Ord a=>Tree a-> Tree a-> Tree a link (Nodexc) (Nodeyd) !x<=y=Nodex(Nodeyd:c) \nly<x =Node y (Node xc :d) Figure2. Afragment of aHaskell implementation of amortized binomial queues. \na binomial queue of size n is two. Suppose that the lowest Example: Worst-Case Binomial Queues m digits \nin the binary representation of n are ones. Then, inserting the element will eventually generate a total \nof m+ 1 calls to add Unique. Now, consider the change in potential. The lowest m digits have changed \nfrom ones to zeros and the next digit has changed from zero to one, so the change in potential is m \n1. The amortized cost of insertion is thus (m+l)-(m-1)=2. The remaining operations supported by binomial \nqueues finding the minimum element, deleting the minimum ele\u00adment, and merging two queues all have an \nactual cost of O(log n). Before these operations can inspect the data struc\u00adture, they must first pay \noff the outstanding debt. However, the outstanding debt is bounded by Z(n) = O(log n), so the total amortized \ncost of these operations is still O(log n). Source code for these operations appears in Appendix A. Eliminating \nAmortization Amortized and worst-case data structures differ mainly in when the computations charged \nto a given operation oc\u00adcur. In a worst-case data structure, all the computations charged to an operation \noccur during the operation. In an amortized data structure, some of the computations charged to an operation \nmay actually occur during later operations. From this, we see that virtually all nominally worst-case \ndata structures become amortized when implemented in an entirely lazy language because many computations \nare un\u00adnecessarily delayed. To describe true worst-case data struc\u00adtures. we therefore need a strict \nlarwua~e. If we want to describe both amortized and worst-ca~e ~at a structures, we need a language that \nsupports both lazy and strict eval\u00aduation. Given such a language, we can also consider an intriguing \nhybrid approach: worst-case data structures that use lazy evaluation internally. Such data structures \ncan be obtained from amortized data structures by appropriately scheduling the premature execution of \ndelayed components. The trick is to regard paying off debt as a literal activity, and to execute each \ndelayed computation as it is paid for. In a worst-case data structure, we no longer have the free\u00addom \nfor certain operations to be more expensive than the desired bound. Incremental functions assume a vital \nrole by decomposing expensive computations into fragments, each of which can be executed within the allotted \ntime. Often a given fragment will depend on a fragment of an earlier com\u00adput ation. The difficult part \nof proving a worst-case bound in this framework is guaranteeing that executions of fragments will never \ncascade. This is done by showing that, when\u00adever a given fragment is executed, all fragments on which \nit depends have already been executed and memoized. Implementing a worst-case data structure in this \nframe\u00adwork requires extending the amortized data structure with an extra component, called the schedule, \nthat imposes an or\u00adder on the delayed computations within the data structure. Every operation, in addition \nto whatever manipulations it performs on the data structure itself, executes the first few jobs in the \nschedule. The exact number of jobs executed is governed by the (previously) amortized cost of the oper\u00adation. \nFor this technique to apply, maintaining the sched\u00adule cannot require more time than the desired worst-case \nbounds. A special case of this general technique was first used to implement worst-case FIFO queues in \n[17]. We now return to the example of binomial queues, and mod\u00adify the earlier implementation to support \ninsertions in O(1) worst-case time. Recall that, in our earlier implementation, insert was monolithic. \nWe first make insert incremental by changing the data structure to represent explicitly the zeros in \nthe binary representation of the size of the queue. Then, every call to add Unique can return a partial \nresult. The final call to add Unique returns a stream whose first element is a One. All the intermediate \ncalls to add Unique return streams beginning with a Zero. This implementation appears in Fig\u00adure 3. In \naddition to modifying the representation, we have also changed the source language from Haskell to Standard \nML (extended with primitives for lazy evaluation). Other than the choice of language, this implementation \nof binomiaJ queues is very similar to that of King [14]. Note that this change in representation does \nnot affect the amortized anal\u00adysis from the previous section. In particular, the amorti~zed analysis \nalso holds for King s implementation. Now, we extend binomial queues with a schedule of de\u00adlayed computations. \nThe only delayed computations in this implement ation are calls to add Unique. Thus, the schedule will \nbe a list of unevaluated calls to add Unique. type Schedule = Digit Stream list To execute a job, we \nforce the first element in this list. If the result is a One, then this is the last fragment of a compu \nta\u00adtion. Otherwise, the tail of the result is another unevaluated call to add Unique, so we put it back \nin the schedule. We ex\u00adecute two jobs by calling ez-ecute twice. fun execute [1 = [1 I execute (job :: \nschedule) = case force job of cons (one t, _) => schedule I Cons (Zero, job ) => job :: schedule val \nexecute2 = execute o execute Finally, we update insert to maint ain the schedule. Since the amortized \ncost of insert is two, we execute two jobs per insertion. type EinQueue = Digit Stream * Schedule fun \ninsert x (q, schedule) = let val q = addUnique (Node (x, [I)) q in (q , execute2 (q :: schedule)) end \n This completes the changes necessary to convert the amor\u00adtized bounds to worst-case bounds. The remaining \noper\u00adations on binomial queues all ignore the schedule and re\u00adturn entirely evaluated queues. Source \ncode for these op\u00aderations appears in Appendix A. The previous amortized analysis guarantees that any \nbinomial queue contains at most O(logn) unevaluated computations, but the overhead of evaluating these \nduring the normal execution of the re\u00admaining operations is absorbed without increasing the al\u00adready \nO(logn) worst-case cost of these operations. We must next show that, whenever execute forces a job of \nthe form add Uniquet q, qhas already been evaluated land memoized. Define the range of acallto add Unique \nto be the partial result of that call together with the partial results of all its recursive calls. Note \nthat every range consists of a (possibly .mpty)fiequence of Zero afollowed bya One. We datatype a StreamMode \n= Nil I Cons of a * a Stream withtype a Stream = a StreamNode susp type Elem = . . . (* elements may \nbe anyordersd type *) datatype Tree = Node of Elem * Tree list (* chikh-en indecreasing onierofgize *) \ndatatype Digit = Zero I One of Tree type BinQueue = Di,git Stream (* digits/t~es inincreasing or-der \nof size *) fun addUnique t q = (* addoraeto!ow-onierdigit,link/mrry ifalready aone *) delay (fn () => \ncase force q of Nil => Cons (One t, emptyStream) I Cons (Zero, q) => Cons (One t, q) I cons (one t , \nq) => Cons (Zero, addUnique (link t t)) q)) fun insert x q = addUnique (Node (x,[I)) q Figure 3. An alternative \nStandard ML implementation of amortized binomial queues. say that two ranges overiap if any of their \npartial results have the same index within thestream of digits. Note that allunevaluated computationsin \na binomial queue are in the range ofsomejobon the schedule. Thus, wecan show that, foranyjobc&#38;/Unique \ntq, q has already been evaluated and memoized by proving that no two jobs in the same schedule ever have \noverlapping ranges. In fact, weprove aslightly stronger result. Define acorn\u00adpleted zero to be a Zero \nwhose cell in the stream has already been evaluated and memoized. Then, every valid binomial queue contains \nat least two completed zeros prior to the first range in the schedule, and at least one completed zero \nbetween every two adjacent rangesin the schedule. Proof Consider a binomial queue immediately prior to \nan insert. Let rl andrz be the first tworanges in the schedule. Let zl and 22 be the two completed zeros \nbefore rl, and let 23 be the completed zero between rl and r2. Now, before execut\u00ading two jobs, insert \nfirst adds anew range to the front of rO the schedule. Note that ro terminates ina One that replaces \nz1. Let m be the number of Zero s in To. There are three cases. Case1.m=O.Theonlydigit in rOisaOne,sorois \neliminated by executing a single job. The second job forces the first digit of r]. If this digit is Zero, \nthen it becomes the second completed zero (along with 22) before the first range. If this digit is One, \nthen rl is eliminated and T2 becomes the new first range. The two completed zeros prior to r2 are Z2 \nand 23. Case2. m= 1. The first two digits of the old digit stream were One and Zero (z1), but they are \nreplaced with Zero and One. Executing twojobs evaluates andmem\u00adoizes both of these digits, and eliminates \nro. The lead\u00ading Zero replaces Z1 as one of the two completed zeros before the first range (rl ). Case \n3. m ~ 2. The first two digits of r. are both Zero s, They are both completed by executing the first \ntwo jobs, and become the two completed zeros before the new first range (the rest of ro). 22 becomes \nthe single completed zero between ro and rl. 0 Once we have an implementation of binomial queues sup\u00adporting \ninsert in 0(1) worst-case time, we can improve the bounds of jindilfin and merge to O(1) worst-case time \nusing the bootstrapping transformation of Brodal and Okasaki [21. The O(log nj bou~d for deleteilJin \nis unaffected by this tra~~\u00adformation. 6 Related Work There has been very little previous work on amortized \nfunc\u00adtional data structures. Schoenmakers [22] used functional notation to aid in deriving bounds for \nmany amortized data structures, but considered only single-threaded data struc\u00adtures. Gries [6], Burton \n[3], and Hoogerwoord [9] described purely functional queues and double-ended queues with amor\u00ad tized \nbounds, but, again, support ed only single-t breaded queues. We first described unrestricted amortized \nqueues in [17] as an intermediate step in the development of worst\u00adcase queues based on lazy evaluation. \nHowever, because of the concern with worst-case bounds, that implementation is more complicated than \nthe implementation in Section 3. We derived the worst-case queues from the amortized queues using techniques \nsimilar to those in Section 5. In [16], we recognized the importance of lazy evaluation to non-single\u00adthreaded \namortized data structures in general, and adapted the banker s method to analyze such data structures. \nWe then used the banker s method to describe an implementa\u00adtion of lists supporting catenation and all \nother usual list primitives in O(1) amortized time. This paper extends our earlier work by adapting the \nphysicist s method to cope with persistence and by generalizing the technique for eliminat\u00ading amortization. \nIn addition, we introduce several new data structures, which may be useful in their own right. For every \namortized functional data structure currently known, there is a competing worst-case data structure that \ndoes not depend on lazy evaluation, Examples include queues [8], double-ended queues [7, 4], catenable \nlists [12], and skew binomizd queues [2]. In every case, the amortized data st ruc\u00adture is significantly \nsimpler than the worst-case version. How\u00adever, the amortized data structure is usually slightly slower \nin practice, mostly because of overheads associated with lazy evaluation. Memorization, in particular, \ncauses problems for many garbage collectors. Of course, if both data structures are implemented in a \nlazy language, then both data struc\u00adtures will pay these overheads. In that case, the amortized data \nstructure should usually be faster as well as simpler. Our research is rho related to earlier studies \nin the im\u00ad perative community. Driscoll, Sarnak, Sleator, and Tar\u00adjan [5] described several techniques \nfor implementing per\u00adsistent imperative data structures, and Raman [18] explored techniques for eliminating \namortization from imperative data structures. Discussion We have shown how lazy evacuation is essential \nto the de\u00adsign of amortized functional data structures, and given sev\u00aderal techniques for analyzing such \ndata structures. In addi\u00adtion, we have described how to eliminate amortization from data structures based \non lazy evaluation by prematurely ex\u00adecuting delayed components in a pattern suggested by the amortized \nanalysis. Finally, we have illustrated our tech\u00adniques with new implementations of FIFO queues and bino\u00admial \nqueues. We have made several observations about the relation\u00adship between evaluation order and kind of \nbound (amortized or worst-case). Amortized data structures require lazy eval\u00aduation, but worst-case data \nstructures require strict evalu\u00adation. Thus, from our point of view, the ideal functional programming \nlanguage would seamlessly support both eval\u00aduation orders. Currently, both major functional languages \nHaskell and Standard ML fail to meet this criterion. Haskell has only limited support for strict evaluation, \nand Standard ML has only limited support for lazy evaluation. We close with some hints on designing amortized \nfunc\u00adtional data structures. Identify a (potentially expensive) procedure for reor\u00adganizing your data \nto make future operations cheap. Make sure that the procedure is executed lazily.  Consider the access \npatterns of the data structure. If operations routinely need the entire result of the re\u00adorganizing procedure, \nattempt to use the physicist s method. If operations routinely inspect partial results, use the banker \ns method instead.  Especially if using the banker s method, make the pro\u00adcedure as incremental as possible. \nIf some or all of the procedure cannot be made incremental, arrange to set up the computation well in \nadvance of when it will be needed to allow time to pay for the computation.  If the entire procedure \ncan be made incremental, and if there is some clear order in which the fragments of the computation should \nbe executed, consider converting the data structure to support worst-case bounds by explicitly scheduling \nthe premature evaluation of each fragment.  References [1] Bror Bjerner and Soren Holmstrom. A compositional \napproach to time analysis of first order lazy functional programs. In Conference on Functional Programming \nLanguages and Computer Architecture, pages 157-165, 1989. [2] Gerth St@lting Brodal and Chris Okasaki. \nOptimal purely functional priority queues. Submitted for publi\u00adcation. [3] F. Warren Burton. An efficient \nfunctional implemerlta\u00adtion of FIFO queues. Information Processing Letters, 14(5):205-206, July 1982. \n [4] Tyng-Ruey Chuang and Benjamin Goldberg. Real-time deques, multihead Turing machines, and purely \nfunc\u00adtional programming. In Conjereracz on F unctionai Pro\u00adgramming Languages and Computer Architectutw, \npages 289 298, 1993. [5] James R. Driscoll, Neil Sarnak, Daniel D. K. Sleator, and Robert E. Tarjan. \nMaking data structures per\u00adsistent. Journal of Computer and System Sciences, 38(1):86-124, February 1989. \n[6] David Gries. The Scienm of Programming. Texts and Monographs in Computer Science. Springer-Verllag, \nNew York, 1981. [7] Robert Hood. The Eficient Implementation of V%y\u00adHigh-Level Programming Language Constructs. \nPhD thesis, Department of Computer Science, Cornell lJni\u00adversit y, 1982. [8] Robert Hood and Robert Melville. \nReal-time queue operations in pure Lisp. Information Processing Letters, 13(2):50-53, November 1981. \n[9] Rob R. Hoogerwoord. A symmetric set of efficient list operations. Journal of Functional Programming, \n2(4):505-513, October 1992. [10] Paul Hudak, Simon Peyton Jones, Philip Wadler, Brian Boutel, Jon Fairbairn, \nJoseph Fasel, Maria M. Guzm.An, Kevin Hammond, John Hughes, Thomas Johnsson, Dick Kieburtz, Rishiyur \nNikhil, Will Partain, and John Peterson. Report on the functional programming lan\u00ad guage Haskell, Version \n1.2. SIGPLAN Notices, 27([5), May 1992. [11] John Hughes. Why functional programming matters. The Computer \nJournal, 32(2):98-107, April 1989. [12] Hairn Kaplan and Robert E. Tarjan. Persistent lists wit h catenation \nvia recursive slow-down. In ACM S~ym\u00adposium on Theory of Computing, pages 93 102, 19915. [13] Chan Meng \nKhoong and Hon Wai Leong. Double-ended binomial queues. In International Symposium on Algor\u00adithms and \nComputation, volume 762 of LNCS, pages 128-137. Springer-Verlag, 1993. [14] David J. King. Functional \nbinomial queues. In Glas!gow Workshop on Functional Programming, pages 141-150, September 1994. [15] \nRobin Milner, Mads Tofte, and Robert Harper. The Definition of Standard ML. The MIT Press, Cambrid~ge, \nMassachusetts, 1990. [16] Chris Okasaki. Amortization, lazy evaluation, and per\u00adsistence. In IEEE Symposium \non Foundations of Com\u00adputer Science, pages 646 654, 1995. [17] Chris Okasaki. Simple and efficient purely \nfunctional queues and deques. Journal of Functional Program\u00adming, 5(4), October 1995. [18] Rajeev Raman. \nEliminating Amortization: On Data Structures with Guaranteed Response Times. PhD the\u00adsis, Department \nof Computer Sciences, University of Rochester, 1992. [19] David Sands, Complexity analysis for a lazy \nhigher\u00adorder language. In European Symposium on Pr-ogmm\u00adming, volume 432 of LNCS, pages 361 376. Springer-Verlag, \n1990. [20] David Sands. A naive time analysis and its theory of cost equivalence. Journal of Logic and \nComputation, 5(4):495-541, August 1995. [21] David A. Schmidt. Detecting global variables in de\u00adnotational \nspecifications. ACM Transactions on Pro\u00adgrvmrning Languages and Systems, 7(2):299-310, April 1985, [22] \nBerry Schoenmakers. Data Structuws and Amortized Complexity in a Functional Setting. PhD thesis, Eind\u00adhoven \nUniversity of Technology, 1992. [23] Robert E. Tarjan. Amortized computational complex\u00adity. SIAM Journal \non Algebraic and Discrete Methods, 6(2):306 318, April 1985. [24] Jean Vuillemin. A data structure for \nmanipulating pri\u00adority queues. Communications oj the ACM, 21(4) :309 315, April 1978. [25] Philip Wadler. \nStrictness analysis aids time analysis. In ACM Symposium on Principles of Programming Lan\u00adguages, pages \n119 132, 1988. A Complete Source Code For Binomial Queues In this appendix, we include the complete implementations \nof two variations of binomial queues. The first supports insertion in O(1) amortized time and the second \nsupports insertion in 0(1) worst-case time. Fragments oft hese imple\u00adment ations appeared earlier in \nSections 4 and 5, respectively. Complete Haskell source code for amortized binomiaJ queues is given in \nFigure 4. Complete Standard ML source code for worst-case binomial queues is given in Figures 5 and 6. \ndata Tree a = Node a [Tree a] childmm in decreasing order oj size type BinQueue a = [(Int,Tree a)] --trees \nin increasing order of size empty :: BinQueue a empty = [] &#38;Empty :: BinQueue a -> Bool isEmpty = \nnull insert : : Ord a => a -> Bin@eue a -> BinQueue a insert x q = addUnique (1, Node x [1) q merge :: \nOrd a => BinQueue a -> BinQueue a -> BinQueue a merge [1 q=q merge q [] =q merge ((nl,tl) : ql) ((n2,t2) \n: q2) I nl <n2 = (nl,ti) : merge ql ((n2,t2) : q2) lnl>n2 = (n2,t2) : merge ((nl,tl) : ql) q2 I nl == \nn2 = addUnique (nl+n2, link tl t2) (merge ql q2) findllin :: Ord a => BinQueue a -> a findMin = minimum \n. map (root . snd) --return the minimum root deleteMin :: Ord a => BinQueue a -> BinQueue a deleteMinq \n= merge c q where (Node x c, q)) = getMin q c) = zip sizes (reverse c) --convert chik%wn into a valid \nBin Queue sizes = 1 : map (2 *) sizes --[1,2,4,8,...] auxiliary junctions root :: Tree a-> a root (Node \nx c) =x add a new tvee and link until all sizes are unique addUnique :: Ord a => (Int,Tree a) -> BinQueue \na -> BinQueue a addUnique (n,t) [1 = [(n,t)] addUnique (n,t) ((n ,t ) : q) I n < n = (n$t) : (n ,t ) \n: q ln==n~ = addUnique (n+n , link t t ) q --make the tree with the larger root a child of the tree with \nthe smaller root link :: Ord a=>Tree a-> Tree a-> Tree a link (Node xc) (Node yd) Ix<=y=Node x (Node \nyd :c) Iy<x =Node y (Node x c : d) --jincl and remove the tree with the minimum root getMin :: Ord a \n=> BinQueue a -> (Tree a, BinQueue a) getMin [(n,t)l = (t,[l) getMin ((n,t) : q) = let (t , q ) = getMinq \n in if root t <= root t then (t, q) else (t , (n,t) : q ) Figure 4. A complete Haskell implementation \nof amortized binomial queues. (* Streams *) datatype a StreamNode = Nil I Cons of a * a Stream withtype \na Stream = a StreamNode susp val empty Stream = delay (fn () => Nil) fun isEmptyStream s = case force \ns of Nil => true 1 Cons (x, s) => false fun cons (x, s) = delay (fn () => Cons (x, s)) fun normalize \ns = case force s of Nil => () I Cons (x, s) => normalize s (* Binomial Queues *) type Elem = . . . \n(* eiements may be any ordered type*) datatype Tree = Node of Elem * Tree list (* children indecrsasing \norder of size *) datatype Digit = Zero I One of Tree type Schedule = Digit Stream list (* list ofdela@c \nallstoaddUnique *) type BinQueue = Digit Stream * Schedule (* digits/trees inincreasing order of8ize \n*) exception Empty local (* auxiliaryfunctions *) fun root (Node (x,c)) = x fun link (Node (x,c)) (Node \n(y,d)) = if x <= y then Node (x, tiode (y,d) :: c) else Node (y, Node (x,c) :: d) fun addUnique t q \n= (* addonetolow-orderdigit,link/carry ijalreadyaorae *) delay (fn () => case force q of Nil => Cons \n(One t, emptyStream) I Cons (Zero, q) => Cons (One t, q) I Cons (One t , q) => Cons (Zero, addUnique \n(link t t ) q)) fusluerge ql qz = (* adddigitstreams,link/mrrywhen two ones are in the same position \n*) case (force ql, force q2) of (Nil, .) => q2 I (., Nil) => ql I (Cons (Zero, ql), Cons (digit, q2)) \n> cons (digit, smergeql q2) I (Cons (digit, qi), Cons (Zero, q2)) > cons (digit, smergeql q2) I (Cons \n(Onetl, ql), Cons (One t2, q2)) => cons (Zero, addUnique (link tl t2) (smerge ql q2)) fun getMin q = \n(* jindand remove thetree with the minimum root *) case force q of Nil => raise Empty I Cons (Zero, \nq) => (* zero isnever the last digit *) let val (t, q) = getlfinq in (t, cons (Zero, q)) end I Cons \n(One t, q) => if isEmptyStream q then (t, emptyStream) else let val (t , q ) = getMin q in if root t \n<= root t then (t, q) else (t , cons (One t, q )) end Figure5. A complete Standard MLimplementation ofworst-case \nbinomial queues (part 1). fun execute [1 = [1 I execute (job :: schedule) = (* execute fit-st~ob in schedule \n*) case force job of Cons (One t, _) => schedule (* addUnique terminates*) i Cons (Zero, job ) => job \n:: schedule (* addUnique corztinues *) val execute2 = execute o execute (* execute two jobs *) in val \nempty = (emptyStream, [1) fun isEmpty (q, schedule) = isEmptyStream q fun insert x (q, schedule) = let \nval q = addUnique (Node (x,[I)) q in (q , execute2 (q :: schedule)) end fun merge (ql,schedulei) (q2,schedule2) \n= let val q = smerge ql q2 in normalize; (* force and rnernoize entine stream *) (q, [1) end fun findMin \n(q, schedule) = let val (t, .) = getMin q in root t end fun deleteMin (q, schedule) = let val (Node \n(x,c), q ) = getMin q fun ones [] = emptyStream I ones (t :: ts) = cons (One t, ones ts) val c) = ones \n(rev c) (* convert childrefa into aqueue *) val q = smerge c q in normalize q ; (* force andmemoize \nentwestream *) (q , [1) end end Figure 6. A complete Standard MLimplementation ofworst-case binomial \nqueues (part 2). ,\n\t\t\t", "proc_id": "232627", "abstract": "Traditional techniques for designing and analyzing amortized data structures in an imperative setting are of limited use in a functional setting because they apply only to single-threaded data structures, yet functional data structures can be non-single-threaded. In earlier work, we showed how lazy evaluation supports functional amortized data structures and described a technique (the banker's method) for analyzing such data structures. In this paper, we present a new analysis technique (the physicist's method) and show how one can sometimes derive a worst-case data structure from an amortized data structure by appropriately scheduling the premature execution of delayed components. We use these techniques to develop new implementations of FIFO queues and binomial queues.", "authors": [{"name": "Chris Okasaki", "author_profile_id": "81100613198", "affiliation": "School of Computer Science, Carnegie Mellon University, 5000 Forbes Avenue, Pittsburgh, Pennsylvania", "person_id": "P46261", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/232627.232636", "year": "1996", "article_id": "232636", "conference": "ICFP", "title": "The role of lazy evaluation in amortized data structures", "url": "http://dl.acm.org/citation.cfm?id=232636"}