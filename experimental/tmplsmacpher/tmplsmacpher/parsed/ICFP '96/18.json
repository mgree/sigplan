{"article_publication_date": "06-15-1996", "fulltext": "\n A Probabilistic Approach to the Problem of Automatic Selection of Data Representations* Tyng Ruey Chuang \nWen L. Hwang Institute of Information Science Academia Sinica Nankang, Taipei 11529, Taiwan E mail: \n{trc,whwang}@iis. sinica.edu.tw Abstract The designs and implementations of efficient aggregate data \nstructures have been important issues in functional pro\u00ad gramming. It is not clear how to select a good \nrepresentation for an aggregate when access patterns to the aggregate are highly variant, or even unpredictable. \nPrevious approaches rely on compde time analyses or programmer annotations. These methods can be unreliable \nbecause they try to predict a program s behavior before it is executed. We propose a probabilistic approach, \nwhich is based on Markov processes, for automatic selection of data represen\u00ad tations. The selection \nis modeled as a random process mov\u00ad ing in a graph with weighted edges. The proposed approach employs \ncoin tossing at run time to help choosing a suit\u00ad able data representation. The transition probability \nfunc\u00ad tion used by the coin tossing is constructed in a simple and common way from a measured cost function. \nWe show that, under this setting, random selections of data representations can be quite effective. The \nprobabilistic approach is used to implement bag aggregates, and the performance results are compared \nto those of deterministic selection strategies. 1 Introduction How to design and implement efficient \naggregate data struc\u00ad tures has been a major concern for both the designers and users of functional programming \nlanguages [1] [2] [3] [5] [6] [7] [11] [14] [17] [18] [20] [23] [25] [26]. The problem be\u00adcomes more \ncomplicated if access patterns to aggregates are highly variant, or even unpredictable. A common situation \nis where there are several representations of an aggregate, with one representation more efficient than \nthe others for certain operations but worst for the remaining operations, and vice versa. Which representations \nshould one chooses, given that there is no a priori knowledge of what operations, and how often, the \naggregates will be mostly used for? This is known as the data representation selection prob\u00adlem for \nvery high-level programming languages [24]. The goal is to determine suitable representations for aggregates \n \"This research is supported, in part, by National Science Council, Taiwan, under contract NSC 84 2213 \nE O01 O04  Permission to make digitalhrd copy of part or ail of this work for personal or classroom \nuse is ranted without fee providad that copies are not made or distributed for pro i t or commercial \nadvantage, the copyright notics, the title of the publication and its date appear, and notica is given \nthat oopying is by permission of ACM Inc. To copy otherwise, 10 republish, to post on servers, or to \nredistribu@ to lists, requiras prior specific permission andlor a fee. ICFP 96 5/96 PA, USA 01996 ACM \n0-89791-771 -5/96/0005 ...$3.50 of builtin data types (such as sets and arrays) such that the representations \nwill exhibit good performance for all pro\u00adgrams. The problem occurs aa well for user defined abstract \ndata types, where there may exist multiple representations of a data type but each with different performance \nchar\u00adacteristics. Naturally, aggregates of the data type will need different retmesentations in different \nmomam contexts in or\u00adder to ach.&#38;ve good performance. &#38;e~ious approaches to the data representation \nselection problem have relied heav\u00adily on compile-time analyses or programmer annotations to help selecting \na good representation of the data t ype, These approaches can be very unreliable since they try to predict \na program s behavior before it is executed. Another approach is to design a representation for the abstract \ndata type such that, t bough not the best possible in every situation, its performance is not too bad \nfor all sit\u00aduations. This representation is used for cdl aggregates of the abstract data type, and designers \nand users of the data type now spare themselves of the problem of selecting the right representation. \nOne drawback of this approach is that users of the abstract data type may pay for overhead they do not \nask for. For example, in some context the users may not use at certain functionality of a data type. \nNevertheless the performance for all aggregates of the data type is de\u00adgraded because they all have to \naccommodate this unused functionality into their representations. This paper takes a different view of \nthe data represent a\u00adtion selection problem, and presents a probabilistic approach to solve the problem. \nWe view the selection problem as an on line problem in the following way. There are several rep\u00adresentations \nof an aggregate, and it costs each representation certain amount of time to process each kind of operations. \nThese representations can also be converted to one another at a cost. There is a sequence of requests \nconsisting of vari\u00adous kinds of operations to be served by the aggregate. The goal is to choose, as the \nrequests arrive, a representation to serve the current request, and to perform a conversion be\u00adtween \nrepresentations if necessary, such that the total cost of serving the entire request sequence is small. \nNote that we have shifted the time of making the selec\u00adtions from compfle-time to run time. But a run \ntime choice may still be inappropriate if it only relies on history of the request sequence to make the \ncurrent selection. There are two reasons for this. First, history is no indication of the future a run \ntime choice may just be as inaccurate as a compfle-time choice. Secondly. keeping the request history \naround increases the space requirement of a program execu\u00adtion the space and time overhead incurred \nby run time choices may be too high to make them feasible. Probabilistic techniques have been used in \non line algorithms to avoid the above two problems [12] [16] [21]. Random choices, based on carefully \ndevised principles, can often be shown to make few bad decisions in the long term. Furthermore, random \nchoices can often be (memoryless, in the sense that they only depend on the current state of execution, \nbut not on previous states. Though the idea of random choices quite simple and ap\u00adpealing, to the best \nof our knowledge, we find few appli\u00adcations of probabfistic approaches to the data representa\u00adtion selection \nproblem. Previously, we used a randomization technique to implement purely functional arrays for efficient \nmultithreaded read/update operations [6]. The technique is shown to be effective. In this paper we further \ndevelop a general framework based on probabilistic choices to solve the data representation selection \nproblem. In Section 2 of this paper, we will describe a somewhat simplified example to be used in this \npaper for both illustration and experimenta\u00adtion. Section 3 will present the general framework and some \npreliminary analyses. An actual implementation and some experiment al performance results are presented \nin Section 4. Section 5 presents other simulation results. Implementation issues are discussed in Section \n6. Section 7 discusses related and future work. 2 An Example We use a somewhat simplified example thorough \nthis pa\u00adper as a demonstration, but the technique applies to others. Suppose that we want to implement \nan bag data type that supports just three kinds of operations: creating an empty bag, inserting an integer \nto a bag, and querying a bag for the membership of an integer. Of course, we may also sup\u00adport deletion \nof an integer from a bag, and so on. But right now let us assume there are just three kinds of operations: \ncreation, insertion, and membership query. A bag can be implemented as a list. The list representa\u00adtion \nprovides constant time insertion by appending the in\u00adsert ed integer at the front of the list. A membership \nquery, however, will takes time linear to the length of the list in the worst cases. A bag can be implemented \nby a balanced search tree as well. Insertion and membership query then each takes logarithmic time, with \nrespect to the number of nodes in the tree. If the bag will be used mostly for insert operations, then \na list representation is preferable. If we have a large bag and the number of membership queries is huge, \nthen we will prefer a balanced search tree represen\u00adtation. But what are the precise criteria for preferring \none representation over the other? The problem can be rephrased as the following. We want to serve a \nsequence of bag operations that starts with a cre\u00adate operation and followed by some number of insert \nand query operations. We have no a prior knowledge of what the sequence looks like, including the tot \nal number of oper\u00adations in the sequence. The problem is to decide which bag representation one should \nuse, and, if more than one repre\u00adsentation are preferred, when to convert one representation to the other. \n3 Models and Analyses We take a probabilistic view of the problem of selecting suit\u00adable aggregate representations \nto serve a sequence of oper\u00adations. The way the aggregate is implemented may change over time to better \nserve the incoming operations. In par\u00adticular, we model the change of representations over time as a \nMarkov process. A Markov process can be described informally as a set of states and a chance process \nthat moves around through these states. In this paper, the states of the Markov pro\u00adcess are just the \ndifferent ways an aggregate can be imple\u00admented. Our goal is to determine good transition probabil\u00adities \nbetween the states such that the total cost of serving a sequence of operations over time is small. We \nuse M arkov processes as models because of their simplicity, and because of the rich techniques developed \nfor them in the literature [8] [10]. Markov processes are also memoryless, in the sense that the probability \nof moving from one state to another one depends only on the current state, not on any other state. The \nfollowing fixes notation convention that will be used later in this paper. s is a set of k distinct states \n91, s2, . . , Sk. Each state rep\u00adresents a particular way the aggregate is implemented. It is equally \nwell for an aggregate to be in any state because each will provide the same functionality (al\u00adthough \nat a different cost, see below). P is a k-by-k transition probabfity matrix for s such that, being in \nstate s,, it will move to state S3 with proba\u00adbility y P, ,j. Naturally, 1> P,,l ~ O for all i,~, and \nk C is a k-by k cost matrix with m> C,,j>O foralli, j. C,,j is the cost of making a move from s, to \nS3. C is not necessarily symmetric. e is a vector of length k, and e, is the expected cost of mak\u00ading \na move out of state s,. That is, e = [cl, ez, . . . . e~IT, and k el = xP,,,cz,, for all i. E is a k \nby-k matrix, representing the expected costs of the random walks between two states, as introduced by \nC and P. E,,j is theexpected cost of first reaching S3 starting from s,. We can formulate E by the following \nrecurrent equation: E=~,3 P,,, Cc,, + ~P,,k(C,,k + E~,j) for all z,j. ~#3 a, /3, ~, . . . symbolize \nthe kinds of operations supported by the aggregate. The set w consists of all kinds of oper\u00adations. n \nis the current size of the aggregate. For example, each bag aggregate has two representations. We let \nstate SI be the list representation and sz the bsL anced search tree representation. The two representations \nare convertible to each other and will support all bag oper\u00ad ations. PI,1 is the probability y that, \nwhile in the list repre\u00adsentation, the next bag operation wiU be performed on the same represent ation, \nand PI,2 the probability y that the next bag operation will be performed on the balanced search tree \nrepresentation (which itself is converted from the current list represent ation). Likewise for P2,2 and \nP2,1. Furthermore, Cl,1 is the cost of performing a bag opera\u00adtion while the bag is implemented as a \nlist. Cl,2 is the cost of converting the list representation to the balanced search tree representation, \nplus the cost of performing the bag op\u00aderation on the new representation. Likewise, for Cz,2 and Cz,l. \nHence, value el is the expected cost of performing a bag operation while the bag is in its list representation, \nand El,2 is the expected cost of transforming the list represen\u00adtation of a bag into the balanced search \ntree representation while serving bag operations. Our bag supports three kinds of operation: creation, \nin\u00adsertion, and membership query. Let a represent the insert operation, ~ represent the membership query, \nand ~ repre\u00adsent the create operation. Then w = {a, ,B, ~}, and a request sequence starts with a y operation \nand followed by interwo\u00adven a or @ operations. For now it suffices to consider n, the aggregate size, \nfixed, and the request sequence consisting of only one kind of op\u00aderation. For example, we may concern \nourselves of serving a sequence of membership queries to a bag of exactly n inte\u00adgers. We will later \nshow how to extend the Markov frame\u00adwork to aggregates supporting multiple kinds of operations, and to \naggregates of variant sizes. Recall that a membership query takes logarithmic time for a balanced search \ntree, and it takes linear time for a list in the worst cases. If the bag is already in its balanced search \ntree representation, then we may want to continue to use the representation to support further membership \nqueries. Troubles occurs when the bag is in its list repre\u00adsentation. Should we perform membership query \non the list presentation (which is costly for each query but is tolerable if there are few of them), \nor should we convert immediately the list representation to the balanced search tree represen\u00adtation \nand then serve all the queries on the new represen\u00adtation (which initially wiU cost O(n log n) time for \nthe con\u00adversion, but it pays off if there are many queries afterward)? This is a difficult decision to \nmake at compile-time because often we cannot predict the number of membership queries the bag has to \nserve. The idea is that, while in the list representation, the bag should graduzdly change to the balanced \nsearch tree repre\u00adsentation to better serve membership queries. This gradual change of implementation \nis modeled as a M arkov process where representations change according to P, the transition probability \nmatrix. It will be desirable if the Markov process bears the following two properties: (1) The expected \ncost of serving a request on a representa\u00adtion (while making the transition to a new representa\u00adtion) \nis comparable to the original cost of serving the request, as if no conversion will ever occur. (2) \nThe expected cost of eventually converting to the new representation is comparable to the cost of an \nimme\u00addiate conversion.  Property (1) make sure that, in the short term (i. e., there are few membership \nqueries in the sequence), the expected cost of serving a request is comparable to the case when the conversion \nto new representation is simply not worthy. Property (2) make sure that, in the long term (i. e., there \nare many membership queries), the expected cost of converting to the new representation is still comparable \nto the case where an immediate conversion is most desirable. Formally, we can put it in the following \nway. Let s, be the current state, and let Sj be the preferred state. A state S3 is preferred if C ~,~ \n1s the sm~est among d Ch,h, where 1< h < k. That is, the cost of serving a request is smallest at state \ns~. Then we want to (1) compare e; (the expected cost of serving a request at state Si ) to C~,; (the \noriginal cost of serving the request as if no conversion occurs), and (2) compare Ei ,j (the expected \ncost of eventually converting to the preferred representation) to Ci,j (the cost of an imme\u00addiate conversion). \nIn the above example of bag aggregates, we may call balanced search tree the preferred representa\u00adtion \nbecause it takes less time to serve a membership query. We want to convert a bag from the list representation \nto the balanced search tree representation if there are many remaining membership queries. In general, \nthere may be more than one preferred states. (For example, there may be several representations of a \nbag aggregate which serve membership queries with equal effi\u00adciency. ) Assume for the moment that the \npreferred states are absorbing. (This requirement is not really necessary, but make easier the proof \nthat follows. ) That is, Pj,j = 1 if s] is preferred. Let s, the set of all states, be partitioned into \ntwo subsets B and D, where B is the set of the u absorb\u00ading states, and D the set of the remaining v \nnon absorbing states. Let s be reordered such that the absorbing states appear before non absorbing states. \nWe then write P as the following where I is a u by u identity matrix, and O a u by v matrix with all \nO. R is a v by u matrix representing the transition probabilities from non absorbing states to absorbing \nstates, and Q is a v by v matrix representing the transition prob\u00adabilities between non absorbing states. \nLet eD be a vector of length o, describing the expected costs of making a move out of the v non absorbing \nstates, and let ED be a vector of length v, describing the expected costs of first reaching an absorbing \nstate from non absorbing states. Lemma .3.1 ED = (I Q) leD o PROOF. Recall that Ei,J = Pi,jci,l i-~Pi,k(ci,k \n+ Ek,j) k#] for all i, j. Because we are interested in the expected cost of first reaching any absorbing \nstate from a non absorbing state, we can simply reformulate the above as P=10 foralli ~BUD, where e,= \nE,= Ofor alli EB. where A,, is the i-th column of A, and A:, is its transpose. In matrix form, it follows \nthat E = e + PE and (1  It follows that P)E = e. Recall that RQ () and we have It follows that (1 \n Q)ED = e. and ED = (1 Q)-le~. 0 The matrix N = (1 Q) l is called the fundamental ma\u00adtrix for the absorbing \nMarkov process P [10]. The entries N,,$ of this matrix have the following probabilistic interpre\u00adtation: \nN,,j is the expected number of times that the process will be in state Sj before absorption when it started \nin St (where both s, and Sj are no absorbing state). [10]. Note that Lemma (3. 1) show how to calculate \nE, . for ./ -,. any two states s, # S2. We simply view SJ absorbing, and let B = {s,}, and D = ~. For \nE,,,, the expected cost of returning to itself when starting from state s?, we can use the following \nequation: where e, and Ej,,, j # i, are already known. Since ED can be used as a performance measurement \nof the Markov pro\u00adcess we are interested in, we show in the following its upper bound in a special case \nwhen Q is a diagonalizable matrix. Lemma 3 .2 (1) The not equal to 1 eigenvalues of the matri\u00adces P and \nQ are the same. (2) Let II . Ii be a norm which satisfies the submultiplica\u00ad tive property, i.e. II \nA.B II < II A [1 II B II, and let {A} be the eigenvalues of Q, X can repeat and 1A I <1, then ,==1 where \nw is the number of non absorbing states in P. O PROOF. Let f(A) be the characteristic polynomial of the \nmatrix A. The eigenvalue A of the matrix P is the root of f(P): = (1 -A) det (Q -M) = (1 -A) f(Q) where \nu is the number of absorbing states in P. We have shown that the not equal to 1 eigenvalues of P and \nQ are the same. Since Q is a diagonalizable matrix, there exists an or\u00adthonormal matrix A and a diagonal \nmatrix A, with (A)t,, = J,, such that Q = AAA where At is the transpose of A. We are investigating criterions \nfor the construction of a P from a given C such that e, and E,,j are each comparable to C,,, and C,,J. \nBelow we illustrate a simple heuristics for constructing P which we call the local construction: The \nconstruction is local because the transition proba\u00adbility y from a state s, to any state SJ is determined \nonly from the costs that are local to state s,. In addition, given CO> C ,,J >Oforall i,j, wehavel>P,,j \n>Oforalli, j. It follows that Q is diagonahzable and Lemma 3.2 applies. We can also bound e, and E,,j \nin the following way. Lemma 3.5 For the local construction, e, < k.min C,,3 for all i 3 where k is \nthe total number of states, and 2.= max e~ k#3 91 = xP,,k k#j o PROOF. First of all, for a local construction, \nwe have Furthermore, q, is the probability that, when in state s,, the next move will not reach state \nS3, and e, the expected cost of that move. It follows that ~ is the upper bound of the probability that, \nwhile not in state SJ, the next move still do not reach them, and ~ an upper bound of the expected cost \nof such a move. We then have &#38; as an upper bound of the total number of times it stays outsides of \nstate Sj when starting from any state s, # Sj. Then &#38; is an upper bound for E,,J, the expected cost \nof all the moves before reaching state Sj, starting from state St. Let D = {sj }, then the bound for \nEi,j is also derivable from the following norm operations [13]: 0 For the bag example, there are only \ntwo states SI and S2, for the list and balanced searc~ tree representations. It follows from the above \nlemma that e] = Pl,l CIJ + Pl,z Cl,z = ;l: ;~;:2 < 2min{C 1,1, Cl,2} s 2C1,1 and 1 E1,z = el = ( C ;:lC1 \n2 )(:l:~~;, ) = 2G,2 1 P1,l Similarly, e2 s 2C2,Z and E2,1 = 2C2,1. 3.1 Aggregates with Multiple Kinds \nof Operations In the bag example, after creation, each bag in fact supports two kinds of operations: \ninsertion and membership query. Insertion is better performed on lists, and membership query is more \nefficient on balanced search trees. In general, for each kind of operations @ 6 w, it will has its own \ncost matrix C@. Furthermore, when operating ~, the representation of the aggregate should be chosen based \non the Markov process derived from C+. Let y: be the cost of operating ~ in state si, and Xi,j the cost \nof converting an aggregate from state si to state S3. Then, by definition, ifi=jY? C:j = X;,j+y~ ifi#j{ \nFrom each C@, we then construct a Markov process P+ to model the change of representations when operating \n~.  3.2 Variant Sized Aggregates Some operations will change the size of an aggregate. Often the performance \nof the aggregate is affected as its size grows or shrinks. For example, an insertion makes the size of \na bag grow by one. If the bag is implemented by a balanced search tree, then each subsequent insertion \nor query to the bag takes more time than it does to the original bag. Therefore, the cost matrix C not \nonly is parameterized by w, the kinds of operations supported by the aggregate, it is also a function \nof n, the size of the aggregate. Let s write C[rz] for the cost mat rix at size n, and P[Tz] for the \ncorresponding transition probability. In general, we want to pre compute P such that, at the moment of \nserving a request, we can make a quick decision based on P to choose a suitable implementation. Since \nC is a function of n it will be impractical to pre compute P for all n. If we delay the construction \nof P until run time, where n is known, then the overhead for making a probabilistic choice at run time \nmay be too great to render the whole approach impractical. As a compromise, we use the following method \nto esti\u00admate P: Pre compute only P[2m], m c N. When serving a request to an aggregate of size n with \n2m-] < n < 2m, make a probabilistic choice based on P[2~]. This e~ima\u00adtion of P[n] works out well in \npractice, but is biased against implementations whose sizes grow faster than the others.  3.3 The Algorithm \nGiven: A specification of an abstract data type that supports op\u00aderations of kind #J E w. Also given \nare k representations SI, S2, . . . Sk of the abstract data type. Preprocessing: Measure C4[n] where \n# c w and n = z~, m ~ N. Build P@[n] from C [n] using the local heuristics. On Line Service: An aggregate \na is requested to serve a # operation. Let SZbe the current state of the aggregate, and n its size. Suppose \n2 -] <n~2m,mcN. Then \" Make a probabilistic choice based on Pi+ [2m]. Let SJ be the new state. Ifs; \n= s,, then return q$i(a).  Otherwise first mutate a from state s~ to state s~, then return 43(a).  \n4 Experimentation We have conducted an experiment, under Standard ML of New Jersey 0.93, to measure the \neffectiveness of the pro\u00adposed approach. We implement an integer bag by two dif\u00adferent representations: \na list with all the integers in the bag, and a mapping that maps an integer to the number of times it \nappears in the bag. The map is taken from the SML/NJ 0.93 library, and has been very efficiently implemented \nby a balanced search tree. The signature of the bag aggregate, as well as its implementations in the \nthe list and map rep\u00adresentations, are described in SML in Figure 2 at Appendix A. Each representation \nsupports three kinds of operation: insert operation (insert ), membership query (member), and creation \nof a empty bag (VOid). In addition, each repre\u00adsentation also supports the following operations: size \nthat returns the size of a bag, and list2bag and bag21ist that convert between an integer bag and an \ninteger list. (Do not confuse this list to the list representation of a bag. See Fig\u00adure 2 for details. \n) The function size is used to determine which cost matrix C (hence, which transition matrix P) to use. \nIt is a constant time function. The function 1 ist 2bag and bag21ist mediate between the list and map \nrepresen\u00adtations for conversion purposes. The performance of the two representations is measured by a \nseparate program by timing the execution time of in\u00adsertions and queries, each for aggregates of different \nsizes. The data is shown in Table 1, and is used to construct cost matrices C[n]. The figure in Table \n1 is measured by using the System .Timer structure of SML/NJ 0.93, on a 40 MHz sec. op. aggregate size \n kind 25 26 2 28 2 210 insert 3.51 x 10-6 3.51 x 10 6 3.70 x 10 6 3.53 x 10-b 3.59 x 10 3.59 x 10 6 \nmember 1.10 x 10 4 2.00 x 10-4 3.61 X 10-4 7.62 X 10 4 1.52 X 10-J 3.75 x 10- bag21ist 2.90 X 10 3.13 \nx 10 6 2.75 X 10-6 2.75 X 10 2.87 X 10 6 2.78 X 10 6 list2bag 1.46 X 10-5 1.46 X 10 5 3.91 x 10 5 5.86 \nX 10-5 1.17 x 10-4 2.34 X 10 4 sec./op. aggregate size kind 2 21 21 214 215 2]6 insert 3.52 X 10-6 3.55 \nx 10-6 3.52 X 10 s 3.55 x 10 3.59 x 10-6 4.25 X 10 6 member 7.81 X 10 J 1.59 x 10 2 3.38 X 10- 6.38 \nX 10 1.30 x 10-1 2.88 X 10-1 bag21ist 2.81 X 10 6 2.85 X 10 6 2.84 X 10-6 2.96 X 10 b 2.38 X 10 6 2.83 \nX 10 6 list 2bag 4.69 X 10 4 9.38 X 10-4 3.13 x 10 7.50 x 1O J 1.50 x 10 2 2.75 X 10 2 (a) For the list \nrepresentation. sec. op. aggregate size kind 25 26 27 28 29 210 insert 6.47 X 10 5 7.81 X 10 5 8.76 X \n10-5 9.89 X 10 5 1.14 x 10 4 1.26 X 10-4 member 1.20 x 10-5 1.19 x 10-5 1.34 x 10-5 1.59 x 10-5 1.38 \nX 10-5 1.83 X 10 5 bag21ist 1.81 X 10 4 3.76 X 10 4 7.23 X 10 4 1.43 x 10 d 2.81 X 10 3 7.34 x 1O-J list2bag \n1.09 x 10 3 2.62 X 10 5.92 X 10 J 1.35 x 10- 3.10 x 10 2 9.02 X 10- sec. op. aggregate size kind 21 \n21 2 3 2 4 215 21 insert 1.36 X 10 4 1.45 x 10-4 1.45 x 10-4 1.57 x 10 4 1.73 x 10 4 1.94 x 10-4 member \n7.17 x 10 6 1.67 X 10 5 2.40 X 10 5 2.36 X 10 5 1.54 x 10 5 2.22 x 10 5 bag21ist 1.50 x 10 3.03 x 10 \n2 6.13 X 10-2 1,25 X 10 2.43 X 10 1 5.00 x 10 ] list2bag 2.15 X 10 1 4.95 x 10 ] 1.08 X 10 2.42 X 10\u00b0 \n5.28 X 10\u00b0 1.15 x 101 (b) For the map representation. 100 ~-7 t 10- I 10-51 map repre.entatlcm = ---+ \nllst repre.entat, an 10-   ,o-6~ 101 102 103 10 10 102 103 104 10 aggregate s. ze aqqreyite s. ze \n(c) Costs of insert and member operations, from the above two tables. Table 1. Performance measurements \nfor both the list and map representations of bag aggregates. time in sec. representation strategy operation \nsequence map list greedy probabilistic rein) max, a simulated rein, max, u (ao.9991Po.ool) uu 1.34 \n0.39 13.84 0.83 (0.70, 1.80, 0.34) 0.63 (0.51, 1.62, 0.35) (clos IPO 5)100 0.83 50.62 929.90 16.25 (13.31,19.23,2.20) \n20.46 (16.03,26.46,3.46) -(CYo.1 po.9)  0.25 17.69 56.35 4..54 (3.37, 7.01, 1.00) 4.42 (3.33, 7.30, \n1.14) (a=lpw)   1.50 0.39 11.99 1.05 (0.72, 2.93, 0.68) 1.04 (0.49, 4.16, 1.15) (a % o) o 1.64 219.16 \n9.16 20.48 (16.62,27.67,3.84) 24.91 (18.45,35.81,5.95) -(a op o) o 3.04 8.24 36.01 15.55 (9.00,20.95,3.38) \n20.55 (12.15,30.43,5.28) (cr oj9 ) 0.30 7.86 1.00 3.54 (3.34, 3.70, 0.13) 2.95 (2.68, 3.18, 0.17) - \n Table 2. Performance ofvarious representations of bagaggregates for some bench\u00admarks. NOTE. For request \nsequences, weuse the notation that, for example, (cYo.999/@o.ool) 10000 is a sequence of 10000 requests \nwhich, at any moment, an a operation occurs with probability 0.999 and a @ operation occurs with probability \n0.001. The probability anoperation occurs may flsodepend onitsordind number tinthesequence, hke(aAl~%)l0000. \nIf there is no subscript, then the operation always occurs. For example, (a 1000/3 000 ) 0 consists of \n10 blocks of subsequence with each blocks containing 1000 a operations followed by 1000 ~ operations. \nWecompare theperformance of the foUowing four representations: theonethat always uses the map representation, \nthe one that always uses the list representation, the one that always converts to the preferred representation \nwhen performing an operation, and the one that uses the local transition probability. Only user time \nis reported; garbage collection time and system time are not measured. The entries under probabilistic \nin the tables are the mean of 10 runs. Also shown are the minimaJ, the maximal, and the standard deviation \nof the 10 runs. As a reference, we also simulate the probabdistic scheme using the benchmarks by accumulating \neach operation costs, as measured in Table 1, when the requests are served. These entries are under the \nsimulated column, and each is also the mean of 10 simulations. The purpose of the simulation is to see \nif the actual running time is consistent with that predicted by the probabilistic cost model. As shown \nin the table, the two are quite consistent. SPARC workstation with 32 MB memory. Only user time is reported; \ngarbage collection and system times are not, mea\u00adsured (because SML/NJ 0.93 garbage collects at indefinite \ntime). Considerable care has been taken to make a (more or less) accurate measurement. However, fluctuations \nremain. The bag elements are randomly drawn from the integer set {0,2,..., 229 1}. In Table 1, the \ntime in entry 2m is the average of the times for aggregate of sizes from 2m 1 + 1 to 2m, if m is small. \nIf m is large, the times in the entry is the average of times for aggregate of sizes from 2m -c to 2m \nfor some constant c. From the two tables, we can construct the cost matrices C s. For example, C;1[25] \n= 3.51 x 10-6 Cjz [25] = 2.90 X 10-6 + 1.09 X 10 3 + 6.47 X 10-5 C~l[25] = 1.81 X 10-4 + 1.46 X 10-5 \n+ 3.51 X 10-6 Cjz [25] = 6.47 X 10-5 A SML functor is then written to accept the two rep\u00adresentations \n(as well as their performance measurements), build the cost matrices, construct the transition probability \nmatrices, and produce a representation that (based on al\u00adgorithms outlined in Section 3.3), when serving \nan insertion or a query, will make a probabilistic choice on whether or not to first perform a conversion \nbefore actuaJly serving the request. The skeleton of the functor is shown in Figure 3 in Appendix A. \nWe run a set of simple benchmarks to evaluate the ~er\u00adformance of the probab&#38;ic scheme. The results \nare sh~wn in Table 2. The probabdistic scheme is never the fastest. However, its performance is between \nthose of map and list represent ations, except in one benchmark (aggo /310)20 where it is worse than \nboth. There is a simple explanation: in this case both the map and list representations happen to use \nabout the same amount of time, while the proba\u00adbilistic scheme pays additional overhead (such as generating \nrandom numbers and performing conversions) for making run t ime choices. We also observe that the map \nlibrary provided by SML/NJ 0.93 is very efficient; it is fastest except in only two occa\u00adsions where \nit loses to the list representation. Notice that in the two occasions, the probabilistic scheme also \nbeats the map repremmtation. The greedy algorithm, which always converts to the preferred state when \nserving a request, of\u00adten performs badly because the conversion cost cannot be amortized by the subsequent \n(short) sequence of operations oft he same kind. We may make the following conclusion: In order for comptie-time \nanalyses or programmer annotations to be effective in making dynamic selection of data represen\u00adtations, \nthe conversion costs between representations must be fully taken into account. Simulation We describe \nin this section simulation results of applying the probabilistic approach to a selection problem with \nthree different representations and four kinds of operations. The data type we choose is double ended \nqueue (deque), where elements can be put into and ends of the queue. Wecalthe end and therear end. Elements \nat the front end by the push are put into and taken away and enject operations. taken away only from \neither twoends ofadequethefront are put into and taken away and pop operations. Elements at the rear \nend by the inject There are many efficient implementations of functional deques [7] [15] [19] [22]. \nIn particular, there are represen\u00adtation where each of the four functional deque operations costs only \nconstant worst case time [7] [19] [22]. We call such representation real time. Red time representation \nof a deque usually has higher overhead associated with each deque operation. If adeque is mostly used \nlike a stack, then asta.ck rep\u00adresentation may suffice. In this representation, push and pop operations \neach takes constant time, while inject and enject operations each takes time linear to the stack sizes. \nWe call this representation front-end stack. Similarly, we can employ a rear end stack representation \nwhere each in\u00adject andenject operation takes constant time but push and pop operations each requires \nlinear time. Note the above three representations can be converted to one another. For the following \ndiscussion, let state S1 be the front end stack representation, state sz the rear end stack representation, \nand state ss the real time representation. Weneedto fix the operation costs for push, pop, inject, and \nenject at each state, and the costs for conversions be\u00adtween states. Suppose that apushoperation onastack \ncosts a units of time, and a pop operation on a stack costs b units of time. Suppose further that each \ndeque operation for the real-time representation costs d units of time. Wethenmodel the cost ofa push \noperation of the three representations as a vector function of aggregate size n is also good for multithreaded \naggregate yses in Section 3 depend only on the pu h[n]=(2(a+~n+c) aggregate, not on any previous states. \nwhere cis a small constant (to make Yz h[O] not zero). The cost models for pop, inject, andenject can \nbe similarly built. The conversion matrix function X[n]== By Section ation, is ((a+$n+a (a+d)n+a cost \nbetween the three representations is a of n ( (a-+b)n (a~b)n (a+d)n (a~d)n 3.1, C ush[n], the cost 3(rl \n+ Z))n+ c 2(a+b)n+ c (3a+2b+d)n+c (b+d)n (b+ d)n O ) matrix for pushover\u00ad (b+d)?z+d (b+d)n+d d ) The \ncost matrix for other kinds of operations can be simi\u00adlarly constructed. Using the above cost functions \nC, and the local transi\u00adtion probability functions, wethensimulate the performance of the probabilistic \nrepresentation on several benchmarks. In the simulations, we have set parameters (a, b,c, d) to (2,4,3,50). \nTheresults areshownin Figurel. Theproba\u00adbilistic representation behaves adaptively and performs well. \nNote that we show just one instance for each benchmark. The performance of the probabilistic scheme for \nan iden\u00adtical benchmark will differ for each instance (although we will expect the performance deviation \nto be small). Also note that, the costs for the probabilistic scheme does not include the overheads associated \nwith computing probability y and generating random numbers. 6 Discussion We face several problems when \nperforming the t ation. First of all, it is really a tedious job accurate measurement of the cost matrices. \none of the reason why we have not experimented complicated aggregates. For example, the least transition \nexperimen\u00adto make an That is also with more measurable unit of time in SML/NJ 0.93 is 0.01 second, and \nwe have to repeatedly run the measurement code and divide the accu\u00admulat ed time to get the time for \na single bag operation. We also have to discount the time spent on spurious activities in the measurement \ncode, such as the generation of sample operation sequences and the skeleton loops to carry out the operations. \nWe also find the floating point support in SML lacking. For example, it will be really nice if one can \nexpress numbers like Inf and NaN, which are in the ANSI/IEEE Standard. That will make easier the task \nof stating certain conversions between representations are impossible (z. e., C,,7 = CO). We use the \nref data type in SML to make mutable rep\u00adresent ation of an aggregate. This is not a problem per se, \nbut is troublesome if we want the bag data type to be poly\u00admorphic. The SML typing rules will then insist \na weak type variable for the bag element. This is not really necessary in this cent ext. All usages of \nthe assignment operator : = are in the Mix functor in Figure 3, and can be shown that the types of the \nnew value and the old value are always the same. Nothing will go wrong there. Last but not least, notice \nthat the probabilistic approach mutate the aggregate when making a peated accesses to the aggregate \nwill conversion again. 7 Related and future works The problem of automatic selection of when approached \nfrom a probabilistic accesses. The anal\u00ad current state of the Also notice that we conversion; hence re\u00adrarely \nneed the same dat a representations, framework, is closely related to the problem of random walks in \nweighted graphs [4] [9] [10]. These works often assume the cost matrices are symmetric, and use more \ncomplicated techniques to de\u00adrive tight bounds of the probabilistic schemes involved. The competitive \npaging problem and its probabilistic solution is related to the data representation selection problem \nas well [12]. More analyses are needed for the local heuristics for constructing the transition probability \nmatrix. In general it is not clear how well it performs when compared to an off-line optimal algorithm. \nWe also need to exploit other construction of the transition probability matrix, perhaps by using techniques \nof multivariate constraint optimization. ;2 . 1 0 100 200 300 400 10\u00b0 o 100 200 300 400 operat 1~ o 100 \n200 300 400 k 106 : 105 : -- \u00ad --. ---\u00ad : I 10 10\u00b01 o 100 200 300 400 ope rat a13 . 1 0 100 200 300 400 \n500 500 I cm 500 500 1 on 500 600 700 600 700 sequence 600 700 -- r-\u00ad 600 700 sequence 600 700 800 \n900 800 900 800 900 f rent rear .eal tm, p;;babill 800 900 800 900 1000 ~ 1000 1000 ic ; 1 J 1000 1000 \n 10 1 i 0 100 200 300 400 500 600 700 800 900 1000 w=ratl.n =wence Figure 1. Selecting representations \n Benchmark (pusho~ linjecto,~ )500 (popo,~ ]enjecto,~ )500 is sim\u00adulated. The best representation for \nthis benchmark is the red-time represent ation, as shown in the totrd cost graph. The probabilistic approach \neventually settles to the rea. time representation as the deque grows in size (although there are still \nrare random conversions to other represen\u00adt ations), as shown by the state chart. At the beginning and \nending of the operation sequence, the deque size is small and the probabilistic approach is indecisive \nabout which of the three representations to use. Total cost: front end stack 1474407, rear-end stack \n= 1531593, re.z-time = 50000, probabilistic =361731. We run the benchmark (push200inject100 enject100poP100)2. \nNone of the three fixed representations performs well. In\u00adstead, a greedy approach (which converts to \nthe most effi\u00adcient representation whenever possible) will perform better because it needs few such conversions \n(4 in this benchmark). The probabdistic approach makes few conversions as well, and outperforms the three \nfixed presentations. Total cost: front end stack= 1442800, rear-end stack= 1201800, real\u00adtime =50000, \nprobabilistic = 28207. Werunthe benchmark ((pusho 81injecto2)300pop200 )2. Push and pop operations out \nnumber inject operation. Both the front-end stack and res&#38;time representations could be good choices. \nThe simulation indicates that red time is better. The performance of the probabilistic approach is between \nthe two, anditmostly uses thetworepresentations as well. Total cost: front end stack = 278994, rear end \nstack = 2365606, real time = 50000, probabilistic = 178521. fordeque aggregates: Simulation results. \nWe SJSO assume that, for aggregates of the same size, [13] all operations of a given kind willcost the \nsame amount of time. In general this is not true, and we need to look into this issue. [14] References \n[1] Annika Aasa, %ren Holmstr6m, and Christina Nils-[15] son. An efficiency comparison of some representations \nof purely functional arrays. BIT, 28(3) :490 503, 1988. [2] Henry G. Baker, Jr. Shallow binding in Lisp \n1.5. Com-[16] munications of the ACM, 21(7):565 569, July 1978. [3] Henry G. Baker. Shallow binding makes \nfunctional ar\u00adrays fast. SIGPLAN Notices, 26(8):145-147, August [17] 1991. [4] Allan Borodin, Nathan \nLinial, and Michael E. Saks. [18]An optimaJ on line algorithm for metrical task system. Journal of the \nAssociation for Computing Machinery, 39(4):745 763, October 1992. [5] Adam L. Buchsbaum and Robert E. \nTarjan. Conflu\u00ad[19]ently persistent deques via data structural bootstrap\u00adping. In Proceedings of the \nFourth Annual ACM-SIAM Symposium on Discrete Algorithms, pages 155\u00ad 164. Austin, Texas, USA, January \n1993. [6] Tyng Ruey Chuang. A randomized implementation [20] of multiple functions arrays. In Proceedings \nof 1994 ACM Conference on Lisp and Functional Programming, pages 173 184. Orlando, Florida, USA, June \n1994. The proceedings also appears as Lisp Pointers, Volume VII, [21] Number 3, July September 1994. \nACM Press. [7] Tyng Ruey Chuang and Benjamin Goldberg. Real\u00adtime deques, multihead Turing machines, and \npurely [22] functional programming. In Conference on Functional Programming Languages and Computer Architecture, \npages 289 298. University of Copenhagen, Denmark, [23]June 1993. [8] Kai Lai Chung. Elementary Probability \nTheory with Stochastic Processes. Undergraduate Texts in hfathe\u00ad matics. Springer Verlag, 1974. [24] \n[9] Don Coppersmith, Peter Doyle, Prabhakar Raghavan, and Marc Snir. Random walks on weighted graphs \nand applications to on line algorithms. Journal of the A SSO\u00ad ciation 1993. for Computing Machinery, \n40(3):421-453, July [25] [10] Peter G. Doyle and J. Laurie Snell. Random Walks and Electric Networks, \nvolume 22 of The Carus Mathe\u00admatical Monographs. The Mathematical Association of America, 1984. [26] \n[11] James R. Driscofl, Neil Sarnak, Daniel D. Sleator, and Robert E. Tarjan. Making data structures \npersistent. Journal of Computer and System Sciences, 38(1):86\u00ad124, February 1989. [12] Amos Fiat, Richard \nM. Karp, Michael Luby, Lyle A. McGeoch, Daniel D. Sleator, and NeaJ E. Young. Com\u00adpetitive paging algorithms. \nJournal of Algorithms, A 12:685 699, December 1991. Gene H. Golub and Charles F. Van Loan. Matrix Com\u00ad \nputations. The Johns Hopkins University Press, second edition, 1989. Robert Hood and Robert Melville. \nRed time queue operations in pure Lisp. Information Processing Letters, 13(2):50 53, 1981. Rob R. Hoogerwoord. \nA symmetric set of efficient list operations. Journal of Functional Programming, 2(4):505-513, October \n1992. Richard M. Karp. An introduction to randomized al\u00adgorithms. Discrete Applied Mathematics, 34(1-3):165\u00ad201, \nNovember 1991. Eugene W. Myers. An applicative random access stack. Information Processing Letters, 17(5):241 \n248, Decem\u00ad ber 1983. Eugene W. Myers. Efficient applicative data types. In Ilth Annual ACM Symposium \non Principles of Programming Languages, pages 66 75. ACM, January 1984. Salt Lake City, Utah, USA. Chris \nOkssaki. Purely functional random access lists. In SIGPLAN SIGARCH-WG2.8 Conference on Func\u00adtional Programming \nLanguages and Computer Archi\u00adtecture, pages 86 95. La Jolla, California, USA, ACM Press, June 1995. Chris \nOkasaki. Simple and efficient purely functional queues and deques. Journal of Functional Program\u00ad ming, \nto appear. P. Raghavan and M. Snir. Memory versus randomiza\u00adtion in on line algorithms. IBM Journal of \nResearch and Development, 38(6):683 707, November 1994. Neil Sarnak. Persistent Data Structures. PhD \nthesis, Department of Computer Science, New York Univer\u00adsity, 1986. Berry Schoenmakers. Data Structures \nand Amor\u00adtized Complexity in a Functional Setting. PhD thesis, Department of Mathematics and Computing \nScience, Eindhoven University of Technology, September 1992. E. Schonberg, J. T. Schwartz, and M. Sharir. \nAn auto\u00admatic technique for selection of data representations in SETL programs. ACM Transactions on Programming \nLanguages and Systems, 3(2):126-143, 1981. Zhong Shao and Andrew W. Appel. Space efficient clo\u00adsure represent \nations. In Proceedings of 1994 ACM Con\u00adference on Lisp and Functional Proqramminq, pages 150 161. Orlando, \nFlorida, USA, Jun~ 1994. ~h~ p~o\u00adceedings also appears as Lisp Pointers, Volume VII, Number 3, July September \n1994. ACM Press. Zhong Shao, John H. Reppy, and Andrew Appel. Un\u00adrolling lists. In Proceedings of 1994 \nACM Conference on Lisp and Functional Programming, pages 185 195. Orlando, Florida, USA, June 1994. The \nproceedings also appears as Lisp Pointers, Volume VII, Number 3, July-September 1994. ACM Press. Code \n slgnazure nmi = sig type Bag val void: unit -> Bag val insert: int * Bag -> Bag val nenber: int * Bag \n-> bool val size: Bag -> i.nt val bag21ist: Bag -> int list val list2bag: int list -> Bag end functor \nBagByList. () : BAG = struct type Bag = int * int list fun void () = (o, [1) fun insert (k, (n, 1)) = \n(n+l, k::l) fun member (k, (-, 1)) = exists (fn h=>h=k) 1 fun size (n, _) =n fun bag21ist (n, 1) = 1 \nfun list2bag 1 = (lengthl, 1) end functor BagByMap (Map: INTMAP) : BAG = struct local funconses (k, O, \n1) = 1 I conses (k, n, 1) = conses (k, n-1, k::l) in type Bag = int Map.intmap val void = Map.empty fun \ninsert (k, b) z case Map.peek (b, k) of NONE => Map.insert (b, k, 1) I SOMEn => Map.insert (b, k, n+l) \nfun member (k, b) = case Map.peek (b, k) of NONE => false I SOME _ => true val size = Map.numItems fun \nbag21ist b = Map.fold conses b [1 fun list2bag 1 = fold insert 1 (void ()) end end Figure 2. The signature \nfor Bag, and its two representations. NOTE. The integer map library (the signature is INTMAP, and the \nstructure IntMap) from SML/NJ 0.93 is used to implement the BagByMap representation of Bag. The inte\u00adger \nmaps are implemented by trees of bounded balance; consult SML/NJ 0.93 documentation for details. Notice \nthat, if structure U = BagByListo and structure V = BagByMap(IntMap), then V.list2bag o U.bag21ist con\u00adverts \na bag from its list representation to the map rep\u00adresentation, and U.list2bag o V.bag21ist converts abag \nfrom its map represent ation to the list representation. signature BAG-COST.MATRIX = sig val cVoid: real \nval cInsert: real vector val cMember: real vector val cBag21ist: real vector val cList2bag: real vector \nend signature TUO.BAGS = sig structure U: sig structure Bag: BAG; structure CostMatrix: BAG_COST_MATRIX \nend structure V: sig structure Bag: BAG; structure CostMatrix: BAG-COST_MATRIX end end functor Mix (TwoBags: \nTWO-BAGS): BAG = struct open TwoBags datatype Union = U of U.Bag.Bag I V of V.Bag.Bag type Bag = Union \nref fun size (ref (Ubag)) = U.Bag.size bag I size (ref (V bag)) = V.Bag.size bag fun state (ref (U _)) \n= O I state (ref (V _)) = 1 fun conv (b as ref (Ubag)) = (b := V (V.Bag.list2bag (U.Bag.bag21ist bag)); \nb) I conv (b as ref (Vbag)) = (b :=U (U.Bag.list2bag (V.Bag.bag21ist bag)); b) fun plainInsert (k, b \nas ref (Ubag)) = ref (U (U.Bag.insert (k, bag))) I plainInsert (k, b as ref (Vbag)) = ref (V (V.Bag.insert \n(k, bag))) val ... P-insert ... = . . . built from C-insert, which is built from U.CostMatrix and V.CostMatrix \n. . . fun insert (k, b) = case . . . the new state, which is decided by coin tossing and based on P-Insert-(state \nb)[size b] . . . of O => plainInsert (k, b) I 1 => plainInsert (k, convb) ..... end Figure 3. A skeleton \nof the SML code that mixes two representations of the bag aggregate into one. NOTE. Actually, we do not \nuse coin tossing if the aggregate size is small (< 24). The operation will always uses the current represent \nation. Also, a void operation returns with equal probability an empty bag of either one of the two represent \nations.  \n\t\t\t", "proc_id": "232627", "abstract": "The designs and implementations of efficient aggregate data structures have been important issues in functional programming. It is not clear how to select a good representation for an aggregate when access patterns to the aggregate are highly variant, or even unpredictable. Previous approaches rely on compile--time analyses or programmer annotations. These methods can be unreliable because they try to predict a program's behavior before it is executed.We propose a probabilistic approach, which is based on Markov processes, for automatic selection of data representations. The selection is modeled as a random process moving in a graph with weighted edges. The proposed approach employs coin tossing at run--time to help choosing a suitable data representation. The transition probability function used by the coin tossing is constructed in a simple and common way from a measured cost function. We show that, under this setting, random selections of data representations can be quite effective. The probabilistic approach is used to implement bag aggregates, and the performance results are compared to those of deterministic selection strategies.", "authors": [{"name": "Tyng-Ruey Chuang", "author_profile_id": "81452596926", "affiliation": "Institute of Information Science, Academia Sinica, Nankang, Taipei 11529, Taiwan", "person_id": "P285674", "email_address": "", "orcid_id": ""}, {"name": "Wen L. Hwang", "author_profile_id": "81350589479", "affiliation": "Institute of Information Science, Academia Sinica, Nankang, Taipei 11529, Taiwan", "person_id": "P297516", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/232627.232648", "year": "1996", "article_id": "232648", "conference": "ICFP", "title": "A probabilistic approach to the problem of automatic selection of data representations", "url": "http://dl.acm.org/citation.cfm?id=232648"}