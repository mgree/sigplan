{"article_publication_date": "06-15-1996", "fulltext": "\n A Provable Time and Space Efficient Implementation of NESL Guy E. Blelloch and John Greiner Carnegie \nMellon University {blelloch, jdg}@cs. emu. edu Abstract In this paper we prove time and space bounds \nfor the im\u00adplementation of the programming language NESL on various parallel machine models. NESL is \na sugared typed J-calculus with a set of array primitives and an explicit parallel map over arrays. Our \nresults extend previous work on provable implementation bounds for functional languages by consid\u00adering \nspace and by including arrays. For modeling the cost of NESL we augment a standard call-by-value operational \nsemantics to return two cost measures: a DAG represent\u00ading the sequential dependence in the computation, \nand a measure of the space taken by a sequential implementation. We show that a NESL program with w work \n(nodes in the DAG), d depth (levels in the DAG), and s sequential space can be implemented on a p processor \nbutterfly network, hy\u00adpercube, or CRCW PRAM usin O(w/p + d log p) time and ? 0(s + dp logp) reachable \nspace. For programs with suffi\u00adcient parallelism these bounds are optimal in that they give linew speedup \nand use space within a constant factor of the sequential space. 1 Introduction This paper presents a \nprovably time and space efficient im\u00adplementation of the parallel programming language NESL [6]. NESL \nis a strongly typed call-by-value functional language loosely based on ML. It has been implemented on \nseveral parallel machines [8], and has been used both for teaching parallel algorithms [9, 7], and implementing \nvarious applica\u00adtions [17, 4, 1]. The parallelism in the language is based on including a primitive sequence \ndata type, a parallel map op\u00aderation, and a small set of primitive operations on sequences. To be useful \nfor analyzing parallel algorithms, NESL was de\u00adsigned with rules for calculating the work (the total \nnumber of operations executed) and depth (the longest chain of se\u00adquential dependence) of a computation. \nThese are standard cost measures in the analysis of parallel algorithms [23, 22]. In this paper we formalize \nthese rules and give provable im\u00adplementation bounds for both space and time. lThe implementation is \nbased on a randomized algorithm for the fetch-and-add operator and will therefore run within the given \ntime with high probability. Permissionto make digitalkrd copy of part or all of tiIs work for personal \norclassroomusais rantadwithoutfeeprovidedthatcopiesarenotmade 1 or distributed for pro t or commercial \nadvantage, the mpyright notice, the title of the publication and its date appear, and notioe is given \nthat copyingisbypermissionofACfvf,Inc.Tompy otherwise,bJrepublish,to post on servers, or to rwdistributs \nto lists, requires prior specific permission andior a fee. ICFP 96 5/96 PA, USA 01996 ACM 0-S9791-771-596/0005...$S.50 \nThe idea of a provably efficient implementation is to add to the semantics of the language an accounting \nof costs, and then to prove a mapping of these costs into running time and/or space of the implementation \non concrete machine models (or possibly to costs in other languages). The mo\u00adtivation is to assure that \nthe costs of a program are well defined and to make guarantees about the performance of the implementation. \nIn previous work we have studied prov\u00adably time efficient parallel implementations of the J-calculus \nusing both call-by-value [3] and speculative parallelism [18]. These results accounted for work and depth \nof a compu\u00adtation using a profiling semantics [29, 30] and then related work and depth to running time \non various machine models. This paper applies these ideas to the language NESL and extends the work in \ntwo ways. First, it includes sequences (arrays) as a primitive data type and accounts for them in both \nthe cost semantics and the implementation. This is motivated by the fact that arrays cannot be simulated \neffi\u00adciently in the A-calculus without arrays (the simulation of an array of length n using recursive \ntypes requires a fl(log n) slowdown). Second, it augments the profiling semantics with a cost measure \nfor space and proves bounds on the space needed by the implementation based on this measure. These bounds \nshow that for programs with sufficient parallelism, the parallel execution requires very little extra \nmemory be\u00adyond a standard call-by-value sequential execution. These space bounds use recent results on \nDAG scheduling [2] and are non trivial. Although we use these extensions to prove bounds for NESL, the \ntechniques and results can be applied in a broader context. In particular we translate NESL into a generic \narray language which could be used to express other array extensions, and the space bounds we derive \ncan be ap\u00adplied with minor changes to most languages with fork-join style parallelism, including the \ncall-by-value A-calculus [3]. Cost model. To model time and space in the semantics, we augment a standaxd \ncall-by-value operational semantics to return two cost measures. The first is a DAG which rep\u00adresents \nthe sequential control dependence in the program. The number of levels in the DAG is the depth of a compu\u00adtation, \nand the number of nodes in the DAG is the work of the computation. Although the operational semantics \nitself is sequential, the rules for combining DAGs explicitly de\u00adfine what constructs are parallel. This \nis similar to Hudak and Anderson s [21] use of pomsets (partially ordered mul\u00adtisets) to add intensional \ninformation on execution order to the denotational semantics of the A-calculus. The second cost measure \nis an accounting of the reachable space used by a sequential implementation of the language. This in\u00adeludes \nboth space for values as well as space for any control structures it assumes a reasonably good sequential \nimple\u00admentation. To account for the sharing of space the semantics uses a store. Intermediate language \nand machine. Tosimplify the proof of the implementation bounds we introduce an intermediate language \nand machine. The intermediate language, which we refer to as the w-my language, makes array allocation \nexplicit and breaks certain array operations into atomic op\u00aderat ions. In Section 4 we define the array \nlanguage and give atranslation from the core of NESL (Core-NESL) to the lan\u00adguage. Our intermediate machine, \nthe P-CEK(q) machine, eval\u00aduates the array language. The machine works by keeping a queue of states (threads) \nthat are ready to execute and on each step processes the first q of these states in parallel. The parameter \nq represents the parallelism of the machine to allow for multithreading this is typically slightly larger \nthan the number of processors of the target machine. The fact that only q states are processed on each \nstep and the order in which the states are processed play a crucial role in proving the space bounds. \nPicking an arbitrary set of q states would not be space efficient. The P-CEK(l) machine (q = 1) is a \nsequential machine and closely corresponds to the CESK machine [13]. Section 5 defines the P-CEK (q) \nmachine. Results. Our results are derived, by mapping the Core-NESL costs first to the array language, \nthen to the P-CEK(q) machine and finally to the target machines. We show that a Core-NESL program with \nw work, d depth and s sequential space will run in O(w/p + d Iogp) time and 0(s + dp logp) space, on \np processors of either a butterfly network, hyper\u00adcube or CRC W PRAM. All machine models are randomized, \nso the time bounds are with high probability. The random\u00adization is needed to implement routing of messages \nand the fetch-and-add operation within the specified bounds. The space bounds refer to the reachable \nspace and do not include the cost of garbage collection. Determining whether garbage collection can be \nperformed within the specified bounds is an interesting area of future research. More detail on our assumptions \nabout the machines are given in Section 6. We note that the implementation discussed in this pa\u00adper does \nnot correspond to the current implementation of NESL [8]. The current implementation is based on a tech\u00adnique \ncalled flattening nested parallelism [5], which has very good performance characteristics, but can be \nspace ineffi\u00adcient because it generates too much parallelism. For exam\u00adple the NESL code {count ({a <b: \nain s}) :bin s}, which calculates the rank of each key in a sequence s, cre\u00adates nz parallelism (Is I \n= n). In the current implementation this would require O(n2 ) space, while in the implementation suggested \nin this paper it would require only O(n + p log p) space since the depth is O(1). We are currently studying \nwhether we can combine the ideas from the two implemen\u00adtations. 2 The Language NESL NESL is a nested \ndata-parallel language designed for pro\u00ad gramming parallel algorithms at a very high level. The goal \n in the design was to make parallel algorithms look as close as possible to standard pseudocode. NESL \nhas a polymorphic . function Quicksort (S) if (#S<=1)then S else let a = s[#s/21 ; Sl={ein Sle < a}; \nS2={ein Sle = a}; S3=fein Sle > a}; R = {Quicksort(v) : v in [S1, S3]}; in RIO] ++ S2 ++ R[ll ; Work \n= O(n log n) (expected) Depth = O(log n) (expected) Space = o(n) (expected) Figure 1: The Quicksort algorithm \nin NESL. The operator # returns the length of a sequence. The expression {e in S I e < a} subselects \nall elements of S less than a in parallel. This operation has constant depth and work proportional to \nthe length of S. The expression {(?uicksort (v) : v in [S1, S3] } applies quicksort to S1 and S3 in parallel-the \ndepth is the maximum of the depth of the two recursive calls. The function ++ appends two sequences. \ntype system, a call-by-value semantics, and other than 1/0 and random number generation, is purely functional. \nParal\u00adlelism in NESL is explicit it uses a set of parallel primitives on arrays and a parallel map construct. \nThe parallel map construct maps any expression over the elements of an array in parallel. Since an expression \ncan itself have parallel calls, this allows for the nesting of parallel calls. Such nested par\u00adallelism \nis crucial for expressing parallel divide-and-conquer or nested parallel loops (most data-parallel languages \ndon t permit nesting [19, 28]). For the purpose of analyzing algo\u00adrithms, the definition of NESL includes \nrules for calculating the work and depth of a computation. These rules specify the work and depth for \nthe primitives, and how these costs can be composed across expressions. For example, when mapping a function \nover an array of data, the total work is the sum of the work of the individual applications, and the \ntotal depth is the maximum of the depths of the individ\u00adual applications. Figure 1 shows an example of \nNESL code for the quicksort algorithm, along with the complexities that can be derived from these rules. \nThe rules are defined some\u00adwhat informally in the original language definition [6] and are formalized \nin this paper. To simplify the presentation of this paper we consider a language Core-NESL instead of \nthe full NESL. Core-NESL in\u00ad cludes a representative sample of the data types, constants, and primitive \nfunctions of NESL. For data types, it in\u00ad cludes integers, booleans, pairs, homogeneous arrays, and functions \nit omits floating-point numbers, characters, and user defined datatypes. For constants and primitive \nfunc\u00ad tions, we choose a set that is sufficient to simulate the full set over the included data types \nwith only constant over\u00ad head, except for input/output and pseudo-random number generation. In NESL, \ninput/output is not permitted in par\u00ad allel calls, so it could be added to the semantics by us\u00ad ing streams \nand threading them through the computation. Pseudo-random number generation can be added to the se\u00ad mantics \nby splitting the seed when making parallel calls. Core-NESL can also be strongly typed, although we do \nnot examine any typing issues in this paper. Core-NESL is a sugared A-calculus extended with a set of \nscalar constants, a set of constant functions on arrays, and a parallel map operation. Its syntax is \ndefined as follows: c ::= i I true I false / fst I snd I (scalar const.) +1-1 *1/1<1 # I elt [ index \nI pack I (array const.) <-I addscan I maxscan e ::= c1 x IXr.e I(el, ez) Iifel ez es 1 el e2Iletrec \nJ z = el in e2I {e : x in e} (parallel map) where i, c, and e range over the integers, constants, and \nexpressions, respectively. The scalar constants have their standard definitions. The function # returns \nthe length of an amay, and the function elt extracts an element from an array. The function pack takes \nan array of (value,flag) pairs. It returns an array containing only the values whose corresponding flag \nwas true. The function <-writes a set of values into an array. In particular it takes a pair of ar\u00adrays \nin which the first is a source array and the second is an array of (index, value) pairs. It copies the \nsource array into a destination mray, and in parallel writes each value into the corresponding index \nof the destination array. If array indices are duplicated, the rightmost (index, value) pair will always \nbe written, which corresponds to the priority write scheme for the CRCW PRAM models. The two scan opera\u00adtions \nexecute parallel prefixes and are used as primitives to implement various other array operations (such \nas flattening nested sequences). NESL also has other syntactic forms not included in Core-NESL because \nthey have simple translations to the core syn\u00adtax. For example the notation {e : x in el I e2}, as used \nin the example of Figure 1, evaluates e using all the el\u00adements in x such that e2 is true. This can be \ntranslated to {e : x in pack {(z, ez) : z in el }}. Similarly x[i] is just syntactic sugar for elt z \ni. 3 Core-NESL Semantics Core-NEsL s extensional semantics is based on that of a stan\u00addard call-by-value \nfunctional language with arrays. We de\u00adfine an operational semantics for the language describing the \nevaluation of an expression to a value. In addition, the se\u00admantics also defines the intensional aspects \nof interest: the computation DAG and the space. To measure space usage accurately, we explicitly model \ndata sharing and space allo\u00adcat ion wit h the use of stores. We now define the semantic domains and notation \nused. A value v is either a constant c or a location 1. We thread a store through the semantics to map \nlocations to store values SV, which can be closures, pairs, or arrays. v ::= C[l Sv ;:= cl(~,z , x,e) \nI (vi, vz) I [vo, . . ..vl]l] A closure represents a potentially recursive function defined via Ietrec \nand consists of its definition environment, its name, its bound variable, and its expression body. By \nin\u00adcluding the function name in the closure, we avoid compli\u00adcating the semantics with recursive environments. \nArrays of values, locations, and other objects are used throughout the semantics, so we introduce some \nconvenient notation for them array [VO,. ... vn-1] is also denoted U, where [?71is its length n. An empty \nmapping is denoted by ., and the ex\u00adtension of a mapping with a binding of x to v is denoted 3pace(R, \na) = ifa(1) =ii if a(l) = (vi, vz) if u(l) = c1(13, z , x, e)  where S = (J1e~ 1ocs(1, u) 1OCS(C,a) \n= 1OCS(1,u) = locs((vl, v2),17) = 1OCS(U,a) = locs(cl(E, d,z, e), u) = wh;;e L = E(FV(e) {x,x }) Figure \n4: Semantics functions used in the profiling semantics for defining space. The space function defines \nthe amount of space reachable from a set of roots R. It uses the 10CSfunc\u00adtion which defines the locations \nreachable from a location. FV(e) denotes the set of free variables that appear in e. 13[z + v], where \nx may be in o!om(ll!). If defined, the ele\u00adment bound to x in E is E(x). We also extend this notation \nto lookup sets of variables, returning the set of correspond\u00ading values. The semantics relation, written \nE, a, R + e =&#38;-v, u ; g,s, adds the costs of evaluation to a standard extensional se\u00admantics. The \ncontext of the evaluation consists of an envi\u00adronment E and store a describing the memory, and a set \nR of root locations pointing to data needed by the continua\u00adtion. In a given context, an expression evaluates \nto a value v and a new store a , with a computation DAG g using s units of space. The relation is defined \nby the inference rules of Figure 2, using the definitions of DAG composition given in Figure 3 and space \ngiven in Figure 4. Figure 5 gives the semantics (J) and work cost (6W) of constant function application. \nThe DAG returned by the semantics represents the de\u00adpendence graph of the computation, in which the nodes \nof the graph represent units of computation and the edges rep\u00adresent the control dependence. All data \ndependence are subsumed by these control dependence and are therefore not made explicit in the graph. \nWhen two computations are executed sequentially, the graph from one is attached in series with the graph \nfrom the other, and when a set of computations are executed in parallel, the graphs are con\u00adnected in \nparallel. This means that all graphs returned by the semantics will be series-parallel and will have \na single source and sink. The rules for composing graphs are given in Figure 3, where @ represents sequential \ncomposition, and @ represents parallel composition. As can be seen from where @ is used in Figures 2 \nand 5, the only parallelism in the Core-NESL semantics is in the array primitives and the EACH rule. \nNESL does not execute the two expressions of a function application el ez in parallel, as does the PAL \nmodel [3]. In the DAGs the ordering among the children of a node is important (it is needed for our space \nbounds) so our representation of DAGs keeps this information. Given a DAG g returned by the semantics, \nits work w is the number of nodes in g and its depth d is the number of levels in g includlng the source \nand sink. E,u, RF c &#38; c,a; l,space(R, a) (CONST) E,u, RF x =&#38;\u00ad E(x), a;l, space(RU {E(z)}, a) \n(VAR) E,a, R t Az.e ~ l,c[l H c1(E,., z,e)]; 1, space(RU {1},0) where 1 @ dom(a) (ABSTR) E,a, RUE!(FV(e2)) \nk E,cT, R+ (el,ea) el &#38; vl, 41,u2[1H m;gl, sl (vi, E,aI, wz)]; l@gl Ru{vI} @g2@l, t-ez ~m, max(sl, \ns2)+l m;gz, sz where 1 # dom(u) (PAIR) E,cT, Ru E(FV(e2)) U E(FV(e3)) E,a, REif el 1-el =&#38; e2 e3 \n4v, true, al; a2; l@gl gl, sl E,cl, RFe2&#38;v, @g2, max(sl +1, s2) m;g2, sz (IF-TRUE) E,cT, RU E(FV(e2)) \nU E(F V(e3)) E,cT, RI\u00adif el E el &#38; e2 es =&#38;\u00ad false, v,u3;1 al; gl, sl E,ml, RFe3&#38;v,03; @gl \n@g3, max(sl + 1,s3) g3, s3 (IF-FALSE) E[z E,u, x 1],0[1 R1-letrec + cl(E, z , x,el)], z z==eline2=&#38; \nR + ez =&#38; v,a ; v,a ; l@g, g,s s+l where 1 # dom(cr) (LETREC) E,a, RUE(FV(e2)) E,a, Rt\u00adel ea ~ ~ \nel &#38; c,m; V,U2 UU3;1 gl, @gl sl @g2 E,ol, @69(c, Rbe2&#38;v2,02; v2, a2), max(sl g2, s2 C5(C,V2, \nU2) + 1,s2 + l,space(RU {v}, = V,U3 aa)) (APPC) E,a, RU E(FV(e2)) 1-el &#38; Z,m; gl, S1 UI(l) = cl(E \n, z , x,e) E [x *l][z E,a, R1--ele2&#38; u,as; l@gl@g2@ E,uI, tiv2], l@g, RU {1} + ez &#38; vz, ffz; \ngz, sz a2, R+e=&#38;v, a3; g,s max(sl+l, s2+l, s) (APP) E[z E,u, E,a, RUE(FV(e )) F e &#38; Z,ao; g,s \ncm(l) = ; E+ vj], aj, RUE(FV(e ) {z}) 1-e ~ v~, aj+l; gj, s~ R1\u00ad{e : x in e} =&#38; l ,a~[l + ~]; g@ \n(@~j~ 1) @ (@j), ~ = l;! Vje{o,...,1}l} mw(s, n+m~(~)) where 1 @ dom(u) (EACH) Figure 2: Profiling semantics \nof Core-NESL. DAG (n, n , g D) (n,;,{}) where n is new ,n, n , {(n, @~=jl(n,, nj,D,) ii)} u (J~=~l{(nj, \n[n ])} where n, n axe new u U~=jl Dj) n on Figure 3: Definition of the unitary DAG (1) and serial (CB) \nand parallel (B) composition of DAGs. DAGs are represented as a tride (n, n , D) of the source node, \nthe sink node, and a set of Dairs.., each consistimz of a node and an ordered set (array) ./// of its \nchildren. The newness conditions could be formalized easily by labelling nodes by the expressions they \nrepresent. c v I 15(c.v.u) = Vu I 6 (c. v. u) .,. , -> .7,., + 1il+z2 u 1 if 0(1) = (ii, iz) elt 1Vj. \n1 if 0(1) = (1 , j), cr(l ) = V #  1I;l. 1 ifu(l) =i7 addscan 1 [1 * [~~=o ij, ~~., ~~~~ ,11 (g);:; \n1 1 if u(l) = ;, n = Ill , 1 @dorn(a) index i 1 [l 1-+[o,...,l ]]]] @;:; 1 if 1 @ dom(u) pack 1 1 \n[1 * [vi,,..., vin_l]] q:; 1 if a(l) = ~ n= 1~ , U(lj) = (Vj, CJ), {iO,. . . ,inl-, } = {i/ct = true}, \niO <... < in~-~, 1 ~ dom(a) <-1 1 [1 * ?7[v o/it), . . . ,v nl-,/inf_,]] (8;:: 1)@ (8$S 1) if a(l) \n= (11,12), a(n) = 0, n = Iq , u(h) =?, n = Iv[ , ~(1 j) = (ij, V j), O ~ io,...)i~l-l < la , 1 @ em(a) \nFigure 5: The semantics (6) and work cost (JW ) of constant function atmlication. The semantics of the \nremaining primitives, -,3, /, <, fst, snd, and rnaxscan, are sim il~ to those given. The s&#38;titution \nin the definition of <-gives p~~ity to the last occurrence of any duplicate indices. The space s returned \nby the semantics represents the maximum reachable space during the computation. This is accounted for \nby tracking all the values that might be needed in the continuation (these are kept as a set of labels \ninto the store). The R in the rules in Figure 2 keep these labels. For example, in a function application \nel e2 when executing el the semantics adds to the current set of labels the labels for the free variables \nin e2. Given a set of root labels, the space required by the data is measured by finding all the locations \nreachable from the root locations R, and summing the space for each object stored at these labels (see \nFigure 4). Space is only measured at the leaves of the evaluation tree (in the rules CONST, VAR, ABSTR, \nand APPC). The addition of 1 to the space for many of the other rules represents space that is needed \nfor control information and will be justified in Section 6. In the EACH rule (for {e : z in e}) the expression \ne is evaluated to return a sequence, and the body is then evalu\u00adated for each element of the sequence. \nThe store is threaded through the evaluations of body so as to specify a sequen\u00adtial order for which \nwe will measure space (the space require\u00adments can be different for different orders of execution). The \nexecution of the subcomputations, however, are independent and therefore can be executed in parallel, \nat a cost of some extra space. The challenge is to show that the parallel exe\u00ad cution does not require \nsequential execution. 4 The Array Language To simplify our abstract the array instructions much more \nspace than the specified machine (Section 5) we translate of Core-NESL to a lower-level lan\u00ad guage, \nwhich we refer to as the array language. We do this to make the memory allocation explicit, and to break \nup the array operations that take non-constant work into a set of tasks that each do constant work. The \narray language includes explicit side-effects since it needs to atomically up\u00addate elements of arrays. \nThe syntax of this language is the same as Core-NESL, except t hat we replace index, pack, <-, addscan, \nmaxscan, and the parallel map expression with the constants store, new, fork, and scanadd, scanmax. Applying \nstore to an axray, an array of indices, and a value writes the value into the indexed locations of the \narray. Ap\u00adplying new to an integer creates and returns a new array of that length. And applying fork \nto an integer i and a function applies the function to each of O,..., i 1 and re\u00adturns a dummy value \nO. Since the fork function returns a dummy value, it is only useful for any side-effects. The new scan \noperations compute the same as their counterparts, but allocate memory differently. The semantics relation \nfor the array language, written E, U,R 1-e ~ v, u ; g,s, with the additions given The translation from \narray language is given parallel map ({e : x in cates a result array, and is defined like that of Core-NESL, \nin Figure 6. Core-NEsL array operations to the in Figure 7. The translation of the e}), for example, \nevaluates e, allo\u00adthen forks n threads each of which applies e to the appropriate element of e s value \nand writes its result in the appropriate element of the result array. The motivation for executing a \nfork in the definition of allocat e is for bounding memory use (see the proof of Theorem 3). The following \ntheorem states that the translation only af\u00adfects the work, depth, and space by a constant factor. Theorem \n1 Ij in Core-NESL, .,., {} ~ e =&#38; v, u; g,s, then in the array language, .,,{} 1-T[e] =% v , a ; \ng , s such that 1. v is the same as v, and a is an extension of U, except that each may point to environments \nlarger than those found in v and u, 2. g contains at most k more nodes and levels than g, and 3. S \n< ks, for some constant k.  c v C$(C,V,U) (ig (c, v, u) store 1 v Ulll+ ii?li 1 if a(l) = (11, lz), \nu(ll) = ii, 0(12) = (i, v ) new i 1 0[1 H array(i)] 1 if 1 @ dom(u) E,u, RU E(FV(e2)) k el ~ fork, u \n; gl, S1 E,u , Rt-e2&#38;l, oO; g2, s2 uo(t) = (i, 1 ) :0(2 ) = cl(E , d,z, e) (FORK) E [d tii ][T tij], \noj, RU {l } Fe=+vj,aj+l;gj,$j VjE{O,...,1}l} Figure 6: The semantics rules for the constants of the array \nlanguage. The function array creates and returns a new array of the specified length containing dummy \nvalues. Proof Outline: This is proved by structural induction on the Core-NESL derivation, after generalizing \nthe statement to hold for all evaluation contexts. 0 T[{e :z in e}] = let x = Z [e] 5 The P-CEK(q) Machine \nn=#x r = allocate n Our implementation of Gore-NESL uses an intermediate ab\u00ad-= fork (n, Ai.r[i] := (Ar.Z \n[e ]) z[i]) stract machine we call the P-CEK(q) machine. The P\u00adin r CEK(q) machine executes a sequence \nof steps in which each step takes a queue of states and a store and processes a sub\u00adZ [index] = Ax. set \n(of size at most q) of these states in parallel to generatelet r = allocate z a new queue of states and \na modified store. Each state is -= fork (n, X.r[i] := i) processed by a transition similar to that of \nthe CESK ma\u00adin r chine [13] (a variant of the SECD machine). The number of states grows as threads fork \nand shrinks as threads finish. Z [pack] = Az. In this section we define the machine and in the next sec\u00ad \nlet z = Z [addscan {if snd (x ) 10: z in z}] tion we prove bounds on the time and space it requires as \nn=#.z a function of the work, depth and space of the Core-NEsL r = allocate z[n 1] semantics. -= fork \n(n, Ai.if snd (a[i]) r[z[i]] := fst (a[i]) O) The P-CEK(q) machine is motivated by the P-ECD ma\u00adin r \nchine [3], which was used for proving bounds on the parallel implementation of the call-by-value A-calculus \n(PAL model). q<-] = k. However, because of the need to be space efficient and han\u00adlet a=fst x dle arrays, \nit has three important extensions. First, instead ia=snd x of processing the full queue of states on \neach step it onlyn=#a processes the first q states from the queue on each step. As r = allocate n will \nbe shown in the next section, this modification greatly -= fork (n, Ai.r[i] := a[i]) (copy dest. array) \nreduces the memory needs of the maxhine by reducing the -= fork (# ia[i], Ai.r[fst ia[i]] := snd ia[i]) \nparallelism. Second, the P-CEK(q) machine uses an explicitin r store. This is needed both to allow us \nto model the sharing of data in the accounting of space and to allow the machine T[addscan] = kc. to \nupdate array contents. Third, it allows for a state to fork let a = allocate (# z) an arbitrary number \nof children states in a single step in\u00adin scanadd (x, a) stead of just a pair. This changes how threads \nsynchronize since they have to synchronize as a group rather than as awhere allocate n = let . = fork \n(n, Ai.0) in new n pair. A fourth minor point is that in NESL function appli\u00adel [e2] = elt (cl, e2) cation \nel ez is defined to execute sequentially (as opposed el[e2] := e3 = store (cl, (e2, e3)) to in parallel \nin the PAL model), and the machine reflects this. Modifying the P-CEK(q) machine to execute function \nFigure 7: Translation from the Core-NESL array opera\u00adapplication in parallel, however, would be a minor \nchange. tions to the array language. We use a multiassignment let The P-CEK(q) mmhine executes programs \nof the arraystatement that executes the assignments in sequential order, language. Each machine step \ni is a transition of the form which can also be translated into Core-NESL. Qi, ui ~ Qi+I, UZ+I where \no, is a store, and Q, is a queue of substates. Each substate S is a (C, E, K) triple which, together \nwith the current store, corresponds roughly to state of the sequential CESK machine (C is an expression, \nE is an environment, and K is a continuation). The possible continuations are defined by K ::= nil I(arg \neE) ::K I(fun v) ::K I(end 1)::K These represent the empty continuation, a function s argu\u00adment to be \nevaluated, a function to be applied, and a fin\u00adishing thread, respectively. To evaluate a program e, \nthe machine starts with an empty store and a queue with a single substate (e, ., nil). On each step the \nP-CEK(q) machine takes min(q, [Ql) sub\u00adstates from the front of Q and processes a substate transi\u00adtion \non each in parallel. Each substate can transition into a single new substate, fork off a group of new \nsubstates, or terminate. The new substates are collected together, main\u00adtaining their order relative \nto the original substates, and added back to the front of the queue. Maintaining the order is important \nfor the space bounds. Substate transition can also return modifications for the store. These modifications \nare merged into the store for the next step. In addition to data the store is used to keep synchronization \ncounters for detecting when a set of forked threads completes. The P- CEK(q) machine terminates with \nthe single result substate (exit w,., nil), where v is the result value. The transition rules for the \nP-CEK(q) are given in Fig\u00ad ure 8. Each step is broken into two parts &#38;-l and &#38;-z. The transition \n&#38;-I is applied to each of the selected sub\u00adstates in parallel. It returns a new set of states and \nany modifications that need to be made to the store. The first five rules are minor modifications of \nstandard CESK rules. They also restrict the domain of the environments to the variables that occur in \ntheir associated expression. This is needed to assure the space bounds. We leave out the rules for pairs, \nconditionals, and recursive bindings since these are minor modifications to the standard sequential seman\u00adtics. \nThe fork rule creates a set of i new substates (threads) and adds a synchronization counter 1 to the \nstore. When\u00adever a thread terminates it decrements the synchronization counter, and if the count goes \nto zero, then it applies the continuation. We note that in the fork and scanadd rules all but the last \nnew substates created are marked with a prime. We will use these markings in our space bounds in the \nnext section and otherwise they do not effect the semantics of the machine. The function appK applies \na cent inuat ion, either creating a new state or an End object to be processed in the following transition. \nAny new locations created axe guar\u00adanteed to be distinct from existing locations and from each other. \nWe will account for the cost of getting new labels in the implementation. The transition =% is used for \nthe sequential process\u00ading of the st ates decrementing the synchronization coun\u00adters and modifying the \nshared store. The store is therefore threaded though the substate transitions. As shown in the next section, \nhowever, these transitions can actually be im\u00adplemented in parallel using a fetch-and-add operation and \na priority write, and are therefore only threaded for the sake of the semantics. The P-CEK(l) machine \nwill always return the same value as the sequential semantics of the array language, but in gen\u00aderal \nthe P-CEK (g) machine (g > 1) can return a different value. This can happen if threads interact for example, \nif one thread writes a value into an array that another parallel thread reads. The P-CEK(q) machine can \npossibly execute these in a different order than the P-CEK(l) machine. We claim, however, that this cannot \nhappen with programs in the array language that are generated from Core-NEsL. This is because the translations \nare specified so that each forked thread writes to a different array location and none of the locations \nare read by other threads. The one exception is in the rule for store. In this rule, however, the modifica\u00adtions \nwill always occur in order (left to right) since in the semantics there is no way for an assignment on \nthe right to execute before an assignment on the left. 6 Bounds on Time and Space In this section we \nprove bounds on the space and time taken by the P-CEK(q) machine aa a function of the work (size of DAG), \ndepth (levels in DAG), and space measures returned by the array language profiling semantics. In particular \nwe show the following: 1. Sequential Space. The space returned by the array language semantics is the \nsame within a constant fac\u00adtor as the space required by the P-CEK(l) machine (the machine that only processes \none state at a time). To prove this we formally define the space reachable by the P-CEK(q) machine. We \nnote that in this paper we only consider the reachable space and therefore do not consider the cost of \ngarbage collecting. 2. Parallel Space. There is a one-to-one correspon\u00addence between substates processed \nby the P-CEK(q) machine and nodes of the DAG returned by the ar\u00adray language semantics. Furthermore, \nwe show that the P-CEK(q) machine executes a p depth-first traver\u00adsal (p-DFT) of the DAG. This allows \nus to use previ\u00adously results on DAG scheduling to show that the P-CEK(q) machine never schedules too \nmany substates prematurely relative to the P-CEK(l) machine. This, in turn, allows us to bound the extra \nreachable space required by the P-CEK(q) machine. It also allows us to bound the number of steps taken \nby the P-CEK(q) machine. 3. Parallel Time. We show that each steD of a P-CEK(p log p) machine can be \nimplemented&#38; the ma\u00adchine models (butterfly, hypercube, and PRAM) in O(log p) time, with high probability. \nSince we have a bound on the number of steps required by the ma\u00adchine, this allows us to bound the total \nrunning time for these machines.  By Theorem 1, all these bounds also hold for the Core-NESL semantics. \nFor brevity, we assume that each stage of the implemen\u00adtation preserves extensional correctness, Z.e., \nthat evaluating an expression in each model results in the same value. Prov\u00ading this would be straightforward \nusing standard techniques. Furthermore, the following theorems assume that the array language evaluation \nderivation in question use expressions derived by the translation T. This is needed to ensure mem\u00adory \nis allocated appropriately for the space bounds and to ensure the determinacy of the P-CEK(q) evaluations \nfor the extensional correctness of these theorems (not shown here). (c, E, K), u &#38; appK c K, Corlst \nant (x, E, K), 0 &#38; appK E(x) K, variable (Axe, E, K), u &#38; appK 1 K, [2 H c1(E , -,x, e)] abstraction \nif 1 $! dorn(o), E = restr(E, e) (el ez, E, K), f7 Al [(cl, E, (arg e, restr(E, e,)):: K)], . apply (C3 \n1 v, ., K), a 4 1 [(e, E[x H l][x 6 v], K)], func-call if o(1) = c1(E, x , x, e) (Cl fork 1, ., K), a \n41 [(@ v 0,., K ) , (@ V I,., K ) ,..., fork (K2 v (i -l),, K )], [1 1+ i] if U(Z) = (i, v), K = (end \nz ) :: K, Z # dom(a) (CJI scanadd 1, ., K), a =%1 [(E 1, 121 0,.,K) ,(X 1,121 l,, K) ,..., scanadd (21, \n121 (n -l),,K )], [1 1+ o] if a(l) = (11,12), n = lcr(ll)l , 1 ~ dom(u) (@ c v, ., K), a 41 appK v K, \nu prim-call if J(c, v,u) = v,a (x 1, h 1 i, ., K), a 41 Z(ll,12,1 , i, K), scanadd vhere appK v nil = \n[(exit v,., nil)] a~pK v (arg e E) :: K = [(e, E, (fun v~:: K)] appK vz (fun V1) :: K = [(@ ~1 v2,, K)] \nappK v (end 1) :: K = End(l, K)  restr (E, e) = the environment E restricted to the free variables in \ne. A++[(o, ., K)], c U a o(2) = 1 (last thread of parallel map) (End(l, K), u ), A, o &#38; A,u[l ++a(l) \n 1]Ua @ # 1 { A*[(j, ., K)], a i = 2 1 (last element to sum)(E(ll,12,1 , i, K), ), A, u =% ~ ~, i# \n7 1 { whe~e u(ll) = ~, u(l ) = j, u = a[12 + a(12)~/i]][l + j + ij] (x, d> 4 ~ &#38;2 z+-FX,Ul_Jd vhere \no U u is the union of the stores such that if a location appears in both stores, the binding from the \nright store is kept. [S,,. ... Sri], a = AqI*[Sq+l,... ,Sn], U,J vhere q = min(q, n), AO = H,ao = a, \nand for 1~ j s q , Sj, u &#38;l Xj, a; (Execute these independently in parallel) (Xi, CT;), Aj-I, aj-I \n&#38; Aj, Uj (Execute these sequentially ) vhere the new locations of u~, . . . . u; are renamed if necessary \nto ensure they are &#38;tinct. Figure 8: Definition of the P-CEK(g) machine, omitting the definition \nof the evaluation of scanmax, pairs, conditionals, and re&#38;rsive bindings. Arrays, pairs, closures, \nand synchr~nization counters are kept in the store. The ~xpressions @ v vi and Z 11 121 i are for internal \nuse in evaluating applications and scans, respectively. In the latter, 11 and 12 point to the source \nand destination arrays, respectively, 1 is the running total, and i is the index. L(Q) = U (L@) U ~K(K)) \n(., E,K)6Q LK(nil) = {} LK((arg e E) :: K) = L~(E) ULK(K) LK((fun v) :: K) = {v} U LK(K) LK((end 1) :: \nK) = LK(K) Figure 9: Definitions for the root values L(Q) of a step of the P-CEK(q) machine. This is \na set of values, where labels act as roots into the store. The function LE (E) returns the range of the \nenvironment, and LK (K) returns the root values of a continuation. Sequential Space. To capture the space \nthat is required to implement the P-CEK(q) machine we need to consider on each step both the space that \nis required for the queue (Q) w well as any space in the store u that can be reached via some label in \nthe queue. As mentioned in the previous sec\u00adtion, certain substates in the queue are marked with a prime \n(see the fork rule in Figure 8). We assign no space for these substates since in our implementation we \nwill store a set of forked states in a compacted form as a record with a start count, end count, and \npoint ers to the common function to apply, environment and continuation. We assign the space for this \nstructure to the last of the forked substates, which the semantics does not put a prime on, and therefore \ndo not include any space for the other substates. This is necessary for the parallel space bounds. For \neach of the non primed CEK substates we include the following spaxe: for the com\u00admand (C) we include \nconstant space, for the environment (E) we include space proportional to the size of the domain, and \nfor the stack of continuations (K) we include space pro\u00adportional to the number of entries in the stack \n(here we are just accounting for space required by the queue itself and not for any values that are in \nthe store). To find the root labels into the store we consider all labels accessible either though an \nenvironment or continuation of any of the substates. Definition 1 The reachable space S, (Q;, u~) oj \na step i of the P-CEK(q) machine is the sum of: 1. the queue space S,(Qi) is the sum of (1 + ]El + ]Kl) \nfor those non-primed states (e, E, K) E Qi, and 2. the store space Sc (Q;, u~) = space (L(Q~), u~) \n where space (., .) and L(.) are defined in Figures 4 and 9, respectively, IE[ is the size of dom(-l?), \nand IKI is the length of the continuation stack K. We note that the space for the synchronization counters \nis included in S~ (Q~ ) and not S~ (Qi, ai ) even though the coun\u00adters are kept in the store (the rule \nLK ((end 1) :: K) in Fig\u00adure 9 does not add 1 to the labels). The motivation for this is to allow an \nexact correspondence between the locations in the array language semantics and the locations in the P-CEK(l) \nmachine. Theorem 2 If .,., {} F e =%-v, u; g,s, and e is a transla\u00adtion of a Core-NESL expression, then \nduring the i steps of the P-CEK(l) ezecution [(e,., nil)], 3* [(exit v,., nil)], a the following bound \nholds on the reachable space for some constant k: To prove this we first generalize the statement. The \nfol\u00adlowing lemma considers the steps of P-CEK(l) required to evaluate an expression in some general context \nand bounds the reachable space during those steps by the space speci\u00adfied by the Core-NESL semantics \nplus the queue space at the beginning of the evaluation. Lemma 1 If E, u,R Ee&#38; v,u ; g,s, where e \nis a subex\u00adpression of a translated Core-NESL expression, and for a step i of the P-CEK(l) machine 1. \nQi = [(e, E, K),. ..], 2. the semantics and machine can access the same loca\u00adtions: L(Q;) = R U I?(FV(e)), \nand 3. . . . and these locations have the same values: u(l) = q(l),  d OCS(L(Qi),q) then on some \nfuture step j > i, the machine will execute appK v K, and for some constant k, m&#38;c Sr(Q., a.) < S~(Q;) \n+ ks n=i Proof: We prove this by structural induction on the array language evaluation derivation and \nshow a representative set of the cases. The remaining cases are similar. case VAR, e = m By VAR, s = \nspace(R U {E(z)}, a), and by the definition of the P-CEK(q) machine, j = i. Thus, md~=i S. (Q., CT.) \n= Sq(Qi) + space(L(Q~), ~i) (Def. 1) = S, (Q~) + space (L(Q;), u) (3rd assump.) = Sq(Qi) + S (2nd assump.) \n The other base cases, CONST, ABSTR, and LETREC, are similar. case APP, e = el ez: By induction, we \ncan assume that the lemma holds for el, ez, and the appropriate func\u00adtion body e3. The steps of the P-CEK(l) \nmachine corresponding to these subderivations are numbered il to jl, etc. By definition of the P-CEK \n(q) machine, this implies that il = i+l, iz = jl +1, and is = jz+2, and step jZ + 1 is the appropriate \nfunc-call transition. Let s look at the queues at these important iterations: Q, = [(cl ez, E, K)]*Q \nQi, = [(e,, E, (arg ez restr(E, ez)) :: K)]I+Q Q,, = [(ez, E, (fun 1):: K)]*Q Qjz+l = [(C31 V2, , K)]*Q \nQi, = [(es, E [z N l][z H VZ], K)]*Q where 1 is the value of el, Ujl (1) = c1(E , x, x , ez), and dom(E \n) is restricted to the free variables of ez. Examining the definition of the P-CEK(q) machine and using \nDefinition 1, we have S,(Q,, at) = S,(Qil, uz,) sr(Qj2+1, crj2+1) < S,(Qt, ut) So the reachable space \nin those two steps is not greater than in the others, and using induction we have Relating the queue \nspaces S~ (Q~. ) to S~ (Q~) we see For S~(Qi,), first observe that IE I + 2 < S1 by the definition of \nspace (., ~) since the closure with E must have been the result of a subderivation of el. Thus, and the \nconclusion holds. The PAIR, IF-TRUE, IF-FALSE, and APPC cases fol\u00adlow in similar, but generally simpler, \ninductive man\u00adners. The FORK case is also similar except that a function is applied to many values instead \nof just one. But since the semantics threads the stores through these applications, induction can be \nused as in the APP case. 1 For the proof of Theorem 2 we specialize the above lemma starting with an \nempty environment, store, and roots. Parallel Space. Given the space taken by the P-CEK(l) machine we \nare now concerned with the space taken by the P-CEK(q) machine for a general q. The P-CEK(q) machine \ncan require more space both because it can create many more simultaneous parallel threads (the queue \ncan become much larger), and because it can have simultaneous access to many more locations in the store. \nOur aim is to place bounds on how much extra space is needed. As mentioned, the idea behind the proof \nis to show that the P-CEK(q) executes a p-DFT traversal of the DAG g re\u00adturned by the semantics, then \nuse previous results on the number of nodes scheduled prematurely in a p-DFT [2], and finally use these \nresults to bound the space. By the machine traversing the DAG we mean there is a one-to-one correspon\u00addence \nbetween substate transitions and nodes in the DAG. This implies that each parallel step of the P-CEK(q) \npro\u00adcesses min(q, IQ I) nodes of the DAG, and the total number of substates processed is equal to the \nsize of the DAG (Z.e., the work). The appendix gives a more formal definition of a traversal and a p-DFT. \nThe following theorem shows the correspondence. Lemma 2 If., ., {} h e =% v,u; g,s, and e is a translation \nof a Core-NEsL expression, then there is a one-to-one cor\u00adrespondence between the nodes in g and the \nCEK substates processed in the P-GEK(q) transitions such that the single CEK substate in IQO [ corresponds \nto the root of g and for every step i of the P-CEK(q) all the ready chddren of the min(q, lQi 1) CEK \nsubstates processed on that step will appear in order at the front of Qi+l. Proof Outline: The proof \nof this is similar to the proof of Theorem 2. In particular we generalize the statement to consider an \narbitraxy context and then prove by induction on the rules of the semantics. 0 This theorem together \nwith Theorem 6 in the appendix and the fact that g is series-parallel imply that the P-CEK(q) executes \na p-DFT of g with parameter q. Theorem 5 then bounds the number of premature nodes on any given step \nof the P-CEK(q). A premature node is a node that gets executed out of order (prematurely) relative to \na sequential traversal (i. e., the traversal executed by the P-CEK(l) ma\u00adchine). Having a bound on the \nnumber of premature nodes, we can bound the memory used by these nodes. Theorem 3 Ij .,., {} E e =%-v,a; \ng,s, where e is a subez\u00adpression of a translated Core-NESL expression, and the num\u00ad ber of levels in \ng is d, then for the i steps of the P-CEK(q) execution [(e,, nil)], ~ W* [(exit v,., nil)], a the following \nbound on the reachable space holds for some constant k: ::ISr(Qn, GJ < k(S + dq) Proof: Since the P-CEK(q) \nmachine executes a p-DFT of g with parameter q, on any step of the P-CEK(q) there can be at most dq nodes \nexecuted prematurely relative to the P-CEK(l) machine (see Theorem 5 in the appendix). If each substate \ntransition in a step i of a P-CEK(q) machine added at most constant space to the next state of the ma\u00adchine, \nthen the proof would be easy. In particuku since the maximum space taken by any step of the P-CEK(l) \nmachine is ks, and on any step of the P-C!EK (q) machine there are at most dq substate transitions that \nwere executed prema\u00adturely relative to some step of the P-CEK(l) machine, each of which allocated at \nmost constant space, the total space will be k (s + dq). The reason for using the compact repre\u00adsentation \nof forked threads discussed earlier is to guarantee that the fork transition in Figure 8 only creates \nconstant space. The one transition that creates more than constant space is @ new i since it allocates \nan array of size i. However, new is only used in the translation from Core-NESL in the rule allocate \nn = let -= fork (n, Ji.0) in new n For the new transition to be premature, all the n forked threads would \nalso need to be premature. We can then account for the n space required by the allocation of the array \nagainst these n forked threads. Our bounds therefore still hold even though this one transition creates \nmore than constant space. 0 [(e,, nil)], ~ ~ [(exit v,., nil)], a Parallel Time. Our final goal is to \nprove bounds on the time taken by each step of the P-CEK(q) machine on the butterfly, hypercube, and \nPRAM machine models, each with p processors. To account for memory latency in the butterfly and hypercube, \nand for the latency in the fetch-and-add operation for all three machines, we process p log p states \non each step instead of just p (z. e., we use a P-CEK(p logp) machine). Our simulation uses the ~etch-and-ado! \noperation [16] (or mult iprefix [26] ). In this operation, each processor has an address and an integer \nvalue i. In parallel all processors can atomically fetch the value from the address while incre\u00admenting \nthe value by i. In our case it is important that the fetch and add is stable if two processors make a \nrequest simultaneously, the processor with the smaller ID will ac\u00adcess the count er first. The stable \nfetch-and-add operation can be implemented in a butterfly or hypercube network by combining requests \nas they go through the network [26], and on a PRAM by various other techniques [24, 15]. For all machines, \nif each processor makes up to m fetch-and\u00adadd requests, all requests can be processed in O(m + log p) \ntime with high probability (the bounds can be slightly im\u00adproved on the CRCW PRAM [15]). These bounds \nassume the butterfly has p logz p switches which can do the com\u00adbining, the hypercube can communicate \nand combine over all wires simultaneously (multiport version), and that the CRCW PRAM is the priority \nwrite version. The fetch-and\u00adadd operation is used in four places in our implementation: decrementing \nthe synchronization variables, the scanadd operation, allocating tasks to processors, and memory allo\u00adcation \nfor creating new arrays and getting new labels. The following theorem assumes that program expressions \nare of constant size and therefore does not include the time for looking up variables in the environment. \nIt would be easy to generalize to account for variable lookup [3]. Theorem 4 A step of a P-CEK(p log \np) machine can be simulated in O (log p) time on the butterfly, hypercube, or CRCW PRAM machme models. \nProof: Each processor takes log p elements from the queue and executes the transit ion ~ 1. Since we \nstore the queue in a compacted form (i. e., a set of forked threads are stored in constant space) we \nuse a fetch-and-add operation to as\u00adsign tasks to processors. We make sure that they are as\u00adsigned to \nprocessors in order (lower numbered processors get lower numbered states). Each of the substate transi\u00adtions \ncan be executed with a constant number of memory references and local operations. This assumes that envi\u00adronment \nlookup takes constant time, as stated above, and that when forking a set of threads the forked threads \nare represented in compacted form (otherwise forking n threads would take n time). We also note that \nany memory alloca\u00adtion that is required can be executed with a fetch-and-add to a global queue. Given \nthese conditions, each processor makes a total of k log p memory and fetch-and-add requests, taking O(log \np) time using the above stated bounds. The second substate transition &#38;-z is more involved since \nwe have to update the synchronization counters and merge the stores as if they were done sequentially. \nTo update the synchronization counters we use the fetch-and-add oper\u00adation. Since each processor can \nhave at most log p requests, this takes O(log p) time. The fetch-and-add can also be used for the transition \non E (~, 17,t , i, K). For merging the stores the only operation that could conflict is a store instruction \nas part of implementing the <-operation. However since the substates have the same order aa the processors, \na priority concurrent write (with higher numbered processors given the higher priority) will guarantee \nthat rightmost value will be written. To finish the step of the P-CEK(q) we need to merge the states \nand put them back on the front of the queue. This can be implemented with a fetch-and-add. 0 To determine \nthe total running time we use the result that a P-DFT with parameter q on a DAG with w nodes and d levels \ntakes O(w/q + d) steps [2]. Corollary 1 If.,.,{} 1-e A v, a; g, .s, and e 2s a transla\u00ad tion of a Core-NESL \nexpression, then the P-CEK(plogp) steps [(e,., nil)],. (4P)* [(exit v,., nil)], a can be simulated using \np processors of a CRCW PRAM in O(w/p + d logp) time, where w and d are the numbers of nodes and levels \nof g, respect wely. Proof: Lemma 2 relates the DAG g to the P-CEK(q) com\u00adputation, where q = p log p. \nThere are w/q + d steps, and each step takes O(log p) time. 0 7 Related Work and Discussion Several researchers \nhave used cost-augmented semantics for automatic time analysis or definitional purposes [29, 30, 31, \n27, 33, 14]. Hudak and Anderson [21] used partially ordered multisets (pomsets) to model the dependence \nin various im\u00adplementations of the A-calculus. Because of the relationship between partially ordered \nsets and DAGs, these are quite similar in concept to our DAGS. For our bounds, however, we also need \nto keep an order among the children of each node, which cannot be represented within the single pomset. \nNone of the above work includes costs that model space or relates the costs of the modeled language to \nthose in ma\u00adchine models. There have been a handful of studies that use semantics to model the reachable \nspace of sequential compu\u00adtations, in the context of both garbage collection (e.g., [25]) and copy avoidance \n(e. g., [20]). None of this work, how\u00adever, has considered the extra reachable space required by a parallel \nevaluation. There have been a sequence of studies that place space bounds on implementations of parallel \nlan\u00adguages [11, 10, 12, 2]. For a shared memory model, which is required to efficiently simulate the \nA-calculus because of shared pointers, the best results are those by Blelloch, Gib\u00adbons, and Matias [2], \nwhich are the results we use in this ~ar)er. Provable time bounds for mapping nested data-parallel languages \nonto the PRAM were considered by Blelloch [5] and in the definition of NESL [6], but the time bounds \nare restricted to a class of program that are called contained. Similar results were shown by Suciu and \nTannen for a par\u00adallel language baaed on while loops and map recursion [32]. Practical issues. The design \nof the intermediate language and machine were optimized to simplify the proofs rather than for practical \nconsiderations. Here we briefly discuss some modifications to make the implementation more prac\u00adtical \nwithout affecting the asymptotic bounds (although they would complicate the analysis). The first modification \nis to cluster the computations into larger blocks, and then have the substate transitions of the P-CEK(q) \nmachine each exe\u00adcute one of these blocks. This would have three advantages: (1) it would allow the use \nof standard sequential compiler to compile and optimize each block; (2) it would reduce the scheduling \noverhead and need for synchronization; and (3) it would improve cache performance since a given block \nis likely to have repeated accesses to the same memory loca\u00adtion. A second modification to the P-CEK(q) \nis to schedule the sequence of state transitions that correspond to a given thread on the same processor \n(assuming it gets scheduled on consecutive transitions). This would further improve cache performance. \nIn terms of the practicality of our target machine mod\u00adels, some readers might complain that they do \nnot prop\u00aderly account for communication costs. First we note that our model does account for communication \nlatency. We already use multithreading for hiding the log p latency in the butterfly and hypercube networks. \nThe effect of hav\u00ading a larger latency L would simply require a higher degree of multithreading and would \nappear in our time bounds as O(w/p + Ld), and similarly would replace the logp in the space bounds. Of \ncourse this requires a machine that can properly hide latency. In terms of throughput we note that on \nthe most recent parallel machines, such as the T3E, with proper latency hiding, global bandwidth is no \nmore of an is\u00adsue than local memory bandwidth on a sequential machine. In both cases performance relies \nheavily on effective use of the cache. Finally, our implementation uses a fetch-and-add operation. Although \nthis is not very practical on many ma\u00adchines, it is likely that the need could be removed using the techniques \ndiscussed in [2]. References [1] G. Blelloch, G. L. Miller, and D. Talmor. Developing a practical projection-based \nparallel Delaunay algorithm. In Proceedings ACM Symposium on Computational Ge\u00adometry, May 1996. [2] Guy \nBlelloch, Phil Gibbons, and Yossi Matias. Prov\u00adably efficient scheduling for languages with fine-grained \nparallelism. In ACM Symposium on Parallel Algorithms and Architectures, July 1995. [3] Guy Blelloch and \nJohn Greiner. Parallelism in sequen\u00adtial functional languages. In Proceedings 7th Inter\u00adnational Conference \non Functional Programming Lan\u00ad guages and Computer Architecture, pages 226 237, June 1995. [4] Guy Blelloch \nand Girija Narlikar. A comparison of two n-body algorithms. In Dimacs implementation chal\u00adlenge workshop, \nOctober 1994. [5] Guy E. Blelloch. Vector Models for Data-Parallel Com\u00adputing. MIT Press, 1990. [6] Guy \nE. Blelloch. NESL: A nested data-parallel lan\u00adguage (version 3.1). Technical Report CMU-CS-95-170, Carnegie \nMellon University, 1995. [7] Guy E. Blelloch. Programming parallel algorithms. Communications of the \nACM, March 1996. [8] Guy E. Blelloch, Siddhartha Chatterjee, Jonathan C. Hardwick, Jay Sipelstein, and \nMarco Zagha. Implemen\u00adtation of a portable nested data-parallel language. Jour\u00adnal of Parallel and Distributed \nComputing, 21(1) :4 14, April 1994. [9] Guy E. Blelloch and Jonathan C. Hardwick. Class not es: Programming \nparallel algorithms. Technical Report CMU-CS-93-1 15, Carnegie Mellon University, February 1993. [10] \nR. D. Blumofe and C. E. Leiserson. Space-efficient scheduling of multit breaded computations. In Proc. \n25th ACM Symp. on Theory of Computing, pages 362 371, May 1993. [11] F. Warren Burton. Storage management \nin virtual tree machines. IEEE Trans. on Computers, 37(3) :321 328, 1988. [12] F. Wzmren Burton and David \nJ. Simpson. Space ef\u00adficient execution of deterministic parallel programs. Manuscript, December 1994. \n[13] Matthias Felleisen and Daniel P. Friedman. A calculus for assignments in higher-order languages. \nIn Proceed\u00adings 13thACM Symposium on Principles of Program\u00adming Languages, pages 314 325, January 1987. \n[14] Cormac Flanagan and Mattias Felleisen. The semantics of future and its use in program optimization. \nIn Pro\u00adceedings 22nd ACM Symposium on Principles of Pro\u00adgramming Languages, pages 209-220, 1995. [15] \nJoseph Gil and Yossi Matias. Fast and efficient simula\u00adtions among CRCW PRAMs. Journal of Parallel and \nDistributed Computing, 23(2):135-148, November 1994. [16] AlIan Gottlieb, B. D. Lubachevsky, and Larry \nRudolph. Basic techniques for the efficient coordination of very large numbers of cooperating sequential \nprocessors. ACM Transactions on Programming Languages and Systems, 5(2), April 1983. [17] John Greiner. \nA comparison of parallel algorithms for connected components. In Proceedings 6th ACM Sym\u00adposium on Parallel \nAlgorithms and Architectures, pages 16-25, June 1994. [18] John Greiner and Guy E. Blelloch. A provably \ntime\u00adefficient parallel implementation of full speculation. In Proceedings 23rd ACM Symposium on Principles \nof Programming Languages, pages 309 321, January 1996. [19] High Performance Fortran Forum. High Performance \nFortran Language Specification, May 1993. [20] Paul Hudak. A semantic model of reference counting and \nits abstraction (detailed summary). In Proceed\u00adings ACM Conference on LISP and Functional Pro\u00adgrammmg, \npages 351 363, August 1986. [21] Paul Hudak and Steve Anderson. Pomset interpreta\u00adtions of parallel functional \nprograms. In Proceedings 3rd International Conference on Functional Pr-ogram\u00adming Languages and Computer \nArchitecture, number 274 in Lecture Notes in Computer Science, pages 234 256. Springer-Verlag, September \n1987. [22] Joseph J&#38;J&#38;. An Introduction to Parallel Algorithms. Addison-Wesley, Reading, Mass., \n1992. [23] R. M. Karp and V. Ramachandran. Parallel algorithms for shared memory machines. In J. Van \nLeeuwen, edi\u00adtor, Handbook of Theoretical Computer Science Vol\u00adume A: Algorithms and Complexity. MIT \nPress, Cam\u00adbridge, Mass., 1990. [24] Yossi Matias and Uzi Vishkin. On parallel hashing and integer sorting. \nJournal of Algorithms, 12(4) :573 606, 1991. [25] Greg Morrisett, Matthias Felleisen, and Robert Harper. \nAbstract models of memory management. In Proceed\u00adings of the Symposzum on Functional Programming and \nComputer Architecture, pages 66-77, June 1995. [26] Abhiram G. Ranade. Fluent Parallel Computation. PhD \nthesis, Yale University, New Haven, CT, 1989. [27] Paul Roe. Parallel Programming using Functional Lan\u00adguages. \nPhD thesis, Department of Computing Science, University of Glasgow, February 1991. [28] J. R. Rose and \nG. L. Steele Jr. C*: An extended C language for data parallel programming. In Proceed\u00adings Second International \nConference on Supercomput\u00ading, Vol. 2, pages 2 16, May 1987. [29] Mads Rosendahl. Automatic complexity \nanalysis. In Proceedings ~th International Conference on Functional Programming Languages and Computer \nArchitecture. Springer-Verlag, September 1989. [30] David Sands. Calculi for Time Analysis of Functional \nPrograms. PhD thesis, University of London, Imperial College, September 1990. [31] David B. Skillicorn \nand W. Cai. A cost calculus for parallel functional programming. Journal of Parallel and Distributed \nComputing, 28(1) :65 83, July 1995. [32] Dan Suciu and Val Tannen. Efficient compilation of high-level \ndata parallel algorithms. In P? oceedzngs 6th ACM Symposium on Parallel Algorithms and Architec\u00adtures, \npages 57 66, June 1994. [33] Wolf Zimmerman. Complexity issues in the design of functional languages \nwith explicit parallelism. In Pro\u00adceedings International Conference on Computer Lan\u00adguages, pages 34 \n43, 1992. A DAG Definitions and Theorems The following definitions and theorems are from Blelloch, Gibbons, \nand Matias [2]. A p-traversal of a DAG g, for p > 1, is a sequence of k > 1 steps, where each step i, \nfor z = 1, . . . . k, defines a set of nodes, V, (that are visited, or scheduled, at this step), such \nthat the following three properties hold. First, each node appears exactly once in the schedule, i.e., \nthe sets VI, . . . . vk partition the nodes of g. Second, a node is scheduled only after all its ancestors \nhave been, i.e., if n E V, and n is an ancestor of n , then n c Vj for some j < i. Third, each step consists \nof at most D nodes, i.e., for allie {l,.. .,k}, Iul <p. Consider a traversal T = VI,. ...Vk of g. A node \nn E g is scheduled prior to a step i in T if n appears in VI U. UV, 1. An unscheduled node n is ready \nat step i in T if all its ancestors (equivalently, all its parents) are scheduled prior to step i. The \ngreedy p-traversal, TP of a DAG g, based on a l-traversal of g, T1, is the traversal that on each step \ni, schedules the p earliest nodes in T1 that are ready. In other words, for all ready nodes n and n , \nif n precedes n in T1, then either both are scheduled, neither are scheduled, or only n is scheduled. \nLet Tp be the greedy p-traversal based on a l-traversal T1. For each prefix, UP, of TP, consider the \nlongest prefix, cm, of T1 that includes only nodes in UP. We say a node is premature with respect to \nUP if it is in UP but not in al. Theorem 5 For any DA G of depth d and any 1-traversal T, the maximum \nnumber of premature nodes an the greedy p-traversal based on T is at most (p l)(d 1). Consider a series-parallel \nDAG g. Let A be an array initially containing the root node of g. Repeat the following two steps until \nall nodes in g have been scheduled: 1. Schedule the first min(p, 1A]) nodes from A. 2. Replace each \nnewly scheduled node by its ready chil\u00addren, in left-to-right order, in place in the array A.  Theorem \n6 The algorithm above makes the greedy p-DFT based on the l-DFT of a series-parallel DA G g. 225 \n\t\t\t", "proc_id": "232627", "abstract": "In this paper we prove time and space bounds for the implementation of the programming language NESL on various parallel machine models. NESL is a sugared typed &amp;lambda;-calculus with a set of array primitives and an explicit parallel map over arrays. Our results extend previous work on provable implementation bounds for functional languages by considering space and by including arrays. For modeling the cost of NESL we augment a standard call-by-value operational semantics to return two cost measures: a DAG representing the sequential dependence in the computation, and a measure of the space taken by a sequential implementation. We show that a NESL program with <i>w</i> work (nodes in the DAG), <i>d</i> depth (levels in the DAG), and <i>s</i> sequential space can be implemented on a <i>p</i> processor butterfly network, hypercube, or CRCW PRAM using <i>O</i>(<i>w/p</i> + <i>d</i> log <i>p</i>) time and <i>O</i>(<i>s</i> + <i>dp</i> log <i>p</i>) reachable space.<sup>1</sup> For programs with sufficient parallelism these bounds are optimal in that they give linear speedup and use space within a constant factor of the sequential space.", "authors": [{"name": "Guy E. Blelloch", "author_profile_id": "81100282539", "affiliation": "Carnegie Mellon University", "person_id": "P100820", "email_address": "", "orcid_id": ""}, {"name": "John Greiner", "author_profile_id": "81100082046", "affiliation": "Carnegie Mellon University", "person_id": "P144053", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/232627.232650", "year": "1996", "article_id": "232650", "conference": "ICFP", "title": "A provable time and space efficient implementation of NESL", "url": "http://dl.acm.org/citation.cfm?id=232650"}