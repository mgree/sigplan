{"article_publication_date": "06-15-1996", "fulltext": "\n Synchronous Kahn Networks Paul Caspi VERIMAG Miniparc-ZIRST Rue Lavoisier 38330 Montbonnot St-Martin \nFrance e-mail: Pad. Caspi@imag.fr Abstract Synchronous data-flow is a programming paradigm which has \nbeen successfully applied in reactive systems. In this context, it can be characterized as some class \nof static bounded memory data-flow networks. In particular, these networks are not recursively defined, \nand obey some kind of synchronous constraints (clock calculus). Based on Kahn s relationship between \ndata-flow and stream functions, the synchronous constraints can be related to Wadler s listless\u00ad ness, \nand can be seen as sufficient conditions ensuring listless ewduation. As a by-product, those networks \nenjoy efficient compiling techniques. In this paper, we show that it is pos\u00ad sible to extend the class \nof static synchronous data-flow to higher order and dynamical networks, thus giving sense to a larger \nclass of synchronous data-flow networks. This is done by extending both the synchronous opera\u00ad tional \nsemantics, the clock calculus and the compiling tech\u00ad nique of static data-flow networks, to these more \ngeneral networks. 1 Introduction 1.1 Some milestones in data-flow programming In the seventies, the Lucid \nlanguage was proposed [5, 4] as a way of providing functional languages with stream based iterations. \nAt the same time, Kahn [15] showed that the se\u00admantics of networks of asynchronous deterministic processes \ncould be described as systems of recursive equations over streams, very similar to Lucid programs. Then, \nthe concept of lazy evaluation emerged [12, 23], accounting for finite and infinite data structures like \nstreams. Today, modern lazy functional languages such as LML and Haskell [6, 22] easily allow to write \ndata-flow programs, by expressing streams as abstract data types. Such languages provide nice features, \nsuch as currying and higher order programming. Yet, these programs are sometimes rather inefficient, \nand Wadler [25] proposed new techniques, which he called listlessness [25] so m to try t~ overcome the \nproblem. The idea wm to This work has been supported by an INRIA fellowship Permission to make digitalrhard \ncopy of part or all of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for profit or commercial advantage, the copyright notice, the titte of the \npublication and its date appear, and notice is given that copying is by permission of ACM, Inc. To copy \notherwise, to republish, to post on servers, or to redistribute to Ma, requires prior specific permission \nandlor a fee. ICFP 96 !Y96 PA, USA GI 1996 ACM 0-69791-771 -5/96/0005 ...$3.50 Marc Pouzet* School Of \nComputer Science McGill University 3486 University Street Montr4al, H3A 2A7 Canada e-mail: pouzet@cs. \nmcgill.ca avoid constructing intermediate lists in list programs, if the elements of those lists were \nto be consumed as soon as pro\u00adduced. Later, he generalized this technique to other recur\u00adsive data types, \nand called it deforestation [26]. 1.2 Synchronous reactive data-flow Meanwhile, automatic control, and \nsignal processing engi\u00adneers faced the problem of moving from analog devices to sequential computers. \nThey have always used systems of recursive equations over streams of values (signals) as a nat\u00adural formalism \nfor reasoning about their systems. They often found translation of these equations into sequential programs \nas boring and error-prone. Some of them found that it was possible to handle this translation automati\u00adcally, \nthat is, to compile such programs. For instance, some of the programs running on the Airbus A320 aircraft \nhave been obtained in this way. This led computer scientists to propose languages (LUSTRE, SIGNAL), toolboxes \n(PTOLEMY) and compilers for application domain [2, 20, 16] which they called Synchronous Data-flow (By \nthe same time, this class of programs was recognized to belong to the so-called synchronous programming \nfamily, whose most character\u00adistic member is ESTEREL [7]). These compilers [21] behave very much like \nWadler s list\u00adless transformer, i.e. associate with each lazy stream of a given type, only one item of \nthis type. This is obtained by: defining synchronous operational semantics, which al\u00adlows only listless \nprograms to be evaluated [21],  providing a static analysis step, aiming at rejecting data-flow networks \nthat cannot be listlessly evaluated. This step is sometimes referred to as either a clock calculus [20, \n2] or consistency checking [16].  finally, providing compiling techniques which product listless sequential \ncode for programs that have been accepted during the preceding step.  1.3 Synchronous data-flow However, \nthose synchronous data-flow languages are closely restricted to the domain of reactive systems; for instance, \nthey don t allow the use of recursive definitions of functions. The reasons for these restrictions are \nquite clear : reactive systems shall continuously interact with their environment, and this can be safely \nachieved only if their reactions are implemented using bounded memory and bounded reaction time. The \npurpose of the paper is to show that the synchronous operational semantics, and the clock constraint \nrules are not bounded to these reactive restrictions and can apply to more general stream languages (section \n3.1 ) providing abstrac\u00ad tion, application and general recursion. For this, we intend to establish a \nclear distinction be\u00ad tween synchrony and reactivity : c Reactivity means the ability to react to external \nevents, and, quit e obviously, requires bounded memory and reaction time, Synchrony is the ability to \nshare a common time scale. In the context of data-flow, it can be interpreted as listlessness in view \nof the already noted analogy be\u00adtween listless transformers and synchronous data-flow compilers. Synchrony \nhas little to do with reactivity, though it can be useful in order to achieve reactivity. This is why, \nup to now, these two concepts have been strongly connected. Thus, we can imagine non reactive synchronous \npro\u00adgrams. This is the intended use of the proposed extension which allows us to give a synchronous meaning \nto higher order and dynamical (recursively defined) net works. In par\u00adticular, truly recursive networks \nwill use unbounded stacks, and thus cannot be reactive. Since we deal with data-flow networks, these \nstacks are expected to be stacks of lists. Synchrony here means simply that it is possible to replace \nthose stacks with stacks of values (cf example 6), and this is expected to be far more efficient. 1.4 \nPaper content The paper will be organized as follows. In section 2 we present a data-flow language based \non a LUSTRE-like toy lan\u00adguage, i.e. a purely functional ] synchronous reactive data\u00adflow language. Yet, \nthe language will deviate from LUSTRE in several aspects: The primitive constructs will be slightly \ndifferent and somewhat more general: our primitives allow easily LUSTRE primitives to be expressed. \n  The clock calculus is more general in the sense that it allows clocks to be inferred. This helps in \nextending it to functional features, thanks to the analogy with classical type inference.  For the same \nreason, we use a curryfied version of the language.  Section 3 presents the core of the extension work \nby: defining a synchronous operational semantics for a functional data-flow language with abstraction, \nappli\u00adcation, recursion and using the data-flow primitives defined in section 2. This allows us to characterize \nsynchronous data-flow behaviors (section 3.2),  defining a clock calculus for this language: this is \nob\u00adtained by expressing this clock calculus as a type sys\u00adtem, which, in turn, allows us to generalize \nit to func\u00adtional features (section 3.3),  LThough presenting some analogy with Signal, this functional \nas\u00adpect makes it very different, as Signal is not a functional language. showing that programs that \ncan be typed in the pre\u00adceding sense, can be synchronously evaluated (theo\u00adrem 1).  and finally, giving \na system of modular com~ilin~ rules producing ~mpe~ative sequential code ob~ying syn\u00adchronous semantics \n(section 3.4).  Section 4 presents implementation matters and section 5 discusses related works. 2 A \ndata-flow language  2.1 Primitive constructs In this section, we define some primitive constructs over \nlazy streams in a Haskell-like syntax [22]. Lazy streams of type a can be defined as: data Stream a \n= a : Stream a.  The constprimitive allows scalar constants to be trans\u00adformed into infinite constant \nstreams, by:  const i =i :const i Where : is the ordinary stream constructor. For in\u00adstance const true \nis the infinite stream: [true; true; . ..] extend allows streams of functions to be applied to streams \nof data : extend (f:fs) (X:XS) = (f x) : (extend fs XS) For inst ante, notl x = extend (const not) x \nX andl y = extend (extend (const and) x) y respectively define an inverter and an and gate which operate \npointwisely over their input streams; thus notl (const true) is the infinite stream: [false; false; . \n..] fby is the followed-by operator (or delay operator): (x :xs)fbyy=x:y It allows recursive expressions \n(feedback networks or circuits) to be safely built, without exhibiting dead\u00ad locks (if the recursion \nappears on the right of the fby); for instance: half = (const false) fby (notl half) has the infinite \nperiodic behavior : [false; true; false; true; . ..] 9 When the first argument of fby is a constant stream, \nwe rather use the fiimpler function pre, whom fimit argument is a scalar: 227 presx=s:x 2.2 l%%y synchrony? \nThus we could have written: half = pre false (notl half) The when primitive allows sub-streams to be \nextracted from streams: (x : XS) when (true : Cs) = x : (XS When CS) (x XS) when (false : CS) = xs when \ncs For nstance, we have: [Z o; z,;...xn]...] x= x when half = [ZI; Z3; . ..%+ I]...] Conversely, merge \nallows streams to be built from sub\u00adstreams: merge (true:cs) (X:XS) y = x: (merge cs xs y) merge (false:cs) \nx (y:ys) = y: (merge cs x ys) For instance, we have: x = [Zo; z,; . ..zn] ...] Y = [yo;y,; . ..gn]... \n] merge half x y = [Ye; zo; yl; zl; . ..ytizt] ...] Example 1 (A program) Aprogram is aset ofmutually \nrecursive equations defining streams of scalar values. Con\u00adsider the following expression: nat = pre \n2 (extend (extend (const (+)) nat) (const 1) It defines the list of positive integers starting from 2. \nThis program has the following Kahn network representation. bconst 1 + pre2 w C Lazy evaluating this \nprogram is costly: intermediate lists are allocated and deallocated by the Garbage Collector during execution. \nOn the contrary, the synchronous data\u00adflow compilers translate it into a sequential program with bounded \nmemory and response time. One could say that those compilers transforms the call-by-need evaluation into \na call-by-value one. Let us consider the example of figure 1 which displays a program, the corresponding \nnetwork, and atypical evolution of the streams involved in the program. In this example, the same input \nis duplicated and one version goes through an odd function2, whose effect is to discard one item out \nof two from its input stream. Then both streams go through an andl function, which consumes one item \nof each stream at a time. Clearly, such a net\u00adwork cannot be executed without using an intermediate list. \nFurthermore, as time goes on, this storage grows, and will sooner or later overflow. This explains why \nthis kind of ex\u00adample, which is called non synchronous has to be rejected when dealing with reactive \napplications. 2.3 Recursive functions For the same obvious reasons of reactivity, actual synchro\u00adnous \ndata-flow tools reject recursively defined functions. A typical example of such a rejected program is \nthe follow\u00ading Eratosthenes sieve, whose recursive data-flow network is de~icted below. In this figure, \ndata flow from left to right. For the first input item, the filter records the input value and returns \nthe value false. In this case, the switches are low and the returned value is true. For the next inputs, \nthe filter box returns true when the current entry is not a multiple of its recorded value. In this case, \nthe switches are high. The first time this happens, a new sieve machine is created and fed with an initial \ninput. The next times this happens, this ma\u00adchine is fed with the corresponding inputs. Otherwise, the \nswitches are low and the returned value is always false. sieve .+ const false pre true Such a data-flow \ngraph can be represented in a functional and recursive way as follows: let first x = letv=xfbyv in v \nsieve x = let filter = (first x) not_divides_l in merge filter (sieve (x when filter)) (pre true (const \nfalse)) . in nat when (sieve(nat)) 21n ~erm~ ~fcontrol theory and hardware x when half Is a half\u00adfrequency \nsampler where not_divides_lis the extension to streams of the or\u00addinary operation on integers. let odd \nx = odd let half = pre false (notl half) in x when half andl + in overflow! X andl (odd X) ml k x .. \nXo xl X2 X3 x4 X5 . . . half = false true false true false true ... x when half = xl X3 ~5 . . . X andl \n(odd X) = zoand Z1 xland x3 xzandxs . .. Figure 1. Anon-synchronous example Yet, itseemsto us that this \nexample doesn t exhibit the pathological character of the previous non-synchronous one, and that it seems \npossible to compile it using no intermedi\u00adate list at all. Yet, since this is a truly recursive function, \nits compilation should use a stack, but this is not a surprise and we are used to it. Though the theoretical \ndistinction be\u00adtween stacks and lists should deserve greater attention, we propose to leave it beyond \nthe scope of the paper: weordy intend to show here that some recursive stream programs appear as reasonable \nextensions of what has been done in the field of synchronous reactive data-flow, and enjoy the same aptitude \nto efficient listless compiling. We propose to call such programs synchronous Kahn networks . 3 Formalization \n 3.1 A functional stream language Let us define the following functional and recursive data\u00adflow kernel. \nExpressions areranged over eandare built from stream variables (x), primitive constructs presented above, \nrecursive expressions (ret x.e), abstractions (Axe) and ap\u00adplications (elez). The language contains scalar \nprimitives which can be extended to streams via the const primitive. (i} denotes integers, (true) and \n(false) denote classical true and false constants. Classical primitives are available such as (+) for \nthe integer addition, (not) and (and) for classical boolean operators, etc. e ::= x \\rec x.e IAxe Iee \n\\conste lmergeeeelefbye Iextend eelpree elewhenelnotle /+/and lnotlil false \\true l... This language \nis a typed language in the sense of the classical Damas-Milner type system [1 I]. We say that an expression \ne is of scalar type if all sub-expression (including e) is of scalar type and does not use the stream \nprimitives. The function scalar is such that scalar(e) is true if e is of scalar type. Thus, our language \nis a classical functional language with special primitives whose semantics have been defined previ-Ously. \n3.2 Synchronous operational semantics Programs written in such a language can be executed in a call-by-need \nmanner, as programs managing lists, thus lead\u00ading to a poor implementation: intermediate structures are \nallocated and then deallocated by the Garbage Collector. We shall see now that a large set of programs \nfrom this language can be executed in a synchronous way, where no list is managed during the execution. \nThe idea behind syn\u00adchrony is to execute one step of each subexpression one step of each node in the \nnetwork such that no interme\u00addiate structure is buffered in a node: an argument crosses the whole network \nin one step. Thus, the synchronous oper\u00adational semantics is a restricted relation of the classical lazy \nsemantics: some reductions rules are not allowed. Definition 1 (Synchronous Semantics) The synchro nous \noperational semantics of the language M defined by the transition relatzon u b e ~ e meaning that in \nthe environ\u00adment u, one execution step of the expression e produces and becomes the expression e . u \n::= [x, uy,,..., zn~yn] v ::= Dle x, and y, denote variable names and v is ezther a scalar expression \ne or empty (denoted by 1). An execution of eo starting in the environment uo can be written as: .O .%+1 \neo-=el~es...~en+l ~ . . . tf U. K e, Z e,+l. The value produced by the expression eo is the infinite \nlist VO.V1 . . . vn . . . . We assume that D is the unit element of hst construction ~.1 = l). The substitution \ne[vl/vz] used here is quite unusual since V2 may contain somes empty constructs. Thus, the substitu\u00adtion \ndoes some pattern matching, and is defined only when the two components are compatible. The complete \ndefinition of u K e ~ e ts given by the set of rules at figure 2. TAUT U, X2 XI FX3XI f. u~f~fl ul-e~e~ \nEXT-V f~e~ u b extend f e extend fl el true do [1 U&#38;c - clukd-dluke -el MERGE-t u 1-merge c d e \n% merge c1 dl el [1 [1 Ut-c-+cl ul-d+dl WHEN-O [1 u 1-d when c + dl when c1 ~ ~ ~false CI ut-d%dl WHEN-f \n[1 uEdwhen c~ dl when c1 U+e%el PRE-v ukprevezpreeoel u}e~el at-flfl FBY-inlt af-efbyflpreeofl u,z~xlt-e~elREC \nu * rec x.e ec~o rec zl. ej [ret zo. vo/xo] u K e ~ el scalar(x) abst C7k )ix. e X-v Ax.el Figure 2. \nThe synchronous CONST-e u + const e ~ const e [1 aEflfl a+e-el EXT-O [1 u k extend f e * extend fl el \n[1   aF.J- [1 cla Fd+dlu Ee~el MERGE-U [1 uEmerge cde~ merge c1dl el false [1at-c + clo-kd-+dlut-e~el \n MERGE-f u Rmerge c d e % merge c1 dl el UkCtr+ue CI ukd%dl WHEN-t do GE dwhen c+ dl when c1 [1 ut-e~el \nPRE-D [1 ukpreve+pre;ve~ uFe~el al-flfl FBY-[] u ~ efby f ~ el fbyfl ut_e~el a+f~fl FBY-pre ul-efbyf~preeofl \n =Pp u E f ~ fl scalar(v) U+ fvf% flv operational semantics const produces a list of constants. Thus \none step of the constv program produces v and the continuation constv. Such an operation may produce \nnothing if no operation is waiting for its value. In this case, we call it an empty rule . The extend \nprimitive applies pointwisely a list of scalar functions to a list of arguments. The seman\u00adtics states \nthat both arguments must be either present or absent in parallel. If none of them is present, the code \nremains the same and produces no value. Remark: The synchronous aspect of these semantics lies in the \nabsence of some rules: if we take into ac\u00adcount all the cases of production and non production of values \nof the arguments, we should consider four caaes. Here, we have only two. Thus, there is no way of evaluating \nthe expression when only one argument is present nor there is a way of storing intermediate results in \ntemporary lists. This is the keg difference with respect to classical operational semantics. Similarly, \nmerge should need eight rules, but syn\u00ad chronous semantics consider only three of them. fby may produces \na value only when its first argument produces a value. In this case, it transforms into a preoperation. \nOperationally, a pre operator acts as a latch: it puts its recorded value on the output and stores its \ninput. The (REC) rule is very natural: yet, in doing this, we must carefully distinguish the cases where \nrec ap\u00adplies to ground expressions and those where it ap\u00adplies to functional expressions: in the former \ncase, (the only one which arises in reactive data-flow) it is allowed to use fix-points like rec zo .eo \nonly if zo is not free in eo. Otherwise, this should be considered as a deadlock. Consider, for example, \nthe program recz. (plus x (const 1)) where plus stand for the classi\u00adcal addition extended to streams. \nThe produced scalar expression is recx. z + 1 which infinitely loops. Yet, static deadlock detection \nis fairly easy since it suffices to show that each variable bound by a reck within the scope of a pre \nor f by (consider example 1). The rule of abstraction over streams (rule ABST) says that a kc.$ expression \nproduces a scalar function AZO.~o and returns a new function JZO .Axl .~l which can be both a function \nof the instant value and of the continuation of its argument. For inst ante, this arises when the instant \nvalue modifies the state of the func\u00adtion (e.g, a (recursive) function containing some fby or pre primitives). \nWhen an abstraction is over a scalar value (rule abst ) then the execution may produce a new abstraction \nif the body produces something new. Application rules match abstraction rules: an appli\u00adcation ~ e over \nstreams produces an application fo eo over scalars and the continuation (~1 eo) el. Thus, next returned \nvalues by the application may depend on eo. Example 2 (A synchronous program) Let us consider again \nexample 1 that computes the set of positive integers and let us write it (for the sake of clarity) as \nfollows: rec z.pre O(z + 1) where we assume that 1 denotes the infinite stream of 1 and + is the classical \naddition extended to lists. One execution step of this program is represented in the following proof \ntree: z~dt-zaz~ x2z~l-l Ll Zo+l , z2zf Ez+l + $+1 z~zll-pre O(3+l)$pre(zO+ 1)($ +1) rec x.pre O(z + 1) \nTecSO.0 i\u00ad rec z .pre (Co + 1) (c + l)[reczO.O/cO] The resulting expression reduces to the following \none: k rec x.pre O(z +1) &#38; rec z .prel (z + 1) It is quite easy to see that the program produces \nthe stream of integers. Moreover, this program is reactive: the program needs bounded memory and bounded \nresponse time. This is due to the fact that the size of the expression and the size of the execution \nproof are bounded. This can be seen as a reactivity property. In particular, this property is verified \nin synchronous static data-flow. C Within the framework, most functions can be executed synchronously. \nYet, some functions cannot. Example 3 (A non synchronous example) Let us come back to the non synchronous \nprogram given at figure 1. We saw previously that the set of buffered values of z increased during execution. \nWe can verify that there is no possible execution step of this expression with the given synchronous \noperational semantics. For the sake of clarity, we prefer to use directly andl instead of its complete \ndefinition. Fact: There is no xo and expression e such that a b x ~ ZI and u 1-z andl (z when hal~) ~ \ne. Proof Suppose we have a transition. The first step in the proof would be: Ot-x%zt u F halj 2 halj \nat-z2z~ o F (z when half) ~ z when halj o E zandl (zvhenhalf) 0 asdyo z andlz when ha/j This is possible \nonly if a 1-half sue hr.dj which does not hold. Thus, there is no possible execution of the program. \nC The adequacy of the synchronous operational semantics wit h the classical lazy semantics is straightforward: \nthe syn\u00adchronous semantics is a subset of the lazy semantics. 3.3 Clock Calculus The idea of the LusT~clock \ncalculus is to provide stati\u00adcally checkable conditions allowing an expression to be syn\u00adchronously evaluated. \nThe version presented here is a gen\u00aderalization of the ones presented in [21, 9], in the sense that it \ninfers the clock and uses unification instead of fix-points. Moreover, the clock calculus is extended \nto functional ex\u00ad pressions. Definition 2 (Clock calculus) The goal oj the clock cal\u00adculus is to assert \nthe judgment: H1--e:ci meaning that (expression e has clock c! in the environment H . An environment \nH is a list of assumptions on the clocks of free variables of e. El is such that: H ::= [zo : C/o,..., \nZn : c~n] A clock cl is either a clock variable u, or a sub-clock of a clock, cl one monitored by some \nboolean stream expression e, or a clock function. Clock expressions are decomposed into clock schemes \n(u) and clock instances (cl). cl ::= cilclone[cl~cl u ::= cl I va] . ..an. cl The sgstem ZS used with \na generalization function. Its defi\u00adnition is the following: GenH(ci) = b al...aclcl if al, ....cr~ # \nFV(H) al> .... @n 6 Left(cl) Left(cll 3 c12) = FV(cl~) U Left(c12) = 0 otherwise  The axioms and ~nference \nrules of the clock system are giuen at figure 3. The clock system has been done in the spirit of the \nclas\u00adsical Damas Milner type system [1 I] with a slight modifica\u00adtion of the recursion rule, coming from \n[19]. Nonetheless, it is an unconventional system since clocks contain expressions. A constant expression \nmatches any clock. Thus, it has a polymorphic clock Va. a. The clocks of the two arguments of an extend \nmust be identical. The clock of a when expression depends on the values of the second argument. This \nis represented using the on construction. The expression merge el ez es uses a ez item when the value \nof el is true, else, it uses a es item. Thus, such an expression is well clocked when the clock of ez \nis the one of el restricted to the case where el is true and the clock of e3 is the one of el restricted \nto the case where el is false. We also have a symmetrical rule. A fby expression is well clocked when, \neither the clock of the two arguments are the same, or the clock of the first is faster than the second. \nThe pre expression preserves the clock of its argument. In abstracting over stream variables, we must \nkeep trace of the variable being abstracted, as it can ap\u00adpear in clock expressions. Application rule \nis consistent with the preceding one: the clock of the application is the clock returned by the function \ninstantiated with its actual argument. Abstraction and application over scalar variables don t modify \nclocks. A variable in a clock expression can be generalized if it is free in the environment H and if \nit appears on the left of a functional clock. This constraint is unusual but has an intuitive explanation: \nin general, the clock of an expression can never be generalized (if a value is present, it cannot be \nabsent at the same time!) unless the expression is a function where the clock of the result depends on \nthe clock of the argument. The instantiation rule is the classical one for type systems. Application \nrule is consistent with the preceding one: the clock of the application is the clock returned by the \nfunction inst an~iat ed with its actual argument. This allows us to state the main result of this section, \nnamely that every clockable expression can be synchronously executed. Theorem 1 (Soundness) For all e \nand cl, if ~ e : cl then it exists v e and cl such that i ) Ee8e , and  ii) k e :cl .  Proofi The \nproof will be roughly sketched as follows: to each clock proof one can always associate at least one \nexecu\u00adtion tree by building a morphism 4 from clock proof nodes to execution proof nodes, which preserves \nsome consistency property, namely that all expressions sharing the same clock should either yield a value \nor evaluate to empty , and any sub-clocked expression should yield a value if and only if its clock evaluates \nto trueFurthermore, it is easy to check that the inference axioms and rules of synchronous operational \nsemantics preserve clockability. This theorem says that when an expression can be clocked, then, it can \nundergo a synchronous execution step, and then rewrites as a clockable expression. Thus, execution can \nproceed. 3.4 Compilation The goal, here, is to transform a well clocked (thus syn\u00adchronous) program \nover streams into an (efficient) transition function over scalars. Whereas the initial program has to \nbe executed in a call-by-need manner, the final program will be executed in a call-by-value manner. This \ncompilation process is based on the synchronous operational semantics: indeed, a general execution of \ne is e=eo3e12e22?. .. stating that one execution step of eo produces the value v and eo is transformed \ninto el, etc. The goal of the compilation of e is to produce a transition function which can be seen \nas one execution step of a ma\u00adchine whose iteration will produce the successive values v,. The transition \ncan be decomposed into two parts: a code function producing the scalar output (v, ) and a modif-state \nfunction modifying some internal memory (or state) in the machine (in order to express the fact that \ne, becomes ez+l ). A memory is a set of links between values and names. Thus, we shall construct a general \nfunction trans A let v=code(s) transo = in s:=modif-state(s);v 232 CONST H k const v : Va.a TAUT H,z \n:Ci~X :Ci ~XT H1-el:cl Hi-e2:cl H k extendel ez : c1 ~~EN H1-el:cl H k el when H1-e2:ci ez : cl onez \nMERGE-1 H1-el:cl Hke2:clonel H E raerge el Hke3 : c1 on(notlel ez e3 : cl ) PRE H!-e:cl H1-preve:cl \nH 1-el : cl H 1-ez z cl on(notlel) H 1-ea : c1 onel ~By-l H+el:cl Hke2:cl MERGE-2 H 1-merge (notlel ) \nez ea : cl HFelfbyez :c1 ~BY-2 H1-el:cl Hi-e2: clone REC H,z:uke:u H 1-el fby ez : c1 one Hkrecx.e :U \n~EN H k e : cl u = GenH(ci) ~N~T H E e : Val . ..a~.cl FV(C1) ~ FV(C1,) = fl HFe:u H k e : cl(cll/Ql, \n....ci~/cr~] ABST H,x:clke: cl ~b,t H k e : cl scalar(x) H 1-Axe :cl ~ cl H k Axe :c1 APP Hke:cl~ cl \nH1-e :ct ~PP H 1-e : ci scalar(e ] H 1-e e : cl [e /x] H1-ee :cl Figure 3. The clock calculus such that \neach call returns a value and modifies some mem-executed code can be written in the following way: ory. \nThen, once the memory s has been properly initialized, . . def-type([vl/xl; . . ..v~]).; successwe calls \nto tTans ( ) wdl yield the sequence vo, w,. . . let trans = More formally, let xs = new([vl/xl; ....vn]) \nin fun () ~ match xs with b Env(zl, ....x~) + try let v = c in m; print(v) Definition 3 (Compilation \nmethod) The compilation with fail * m method is defined by the relation u + e : (c, m,s). It means .in \nloop transo end that in the environment ~, compiling e produces the code C, The definition of the compilation \nrules, the clef-type andthe sequence oj instructions m modifying the memory and an new functions are \ngiven at figure 4. initial memorys. These two codes are expressed in any ML\u00adlike language with side effects \n(the adopted syntax is close to Compilation rules follow exactly semantic rules: the code the one of \nCamLlight [1 7]). An environment u is such that: part produces the value and the memory part modifies \na state to take into account the fact that the resulting expres\u00adsion has changed. Let us comment it: \na ::= [ZI : (cl, ml, sl),..., zn: (en,mn,sn)] const returns the value of its argument. The code part \nof the compilation of const v is v. The constmachine A memory expressions (or internal state) can be \nan empty needs no internal state, thus nothing has to be mod\u00admemory fl), a memory name (x), a set of \nlinks between ified. The modif-state part is the empty instructionnames and either memories or scalar \nvalues, a recursive memory (ret x.s) or a function of memories (s * s). () merge produces a simple if \nstatement. modif-state in\u00adstructions come from the three parts of the merge and s ::= O I [al/z,,. ... \nam/zm] the memory part is the union of the three arguments. lxl Tecx.sls--+s a ::= Slv when produces \na conditional which may raise the ex\u00adception fail when the test is false. Here also, the rnodif-state \nis the composition of the two modif-stateThis structure wilt be explained in more details further. and \nthe memory part is the union. The relation + is only defined for c!ockabte expressions with functional \norder less or equal to one and where the Operationally, pre acts a-s latch : it outputs its clocks of \n(const v) sub-expressions are clock variables. If recorded value and then records its input. This is \nim\u00ade has order zero and s = [vi/xl; . . . . Vn] + o, theta the final plemented by a variable. Thus, the \npre construction 233 CONST a 1= constv : (v, (), o + o) TAUT U,z:(c, ?n, $)+z:(c, rn, s) MERGE a+el \n:(cI, u ~mergeel rm, sl +0) ez es : (if a+e2 c1 then : (c2, rn2, s2 +U) a+e3 cz else cs,ml;mz;ms,(sl \n:(c3, Usz m3, s3 +-o) Ust) -+ n) WHEN a ~ a~el:(cl,ml,sl+o) elwhenez : (if C2 then CI u+ez:(cz,mz,sz+o) \nelse fail, ml; m2, (sl U S2) + o) PRE u * IJ ~ pre e : (c, m,s ~ v e : (!pre.x, o) try with pre.x pre_z \nfail # Don(s) FV(v) := c , (s U [w/pre_z]) ~ () = 0 --+ u) FBY u ~ elfbyez a +e~ : (if : (cl, !init ml, \nthen sl + 0) c1 else u ~preve2 !pre-x, ml; : (c2, mz, (s] m2, [~/pre-Xl Us2 U sz U [true/!nit] ~ 0) U \n[?/pre_x] - n) EXTEND a~el a ~ : (cl, extendel ml, s] e2 : -+0) a*e2 ((cl c2), ml; :(c2, m2, (s, m2, \ns2 +0) US2) + 11) ABST a ~ k.e : (krs, xc. u,z match Env(xl, : (zc, (), !XS with . . ..x~) O + o) * e \n: (c, rn, [al/zl; . . ..azn]n] , Axs, xc. match !XS with -+ c Env(xl, . . ..zw) ~ m + s) ,o+[al/zl; . \n. ..an/GJ -s) a~el : (cl, ml, sl +[a I/xl;...; an/xn] + s3)) a 1= e2 : (c2, m2, s4 -+ 0) APP c + e, \ne2 : (cl XC2), try ml xc2m2 ,(s~ us4 U [[al/zl; . . ..axn]nxl)l) ~ s3 with fail -+ mz U l=el : (cl, ml, \nsl + recxs.s2 ~ $3)) ~ ~e2 : (c2, m2, s4 + g) APP-rec a ~ el e2 : ( match !Z with , match !X with , \n(SI U s, U [recxs.s2/x]) + s,) NoEnv + x := new(s2); c1 z C2 NoEnv + x := new(s2); m2 I--+ C,XC2 j-+ \ntrymlxc2; m2 with fail + m2 a~el:(cl,ml,sl+D-+s3) a+e2:(c2, m2, s4-+B) APP-U a + el e2 : ((cl c2), try \nml c2; m2 ,(s, US4) --+ S3) with fail --+ m2 U,X : (xc, xm, recxs. s) 1= e : (c, m, recxs. s) REC g \nK recx. e : (recxc. c, let xc = recxc. c in recxrn. m, recx~.s) new(o) = ref NoEnv def.tgpe(o) = NoEnv \nnew(recx. s) = new(s) def.tgpe( recz.s) = type z = clef-type(s) [ NoEnv new([al/tl; .... a~/z~]) = Env(new(al), \n.... new(a~)) def-type([al /xl; ....a~/z~]) = Env of def_tgpe(al ) ref * new(v) v . . . * def-type(an) \nref  new(x) = ref NoEnv def-tgpe(v) = type(v) def-t~pe(z) =Z Figure 4. The compilations rules 234 produces \na code which returns the content of this vari\u00ad able (pre.z). In the rnodif-state part, the new value \nof pre.x is computed if c produces a value (does not raise any exception). Else, no modification has \nto be done. The memory part associated with preis thus the memory part of its argument plus the new link \n[v/pre-z]. Note inthis rule that theintroducedvari\u00ad able pre.m must be a new variable (not belonging \nto the domain of s).  fbyis very similar topre except that the initial value of the memory is not known. \nfby has two states: an initial state where it returns the value of its first ar\u00adgument and a general \nstate where it returns the value of its second argument. Two memories are necessary: thez nit memory \nis first initialized totrue and becomes false forever and the pre_z variable which acts as a latch . \npre-z is initialized with any dummy value of the correct type. It is important to notice that only pre \nand fby primitives need some memory.  extend produces a simple application with no new memory.  When \ndealing with abstraction Jz. e we can distinguish two different cases (rules (ABST) and (ABST-0)):  \ne may need some memory. An abstraction may be called from different places in the code and the mod\u00adifications \non the internal state must be different for each call. The solution we propose is to produce an abstraction \nwith a new argument xs representing the internal state. This memory can then be modified by the function. \nMoreover, the function has to match the structure of the memory it may receive. The memory is written \nD ~ s saying that no memory is used to pro\u00adduce the abstraction whose body uses s. We can now explain \nthe structure w + sz. It means that the initial memory S1 is used for creating a value and that value \nuses sz. Thus, constant values (like const 1) have the memory H ~ u. - e may not need any memory. In \nthat case, the ab\u00ad straction kc .e (where x is a stream) corresponds ex\u00ad actly to a scalar abstraction \nwith the same pattern: the returned value of the abstraction does not depend on the previous calls to \nthe function. For example, con\u00ad sider the expression h .(extend(extend+ (const 1)x). The scalar code \nassociated to this function over lists is simply Am. (+1 xc). Thus, the code part associated to an abstraction \nwithout internal state is also an ab\u00ad st raction with only one parameter. Application rules match abstraction \nrule: There are two cases: -e] may modify an internal state. In that case, this memory has to be given. \nThus we add a link between a new name z and the initizd memory, the body is wait\u00ading for. We then produce \nthe cases depending on the possible values of z. If the memory is recursive, we add a special case for \ncreating a new memory dynam\u00adically in case the entry of the function is empty (rule (APP-ret)). This \ntime, the memory size is not bounded anymore: a new entry is created each time the func\u00adtion is called \nwith an empty entry. We shall discuss thk point further, explaining the possible optimization in order \nto keep reactivity constraints. eI may not modify an internal state. In that case, the application over \nlists is similar to the application over scalars.  Recursion produces three recursive expressions. A \nre\u00adcursion over streams becomes a recursion over scalars. Nonetheless, as we shall see on examples, such \na recur\u00adsion rec xc. c is not always a true recursion in the sense that xc may not be free in c, In that \ncase, it simpli\u00adfies to c. This is exactly what happens for LUSTRE programs: they are compiled into non \nrecursive ones. The rnocfif-state may behave similarly. The memory part may also be recursive: in fact, \nthis corresponds to the definition of a recursive data type.  Finally, the clef-type and new constructions \ntrans\u00adlate memories into Carol-light data structures. The def-tgpe function has to produce flat definitions \nfrom (possibly nested recursive) memory expressions. We only give here the case of non nested memories. \nMore\u00adover, we restrict the definition to first-order (recursive or not) memories without abstraction \nor application. The general case is a matter of further improvements.  A lot of oDtimizations can (and \nshould) be armhed on the produced code. For example, several rnatch&#38;-can be factorized and empty \ncomputation like z; m where x is a variable can be simplified. This is still to be done. 4 Implementation \nThis section deals with implementation aspects. The final goal is to include a special (efficient) stream \nstructure inside some ML language, based on the application of the clock calculus and the compilation \nmethod. A first prototype tool is under construction. Clock cal\u00adculus and compilation are applied to \na simple functional language as defined in the previous section. Latter, we plan to include it as a front-end \nof the Carol-light system. 4.1 Clock calculus The implementation of the clock calculus has been done \nin the spirit of claasical implementations of the Carol-light type system [17] wit h a slight modification \nfor the recursion rule. The clock calculus is obtained using a destructive unifica\u00adtion, Here, the unification \nis slightly different from the claa\u00adsical one since clocks may contain expressions. In this imple\u00admentation, \nwe restrict the unification between expressions in clocks to the syntactical equality (modulo a-conversion). \nRecursive expressions should be clocked in an iterative way, as noted in [19]. We have decided to implement \na sub-system by restricting the iteration to two. It seems to be sufficient for many examples. Example \n4 Expressions can be defined at top-level. The clock expression is returned for every entry. let plus=fun \nx->fun y-> (extend (extend (const +) X) y) clock : pa-<>-> a-<>-> a let filter = fun x->let half=rec \nhalf. (const true) fby (notl(half ) ) in x when half clock: ~a-<>-> a on (ret half. (const true) fby \n(notl half)) MoEnv -> e :=newo ; sieve-code e x let natx=fun x -> rec natx. pre O (plus natx ((const \n1) when x)) clock : a-<x>-> a on x fun x -> fun y -> merge (natx x) y (const 1) clock: ~a-<x>->( a on \nx) on (natx x)-<>-> a on x Q  4.2 Compilation The implement ation produces a Carol-light program with \nside effects. It preserves, as much as possible, the func\u00adtionality and structure of the initial program. \nWe could have chosen to produce directly C code. Yet, our choice is quite natural: the point in our work \nis to show how to com\u00adpile efficiently the data part of the language without dealing with the cent rol \npart. So, by translating our programs into ML, we preserve every other possible optimizations (func\u00adtion \ncalls, closures, . . . ) related to control. Moreover some efficient C code can then be obtained using \nML to C trans\u00ad . lators. Example 5 (A recursive value) For instance, the pro\u00adduced Carol-light code \nfor the recursive expression nut (ex\u00adample 1) is (after applying some simplifying rules): let trans = \nlet prex = ref 2 in fun () -> let nat-code = !prex in let nat-mem = prex:=!prex+l in print-int (nat-code) \n .. ,, t Tans is the transition function. The memory is simply [0/pTez] due to the preconstruct. The \nnat_code part re\u00adturns the current value which is initialized to 2 and nat .mem computes the next value \nand puts it into the accumulator. In this example, all useless instructions have been deleted. C Example \n6 (The Eratosthenes Sieve) Let us see now the case of the Eratosthenes sieve (section 2.3). Its clock \nis Va. a -a. The compilation process produces the following code (with some hand-made simplifications). \ntype env = MoEnv I Env of (bool ref) * (int ref) * (bool ref) * (env ref) >; let newo = Env(ref true \n,ref (-10) ,ref true ,ref MoEnv) (* -10 is a dummy integer value *) .. if ! init then x else !prev ~~t \nfirst-code (init ,prev) x = ;; let first-ream (init ,prev) x = (prev:=(f irst-code (init ,prev) x) ; \ninit := false) ;; let rec sieve_code e x = match !e with Env(init ,prev, prex, e) -> let filter-code \n= (x mod (first-code (init ,prev) x)) !=0 in if filter-code then match !e with 1- > sieve-code e x else \n!prex ;; let rec sieve-mem e x = match !e with Env(init ,prev, prex, e) -> begin let filter_code = (x \nmod (first -code (init ,prev) x)) !=0 in first-mem(init ,prev) x; if filter-code then match !e with \nlloEnv -> e :=neu( ) 1--> sieve-mem e x else prex:=false end  ;; let e = ref (n,e~()); ; let trans = \nlet prex = ref 2 in fun () -> let nat = !prex in if sieve-code e nat then print. int (nat ) else () ; \nsieve-mem e nat; prex: =nat+i ;; code ( ) first returns 2, then 3, then nothing ( ( ) ) since the fourth \ninteger is not a prime integer, etc. This corresponds exactly to the compilation of nat when sieve (nat \n) that produces an output only when the condition is true. Dur\u00ading the execution of this program, the \nmemory e grows. A eon is created each time the entry is a prime number so the internal memory has the \nform of a stack that contains the previous prime numbers. IJ Example 7 (A tail-recursive function) Let \nus consider a recursive function computing the list of positive integers. It can be written as: rec f.(kc.rfby \n(f(p~uso (corwt l)))) where plus stands for the addition over streams. $(constO) produces the list of \npositive integers. The clock of ~ is Va.a ~ cr. The compilation method will produce the fol\u00adlowing Carol-light \nrecursive code. type env = I?oEnv I Env of (bool ref) * (int ref) * (env ref) ;: let newo = Env(ref true \n,ref (-10) ,ref lIoEnv) (* -10 is a dummy integer value *) ;; let rec f-code e x = match ! e with Env(init \n,prex, e) -> if ! init then x else !prex .. ,, let rec f-mem ex = match !e with Env(init ,prex, e) -> \nmatch !e with EoEnv ->e :=newo ; init :=false; prex: =f -code e (x+1) 1-->f -mem e (x+i ) ; init :=false; \nprex:=f-code e (x+1) ;; let code = let e = ref (newo) in fun () -> let v=f-code e1 in f -mem e i ;print-int \n(v) ;; The memory is recursive since the function is recursive. it hardly deals with non length preserving \nfunctions It cent tins the successive init values of the nested machines. like filter. On the contrary, \nour when primitive is Of course, this function is not truly recursive and can be a filter-like, non length \npreserving, function, which is implemented in a finite way (all the recorded values in the central to \nour approach3 structure are always the same). We call it a tad-recursive function in the usual sense \nsince the body of the function dies when it creates anew son. This kind ofrecursion is not treated efficiently \nby the current compilation method that focused on truly recursive functions. To be implemented efficiently, \nthese recursions should be rewritten in a non re\u00adcursive and non functional way. For example, the function \nf should be rewritten as: Az.z fby (ret fi.z fbypiusfl (const 1)) which could then be compiled into \na bounded transition function. The recognition of tail recursive functions is a matter of further work. \nC 4.3 Preliminary results For comparison purposes, we report here executions times (in seconds on a \nSpare 10 workstation) of two examples, a reactive and a truly recursive one, using three different approaches: \nthe source program is compiled with the LM L com\u00adpiler. . the compiled program is compiled using carol-light \n0.7 compiler. the compiled program is directly compiled into C Whereas the implementation is still limited, \nthis very pre\u00adliminary comparison shows that, when implemented in C, our method is more efficient for \nreal-time programs and some truly recursive functions than usual lazy semantics methods, represent ed \nhere by the LML compiler. Note also that our approach compares favorably with respect to code length: \nLML code is about 500K bytes. Related works This work is clearly related to the topics of listlessness \n[25] and deforest ation [26]. There have been extended works on the subject, and it is quite difficult \nto summarize all of them. Yet, roughly speaking, these can be classified into two groups: Particular \nmethods, based on some set of special pur\u00ad pose primitives, among which we can cite deforestation shortcuts \n[3], communication lifting [24], and the one presented here. It is quite difficult to compare them, because \neach has its proper set of primitives and con\u00adstructs as well as its own goal. The communication lifting \nmethod [24] is very similar to that can be achieved with LUSTRE, and thus doesn t intend to deal with \ndynamical networks. Furthermore, Deforestation shortcuts [3], based on foldr-build transformation, is \nfairly general; yet it seems to us that it fails in addressing the problem of functions having several \nstream arguments, which are also central to our approach. General purpose methods like the deforestation \nalgo\u00adrithm [26], which applies to any recursive data type. With respect to these methods, ours is less \ngeneral, but better fits its particular domain of application. For in\u00adst ante, it is well-adapted to \nnon linear (in the sense of [26]) expressions, for which the deforestation algorithm is not guaranteed \nto terminate. For instance? it can be checked that deforestation doesn t termmate with: y = merge c (x \nwhen c) ((pre O y) when (notl c)) while we easily deforest it. Moreover, we can treat recursive functions \n(as sieve) without considering them as macros. Finally, Boussinot [8] has defined an execution scheme \nfor process networks, consisting in synchronously executing one step of each process at a time, and this \nyields an operational semantic very similar to ours. Yet, he doesn t define cor\u00adresponding clock calculus \nand compilation schemes. Thus his approach doesn t allow communications to be efficiently compiled. 6 \nConclusion Finally, since the early days of Kahn and Lucid models, sev\u00aderal synchronous data-flow languages \nhave been proposed, for programming reactive systems [20, 2, 16], for program\u00adming parallel machines \n[14, 13], for describing and synthe\u00adsizing hardware ([18] and the data-flow part of VHDL [I]). Yet, none \noft hese proposals seem to have considered recur\u00adsion. By characterizing synchrony in data-flow languages \nas the possibility of restricting to listlessly compilable pro\u00adgrams, we intended to show that this is \nnot contradictory wit h recursion. We have provided a synchronous operational semantics, clock constraints \nand compiling rules for handling recursion as well as higher order programming. Thus, both features can \nbe added to those languages yielding a large increase in expressive power, possibly even reaching the \nex\u00adpressive power of general purpose languages. A possible achievement can consist of applying the tech\u00adniques \ndeveloped in this paper in order to add a lazy syn\u00adchronous stream type on the top of a strict ML-like \nlanguage. In doing this, it will be easy to characterize a reactive subset of this language: reactive \nprograms will be those whose only recursive types are streams, and which don t use recursive functions. \n31n [24], It M clalmed that Lustre can be simulated using commu\u00adnication lifting; this M true, but It \nis only a simulation. instead of being t eJeCtedat compile time thanks to some clock calculus, non listless \nprograms yield execution errors Yet, this reactive subset can be enriched by considering tail recursive \nfunctions as in example 7. As already noted in [1 O] tail recursion is expected to help in clarifying \nand simplifying programs, by providing control structures. Thus efficient tail recursion handling will \nbe a matter of future work. References [1]IEEE standard VHDL reference manual. Technical re\u00adport, 1988. \n[2] P. LeGuernic A. Benveniste and Ch. Jacquemot. Syn\u00adchronous programming with events and relations: \nthe SIGNAL language and its semantics. Science of Corn. puter Programming, 16:103-149, 1991. [3] J. \nLaunchbury A. Gill and S.L. Peyton Jones. A short cut to deforestation. In 6th Functional programming \nlanguages and computer architecture, pages 223-232, Copenhagen, June 1993. ACM. [4] E. A. Ashcroft and \nW. W. Wadge. Lucid, the data-f70w programming language. Academic Press, 1985. [5] E.A. Ashcroft and W.W. \nWadge. Lucid, a non proce\u00addural language with iteration. Communications of the ACM, 20(7):519-526, 1977. \n[6] L. Augustsson and T. Johnsson. Lazyml user s manual version 0.999.4. Technical report, Chalmers University \nof Technology, Goteborg, Sweden, 1993. [7] G. Berry and G. Gonthier. The Esterel synchronous programming \nlanguage, design, semantics, implement a\u00adtion. science of Computer Programming, 19(2):87-152, 1992. [8] \nF. Boussinot. R6seaux de processus r~actifs. Techniczd Report 1588, INRIA Sophia-Antipolis, janvier 1992. \n[9] P. Caspi. Clocks in dataflow languages. Theoretical Computer Science, 94:125 140, 1992. [10] P. Caspi. \nTowards recursive block diagrams. In 19th IFA C/IFIP Workshop on real-time programming, Isle of Reichenau, \nGermany, June 1994. IFAC. [11] L. Damas and R. Milner. Principal type-schemes for functional programs. \nIn Conference on Principles of Programming Languages, 1982. [12] D.P. Friedman and D.S. Wise. CONS should \nnot evalu\u00adate its arguments. In Preceding of the lCALP 76 Con\u00adference, pages 257-284, 1976. [13] J.L. \nGiavitto. A synchronous data-flow language for massively parallel computers. In Parallel Comput\u00adtng 91, \nLondon, 1991. [14] D. C. Cann J. T. Feo and R. R. Oldehoeft. A report on the Sisal language project. \nJournal of Parailel and Distributed Computation, 10:349-366, 1990. [15] G. Kahn. The semantics of a simple \nlanguage for paral\u00adlel programming. In IFIP 74 Congress. North Holland, Amsterdam, 1974. [16] E. A. Lee \nand D. G. Messerschmitt. Static scheduling of synchronous data flow programs for digital signal pro\u00adcessing. \nIEEE Trans. on Computers, 36(2), 1987. [17] Xavier Leroy. The carol light system : release 0.7 : documentation \nand user s manual. TechnicaJ report, INRIA, INRIA, Domaine de Voluceau-Rocquencourt, 78153 Le Chesnay \nCedex, France, July 1995. [18] C. Maurras. Alpha, un lcmgage kquationnelpour ta con\u00adception d architectures \nparallkles synchrone. PhD the\u00adsis, Universit6 de Rennes 1, France, 1989. [19] Alan Mycroft. Polymorphic \ntype schemes and recursive definitions. In M. Paul and B. Robinet, editors, Pro\u00adceechngs of the Sixth \nInternational Symposium on Pro\u00adgramming, Toulouse, pages 217 228. Springer-Verlag LNCS 167, April 1984. \n [20] P. Raymond N. Halbwachs, P. Caspi and D. Pilaud. The synchronous dataflow programming language \nLUSTRE. Proceedings of the IEEE, 79(9):1305-1320, 1991. [21] D. Pilaud P. Caspi, N. Halbwachs and J. \nPlaice. Lustre: a declarative language for programming synchronous systems. In lJth ACM Symposium on \nPrinciples of Programming Languages. ACM, 1987. [22] S. Peyton Jones P. Hudak and P. Wadler. Report on \nthe programming language haskell, a non strict purely functional language (version 1.2). ACM SIGPLAN \nNo\u00adtices, 27(5), 1990. [23] D.A. Turner. A new implementation technique for ap\u00adplicative languages. Software \nPractice and Experience, 9:31 49, 1979. [24] W .G. Vree and P.H. Hartel. Communication lifting : fixed \npoint computation for parallelism. Journal of Functional Programming, 1(1):1-33, 1993. [25] P. Wadler. \nListlessness is better than laziness: Lazy evaluation and garbage collection at compile time, In ACM \nSymposium on LISP and Functional Program\u00adming, pages 45 52, 1984. [26] P. Wadler. Deforestation: transforming \nprograms to eliminate trees. Theoretical Computer Science, 73:231\u00ad248.1990.  \n\t\t\t", "proc_id": "232627", "abstract": "Synchronous data-flow is a programming paradigm which has been successfully applied in reactive systems. In this context, it can be characterized as some class of static bounded memory data-flow networks. In particular, these networks are not recursively defined, and obey some kind of \"synchronous\" constraints (<i>clock calculus</i>). Based on Kahn's relationship between data-flow and stream functions, the synchronous constraints can be related to Wadler's listlessness, and can be seen as sufficient conditions ensuring listless evaluation. As a by-product, those networks enjoy efficient compiling techniques. In this paper, we show that it is possible to extend the class of static synchronous data-flow to higher order and dynamical networks, thus giving sense to a larger class of synchronous data-flow networks.This is done by extending both the synchronous operational semantics, the clock calculus and the compiling technique of static data-flow networks, to these more general networks.", "authors": [{"name": "Paul Caspi", "author_profile_id": "81100202655", "affiliation": "VERIMAG, Miniparc-ZIRST, Rue Lavoisier, 38330 Montbonnot St-Martin, France", "person_id": "PP18000757", "email_address": "", "orcid_id": ""}, {"name": "Marc Pouzet", "author_profile_id": "81100032823", "affiliation": "School of Computer Science, McGill University, 3486 University Street, Montr&#233;al, H3A 2A7, Canada", "person_id": "PP31092932", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/232627.232651", "year": "1996", "article_id": "232651", "conference": "ICFP", "title": "Synchronous Kahn networks", "url": "http://dl.acm.org/citation.cfm?id=232651"}