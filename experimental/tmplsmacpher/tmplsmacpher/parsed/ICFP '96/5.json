{"article_publication_date": "06-15-1996", "fulltext": "\n Storage Use Analysis and its Applications Manuel Semanol 2 and Marc Feeleyl {Serrano,Feeley }@ IRO.UMontreal.CA \n1 Universit6 de Montr6al C.P. 6128, SUCC. centre-ville, Montr6al Canada H3C 3J7 2 INaIA B.P. 105, Rocquencourt, \n78153 Le Chesnay Cedex, France Abstract In this paper we present a new program analysis method which \nwe call Storage Use Analysis. This analysis deduces how objects are used by the program and allows the \nopti\u00admization of their allocation. This analysis can be applied to both statically typed languages (e.g. \nML) and latently typed languages (e.g. Scheme). It handles side-effects, higher or\u00adder functions, separate \ncompilation and does not require CPS transformation. We show the application of our analysis to two important \noptimizations: stack allocation and unbox\u00ading. The first optimization replaces some heap allocations \nby st aek allocations for user and system data storage (e.g. lists, vectors, procedures). The second \noptimization avoids box\u00ading some objects. This analysis and associated opt imitations have been implemented \nin the Bigloo Scheme/ML compiler. Experimental results show that for many allocation inten\u00adsive programs \nwe get a significant speedup. In particular, numerically intensive programs are almost 20 times faster \nbecause floating point numbers are unboxed and no longer heap allocated. Introduction Modern strict \nfunctional languages such as Scheme and ML are still often much less efficient than traditional imperative \nlanguages such as Fortran and C. Few compilers for fict\u00adional languages are able to produce executable \nprograms whose efficiency is close to that of imperative ones [12]. To a large extent, this inefficiency \nis due to poor use of memory. Because read and write operations are much more expen\u00adsive than arithmetic \noperations and control operations on modern computers, memory access is a major performance issue. Consequently, \nan efficient system must allocate as few objects as possible and must choose very carefidly the loca\u00adtion \nwhere the objects are allocated. Let s discuss these two points further. High allocation rate: For languages \nlike Scheme and ML polymorphism is difficult to implement efficiently. With these languages, functions \nwhich accept several kinds of arguments are legal, such as an identity function which accepts characters, \nfixrmms and flonums. This feature is hard to handle efficiently (fixnums and flonums are not gen\u00aderally \nof the same size and cmot be stored in the same kind of hardware registers). The traditional solution \nis Permission to make digitablwd copy of part or all of this work for personal or classroom use is ranted \nwithout fee provided that mpies are not made or distributed for pro !/t or commercial advantage, the \ntxpyright notice, the title of the ublication and ita date appear, and notice is given that copying is \n!)y permission of ACM, Inc. To copy otherwise, to republish, to post on servers, or to redistribute to \nlists, requires prior specific permission andlor a fee. ICFP 96 !5/96 PA, USA Q 1996 ACM 0-69791-771 \n-5/96/0005... $3.50 to box values, i.e. use pointers to values in memory rather that direct values. This \nuniform representa\u00adtion [19] is inefficient because it requires memory allo\u00adcation for all objects. In \nthis paper, we present an al\u00adgorithm whi+ allows mized representation. With this framework, values can \nbe directly represented without requiring indirection. There are two benefits: mem\u00adory allocation is \nless frequent and values are accessed more efficiently. Location of allocation: Some programs make heavy \nuse of objects with dy\u00adnamic extent (nested lifetime). Stack based language implement ationsl which do \nnot exploit this charac\u00adteristic will pay a higher cost for allocation than is required. Instructions \nare already present in the pro\u00adgram to allocat e and deallocate activation frames, Ob\u00ad jects with dynamic \nextent could be allocat ed (and deal\u00adlocated) at no cost in the frames. In order to automat\u00adically find \nwhen objects can be allocated in the stack we have designed a conservative analysis which deter\u00admines \nif objects have dynamic extent. The two optimizations presented in this paper (mixed representation and \nstack allocation) use the same analysis but in different ways. Section 2 first develops this analysis \nfor a small source language and then the language is ex\u00adtended to obtain a language with data storage, \nside effects and modules. The stack allocation decision algorithm is presented in Section 3 and the mixed \nrepresentation in Sec\u00adtion 4. We have implemented these two optimizations in the Bigloo Scheme/ML compiler, \nand have measured the gain in performance on benchmark programs. The experimental results are present \ned in Section 5. 2 Storage Use Analysis (SUA) In this section, we will present Storage Use Analysis (SUA) \nby first describing the analysis for a simple fist-order language with only fixnum and flonum values \nand then we will extend it by adding several data types (higher-order fimct ions, lists and vectors). \n2.1 The input language A The input language for the first version of our analysis is a simple language \nresembling Lisp (functions are second class citizens and closures do not exist), with only immediate \nval\u00adues (flamm and flonum), and without any data storage. A s grammar is shown below: 1Some systems like \nSml/NJ [2] allocate activation frames in the heap. 50 Syntactic categories v E Varld (Variables identifier) \nf E Funld (Functions identifier) A C Exp (Expressions) Is E Cnst (Constant values) HE Prgm (Program) \nr c Def (Definition) Concrete syntax rI ,:= r... r ::= r (define (~ v ...v) A) ::= A 1: I (labels ((f \n(v ... v) A) ...) A) I (if AAA) ~ [~~! v A) ... I (+ AA)A) Note that since fimctions are first-order \nA is not a ftmc\u00ad tional language. A program is composed of several global function definitions; local \nfunctions are introduced by the labels special form. The language includes side effects on variables \n(the set ! form). 2.2 The first-order SUA For the sake of simplicity, we will consider the last fi.mction \ndefinition as the entry point of the program (equivalent to the C main function). So running a A program \nmeans calling the last function defined, with no arguments. Sw (II )= repeat SUa~PP ( 11.$m.i~ ) until \nno approximation set changed in this iteration Sua~PP( f, al, .... an )= Vic[l..ra] let ie=Sua~,t ( \n17i ) w%.( f$cwgi ) t Awu( f larst )Um, if f not yet processed in this iteration then let dka:,t ( f$b~d~ \n) A, ( f$ren ) -&#38;r ( ibe. )IJz, t .4Vmr ( j$re. ) Sua~4t ( atree )= case atree [k]: {7( k)} [warn: \nA..,? war ) (if atree street street) ]: Sua: t ( atree ), Sua~~t ( street ) USua~,t ( atreef ) [ (set \n! war vd) ]: let z=Sua~,t ( val ) Avmr( war ) t Avm,( war ) Uc, 0 (labels ((fl (w . . . %) atr-1) . ..) \natree) ]: Sua~~t ( atree ) (+ al aa) ]: {fixnum, flonum} (f al . . . an) ]: Sua&#38;( f, al, . . . . \nan ) i Algorithm 2.1: A first-order analysis The SUA algorithm shown in algorithm 2.1 computes t ype \ninformation about variables and function results (which will both be called variables). The result of \nthe analysis is an approximation set for each variable, which indicates the type of values that can be \nbound to the variable. Since the only data types are fixnum and flonum, an approximate ion set is a subset \nof the set do = {f ixnum, f lonum}. Note that because the analysis is conservative an approximation set \nis a superset of the true set of types that can be bound to the variable. Note that the SUA algorithm \nrequires a-converted pro\u00adgrams. It is written in en intuitive pseudo language which uses the following \nnot ation: T(k) The type of a constant (f ixnum or f lonum). II .Jmain The program entry point. d,,,(v) \nThe approximation set of variable v. f ~bedrj The body of function ~. f b,, The it formal parameter of \nfimction f. f The artificial variable representing the re- J+.. sult of function f. The algorithm performs \na fix point iteration. Each itera\u00adtion is a depth first traversal of the entire call graph initiated \nby the program s entry point. The fix point iteration stops when an iteration does not add any new information. \nThis process is guaranteed to stop because there is a finite number of variables, a finite number of \npossible approximation sets, and no element is ever removed from a variable s approxi\u00admation set. Let \ns study SUA s behavior with the program: (define (id .) .) (define (plus a b) (+ a b)) (define (foo) \n(plus (id 4) (id 5.0))) The analysis collects approximation sets for x, a and b and the result of id, \nplus and f oo. The traversal of the call graph starts with the body of foo which leads to a call of the \nfimction id with the value 4. We are collecting type information so this call to id assigns the approximation \nset {fixnum} to x. Since id returns x, the approximation set {f ixnum} is also assigned to the result \nof id. After process\u00ading this first call to id, the analysis examines the second call (id 5. O). This \ntime, the approximation set for id s argu\u00adment is {f lonum} so the analysis assigns the approximation \nset {f ixnum, f lonum} to x. In a given iteration, the depth first traversal of the call graph will not \nvisit a function s body more than once, so id s body is not processed again and the approximation set \nof id s result is not changed (this is done in the next iteration). After the second call to id, the \ncall to plus is processed. Since at this point the approx\u00adimation set of id s result is {f ixnum}, the \nanalysis assigns the approximation set {f ixmuo} to a and b. During the second iteration, when the analysis \nprocesses the body of id, f lonum is added to the approximation set of id s result. This approximation \nset is propagated to a and b and the fix point is reached in 3 iterations. SUA concludes that all variables \nand function return values of this program can be a fixnum or a flonum. Implementation note: The set \nof variables and firaction re\u00admits is finite and known at compile time. This property is important becauae \nit allows eficient implementation of A.=, table using eficient set representations (e.g. using bit\u00advector3). \n51  2.3 The first-order SUA with modules We now extend A to support modules. Rather than add new constructions \nto the language we will assume that all global functions are ezported (i.e. that they are visible in \nother modules). From the compiler s point of view, the fact that a func\u00adtion is exported means that its \nactual parameters may be unknown because it can be invoked outside the current mod\u00adule. To handle this \nwe have to introduce a representa\u00adtive for unknown values. As customary [8] this is noted top (T). Approximation \nsets are now subsets of Al = {T, f ixmm, f lonunr}. When a variable s approximation set cent ains T, \nit means that any value may be bound to the variable. Algorithm 2.2 contains the updated SUA algorithm \nIt makes use of the new notation: II $..Pcr~ The set of II s exported functions. In this new version \nof the analysis, the traversal of the call graph is initiated by all exported functions. The formal pa\u00adrameters \nof all exported functions are initially approximated by {T}. The fhnction dspread. ~ will be useful later \non to spread T into data storage approximations. Stml( 11 )= Vfaue.port Sua&#38;rt (f ) Sua~=port ( \n~ )= let rb=f s arity Vk[l..? b] L., ( fJwgi ) +-A.., ( f$.arg, ) U{ T}, if f not yet processed in this \niteration then let z=~ua~~t ( f $b..iy ) A.., ( f$.e. ) i-Aw( f$res )Ua, 4~pread-~ ( Awr ( t~res ) ) \n Stmipp( f, al, . . .. an )= if f is imported then {T} else Vie [1 ..n] let z=tkm~,t ( a~ ) A... ( f \nlar~{ ) + Avmr ( flargi ) u~, if f not yet processed in this iteration then let ic=St4a&#38;~t ( f$b~dv \n) Av.r( f$.es ) + A.., ( f$m,, )Uz, A.., ( /$... ) spread-T ( a )= a Algorithm 2.2: A first-order analysis \nwith modules 2.4 The higher-order SUA with modules We now extend the analysis to accept as input a higher-order \nfunctional language. Three new constructions are added to A: make-closure (to create closures), closure-ref \n(to ac\u00adcess a closure s free variables) and cl o sure-call (to invoke a closure). Here are the modifications \nto A s gramman ::= A ... [ (mike-closure f w . . .v) function sua~,i is not defined here becsuse it \nhas the same clef. inition as ~ua~,, (with references to ~ua~,, replaced with ~ua~,,). This kind of misuse \nwill be used in the remainder of the paper to avoid redundancy. I (closure-ref w k) \\ (closure-call A \nv . . .v) The usefulness of these constructs rests in the ability to easily translate Scheme programs \ninto A. The translation of the following program: (clef ine (curry-plus X) (lambda (y) (+ x y))) (clef \nine (add a b) ((curry-plus a) b)) (clef ine (main) (add 3.4 5.6)) is: (clef ine (curry-plus x) (nrake-closurel \nf x)) (clef ine (f p y) (+ (dosure-ref p O) y)) (clef ine (add a b) (closure-call (curry-plus a) b)) \n(clef ine (main) (add 3.4 5.6)) The translation required to map Scheme, ML or other higher-order functional \nlanguages ~o A is the so-called J\u00adlifting transformation [16]. Sua~~t ( atree )= case utree [ (ilosure-call \ne al . . . an) ]: u Sua2.Iocall( f, al, .... an ) f6~+L( e ) [ (make-closure; f VI . ..) ]: let al= \nS74a~,t ( q ~. . A.1.( i ) t msdre-closure( j, al, . . . ), {clO;} u [ (closure-ref f k) ]: clos~-ref( \nAA ( i ), k ) clo~@ua~at( f ) end Sua2 ,lOcail( e, al, . . . )= case e ClOi : Sua&#38; ( closur~mction( \nA.to ( i ) ), al, . . . ) else: Sua~ailUre () end Su::pp ( j, al, .... an )= If n = ~ s arity then Sua&#38;( \nf, w, .... an ) else Sua~ailUre ( ) Sua$ailure ()= 0 A s%:::: a )= Su;:=pmt ( closure=mction( A.,. ( \ni ) ) ), a Algorithm 2.3: A higher-order analysis with modules Closures introduced bv make-closure are \nfor now the only data structures of our language. SUA is modified to compute information about types \nand data storage by adding closure approximations . A closure approximation is a tuple cent aining a \nclosure fimction and a closure environ\u00adment (a list of approximation sets, one for each free vari\u00adable). \nClosure approximations are created by the function 52  mske-closure. The fimction associated with closure \nap\u00adproximation a is obtained with closure~unct ion(a) end clos~-ref(a, i) returns the approximation set \nassociated with the ith free variable of closure approximation a. Closure approximations are stored in \na closure approxi\u00admation table named d.,.. There is a one-to-one correspon\u00addence between entries in this \ntable and make-closures in the program. To add a closure approximation to an approxima\u00adtion set, clo; \nis added to the set, where i is the entry s index in A.,.. In this version of our algorithm, approximation \nsets are subsets of da = {T, f ixn~ f lonum, CIOI, . . ., C1O*}, where k is the number of make-closures \nin the program. A new problem arises with functional values. In latently typed languages closure-call \ncan lead to two possible er\u00adrors: the object given to closure-call is not a closure or the number of \narguments is incompatible with the fimction called. We have to deal with these possible errors in the \nSUA. For the sake of simplicity we suppose that closure-ref is always correct. This is reasonable since \nthese constructions are inserted by the program which is in charge of the J-1ifting end not by the user. \nThe treatment of errors is straightfor\u00adward: errors just produce empty approximation sets. This means \nthat an error leaves all the approximation sets as they are. This is sound because at run time, if an \nerror occurs, the program is interrupted, so errors do not return values. Our handling of closures has \nbeen guided by their special nature: they are immutable data (since we are using flat closures, mutable \nfree variables are stored in cells) and they are always accessed via the closure-ref procedure which \nrequires a constant index as second argument. It is thus possible to distinguish the free variables. \nAlgorithm 2.3 presents the extensions to SUA needed to accept the higher-order version of A (we assume \nfinction sua~pp in the algorithm uses the new version of the graph traversal function, i.e. Sua~,t ). \nThe dspread-~ fimction also requires a slight modification: if a closure can be returned by an exported \nfunction, this closure is also exported as it can be invoked with unknown actual parameters. The new \nT handles this. A spread\u00adLet s study SUA on the example of the curried addition curry-plus. Assuming \nno exported functions, the itera\u00adtion process starts by traversing main. The call to add as\u00adsigns the \napproximation set {f lonum} to a and b. The call curry-plus in turn assigns the approximation set {f \nlonum} to x, and the fimction s result is assigned the approximation set {clol } after storing a closure \napproximation over one flonum (i.e. make-~oaure(f, {f lonum}) ) in A.,.(l). The first argument of the \nclosure-call has the approximation set {c101 }, so SUA continues by analyzing f s body with the approximation \nset {C1O1 } for p. The closure-ref thus returns the approximation set {f lonum}. 2.5 The higher-order \nSUA with modules and lists SUA can be easily extended to accept other data types. In this section, we \npresent how lists are added to the analysis. Lists (in Lisp and Scheme) differ from closures because \nthey are mutable data. Lists are built out of pairs. The two fields of a pair can be distinguished in \nour approximation scheme (just like all the free variables of a closure are distinguished). ::= A ... \nI (cons A A) I (car A) [ (cdr A) I (set-cu! AA) [ fset-cdr! A A) The handling of pairs in the SUA is \nvery similar to clo\u00adsures. The SUA extended for pairs is shown in Algorithm (the cases for cdr and set-cdr \n! are left out because of their obvious symmetry with car and set-car !). d..., is a ta\u00adble similar to \nA.,o but for pair approximations. A pair approximation is a tuple of two approximation sets (one for \neach field of the pair) and is created with the func\u00adtion c=a. c~r(a) and c~r(a) respectively return \nthe ap\u00adproximation set associated with the car and cdr field of pair approximation a to which is added \nthe special approx\u00adimation obj (as explained in section 4.5, obj denotes the generic Scheme object type \nand is needed to prevent un\u00adboxed pairs). Approximation sets are now subsets of A3 = {T,f imuuz,flonum, \nC1OI,..., clo~, consl,..., cons., obj}, where c is the number of cells to cons in the program. The main \nchange is in the dspread-~ fimction. Even when closures are exported, the values they hold cannot be \nchanged because closures are immutable data storage. Because pairs are mutable they may be altered when \nex\u00adported (i.e. the fields of the pair can be changed using the set-car ! and set-cdr ! functions). The \nnew dspread_~ fimction handles this. When pairs are exported, T is added to the approximation set of \neach field. Note that a pair is spread at most once per iteration by dsnread-~. This is . necessary \nto handle cyclic approximations. Sua~Bt ( atree )= case atree [ (consi a d) ]: +t.on.( i ) + C==( h:,t( \na ), tk:,t( d ) ), {COl lSi} [ (carp) ]: u C=( Ln.( i ) ) Consiesua:#t ( p ) [ (set-cam! p =) ]: let \ns+=Sua~~, ( z ) Vcons~6Suag~t ( p ) A..., ( i ) t C=S( c=( Awn,(i ) )Uiz , c~r( Aeon,(i ) ) ), 0 end \n spread-T ( a ) = VcOnsiCa if cOnsi not already spread in this iteration then let a =c~( &#38;,n$ ( i \n) ), let d =c~r( .4 . . . . ( i ) ) %pread-T ( a ) ~ A spread-T ( d ), 4 .O=S( i ) + CWS( a (J{T}, \nd U{T} ), .... a Algorithm 2.4: A higher-order analysis with modules and lists Let s study SUA on the \nfollowing Scheme program (the program is presented in Scheme rather than in A so that it 53  is easier \nto read): 1: (define 1st (let ((pi (consl 1 o))) 2: (let ((p2 (consz 2 PI))) p2))) :: (define (length \n1) 5: (if (pair? 1) (+ 1 (length (cdr l))) O)) 6: (length lst) Types used by this program are: f ixnum, \npairs (i.e. consl and consz) and the special obj type (we omit boolean, needed for the pair? predicate, \nbecause it does not ap\u00adpear in a variable s approximation set). Here is the state of the tables at the \nend of the analysis: A..,(pi) = {consl} A...(p2) = {consa} Av..(lst) = {conaz} Am,(l) = {f ixnum, consl, \nconaz, obj} A....(I) = c~({flxnum}l {flxnuzIl) A...,(2) = cons({f ixnum}, {cons, }) The invocation of \nlength at line 6 has added consz to 1 s approximation set. Because of the call to cdr, the recursive \ncall at line 5 has added c~r(A..., (2)), that is {consl, obj}, in one iteration and {f ixnum, obj} in \nthe next iteration. 2.6 The higher-order SUA with modules, lists and vectors We conclude this section \nby adding vectors. ::= A ... [ (make-v-t A M / (vref A A) \\ (vset! A A A) Sua~,t ( atree )= case atree \n [ (mSke-VeCti Zen tiller) ]: Sua~~t ( len ), A..., ( i ) + maii=ect( t%ta~.t ( filler ) ), {VeCti} [ \n(vref u O) ]: Sua~4t ( o ), u V=f ( 4,.=*(i)) end A spread-~( a = VvectiGa if vect< not already spread \nin this ite.atic.n then let r =v~f( &#38;.t( i ) ) A spread-T ( r ), A...,( i ) + mti~ect( . U{T} ), \n.... a Algorithm 2.5: A higher-order analysis with modules, lists and vectors Vectors differ from closures \nand lists in that they are mutable and because it is not possible, a priori, to know, at compile time, \nwhich part of a vector is addressed when us\u00ading vector accessors. SUA computes information about types \nand data storage but it does not discover the exact value of a fixnum. So for vectors the SUA merges \nall possible values cent ained in a vector into a single approximation set (e.g. if a vector is composed \nof a character and a fixnum, SUA will indicate that each entry is a character or a fixnum ). Algo\u00adrithm \n2.5 presents the modification to our previous analysis to support vectors. A...t is a table similar to \nA..., but for vector approx\u00adimations. Vector approximations are created by the func\u00ad tion mak~vect and \nv~f (a) return the approximation set associated with the vector approximation a. Approxima\u00adtion sets \nare now subsets of #14 = {T, f ixnum, f lornq c101, . . . . clok, canal,... , cons=, vectl, ..., vect \n, obj}, where is the number of calls to make-vector in the program. Just like for pairs, vector exportations \nhave to be handled carefblly. This kind of object is mutable so when exported vectors can hold any value. \n 2.7 Related work The SUA is an extension of Shivers Ocfa (Oth order control flow analysis) [29, 28]. \nWe have generalized his analysis to different data storage. Shivers analysis only handles clo\u00adsures, \nour analysis also handles lists and vectors. In a previous paper [26] we have presented an algorithm \nwhich is close to the present Sual. The goal of that work was to study the impact of control flow analysis \non function compilation. The analyses presented here are more gen\u00aderal because closures are only considered \nas one special data storage. Efficient closure compilation is not the focus here. We study the problems \nof unboxed representation and stack allocation. Ayers has studied similar improvements to Shivers Ocfa. \nIn his PhD thesis, he presents extensions for lists, vectors, etc. Our work has been realized concurrently \nwith his [24, 3]. The large difference between our analyses comes from the formalism. Ayers uses Galois \nconnection while we chose a more algorithmic approach. Jagannathan and Wright describe in [15] a control-flow \nanalysis and an application which removes type checks. Their analysis gives more precise type information \nthan ours be\u00adcause it does not merge types for polymorphic programs. More precise type information is \nvaluable to remove type checks but is not more valuable for a rnized representation. As explained in \nsection 4, we use unboxed representation for monomorphic program parts which are efficiently detected \nby our analysis.  2.8 Extensions Our input language A, is still much simpler than a full pro\u00ad gramming \nlanguage such as Scheme or ML. Some important constructions are missing. We present here how to add them \nto SUA. b Variable arity functions: this construction can be added to SUA by splitting fictional application \nin two sepa\u00adrate cases. Each time a function is applied (in a direct call or in a closure-call construction), \nthe analysis handles the last parameter of variable arity functions specially. In Scheme, this parameter \nis bound to the 54 list of the optional actual parameters. In SUA, this means that the approximation \nset of the last formal parameter is the approximation set of the list of the optional actual parameters. \n The Scheme special form apply: this construction is an alternative way to apply functions. Rather than \napply a function to its n actual parameters, it is applied to a list of length n which holds the actual \nvalues. Since our analysis is able to distinguish individual elements of a list, the apply form can be \nefficiently handled: each formal parameter is assigned the corresponding approximation set from the list \ns approximation set.  call/cc: the analysis does not treat call/cc spe\u00adcially. This library function \ntakes closures as argu\u00adments. These closures therefore escape because call/cc is managed as any imported \nfunction. call/cc s result is simply approximated by the set {T}.  Multiple values can be easily added \nby the addition of a treatment similar to the one for vectors.  Scheme and ML global variables: global \nvariables can be managed using a global environment. A subtle problem with global variables can arise \nwhen the source language allows references to global variables before their declaration. For example, \nthe Bigloo Scheme compiler considers this program as legal:  (module foo (static X)) (clef ine (foo) \n(print x)) (foo) (define x 8) Before its declaration x holds a special value (uninitial\u00adized, which stands \nfor the lack of initialization) which has to be stored in x s approximation set. This im\u00adplies that no \noptimization can be applied to x because it holds at least two types: the type of the uninitialized value \nand the fist value used on the declaration site. In order to give a unique type to global variables, \nbe\u00adfore the SUA ansdysis, we perform a simple conservative analysis to determine the set of variables \nwhich are al\u00adways defined (then initialized) before being referenced. This analysis is straightforward, \nbecause it consists of a simple abstract tree traversal. These variables do not hold the special uninitialized \nvalue in their approxima\u00adtion set. Stack allocation The storage allocation optimizations discussed here \nassume an area of memory managed by a garbage collector and an area of memory managed as a stack. The \nstack is scanned by the collector to find the root pointers. Activation records are allocated on the \nstack when entering a procedure and they are removed from the stack upon procedure exit. Within a procedure \nactivation the allocation of additional storage from the stack is permitted; this storage is freed when \nthe procedure exits. We present a conservative optimization based on SUA which automatically replaces \nheap allocations by stack al\u00adlocations when it is legal to do so. This optimization is valuable if stack \nallocations and desdlocations are fast with respect to heap allocations. For the sake of simplicity we \nadd a let form to A: ::= A ... I (let ((u A)) A) This form has no impact on the SUA algorithm (it can \nbe seen as a macro over fhnction application). For our stack allocation optimization, we assume that \nin A source pro\u00adgrams all allocations (the result of make-closure, cons and make-vect) are bound to local \nvariables using let forms. Consequently, each allocation has a unique name. Here are three examples that \npresent interesting situa\u00adtions for our optimization. 1: (define (foo) 2: (let ((x (consl 1 2))) (car \n(id x)))) :: 5: (define (id Z) Z) In this first example, the pair bound to x can be stack allo\u00adcat \ned since it is never used outside of x s let extent. 1: (define (bar) 2: (let ((x (cons~ 1 2))) (let \n((y (consz 3 x))) : (cdr y))))  In this second example, the pair allocated at line 2 is live when bar \nexits (since it is the result of bar) while the one at line 3 is dead outside x s scope. Only the second \npair can be stack allocated. 1: (define (hux) 2: (let ((PO (consl 1 2))) (let ((pi (consz 3 4))) : (let \n((pz (gee PO PI))) 5: po)))) 6: 7: (define (gee a b) (set-cclr! a b)) Fh-mlly, in this example, no \npairs can be stack allocated because they are all live when hux returns. 3.1 When is it legal to stack \nallocate? We present in this section the condition an allocation must satisfy to be done on the stack \nrather than in the heap. For now we are not concerned with preserving the tail-recursive property of \nthe program (at the end of this section we discuss modifications of our optimization to make it suitable \nfor lan\u00adguages like Scheme which have to implement tail-recursive calls without consuming stack space). \nAn allocation can be done on the stack if the data stor\u00adage allocated is not live at the end of the procedure \nthat allocates it. Data storage is live at the end of a function if it appears (directly or indirectly) \nin the result of the function that allocated it or if it appears (directly or indirectly) in a global \nvariable. Compile time computation of the liveness property requires information about data storage which \nis provided by SUA. This information is: the set of allocations a variable may be bound to, the set of \nallocations possibly contained in an allocation, and the set of allocations possibly returned by a fi.mction. \n 3.2 Stack allocation decision algorithm Each allocation is marked with a stamp. The current stamp is \nincremented each time a let form is encountered. When a fimction definition for f is reached, the current \nstamp is saved in h and then f s body is processed. Each allocation in the approximation set of f s result \nthat is stamped with a more recent value than h escapes from f and so cannot be stack allocated. In addition \nall allocations which are acces\u00adsible from a global variable cannot be stack allocated. The first part \nof the algorithm (algorithm 3.1) dispatches bet ween two function types: exported functions and static \nfimctions. These two kinds of fwction differ. For the first one, no returned (or pointed by) value can \nbe stack allo\u00adcated (since the function is exported the result value usage is not known to the compiler). \nFor the second one, only data storage allocated by the function cannot be stack allocated. *A!*: o (O \nstands for an initial stamp value) Sta.lsprog ( fl )= Vfa if f6Wexp0rt then Stackeaport ( f ) else Stack,t~t;. \n( f ) Stack, tati. ( $ )= let h=w?te Stack ( f +bo.+~ ) , sprea~n,~ac~ab~e( A.., ( f ) , h, *% ) Stacke=po,t \n( f )= Stack ( f &#38;o@ ), SPrIXi&#38;natac~ab~e ( &#38;.IIt ( f ) , 1, 1 ) Stack ( atree )= case atree \n (closure-call e al . . . an) ]: Stack (e), W[l..rl] Stack (a; ) (set! 7Jar val) ]: Stack ( wal ) (labels \n((fl . ..) . . . (f~ . ..)) atree) ]: Vw[l..n] Stack~tatiC ( fi ) , Stack( atree ) (let ((mm vai) ) \natree) ]: *%!* +-* M* + 1, Stack( vat ), Stack( atree ) (f al ... an) ]: if f is an allocator then \nmark! ( atree, xl% ), VKII..?3] Stack( ai ) end Algorithm 3.1: The st ackabilit y algorithm OUr stack \n~gorithm uses a spread...ekabieie f~ction. This fimction is similar to A spread-~. It follows a data \nstorage chain to mark as unstackable all allocations which are younger (marked as younger) than the value \nof the second argument. The algorithm s main part is the fimction Stack. It dis\u00adpatches on the abstract \nsyntax tree. Before calling Stackp.~g all allocations which have been passed as argument to A spread-~ \nhave to be marked as wa.stackable. These al\u00ad locations escape from the current module. The compiler is \nnot able to discover the exact usage of these allocations and thus, it cannot make any assumption about \ntheir lifetime. Once the algorithm has completed, allocations which have not been marked as unstackable \ncan be stack allocated (we will introduce new constraints to safely allocate data storage in stack in \nthe next section). ( a, rnin, maz )= if a not yet processed for the values min and maz then case a pairi: \nifmark( a)>min and mark( a)<macr then mark-unstackable! ( a ), Va 6c-=( AP.ir ( i ) ) Pr~dun.tack.bie \n( a , rein, maz ), va ~cdr( dp.i~ ( i ) ) spreadum$ta=bab~e ( a , rein, maz ) VeCti : ... GIOi : ... \nend wre+nat~ckable Algorithm 3.2: Spreading unstackabllity Let s study the algorithm s behavior on our \nprevious bar function example. The fmction allocates two pairs consl and consz. SUA proves that y points \nto consz (which points to consl) and x points to consl. The consl pair is pointed to by the result of \nbar. So, the algorithm concludes that this pair cannot be stack allocated. 3.3 Extension for proper tail-recursion \nimplementations and safety considerations Some languages like Scheme require that executions of an iterative \ncomputation t eke constant space. Let s consider the following two functions: (define (fool x y) (define \n(foo2 x y) (if (=yO) (if (=yO) (display x) (display x) (fool (cons 1 2) (f002 (cons x x) (-y l)))) (-y \nl)))) The two functions differ only in their recursive call. In f 001, only one allocated pair of the \nrecursive call is live at a time; in f 002, allocated pairs are linked together and they are all live \nat any given point. The common intuitive idea of the tail-recursive property imposes an implementation \nto require only one free pair to run f 001. Our algorithm presented in algorithm 3.1 provides rough data \nstorage lifetime. It is not able to distinguish that pairs allocated in f 00 I cannot be stack allocated \nwhile pairs allocated in f 002 can be. The problem is more general than tail-recursion. As re\u00advealed \nby Chase in [7], there is a general safety problem for stack allocation optimizers. Sometime, allocating \nan object in the stack rather than in the heap cxt ends its lifetime. For inst ante, in f 00 i, a garbage \ncollector is free to reclaim pre\u00advious allocated pairs but if these pairs are stack allocated they will \nbe all freed at the same time and required space to run this program is no longer constant. Stack allocation \ncan convert a running program into one that fails. In his paper, Chase, presents safety conditions for \nstack allocation in order to decide the replacement of heap allocations by stack allocations in presence \nof loops or recursions. Hk method and our work are complement ary.  3.4 Related work Kranz presents \nin [17] the strategy used by Orbk to realize stack allocations. Hk method is less precise than ours. \nIn Orbit only closures can be stack allocated and only if they are passed as an argument or applied. \nThese conditions are very restrictive. In [9], B. Goldberg and G. Park present a method for op\u00adtimizing \nthe allocation of closures in memory, Their method is based on what they call an escape analysis, an \napplic\u00adtion of abstract interpretation to higher-order functional lan\u00adguages. Escape analysis determines, \nat compile time, if any arguments to a function have a greater lifetime than the function call itself. \nThe language studied does not contain side effects and the only data storage used are closures and lists. \nLkt management is very rough because their analysis is not able to distinguish the elements of a list. \nSeparate compilation is not studied in that paper. Ruggieri and Murtagh present in [23] a data storage \nal\u00adlocation framework called sub-heap allocation. This frame\u00adwork consists of partitioning the heap into \nsub-heaps, one associated with each active procedure. The contents of the sub-heap associated with a \nprocedure is exactly the objects whose lifetime are guaranteed to be contained by the life\u00adtime of the \nprocedure but not by any younger procedure. The paper presents an algorithm to compute lifetime analy\u00adsis \nin order to divide the heap with an input source language which cent ains no higher-order functions nor \nside effects. Ayers also presents sub-heap optimization in [3]. Our lifetime analysis is similar to his \nbut we do not use it for the same goal. We decided to stack allocate rather than sub-heap allocate for \ntwo reasons: Safety considerations presented by Chase [7] are very difficult to satisfy with the sub-heap \nallocation frame\u00adwork because it tends to enlarge object lifetime. Opti\u00admized objects are not freed when \nleaving the function they have been created but when leaving the function which is the upper bound of \ntheir lifetime.  Sub-heap allocation is difficult to implement efficiently. This framework needs allocated \nmemory to hold sev\u00aderal objects which share a lifetime upper bound. If all objects are freed at the same \ntime, they are al\u00adlocated at different moments. This has two negative incidence:  o Sub-heap size is \ndifficult to estimate. Sub-heaps will probably need linking machinery to be ex\u00adtended which slows down \nthe allocation process. o Sub-heaps must be allocated empty (i.e. a sub\u00adheap cannot be filled up at \nthe moment of its creation). Included in a runtime with automatic memory management, uninitiahzed blocks \nof mem\u00adory are annoying.  Tofte and Talpin present in [31] a way of implementing A calculus based languages \nusing regions for memory manage\u00adment. At runtime the store consists of a stack of regions. All values \nare put into regions with the intended goal to avoid garbage collection in the runtime system. The allocation \nof new regions and the bkdings of values to regions rely on a typing system and so this technique cannot \nbe applied to dynamically typed languages. In [4], Banerjee and Schmidt present a static criterion to \ndetect stackabllit y of environments for a call-by-value .X calculus. The presented analysis does not \ninclude higher\u00adorder nor imperative features. Thus their approach and ours are hard to compare. Other \napproaches to stack allocation have been proposed by Hudak in [14]. 4 Data representation Approximations \ncomputed by our SUA can be used to remove some runtime type checks. A type check can be removed when \nSUA proves that all the values possibly contained by the argument of the test is (or is not) of the tested \ntype. This idea has been presented by Shivers [29, chapter 9] in his t~pe-recouery. The goal is to speed \nup program execu\u00adtion of latently typed languages. In the same way, Henglein in [13] and Ayers in [3, \nchapter 6] have presented frameworks to remove useless tagging/untagging operations. Heinglein uses type \ninference while Ayers uses an extended control flow analysis close to our SUA. The intended goal is more \nthan compile-time type check reductions. Appel claims the use of tag bits leads to inefficiency [1], \nSteenkiste and Henessy evaluate, in [30], at 25~o the cost of type checking and tag\u00adging operations for \nclassical Lisp applications. We think this time figure is an upper bound of the real cost. Classical \ndata flow optimization (such as copy propagation) removes most type checks and, for smart runtime design, \ntagging and untagging operations could require only a logical mask in the most frequent cases. On modern \ncomputers, applying a mask costs one cycle and these operations are much cheaper than memory fetches. \nThey have a very small impact on global performance (for more details see [25]). We think a much more \nimportant source of inefficiency for language like Scheme or ML come from uniform data representation. \nTag handling is cheap but uniform repre\u00adsentation is very expensive. 4.1 Uniform representation Using \nuniform data representation, all objects have exactly the same size (usually one word, i.e. pointer size). \nObjects that do not fit naturally in one word, such as long floating\u00adpoint numbers, have to be boxed \n(allocated in the heap and handled through a pointer). This scheme makes it possible to assume a default \nsize, common to all objects, and default calling conventions, common to all functions. Polymorphism leads \nto the use of the uniform representa\u00adtion because an object can belong to several different types at \nthe same time and the actual type cannot be known at compile-time. Polymorphic fimctions (e.g. the identity \nfimc\u00adtion) can be applied to arguments of any type. Therefore, when compiling these functions, the compiler \nknows neither the size of the argument nor the correct calling convention. 4.2 The uniform representation \nis inefficient We claim uniform representation results in a serious loss of efficiency and we present \ntwo arguments for this assertion. Objects that do not fit in one word have to be boxed. Long floating-point \nnumbers dramatically illustrate this. For floating-point intensive programs, boxing numbers can slow \ndown applications by a large factor. This problem has such an important impact that many ML and Lkp implementa\u00adtions \nuse ad hoc methods to reduce the creation of number handles (descriptions of these methods for modern \nimple\u00adment ations can be found in [20, 12]). Mainly, they consist of local optimizations to avoid boxing \nnumbers for interme\u00addiate results. Another negative impact of floating-point boxing is reg\u00ad ister allocation. \nWhen flonums are allocated in memory, every floating-point operation, requires a memory fetch for each \noperant. These operations are expensive and much less efficient than a solution where the numbers are \nheld in reg\u00adist ers. Tagging optimization is not boxing optimization in the sense that it removes tagging/untagging \noperations but such optimized programs must still satisfy the polymorphism con\u00adst raint. The y are still \nobliged to box numbers (even if no tag is written on the handle or stored in the allocated memory). Small \nobjects (i.e. characters) are also inefficiently man\u00adaged by uniform representations as memory is wasted. \n 4.3 Mixed representation Mixed representation is a representation where all objects are not reauired \nto be of the same size. It mixes boxed ob- L\u00ad jects and unboxed objects. Initial efforts using mixed \nrepre\u00adsentation are Leroy [18, 19] and Peyton-Jones and Launch\u00adbury [21], and more recently Shao and \nAppel [27]. Their works are complementary because Peyton-Jones and Launch\u00adbury introduce a (non-strict) \nlanguage where boxing opera\u00adtions are explicit &#38;d introduce several source-to-source op\u00adtimization \nfor this language while Leroy and Shao and Ap\u00adpel present a translation of ML to this mixed language. \nIn this paper, we will focus on the translation of source lan\u00adguages (like ML or Scheme) to mixed languages, \ncomparing our work to Leroy s. Leroy s translation only uses type information. It mixes specialized representations \nwhen the static types are monomor\u00adphic and uniform representation when the static types are polymorphic. \nCoercions between the two representation styles are performed when a polymorphic object is used with \na more specific type. As Leroy presents, in the case of a polymorphic fimction, for inst ante, coercions \ntake place just before the function call and just after the function result . This solution is very elegant \nbecause the translation s qual\u00adity does not suffer from separate compilation (type informa\u00adtion are propagated \nacross ML modules) but is has some disadvantages. Every time a polymorphic function is used, objects \nhave to be boxed. Some data accessors as-epolymor\u00adphic. For instance, vector accessors are polymorphic \nwhere the same fimction is used to access a vector of iixnums or a vector of flonums. Vectors are a too \nimportant kind of data storaee to be out of the scoDe of this transformation. In other words, Leroy s \ntranslation requires some ad hoc treat\u00adments for some special functions. The second restriction to Leroy \ns work is about the input languages of its translation. Programs have to be statically type checked, \nand as a con\u00adsequence Leroy s optimization is not applicable to the Lkp family. 4.3.1 Untagging vs. unboxing \nOptimizing taggingluntagging operations as in [13, 3] does not require the same analysis as mixed representation. \nCon\u00ad straints about untagged representations are weaker than for unboxed representations. An object can \nbe untagged as soon as its type is never required at runtime, without any poly\u00ad morphism consideration. \nFor instance, with untagged rep\u00adresentation, a vector can hold in its first slot an untagged floating \npoint number and in its secand slot a tagged one. This is not possible with unboxed representation because \nthese two kind of objects do not have the same size. Untag\u00adging optimization consists of type analysis \n(possibly using type system as Henglein, control flow analysis as Ayers or any other data flow analysis) \nwhile unboxing optimization requires type and polymorphism analyses. 4,3.2 Other polymorphism implementation \nimprovements In [IO], Goubault presents an optimization for latently typed languages. Like our efforts, \nits source language is not re\u00adquired to be statically type checked. Goubault uses data flow equations \nto choose unboxed representation. However it is difficult to compare his work with our because the method \nemployed is very different than ours and the paper cent ains neither measurements nor examples. Harper \nand Morrisett present in [11] a new scheme to im\u00adplement polymorphism. The key idea is to separate values \nand types for polymorphic functions and to defer the selec\u00adtion of the code to execute until types are \nknown (e.g. at run\u00adtime). Unfortunate elly this work addresses statically typed languages and cannot \nbe applied to languages such as Scheme.  4.4 SUA and mixed representation In this section, we use the \nSUA approximation to introduce unboxed represent ations. This is done in two stages. 4.4.1 Type election \nA first stage after the SUA analysis is the type election which gives types to all variables and function \nresults. This pass obviously uses SUA approximate ions. It does not perform any data flow analysis to \nchoose better type. Let us study type election on the following Scheme program (-f x and =f x are the \nfixnum subtraction and equality test procedures): (clef ine (bcopy ! clst src size) (let loop ((i (-fx \nsize l))) (if (=fx i -1) o (let ((c (string-ref src i))) (string-set ! dst i c) (loop (-fx i l)))))) \n(define (copy-string src) (let* ((len (string-length src) ) (new Gmke-string len) ) ) (bcopy ! new sr. \nlen) new) ) (copy-string foo ) SUA shows that variables new, dst and src are strings, vari\u00adables len \nand i are only fixnums and variable c is a char\u00adacter. SUA is able to compute these type approximations \nbecause the types of the library functions (string-length, rnake-string, etring-ref and string-set ! \n) are known by the compiler. Each one of the variables contains one type in its approximation set. Hence \ntype approximations also are the results of type election. If a variable contains more than one type \nin its approximation set, then it is given the special obj type. SUA merges ~ possible values in single \nsets. Hence, if a variable cent sins one unique type k its approximation, this variable can only take \nplace in a monomorphic pro\u00adgram. For instance, if SUA shows that the formal parameter of the identity \nfunction can only be a fixnum it means that this function has only been given flxnums as argument, no \nmatter the polymorphism of this function. This is the main adventage of our method compared to Leroy \ns one. SUA isolates monomorphic parts of polymorphic programs, thus our method allows us to use unboxed \nrepresentation when Leroy s fails.  4.4.2 Type conversion The second stage is celled type conversion. \nIt introduces conversions between boxed representation and unboxed rep\u00adresentation in the abstract syntax \ntree. Objects can be boxed or unboxed. One type exists for these two states. The boxed state is denoted \nby the obj type. Conversion introduction is straightforward because the abstract syntax tree is fi.dly \nennotated. Here is an example that illustrates type conversion (define (id x) x) (define (foo y) (id \n(+fl 1.0 (id y)))) Let s assume that f oo is exported, hence, y end f oo s result have type obj. Identity \nid is invoked with a flonum (result of +f 1 invocation) and an obj, so formal x end the function result \nare typed &#38; obj. Conversions are then inserted. (define (id x) x) (define (foo y) (id (float-box \n(+fl 1.0 (float-unbox (id y)))))) Cast( atree, T )= case atree [k]: cormert!( k, 7( k ), T ) [v]: convert!( \nu, T( v ), T ) [ (if atree street street) ]: let atree=Ca~t ( atree, boolean ), let atreet=Ca~t ( street, \n7 ), let atreef =Ca~t ( street, 7 ) (if atree atree: street) [ (jai . ..) ]: let atree=[ (j Cast ( al, \nT( J$arg, ) ) . ..) ] mnvert!( atree, T( f ), T ) end Algorithm 4.1: Type conversion introduction type \ncheck using the char? predicate. No type checks are introduced for statically type checked languages. \n 4.5 Unboxed data storage In this section we discuss the unboxed representation of the three A data types \npresented in section 2.   4.5.1 Unboxed pairs Pairs have a special status: they are widely used (many \nlibrary fimctions exist to manage them) and in Scheme they are heterogeneous data structures (i.e. elements \nof a list can be of different types). For these reasons, we have decided to make pairs hold boxed values. \nIf pairs were allowed to hold unboxed values, they would not have a fixed size and library functions \nwhich have to be applicable to all pairs would be inefficient and difficult to write. To prevent pairs \nfrom holding unboxed objects, we simply force c~r and car to have the obj type in their approximation \nsets. 4.5.2 Unboxed vectors Vectors are widely used in all programming languages. We think vectors are \nnot used in the same way as pairs. Even if Scheme vectors are heterogeneous (each vector slot can be \nof a different type), we think they are mostly used as homogeneous data storage and thus have allowed \nunboxed values in vectors. Mixed vectors (vector holding boxed and unboxed objects) are forbidden because \nthis would prevent the efficient implementation of vector indexing fmctions. If a vector only contains \nelements of a given type, it will be transformed into an unboxed vector. If a vector contains at least \ntwo elements of different types, it will be a boxed vector. As shown in section 2.6, SUA merges all possible \nvalues held by a vector in a single approximation so it is easy to check if all its elements are of the \nsame type. 4.5.3 Unboxed procedures Because closure creation and access to the free variables are handled \nby the compiler, each closure creation can be treat ed independently. Closures can hold boxed and un\u00adboxed \nvalues (because SUA distinguishes approximated val\u00adues in the closures free variables) but unboxed closures \nare not allowed. 5 Experimental results SUA has been implemented in the new release of the Bigloo Scheme/ML \ncompiler. Both stack optimization and un\u00adboxed representation are implemented. Hence, we have been able \nto make experimental analyses and performance mea\u00adsurements. Algorithm 4.1 presents a fragment of the \ncomplete type con\u00adversion algorithm. Function j _ is a function that returns the type of a-expression. \nFunction convert! takes three argu\u00adments: an abstract syntax tree, a from type, and a to type. It introduces \nboxing operations required by the translation. Function convert! is source language dependent. For la\u00adtently \ntyped languages with boxing operations, it introduces runtime type checks to ensure the soundness of \nthe transla\u00adtion. For instance, when introducing conversions from o bj to character, the Scheme convert! \nversion also introduces a Experimental results obtained by running some Scheme benchmarks on a DEC Alpha \n(DEC 3000/300 (150 MHz), running OSF/1 v3.0, with 160 MBytes of memory) are given in F@re 1. The times \ngiven are user+syst em time, in\u00adcluding garbage collection time. Biglool.7 is the current distributed \nversion of the system, Biglool.8 is the new ver\u00adsion including the unboxed representation and the stack \nal\u00adlocation optimization. Both versions of Bigloo use Boehm s garbage collector release 4.7 [6]. This \ncollector allows am\u00adbiguous pointers and uses a traditional mark &#38; sweep algo\u00adrithm. Gsc is the Gambit-C \ncompiler version 2.3a, S2C is Bartlett s Scheme-to-C compiler version 15mar93jfb [5] and Gcc is the popular \nGnu C compiler version 2.6.3, used at optimized level 2. Here is a short description of the Scheme test \nprograms we used: Nucleic (3496 lines) : Floating-point arithmetic. Fft (127 lines) : Floating-point \narithmetic, loops. Bcopy (43 lines) : Strings, chars, fixnum, loops. Ttak (2o lines) : Function calls \n(with tuples). Beval (548 lines) : Functional, conditional. Boyer (6o6 lines) : Term processing, functional. \nMaze (800 lines) : Arrays, fixnum, iterations. Slatex (2821 lines) : IO, strings, lists. Mbrot (46 lines) \n: Floating-point arithmetic, loops. Compder Test i@Ri% Nucleic 9.0 s - Fft 1.3s 22.7 S 5.6 S 39.7 s 1.1s \nBcopy 9.9s 12.2 s 14.5 s 12.2 s 9.9 s Ttak 2.9S 12.0s 4.8 S 57.2 S 1.9s Beval 6.7 S 6.5 S 6.9 S 14.8 \nS Boyer 9.4 s 3.4s 3.8S 4.1 s Mase 6.2 S 7.7s 8.0S 18.7 S Slatex 7.8 S 7.8 S 23.9 S 22.9 S Mbrot 1.0 \ns 20.1 s 9.2 S 35.6 S 1.0: F&#38;me 1: RUI ime statistics on )EC Alpha Significant speed up occurs with \nthe numerical bench\u00admarks Nucleic, Fft and Mbrot. On Mbrot and Fft (which is a translation of a C routine \nfrom [22], not the Lkp version from the Gabriel suite) Bigloo s performance is very close to Gee. Fft \nand Mbrot are efficiently compiled by Bigloo; no floating point values get boxed. Fft makes use of vectors \nof floats which are optimized as described in Section 4.5.2. Nucleic computes 7 million floating point \nvalues. Our unboxing optimization allows Bigloo to only allocate 13608 flonums in the heap. The difference \nin performance between the Bigloo and Gcc versions is mainly due to the use of struc\u00adtures to hold 3 \nD points. In the C version, all the structures are explicitly allocated on the stack. The Scheme version \ndoes not allow our stack optimization to be frequently ap\u00adplied. Hence, profiling the Bigloo executable \nshows that even though many heap allocations are avoided, 30% of the execution time is still spent in \nthe garbage collector. Ttak is written in a ML style using tuples to pass ar\u00adguments. Our stack optimization \navoids heap allocation en\u00adtirely and the speedup is thus important. The impact of stack allocation depends \non the program tested. The most import ant speedup is observed for Tt ak (75 YOof memory is allocated \non the stack, which leads to a speed up factor of 5). F&#38;-ure 2 presents dynamic statistics on the \namount of memory allocated by the programs. For each program tested and for each compiler, the amount \nof heap memory allocated is given. The total amount of memory allocated on the stack for Biglool.8 is \nalso given. In accordance with the execution time speedup, the main reduction of heap allocation is observed \non numerical pro\u00adgrams (Nucleic, Fft and Mbrot). Except for the Ttak program, stack allocations are not \nwidely applied. This poor result may come from the style of our programs. The nat\u00adural Scheme style is \nto write allocating>; functions which return fresh allocations as in: . Ted Biglool.7 Biglool.8 (heap) \nBiglool.8 (stack) Nucleic 747589 k 127523 k 5655 k Fft 425432 k 174 k Ok Bcopy 140 k 98k Ok Ttak 301148 \nk Ok 301148 k Beval 32947 k 32903 k Ok Boyer 14369 k 14318 k Ok Maze 17563 k 15902 k Ik Slatex 67607 \nk 67431 k 2k Mbrot 265589 k 27 k Ok F@we 2: Allocation statistics on DEC Alpha (define (foo x) (CU (gee \nx))) (clef ine (gee x) (cons x x)) The pair built in gee cannot be stack allocated by our method. The \nworst case complexity of the SUA algorithm is high. The maximum number of iterations to reach the fix \npoint is the product of the maximum size of approximation sets and the maximum number of approximation \nsets (i.e. nz for a program of size n). Each iteration has a 0(n2 ) complexity (the call graph traversal \nis O(YZ) and operations performed on the t ree s nodes are 0(n)). The overall complexity is thus 0(ra4 \n). In spite of this complexity, our analysis is relatively fast in practice. F@.we 3 presents statistics \non compilation time. For each program tested, we have measured the time required by SUA, the compilation \ntime until the C code pro\u00adduction and the global compilation time including the C compilation. In the \nworst case (Slat ex), the time required by SUA is only 20% of the overa~ compilation. \u00ad (70mpdat$on tame \nTest SJA Biglool.8 6 Biglool,8+cc t Nucleic 7.1 s 22.9 S 0.31 538 S 0.01 Fft 0.1 s 0.8 s 0.19 2.9 S 0.09 \nBcopy 0.1 s 0.5 s 0.20 1.5 s 0.07 Ttak 0.1 s 0.7 s 0.14 2.6 S 0.04 Beval 1.8 S 3.9 s O.~6 17.8 S 0.11 \nBoyer 0.1 s 0.9 s 0.11 3.5 s 0.03 Maze 0.4 s 2.0 s 0.20 6.9 S 0.06 Slatex 15,7 s 22.6 S 0.69 79.7 s 0.20 \nMbrot 0.0s 0.5s o 1.8S o F@u-e 3: Compilation statistics on DEC Alpha 6 Conclusion We have presented \nin this paper a new static analysis method called Storage Use Analysis (SUA) which extends Shivers Ocfa \nto modules and general data storage. This analysis al\u00ad10WS two important optimization: unboxed representation \nand stack allocation. None of these optimizations require type information, so both statically typed \nlanguages like ML and latently typed languages like Scheme can use them. Ex\u00adperimental results demonstrate \nimportant speedups for nu\u00admerical applications where a speedup factor of 20 has been measured for some \nprograms. Acknowledgments Many thanks to Pierre Weis and Alain Deutseh for early discussions and to Xavier \nLeroy and Joel F. Bartlett for their helpfhl feedbacks on this work. References [19] X. Leroy. Unboxed \nobjects and polymorphic typing. In [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] \n[16] [17] [18] Symposium on Principles of Programming Languages, pages A. Appel. Runtime Tags Aren t \nNecessary. Technical Report 177-168, Albuquerque, New Mexico, January 1992. CS-TR-142-88, Princeton university, \n1989. [20] R. MacLachlan. The Python Compiler for CMU Common A. Appel. Compihng with continuations. Cambridge \nUni- Lisp, In Proceedings of the 1992 ACM Conference on Lisp versity Press, 1992. and Functional Programming, \npages 235 246, San Francisco, CA, USA, June 1992. A. Ayers. Abstract Anal~sis and Optimization of Scheme. \n PhD thesis, Massachusetts Institute of Technology, Septem-[21] S. Peyton Jones and J. Launchbury. Unboxed \nValues as First ber 1993. Class Citizens in a Non-Strict Functional Language. In Pro\u00adceedings of the \nACM Conference on Functional Progmm- A. Banerjee and D. Schmidt. Stackability in the Simple\u00adming Languagea \nand Computer Architecture, pages 636-666, Typed Call-By-Value Lambda Calculus. In lfst Static Anal-Cambridge, \nMA, USA, August 1991. ysis Symposium, pages 131 146, Namur, Belgium, Septem\u00adber 1994. [22] W. Press, \nB. Flannery, S. Teukolsky, and Vctterling W. Nu\u00ad mem cal Recipes in C. Cambridge University Press, 1988. \nJ.F, Bartlett. Scheme->C a Portable Scheme-to-C Compiler. Research Report 89 1, DEC Western Research \nLaboratory, [23] C. Ruggieri and T. Murtagh. Lifetime Analysis of D ynami-Palo Alto, CA, January 1989. \ncally Allocated Object. In Symposium on Principles of Pro\u00adgramming Languagea, pages 285-293, 1988. H.J. \nBoehm. Space efficient conservative garbage collection. In Conference on Programming Language Design \nand Im-[24] M. Serrano. De I utilisation des analyses de flot de contrde plementation, number 28, 6 in \nSIGPLAN NOTICES, pages 197-clans la compilation dcs langages fonctionnels. In Pierre Les\u00ad206, 1991. canne, \neditor, Actes den journ.4es dts GDR de Programma\u00ad tion, September 1993. D. Chase. Safety considerations \nfor storage allocation opti\u00ad mization. In f70njerence on Programming Language Design [25] M. Serrano. \nVera une compilation portable et performance and Implementation, Atlanta, Georgia, USA, June 1988. des \nlangages fonctionnels. Th.5se de doctorat d univcrsit6, Universit6 Pierre et Marie Curie (Paris VI), \nParis, France, P. Cousot and R. Cousot. Abstract interpretation: a uni-December 1994. fied lattice model \nfor static analysis of programs by con\u00ad struction or approximation of fixpoints. In Symponium on [26] \nM. Serrano. Control Flow Analysis: a Functional Lan-Principles oj Programming Languagea, pages 238 252, \nLos guages Compilation Paradigm. In 10th Symposium on Ap- Angeles, CA, USA, January 1977. plied Computing, \nNashville, Tennessee, USA, February 1995. B. Goldberg and G. Park. Higher order escape analysis: Op\u00ad[27] \nZ. Shao and A. Appel. A Type-Based Compiler for Standard timizing stack allocation in functional program \nimplemen-ML. In PToceedinga of the SIGPLAN 95 Conference on tations. In European S9mposium on Programming, \nnumber Programming Language Design and Implemenfafion, June 432 in Lecture Notes on Computer Science, \npages 152-160, 1995. May 1990. [28] 0. Shivers. Control flow analysis in scheme. In Proceedings J. Goubault. \nGeneralized Boxings, Congruences and Partial of the SIGPLAN 88 Conference on Programming Language Inlining. \nIn lf.t Static Analyaia Symposium, pages 147 161, Design and Implementation, Atlanta, Georgia, June 1988. \nNamur, Belgium, September 1994. [29] 0. Shivers. Control-Flow Analysis of Higher-Order Lan- R. Harper \nand G. Morrisset. Compiling polymorphism using guages or Taming Lambda. CMU-CS-91-145, School of Com\u00adintensional \ntype analysis. In 22 Annual ACM Symposium on puter Science, Carnegie Mellon University, Pittsburgh, PA \nPrinciple4 of Programming Languages, pages 130 141, New 15213, May 1991. York, NY, USA, January 1995. \n[30] P.A. Steenkiste and J. Hennessy. Tags and Type Checking in P. Hartel et al. Pseudoknot: a Float-Intensive \nBenchmark for LISP: Hardware and Software Approaches. In Architectural Functional Compilers. Journal \nof Functional Programming, support for programming languages and operafing sysfems, To appear, 1996. \npages 50-59, Palo Alto. CA US, 1987. F. Henglein. Global Tagging Optimization by Type Infer\u00ad [31] M. \nTofte and J-P. Talpin. Implementation of the Typed Call\u00adence. In Conference on Lisp and Functional Programming, \nby-Value A-calculus using a Stack of Regions. In 21sf ACM 1992. SIGPLAN-SIGACT .$ympoaium on principles \nof programming P. Hudak. A semantic model of reference counting and its Languages, pages 188-201, Portland, \nOregon, USA, January abstraction. In Abstract Interpretation of Declarative Lan-1994. guages, pages 45 \n62. Ellis Horwood, 1987. S. Jagannathan and A. Wright. Effective Flow Analysis for Avoiding Run-Time \nChecks. In 2nd Static Analysis Sympo\u00adsium, Lecture Notes on Computer Science, pages 207 224, Glasgow, \nScotland, September 1995. T. Johnson. Lambda Lifting: Ikansforming Programs to Recursive Equations. \nIn Proceeding of the ACM Confer\u00adence on Functional Programming Languages and Computer Architecture, pages \n190-203, 1985. D.A. Kranz. ORBIT: An Optimizing Compiler For Scheme. PhD thesis, Yale university, February \n1988.  X. Leroy. Efficient data representation in polymorphic languages. In P. Deransart and J. Maluszyfiski, \neditors, Int. Symp. on Programming Language Implementation and Logigue Programming, volume 456 of Lecture \nNotes on Com\u00adput er Science. Springer-Verlag, 1990. Also available as IN-  RIA research report 1264. \n   \n\t\t\t", "proc_id": "232627", "abstract": "In this paper we present a new program analysis method which we call <i>Storage Use Analysis</i>. This analysis deduces how objects are used by the program and allows the optimization of their allocation. This analysis can be applied to both statically typed languages (e.g. ML) and latently typed languages (e.g. Scheme). It handles side-effects, higher order functions, separate compilation and does not require CPS transformation. We show the application of our analysis to two important optimizations: stack allocation and unboxing. The first optimization replaces some heap allocations by stack allocations for user and system data storage (e.g. lists, vectors, procedures). The second optimization avoids boxing some objects. This analysis and associated optimitations have been implemented in the Bigloo Scheme/ML compiler. Experimental results show that for many allocation intensive programs we get a significant speedup. In particular, numerically intensive programs are almost 20 times faster because floating point numbers are unboxed and no longer heap allocated.", "authors": [{"name": "Manuel Serrano", "author_profile_id": "81100128092", "affiliation": "Universit&#233; de Montr&#233;al C.P. 6128, succ. centre-ville, Montr&#233;al Canada H3C 3J7 and INRIA B.P. 105, Rocquencourt, 78153 Le Chesnay Cedex, France", "person_id": "PP39083103", "email_address": "", "orcid_id": ""}, {"name": "Marc Feeley", "author_profile_id": "81100041153", "affiliation": "Universit&#233; de Montr&#233;al C.P. 6128, succ. centre-ville, Montr&#233;al Canada H3C 3J7", "person_id": "PP39024470", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/232627.232635", "year": "1996", "article_id": "232635", "conference": "ICFP", "title": "Storage use analysis and its applications", "url": "http://dl.acm.org/citation.cfm?id=232635"}