{"article_publication_date": "06-15-1996", "fulltext": "\n Simplifying subtyping constraints Franqois Pottier Ecole Normale Sup6rieure INRIA* Francois.Pottier@ \ninria. fr Abstract This paper studies type inference for a functional, ML-style language with subtyping, \nand focuses on the issue of simpli\u00ad fying inferred constraint sets. We propose a powerful notion of entailment \nbetween constraint sets, as well as an algo\u00ad rithm to check it, which we prove to be sound. The algo\u00ad \nrithm, although very powerful in practice, is not complete. We also introduce two new typing rules which \nallow simpli\u00ad fying constraint sets. These rules give very good practical results. 1 Introduction The \nconcept of subtyping has been introduced by Cardelli [4] and by Mitchell [9]. It is of great importance \nin many record and object calculi. Subtyping has been extensively studied in the case of explicitly typed \nprograms; ML-style type inference in the presence of subtyping is less under\u00adstood. Fuh and Mishra [7] \nhave studied type inference in the presence of subtyping and polymorphism. However, they consider only \nstructural subtyping, i.e. their subtyping rela\u00adtion ia entirely derived from subtyping on base types. \nMore recently, Aiken and Wimmers [1] have proposed a general al\u00adgorithm for solving systems of subtyping \nconstraints. Their constraint language is rich; in particular, it includes n and U type operators. The \nbasis for our type system has been proposed by Eifrig, Smith and Trifonov [5]. It is baaed on constrained \ntypes, i.e. types of the form 7 I C, where ~ is an ordinary type expression and C is a set of subtypzng \nconstraints of the form ~1 D 7Z, meaning that n must be a subtype of 72. Palsberg [10] has introduced \na similar system. This constraint language is more restricted than Aiken and Wimmer s. It does not have \n1 and T types, nor inter\u00adsection and union operators. However, 1 and T are easily encoded with subtyping \nconstraints. Besides, we introduce notions of greatest lower bounds and least upper bounds of ground \ntypes, and we will use them in a restricted way in constraints. Frangois Pottier, Pro jet Cristal, INRIA \nRocquencourt, BP 105, 78153 Le Chesnay Cedex, France Permission to make digitalhard copy of part or \nall of this work for personal or classroom uss is ranted without fee provided that copies are not made \nor distributed for pm IIt or commercial advantage, the copyright notice, the title of the ublication \nand its date appear, and notice is given that copying is Ey permission of ACM, Ino. To copy otherwise, \nto republish, to ~J:rnaeeers, or to redistribute to Iisls, requirss prior specific permission ICFP 965496 \nPA, USA Q 1996 ACM 0-69791 -771 -0/96/0005 ...$3.50 The type inference algorithm analyzes the program \nand accumulates a set of subtyping constraints. Instead of try\u00ading to solve these constraints, one defines \na notion of consis\u00adtency, and the program is declared well-typed if and only if its constraint set is \nconsistent. We prove that a constraint set is consistent if and only if it haa a solution, so this is \nnot a fundamental departure from other systems, only a difference in presentation. This system is theoretically \ncorrect. However, because each function application node in the program causes a sub\u00adtyping constraint \nto be generated, the size of inferred types ia at best linear in the program size. In fact, it is even \nworse, because constraint sets are duplicated by every use of a let-bound variable. As a result, inferred \ntypes are much too large to be read by humans, and they are unwieldy for the type-checker itself. Hence, \nsimplifying the inferred con\u00adstraints is necessary for the system to be useable at all in practice. We \nprovide formal tools to this end. We strengthen Smith s subtyping rule by introducing a new, powerful \nnotion of entailment between constraint sets. We define it formally and give an algorithm to verify it, \nwhich is unfortunately incomplete but useful in practice. Then, we introduce two new, independent typing \nrules to simrJifv constraint sets. The first one removes so-called un\u00ad .. reachable constraints. The \nsecond one applies substitutions to the constraint set. Both are proved to be sound. These simplification \nmethods have been implemented in a proto\u00adtype type-checker, and give good practical results. 2 An overview \nof constraint-based type-checking Before beginning a formal description of our framework, let us describe \nthe principles of constraint-based type inference. We assume given a functional, ML-like language with \nsub\u00adtyping. During type-checking, a classical ML-style syatem gener\u00adat es equations between types: basically, \nevery time a func\u00adtion is applied, the type of the supplied argument must be equal to the function s \ndomain type. Unification is used as an efficient way to solve the resulting system of equalities. A constraint-based \naystem works in a similar way, except that subtyping constraints are generated instead of equations: \nthe supplied argument must be a subtype of the function s do\u00admain type. How does aubtyping make type \ninference more difficult? In a language without subtyping, when faced with an equation o = -r, one can \nuse unification to equate a to T in a very efficient way. This does not work any more for a 122 constraint \na D r. Fuh and Mishra [7] have studied the problem in the case of structural subtyping, where all type \ninclusions are conse\u00adquences of inclusions between atomic types. Their approach, when encountering a \nconst mint such as a Drl ~ rZ, is to iden\u00adtify a with a function type al -cw, where the a; are fresh \ntype variables. This generates sub-constraints a2 D 72 and ~1 b al. The program is correct if all constraints \ninvolving atomic types are true. In our case. however. the svstem has non-structural sub\u00adtyping. For \ninstance, the re~ord types { fl : al; fz : W} and {fl : al; j3 : CX3} are both subtypes of {~1 : al}, \nyet they have different shapes. Hence, when faced with the con\u00adstraint a D {jl : al}, we do not know \nthe precise structure of the type represented by a, so we cannot identify a with a more precise type. \nType-checkers for ML-style languages use unification to solve systems of equations, that is, find a principal \nmap ping from type variables to ground types which satisfies all constraints. However, replacing equations \nwit h constraints makes resolution much more complex; it is no longer obvi\u00adous that a principal type \nexists, and computing the set of all solutions becomes non-trivial. This is the approach taken by Aiken \n[1]. The other approach, proposed by Smith [5], consists in merely demanding the constraint set to be \nconszs\u00adtenfi writing an algorithm to check consistency is trivial. A subject reduction theorem then guarantees \nsoundness. FoL lowing Smith, we take the second approach. Palsberg [11] has proved that consistency and \nsolvability are equivalent, and we adapt his proof to our system. 3 Examples This section gives examples \nand motivations for the defini\u00adtions we will give later. The reader might wish to have a look at section \n5, which describes the basic type system, if certain notions are unclear. 3.1 A strong notion of entailment \nEntailment between constraint sets is an important notion: the power of the subtyping rule (see figure \n3) depends on it. Cl 1+ C2 is defined in [5] as elements c,f C2 are elements of Cl s closure, or reflexive \nconstraints . This notion is not powerful enough to prove that certain constraint sets are equivalent. \nLet us examine the subtyping rule. Assume we have derived a certain constraint set Cz for an expression, \nand we would like to replace it safely with another set Cl. We need to make sure that if the expression \nis part of a larger program which is invalid, this change will not cause the program to be accepted. \nThe program is invalid if and only if the constraint set D generated for the rest of the program is such \nthat D U CZ is inconsistent. It will be declared valid, after replacing C2 with Cl, if and only if DuC1 \nis consistent. This must not happen. Hence, we define Cl 11-Cz as for any constraint set D such that \nCl U D is consistent, Cz U D iti consistent . This definition seems to be the most powerful possible \none. It can be qualified as observational , because entailment between constraint sets depends on their \nbehavior with respect to the external world D. In section 6, we show that Cl lb Cz is equivalent to the \nstatement every solution of Cl is a solution of Cz , which proves that this definition is indeed natural. \nOur definition of Cl l+ Cz is strictly more powerful than checking whether Cl s closure contains Cz. \nLet us give a few interesting examples. If F is a non-trivial, covariant context, then {abF(a),~(@b~} \nk cYb~ Another typical example is Both statements can be proved by using the definition of entailment. \nHowever, this definition involves a universally quantified constraint set D. This gives it great power, \nbut also makes it difficult to verify automatically. In section 8, we introduce an algorithm, in the \nform of a set of inference rules, to verify entailment wit bout recourse to its definition. Let us hint \n(informally) at how the algorithm copes with these two examples. In the case of the first example, to \nshow that a b ~, it is sufficient to show that F(a) D fl, because of the hypothesis a b ~(cr). Symmetrically, \nit is sufficient to prove ~(a) D F(/3). Since F is a covariant context, after applying a certain number \nof propagation rules, we will realize that we need to show a b P again. This suggests building an inductive \nproof, and declaring success when the original goal is encountered again. In the case of the second example, \none uses a least upper bound, as follows. To show that al * ,Bz is smaller than y, it is sufficient to \nshow that it is smaller than one of ~ s lower bounds, i.e. to prove one of these assertions: But none \nof them holds. Better yet, it is sufficient to prove that it is smaller than the least upper bound of \ny s lower bounds, i.e. Now, because U is distributive with respect to product, this rewrites to al *@2 \nD(cq U/31) *( CY21J/32) which is equivalent to al Dal 1-l~1 and ~Z b CY2 U ~2, both of which are immediate. \nThis notion of least upper bound will be formalized in section 7. Note that we need to allow u symbols \non the rightl side of constraints, so defining leaat upper bounds by declaring al U uz b T is equivalent \nto al D r A uz D T would not be of any help here. Both of these examples are of practical interest: they \nap\u00adpear while simplifying the types of various classic functions. 3.2 Removing unreachable constraints \nConsider the following expression: let Y=fun f -> (fun g -> fun x->f (gg)x) (fun g->fun x->f(gg)x)in \nlet compute = Y (fun f -> fun x -> plus 1 (f x)) in compute 1 1 More precisely, in contravariant positions. \n Here Y is the classic fix-point combinator, and plus is a 3.4 Practical results primitive addition function. \nTherefore, compute is a func- Our simplification methods have been implemented into ation which takes \nan integer and returns an integer (it is of no small type-checker. It gives very good results on classic \nex\u00adsignificance here that the computation does not terminate). amples. For instance, we have coded the \nclassic quicksortNow, the type obtained for this expression is int together function on lists; we omit \nthe code here for brevity. Its un\u00adwith the constraints simplified type contains 300 constraints, only \n60 of which are reachable; after simplification, the type is trimmed down to lIW bT The inference algorithm \nindeed reports that the expression has type int; but it also produces a series of inclusion con\u00adstraints, \nmaking the result unexpectedly complex. Assume that this expression is part of a larger program. Other \nconstraints will be generated for the other parts of the program; then the constraint set will be closed \nand checked for consistency. However, the new constraints cannot pos\u00adsibly involve any of the type variables \nabove, because these variables are unreachable: they are not part of the result type of the expression, \nnor are they part of the environment the expression was typed in. Hence, these constraints will not affect \nthe consistency of the whole program; we might as well discard them now. Section 9 formalizes the concept \nof unreachable type vari\u00adables and shows that constraints involving those variables can be safely removed \nfrom the constraint set. Because the use of let constructs leads to duplication of constraint sets, removing \nunreachable constraints early is vital,  3.3 Simplifying constraint sets We introduce a substitution \nrule to simplify constraint sets. Let us explain the idea. Assume we have written a function f from integers \nto integers. The type inference system might assign it type When we apply f to an argument of type u, \nthe function ap plication rule will declare the result to be of type ~, together with constraints UDO!Dlnt \nintb~br Clearly, variables a and ,B add no information to this con\u00adstraint set. So we might just as well \nget rid of them imme\u00addiately, and declare that f has type int ~ int We have applied substitution [int/a, \nint/~] to the original type. While doing type inference, we must be careful to choose substitutions which \nare not restrictive; otherwise, they will yield a less general type, possibly causing us to reject a \ncorrect program later. To make sure that the new type is as expressive as the previous one, we will require \nthat it be possible to cancel the effect of the substitution by applying the subtyping rule. That is, \nbefore applying substitution ~ to constrained type T I C, we will require that C II-Y(C) and C IFY(7) \nbT. together with the constraints This is identical to the usual ML type of quicksort, ex\u00adcept that the \nfirst element of the argument list has type ~, whereas the remaining ones have type a. This is bewilder\u00ading \nat first; it seems that a list can only be sorted if all of its elements have the same type. However, \na closer look reveals that this type is correct, and slightly more general than the ML type. This difference \ncomes from the fact that our ver\u00adsion of quicksort always uses the jirst element of the list aa a pivot. \nThis is of no practical interest, of course, but it is amus\u00ading to see that the inferred type is, unexpectedly, \nmore gen\u00aderal than the usual ML type. If we restrict the inferred type by applying substitution [cY//3, \n-y/c$], we obtain the ML type as a special case. Note that even then the function is more powerful than \nits ML counterpart, since it can sort hetero\u00adgeneous lists, provided that each element s type is a subtype \nof the comparison function s domain type. The time needed to infer and simplify quicksort s type was \nslightly under 10 seconds. The prototype typechecker was compiled to native code with Carol Special Light, \nso the performance still leaves to be desired. However, the proto\u00adtype uses naive data structures to \nrepresent constraints, so substantial speed improvements should be possible. 4 The expression language \nThe expression language is given by figure 1. While the concrete syntax of our language has constructs \nfor building and accessing pairs, records and variants, they are not necessary in the theoretical definition \nof the lan\u00adguage. Instead, they can be regarded as an infinite collection of primitive functions. Instead \nof type-checking programs in an empty environ\u00adment, we will always assume a basic environment containing \nall of these primitives. This way, there is no need to intro\u00adduce the primitives as constants; they are \nhandled exactly in the same way as variables. The advantage of introducing such constructs through primitive \nfunctions is to reduce the number of typing rules, thus shortening proofs and making it obvious that \nthe typ\u00ading rules for pairs, records and variants are derived from the function application rule (together \nwith the type of a primi\u00adtive function). Notice that the expression language only haa four constructs: \nit is a A-calculus with let. In order to simplify reasoning on records and variants, we assume given \na collection of label fields ~~, as well as a collection of constructors K~, where i ranges over natural \nintegers. Note that pairs and variants do not add any essential complexity to the system with records; \nthey have been added to allow writing classic examples, such as sorting lists. Clas\u00adsic examples based \non records are less cclmmon, and often involve contrived pseudo-object-oriented message passing. 5 The \ntype system 5.1 The type language The type language is defined by figure 2. We denote the set of all \ntype terms by T. 5.2 The type inference rules Type inference rules are given by figure 3. Judgments \nare of the form A F e : ~ I C, where A io an environment, e is the expression to type-check, and ~ I \nC is the inferred constrained type. All constraint sets appearing in judgments are implic\u00aditly required \nto be consistent, aa defined below. If a type inference rule yields an inconsistent set, its use is invalid. \nThe rules are the four classic typing rules for variable instantiation, A-abstraction, application, and \nlet binding, plus a subtyping rule. The latter is based on our entailment relation (see section 5.5) \nand is more powerful than the one introduced in [5], which is based on set ccmtainment. Using properties \nof the entailment relation, one verifies that it is possible to rewrite any typing proof so that it uses \nthe subtyping rule at most once at the bcttom. As a result, we have Theorem 5.1 The type system has \nsubject reduction. Pr-oo-f: This proof is based on Smith s proof of subject re\u00adduction. Assume A 1-e \n: ~ I C. Then there exists a typing proof which uses the subtyping rule at most once at the end. Thus, \ne is typable without using the subt,yping rule (hence, in Smith s system) which yields a certain type \nTO I CO. So, if e reduces to e , then according to Smith s subject reduc\u00adtion property, e also has type \nTO I CO in Smith s system. Because Smith s subtyping rule is weaker than ours, e also has type TO I CO \nwithin our system; and. adding back the final subtyping rule shows that A Fe : ~ I C holds.O 5.3 Closure \nof a constraint set A constraint set C is closed if and only if it is closed by transitivity and by propagation. \nC is closed by transitivity if and only if where a is a type variable3. C is closed by propagation if \nand only if it verifies the following conditions: 2 Although we have a larger set of primitive functions, \nwhose se\u00admantics should be defined and checked, and we have introduced vari\u00adant types. A direct proof \ncould also be easily written. 30ne usually allows any type T2 instead of only type variables. We have \nshown the two definitions to be equivalent. In particular, although this modification alters the definition \nof a set s closure, it does not change the consistency of its closure. {fi:Ui}icl b{ fj:7j}jcJEC*Vk CIn \nJ akbrk~c The closure of C, denoted C-, is the smallest closed constraint set containing C. 5.4 Consistency \nof a constraint set A constraint set is consistent if and only if all constraints in its closure are \nconsistent. A constraint m b TZ is consistent if and only if one of the following conditions holds: e \n~1 or 7-2 is a type variable 71 and 72 are the same atomic type e 71 and rz are function types  T1 \nand 72 are product types   5.5 The entailment relation Let Cl and CZ be coercion sets. We say that \nCl entails CZ (and we write Cl It-CZ) if and only if for any coercion set D such that Cl U D is consistent, \nCZ U D is also consistent. This relation behaves like an implication, as demonstra\u00adted by the following \nproperties: C1l-C1and ClFC2 ~ CIFCIUCZ Ail-Band Bll-C ~ All-C C1lh Canal C1~C2 ~ C21+ c If ~ is a substitution, \n  c, IE C2* ~(c,) IE7(C2) 6 Consistency versus solvability After accumulating subtyping constraints, \none has a choice between trying to solve them, as does Aiken [1], or merely verifying that they are consistent, \nas suggested by Smith [5]. In the former case, the existence of a solution guarantees that the program \nhas a valid monomorphic type, and its soundness can be established from there; in the latter case, the \nprogram s soundness is guaranteed by a subject reduc\u00adtion theorem. Palsberg [11] has established that \nboth approaches are equivalent, in a system similar to ours, and we adapt his proof to our case. Then, \nwe further show that our notion of entailment, which is based on consistency, is equivalent to a natural \nnotion of entailment based on solutions of constraint sets. Hence, it appears that both approaches are \nentirely equiva\u00adlent, and many questions about constraint sets can be for\u00admulated in both manners. Expressions: \n.. e ., I ;x.e lee I let z Figure Types: T ::= o! la IT47 IT*7 I {ii : G}ier I [K; of Ti]i~l Constraints: \nc::= rbr Type schemes: 6 ::=E.T I c Figure A(z) A1-el:711Cl = VZI.r I C p is At-z: A1-ez:T2]C2 a substitution \nyY(~l C) AFe:~l Figure variable or constant function function application = e in e polymorphic let \n1. The expression language type variable atomic type: bool, int, etc. function type product type record \ntype variant type a quantified (type, constraint set) pair 2. The type language of domain z A;z:7Fe:7 \nlC A1-Xc.e:7e/lC Ai-el:Tll C1 A;x:V?Y.~llCll-e2:~zlCz z = FV(rl I Cl ) \\ FV[A) v is a substitution of \ndomain Z Ai-letz=elinez:~zlq(Cl)UCz C c 1-c C k7DT A1-e:#l C 3. Type inference rules 6.1 Definition \nof solvability 7.1 On ground terms We need to give a few definitions before we can introduce the notion \nof solvability. These definitions are essentially taken from [11], with a few adjustments to accommodate \nour richer type language, which in particular adds record types. Ground types are defined as regular \ntrees and admit a least type 1 and a greatest type T. They are ordered by a sub\u00adtyping relation adapted \nfrom Palsberg s [11], the latter being equivalent to Amadio and Cardelli s [3]. These definitions are \nstraightforward and given in ap\u00adpendix A. 6.2 Consistency versus solvability y Theorem 6.1 A constraint \nset is consistent if and only if it is solvable. 6.3 Syntactic versus semantic entailment We have proved \nthat the notion of consistency can be re\u00adgarded as a more algorithmic definition of solvability, which \nis a more semantic notion. Now, we would like to com\u00adpare our syntactic notion of entailment, which is \nbased on consistency, with a semantic notion of entailment baaed on solvability. The natural way of defining \na semantic notion of en\u00adtailment between two constraint sets Cl and C2 (denoted CI ~ CZ) would be to \nrequire that every solution of Cl be a solution of C2. It is actually slightly more complex than that, \nbecause C2 might have more free variables than Cl. For instance, the statement {a Dint, aDbool}~a D~ \n should hold because the principal soluticm of the left-hand set is a w L, and 1 D@will hold for all \n/?. However, a * 1 is not, strictly speaking, a solution of the right-hand set because it doesn t assign \nany value to lY. Obviously, free variables such as /3 should be universally quantified. We achieve this \neffect by giving the following definition: Definition 6.1 Let Cl and Cz be two constraint sets, We define \nCl + C2 by Here, @ ranges over maps from type variables to ground types and the quantification over + \nis, in effect, a universal quantification over the variables in FV(C .Z) \\ FV(C1 ). We can now make the \nfollowing statement: Theorem 6.2 Let Cl and Cz be two constraint sets. Then  Cllk-czecl+cz 7 Introducing \nlub s and glb s As explained in section 3, the entailment algorithm will need notions of greatest lower \nbounds and least upper bounds of sets of types (sometimes referred to as glb s and lub s for short), \nwhich we define now. Theorem 7.1 The set TX of ground terms, ordered by re\u00adlation <, forms a lattice. \nThat is, there exist operations u and n which, when applied to a set of ground terms, yield its least \nupper bound and its greatest lower bound, respectively. They are commutative and associative. There is \nnot enough room hereto give the explicit definition of u and il. It is very straightforward and yields \na series of identities which can be used to compute least upper bounds and greatest upper bounds. Proposition \n7.1 For any s, s , t, t c TX, these equations hold: tuT =T tul = t (s+s )u(t-+ t ) = (snt)+(s ut ) (s*s \n)u(c*t ) = (sut)*(s ut ) For the sake of brevity, we do not give all of them here. Note that in particular, \nu and n are distributive over type constructors. 7.2 On types In order to be able to use glb s and lub \ns in constraints, we make the following definition: Definition 7.1 A generalized type is a type term \npossibly containing occurrences of the u and n constructors, as well as of 1 and T, as follows: Generalized \ntypes: T .. ., .,, I IJ{7-*}KI I fl{Ti}icI IL IT Generalized types are considered modulo the equations \ngiven in proposition 7.14. A generalized constraint is a subtyping constraint involv\u00ading generalized \ntypes. This time, we have introduced U, fl, 1 and T as con\u00adstructors into the syntax of our constraint \nlanguage. We give them a semantics by defining how they are instanti\u00adated: Definition 7.2 For any mapping \np from type variables to ground terms, p is extended to generalized type terms as a homomorphism, that \nis @J{Ti}) = U{~(T;)} p(n{~i}) = n{p(~i)} p(l) =1 P(T) =T Two generalized types which are equal up to \nthe equa\u00adtions given in proposition 7.1 are mapped to the same ground term by p, so this definition does \nmake sense. This is enough to extend the notions of solution, solvabil\u00adity and semantic entailment to \ngeneralized constraint sets. 4That is, these equations can be freely used inside a term. There is a slight \nabuse of language, as they originally applied to ground terms, and they are used for generalized types \nhere. 8 An algorithm to verify entailment 8.1 Description We now give an algorithm to verify the entailment \nrelation; it is defined by a set of inference rules given in figure 4. Let us comment on these rules. \nJudgments inferred by the algorithm are of the form C, HI, HO t-TI DT2 where C is a closed constraint \nset, HI and HO are generalized constraint sets, and -n Drz is a generalized constraint, which we call \nthe goal. Each rule replaces the current goal with zero or more sub-goals. In goals, u and n symbols \nalways occur in contravari\u00adant and covariant positions, respectively. That is, each rule produces sub-goals \nwhich verify this property, provided that the original goal does. TIPT2EC TlbrzEHl (1) (2) C, HI, HO*T1DT2 \nC, H1, HO FTIP T2 G , H1, HOt-uila DaLJr (3) R={p[~Dp EC Ap#~} c, Hl, Hou{an LYDT}t-arl(nR)b T (5) C,Hl,HOk~rla~r \nC, H1, HO EODT (6) C, Hl, HO1-l D~ (7) C, HI, HO ECZDa (8) C, HIUHo, Ol-rbu CjHIUHo, Ok U b T (9) C, \nH1, HO FU*U PT+T C, H,u HO, OF UPr C7, HIUHo,01-G D# (10) C, H1, HO EO*U PT *T Figure 4. Algorithm definition \nOne of the most interesting features of this algorithm is its ability to reason by induction. The algorithm \nuses HI and Ho as a trace, which contains the goals which have been encountered so far. More precisely, \nif a goal has been encountered and a propagation rule has been used since, then it belongs to HI; if \nno propagation rule has been used since, the goal belongs to Ho. The trace is initially empty, i.e. we \nwill be interested in proving statements of the form G,0,0~TlbT2. Now, in addition to accepting elements \nof C as hypothe\u00adses (rule (1) ), the algorithm also regards elements of HI as true, thanks to rule (2). \nThis is reasoning by induction: the algorithm realizes that this goal has already been en\u00adcountered before, \nso continuing to build a proof for it would cause an endless loop. Proving the goal can be thought of \nas comparing two infinite trees. However, because at least one propagation rule has been used since the \ngoal was last encountered, we know that part of the trees have been suc\u00adcessfully compared. Hence, by \ncarrying on the comparison, we would be able to verify that the two trees can be success\u00adfully compared \ndown to an arbitrary depth. So, it is valid to declare that the goal holds. The algorithm owes a lot \nof its power, and of its complexity, to this induction rule. The reader might wonder why G and HI are \nkept sepa\u00adrate, since constraints found in both sets are considered as true by the algorithm. One reason \nis that C contains hy\u00adpotheses (i.e. regular constraints), while H1 contains goals (i.e. generalized \nconstraints), and merging the two would lead us to generate malformed goals (e.g. with gll$s in con\u00adtravariant \npositions), which we cannot handle. Let us now comment on the remaining rules. Which one is to be used \ndepends on the structure of the goal. Distribu\u00adtivity of I-I and n over type constructors is used whenever \npossible to push down lub s and glb s as far as possible; they remain at top level only when one of their \narguments is a variable, or when two arguments have incompatible head constructors. The reflexivity rule \n(3) states that a goal holds if a single type variable appears on both sides of it. Next come the variable \nelimination rules, (4) and (5), which are symmetrical. Let us consider rule (4), which can be used when \nthe lub on the right side of the goal contains a type variable a. To prove that the left side of the \ngoal, a, is smaller than its right side, it suffices to show that it is smaller than a. To do this, it \nwould be sufficient to find a type p such that p b CY E C and prove that a is smaller than p. However, \nthis is too restrictive; instead, it is still sufficient (and easier) to prove that a is smaller than \nU R, where R is the set of a115 lower bounds of a in C. The rules discussed so far deal with goals which \nhave at least one type variable at top level. The remaining rules deal with the cases where all types \nappearing at top level are constructed types. Rules (6) and (7) are symmetrical. T and 1 appear in goals \nwhen computing the lab or glb of types with incom\u00adpatible head constructors, such as int U bool. The \nremaining rules deal with goals where both sides have the same head constructors. These goals can be \nde\u00adcomposed into smaller sub-goals, so these rules are collec\u00adtively called propagation rules. There \nis one per construct: atoms, arrows, pairs, records and variants. Rule (8) is the propagation rule for \natomic types; we do not allow subtyping between base types, so this rule has the look of a reflexivity \nrule. The other propagation rules are similar to the propaga\u00adtion rules used when computing the closure \nof a constraint set, only they work in the opposite way. 5 except a itself, otherwise the algorithm would \nnot terminate.  8.2 Termination 9 Removing unreachable constraints To make sure that our rules actually \ndefine an algorithm, we need to verify that given a goal, determining whether it is provable is a finite \nprocess. A closed coercion set C contains a cycle if and only if there exist n type variables (n. ~ 2) \nal . . . an such that {al b @! D.. .Dan Dcll}~c. Theorem 8.1 Let C be a closed coercion set without cycles. \nThen checking whether there exists a proof of C, 0,01-rl DTZ fails or succeeds in finite time. Hence, \nthe algorithm works only on coercion sets without cycles. This is not a problem, because we will prove \n(using the substitution rule) that any cycle can be eliminated by identifying all of its variables6. \n 8.3 Correctness The algorithm is sound with respect to entailment: Theorem 8.2 Let C be a constraint \nset and rl, TZ be type terms. Then A sketch of the proof is given in appendix B. 8.4 Completeness Although \nwe have long believed the algorithm to be com\u00adplete with respect to the definition of entailment, we \nhave recently found evidence to the contrary. Since we have shown the equivalence between solvabil\u00adity \nand consistence (see 6), we present the counter-example from the point of view of solvability. It is \nslightly simpler. TakeC={a~ int Da} and~=(1+ T)~ int. Then we have cik rbff but the following assertion \ndoes not hold: This means that rule (4) of the algorithm (and, symmet\u00adrically, rule (5)) is not complete, \nsince it would replace the goal ~ D a with the sub-goal ~ D a + int. These are the only two incomplete \nrules for all other rules, we have shown that the premises hold if and only if the conclusion holds. \nWe have not yet grasped the full significance of this counter-example, ancl we do not know how wide a \nrange of counter-examples could be produced. It should be emphasized that even though the algorithm is \nnot complete, it remains powerful and useful in practice; the 60 reachable coercions in quicksort s type \nare reduced to 3 meaningful ones. 6Alternative1y, we could modify rules (3), (4) and (5) so that the \nalgorithm still works with sets containing cycle!l. 7AC~U~lly, L ~~dT a,re not allowed in types, so one \nshould define -r = (~ + ~) + int and add suitable constraints to C so i,hat all solutions of C assign \n1 to /3 and T to 7. We omit it for the sake of simplicity. Consider a typing judgement A 1-e : ~ I C. \nThe set of reach\u00adable variables of this judgement, written as RV(A, r, C), is the smallest set V of type \nvariables such that V > FV(A) U FV(~) A constraint belonging to C s closure is said to be reach\u00adable \nif it contains only reachable variables. That is, the set RC (A, T, C) of reachable constraints is defined \nas We now prove a few lemmas which will be used in forth\u00adcoming proofs. We then introduce the following \nrule, called the connezity rule, which allows removing all unreachable constraints: A+e:~lC C = RC(A, \n~,C) AEe:~l C From now on, this rule is part of the typing rule and the statement A 1-e : 7 I C represents \na derivation which is allowed to make use of this rule. The following proposition will guarantee the \nsoundness of the new rule (see section 11): Proposition 9.1 The connexity rule commutes towards the bottom \nwith the basic typing rules (i. e. all but the subtyping rule), possibly demanding that hidden variables \nabove it be renamed, and yielding a new constraint set which is a subset of the original set s closure. \n10 Simplifying constraints We prove the following substitution lemma: Lemma 10.1 If A k e : T I C and \n-y is a substitution such that 7(C) is consistent, then 7(A) !-e : -y(T) I Y(C). The proof is very similar \nto that of ML s substitution lemma. Only the case of the subtyping rule is new. Thus, the type system \nis not affected by the addition of the following rule: However, during type inference, this rule must \nnot be ap\u00adplied blindly, because it restricts the generdlty of the type and can eventually lead to rejecting \na valid program. Since we do not wish to have the type inference algorithm back\u00adtrack, we must ensure \nthat it always applies the rule in such a way that the new typing is as general as the previous one. \nWe do this by requiring that the old judgement be a conse\u00adquence of the new one through the subtyping \nrule. That is, we will use the substitution rule only as follows: The extra hypotheses ensure that the \neffect of a sub\u00adstitution rule can always be cancelled by a subt yping rule. Consider a typing derivation. \nAt any point in this deriva\u00adtion, we can insert a substitution rule, immediately followed by a subtyping \nrule which cancels it. Now, the subtyping rule can be pushed towards the bottom of the proof, and eventually \ndiscarded, yielding a proof which has the same skeleton as the original proof, except a substitution \nrule has been added to it. This shows that the inference algorithm can use the substitution rule at any \npoint without fear or failing in the future. 11 Correctness of the new typing rules Because the substitution \nrule is derived from the other rules, and because the cormexity rule and the subtyping rule com\u00admute \nwith the four basic typing rules, adding our two new rules to the type system does not affect the set \nof typable programs. What s more, Theorem 11.1 The extended type system has subject re\u00adduction. Proof: \n(Sketch) Consider a typing judgement A + e : r I C. Since the substitution rule is derived from other \nrules, there exists a proof of this judgement which doesn t use the substitution rule. Since the connexity \nrule, as well as the subtyping rule, commutes with the four basic ruless, the proof can be rewritten \nso as to use these two rules only at the bottom of the proof, and we have a typing A + e : # I C which \nuses only the four basic rules. Now, assume expression e reduces into e . Because the original typing \nsystem has subject reduction, there exists a proof of A 1-e : # I C in the original system (consisting \nof the five rules in figure 3). Now, the same sequence of rules which was used to derive A1-e:~/Cfrom \nA\\e : -r I C can be used to derive A>e :~lC.O 12 Choosing adequate substitutions Given a typing judgement \nand a substitution -y, we have de\u00adfined how to formally verify whether ~ can be safely applied to this \njudgement. Now, a practical simplification algorithm needs to be able to determine which substitutions \nare likely to succeed on a given constraint set. This is important both for speed and for effectiveness \nof the simplification process. Here, we have only heuristics. We give a few of them below. More study \nis needed in this area, as we have no formal results. 12.1 Basic heuristics One heuristic is to look \nfor cycles and eliminate them by identifying all variables within a cycle. Another idea is to look only \nfor substitutions which af\u00adfect one variable at a time (although this is not sufficient, see 12. 3). \nConsider a variable a. Two likely candidates for its replacement are the least upper bound of a s lower \nbounds in C, and, symmetrically, the greatest lower bound The connexity rule does not exactly commute: \nwhen pushing it towards the bottom, it yields a constraint set smaller than the original set. This is \nnot a problem this change has to be propagated down, and it obviously can: since there are fewer constraints \nthan there used to be, no new inconsistencies can appear. of its upper bounds. For instance, if C contains \nthe con\u00adstraints a D{~I : n } and CYB{lz : 72}, trying to replace a with {11 : 71; 12 : 72} is probably \na good idea. However, the terms of least upper bound and (greatest lower bound used above are only informal; \nunlike Aiken [1], we have no such notions when the types involved contain variables. When our heuristic \nencounters type variables, for instance when computing a u ~, it tries both choices, a and ~, one after \nthe other. 12.2 Turning recursive constraints into equations We find it desirable, whenever possible, \nto replace recursive constraints with equations, because a type variable involved in a recursive equation \nis easily understood as a fixpoint, whereas the effect of recursive constraints is less clear. We have \nfound that this is possible in many cases. For instance, if we write a function which computes the length \nof a list, the following type might be inferred for it: [Nil ICons of a */3]-+ int I{/?D[Nil ICons of \na* ~]} Here, the type variable ~, which obviously represents lists of elements of type a, is bound by \na recursive constraint, instead of an equation. Using the subtyping rule, we can introduce a new equa\u00adtion \ninto the constraint set ~ = [Nil 1Cons of a * -y] where -y is a fresh variableg. We can now apply the \nsubstitution [y//7]. To verify that the premises of the substitution rule are met, we need to show ,6 \nDINill Cons of a*~], -y=[Nil] Cons of a*-y]l+pDv which is immediate using the entailment algorithm. Hence, \nour function has type [Nil I Cons of a *~] -+ int I {~= [Nil/ Cons of a * ~]} and we have effectively \nreplaced the constraint with an equa\u00adtion. 12.3 One variable at a time is not enough The suggestion to \ntry only substitutions whose domain is a singleton, although efficient in most cases, is too restrictive. \nFor instance, suppose we have inferred type j3 I C for a certain expression e, where C= {~= [Nil I Cons \nof cI*~], int Da} This intuitively represents a list of integers, so we would like to apply substitution \n[int/a]. However, the substitution rule requires us to prove that C 1+,6 = [Nil I cons of int * ,8] which \nis false. In fact, the variable ,6 has been introduced only to rep\u00adresent a recursive type. If the type \nlanguage had p-binders, we could write the type of e as p,6. [Nil I Cons of a * ,L?]I {int b a} This \ncan be done without fear of making the inference algorithm fail, because tbe new constraints are unreachable \nand could be re. moved by the connexity rule. which the substitution [int /a] would turn into @.[Nill \nCons of int *6] Here, @represents a different fixpoint. Thissuggestst hat, in the language without p-binders, \nthe substitution should also affect ~. Indeed, we can add a fresh variable v together with the equation \ny = [Nil I Cons of int *~] in the same way as in 12.2, and then apply substitution [int/a, ~//3]. This \ntime the conditions of the substitution rule are met and we end up with type Y I {v= [Nil I Cons of int \n* 7]} as expected. We have implemented a heuristic which looks for vari\u00adables bound by recursive equations, \nsuch as ~, and extends the scope of the substitution as described above. It is neces\u00adsary in practice, \nfor example when simplifying quicksort s type. 13 Related work Amadio and Cardelli [3] have defined an \nalgorithm to check whether two types are in the subtype relation. This al\u00adgorithm uses a set of subtyping \nhypotheses similar to our constraint sets, except that there is only one bound per type variable. Our \nalgorithm presents interesting similar\u00adities wit h theirs. Their rule for dealing with recursive types, \nPA, reads This is similar to our variable elimination rules: variables are replaced with their bounds \nand the current goal is added to the execution trace. Their requirement that types should in be in canonical \nform, i.e. the body of a Ii should not be an\u00adother p, corresponds to our requirement that the constraint \nset should contain no cycles. Their algcmit hm is complete, whereas ours isn t. This is because their \ninitial set of as\u00adsumptions only has equations, while ours cent ains inclusion constraints; hence replacing \na variable with its bound poses a problem. Fuh and Mishra [7] have described several methods for simplifying \nconstraint sets. Each of them is subsumed by our substitution rule. The conditions for each of their \nmet h\u00adods are sufficient to verify that our substitution rule applies; however, they do not deal with \nthe more complex cases han\u00addled by our rule. Aiken [2] has built a constraint simplification algorithm \ninto Illyria, a Lisp implementation of his constraint solver. Aiken s type language has intersection \nand union types, so each type variable has at most one upper bound and one lower bound. To simplify constraint \nsets, variables which occur only in covariant (resp. contravariant) positions are identified with their \nupper (resp. lower) bound. This is a special case of our substitution rule, provided that the variable \ns bound can be expressed in c,ur more restricted type language. Aiken does not seem to handle variables \nwith mixed variance; our substitution rule does (and such cases are easily encountered in practice). \nJones [8] has established a general framework for infer\u00adring types together with sets of constraints, \nwithout speci\u00adfying the semantics of constraints. Our system is a special case of his. Namelv. his so-called \nsimdification rule is a special case of our subtyping rule (Jon~s requires that the constraint sets be \nequivalent in order to ensure complete\u00adness of the type inference algorithm, otherwise the two rules \nwould be identical), and our substitution rule is a special case of his improvement rule. Indeed, his \ndefinition of ~ improves C is (informally) equivalent to our requirement that C Ii-Y(C). Of course, Jones \nframework is very abstract and does not tackle the issues of verifying the IF relation and of findirm \nadequate substitutions -i. Smit~ [6] h-as defined a rule to remove unreachable con\u00adstraints which is \nvery similar to ours, as well as a series of substitution rules (replacing a variable by its bound, merg\u00ading \nvariables). His method for removing excess constraints is more powerful than ours, because it tests whether \nvariables occur in covariant or contravariant positions, whereas ours only checks whether they appear \nat all. For instance, given an expression of type a I { lnt D a, a D lnt }, Smith can re\u00admove the second \nconstraint, whereas we cannot 10. However, this does not seem to be a limitation in .mactice. because \nactual constraint sets correspond to a program s data flow, and variables tend to appear in natural positions, \nso that Smith s rule has few extra possibilities aa compared to ours and these cases are usually solved \nanyway by our sim\u00adplification rule. Of course, this statement is only informal. Smith s substitution \nrules. on the other hand. are restricted cases of ours (they handle only variables with single variance \nand with a single bound). 14 Conclusion We have evidenced that the constraint-based type inference system \nintroduced in [5], although theoretically correct, de\u00adpends on type simplification in order to be useable \nin prac\u00adtice. Smith has introduced some simplification rules [6]; we have shown that most of them can \nbe understood as re\u00adstricted cases of a general substitution rule. Smith s reacha\u00adbility rule is similar \nto our connexity rule, and slightly more ~owerful; thev were developed independently. We have defined \na new ~otion o~ entailm~nt, which gives great power to the subtyping rule, Our substitution rule makes \nuse of entailment, so we have introduced an algo\u00adrit hm to verify entailment and proven its soundness. \nThe algorithm, although incomplete, seems to be powerful and gives good results in actual use. Building \non work by Palsberg [11], we have related con\u00adsistency to solvability and proved that our notion of entail\u00adment \nis equivalent to a more natural, semantic one. Several directions for future work can be followed. First, \ndetermining whether it is possible to strengthen the algo\u00adrithm so as to achieve completeness with respect \nto entail\u00adment is desirable. Then, as mentioned in section 12, comes the issue of choosing adequate substitutions, \nwhich has not been formally investigated yet. It might desirable to find a better formulation for the \ntyp\u00ading rules. For inst ante, we often had the intuition that un\u00adreachable variables should be existentially \nquantified, thus easing some thorny renaming problems. Also, it might be possible to enhance the generalization \nrule. Currently it en\u00adters the whole constraint set into the environment and each instantiation rule \nduplicates the constraints; ideally, only 10FJxtending our ~onnexitY rule after Smith might be possible \nbut is not trivial it breaks our proof of the substitution lemma. constraints which have an effect \non the generalized variables should be duplicated and instantiated. Finally, a desirable goal is to obtain \nprincipal types. Yet, even if one achieves it, the notion of simplest type remains subjective, because \nthere seems to be no way to prove that no other type inference system will yield a %impler7~ type for \na given program. Attaining these goals would allow a better understanding of subt yping constraints. \nThe results obtained so far, both theoretical and practical, are promising and encourage us to continue \ninvestigating this field. References [1] Alexander Aiken and Edward L. Wimrners. Type in. elusion constraints \nand type inference. In Conference on Functional Programming Languages and Computer Architecture, pages \n31 41. ACM press, 1993. [2] Alexexander Aiken. Illyria system, 1994. Available on\u00ad line as http: //http. \ncs. berkeley. edu: 80/ aikeniftp /Illyria. tar. gz. [3] Roberto M, Amadio and Luca Cardelli. Subtyping \nre\u00ad cursive types. In Proceedings of the Eighteenth ACM Symposium on Principles of Programming Languages, \npages 104 1 18, Orlando, FL, January 1991. Also avail\u00adable as DEC Systems Research Center Research Report \nnumber 62, August 1990. [4] Luca Cardelli. A semantics of multiple inheritance. In Semantics of Data \nTypes, volume 173 of Lecture Notes in Computer Science, pages 51 68. Springer Verlag, 1984. Also in Information \nand Computation, 1988. [5] J. Eifrig, S. Smith, and V. Trifonov. Type inference for recursively constrained \ntypes and its application to OOP. In Mathematical Foundations of Programming Semantics, New Orleans, \nvolume 1 of Electronic Notes in Theoretical Computer Science. Elsevier, 1995. To aPPear. Currently available \nas f tp: //f tp. cs. jhu. edu /publscottlooin fer. ps. Z. [6] Jonathan Eifrig, Scott Smith, and Valery \nTrifonov. Sound polymorphic type inference for objects. In OOP-SLA 95, 1995. Available as f tp: //f tp. \ncs. jhu. edu Ipublscottlsptio .ps. Z. [7] You-Chin Fuh and Prateek Mishra. Polymorphic sub\u00ad type inference: \nClosing the theory-practice gap. In TAPSOFT 89, 1989. [8] ,Mark Jones. Simplifyin~ andimproving qualified \ntypes. Technical Repor/YALE U/DCS ~RR-1040, Yale Uni~er\u00adsity, New Ha~en, Connecticut, USA, June 1994. \n[9] John C. Mitchell. Coercion and type inference. In Eleuenth Annual Symposium on Principles Of Program\u00adming \nLanguages, 1984. [10] Jens Palsberg. Efficient type inference ofobject types. In Ninth Annual IEEE Symposium \non Logic in Com\u00adputer Science, pages 186 195, Paris, France, July 1994. IEEE Computer Society Press. \nTo appear in Informa\u00adtion and Computation. [11] Jens Palsberg and Patrick O Keefe, A type system equivalent \nto flow analysis. To appear in Proc. POPL 95. Currently available as f tp: //ftp. dairni. aau. dld pub/palsberg/paperslpop195.ps.Z, \n1995. A. Definition of solvability  Definition A.l Let E bea ranked alphabet composed of: J_, T with \nnull arity  Atomic types (int, bool, etc.) with null arity  ~, * with artty 2  {}1 with aritv I ~ \n1, for each set of labels I  [ ]1 with arlty I ~ 1, for each set of labels I  A ground type is a regular \ntree over X. A path from the root of such a tree is a string over P = {d, r, f, s} U N, where d and r \nstand for domain and (range respectively, f ands stand for ~rst and second , and a natural integer indicates \na record or variant field number. Definition A.2 A ground type is represented by a ground term, that \nis, a partial function from P to X which maps each path to the symbol at the end of that path. The set \nof all such terms is denoted TE. Ground types are ordered by the subtype relation <, as follows. Definition \nA.3 The parity of p ~ P , denoted np, is the number of d s in p, taken modulo .2. Let <O be the partial \norder on X given by Vac X L<oa<o T l~.J+{}J<o{}l ~s~*[]J<O[]I and let <1 be its reverse VCCZ T<la<ll \n~g~+{}J<l{}I ~~~+[]J<l[]I Two trees s, t TX are in the subtype relation s ~ tif and only if Vp E Dom(s) \nrl Dom(t) S(P) <=, t(P) We can now define the notion of solvability: Definition A.4 Let C be a constraint \nset. Let p be a map from FV(C ) to TX. p is said to be a solution of C if and only if +1 PT2 c c q(n) \ns Y4~2) The set of all solutions of C is denoted S(C). C is solvable if and only if S(C) # 0. B Correct \nnees oft he entailment algorithm This appendix contains a sketch of the proof of correctness for our \nentailment algorithm. Reading it is not necessary to gain an understanding of the paper. B. 1 Weaker \nnotions of solvability y and entailment Le k > 0. We define a new ordering relation on ground terms, \ndenoted ~~, by saying that s ~k tif and only if Vp E Dom(s) n Dom(t) [p 1< k = s(p) <=, t(p) s k is \na weaker version of < which compares trees only up to depth k. Now, by replacing < with ~~ in the definitions \nof S and ~, we obtain weaker notions of solutions c,f a constraint set, and of entailment, denoted sk \nand ~~. These new notions of entailment are related to the regu\u00adlar entailment relation by the following \nimplication: k-entailment shares the three basic properties of entail\u00adment given in section 5.5.  B.2 \nSketch of the proof We now want to prove that the algorithm is sound, i.e. if C, 0, @1-~1DTZ, then C \n~ 71DTZ, where 71D72 is a generalized constraint. The proof is made difficult by the existence of the \ninduc\u00adtion rule (2). We would like to prove directly that but this is obviously false in the absence \nof any hypothesis on H1. Rather than trying to express a complex invariant for Hl, we choose to build \nan inductive proof based on relation +,. Let P be a proof of assertion C, HI, ~Yo E 71 D TZ. One calls \ndepth of P the number of propagation n rules used in the shortest branch topped by an induction rule. \nIf the proof uses no induction rule at all, the depth of P is said to be infinite. The proof of correctness \nis cut into two parts. The hard\u00adest part is proving Theorem B.1 If C, H1, HO k T1 ~ T2 has a proof of \ndepth k, then Vk <k C&#38;kIT1DT2 The proof is by induction on the proof s structure. The idea here is \nthat the induction rule is considered {unsafe , because it takes a hypothesis from HI, and we know nothing \nabout H1. So, if all induction rules are used at depth k or deeper, we should be able to show C ~=~) \n~1 D TZ, which involves only constraints derived with a derivation of height k 1at most. Note that in \nthe proof of this theorelm, the case of the induction rule is trivial, because then k = O, and relation \n~0 holds for all constraint sets. The second part of the proof consists in the following theorem: Theorem \nB.2 Let P be a proof of assertion C, O, @1-TI D TZ of jinde depth k. Then there exists a proof P of the \nsame assertion, with depth strictly greater than k. This is rather easy to understand. Whenever the induc\u00adtion \nrule is used in a proof, the current Igoal is part of H1, so it has been encountered before. Insteikd \nof using the in\u00adduction rule immediately, one can modify the proof to use regular rules. Necessarily, \nthe same goal will be encountered later, and only then will the induction rule be used. This causes at \nleast one more propagation rule to be used (be\u00adcause the goal was in H1, and only propagation rules add \ngoals to HI). So, doing this for every induction rule at depth k yields a proof of depth strictly greater \nthan k. The combination of these two theorems gives the sound\u00adness theorem: Theorem B.3 Let C be a constraint \nset and T1, T2 be type terms. Then C,@, O~Tlb Tz+Cl=TIDTz If a proof of C, 0,01-71 DT2 exists, then \nthere exist proofs of arbitrary depth, thanks to theorem B.2. So, according to theorem B. 1, C ~~ T1 \nD 72 holds for arbitrarily large k. Hence, C ~ TI D TZ holds. \n\t\t\t", "proc_id": "232627", "abstract": "This paper studies type inference for a functional, ML-style language with subtyping, and focuses on the issue of simplifying inferred constraint sets. We propose a powerful notion of entailment between constraint sets, as well as an algorithm to check it, which we prove to be sound. The algorithm, although very powerful in practice, is not complete. We also introduce two new typing rules which allow simplifying constraint sets. These rules give very good practical results.", "authors": [{"name": "Fran&#231;ois Pottier", "author_profile_id": "81100490085", "affiliation": "Projet Cristal, INRIA Rocquencourt, BP 105, 78153 Le Chesnay Cedex, France", "person_id": "PP39077387", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/232627.232642", "year": "1996", "article_id": "232642", "conference": "ICFP", "title": "Simplifying subtyping constraints", "url": "http://dl.acm.org/citation.cfm?id=232642"}