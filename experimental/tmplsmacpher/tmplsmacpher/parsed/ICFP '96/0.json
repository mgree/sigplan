{"article_publication_date": "06-15-1996", "fulltext": "\n Let-floating: moving bindings to give faster programs Simon Peyton Jones, Will Partain, and Andr6 Santos \nUniversity of Glasgow Email: {simonpj,partain,andre}tldcs .glasgow.ac.uk. Abstract Virtually every compiler \nperforms transformations on the program it is compiling in an attempt to improve efficiency. Despite \ntheir importance, however, there have been few sys\u00adtematic attempts to categorise such transformations \nand measure their impact. In this paper we describe a particular group of transforma\u00adtions the let-floating \ntransformations and give de\u00adtailed measurements of their effect in an optimizing compiler for the non-strict \nfunctional language Haskell. Let-floating hasnotreceived much explicit attention inthepast, but our measurements \nshow that it is an important group of trans\u00adformations (atleast forlazy languages), offering a reduction \nof more than 3070 in heap allocation and ls~o in execution time. Introduction Consider the following \nexpression: let v = let w = <w-rhs> in Cons w Nil in <body>  A semantically-equivalent expression which \ndiffers only in the positioning of the binding for wis this: let w = <w-rhe.> in let v = Cons wNil in \n<body>  While thetwoexpressions have thesame value, thesecondis likely to bemoreefficient than the first \nto evaluate. (We will say why this is so in Section 3.3. ) A good compiler should transform the first \nexpression into the second. However, the difference in efficiency is not large, and the transformation \nbetween the two is easy none of the romance of strictness analysis here and so not much attention has \nbeen paid to transformations of this kind. We call them let-floating transformations, because they concern \nthe exact placement of let or Ietrecbindlngs; in the example, it is the binding Permission to make digitabharrt \ncopy of part or all of tMs work for personal orclassroomuaeis ranted without feeprovided thatrmpias aranotmacfe \nor distributed for pro { t or commercial dmntage, the copyright notice, the titie of the publication \nand ik date appear, and notice is given that oopying is by permission of ACM, Inc. To copy otherwise, \nto republish, to post on servers, or to redistribute to lists, requires prior specific permission andlor \na fee. ICFP 96 !Y96 PA, USA @ 1996 ACM 0-69791-771 -0/96/0005...$3.50 for w which is floated from one \nplace to another. The Glasgow Haskell Compiler (GHC) is an optimizing com\u00adpiler for the non-strict purely-functional \nlanguage Haskell (Hudak et al. [1992]). Its guiding principle is that of compi\u00adlation by transformation \n that is, as much as possible of the compilation process is expressed as a series of correctness\u00adpreserving \nprogram transformations. As we developed the compiler we gradually discovered the importance of well\u00adtargeted \nlet-floating. This paper reports on and quantifies thk experience. We make the following main contributions: \nWe identify and describe three distinct kinds of let\u00adfloating transformations: Floating inwards moves \nbindings as far inwards as possible (Section 3.1).  The full laziness transformation floats selected \nbind\u00adings outside enclosing lambda abstractions (Sec\u00adtion 3.2)  Local transformations fine-tune the \nlocation of bindings (Section 3.3).  o We give detailed measurements of their effectiveness (Section \n5). We do not simply measure bottom-line performance changes, but also quantify many of the effects predicted \nin Section 3, and explore the effects of some variants of the basic transformations. These measurements \nare made in the context of a state\u00adof-the-art compiler and a set of substantial programs. Our results \nare encouraging: on average, let-floating reduces heap allocation by over 30% and execution time by more \nthan 15%. In the context of an optimizing compiler all the other optimisations are turned on for these \nmeasurements, so there are no easy pickings this is a very worthwhile gain. For particular programs \nthe gains can be spectacular; one program ran twice as fast with let-floating. We believe that our results \nare not very GHC-specific, but probably are rather lazy-language-specific. Analogous trans\u00adformations \nshould be equally successful in other compilers for non-strict languages, but the whole issue is probably \nmuch less important for a strict language. Program Prog +-Bindingl ; . . . ; Birding. n >1 Bindings Binding \n+ Bind ] rec Bindl . . . Bindn Bind + var = Expr Expression Expr -+ Expr Atom Application ~ ~;;r: Type \napplication . . . varm -> Expr Lambda abstraction I A ty -> Expr Type abstraction I case Expr of Alts \nCase expression I let Bindzng in Ezpr Local definition I con varj . . . uarn Constructor n ~ O I prim \nuarl . . . varm Primitive fl>o 1 Atom Atoms Atom + var Variable I Literai Unboxed Object Literal values \nLiteral -+ integer I j70at I . . . Alternatives Alts + Cultl ; . . . ; Caltn; Default n~O I Laltj ;...; \nLaltn; Default n~O Constr. alt Cult -+ Con uarl . . . uarn -> Expr n~O Literal alt Lalt + Literal -> \nExpr Default alt Default + NoDefault I var -> Expr Figure 1: Syntax of the Core language 2 Language \nframework 2.1 The operational reading Another unusual feature of the Core language is that it hasBefore \ndescribing the transformations themselves, we must a dzrect operational interpretation, as well as the \nconven\u00adfirst introduce the language we use. Its grammar is given tional denotational semantics. If we \nare to reason about thein Figure 1, and consists essentially of the (non-strict) sec\u00adusefulness of a \ntransformation and thk paper contains aond order lambda calculus augm cmt ed with let ( re c ) ex\u00adgreat \ndeal of such reasoning we must have some model forpressions, case expressions, and constructors. A program \nhow much it costs to execute it, so an operational interpreta\u00adconsists of a set of bindings. The value \nof the program is tion is very desirable. The two most important operationalthe value of the variable \nmain. intuitions are as follows: A value is an expression in weak head normal form a vari\u00ad able, lit \neral, constructor application, or lambda abstraction. 1 let (ret) bindings (and only let (ret) bindings) \nper\u00ad form heap allocation. For example:An unusual feature of the Core language is that it is based on \nthe polymorphic, second-order lambda calculus (Fortune, let v = factorial 20Leivant k O Donnell [1983] \n), featuring type abstractions (in\u00adintroduced by A) and type applications (denoted by simple fv3juxtaposition). \nIts usefulness to us is that it provides a simple, well-understood way of attaching types to the pro- \n The operational understanding is first allocate in the gram which can be maintained through substantial \nprogram heap a thunk (or suspension) for factorial 20, and transformations. We dkcuss this aspect in \nPeyton Jones et bind it to v, then call f passing it the parameters v andal. [1993], but here we tend \nto omit the type abstractions 3 . The language is non-strict, so v is not evaluated and applications, \nsince they do not play an important role. before calling f. Rather, a heap-allocated thunk is built \nThroughout the paper we take a few liberties with the syn-and passed to f. If f ever needs the value \nof v it will tax: we allow ourselves infix operators (eg <e 1> + <e2>), force the thunk which, when its \nevaluation is complete, and special syntax for lists ([] for nil and infix : for cons). will update (overwrite) \nitself with its value, If f needs We allow multiple definitions in a single let expression to the value \nof v again, the heap object now contains its abbreviate a sequence of nested 1 et expressions. We use \nthe value instead of the suspended computation. notation <e> to denote an arbitrary expression. A let \n(ret) binding may also allocate a value rather than a thunk. In our implementation, the allocated object \n(be it a thunk or value) consists only of a code pointer together with a slot for each free variable \nof the expression. Only one object is allocated, regardless of the size of the expression (older implementations \nof graph reduc\u00adtion do not have this property). We do not attempt to share environments between thunks \n(Appel [1992]; Kranz et al. [1986]). 2. case expressions (and only case expressions) perform evcduata \non. For example: case x of [1 ->0 (y:ys) -> y+gys Theoperational understanding is evaluatex,andt hen \nscrutinise it to see whether it is an empty list, [], or a cons cell of form (y:ys), continuing execution \nwith the appropriate alternative. case expressions subsume conditionals, of course. The conditional if \n<cond> <cl> <e2> is written case <cond> of True -> <cl> False -> <e2> The syntax in Figure 1 requires \nthat function arguments must be atomsf (that is, variables or literals), and now we can see why. If the \nlanguage allowed us to write f (factorial 20) 3 the operational behaviour would still be exactly as \ndescribed in (1) above, with athunkallocated as before. The let form is simply more explicit. Furthermore, \nthe let form gives us the opportunity of moving the binding for v elsewhere, if that turns out to be \ndesirable, which the apparently-simpler form does not. Lastly, the let form is more economical, because \nmany transformations on let expressions (concern\u00ading strictness, for example) would have to be duplicated \nfor function arguments if the latter were non-atomic. It is also important to note where atoms are not \nrequired. In particular, the scrutinee of a case expression is an arbitrary expression not just an atom. \nFor example, the following is quite legitimate: case (reverse XS) of [1 -> <nil-case> (y:ys) -> <cons-case> \n Operationally, thereis noneed to buiklat hunkforreverse xs and then evaluate it; rather, we can simply \nsave the contin\u00aduation and call reverse XS. Again, the operational model cletermines the syntax. These \ninformal operational notions help identify transfor\u00admations that might be beneficial, but they do not \nconstitue a formal model of efficiency . We have started to develop such a model, but technical difficulties \nremain (Santos [1995, Chaper 9]). 2.2 The costs of a let(rec)-binding Consider the expression I This \nsyntax is becommg quite w]dely used (Arlola et al. [1995]; Flanagan et al. [1993]; Launchbury [1993]; \nPeyton Jones [1992]). letx=fy in . ..x. .. x... What, precisely, arethecosts performance of modern prmemory \nbandwidth, so we the major cost: of ocessors treat thelet-binding is often interaction limited with forx? \nby memory The their as 1. Allocation. The thunk for (f y) has to be allocated and initialised. U. \n 9 Evaluation. The first time x is evaluated, the contents of the thunk must be read from the heap into \nregisters. 3. Update. When the evaluation x is started, an update frame must be stored on the stack; \nwhen its evalua\u00adtion is complete, its final value must be written back into the heap, overwriting the \nthunk identified in the update frame. This value will either be a data value (such as an integer, character, \nor list cell) or a function value. 4, Further evaluations. Anv subsequent evaluations ofx will find the \nevaluated form, but each still entails read\u00ading the value from the heap into processor registers. Sometimes, \nthe right-hand side (RHS) of a binding is already manifestly a value, rather than a thunk: let y = (p)q) \nin. ..y. .. y... The let-bindlng for yis somewhat cheaper than that forx, because no update need be \nperformed. 2.3 Strictness analysis Strictness analysis is a static program analysis that iden\u00adtifies \nexpressions which are sure to be evaluated. Though strictness analysis is not the subject of thk paper2, \nit helps to have some understanding of how the results of the anal\u00adysis are used, because some let-floating \ntransformations are designed to improve the effectiveness of strictness analysis. Suppose we start with \nthe expression letx=fy in ., .x... If the strictness analyser is able to prave that x is sure to be \nevaluated, and annotates its binding to say so, then we can subsequently make the following transformation \n(which we call let-to-case ): case (f y) of x -> . .. x... The operational reading of the latter form \nis just as for any case expression: evaluate (f y), bind its value to x, and continue with the code for \n. . . x. . . In effect, this form encodes a strict let expression. The second form is significantly cheaper \nto execute than the first. In effect, the first-evaluation and update costs of the thunk (items 2 and \n3 in Section 2.2) are eliminated, quite a worthwhile saving. z Peyton Jones &#38; Partain [1993] presents \ndetaded measurements of ,t. effectiveness in the same spr, t as thm papa-, 3 What we hope to gain case \ny of We are now ready to describe the three let-floating transfor\u00admations mentioned earlier, and to say \nwhat we hope to gain by each. The details of how each is implemented, and what the actual gains achieved, \nare discussed subsequently. 3.1 Floating inwards The floating-inward transformation is based on the \nfollowing observation: other things being equal, the further inward a binding can be moved, the better. \nFor example, consider: let x=y+l in case zof [1 -> X*X (p:ps) -> 1 Here, the binding for x is used in \nonly one branch of the case, so it can be moved into that branch: case z of [1 -> let x =y+I in X*X (p:ps) \n-> 1 Moving the binding inwards has at least three distinct be\u00adnefitss: ~ The binding mayneverbe executed \n. Intheexample, z might turn out to be of the form (p:ps), in which case the code which deals with the \nbinding for x is not executed. Before the transformation a thunk for x would be allocated regardless \nof the value of z. / Strictness analysis has a better chance. It is more likely that at the point at \nwhich the binding isnow placed it is known that the bound variable is sure to be evalu\u00adated. This in \nturn may enable other, strictness-related, transformations to be performed. In our example, in\u00adstead \nof allocating a thunk for x, any decent compiler will simply evaluate y, increment it and square the \nresult, allocating no thunks at all (Section 2.3), ~ Redundant evaluation. may be eliminated. It is pos\u00adsible \nthat the RHS will see the evaluation state of more variables than before. To take a similar example: \nlet x=case yof (a,b) -> a in case y of (p, q) -> X+p If the binding of x is moved inside the case branch, \nwe get: case y of (p,q) -> let x=case yof (a,b) -> a in X+p Now the compiler can spot that the inner \ncase for y is in the RHS of an enclosing case which also scrutinises y. It can therefore eliminate the \ninner case to give: Throughout the paper, advantages are marked with / and dis\u00adadvantages with x O indicates \nmoot points (p, q) -> p+p The first two benefits may also accrue if a binding is moved inside the RHS \nof another binding. For example, floating inwards would transform: let x=V+W in y=. ..x. ..x. ... <body> \n(where <body> does not mention x) into lety=let x=v+win .,.x. ..x. . . in <body> (The alert reader will \nnotice that this transformation is pre\u00adcisely the opposite of that given in the Introduction, a point \nwe return to in Section 3.3. ) This example also illustrates another minor effect of moving bindings \naround: o Floating can change the size of the thunks allocated. Recall that in our implementation, each \nlet (ret) bind\u00ading allocates a heap object that has one slot for each of its free variables. The more \nfree variables there are, the larger the object that is allocated. In the example, floating x into y \ns RHS removes x from y s free vari\u00adables, but adds vandw. Whether y sthunk thereby be\u00adcomes bigger or \nsmaller depends on whether v and/or w were already free in y. So far, we have suggested that a binding \ncan usefully be floated inward to (as far as possible ; that is, to the point where it can be floated \nno further in while still keeping all the occurrences of its bound variable in scope. There is an important \nexception to this rule: it is dangerous to float abinding in.side a lambda abstraction. Why? Because \nif the abstraction is applied many times, each application will instantiate a fresh copy of the binding. \nWorse, if the binding contains a reducible expression the latter will be re-evaluated each time the abstraction \nis applied. The simple solution is never to float a binding inside a lambda abstraction, and that is \nwhat our compiler currently does (but see Section 7). But what if the binding is inside the abstraction \nto start with? We turn to this question next. 3.2 Full laziness Consider the definition f = \\xs -> letrec \ng=\\y -> let n=length xs in ...g. ..n. .. in .. -g... Here, the length of xs will be recomputedon each \nrecursive call to g. This recomputation can be avoided by simply floating the binding fern outside the \n\\y-abstraction: f = \\xs -> let n = length xs in letrec g=\\y->. ..g. ..n. . . in .. g... 4 This transformation \niscalled julllazmess. It was originally invented by Hughes (Hughes [1983]; Peyton Jones [1987]), who \npresented it as a variant of the supercombinator lamb\u00adda-lifting algorithm. Peyton Jones &#38; Lester \n[1991] subse\u00adquently showed how to decouple full laziness from lambda lifting by regarding it as an exercise \nin floating let (ret) bindings outwards. Whereas the float-in transformation a\u00advoids pushing bindings \ninside lambda abstractions, the full laziness transformation actively seeks to do the reverse, by floating \nbindings outside an enclosing lambda abstraction. The full laziness transformation can save a great deal \nof repeated work, and it sometimes applies in non-obvious sit\u00aduations. One example we came across in \npractice is part of a program which performed the Fast Fourier Transform (FFT). The programmer wrote \na list comprehension similar to the following: [xs-dot (map (do-cos k) (thetas n)) I k<-[0 . . n-l]] \n What he did not realise is that the expression (thetas n) was recomputed for each value of k! The list \ncomprehen\u00adsion syntactic sugar was translated into the Core language, where the (thetas n) appeared inside \na function body. The full laziness transformation lifted (thetas n) out past the lambda, so that it was \nonly computed once. A potential shortcoming of the full laziness transformation, as so far described, \nis this: it seems unable to float out an expression that is free in a lambda abstraction, but not let \n(ret) bound. For example, consider f =\\x -> case x of [1 ->gy (p:ps) -> ...  Here, the subexpression \n(g y) is free in the \\x-abstraction, and might potentially be an expensive computation which could potentially \nbe shared among all applications of f. It is simple enough, in principle, to address this shortcoming, \nby simply let-binding (g y) thus: f =\\x -> case x of [1 ->leta=gy in a (p:ps) -> ...  Now the binding \nfor a can be floated out like any other binding. The full laziness transformation may give rise to large \ngains, but at the price of making worse all the things that floating inwards makes bet t er (Section \n3.1). Hence, the full laziness transformation should only be applied when there is some chance of a benefit. \nFor example, it should not be used if either of the following conditions hold: 1. The RHS of the binding \nis already a value, or reduces to a value with a negligible amount of work. If the RHS is a value then \nno work is saved by sharing it among many invocations of the same function, though some allocation may \nbe saved. 2. The lambda abstraction is applied no more than once.  We are experimenting with a program \nanalysis which detects some situations in which a lambda abstraction is applied only once (wadler turner \npop195]). There is a final disadvantage to the full laziness which is much more slippery: it may cause \na space leak. Consider: f = \\x -> let a = enumerate 1 n in <body> where enumerate I nreturns the list \nof integers between andn. Isit agood idea to float the binding for a outside the \\x-abstraction? Certainlv. \ndoin~so would avoid recomtmt\u00ad ., . ing a on each call of f. On the other hand, a is pretty cheap to \nrecompute and, if nis large, the list might take up slot of store. It might even turn a constant-space \nalgorithm into a linear-space one, or even worse. In fact, asour measurements show, space leaks do not \nseem to be a problem for real programs. We are, however, rather conservative about floating expressions \ntothe top level where, for tiresome reasons, they areharder to garbage collect.  3.3 Local transformations \nThe third set of transformations consist of local rewrites, which fine-tune the placement of bindings. \nThere are just three such transformations: (let v=e inb) a -+ (letv=e inba) case (let v=e in b) of ---+ \nlet v=e alts in case b of alts let x=let v=e in b ---i let v=e in c in let x=b in c Each of the three \nhas an exactly equivalent form when the binding being floated out wards is a let rec. The third also \nhas a variant when the outer binding is a letrec in this case, the binding being floated out is combined \nwith the outer letrec to make a larger letrec. Subsequent depen\u00addency analysis (see Section 3.4) will \nsplit the enlarged group up if it is possible to do so. The first two transformations are always beneficial. \nThey do not change the number of allocations, but they do give other transformations more of a chance. \nFor example, the first moves a let outside an application, which cannot make things worse and sometimes \nmakes things better for ex\u00adample, b might be a lambda abstraction which can then be applied to a. The \nsecond floats a let (ret) binding out\u00adside a case expression, which might improve matters if, for example, \nb was a constructor application. The thh-d transformation, the let-from-let transformation, which floats \na let (ret) binding from the RHS of another let (ret ) binding, is more interesting. It has the following \nadvantages: J Floating a binding out may reveal a head normal form. For example, consider the expression: \nlet x=let v=<v-rhs> in (v,v) in <body> When this expression is evaluated, a thunk will be al\u00ad located \nfor x. When (and if) x is evaluated by <body>, the contents of the thunk will be read back into reg\u00ad \nisters, its value (the pair (v, v)) computed, and the heap-allocated thunk for x will be overwritten \nwith the pair. Floating the binding for v out would instead give: let v = <v-rhs> x = (V,v) in <body> \n When this expression is evaluated, athunkwill be al\u00adlocated for v, and a pair for x. In other words, \nx is allocated Zn its jinal form. No update will take place when x is evaluated, a significant saving \nin memory traffic. { There is a second reason why revealing a normal form may be beneficial: <body> may \ncontain a case expres\u00adsion which scrutinises x, thus: . . . (casex of (p,q) -> <case-rhs>) . . . Now \nthat x is revealed as being bound to the pair (v, v), this expression is easily transformed to . ..(<case-rhs>[v/p \n,v/q])... (Wecall this the known-branch transformation, be\u00adcause it uses information about the scrutinee \nofa case expression to choose the correct branch of the case. ) ~Floatingv s binding out may reduce the \nnumber of heap-over flow checks. A heap-overflow check isnec\u00adessary before each sequence of let(rec) \nbindings, to ensure that alarge enough contiguous block ofheap is available to allocate all of the bindings \nin the sequence. For example, the expression let v = <v-rhs> x = (V,v) in <body> requires a single check \nto cover the allocation for both v and x. On the other hand, if the definition ofv is nested inside the \nRHS of x, then two checks are required. These advantages are all very well, but the let-from-let trans\u00adformation \nalso has some obvious disadvantages: after all, it was precisely the reverse of this transformation which \nwe ad\u00advocated when discussing the floating-inward transformation! Specifically, there are twodkadvantages: \nx Ifx is not evaluated, then an unnecessary allocation for v would be performed. However, the strictness \nanal\u00adyser maybe able to prove that x is sure to be evaluated, in which case the let-from-let transformation \nis always beneficial. x It is less likely that the strictness analyser will discover that v is sure to \nbe evaluated. This suggests that the strictness analyser should berun before performing the let-from-let \ntransformation. It is possible that the let-from-let transformation is worth while even if x is not sure \nto be evaluated. We explore vaxious compromises in Section 5.3.  3.4 Composing the pieces We have integrated \nthe three let-floating transformations into the Glasgow Haskell Compiler. The full laziness and float-inwards \ntransformations are implemented as separate passes. In contrast, thelocal let-floating transformations \nare combined with a large collection of other local transforma\u00adtions in a pass that we call the Simplifier \n(Peyton Jones &#38; Santos [1994]; Santos [1995]). Among the transforma\u00adtions performed by the Simplifier \nis dependency analysis, which splits each letrec binding into its minimal strongly\u00adconnected components. \nDoing this is sometimes valuable because it lets the resulting groups be floated independently. We perform \nthe transformationsin the following order, 1. Do the full laziness transformation. 2. Do the float-inwards \ntransformation. This won t affect anything floated outwards by full laziness; any such bindings will \nbe parked just outside alambdaabstrac\u00adtion. 3. Perform strictness analysis. 4. Do the float-inwards \ntransformation again.  Between each of these passes, the Simplifier is applied. We do the float-inwards \npass before strictness analysis be\u00adcause it helps to improve the results of strictness analysis. The \ndesirability ofperforming the float-inwards transforma\u00adtion again after strictness analysis surprised \nus. Consider the following function: fxy=ify==O then error ( Divide by zero: ++ show x) else x/y The \nstrictness analyser will find f to be strict in x, because calls to error are equivalent to -L, and hence \nwill pass x to f in unboxed form. However, the then branch needs x in boxed form, to pass to show. The \npost-strictness float\u00adinwards transformation floats a binding that re-boxes x into the appropriate branch(es) \nof any conditionals in the body of f, thereby avoiding the overhead of re-boxing x in the (common) case \nof taking the else branch. 4 Implementing let-floating The implementation of the float-in transformation \nand local let-floating is straightforward, but the full laziness transfor\u00ad mation has a few subtleties. \nWe use a two-pass algorithm to implement full laziness: 1. The first pass annotates each let(rec) binder \nwith its (level number 4. In general, level numbers are defined like this. The level number of a let-bound \nvariable is the maximum of the level numbers of its free vari\u00adables, and its free type variables. 4 Actually, \nall the other binders are also annotated, but they are never looked at subsequently  The level number \nof a letrec-bound variable is the maximum of the level numbers of the free vari\u00adables of all the RHSS \nin the group, less the letrec\u00adbound variables themselves.  The level number of a lambda-bound variable \nis one more than the number of enclosing lambda abstractions.  The level number of a case-or tvpe-lambda-bound \nvariable is the number of enclos~~g (ordinary) lamb\u00adda abstractions.  2. The second pass uses the level \nnumbers on let (rec)s to float each binding outward to just outside the lambda which has a level number \none greater than that on the binding. Notice that a binding is floated out just far enough to escape \nall the lambdas which it can escape, and no further. This is consistent with the idea that bindings should \nbe as far in as possible. There is one exception to this: bindings with level number zero are floated \nright to the top level. Notice too that a binding is not moved at all unless it will definitely escape \na lambda. This algorithm is much as described by Peyton Jones &#38; Lest er fi991], but there are a few \ncomplications in practice. Firstly, type variables are a nuisance, For example, suppose that f and k \nare bound outside the following \\x-abstraction: \\x -> . . . (/\\a -> . ..letv=f akin...) We d like to \nfloat out the v = f a k, but we can t because then the type variable a would be out of scope. The rules \nabove give a the same level number as x (assuming there are no intervening lambdas) which will ensure \nthat the binding isn t floated out of a s scope. Still, there are some partic\u00adularly painful cases, not \nably pat t ern-match:ng failure bind\u00adings, such as: fail = error a Pattern fail We really would like \nthis to get lifted to the top level, de\u00adspite its free type variable a. There are two approaches: ignore \nthe problem of out-of-scope type variables, or fix it up somehow. We take the latter approach, using \nthe fol\u00adlowing procedure. If a binding v = e has free type variables whose maximum level number is strictly \ngreater than that of the ordinary variables, then we abstract over the offending type variables, al. \n. an, thus: v=letv~= /\\al. .an->e inv al . . . an Now v is given the usual level number (taking type \nvariables into account ), while v is given the maximum level number of the ordinary free variables only \n(since the type variables al . .an are not free in v ). The reason this is a bit half baked is that some \nsubsequent binding might mention v; in theory it too could be floated out, but it will get pinned inside \nthe binding for v. (It s the binding for v which floats. ) But our strategy catches the common cases. \nThe second complication is that there is a penalty associated wit h floating a binding between two adj \nscent lambdas. For example, consider the binding f=\\xy->letv= length xin . . . It would be possible \nto float the binding for v between the lambdas for x and y, but the result would be two functions of \none argument instead of one function of two arguments, which is less efficient. There would be gain only \nif a partial application of f to one ar ument was applied many times. 5 Indeed, our measurements indicate \nthat allowing lambdas to be split in this way resulted in a significant loss of per\u00adformance. Our pragmatic \nsolution is to therefore treat the lambdas for x and y as a single lambda group , and to give a single \nlevel number to all the variables bound by a group. As a result, lambda groups are never split. The third \ncomplication is that we are paranoid about giving bindings a level number of zero, because that will \nmean they float right to the top level, where they might cause a space leak6. We use several heuristics \nwhich sometimes decide (conservatively) to leave a binding exactly where it is. If this happens, instead \nof giving the binding level number zero, it is given a level number of the number of enclosing lambdas \n(so that it will not be moved by the second pass). 5 Results We measured the effect of our transformations \non a sam\u00adple of 15 real programs from our NoFib test suite7 (Par\u00ad tain [1993]). By real we mean that \neach is an application program written by someone other than ourselves to solve a particular problem. \nNone was designed as a benchmark, and they range in size from a few hundred to a few thousand lines of \nHaskell. Table 5 gives the absolute performance numbers for each program, compiled with ghc-O. 26 -o, \nwhich includes all the let-floating transformations of this paper. The SPARC in\u00adstruction counts were \ncollected with SpixTools, kindly pro\u00advided by Bob Cmelik, then at Sun; we also report how many of the \ninstructions were memory loads and stores, and how many were used by the garbage collector. Instruction-count \nchanges tend to understate execute-time changes, so per\u00adformance changes might be a bit better In Real \nLife. The time numbers (from a single use of /bin/time) are inher\u00adently fuzzy; we provide them mainly \nas a sanity check for the instruction-count numbers. A11ocs gives the number of words in heap-allocated \nobjects, a widely-used measure which is surprisingly poorly correlated with execution time; we place \nlittle faith in it. Resid (residency) gives the average and maximum amount of live data during execution, \na number that directly affects the cost of garbage collection, and is the best measure of the space consumption \nof a program. The residency num\u00adbers were gathered by sampling the amount of live data at frequent intervals, \nusing the garbage collector. Frequent 5See Santos [1995] for these figures, we do not present them here. \n61n our implementation, all top-level values are retained for the whole life of the program. It would \nbe poss]ble for the garbage collec\u00ad tor to figure out which of them cannot be referred to again, and \nhence wh]ch could safely be garbage collected, but doing so adds complexity and slows both the mutator \nand the garbage collector The results reported in this paper are by no means all that we collected. \nWe pored over results for all 70-Ish NoFib programs, not just the [real ones used here Frost we budt \n14 special versions of prelude libraries (145MB worth) Then we built each NoFib program 16 ways (the \nextra 2 ways used standard prelude hbranes), taking up a total of 1,415MB In disk space Compde t]me and \nrun time for all the testsv we d rather not think about it! Allots. Resid. (Kwords) Instructions (M) \nTime Program (Kwords) Avg. real/HMMS 1,008 74,233 real/anna 5,873 329 real/bspt 1,015 288 real/compress \n34,735 158 real/fulsom 51,488 1,177 real/gamteb 20,053 279 real/gg 1,628 215 real/hidden 93,494 211 real/hpg \n13,440 355 real/infer 2,531 961 real/parser 2,465 470 real/pie 1,208 208 real/reptile 1,108 488 real/rsa \n7,315 3 real/symalg 46,960 23,210 Table 1: Base case: absolute sampling means that any spikes in live \nmemory usage are unlikely to be missed. The following sections give the results of measuring the ef\u00adfect \nof the three transformations (floating inwards, floating outwards, and local floating) separately, followed \nby mea\u00adsurements of their combined effect, In each case, we try to quantify the effects predicted in \nSections 3.1 3.3, as well as measuring the overall effect on execution time and space. Most results will \nbe given as percentage changes from these -O numbers, which we treat as the base case. A positive value \nmeans more than the base case , negative means less than the base case ; whether that is good or bad \nde\u00adpends on what is being measured. Changes to percentages, e.g. percentage of instructions in garbage-collection \n, are given as raw numbers; if the base number was 35.1% and the change is given as -0.7, then the percentage \nin the changed case is 34.4%. In all tables, a dash indicates a zero figure. Space precludes listing \nthe results for each individual pro\u00adgram. Instead, we report the minimum, maximum, median and mean figures \nfor the whole set. The Means listed are geometric means, since they are the means of perfor\u00admance ratios \n(Smith [1988] ). While the mean improvements we find are often modest, it is important to bear in mind \nthe outliers) , reflected in the (Min and Max columns. A production-quality compiler should have a lot \nbullets in its gun; each individual bullet may only target a small class of programs, but it may make \na large difference to that class. 5.1 Floating inwards In this section, we quantify the effects of the \nfloating-inwards transformation. Table 2 shows the effects ofswitching float\u00ading-inwards off. In most \nof the table, all the other optimi\u00adsations are left on, except for the second block of the table where \nstrictness analysis is also switched off. The first block gives the bottom line . Without floating inwards, \nwe write about 670 more words into memory, and execute somewhat under l% more instructions. Residency \nis not affected significantly. The largest improvement came from fulsom, with 55.7Yo more allocations, \nand infer, with 3.9% more instructions. Max. total %mem %gc (sees.) 1,885 3293.0 35.5% 20.5% 67.8 542 \n252.0 46.4% 10.5% 6.4 365 26.6 41.0% 18.6% 0.7 164 918.9 50.4% 14.070 19.8 3,328 1142.0 43.9% 20.2% \n25.9 528 594.4 26.7% 16.1% 10.9 369 47.5 34.2% 17.0% 1.0 315 1937.0 38.6% 4.7% 37.3 597 344.7 33.6% 24.7% \n11.3 1,935 201.1 46.3% 32.9% 4.8 860 110.9 41.1% 29.0% 2.5 296 38.7 32.1% 12.4% 0.9 606 28.5 41.3% 15.8% \n0.7 9 1122.1 8.3% 1.8% 12.9 46,160 7449.2 3.3% 1.1% 109.1 numbers for what happens with -O Mean Min. \nMedian Max. Allots. 6.1% -0.5% 0.1% 55.7% Avg. resid. -2.2% -29.6% 7.6% Max. resid. -2.7% -38.0% -0.1% \n13.3% Insns. 0.6% -1.9% 3.9% %mem -0.1 \u00ad%v2c..u. +0.1 -1.8 .- -0.5 I .. - no3A-Fl vsI noSA-noFI Allot \nd 5.0% n..-2% 54.9% I .. AvgSize -0.1% -2.4% 0.8% Strict bindings found \\ -o 37.7% 24.0% 37.9% 69.4% \nnoFI -0.7 -0.4 -0.5 Effect on number of enters I Enters: 0.2% -0,3% 0.1% 1.6% 1 Table 2: The effects \nof turning off floating inwards Next, we try to quantify the effects predicted in Section 3.1. How man~ \nheap ob~ects are not allocated at all as a re\u00adsult of j70atzng;nw; rds, and how is the size of allocated \nobject a~ected ? There is a complication here: floating inwards helps strictness analysis, and successful \nstrict\u00adness analysis also reduces allocation. Hence, to mea\u00adsure the reduction in allocation due only \nto floating inwards we turned off strictness analysis (no SA), and compared how many objects were allocated \nwith and without floating inwards (F I and noFI). The results are given in the second block of Table \n2. About 5% fewer objects are allocated when floating in\u00adwards is on, and there is essentially no effect \non object size (-0.190). How much is the strictness analyser helped by floating inwards ? The third block \nof Table 2 shows the pro\u00adportion of binders that are identified by the strictness analyser as sure to \nbe evaluated in the base case ( O), and how this proportion changes when floating inwards is switched \noff. With floating inwards, strictness analysis manages to tag 37.7% of binders as certain to be demanded \n. Float outwards: Lambda Top-level Inner m %gc +1.3 -+0.3 +1.3 U~dates: 7.0% -0.2% 2.9% 60.1% Table \n3: Effect of turning off the full laziness transformation Things are slightly worse without floating \ninwards (-0.7%). So floating inwards does help the strictness analyser, but not much. e How many evaluations \nare eliminated by floating in\u00adwards ? The final block of Table 2 concerns the num\u00adber of times a heap \nclosure (be it a thunk, data value or function value) was evaluated or entered during the execution \nof the program. Again, floating inwards has a small but beneficial effect (mean 0.2970, max 1.6~o). \n 5.2 Full laziness Next we turn our attention to the full laziness transforma\u00adtion. The results are summarised \nin Table 3, which shows the effect of switching off full laziness while leaving all other optimisations \non. Overall. full laziness buvs a reduction of 10% in allocation, and 7% in instructions executed. We \nwould expect the num\u00adber of updates to decrease, because of the extra sharing of thunks caused by full \nlaziness, and indeed it does go down, by about 7%. As the Max column shows, one program (hidden, a hid\u00adden-line \nremoval algorithm) is dramatically improved by full laziness. Leaving bindings unnecessarily stuck inside \nan in\u00adner loop is a Really Bad Idea. Does full laziness affect a very few expressions in each pro\u00adgram, \nor is it widely applicable? Initially we guessed the for\u00admer, but the measurements in Table 4 contradicts \nthis guess. The table counts how many bindings were floated at all by full laziness. In order to make \ncomparable the figures for dif\u00adferent programs, we (somewhat arbitrarily) normalised them to the number \nof lambda groups in the program. We also identify separately bindings which float to the top level these \nare just constant expressions and those that float to some other place. For example, in HMMS, on average \n1.4 con\u00adstant bindings and 0.3 non-constant bindings floated past each lambda group. As you would expect, \nfloated constant bindings are about 5 times as common as floated non-constant bindings. Overall, we find \nthe figures in Table 4 surprisingly high, and we be\u00adlieve that there is probably scope for increased \nselectivity in the full laziness transformation.  5.3 Local t ransformat ions Next, we compare four \ndifferent strategies for local let-float\u00ad ing: groLlps ratio ratio real/ IIWIS 435 1.4 0.3 real/anna \n2,221 0.7 0.1 real/bspt 290 0.8 0.1 real/compress 19 0.3 \u00adreal/ebnf2ps 487 1.3 0.4 real/f luid 468 1.0 \n0.2 real/ fulsom 217 1.0 0,3 real/gamteb 52 2.1 \u00adreal/gg 363 0.9 0.3 real/grep 173 0.5 0.1 real/hidden \n269 0.3 0.1 real/hpg 310 0.6 0.3 real/infer 258 0.6 0.3 real/lift 151 0.6 0.2 real/maillist 37 0.6 0.2 \nreal/parser 424 0.9 0.3 real/pie 124 1.0 0.6 real/prolog 185 0.6 (3,2 real/reptile 246 1.5 0.1 real/rsa \n24 1.9 0.1 real/symalg 148 1.8 \u00adreal/veritas 1,047 2.0 0.2 MEAN 0.9 0.2 MIN 0.3 -MEDIAN 0.9 0.2 MAX 2.1 \n0.6 Table 4: Details of full laziness floats None. Donolocal let-floatingat all. Strict. Bindings are \nfloated out of strict contexts only; namely, ap~lications, case scrutinies, andthe RHSsof str~ct lets. \nThese floats cannot increase the number of closures allocated, so strict should out-perform none . Base \ncase (-O). Like strict , butinadditiona binclingis floated out of a let (ret) RHS if doing so would reveal \na value. Always, Like strict , but any binding at the top of a let(rec) RHSis floated out. Table 5 shows \nthe effects of these four strategies; as always we report percentage changes from the base case ( O). \nOver\u00adall, the base case consistently out-performs the three other variants (which is, of course, why \nwe make it the default choice). None is, unsurprisingly, terrible (15 70 more al\u00adlocation, 8% rnoreinstructions, \n8% worse peak residency). Strict is relatively dke (5% more allocation, 6% more in\u00adstructions, 6~0 worse \npeak residency); always has more gentle effects (e.g., only 0.5 % more instructions) but cannot besaid \nto justify itsmore aggressive floating strategy. Note also that strict isprone tovery unpleasant outliers \n(e.g., 70%+ residency degradation); moreover, these outlier effects are spread across a range of programs \n(it isn t just one program being hit very badly). We now compare the three defensible strategies for \nfloating bindings out of let(rec) RHSS, using theeffects preclicted in Section 3.3. (Since none is so \nobviously terrible we don t consider it further.) Mean Allots.: always 3.4% strict 5.3% none 15.7% Avg. \nresid.: always -0.8% strict 3.9% none 5.7% Max. resid.: always -0.5% strict 6.1% none 8.3% Insns.: always \n0.5% strict 6.0% none 8.7% %mem: always +0.7 strict none +0.4 Vogc: always +1.2 strict +0.6 none +1.6 \nUpdates: always 2.4% strict 28.2% none 30.7% AvgSize: always -2.3Y0 strict 1.0% none -1.3% Hea~ Chks: \na always -0.3% strict 14.2% none 25.5% Known branches: always -3.8% strict -2.5% none -7.57, Enters: \nalways 1.5% strict 2.5% none 6.5% Table 5: Effects Min. 0.1% 0.1% 0.2% -30.8% -19.9% -19.9% -38.5% -19.8% \n-19.8% -2.1% -0,1% +1.2 +0.6 +0.7 -+0.7 -12.8% 2.0% 3.1% -7.5% -3.6% -12.7% -14.7% 0.7% 0.8% -14.8% -20.0% \n-36.7Y0 -0.170 -1.1% -0.8% of local Median 1.3% 3.2% 10.1% 0.4% 1.0% 4.1% 0.5% 0.8% 4.0% 4.7% 5.7% -+0.3 \n+0.2 -0.5 -0.3 +0.3 0.5% 26.9% 30.5% -1.2% 0.7% -0.6% -0.2% 10.1% 16.8% -3.2% -0.9% -5.5% 0.4% 1.4% 4.5% \nlet-floating Max. 15.5% 16.1% 118.2% 29.0% 75.8% 50.9% 19.1% 71.1970 69.0% 3.2% 18,670 35.3% +0.1 -0.2 \n-0.2 -2.4 -2.3 33.7% 103.1% 103.1% 0.1% 6.6% 5.7% 19.3% 35.7% 96,6% 9.3% 8.0% 16.9% 32.0% How many extra \nallocations are performed? Is the av\u00aderage closure size increased or decreased? Allocation is up 3% in \nthe always case, which is unsurprising, because floating a binding out of a let (ret) RHS will cause \nthat binding to be allocated when it might not otherwise be. It is more surprising that allocation also \nrises in the strict case, which is less aggressive than the base case. The reason turns out to be that \nthe strictness analyser is foxed by definitions like this one: f = let x = <x-rhs> in \\y -> <f_body> \nWith the strict strategy the binding forx may not be floated outofthatforf. Whilst the strictness anal\u00adyser \nspots that f is strict, it does not exploit that fact because doing so naively would involve recomputing \n<x-rhs> on each call of f (Peyton Jones &#38;z Launch\u00adbury [1991]). Since our default -O fioating strategy \ndominates strict in other ways, and solves this dif\u00adficulty by floating x out one level, we have not \nmade thestrictness analyser able to deal withit directly. How many updates are saved? It is no surprise \nthat updates are more common (28%) with the less aggressive strict strategy, because fewer let(rec) RHSS \nare values which require no update. It is slightly surprising that [always seems to make updates increase \nagain (2.4~0). Why? Perhaps because it undoes floating inwards, and hence gives less good strictness \nanalysis and hence more updates. How many known-branch trans~ormations are elimi\u00adnated? Both (strict \nand always reduce the number of known-branch transformations. Since this transfor\u00admation is a guaranteed \nwin, this reduction is undesir\u00adable. It is easy to explain why strict offers fewer known-branch opportunities, \nbecause the non-floated bindings may hide a constructor (Section 3.3). We do not yet understand why always \nhas the same effect; one would expect the reverse.  How manyjewer heap-a!tocation checks are performed? \nThe always strategy does indeed clump lets so that we do fewer heap checks, but it is a mere 0.3%  improvement \nover the base case. The downside of the strict strategy is quite a bit worse (14.2~o).  5.4 Overall \nresults Table 6 summarises the total effect of switching all three floating transformations off (using \nstrict as the no local floating case). The total reduction in allocation due to floating is 34.4%, which \nis close to the sum of the gains measured for each transformation separately (6. 1 + 10.9 + 15. 7 =.32.7%). \nThe reduction in instruction count is 16.4% (with measured\u00adbut-fuzzy time savings of 18.770). This, too, \nis not far from the sum of the gains for each transformation independently (0.6+ 7..5 +8.7= 16.6%). We \nhave more than once found that the effects of an optimi\u00adsation are drastically reduced when it is done \nalong with a 10 Resid. Instructions Program Allots. Avg. real/HMMS -TiZZK -1.3% real/anna 32.3% 7,3% \nreal/bspt 33.9% -27.9% real/compress 24.8% -18.5% real/fulsom 55.5% 5.3% real/gamteb 27.7% -8.1% real/gg \n47.9% -41.7% real/hidden 90.2% 25.4% real/hpg 24.7% -12.8% real/infer 66.7% 1.3% real/parser 62.4% 9.3% \nreal/pie 26.7% 2.4% real/reptile 14.7% 0.2% real/rsa 1.3% -1.0% real/symalg 0.2% -0.2% MEAN -3Z-Z% -5.5% \nMIN 0.2% -41.7% MEDIAN 32.3% -0.2% MAX 90.2% 25.4% Table 6: Bottom line: how slew of others, because \nseveral transformations were hit tirm the same targets. In this case, however, the fact that th~ three \nlet-floating transformations add up reasonably well means that they are hitting genuinely different targets. \nWe made some measurements of the effect on compile time of the floating transformations. Generally, compile \ntimes are a few percent worse with no floating at all, presumably because other parts of the compiler \n(such as the code gen\u00aderator) have to work harder. Certainly, none of the floating transformations cause \na noticeable increase in compile time. All these bottom-line figures should be taken with a pinch of \nsalt. Since the rest of the compiler was written in the expectation that at least the more basic let-floating \ntrans\u00adformations were implemented, the figures probably overstate the penalty for turning them off. Related \nwork Using correctness-preserving transformations as a compiler opt imisation is, of course, a well \nestablished technique (Aho, Sethi &#38; Unman [1986]; Bacon, Graham &#38; Sharp [1994]). In the functional \nprogramming area especially the idea of compilation by transformation has received quite a bit of attention \n(Appel [1992]; Fradet &#38; Metayer [1991]; Kelsey [1989]; Kelsey &#38; Hudak [1989]; Kranz [1988]). \nPerhaps because it seems such a modest transformation, however, there are few papers about let-floating, \nexcept in the context of hoisting invariants out of loops. Appel s work on let-hoisting in the context \nof ML is the only substan\u00adtial example we have uncovered (Appel [1992, Chapter S]). He identifies both \nfloating inwards ( hoisting downwards ) and floating outwards ( hoisting upwards ). Because MLis strict, \nthough, floating outwards is only sure to save work if the loop is guaranteed to execute at least once, \nwhich restricts its applicability. The local let-floating transforma\u00adtions are done automatically by \nthe CPS transform be\u00adcause the language is strict all three local strategies coincide. Max. total %mem \n%gc Time .1.2% 7.5% -0.4 +1.8 m 3.8% 12.0% -0.8 44.3 12.5% -34.2% 13.2% +0.6 +1.9 14.3% -18.5% 14.1% \n-0.7 15.2% 10.9% 31.0% -1.8 +2.7 33.6% -7.9% 6.4% +0.1 6.4% -5.5% 19.8% +1.1 +2.1 30.0% 0.1% 10.3% +6.4 \n107.2% -14.470 11.4% +0.6 +1.1 4.4% 3.0% 4.1% +0.1 -2.2 4.2% 10.8% 23.4% -1.2 +6.2 20.0% -9.2% 13.6% \n+1.0 +0.2 33.3% -1.1% 7.6% -0.8 -1.4 14.3% -3.8% 0.1% 0.8%  0.1% 1.4% +0.6 +1.3 2.7%  ..5.2% ZZZ%---+0.3 \n+2.2 m -34.2% 0.1% +0.6 +0.7 0.8% -1.2% 12.0% -+0.1 14.3% 10.9% 110.3% -+2.3 107.2% no floating compares \nwith -O Armel reports some outline results that show instmction\u00ad ..\u00ad count improvements on the order \nof 1 Yo for hoisting down and 270 for hoisting up. 7 Contributions We have described a group of three \nrelated transformations that each attempt to improve the location of let (ret) bind\u00adings in a purely-functional \nprogram. We found it very help\u00adful to identify three independent flavours of let-floating. Our result,s \nsuggest that they really are independent: they aren t just various ways to get the same optimisation \nbenefits. We have measured the effects of the transformations, both on the bottom line and on more insightful \ninternal mea\u00adsures. The improvements we obtain are modest but signifi\u00adcant. Any serious compiler for \na non-strict language should implement (a) local floating to expose values (the less ag\u00adgressive strict \nstrategy has all sorts of unfortunate effects); (b) floating out of constants, The benefits of the other \ntransformations namely floating inwards and complete full-blown full laziness are more modest. One \nlesson that we learned repeatedly is that it is very hard to predict the interactions between transformations. \nA major benefit of performing all these measurements is that they threw up many individual cases where \na usually-useful transformation was counter-productive. Investigating these cases led us to some new \ntransformations, and a consider\u00adable amount of fine-tuning of the existing one. So far as let-floating \ngoes, the net result is fairly good: collectively, the let-floating transformations never increase instruction \nthe count, and seldom do so individually. We are now adding a linear-type inference pass to GHC, to spot \nlambda abstractions that are guaranteed only to be applied once (wadler turner pop195]). Thk information \ncan increase opportunities for floating inwards, and reduce unnecessary floating outwards. It is also \nuseful for other reasons, such as arity expansion and inlining (Gill [1996]). References AV Aho, R Sethi \n&#38; JD Unman [1986], Compilers -principles, techniques and tools, Addison Wesley. AW Appel [1992], \nCompiling with continuations, Cambridge University Press. Z Ariola, M Felleisen, J Maraist, M Odersky \n&#38; P Wadler [Jan 1995], A call by need lambda calculus, in 21st ACM Symposium on Principles of Program\u00adming \nLanguages, San Francisco, ACM. DF Bacon, SL Graham &#38; OJ Sharp [Dee 1994], Compiler transformations \nfor high-performance computing, ACM Computing Surveys 26, 345-420. C Flanagan, A Sabry, B Duba &#38; \nM Felleisen [June 1993], The essence of compiling with continuations, SIG-PLAN Notices 28, 237-247. S \nFortune, D Leivant &#38; M O Donnell [Jan 1983], The ex\u00adpressiveness of simple and second-order type \nstruc\u00adtures, JACM 30, 151-185. P Fradet &#38; D Le Metayer [Jan 1991], Compilation of func\u00adtional languages \nby program transformation, ACM Transactions on Programming Languages and Sys\u00adtems 13, 21 51. AJ Gill \n[Jan 1996], Cheap deforestation for non-strict func\u00adtional languages, PhD thesis, Department of Com\u00adputing \nScience, Glasgow University. P Hudak, SL Peyton Jones, PL Wadler, Arvind, B Boutel, J Fairbairn, J Fasel, \nM Guzman, K Hammond, J Hughes, T Johnsson, R Kieburtz, RS Nikhil, W Partain &#38; J Peterson [May 1992], \nReport on the functional programming language Haskell, Version 1.2, SIGPLAN Notices 27. RJM Hughes [July \n1983], The design and implementation of programming languages, PhD thesis, Program\u00adming Research Group, \nOxford. R Kelsey [May 1989], Compilation by program transforma\u00adtion, YALEU/DCS/RR-702, PhD thesis, Depart\u00adment \nof Computer Science, Yale University. R Kelsey &#38; P Hudak [Jan 1989], Realistic compilation by program \ntransformation) in Proc ACM Confer\u00adence on Principles of Programming Languages, ACM, 281-292. DA Kranz \n[May 1988], ORBIT -an optimizing compiler for Scheme, PhD thesis, Department of Computer Science, Yale \nUniversity. DA Kranz, R Kelsey, J Rees, P Hudak, J Philbin &#38; N Adams [1986], ORBIT -an optimizing \ncompiler for Scheme, in Proc SIGPLAN Symposium on Com\u00adpiler Construction, ACM. J Launchbury [Jan 1993], \nA natural semantics for lazy eval\u00aduation, in 20th ACM Symposium on Principles of Programming Languages, \nCharleston, ACM, 144\u00ad 154. WD Partain [1993], The nofib Benchmark Suite of Haskell Programs, in Functional \nProgramming, Glasgow 1992, J Launchbury &#38; PM Sansom, eds., Work\u00adshops in Computing, Springer Verlag, \n195 202. SL Peyton Jones [1987], The Implementation of Functional Programming Languages, Prentice Hall. \nSL Peyton Jones [Apr 1992], Implementing lazy functional languages on stock hardware: the Spineless Tagless \nG-machine, Journal of Functional Programming 2, 127-202. SL Peyton Jones, CV Hall, K Hammond, WD Partain \n&#38; PL Wadler [March 1993], The Glasgow Haskell com\u00adpiler: a technical overview, in Proceedings of \nJoint Framework for Information Technology Technical Conference, Keele, DTI/SERC, 249-257. SL Peyton \nJones &#38; J Launchbury [Sept 1991], Unboxed values as first class citizens, in Functional Pro\u00adgramming \nLanguages and Computer Architecture, Boston, Hughes, ed., LNCS 523, Springer Verlag, 636-666. SL Peyton \nJones &#38; D Lester [May 1991], llA modular fully\u00adlazy lambda lifter in HASKELL, Software Practice \nand Experience 21, 479 506. SL Peyton Jones &#38; WD Partain [1993], Measuring the ef\u00adfectiveness of \na simple strictness analyser, in Func\u00adtional Programming, Glasgow 1993, K Hammond &#38; JT O Donnell, \neds., Workshops in Computing, Springer Verlag, 201-220. SL Peyton Jones &#38; A Santos [1994], [Compilation \nby trans\u00adformation in the Glasgow Haskell Compiler, in Functional Programming, Glasgow 1994, K Ham\u00admond, \nDN Turner &#38; PM Sansom, eds., Workshops in Computing, Springer Verlag, 184 204. A Santos [Sept 1995], \nCompilation by transformation in non-strict functional languages , PhD thesis, De\u00adpart ment of Computing \nScience, Glasgow Univer\u00adsit y. JE Smith [Ott 1988], (Characterizing computer performance with a single \nnumber, Communications of the ACM 31, 1202-1207.   \n\t\t\t", "proc_id": "232627", "abstract": "Virtually every compiler performs transformations on the program it is compiling in an attempt to improve efficiency. Despite their importance, however, there have been few systematic attempts to categorise such transformations and measure their impact.In this paper we describe a particular group of transformations --- the \"let-floating\" transformations --- and give detailed measurements of their effect in an optimizing compiler for the non-strict functional language Haskell. Let-floating has not received much explicit attention in the past, but our measurements show that it is an important group of transformations (at least for lazy languages), offering a reduction of more than 30% in heap allocation and 15% in execution time.", "authors": [{"name": "Simon Peyton Jones", "author_profile_id": "81100271851", "affiliation": "University of Glasgow", "person_id": "PP14102538", "email_address": "", "orcid_id": ""}, {"name": "Will Partain", "author_profile_id": "81100584820", "affiliation": "University of Glasgow", "person_id": "P298385", "email_address": "", "orcid_id": ""}, {"name": "Andr&#233; Santos", "author_profile_id": "81545693056", "affiliation": "University of Glasgow", "person_id": "P16920", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/232627.232630", "year": "1996", "article_id": "232630", "conference": "ICFP", "title": "Let-floating: moving bindings to give faster programs", "url": "http://dl.acm.org/citation.cfm?id=232630"}