{"article_publication_date": "06-15-1996", "fulltext": "\n Cogen in Six Lines Peter J. Thiemann* Abstract We have designed and implemented a program-generator \ngenerator (PGG) foranuntyped higher-order functional programming language. Theprograrn generators perform \ncontinuation-based multi-level offline specialization and thus combine the most powerful and general \noffline partial evaluation techniques. The correctness of the PGG is ensured by deriving it from a multi-level \nspecialize. Our PGG is extremely simple to implement due to the use of multi-level techniques and higher-order \nabstract syntax. Keywords: partial evaluation, multi-level computation, continuations. Introduction An \nattractive feature of partial evaluation is the abil\u00adity to generate generating etiensions. A generating \nex\u00adtension for a program p with two inputs inp, and inp~ is a program p-gen which accepts the static \ninput inp~ of p and produces a residual program p. which accepts the dynamic input inpd and produces \nthe same result as ~] lnp, lnpd, provided both p and p, terminate. [p-genl inp, = P. ~~] inp~ = result \n[p] rips npd = esult Generating extensions result from applying a program\u00adgenerator generator (PGG) to \np. Customarily, PGGs re\u00adsult from double self-application of a partial evaluator as described by the \nthird Futamura projection [15, 20]. They are often called compiler generators (cog en) or par\u00adtial evaluation \ncompilers (pecom) se they can turn inter\u00adpreters into compilers. We will use pecom for a PGG constructed \nby self-application of a partial evaluator. The interest in generating extensions derives not only from \ntheoretical considerations. Using generating exten\u00adsions speeds up speciaEzation by a factor of three \nto four compared with a generic specialize [4]. Thk is also called the cogen approach to partial evaluation. \nWriting PGGs by hand is a recent trend in partial eval\u00aduation [23, 3, 16]. This technique solves some \ndata repre\u00adsent ation and efficiency problems: Partial evaluation for typed languages suffers from an \nencoding problem because static values and pro\u00adgrams need to be represented as values [22,1, 13]. As \n*Address: Wilhelm-SchMmrd-Institut, Universitat TuVlngen, Sand 13, D-72076 TuWlngen, Germany. E-mail: \nthiemaun@informatik .uni-tueLringen. de Permission to make dgitelhard copy of part or all of thii vmrk \nfor personal or classroom use is rancsd without fee provided that mpiaa are not made or d~tributed \nfor pro IIt or commercial advanta e, the copyright notioe, the tifkof tty publication and ifs date appear, \nan#notke is given that copying IS y permission of ACM, Inc. To oopy otherwise, b republish, to pod \non earvem, or to mdisbiblh to lit% rWllJir8S Prior SP8d~ Wm@On andlor a fee. ICFP 965/ 96 PA, USA CI \n1996 ACM O-69791 -771 -519WOW5...$~.~ a consequence, self-application is impractical, and explicit tagging \nand untagging operations may re\u00ad main in the residual programs. Pecom inherits some inefficiencies from \nthe original specialize due to its interpretive nature. Pecom can only make use of language features \nwhich are dealt with by the specializ~r. -This restriction also applies to the generating extensions. \nWriting the PGG by hand makes all these problems disap\u00adpear: the PGG only handles syntax trees of the \nlan~age, it does not cent ain an interpreter, and all language fea\u00adtures can be freely used inaide the \nPGG as well as in the generating extensions. An important feature of offline partial evaluators is continuation-based \nspecialization [12, 5, 8, 24, 25]. It is a fully automatic binding-time improving transforma\u00adtion that \nfacilitates avoiding duplication of computation, thereby simplifying unfolding strategies and the imple\u00admentation \nof partially static data. We develop multi-level continuation-based specializa\u00adtion and extend the approach \nto the construction of a hand-written multi-level continuation-based PGG. Con\u00adtrary to the usual ad-hoc \nconstruction of PGGs we pro\u00advide a simple derivation of the PGG from a specialize. A key feature in our \nderivation is the transformation to higher-order abstract syntax (HOAS) which considerably simplifies \nthe resulting PGG and the proof of its correct\u00adnew. By exploiting the multi-level features it is possible \nto write the PGG in direct style without using control op\u00aderators. Consequently, multi-level specialization, \nwhich is interesting in its own right, mainly serves to simplify the construction of the PGG. As far \nas we know, multi-level specialization has not been combined with continuation\u00adbased partial evaluation \nto date. Finally, we demonstrate that the techniques presented scale up to the fictional subset of Scheme \nand exhibit remarkable speed-ups compared to an available state-of\u00adthe-art partial evaluator. Overview \nIn Sec. 2, we introduce some notation. Sec\u00adtion 3 presents multi-level partial evaluation and ex\u00adplains \nthe standard approach and the cogen approach. Continuation-based specialization is the subject of Sec. \n4. Section 5 generalizes continuation-baaed reduction to more than two levels and cent tins the derivation \nof PGGs from specializers-all written in continuation\u00adpass.ing style. The following Section 6 performs \nthe same development for direct style PGGs and specializes using control operators. We extend our approach \nto the full Scheme language in Sec. 7. Empirical data on our PGG including performance figures are presented \nin Sec. 8. In Sec. !3 we consider related work. 2 Notational Preliminaries The source and target language \nof our specializes consists ofmulti-level expressionsll. For brevity ssake we restrict the formal presentation \nto the J-mix fragment, E G MEzpr ..  E  vl&#38;vJJl~QiEIMv=~iA E where O ~ i < m and m >1 is the number \nof levels. The set of free variables of E, FV(E) is defined as usual. The metalanguage used in all definitions \nis an enriched, untyped, call-by-value lambda calculus extended with quoting constructs. e E Expr e .. \nw[o(e . . . .. e)l Xv.e[(e e)lletv=eine [ [el [ (e) Expressions e are variables v, applications of built-in \noperators o (including constants), abstractions, appli\u00adcations, and let expressions. Additionally, quoted \n[el and unquoted expressions (e) correspond to Scheme s quasiquote and unquote forms [19]. In the metalanguage \nwe deal with values of the follow\u00ading types. VaT-, variables, Value, everything which is not an MExpr, \n PEVal = MEzpr + Value, specialization-time values, PEEnv = Var d PEVal, specialize environments, PECont \n= PEVal ~ PEVal, specialize continua\u00adtions. For p PEEnv, p[v ~ e] denotes the usual extension of environment \np by the binding v H e. For an expres\u00adsion e c Expr, e[v := e ] denotes substitution of e for v in e. \nVariables in lower case are usually of type Value wherss variables in upper case are oft ype MExpr. We \nlet = denote ~q-equivalence of expressions, fresh variables i#roduced on right sides of definitions are \nmarked with Below, we consider the syntax constructors&#38;, Q, and @ as built-in operators of the metalanguage \nwith types : Var x MExpT + MExpT &#38; Q : MEzpT x MExpT ~ MExpT ~: Var x MExpT x MExpr ~ MExpT 3 Multi-Level \nPartial Evaluation Partial evaluation is a program specialization technique based on aggressive constant \npropagation [11, 20]. In multi-level partial evaluation [16], the execution of a pro\u00adgram is staged in \na finite number m >1 of levels, guided by an initial assignment of binding times O,..., m 1 to the inputs. \nThe smaller the binding time, the earlier the value becomes available. We call values static if they \nare available at level O, otherwise they are dynamic. 3.1 The Standard Approach An offline partial evaluator \nconsists of a binding-time analysis and a reducer. The binding-time analysis trans\u00adforms a subject program \nto an annotated program mark\u00ading every expression with the least level where it can be executed without \nreferring to values that only become available later. The reducer processes the annotated pro\u00adgram and \nthe level-O (static) parts of the input. It pro\u00adduces a residual program by reducing level-O expressions \nand rebuilding dynamic ones with their level decremented by one. Unlike standard two-level partial evaluation, \nthe residual program may still be an annotated program which can be passed to the reducer once more. \nLet bta be the binding-time analysis, red the static re\u00ad ducer, p the subject program, inpi the input \nat level i, result the final result, b the initial binding-time assign\u00adment, and m > 1 the number of \nlevels. Specialization of an m-level program works as follows: p-anno = [bta] p b p-annl = [red] p-arm, \ninpo ... = [red] p-annm-, inP~-z p-a~m. 1 result = ~-annm.l] inpm-l In the final step, p-annm_l is a \nlevel-O program and hence executable with the standard semantics. The mix equation the correctness criterion \nfor the specializer generalizes to ~] inpo... inpm_l = [[red] . . . ([red] p-anno inpo) . . . inpm_,] \ninpm_l.  3.2 The Cogen Approach As opposed to using a standard partial evaluator directly, constructing \na generating extension first and performing the specialization proper with the generating extension results \nin three to four times faster specialization [4]. We can get the benefit of using generating extensions \nhere by constructing a generating extension red-gen for red with a standard two-level pecorn. From the \nmix equation and the third Futamura projection, we find p-anni+l = [red] p-am+ inpi = [[red-gen] p-ann$] \ninpj. Using red-gen, the above specialization pipeline is shown in Fig. 1. However, the binding-time-annotated \nprogram p-arm is already a generating extension p-gen given a suitable interpretation of the annotations \n[17, 16]. This effectively removes every second step in the red-gen spe\u00adcialization pipeline. That is, \np-anni+l = ~-anui] inp, This observation has been exploited by Gluck and J@gensen [16] who present a \nlibrary of functions to inter\u00adpret multi-level annotated programs as multi-level gener\u00adating extensions. \nWe extend their approach by integrat\u00ading it with continuation-based partial evaluation [5, 24], once \nin continuation-passing st yle and once in direct style with control operators. 4 Continuation-Based \nReduction Context mo~a~ation is an important binding-time im\u00adproveme&#38;. ?h; standard ex&#38;ple is \n(let z= din 17)+ 4 p-anno = [bta] p b p-anrq = ~-gene] inpo . . . p-annm\u00ad 1 = ~-genm.,] inpm-, I Figure \n1: Multi-level with dynamic d. A naive specialize regards the entire expression (let x = d in 17) as \ndynamic and does not reduce the above expression. Continuation-based reduction [5, 24] propagates the \nstatic context [ ] + 4 over the dynamic let and the ad\u00addition can be performed at specialization time. \nNow the residual code is let z= din 21. Much of the importance of this kind of context prop\u00adagation \noriginates from the fact that dynamic lets are indispensable to avoid duplication of computation. Of\u00adten, \nthese lets are automatically inserted around function bodies and arguments of partially static data constructors \nby the specialize [7]. Therefore, continuation-based re\u00adduction is a vital feature for offline partiaJ \nevaluators. It must be noted, that the basic idea of this kind of context propagation is already present \nin Danvy and Fil\u00adinski s work on a one-pass transformation to continuation\u00adpassing style (CPS) [12]. \nIn fact, their CPS transforma\u00adtion is really a continuation-based reducer processing a hard-wired CPS \ninterpreter. In other words, it is a gener\u00adating extension performing continuation-based reduction. As \na running example, we will consider specializing the source expression Al~.(Ml ~ = @lb a (&#38;z.z))Qz \n where a and b are dynamic variables. The static context [ ]K&#38;z must be propagated inside of the \ndynamic let in order to fzwilit ate any reduction. 5 Multi-level Continuation-Based Reduction In order \nto generalize continuation-based reduction to more than two levels it is only necessary to distinguish \nbetween levels >0 (dynamic) and level O (static). This is the only plaxe where dynamic parts can corrupt \nstatic contexts. The reducer (see Fig. 2) is easily adapted from published accounts [5, 20]. The result \nfrom specializing the example is RCIA,Z.(M1 w = aQlb Q (.&#38;z. z))K&#38;z] [a I+ a, b I+ b](Az.z) = \n&#38;z.(ko v = aQob h z). 5.1 Introducing Higher-Order Abstract Syntax We aim at interpreting a multi-level \nannotated program as a program generator [17, 16]. In order to find the inter\u00adpretations of the syntax \nconstructors&#38;, Q, and ~i we derive their defining equations from the specialize shown in Fig. 2. \nIn a first (trivially correct) transformation we simply transfer the abstraction of the continuation \nK from the left side to the right side of the equations, as illus\u00adtrated by 7Zc[v]/1 = AK. K(p[v]). p-geno \n= [red-gen] p-enno p-genl = [red-gen] p-annl re suit = Up-annm-l] inpm-l Specialization Pipeline Now \nwe almost have a definition of the necessary opera\u00adtors. Only the environment p stands in the way-of \nmak\u00ading it a stand-alone definition. Therefore, we change the way the abstract syntax is presented from \nfirst-order to higher-order. Higher-order abstract syntax (HOAS) [28] is a way to transfer the burden \nto correctly handle bindings, scoping, substitution, and so on to the metalevel. Using HOAS, the binding \noccurrences of variables vanish and after an intermediate step---we can use met avariables to represent \nvariables. Using MEq# as type for a term in HOAS, we introduce the syntax constructors: &#38; : (MEzpt \n~ MEw+) + MExpr Cl : MEw+ X MExp~ d MExp# =i U; : MEqd x (MEW# ~ MEq#) -+ MEzpd We can freely transform \nfrom HOAS and back (up to vari\u00adable names) using straightforward functions @ : MEzpT ~ MExp~ and III \n: MExpr ~ MExpr. @F] = &#38;.v .WIFvO] lJIE@iEZ] = W[E1]Q1V[E2] T[~ E&#38; F] = ~i VO=W[E] &#38; VIFv \n] We extend the above mappings@ and W compatible to the met alanguage, i.e. application of @ to e c \nExpr changes the syntax constructors which appear in e from MExpr to MExp~. By induction on multi-level \nterms we can prove the obvious connection between first-order and higher\u00adorder abstract syntax: Lemma \n1 1. @(VE) = E for all E G MExpr and 2. W(@E) = E for all E G MExpr. Transforming our example to HOAS \nyields ~lkc.(~l aQ1b &#38; Av.(&#38;Az.z))Q&#38;z. Writing the reducer R; [ ] for higher-order abstract \nsyn\u00adtax and with the continuation abstraction transferred to the right sides we need to make explicit \nthe variable con\u00adstructor var : MEqn- G MEzp~ which serves to stop the reducer from processing variable \nbindings. The resulting reducer is shown in Fig. 3. Our example is transformed M follows: Zc : MExpr \n+ PEEnv -PECont + PEVal % [v]pK = /c(p[v]) 7q&#38;v.E]pK 7?. ElQ~E2]pN I._ %?Clet. v = El 73&#38;+lV.E]pK \n73.[E@?i+lEZ]pK 7Zc[&#38;?&#38;+1 v = Figure 3: Multi-level Continuation-Based Reducer The following \nlemma captures the correspondence be\u00ad tween the reducers Appendix A. Lemma 2 Let E [5/, and p = [vi FV(E) \n~ {Vi,...,%}. K@(E ) = @(K(VE )) @(Rc [E]ptc) simple and ation from evaluation F(AK.KE). To obtain constructors \ncounterparts = = * E2]pK = = = 1!?] h E2]pK =  K(Ay.7Lc[E]p[v 1+ g]) %[J% p(hfl .%[~z]p(%z.(111 7?C[E1 \np(Ay.7?c[E2]p[v * y]tt) /c(&#38;v .7?@]p[v H V ](hz)) nc[El]p(AE;.%[E2jP(AEi.K(E;QiEi))) 7qEl]p(Ml{.Mi \nv = E{ Q 1 Figure 2: Multi-level Continuation-Based Reducer 7?: [-] smd %?,c~]. Its proof is found \nin G MExpr, n G PECont I+ E, I1~i~ Let further n+ for all 1? c MEzp~. well-behaved m] such that be defined \nby Then = R:[4@)[vi Roughly, a specialization-time behaved [5] if it coincides with tinuation and does \nnot capture the source program. All continuations introduced by our continuation-based reducers are \nwell-behaved. 5.2 A Cogen for Multi-Level Continuation-Based Ra\u00adduction Only one thing in the definition \nstill keeps us from replac\u00ading R; ~ with standard evaluation and considering the equations as combinator \ndefinitions A variable v is not simply interpreted as an environment lookup but as a con\u00adtinuation returning \nthe value: JK.KV. However, there s a correct fix Bind the variable to the continu\u00adthe beginning. In the \ntransition to standard we have to replace every occurrence of FE by And that is all! a generating extension \nwe make the HOAS executable. The and their defining Fig. 4, using q = AX.AMCX and to R: [-] is stated \nas follows. Lemma 3 For all E ~ MExp#, 7?; [E[v~ := Ej]]N = := @(EJ)]]lc* . continuation is well\u00ad an \nevaluation-time con\u00ad any variable bindings of types of their executable equations are specified in b \n= Az.z. The relationship K ~ PECont: (E[vi := qE~])K. 183 The proof is found Lemma 3 together just \nconstructed. Theorem 1 Let E and p= [vi I-) Ei {v,,... ,Vm}. Let @(K(W(E ))) for all ?42) ~)) 7?.[E2]p[v \n1+ Vqt$) for HOAS in Appendix B. Taking Lemma 2 and we obtain the correctness of the PGG G MEzpT, K \nG PECont well-behaved, I1< i < m] such that FV(E) ~ further K* be dejined by KG (E ) = E G MEzpr- . l \nhen 4@c[E]p/c) = @( E)[vi := q(@(Ej))]no. To complete our running example, we can now write down the \nprogram generator which is the generating ex\u00adtension for our example. ~, ~~.(~l @,~ &#38; ~v.(Qz.z))go~ \n It is actually just the ning the program with multi-level the program interpretation in HOAS. Run\u00adin \nFig. 4 yielcb &#38;~x.~ aQb h Av.x 6 Using Control Op erators Lawall and Danvy continuation-based reing \ncontrol operators. [24] duction In show how in direct an extension to style of implement (DS) us\u00adthat \nwork, Lawall and Danvy [25] present the first hand-written PGG performing continuation-based reduction \nin direct style. Both exploit the control operators shitl and reset [12]. shift ad reset provide composable \ncontinuations: reset delimits a continuation and shift abstracts the con\u00adtinuation up to the innermost \nlexically enclosing reset. A continuation abstracted with shift is composed with the current continuation, \nin contrast to continuations pro\u00advided by call/cc which dwcard the current continuation. ~ : Level x \n(PECont + PECont) + PECont ~ PEVal Q: Level x PECont x PECont + PECont + PEVal iet : Level x PECont x \n(PECont + PECont) + PECont + PEVal = &#38;(f) = Afdly.j(qy) @lo(El , E2) = k.EI (Ael.E2(Ae2.(e] e2)fi)) \n&#38;to(El , f) = Mf..l?l (Ae.f(~e)K) ~j+, (f ) = AWQ,. Qw .(f (VJ )d)l Q+I(E1, E2) = AK. Ei(AEj.EZ(AEj.K \n[~{i) ((E{), (J%))l)) ~,+l(EI> f) = ~~.~l(~~;.[~{,) ((~{ )>~~o.(f (w h))l) FiKure 4: Multi-level Continuation-Based \nPGG I % : MExrw + PEEnv + PEVal I Figure 5: Multi-level Continuation-Based Reducer in Direct Style We \nprovide the standard definition in terms of a CPS we would simply use shift i and reset ~ in the equation \ntransformation T in Appendix C. Using them we can for ~ and the highest level reset in all equations \nfor ~ write the multi-level reducer %d in DS as shown in Fig. 5. we would effective] y exchange let binders. \nFor example, R= and ~d are related by the CPS transformation. shiftz in the equation for ~i (for i ~ \n2) would move Lemma 4 ~[~d[.]] = ~. [.]. let binders outside of ~1 binders. As such a trans\u00ad -. formation \ncan break variable bindings we conclude that If we again make the transition to HOAS, as demon\u00adshiftz \nis not applicable in general. strated for the CPS version, the result has an almost immediate interpretation \nas a PGG. The final result is 6.2 Why the PGG need not tamper with controlshown in Fig. 6. Again, the \ntwo variant interpretations of the HOAS constructors are related via the CPS trans- In other approaches \ndealing with PGGs for continuation\u00adformation. For the DS generator we have the following baaed specialization \n[8, 25], the PGG itself needs to ma\u00adconvenient result which is proved by CPS-transforming it nipulate \ncontrol. This way, more static reductions can be to Theorem 1. performed at generation time. However, \nthe additional Lemma 5 FOT all E MExpT: complexity is considerable. In our approach, multi-level specialization \ndoes the job: The PGG generates the pro\u00ad ~d[E][v4 x Ei] = @( E)[v; := CP(Ei)]. gram with every level \nannotation incremented by one. A first generation run without any static input performs the 6.1 Higher-Level \nshift and reset? generation-time reductions and yields the program gen\u00ad erator ready to receive the static \ninputs of the first level. As shift and reset can be generalized to abstract more than one context, it \nis natural to ask whether multi-level 7 Implementation shift and resets may be used to advantage in multi-level \ngenerating extensions. The development presented in the preceding sections ex- In a program generated \nby continuation-based special\u00adtends smoothly to the full effect-free subset of Scheme. ization no subexpressions \nof the forms (let v = e in e) @e We illustrate this by giving some samples from our li\u00adand e@(let v = \ne in e) arise. All let expressions aggre\u00adbrary. In all the example functions, the parameter lV gate inside \nof lambdas and on the outermost level, i.e. denotes the level argument. residual expressions have the \nform N where N ::= N llet. v=N~N 7.1 PGG library functions in CPS N ::= v I N QN I&#38;.N Application \nThe library code for function application However, the nesting order of dynamic let expressions accepts \na function continuation f c and a list of argument is not changed with respect to the subject program. \nIf continuations A* c. A : Level x (MExpr + MEzpr) + MExpr g : Level x MExpr x MExpr + MExpr ~: Level \nx MExpr x (MExpr + MExpr) -+ MExpr  * = &#38;f J ~(el,ez) = el ez ~(e, f) =fe ~i+l f = (~(i)~vo.(reset(fvo))l \n~+l(el, ez) = [~(i.((cl), (e2))l (e, f) = shift k. [~{.) ((e), Av .(reset (Ic( fv ))))l 4%+1 Figure \n6: Multi-level Continuation-Based PGG in Direct Style (define (-app Iv fc . A*c) (lambda (kappa) (fc \n(lambda (f) (process-arg-list A*c (lambda (A*) (if (zero? lv) ((apply f A*) kappa) (kappa (-APP ,(-lV \n1) ,f ,@A*))))))))) It makes use of another library function process-arg-list which processes the list \nof argu\u00adment continuations. Lambda Abstraction Lambda abstraction accepts the arity arity of the lambda \n(a list of symbols in order to generate readable residual programs) and a function f which accepts the \nlist of abstracted variables and pro\u00adduces a continuation which constructs the body of the lambda. (define \n(-lambda lV srity f) (lambda (kappa) (let* ((V* (map gensym srity)) (V*C (map result V*)) (f~ (LMBDA \n,W ,((applyf V*c) id)))) (if (=lV 1) (kappa fun) (kappa (-LAMBDA ,(-lV 1) ,arity ,fun)))))) The function \nresult is q = AX. AK.KX and id is the iden\u00adtity function. Let Expression Aletexpression accepts thelet \nheader expreeeion continuation e and a function f with the same meaning as for lambda abstractions. (define \n(-let lV e f) (lambda (kappa) (let* ((V (gensym fresh) ) (Vc (result v))) (e (lambda (value) (cond ((zero? \nlv) ((f (result value)) kappa)) (else (-LET ,(-lV 1) ,vslue (LAMBDA (,v) ,((fVc) kappa)))))))))) Primitive \nOperators Theinterpretation ofprimitiveop\u00aderators accepts the operator op (a symbol, if lV > O; otherwise \nthe operator itself) and a list A*c of argument continuations. (define (-op lV op . A*c) (lambda (kappa) \n(procese-arg-list A*c (lambda (A*) (kappa (cond ((zero? lv) (apply op A*)) ((= 1 lV) (-OP O sop ,@A*)) \n(else (-Op ,(-lV 1) ,op ,0A*)))))))) Conditional The conditional accepts three continua\u00adtions. (define \n(-if lV elc e2c e3c) (lambda (kappa) (elc (lambda (cl) (if (zero? lv) (if el (e2c kappa) (e3c kappa)) \n(-IF ,(-lV 1) ,el ,(e2c kappa) ,(e3c kappa)))))))  7.2 PGGlibrary functionsin DS As expected, the library \nfunctions presented below get even simpler in DS. However, there is a slight complica\u00adtion. Intheimplementation \nof theconditionzd we require that thespecializer controls the continuation. Thesolu\u00adtion is discussed \nin the respective paragraph below. Application This function ie identical to theimplemen\u00adtationof Gluckand \nJorgensen [16]. Itisoxdy included for completeness. [define (-app lv f . args) (if (=lv 1) (,f ,@args) \n(-APP ,(-Iv 1) ,f ,@args))) Lambda Abstraction Here, wearefortunate because the higher-order nature \nof the abstract syntax gives us control over the continuation of the specialization of the lambda s body. \nAs the specialization is wrapped in the function f (which accepts a list of variables and returns the \nspecial\u00adized body) we can easily wrap a reset around its execu\u00adtion. (define (-lambda lV arity f) (let* \n( (vars (map gensym arity) ) (fun (LAMBDA , vars ,(reset (apply f vars))))) (if (=lV 1) fun (-LAMBDA \n,(-lV 1) ,arity ,fun)))) Let Expression The same situation arises as in the pre\u00advious case. Due to HOAS \nwe are in control of the body s specialization continuation. (define (_let lV e f) (let ((var (gensyrn \nfresh))) (cond ((zero? lv) (f e)) (else (shift k (_LET ,(-lV D ,e (LAMBDA (,var) ,(reset (k (f van)))))))))) \n Primitive Operators Taken literally from [16]. (define (-OP lV Op . args) (if (=lv 1) (jop ,Wrgs) (-OP \n,(-lV 1) ,op ,@args))) Conditional At the conditional, the specializer mustbe able to control the continuations \nof the then and else branches. This is trivial to achieve in the CPS version. In DS it requires to change \nthe abstract syntax so that the branches of the conditional arethunks returning their specialized code. \nThis way, we retain control over their specialization continuation and can insert the appropriate resets. \n(define (-if lV el et2 et3) (if (=lV 1) (IF ,el ,(reset (et2)) ,(reset (et3))) f(JF ,(-IV 1) ,e (L;~DA \n() ,(reset (etZ))) (LAMBDA () ,(reset (et3)))))) Results Here, we report some figures of our implementation. \nIt hamdles the functional subset of Scheme augmented with user-defined (partially static) data structures. \nIt consists ola preprocessing phase, aconstraint-based binding-time analysis [9], themulti-level code \ngeneration, and the run\u00adtime libraries in CPS and in DS. Therun-time libraries are constructed as outlinedin \nSections 5 and 6. The DS version uses Filinski s implementation of shift and reset [14]. Figure 7showsa \nsizesummary of theprogram. In order to substantiate the claim that hand-written PGGs are more efficient \nwe have performed a comparison with Similix [6]. We have used three subject programs app (standard list \nappend), ctors (testing partially static data), and lambda (partially static functions). Alltimes aremean \nvalues over 100runsonan IBM 320 with 32MB main memory using Scheme48 [21]. Timings are also shown in \nFig. 7. The construction of the gen\u00aderation extension with our PGG is almost an order of magnitude faster. \nThe PGG constructed generating ex\u00adtensions in direct style me faster by a factor of two to five, the \nCPS versions by a factor of one to six. Furthermore, we have performed another comparison using realistic \napplications. We have applied Similix and our DS PGG to generate LR parsers (h-pars) [31] and online \ncompiler generators (ocogen) [30]. The results are presented in Fig. 8. The time differences with respect \nto Similix are partly due to two facts: There are additional static analysis phases (which add to the \npreprocessing time) and there is a sophisticated postprocessing pass (which adds to the specialization \ntime). 9 Related Work Context propagation for two-level programs can be achieved in se;e;al ways. The \nbasic idea appears in Danvy and Filinski s work on a one-pass CPS transformation [12]. Consel and Davy \n[10] suggest to transform the subject program into CPS, Bondorf [5] writes the special\u00adize in non-standard \nCPS in essentizdly the same style as [12] , and Lawall and Danvy [24] use control operators in the specialize. \nHand-written PGGs are reported in several papers for a variety of languages [23, 2, 3]. The context propagation \nideas can be applied to hand-written PGGs: Bondorf and Dussart [8] construct and prove correct a hand-written \ncontinuation-based PGG in CPS, Lawall and Danvy [25] present several continuation-based PGGs in direct \nstyle. None of them considers multi-level specialization. Gluck and J@rgensen [16] consider PGGs for \nmulti-level specialization, but they do not cover continuation-based specialization. Their implementation \nis also library-based but the library functions are constructed in an ad-hoc manner. Higher-order abstract \nsyntax has been reported by Pfenning and Elliot [28] as a tool to alleviate program transformations from \nexplicitly dealing with bindings, scoping, and substitution issues. Mogensen has used it to write self-interpreters \nand self-applicable online par\u00ad tial evaluators for the lambda calculus [26, 27]. Our work is another \nindication of the power of higher-order abstract syntax in program transformations. 10 Conclusion We \nhave presented the first PGG for multi-level continuation-based specialization. We have shown that continuation-based \nspecialization extends smoothly to multi-level specialization. Using our calculational ap\u00adproach it is \neasy to derive a PGG from a specialize. Compared to previous presentations, ours is much sire\u00adpler and \nyields the same results. program part I lines I bytes I program I app I Ctors I lambda auxiliary definitions \n462 13543 Similix bindirur-time analvais 545 I 1156 i 1195 PGG ~onstructio~ 1601 4870 6113 specitilzat \nion 57 202 182 our PGG binding-time malysis 160 470 427 PGG construction (DS) 48 97 121 specialization \n(DS) 30 37 109 PGG construction (CPS) 62 127 147 .wwxialization (CPS\\ 61 93 34 Figure7: Sizes andcomparative \nruntimee (inms)ofour PGG program I size preprocess construct generating extension specialize lrpars I \n793 15.95 87.69 84.55 G 11.81 1.71 35.42 ocogen I 2311 57.57 304.01 132.85 G: 33.72 3.94 34.86 all sizes \nin cons cells, times in seconds (first line Similix, second line PGG) Figure8: Realistic Examples We \nbelieve that it issimpler to write PGGs by hand than writing self-applicable specializes, as we do not \nhave to be wary of binding times while we construct the PGG and as we can freely exploit all features \nof the language both in the PGG and in the generated generating exten\u00adsions. It is also beneficial from \nan efficiency standpoint. Acknowledgements Thanks are due to Michael Sperber for reading drafts of this \npaper and to the referees for their valuable comments. References [1] L. O. Andersen. Self-applicable \nC program special\u00adization. In C. Coneel, editor, Workshop Partial Eval\u00aduation and Semantics-Based Program \nManipulation 92, pagee 54-61, San Francisco, CA, June 1992. Yale University. Report YALEU/DCS/RR-909. \n [2] L. O. Andersen. Program Analysis and Spmializa\u00adtion for the C Programming Language. PhD thesis, \nDIKU, University of Copenhagen, Dept. of Com\u00adputer Science, University of Copenhagen, Univer\u00adsitetsparken \n1, DK-21OO K@benhavn 0, May 1994. [3] L. Birkedal and M. Welinder. Partial evaluation of Standard ML. \nRapport 93/22, DIKU, University of Copenhagen, 1993. [4] A. Bondorf. Automatic autoprojection of higher \nor\u00adder recursive equations. Science of Programming, 17:3-34, 1991. [5] A. Bondorf. Improving binding-times \nwithout ex\u00adplicit CPS conversion. In Proc. Conference on Lisp and Functional Programming, pages 1 10, \nSan Fran\u00adcisco, CA, USA, June 1992. [6] A. Bondorf. Simdiz 5.0 Manual. DIKU, University of Copenhagen, \nMay 1993. [7] A. Bondorf and O. Danvy. Automatic autopro\u00adjection of recursive equations with global variables \nand abstract data types. Science of Programming, 19(2):151-195, 1991. [8] A. Bondorf and D. Duesart. \nImproving CPS-based partial evaluation Writing cogen by hand. In P. Ses\u00adtoft and H. Sondergaard, editors, \nWor-kshop Partial Evaluation and Semantics-Based Program Manipula\u00adtion 94, pages 1-10, Orlando, Fla., \nJune 1994. ACM. [9] A. Bondorf and J. J@rgensen. Efficient analysis for realistic off-line partial evaluation. \nJournal of i-lmc\u00adtional Programming, 3(3):315 346, July 1993. [10] C. Consel and O. Danvy. For a better \nsupport of static data flow. In Hughes [18], pagea 496 519. LNCS 523. [11] C. Consel and O. Danvy. Tutorial \nnotes on par\u00adtial evaluation. In Proc. 20th ACM Symposium on Principles of Programming Languages, pages \n493\u00ad501, Charleston, South Carolina, Jan. 1993. ACM Press. [12] O. Danvy and A. Filinski. Abstracting \ncontrol. In Proc. Conference on Lasp and Functional Program\u00adming, pages 151 160, Nice, France, 1990. \nACM. [13] A. de Niel. Self-Applicable Partial Evaluation of PolymorphicaUy Typed Functional Languages. \nPhD thesis, Katholieke Univereiteit Leuven, Leuven, Bel\u00adgium, Jan. 1993. [14] A. Filinski. Representing \nmonads. In Proc. 21st ACM Symposium on Principles of Programming Languages, pages 446457, Portland, OG, \nJan. 1994. ACM Press. [15] Y. Futarnura. Partial evaluation of computation pro\u00adcess an approach to a \ncompiler-compiler. Systems, Computers, Controls, 2(5):45-50, 1971. [16] [17] [18] [19] [20] [21] [22] \n[23] [24] [25] [26] [27] [28] [29] [30] [31] R. Gluck and J. J@gensen. Efficient multi-level gen-A Proof \nof Lemma 2 crating extensions for program specialization. In D. %vierstra and M. Hermenegildo, editors, \nPro-We prove by induction on E that gramming Languages, Implemen~ations, Logi&#38;, and @(7?c[E]prc) \n= ~j[@(E)[vi := @( Ei)]]K*) Programs (PLILP 95), Utrecht, The Netherlands, Sept. 1995. Springer-Verlag. \nwhere fi (l? ) = @(K(W(E ))) for all E E A4Ezp#. C. K. Hoist. Syntactic currying. student report, case \nvi DIKU, 1989. qnc[wj](m) . definition of%&#38;J. Hughes, editor. Functional Programming Lan\u00ad qft(p[vj])) \nguages and Computer Architedtn-e, Cambridge, MA, = assumption on p1991. Springer-Verlag. LNCS 523. @(~Ej) \n Institute of Electrical and Electronic Engineers, Inc. = definition of tsa IEEE standard for the Scheme \nprogramming lan-/t*(@(Ej)) guage. IEEE Std 1178-1990, New York, NY, 1991. = definition of 7Z~ R!-[V~ \n(@Ej)jK* N. D. Jones, C. K. Gomard, and P. Sestoft. Par\u00ad= substitution tial Evaluation and Automatic \nProgram Generation. R!-[(V~ ?Jj )[Vi := @J?7j]]tC Prentice Hall, 1993. = definition of 7?: R: [(@~j)[~i \n:= @Ej]]K* R. A. Kelsey and J. Rees. A tradable Scheme implementation. Lisp and Symbolic Computation, \n7(4):315-335, 1994. case El% E2 7?:[@(ElQ!oE2)[. . .]]/c* J. Launchbury. A strongly-typed self-applicable \npar\u00addefinition of+tiid evaluator. In Hughes [18], pages 145-164. LNCS W(CWEl)g@(E2))[.. .]]K* 523. substitution \n J. Launchbury and C. K. Hoist. Handwriting co-72:[@(E,)[. . .]~@(E,)[. . .]J# gen to avoid problems \nwith static typing. In Draft definition of Z: Proceedings, Fourth Annual Glasgow Workshop on 7?:[@(E,)[. \n. .]](AY1.RL[*(E2)[. . .]](AI/2.v1v2K*)) Functional Programming, pages 210-218, Skye, Scot\u00adinduction \nhypothesis land, 1991. Glasgow University. @(Rc[El]p(Avl.m(R: [@(E2)[. . .J](Av2.*(Yl)v2 fi@)))) induction \nhypothesisJ. Lawall and O. Danvy. Continuation-based partial @(RCIE,]p(AyI .V(@%[(E2)]P(AV2 .~(@(91 )@ \n(IJ2)~*)))))evaluation. In Proc. Conference on Lisp and Func\u00adcancellation W*tional Programming, pages \n227 238, Orlando, Fla, @(7Zc[E,]p(Ayl.7&#38; [(E2)]p(Av2.1q* (Yl)@(v )fi*)))) USA, June 1994. ACM Press. \ndefinition of # J. LawaU and O. Danvy. Continuation-based @(~@l]P(&#38;A .Rc [(E2)]P(AI/2.V(@ (V1V2tt))))) \n partial evaluation. Extended version of [24] from cancellation W* ftp://ftp.daimi. aau.dk/pub/danvy \n/Papers/, Jan. @(zc[E1]p(Avl.Rc [( E2)]P(Av2.v1v2K))) 1995. definition of R, T. ill. Mogensen. Efficient \nself-interpretation in lambda cal&#38;lus. Journal of Functional Program\u00ad case let .+I v= EI &#38; E2 \nrnang, 2(3):345-364, July 1992. ??;[@(@+l v = El .&#38; E2)[. . .]]K* T. Al. Mogensen. Self-applicable \nonline partial evalu-= substitution and definition of @ ation of pure lambda calculus. In Scherlis [29], \npages R;&#38;+l f@)[. . .] ~ Av.@(E2)[. .]]/c* 39-44. = definition of l?: Ii?: [@(El )[. . .]](AE; .=; \nE; &#38;  F. Pfenning and C. Elliot. Higher-order abstract syn\u00adtax. In Proc. Conference on Programming \nLanguage kJO.~: [(Av.@(E2)[. . .])vO]~@) Design and Implementation 88, pages 199-208, At-= induction, \n~ lanta, July 1988. ACM. @@@l]p(AE;.@(~i @(@) Q Av0.7?~[@(E2)[. . . ,V := v0]]6*)))W. Scherlis, editor. \nACM SIGPLAN Symp. Partial = induction Evaluation and Semantics-Bimed Program Manipu\u00ad @(7?C[EI]P(AE;. \nW(Uti *(E{) ~ lation 95, La Jolla, CA, June 1995. ACM Press. Avo.@(7u[(E2)]p[v * ?P]tt)))) definition \nof@ and cancellation *IIM. Sperber. Self-applicable online partizd evaluation. = @(Rc~El]p(XE{.~i V \n= E; &#38; Submitted for publication, Dec. 1995. 7?c[(E2)]p[v I-) V ]K)) definition of Z&#38; M. Sperber \nand P. Thiernann. The eeeence of LR = parsing. In Scherlis [29], pages 146-155. +(%[klj+, v = EI &#38;L \nE2]PH) The remaining cases are proved similarly. case &#38;i+lAv.E B Proof of Lemma3 We prove by induction \non E: 7?j[E[vi := Ei]]/c = (E[wi := @i])K case Vj %l~[var vj[v~ := E~]]rc . substitution RL[var Ej]fi \n= definition oflZ~ ~Ej = @expansion (A~.~Ej)~ = definition ofq and substitution (vj[v~ := qE,])K case \n&#38; Av.E 7?; [(QwE)[. . .]]K definition of l?; /@y.7L;[((Av.E) I/)[. . .]]) q expansion K(Ay.AK.7?; \n[(( Av.E)y)[. . .]]K) ~ reduction K(AILAK.7?;[E[. . . . V := v]]K) induction K(&#38;/.kE[. . . . V := \nW]K) 8 /&#38;k. (( Av.E)(qy)[. . .])/s) 2(Ay.((Av.E)(qy)[. . .])) @ &#38;&#38;;$~.E)(qy)[. . .])K (&#38;@.E))[. \n. . K + 7?~[(EIQ.oE,)[. . .]]Ic substitution = 73~[(EI [. . .])f&#38;(B2[. . .])]Ic = definition of l?,: \n7?~[E, [. . .]](AY1.R:[E2[. . .]](AY2.Y1v2K)) induction = El [. . .](Jgl .E2[. . .](~Y@1v2~)) = substitution \nand definition of Cl =0 (go(E1, E2))[.. .]K 7?; [(~+1hLE)[. . .]]ti substitution 7?:Qi+1 (AV.E)[. . .]]6 \ndefinition of l?; K(@0.7Z;[(Av.E)[. . .]v*](Az.z)) P K(J;AV*.7?:[E[. . . . v := V*]](AZ.Z)) induction \nK(~jAvO.E[. . . ,V := qv ](k.z)) interpreting ~i K[&#38;AvO.(E[. .., 0 := qv ](~z.z))l f-l @vO.((Av. \nE)[. . .](qv )(~z.z))l definition of ~i (A.+, (Av.E)[. . ~j~ substitution (A.+l (AV.E))[. . .]K The \nremaining cases are proved similarly. C Definition of shift and reset The control operators shit% and \nreset are defined through an extended CPS transformation [12] derived from Plotkin s call-by-value CPS \ntransformation. reset executes its argument in a fresh context. shift captures (and abandons) the context \nup to the next lexically en\u00adclosing reset and abstracts it as a function. T[v] = k.frv TQJv.e] = k. fAv. \nT[ej T[el e2] = ht.elAvl .e2Au2.vlv2K T[reset e] = k.tc( T[e]Az.z) T[shift j.e] = k.T[e]~ := Av.k .K \n(Kv)](k.z)    \n\t\t\t", "proc_id": "232627", "abstract": "We have designed and implemented a program-generator generator (PGG) for an untyped higher-order functional programming language. The program generators perform continuation-based multi-level offline specialization and thus combine the most powerful and general offline partial evaluation techniques. The correctness of the PGG is ensured by deriving it from a multi-level specialize. Our PGG is extremely simple to implement due to the use of multi-level techniques and higher-order abstract syntax.", "authors": [{"name": "Peter J. Thiemann", "author_profile_id": "81100458917", "affiliation": "Wilhelm-Schickard-Institut, Universit&#228;t, T&#252;bingen, Sand 13, D-72076 T&#252;bingen, Germany", "person_id": "P225398", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/232627.232647", "year": "1996", "article_id": "232647", "conference": "ICFP", "title": "Cogen in six lines", "url": "http://dl.acm.org/citation.cfm?id=232647"}