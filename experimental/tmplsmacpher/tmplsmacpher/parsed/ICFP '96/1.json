{"article_publication_date": "06-15-1996", "fulltext": "\n A Reflection on Call-by-Value Amr Sabry Philip Wadler University of Oregon University of Glasgow Eugene, \nOR 97403 Glasgow G12 8QQ, Scotland sabry@cs. uoregon. edu wadler~dcs.glasgow .ac.uk Abstract A number \nof compilers exploit the following strat\u00ad egy: translate a term to continuation-passing style (CPS) and \noptimize the resulting term using a se\u00adquence of reductions. Recent work suggests that an alternative \nstrategy issuperior: optimize directlyin an extended source calculus. We suggest that the appropriate \nrelation between the source and tar\u00adget calculi may be captured by a special case of a Galois connection \nknown as a reflection. Pre\u00advious work has focused on the weaker notion ofan equational correspondence, \nwhich is based inequal\u00ad ity rather than reduction. We show that Moggi s monad translation and Plotkin \ns CPS translation can both be regarded as reflections, and thereby strengthen a number of results in \nthe literature. 1 Introduction Like the wheel, continuation-passing style, or CPS, is such an excellent \nidea that it was reinvented many times (Reynolds 1993). Subsequently it was formalized by Plotkin (1975), \nand utilized in com\u00ad pilers for higher-order call-by-value languages writ\u00ad ten by Steele (1978), Kranz \net al. (1986), Appel (1992), and others. The front end for all these compilers is similar: translate \na term to CPS and optimize. In symbols, M -T P, (1) where &#38;l is a term of the source language S, \nP is a term of the target language T (terms in CPS), * : S + T is the compiling transform (the CPS translation), \nand -T denotes reduction in T. Recently a number of researchers have suggested an alternative way to \nbuild a compiler: rather than Most of this work was done while the author was at Chalmers University, \n41296 G6teborg, Sweden. Permission to make digitahtwrd copy of part or all of this work for personal \nor dassraom use is rantad without fee provided that copies are not made or distributed for pro i t or \ncommercial advantage, the mpyright notice, the titte of the publication and its date appear, and notice \nis given that copying is by permission of ACM, Inc. To copy otherwise, to republish, to post on servers, \nor b redistribute to lists, requires prior specific permission andlor a fee. ICFP 86 5/96 PA, USA 01996 \nACM 0-89791 -771 -51WOO05...$505O translate the term to CPS and optimize, perform the optimizations in \nan extended source calculus. In symbols, M-s P#, (2) where # : T + S is the decompiling transform, and \n-+s denotes reduction in the (extended) source calculus. If equation (1) holds exactly when equation \n(2) holds, we say that maps * and # form a Galois connection from S to T. Galois connections guar\u00adantee \nthat for each optimization in the target lan\u00adguage there is some corresponding optimization in the source \nlanguage. We might also reasonably re\u00adquire that compiling is the inverse of decompiling, so P*# is identical \nto P. A Galois connection satis\u00adfying this additional constraint is called a reflection in S of T. Whenever \na reflection exists, there must be a subset of S that is isomorphic to all of ~, we call this subset \nthe kernel, Our claim is that a reflection describes precisely the situation of interest to compiler \nwriters: it re\u00adlates the intermediate languages of direct and CPS compilers and enables direct compilers \nto work with a representation that is isomorphic to CPS terms. Previous work, such as Sabry and Felleisen \n(1993) and Hatcliff and Danvy (1994), has focused on the weaker notion of equational correspondence, \nwhich is based on equality rather than reduction. We show that two widely-studied translations may be \nregarded as reflections. Our source calcu\u00adlus is the computational lambda calculus, A., of Moggi (1988), \nwhich extends the call-by-value cal\u00adculus ~V of Plot kin (1975). Our first translation is into the monadic \nmeta-language, Aml, also of Moggi (1988); and our second translation is the CPS trans\u00adlation, also of \nPlotkin (1975). The monad trans\u00adlation may be regarded as a generalization of the CPS translation, and \nindeed the latter translation factors through the former. The computational lambda calculus A.. Moggi \n(1988) introduced monads as a general notion of computation. The monadic meta-language Aml was designed \nto express any semantics based on mon\u00adads, and the computation lambda calculus A ~ was designed as an \nextension of the call-by-value lambda calculus that is sound and complete for any monadic semantics. \nSo, by design, two terms are equal in AC if and only if their translations are equal in Aml. What is \nsurprising is that very little attention was paid to reductions. The original technical re\u00adport (Moggi \n1988) specified theories of reduction and of equality for AC, but only the equfllty theory of AC appears \nin the conference paper (Moggi 1989), and Ac rates barely a line in the journal version (Moggi 1991). \nNone of these contains a reduction theory for Aml, but this was considered by Hatcliff and Danvy (1994). \nHowever, ours is the first work we know of to relate the reductions of J C and ~ ml. Moggi (1988) presents \nAC as an unt yped calculus of reductions, and presents Aml as a typed calculus of equalities. Here we \nuniformly use untyped calculi of reductions. Everything works equally well for typed calculi of reductions, \nsuch as those considered by Hatcliff and Danvy (1994), since our translations preserve types. Related \nwork. As noted above, a number of re\u00adsearchers have considered ways to reflect the ben\u00adefits of CPS within \nan extended source calculus. Sabry and Felleisen (1993) proposed such a calcu\u00adlus, and noted that the \nkernel arising by normal\u00adization with regard to the so-called A reductions is closely related to the \ntarget of the CPS translation. Some practical ramifications of this were explored by Flanagan, Sabry, \nDuba, and Felleisen (1993). Lawall and Danvy (1993) make much of relat\u00ading forward and inverse translations \nvia two Galois connections and an isomorphism. Their Galois con\u00adnections are based on artificial orders, \nand as they note their isomorphism does not preserve these or\u00adders. Our choice of order solves these \nproblems and is more natural: we use the usual reduction relation on terms. Administrative reductions \nplay a central role in many CPS translations, from Plotkin s work on\u00adwards. Recent work explains administrative \nreduc\u00adtions of the target in terms of reductions in the source (Sabry and Felleisen 1993, Flanagan et \nal. 1993, Hatcliff and Danvy 1994). Each of our trans\u00adlations will incorporate administrative reductions, \nand each of our reflections will have a kernel that can be explained as source terms reduced to admin\u00adistrative \nnormal form. Outline. The remainder of this paper is struc\u00adtured as follows. Section 2 summarizes our \nresults. Section 3 introduces Galois connections and reflec\u00adtions, and explains why they embody a property \nevery compiler writer seeks. Section 4 reviews the traditional translation of ~o into Aml and why it \nfails to be a reflection. Section 5 shows that there is a reflection between J C and a variant Aml, of \nAml. Section 6 factors this reflection through an intermediate calculus AC* corresponding to the iso\u00admorphic \nimage of Jmz* in A c. Section 7 extends the results to translations into CPS. Section 8 describes related \nwork. Section 9 concludes. Figure 1. Summary of results 2 Summary If you like to see a summary in advance, \nread this section now; if you like to see a summary on com\u00adpletion, save it until the end. Depending \non your preference, you may thereby read this section once, twice, or never. Figure 1 illustrates a road-map \nof the terrain we cover. First we consider the traditional monad translation from Av into Aml. This translation \nis not a reflection; but expanding AU into A c, shrink\u00ading ~ ml into Aml,, and fine-tuning the translation, \nfinally yields a reflection in Jc of ~m~.. The exis\u00adtence of a reflection guarantees that there is a \nkernel Ac* of Ac that is isomorphic to Jml.. Crdculus AC has seven reduction rules, and Ac. arises by \nnor\u00admahzing with respect to two of these rules, (let. 1) and (/et.2). Second we consider the traditional \nCPS transla\u00adtion from AV back into Av. Again, this translation is not a reflection; but expanding (source) \nAv into A c, shrinking (target ) ~V into Acps, and fine-tuning the translation, finally yields a reflection \nin of ~ cPs. AC Again, the existence of a reflection guarantees that there is a kernel AC** of ~. that \nis isomorphic to ~~~,. Calculus A.., arises by normalizing Ac. with regard to one further reduction rule, \n( awoc). Fur\u00ad thermore, the second translation factors through the first; and so there is also a kernel \nAm/*. of AmI* that is also isomorphic to A.p$. 3 Galois Connections and Reflections Let s review the \nstandard results about Galois con\u00ad nections and reflections (Mac Lane 1971, Davey and Priestley 1990). \nThe standard results need to be adapted slightly, as reduction is a preorder (it is reflexive and transitive) \nbut not a partird order (it is not anti-symmetric). We write x for a single-step of reduction; for the \nreflexive and transitive closure of reduction; = for the reflexive, transitive, and symmetric clo\u00ad sure \nof reduction; and s for syntactic identity up to renaming. Assume a source calculus S with reduction \nrela\u00ad tion + S, a target calculus T with reduction rela\u00ad tion -1 . Reductions are directed in such a \nway M .-..-----------.-s> M* #* I------------z> P P$ G:::-i ------------\u00ad# -.. -.\u00ad -.~~ -.! P Figure \n3. A reflection and its ker\u00adnel isomorphism Figure 2. Motivating reflections Definition 3.2 Maps * and \n# form a reflection in S of T if they form a Galois connection and P G that they naturally correspond \nto evaluation steps P** . or optimizations. Let the maps * : S + T and # : T + S correspond to compiling \nand decom-For a reflection, # is necessarily injective. piling, respectively. Finally, let M,N range \nover Galois connections and reflections compose. terms of S, and P, Qrangeover terms of T. Proposition \n3.2 Let +1 and #l form a Galois con-Definition 3.1 Maps * and # form a Galois con-nection (reflection) \nfrom S to T, and *2 and #2 nection from S to T whenever form a Galois connection (reflection) from T \nto U. Then *1*2 and #2#1 form a Galois connection (re-M -~ P* if and only if M* -~ P. flection) from \nS to U. There is an alternative characterization of a Ga-Every reflection factors into an inclusion and \nan lois connection. order isomorphism. Write S** for the subset of S Proposition 3.1 Maps * and # form \na Galois con-cent aining just those terms of the form M *X, and nection from S to T if and only if the \nfollowing four write Id : S x + S for the trivial inclusion func\u00adconditions hold. tion. A reflection \ne and # in S of T is an inclusion if # is the identity (and hence S ~ T), and is an or\u00ad (i) M S M**l \nder isomorphism if * and # are inverses (and hence (ii) P * -, P, S = T). (iii) M S N implies M* -1 N*, \n(iv) P -T Q implies P# -S Q#. Proposition 3.3 Let * and # form a reflection in If the same four conditions \nhold with +s and S of T. -T replaced by =S and =T, then one has an equational correspondence, as defined \nby Sabry and 1. Translations *# and Id form an inclusion in s of S**. Felleisen (1993). Hence, every \nGalois connection implies an equational correspondence, though not 2. Translations * and # form an order \nisomor\u00ad conversely. The alternative characterization reveals a diffi\u00ad phism between S e and T. culty, \nas sketched in Figure 2. Consider a source The composition of the inclusion and the isomor\u00adterm M compiling \nto a target term M*, and con\u00ad phism is the original reflection. sider an optimization M -T P in the target. \nA Galois connection guarantees the existence of a cor\u00adresponding optimization M -s P# in the source. \nThe proposition is illustrated in Figure 3, which However the Galois connection does not guaran-shows \ncalculi S and T, and the images S*, T*, tee that this optimization is as good as the target S*#, and \nT** of these calculi under maps * and #. optimization. In fact, as revealed by law (ii) of The kernel \nS*# of S is isomorphic to T. Proposition 3.1, the source optimization is neces-The summary illustrated \nin Figure 1 demon\u00adsarily worse than the original optimization, since strates repeated use of this proposition. \nEach re- P** *T P. flection is factored into an inclusion (shown verti-We therefore reasonably insist \non having not cally) and an order isomorphism (shown horizon\u00adjust a Galois connection, but a reelection. \ntally). 4 Monads: The Problem Proposition 4.3 The translation * : A. --+ Aml is sound, but does not generate \nan equational corre\u00ad Let s review the traditional translation from the call-by-value calculus into the \nmonad calculus, and see why it fails to be a reflection. Plotkin s call-by-value calculus AU is summa\u00adrized \nin Figure 4. We let z, y, z range over variables, L, M, N range over terms, and V, W range over val\u00adues. \nA term is either a value or an application, and a value is either a variable or an abstraction. The call-by-value \nnature of the calculus is expressed by limiting the argument to a value in (~. v), and by limiting the \nfunction to a value in (q. v ). An impor\u00adtant aspect of each calculus we deal with is that it is confluent. \nProposition 4.1 The reductions of Au are conflu\u00adent. Moggi s monadic meta-language Aml is summa\u00adrized \nin Figure 5. This calculus distinguishes vaiues from computations. Functions may accept and re\u00adturn either \nvalues or computations, and are defined by the call-by-name rules (~) and (q). Two terms relate values \nto computations: the term [M] de\u00adnotes the computation that does nothing save re\u00adturn the value M; and \nthe term let z + L in N denotes the computation that performs computa\u00adtion L, binds z to the resulting \nvalue, and then performs computation N. The interaction of these terms is described by the three rules \n(0.let), (q.let), and (assoc). Proposition 4.2 The reductions of Aml are con\u00ad fluent. The translation \n* from AV to Aml is described in Figure 6. Translation * on terms uses an auxihary translation t on values. \nThe two translations are related by a substitution lemma, N *[z := Vt] ~ (N[x := V])*, which is easily \nchecked by induction over the structure of terms. The translation * is sound in that M ---+V N implies \nM* m/ N*. This is easily checked by looking at the translation of (/3. v), ((Ax. N) V)* ~ let y-+ [k. \nN* I in let z~ [V+l in gz (6./et) (~z. N*)Vt N*[s := Vt] -(P) = (N[z := V])*, and similarly for (q. \nu). However, the translation is not complete even in the weak sense required by equational correspon\u00addence: \nM* =ml N* does not imply M =. N. For example, it is easy to check that ((k. zM)L)* =m, (LM)* while if \nL is not a value then the equivalence does not hold in the call-by-value calculus. spondence. 5 Monads: \nThe Solution To refine the translation * : AV + Aml into a re\u00adflection requires three steps: first, grow \nAW into Ac; second, shrink Aml into A mix; third, fine tune the translation *. Step one grows AV into \nAc, which is summa\u00adrized in Figure 7. The new calculus was carefully designed by Moggi to model directly \nthe effect of translation into ~mi. This is achieved by adding to Ac a term let z = M in N which mimics \nthe term let x + M in N of Aml; by adding to A. reduc\u00adtions corresponding to each reduction of Am?; and \nby adding to A, two more reductions, (let.1) and (let.2), which mimic the effect of the translation from \nJV into Aml. Let P, Q range over non-values. Rules (let. 1) and (let .2) are restricted to act on non\u00advalues, \nsince values yield a reduction in the opposite direction via (@let). You may wonder: Is the new form \n(let z = M in N) necessary, or could it be represented by (Az. N)M instead? The latter is possible, but \nthen the rules (~.let), (q.let), (assoc), (let.1), and (let.2) all become more difficult to read. Further, \nwe pre\u00adfer for historical reasons to stick to Moggi s formu\u00adlation. You may also wonder: Do the rules \n(let. 1) and (let.2) point in the right direction? They do: the direction is dictated by the desire that \nreduction be confluent. For example, lety=(let z= Lin M)inyN +(rev,let,l) (let x = L in kf)N, lety=(let \nx= Lin M)inyN (assoc) letz=Lin (let y= MinyN) +(reV, ret,l) let t = L in MN shows that confluence fails \nif (let. 1) is reversed. But the system as given is confluent, w was shown by Moggi (1988). Proposition \n5.1 The reductions oj Ac are conflu\u00adent. Step two shrinks Jm/ into Am/*, which is sum\u00admarized in Figure \n8. The grammar is the smali\u00adest one that contains all terms in the image of the translation * : AU + \nAml of the previous section, and that is closed under the reductions of A ~1. The new grammar differs \nfrom the old in two key ways: applications MN in A ~r are restricted to the form VWinAl ~ *, and computations \n[M] in Aml are restricted to the form [V] in &#38;l*. The reduction rules are restricted accordingly. \nSince all applications have the form VW, rules (@) and (q) and rules (@o) and (q. v) have the same effect \non the calculus. This provides an analogue of terms L,M, N ::= VILM values V,W ::= x IAx.N (P.v) (XKN)V \n+ N[z := V] (V.v) Ax. (Vx) + V, ifz@jv(V) Figure 4. The call-by-value calculus, A. terms L, M, N ::= \nzlAz. MI MNl[Mllletz ~Min N (P) (k N)M + N[z :=M] (n) k. (Mz) + M, if z @jv(kf)  (@.lL?t) let z ~ [M] \nin N + N[z := M] (T1.let) let z ~ M in [z] -M, if z # ju(kf) (assoc) lety~(let z+ Lin M)in N + letc~Lin \n(let y~Min N) Figure 5. The monadic calculus, Aml *: Av4Am[  ~ [w] ;M)* E letx~L* inlet y+ M*inxy (X)t \n(Ax. N)t : ;x. N* Figure 6. Translation of A into Amr Plotkin s indifference property, which states that \ncall-by-value and call-by-name evaluation yield the same result for terms in CPS. Proposition 5.2 The \ngrammar of Aml* is closed under the reductions of Aml. That is, if M b N in Aml and M is in Aml*, then \nN is also in AmI., andM_ N inAml.. The proof is an easy case analysis. It follows as a corollary from \nProposition 4.2 that the reductions of Amlx are confluent. Step three adapts the translation x : AU + \nAml to the new calculi. The straightforward choice is to leave the translation as is, adding a line for \nlet. ~ [p] v* (LM)* -~x+L* in~y~M* in xy (let x = M in N)* z letx+M*in N* Here t is as in Figure 6. \nThe meaning of the over\u00adbars will be explained shortly. Alas, this translation is not even sound in our \nstronger sense: it preservee the equalities of A ~ but not reductions. The key problem is with rule (let.2), \nwhich requires (f?./et) reductions in both di\u00ad rections. (VQ)* ~ let z ~ [Vt] in let y + Q* in zy p. \nlet lety~Q*in Vty -e,l,t let y ~ Q* in let z @ [Vt] in let y @ [y] in z y ~ (let y = Q in Vy)* The \nsolution is to consider the /?./et reductions as part of the translation. The two overlined occur\u00adrences \nof let introduced in the translation of appli\u00adcations are regarded as administrative occurrences. A second \nstage is added to the translation where all administrative (~. let) redexes (that is, ones where the \nrelevant let is overlined) are reduced. Thw somewhat awkward description as a two\u00adstage translation with \nadministrative redexes may be replaced by the equivalent translation given in the first half of Figure \n9. This was derived by a simple case analysis on applications, with according simplification. The obvious \nhomomorphism, ehown in the second half of the figure, serves as the inverse translation. It is now straightforward \nto verify that these constitute a reflection. Proposition 5.3 Translations * and # (Figure 9) form a \nreflection in ~. of Amt.. terms L,M, N ::= VIP values V,w ::= zIAz.N non-values P,Q ::= LMlletz=Min N \n(k. N)V -N[z :=V] (P.v) (q.v) Ax. (Vx) * v, ifz $!.fv(V) (p.let) letx=Vin N + N[z := V] (?l.kt) letz= \nMinx -M, if x @ jv(kf) (assoc) lety=(let x= Lin M)in N + letz=Lin (let y= Min N) (ret.1) (let.2) ;&#38; \nFigure 7. The terms values non-values L,M, v,w P,Q (9.V) (Tv) [:.;::] (Ax. N)V Ax. (Vx) let x ~ let z \n~ [V] in N M in [m]  b letz=PinzM -+ lety=Qin Vy computational calculus, A. N ::= [v] [ P ::= zIAz. \nN ::= VWlletx+Min N (a;soc) lety+(let c+ Lin M)in N Figure 8. The simplified monadic calculus, ~m~x v* \n(PM)* (VQ)* (VW) (let z = M in * N)* : AC -+ ~ E z ~ Aml, [Vt] let z + P* let y + Q* Vtwt letx+M*in \nin in (zM)* (VY)* N* ~# (Ax. M)# (vw)# ([vl)# (letz<Min # : Am/* . ~ ~ ~ N)# = + Ac x Az. M# v#w# v# \nletz=M#in N# (m)t (,kz. M)t =Z E Ax. M* Figure 9. Reflection in A c of &#38;l* To prove this we verify \nseparately parts of Proposition 3.1. Parts (i) ified by induction over the structure parts (iii) and \n(iv) are verified by each of the four and (ii) are ver\u00adof terms, and induction over the structure Monads: \nProposition isomorphic of reductions. The Factorization 3.3 guarantees that there must be an image of \n~ ~i * within AC. It consists of exactly those terms of the form M *#. We name this calculus AC*, and \nit is summarized in Figure 10. The grammar is identical to AC, except general ap\u00adplications MN are replaced \nby value applications VW, much as in the move from Ami to Jmr.. The terms of AC* can be characterized \nas the terms of AC in (let.1) and (let.2) normal form. Once a term has been normalized with respect to \nthese two rules, they are never required again that is, none of the other rules will int reduce a (let. \n1 ) or (let.2) redex. As guaranteed by Proposition 3.3, the reflection * : ~. + ~m~. factors into an \ninclusion *1 Ac. and an order isomorphism *2 : /lc, + given in Figures 11 and 12. The proposition shows \nhow to compute these: *1 is *#, #l identity, *2 is a restriction of *, and #2 is #. is a pleasant bonus, \nnot guaranteed by the sition, is that *1 has a simple interpretation: reduces a term of AC to (let.1) \nand (/et.2) :A. + Jmim, even is the What propo\u00adit normal form, hence yielding a term of J.*. 7 Continuations \nThe development for continuations the development for monads. Just Jml via the traditional call-by-value \nlation, so does ~u map into AU via is paralleled by as AU maps into monad trans\u00adthe traditional terms \nL, M, N ::= VIP values ::= ZI Az. N non-values Y% ::= VWlletx=Min N Reductions (~.v), (q.o), (/l./et), \n(q./et), (msoc), as in Figure 7 Figure 10. The kernel computational calculus, Ac. *1 :Ac + AC* #~ :Ac. \n+ AC v* ~ Vt the trivial inclusion (PM)* s let z = P* in (zM)* (VQ)* s let y=Q* in(Vy)*  (VW)* ~ Vtwt \n (let z = M in N) G letx=M*in N* ~t (Az. M)t = ;x. M* Figure 11. Inclusion in ~c of ~cm *2 :AC* + Aml* \n#2 :Am(, + AC. v* ~ [Vt] same a. # from Figure 9 (VW)* ~ Vtwt (let z = M in N)* g letx+M*in N ~t x \n(Az. M)t ~ Ax. M*  Figure 12. Isomorphism of ~c, and ,lml* call-by-value CPS translation: beled aa administrative. \n v* s n.kvt (LM)* E ~k. L (%. M*(~y. zyk)) v* s M.kvt (LM)* ~ M. L*(Az. M*(AI/. q/k)) (let z = M in \nN)* -M. M (kc. IV*k) Xt x ~t (Ax. N)t : Ax. N* (Az. N)t ~ ;z. N* The translation now takes place in three \nstages. Stage one applies the translation proper, Stage two A remarkable property of this translation \nis that reduces any administrative (/3. u) redexes that is, one may always choose k to be exactly the \nsame ones where the relevant A is overlined. Stage three name, without fear of name clash. Again, this \nstrips the leading M , which always appears and translation is sound but not complete. so is redundant. \nThis three-stage translation may be replaced by TO refine the translation * : Av + Au into a the equivalent \ntranslation given in the first half of reflection also requires three steps: first, grow the Figure 14. \nThe old translation relates to the new source AV into A ~; second, shrink the target Au into M* Old \nas follows: E M. M*new, where k is the ~ CPS; t bird, fine tune the translation *. Step one distinguished \ncontinuation variable. The auxiliary was already accomplished as part of the monad de\u00ad translation M: \nK closely resembles a translation in\u00ad velopment. Steps two and three are best considered in reverse order, \nas the calculus of step two should be the image of the modified translation of step three. is summarized \nin Figure 13. The grammar is the To refine the translation * : JV + AO, it is suit-smallest one that \nis in the image of the refined ably extended for let, and has some reductions la-translation *. An additional \nclass of non-terminals, terms values continuations L,M, v,w K N ::= ::= ::= KV I VWK z IAz.M.M k I AZ. \nM (0.v) (n.v) g.;::j (Az. A!c. M)VK ~X, Ak. Vxk (Az.M)V Ax. Kx -+ + + M[z V, M[z K, := V][/c := K] ifz \n@jJ(V) := v] if z@ fv(K) Figure 13. Continuation-passing style calculus ACPS (PM) : K (VQ) : K (VW) : \nK (letx=Min ~t N):K E E E ~ =$ P : (As. ((zM) : K)) Q : (Ay. ((vy) : K)) vtwtK M:(XU. (N: K)) ~b (k.Ak. \nM)q kb (kN)b =Z ; = ~ kc. M# [1 letz=[]in N# Figure 14. Reflection in Ac of Acps K, is added to the \ngrammar, ranging over contin\u00aduations. This class contains the distinguished free variable k, and cent \nsins those lambda expressions which may be substituted for k. Examination reveals that each of the rules \n(/3. v) and (q. u) arises in exactly two situations, yielding four rules in the target ACPS corresponding \nto four of the seven rules in the source AC. It is easily verified that the grammar is indeed closed \nunder reduction. Every application haa a value as an ar\u00adgument, so call-by-value and call-by-name reduc\u00adtions \nhave the same effect on the language, which explains Plot kin s indifference property. Proposition 7.1 \nThe grammar of A,p, is closed under the reductions of Aw. That is, if M -+ N in Av and M is in Acps, \nthen N is also in A.ps, and M-N inAcPs. The proof is an easy caae analysis; it follows as a corollary \nthat the reductions of A Cps are confluent. Further, the same result holds if the call-by-value calculus \nAV is replaced by the call-by-name calculus An. The inverse translation # of Figure 14 is now easily \nderived. It has three parts, one for each com\u00adponent of the target grammar. A term M in ACPS maps to \na term M# in Ac; a vrdue V in ACpS maps to a value Vb in ,4C; and a continuation K in A CPS maps to an \nevaluation context Kb in A c. An eval\u00aduation context C is a term with a hole [ ], and if C is an evaluation \ncontext then C[M] denotes the result of replacing the hole in C by the term M. The filling operation \nis straightforward since our evaluation contexts do not bind any variables. We can now verify that the \nmaps of Figure 14 constitute a reflection. Proposition 7.2 Translations * and # (Figure 14) form a reflection \nin Ac of ACPS. As before, we prove each of the four parts of Proposition 3.1 separately. To prove part \n(i), M -M*# requires we strengthen the inductive hypothesis to Kb[M] -(M : K)x. Each proof is now straightforward, \ngiven the follow\u00ading two lemmas, which are equally straightforward. Lemma 7.3 Let M, V bein AC, and K \nbein ACPS, then (M : A )[z := Vt] ~ (M[z := Vl) : K. Lemma 7.4 Let M, V, K be in Acp., then: Kb[M#[x \n:= Vb]] -(M[z := V][k := K])*. This completes the parallel development of the reflection; there is also \na parallel development of the factorization, Proposition 3.3 guarantees that there must be an isomorphic \nimage of ACP$ within Jc. We name this calculus At.., and it is summa\u00adrized in Figure 15. Just as the \ngrammar of ~ CPS has 20 terms L, M,N ::= K[V] I KIVWl values v,w ::= aI Az. M contexts K ::= []lletz=[]in \nM K[(Az.M)V] + MIo:=V]:K Ax. (Vx) -+ V,ifz@jv(V) (~llet) letx=Vin M + M[z := v] (q./et) let z = [] in \nK[z] + K,if z @ fv(K) [:.$ V:K E K[V] (VW) : K s K[VW] (letz=Vin M):K ~ letz=Vin(M:K) (letz=VWin M):K \ns letr=VWin(M:K) Figure 15. The kernel computational calculus Ac.. ~ M* the trivial inclusion M:[l \n V:K = K[V] (PM) :K P : (let z = [] in ((zM) :K)) (VQ) :K Q :(let ~= [] in ((Vy) :K)) ~ (VW) : K K[VtWt] \n ~ (letz=Min N):K z M:(letz=[]in(N:K)) ~t = z (Ax.M)t = Ax. M*  Figure 16. Inclusion in AC of Ac.. \n*2 :AC** + Acps #~ :&#38;ps + AC** (K[V])* E K#Vt same as # from Figure 14 (K[VW])* E Vtwth t Figure \n17. Isomorphism of Ac.. and Acps three components terms, values, and continua-by the counterexample, \ntions so the grammar of A... has three compo\u00ad let x= ((~y. let z= abin c) d)in e nents terms, values, \nand contexts. Observe that, ~ letx=(let z=abinc)ine. despite the introduction of contexts, each term \nstill possesses a unique decomposition in terms of the To gain insight into the problem, consider the \nsyntax. (For instance, the term xv corresponds to corresponding CPS reduction, K[VW], where K is [], \nV is z, and W is y.) ((Ag. M. (a b (Az. kc))) a (k. kc)) + (a b (Az. (k. ke)c)). The terms of At.* can \nbe characterized .S the The image of this in &#38;,* isterms of Ac in (let.1), (let.2), and (assoc) \nnormal form. As in the previous development, once a term let z= ((Av. let z= abin c) d) in e haa been \nnormalized, no further reductions ever in\u00ad e letz=ab inlet x=cine. troduce a (let.1) or (/et.2) redex. \nAlas, the same cannot be said of (assoc). The reduction (~. u) may where the right hand side is in ( \nassoc)-normal form. indeed introduce a further ( assoc) redex, as shown The CPS language achieves this \nnormalization us\u00ad 21  ing the meta-operation of substitution which tra\u00adverses the CPS term to locate \nk and replace it by the continuation thus effectively pushing the con\u00adtinuation deep inside the term. \nIn order to properly match the behavior of CPS, we therefore add a corresponding meta-operation to AC*X, \nM : K shown in Figure 15. Using the new meta-operation we can extend the problem\u00adatic source reduction \n/3. v with a built-in ( awwc)\u00adnormalization action that mirrors the action of ~. v on CPS terms. (An \nalternative to adding meta-notation for sub\u00adstitution may be to move to calculi that use explicit substitution, \nbut we have not explored this possi\u00adbtity.) There is one pitfall to avoid: since reductions apply to \nany subterm, one may be tempted to apply rule (/3. W) to the subterm ((Jg. let z = ab in c) d) in the \ncounterexample by taking K to be [ ] rather than let z = [ ] in e. But it is natural to insist that the \nreduction rule (/3. rJ) is only applied when K is determined according to the unique decomposition afforded \nby the grammar of ~c**, and this steps neatly around the pitfall. Again, as guaranteed by Proposition \n3.3, the reflection * : J. + J .P, factors into an inclu\u00adsion *I : Ac + A... and an order isomorphism \n:A M. + Acp$, given in Figures 16 and 17. ~gain, these can be computed directly from the proposition, \nand again there is a bonus: xl has a simple interpretation as reducing a term of ~c by applying rules \n(let.1), (let.2), and (assoc) judi\u00adciously. In addition, the CPS translation factors through the monad \ntranslation. One may translate &#38;/* into A ~pS as fOllOws. [V] * E XC. kvt (VW)* z m. Vtwfk (let z \n-@M in N)* Z lk. M*(Az. N*k) Regarding this as a two-stage translation with ad\u00administrative reductions \nyields a reflection. As be\u00adfore, the reflection factors through a calculus Am(,., corresponding to the \nsubset of Amt. consisting of terms in ( assoc) normal form. The three calculi Ac.., b.., and ~~p, are \nisomorphic. Figure 1 di\u00adagrams the situation. Related YVork Plotkin (1975), among other contributions, \nformal\u00adizes the call-by-value CPS translation and shows that it preserves but does not reflect equalities: \nif M = N in Ati then M* = N* in Av, but not con\u00adversely. (Here and throughout we write * for the variant \nof the CPS translation under consideration, and hope this will lead to no confusion.) Sabry and Felleisen \n(1993) strengthen Plotkin s result by making the implication above reversible. They extend the call-by-value \nlambda calculus AU with a set of reductions X such that M* = N* in AUX if and only if M* = N in Av. As \nthey note, AV X and Ac prove the same equalities; but they do not prove the same reductions. The rules \nof A ~ X were derived by mapping back from J CPS, and hence are somewhat convoluted; in cent rast, Moggi \ns rules for A c are pleasingly natural. Sabry and Felleisen introduce the notion of equa\u00adtional correspondence \ndescribed in Section 3, and they prove that their translation constitutes such a correspondence. In fact, \nthey prove something stronger, making their translation almost, but not quite, a Galois connection: their \ntranslations sat\u00adisfies all four conditions of Proposition 3.1, except that condition (ii), P#* +P, is \nreplaced by the weaker P#* = P. (Compare this to the stronger P#* s P required of a reflection.) Sabry \nand Felleisen single out a subset A of X, and observe that these A reductions correspond directly to \nthe administrative reductions on CPS terms. Their terms in A normal form roughly cor\u00adrespond to our kernel \ncalculus A c. *, of terms in (let.1), (let.2), and (assoc) normal form. Flanagan, Sabry, Duba, and Felleisen \n(1993) ap\u00adply the results of Sabry and Feileisen. They sug\u00adgest that CPS translation may not be so beneficial \nafter all: it may be better to work directly in the source calculus. They show that terms in A normal \nform behave similarly to CPS terms, demonstrating this via a sequence of abstract machines. They also \nbriefly sketch possible applications of the full set of reductions X. But they fail to observe our central \npoint: that for optimization purposes one wants a result showing correspondence of reductions rather \nthan correspondence of equations. Lawall and Danvy (1993) give a factoring of CPS similar to the one \ndescribed here, and also refer to Galois connections. They relate four lan\u00adguages: a source language, \na kernel source lan\u00adguage, a kernel target language, and a target lan\u00admace. The first two relate via \na Galois connection. ~he middle two are isomorphic, and the last two again relate via a Galois connection. \nThe first two parts of their factorization are similar to our inclu\u00adsion in A c of A ~. *, and our order \nisomorphism from ~,** to Acps. Their third step schedules evalua\u00adtion, determining for each application \nwhether the function or argument evaluates first. Here we use a language where the function always evaluates \nbe\u00adfore the argument, and so we need no counterpart of their third step. The Galois connections of Lawall \nand Danvy are based on an artificial ordering induced directly from the translations. One might argue \nthat they are misusing the notion of Galois connection, and are instead dealing with the somewhat weaker \nno\u00ad tion of a pair of translations * and # satisfying M*S* ~~ M* and P*** ss P*. As they note, 22 their \nartificial ordering is unsatisfactory, because their middle isomorphism between the source ker\u00adnel and \ntarget kernel does not respect this ordering: it is not an order isomorphism, and hence not a Ga-Iois \nconnection. Thus, their factorization cannot be viewed as a composition of Galois connections. In contrast, \nwe use a natural ordering relation, and our Galois connections do compose. Whereas their iso\u00admorphism \nuiolate~ the ordering of their Galois con\u00adnection, our isomorphism arises as a consequence of our Galois \nconnection, as shown by Proposition 3.3. Hatcliff and Danvy (1994) consider translations analogous to \nour translations from ~V to ~ml, and from Aml to A.P,. They also look at translations from other source \nlanguages into Aml, an issue we ignore. They show the translation from &#38; to Jml is sound; we give \nthe stronger result that the transla\u00adtion from AC to Amr is a reflection. They also show that the translation \nfrom Aml to JCP* is an equa\u00adtional correspondence; again, we give the stronger result that it is a reflection. \nConclusion Our results might extend in a number of ways. Most lambda calculi possess a notion of standard \nreduction, characterized by two properties. First, at most one standard reduction applies to a term. \nSecond, if any sequence of reductions reduces a term to an answer, then the sequence of standard reductions \nwill also do so. Hence standard reduc\u00adtions capture the behavior of an evaluator. Plotkin (1975) specified \nstandard reductions for A , and his results for CPS demonstrate not only that reduc\u00adtions are preserved, \nbut also that standard reduc\u00adtions are preserved. (He expresses this in a differ\u00adent but equivalent form \nby saying that evaluation is preserved.) Hatcliff and Danvy (1994) give similar results for translation \nfrom Moggi s Jml into CPS. It appears straightforward to extend the work here by specifying a suitable \nnotion of standard reduc\u00adtion for each of the calculi involved, and to show that the given translations \nare still reflections if one replaces reductions by standard reductions. Sabry and Felleisen (1993) deal \nwith a variant of the CPS translation, due to Fisher (1972), that reverses the order in which the argument \nand the continuation are passed to functions. The effect of this reversal is to enable additional administra\u00adtive \nreductions to be performed. Just as the ordi\u00adnary CPS translation corresponds to a kernel of Ac where \nterms are normalized with regard to (let. 1), (let.2), and (assoc), so does the Fisher CPS trans\u00adlation \ncorrespond to a kernel of ~c where terms are normalized with respect to all three of these rules and \n(/3. iet) as well. Maraist et al. (1995) consider translations into a linear lambda calculus, stressing \nthe analogue with the CPS translation. That paper studies a call-by\u00adlet calculus that is closely related \nto AC and that translates into linear logic. By extending the call\u00ad by-let calculus with just one law, \nletx=Min N * N, if z @ ~v(N), the authors derive a call-by-need calculus (Ariola et al. 1995) that translates \ninto affine logic. We conjecture that when augmented with the above law, ~c also yields a model of call-by-need. \nThe call-by-value translation of Maraist et al (1995) carries over to a reflection between A. and a suitable \nlinear lambda calculus; the kernel of the reflection is given by the calculus ~cm, normalized with respect \nto (Zet.1) and (let.2), but not (a.woe). Hence, an analogue with the monad translation is more apposite \nthan an analogue with CPS. A reflection on call-by-value. Plotkin s original paper on A. layed out two \nkey properties of this calculus: first, it is adequate to describe evaluation, and second, it is inadequate \nto prove some equal\u00adities that we might reasonably expect to hold be\u00adtween terms. The first was demonstrated \nby a cor\u00adrespondence between ~. and Landin s SECD ma\u00adchine (Landin 1964). The second was demonstrated \nby observing that there are terms that are not prov\u00adably eqwd in ~o, but whose translations into CPS \nare provably equal. Moggi defined A= as an extension of A. that is sound and complete for all monad models, \nand hence proves a reasonably large set of equalities. He picked a confluent calculus to ease symbolic \nmanipulation, but made no claims that J= was it\u00adself a reasonable model of computation, Sabry and Felleisen \nshowed that Ac proves two terms equal ex\u00adactly when their CPS translations are equal. This reinforces \nthe claim that A. yields a good theory of equahty, but because they dealt only with equa\u00adtional correspondence, \nagain says nothing about A. as a model of computation. Our results here re\u00adlate A= reductions to reductions \nin AA and &#38;Ps, both widely accepted as models of computation. We hereby put forward ,lC as a model \nof caU-by\u00advaJue computation that improves on &#38;. References [1] APPEL, A. Compiling with Continuations. \n Cambridge University Press, 1992. [2] ARIOLA, Z., FELLEISEN, M., MARAIST, J., ODERSKY, M., AND WADLER, \nP. A call-by\u00adneed lambda calculus. In ACM Symposium on Principles of Programming Languages (1995), pp. \n233 246. [3] DAVEY, B. A., AND PRIESTLEY, H. A. in\u00adtroduction to Lattices and Order. Cambridge University \nPress, 1990. [4] FISCHER, M. Lambda calculus schemata. In ACM Conference on Proving Assertions about \nPrograms (1972), SIGPLAN Notices, bf 7, 1, pp. 104 109. Revised version in Lisp and Sgm\u00adbolic Computation, \n6, 3/4, (1993) 259-28 7. [5] FLANAGAN, C., SABRY, A., DUBA, B., AND FELLEISEN, M. The essence of compiling \nwith continuations. In ACM SIGPLA N Conference on Programming Language Design and Imple\u00admentation (1993), \npp. 237 247. [6] HATCLIFF, J., AND DANVY, O. A generic account of continuation-passing styles. In Proceedings \nof the 21 th ACM Symposium on Principles oj Programming Languages (1994), pp. 458 471. [7] KRANZ, D., \nET AL. Orbit: An optimizing com\u00adpiler for Scheme. In ACM SIGPLAN Sympo\u00adsium on Compiler Construction \n(1986), SIG-PLAN Notices, 21, 7, pp. 219-233. [8] LANDIN, P. The mechanical evaluation of ex\u00adpressions. \nComputer Journal 6, 4 (1964), 308\u00ad 320. [9] LANE, S. M. Categories for the Working Math\u00adematician. Springer-Verlag, \n1971. [10] LAWALL, J., AND DANVY, O. Separating stages in the continuation-passing transform. In Proceedings \nof the 20th A CM Symposium on Principles of Programming Languages (1993), pp. 124 136. [11] MARAIST, \nJ., ET AL. Call-by-name, call-by\u00advalue, call-by-need and the linear lambda cal\u00adculus. In Conference on \nMathematical Foun\u00addations of Programming Semantics (1995). [12] MOGGI, E. Computational lambda-calculus \nand monads. Tech. Rep. ECS-LFCS-88-86, University of Edinburgh, 1988. [13] MOGGI, E. Computational lambda-calculus \nand monads. In Proceedings oj the Symposium on Logic in Computer Science (1989), pp. 14 23. [14] MOGGI, \nE. Notions of computation and mon\u00adads. Information and Computation 93 (1991), 55-92. [15] PLOTKIN, G. \nCall-by-name, call-by-value, and the A-calculus. Theoretical Computer Science 1 (1975), 125-159. [16] \nREYNOLDS, J. C. The discoveries of continu\u00adations. Lisp and Symbolic Computation 6, 3/4 (1993), 233-247. \n[17] SABRY, A., AND FELLEISEN, M. Reasoning about programs in continuation-passing style. Lisp and Symbolic \nComputation 6, 3/4 (1993), 289-360. [18] STEELE, G. L. Rabbit: A compiler for Scheme. MIT AI Memo 474, \nMassachusetts Institute of Technology, 1978. \n\t\t\t", "proc_id": "232627", "abstract": "A number of compilers exploit the following strategy: translate a term to continuation-passing style (CPS) and optimize the resulting term using a sequence of reductions. Recent work suggests that an alternative strategy is superior: optimize directly in an extended source calculus. We suggest that the appropriate relation between the source and target calculi may be captured by a special case of a <i>Galois connection</i> known as a <i>reflection</i>. Previous work has focused on the weaker notion of an <i>equational correspondence</i>, which is based on equality rather than reduction. We show that Moggi's monad translation and Plotkin's CPS translation can both be regarded as reflections, and thereby strengthen a number of results in the literature.", "authors": [{"name": "Amr Sabry", "author_profile_id": "81100016804", "affiliation": "University of Oregon, Eugene, OR", "person_id": "P16266", "email_address": "", "orcid_id": ""}, {"name": "Philip Wadler", "author_profile_id": "81100173596", "affiliation": "University of Glasgow, Glasgow G12 8QQ, Scotland", "person_id": "PP39030941", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/232627.232631", "year": "1996", "article_id": "232631", "conference": "ICFP", "title": "A reflection on call-by-value", "url": "http://dl.acm.org/citation.cfm?id=232631"}