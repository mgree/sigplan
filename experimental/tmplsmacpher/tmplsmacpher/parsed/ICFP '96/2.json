{"article_publication_date": "06-15-1996", "fulltext": "\n Functional Back-Ends within the Lambda-Sigma Calculus Th&#38;&#38;e Hardin t, Luc Marangettand Bruno \nPagano*t Abstract We define a weak A-calculus, ,luw, as a subsystem of the full Xcalculus with explicit \nsubstitutions ~Ufi. We claim that AuW could be the archetypal output language of functional compilers, \njust as the A-calculus is their universal input lan\u00adguage. Furthermore, ~afi could be the adequate theory \nto establish the correctness of simplified functional compilers, Here, we illustrate these claims by \nproving the correctness of two simplified compilers and runtime systems modeled as abstract machines. \nWe first present the Krivine machine. Then, we give the first formal proofs of Cardelli s FAM and of \nits compiler. 1 Introduction It is folklore to define a compiler as a translator from a high\u00adlevel language \nintended for humans to a low-level language intended for machines. For mostly theoretical issues such \nas semantics or correctness of high-level program transforma\u00adtions, real programming languages are too \ncomplicated and lack generality. Instead, it is convenient to use an archety\u00adpaf language, standing as \na suitable abstraction of a whole class of programming languages. The A-calculus is widely accepted as \nsuch a paradigm of all functional programming languages, due to its simplicity, consistency and generality. \nMore precisely, the A-calculus captures the essence of func\u00adtionality. It is a non-ambiguous (i.e., Church \n-Rosser) reduc\u00adtion system where any strategy can be specified, yielding call-by-value or call-by-name \nfunctional languages. Mo~e\u00adover, it can be extended by adding extra rewriting rules to treat arithmetic \nor data structures [22]. In opposition to this commonly accepted view of A-calculus as universal abstract \nsyntax, there is no consensus among writers of functional compilers about the choice of an archety\u00adpal \ntarget language. With respect to the formal description of their output, published compilers for functional \nlanguages fall into three classes: they either compile to combinator or snpercornbinator [5] terms, to \n~-terms in continuation pass\u00ading style (CPS) [2], or to abstract machines [16, 7, 6, 8, 18]. These different \napproaches are praised for their peculiarities: *IBP-LITP, Universit4 Pierre et Marie Curie, 75252 Paris \nCedex 05, France. tINIUA Flocquencourt, BP 105, 78153 Le Chesnay Cedex R.ante Email: {Therese.Hardin, \nLuc.Maranget,Bruno Pagano}@inria fr This work was part]ally supported by the ESPRIT Basic Research Project \n6454-CONFER. Permission to make digitahlmrd copy of part or all of this work for personal or classroom \nuse is ranted without fee provided that copies are not made or distributed for pro i t or commercial \nadvanta e the oopyright notim, the titie of the pubiioation and its date appear, an 1 nobrx is given \nthat copying is by permission of ACM, Ino. To copy otherwise, to republish, to post on servers, or to \nredistribute to lists, requires prior specific permission andlor a fee. ICFP 96 5/96 PA, USA 01996 ACM \n0-69791 -771 -5/96/0005 ...$3.50 combinators for their rewriting aspects and adequation to lazy evaluation \n[23], CPS for its ability to encode explicitly a given strategy and abstract machines for their closeness \nto real computers. None of these frameworks is designed to ex\u00adpress the others. In fact, they do not \nclaim to be universal, but each claims to be the best. Nevertheless, a few common concepts arise here. \nThe functions are to be compiled, that is, a fixed code should perform the actions specified in the body \nof a function, this code remaining unchanged at every invocation of the func\u00adtion. The variables in a \nfunction are of two kinds: either formal parameters or free variables. The values of param\u00adeters change \nat every function call, whereas the values of free variables remain the same. Thus, the low-level object \nthat represents a function is a closure: a pair of a code and an erauironment that collects the values \nof the free variables at function creation time. Therefore, our universal target language should be a \ncalculus of closures. Furthermore, the basic operations perfo~med by the various existing run\u00adtime systems \nare the same: applying a closure, creating a closure, or retrieving the value of a variable in some en\u00advironment. \nThese operations are best unveiled in abstract machines. Thus, in the rest of this paper we focus mostly \non them, as a still widely accepted formal description of functional runtime systems, which we intend \nto surpass. Closures are naturally expressed in the A-calculus with explicit substitutions [1, 10] as \na term (Mkf) [s], where M is a term standing for a piece of code, and s is a substitu\u00adtion, that is, \nan environment, collecting the values of free variables. We now need some rules to compute on closures. \nFirst of all, we need to apply a closure (Ml) [s] to an ar\u00adgument N, yielding the explicit application \nof the new sub\u00adstitution t = N.s to the body M, written M [N. s]. Then, we have to propagate the substitution \nt inside the body M, until the substitution t reaches a variable, which should then be replaced by its \nvalue, or a A abstraction, whose body is code for a new closure. The rule for applying closures along \nwith simple rules to propagate substitutions define the weak k-calculus, Jaw. While designing krW as \nyet another cal\u00adculus of explicit substitutions, we took particular care to select only the term constructs \nand rewriting rules that are actually required to express the basic steps performed by the existing abstract \nmachines. We are satisfied that this pragmatic approach yields a confluent subcalculus of Jao, one of \nthe Ja-calculi introduced in [10]. In this paper, we first introduce the weak h-calculus and give a unified \npresentation of abstract machines. After\u00adwards, we show that the basic operations of abstract ma\u00adchines \ncorrespond to certain rewriting steps in the weak Arr\u00adcalculus. More precisely, the deterministic evaluation \nstrat\u00ad egy implemented by an abstract machine is identified as a rewriting strategy in AuW. We make this \ncorrespondence fully explicit for the Krivine machine and the FAM. The lat\u00ad terexample involves a true \ncompilation of the input A-term Informally, reduction is not allowed under ~ s is replaced to a Au-term. \nThereby, we give the first known proof of the correctness of a FAM-based compiler and runtime system. \nOther execution models are briefly discussed in section 5. We thus illustrate our claim that weak A-calculus \nwith explicit substitutions is an adequate tool to study theexecu\u00adtion of compiled functional programs \n[1, 10, 20]. Moreover, as shown by the FAM, the full A-calculus with explicit sub\u00adstitutions may be a \ngood formal language to describe the whole compilation process. This confirms the versatility of Au, \nwhich has been used recently to study advanced topics in the ~-crdculus, such as higher order unification \n[12], or issues in logic, such as the interpretation of sequent calcu\u00adlus [15]. 2 Preliminaries 2.1 \nThe lambda-calculus with explicit substitutions The traditional weak J-calculus is an attempt to model \nthe execution of machine code within the J-calculus; it conforms with the basic intuition that, functions, \nonce compiled, are code and cannot change (otherwise, there would be no com\u00adpilation). A tentative definition \nof weak reduction is thus to suppress the (f) rule from the definition of the J-calculus. M + A4 (() \nAX.M + AX.A4 This negative definition has the major drawback that it does not lead to a consistent definition \nof the weak A-calculus as a Church-Rosser rewriting system. To see this, consider the following derivations: \n(Xtq/.y z) (( AZ.Z) (AZ.%)) /- \\ Ay.g (( AZ.Z) (Az.z)) (key.y z) (Az.z) The problem lies in a discrepancy \nbetween intuition and formalism. Using ordinary A-terms only, what is intuitively perceived as the invariable \ncode Jy.y z with respect to the possibly changing binding [z\\(Az.z) (~z. z)] has to be repre\u00adsented by \nthe fully substituted abstraction Ay.y (( Az. z) (Az.z)), so that the redex (Az. z) (~z. z) is now located \nunder a ~ and cannot be contracted without invoking the (<) rule. This undesirable divergence can be \ncorrected by delaying substi\u00adtution, that is, by introducing explicit closures. Then we get: (Azy.y $) \n((AZ,?) (Az.z)) / \\ (,ly.y z)[z\\(Az.z) (Az.z)] (kzy.7J z) (k.%) \\/ (Ay.v z)[z\\Az.z] by substitution does \nnot cross A s . A natural setting for a formal treatment of closures is k, the J-calculus with explicit \nsubstitutions [1, 10]. We first reca31 the definition of AC, the two sorted algebra of Au-terms. TERMS \n: JW ::= nl (MM) I AM I M[~] SUBSTITUTIONS : s ::= idl~lkf. slsos Note that variables are represented \nby De Bruijn indices. The new term construct M [s] represents explicitly the ap\u00adplication of substitution \ns to term M. The substitutions themselves are made explicit: we have two special substitu\u00adtions id and \nt, whereas substitutions are structured as lists of terms M.s or as compositions s o s. We note ADB the \nsubset of Am that coincides with ordi\u00adnary A-terms. ADB-TERMS: M ::= n I (Mitf) 1 MM The propagation \nof substitutions inside terms and sub\u00adstitutions is defined by the following rewriting system a~: (App) \n(M N) [s] (M [s]) (N [s]) (FVar) l[M. s] M (RVar) (n+l) [M.s] n [s] (Clos) (A!f [s]) [t] M[sot] (AssEnv) \n(Sot)ou So(tou) (MapEnv) (Af. s)ot M [t] .(S O t) (ShiftCons) 1 0(J4. s) s (IdL) ides s There is one \nrule per term construct in the algebra of Au\u00ad terms, except for J. The propagation of substitutions through \napplication nodes and accesses inside list-structured Sub\u00adstitutions are handled by the first three rules \nabove in a straightforward manner, The case of functions is more sub\u00adtle. In the simplest case of the \nso-called shared environ\u00adment machines (Krivine machine or SECD, for instance), functions are compiled \nas abstractions i.e., as code , and execution will only pair these abstractions with an envi\u00adronment, \nproducing closures. The copied environment ma\u00adchines >, such as the FAM, are more sophisticated: a function \nAM is compiled into a closure (AM ) [s], where s collects the references of M to a global environment, \nwhereas the free variables in JM refer only to s. At run-time, applying some substitution tto the closure \n(AM ) [s] will result in apply\u00ading t to s, thus composing the two substitutions into one new substitution \ns o t as illustrated by the rewriting rule (Clos). Hence, we need rules to substitute inside substitu\u00adtions; \nin other words, rules to compose substitutions. These rules are the remaining four rules above. Here \nagain, there is one rule per term construct in the sort of substitutions. As we need the composition \noperator on substitutions o to express certain computations, we differ significantly from recent calculus \nof explicit substitutions without com\u00ad position [19]. Since there is no rule for crossing A s, a Ja-closure, \ni.e. a term of the form (JM) [s], cannot be reduced at its root. Hence, h-closures are (weak) values, \nsimilar to weak head normal forms. In our case of an archetypal target language, k-closures are the only \nvalues. In a more generrd setting that would also consider arithmetic and data structures, ad\u00ad ditional \nvalues would be integers, lists,. . . In the weak setting, Au-closures are restructured only by applying \nthem to arguments: (Beta) ((MM) [s]) N -M [N..] The system bW is defined by the rules of a~ plus the \nrule (Bet a), From [1 O], where a system very close to AuW is studied, we deduce that the weak substitution \nsystem UW is both strongly normalizing and confluent and, by using the interpretation method, that the \nreduction system AaW is confluent. Moreover, our system ,!aW is a subcalculus of Aufi, themost general \nJ-calculus with explicit substitutions. The terms of /lafi are the terms of k plus the additional lifted \nsubstitution construct O (s), whereas the rules of the strong substitution system at are the rule of \nUW plus the following strong substitution rules: (Lambda) (AM) [s] + A(M [O (s)]) (VarShiftl) + n+l \n n [t] (VarShift2) n[tos] ~ n+ 1 [s] (FVarLiftl) 1 [fi (9)] +1 (FVarLift2) I[fi(s)ot] + 1 [t] (RVarLiftl) \nn+l [fi (s)] ~ n[so~] (RVarLift2) n+l[fi (s) o t] +-n[so (tot)] (ShiftLiftl) to fi(s) + sot (ShiftLift2) \nto(fi(s)ot) + So (tot) (Liftl) t (8) o fi (t) + fi(sot) (Lift2) fi(s)o(fi(t)ou) + fi(sot)ou (LiftEnv) \nO(s)o(ilf.t) + M.(sot) (LiftId) O (id) + id (IdR) soid +s (Id) h{ [id] -+M With respect to strong substitution, \n,%-closures are not values any more, since any Au-closure (AM ) [.s] now immedi\u00adately reduces to A(M \n[ O (s)]), by the rule (Lambda). Most of the other rules of at explicit the De Bruijn indices adjustm\u00adents. \nThe remaining two rules (IdR) and (Id) define the substitution id as the identity. As it can be expected, \nat is a terminating and confluent rewriting system [10]. The full system Aafi is defined by the strong \nsubstitution rules of at plus the following rule for applying ~-abstractions to their arguments: (BetaStrong) \n(klf) N -+ M [N ~id] The system Aut is both confluent and correct with respect to the A-calculus [10]. \nOne easily sees that AOW is a subcalculus of Aafi, since the weak /3-rule (Beta) is a shortcut for the \nfollowing AYO\u00adderivation: ((AM) [s]) N La~da) (A(M [fi (s)])) N Be gng) (M [O (s)]) [N. id] M) M [O (s) \no (N. id)] Li~v) 2.2 Abstract machines In the rest of this paper, we describe two machines, while unifying \ntheir presentations. Instructions differ between machines, but, for any ma\u00adchine, the code always is \na possibly empty list of instruc\u00adtions: CODE ::= () [ INSTRUCTION; CODE A closure is a piece of code \nassociated with an environment, an environment being a list of closures: CLOSURE ::= (CODE/ENVIRONMENT) \nENVIRONMENT ::= () ! CLOSURE. ENVIRONMENT A typical code will be written C, a typical environment e and \na typical closure ~ or (C/e). The states of the machines are non-empty lists of frames, the exact structure \nof frames depending upon the machines. STATE ::= FRAME I FRAME :: STATE The behavior of an abstract machine \nis specified by a deterministic transition system. Briefly, a transition system is a triple (E, Et, -+), \nwhere E is a set of states, Etisthe subset of terminal states and ~ is the transition relation defined \nover (E Et) x E. The transitive closure of ~ is written +*. A transition system is deterministic when, \nfor every state D, there exists at most one state D such that D + D . At this point, the puzzled reader \nmay have a look at the machine descriptions of sections 3 and 4, keeping in mind the following notations: \nConvention: We will make intensiwe use of lists. When appropriate, we freely interpret them as sequences, \nwords or even arrays. That is, given n elements XI, x2, . . . . zn, thelist S=zl :x2 :.. .:zn : () is \nsirnp/y written zl:zz:. ..:zn. A sublist x ,: . . . z Z] iS written < and with a slight notational abuse \nwe sometimes decompose S as follows: s= x= : z= ; x= 2.3 Implementation of a strategy by a machine In \n[24], bisimulations are used to establish the equivalence of two transition systems El and EZ. In our \nwork, xl always defines an abstract machine whereas X2 is a subsystem of Auw. The states of Z2 are b-terms \nand its transition re\u00ad lation is a rewriting strategy ~, a strategy being a deter\u00administic subrelation \nof the general Aaw-reduction relation. We present a simplified setting, where a bisimulation is given \nby two partial functions, the compile-and-load func\u00adtion t that translates b-terms into machine states, \nand the recompilation function ~ that translates machine states into Au-terms. We say that a machine \nimplements a strategy S when the following three conditions are satisfied: 1. Initial condit~on: Let \nM be a k-term such that Z(M) exists. Then L(M) = M fid]. M [N(s o id)] ~) M [N.s] 2. The machine follows \nthe strategy S: If D1 + Dz and x exists, then n exists and we have either z = n orThus, as a subsystem \nof the strong system Aafi, the weak s system Jaw is also correct with respect to the Ycalculus. D1-D2. \n3. Terminal statea translate to normal forms: Let M be a Au-term such that .C(i14) exists. If L(M) ~ \nD and D is a terminal state, then ~ is a S-normal form. The condition 1 is justified by the rule (Id): \nM ~d] + M of ht. Basically, the rule (Id) states that id is the identity substitution that maps variables \nto themselves. This point is important, since it is a first illustration of using the strong k-calculus \nto assert a correctness property. 3 The Krivine Machine First, we describe the Krivine Machine [8]. This \nmachine is very simple: INSTRUCTION ::= Grab I push(coDE) I Access(n) FRAME ::= CLOSURE A typical state \nD is thus a stack of closures, which we write fl::fz::... ::fn. A ~DB-term is compiled as follows: [n] \n= Access(n) [Ml] = Grab; [M] [(M N)] = Push([N]); [M] Loading of compiled code is just pairing with the \nempty environment: Z(M) = ([ J4]/()). The execution of programs is defined by the following transition \nrules: (Access(l) /( CO/eO) ..):: D 12 (CO/eO) :: D (Access(n + l)/(CO/eO) . e) :: D ~r (Access(n.)/e) \n:: D (Push(C ); C/e) :: D ~h (C/e) :: (C /e) :: D (Grab; C/e) :: (C /e ) :: D Zb (C/( C /e ) . e) :: \nD Then, we define the recompilation procedure from ma\u00adchine states to k-terms. First, we just reverse \nthe compi\u00adlation scheme [ ], and extend the resulting recompilation procedure to closures and environments: \nAccess(n) = n Grab;C = Au Push(C ); C = (C C ) ~ = V[z] fl. id f.. = y.~ Finally, observing that new \nframes are introduced by the ex\u00ad ecution of the Push instruction, which is the code equivalent of an \napplication, the state constructor :: is decompiled as an application: f1::f2 ::...:: fn=(... (flf2) \n. . . z) In the expression above, the %-term X is said to be in head position. The head position is the~eftmost \nposition with respect to application nodes, since -fl = ~ [~] is not an application. Now, we show several \nproperties of the compilation and recompilation functions. First, these two transformations are another \ninverse: Lemma 1 [M] = M, for ang ~DB-term M. As a corollary, we get the initial condition 1. Then, \nwe show a weakened condition 2, in order to make the strategy of the Krivine machine appear naturally. \n~emma 2 Let D and D be two machine states such that Then ~ exists and we haue the :;:c:,:~;d g: ~~ Proofi \nWe give the example of a push transition: push (Push(C ); C/e) ::11, _ (C/e) :: (C /e) :: DO 1 1 . k. \n  (.. .(c c )[E]...Z) (...(a[z])(u[E]) ...~) Therefore, the execution of the instruction Push is equiv\u00adalent \nto the application of the AaW-reduction rule (App). Similarly, the transitions lvar, rvar, and grab implement \nthe reduction rules (FVar), (RVar) and (Beta). Finally, all reductions are performed in head position. \n0 By the detailed proof of the previous lemma, it is not difficult to see that the Krivine machine follows \nthe weak leftmost strategy, or K-strategy, described below in the small step formalism: IIM. S]LM (n+l) \n[Ms] ~ n[s] (M N) [s] L (M [s]) (N [s]) ((AM) [s]) N ~ M [N. s] ~>M! h4NAM N It remains to show condition \n3 on terminal states. Lemma 3 Let D be a reachable terminal state, then ~ is a K-normal form: Proofi \nThe Krivine machine may stop for two reasons: * When D = (Grab; C/e). Then, we get ~ = (A@) [z], which \nis a Ja-closure and a K-normal form. * When D = (Access(m)/()) :: Do, i.e, when an access fails. Then, \n~ = (.. .(M z)... ~), which is also a K-normal  form. This case cannot happen if the initial program \nis a closed ~DB-term. l Finally, as a corollary of the previous three lemmas, we get the final result: \nProposition 1 The Kriuine machine implements the I{\u00adstrategg. 4 The Functional Abstract Machine 4.1 \nBasics The Functional Abstract Machine (FAM) was designed by Cardelli [6] as a SECD machine optimized \nto allow very fast function application and the use of true stack . 28 (AS : j,e, Local; C) :: D 1=1 \n(f: AS : f,e, C) :: D (AS, G, Global(i); C) :: D Ual (f, : AS, ~, C) :: D (( CO/eO) : g : AS, e, Apply \nC) :: D a@y (g, e~, Co) :: (AS, e,C) :: D (~: AS, e, Fun(n, CO); C) :: D cl~re ((CO/~): AS, e, C) :: \nD (f: .,., ()) :: (AS, e,C) :: D e~n (f: AS, e,C) :: D Figure 1: Transition rules for the FAM The FAM \nhas four instructions: INSTRUCTION ::= Local I Global(n) 1 Apply I Fun(nz, coDE) (n>l,7n>o) The frames \nof the FAM consist in an argument stack, an environment and a code. STACI< ::= () i CLOSURE: STACK FRAME \n::= (sTACI<, ENVIRONMENT, CODE) The transitions of the FAM are given in figure 1. Specif\u00adically, observe \nhow the instruction Local selects the bottom element j~the argument stack (AS : f) and how the n ar\u00adguments \n~~,1 that the instruction Fun(n, CO) pus are taken in reverse order to build a new environment $],~. \nBy con\u00adtrast with the Krivine machine, the FAM builds a full envi\u00adronment when it creates a closure. \nOne says that the I?AM has copied environments (whereas the Krivine machine has shared environments ). \nIn [6], Cardelli only gives a few compilation hints for the FAM. One of these hints consists in compiling \na func\u00adtion ~M as a closure (C/e), where the environment e has been optimized to retain only the global \nvariables of JAI. This idea is described as a simple transformation in b. For instance the J-abstraction \nM = A( 1 (5 7)) is transformed into the b-closure Af = (A(l (2 3))) [4. 6. id]. That is, the free variables \ns and 7 are abstracted out or lifted and regrouped in the closure environment 4.6. icl, whereas, in the \nbody of M, the lifted variables 5 and 7 are replaced by 2 and 3 respectively, the new indices reflecting \nthe final positions of lifted variables in the closure environ merit.. As a first intuition of the correctness \nof such a transformation, observe that the terms M [id] and M are the same function. For any argument \nN, we get: (M [id]) N B~) (1 (57))[N.id] ~ N (46) ~4 N ~) (1 (23)) [N.4.6id]~*N (46) We now describe \nour general free variable abstraction procedure. Formally, the set of f~ee variables in a ~DB\u00adterm M \nare collected by calculating 30(M), where Y is de\u00adfined by: >~(n) = II ifn<d Xd(n) = {n d} ifn>d ~~(~ \nN) = 7~(fi4) U ~d(N) %d(~fl~) = ~d+l(flf) The set Fo(J4) = {n,,..., n~} can now be arbitrarily or\u00adderedas \nalist~=nl: . . . : n,~. This list is then given as a first argument to our abstraction scheme C, which, \ngiven any ~DB-term J4, outputs a k-term C(FO (f14) , A!f): C(= ,Ili) = i c(= ,(J4 N)) = (C(~ ,M) C(= \n,N)) C(= ,JA4) = (Ac(l : PI-I-1 : . ..p~+l. M))[C(~, P1. Pk id)] where pl :pz : ... :pk = 70(X14) { \nc(~, J4. s) = C(~, A4). C(~, S) C(= ,id) = t To get the intuition behind the scheme C, think that the \noutput k-term is to be executed at run-time in an envi\u00adronment s = N1 ...N, ...N~ .id, where N, is the \nrun-time value of the variable nl of the input term. The translation (JA4 ) [t] of a ~-abstraction MM \nis also to be interpreted in this environment s. At run-time the current substitution s will be applied \nto t, in order to yield a new current substitution that only retains the values of the free variables \nof AA4. Thus, t is the list of the positions in s of these free variables. The substitution t ends with \nthe new special substitution Tm, which ultimately discards s. This discarding operator is an ordinary \nk-substitution: T = id, tl=t, tm=totm-lwhenrrz>l The action of ~ ~ is then expressed by the following \nu~\u00ad derivatiou: (AssE_nv)T~o(N1.Nz..N~.id) fo(TW -l o(N, NzN~. id)) ~ . . . 1 O (N.l . id) Sh =) id \nThe correctness of the procedure C is stated with respect to the strong substitution rules of at. Lennna \n4 The C compila~ion scheme neuerfails and is cor\u00adrect. More precisely, given u closed A~B-term M, then \nC(O , M) is a Au-term J4 such that M [id] and J4 are ufi-equal. Proof: We show the following property \nby induction on M: given a JDB-term f14, a substitution s and a vector ~ SUCh that XO (flf) g ~, C(=, \nM) exists and we have: C(~, Jf)[n1 n2 nm s]=~t M u It is important to notice that the b-terms M and \nM ~d] only differ by substitutions steps. As a consequence, M fid] and M are more than just /3-equivalent \n~-terms, they are the same J-term. The second phase ~ ] of our compilation scheme applies to Au-terms \nof the kind produced by the first phase C and produces FAM code: [1] = Local [n+ 1] = Global(n) [(J4 \nN)] = [N]; [M]; Apply [(AM)[NI.N~~m]] = [N~]; . . ..[N~]. Fun(n, [M]) Finally, a closed ~DB-term ~ is \ncompiled to the code C = [ C(O , M)] and its execution starts from the initial state Z(M) = ((), (), \nC).  4.2 The recompilation First, we inverse the compilation procedure [ ]. We do so by proving judgments \nC ok M, which read the code segment C stands for the ~a-term M in an environment of siz-e k . Local Uk \n1 (Local) Global(i) uk i+i Global) Cuklv C uklvf (Apply) C; C ; Apply JJk (J4 N) Cl JJk N1 . . . C,lJk \nN, CJJ ~lM -(Fun(i)) cl; . . . ;C,; Fun(i, C) J,!-k (JM)[N, ...N,.T 1 The recompilation of FAM closures, \nenvironments and stacks naturally follows from code recompilation. (c/fI .f2 f~) = (JfW [fI f2 ~ fnl, \nwhere c Un+l M . jI.fz.f~ = flfz.~id fn:... :f, :fl = K:...:z:x The recompilation of machine states is \nbest understood as a two-stage process. In a first step, we translate the frames (AS, e, C) into triples \n(S, s, C), where the stack S is the translation of the argument stack AS and the substi\u00adtution s is the \ntranslation of the environment e, up to the bottom element of AS, whose translation may be incorpo\u00adrated \neither in S or in s. @((ASn : fR,e~,C~) ::...:: (ASZ : fZ,eZ,Cz) :: (AS~,e~,C~)) II . . (AS~,f~.v,C~) \n::...:: (ASz,f,.m,Cz) :: (~, G, C,) Then,t_he value of ~ is computed by proving ajudgement O(D) U D, \nwith the axioms and inference rules given in figure 2. At first, our state decompilat.ion procedure may \nseem a bit complicated. However, it is a simple extension of the code decompilatiou procedure. The basic \nidea is as follows: in a triple D = (S, s, C), the stack S is the list of the subterms of ~ that have \nalready been computed by tl~ machine, whereas the code segment C encodes the part of D whose computation \nis still pending, the external references in C being relative to the current substitution s. To see this, \nfirst consider the extreme cases, where ev\u00aderything has already been computed (rule (Res)), or whele \nnothing has been computed yet (rule (Code)). Then, con\u00adsider a mixed case, supposing, for instance, that \n~ is an application (Ml M2 ) (i. e., assuming that the last instruc\u00ad tion of C is Apply). If M2 is a \nreduced term, then it stands at the bottom of the stack S, while the term Ml is the recompilation of \na substate of D (rule (AppLeft)). If Mz is not fully reduced yet, then it is the recompilation of a substate, \nwhereas Ml is M [s], where M is the recompila\u00adtion of a code segment whose execution has not even started \n(rule (AppRight)). The recompilation of a triple (S, s, C ; Fun(n, C)) is per\u00adformed by the rule (Fuu(n,i)). \nThis operation resembles the recompilation of a triple (S, s, C ; Apply). More specif\u00adically, the ~a-substitution \nN] . . . N, .((N,+l . . . Nn . Tm) o s) stands for a closure environemeut that is not fully computed \nyet, where the term N, is the subterm being currently com\u00adputed, while the subterms N1,. . . . N,_l are \nfully reduced and the computation of the subterms N,+l,. . . . N~ is yet to be started. Finally, the \nrule (State), which performs the recompilation of multi-frame states, is inspired by the FAM transition \nreturn. We now prove that the rules of figure 2 effectively de\u00adfine a deterministic procedure for decompiling \nFAM states. First, we prove a strong non-ambiguity property for code segments. Lemma 5 A decompilable \ncode is any code segment C, such that tlzere exists a integer n and a k-term M, with C U M. A strtct \nsufix is any code segement C, such that there exists a non-empty code segment C! and thrrt the concatenation \nC ; C is a decompilalde code. 1. In a proof tree, all the code segments that appear in judg\u00adments S, \ns, C JJ M, where S zs a non-empty stack, are strict stifixes. 2. Strict sufixes cannot be decompiled. \n Proof: The first proposition is proved by induction on proof trees, starting from the fact that the \nempty code is a strict suffix. The second proposition is proved by induction on the length of decompilable \ncodes. Lenmla 6 The decornpilation of machines states is vzon\u00adambiguous. Proofi The proof is by induction \non state size. Consider any state D, if D is made of two or more frames, the deter\u00administic recompilation \nrule (State) applies. Now, suppose that D is a single frame, i.e., O(D) = (S,s, C). If the code C is \nempty or ends by a Local or Global(i) instruction, only one non-recursive rule may apply and re\u00adcompilation \nis over. Otherwise, we have two subcases, either C ends by an Apply or by a Fun(n., Co ) instruction, \nAs far as ambiguity is concerned, these subcases are the same. Thus, for instance, we assume O(D) = (S, \n.s, C ; Fun(n, Co)). If the stack S is empty, then only the rule (Code) may apply. If S contains at least \none elememt, then we must apply the recompilation rule (Fun(n, i)). By the previous lemma 5-2, there \nis at most one way to cut C into C,; C,~I ; . . . ; Cn, where C, is a strict suffix and C,+l , . . . \n. C,l are decompilable code segments. In other words, the rule (Fun(n, i)) is applied unambiguously. \n0  c urnM M, S,() u M (Res) (), s,C JJ M [s] code) S,s, CJj N C Qm M S, S, CJJM (AppRight) (AppLeft) \nS, s, C; C ; Apply J.!-(M [s] N) S:N, S, C; APPfy U (M N) S, S,C, J N, C,+] urnN,+ I ... C,, JJm Nn CJJ \n+IM, forl<i<n (Fun(n,i)) S: N,-I. . . . ..NI. s, C ,; C ,+l; . . .. C~. Fun(n, C)l(AM)[NINl ..N~.((N,+l \n...N~.tm) OS)] S~,s~,CnJ.lM (M: Sri-I, Sri-I, en-1):: . . . ..(SI. SI, cl) VN (State) (Sri, sn, cn) :: \n(sn-l, sn_l, cn_l) ::...:: (Sl, sl, cl)$lv (Where S is a non-empty stack and m is the length of the substitutions) \nFigure 2: The recompilation of machine states 4.3 Strategy and correctness This section is devoted to \nthe C-strategy that the l?AIvl implements. This strategy is described in the small step formalism (see \nfigure 3). The axioms of the C-strategy are deduced from the FAM transition rules. The most significant \naxiom is (Beta), which we deduce from a simplified transition apply. apply ((Co/co) : g, e, APPIY) (9, \neo, co) :: (())e, ()) 1 1 tzi[~.ai] (((m [a)@ Thus, we know that both function and argument are FAM \nclosures when a funct@r call occurs. Moreover, given a clo\u00adsure ~, the k-term ~ is both a b-closure and \na normal form. Such a Au-term is called a C-value. v ::= C-VALUES: (Ahf) [e] C-ENVIRONMENTS: e ::= id \nI V. e Hence, we get the rule (Beta) of figure 3, which characterizes the C-strategy as call-by-value. \nThe compilation scheme u j provides some intuition about the inductive rules of the C-strategy. Consirfer, \nfor instance, that an application (M N) compiles as ~ N ]; [ M ]; Apply. Thus, code being executed left-to-right, \nthe C-strategy should be right-to-lejt call-by-value. However, the exact rules of the C-strategy can \nbe found only by closely examining the state decompdation rules of the previous section. Consider a FAM \ntransition D ---D , such that D is a single frame that decompiles to an ap\u00adplication. Fir@, assume that \nthe last steps of both de\u00adcompilations D and ~ are applications of the rule (Ap\u00ad pRight). Then, with \nthe notations of figure 2, we get ~ = (M [s] N) and ~ = (M [s] N ), with N &#38; N by induc\u00ad tion. Hence, \nwe get the rule (AppRight) of the C-strategy (figure 3). Assuming that both ~ and ~ are computed by the \nrule (AppLeft), we get the rule (AppLeft) of the C\u00adstrategy, by an identical argument. A more interesting \ncase occurs when ~ is computed by the rule (Code). Suppose ~(~) = ((), s, i; C; C ; Apply), we have: \nCQn%N C ~m M (Apply) i; C; C ; Apply urn (M N) (), s, i; C; C ; Apply U (AI N) [s] code) Then, D is \ncomputed from D by executing the instruction i. As for ~, we get: s,s, C~N C UmhI S, s, C; C ; Apply \nU (M [s] N ) AppR ght) By induction hypothesis applied to the substates DO and D~, such that @(Do) = \n((), s,i; C) and @(D{) = (S, S, C), we get N [s] ~ N , Hence we get the rule (MapApp) of the C-strategy. \nObserve that, using this rule, the C-strategy may combine several UW reductions in one step. For space \nconcern, we do not prove here that the FAM implements the C-strategy. Intuitively, the C-strategy com\u00adputes \nsubstitutions left-to-right in the environment part of closures (cf. the rules (ConsLeft), (ConsRlght) \nand (Clos-Right)). hloreover, the propagation of substitutions through closures and cons nodes . is combined \nwith other aW\u00adreduction steps (cf. the rules (MapClos) and (MapEnv)). The full proof, which we omit, \ngeneralizes the case of ap\u00adplication nodes given iu this section. It does so in two di\u00adrections: first \nwe must consider closures nodes (i.e., the re\u00adcompilation rule (Fnu(n, i)), second we must consider multi\u00adframe \nstates, (i.e., the recompilation rule (State)). 5 Other execution models In the previous sections, we \nhave exhibited the Jaw-calculus strategies that hidden inside the Krivine machine and the FAhI. In [21], \nthe strategies implemented by two other envi\u00adronment machines, the SECD machine [16] and the CAM [7] \nare described. We also made explicit the strategy of the ZAM [18]. Briefly, the SECD and the 2AM implement \nthe right-to-left call-by-value strategy, whereas the CAM imple\u00adments the left-to-right call-by-value \nstrategy. The G-machine and the TIM [13] can also be understood in the Am-calculus, although these machines \nlook quite dif\u00adferent from environment machines. In order to simplify the (Mf) [s] and N are C-values \n(Beta)  n[N1.. . N.. s] &#38; N. (Varn) ((AM) [s]) N ~ M [N. s] Tn o (Nl ...Nn id) s id (Shiftn) NAN \nM ~ M N is aC-value (AppRight ) (AppLeft ) MN2 MN MN~M N S.X+SI (C1osRight) (AM) [s] ~ (AM) [s ] M&#38;M \nM is aC-value s~ s (ConsLeft) (ConsR,ght) M.s~M . s M.s~M. s Sot Ls .. .. (MapApp) (MapClos) (MapEnv) \n(J4 N) [s] ~ (M [s]) N ((AM) [s]) [t] L (MI) [s ] (N. t)oss N (tos) Figure 3: The C-strategy management \nof variables at run-time, compilers for these AC can also account for a schematic CPS-based compiler. \nmachines translate input ~-terms into supercombinators, by By no way, do we attempt to render the complexity \nof a the so-called J-1ifting operation [23]. Supercombinators are full-fledged compiler as [2] does, \nusing enriched A-calculus n-ary functions without free variables, that is, in terms of (in CPS) as a \nformal language. Jaw, closures (M. . .Jfi4) fid], where M is a Jr-term whose variables are all J-bound. \nWe call suchzz closures Aa-super\u00ad 6 Related work and conclusion combinators. In the Au-framework, A-1ifting \nis actually quite similar to the first phase C of the FAM compilation. Where, The main contribution \nof this paper resides in the introduc\u00adfrom a function, the transformation C produces a closure, tion \nthe krW-calculus as the weak A-calculus, that is, as the A-lifting will produce the partial application \nof a hJ\u00ad the adequate framework for the formal study of the execu\u00adsupercombiuator. For instance, our \nscheme C translates the tion of compiled functional programs. Additionally, the full abstraction J(1 \n(5 7)) into (A(l (2 3))) [4. 6. id], whereas a k-calculus appears as an adequate formalism for proving \nsimple A-1ifting procedure would transform this abst, ract, ion the correctness of skeleton compilers. \nPresently, the most into the k-term (( XL4(1 (2 3))) ~d]) 64. salient illustration of this claim is our \ncomplete description Any Juw-strategy that accounts for supercombiuator re\u00adand proof of a schematic FAM \nbased compiler. duction must include a n-ary (Beta) rule, which expresses Our work is to be compared \nfirst with similar attempts the application of a Aa-supercombinator to all its arguments to prove, formalize \nor derive several functional back-ends in one step: in an unified formalism. In [9], the K rivine machine \nand the CAM, two shared environment abstract machines, are(Betan) ((A. ..MI) [id]) N1 ... Nn + M [Nn \n~NI id] derived from deterministic strategies of Jp, a calculus of The G-machine and the TIM implement \nvery similar strate-closures. The system Ap is a conditional rewriting term gies, which basically amounts \nto first contracting the leftmost-system (see also [20]) and can be seen as a predecessor of outermost \n(Bet a,, ) redex and then propagating the gener-our standard term rewriting system AuW. A recent publica\u00ad \nated substitution. This simplified term-based presentation is tion [1 I] resembles our work, since it \nmodels many compilers sufficient for establishing the correctness of both machines. and abstract machines, \nusing the J-calculus extended with The SML/NJ compiler departs from the abstract ma-appropriate combinators \nas a formal language. We differ chine approach [2]. Roughly speaking, a schematic ShfL/NJ from this work \non an important point: we insist on what all compiler would first translate a source A-term into a A-term \nfunctional runtime system have in common, to the point of in continuation-passing style ( CPS). Then, \nthis CPS A-term proposing a definition for compiled functionality in a rela\u00adwould be further transformed \nby the so-called closure con-tively well established formalism, whereas []]] focuses more version. This \nconversion transforms functions into record on modeling the exact structure of abstract machines, in \nor\u00addata structures, which encode closures. Snch records can der to establish their taxonomy of functional \nlanguages easily be expressed in our framework by adopting a more di-implementation. rect encoding of \nclosures. The resulting modified schematic Onr work is to be compared also with other works that compiler \nwould now produce k-terms in continuation-passing formally prove one or a few abstract machines. Here, \na first style. Note that the above schematic description of the benefit of our approach of describing \nabstract machines in SML/NJ compiler is ours and only intends to show that terms of a b-calculus rewriting \nstrategy is that their cor\u00adrectness is a direct consequence of the correctness of the A&#38;-calculus \nwith respect to the A-calculus. As a conse\u00adquence, our correctness proofs appear to be quite simple. \nBy contrast, the correctness proofs of the CAM in [4] and of the SECD machine in [24] were complicated. \nMoreover, our simple technique enabled us to prove the correctness of the FAM, which has never been done \nbefore. We believe that this simplicity owes much to the fact that our overaJl framework (i.e., the Aufi \ncalculus) includes both our archety\u00adpalsource andtarget languages ss consistent (i.e., closed by reduction \nand Church-Rosser) subcalculi. The second benefit of our approach lies in the generality and precision \nof our correctness results: all machines are de\u00adscribed in the same framework and we describe every step \nof their execution. Here, we differ from [14], which relied upon natural or big-step semantics and from \n[8, 18], which proved the Krivine machine and the ZAMin h but do not specify their Au-strategies. Our \nsmall-step approach to semantics enables us to compare the termination properties of different machines \nnaturally. For instance, we can say that the Krivine machine terminates more often than the CAM or the \nSECD, since it follows the leftmost-outermost strategy, which terminates more often than any other strat\u00adegy \nof the orthogonal weak A-calculus [20]. As a second example, given the same J-term as input, the SECD \nand the CAM compute exactly the same closure, whereas the FAM computes a different, Aufi-equivalent closure. \nA first direction for future work is to study graph-based implementations. Considering that the call-by-need \nstrat\u00ad egY is the natural implementation of call-by-name in graph rewriting systems, such a strategy \ncan be modeled in a sim\u00adple extension to term-graphs of the weak J-calculus with explicit substitutions. \nTo render sharing while preserving the desirable simplicity of terms, several techniques already exist, \nsuch as subterms labeling [20], explicit recursive equa\u00adtions [3], or specialized bindings [17]. A second \ndirection is to examine how the full k-calculus can be used to assert the correctness ofsome phases \nofrealis\u00ad tic compilers. Various optimizations at the closure level are a first natural target for such \na study. Considering skeleton compilers that are closer to real compilers would require first to extend \nAu to handle common programming constructs, such as data structures, recursive bindings, exceptions,. \n. . References p] M. Abadi, L. Cardelli, P.-L. Curien, and J.-J. L6vyi Explicit Substitutions , POPL \n90. [2] A. W. Appel, Conlpiling with Continuations , Cam\u00adbridge University Press, 1992. [3] Z. Ariola, \nM. Feleisen. J. Maraist, M. Odersky, and P. Wadler, A call-by-need lambda-calculus . POPL 95. [4] A. \nAsperti, A Categorical Understanding of Environ\u00adment Machines , JFP, 2(l), 1992. [5] L. Augustsson, A \nConlpile rForLazyML , LF P 84. [6] L. Cardelli, Compiling a Functional Language , LFP 84. [7] G. Cousineau, \nP.-L. Curien and M. Mauny, TJle Cat\u00adegorical Abstract Machine , FPCA 85. [8] P. Cr6gut, An Abstract Machine \nfor the Normalization of Lambda-terms , LFP 90. [9] P.-L. Curien, An Abstract Framework for Environ\u00adment \nMachines , TCS. 82, 1991. [10] P.-L. Curien, T. Hardin and J.-J. L&#38;y, Confluence Properties of Weak \nand Strong Calculi of Explict Sub\u00ads tit u tions , INRIA Research Report No 1617, 1992. Also to appear \nin JACM in april 1996. [11] R. Douence and P. Fradet, Towards a Taxonomy of Functional Language Implementations \n. PILP 95. [12] G. Dowek, T. Hardin and C. Kirchner, Higher-Order Unification via explicit Substitutions \n, LICS 95. [13] J. Fairbairn and S. Wray, Tim: A SimpJe, Lazy Abstract Machine to Execute Supercombinators \n, FPCA 87. [14] J. Hannan, D. Miller, From Operational Semantics to Abstract Machines , Journal of Mathematical \nStruc\u00adtures in Computer Science , 2(4):415-459, 1992 [15] H. Herbelin, A J-Calculus Structure Isomorphic \nto Se\u00adquent Calculus Structure , CSL 94. [16] P. J. Landin, The Mechanical Evaluation of Expres\u00adsions \n, Computer Journal, Vol 6, No 4, 1964. [17] J. Launchbury, A Natural Semantics for Lazy Evalu\u00adation >, \nPOPL 93. [18] X. Leroy, TJle ZINC Experiment: An Economical Im\u00adplemen tation of the ML Language , INRIA \nTechnical Report 117, 1990. [19] P. Lescanne, F~onr b to Jv: a journey through Cal\u00adculi of Explicit Substitutions \n, POPL 94. [20] L. Maranget, OptimaJ derivation in Orthogonal Rewriting Systems and in Weak Lambda calculi \n, POPL 91. [21] B. Pagano, Bi-sim u]ation de machines abstraites en Iambda-sigma-calcul , Journr5es ,Francophone \ndes Lan\u00adgages Applicatifs 1995, INRIA Editions (In french). [22] G.D, Plot kin, L CF Considered as a \nProgramming Lan\u00adguage . TCS, 5:225-255, 1977. [23] S. L. Peyton Jones, The implementation of Functional \nProgramming Languages , Prentice-Hall, 1987. [24] M. Rittri, Proving the Correctness of a Virt uaJ Ma\u00adchine \nby a Bisim ulation , Phd thesis, University of Goteborg and Chalmers University of Technology, 1988. \n \n\t\t\t", "proc_id": "232627", "abstract": "We define a weak &amp;lambda;-calculus, &amp;lambda;&amp;sigma;<inf>w</inf>, as a subsystem of the full &amp;lambda;-calculus with explicit substitutions &amp;lambda;&amp;sigma;&amp;uArr;. We claim that &amp;lambda;&amp;sigma;<inf>w</inf> could be the archetypal output language of functional compilers, just as the &amp;lambda;-calculus is their universal input language. Furthermore, &amp;lambda;&amp;sigma;&amp;uArr; could be the adequate theory to establish the correctness of simplified functional compilers. Here, we illustrate these claims by proving the correctness of two simplified compilers and runtime systems modeled as abstract machines. We first present the Krivine machine. Then, we give the first formal proofs of Cardelli's FAM and of its compiler.", "authors": [{"name": "Th&#233;r&#232;se Hardin", "author_profile_id": "81100178540", "affiliation": "IBP-LITP, Universit&#233; Pierre et Marie Curie, 75252 Paris Cedex 05, France and INRIA Rocquencourt, BP 105, 78153 Le Chesnay Cedex France", "person_id": "P279595", "email_address": "", "orcid_id": ""}, {"name": "Luc Maranget", "author_profile_id": "81100574739", "affiliation": "INRIA Rocquencourt, BP 105, 78153 Le Chesnay Cedex France", "person_id": "PP37029165", "email_address": "", "orcid_id": ""}, {"name": "Bruno Pagano", "author_profile_id": "81100622646", "affiliation": "IBP-LITP, Universit&#233; Pierre et Marie Curie, 75252 Paris Cedex 05, France and INRIA Rocquencourt, BP 105, 78153 Le Chesnay Cedex France", "person_id": "P33939", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/232627.232632", "year": "1996", "article_id": "232632", "conference": "ICFP", "title": "Functional back-ends within the lambda-sigma calculus", "url": "http://dl.acm.org/citation.cfm?id=232632"}