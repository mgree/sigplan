{"article_publication_date": "06-15-1996", "fulltext": "\n Static and Dynamic Partitioning of Pointers as Links and Threads David S. Wise and Joshua Walgenbach \nComputer Science Department, Indiana University Bloomington, Indiana 47405-4101 USA dswise, j walgenb@cs. \nindiana. edu Abstract Identifying some pointers as invisible threads, for the pur\u00adposes of storage management, \nis a generalization from sev\u00aderal widely used programming conventions, like threaded trees. The necessary \ninvariant is that nodes that are accessi\u00adble (without threads) emit threads only to other accessible \nnodes. Dynamic tagging or static typing of t breads ame\u00adliorates storage recycling both in functional \nand imperative languages. We have seen the distinction between threads and links sharpen both hardware-and \nsoftware-supported storage management in SCHEME, and also in C. Certainly, there\u00adfore, implementations \nof languages that already have ab\u00adstract management and concrete typing, should detect and use this as \na new static type. Categories and subject descriptors: D, 3.3 [Programming Languages]: Language Constructs \nand Features data types and structures, dynamic storage man\u00adagement, abstract data types; E. 2 [Data \nStorage Represen\u00adtations]: Linked representations; B.3.2 [Memory Structures]: Design Styles primary memory. \nGeneral Term: Languages. Additional Key Words and Phrases: storage manage\u00adment, reference counting, garbage \ncollection, tags. Introduction All active references or pointers originate from roots in the programming \nenvironment; common roots are the regis\u00adter file and recursion stack. Storage management preserves linked \nstructures that are accessible from roots, where the garbage-collecting traversal begins. Definition \n1 A pointer is a link if it is essential to the integrity of a linked structure. Informally, a linked \nstructures must be rooted and spanned by links. Definition 2 A pointer or reference is a thread [12] \nif it is optional in a linked structure, in the sense that it can be inferred from a traversal from the \nroots that follows links exclusively. Permission to make digitablwd sopy of pari or all of this work \nfor personal or daesroom use is ranted without fee provided that copies are not made or distributed for \npm I t or commercial advantage, the copyright notice, the title of the publication and ik date appear, \nand notice is given that copying is by permission of ACM, Inc. To cupy otherwise, to republish, to post \non servers, or to redistribute to lists, requires prior spedfic prnieaion andfor a fee. ICFP 965/ 96 \nPA, USA ID 1996 ACM 0-69791-771 -5/96!0005...$3.50 On first reading of any program, one may assume that \nall pointers/references are links; threads can be introduced later, Usually there are several ways to \nestablish a partitioning between links and threads. Of course, it is best to choose a simple one; for \ninstance, links originate from roots and extend homogeneously to the more remote nodes of a struc\u00adture. \nSome fields in each record can be statically typed to link and others to thread. Often, however, a dynamic \nattribute is used with run-time tags present to distinguish the kind of pointer. Whether or not threads \nare identified, however, the se\u00admantics of a program must remain the same. Our purpose in identifying \nthem is to unburden the storage manager from dealing with them. Because threads are redundant, it can \nignore them and the performance of the program improves without changing its result. That is, link/t \nbread can be a static type like lazy/strict (w.r.t. evaluation of a function s argument) or sticky/unique \n(w.r.t. reference counting); we might conservatively presume that an unknown type is the first of each \npair but, whenever we can discover the second, we can compile for better run-time performance, Examples \nof threads are already familiar to the reader. The REAR pointer to the end of a singly-linked queue should \nstatically be treated as a thread. Threaded trees [12], in cent rast, are an example of a dynamic link/t \nbread dist inc\u00adtion, because a pointer requires a run-time tag to imply its meaning. Another familiar \nthread is the (reverse pointer paired to every [forward link as an edge in a doubly-linked list. Others \ninclude weak pointers [15] in many LISP and SCHEME implementations. The new principle for implementors \nof programming sys\u00adtems and for low-level programmers is (Invariant 1) that threads point only to nodes \naccessible exclusively via links. Programmers should recognize dynamic threading more effectively than \nthey do now; compilers should better rec\u00adognize static threading [17]. Their use accelerates produc\u00adtion \ncode (sometimes called the mutator in systems with automatic storage management) by short-circuiting \nredun\u00addant reallocation, and also they can accelerate space re\u00adcovery (the collector when one exists). \nThis observation cert airily applies to systems with automatic collection, like LISP, SCHEME, ML, HASKELL, \nand SMALLTALK, but we found it to be already useful under languages like C and C++ where space recovery \nis manual. The payoff for sustaining the live/dead distinction on pointers is that the work for the collector \nis considerably reduced, and the locality of the mutator is similarly in\u00adcreased. The exact impact depends \non the kind of stor\u00ad 42  age manager used. (This paper takes the perspective that reference counting \nis distinguished from garbage collection [24, 8,12, p. 412], rather than one of its techniques [6].) \nFor instance, a garbage collection can ignore all threads, saving the time to traverse them. Under reference \ncounting, some counts will be one tick lower, saving both their increments and decrements, and often \navoiding troublesome cycles. By focusing only on links, we foresee further improvements from compile-time \nspace analysis that uses techniques like linear logic and monads. In all these cases the mutator runs \nmore often, and can run more locally whenever nodes are discov\u00adered to be uniquely referenced, enabling \nin situside-effects instead of allocating more space. The payoff on cache-based architecture is fewer \ncache misses and far greater speed., Improved performance is not obtained without a task for the compiler \nor a burden on the programmer. Uncon\u00adstrained use of threads leads directly to errors from dangling\u00adreferences. \nRemoval of the last link to a node renders un\u00adstable any remaining threads there; the node can be recy\u00adcled \nunpredictably, transforming any dangling threads into a dangling reference (to some new incarnation at \nthat ad\u00ad dress.) Treating this last thread as a link would have pre\u00ad cluded recycling, and avoided a \nnasty error. The ultimate resolution of such dangling threads is to require the compiler, rather than \nthe programmer, to enforce Invariant 1, below. The compiler must be able either to infer statically or \nto provide run-time code verifying that certain pointers are threads, ensuring that they are manipulated \nin a manner that sustains the invariant. That is, the compiler should be able to learn which pointers \nare of thread type, and then to validate their consistent use. This paper has six sections, including \nthis introduction. Section 2 gives definitions and Invariant 1. The next section reviews historical examples \nof the concept, and is followed by Section 4 on a logic for the invariant and anticipates its formal \ntype. Section 5 describes our motivation and results, the motivation for the general observation, and \nthe final sec\u00adtion offers conclusions and a challenge for compiler-writers. 2 Definitions and Invariant \n Definition 3 T = {live, dead} is the set of tags. Tag, is used as in threading of binary trees [12]. \nLinks are live, and threads are dead. Definition 4 Let the set of active nodes in the heap be N; each \none is perceived as a record with a small set of fields, each identified from a finite set L of labels. \nThe edges or pointers in our linked structure comprise the mappings from a function in N x L -+ N, a \ndigraph with labeled edges. Of course, the function defining pointers changes with every step that \nchanges linking. Definition 5 The source of pointer (m, 1) ~ n is m. Its label is 1; its destination \nis n. Definition 6 Node m is said to emit any pointer of which it is a source. Node n is said to absorb \na pointer of which it is a destination. Definition 7 The set of pointers in any structure is parti\u00ad \ntioned into a set of links and a set of threads. There are two ways to achieve this partitioning. One \nis to use a static function in L + T to tag every label; this cor\u00adresponds to static typing on pointers \nthat does not change even if a pointer does; see Section 4. Another is to extend the pointer function \nto one in N x L + N x T so every pointer carries a tag; this corresponds to dynamic tagging of an attribute \nat run time. Definition 8 [7] The reference count of a node is the num\u00ad ber of links absorbed by it. \nDefinition 9 [27] A link is unique when its destination has a reference count known to be one. Otherwise, \nit is sticky, The term sticky is borrowed from the convention of fitting a static infinity into the range \nof reference counts [5], whence neither increments nor decrements change it. (But a full gmbage collection \nmight [24, 27].) A reference count can be both one and sticky, after an imperfect counting protocol loses \nthe precise count on a node and no longer knows it. Convention Roots emit only links. Definition 10 A \nnode is accessible if it absorbs a link from a root or an accessible node. Invariant 1 Accessible nodes \ncan emit threads only to other accessible nodes. There is no requirement that threads be introduced at \nall, but if Invariant 1 can be assured, then run-time per\u00adformance will be enhanced by them. Two more \nverbs are useful: Definition 11 Changing a tag from live to dead kills the associated pointer; changing \na tag from dead to live resurrects it. Maintenance of Invariant 1 is the obligation of a pro\u00adgrammer \nwho would use threads, or of a compiler that would kill links. Although it seems to be burdensome, it \ncan of\u00adten be sustained by simple coding practice or by stronger, verifiable constraints. 3 Historical \nExamples 3.1 Static Links and Threads The REAR pointer to the endqof a singly-linked queue should be \ntreated as a thread. Then FRONT and all its internal links likely remain unique. A corollary to this \nconvention, that directly yields superior code, is that REAR of an empty queue must be undefined: -L \n[11], rather than something meaningful [12, pp. 256-257] [20, p. 29] [13, pp. 79-80], because there is \nno accessible node to absorb a thread. In doubly-linked lists where forward pointers are links, we can \nt rest reverse pointers as threads, Definition 12 Reverse pointers in a doubly linked list are called \ncounterpointers. In the instance that the list is not circular (when both ends point to NIL), the links \nform a singly linked list, referenced by a link FRONT and the counterpointing threads form a singly linked \nlist in the reverse direction. beginning from the REAR thread. In all cases of a linear list, the links \nform a simple, singly linked list whose space might easily be recycled, either by hand, by elementary \nreference counting, or by unique typ\u00ading at compile time. The use of reference counting in early list-processing \nsys\u00adtems [7, 23] reveals other kinds of pointers long treated as threads. The READERS of SLIP, which \nused reference count\u00ading well [24], contained references that were never counted [23, Fig. 3]. From a \nmodern perspective, the READERS im\u00adplemented a crude recursion stack of threads. Weizenbaum s convention \nworked because the extra references were neces\u00adsarily redundant, satisfying Invariant 1. Alternatively, \nwe might view their destinations as nailed down by other pointers. Section 5 describes how the recognition \nof thread-as\u00adtype was used implicitly in a C program to simplify storage 43 management tremendously. \nUsing a hardware device that manages the heap in real time, purely local transactions provided storage \nmanagement directly from the mutator, without its processor accessing more RAM .  3.2 Dynamic Links \nand Threads Singly-linked circular lists can be recovered by real-time ref\u00aderence counting when the enclosing \nlink can be treated as a thread [9]. However, there must be a run-time tag on the pointer field to distinguish \nthe enclosing thread from an ordinary link. (The tag can be manifested as a relative ad\u00address if the \nlist were otherwise stored in genetic order at monotonically increasing addresses.) If a doubly-linked \nlist is circular, then the closing forward link must also be also treated as a thread like the circular \nlist s but it must also be tagged; all counterpointers are threads and, so, need not be. The presence \nof header nodes [23, 12, $2.2.5] alters these conventions only slightly. Nailing down a node is a dynamic \ntrick that uses a static protocol. The term means that the programmer is certain that it is referenced \nat least once (by the nail. ) Precise reference counting on several intervening reference transactions \nbecomes redundant, so long as all of them are abandoned before the distinguished reference is pulled. \nIn this context, the nail is perceived as the link, with all intervening references created as t breads. \nNo explicit tags are necessary because a static partitioning exists only for the span of code while the \nnail is in. There are several ways to thread a tree [12, $2.3.1-2]: for example, inorder [19] or level-order \n[12, p. 350] succes\u00adsors. Tarj an int reduces several threadings to explain his palm trees [21]; his \ntree arcs are links, but his fronds, reverse fronds, and cross-links are all threads. Where these threads \nare overloaded in a field that could alternatively contain a link, they must be treated as dynamic. Several \nLISP and SCHEME implementations provide weak pointers. These are pointers installed by the programmer \nas a convenience, flagged so not to be traversed by the garbage collector. In MACLISP these appeared \nfirst in un-garbage\u00adcollected arrays [16, pp. 79 80] whose content were threads, but with a caution instead \nof Invariant 1. Later incarna\u00adtions [15, $2.2.2] had the garbage collector replace them with NIL (or \nequivalent ). While this protocol enforces Invari\u00adant 1, it renders a pointer only mostly dead [10], \nbecause it causes work for the collector and because a NIL weak pointer now confuses the empty list with \nlost denotation that dis\u00adappeared in an intervening collection.  3.3 More Examples are Solicited The \nexamples cited here are by no means intended to be complete. The authors seek other (especially classic) \nexam\u00adples of extant use of link/thread typing, even if only im\u00adplicit in design or validation of an algorithm. \nWe conjec\u00adture that many programmers use these ideas subliminally, enabling them to manage heaps well \nwithout algorithmic storage management. 4 Programming Logic and Types This section presents a programming \nlogic in the form of preconditions for imperative programming that preserves In\u00advariant 1, and anticipates \na type theory for strongly typed languages. Invariant 2 Invariant 1 can be restated: Z] node m M ac\u00adcessible \nand (m, 1) w n is a pointer, then either it is a link or n is accessible. This formulation of the invariant \ntranslates into four cases for imperative programming that are itemized below: resurrection and killing, \nand two cases for pointer assign\u00adment to a live/dead pointer variable or field. 4.1. Dynamic tag changes: \n(a) Resurrection is always safe. Enlivening cannot violate Invariant 2. (b) Killing is safe only if \nthe pointer is not a unique reference. If the pointer were unique, killing it would render its destination \ninaccessible.   4.2. pointer assignment P + Q, is analyzed according to the cases fo~ P before assignment. \nThe delicate ~ase occurs when P is not unique. The tag of Q is copied along with its pointer. (a) P a \nthread: Like resurrection, the assignment P + Q cannot violate Invariant 2. (b) P a link: If P is non-unique, \nthen assignment is safe. If P is unique then the assignment is safe only if all nodes in P s structure \nthat are uniquely accessible (that is, its prefix that is about to be\u00adcome inaccessible), absorb threads \nonly from those same prefix nodes.  Points 4.la and 4.2a are simde: losimz a thread does not threaten \nthe invariant. Point ~. lb is als~easy to underst and. The interesting case is Point 4.2b, which has \nan impor\u00adtant subtlety when Q is also a link and shares part of P s structure, as frequently happens \nwhile deleting nodes. In that case and at that point, the structure shared by both P and Q is not uniquely \nreferenced, and so the critical nodes are those accessible from P but not Q (or any other link). This \ncase is very useful, for instance, when deleting a node from the middle of a singly linked list. (vi. \nFigure 5.) On entering a new block that declares a local pointer variable, for the purposes of this analysis \nthe uninitialized pointer should be treated as a thread. Similarly, any variable that is released on \nblock termination can be treated as if NIL were assigned to it there, That is, it loses its role as a \nroot. A particularly interesting case arises when unique pointers are returned as values from a function \nto the calling environment. Formal typing depends on our ability to identify unique references automatically, \nwhich work is in progress, The live/dead domain, T, has only two points; live is 1, dead is T. If any \npointer declaration in a program can be vali\u00addated as T and maintain the invariants, then reference coun\u00adters \ncan ignore it and garbage collectors need never traverse it, A type checker needs help getting started, \nbecause no explicit constants, either atoms or functions, locally distin\u00adguish 1 from T. Either the programmer \nmust identify links as seeds, or an eager compiler might optimistically conjec\u00adture the threads. (Fat \nchance!) In either case, the checker must validate these invariants. 5 Experience The perspective above \nis a result of experience with a de\u00adsign for hardware that was built [28] (in prototype without 44 \ninitialization of its live/dead tag) and subsequently used in serendipitous ways that were not anticipated \nat its design [26]. The short story is that tagging of threads proved most useful at both development \ntime and at run time; code writ\u00adten with this perspective was more succinct and reliable, and would (with \nthe designed hardware tagging) run faster. 5.1 Reference-Counting Hardware The hardware, briefly described \nbelow, understands and acts on dynamic live/dead tags at run time. That experience led to our vision \nof static link/thread typing at compile time. The reference-counting memory (RCM) was designed in 1984 \n[26] and built in 1989 [28] as an experiment in rapid prototyping, in digital design derivation, and \nin memory support for multiprocessing. The uniprocessing hardware currently supports an elementary SCHEME \ncompiler and di\u00ad rect manipulation through languages like C or C++. The latter programming style is reported \nhere. At its core, RCM is a heap of 8-byte nodes, each of which is either a homogeneously atomic datum \n(like a floating\u00adpoint number) or two 4-byte pointer fields. A write to the former, atomic type does \nnot invoke reference counting, but writing to the latter, binary node does. (Any pointer cache must be \ntreated as write-through.) When a node is allo\u00adcate, by reading its address from either of two distinguished \naddresses, a hidden tag on the node is set, distinguishing between these two types. Each 4-byte field \nalso carries a hidden, hind-wired live/dead tag from T in hardware, associated with its cur\u00adrent content. \nThat tag is both used and reset as each field is written. The tag on the old content, being destroyed, \nde\u00adtermines whether a decrement must be dispatched to that address (if it were a link). The type of the \nnode (if it is a binary node) determines whether the field is to be tagged as live and, inseparably, \nwhether an increment is dispatched to that address. The three tags, just described, ride with a node \nthrough\u00adout its lifetime and back through AvAILable space as the node is recycled. Although they reside \nat the address of the node, they do not consume the address space usually asso\u00adciated with main memory; \nhidden memory contains the tags and the reference counts at the same address. (This illusion contrasts \nwith Baker s assertion [3] that reference counting consumed both address space and processor cy\u00adcles. \n) Although the type is reset at allocation, the content and live/dead tags remain meaningful until a \nwrite instruc\u00adtion changes both them and the visible content of the node. That is, when a binary node \nis first allocated to receive two links, it may yet contain dead bit patterns from its for\u00admer incarnation \n(e.g. as an atom. ) Cent rariwise, when an atom is first allocated, it might yet contain live, counted \nref\u00aderences to archaic structure that only becomes collectible as its content is overwritten, as those \nlive references dispatch decrements. and as their reference counts reach zero. This is a hardwired revision \nof an algorithm due to Weizenbaum [23, p. 527]. RCM S design also contains an unimplemented provision \nthat a pointer can be tagged (in its units bit that should be O with word addressing) as a thread. It \nthat bit were 1 as a pointer were written i~~o a binary node, not only would the usual increment be cancelled \nbut also that field would be tagged as dead aa if it were in an atomic node. Intended to provide circular \nreferences [27], that protocol is simulated in this work by three write instructions that tell RCM to \nwrite the pointer as if it were live;  to decrement the reference count of its cent ent, just increment \ned;  to reset the tag on that field to be dead, canceling the future decrement.  Therefore, it now \nrequires two more control instructions to simulate the single write, as designed for multiprocessing. \nSpeculation that this work triples the timing, however, is in\u00adappropriate because the implementation \n(on a Nu-bus with NEXT S controller chip) muddles such analyses [28, $2]. As mentioned, we discovered \nthat this simulated instruc\u00adtion proved to be far more useful than merely to build simple circular lists. \nIndeed, our experience implementing a sample data base showed that it handled many kinds of cycles and \nreduced reference counts, even in acyclic structures. 5.2 Skippy-list example As a demonstration of \nRCM and of the impact of the proper use of threads there, a very simple example was derived from the \nmodel of skip lists. Its main purpose here is to charac\u00adterize the impact on storage management of distinguishing \nlinks from threads, and to lay a foundation for understand\u00ading the tables in the next subsection. A skip \nlist [18] is a search structure generalized from a sorted linear list, with additional pointers woven \ninto it that atlow its search algorithm to take long strides. As defined, it is an acyclic structure. \nAn immediate observation here is that all the additional pointers should be statically typed as threads, \nso that all its linear links are unique. The exer\u00adcise is to allocate a list of length 1000, 2000, 5000, \nor 10,000, and then to release it merely by overwriting the unique ref\u00aderence to its root. A simple program \nwas written in C using RCM that represents a node in a level-l skip list as two RCM nodes. One FtCM node \nlinks the spine of the skip list. The other RCM node contains the search key, and every-other one has \nan additional pointer. In order to generate a cyclic structure, however, that additional pointer is corrupted \nto point to a random sibling. We call it a skippy list here to suggest that it is so very like a skip \nlist. A skippy list is very simple, highly circular, and fairly useless. Four tests were run, three times \neach, and the average counts are presented in Table 1. Details of the tests are described below. The \ncolumns are headed by the length of the skippy list, with the last column extrapolating a rough ratio \nof the counts in that row to the length of the list. Since each node in the skippy list is represented \nby two RCM nodes, each wit h two fields, two reads and four writes are required just to allocate and \ninitially fill it. Four tests compare performance under different manage\u00adment modes. The first is RCM \nas built, which requires extra writes to reset the dead tag and count on any thread; also, an extra RCM \nwrite is required to nail successively newer roots to the skippy list. The second set of counts is identi\u00adcal, \nbut adjusts for those resets, which ought to have been done while writing the pointer. The third test \ndoes not use threading as a node is allocated, but must traverse the skippy list to reset them later \nas the list is released. The last test neglects this traversal, allowing the highly cyclic list to confound \nthe reference-count machinery and leaving almost all nodes to C s garbage collector (:-). 45  Test Count \nI lk nodes I 2k nodes I 5k nodes I lUk nodes 11Natlo Real RCM I Reads I 9499 I 18999 I 47499 I 94999 \nII 4.75 Writes 12001 24001 60001 120001 6 Ideal RCM Reads 9499 18999 47499 94999 4.75 Writes 7500 15000 \n37500 75000 3.75 No threads Reads 12998 25998 64998 129998 6.5 Writes 10495 20999 52500 104999 5.5 No \ncleaning Reads 9499 18999 47499 94999 4.75 Writes 7500 15000 37500 75000 3.75 GC nodes 1988 3996 9999 \n19997 2 Table 1. Counts of memory hits for a skippy-list example. The results show that we really should \nhave built thread\u00ading initialization into the RCM hardware. The cost to reset the dead tags is visible, \ntwo writes per node, but not as bad as the traversal to erase the cycles at release, which costs 70~o \nmore probing. And, finally, a mutator runs faster when it can completely ignore storage management, provided \nhere by RCM.  5.3 Relational Object Database The Relational Object Database (ROD) is a system for cre\u00adating, \nmanipulating, and searching objects and relations be\u00adtween objects. It was originally conceptualized \nfor checking dependencies between program constructs. For example, a function ~ can be related in ROD \nto another function g: f uses g. Once it is known that uses has an inverse and stated to the database \nthat ~ uses g, ROD infers the in\u00adverse relationship of uses and also makes it a known fact that g is-used-by \nj. ROD infers such symmetry and an\u00adswers questions by searching its database, a dynamic struc\u00adture like \nany typical database. ROD was originally written in C, using Iinked lists and arrays for data structures, \nat New Mexico State Univer\u00adsity in Fall 1993 and used there in later semesters [14], In Spring 1995, \nROD was converted to run using the Reference-Counting Memory at Indiana University, as an experiment \nto excise explicit memory management from the ROD sys\u00adtem, replacing it with RCM S hardware support. \nIn a sense this was to be a test of the genertilty of RCM because ROD had been designed independently \nof it. The basic List structures of ROD follow: 5.3.1. 5.3.2. 5.3.3. 5.3.4. A global doubly-linked list \nof types. Every object is classified as exactly one type, such as function or pro\u00ad cedure. A global doubly-linked \nlist of relations containing the name and a reference to its inverse relation (in this list) if an inverse \nis known. A global doubly-linked list of objects that are being manipulated, each cent aining the name \nof the object, a pointer to its type (in List 5.3.1), and a doubly-linked list described as List 5.3.4. \nEach object occupies two RCM nodes in the tests, but the space is static, A doubly-linked list hung off \neach object in List 5.3.3 representing relationships to other objects contained in List 5.3.3. Each contains \na pointer to a relation (in List 5.3.2) and a pointer to the object-being-related\u00adto (in List 5.3.3). \nIf appropriate, it also contains a pointer to the corresponding inverse relationship in the list of relationship \nthat is hung off the obiect-bein~\u00adrelated-to (in List 5.3.3). Each relationship occupies three RCM nodes \nin the tests, forty relationships per object, and this space is recycled. Figures 1 6 show an example \nthese structures. The first discovery in linked lists are, indeed, all counterpointers are lustrated \nby the solid that includes the last two of this exercise was that doubly easy for RCM to manage if \nthreads. This convention is il\u00adpointers paired with inverted, dashed pointers in Figure 1. The convention \nis used in implementing all four kinds of lists. As illustrated in Figure 1, this casts any list as \nsingly-linked with re\u00ad spect to links, and Lists 5.3.3 and 5.3.4, of objects and their relationships, \nbecome a simple tree (ignoring, for a moment, the pointers to inverse relationships.)  A second insight \nis that a queue (in this case the ex\u00adtant queue of objects, doubly linked as above) should also have \nits REAR pointer as a thread. This conven\u00adtion suffices to reduce a redundant reference counts, but is \nnot necessary to recover a cycle. It was a clue that the live/dead tag could be useful beyond circu\u00adlar \nstructures. As sketched in Figure 1, it makes the queue everywhere uniquely referenced.  The interesting \ndiscovery is that both Dointers to in\u00adverse relations~ips, which must necessarily be symmet\u00adric, should \nalso be threads; their manipulation is ex\u00adplained in some detail below. That is, the pair of rela\u00adtionships, \nthat is a single relationship and its inverse hanging from two different objects, contain a trivial \n cycle that need not ers were live then the to recover just those completes the picture respect to live \nnodes.) to destroy all objects one cycle. be counted. If these two point-RCM exercise would have failed \nnodes. This last step, moreover, of the object list as a tree (with It makes it simple, for instance, \nand the relationships on them in The most interesting operation on the ROD data struc\u00adtures, deletion \nof an element from the list of relationships (List 5.3.4), is now reviewed as an illustrative example \nof how the system works, and of how it uses dynamic live/dead tags. While deleting a relationship node, \nits inverse relation\u00ad ship if there is one must be deleted at the same time. Using live and threads, \nthe relationship structures can be arranged so that all the nodes can be deleted as a chain re\u00adaction \nwith the deletion of one node. Figures 1 6 show the sequence of event in a node deletion. 46 Figure \n1.: Solid arrows are links and the dotted arrows are threads. The pointer O refers to the structure contain\u00ading \nboth the HEAD and TAIL of the object queue [25]. The nodes S1 and S3 are relationships that are inverses \nof each other, relating objects 01 and 03. That is, 01 uses 03 and 03 is-used-by 01 . P is a reference \nto the relationship node to be deleted, S1. The only node in Figure 1 with a reference count greater \nthan one is S1; P emits its second reference. Figure 2.: SI is removed from the relationship list hanging \nfrom 01. [Points 4.2a, and 4.2b without uniquely ac\u00adcessible nodes.] Its reference count drops to one. \nThe reference count on SZ increases because it is now ref\u00aderence from both S1 and 01. Figu~e 3.: The \npointer from S1 to S3 is resurrected. [Point 4.la,] S3 s reference count rises to two. Figure 4.: S3 \nis removed from the relationship list hang\u00ading from Os, and its reference count drops to one. [Points \n4.2a, and 4.2b without uniquely accessible nodes.] The reference count on S1 increases because it is \nnow referenced from Ss and OS. P now points to a linked structure whose uniquely referenced pre\u00adfix, \nup to the sharing at S4, comprises the nodes to be removed. Figure 5.: The reference from P is lost. \n[as if P + NIL; Point 4.2b without any incoming threads.] The refer\u00adence count on S1 automatically drops \nto zero and can be recycled. When in due tours-Sl is overwritten, the reference count on SS will also \ndrop to zero and it will be recycled. No further traversal is necessary on RCM. Figure 6.: The ROD data \nstructures after deletion. All ref\u00aderence counts have decremented to reflect accessible references. The \nfollowing test was run to exercise RCM and to reveal the impact of proper threading. The data structures \nabove were build up serially for 500, 1000, 2000, and 2500 objects, each participating in 40 relationships \nwith its peers. Then the relationships were removed in a random order, ideally releasing all their space. \nThe order that the structure is built is immaterial, since new nodes are insert ed at the front of List \n5.3.4. Remov\u00ading them in random order causes that list to be traversed repeatedly, searching for the \nrelationship to be removed. So the exercise becomes read-intensive. The results appear in Table 2, read \nlike Table 1. Again, the experiments were run three times and the ratios are remarkably stable. There \nare far more reads than writes! but writes will have cost more because of their dirty-bit and write-through \nrequirements. Installing hardware support for writing threads to RCM would save about six writes per \nnode, arising from the three threads there; this is about 5.5% of the memory cycles. Al\u00adternatively, \nthe relationships can be traversed and returned by ad hoc unthreading code ( no threads ), but more reads \nand one more write to each node are needed; in fact, the cost without threads is quite nominal because \nRCM only returns a very few nodes at a time here. Finally, the test, run without returning any nodes \nat all, obtains the timings for the idealized RGM, but leaves behind all the space for garbage collection. \n6 Conclusions The point of this paper is to identify the domain T of links fthreads as an attribute or \ntype to be used by the pro\u00adgrammer or the compiler to improve the efficiency and effi\u00ad~acy of run-time \nstorage management. An important sym\u00adbiosis exists between static typing by T, and identification of \nunique references. Killing pointers will decrease the aggre\u00adgate reference count in a ~ystem, making \nit easier to discover uniquely referenced nodes. On the other hand, accurate identification of unique \nreferences may help to maintain In\u00advariant 1, through more effective elimination of inaccessible pointers \nfrom the verification problem. The experiments show that run-time use of threads, both by hardware and \nby the programmer, improves storage man\u00adagement. The number compares favorably with the best col\u00adlectors \nwithout requiring extra memory for recopying. The result ing savings can be significant even for C programs. \nIn\u00ad tegrating threads as a static type at compile-time remains as futume work. Uniquely referenced nodes \nshould be identified at com\u00ad pile time by a type system or at run time by tagging. Then they can be \nreused through purely local transactions (while resident in cache) or recycled with less resources, even, \nthan idealized collection [1]. Baker offers a good example of this behavior [4]. Distinguishing threads \nfrom links has the ef\u00adfect of reducing reference counts, making unique references more frequent and more \nfrequently useful. Generation-scavenging, however, will suffer from viola\u00adtions on the genetic order \nof addresses that arise from this sort of in-situ reuse. When measuring the locality of genera\u00adtion scavenging, \nits advocates should better address locality lost in the rnutator whenever it avoids reuse in order to \nsup\u00adport the collector. In contrast, multiprocessor architectures demands such reuse: both to preserve \nlocality in the mu\u00adtators and to avoid synchronizations among the collectors. We offer static threading \nand compile-time space analysis as tools necessary to realize the dream that Functional Pro\u00adgramming \nwill yet become a lingua jranca for parallel pro\u00adcessing. The original purpose of this work was an exercise \nto test the generality of the hardwired implementation of reference\u00adcounting memory. In extending this \ndevice to handle the typically circular structures from data-base systems, we dis\u00adcovered that the distinction \nof link/thread was far more use\u00adful, and far more important than was realized when it was designed and \nnot built into our prototype. And RCM S effectiveness, as described in Section 5.1, was considerably \nimproved by enforcing the invariants, above. Since it is likely that low-level programmers now subliminally \nuse such a type to manage storage manually, it becomes important for auto\u00admated managers (even garbage \ncollectors) that would strive for high performance to recognize it and to use it well. 7 Acknowledgements \nWe thank Henry Baker for critical help in tracking down origins of the term iweak pointer, and referees \nfor suggest\u00ad ing clarification between static and dynamic threads. High recognition for Caleb Hess and \nWillie Hunt who build the RCM prototype! 47 ~-f ___________________ )   I m------------------\u00ad 01o* \n03 ----\u00ad /- J__ +4  II-\u00ad ~ s - +-----------\u00ad 1 S3 ,_____________ @--------------- ----\u00ad /~. / S2 \n1 S2 Figure S4 Figure 2 S4  b:;; :. ti b  TJ17 I o E_________________ --, I m------------------\u00ad v \n>> 0, 0203 . -------- --L-+4 --1.- I -\u00ad 1! I (_ > S1: S3 *,------------\u00ad  +izk== 1 -.) --1 /--I \n(( J i: 9 s~ Figure 4 S4 s~ Figure 3 S4 v-lb II 1  Ku  48 Number of objects 500 1000 Number of \nrelationships 20,000 40,000 Real RCM Reads 2,051,511 4,105,761 Writes 329,131 658,381 Ideal RCM Reads \n2,051,511 4,105,761 Writes 209,711 419,461 No threads Reads 2,120,982 4,244,982 Writes 229,382 458,882 \nNo cleaning Reads 2,051,511 4,105,761 Writes 209,711 419,461 GC nodes 60,000 120,000 Table 2. Counts \nof memory References [15] [1] A. W. Appel. Garbage collection can be faster than stack allocation. Inform. \nProc. Ltrs. 25, 4 (June 1987), 275-279. [16] [2] A. W. Appel &#38; Z. Shao. An empiricaJ and analytic \nstudy of stack vs. heap cost for languages with closures. [17] J. Fbnct. Programming (to appear).  \n[3] H. G. Baker, Jr. List processing in real time on a serial computer. Comm. ACM 21, 4 (April 1978), \n280-294. [18] [4] H. G. Baker, Jr. Lively linear L1sP Look Ma, no garbage. ACM SIGPLAN Notices 27,8 (August \n1992), [19] 89-98. [5] D. W. Clark and C. C. Green. A note on shared struc\u00adture in LISP. Inform. Proc. \nLtrs. 7, 6 (October 1978), [20] 312-314. [6] J. Cohen. Garbage collection of linked data structures. \n[21] 6 omput. surveys 13, 3 (September 1981), 341-367. [7] G. E. Collins. A method for overlapping and \nerasure of [22] lists. Comm. ACM 3, 12 (December 1960), 655-657. [8] L. P. Deutsch &#38; D. G. Bobrow. \nAn efficient, incremen\u00adtal, automatic garbage collector. Comm. ACM 19, 9 [23] (September 1976), 522-526. \n[9] D. P. Friedman and D. S. Wise, Reference counting can [24] manage the circular environments of mutual \nrecursion. Inform. Proc. Ltrs. 8, 1 (January 1979), 41-44. [10] W. Goldman. The Princess Bride, (screenplay). \nNelson [25] Entertainment &#38; Twentieth-Century Fox Film Corp. (1987), Miracle Max scene. [26] [11] \nE. Horowitz &#38; S. Sahni. Fundamentals of Data Struc\u00adtures in PASCAL, Rockville, Md, Computer Science \nPress (1983). [12] D. E. Knuth. The Art of Computer Programming, I, Fundamental Algorithms (2nd cd.), \nReading, MA, [27] Addison-Wesley, (1973.) [13] H. R. Lewis &#38; L. Dennenberg. Data Structures &#38; \ntheir [28] Algorithms, New York, HarperCollins (1991). [14] D. Liles, P. Mamnami, R. Sinclair, J. Walgenbach, \n&#38; S. Williams. ROD User s Guide. Class notes for Soft\u00adware Development, Computer Science Dept., \nNew Mex\u00adico State Univ. (Spring 1994).  2000 2500 80,000 100,000 Ratio 8,214,261 10,268,511 102.68 \n1,316,881 1,646,131 16.46 8,214,261 10,268,511 102.68 838,961 1,048,711 10.48 8,492,982 10,616,982 106.17 \n917,882 1,147,382 11.47 8,214,261 10,268,511 102.68 838,961 1,048,711 10.48 240,000 300,000 3 hits \nfor ROD. J. S. Miller. MultiScheme: a Parallel Processing System Based on MIT Scheme, Ph.D. dissertation, \nMass. Insti\u00adtute of Tech. (1987). Moon, David A. MA CLISP Reference Manual, Project MAC, Mass. Institute \nof Tech. (April 1974). Y. Park &#38; B. Goldberg. Static analysis for optimiz\u00ading reference counting. \nInfo. Proc. Lett. 55, 4 (August 1995), 229 234. W. Pugh. Skip lists: a probabilistic alternative to bal\u00adanced \ntrees. Comm. ACM 33, 6 (June 1990), 668 676. A. J. Perlis &#38; C. Thornton. Symbol manipulation by threaded \nlists. Comm. ACM 3, 4 (April 1960), 195\u00ad 204. T. Standish. Data Structure Techniques, Reading, MA, Addison-Wesley \n((1980). R. Tarjan. Finding dominators in directed graphs. SIAM J. Comput. 3, 1 (March 1974), 62-89. \nD. N. Turner &#38; P, Wadler. Once upon a type. Confer\u00adence on Functional Programming and Computer Archi\u00adtecture. \nNew York, ACM Press (1995), 1-11. J. Weizenbaum. Symmetric list processor. Comm. ACM 6, 9 (September \n1963), 524-544. J. Weizenbaum. More on the reference counter method of erasing list structures. Comm. \nACM 7, 1 (January 1964), 38. D. S. Wise. Referencing lists by an edge. Comm. ACM 19, 6 (June 1976), 338-342. \nD. S. Wise. Design for a multiprocessing heap with on-board reference counting. In J. P. Jouannaud (cd.), \nFunctional P~ogramming Languages and Computer Ar\u00adchitecture, Lecture Notes in Computer Science 201, Berlin, \nSpringer (1985), 289-304. D. S. Wise. Stop-and-copy and one-bit reference count\u00ading. Inform. Proc. Ltrs. \n46, 5 (July 1993), 243 249. D. S. Wise, B. Heck, C. Hess, W. Hunt, and E. Ost. Uniprocessor performance \nof reference-counting hard\u00adware heap. Technical Report 401, Computer Science Dept., Indiana Univ. (June \n1994). 49  \n\t\t\t", "proc_id": "232627", "abstract": "Identifying some pointers as invisible threads, for the purposes of storage management, is a generalization from several widely used programming conventions, like threaded trees. The necessary invariant is that nodes that are accessible (without threads) emit threads only to other accessible nodes. Dynamic tagging or static typing of threads ameliorates storage recycling both in functional and imperative languages.We have seen the distinction between threads and links sharpen both hardware- and software-supported storage management in S<sc>CHEME</sc>, and also in C. Certainly, therefore, implementations of languages that already have abstract management and concrete typing, should detect and use this as a new static type.", "authors": [{"name": "David S. Wise", "author_profile_id": "81100462823", "affiliation": "Computer Science Department, Indiana University, Bloomington, Indiana", "person_id": "PP39043901", "email_address": "", "orcid_id": ""}, {"name": "Joshua Walgenbach", "author_profile_id": "81100260432", "affiliation": "Computer Science Department, Indiana University, Bloomington, Indiana", "person_id": "P149719", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/232627.232634", "year": "1996", "article_id": "232634", "conference": "ICFP", "title": "Static and dynamic partitioning of pointers as links and threads", "url": "http://dl.acm.org/citation.cfm?id=232634"}