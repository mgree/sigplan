{"article_publication_date": "06-15-1996", "fulltext": "\n pHluid: The Design of a Parallel Functional Language Implementation on Workstations Cormac Flanagan \nRice University Department of Computer Science Houston, Texas 77251-1892, USA cormac@cs rice. edu Abstract \nThis paper describes the distributed memory implemen\u00ad tation of a shared memory parallel functional language. \nThe language is Id, an implicitly parallel, mostly func\u00ad tional language that is currently evolving into \na dialect of Haskell. The target is a distributed memory machine, because we expect these to be the most \nwidely available parallel platforms in the future. The difficult problem is to bridge the gap between \nthe shared memory language model and the distributed memory machine model. The language model assumes \nthat all data is uniformly ac\u00ad cessible, whereas the machine has a severe memory hi\u00ad erarchy: a processor \ns access to remote memory (using explicit communication) is orders of magnitude slower than its access \nto local memory. Thus, avoiding com\u00ad munication is crucial for good performance. The Id lan\u00ad guage, and \nits general dataflow-inspired compilation to multithreaded code are described elsewhere. In this pa\u00ad \nper, we focus on our new parallel runtime system and its features for avoiding communication and for \ntolerat\u00ad ing its latency when necessary: mukithreading, schedul\u00ad ing and load balancing; the distributed \nheap model and distributed coherent cacheing, and parallel garbage col\u00ad lection. We have completed the \nfist implementation, and we present some preliminary performance measure\u00ad ments. Keywords: parallel and \ndistributed implementations; garbage collection and run-time systems; data flow. 1 Introduction This \npaper describes the distributed memory implemen\u00ad tation of a shared memory parallel functional language. \nThe language is Id [19], an implicitly parallel, mostly functional language designed in the datailow \ngroup at MIT. Id is semantically similar to Haskell [14] and is in fact currently evolving into pH [I], \na dialect of Haskell. Permission to make digitablwtrd copy of pal or ail of thk vmrk for pwsonal or classroom \nuse is ranted without fee provided that copies are not made or distributed for pro k or commercial advanfa \ne, the copyright notice, the title of the publication and ik date appear, an8notice is dven that copying \nis by permission of ACM, Inc. To copy otherwise, to mpublii, to post on aervars, or to redistribute to \nIis&#38;, requires prior apaoifio parmieaion and/or a fee. ICFP 96 5/96 PA, USA 01996 ACM o-69791 -771 \n-W6/0005...$3SO Rlshiyur S. Nikhil Digital Equipment Corp. Cambridge Research Laboratory  One Kendall \nSquare, Bldg. 700 (~ambridge, Massachusetts 02139, USA nikhil@crl. dec. com Economics seems to dictate \nthat most scalable parallel platforms in the next five to ten years will be clusters of SMPS (symmetric/shared \nmemory multiprocessors), i.e., machines consisting of a number of nodes that com\u00admunicate using message \npassing over a switched inter\u00adconnection network, where each node may be a small SMP (bus-based, 2-4 \nprocessors). Larger shared mem\u00adory machines are of course possible, as demonstrated by the Stanford DASH \nmultiprocessor [17], and the I&#38;R machines [15], but they are likely to be high-end ma\u00adchines and \nnot widely available. Further, scala le shared memory machines (like the DASH) are also b lt with physically \ndistributed memories for scalability, k d face some similar problems with their memory hierarchies. Even \nat small numbers of processors (such as 4 to 8), many more people are likely to have access to clusters \nof uuiprocessors of that size than SMPS. Thus, our target for Id is a distributed memory, message passin \nma\u00adchine. In our initial implementation, each nod i is a conventional uniprocessor workstation, not an \nSMP. We know how to extend this to exploit SMP nodes and be\u00adlieve it will be easy (handling distributed \nmemory is the major hurdle). The difficult problem is to bridge the gap between the shared memory model \nof the language and the distribu\u00adted memory model of the machine. In Id, as in Haskell, SML and Scheme, \nthe language model assumes that all data is uniformly accessible, whereas the machine has a severe memory \nhierarchy: a processor s access to re\u00admote memory (using explicit message passing) is typi\u00adcally orders \nof magnitude slower than its access to local memory. Thus, minimizing communication, avoiding communication \nif possible, and tolerating the latency of remote operations, are all crucial for good performance. The \npHluid system is a compiler and runtime system for the Id language that we have been building at Digital \ns Cambridge Research Laboratory for some years. Based on ideas originating in dattiow architectures [11] \nthe compiler produces multithreaded code for conventional machines. In this paper, we focus on novel \naspects of a new par\u00adallel runtime system for pHluid, in particular features that avoid communication \nand tolerate its latency when necessary: multithreadlng, scheduling and load balanc\u00adin~; a distributed \nheap model and distributed coher\u00ad lThe name pHluid IS a play on Id, pH and datajlow. ent cacheing, and \nparallel garbage collection. We also present some preliminary performance data. 2 Background on Id and \nits compilation to mul\u00adtithreaded code FThe importmt messages of this section are summarized ~n its last \nparagraph. Readers familiar with topics such as Id, Id compilation, dat aflow, message driven execu\u00adtion, \nfine grain multithreading, etc. may wish to skip this section and just read the last paragraph.] Id [19] \nis an implicitly parallel, mostly functional, lan\u00adguage designed in the dataflow group at MIT. It has \nmany features common to other modern functional Ian\u00adguages like Haskell and SML higher-order functions, \na Hindley-Milner polymorphic type system with user\u00addefined algebraic types, pattern-matching notation, \nar\u00adray and list comprehensions, etc. The main novelty of Id is its implicitly parallel evaluation model: \neverything is evaluated eagerly, except for expressions inside con\u00additionals and inside lambdas. This \nis described in more detail in [19], but a key behavior relevant to this paper is that most data structures \nhave I-structure semantics: given an expression of the form: cons el e2 we allocate the data structure \nand evaluate e 1 and e2 in parallel. The reference to the cons cell is immediately available as the result \nof the expression. Any consumer of the data structure that attempts to read the head (or the tail) of \nthe cell will automatically block, if necessary, until el (or e2) has completed evaluation and is available \nin the data structure2 The major phases of our pHluid compiler for Id are: Parsing, typechecking, simplification, \nlambda lift\u00ading, optimization, etc., eventually producing P-RISC assembler, a fine grain mukithreaded \nab\u00adstract machine code ( parallel RISC ).  Translation and peephole optimization, converting P-RISC \nassembler to Gnu C.  Gnu C compilation, and linking with our new par\u00adallel runtime system, written in \nGnu C.  We use various C extensions provided by Gnu C, such as first class C labels, and mapping C variables \nthat con\u00adtain frequently accessed data such as the heap allocation pointer into specific machine registers. \nFirst class C la\u00adbels allow us to represent fine grain, dynamically sched\u00ad uled threads conveniently. \nThese Gnu C facilities have also been used by other researchers for the same pur\u00adposes. An important \npoint is that the entire compiler is geared towards producing line grain multithreaded 2This non-strict \nbehavior in fact makes Id semantically C1OSW to Haskell than to SML, despite its eager evaluation. Recogniz. \ning this semantic similarity, and because of various other syn\u00adtactic similarities, Id is currently evolving \nmto pH [1], a dialect of Haskell. code for latency tolerance we do not start with a com\u00adpiler for sequential \nmachines and add parallelism as an afterthought. Figure 1 shows an example P-RISC assembler transla\u00adtion \nof the following function that counts the nodes in a binary tree: . def leaves Empty 1 I leaves (Node \nx 1 r) = leaves 1 + leaves r; It is displayed here in graphical form as a control-flow graph, but it \nis trivial to linearize with labels and ex\u00adplicit control transfers. The following features are worth \nnlewe% entry cfp cip T if null?(T) L i { hiload a T[l] ~ r hilosd b ~2] I 4 k  ~FdR~ c: ent~ tl I til \nenttyt2] k join 2i I res. fi+f2 J + msrge return cip cfp ras J Figure 1. P-RISC assembler: An ex\u00adample \nnoting: The function s code can be viewed as a collection of threads, each of which is activated by \nthe arrival of a message (in practice, we optimize away many messages, both statically and dynamically). \nMes\u00adsages always arrive at entry instructions, whose arguments correspond to the message payload.  \nAt the top, a call message arrives, which allocates a frame for this function, and delivers three argu\u00adments \nto the top entry instruction: a continuation frame pointer, cfp; a continuation label (instruc\u00adtion pointer) \nc ip, and the tree itself, T. The if tests if the tree is empty; if so, the result is 1, and the return \ninstruction sends a message to the continuation (cf p, c ip) carrying the result.  If the test fails, \nwe initiate two heap I-structure loads : the hiloads conceptually send messages to the heap locations \nT [ 1] and T [2] requesting their contents, passing the current frame pointer (implicitly) and the labels \na: and b: respectively (explicitly) as their respective continuations. The current function invocation \nthen goes dormant, with no threads active.  The heap location T [1] eventually responds with a message, \nkicking off the thread at a:, placing the value (the left sub-tree) into local variable L. The last action \nof this thread is to initiate a recur\u00adsive call by sending a call message containing the  170 continuation \n(current frame pointer and label c:) and argument subtree L. This recursive call even\u00adtually returns \na message that starts the thread at c:, loading the count of the left subtree into local variable t i. \nSimilar actions take place at b: and and d:. Thus, two threads (from c: and d:) arrive at the join instruction. \nThis instruction counts up the local variable j (initialized to O). Only the last thread proceeds, finding \nthat the count j has reached the terminal count (2); earlier threads die at the j o in. Thus, t 1 and \nt 2 are guaranteed ready when the sum is computed. This final thread computes the sum and returns the \nresult. It is undetermined whether thread a: executes be\u00adfore b: and whether c: executes before d:. \nThey are scheduled asynchronously, as and when mes\u00adsages arrive. Thus, although these threads execute \nin some sequential order (because they all execute on the same processor), the two overall load ac\u00adtions \n(communicating to the heap and back) occur in parallel, and the two recursive function invoca\u00adtions execute \nin parallel. This kind of fundamental attention to latency is quite unique to Id compil\u00aders.  The round-trip \ntime for the request and response messages of a function call of course depend on how much work the function \ndoes, and whether the function call is done locally or remotely. The round-trip time for an hiload depends \non a num\u00adber of factors:  Whether the heap location is local or remote.  The current load on the processor \nthat owns the heap location, which affects how soon it can handle the request.  Whether the heap location \nis empty or not when the request message arrives. Due to the non-strict, I-structure semantics, the pro\u00adducer \nmay not yet have delivered the value; in this case, the request message has to be queued on that location \nuntil the value ar\u00adrives.   However, note that all these sources of delay are handled uniformly by \ncouching the code as mul\u00ad tiple, fine grain, message-driven threads, each of which never suspends. All \nthis was by way of background, and is discussed in more detail in [18]. The important points to remember \nfor purposes of this paper are: Parallelism in pHluid is at two levels: the function call is the unit \nof work distribution across proces\u00adsors. This is real parallelism, in that these proces\u00adsors actually \nexecute simultaneously. Within each function are a number of fine grain threads that are scheduled asynchronously \nbased on message arrival. Thk multithreading is pseudo-parallelism, in that all threads in a function \ninvocation are multiplexed on the same processor, but it is vi\u00adtal because it permits overlapping communication, \nsynchronization and congestion latencies with use\u00adful work, and is a clean model for adaptively re\u00adsponding \nto the dynamic parallelism of the pro\u00adgram. Unlike many other parallel languages, threads in pHluid \nare unrelated to function boundaries. Each function invocation allocates a jrame that con\u00adtains, among \nother things, all the local variables for all the threads of the function. However, be\u00adcause thread invocation \ndoes not involve any frame or stack resource allocation, threads are extremely lightweight.  All functions \nshare the same view of the heap, i.e., the heap is a shared memory.  Threads in pHluid never suspend. \nAll function calls and heap accesses are couched as split-phase transactions: one thread issues a request \nmessage, and a response message later initiates a separate continuation thread. For heap accesses, the \nre\u00adquest message may be queued at the heap location if the value is not yet available. Thus, data access \nsynchronization occurs at heap locations, never in the accessing functions.  Note that checking the \nfzdi/em~tv state of a heap loca\u00adtion is not an issu; tha~ is uniqu~ to I-structure; in Id. Almost exactly \nthe same issue is faced in lazy language implement ations where we have to check whether a lo\u00adcation \nhaa been evaluated yet or still contains a closure. This relationship is not surprising, because they \nare two different ways of implementing non-strictness (and they are both equally difficult to optimize \naway). 3 The new parallel runtime system This section describes the novel aspects of pHluid s new parallel \nruntime system. Recall that our target plat\u00adform is a distributed memory machine: each PE (Pro\u00adcessing \nElement ) is a conventional uniprocessor work\u00adstation. These PEs communicate by explicit message\u00adpassing \nusing active messages [21], i.e., each message contains a code pointer (a handler ), and the message \nis consumed by simply jumping to the handler with the message itself as an argument. The handler is responsi\u00adble \nfor extracting items from the message and executing arbitrary code that may free or reuse the message. \n3.1 Scheduling and Work Distribution 3.1.1 Multithreading When a thread is executed, it may enable other \nthreads. For example, consider the following code fragment de\u00adscribing two threads, starting at labels \na: and b:, re\u00adspectively: a: b: ... h~~oad b T[l] ... foo ...  the thread at a: executes an hi.load \ninstruction to ini- A fast entry point which assumes the call argu\u00ad tiate access to a heap location, \nand continues (at foo). Conceptually, it sends a message to the heap location, which eventually produces \na response message that en\u00adables the other thread at b:. Each processor maintains a scheduling stack \ncontaining threads that are ready to run. Each entry in the stack consists of a code pointer, a pointer \nto the frame of the enclosing function, and a variable number of arguments. A thread is pushed on this \nstack when a response to a split-phase operation is received. When the current thread terminates, the \nsystem pops and executes the next thread on this stack. We avoid a stack empty check on pops from the \nschedul\u00adingstack by including astackempty handler asthebot\u00adtom entry in thk stack. This handler is invoked \nwhen\u00adever the system runs out of executable threads. The handler pushes itself back onto the stack, and \nthen calls the scheduler (described below) to create new threads.  3.1.2 Function calls In pHluid, the \nfunction call is the unit of work distri\u00adbution across PEs. By default, the system chooses how to do \nthis distribution. However, by means of a source code annotation at a function call site, the programmer \ncan direct that the call must run on a specific PE. Prim\u00aditive functions are available to discover the \nPE on which the current function is executing, and the total number of PEs in the current run (this number \nis decided when starting the executable; Id code is not compiled with a knowledge of the number of PEs, \nexpect for the special case where we know we wish to compile for a uniproces\u00adsor). Each function call \nis encoded as a split-phase action. The function call itself involves creating a call record containing \nthe function pointer (or code biock, described below) and its arguments, including the continuation argument. \nThe programmer can annotate anY function call to specify which processor it should run on. When there \nis no annotation (the usual case), the runtime sys\u00adtem chooses where to execute the function. Each pro\u00adcessor \nmaintains two scheduling queues containing call records: The fixed queue cent ains call records that \nmust be executed on this processor.  The stealable gueue contains call records that can be executed \non any processor.  For function calls that have a processor annotation, the call record is dispatched \nto the fixed queue of that pro\u00adcessor. For calls without a processor annotation, the call record is placed \non the local stealable queue. Call records may migrate between the stealable queues of various processors \naccording to the work stealing algo\u00adrithm described below. However, a function call does not migrate \nonce it has begun executing. Each function is described by a code block, which consists of three function \nentry points: ments are already in registers. A fixed queue entry point which assumes that the arguments \nneed to be retrieved from a call record on the fixed queue.  A stealable aueue entrv Doint which assumes \nthat the argume~ts need t: be retrieved from a call record on the st ealable queue.  The scheduler invokes \na call record by jumping either to the fixed or to the steabable queue entry point. The fast entry point \nis used for an optimization described below. The code at the entry point then extracts the srgu\u00adments \nfrom the call record, allocates a frame and starts evaluating the function s body. The function later \nter\u00adminates when one of its threads sends a return message to the function s continuation, deallocates \nthe function s frame, and dies. 3.1.3 Work Scheduling Naive parallel work scheduling algorithms can \nresult in an exponentizd growth of the memory requirements of an application as compared to a sequential \nexecution by creating a very large number of parallel threads, each of which simultaneously requires \nsome local storage. The pHluid work scheduling algorithm is designed to avoid this problem by reducing \nthe number of simulta\u00adneously active functions, while still exploiting the avail\u00adable parallelism to \nkeep all the PEs busy. The scheduler gives work on the scheduling stack a pri\u00adority higher than either \nthe fixed or the stealable queue, in order to complete existing function activations (if possible) before \ncreating new ones. The scheduler also invokes call records on the fixed queue in preference to those \non the stealable queue, since call records on the stealable queue may later be stolen to keep a different \nPE active. The scheduler tries to approximate the depth first traver\u00adsal of the function call tree performed \nby a sequential execution by treating the fixed and stealable queues as LIFO queues, or stacks. When \nwe execute a function call with no processor annotation, or with an explicit processor annotation specifying \nthe current PE, we push the call record on the appropriate local queue. Each PE s scheduler always pops \nwork from these queues. The net effect of our scheduler is that once all PEs have work, each PE tends \nto settle into a depth-fist traversal of the call tree similar to that performed by a sequential implement \nation.  3.1.4 Work Stealing When a PE becomes idle, it sends a steal message to a randomly-chosen victim \nPE. The steal message at\u00adtempts to steal a call record from the victim PE s steal\u00adable queue. Of course, \nthe victim PE may not have any work to steal, in which case the idle PE randomly chooses a new victim \nPE. In order to avoid repeatedly bothering active PEs with steal requests they cannot satisfy, we use \na linear backoff scheme to decide how long the idle PE must wait before asking that victim PE again, \ni,e., the wait time increases linearly with the number of times we fail. This scheme successfully adapts \nto both low\u00adgramdarit y and high-granularity computations.3 If the victim PE s stealable queue is nonempty, \nthen the steal message returns the call record from the bottom of that PE s stealable queue (i. e., away \nfrom the stack-like end) to the idle PE. The reason for this approach is that items deeper in the stack \nare likely to represent fatter chunks of work, being higher in the call tree [12, 16]. The net effect \nof our work-stealing algorithm is that we do not gratuitously fork functions to other PEs. In\u00adstead, \nwork request messages are only sent when some PE is idle, and call records only migrate from busy PEs \nto idle PEs. 3.1.5 Optimizations The pHluid system is designed using general message\u00adsending mechanisms \nthat work in all cases. For example, the split-phase instruction: hiload b TIO] conceptually involves \nsending a hiload message to the appropriate PE, which accesses the memory location T [0], and then sends \na split-phase return message to the original PE. For cases where the original PE contains the memory \nat location T [0], we avoid the overhead of sending these messages by immediately performing the hil \noad oper\u00adation and pushing the enabled thread at b: onto the scheduhng stack. We also avoid the message \nsending overhead in other cases by inlining other split-phase op\u00ad erations where possible. We also optimize \nsplit-phase operations that are imme\u00ad diately followed by a halt instruction. For example, consider the \nfollowing code fragment: a: hiload b TIO] b: ... halt If the hiload operation can be satisfied on the \ncurrent processor, then the pHluid system performs the load and immediately jumps to the thread at b:, \nwithout the overhead of pushing that thread on the scheduling stack and invoking the scheduler. Similarly, \nfor a function call with no processor anno\u00adtation followed immediately by a halt instruction, we avoid \nthe overhead of manipulating the st ealable queue by simply placing the arguments into appropriate regis\u00adters \nand invoking the fast entry point (described above) of that function s codeblock. 3 Our work-stealing \nalgorlthm N a variant of that used in the Cilk system developed at MITIs], Our algorithm was developed \njointly with Martin Carlisle of Princeton University 3.2 Distributed memory model and distributed coherent \ncacheing The memory of each PE is divided into five regions: Compiled code and static data: Static data \nin\u00adcludes data structures that describe codeblocks and the other memory areas, etc. These are simply \nreplicated at the same address on all PEs. The heap: this contains actual Id data structures (constructed \ndata, arrays and closures), and is de\u00adscribed in greater detail below. The store: this area contains \nobjects that are ex\u00adplicitly allocated and deallocated by the runtime system, and which are never accessed \nremotely. These include frames for function invocations, the fixed scheduling queue, heap access requests \nthat are waiting on empty locations, etc. These objects are managed in a freelist organized by object \nsize. Each PE maps its store at a different address, so that we can determine where a frame is located \nby looking at its address (frames are never accessed remotely, but we do need to know a frame s PE so \nthat we can send a message to it, e.g., for sending a heap access response or for sending a result to \na function s continuation). The stealable queue: in principle this could be al\u00adlocated in the store, \nbut because it is manipulated so heavily (since most function calls go through the stealable queue), \nwe allocate a special region for it and treat it as a deque with contiguous en\u00adtries, allowing cheap \npushes and pops from both ends. The scheduling stack: this stack is also allocated in a contiguous memory \narea for performance rea\u00adsons. In a previous sequential implementation, and in an ini\u00adtial parallel \nimplementation, we allocated everything in a single heap (as in the SML/NJ implementation [4, 3]). However, \nwe chose to separate out those objects that can be explicitly deallocated and which are not accessed \nremotely, in order to reduce garbage collection pres\u00adsure. Although explicit deallocation of system objects \nincurs a small overhead, it does not require any com\u00admunication, and significantly reduces the frequency \nof communication-intensive global garbage collection. 3.2.1 The heap Heap objects are represented quite \nconventionally, as contiguous chunks of memory. An object consists of a header word followed by one word \nfor each field (arrays have extra words for index bounds and pre-computed index calculation coefficients). \nWe use the lowest-order bit to tag immediate. Thus, for example, integers lose 1 bit of precision, but \nthis is not a problem on our cur\u00adrent platform, 64-bit Alpha workstations. If a field of a heap object \ndoes not contain an immediate value, it always contains a pointer, either to another heap ob\u00adject or \nto a deferred list of continuations waiting for the field to transition from empty to fuli. Since the \nfield contains a pointer in either case, and since our point\u00aders are always aligned to word boundaries, \nthis frees up another low-order bit to use as a full/empty bit and distinguish between these two cases. \nThis works even if the field is of immedlat e type (e ,g., integer), because for the duration that it \nremains empty, it contains only a pointer and not an immediate, and the full/empty bit is available. \nThe heap is the only area that requires global access. We use the operating system s virtual memory mapping \nfacilities (remap) so that the heap occupies the same ad\u00address range on all PEs. The heap consists of \na number of fixed-size pages , each of which is currently 1 KB in size. We partition ownership of these \npages across the PEs. This, although each PE sees the whole address range, it only owns pages representing \nonly a chunk that is l/P of the total heap size (where P is the number of PEs), and it can only allocate \nobjects into these pages. The remaining pages are treated as a cache for the data owned by other PEs. \nThe advantage of this approach is that heap addresses do not have to be translated or undirected in going \nfrom one PE to another. Further, by examining an object s heap address, a PE can cheaply determine whether \nit owns the object or only a cached copy. The downside of this approach is that large amounts of address \nspace are required as we increase the number of processors. Although this may somewhat limit the scalability \nof our system on 32-bit machines, it is not a problem on next\u00adgeneration 64-bit architectures. We conjecture \nthat the amount of physical memory re\u00adquired per PE will scale in a reasonable fashion, since only a \nsmall portion of the cache pages may actually be used between garbage collection cycles. If this conjec\u00adture \ndoes not hold in practice, we intend to implement a scheme that limits the number of cache pages in use \nat any time by explicitly deallocating cache pages. 3.2.2 Heap cacheing Because Id is a mostly functional \nlanguage, the vast majority of objects in the heap are I-structure objects. These objects are accessed \nvia the hiload operation, therefore we design our heap caching protocol to opti\u00admize this operation. \nEvery heap location is tagged to specify whether it is full or emptg in order to implement I-structure \nsemantics. When a heap object is allocated, all its slots are initially empty, and these slots become \nfull when they are initial\u00adized. Once an I-structures is initialized, it can never be changed. This funct~onal \nnature of I-structures allows us to implement a very simple cache-coherence strategy, The invariant maintained \nby our strategy is that each cache page is always consistent with the corresponding real page, in the \nsense that the only allowable differ\u00ad ence is that a cache page may be empty at a location that is full \n(or defined) in the corresponding real page. We associate a valid bit with every cache page. If a cache \npage is marked invalid, it implies that each I\u00adstructure in the page should be interpreted as being empty. \nInitially, every cache page is marked invalid, thus ensuring that the cache invariant holds at the start \nof program execution. On a local hiload, i.e., an hiload to a heap address that is owned by the current \nPE, we check the full/empty tag; if full, we return the value (more accurately, as de\u00adscribed before, \npush the value and enabled thread on the local scheduling stack). If empty, we queue the hiload s continuation \non the heap location, allocating the queue entries themselves in the store, not the heap. On an hiload \nto a location owned by some other PE, we check our cache at that location. If we re lucky, the location \nis full, and the value can be read imme\u00addiately. Otherwise, we need to retrieve the location s value \nfrom the owner PE of that page. Since we need to communicate with the owner PE anyway, it makes sense \nto request an up-to-date copy of that page at the same time, in the expect at ion that future h i.loads \nto the same page can be satisfied immediately using the new copy of the page. Provided there is no outstanding \nrequest for that page, we simply send a hiload message to the owner of the page, and also request an \nup-to\u00addate copy of the page. However, if there is already an outstanding request for that page, then \nwe queue the hi load locally, and reprocess it as soon as the new copy is received, since the new copy \nmay allow us to satisfy the hiload locally, without performing any additional communica tion. An hist \nore ( heap I-store ) operation always writes through the cache to the owner PE (there, it may cause \nwaiting continuations to be released). However, most hist ores are to locally-owned locations. Consider \na typical Id constructor expression of the form (e 1, e2 ): the heap allocation for the pair, and the \ntwo hist ores initializing the pair are part of the same function, and so the histores are guaranteed \nto be local, since memory allocation is always performed using the local memory of each PE. Because the \nhist ore operation only initializes a pre\u00adviously empty location, it does not affect the consis\u00adtency \nof existing cache copies of the page containing that location. Thus, the PE that owns a page never has \nto send any invalidation messages to other PEs that have previously obtained copies of that page. In \nfact, the owner does not have to keep track of which PEs obtained cached copies, and it is perfectly \nsafe for the owner to send multiple copies of the same page at different times. A new copy of the page \nis guaranteed to be consistent with a previous copy the only differ\u00adences will be that some previously \nempty locations may now be full. A general observation here is that our distributed cache\u00adcoherence protocol \nrelies heavily on the mostly-function\u00adal nature of the source language; the protocol is trivial compared \nto those required for imperative languages [2]. We mentioned that Id is not a pure functional language \nit has side-effecting constructs that operate on S\u00adtructures . However, as in SML and unlike Scheme, \nthese side-effects are constrained by the type system to specific objects and operations, and this allows \nthe compiler to isolate them into special P-RISC assembler instructions: bmload (for heap M-structure \nload ) and hmst ore (for heap M-structure store ). These never interfere wit h the functional hi load \nand h ist ore op\u00aderations, i.e., the same heap location cannot be accessed by both I-structure and M-structure \noperations. Thus, the consistency of I-structure cacheing is not compro\u00admised. Our preliminary implement \nation of these side\u00adeffecting instructions does not involve caching they always read and write through \nto the owner. We expect the overhead to be acceptable on many programs be\u00adcause, as in SML, side effecting \noperations have a much lower frequency compared to normal functional opera\u00adtions. However, programs that \nuse M-structures exten\u00adsively will require a more sophisticated implementation of these operations. \n 3.3 Parallel distributed garbage collection The pHluid system uses a stop-and-copy approach for performing \nparallel garbage collection. Whenever any PE runs out of heap space, all the processors suspend their \ncurrent activities to participate in a global garbage collection cycle. The garbage collector is a parallel \nex\u00adtension of the conventional Cheney two-space copying collector [9]. The first action taken by each \nPE during garbage col\u00adlection is to deallocate the cache pages (using munmap), since these pages will \nbecome inconsistent during gar\u00adbage collection. Special care must be taken with hiload requests queued \non a cache page. These hiload requests are dispatched to the owner PE, where they are queued on the specific \nlocations that they refer to. This commu\u00adnication can amortized by bundling it with other com\u00admunication \nthat is necessary during GC anyway. Next, each PE allocates (with remap) a to-space into which local \nreachable objects will be copied. We ex\u00adpect that physical memory pages previously used for the cache \nwill be reused for the to-space. Each PE maintains two pointers into its to-space in the usual manner: \na scan pointer and a tree pointer. Ob\u00adjects in to-space below the scan pointer only contain pointers \nto other objects in to-space, whereas objects above the scan pointer may refer to objects in from\u00adspace. \nDuring GC, each PE then calls the function move-oh j ect on each of its local roots. This function copies \nan ob\u00adject from the from-space into the to-space. For local objects, this function behaves exactly as \nin a conven\u00adtional Cheney collector it copies the object to the free pointer, incremented the free pointer \nand returns the new address of that object. For non-local objects, the function sends a move-obj ect \nmessage to the owner PE of that object. That PE then copies the object into to-space, and returns an \nactive message that, when invoked, updates the appropriate location in the memory of the original PE \nwith the new address in to-space of the object. Since this operation is implemented in a split-phase \nmanner, the original PE can continue copying other objects while the operation is in progress. To avoid \ndeadlock, the pHluid system uses separate queues for regular computation messages and GC mes\u00adsages. Each \nprocessor treats its incoming computation message queue as part of its root set, and updates all messages \n(via the function move.ob j ect ) to refer only to to-space objects. We ensure that all outstanding com\u00adputation \nmessages are received, and hence updated, by their destination PE during GC by sending a flush mes\u00adsage \nalong the FIFO pipe connecting each pair of PEs. Detecting termination of a global GC cycle is difficult \nbecause each PE only has direct knowledge of its own status, and messages characterizing the status of \nother PEs may be out-of-date by the time they are received. We characterize the status of a PE as inactive \nonce that PE has: 1. Copied all known live objects into its to-space, 2. Updated all entries in the \ncomputation message queue, 3. Received all incoming flush messages, and 4. Received a response to each \nmove-oh j ect split\u00adphase operation.  A PE that is inactive may become active again on re\u00adceipt of a \nmove.obj ect message. The global garbage collection cycle can only terminate when all PEs are in\u00adactive \nsimultaneously. We detect this situation using the following protocol: Conceptually, the PEs are organized \nas a ring. At any time a particular PE (the terminator-PE > ) is respon\u00adsible for detecting GC termination. \nWhen this PE be\u00adcomes inactive, it tries to send a PEs -inact ive? mes\u00adsage around the ring of PEs. If \nthis message encounters an active PE, then that PE becomes the terminator-PE. Otherwise, the message \nis successfully forwarded around the ring and returned to the terminator-PE. At this stage, the terminator-PE \nneeds to ensure that all PEs are still inactive. It does this by sending a PEs-st ill-inactive? message \naround the ring of PEs. If this message encounters a PE which has been ac\u00adtive since receiving the PEs \n-inact ive? message, then that PE becomes the terminator-PE. Otherwise, the PEs-st ill-inactive? message \nis successfully returned to the terminator PE. At this stage we can guaran\u00adtee that all PEs are inactive. \nThe terminator-PE then broadcasts a message announcing the termination of the GC cycle. On receipt of \nthis message, each PE returns an acknowledgement and resumes computation. To ensure that distinct GC \ncycles do not overlap, a PE that exhausts its heap space sends a request for a new GC cycle to the terminator-PE \nof the previous cycle. That terminator-PE waits, if necessary, until all PEs acknowledge termination \nof the last GC cycle, before broadcasting a message initiating the new cycle. 4 Preliminary Performance \nMeasurements We have just completed a fist implementation of the system as described above. We have taken \nsome pre\u00adliminary performance measurements, but we have yet to perform a detailed analysis of these timings \nin order to understand exactly where the time goes. Our implementation (both the compiler and the com\u00adpiled \ncode) runs on Digital Alpha workstations under Digital Unix, using gcc to compile the C object code and \nthe runtime system. The parallel system runs on any network of Alpha workst at ions. Currently, it uses \nUDP sockets as the basic packet delivery mechanism. UDP sockets are unreliable (packet delivery and packet \norder are not guaranteed), so we implemented a thin reliability layer above UDP sockets called RPP, for \nRe\u00adliable Packet Protocol; this is still advantageous over TCP. which is a reliable m-otocol.. for several \nreasons: it is packet based, not stream based, and so does not need packet parsing by the receiver, and \nit automat\u00adically multiplexes incoming packets from all peers into a single receive queue, instead of \nhaving to wait on N receive aueues. one ~er Deer. With RPP. It will be triv\u00ad .,. . ial for us in the \nfuture to exploit faster packet delivery mechanisms, even if they are unreliable. Our parallelism measurements \nwere taken on a par\u00adticular configuration with eight 225 MHz Alpha work\u00adstations with an ATM interconnection \nnetwork, run\u00adning Digital Unix. Communication is still rather ex\u00adpensive relative to computation speed. \nThe following table shows the round trip latency from a UDP socket send to a UDP socket receive, for \nvarious packet sizes: Packet size Round trip latency (Bytes) (psecs) 64 550 128 580 256 680 1024 908 \n4096 1820 The maximum bandwidth is about 12 MBytes/see (close to the ATM maximum). Thus, it is not an \neasy platform on which to get any parallel speedup. For a sequential, uniprocessor baseline, we compared \nan Id program compiled for a single processor and executed on a single processor, with a conventional \n(sequential) C program that implements the same algorithm but uses conventional C idioms. The pHluid \ncompiler was given a flag to produce code for a uniprocessor, so that it could optimize away all message-passing \ncode. For the C compilation phase of the Id program, and for the C program, we used the same gcc compiler \nwith the same -02 optimization level. The program we used does a lot of heap allocation: paraf f ins \n(18) enumerates all paraffin molecules containing up to 18 carbon atoms, unique up to certain symmetry \nconditions the Id pro\u00adgram is described in detail in [6]. The C version did not have any garbage collection \n(it simply had a large enough heap). One version of the C code used mallo c ( ) for each heap allocation, \nand another version did its own allocation out of a large chunk of memory allocated once using sbrk ( \n) at the start of the program. paraf f ins (18) Time (sees) Relative speed pHluid 0.53 lx E (malloc) \n0.40 1.3X C (sbrk) 0.20 2.6x I  We believe that these indicate that pHluid s uniproces\u00adsor performance \nfor Id programs is approaching com\u00adpetitive performance for functional language implemen\u00adtations (the \nstudy in [13] measures the performance of another program, the pseudoknot benchmark. It com\u00adpares a functional \nversion compiled with over 25 func\u00adtional language compilers, and a C version. The best functional versions \nwere about 1.4x, 1.5x, 1.9x and 2.ox slower than C). The following table shows the elapsed time to call \nand return from a trivial function that takes a void argu\u00adment and simply returns a void result (in Id, \nvoid is the trivial type with just one value of type void). We mea\u00adsured this for two cases: a local \ncall, where the function executes on the same PE as the caller, and for a directed remote call, where \nthe function is directed, by means of an annotation, to execute on a different PE from the caller (this \ndoes not involve any work stealing): Local function call 9 psecs Remote function call 1987 psecs Recall \nthat the remote function call involves allocating a call record containing the function pointer, normal \nar\u00adguments and continuation arguments; sending it to the remote PE; at the remote PE, reading the message \nand executing its handler which, in this case, places it on the fixed queue; executing the scheduler \nwhich takes the call record off this queue, allocates a frame, and invokes the function at the fixed \nqueue entry point, which un\u00admarshalls the normal and continuation arguments; and, follows a similar process \nfor the return. The messages in each direction are of the order of 100 Bytes, whose raw round-trip latency \nis about 580 psecs (from the previ\u00adous table); thus, the overhead is quite high, about 1400 psecs, and \nis clearly an area with much room for im\u00adprovement. The following table shows the speed of the hiload \nand hist ore heap access operations. All accesses are to full I-structure locations (but they still perform \nthe tests for emptiness, etc.). Operation (psecs) 1 Local hiload 7 2 Remote hiload, uncached, unmapped \n1466 3 Remote hi.load, uncached, mapped 1196 4 Remote hiload, cached 4 5 Local histore 5 6 Remote histore \n1047 The first line describes an hiload to a heap object that is local, i. e., on the same PE. The next \nthree lines de\u00adscribe an hiload to a field of a remote object, i.e., allo\u00adcated on a different PE. Line \n2 is for the case when the remote object has not yet been locally cached, and the page containing that \nobject has not yet been mapped into the loczd address space. In line 3, the object is not yet locally \ncached, but it is on a page that has been lo\u00adcally mapped (due to an earlier remote access to some other \nlocation on that page). Line 5 is for the case when it has been locally cached. For lines 2 and 3, we \nsend a relatively small request message, and we get back an entire heap page (1 KB in size). The raw \nUDP commu\u00adnication latencies for this are about 225 + 454 psecs. Thus, there is still about 517 ~secs \noverhead beyond the raw communication cost to fetch and install a cache copy of a heap page, plus about \n270 ,usecs if we have to map the page in. Line 4 just shows that hiloads to cached data proceed much \nfaster (we do not know why it is faster than the the local hi load in line 1). Simi\u00adlarly, lines 5 and \n6 are for a local and remote hlstore, respectively. A remote hist ore involves a round-trip communication \n(the return message is treated as an ac\u00adknowledgement that the histore has completed). We have been able \nto measure parallel speedups only for some small programs because of a current prob\u00adlem in an early phase \nof the pHluid compiler (which was implemented before the parallel runtime system) wherein list comprehensions \nare translated into sequen\u00adtial loops, thus eliminating parallelism (for example, our paraf f ins program \nuses list comprehensions heavily, and so we have not yet been able to measure its parallel performance). \nFigure 2 shows speedups for three small programs. The trees program is a synthetic benchmark that creates \na balanced binary tree to a given depth, copies the tree several times, counts the leaves and re\u00adturns \nthe count. nqueens ( 12) is the standard N-queens problem with a board size of 12. matrix-multiply (500) \ncreates two 500x500 floating point matrices, multiplies them, and sums the resulting matrix. The last \nprogram uses explicit distribution of work, directing a sub-matrix computation to be performed on a specific \nPE. These figures were measured on a testbed system with only eight processors. Eight processors is certainly \nwell within the range of SMPS, and one may argue that there is no need to go to a distributed memory \nimple\u00admentation. However, we believe that it is important to look at distributed memory systems even \nat this size, for several reasons. First, eight-processor workstation clus\u00adters are a lot more common, \nand cheaper, than eight\u00adprocessor SMPS. Fhrther, there is a growing trend away from massively parallel \nmulticomputers towards achiev\u00ading the same power by assembling small-degree clusters of small-degree \nSMPS. Although these figures are for relatively small programs and do not supply any diagnostic detail \nabout how and where time is spent, we find them encouraging consider\u00ading that we are using such an expensive \ncommunication layer. It would not have been surprising to see very little speedup at all, or even a slowdown, \nIt appears that our runtime system mechanisms for avoiding, eliminating and tolerating communication \nare effective. We hope to do much more detailed performance studies in the near future. Related Work \n We are not aware of any other implementations of con\u00adventional shared memory functional languages for \ndis\u00adtributed memory machines, that address the cost of re\u00admote communications as aggressively as pHluid \ndoes. There have of course been several implementations on shared memory machines (such as [16] and [7]), \nand im\u00adplementations of message-passing functional languages on distributed memory machines (such as \nErlang [5]). However, implementing a shared memory language on a distributed memory machine appears to \nrequire a sub\u00adst anti ally different approach, with latency-tolerance a high priority throughout the \ndesign. The Berkeley Id/TAM compiler [10] shares the same dataflow heritage as our pHluid compiler and, \nnot sur\u00adprisingly, has many similarities (although they share no code). TAM itself (Threaded Abstract \nMachine) is somewhat similar to our P-RISC Assembler, but our scheduling discipline for threads is quite \ndifferent (these two scheduling disciplines have been compared in detail by Ellen Spertus at MIT, and \nis reported in [20]). The Berkeley Id-on-TAM system has been implemented on dktributed memory machines \nwith relatively fast com\u00admunication facilities, such as the Connection Machine CM-5 and the MIT J-Machine, \nbut not to our knowl\u00adedge on workstation farms. The Berkeley Id/TAM sys\u00adtem does not use distributed \ncacheing, nor does it have a garbage collector. We do not know what mechanism is used for work and data \ndistribution, and if it has any automatic load balancing mechanism. We have become aware of other implementations \nof func\u00adtional languages on distributed memory machines on Transputer networks some years ago, a parallel \nHaskell at Glasgow over PVM more recently, and a distributed memory implementation of Sisal currently \nin progress but we do not know what, if any, latency-tolerance fea\u00adtures they use. All these systems \nstart with a com\u00adpiler optimized for sequential execution, and incremen\u00adtally modify them for parallel \nexecution. In contrast, the pHluid and Berkeley TAM compilers have latency\u00adtolerance at the forefront \nthroughout, and consequently have a very different abstract machine model based on fine grain, message \ndriven multithreading. 6 Conclusion In this paper we have described our implementation of a modern shared \nmemory parallel functional language; the novelty is in the way we target distributed mem\u00adory machines, \nincluding workstations clusters with rel\u00adatively poor communication facilities. Here, aggres\u00adsive latency-tolerance \nis a central preoccupation, and is achieved by a combination of the compilation strategy (producing &#38;e \ngrain threads), a work-stealing mech\u00adanism that avoids distributing work unnecessarily, a distributed \ncoherent cacheing mechanism for the heap that exploits the functional nature of the language, a memory \norganization that reduces pressure for global garbage collection by managing several dynamically al\u00adlocated \nobjects purely locally, and a parallel garbage collector. We also presented some preliminary perfor\u00admance \nmeasurements. Our pHluid implementation is new and still immature, and there is a lot of work ahead: \nmaking it sturdy enough to handle more substantial programs; imple\u00ad menting it on distributed memory \nplatforms with faster interconnects (such as Digital s Memory Channel, which achieves communication latencies \nof less than 5 psecs), Q trees -9 .qu6+ns(i2) \u00ad linear speed.p Ihnw Weed p 88 7 7 6 6 5 5 4 4 3 3 2 \n2 i :/: i :/: ,~ 01 1 01 J 012 5678 012 678 0123 678 Nui%ber of Processors N.kw o! Pmca5sors Number of \nPro&#38;ors Figure 2. Parallel speedups of some small Id programs taking detailed performance measurements \nand charac\u00adterizing the system to understand the bottlenecks; im\u00adproving the heap cacheing model to be \nmore economical in memory use (to avoid each PE having to map the full heap); exploiting SMPS as cluster \nnodes (i. e., us\u00ading shared memory when available); adding support for large distributed arrays (currently \nno object, including arrays, may be larger than one PE s heap); etc. [7] Acknowledgements: The P-RISC \nto Gnu C phase of the compiler was originally designed and implemented on Sparcstations by Derek Chiou \nof MIT s Lab for Com\u00adputer Science. We ported this implementation to our Digital Alpha workstations. \nThe work-stealing algo\u00adrithm with linear backoff was jointly developed with Martin Carlisle of Princeton \nUniversity; he implemented it in Cid, another parallel language implementation at Digital CRL. [8] [9] \nReferences [10] [1] ADITYA, S., ARVIND, AUGUSTSSON, L., MAESSEN, J.-W., AND NIKHIL, R. S. Semantics of \npH: A Par\u00adallel Dialect of Haskell. !n in Proc. Haskell Work\u00adshop (at FPCA 95), La Jolla, CA (June 1995). \n[11] [2] AGARWAL, A., SIMONI, R., HENNESSY, J., AND HOROWITZ, M. An Evaluation of Directory Schemes for \nCache Coherence. In Proc. 15th. Ann. Intl. Sgmp. on Computer Architecture, Hawaii (May 1988). [12] [3] \nAPPEL, A. Garbage Collection Stack Allocation. Information 25, 4 (1987), 275 279. can be Faster than \nProcessing Letters [13] [4] APPEL, A., AND MAC QUEEN, D. B. A Standard ML Compiler. In Proc. Conf. on \nFunctional Pro\u00adgramming and Computer Architecturel Portland, Oregon (September 1987). Springer-Verlag \nLNCS 274. [5] ARMSTRONG, J., VIRDING, R., AND WILLIAMS, M. Concurrent Programming in Erlang. Prentice \nHall, 1993. ISBN: 0-13-285792-8. [6] ARVIND, HELLER, S., AND NIKHIL, R. S. Program\u00adming Generality and \nParallel Computers. ESCOM Science Publishers, P.O .Box 214, 2300 AE Leiden, The Netherlands, 1988, pp. \n255 286. Proc. 4th Intl. [14] Symp. on Biological and Artificial Intelligence Sys\u00adtems, Trento, Italy, \nSeptember 1988. AUGUSTSSON, L., AND JOHNSSON, T. Parallel Graph Reduction with the <nu,G>-machine. In \nProc. Fourth Intl. Conf. on Functional Program\u00admmg Languages and Computer Architecture, Lon\u00addon (September \n1989), pp. 202-213. BLUMOFE, R. D., JOERG, C. F., KUSZMAUL, B. C., LEISERSON, C. E., RANDALL, K. H., \nAND ZHOU, Y. Cilk: An Efficient Multithreaded Runtime System. In Proc, 5th. ACM Syrnp. on Principles \nand Practice of Parallel Programming (PPoPP), Santa Barbara, CA (July 19-21 1995), pp. 207 216. CHENEY, \nC. J. A Nonrecursive List Compacting Algorithm. Communications of the ACM 13, 11 (November 1970), 677-678. \nCULLER, D. E., GOLDSTEIN, S. C., SCHAUSER, K. E., AND VON EICKEN, T. v. TAM A Com\u00adpiler Controlled Threaded \nAbstract Machine. J. Parallel and Distributed Computing, Special Issue on Dataflow 18 (June 1993). GAUDIOT, \nJ.-L., AND BIC (EDITORS), L. Ad\u00ad vanced Topics in Data-flow Computing. Prentice Hall, 1991. HALSTEAD, \nR. Multilisp: A language for concur\u00adrent symbolic computation. ACM Trans. Pro\u00adgram. Lang. Syst. 7, 4 \n(1985), 501 538. HARTEL, PIETER, H., FEELEY, M., ALT, M., Au-GUSTSSON, L., BAUMANN, P., BEEMSTER, M., \nCHAILLOUX, E., FLOOD, C. H., GRIESKAMP, W., VAN GRONINGEN, J. H. G., HAMMOND, K., HAUS-MAN, B., IVORY, \nM. Y., JONES, R. E., LEE, P., LEROY, X., LINS, R. D., LOOSEMORE, S., Ro-JEMO, N., SERRANO, M., TALPIN, \nJ.-P., THACK-RAY, J., THOMAS, S., WEIS, P., AND WENT-WORTH, E. P. Benchmarking Implementations of Functional \nLanguages with Pseudoknot , a Float-Intensive Benchmark. In Workshop on Implemen\u00ad tation of Functional \nLanguages, J. R. W. Giauert (editor), School of Information Systems, Untv. of East Anglia (September \n1994). HUDAK, P., PEYTON JONES, S., WADLER, P., BOUTEL, B., FAIRBAIRN, J., FASEL, J., GUZMAN, M. M., \nHAMMOND, K., HUGHES, J., JOHNSSON, T., KIEBURTZ, R., NIKHIL, R., PARTAIN, W., AND PETERSON, J. Report \non the Programming Language Haskell, A Non-strict, Purely Functional Language, Version 1.2. ACM SIGPLAN \nNotices 27, 5 (May 1992), [15] KENDALL SQUARE RESEARCH. Kendall Square Re\u00adsearch Technical Summary, \n1992. [16] KRANZ, D. A., HALSTEAD JR., R. H., AND MOHR, E. MuLT: A High Performance Parallel Lisp. In \nProc. ACM Syrnp. on Programming Language De\u00adsign and Implementationl Portland, Oregon (June 1989). [17] \nLENOSKI, D., LAUDON, J.l GHARACHORLOO, K., WEBER, W.-D., GUPTA, A., HENNESSY, J., HOROWITZ, M., AND LAM, \nM. S. A. The Stanford DASH Multiprocessor. IEEE Computer (March 1992), 63-79. [18] NIKHIL, R. S. A Multithreaded \nImplementation of Id using P-RISC Graphs. In Proc. 6th. Ann. Wk\u00adshp. on Languages and Compilers for Parallel \nCom\u00adputing, Portlandl Oregon, Springer-Verlag LNCS 768 (August 12-14 1993), pp. 390-405. [19] NIKHIL, \nR. S. An Overview of the Parallel Lan\u00adguage Id (a foundation for pH, a parallel dialect of Haskell). \nTech. Rep. Draft, Digital Equipment Corp., Cambridge Research Laboratory, September 231993. [20] SPERTUS, \nE., AND DALLY, W. J. Evaluating the Locality Benefits of Active Messages. In Proc. 5th. ACM SIGPLAN Symp. \non Principles and Practice of Parallel Programming (PPoPP), Santa Barbara, CA (July 19-21 1995), pp. \n189-198. [21] VON EICKEN, T., CULLER, D. E., GOLDSTEIN, S. C., AND SCHAUSER, K. E. Active Messages: a \nMechanism for Integrated Communication and Computation. In Proc. 19th. Ann. Intl. Symp. on Computer Architecture, \nGold Coast, Australia (May 1992), pp. 256-266. \n\t\t\t", "proc_id": "232627", "abstract": "This paper describes the distributed memory implementation of a shared memory parallel functional language. The language is Id, an implicitly parallel, mostly functional language that is currently evolving into a dialect of Haskell. The target is a distributed memory machine, because we expect these to be the most widely available parallel platforms in the future. The difficult problem is to bridge the gap between the shared memory language model and the distributed memory machine model. The language model assumes that all data is uniformly accessible, whereas the machine has a severe memory hierarchy: a processor's access to remote memory (using explicit communication) is orders of magnitude slower than its access to local memory. Thus, avoiding communication is crucial for good performance. The Id language, and its general dataflow-inspierd compilation to multithreaded code are described elsewhere. In this paper, we focus on our new parallel runtime system and its features for avoiding communication and for tolerating its latency when necessary: multithreading, scheduling and load balancing; the distributed heap model and distributed coherent cacheing, and parallel garbage collection. We have completed the first implementation, and we present some preliminary performance mearsurements.", "authors": [{"name": "Cormac Flanagan", "author_profile_id": "81100538763", "affiliation": "Rice University, Department of Computer Science, Houston, Texas", "person_id": "PP14187273", "email_address": "", "orcid_id": ""}, {"name": "Rishiyur S. Nikhil", "author_profile_id": "81100272692", "affiliation": "Digital Equipment Corp., Cambridge Research Laboratory, One Kendall Square, Bldg. 700, Cambridge, Massachusetts", "person_id": "P243679", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/232627.232646", "year": "1996", "article_id": "232646", "conference": "ICFP", "title": "<italic>pHluid</italic>: the design of a parallel functional language implementation on workstations", "url": "http://dl.acm.org/citation.cfm?id=232646"}