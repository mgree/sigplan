{"article_publication_date": "05-01-1991", "fulltext": "\n Size and Access Inference for Data-Parallel Programs Siddhartha Chatterjee Guy E. Blelloch Allan L. \nFisher School of Computer Science Carnegie Mellon University Pittsburgh, PA 15213-3890 Abstract Data-parallel \nprogramming languages have many desirable fea\u00adtures, such as single-thread semantics and the ability \nto express fine-grained parallelism. However, it is challenging to imple\u00adment such languages efficiently \non conventional MIMD multipro\u00adcessors, because these machines incur a high overhead for small grain sizes. \nThis paper presents compile-time analysis techniques for data-parallel program graphs that reduce these \noverheads in two ways: by stepping up the grain size, and by relaxing the synchronous nature of the computation \nwithout altering the program semantics. The algorithms partition the program graph into clusters of nodes \nsuch that all nodes in a cluster have the same loop stmcture, and further refine these clusters into \nepochs based on generation and consumption patterns of data vectors. This converts the fine-grain parallelism \nin the original program to medium-grain loop paral\u00adlelism, which is better suited to MIMD machines. A \ncompiler has been implemented based on these ideas. We present performance results for data-parallel \nkernels analyzed by the compiler and con\u00adverted to single-program multiple-data (SPMD) code running on \nan Encore Mukimax. 1 Introduction The diversity of parallel computer hardware makes it very difficult \nto port parallel programs without sacrificing execution efficiency. Research in portable parallel programming \nhas largely taken one of two forms: those based on parallelism extraction [34] and those basedon virtual \nmachine emulation [17]. However, both approaches have their drawbacks. Parallelism extraction systems \ncan only ex\u00adtract parallelism that exists in the original code. Virtual machine emulators are usually \nlimited in the machine topologies they can suppor~ and often have large overheads associated with mapping \n* IM.s research was sponsoredby the.Avionics Laboratory,Wright Res=r.b and Development Center, Aeronautical \nSystems Division (AFSC), U.S. Au Force, Wright-Patterson AFB, Ohio 45433-6543 under Contract F3361 5-90-C-1465, \nARPA Order No. 7597. The views and conclusions contained in this document are those of the authors and \nshould not be interpreted as representing tbe official poticies, either expressed or impticd, of the \nU,S. government. Permission to copy without fee all or part of this material is granted provided that \nthe copies are not made or distributed for direct commercial advantage, the ACM copyright notice and \nthe title of the publication and its date appear, and notice is given that copying is by permission of \nthe Association for Computing Machinery. To copy otherwise, or to republish, requires a fee andlor specific \npermission. Q 1991 ACM 0-89791-428-7/91/0005/01 30..,$1.50 and emulation. They also usually rely on the \nuser to solve the difficult problem of program and data partitioning. We believe that parallelism should \nbe explicit in the source pro\u00adgram. However, it should not be basedon the notion of processes, as this \nrequires the programmer to manage a great amount of difficult low-level detail like process creation, \nload balancing and synchro\u00adnization. Instead, we have chosen a data-paraflel style of programm\u00ading, where \nparallelism is expressed as (parallel) operations over (large) sets of data. A large fraction of the \nexisting parallel algo\u00adrithms for PRAM and other machine models are either data-pamllel in nature or \ncan be easily converted to such a form [5, 16,31]. Data-parallel languages have historically been linked \nwith SIMD parallel computers, and researchers have largely shied from imple\u00admenting such languages on \nMIMD parallel machines. A naive implementation of data parallelism on a MIMD machine has the following \nperformance bottlenecks, which affect both the asymp\u00adtotic performance of the parallel program and the \nperformsmce for small problem sizes. The parallelism of the source language is too fine-grained for the \nmultiprocessor to handle. The startup overheads are too large, Since loop overhead scales with problem \nsize, it limits the asymptotic performance of the parallel program. Serial overhead related to load balancing, \non the other hand, depends on the machine, but is independent of the problem size, and therefore influences \nthe small problem size performance but not the asymptotic performance. The implicit lock-step synchronization \nof the data-parallel lan\u00adguage is expensive to implement on MfMD machines, whereas it comes for free \non SIMD machines. As the cost of a barrier on a MIMD machine depends on the number of processors but \nis independent of problem size, this only affects the performance at small problem sizes. The intermediate \nresults generated by the fine-grained paral\u00adlelism cause problems for the memory organizations typically \nfound in MIMD machines. In particular, locali~ of reference and its concomitant benefits are compromised. \nLoss of local\u00adity increases data access times; it scales with the problem size, and limits asymptotic \nperformance. This paper demonstrates how to solve these problems, and how to make data parallelism an \nappropriate programming model for both classes of machines. Maintaining efficiency on MIMD machines requires \naggregating the fine-grained operations into larger-grained tasks (loops) and relaxing the lock-step \nsynchronization while main\u00adtaining semantic equivalence. The aggregation of multiple opera\u00adtions also \nallows traditional code improvement techniques to be Proceedings of the ACM SIGPLAN 91 Conference on \nProgramming Language Design and Implementation Toronto, Ontario, Canada, June 26-28, 1991. =\u00ad applied \nto the aggregate. Recently, Quinn and Hatcher [28] have demonstrated techniques for compiling the data-parallel \nlanguage C* [30] for MIMD multiprocessors. However, the programs they can handle are limited by the restricted \nsemantics of C*. Our work goes beyond this, showing how to handle loops with dependence, such as the \nscan primitive of APL [19]. Given a data-parallel program, the problem, then, is to gather information \nabout the program variables and statements to obtain a good aggregation. In this paper we develop compile-time \ntech\u00adniques called size and access inference that extract such information and perform the aggregation. \nThe steps involved in this process are as follows: Size inference: This technique is used to derive symbolic \nrela\u00adtions between sizes of vectors (the variables in the data-parallel program) based on the semantics \nof operations. This is completely symbolic; the user does not have to specify sizes of vectors. The relations \nderived for vector sizes are used to characterize the loop structures of the operation nodes. Cluster \nformation: The size information produced by the previ\u00adous step is used to partition the program graph \ninto clusrers based on loop structure and scheduling constraints. Operations that have provably different \nloop structures are put into different clusters. Clusters serve as units of work allocation. Access inference: \nThis technique analyzes generation and con\u00adsumption patterns of vectors within clusters and identifies \nconflicts requiring synchronization. Epoch formation: The conflict information produced by access inference \nis used to subdivide clusters into epochs. Operations within a single epoch do not require synchronization \nand can be performed in any order that respects data dependence. Epochs map fairly naturally to loops, \nwhich can be executed in parallel by scheduling different iterations of the loop on different processors \n[34]. Our adoption of loop parallelism as our model of parallel execution has been guided by the Fortran \nexperience, which suggests that this form of parallelism is well-matched to the capabilities of MIMD \nmultiprocessors. In our model, some number of threads (specified as a command line parameter) are spawned \nwhen the program begins execution, and remain active until the program terminates. Other models of parallel \nexecution, such as macro-actors [32] or functional pipelines [11], are beyond the scope of this paper. \nA complementary issue is that of storage. A naive implementa\u00adtion often creates many unnecessary large \nintermediate results, Our storage allocation techniques tie in with our analysis to remove such intermediate \nstorage, or reduce it to storage for a small section of a vector. This is similar to drag-through transformations \nin APL or loop fusion in Fortran. The work described here is related to compilation techniques for APL, \nFP [3] and similar languages, the compilation of C* for multiprocessors, and optimization performed by \nFortran compilers. It differs in the kinds of operations it can handle, and we will return to these differences \nfurther in Section 10. The analysis techniques and program transformations discussed in this paper apply \nto uniprocessor as well as multiprocessor sys\u00adtems. They provide benefits in the following areas: Granularity: \nThe grain size of the output program is larger than that of the input program, making it suitable for \nexecution on MIMD multiprocessors. Synchronization: The only synchronization in the output pro\u00adgram is \nthat required for maintaining semantic equivalence with the original program. Storage: Storage requirements \nare reduced, and many temporwy vectors are eliminated. Vector storage can be reduced to storage for small \nsections of vectors, and can therefore take advantage of scalar and vector registers. Locality: Combining \nmultiple operations intn single loops im\u00adproves locality of reference, This can take advantage of chaining \nin vector machines, and of registers and caches in general. 2 Language and Machine Models The compiler \noperates on data-parallel program graphs, where graph nodes are data-parallel operations and graph edges \nrepresent data inputs and outputs of the operations. These are similar to dataffow graphs [25], except \nthat operations compute on entire vectors at a time. The primitive data type is the homogeneous vector \nof atomic types. Scalars are treated as singleton vectors. The primitive op\u00aderations are shown in Table \n1 and all take vectors as arguments; they include traditional arithmetic and logicaf operations applied \nelementwise to vectors (such as A+ B), as well as associative scans (+\\A in APL), permutations (A [ B \n] in APL) and distibute opera\u00adtions (similar to the spread intrinsic in Fortran 90 [2]). Any set of primitives \ncan be used as long as each primitive has an efficient parallel implementation. This excludes operations \nsuch as nonasso\u00adciative scans as primitives. The set we have chosen is based on the scan vector model \nof computation [5], and is part of an experimental data-parallel language called VCODE [6]. It is the \nability to handle scans and permutations that sets our work apart horn other work in this area. The code \ncan be equivalently represented in single\u00adassignsnent form, which we also use (with a LISP-like syntax) \nfor ease of understanding. Using graphs as the internal representation of programs allows us to formulate \nthe analysis problems in graph-theoretic terms, and also gives a good handle on storage optimization, \nIt also allows us to handle various input languages without much effort. We now give a formal definition \nof a computation graph for a function. This is similar to the IF1 graph representation for SISAL functions \n[25]. Definition 1 (Computation graph) The computation graph G(F) for afunction F is a directed acyclic \ngraph (N, E), where N is a set of nodes. A node n E N is the tuple (t,op, in, out), where t is the type \nof n; tc T = {SIMPLE, IE FNCALL, INPUT, OUTPUT). op is: the operation performed by n, f n.1 = SIMPLE; \nthe function called by n, if n.t = FNCALL; undefkd otherwise. in is the number of input ports of n, numbered \nI..in. out is the number of output ports of n, numbered 1..ottt. E is a set of edgesrepresenting data \ninpu~sand outputs of the nodes. An edge e ~ E is the pair ((n,, p$), (rid,p~)), where n. E N is the nodej?om \nwhich the edge originates. -P. is the outputport of n.from which the edge origi~tes; 1 <p, < ns.out. \n Name Symbol init kernel 10, hi plus + out [i] = inl [i] + in2 [i] Or inl siz times out [i] = inl [i] \n* in2 [i] O, inl siz * ! select SEL out [i] = inl [i] ? in2 [i] : in3 [i] Or inl siz plus-scan +\\ Out \n[o] = o out [i] = out[i 1] + inl[i 1] 1, inl~siz plus-reduce +/ out = o out = out + inl[i] O, inl_siz \nrein-reduce MIN/ out = INT MAXout = min(out, inl[i]) O, inl_siz not out [i] = ! inl [i] O, inl~siz length \nLEN out = inl sizo, 0 dist DIST out[i] = inl 0, in2 distv DISTV out[i] = inl O, in2_siz permute PERM \n0ut[in2 [i]] = inl[i] O, inl_siz bpermute BPERM out[i] = inl[in2 [ill O, inl_siz dpermute DPERM out [i] \n= in3 [i] out[in2 [i]] = inl[i] O, inl siz index INDEX out[i] = i O, inl get GET out = inl[in21 or o \nTablel: Primitive data-parallel operations, and corresponding Ccode. The input vectors to an operation \narecalledinl, in2 and soon. Thetemplate fortheCcode isasfollows: init; for (i = 10; i < hi; i++) {kernel;}. \nTheoutputvector iscalled out. All operations exceptthe permutes can be done nd G N is the node at which \nthe edge terminates. -pdisthe inpu$port ofmatwhich the edge terminates; ~ <pd <nd.in. We abbreviate an \nedge to (n,, m)$ports are unimportant, or can be inferredfiom context. There aretwocontrol flow and \nrecursion (for iteration). graph and an ELSE subgraph, computation graph. Oneorthe mechanisms: anif-then-else \nform, The IF node contains a THEN sub\u00adand thus introduces hierarchy in the other of thesubgraphs is executed, \n depending on the value of a scalar Boolean control input. The two subgraphs must produce the same number \nof results of matching types. It would be easy to add other structured looping constructs to the language. \nThe complete language model [6] supports additional data types, segmented versions of the operations \n[5], and function defini\u00adtion/call as encapsulation mechanisms. For the present, we ignore segmentation \nand assume that all nonrecursive function calls are inlined, and defer a discussion of these issues to \nSection 9. The parallel execution model is based on data partitioning, i.e., each processor is responsible \nfor a contiguous portion of the vec\u00adtor. The output of the compiler is single-program multiple-data (SPMD) \nloop code [21] suitable for execution on MIMD multi\u00adprocessors. Storage is of two types: vecror storage, \nwhich holds a complete vector, and buffer storage, which holds an iteration s worth of computation on \na vector. Given a computation graph, on which initial transformations such as function call inlining \nand common subexpression elimination have been performed, we first use size inference to identify nodes \nthat have the same loop structure. We use this information to partition the graph into clusters, and \nallocate vector storage to cross\u00adcluster arcs. We then examine clusters and further refine them into \nepochs based on information provided by access inference, allocating vector and buffer storage where \nneeded. Finally, we generate code in a straightforward manner for each epoch. in-place. (defy yxpy (a \nx y) *M :::::::::::::, q Yl,---\u00ad--\u00ad; + -\u00ad,I 1 1 1 ---\u00ad(* -\u00ad(fi*t~ x))) ~ .) 1 , I t t I for (L r[i] \n= O; 1 = y[i] < x_Rlz; + (a l++) .[1]); { 1 1 } I 1 1 1 1 1 1 , I Serial: 033.8 um Unlprocessor: 882.6 \nras Multtpxocessor (8 way) :108.7 ms Fitzure 1: The SAXPY o~eration. The fiaure shows the oricl\u00adin;l \nprogram, the computation graph, ;nd the final C co~e produced by the compiler, The dashed rectangles \nshow the grouping of nodes into clusters and epochs. Running times are also shown for an input size of \n64000 elements. 3 A Few Examples Before getting to the details of the analysis techniques, we present \na few simple examples to give the reader a feel for the end-to-end effect of the techniques. For each \nexample, we show the Lisp\u00adlike code for the original program, the original program graph, the (uniprocessor) \nC code generated by the compiler, and running times fo~ (a) efficient serial code for the problem, (b) \nuniprocessor code generated by the compiler, and (c) multiprocessor code generated by the compiler. All \ntimes are on the Encore Multimax [14]. The data size is 64000 elements, and the multiprocessor times \nare on eight processors. The first example (Figure 1) is the SAXPY operation. This com\u00ad (aerun nonaauze \n(v) me scans musl De spot Lo anow mumprocessmg. { I ne reason mr m (/ v this is explained in Section \n7.) In either case, the C code produced Vv))) by the compiler has two loops and one intermediate vector. \nFs4 V))) 1I/ 4 Size Inference distv ~ - . .-, for (i= O;i <vsiz; i++) { ] *-v[i]; }  =4% - 1 ::::.[. \nLlqrt sums= sqrt( (double) sum); for (L = O; * <v_siz; ~++) { .... ... .J M r[i] = V[i] / sum; ,-----, \n ) , t +/ : , , t Q,ll Serial: 1784.4 Em Uniprocessor: 1901.6 ms Multiprocessor (8 way) : 248.9 rns Figure \n2: Normalizing a vector. The figure shows the orig\u00adinal program, the computation graph, and the final \nC code produced by the compiler. The dashed rectangles show the grouping of nodes into clusters and epochs. \nRunning times are also shown for an input size of 64000 elements. putation takes two vectors ~ and ~ \nand a scalar A, and returns A.f + ~. This routine is one of the Basic Linear Algebra Sub\u00adprograms [24]. \nThe straightforward implementation requires three loops (one each for distribution, multiplication, and \naddition), and storage for the distributed scalar and the intermediate result of the multiplication. \nThe analysis techniques produce one cluster con\u00adtaining one epoch, and the C code produced by the compiler \nhas the desired single loop with the distribution of the constant folded into the multiplication, and \nno intermediate storage. The second example (Figure 2) is normalizing a vector, i.e., dividing eachelementof \nthevectorby its norm, The sqrt operation in the middle of the graph is a scalar operation separating \ntwo loops over the vector. A naive transcription produces four loops, a scalar operation, and storage \nfor two additional vectors. Size analysis infers that the s qrt operation is scalar, and the other nodes \nhave the same loop structure. However, such a partitioning cannot be scheduled, and the vector cluster \nmust be split into two as indicated. Each cluster has a single epoch. The C code produced by the compiler \ncontains two loops and no additional vector storage. The final example (Figure 3) is the split operation, \nwhich takes a vector of values (integers) and a vector of flags (boolean, encoded as 0/1 integers), packs \nthe values corresponding to the O flags to the bottom and those to the 1 flags to the top, maintaining \norder within each parL and returns the resulting vector. This op\u00aderation forms the core of sorting algorithms \nsuch as radix sort and stable quicksort [22]. The computation involves two plus-scans, a plus-reduction, \nthree elementwise operations, a distribute, and a final permutation. The unoptimized code has eight loops \nand six intermediate vectors. Size inference infers that all the nodes can be put into a single cluster. \nHowever, the scalar output of the +/ node causes an access conflict, which results in the cluster being \nsplit into two epochs as shown. In the multiprocessor case shown on the righ~ Our intent is to partition \nthe program graph into chunks that can be executed in parallel, with synchronization and loop partitioning \noccurring only between chunks. Since all our primitives can be expressed as loops in C or Fortran, we \nuse the size of the iteration space (the difference of the loop bounds)of the nodes as the basis for \npartitioning. In this section, we introduce size inference, which uses the semantics of the primitives \nto infer relations among the sizes of variables and the iteration space sizes of nodes. In Section 6, \nwe show how to use this information to partition the graph into clusters, such that all nodes within \na cluster have the same iteration space size. 4.1 Node and edge attributes We associate a vector with \neach output port of every node in the graph, and map the vector associated with ( ns, P ~) to each edge \ne = ((n S,p$)!(nd, pal)). The size of a vector is the number of elements it contains. Given this map \nbetween vectors and edges, we define the following attributes for a graph edge e: size(e), the size \nof the vector associated with that edge.  ge~e), the generationpattern of the vecto~ this refers to \nthe order in which the elements of the vector are generated by the source node of the edge.  use(e), \nthe consumption pattern for the vector on that edge; this refers to the order in which the elements of \nthe vector we used at the destination node of the edge.  The gen(e) and use(e) attributes are required \nonly for access infer\u00adence (Section 7), and will not be used further in this section. Analogously, an \noperation node n (i.e., one of type SIMPLE) has the following attributes: A set [n(n) of constraints \non the sizes of its input vectors of the form 1,= lj orli = k,where 1~ i,j ~ n.in, 1,isthesize of the \nvector associated witb the edgee = ((~s, p$), (n, z)), arnd k is a constan~ these constraints define \nthe vector sizes for which the computation is well-formed. The only value of k we currently use is 1. \n A set of transfer functions OUZ(TZ,i) (1 ~ z ~ n,out) that compute the sizes of the output vectors \nof the node in terms of the sizes of its input vectors.  An iteration size function L&#38;w(n) that \nCOmPUteS the Size of the iteration space of the oper~tion performed by the node in terms of the sizes \nof its input and/or output vectors. This is important as nodes can take inputs of several sizes, or produce \noutputs whose sizes differ from those of the input.  The node and edge attributes of the operations \nshown in Table 1 are shown in Table 2. Note that the size of the output may be different from the iteration \nspace size, as for the +/ operation. (defun split (v f) :i$j!j (let* ( (NotF (not f)) .,.,:,:.:!:. \n ,, (Up (plus-scan f)) A (Down (plus-scan NotF) )  4!E---\u00ad ,------------------, (Sum (pius-reduce \nNotF j ) 1 (RealUp (+ (dl.stv Sum V) Up)) n .Pmo4uTE (Index (select f RaalUp Down)) : 1 (permute v \nIndex))) 1 1 I I Example:  t SELECT 1 I1 # v [5481092] , f [0110100] Not f rloololll up ~OO12233j Down \n[jll 1223] sum 1r Raalup [4456677] 1 Indax [0451623] 1 1 r [5192480] 1 I 1 1 sum = o; for (i =O; i \n<v siz; i++)f sum += (Notf[~] = ! f[i]); ) Up =down =O; for (i =O; i <v siz; i++) ( r[f[i]?sum+upYdown] \n= v[i]; up += f[i]; down += Notf[i]; ) w ~ ,:::::; Serial: 825.9 MS ,,,,,,, Uniproces nor: 1402.4 \nM Multiprocessor (8 way) : 299.9 ms Figure 3: The split operation, The figure shows the original program, \nthe computation graph, an example computation, and the final C code produced by the compiler. The graph \non the left (respectively, right) is the uniprocessor (respectively, multiprocessor) version. The dashed \nrectangles show the grouping of nodes into clusters and epochs. Running times are also shown for an input \nsize of 64000 elements, n.op In(n) Out(n, 2) Loop(n) size gen use + {1, = 1,} 11 11 11 ind ind, ind * \n{1, = /2} 11 11 11 ind ind, ind 11 tl 11 ind ind SE+ {1, =Jh =1, } 1, k h ind ind, ind, ind +\\0 k11 L \nind ind +/II f! I 1 I 1, H1IaccII -, -.. ind II I 1III I MTN1/ (A 1 1. 1 .-C tt ,- ,, . . .1 L ... IIind \nII I IIII LEN 0 1 1 1indind DIST {1,=1,12=1} J-lo 10 ind ind, ind i DISTV {1,=1} 12 12 12 ind ind, llnl~<~~ \nPERM {r, = 12} 11 11 11 arb ind, u.. E3PERM 0 1? i, L--ind arb, ind )! 1I1{!II DPERM {/1 = 1,1 13 1; \n13 arb ind, ind, ind 1 lo 10 ind ind ,, 1 GET ]1 {2, ~l,22&#38;} I 1 1 1 ind ind, ind Table 2: Node and \nedge attributes for the data-parallel operations defined in Table 1. 10is the size of the output vector. \nEach node shown here has exactly one output. A out(n, i) value of 1 indicates that the output sizes cannot \nbe computed in terms of the input sizes. The edge attributes are explained in Section 7.  4.A 1 ne Daslc \nalgomnm The goal of size inference is to symbolically determine which nodes of a computation graph have \nthe same loop structure. To this end, we first symbolically determine the size(e) attributes for the \nedges, and then use the formula for Loop(n) to determine the loop structures of the nodes. With perfect \ninformation, we could assign symbolic size labels to the edges of the graph such that two edges would \nhave the same label if and only if the vectors associated with them always had the same size at runtime \nin order for the computation tQ be well-formed. Such an assignment is not always possible with compile-time \ninformation, because the sizes of vectors can depend on values contained in other vectors, information \nthat is not available until runtime. This lack of information shows up as an imprecise transfer function. \nThe D I ST operation is an example. Its transfer function requires the value of the second argument in \norder kr compute the size of the result. Hence, the analysis performed by the compiler makes a conservative \napproximation to this ideal; if two edges are assigned the same size label, then the vectors associated \nwith them are guaranteed to always have the same size at runtime if the function call is well-formed. \nIn order to make this .. approx~a~on, we restrict Out(n, i) so that it returns eitier tie size of one \nof the input vectors, a constanL or the reserved value J_ In general, the following cases may arise in \nthe course of analysis: b The compiler infers that two vectors are guaranteed to have the same size. \nNo runtime checks are required. The compiler infers that two vectors cannot have the same size, making \nsome operation ill-formed. This is a program error, and the program is rejected.  The compiler infers \nthat two vectors must (in general) have different sizes, but cannot determine a functional relation be\u00adtween \nthe sizes due to lack of knowledge in the compiler (about symbolic arithmetic on vector sizes, for instance). \n IYo vectors determined to be of unequal size may, in fac~ have the same size in a certain run of the \nprogram (because of the the data values at runtime). The compiler cannot take advantage of this possibility. \n  The compiler determines that the operation is well-formed provided two vectors have the same size, \nbut can neither prove nor disprove this equality, since it may depend on runtime values. The compiler \nproceeds on the assumption that the sizes are equal, but inserts code to check for equality at runtime. \n The Loop attribute of nodes is trivial to compute given the size attribute of edges. The key component \nof this evaluation, therefore, is the assignment of size labels to edges of the graph. The method is \nshown in Algorithm 1. Algorithm 1 (Siiie inference of a data-parallel computation graph.) Input: A data-parallel \ncomputation graph G. Output: Size labels for the edges of G. Method: 1. Arbitrarily assign distinct symbolic \nsizes to each vector of the computation graph, and assign this size to each edge the vector maps to. \n 2. Form a system of equations S representing the constraints on symbolic sizes as follows.  PESMUTE \ne Figure 4: How size inference works on the split operation. The figure shows the initial assignment \nof size labels to the graph edges. (a) For each non-IF node n in the graph, instantiate each element \nof In(n) with the sizes assigned to its input edges and add the constraint to t. (b) For each node n \nin the graph whose transfer function does not return 1, instantiate Out(n, Z) with the sizes assigned \nto its input edges, equate it to the size assigned to the output vector, and add it to $. (c) For each \nIF node in the graph:  i. Equate the size of the boolean input to 1 and add it to &#38;. ii. Recursively \nlabel the then-and else-subgraphs. iii. Merge the output sizes from the two subgraphs. This point is \nelaborated in Section 4.3. 3. Solve t. Given the forms of the constituent equations, the sohstion partitions \nthe set of sizes into equivalence classes. Assign distinct labels to each class. 4. Replace the size \nlabel on each edge of the graph with the label of the equivalence class to which the size belongs.  \nTo illustrate how the above algorithm works, consider the spl it operation. Given the initial labeling \nof the edges as shown in Figure 4, and using the node characteristics shown in Table 2, we get the following \nsystem of equations: $={ b=c, c=d, e=l, b=~, g=a, g=~, h=g, b=d, b=h, i= b,a=i, j=a} which upon solution \nyields the following two equivalence classes: fCl = {e},2C2 = {a, b,c, d,j,g,h,t,j}. This gives the \nedge labeling shown in Figure 5. The other node and edge attributes are easily computed given the size \nlabels on the edges and are also shown in Figure 5. ,., ,, ;! a [.rb, --) PEPMUTE I.) . (Id, ~ d) > \nSELSCT [,] k Q f-00 P Figure 5: Computation graph of split operation, with node and edge attributes \nshown. The edge label is the size, with generation and use patterns in parentheses. The node label (in \nsquare brackets) gives the size of the iteration space. 4.3 Refinements to the basic algorithm A complication \narises in Algorithm 1 concerning IF nodes. In general, it is not necessary for corresponding results \nfrom the THEN and ELSE subgraphs to have the same size. Therefore, if the result vectors have different \nsymbolic sizes, we must take a conservative view and assign a new symbolic size after merging. This, \nhowever, is not as discriminating as we would like, In particular, it works poorly in the presence of \nrecursive function calls. There are some important special cases where we can do better. We classify \nIF nodes into two categories: simple and recursive. We only consider self-recursion for the moment. Simple \nIF nodes do not have recursive calls in either branch, while a recursive IF node has a recursive call \nin at least one branch. If a function contains a recursive IF node, wellformedness requires that there \nbe at least one source-to-sink path in the graph free of recursive calls, As simple IF nodes do not present \nany problems for the size inference algorithm, we will only consider recursive IF nodes, The general \nform of a recursive IF node with a recursive call in one branch is shown in Figure 6. Without loss of \ngenerality, we assume that the IF node is the last node in the function definition, In the absence of \nadditional information, we can only infer that the output size b is either ,B or y. If we know that the \ncomputation following the recursive call is size-preserving, then we can conclude that the final output \nsize will be ~. Note that/3 may not be simply related to a. If, in addition, the nonrecursive branch \nis size-preserving, all we can conclude is that the final output size is either p or a. Now, if we also \nknow that the computation preceding the recursive call is size-preserving, we can infer that the entire \ncomputation is size-preserving. The last line deals with the special case where the output variable is \na scalar. The compiler incorporates these refinements, choosing a new symbolic size when there are multiple \npossibilities for a size. Figure 6: Size possibilities for a recursive IF node with a recursive call \nin one branch. The Greek symbols indicate vector sizes, and the right hand sides of the equations indicate \ninferences that may be made if the conditions on the left hand side are fulfilled. The shaded ovals represent \narbitrary nonrecursive computation graphs. 4.4 Relationship to other work This work is related in a \ngeneral way to research in abstract inter\u00adpretation; Algorithm 1 is superficially similar to the type \ninference algorithm of ML [26]. However, the two differ in the following fundamental ways: o Size inference \nstill requires runtime checks of some equalities, ~pe inference is completely a compile-time operation. \n o The output sizes of a function can be unrelated to the input sizes; output types must be related to \ninput types.  Sizes need not match in the two branches of an IF statements to guarantee well-formedness. \nTypes must match. The domain constructing C* essentially provides the information that size inferencing \ncomputes. However, the C* model is more restrictive. since domains cannot be created dynamically within \na program, and domain sizes must be known at compile time.  5 Scheduling In the remainder of the paper, \nwe will be replacing the partial order of the computation DAG with a linear order in various ways. The \nproblem may be described as follows. We are given the DAG with some subset of its edges marked (represented \nas a mapping 6: E ~ {O, 1}), and are asked to pack the nodes into a chain of buckets, each of infinite \ncapacity. If edge (m, n) is marked, we must place node m in a bucket that occurs earlier in the chain \nthan the bucket in which we place node n. We also want to minimize the number of buckets used. The length \nof a packing is the number of buckets it consumes. A schedule of G under 8 is a packing of minimal length, \nand its length is denoted Z ( G, 6). Define 6 on a path as the sum of the h vahtes of its constituent \nedges. Then the following lemma is obvious. Lemma 1 IC(G, 6) = 1 + max{8(P): P is a source-to-sink path \nin G}. 5.1 Lower and upper bounds The scheduling problem has the characteristic that nodes have upper \nand lower bounds on their position in any schedule. We separate policy horn mechanism by splitting the \nscheduling process in two: finding these bounds, and then actually choosing a schedule. The method for \nfinding the bounds is given in Algorithm 2. Algorithm 2 (l-h labeling of a graph.) Input: A graph G = \n(N, E), and a function 8: E -{O, 1 }. Output: Functions 1 and h: N 4 {O, ... L(G, 8) 1} that give the \nlower and upper bounds on the position of a node in any schedule of N under 6. Method: 1. Sort the node \nset N into list F such that all ancestors of a node precede it on the list. This can be done, for instance, \nby a breadth-first traversal of the DAG, 2. Sort the node set N into list B such that all descendant \nof a node precede it on the list. This can be done, for instance, by a breadth-first traversal of the \ngraph with the edges reversed. 3. For each node non F, in ordec  l(n) = O, if n is a source node min \n{1(m) + ~(e)}), otherwise, e=(m, n) 4. Compute L = max~{l(n)}. 5. For each node non B, in order  h(n) \n= L, if n is asink node max {h(m) ~(e)}, otherwise. e=(n, m)  5.2 Scheduling policies Two obvious schedules \nare S = i and S = h, where 1 and h are the labelings calculated by Algorithm 2. These schedules are derived \nfrom purely local considerations. The next natural step is to determine a good schedule based on some \nnotion of cost. In our contex~ a useful cost measure is the intermediate storage required by the schedule. \nWe will see in later sections that storage is needed only for edges that cross between buckets, and the \nsize of the vector associated with that edge gives the storage requirement for the edge. Let Opf be a \nschedule that minimizes storage lifetime, which we define as the product of the size of the storage and \nits extent. In the absence of additional information, we assume that all vectors have the same size, \nand normalize this to 1. We can formulate this as the following integer programming problem: Minimize \n~[m,n)e~ (O@(n) Opt(m))subject to i(n) < opt(n) (lower bound) opt(n) < h(n) (upper bound) Opt(m) ~ Opt(n) \n &#38;((m, n)) (marked edge) It is easy to verify that the corresponding linear programming prob\u00adlem \nhas integer solutions, and hence it is sufficient to solve the linear programming problem to determine \nOpt. The compiler provides switches for all these scheduling mecha\u00adnisms. The default scheduling is S \n= h, since this is inexpensive to compute and also tends to reduce storage requirements. 5.3 Comparison \nwith other work It is interesting to compare our scheduling problem with previous work in the field, \nsuch as microcode compaction [15] and schedul\u00ading of macro-dataflow graphs [32]. Those techniques use \ninteger weights on nodes and edges to represent computation and commu\u00adnication costs respectively. As \nour interest is in grouping nodes together rather than forming an actual timing schedule, we use 0/1 \nweights for edges and no weights at all for nodes. Further, the criti\u00adcal resource in our problem is \nrelated not to nodes (as in microcode compaction) but to edges.  6 Cluster Formation and Refinement \nSize inference provides information about the iteration spacesizes of the graph nodes. We want to group \ntogether computation nodes that have the sameLoop(n,l value into larger clusters. Conversely, nodes with \ndifferent Loop(n) values must be placed in separate clusters. We must also ensure that clusters can be \nscheduled, i.e., a cluster can run to completion once all its inputs are available. As all nodes in a \ncluster have the same iteration space size, the data needs to be divided among the physical processors \nonly at cluster entry. Thus, clusters serve as natural units of loop partitioning (load balancing). We \nare interested in making clusters as large as possible, since the optimization that follow do not go \nbeyond cluster boundaries. We now present a framework for achieving these ends. Definition 2 (Critical \nedge) A graph edge (n,, n~) is said to be critical tffLoop(nJ #Loop(m). Definition 3 (Cluster) A cluster \ng = (Af, ~) of a computation graph G is a connected subgraph of G containing no critical edges. Definition \n4 (Clustering) A clustering P = {G,...,!%} iS a node parti[ion of G iruo clusters. Definition 5 (Refinement) \nA clustering P is a refinement of an\u00adother clustering Q iff every cluster of P is a subgraph of some \ncluster ofQ. The maximal clusters of the computation graph can be found by removing all critical edges \nof the graph and finding the connected components of the resulting graph. The set of maximal clusters \nform a clustering of the computation graph, which we will refer to as A4 (G). It is clear that A4 (G) \nis unique. Define the condensation graph of a computation graph under a clustering as follows: Definition \n6 (Condensation graph) Let P = {Ll,..., &#38; } be a clustering of a conrptdationgraph G = (N, E), where \nG, = (JV(, f, ]. The condensation graph of G underP is the graph C: = (P, EP), where (Q,, Gj ) c EP iff \nthere exist nodes n. E N, and w E ~, (i #J with (n., w) 6 E. Definition 7 (Viability) A clustering P \nof G is said to be viable ~ C~ is acyclic. I$jj Elcl . ... .---, . : - m .. : . : .* cl :fi- . E2 Figure \n7: A program graph G, M(G), and the condensation M(G) The graph is that of the split operation where \ngraph C~ . +-REDUCE k not considered a primitive operation. Instead, it is simulated by performing a \n+-SCAN on the vector, and adding the final values of the original and the scanned vectors. The maximal \nclusters of the graph are indicated by dashed lines, and condensation graphs are shown on the right. \nThe upper graph is CG (G) which shows that M(G) is not viable. The lower condensati&#38; graph is acyclic, \nbut the mapping of nodes to clusters is not unique, as shown by the indicated + SCAN node. The condensation \ngraph collapses each cluster to a vertex and captures the data dependence between clusters of a computation \ngraph. Intuitively, the clusters of a viable clustering can be sched\u00aduled (linearized) based on data \ndependence, and a cluster can run to completion once all its input data are available. M(G) is not nec\u00adessarily \nviable, as Figure 7 shows. However, the following property holds of any clustering (hence, of any viable \nclustering), due to the maximality of M(G). Lemma 2 Any clustering of G is a rejhement of M(G). As we \nare interested in finding large clusters, the only candidates for refinement are the clusters that form \nsome non-trivial strongly connected component (SCC) of C %( ). We need a way to break the cycle in an \nSCC in order to achieve viability. While the minimum number of breaks required for this is unique, there \ncan, however, be several ways of breaking the cycle with that number of breaks. For instance, in Figure \n7, the cluster C 1 must be refined into two clusters to make the resulting clustering viable. However, \nthe rightmost +\\ node can be scheduled in either of the two refinements. The following lemma captures \nthe property that nodes must satisfy in a viable clustering. Lemma 3 Two nodes nl and n2. cannot be in \nthe same cluster of a viable clustering if there exists a path from nl to n2 containing a critical edge. \nProoft By contradiction. By definition, the two nodes cannot be in the same cluster if Loop (nl ) # Loop \n(nz). We therefore only El Figure 8: Proof of the viability lemma. Straight lines represent edges, and \nwavy lines represent paths in the graph. consider the case where bop(nl,) =Loop(n2). Refer to Figure \n8 and suppose that nl and nz satisfying the given condition are in the same cluster on of a viable clustering \nP. Observe that since nl and nz are in the same cluster, there must be at least two criticai edges on \nsome path between them, one leaving tire cluster and another entering it. Call these critical edges El \nand E2. Now consider the path p = (el, . . . . El,..., &#38;,..., (?k) between nl and nz. Since El and \nG are critical edges, Loop(a) # Loop(nl) and Loop(b) # Loop(nZ). Let nodes a and b be in clusters (7. \nand ~b (not necessarily distinct) of P. But then c: contains the cycle ~~ -~a -% * ~~, and hence P is \nnot viable. We call (nl, nz) above a critical pair if a path between them begins and ends with criticai \nedges. To separate the elements of critical pairs, we add separator edges between them, producing the \nseparator graph G.s = (IV, E U Es), where E,s is the set of separator edges. The separator edges encapsulate \nthe interactions between clusters, so that we can now examine and refine each cluster in isolation. As \nexplained in Section 5, the number of refinements required to make a cluster viable is equal to the maximum \nnumber of separator edges on any path within the cluster, but there is some flexibility in the actual \nassignments of nodes to refined clusters. Intuitively, separator edges must span refined clusters. A \nviable partition of a graph is generamd by Algorithm 3. Algorithm 3 (Viable partitioning of a data-parallei \nprogram graph.) Input: A data-parallel computation graph G on which size infer\u00adence has been performed. \nOutput: An assignment of cluster numbers to nodes of G such that the resulting clustering is viable. \nMethod: 1. Compute lkf(G) by finding the connected components of the graph (IV, E EC), where EC is the \nset of cnticai edges of G. 2. Find L, the set of non-trivial strongly connected components of C%(G) [1, \np. 193]. 3. For each! c L: (a) Identify critical pairs and create the separator graph 1.s. sum .hlmnts \nin am mectbn starting SCAN-2 with . c art-offset. (b) Apply Algorithm 2 with G = 1S and A defined as \nfollows: 1 if(2, j)c Esofls  scan * W fzom .11 scan sln09f... all c$((i, J) = 0 othewise. ynchrc.niratlo \nynchroniz.ti.a proo...orstoget { correct Start-o ffSet. global sum. P~.-.*0r. to W J?+ 4. Assign refined \ncluster numbers to nodes n from the range [~(n), h(n)], using one of the scheduling policies mentioned \nin Section 5.2. The clusters of the resulting viable clustering can now be sched\u00aduled in any way that \nmaintains the inter-cluster data dependence.  Access Inference Each cluster of a viable clustering \nrepresents a section of code that could potentially be fused into a single loop. It does not necessarily \nguarantee that this is always possible, as operations within a cluster may conflict in the order in which \nthey produce and use vectors, or may require other kinds of synchronization. Access inference identities \nthese conflicw and further refines clusters into epochs, where each epoch corresponds to a single loop \nin C. Recall that we defined gen(e) and use(e) attributes for edges in Section 4. We now define thepossiblevalues \nof these attributes. We choose ge~e) and use(e) from the following set of stylized patterns: 1. unused \nnot used. This means that the data values are not required for the operation. DI STV is an example, where \nonly the length of the second input is needed. 2. ind: generated/consumed in index order. 3. ace: generated \nby accumulation, This means that the value is available only after all iterations of the loop have completed. \n 4. arb: generated/consumed in some data-dependent order that cannot be predicted at compile time, e.g., \nin a PERMUTE operation.  These patterns are representative, not exhaustive. Also, because a vector may \nbe mapped to multiple edges with the same source (corresponding to fanout), the consumption pattern for \na vector is defined as the most constrained consumption pattern among all the edges to which the vector \nis mapped, arb being more constrained than ind, which is more constrained than unused. Table 2 shows \nthe edge attributes for various nodes. Thelanguagepnmitives can bedividedinto the following groups: elementwise \noperations, structure accessors (such as LENGTH), per\u00admutes and distributes, and scans and reductions. \nThe first three groups can be implemented on a multiprocessor in a single loop, as there are no loop-canied \ndependence [34] in these operations. This is not true for scans and reductions, however, where processors \nneed to communicate state information. In order for access inference to effectively handle SCAN and REDUCE \noperations, we must expose their microstructure. The idea is to decompose these operations into more \nprimitive loops between the loops. The decomposition for scans [23], where phases: with inter-processor \ncommunication isolated follows from the standard parallel algorithm each processor performs the following \nthree Sum the elements in its partition of the data. This is an ele\u00admentwise loop requiring no interaction \nwith other processors. 139 elements Sum *lmmnts in own sum inown scm_l scAN_l ion of m.tor. parti.n \nof Yuct.ar. port v? SCAN KiDfrcs Figure 9: Templates for SCAN and REDUCE operations that expose their \nmicrostructure. o Scan the sums from all the processors to obtain the cor\u00adrect start-offset. This involves \ninter-processor communication (synchronization). 8 Sum the elements of its section starting with the \nstart-offset derived from the previous step, This is another elementwise loop that requires no interaction \nwith other processors. Note that we do not have to know the exact number of processors, or the algorithm \nused for the synchronization step. These details are relegated to the runtime system. A reduction is \ntreated in much the same way, except that the third phase is not required, and the synchronization step \nreturns to each processor the global sum in\u00adstead of the start-offset. The templates for these two operations \nare shown in Figure 9, We replace occurrences of scan and reduce nodes with the corresponding templates, \nand perform common subexpres\u00adsion elimination to remove any redundant nodes. For the sp 1 i t operation, \nthis results in the graph shown in Figure 10. The identification of edges requiring synchronization is \nbased on clef-use conflicts for the vectors corresponding to those edges, and a special case for scan \noperations. Intuitively, the idea is as follows. If a vector is produced and consumed in ways that are \ncompatible (such aa ind andind), then storage is only required for a small section of the vector at any \ngiven time, the producer and consumer nodes can be executed in a single loop, and no synchronization \nis needed. Conversely, for incompatible patterns, the entire vector must be generated before any of it \ncan be used, the producer and consumer nodes must be in different loops, and synchronization is required \nbetween the loops. (However, loops need not be repartitioned, as we are still within a single cluster.) \nWe capture this notion by defining an incompatibility relation R.(gen ( e), use(e)) between gen(e) arid \nuse(e), as shown in Table 3, We define the access weight of an edge as follows: Definition 8 (Access \nweight) The access weight 8a(e) of an edge e z (n$, ~) is I l~n..op = SCAN.1 and w.op = SCAN2, and is \nequal to 7Z(gen(e), use(e)) otherwise. As explained in Section 5, while the number of epochs that a given \ncluster must be broken into is well-defined, there is some flexibility in the mapping of nodes to epochs. \nThe upper and lower bounds can be computed for each node of a cluster by Algorithm 4, which is similar \nto Algorithm 3. 8 Implementation and Results m DJIUUl Synchronization ::::~ E@ O Cluster Figure 10: \nThe split operation after template expansion an~ CSE, Cluster-and epoch boundaries are shown, along with \ncomputation nodes and synchronization events. gen(e) Table 3: Incompatibility relation 7Z.(gen(e),use(e)) \nbetween generation and usage patterns. A O entry indicates that the two patterns are compatible, while \na 1 indicates an incompat\u00adibility. Algorithm 4 (Access inference of a cluster of a data-parallel computation \ngraph.) Input: A cluster C from a viable clustering of a data-parallel program graph. Output: An epoch \nnumbering of the nodes of the cluster. Method: 1. Apply Algorithm 2 with G = C and 6 = fa. 2. Choose \nan assignment of nodes to epochs, using one of the policies in Section 5.2.  The epoch assignments \nfor the split operation are shown in Figure 10. Note that multiple synchronization events at an epoch \nboundtuy can be combined, as in the case of the two scans and the reduce at the end of the first epoch. \nWe have implemented a compiler for data-parallel computation graphs incorporating the above ideas. The \ncompiler produces C Threads code [13] suitable for execution on a shared-memory mul\u00adtiprocessor. In this \nsection, we briefly discuss storage management and code generation, and then present some preliminary \nresults on the Encore Multimax. 8.1 Storage management Intermediate storage required for the computation \nis of two types: vector storage and bufler storage. The former is needed for those edges that cross cluster \nor epoch boundaries, while the latter is needed for intra-epoch edges with fartout. Vector storage can \nbe further subdivided into three categories based on the persistence of the vector, as follows: Vectors \nthat persist across epochs within a single cluster.  Vectors that persist across clusters of a single \nfunction.  Vectors that persist across functions.  This distinction is important because of the high \noverhead of parallel memory allocation on a multiprocessor. Heap storage is re\u00adauired for vectors that \npersist across functions, while stack storage is sufficient for the other two categories. The rtmtime \nsystem uses separate. vector stacks for the first two categories. Allocation and reclamation is much \ncheaper for stack storage than for heap stor\u00adage, since it can be done without interprocessor communication. \nStandard liveness analysis techniques can be easily augmented to optimize reuse of storage. The extension \nconsists of taking into account the size of a memory block allocated for a vector when considering it \nfor reuse. 8.2 Code generation Code generation is fairly straightforward, since at this point thecom\u00adpiler \nis dealing with well-structured loops. The compiler generates C code and relies on the native C compiler \nto perform machine\u00adspecitic optimization. This use of C as a universal assembly lan\u00adguage allows the \nuse of a single back end in the compiler. The compiler does, however, perform source-level transformations \nsuch as strength reducing array indexing to pointer incrementing, and unrolling loops to reduce loop \noverheads, as native C compilers are not very good at such optimizations. Code generation is done by \ntraversing backwards through epochs; buffer storage associated with nodes with fanout is used to avoid \nrecomputation of results, 8.3 Results In Figure 11, we show preliminary performance numbers for eight \ntest kernels, for various data sizes and number of processors. Note that the speedup is calculated with \nrespect to a good serial algo\u00adrithm for the problem, notthe parallel algorithm running on one processor. \nIn some cases the serial code uses completely different data structures and rdgonthms from those used \nby the parallel code. Source level tuning [4] was done for the serial programs, includ\u00ading loop invariant \nmotion and converting array indexing to pointer incrementing. The measurements were taken on a 16-processor \nEn\u00adcore Multimax (with NS32332 processors and 96 Mbytes of main memory) running the Mach operating system \n[29]. Timing was done using the memory-mapped free-running microsecond timer, al4- 12 / 12 - Normalize \ni II:u/ 10 4000 / 1s000 SAXPV / 10-. 54000 / 0 4000 / 8. /. 160W . 64000 / / 6-/ / 8. 6\u00ad/ 4. 4. 2\u00ad2- \n I I o 24681012 o 246 Numb13r0f pmeessors Numkr of n&#38;660i2 12 / / / 1/ Fkst mhlmum 10 n 4000 / \n16000 / 84000 / [n / / 8 / I/ 6. : -A o 24661012 o 24661012 NumOer of processors Number ofproceseors \n3 2-/12 / / 1 / Pack Leefflx / / /  /l !: 10. m 5000 10 0 4000 / / 25OOO 16000 . 125000 / / 64000 \n/ / 8. / 8. / / / / // 6-/ 6\u00ad /// 4-/ 4. / 2-/// 2\u00ad [ 1. 0 246 0 246 Numkr of p~eeso;z timkr of Aess0J2 \n3 2- / *12\u00ad / Qukk median Prime ewe /i ii 10. 0 woo / 10 26000 / 26000 126000 / 125000 / 500000 \nu cl / 6-/ 6/ / / / / 6. / 6./ / / 4-/ 4p /// / 2\u00ad 21\u00ad i o 2468 0 246 Number 0fpr%ss0i2 Mrmkr of pr%sso;z \nFigure 11: Speedup curves for the eight benchmarks kernels discussed in Section 8.3. Speedups are with \nrespect to the best serial C code for the problem. The dashed line in each graph shows the ideal speedup \ncurve. 141 with averaging over multiple trials. The processor allocation fa\u00adcility of the Mach kernel \nwas used to gain exclusive access to the appropriate number of processors. We now analyze the results \nin greater detail, We emphasize once again that the results are prelim\u00adinary. We are currently adding \nmore optimization to the compiler, and we expect the numbers tQ improve in future. Example 1: (SAXPY) \nThis computation was discussed in Sec\u00adtion 3. It is a straightforward elementwise loop, and shows the \nexpected linear speedup. The superlinear speedup for moderate data sizes is a cache effect the data overflows \nthe cache on a single processor, but not on multiple processors. Example 2: (Normalize) This computation \nwas discussed in Section 3. The reduction at the end of the fist loop causes a serial bottleneck. This, \nhowever, becomes less significant for larger data sizes, where the speedup shows a linear trend. Example \n3: (First minimum) This computes the first location of the minimum value of a vector. The computation \ncan be written in a trivial serial loop (Livermore Loop 24), which unfortunately does not parallelize \nwell. The data-parallel algorithm involves two rein-reductions, two distributes, a comparison, and the \nINDEX op\u00aderation. Our analysis coalesces this graph into two loops with two synchronization points. The \nspeedup is limited to about 3 because of the extra work done by the parallel algorithm. This is a result \nof the limited set of reduction operators we allow, and could be im\u00adproved by augmenting the language \nmodel to allow reduction with user-defined functions. Example 4: (Split) This computation was dk.cussedin \nSection 3. Note that the parallel version of scan performs twice as many oper\u00adations as the serial version. \nThis is inherent to the parallel algorithm for scan and is not an artifact of the compiler. Any parallel \nalgo\u00adrithm for split requires this extra work. This limits the speedup we can expect. The additional \nmemory traffic due to the permute gives a maximum speedup of about 5. Example 5: (Pack) This takes a \nvector of values and a vector of flags, and returns only those values whose corresponding flags are 1. \nThe serial algorithm can take advantage of the fact that the size of the result is less than that of \nthe input to simplify computing the size of the output. The parallel version requires two loops, one \nto count the number of flags that are 1, and the second to actually permute the values into the result. \nThis extra work and bus contention limits the speedup to about 2. The compiler does not yet generate \noptimal code for this benchmark. Example 6: (Leaffix) This operation takes a tree with a value at each \nnode, arrd returns to each node the sum of the values at all its descendants. The tree is represented \nas an Euler tour, and the details of the representation and algorithm maybe found in [5]. This computation \ncan be used as a kernel for many tree operations, such as determining the number of descendants for each \nvertex, The size of the iteration space changes during the computation (the scan operates on a vector \nthat is twice the length of the original vectors). The analysis techniques transform this graph to four \nloops with three synchronization points. A speedup of 6 is achieved on this benchmark, which is close \nto the bound based on operation counts. Example 7: (Quick-median) This computes the median of a vec\u00adtor \nof values using an algorithm of O(n) average-case complexity. It is quite similar to quicksort. It chooses \na pivot, and finds the number of elements whose value is less than the pivot value. Depending on this \nvalue, it either packs those elements or those that are greater than the pivo~ and calls itself recursively \non that packed vector. The graph contains nested recursive IF nodes. The synchronization re\u00adquired around \nrecursive calls limits speedup to about 4 over the serial program. This could be improved with further \noptimization of function calls, as explained in Section 9.1. Example 8: (Prime sieve) This finds all \nprime numbers less than a given value using the sieve of Eratosthenes. A Boolean array distinguishes \nprime and composite numbers in the desired range. In successive steps, the next prime is located, and \nits multiples are marked as composite. The program generates a large number of writes, which show up \nas bus transactions because the caches are write-through. We therefore expect the performance to be limited \nby bus performance. We determined in a separate experiment that a steady write traffic causes bus saturation \nat seven processors, and the peaking of the speedup curve at that value confirms our hypothesis. 9 Extensions \n9.1 Function calls Function calls can sometimes be irtlined into the calling context. In\u00adlining is simplified \nby the single-assignment nature of our language. This transformation is generally desirable, as the compiler \ncan then use information from the call site, and potentially generate larger clusters and epochs. However, \ninlining may not always be possible due to various reasons: The function definition may not be available \nto the compiler (library functions, separate compilation);  Inlining the function call may cause an \nunreasonable expan\u00adsion in code size;  o The function maybe recursive. If the compiler cannot access \nthe function definition, the function call must be treated as a black box that is placed in a cluster \nby itself. This means that gen(e) and use(e) patterns for input and output edges must be considered to \nbe at%, requiring storage and synchronization at those edges. We can do much better if the definition \nis available, or some properties of the function are known, In this case, we can do the following things, \nin increasing order of sophistication: We can derive an end-to-end Out(n, i) for the function call by \nperforming size inference on the definition and deriving the size of each output of the function in terms \nof the input sizes. This allows information to pass through the function call in the calling context. \nWe can tag each function to indicate whether it has a synchro\u00ad nization point. For functions that do \nnot have a synchroniza\u00ad tion point, we can associate a Loop(n) with the call, and treat it as a user-defined \noperator. This requires compiting a singlc\u00adelement version of the function to allow fusion with adjacent \nloops. We can partially irdine the function call. If the function has multiple clusters, or multiple \nepochs within a single cluster, we can inline the first and last ones and treat the intenor as a black \nbox. This allows optimization of the inlined clusters and does not introduce any additional synchronization. \nPartial inlining can be particularly useful for recursive functions that are not tail-recursive. Tail-recursive \nfunctions can be converted to an equivalent iterative form. Our current compiler performs the first \nof these optimizations, and we are working on incorporating the others. 9.2 Nested parallelism As mentioned \nin Section 2, we can augment the language with segmented vectors and segmented versions of the primitive \noper\u00adations. These augmentations allow the implementation of nested parallelism [7]. The analysis techniques \nextend quite naturally to handle segmented operations. Segmented operations simply intro\u00adduce more levels \nof loops, and the Loop(n) function must now return a triple instead of a single value. There is also \nthe question of t ttntime models for segmented vectors based on the data signature. These issues me beyond \nthe scope of this paper. 9.3 Alternative traversal orders One drawback of the current analysis is the \nlimited choice of gen\u00aderation and use patterns of vectors. In particular, this restricE our ability to \nanalyze structured permutations such as reverse and rotate. We are working on refinements to access inference \nsimilar to ideas in [33] that would allow us to identify such permutations and use this information to \nchoose ahemative and more efficient traversal orders through the iteration space.  10 Relationship to \nOther Work The work described in this paper is related to research in compiling functional and applicative \nlanguages such as FP, APL and SISAL, loop fusion techniques in Fortran, and compilation of C* for mul\u00adtiprocessors. \nWe now discuss how our work stands with respect to each of these. Applicative languages: APL has a long \nhistory of compilation efforts, and some of the techniques in this paper can be traced back to ideas \npresented by Guibas and Wyatt [18], such as stylized access modes, the compilation of streams, and slicing. \nHowever, they were investigating these issues for uniprocessors, and therefore did not consider multiprocessor \nissues such as synchronization. They also did not handle scans. More recently, Budd has looked at generating \nvector code from APL [8, 9]. Again, he confines his drag-through transformation to elementwise sections \nof code, and, in particular, does not deal with pipelining scan and reduction operations. This is partly \ndue to the fact that APL allows nonassociative scan opera\u00adtors. Ching [12] presents results for compiling \nAPL into System/370 assembly code. His type-shape analyzer is very similar to size infer\u00adence, but his \nprimary aim is to get tight bounds on the types, ranks and sizes of variables. He also does not treat \nmultiprocessor issues such as load balancing and synchronization. Ju and Ching [20] present similar results. \nWhile they are aware of the benefits of loop fusion, their compiler does not perform this transformation \nautomatically, Our techniques provide a systematic way of doing this. Similar work has also been reported \nfor FP by Budd [10], and more recently by Walinsky and Banerjee [33]. The goal of the latter work was \nto treat permutation computations as index manipulations. These suffer from the same limitation of being \neffective only in sections of code containing only insert and apply-to-all functional. Our work shows \nhow to decompose scans and allow size and access information to flow through them. Compilation of SISAL \nby Sarkar [32] uses a similar approach to partitioning and scheduling. However, hia work requires estimates \nof execution times for the nodes of the graph, and does not explore the epoch structure within clusters. \nFortran: Loop fusion [34] ia a well-known optimization tech\u00adnique in Fortran. The idea there is to fise \nadjacent loop bodies, thereby reducing loop overheads, and allowing for further inter\u00adstatement optimization. \nIts use is again limited to elementwise sections, and cannot work through operations such as scans due \nto data-dependence considerations. Permutes present a major obsta\u00adcle because they are impervious to \ndependence analysis. Recently, there has been some work on identifying idioms such as scans and reductions \nin Fortran programs [27]. C*: Quinn and Hatcher have worked on compiling C* forMfMD machines [28]. Their \nwork has some of the same goals as ours. It differs from ours in two main ways: their rnntime model involves \nvirtuaJ processor emulation by the physicrd processors, and they do not attempt any inter-statement storage \noptimization. They also do not attempt to perform source-to-source optimization such as loop fusion. \nThe domain construct in C* essentially provides the information that our size inferencing computes. However, \nthe C* model is more restrictive, since domains cannot be created dynami\u00adcally within a program, and \ndomain sizes must be known at compile time. It is not cleru how their techniques would handle scans, \nor extend to nested parallelism, 11 Conclusions This paper has introduced two techniques for the analysis \nof data\u00adparallel program graphs. The firsL size inference, derives symbolic relations between the sizes \nof program vectors, and uses this infor\u00admation to partition the program graph into regions (called clusters) \nthat have different loop sizes. The second technique, access in\u00adference, analyzes generation and usage \npatterns of vectors, and uses conflicts between these patterns to further refine clusters into epochs. \nThese techniques are used to step up the grain size, reduce storage and synchronization requirements, \nand improve locality of data-parallel programs, making it viable to run them on traditional MIMD multiprocessors, \nThe major contribution of this paper lies in demonstrating how to make the techniques work in the presence \nof scan, reduction, distribute and permutation operations. A compiler based on these ideas has been implemented, \nand results have been presented for several benchmarks.  Acknowledgments We would like to thank Eric \nCooper, Allan Heydon, Peter Lee, Mar\u00adgaret Reid-Miller, Jay Sipelstein, Peter Steenkiste, Jaspal Subhlok, \nand Marco Zagha for their comments on early drafts of this paper. References [1] Alfred V. Aho, John \nE. Hopcrof\\ and Jeffrey D. Ulhnan. The Design and Analysis of Computer Algorithms. Addison-Wesley Publishing \nCompany Reading, MA, 1974. [2] American National Standards Institute. American National Standard for \nInformation Systems Programming Language Fortran: S8(X3.9-198X,), March 1989. [3] [4] [5] [6] [7] [8] \n[9] [10] [11] [12] [13] [14] [15] [16] [17] [18] John Backus. Can Programming Be Liberated from the von \nNeumann Style? A Functional Style and Its Algebra of Pro\u00adgrams. Communications of the ACM, 21(8):613-641, \nAugust 1978. Jon L. Bentley. Writing Ejjicient Programs. Prentice-Hall, 1982. Guy E. Blelloch. Vector \nModels for Data-Parallel C~mputing, The MIT Press, Cambridge, MA, 1990. Guy E. Blelloch and Siddhartha \nChatterjee. VCODE: A Data-Parallel Intermediate Language. In Proceedings of the Third Symposium on the \nFrontiers of Massively Parallel Computa\u00adtion, pages 471-480, College Park, MD, October 1990. Guy E. Blelloch \nand Gary W. Sabot. Compiling Collection-Oriented Languages onto Massively Parallel Computers. Jour\u00adnal \nof Parallel and Distributed Computing, 8(2]: 119 134, February 1990. Timothy A. Budd. An APL Compiler \nfor a Vector Processor. ACM Transactions on Programming Languages and Systems, 6(3):297-313, July 1984. \nTimothy A. Budd. A New Approach to Vector Code Gener\u00adation for Applicative Languages. Technical Report \n88-60-18, Department of Computer Science, Oregon State University Corvallis, OR, August 1988. Timothy \nA. Budd. Composition and Compilation in Func\u00adtional Programming Languages. Technical Report 88-60-14, \nDepartment of Computer Science, Oregon State University, Corvallis, OR, June 1988. Siddhartha Chatterjee \nand Prathima Agrawal. Connected Speech Recognition on a Multiple Processor Pipeline. In Pro\u00adceedings \nof the 1989 IEEE International Conference on Acous\u00adtics, Speech and Signal Processing, pages 774 777, \nGlasgow, Scotland, May 1989. Wai-Mee Ching. Program Analysis and Code Generation in an APLf370 Compiler. \nIBM Journal of Resea~h and Devel\u00adopment, 30(6):594-602, November 1986. Enc C. Cooper and Richard P. Draves. \nC Threads, Techni\u00adcal Report CMU-CS-88-154, Computer Science Departmen~ Carnegie Mellon University, June \n1988. Encore Computer Corporation. Multimax Technical Summary. Encore Computer Corporation, 1988. Joseph \nA. Fisher. The Optimization of Horizotial Microcode Within and BeyondBasicBlocks: An Application of Processor \nScheduling with Resources. PhD thesis, New York University, New York, NY 1979. Geoffrey C. Fox. What \nHave We Learnt from Using Real Parallel Machines to Solve Real Problems? In Geoffrey Fox, editor, Proceedings \nof the Third Conference on Hypercube Concurrent Computers and Applications, Volume II, pages 897 955, \nPasadena, CA, January 1988. Eran Gabber. VMMP: A Practical Tool for the Development of Portable and Efficient \nPrograms for Multiprocessors. IEEE Transactions on Parallel and Distributed Systems, 1(3):304\u00ad317, July \n1990. Leo J. Guibas and Douglas K. Wyatt. Compilation and De\u00adlayed Evaluation in APL. In Conference Record \nof the Fijih Annual ACM Symposium on Principles of Programming Lan\u00adguages, pages 2 8, Tuscon, AZ, January \n1978. [19] Kenneth E. Iverson. A Programming Language. Wiiey, New York, NY 1962. [20] Dz-Chirtg Ju and \nWai-Mee Ching. Exploitation of APL Data Parallelism on a Shared-memory MIMD Machine. In Pro\u00adceedings \nof the Third ACM SIGPLAN Symposium on Prin\u00adciples and Practice of Parallel Programing, pages 61-72, Williamsburg, \nVA, April 1991. [21] Alan H. Karp. Programming for Parallelism. Computer, 20(5):43 57, May 1987. [22] \nDonald E. Knuth. Sorting and Searching, volume 3 of The Art of Computer Programming. Addison-Wesley Publishing \nCompany, Reading, MA, 1973. [23] Richrud E. Ladner and Michael J. Fischer. Parallel Prefix Computation. \nJournal of the ACM, 27(4):831-838, October 1980. [24] C. L. Lawson, R. J. Hanson, D. R. Kincaid, and \nF. T. Krogh. Basic Linear Algebra Subprograms for Fortran Usage. ACM Transactions on Mathematical Software, \n5(3):308-323, September 1979. [25] James McGraw, Stephen Skedzielewski, Stephen Allan, Rod Oldehoef~ \nJohn Glauer~ Chris Kirkham, Bill Noyce, and Robert Thomas. SISAL: Streams and Iteration in a Single Assignment \nLanguage, Language Reference Manual Version 1.2. Lawrence Livermore National Laboratory, March 1985. \n [26] Robin Milner. A Theory of Type Polymorphism in Program\u00adming. Journal of Computer and System Sciences, \n17:348-375, 1978. [27] Shlomit S, Pinter and Ron Y. Pinter. Program Optimization and Parallelization \nUsing Idioms. In Conference Record of the Eighteertih Annual ACM SIGACT-SIGPLAN Symposium on Principles \nof Programming Languages, pages 79 92, ~\u00adlando, FL, January 1991. [28] Michael J. Quinn and Philip J. \nHatcher. Data-Parallel Pro\u00adgramming on Multicomputers. IEEE Software, 7(5):69 76, September 1990. [29] \nRichard F. Rashid. Threads of a New System, Unix Review, 4(8):37-49, August 1986. [30] J. R. Rose and \nG. L. Steele Jr. C*: An Extended C Language for Data Parallel Programming. In Proceedings of the Second \nInternational Conference on Superconywting, Vol. 2, pages 2-16, Sars Francisco, CA, May 1987. [31] Gary \nW. Sabot. The Paralation Model: Architecture-Independent Parallel Programming, The MIT Press, Cam\u00adbridge, \nMA, 1988. [32] Vivek Sarkar. Partitioning and Scheduling Parallel Programs for Execution on Multiprocessors. \nPhD thesis, Computer Sys\u00adtems Laboratory, Stanford University, Stanford, CA, April 1987. [33] Clifford \nWalinsky and Deb Banerjee. A Functional Program\u00ad ming Language Compiler for Massively Parallel Computers. \nIn Proceedings of theACM Conference on Lisp and Functional Programming, pages 131 138, Nice, France, \nJune 1990, [34] Michael Wolfe. Optimizing Supercompilersfor Supercomput\u00aders. The MIT Press, Cambridge, \nMA, 1989.  \n\t\t\t", "proc_id": "113445", "abstract": "", "authors": [{"name": "Siddhartha Chatterjee", "author_profile_id": "81100051284", "affiliation": "School of Computer Science, Carnegie Mellon University, Pittsburgh, PA", "person_id": "PP14029125", "email_address": "", "orcid_id": ""}, {"name": "Guy E. Blelloch", "author_profile_id": "81100282539", "affiliation": "School of Computer Science, Carnegie Mellon University, Pittsburgh, PA", "person_id": "P100820", "email_address": "", "orcid_id": ""}, {"name": "Allan L. Fisher", "author_profile_id": "81502691939", "affiliation": "School of Computer Science, Carnegie Mellon University, Pittsburgh, PA", "person_id": "P15257", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/113445.113457", "year": "1991", "article_id": "113457", "conference": "PLDI", "title": "Size and access inference for data-parallel programs", "url": "http://dl.acm.org/citation.cfm?id=113457"}