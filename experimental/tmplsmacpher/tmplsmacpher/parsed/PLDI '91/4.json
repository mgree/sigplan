{"article_publication_date": "05-01-1991", "fulltext": "\n Predicting Program Behavior Using Real or Estimated Profiles David W. Wall Digital Equipment Corporation \nWestern Research Laboratory Abstract There is a growing interest in optimization that depend on or benefit \nfrom an execution profile that tells where time is spent. How well does a profile from one run describe \nthe behavior of a different run, and how does this compare with the behavior predicted by static analysis \nof the program? This paper defines two abstract measures of how well a profile predicts actual behavior. \nAccording to these measures, real profiles indeed do better than estimated profiles, usually. A perfect \nprofile from an earlier run with the same data set, however, does better still, sometimes by a factor \nof two. Unfortunately, using such a profile is unrealistic, and can lead to inflated expectations of \na profile-driven optimization. 1. Introduction Many people have built or speculated on systems that use \na run-time profile to guide code optimization. Applications include the selection of variables to promote \nto registers [7,8], placement of code sequences to improve cache behavior [3,6], and prediction of common \ncontrol paths for optimizations across basic block boun\u00addaries [2,5]. We can evaluate such a technique \nby timing the program, profiling it, optimizing it based on the profile, timing the optimized version, \nand finally comparing the two times. Unfortunately, it is common for researchers to use the same data \nset for the profiling run as for the timing runs. This may give a distorted picture of the efficacy of \nthe technique, because in practice we will optimize based on some profiles, and then run the pro\u00adgram \nmany times on data sets that may not match those of the profiling runs. If there is considerable difference \nin program behavior from one run to another, we might Permission to copy without fee all or part of this \nmaterial is granted provided that the copies are not made or distributed for diract commercial advantage, \nthe ACM copyright notica and tha titla of the publication and its date appear, and notice ia given thet \ncopying is by permission of the Association for Computing Machinery. To copy otherwise, or to republish, \nrequirea a fee and/or specific permission. @ 1991 ACM 0-89791 -428 -7/91 /0005 /0059 ...$1 .50 Proceedings \nof the ACM SIGPLAN 91 Conference on Programming Language Design and Implementation. Toronto, Ontario, \nCanada, June 26-28, 1991. 59 find that a real profile is no better than an estimated profile derived \nfrom a static analysis of the program. Thus two questions arise that are rarely adequately answered, \nHow well does a profile from one run predict the behavior of another? And how well can you do with a \nstatically estimated profile? It is important to answer these questions in general terms as well as specific. \nA profile from a different run may be very useful for one kind of optimization but nearly useless for \nanother. The optimization may require finding the specific program entities that are most used, or it \nmay require only finding some that are used a lot. This paper describes a study of how well an estimated \nprofile predicts real behavior, and how well a profile from one run predicts the behavior of a different \nrun. 2. Methodology. For the purposes of this paper, a projik is a map\u00adping from instances of some kind \nof program entity, like variables or procedures, into numeric weights. The weight may be the number of \ntimes the program entity was referenced, or the total cost of all those references, or something else \naltogether. We assume that a profile is sorted in decreasing order of weight. We will be concerned with \nfive kinds of profiles. The first is the basic block profile, which is the mapping from each basic block \nto the number of times it is exe\u00adcuted. The second is the procedure entry profile, which maps each procedure \nto the number of times it is entered. The third is the procedure time profile, which maps each procedure \ninto the number of instructions executed dur\u00ad ing all of its calls.* The fourth is the call profile, \nwhich maps each distinct call site to the number of times it is executed. The last is the global variable \nprofile, which . * This is time only in an abstract sense, as it neglects cache misses and fptr stalk.; \nnevertheless it estimates a procedure s rnrttirne beuer than a count of its invocations does. maps each \nglobal variable to the number of times it is directly referenced. We might use a global variable profile \nto decide which globals to promote to registers. A call profile might show us which calls are worth expanding \ninline, and a procedure profile which procedures are worth optimizing extra hard. Block profiles are \nuseful for cache optimization. We are interested both in real profiles and in estimated profiles. In \neither case we construct the profile by dividing the program into basic blocks, deciding how many times \neach basic block is executed, and combining these counts with static information from the code seg\u00ad ment \nof the program and its loader symbol table. For a real profile, we use real basic block counts obtained \nby running the program on a particular set of test data. The pixie tool horn Mips [4] instruments an \nexecutable file with basic block counting; when the instrumented program is run, it produces a table \ntelling how many times each basic block was executed. From this table, in combination with static information \nfrom the uninstrumented executable file, we derive the five kinds of profiles. For an estimated profile, \nwe use estimated basic block counts obtained from a static analysis of the pro\u00adgram code. We divide the \nprogram into basic blocks, and connect them into procedm-es and flow graphs based on the branch structure.* \nWe then identify the loops by computing the dominator relation and finding the back edges, edges such \nthat the tail dominates the head. A loop consists of the set of back edges leading to a single dominator, \ntogether with the edges that appem on any path from the dominator to the head of one of the back edges \n[1]. We also build a static call graph by finding all the direct calls in the program. This graph will \nnot include calls through procedure variables, but we will note the presence of such calls so we can \nidentify pro\u00adcedures that not distant from true leaf procedures, Given this information, we considered \nfour different ways of estimating basic blocks counts. Each of these estimates has one or two parameters. \nThe first is the loop-only(n) estimate, in which a block s count is initially 1 and is multiplied by \nn for each loop that con\u00adtains it; this ignores the effects of the call graph. The second is the leaf-loop(njk} \nestimate, in which we com\u00adpute the distance d of each procedure from the most dis\u00adtant leaf in the call \ngraph, and then multiply the loop\u00ad only(n) count by the larger of 1 and 1024 / @ for the pro\u00adcedure containing \neach basic block. The third is the call-loop(n) estimate, in which the loop-only(n) count is The Mips \ncode generation is stylized enough that we can recognize indirect jumps that represent case-statements, \nand can deduce what the possible successor blocks are. multiplied by the static number of direct calls \nof the block s procedure. The fourth is the call+ l-loop(n) esti\u00admate, which is the loop-only(n) count \nmultiplied by one more than the static number of direct calls of the block s procedure. The call+ l-loop \nestimate is like the call-loop estimate, but procedures that are called only indirectly will not be shut \nout altogether; unfortunately procedures that are never called are also readmitted. As a reality check, \nwe also computed rantim profiles in which the items in the profiles are permuted randomly. If a random \nprofile predicts real behavior as well as an estimated profile, we know our estimating technique is not \nvery good. An optimizer would use a profile by selecting the heaviest entries in it and doing something \nspecial to them: promoting them to registers, optimizing them extra hard, or whatever. The question is \nhow well an optimiz\u00ading profile, real or estimated, predicts a timing profile assumed to describe the \nbehavior of a production run we are hoping will be fast. For this study we considered two general methods \nand two specific mentods of evaluating an optimizing profile. In the first general method, key matching, \nwe take the top n entries of the optimizing profile and see how many of them are also in the top n entries \nof the timing profile. For instance, consider the procedure profiles in Figure 1. If we let n = 8, we \nsee that the first 8 members of the optimizing profile include 5 of the first 8 members of the timing \nprofile. Thus the optimizing profile gets a score of 5/8, or 0.625. 306068 full_row 878373 cdistO 242254 \nforce_lower 245657 dl_order 190252 malloc 138058 force_lower 190250 free 72374 selp_disjoint 126993 set_or \n48672 cdistOl 86450 setp_implies 47029 malloc 71835 dl_order 47027 free 60790 set_clear 36491 full_row \n. . . . *. 19131 set_or 15065 set_clear . ** 4792 setp_implies . . . Figure 1. Optimizing profile (left) \nand timing profile. In the second general method, weight matching, we take the top n entries of the optimizing \nprofile and look up their weights in the timing profile, and then com\u00adpare the total to the total of \nthe top n entries of the tim\u00ading profile, For example, taking the profile in Figure 1 and again assuming \nn = 8, the total of the optimizing profile s top 8 entries as recorded in the timing profile is program \nglobals procs calls blocks description bisim 283 1193 4645 16054 multi-level hardware simulator bitv \n383 1384 4650 17565 timing verifier Udraw 534 4806 17653 46530 drawing editor egrep 43 72 182 1475 file \nsearcher sed 54 73 277 2049 stream editor emacs 487 1024 3568 15098 Gosling s text editor yacc 66 104 \n501 2870 parser generator ccom 170 478 3188 9483 Titan C front end gccl 610 1554 8526 40833 gnu C front \nend eqntott 55 129 477 2696 truth table generator espresso 73 434 2813 10505 set operation benchmark \nFigure 2, The eleven test programs. 553250, while the total of the timing profile s top 8 entries is \n1513681. By this measure, then, the optimizing profile gets a score of 553250/1513681, or 0.365. Note \nthat key matching is symmetric (we get the same score comparing A to B as comparing B to A), but weight \nmatching is asymmetric. For each kind of profile, we will apply this approach for different values of \nn, to see how well one real profile predicts others, and how well an estimated profile predicts real \nones, For each test program and profile class, we have several different methods of estimating profiles \nand several different test runs that pro\u00adduce real profiles, so we can get a lot of individual scores \nof one profile against another. With such a wealth of data, little of which corresponds to any kind of \ncontin\u00aduum we can graph, our only choice is to present selec\u00adtions and averages across several different \ndimensions. Key matching and weight matching are fairly abstract methods, so we also considered two specific \nways of scoring an optimizing profile. In each of these we assume a hypothetical optimization that depends \non a profile, and compare the improvement in performance from an optimizing profile to the improvement \nfrom using the timing profile as its own optimizing profile. One optimization is the promotion of global \nvariables to registers. The other is intensive optimization of the most important procedures. We should \nnote one important limitation of this approach of this paper. It does not address the stability of a \nprofile over successive versions of the same program undergoing development. One would expect that some \nkinds of profiles, such as global variable use or procedure invocation, might be relatively stable even \nwhen the pro\u00ad gram is modified. Although a program under develop\u00adment might not be run enough times to \nmerit profile\u00adbased optimization, it would still be interesting to know whether it would be feasible. \nA thorough study of this question may be in order, but is not considered here. 3. Programs and data used \nOur test suite consists of eleven programs. Two of them, a text editor and a drawing editor, are interactive, \nTwo are CAD tools used at WRL. Two are different C compiler front ends; one is recursive descent, the \nother yacc-based. Three of them are SPEC benchmarks. Fig\u00adure 2 describes the complete test suite. Wherever \npossible, we tried to give the programs realistic but quite different input data, in the hopes of maximizing \nthe differences in their behavior, We ran bisim three different ways: completely high-level simttla\u00adtion, \nhigh-level functional units with a transistor-level register file, and transistor-level functional units \nwith a high-level register file. Bitv was run to verify a datapath, a register file, and a write buffer. \nThe drawing editor was used to draw schematics and also a home landscape design. Egrep and sed were run \nwith both simple and complicated patterns, and with large and small inputs. Emacs was used to edit source \nfiles, English text files, and very long simulation configuration files. Yacc was used with a high-level \nlanguage grammm, an intermedi\u00adate language grammar, and a command grammar for a window manager. The two \nC compilers were run with the same four source files, two written by humans and two generated by the \nC++ front end. The eqntott and espresso benchmarks from SPEC were run with different inputs provided \nby SPEC. 4. Results We used both key matching and weight matching for n = 1, 2,4, 8, 16, 32, 64, and \n128. Given a test pro\u00adgram and a value of n, we proceeded as follows. We computed estimated profiles \nnine different ways, includ\u00ad ing the random profile. An estimated profile was scored against each real \nprofile for the same test program; we then averaged these scores. Each real profile was scored against \neach of the other real profiles, but not against itsel~ we then averaged all the scores comparing two \nreal call profile (!)(5000(9(90 m!mmMxm 0(900(!)00(!) (!)(!)(!)0(9(!)(!)0 cmcmmcmcl (!)mmc)cm(!) 00(9C)DCIC)C) \n0000(!)0(9(9 (9C)C)O(!3C)(9C9 (!)0(!)0(9(!)0(!) QOQ(B(9Q(B(D block profile C H!mcx?mm!) (!)(90(900(!)0 \n(90(9(9(900(!) (!)00(9000(9 Oc)ooaoao  0C9C)O(9C)(9Q 1 2 48163264128 Figure 3. Average proc entry profile \n(9C)c)c)oc)c)c) (900(9(900(9 (mcmcm(!m c)c)mm(!mi!) 00c)c)aooo (BOQQCB(B(B(3 proc time profile  0QOOQOQ(5 \n (!)(!)(!)(!)(!)00(!3 (9C)oc)c)c)oo 1 248163264128 key-matching scores for gccl. random loop-only(3) \nIoop-only(l O) leaf-loop(3,2) leaf-loop(10,2) leaf-loop(3,10) call-loop(3) call-loop(l O) call+l-loop(3) \ncall+l-Ioop(lO) other runs random loop-only(3) loop-only(l O) leaf-loop(3,2) leaf-loop(10,2) leaf-loop(3,1 \nO) call-Ioop(3) call-loop(l O) call+1400p(3) call+1400p(10) other runs random loop-only(3) loop-only(l \nO) leaf400p(3,2) leaf-loop(10,2) Ieaf400p(3,10) call-loop(3) call-loop(l O) call+l-loop(3) call+l-loop(lO) \nother runs call profile C)oooc)ooa (WX!M!)C)CH9(3 CNMN!x9cmo C)Oc)c)ocla(!l Cmcmoocm (N!M!m(mcm  C)cmocmem \n0(!)(!)(9(.!)(!)(!)0 mmmcmm!l c!)(!)mx!K9cm (B(B(MN3Q@@ block profile Cmcmm!x!m C)Cmcmcmo Om!)cx!mcm \nOcmocmcm Cmcx!mcwo 0(9(900(900 Cmcmcmcm (3(m mm9cH9  0(9(!)(!)(!)00(!) 1 2 4 8163264128 Figure globals \nprofile QCM?K900@@ andom oCU90CM.BGBe loop-only(s) () o () o (309 e loop-only(lo)  0 c)o (!9(!9090 leaf-ioop@ \n2J 0 (900 Q 009 leaf-loop(lo 2) 0 (9o () 000 fit Ieaf-loop(s l 0) 0 (5C!3o (s(itQe Catl-loop(s) () o \n(!3o c)00 e Call-loop(lo) (3o (3(?!00 Q e Call+l-loop(s)O(!30 (3(J@@@ call+ l-loop(lO) @@@@@@@@ other \nruns proc entry profile OC)C)OCM?NHJ andom 0 (!) 000 (500 loop-only(s) c)0000 Qc)o loop-only(lO) 0 \nc)00 (900 Q Ieaf-loop(s z) 0 c)(900 c)00 leaf-loop(lo z) 00 c)c100 Ieaf-[oop(s}l c!o 0) (9(900 (!9c!)o \nQ call-loop(s)  000 (3C!J0) o (9@ Call-loop(l0 (5O(3(3(J( J@ call+ l-lo0p(3) 0 (9 00 C3 @ ~ @ call+ \nl-loop(lo) @@@@@@@@ other runs proc time profile 0000O@~~ random 0000000 Q loop-only(s) 00000 Q @ @ loop-only(lO) \n(9 CJ 00000 (B leaf-loop(s~p) 00000 () Q o Ieaf-loop(lo z) 0 c) 00 (!) 000 leaf-loop(s~l 0) 00 e 00 @ \n(D o cali-loop@) 00000 Q (D @ Call-loop(lo) 0 (9 00 (9 @ ~ @ call+ l-loop(3) 00000 Q @ @ call+ l-loop(lo) \n~@@@o@@o other runs 1 24 8163264128 4. Average key-matching scores. profiles. For each test program and \neach of the two 4.3. Weight matching by percentages matching techniques, this gave us 352 scores: the \ncross product of the four profile classes, the eleven profile acquisition techniques (one random, nine \nestimated, and aggregated real profiles), and the eight values of n. As an example, Figure 3 shows the \n352 key-matching scores for the gccl test. The fraction of the circle filled with black is the score, \nso a completely black circle is perfect and a completely white circle is terrible. 4.L Key matching We \ncomputed the 352 key-matching scores for each of the test programs, and then averaged these over all \nprograms, Since the 352 scores are themselves aver\u00adages over several program runs, this means we are \ntaking averages of averages; this gives each program equal weight even though some had more datasets \nthan others. The results are shown in Figure 4. We can see that predicting which globals will be used \nis fairly easy, which is unsurprising since there are fewer of them than of the other profiled entities. \n(In some cases there are fewer than 128 globals, which lets even a random profile get a perfect score.) \nThe call-loop estimates do rather better than the other estimates. There is relatively little difference \nbetween estimates computed using the same technique parameterized differently. As we would expect, actual \nprofiles do considerably better than esti\u00admates, but even actual profiles are disappointingly bad at \npredicting which basic blocks will be executed most. Some of the estimates are surprisingly bad, doing \nlittle better than the random profile! The random profile did so well, in fact, that we suspected that \ncoincidence had given us an anomously good random order. We tested this by computing 50 random profiles \nin each case and then averaging the resulting scores. These average scores were about as good as that \nof our original random profile, and in many cases even better, so it would seem that we did not just \nhappen to hit it lucky. The fact that a random profile comes so close to some of the estimated profiles \nsuggests that these estimated profiles aren t really buying us that much. 4.2. Weight matching We also \ncomputed the 352 weight-matching scores for each of the test programs, and then averaged these over all \nprograms. The results are shown in Figure 5. We were rather more successful at weight matching than at \nkey matching.* The trends, however, are much the same: globals are easy to predict, blocks are hard, \ncall\u00adloop estimates work better than the others, and actual profiles work best of all.  * This is not \nguaranteed in general: the optimizing profile in Figure 1, for example, got a better score at key matching. \nWe observed that it was easier to predict globals and procedures than calls and blocks, at least in part \nbecause there are fewer of them. To see whether there was more to it, we tried scoring each optimizing \nprofile for values of n that are percentages of the total number of entities in the profile s domain. \nThe results are shown in Figure 6. Here we see several interesting things. As one might expect, the random \nprofile gets a score roughly equal to the percentage of items we tried to predict, more evidence that \nthese random profiles are not freaks. The differences between the profile kinds are much smallec the \nfive rows for real profiles are very similar. On the other hand, the advantage of globals is not completely \ngone: we can predict the top 2% or 10% of globals a bit more accurately than we can of blocks or even \nof pro\u00adcedures. Furthermore, the similarity between the other runs rows does not carry over as well to \nthe rows for estimated profiles: the estimated profiles get scores several times better for the top 270 \nof globals as for the top 2 %0of calls. Globals really do seem to be easier in some absolute sense, and \nnot just because the domain is smaller. 4,4. Differences between test programs There is a substantial \nvariation in the predictability of the different programs. Figure 7 shows the average score for real \n(not estimated) profiles, using the weight matching criterion. This figure shows the last rows of each \nprofile class in Figure 5, broken down by program. Emacs is astonishingly consistent from one run to \nanother, perhaps because it is built around a Lisp inter\u00adpreter, so that much of its control logic (and \nthus much of its variability) is hidden in the data structure. Unfor\u00adtunately, this argument would lead \nus to suppose that gcc 1, with a table-driven parser, might be more predict\u00adable than ccom, with a recursive \ndescent parser. But in fact ccom is noticeably more predictable than gcc 1. The least predictable programs \nare egrep, seal, and eqntott, which is quite surprising because they are also the smal\u00adlest. Figure 8 \nis analogous to Figure 7, but shows the results for the call-loop(3) estimate instead of for real profiles. \nFigures 4 and 5 suggest that this estimate was the best one we considered. Even this estimate was rarely \nmuch good at predicting the block profile, but it was quite good at predicting the global profile, particu\u00adlarly \nfor n at least 16. The estimated profile was a bit more likely than a real profile to make an anomalous \nlucky guess, giving a higher score for low n than for high n. Although emacs was predicted quite well \nby real profiles, it is predicted relatively poorly by this estimat~ espresso and bisim, among others, \nare noticeably better. globals profile (xxmocma Cmcmcxmo (!)(!)0(90000 @@Qfa@4B@@ block profile 0(!)(!)(!)00(!)(!) \n 0(9000000 0(3(900(9(!)(!) Oc)oc)Qaao 00(!)00(9(!)0 C)OOO(9CWH9 00000(9(?)(!) 0(9000(900 C)C)c)o(!)ac)(!) \n(Nmc)cmc)c) (D(J(3@999e 1 2 48163264128 Figure oooooo@Q o(90(J@96Be QOW3WMD69 CK9(MMPQQ6D C)acJocB@@e \ncmm9QwwiB eooaaeee Q(50(9@6DQe OQQWMMMD Ooc!(?G#@@@  @@@@@@em c)ac)D(9QQ(B c)ac)c)(!9c5@(B proctime \nprofile 1 248163264128 5. Average weight-matching scores. random loop-only(3) loop-only(l O) leaf-loop(3,2) \nleaf-loop(10,2) leaf-loop(3,10) call-loop(3) call-loop(l O) call+l-loop(3) call+l-loop(lO) other runs \nrandom loop-only(3) loop-only(l O) leaf-loop(3,2) leaf-loop(10,2) leaf-Ioop(3,10) call-loop(3) call-loop(lO) \ncaH+l-loop(3) call+l-loop(lO) other runs random loop-only(3) loop-only(l O) leaf-loop(3,2) leaf400p(10,2) \nleaf400p(3,10) call-loop(3) call-loop(lO) call+l-loop(3) call+l-loop(lO) other runs globals profile 00000 \n andom 00 @ Q 0 loop-only(s) 00 (B Q @ Ioop-oflly(io) 0 @) Q @ @ leaf-loop(3,2) 0 Q 0 @ @ leaf-loop(10,2) \n(!9 C!!l Q @ @ leaf-loop(3,10) 0 @ 00 @ call-loop(a) 000 @ 0 call-~oop(lo) 00000 cali+l -loop(s) o @ \n0 @ o Call+l-loop(lo) @ @ @ @ @ other runs proc entry profile 00000 andom 00000 loop-@ @) (!) 00 Q @ \nloop-only(lO) 00 () Q 0 k=f-loop(a~p) @ ~ ~ @ @ leaf-loop(l 0,2) Cl 0 e 00 Ieaf-loop(aJlo) (!9 (3 0 @ \n~ call-loop(3) 00 @ 0 6D Call-loop(lo) () 000 @ call+l -loop(s) 000 @ o Call+l-loop(lo) Q @ @ @ @ other \nruns 2 5 10 25 50% proc time profile c) 0000 andom 0 (!9 00 @ loop-only(s) 0 Q 00 Q Ioop-only(lo) 000 \n~ @J leaf-loop(3,2) 000 (9 0 leaf-loop(l 0J2) 000 (9 0 k=f-loop(a~l o) 0 @ @ @ @ calHoop(3) 0 @ @ @ @ \ncall-loop(lo) 0 (J @ @ @ call+l -loop(3) (3 (3 @ @ @ call+ l-loop(lo) @ @ ~ @ ~ other runs 2 5 102550% \nFigure 6. Average weight-matching scores for n equal to a percentage of the number of profile entries. \n call profile global profile @o@@Q@@@ e@Qee9Bee . eeaeeee . oeeeoom 00999999 . 0000000  aoaQa9@e oo(HiBeeae \n. *eeeeee . *eme*o 00(HMHM2Q. eewbeee QWW39Q99. 0000000 @cNwxDQ@@QWB96B4MB6B eoQQ@Q@6D00009900 QQwMiMBaiD \n@@GMiMDeae . eeeeeee (D@flDe@e*e block profile proc entry profile CJGDeeeeee @ee@@e(De aQo(BaCS(De . \neeeaeee 9@C#C#@ee@. eeeeeee oQQoQe@eeoooeeee @e@@a@e@@@eeoeee (9QQooaQ(J000(909690 (B(9Q(Bc#@QeaCBGB@eee* \n99999999 9c9C#@99Qe (90(saa(,ac)aoa9Qe*e 99999999 @@@@@eee (J(9Q(B@e@dD eaeeeeee 1 248163264128 proc \ntime profile eeeeeeee C300faeeee 99@@eee$D (90000000 Oeooooco Qo@oQeeo eeeeeeeo WB9996MMB Doaaeaoe \nC)(BQGBGDeee @@@@@@*@ 1 2 4 8163264128 Figure 7. Average weight-matching scores for real profiles. bisim \nbitv ccom egrep emacs eqntott ~ espresso gccl sed udraw yacc bisim * bitv ccom egrep emacs + eqntott \nespresso gccl sed ud raw yacc bitim bitv Ccom grep mats   eqntott espresso gccl ed draw yacc bisim \nbitv cco m egrep emacs eqntott espresso gccl sed udraw yacc 1 2 4 8163264128 Figure 8. Average weight-matching \nscores for calI-loop(3) estimates. %5. Global register allocation To apply this technique to a realistic \nspecific exam\u00adple, let us suppose that we suddenly have eight registers available that we can use to \npromote eight global vari\u00adables or constants. They payoff of doing this is that all the loads and stores \nof the globals we seleet will be removed. We can estimate our improvement in perfor\u00admance by counting \nthe executions of these loads and stores and dividing the total by the total number of instructions executed.* \nWe did this both for a timing profile (to see how well we could possibly have done) and for an optimizing \nprofile, in each case computing the counts using the timing profile, The results are shown in Figure \n9. This optimiza\u00adtion by itself doesn t do a lot for performance: even if magically driven by the counts \nfrom the timing profile, the improvement in performance is only 2.7%. A good estimated profile gives \nus about half of the maximum possible performance improvement, and au actual profile gives us about 85% \nof the maximum, improv max ratio random 0.6?40 2.7% CJ loop-only(3) Ioop-only(l O) 1.370 1.270 2.7% \n2.7% () Q leaf-loop(3,2) Ieaf-loop(l 0,2) 1.270 1.270 2.7% 2.7% Q Q leaf-loop(3,10) call-ioop(3) 1.0940 \n1.2!40 2.7% 2.7% Q Q call-loop(l O) 1.3% 2.7% o call+l -loop(3) 1.3!/0 2.7% Q call+l other -Ioop(l O) \nruns 1.270 2.3!Xo 2.7% 2.7% u) e Figure 10. Improvement from intensive optimization of procedures selected \nusing procedure entry profile.  Figure 9. Improvement from global register allocation. 4.6. Selective \nintensive optimization As a second specific example, let us suppose we have an excellent but expensive \noptimization algorithm that will cut the execution time of any procedure in half, but that is so expensive \nthat we can apply it only to 5% of our procedures. * This does not take pipehne stalts mto account, nor \ndoes it consider cache effects, which are likely to increase the benefit of promotiug globals to registers. \nIt also assumes that the globals selected are not ineligible because of aliasing. We are interested only \niu rongh numbers here, as an example. First we will select as the procedures to optimize those we believe \nwill be invoked most often, by picking the first 5% of the entries in the procedure entry profile. As \nbefore, we will do this both for an optimizing profile and also for a timing profiky we will compute \nthe improvement in performance using only the counts from the timing profile. improv max ratio random \n2.4!Xo 31 .2?4 o loop-only(3) 7.470 31.270 () Ioop-only(l O) 7.4% 31 .2?40 leaf-loop(3,2) 3.7% 31.270 \nIeaf-loop(l 0,2) 3.770 31 ,2Y0 leaf-loop(3,10) 3.770 31.270 call-loop(3) 7.370 31.270 call-loop(l O) \n7.3940 31.270 call+l -loop(3) 7.3% 31.270 call+l -Ioop(l O) 7.3% 31.2% other runs 24.3% 31 .2!40 The \nresults are shown in Figure 10. This optimiza\u00adtion would speed up our programs by a third if it were \ndriven by a perfect profile. A real profile gives us about three-fourths of that, but even the best estimated \nprofile -\u00adwhich oddly enough was the simple loop-only estimate -\u00adgives us barely one-fourth. The worst \nestimates are hardly better than random profiles, Picking the procedures to optimize based on how many \ntimes they are called is imprecis~ we are more likely to want to optimize the procedures where we spend \nthe most time. Let us do selective intensive optimization again, this time choosing the top 5~0 of the \nprocedures in the procedure time profile instead of the procedure entry profile, Figure 11 shows the \nresults, As one might expect, applying the optimization in this fashion is better, giving us a possible \nimprovement of 43.7Yo.t As before, a real profile from a different run can get about three\u00adfottrths of \nthat. This time, though, the best estimates give us nearly half, and the worst are still better than \nrandom profiles. This suggests that procedure times are rather easier to predict than procedure invocations, \nat least at the top 5 %. level. This is borne out by a study of Figure 6; there is somewhat more black \nink in the procedure time profile than in the procedure entry profile. t So this would be a really great \noptimization if it existed! References improv max ratio random 2.0% 43.770 ~ loOp-only(3) 14.5% 43.7!40 \n@ Ioop-only(l O) 15.1!40 43.770 ~ teaf-loop(3,2) 8.3% 43.7?40 ~ loaf-loop(10,2) 9.470 43.770 ~ leaf-loop(3,10) \n6.3% 43.770 ~ caH-loop(3) 18.4% 43,770 ~ call-loop(l O) 19.39 0 43.770 ~ Call+l -loop(3) 18.670 43.70/0 \n@ Wdl+l -Ioop(l o) 19.3% 43.770 (J other runs 34.570 43.770 @ Figure 11. Improvement from intensive optimization \nof procedures selected using procedure time profile.  $. Cohchwions Real profiles horn different runs \nworked much better than the estimated profiles discussed in this paper. The best estimations were usually \nthose that combined loop nesting level with static call counts. Basing the esti\u00admate on the procedure \ns distance from leaves of the call graph was less effective. The worst estimates were hardly better than \nrandom profiles. There may of course still be better ways to estimate a profile: this is an interesting \nopen question both in the general case and in specitic applications. Even a real profile was never as \ngood as a perfect profile from the same run being measured. It was often quite. close, however, and was \nusually at least half as good. Profile-based optimization would seem to have a future, but we must be \ncareful how we measure it, lest we expect more than it can really deliver. Acknowledgements My thanks \nto Alan Eustace for goading me into finally doing this study, to Patrick Boyle, Mary Jo Doherty, Ramsey \nHaddad, and Joel McCormack for help\u00ading me obtaifi some of the data, to Anita Borg, Joel McCormack, and \nScott McFarling for helpful comments on the draft, and to Joel Bartlett for the ezd tool that let me \ndraw the 2508 pie-charts in this paper. [1] Alfred V. Aho, Ravi Sethi, and Jeffrey D. Unman. Compilers: \nPrificiples, Techniques, and Tools, pp. 602-605. Addison-Wesley, 1986. [2] Joseph A. Fisher, John R. \nEllis, John C. Rutten\u00adberg, and Alexaudru Nicolau. Parallel processing: A smart compiler and a dumb machine. \nProceed\u00adings of the SIGPLAN 84 Symposium on Compiler Construction, pp. 37-47. Published as SIGPLAN Nolices \n19 (6), June 1984. [3] Scott McFarling. Program optimization for instruction caches, Third International \nSympo\u00adsium on Architectural Support for Programming Languages and Operating Systems, pp. 183-191, April \n1989, Published as Computer Architecture News 17 (2), Operating Systems Review 23 (spe\u00adcial issue), SIGPLAN \nNotices 24 (special issue). [4] MIPS Computer Systems, Inc. Language Programmer s Guide, 1986. [5] Scott \nMcFarling and John Hennessy. Reducing the cost of branches. Proceedings of the 13th Annual Symposium \non Computer Architecture, pp. 396-403. Published as Compuler Architecture News 14 (2), June 1986, [6] \nKarl Pettis and Robert C Hansen. Profile guided code positioning. Proceedings of the SIGPLAN 90 Conference \non Programming Language Design and Implementation, pp. 16-27. Published as SIGPLAN Notices 25 (6), June \n1990. [7] Vatsa Santhanam and Daryl Odnert. Register allocation across procedure and module boun\u00addaries. \nProceedings of the SIGPLAN 90 Confer\u00adence on Programming Language Design and Implementation, pp. 28-39. \nPublished as SIG-PLAN Notices 25 (6), June 1990. [8] David W. Wall. Global register allocation at link-time. \nProceedings of the SIGPLAN 86 Sym\u00adposium on Compiler Construction, pp. 264-275. Published as SIGPLAN \nNotices 21 (7), July 1986. Also available as WRL Research Report 86/3.  \n\t\t\t", "proc_id": "113445", "abstract": "", "authors": [{"name": "David W. Wall", "author_profile_id": "81100439103", "affiliation": "Digital Equipment Corporation, Western Research Laboratory", "person_id": "PP39042865", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/113445.113451", "year": "1991", "article_id": "113451", "conference": "PLDI", "title": "Predicting program behavior using real or estimated profiles", "url": "http://dl.acm.org/citation.cfm?id=113451"}