{"article_publication_date": "05-01-1991", "fulltext": "\n Strictness and Binding-Time Analyses: Two for the Price of One John Launchbury University of Glasgow \njl@dcs.glasgow. ac,uk Abstract Binding-time analysis has received increased att en\u00ad tion in the literature \nrecently due to its importance in partial evaluation. This mimics the growth in int crest over the last \ndecade in strictness analysis, due to its implications for code quality of compiled lazy functional languages. \nIn this paper we eX\u00ad amine both analyses in a common framework, and demonstrate that they are equivalent. \nAs a prac\u00ad t ical consequence of the equivalence, we show how Hughes work on polymorphic strictness analysis \ncarries over to binding-time analysis. 1 Introduction Program analyses have always been important in \nprogram-manipulation tools. Two classic examples occur in compilers, namely type checking and dead\u00adcode \nelimination, As programming tools become more advanced, more detailed and intricate anal\u00adyses are required, \nresulting in a large proliferation with each analysis appearing in many different va\u00adrieties, In this \npaper we take a step towards reduc\u00ad ing the diversity by demonstrating that two impor\u00ad tant analyses \nstrictness analysis and binding-time analysis are actually the same analysis, but tai\u00ad lored to different \napplications. Consequently, prac\u00ad tical techniques or theoretical considerations of one analysis are \nlikely to be directly relevant to the Permission to copy without fee all or part of this material is \ngranted provided that the copies are not made or distributed for direct commercial advantage, the ACM \ncopyright notice and the title of the publication and its date appear, and notice is given that copying \nis by permiaaion of the Association for Computing Machinery. To copy otherwise, or to republish, requirea \na fee and/or specific permiaaion. @ 1991 ACM ()-89791 .428 -7/91 /()() 05/() ()80... $1 .50 Proceedings \nof the ACM SIG PLAN 91 Conference on Programming Language Design and Implementation. Toronto, Ontario, \nCanada, June 26-28, 1991.  80 other, We also provide a concrete example of this in applying the methods \nof polymorphic strictness analysis to polymorphic binding-time analysis. Familiarity with either strictness \nanalysis or binding-time analysis will be usefd in reading this paper. In particular, we rely heavily \non the work of Wadler and Hughes in projection-based strictness analysis [WH87], and on Jones re-examination \nof partial evaluation [Jon88]. As in these papers, the results of this paper are largely intended to \nbe lan\u00adguage non-specific. Most of the work, therefore, is performed at a semantic level, but using only \nba\u00adsic notions of domain and category theory. All we assume about the language is that it is purely func\u00adtional, \nfirst-order and strongly-typed. For the sec\u00adond part of the paper we will also assume that the type system \nprovides parametric polymorphism, as in the Hindley-Milner type syst em, for example.  2 Strictness \nAnalysis A function ~ is called strict if ~ 1 = J-, Strictness has provoked a lot of interest because \nof its impli\u00adcations for improving the code quality of compiled lazy functional languages. In a lazy \nlanguage, ar\u00adguments to fimctions are normally passed uneval\u00aduated, in case their value is not needed \nto produce the result of the fi.mction. If, however, the function is strict, its argument may be quite \nsafely evalu\u00adated prior to evaluating the body of the function. Consequently, strict functions may take \ntheir argu\u00adments in an evaluated form rather than in a more costly closure. Determining whether or not \nf L = L is same as deciding whether or not j will terminate with in\u00adput -!-so, in general, strictness \nanalysis is equiva\u00adlent to the halting problem. However, despite the halting problem being uncomput able \nin its full gen\u00ademlity, there are many programs which clearly do terminate, and there are many which \nclearly do not. This means that we can write an analysis pro\u00adgram which approximates the halting problem \nin the following sense: if the analysis can be sure that the input program definitely loops then it will \nsay so, otherwise it will suppose it halts. Strictness analysis is such an approximation. The notion \nof strictness may be extended to cover more interesting situations. For functions over list domains, \nfor example, simple strictness is not particularly useful, The argument to a func\u00adtion which is merely \nstrict maybe safely evaluated to the first cons-node but no further: other parts of the list must be \nleft unevaluated, If however the function is tail-strict, then it is safe to evaluate the spine (or structure) \nof the list prior to the call. An example of this is the length function. There are essentially two main \napproaches to the analysis: forwards and backwards. Forwards anal\u00adysis attempts to address the strictness \nquestion di\u00adrectly by considering if the function returns L when applied to L. This is most commonly \nattempted via abstract interpretation ([AH87]). In contrast, backwards analysis considers how demand \nis prop\u00ad agated, It deduces how much input a function re\u00adquires to produce a certain amount of output. \nThe name backwards analysis arises because informa\u00adtion is propagated from a function result to its ar\u00adgument. \nIn [WH87] domain projections are used to specify what is meant by a certain amount of information. Defhition \nA domain projection y on a domain D is a continu\u00ad ous function 7: D + D such that (i) 7 ~ ID, and (ii) \n707 = 7 (idempotence). Suppose, we are performing a backwards analysis and want to know how much of its \nargument some function j : X + Y needs in order to be able to return 7 s worth of result (where 7 is \nsome projec\u00adtion 7: Y + Y). Let us call this amount $ (that is, a projection /3 : X -+ X). How are f, \ny, and ~ related? The answer is that they must satisfy the safety condition: Consider applying both sides \nto some value z. The safety condition implies that the application of (7 o f) to z gives exactly the \nsame value as apply\u00ad ing it to (~ z). So to get 7 s worth of information about the result of (~ z) we \nonly need to know /3 s worth about z. Of course, we could still get at least -y s worth if we knew more \nabout z. That is, if 6 is another projection such that ~ L 6 then 70 f = 70 f 06 also holds. This means \nthat it is always acceptable for a backwards strictness anal\u00adyser to approximate upwards a larger projection \nthan the optimum will still be safe. In backwards analysis smaller projections convey more accurate information. \nThere is an equivalent formulation of the safety condition that is often useful in proofs, namely, that \n70 f = 70 f o ~ holds exactly when the con\u00addition ~ o f ~ f o/3 holds. The proof follows easily from \nthe fact that both v and ~ are projections, and may be found in [WH87]. We will freely swap between the \ntwo formulations and use whichever is most appropriate e at the time, While we concentrate on projection-based \nstrict\u00adness analysis in this paper, it is directly related to strictness analysis based on abstract interpretation. \nThe relationship is described in detail in [Burn90].  3 Binding-Time Analysis Binding-time analysis \nhas been shown to be vital in the process of partial evaluation [BJMS88]. At its simplest, partial evaluation \nmay be thought of as currying on programs. From a program describ\u00ading a function ~ : A x B + Y and a \ndescription of a value a c A, a partial evaluator produces a program describing the corresponding specialised \nfunction fa : B -+ Y. The new program is to be an optimised version of the old, having taken the input \nvalue into account. It is a very powerful technique with applications ranging from the gen\u00aderation of \ncompilers from interpreters, to the au\u00adtomatic optimisation of expert systems or theo\u00adrem provers. The \nprocess is described in detail in [JPS85, Ses86, JPS88] and in many of the papers in [BEJ88]. The situation \nabove represents the simplest case and, in general, there is no reason to expect things to be so straightforward. \nFor example, the pro\u00adgram may have many arguments with data sup\u00adplied for the first and third, say. Alternatively, \nthere may be incomplete information given about a single argument we might know that the value of an \nargument is a list of three elements, for ex\u00adample, but know nothing about the values of those elements. \nIn the general case, a partial evaluator takes a program describing a function ~ : X + Y, where X and \nY are arbitrary domains defined within the type system of the programming language, and a partial description \nof an input value z c X for j, The purpose of binding-time analysis is to determine exactly which portion \nof X will be available during partial evaluation (the static part ) and which will not (the dynamic part \n). Differ\u00adent analyses are able to discover different degrees of separation. For example, the early analysis \nin [JPS85] distinguished only between whole param\u00adet ers (or, equivalently, between whole parts of a \ntuple), whereas others are able to describe trees or other data structures which are only partially\u00adstatic \n[Lau88, Mog88]. Given a program and a description of which parts of the input are static, binding-time \nanalysis pro\u00adduces a description for each of the functions in the program. This description details which \narguments (or, in general, which parts of which arguments) are static. The aim of partial evaluation \nis to replace each function call with a call to an appropriate spe\u00adcialised version of the function. \nThese specialised versions will ret ain the dynamic parameters only, as the static parameters of the \noriginal function are fixed to particular values. There are two senses in which the binding-time description \nis an approximation. Firstly, an argu\u00adment (or, as ever, part of an argument) is only cle\u00adscribed as \nstatic if the analysis may be sure that it only depends on static input. If the analysis cannot tell, \nthe argument is dubbed dynamic. Secondly, all the uses of a function are usually lumped together and \nonly the intersection of the static portions is considered static. 3.1 Congruence In order to specify \ncorrectness of the analysis Jones defines a condition called congruence [Jon88]. This has become the \nstandard correctness condition in binding-time analysis and guarantees that argu\u00adments marked static \nonly depend on other static values. This is crucial, for it guarantees that there is enough information \npresent to decide which spe\u00adcialised version of the function to call. Other\u00adwise, the partial evaluator \ncould not tell which spe\u00adcialised version of a fmction should be used to replace any particular function \ncall, and instead would have to introduce a (possibly infinite) con\u00ad ditional branch. Jones models a \nprogram in terms of its step\u00ad wise behaviour and then uses this model to define congruence. The program \nis regarded as a triple (P, V, nz) where P is a set of program points, V a set of values (states) and \nnx a step function map\u00adping (p, v) pairs into (p , v ) pairs. Each (p, v) pair represents a single point \nin the computation, and the function nx defines a single computation step from program point p and value \nv the com\u00adput ation proceeds to program point p and value v . The program is understood to have terminated \nwith value v whenever nx (p, v) = (p, v). In fuc\u00adtional programs, we may use function names as the program \npoints. The choice of the destination program point un\u00adder the action of nz depends, in general, on both \nthe initial program point and the value. So from any given program point p, the destination point depends \non the value at that point. At p, therefore, we can partition the value set V into subsets { Vi} such \nthat if v G V; then the destination point is pi. Moreover, we can define functions j : Vi + V such that \nvc Vi + nx (p, v) = (pi, A v). Any such choice of partition and functions is called a control transfer, \nA collection of control transfers, one for each program point, is called a control structure. Jones defines \ncongruence in terms of a control structure and a program division. A division con\u00adsists of three collections \nof functions-static, dy\u00adnamic, and pairing functions-indexed by the pro\u00adgram points. We will typically \ncall these u, 6, and m respectively, There are three conditions that must be satisfied: (0 ~P(u.~,q v) \n= v (ii) Up (Tp(v,, vd)) = U, (iii) 6P (TP (v$, vd)) = Vd where v~ ranges over static values, vd over \ndynamic values, and v over the whole of V. The first con\u00ad dition requires that between them, the static \nand dynamic functions do not lose any information the pairing function 7P is able to reconstitute the \noriginal value from the two parts, The other two conditions imply that the static parts stay static, \nand the dynamic dynamic. Definition A division (a, 6, m) is congruent at a program point p with respect \nto a given control structure {(Vi, fi : Vi + V)} if for each i, Vv, wex . opv=upw * up~ (j v) = Op$ (fi \n~) The definition requires that any two values with equal static parts are mapped to new values whose \nstatic parts are also equal the dynamic portion of the value is not required. Thus, if a division is \ncon\u00adgruent, we will be able, during partial evaluation, to calculate the static part of a value at any \npoint in the computation: we can calculate the initial static value it is given to us and if we assume \nwe can calculate the static value at some program point, congruence ensures that we will be able to calcu\u00adlate \nit at its immediate successors. Thus, given a congruent division there is enough information present \nto calculate the value of UP v, and so to choose which specialised version of p will replace (p, v). \n 3.2 Congruence Revised It turns out that, as currently defined, congruence is too weak a condition for \nmost partial evaluators. To see this, consider the following example, Sup\u00adpose we have the function, \npO (x, y) = if y=3 then pi (x*y) else p2 x and a division Upo = fst, Upl = ID and 0P2 = ID (where jst \n(z, g) = z). The set of values V at p. is N x N and the control structure is given by, Vi = {(z,3)[zc \nN} v* = {(%1/) l~GN, YE N\\{3}} The transfer functions are given by ff (X,Y) = Xxy f2(~,Y) = ~ To see \nthat this division is congruent suppose that V,W ~ VI and that aPo v = aPo w. Then UP2(ff v) = ff v = \nfst v x 3 {defn of ~1} = fstw x 3 {UPOV=UPOVJ} =f, w = Upl (fl w) and the case of V2 is as easy, But, \neven though the division is congruent it would cause problems for most partial evaluators. Congruence \nonly con\u00adstrains ~1 in the restricted context in which it will actually be called, and not over the whole \ndomain of values, This means that divisions may still be congruent even though they take into account \nfacts implied from surrounding conditionals. Thus ff is allowed to know that its parameter y will have \nvalue 3. If a division takes advantage of this then so must the specialisation algorithm in effect, it \nmust perform driving [Tur86]. What is more, the partial evaluator must extract and use all the infor\u00admation \nimplied by the conditional just in case the division has taken advantage of it. In general this is uncomput \nable, of course. Yet, if the specialisation is performed by an ordinary partial evaluator then congruence, \nis normally used. However, this is de\u00adfined syntactically rather than semantically which makes it heavily \nlanguage dependent. It is possi\u00adble to revise the definition of congruence so that it loses this value \ndependence but otherwise remains the same. Being a denotational definition, it re\u00admains largely language \nindependent. some divi sions will act as if they were not c ongru\u00ad in the remainder of the paper, we \nmention congru\u00ad ent, as the partial evaluator w ould not be able to ence we will be referring to the \nrevised defln.ition. determine w hich s p ecialised ver sion of the fu nction to call. In p ractice, \nproblems do not occur as a far stronger vers ion of congruence, namely intenti onal 4 Strictness and \nBinding-Time In the definition of control structures the func\u00adtions {fi} were only defined on the particular \nVi, and so it only made sense to draw the values v and w from Vi. This led to value dependence. There \nis actually no reason why the functions {~i} should not be defined over the whole of the value domain \nV. After all, that is the range of their definition in the program. The original {fi} are just restricted \nversions of these. Let us now use {ji} to denote the unrestricted versions, so that A : V + V for each \ni. Now we can define a revised variant of congru\u00adence which is value independent. Definition Revised: \ngram point A division p with (u, $, T) respect is to coa ngruent control at stru a cture pro\u00ad {(vi, j \n: V + V)} if for each Z, VV, WE + V. Upi UP V=UPW (fi v) = CrPi (J w) Not e that, unlike the original \ndefinition of con\u00ad gruence, the values v and w are free to range over the whole of V, As this is a stronger \ncondition than before, divisions that are congruent under the re\u00advised definition are also congruent \nunder the orig\u00adinal definition, but the converse is only true if any two values with equal static parts \nare given equal static parts by ~i. Even though the revised ver\u00adsion of congruence is a stronger condition \nthan the original it is still weaker than the various forms of int ensional congruence used in practice. \nBy com\u00adparing the two definitions of congruence it appears clear that the revised version cent ains the \nmini\u00ad mal strengthening necessary to remove value de\u00adpendence and the implied need for driving. when, \n  Analyses Are Equivalent We will cast the material of the previous section in terms of domain projections \nand, as a conse\u00ad quence, demonstrate that the congruence condition is equivalent to the safety condition \nfrom strictness analysis. To do this we have to make a small ex\u00ad tension to Jones program model. The \ndefinition of nz assumes that it will always be possible to deter\u00ad mine which program point is the destination. \nThis is not unreasonable in an iterative language where the value v is computed using built in operators \nonly. In a recursive language, the comput at ion of v may be given by user defined functions and so may \nnot terminate. Then nz (p, v) will be unde\u00ad fined. This must be reflected in the control struc\u00ad ture. \nWe add a new program point pl and define nx (PL, V) = (pJ., v) for all values v c V. Adding an ordering \nwhere pl L p for all p c P makes P into a (flat) domain. V likewise becomes a do\u00ad main and the { Vi} \ndisjoint Scott open sets in V. The rest of V (that is, V \\ U{ V;}) is a closed set which we will call \nVL. Finally we define the trans\u00ad fer function jl :V-+Vby f~v=l. So, ifv6Vi for some i(+ L), then nx (p, \nv) = (pi, f v) as be\u00ad fore, but if v c VI then mc(p, v) = (p~, 1) and the value of the program is -i-. \nNotice that V1 may be empty at some program points. At every program point ~1 v = L, so a division which \nis congruent with respect to some control structure will still be congruent if we extend the control \nstructure with V1, This means that we can be a little sloppy with our notation. We will typically include \nV~ in the { Vi}. 4.1 Projections in BTA The program divisions used to define congruence allowed for \na wide choice of funct ions u, 6 and r and freedom in choosing where static values live. One fairly natural \nchoice is to specify that the static values form a sub-domain of the whole value space and that a is \na domain projection. The intuition behind this is that (i) projections remove informa\u00adtion (capturing \nthe intuition that the static portion of a value can be no greater than the value itself), and that (ii) \nthey remove it all in one go (so that static values stay static). For suit able choices of 6 and r, projections \nindeed satisfy the requirements to be part of a division. In fact, if we choose m to be least upper bound, \nthen the definition of divisions forces both u and 8 to be projections. The following definition uses \nthe notion of safety from strictness analysis and applies it to binding\u00adtime analysis. Deflnit ion A \ndivision (o, d, ~) is safe at a program point p with respect to a control structure {( Vi, j : V + V)} \nif for each i, Opiofi = Opio f o Up Theorem 1 Let A (= (~, 8, m)) be a division. A is safe if, and only \nif, it is congruent. Proof Assume A is safe. Let p be a program point and let {( Vi, j : V + V)} be the \ncontrol structure at p. As A is safe we know that eP~o~ = OPiof oOP for each i. We want to prove that \nif UP v = UP w then uPi (fi v) = uPi (Xw)foralliandv, w~ V. So, assume that up o = afl w for some arbitrary \ni and V,W < V, Then, pi (j ) = (~piOj) V = (Upi OjO~*)V {safety} = ( ~i J) (o, ) = (aPi o J) (a, w) {assumption} \n= ( ~iojoap)~ = (U,, oj) w {safety} = up~ (A w) and so A is congruent a p, Conversely, let us assume \nA is congruent. Let p be a program point and let {( Vi) &#38; : V --+ V)} be the control structure at \np, The projection UP is idempo\u00adtent so UPv = UP(OP v) for any value v E V, As the division is congruent \nwe may conclude that up; (i ~)= ~P~ (j (~, v)) for any value v 6 V. In other words, aPi o j = oPi o fi \no Qp as required, 0 The theorem demonstrates that, in a projection\u00adbased binding-time analysis, we may \nquite safely use the safety condition from strictness analysis to specify correctness. Conversely, for \nstrictness anzd\u00adysis, we can equally well use the congruence condi\u00adtion, if we so choose. Thus once either \na strictness analyser or a binding-time analyser has been writ\u00adten, we may, at least in principle, use \nit for the other analysis. There are, however, distinctions between the two analyses that should be noted. \nThe first is that the particular choice of projec\u00adtions valuable for one analysis may not accord with \nthose most appropriate for the other. This may be seen by contrasting the construction of domains of \nprojections in [Lau88] with that of [Hug89]. How\u00adever, the particular choice of projections is unlikely \nto tiect the guts of the analysis machine, so this difference is relatively minor. A second reason has \nfurther reaching implica\u00adtions. In [WH87] it is argued that projection\u00adbased strictness analysis is naturalIy \na backward analysis, in that demand is propagated from out\u00adput to input. In contrast, binding-time analysis \nseems to be most natural as a forwards analysis: information about the initial input is propagated through \nthe program. There is some benefit in this dichotomy, Having two equivzdent amdyses, one of which is \nnaturally backward while the other is forward, promises to allow the understanding of the relationship \nbetween forwards and backwards analyses to be furthered. Similarly, the real value of the equivalence \nbetween the analyses is not so much that one analysis program may be used for the other, but that the \ntechniques and theory ap. plicable to the one may be used in the other. It is this aspect that we develop \nby example in the next section.  5 Polymorphism In [Hug89], Hughes presents a theory of projection\u00adbased \nstrictness analysis of polymorphic functions, We will follow a parallel presentation, leading to a corresponding \napproach for binding-time analysis of polymorphic functions. A polymorphic function is a collection \nof distinct monomorphic inst antes which, in some sense, be\u00adhave the same way. Ideally, we would like \nto take advantage of this uniforrnit y to analyse (and per\u00adhaps even specialise) a polymorphic fimction \nonce, and then to use the result in each instance. Up to now the only work in polymorphic partial evalua\u00adtion \nhas been by Mogensen [Mog89]. However, with his polymorphic-inst ante analysis, each inst ante of a polymorphic \nfunction is analysed independently oft he other inst antes and, as a result, a single fimc\u00adtion may be \nanalysed many times. Hughes approach to strictness analysis of poly\u00ad morphic functions uses the increasingly \npopular view of polymorphic functions as natural trans\u00ad formations. This view was implied by Reynolds \nearly work on the polymorphic lambda calcu\u00adlus [Rey74], and has recently been developed in [BFSS87, FGSS88, \nWad89] amongst others. In this paper we address the issues raised by pammet\u00ad ric polymorphic only. As \nad hoc polymorphism can be modelled as parametric polymorphism in a higher-order language [WB89] we leave \na discussion of this brand of polymorphism to a paper describ\u00ad ing the analyses at higher orders. Parametric \npolymorphism corresponds to a reuse of essentially the same fuction applied to objects of different types. \nThe basic intuition behind such functions is that they do nothing to the polymor\u00adphic parts of their \narguments except possibly dis\u00adcard or duplicate them. The very same reverse fimction, for example, will \nwork identically on both lists of integers and lists of booleans. One way to express this is to imagine \nsome function from inte\u00adgers to booleans being applied to each of the ele\u00adments of a list. Because the \nbehaviour of reverse is consistent across these types we could apply the function either before or after \nreversing the list without affecting the final result. We can state this more generally. If a function \nis truly polymorphic (in the parametric sense) then we cannot trick it into altering its action by apply\u00ading \nsome coding function to the polymorphic parts of its argument prior to application. We would ob\u00adtain \nthe same result by applying the same coding function aft er application. The fact that the val\u00adues of \nthe polymorphic parts of the argument are different in each case will not result in a different behaviour. \nOf course this is still rather vague. For example we have not specified what we mean by the polymorphic \nparts of an argument . We use the language of category theory to supply the nec\u00ad essary precision. 5.1 \nTypes as Functors We focus on one particular category which we de\u00adnot e by C, that of Scott domains with \ncent inuous functions. In a monomorphic language it is sufR\u00adcient to model types by domains and program \nfunc\u00adtions by continuous functions, but this is not suf\u00adficient if the language is polymorphic. It is \nuseful to consider type constructors to see the necessary generalisation. Type constructors, such as \nList or Pair, takeone or more types and return a new type. They may be successfully modelled by functors. \nFor exam\u00adple, from the domain of integers, the List functor will return the domain of lists of int egers. \nFunct ors act on arrows also. Assuming they are defined in terms of basic type constructions, then by \ndefining the actions on arrows of the basic type construc\u00adtions in an obvious way, we can derive the \naction of any type constructor. So, for example, the ac\u00adtion of List on arrows is given by map (the arrow \n(function) is applied to each element of the list). List is a functor List : C + C but as an arbitrary \ntype constructor may have many arguments each will correspond to a functor F : C --+ C for some n. Monomorphic \ntypes may be included in the same scheme. Such types, for example .Boo1, are funct ors Bool :1 + C where \n1 is the terminal category C 0 cent aining only the one point domain and the iden\u00adtity function. Any \nsuch functor has no opportunity to vary and so is constant. The image of the Bool functor, for example, \nis just the boolean domain. Types themselves, therefore, are no longer mod\u00adelled by domains directly, \nbut by funct ors. An\u00adother monotype is List Bool. Because we treat monotypes such as Boot as functors \nBoot :1 + C, the usual application of type constructors to types must be replaced by functor composition. \nThen ListBool (actually, List o Bool of course) is also a fi.mctor List BOOl :1 + C. !5.2 Natural Transformations \nFunctions defined in the program correspond to mappings bet ween types. As types are modelled by fimctors, \nthese functions should be modelled by transformations between functors. In fact, by natural transformations, \nA natural transforma\u00adtion ~ : F -+ G between functors is a collection of functions (which correspond \nto the monomor\u00adphic instances). If the source and target of F and G are the categories D and &#38; respectively \nthen for each object D 6 D there is a correspond\u00ading function fD : FD + GD in &#38;. These fuctions are \nuniform (or natural) in the following sense: If 7: D + D is any function in D then the property that \nG7 o fD = fD# o F7 must hold. This captures precisely the notion that all the in\u00adst antes of a polymorphic \nfunction behave, in some sense, in the same way. It also expresses our intu\u00adition about applying coding \n(or other) functions to the polymorphic parts either before or after appli\u00adcation of the polymorphic \nfmction without chang\u00ading the result. In the case of reverse, for example, this means that List f o reverse \n= reverse o List f for any function ~ : X + Y, or to use more usual not ation, that map f o reverse = \nreverse o map f. To strengthen the intuition fwther we shall con\u00adsider a couple of examples. We have \nseen the im\u00adplications for the Lid functor with the fhnction reverse. Now consider the selection function \nfst. Its type is fst : Vs.Vt.(s x i!) + s. Expressed in the functor notation we could write j st : Pair \n+ F$t where Pair st=sxtandFstst=s.Eachof these are functors C2 + C. The naturality con\u00addition says that, \nfor any continuous fmctions Y: A+ Band 6: C+ D, it must be the case that Fst 780 fst = fst o Pair y $, \nIn other words, that 7 (fst (z, y)) = fst (7 x,8 y) for all z, y. This equality is clearly true. 1Things \nare this simple only because we are considering a first-order language. In a higher-order language somewhat \nmore complex constructions are required. All this works for monomorphic functions as well, Recall that \ntypes such as Bool or Int correspond to fi.mctors Bool :1 + C and Int :1 + C. Consider an arbitrary function \nf : Int + Bool, say. There is no polymorphism here as the function is purely monomorphic, so how does \nthe naturality condition apply? The only function in the trivial category 1 (i.e. Co) is the identity \nfunction, which is mapped by any functor 1 + C onto the identity fimction of the object picked out by \nthe functor. Thus the nat\u00adurality property reduces to the condition that ~ sat\u00adisfies the equation lD~OOz \no f = f o IDrmt, But this is no restriction at all, and so f may be any fimc\u00adtion. It is only when a \nfunction is not monomor\u00adphic, therefore, that the naturality condition has any effect. Depending on the \nemphasis at any particular time, we will either give the type of polymorphic functions in the usual notation \n(possibly involv\u00ading quantified type variables) or in funct or not a\u00adtion (having abstracted over the \ntype variables). It should be clear that the two axe interchangeable by abstraction. There is a small \nmodification necessary that we will mention briefly. In most languages with recur\u00adsion, the fact that \nL is an element of every type de\u00admands a minor change to the above. Consider the fwction definition, \nf x = f x. The function f is the constant 1 function and has type f : Vt .t + t or, equivalently, f : \nId + Id. For the naturalit y property to hold, that is, for 1d7 o f = f o Id 7 to be true, 7 must be \nstrict. Domains together with the strict continuous functions form a sub-category of C which we write \nC,. We change our view of type constructors and regard them as functors C; + C~. We can do this since \nall the usual type constructions preserve strict functions. However, there is a minor technical\u00adity. \nRegarding program defined functions as natu\u00adral transformations bet ween funct ors C; + C$ only cat ers \nfor strict polymorphic functions. But, as ev. ery functor C; + C, may be viewed as a funct or C: + C \nby inclusion, this problem may be solved by treating program defined fwctions as natural transformations \nbetween functors C: + C.  5.3 Polymorphic Analysis Theorem In the same way that we can talk about polymor\u00adphic \nfunctions, so we may talk about polymorphic projections. A polymorphic projection is a natural transformation \nwhose every inst ante is a projec\u00adtion. Equivalently, as natural transformations are closed under composition, \nand as Ill is itself a nat\u00adural transformation over ever y type, we can use the usual definition at the \nlevdl of natural transforma\u00adtions. In addition to ID, the projection ABS (the constant 1 function) is \nalso polymorphic over every type. The following theorem (taken from strictness analysis, [Hug89]) tells \nus that polymorphic projec\u00adtions go a long way to capturing the strictness or binding-time properties \nof polymorphic functions. Theorem 2 Hf :F + Gispolymorphic, andifa:G + Gand @ : F + F are polymorphic \nprojections such that a of = a o/3, then for any projection ~ :X + X (CYx OGy)Ofx =( Ctx OGy)Ofx O(@x \nOF7) Proof We will use the equivalent statement of safet y. (axo G~)ofx = axofxo F~ {naturality} C (~xopx)oFY \n{assumption} = fxo(/?Xo F7) as required. 0 The assumption needed in the theorem is a polymorphic fact \nabout the polymorphic func\u00adtion which has implications at every instance. 5.4 Approximate Factorisation \nof Pro\u00adjections The practical consequence of the theorem is to im\u00adprove the efficiency of binding-time \nanalysis. Each function ~ has an abstract version jx associated with it, with the property that f #@ \no f Q f o/3 for any projection ~. By the above theorem it is clear that we can define f#(~x o F7) = (f~/3)x \no G7, If we restrict ourselves to projections which may be factorised in this way then f# will be fast \nto com\u00adpute. In general there are far fewer polymorphic projections than monomorphic, For example, over \nthe Pair functor there are only four polymorphic projections (ABS x ABS, ID x ABS, ABS x ID, and ID x \nID) but over any particular product do\u00admain we have these and more. Thus, instead of having to find a \nfixed point in some large domain we can do as well by computing it in a far smaller domain. There is \na second advantage, namely that the results of the analysis are not restricted to one particular instance \nbut may be used in all. Sep\u00adarately computing fx for each monomorphic in\u00adstance loses on two accounts-the \nsize of the do\u00admains, and the repeated work. To discover whether the method will be generally applicable, \nhowever, we must ask whether it is suf\u00adficient to consider only those projections that can be factorised \nin this way, Certain restrictions are easy to see. For example, in the case of the List functor, only \nthose projections which treat every element of the list identically may be factorised. Similarly, if \na type variable is ever repeated in a type definition (in the case of List the repetition occurred through \nrecursion), then the projections associated with those parts of the structure will be constrained to \nbe the same. If an exact factorisation is not possible, then an approximate e factorisation may be found. \nIn prac\u00adtice it seems that little information is lost in this way. See [Lau89] for a discussion of the \nbind.ing\u00adtime analysis case.  6 Polymorphic Specialisation, Binding-time analysis is not the only beneficiary \nfrom taking polymorphism into account. The pro\u00ad cess of function specialisation may also be improved \nby using such information. If we have a polymorphic function which we wish to specialise to part of its \nargument, we have two choices. Either all the available information can be used in the specialisation, \nor only the parts of the information over which the function is not poly\u00admorphic. So long as efficiency \nis not lost the latter is clearly better. The residual function will be more general than the in the \nformer case, and will retain a polymorphic type. Consequently, we will need to produce fewer residual \nfunctions, and each may be used in many situations. The residual fimctions will be at least as polymorphic \nas the source func\u00ad tion because no inst ante information is supplied. Is efficiency lost? To answer \nthis we must con\u00adsider what might happen to polymorphic values within the body of a polymorphic function. \nThere are two possibilities. Either the values appear in the result of the function, possibly as part \nof a dat a structure, or, alternatively, they are provided in an argument to another function. In this \ncase the type rules will guarantee that this other fimction must itself be polymorphic. In neither case, \ntherefore, can any significant computation take place. The apparent circularity of this argument may \nbe re\u00admoved by noticing that the polymorphic primitives can themselves do no processing on the polymor\u00adphic \nparts of their arguments (e.g. fst). Again, this is an appeal to the basic intuition about poly\u00admorphic \nfunctions. We conclude, therefore, that because the source function is (by assumption) parametrically \npolymorphic, the only possible loss of efficiency is that some values will be passed as parameters rather \nthan appearing as in-line con\u00adstants. Any increase in cost is restricted merely to an increase in the \nnumber of parameters. This penalty is expected to be minimal on most imple\u00adment ations. It should be \nre-emphasised that this whole argument depends on the source language being first order with parametric \npolymorphism only. Let us consider an example, that of the st an\u00addard lookup function. It is sometimes \nascribed the type lookup : VnVv.( [( n, v)], n) + v. However, this requires the use of a polymorphic \nequal\u00adity fwction. The behaviour of such a function can easily be altered by coding its arguments in \na non-one-to-one manner. Following the argument above, therefore, this brand of polymorphism is ad hoc \nand not parametric. If we replace the over\u00adloaded equality function with a monomorphic ver\u00adsion, then \nthe actual type of the lookup function is lookup : VU. ( [(lVame, v)], IVame) + v for some fixed type \nName. We consider a case where the values are static but the names are dynamic. When specializing an \ninterpreter we might expect the reverse, of course, but in other contexts the situation we describe could \narise. From the discussion above we recognise that even though the values are actually present we will \ngain nothing by using them in the specialisa\u00adtion. As the value part is polymorphic we treat it as if \nit were dynamic. Suppose we specialise the lookup f~ction to the value ([(x,3 ),(y,4)] ,z) where x, y \nand z are dynamic, The values are indeed st atic they are provided as const ants. Using the approach \noutlined above we obtain the residual function lookup-l (a, b,c, d,e) = if eq-tiame a e then b else if \neq.Name c e then d else fail The original function call is then replaced by a call lookup.1 (x,3, y,4 \n,z). The same residual func\u00adtion lookup_l is suitable for any two-list. Contrast this with the situation \nthat would have arisen if the values were used in the specialisation. Then the residual function would \nhave been lookup_l (a, b,c) = if eq_Name a e then 3 else if eq-Name c e then 4 else fail Granted that \nthere are two fewer parameters, but this residual version of lookup is only suitable for this particular \nassociation list. Any other list, even if it had two elements, would require a new residual function \nto be produced. 6.1 Consequences for BTA If a polymorphic function is only ever to receive the non-polymorphic \nparts of its argument dur\u00ading specialisation, then its static projection will have A.BS in the polymorphic \npositions. Because ABS is polymorphic, this means that the projec\u00adtion associated with a polymorphic \nfunction is it\u00adself polymorphic, Therefore, we only need to con\u00adsider a finite domain of polymorphic \nprojections when calculating the pro jection associated with a polymorphic function. There are, of course, \nfewer of these than projections over axbitrary instance types. This means the search space is smaller \ngiv\u00ading an additional benefit for binding-time analysis.  7 Conclusion There are two immediate benefits \nthat accrue from the equivalence between strictness analysis and binding-time analysis. The first is \nthat theoreti\u00adcal considerations explored for one are likely to be directly applicable to the other. \nSecondly, practi\u00adcal techniques developed for one are likely to be use able for the other, perhaps even \nto the point of direct code-reuse. There is an additional benefit for program analysis in general: here \nare two anal\u00adyses, equivalent except that one is forwards and the other backwards. A close comparison \nof the equations in each ought to lead to a deeper under\u00adstanding of the relationship between forwards \nand backwards analyses in general. 7.1 Acknowledgements Most of this work was performed during my Ph.D. \nstudy at Glasgow University while being funded by an SERC studentship, The remainder was with the financial \nsupport of the Semantique ESPRIT project. I owe my thanks to John Hughes who was my supervisor throughout \nmy Ph.D. study and provided me with many hours of useful and exciting discussions, My thanks to Torben \nMogensen for our discussion at DIKU, Copenhagen, during which we which first realised the possibility \nof there being a link between strictness analysis and binding-time analysis, and to Graham Hutton for \nhis suggestions for improvements to the paper.  Fteferences [AH87] S. Abramsky and C. Hankin eds. Abstract \nInterpretation of Declarative Languages. Ellis Horwood, Chichester, England, 1987. [BEJ88] D. Bj@rner, \nA. Ershov and N.D. Jones eds, partial Evaluation and Mixed Conaputa\u00adtion. Proc. IFIP TC2 Workshop, Gammel \nAv\u00adernaes, Denmark, October 1987. North-Holland, 1988. [BFSS87] E.S. Bainbridge, P.J. Freyd, A. Sce\u00addrov, \nand P.J. Scott. Functorial Polymorphism. In Logical Foundations of Functional Program\u00adming, Austin, Texas, \n1987, editor G. Huet. Addison-Wesley, 1989. [BJMS88] A, Bondorf, N.D. Jones, T. Mogensen and P. Sest \noft, Binding Time Analysis and the Taming of Self-Application, Tech. Report, DIKU, Copenhagen, 1988. \n[Burn90] G, Burn, A Relationship Between Ab\u00adstract Interpretation and Projection Analysis, POPL 90, San \nFrancisco, 1990. [FGSS88] P.J. Freyd, J.Y. Girard, A, Scedrov, and P,J. Scott, Semantic Parametricity \nin Poly\u00admorphic Lambda Calculus. In 3 d Annual 5 ym\u00adposium on Logic in Computer Science, Edin\u00adburgh, \nScotland, 1988. [Hug88] R.J.M. Hughes. Backwards Anaiysis of Functional Programs. In [BEJ88], pp 187-208, \n1988. [Hug89] R.J.M. Hughes. Projections for Polymor\u00adphic Strictness Analysis. [JPS85] N. Jones, P. \nSestoft and H. S@ndergaard. An E~periment in Partial Evaluation: The Generation of a C ompiler Generator. \nIII Rewriting Techniques and Applications, editor J.-P. Jouannaud, LNCS 202, pp 124-140, 1985. [JPS88] \nN. Jones, P. Sestoft and H. S@ndergaard. Mix: A Self-Applicable Partial Evaluator for Experiments in \nC ompiler Generation. Lisp and Symbolic Computation, 2, pp 9-50, 1989. [Jon88] N.D. Jones. Automatic \nProgram SpeciaL ization: A Re-Examination from Basic Prin\u00adciples. In [BEJ88], pp 225-282, 1988. [LW-188] \nJ. Launchbury. Projections for Specialisa\u00adtion. In [BEJ88], pp 299-315, 1988, [Lau89] J. Launchbury. \nProjection Factorisations in Partial Evaluation. Ph.D. Thesis, Glasgow University, 1989; Distinguished \nDissertations in Computer Science, Vol 1, C$U,P. 1991, [Mog88] T. Mogensen. Partially Static Structures \nin a Self-Applicable Partial Evaluator. In [BEJ88], pp 325-347,1988. [Mog89] T. Mogensen. Binding Time \nAspects of Partial Evaluation. Ph.D. Thesis, DIKU, Uni\u00adversity of Copenhagen, 1989. [Rey74] J.C. Reynolds. \nTowards a Theory of Type Structure. Proc. C olloque sur la Programma\u00adtion, B. Robinet cd., LNCS 19, 1974. \n[Rom88] S. Romanenko. A Compiler Generator Produced by a Self-Applicable Specialize Can Have a Surprisingly \nNatural and Understand\u00adable Structure. In [BEJ88], pp 445-463, 1988. [Ses86] P. Sestoft. The Structure \nof a Self-Applicable Partial Evaluator. In Programs as Data Objects, H. Ganzinger and N. Jones eds., \nLNCS 217, pp 236-256, 1986. [SG89] D. Scott and C, Gunter. Semantic Do\u00admains. Draft, in Handbook of Theoretical \nC om\u00adputer Science, North Holland, to appear, [Tur86] V.F. Turchin. The C oncept of a Supercom\u00adpder. \nACM TOPLAS, Vol. 8, No. 3, pp 292-325, 1986. [WH87] P. Wadler and R.J.M. Hughes. Projections for Strictness \nAnalysis, FPCA 87, Portland, Oregan, 1987. [Wad89] P. Wadler. Theorems for Free! FPCA 89, Imperial College, \nLondon, 1989. [WB89] P. Wadler and S. Blott. How To iWake Ad-hoc Polymorphism Less Ad-hoc, POPL 89, Austin, \n1989.  \n\t\t\t", "proc_id": "113445", "abstract": "", "authors": [{"name": "John Launchbury", "author_profile_id": "81100462557", "affiliation": "University of Glasgow", "person_id": "PP39043890", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/113445.113453", "year": "1991", "article_id": "113453", "conference": "PLDI", "title": "Strictness and binding-time analyses: two for the price of one", "url": "http://dl.acm.org/citation.cfm?id=113453"}