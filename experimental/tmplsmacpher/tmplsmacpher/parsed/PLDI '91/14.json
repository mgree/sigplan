{"article_publication_date": "05-01-1991", "fulltext": "\n Effective Static-graph Reorganization to Improve Locality in Garbage-Collected Systems Paul R. Wilson, \nMichael S. Lam, and Thomas G. Moher Electrical Engineering and Computer Science Dept. University of Illinois \nat Chicago Box 4348 (m/c 154) Chicago, Illinois, 60680 wilsoni@bert.eecs. uic.edu ABSTRACT Several \nresearchers have attempted to improve locality in garbage-collected heaps by changing the traversal algorithm \nused by a copying garbage collector. Unfortunately, these studies met with small success. We hypothesized \nthat the disappointing results of these previous studies were due to two flaws in the traversal algorithms \ntested. They failed to group data structures in a manner reflecting their hierarchical organization, \nand more importantly, they ignored the dkastrous grouping effects caused by reaching data structures \nfrom a linear traversal of hash tables (i.e., in pseudo-random order). To test this hypothesis, we modified \nthe garbage collector of a Lisp system (specifically, the Scheme-48 system) to avoid both problems in \nreorganizing the system heap image. We implemented our hierarchical decomposition algorithm (a cousin \nof Moon s approximately depth-fiist algorithm) that is quite efficient on stock hardware. We aleo changed \nthe collector to traverse global variable bindings in the order of their creation rather than in the \nmemory order imposed by hash tables. The effects of these changes confirm our hypothesis. Some improvement \ncomes from the basic traversal algorithm, and a greater effect results from the special treatment of \nhash tables. Initial page faults are reduced significantly and repeated page faults are reduced tremendously \n(by roughly an order of magnitude). In addition, improved measures of static locality (such as the percentage \nof on-page pointers) indicate that heap data can be cheaply and effectively compressed, and this may \nallow more effective paging and prefetching strategies; we suggest a level of compressed in-RAM storage \n, with price and performance between those of RAM and disk. These results call for a reinterpretation \nof previous studies, and for further studies of static-graph traversal techniques. They reduce the attractiveness \nof the exotic hardware needed for dynamically reorganizing objects within pages, and show that good locality \ncan easily be achieved by straightforward and efficient techniques on stock hardware. Permission to copy \nwithout fee all or part of this material is granted provided that the copies ere not mede or distributed \nfor direct commercial advantage, the ACM copyright notice and the title of the publication and its date \nappear, and notice is given that copying is by permission of the Association for Computing Machinery. \nTo copy otherwise, or to republish, requires a fee and/or specific permission. @ 1991 ACM 0-89791-428-7/91/0005/01 \n77...$1.50 Proceedings of the ACM SIGPLAN 91 Conference on Programming Language Design and Implementation. \nToronto, Ontario, Canada, June 26-28, 1991. I. Introduction Heap-baaed systems such as SmaHtalk and \nLkp systems typically use copying garbage collectors that reorganize data structures in memory. The resulting \ndata organization often has poor locality, even when all of the objects are live and remain live. As \nin previous copy-collector loce.lhy studies, our focus is on the system image, or saved world , that \nis, the relatively persistent heap data that make up applications, development tools, and the systems \nthat support them. In our Scheme system, as in most Lisp and Smalltalk systems, this includes the compiler, \nuser interaction facilities, application codq and related data structures. Basic issues of space-time \ntradeoffs in memory usage, especially the pollution of memory with garbage objects, have been addressed \nin [L~e83, Unga84, Shaw88, Appe87, WiMo89, Zrrm89,Wils90a]; we do not deal with tttem here. For this \nstudy, we restrict ourselves to locali~ within live objects retained by the garbage collector; we also \navoid issues of locality within objects created by executing programs, because of the tremendous number \nof variables involved problems with the representativeness of the programs chosem the many variations \nin garbage collector strategies, and the many subtle interactions between them, (Locality characteristics \nof data outside the system image are qualitatively different, such as the fast cycling through a fried \namount of memory in the yotmgeat generation of typical generational systems [Wils90], or the highly program-dependent \npatterna of access to intermediate-lifetime data created during the execution of a program. Future studies \nwill address these important issues, but they are complex beyond the scope of this paper.l ) 1 A few \npoints are perhaps worth noting. Because of the frequent cyclic reuse of memory in the youngest generation, \nthat generation tends to remain in main memory at all times, so fhat locality within it is irrelevant \nand its cost is effectively fixed, at least from the point of view of virtual memory. The stmy at the \nlevel of cache is quite different, however, end cache-level locality is dominufed by references to young \nobjects, particularly during allocation [PeSo89, KoLe90]. Oar prclimirtaty studies of cache-level locality \nindicate that large caches may be very effective if they too can hold the youngest generation, plus anything \nelse referenced at the same time scale. Because of the cyclic reuse of a relatively large amount of memory, \nhowever, direct-mapped caches such as those simulated by Zom [Zom89] tend to be defeated very wide (youngest \ngeneration-sized) bands of serious interference are spaced at The system image itself is an important \naspect of the total system. For many users of large usp systems, for example, most of the RAM they purchase \nis merely to support the huge system images they run, which typically include at least one large compiler, \na browser, a debugger, perhaps an edhor or two, and a variety of nontrivial utilities, sometimes including \ncode analyzers and performance-tuning tools, as well as version management and typical general-purpose \nlibrary \u00adlike code. The locality of reference to large system images is often quite poor, (For example, \nPat Caudill reports [CaWB86] that for Tektronix Smalltalk, good performance is achieved when most of \nthe heap image is in RAM. This is a far cry from the oft-touted goal of virtual memory systems, to provide \nnearly the performance of RAM at nearly the cost of magnetic disk. We should note that this phenomenon \nis not restricted to Tektronix Smalltalk, which we believe to have one of the best garbage collectors \naround [CaWB86].) Another, subtler problem can arise if page miss rates increase extremely rapidly at \nthe knee of the curve. This means that a few programs, or phases of programs, that are a little too large \nfor the available memory may thrash unacceptably, rather than paging gracefully; these segments of execution \ncan have a large impact on the average performance of the system. This can lead to a memory maximization \npolicy of buying enough memory to accommodate the worst-behaved programs, which is a hidden cause of \npoor memory utilization. II. Previous work Previously, several Sma.lltalk researchers have attempted \nto reorganize the standard Smalltalk system image using depth-first traversals, rather than the usual \nbreadth-first traversal [Stam84, Blau83, WiWH87]. Their results indicate that depth-first organization \nresults in better locality than breadth-firs~ but the difference is disappointingly small, and sometimes \nnegligible, or even worse. In the Lisp world, the most relevant work we know of is that done at Symbolics, \nInc., in organizing their Lisp Machines system images. A large number of complex techniques were used, \nwith apparently quite good overall results. This work was reported in a much-overlooked Masters thesis \nby David L. Andre [Andr86], but apparently never found conference or journal publication. Part of this \nis due to the informality and lack of controls in experimentation, which is quite natural in commercial \nproduct development. We found two compelling facts in Andre s thesis. Symbolics disabled the normal garbage \ncollection of compiled code, fiiding the original creation order of code much superior in terms of localhy \nto any other ordering they tried.2 Further, they achieved improvements in locality by (cache-sized) intewals \nthrough memory. Modestly set\u00adassociative caches work much better, with miss rates as much as a factor \nof four lower NiLM90]. 2 That is, the textnal definition or&#38;fi it is not the order of the latest \ndefinition, for redefined procedures. other variations on this basic ordering were used, such as grouping \nprocedures with a single caller together with that cat.ter. This apparently had little effect on locality, \nhowever, because the original definition ordering tended to group things satisfactorily. procedure cell \nrelocation, moving the binding cells of procedure variables out of hash tables and into the compiled \ncode objects themselves using forwarding pointer techniques. (This avoids touching different pages during \nnormal procedure calls, in order to fetch the procedure object pointer and then the procedure itself.)3 \nMore recently, Courts and Johnson have published results for a dynamically-reorganizing garbage collector \nused on the Texas Instruments Explorer, a Lisp machine [Cour88, John91]. The Explorer s Temporal Garbage \nCollector takes advantage of specialized hardware to provide incremental garbage collection, which often \nhas the effect of moving objects to tospace in the order that the program actually accesses them. This \ncan dramatically increase locality if properly exploited . It should be noted that this is essentially \nan incorporation of Jon L. White s proposed locality-increasing incremental copier technique [Whit80] \ninto a generational garbage collector.4 The MUSHROOM group at the University of Manchester has simulated \na comparable policy for reorganizing objects within pages [WiWH87], exploiting the features of their \nnovel object oriented memory architecture, which directly supports a notion of objects with slots. Their \nsimulations show that the dynamic reorganization improves locality considerably when compared to normal \nbreadth-or depth-first traversals. (We believe their analysis to be somewhat misleading, however, preferring \nto view their technique as reordering tiny (object-sized) pages within small (1 KB) units of disk transfer.) \nUnfortunately, the dynamic reorganization techniques used by the Explorer and MUSHROOM are prohibitively \nexpensive on stock hardware. In the conventional database world, clustering has been done based on profile \ninformation that tells which tuples (possibly in different relations) are likely to be accessed together. \nRecently, Chang and Katz have exploited knowledge of higher-level organizational principles in an object-oriented \ndatabase system [ChKa9 1]. While such techniques are interesting and useful for their respective systems, \nour goal is to provide a simple but effective clustering scheme for a general heap. We could not count \non trace information or on the semantics of particular pointer fielda of particular kinda of structures, \nexcept for known system objects. 111. Our hypothesis Reading Blau &#38; Stamos reports surprised us, \nin that we had also expected depth-first reorganizations to yield much better locality than breadth-first. \nBut a detailed look at how these traversals affect data structures can explain why they may not. The \nsame basic idea has been apptied successfully in a utility for purifying CMU Common Lisp images Rob MacLachkm, \nraonal communication 1991J. r These results are unsurprising in light of the literature on program reorganization \nin more conventional systems; it has been found that preserving the textual ordering from the source \ncode usually yields quite good locality (see, e.g., [Ferr74]). 4 This technique exploits the fact that \nwhen using a fine grained incremental copy collector [Bake7S], the running program can encounter obsolete \nobjects, requiring them to be copied to tospace immediately. Figure 1. Breadth-first reorganization \nof a tree Figure 2. Depth-first reorganization of a tree Figure 1 shows theeffect oftraversing aroughly \ntreelike structure breadth-first, and grouping objects onto pages. (This is the most commonly-used traversal, \nbecause it is simple and cheap using the Cheneyalgorithm [Chen70].) The overall effect is to make horizontal \nslices through the graph of reachable objects. Thw tends to put parents on different pages from their \noffspring, whlchisbad, but does group objects with siblings and progressively more distant cousins, which \nis not so bad. Figttre 2showsthe effect of adepth-first traversal. Note that lower-level subgraphs are \noften grouped entirely on a page. But consider the upper-level nodes, which are likely to be important \nindexing nodes (i.e., they re on many paths from the root to the leaves). The traversal tends to plunge \nheadlong into the graph of objects, grouping high-level objects that are likely to be important with \nother, progressively more distant objects that are much less likely to be important. Note that the top \n(and likely most important) node is grouped with an entire low-level subgraph including several leaves. \nThus any traversal from the root of this tree will bring in this set of low-level nodes. Unless the accesses \nto the tree are very stereotyped and the depth-first traversal has a strong tendency to traverse the \nmost important child firsb this will probably not be the best set of nodes to bring into memory. In addition, \ntraversing the tree to a terminal node against the grain (opposite the prefemed fiist child direction) \ntends to touch art inordinate number of pages full of unwanted subgraphs. Random traversals suffer considerably \nfrom this as well. This suggests that the proper grouping is neifher depth\u00adnor breadth-fiis~ but a hierarchical \ndecomposition of the tree, as shown in Figure 3. The upper nodes of the tree are grouped together on \na page, and the same kind of grouping is appl;ed recursively to each of the subgraphs below that. The \nintuition behind this is to group the upper levels of the tree into something that approximates art index \nnode in a multiway tree such as a B-tree, and likewise for each sub~ee. From the point of view of virtual \nmemory, it is irrelevant that the internal structure of each index node (page) is searched by traversing \non-page pointers, rather than by binary search of an ordered array. Naturally, due to the irregulruities \nof data structures, this grouping cannot be perfect. On the other hand, it is interesting to note that \nmost data structures are more tree-like than not. (Most objects are pointed at by exactly one pointer \n(C!lGr77, Starn84, DeTr90), rather than creating shwed\u00ad structure tangles in the graph, as long as most \ntangles are small, they will end up entirely within a page anyway. And when applied to simple linear \nlists, hierarchical decomposition reduces automatically to list linearization.) This philosophy of exploiting \nthe nearly-hierarchical nature of data structures is intuitively appealing, but one must be careful not \nto (ahnost literally) miss the forest for the trees. Typical system images are notfundamentally structured \nas normally-proportioned trees, at every granularity. They tend to have one or a few extremely wide root \nnodes consisting of many hundreds or thousands of roots, which anchor data structures that are usually \nrelatively small. That is, the global (or package) hash table(s) are large compared to typical page sizes, \nbut the data structures they hold are often much smaller than the page size. Using hash tables to implement \nlarge namespaces as is typical in Lkp and Smalltalk is potentially a source of problems when applying \ntraversal algorithms to reorganize data. We must take into account that slmcture grouping strategies \nmay typically group whole data structures together on a page, but they may group several differenf data \nstructures consecutively on the same page, as shown in Figure 4. The order in which data structures are \nreached (and copied) by the copying traversal is therefore significant. If they are reached in an order \nthat is well-comelated with future access patterns, locality is likely to be good a relatively few pages \nwill hold the swapping set relevant to a program, or to a phase of a program. On the other hand, if unrelafed \ndata stntctttres are grouped on a page, irrelevant data structures will occupy memory whenever a particular \nstructure is touched and its page brought in. This will lead to very poor memory utilization. If we consider \ntypical traversals, and how hash tables have their elements ordered, we see that this disaster is the \nusual case. The locality within a hash table is extremely poor, because that s essentially a hash table \ns job elements are scattered in a deterministic but pseudo-random order through the table. If we scan \nthrough this table in memory order, as the Cheney algorithm typically does, we reach the elements in \nan order that is unlikely to be well-correlated with anything of interest. This tends to group the objects \nimmediately reachable from the hash table (as siblings) in pseudo-random order, and the Cheney algorithm \nproceeds to interleave their descendants in memory. A depth-first algorithm, on the other hand, will \nrepeatedly backtrack into the table and then plunge down through data structures; this groups whole data \nstructures together, but in a pseudo-random consecutive order. The depth-fust traversal has a slight \nadvantage in that it reaches many more objects via normal pointers than from hash tables, because of \nthe exhaustive nature of the sub-traversals; it is more likely to group objects that have some relationship, \nhowever tenuous. We therefore hypothesized that hash tables are the dominant data structure in terms \nof locality effects, and that grouping objects in a more coherent order, correlated with the modular \nstructure of programs, would be far superior. We further hypothesized that more gains in locality would \nresult from grouping these data structures hierarchically; this should be true even after eliminating \nits relative advantage of reaching more things in something other than hash-table order. (Previous experiments \nobviously did not control for this effect, since they made no attempt to separate out the effects of \ntraversing hash tables from those of traversing normal data structures.) A more subtle expectation is \nthat small programs in large namespaces should suffer from hash tables effects more than programs that \ndominate their narnespaces. That is, if most of the contents of a hash table are relevant to a particular \n program, that program s total memory usage should be better on average, a pseudo-randomly selected data \nstructure will be one that is somewhat useful to the progrant. In contras~ a data structure belonging \nto a relatively small program will most likely be grouped with completely unrelated data structures that \nhappen to be in the same rtamespace. (This can also occttr with a single large, feature\u00adladen program, \nmost of whose features typically go unused for extended periods; it will suffer similarly if everything \nis scrambled together.) 180 Figure 3. Hierarchical decomposition of a tree BIG HASH TABLE ,,,,,.,,, \n,,...,,,,, ,.,,, .,, ,,.,,, ,, ,,, ,,, ,,, ,., ,, , , ,,, !ithl ,, Figure 4. Grouping several data structures \nper page This expected difference between (comparatively) small and (comparatively) large programs applies \nprimarily to initial page faults, which bring the program into memory for the first time, however. For \na large program with distinct phases, a more modular grouping wiil stiil be of great benefit in terms \nof repeated page faults because it wiil aiiow the swapping set for a particular phase to reside in memory, \nand then be quickly displaced by the swapping set for the next phase. If the objects relevant to the \ndifferent phases are interleaved pseudo-randomly, however, locaiity wiii suffer considerably. IV. Algorithms \nused For our experiments we chose to use the cheapest and most common traversal aigorithrn, the Cheney \nbreadth-fiist traversai, and compare it to our own hierarchical decomposition algorithm, which is comparable \nin cost. We augmented dtis with an initiai traversai of globai variables in defiition order, at the beginning \nof garbage collection. We will describe the algorithms in a fair amount of detail, even the weil-known \nCheney aigorithrn, because a clear understanding of it is neeessary for understanding our algorithm. \nIV.1 The Cheney Breadth-first Algorithm The Cheney aigorithtn ailows objects to be traversed in breadth-first \norder, copying them as it goes. Like arty breadth-fust traversai, a queue is maintained, and objects \nare removed from the front of the queue as their chiidren, if any, are appended to the rear of the queue. \nWhen ail of the objects have been removed from the queue and none are left in i~ the traversal is complete. \nThe Cheney aigoritinn accomplishes this in a clever way [Chen70]. Since it is copying the objects as \nit goes, it uses the copies it is making as the queue elements for the breadth-first traversal. It therefore \nneeds no extra space to hold the queue of objects; it oniy needs a pair of pointers that show which pmt \nof tospace (the area being copied to) holds objects that have been copied (i.e. added to the queue), \nbut which have not been scanned and had their chikiren copied, Figure 5 shows the basic operation of \nthe Cheney traversal. The objects immediately reachable from the fixed roots (registers, stack, global \nvariables) are copied into tospace from frornspace (the obsolete area being evacuated) and constitute \nthe initiai contents of the queue . The scan pointer is set to point at the beginning of the first of \nthese objects; this is the start of the queue, and the @e pointer is set to point at the end of the last \none. Throughout the subsequent operation of the aigorithm, these pointers are interpreted as follows: \nthe scan pointer separates the data that have been scanned for offspring (removed from the queue) from \nthose that have not (i.e., those that are still in the queue ). The free pointer marks the place where \nthe next offspring we reach wiil be copied (i.e., marks the free space at the end of the queue.) The \nactuai traversal consists of advancing the scan pointer through subsequent locations, examining ail pointers \nencountered. When a pointer into fromspace is found, its referent is copied to tospace and the pointer \nis updated to point to it. (If the referent has already been reached by some other pointer and the object \nhas therefore been tm.nsported to tospace, the scanned pointer is simply updated to point at the tospace \ncopy of the object. This is done by marking the old version of the transported object with a forwarding \npointer indicating its new location. This avoids making multiple copies of objects reached via multiple \npointers, and preserves the topology of data structures.) When a referent is copied to tospace, the free \npointer is correspondingly advanced. This scanning process proceeds through all the objects in tospace, \nscanning them and transporting their offspring to tospace, Eventttaily, aii of the children have been \ntransported and (later) scanned, and the scan pointer catches up with the free pointer. This indicates \nthat no more untransported offspring exist, and the traversai is finished. IV.2 Hierarchical Decomposition \nOur hierarchical decomposition aigorithm is based on the Cheney algoriti it is simply a two-level version \nwith a large queue of smailer queues. The smaii queues represent locai breadth-first traversals that \nattempt to fill a page with the descendants of some particular object. The large queue links these smaii \nqueues together in a large-scaie breadth\u00adfiist traversal of ail iive objects. Figure 6 illustrates a \nsnapshot during the hierarchical decomposition process. The major scan pointer points to the first p;ge \nof tospace that has not been completely scanned. The mujor Pee pointer points to the first incompletely-filled \npage in tospace. Each page of tospace aiso has a (locai) scan queue deknhed by its own minor scan and \nminor free pointers. The basic operation of the aigorithm is to repeatedly scan the fiist unscanned location \n(pointed at by the minor scan pointer) in the fiist incompletely-scanned page in tospace (which is pointed \nat by the major scan pointer); if its referent is an untransported fromspace object, the following two \nactions are taken (1) the object is transported to the end of tospace, and the pointer is updated and \n(2) a local breadth\u00adfirst traversai is done within the page the object is transported to. This traversai \nstops when either the page is fiiled or all of the object s descendants are reached, whichever comes \nfwst. Step 2 is reaily just a normai Cheney traversai within the last page of tospace, and this step \nmay copy many objects. (Most objects are reached during these local traversals). If the page is filled, \nthe traversai stops and the unscanned area is recorded using the minor scan and minor free pointers for \nthat page. If en object reached in step 1 (via the first unscanned pointer in tospace) will not fit within \nthe major free page, one of two things may happen. If it is smaii (less than a few words, or roughly \n0.170 to 0.5% the size of a page), the extra space is simply wasted, and a new minor queue is started \nin the first free tospace page. If it is large, it is ailowed to straddle the page boundary, and a new \nqueue is begun in the page it crosses onto. (As in the simple Cheney aigorithm, if a pointer to art already-transported \nobject is encountered, it k simply updated to reflect the object s new location.) Eventually, every filled-but-incompletely \n-scanned page of tospace becomes the major scan page, and it is scanned for seeds of locai traversals \nto fiil more pages. When aii pages have been completely scanned in this fashion, the global breadth-first \ntraversai is fiished. Tospace I Fromspace roots I, AB 1) i111 1 scan 11) 1 1 4# scan free Al , iii) \nI1 1 4 scan Iv) 1 scan v)I 1 44 scan free Figure5. The CheneyaIgorithm's breadth-first copying traversal \nFor clarity, we show the actual linear ordering within tospace; we omit pointers from unscanned objects \nin tospace into fromspace. At fop is the state just after copying the objects immediately reachable from \nthe roots into tospace; this creates the initial queue of data to be scanned for offspring, shown by \n(i). It is followed by the scanning of the first few cells of tospace (ii-v). 1\u00ad We should note that \nMoon s approximately depth-first algorithm [Moon84] results in very similar groupings. In fact (based \non our analysis above), we believe it is likely to be better than depth-first rather than just approximate \nit. Our algorithm is somewhat different, in that Moon s algorithm re\u00adscans some locations, while our \nminor free and major free pointers allow us to avoid the redundant scanning; this enables us to make \nour algorithm only negligibly slower than the simple Cheney algorithm on stock hardware. (If this is \nnot apparent, consider the fact that it spends most of its time in local Cheney traversals, Other costs \ncan easily be reduced by techniques such as loop unrolling, especially if objects are of bounded size \nbecause of the use of a separate large object area [CaWB86], which is generally a good idea anyway [UnJa88, \nWils90].) Our traversal is also somewhat more regular. Moon s algorithm tends to go through phases of \nmore depth-first global traversal because objects of any size are allowed to straddle a page boundary, \nand the part that crosses onto the next page is always immediately scanned in an attempt to fill the \npage. Rather than doing true depth-first backtracking at the global level, however, Moon s algorithm \ntends to alternate between depth-first plunges and breadth-first traversal. (The local traversals are \nalways breadth-fiist, however.) 7 We also avoid the stack used by a true depth-first traversal, which \nin the worst case may be of size equal to the number of live objects. Our algorithm only requires at \nmost a pair of pointers per page of tospace, plus the Cheney algorithm s two pointers. (Actually, we \ncan optimize away most of these costs. A simple modification to the algorithm lets us eltilnate the per-page \n(minor) free pointers; usually only one page is incompletely-fdled anyway. We could also reduce the minor \nscan pointers to offseta, perhaps just a byte per page if pages are small or a very small amount of redundant \nscanning is acceptable.) IV.3 Special treatment of top-level variables In our system, there are only \ntwo major hash tables, the system environment hash table holding the internal system functions, and the \nuser environment haah table, holding 5 It also resembles the groupings created by a mostly-depth\u00adfirat \ntraversal using a bounded stack, though such a grouping does not respect page boundaries, Bob Courts \nimplemented such an algorithm for the TI Explorer Lisp System, which gave some improvement in locality, \nbefore switching to dynamic reorganization techniques. [Courts, personal communication, 1989.] We also \nhave recenrly learned rhat Moon s atgorithrn was inspired by an (unimplemented) atgorirhm of Knuth s, \nwhich bears art even stronger resemblance to our hierarchical decomposition. [Moon, personal communication \n1991; D. Krmth, unpublished Stanford course notes]. 6 In one simple experiment, about 30% of all scanned \nlocations were scanned an extra time [Moon, personrd communication, 1991]. 7 We (and Muon) believe our \nalgorithm should have an edge here, because the globtd breadth-first traversal tends to reach objects \nby the shortest path more often than a global depth-first strategy, such paths may be more likely to \nbe important paths. Extensive testing with a large variety of programs would be required to verify this, \nhowever. standard user-visible functions. To avoid reaching objects via haah tables, we altered our compiler \nto make a list of the binding cells that hold vrdues of global variables, as the global variables are \ndefined. We then recompiled the system in the usual way. We modified the virtual machme to linearize \nthis list at the beginning of garbage collection, so that it would not be interspersed with other data \nstructures. Since the list itself is linearized and is never referenced by normally-executing programs, \nit has essentially no effect on our locality measurements. We also modified the virtual machme so that \nit can optionally traverse this list and transport its referents before commencing normal garbage collection; \nthis ensures that the objects directly referenced by global variables will always be reached first in \nthis way, rather than through hash tables. A further modification allows the virtual machine to avoid \ntraversing the large hash tables until last. The result of this is that the gwbage collector reaches \nvariables in dte order that the definitions are created. While this may be viewed as a particular kind \nof dynamic information that is being recorded, we view it as reflecting the static organization of program \nsource code. We simply exploit an artifact of compiler technology (the fact that files are read in sequentially \nwhen rebuilding the whole system) to extract the information. A further enhancement to this scheme is \nto group global procedures with the locations that point to them. In traversing the list of global variable \nbindings, each is checked to see if its vahte is a procedure. (h Scheme, the nantespace is unified grocedure \nvariables are not distinguished from other vruiables, except in that they happen to hold a procedure \nobject.) If it is a procedure, the basic components of the procedure are immediately grouped together. \nThis includes the top-level closure object (which provides a particular procedure with a particular captured \nbinding environment in which it executes) the actual code object and its template (literal frame). Since \nall of these are ahnost invariably touched whenever a procedure is called, and procedure-valued variables \nare not usually touched for any other reason, there is little to lose by grouping them together. V. The \ntest programs We chose three programs to test our hypothesis. The largest program we have running is \nour compiler, which consists of several thousand lines of Scheme code. It includes a reader, a macro \nexpansion facility, various source transformations, and a back-end that generates the bytecode objects, \nliteral frames, and the closure objects that bind them in environments. Thk makes up most of the 300KB \ntest image. Two smaller programs were also studied. One is the Boyer benchmark, which we view as the \nrealest of the Gabriel suite of Lisp benchmarks. It is a small rewrite-based theorem prover, executed \nto prove a tautology. The other (somewhat bigger) program computes type system conformance relationships; \nit was written for programming language research [Andrew Black, personal communication 1990]. All of \nthese programs execute tens of millions of virtual machine instructions and allocate megabytes of data \nduring a run. While the latter two programs are nontrivial, they are considerably smaller than the compiler, \nwhich makes up the 185  COMPILER 50000 g m o ~ 40000\u00ad30000\u00ad20000\u00ad ~ ~ ~ breadth-first B-F+ oral. glo \nHD + Old. glob. 10000\u00ad 0 o 20 40 60 80 100 120 KB memory 140 160 180 200 220 1000 T COMPILER (expanded \nscale) 800 600\u00ad400\u00ad ~ ~ ~ braadth-first B-F+ oral. glo HD + OPd. giob. 200\u00ad o o 1 20 i 40 I 60 I 80 \nI I 100 120 KB memory I 140 I 160 I 180 I 200 ( 220 Figure 7. 186 1200000- TYPECC)NFORMANCE g 3 : m \nm = 1oooooo\u00ad800000\u00ad600000\u00ad4ooooo\u00ad ~ ~ ~ breadth-first B-F+ oral. rts. H.D. + oral. rts 200000\u00ad 0 0 1 \n20 40 60 80 100 KS memory 12000 TYPE CONFORMANCE (expanded scale) 10000\u00ad8000\u00ad ~ ~ ~ breadth-first B-F+ord, \nrts. H.D.+ord. rts 6000\u00ad 4000\u00ad 2000\u00ad 0\u00ad0 1 20 I 40 60 80 100 KS memory Figure 8. 187 50000\u00ad $$J = m \na) g 40000\u00ad3oooo -2oooo\u00ad ~ -_@__ ~ breadth-first B-F+ord, glob I-i.i)+ord. 10000 0 0 20 40 60 80 BOYER \n(expanded scale) g j 3 J g w m m CL KB memory 188 bttlkof ottrsystemitnage. llteseprogratn swerecompile \ndin thesystem environmen~ because that s thenormrd way we compile benchmarks. (The compiler currently \nonly performs certain optimizations on code compiled in the system environment, rather than in the user \nenvironment.) According to our understanding of heap locality, the two small programs initial page fault \nrates should be more strongly affected by our reorganization. Since the compiler makes up the bulk of \nour system image, and since most of its code is executed during normal use, bringing any data structure \ninto memory has a good chance of bringing in somethiig useful, even if the data structure is randomly \nselected. And since this code is such a large fraction of the whole image, the potential for wasted memory \nis relatively smaller there just isn t all thar much other code to dilute it. Conversely, the smaller \nprograms run the great risk of having their objects reside on pages mostly taken up by objects belonging \nto the compiler. On the other hand, the compiler s repeated fault rates have much more potential for \nimprovement than its initial fault rates. If memory is too small to hold the whole compiler, it will \nbe very good if the code and data for the reader can be grouped together, and likewise for the macroexpansion \nfacility, the back end, etc. This should be beneficial because the compiler repeatedly cycles through \ndistinct phases when compiling a file. The swapping set for the reader can thus be paged in, and replaced \nby the swapping set for macro expansion, etc., without the code for each phase diluting the code for \nthe others. VI. The experiment We equipped our virtual machine with reference tracirtg facilities and \non-the-fly virtual memory simulation. This allowed us to simulate several different page sizes for all \npotentially relevant sizes of memory, in a single pass. (We used standard LRU replacement simulation \ntechniques, using four simulators for different page sizes, with each simulating all sizes of memory \nsimultaneously.) We then measured locality within the oldest generation (which contains only the system \nimage, including the test programs) for the execution of the three benchmark programs. our locality measures \ninclude total initial and repeated misses, for four page sizes and for all memory sizes up to the total \nusable by each program. We used three different versions of our system image. The first used the simple \nCheney breadth-fust organization, the second ordered global variables chronologically (and grouped global \nprocedures components) before doing a Cheney traversal; and the third combined the special traversal \nof globals with hierarchical decomposition scavenging. VII. Results As expected, initial page faults \nwere reduced significantly, especially for the programs that were small relative to the system image. \nWhile the compiler s initial page faults (total memory usage) were only reduced by a few percen~ those \nfor the smaller and smallest programs were reduced by roughly a third. The repeated page faults were \nreduced dramatically, by roughly an order of magnitude for all programs, for all page sizes from a half \nkilobyte to four kilobytes, and for all reasonable memory sizes. They are also sign~lcantly reduced for \nvery small memories (only a few pages), which indicates that this strategy would improve the effectiveness \nof translation lookaside buffers. (See Figures 7 through 9. The data shown are for 1 ICB pages, but graphs \nfor other page sizes are similar.) It should be noted that even given the reduced total memory requirements \nof the reorganized images, the page miss rates are somewhat less affected by modest decreases in memory \nsize. Where the breadth-fiist curves rise rapidly as memory is reduced (i.e., repeated page faults very \nquickly dominate inhial page faults), the reorganized images are less sensitive, paging more gracefully \nas memory becomes somewhat tight. This should help avoid the need to maximize memory purchases to suit \nthe worst-behaved programs or program phases. (In interpreting the pictures, it must be borne in mind \nthat though the slopes of the curves generally are similar, that represents a more gradual tradeoff when \nthe curves are displaced to the lef~ i.e., the same memory size increment represents a larger percentage \ndecrease for a small memory.) In most cases, much of the locality improvement results from simply traversing \nthe top-level variables in definition order. This indicates that the textual ordering of procedures is \na good one? provi~lng good loc~ity within phases Of program operation. Hierarchical decomposition consistently \nyields further increases in locality, across the range of page sizes. We also measured static locality \ncharacteristics, such as the percentage of pointers that point on-page, and locality within the pages \npointed to by off-page pointers. Our algorithms greatly increase these measures of locality as well. \nIn fact, it turns out that we end up with the majority of pointers pointing on-page, and most of those \npoint to locations that are entirely determined by the local topology of the data structures. Thus a \npagewise compression scheme could omit half of all addresses; most other addresses could be compressed \ndown to a few bits because they point to one of the same few pages as one of the last few off-page pointers, \nor to another object on-page, requiring only enough bits to specify an offset within the page. VII. Conclusions \nand Directions for Future Work We believe these results to be significant because they provide implementors \nwith simple and effective tecltrdques that can improve locality. In fact, most of the benefits can be \nobtained by an ahnost trivial hack. Naturally, the large variations between systems require some adaptation \nof these ideas, but this paper gives a starting point from which to work it identifies some important \nsources of trouble, and gives art example of an approach that works for our system, analogous approaches \nseem likely to work for others. The gains in both static and dynamic locality should be beneficial to \na variety of systems, particularly those that exploit pagewise virtual memory privileges and protections \nto implement checkpointing, address space extension, distributed shared memory, or incremental garbage \ncollection (see, e.g., [Wils90b] and [ApLi91]). On theoretical grounds, tltk work is important because \nit shows that static graph traversals are quite promising, and are worthy of continued study. Dynamic \nreorganization schemes, and the hardware they require, should be re-evaluated in this light. Future studies \nshould determine whether the benefits of adaptive reorganization can be combmed with those of static \ngraph traversals, or whether the main beneftt of adaptive schemes is to repair locality damage that can \nsimply be avoided by our techniques. (We ourselves fiid the idea of dynamic reorganization quite appealing, \nand believe that at least some of the measured gains are real; only better-controlled studies of dynmnic \nreorganization will tell. But perhaps reordering individual objects widtin pages is applying the principle \nat too fiie a granularity. We are looking into dynamically reorganizing small pages within larger units \nof disk transfer. This was tried in the mid-seventies [BaSa76] with disappointing results, but we believe \nthose disappointing results to be an artifact of the details of their experiments. We believe that such \nreorganization is properly viewed as a sophisticated form of adaptive (nonlinear) prefetching, and that \nthis can illuminate issues in the design of effective reorganization policies.9) It will be particularly \ninteresting to attempt to transfer our static-graph traversal techniques to a Smalltalk system. Since \nSmalltalk doesn t have a notion of load order of ftles, the techniques are not directly applicable. We \nbelieve that a similar result can probably be obtained by hierarchically decomposing the class hierarchy \n(following subclass links only) as the initial phase of copy collection. This should yield a grouping \nof code that is roughly similar to our file load order, and which also correlates well with conceptual \nmodule structure. We intend to extend these experiments to encompass locality in data outside the system \nimage, created during program execution. We are especially in programs that deal with large amounts of \ndata in database-like ways. Such programs are of particular interest because their performance is likely \nto depend heavily on locality effects. (Jn fact, they were the inspiration for the hierarchical decomposition \nalgorithm, which is intended to be scalable because the local clustering of objects within pages is not \nstrongly dependent on higher-level data structure.) Some initial experiments with synthetic benchmarks \nindicate that hierarchical decomposition is successful in this respect, but more experiments with real \nprograms are necessary. We would also like to use hierarchical decomposition to allow optimized cluster \ncollection of large memories. If the pointers into a page haven t changed since the last collection, \nand if the page is not dirty, then all of the objects are still live, and the pointers out of the page \nhave not changed either. Such stable pages needn t be traversed 8 me ~o$t fiponmt problem is that the \nrnerrmw tbeY simulated had unrealistically large pages relazive to the memory sizes sirndared. This tends \nto make prefetchirrg less attractive, because tbe marginal cost of reserving a page for prefetclrhg is \nunrealistically high. For much larger memories, as are now ubiquitous, it may work much better. [Baer, \npersonal conununication 1989]. .9 Further support for the notion of adaptive prefetching can be found \nin [HoHu87]. tlds paper discusses prefetching poticies that preserve an inclusion property, allowing \nmany sizes of memory to be simulated at once. An unexpected side effect was that the modifications made \nto preserve inclusion also decrease the number of unsuccessful prefetches. We believe that s because \nit makes the poticy approximate a fool me once strategy useless prefetches are usually not repeated. \nagain. This could be the basis for a hybrid copying/mark\u00adsweep collector that would work well for very \nlarge memories. The high static locality we achieve (i.e., the low information content in pointers) implies \nthat fast algorithms, mirroring the structure of the local breadth-fiist traversals, could compress pages \nof pointer data quickly and effectively. Given the well-known compressibility of integer data, and the \neffectiveness of recent object code compression schemesio, there are several interesting possible applications. \nour favorite is to add a distinct level of compressed in-RAM storage [Wils90] to the memory hierarchy; \nsome RAM would be reserved for compressed heap storage, requiring only decompression (rather than a disk \nseek) to access it. As CPU cycles become progressively cheaper relative to disk seeks, it is increasingly \nattractive to have such an intermediate level of storage, between that of normal RAM and that of disk \nin both price and performance. Acknowledgements The ffist author would Iiie to thank all of the experimenters \nand developers whose systems we ve d=cussed in this paper. They have all been quite helpful in describing \ntheir systems. I m particularly indebted to Bob Courts, who commented that the overall structure of a \nLisp system is more like a forest of scraggly bushes than a single big tree. This comment led me, after \na bit of cogitation, to the conclusion that hash tables are the major culprit in the locality problems \nof garbage-collected heaps. I m also indebted to David Andre, whose M.S. thesis convinced me my hypotheses \nwere worth checking, and especially to David Moon, a primary developer of the Symbolics system. Moon \ndirected me to Andre s thesis, and provided many helpful comments on an earlier draft of this paper. \nJ d also like to thank Rob MacLachlan, Jim Stamos, Pat Caudill, and David Ungar, for informative comments \nand interesting facts. References Andr86 Andre, David L., Paging in Lisp programs, MS Thesis, University \nof Maryland, 1986. Appe87 Appel, A.W., Heap allocation can be faster than stack allocation, Information \nProcessing Letters 25, 1987, pp. 275-279. ApLi91 Appel, Andrew W., and Kai Li, Virtual memory primitives \nfor user programs, Proc, ASPLOS-IV, Santa Clara, California, April 8-11, 1991, BaSa76 Baer, J.L., and \nSager, G.R. Dynamic improvement of Locality in virtual memory systems, IEEE TSWE, vol. SE-2, no.1 (March \n1976). Bake78 Baker, Henry, List processing in real time on a serial computer, CACM 21, 4 (April 1978), \npp. 280\u00ad 294. Blau83 Blau, R. Paging on an object-oriented computer for Smalltslk, Proc. ACM SIGMETRICS \n1983. 10 For example, the Squeere program used by the Acorn RISC machines [L. Smith and R.Wilson, personal \ncommunications 1990], which generally compresses code by a factor of two or more. 190 CaWB86 Caudill, \nP.J., and A.Wirfs-Brock, A third\u00ad generation Smrdltalk implementation, Proc. 00PSLA 86, pp. 119-130. \nChKa91 Chang, Ellis E., and Randy H. Katz, Exploiting inheritance and structure semantics for effective \nclustering and buffering in an object-oriented DBMS, Proc. ASPLOS-IV, Santa Clara, Californi~ April 8-11, \n1991. Chen70 Cheney, C.J., A nonrecursive list compacting algorithm, Communications of the ACM, 13(1 \n1):677 678, November 1970. ClGr77 Clark, D. W., and Green, C.C., An empirical study of list structure \nin Lkp. CACM 20(2), Feb. 1977, pp 78-87. Cour88 Courts, R., Improving locrdity of reference in a garbage-collecting \nmemory management system, CACM 31,9 (Sept. 1988), pp. 1128-1138. DeTr90 DeTreville, John, Heap usage \nin the Topaz environmen~ DEC Systems Research Center report, August 20, 1990. Ferr74 Ferrari, Domenico, \nImproving locality by critical working sets, CACM 17(11), Nov. 1974, pp 614-620. HoHu87 HorsPool, R. \nNigel, and Ronald M. Huberman, Anrdysis and development of demand prepaging policies, Journal of Systems \nand Software, 1987. John91 Johnson, Douglas, The case for a read bimrier, Proc. ASPLOS IV, Santa Clara, \nCalifomi~ April 8-11, 1991. KoLe90 Koopman, P., and P, Lee, Cache performance of combinator graph reduction, \nProc. 1990 Int 1. Conf. on Computer Languages. LiHe83 Lieberman, Henry, and Carl Hewit~ A realt\u00ad ime \ngarbage collector based on the lifetimes of objects, CACM 26,6 (June 1983), pp. 419-429. Moon84 Moon, \nDavid, Garbage collection in a kmge Lisp system, ACM Symp. on Lisp and Functional Programming 1984, pp. \n235-246. PeSo89 Peng, C.-J., and Sohi, G. Cache memory design considerations to support languages with \ndynamic heap allocation. Technical Report 860, Computer Sciences Dept. University of Wkconsin Madison, \nJuly 1989. RoDe90 Robinso~ J., and Devsrakond~ M. Data Cache Management Using Frequency-Based Replacement \nProc. SIGMETRICS 90. Shaw88 Shaw, Robert A., Empirical analysis of a LISP System, PhD thesis, Stanford \nUniversity, February 1988. Smit78 Smith, Alan J. Sequential Program Prefetching in Memory Hierarchies, \nIEEE Computer, December 1978. Soba88 Sobalvamo, Patrick G., A lifetime-based garbage collector for LISP \nsystems on general-purpose computers, B.S. thesis, MIT EECS Dept. 1988. Stant84 Starnos, James W., Static \ngrouping of small objects to enhance performance of a paged virtual memory, ACM Transactions on Programming \nLanguages and Systems 2(2), May 1984, pp. 155-180. Unga84 Ungar, David, Generation Scavenging: a non-disruptive \nhigh-performance storage reclamation algorithm, ACM SIGPLAN Notices, 19,5 (May 1984), pp. 157-167. Whit80 \nWhite, J.L., Address/memory management for a gigantic Lisp environment, or, CC considered harmful. Conf. \nRecord of the 1980 Lisp Conference, pp.119-127. WiWH87 Williams, I.W., Wolczko, M.I., and Hopkins, T,P. \nDynamic grouping in an object-oriented virtual memory hierarchy. Proc. 1987 European Conf. on Object-Oriented \nProgramming. Springer-Verlag 1987. Wils90a Wilson, Paul R. Issues and strategies in heap management and \nmemory hierarchies, 00PSLA/ECOOP 90 Workshop on Garbage Collection in Object-Oriented Systems; also in \nSIGPLAN Notices, March 1991. Wils90b Wilson, Paul R., Pointer swizzling at page fault time efficiently \nsupporting huge address spaces on stock hardware, Technical Report UIC-EECS-90-6, University of Illinois \nat Chicago EECS Department, December 199Q also in Computer Architecture News, April 1991. WiLM90 Wilson, \nPaul R., Michael S. Lam, and Thomas G. Moher, Caching considerations for generational garbage collectiotx \na case for large and set\u00adassociative caches, Technical report UIC-EECS-90-5, University of Illinois at \nChicago EECS Department, December 1990. WiMo89b Wilson, P. R., and Moher,T.M Design of the opportunistic \nGarbage Collector , Proc. 00PSLA 89. Zorn89 Zom, B. Comparative performance evaluation of garbage collection \nalgorithms, Ph.D. Thesis, UC Berkeley EECS Dept., 1989. Zorn90 Zom, B. Comparing mark-and-sweep and stop\u00adand-copy \ngarbage collection, 1990 ACM Conference on Lisp and Functional Progr amming, Nice, France, June 27-29, \n1990, pp. 87-98.  \n\t\t\t", "proc_id": "113445", "abstract": "", "authors": [{"name": "Paul R. Wilson", "author_profile_id": "81100530836", "affiliation": "Electrical Engineering and Computer Science Dept., University of Illinois at Chicago, Box 4348 (m/c 154) Chicago, Illinois", "person_id": "PP39047172", "email_address": "", "orcid_id": ""}, {"name": "Michael S. Lam", "author_profile_id": "81100237954", "affiliation": "Electrical Engineering and Computer Science Dept., University of Illinois at Chicago, Box 4348 (m/c 154) Chicago, Illinois", "person_id": "P197765", "email_address": "", "orcid_id": ""}, {"name": "Thomas G. Moher", "author_profile_id": "81100538646", "affiliation": "Electrical Engineering and Computer Science Dept., University of Illinois at Chicago, Box 4348 (m/c 154) Chicago, Illinois", "person_id": "PP39065257", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/113445.113461", "year": "1991", "article_id": "113461", "conference": "PLDI", "title": "Effective &#8220;static-graph&#8221; reorganization to improve locality in garbage-collected systems", "url": "http://dl.acm.org/citation.cfm?id=113461"}