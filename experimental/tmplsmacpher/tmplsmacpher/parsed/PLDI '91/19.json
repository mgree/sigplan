{"article_publication_date": "05-01-1991", "fulltext": "\n Global Instruction Scheduling for SuperScalar Machines David Bernslein Michael Rodeh IBM Israel ScientKlc \nCenter Technion City Haifa 32000 ISRAEL Abstract To improve the utilization of machine resources in \nsuperscalar processors, the instructions have to be carefully scheduled by the compiler. As internal \nparallelism and pipelining increases, it becomes evident that scheduling should be done beyond the basic \nblock level. A scheme for global (intra-loop) scheduling is proposed, which uses the control and data \ndependence information summarized in a Program Dependence Graph, to move instructions well beyond basic \nblock boundaries. This novel scheduling framework is based on the parametric description of the machine \narchitecture, which spans a range of superscakis and VLIW machines, and exploits speculative execution \nof instructions to further enhance the performance of the general code. We have implemented our algorithms \nin the IBM XL family of compilers and have evaluated them on the IBM RISC System/6000 machines. Permission \nto copy without fee ell or part of this material is granted provided that the copies are not made or \ndistributed for direct commercial advantage, the ACM copyright notice and the title of the publication \nand its date appear, and notice is given that copying is by permission of the Association for Computing \nMachinery. To copy otherwise, or to republish, reauirea a fee end/or aDecific oermiasion. @1991 ACM 0-89791-428-7/91/0005/0241 \n. ..$1.50 Ii Proceedings of the ACM SIGPLAN 91 Conference on Programming Language Design and Implementation. \nToronto, Ontario, Canada, June 26-28, 1991. 1. Introduction Starting in the late seventies, a new approach \nfor building high speed processors emerged which emphasizes streamlining of program instructions; subsequently \nthis direction in computer architecture was called RISC [P85J It turned out that in order to take advantage \nof pipelining so as to improve performance, instructions have to be rearranged, usually at the intermediate \nlanguage or assembly code level. The burden of such transformations, called instruction scheduling, has \nbeen placed on optimizing compilers. Previously, scheduling algorithms at the instruction level were \nsuggested for processors with several functional units [BJR89], pipelined machines [BG89, BRG89, HG83, \nGM86, W90] and Very Large Instruction Word (VLIW) machines [EIEJ While for machines with n functional \nunits the idea is to be able to execute as many as n instructions each cycle, for pipelined machines \nthe goal is to issue a new instruction every cycle, effectively eliminating the so-called NOPS (No Operations). \nHowever, for both types of machines, the common feature required from the compiler is to discover in \nthe code instructions that are data independent, allowing the generation of code that better utilizes \nthe machine resources. It was a common view that such data independent instructions can be found within \nbasic blocks, and there is no need to move instructions beyond basic block boundaries. Virtually, all \nof the previous work on the implementation of instruction scheduling for pipelined machines concentrated \non scheduling within basic blocks [HG83, GM86, W90]. Even for basic RISC architectures such restricted \ntype of scheduling may result in code with many NOPS for certain Unixl -type programs that include many \nsmall basic blocks terminated by unpredictable branches. On the other hand, for scientific programs the \nproblem is not so severe, since there, basic blocks tend to be larger. Recently, a new type of architecture \nis evolving that extends RISC by the ability to issue more than one instruction per cycle [G089]. This \ntype of high speed processors, called superscalar or superpipelined architecture, poses more serious \nchallenges to compilers, since instruction scheduling at the basic block level is in many cases not sufficient \nto allow generation of code that utilizes machine resources to a desired extent [JW89]. One recent effort \nto pursue instruction scheduling for superscalar machines was reported in [GR90], where code ,replication \ntechniques for scheduling beyond the scope of basic blocks were investigated, resulting in fair improvements \nof running time of the compiled code, Also, one can view a superscalar processor as a VLIW machine with \na small number of resources. There are two main approaches for compiling code for the VLIW machines that \nwere reported in the literature: the trace scheduling [F81, E851 and the enhanced percolation scheduling \n[EN89]. In this paper, we present a technique for global instruction scheduling which permits the movement \nof instructions well beyond basic blocks boundaries within the scope of the enclosed loop. The method \nemploys a novel data structure, called the Program Dependence Graph (PDG), that was recently proposed \nby Ferrante et. al [FOW87] to be used in compilers to expose parallelism for the purposes of vectorization \nand generation of code for multiprocessors. We suggest combining the PDG with the parametric description \nof a family of superscalar machines, thereby providing a powerful framework for global instruction scheduling \nby optimizing compilers. While trace scheduling assumes the existence of a main trace in the program \n(which is likely in scientific computations, but may not be true in symbolic or Udx-t ype programs), \nglobal scheduling (as well as enhanced percolation scheduling) does not depend on such assumption. However, \nglobal scheduling is capable of taking advantage of the branch probabilities, whenever available (e.g. \ncomputed by proffig). As for the enhanced percolation scheduling, our opinion is that it is more targeted \ntowards a machine with a large number of computational units, like VLIW machines. Using the information \navailable in a PDG, we distinguish between useful and speculative execution of instructions. Also, we \nidentify the cases where instructions have to be duplicated in order to be scheduled. Since we are currently \ninterested in machines with a small number of functional units (like the RISC System/6000 machines), \nwe established a conservative approach to instruction scheduling. First we try to exploit the machine \nresources with useful instructions, next we consider speculative instructions, whose effect on performance \ndepends on the probability ~f branches to be taken, and scheduling with duplication, which might increase \nthe code size incurring additional Unix is a trademark of AT&#38;T Bell Labs costs in terms of instruction \ncache misses. Also, we do not overlap the execution of instructions that belong to different iterations \nof the loop. This more aggressive type of instruction scheduling, which is often called sofware pipelining \n[J-X8], is left for future work. For speculative instructions, previously-it was suggested that they \nhave to be supported by the machine architecture [ESS, SLH90]. Since architectural support for speculative \nexecution carries a si~lcant run-time overhead, we are evaluating techniques for replacing such support \nwith compile-time analysis of the code, still retaining most of the performance effect promised by speculative \nexecution. We have implemented our scheme in the context of the IBM XL family of compilers for the IBM \nRISC System/6000 (RS/6K for short) computers. The preliminary performance results for our scheduling \nprototype were based on a set of SPEC benchmarks [ss9]. The rest of the paper is organized as follows. \nIn Section 2 we describe our generic machine model and show how it is applicable to the RS/6K machines. \nThen, in Section 3 we bring a small program that will serve as a running example. In Section 4 we discuss \nthe usefulness of the PDG, while in Section 5 several levels of scheduling, including speculative execution, \nare presented. Finally, in Section 6 we bring some performance results and conclude in Section 7. 2. \nParametric machine description Our model of a superscalar machine is based on the description of a typical \nRISC processor whose only instructions that reference memory are load and store instructions, while all \nthe computations are done in registers. We view a superscalar machine as a collection of functional units \nof m types, where the machine has nl, nz, ....n~ units of each type. Each instruction in the code can \nbe potentially executed by any of the units of a speci.tied type. For the instruction scheduling purposes, \nwe assume that there is an unbounded number of symbolic registers in the machine. Subsequently, during \nthe register allocation phase of the compiler, the symbolic registers are mapped onto the real machine \nregisters, using one of the standard (coloring) algorithms. Throughout this paper we will not deal with \nregister allocation at all. For the discussion on the relationships between instruction scheduling and \nregister allocation see [BEH89]. A program instruction requires an integral number of machine cycles \nto be executed by one of the functional units of its type. Also, there are pipelined constraints imposed \non the execution of instructions which are modelled by the integer delays assigned to the data dependence \nedges of the computational graph. Let 11 and L be two instructions such that the edge (11,12) is a data \ndependence edge. Let I (t > 1) be the execution time of 11 and d (d z O) be the delay assigned to (11,14. \nFor performance purposes, if 11is scheduled to start at time k, then L should be scheduled to start no \nearlier than k + t+ d. Notice, however, that if Zzis scheduled (by the compiler) to start earlier than \nmentioned above, this would not affect the correctness of the program, since we assume that the machine \nimplements hardware interlocks to guarantee the delays at run time. More info~ation about the notion \nof delays due to pipelined constraints can be found in [BG8!J, BRG89]. 2.1 The RS/6K model Here we show \nhow our generic model of a superscalar machine is cotilgured to fit the RS/6K machine. The RS/6K processor \nis modelled as follows: m = 3, there are three types of functional units: fixed point, floating point \nand branch types. ni= 1, nz= l,n3= l,there isa single fixed point unit, a single floating point unit \nand a single branch unit.  Most of the instructions are executed in one cycle, however, there are also \nmulti-cycle instructions, like multiplication, division, etc.  s There are four main types of delays: \n a delay of one cycle between a load instruction and the instruction that uses its result register (delayed \nZoad);  a delay of three cycles between a fixed point compare instruction and the branch instruction \nthat uses the result of that compare2 ;  a delay of one cycle between a floating point instruction and \nthe instruction that uses its result;  a delay of five cycles between a floating point compare instruction \nand the branch instruction that uses the result of that compare. There are a few additional delays in \nthe machine whose effect is secondary. In this paper we concentrate on fixed point computations only. \nTherefore, only the first and the second types of the above mentioned delays will be considered.  3. \nA program example Next, we present a small program (written in C) that computes the minimum and the maximum \nof an array. This program is shown in Figure 1 and will serve us as a running example. In this program, \nconcentrating on the loop which is marked in Figure 1, we notice that two elements of the array a are \nfetched every iteration of the loop. Next, these elements of a are compared one to another (zfiu > v)) \n, and subsequently they are compared to the max and mi n variables, updating the maximum and the minimum, \nif needed. The RS/6K pseudo-code for the loop, that corresponds to the real code created by the IBM XL-C \ncompiler3 , is presented in Figure 2. For convenience, we number the instructions in the code of Figure \n2 (I 1-120) and annotate them with the corresponding statements of the program of Figure 1. Also, we \nmark the ten basic blocks (BL1-BL1O) of which the code of Figure 2 comprises for the purposes of future \ndiscussion. For simplicity of notation, the registers mentioned in the code are real. However, as was \nmentioned in Section 2, we prefer to invoke the global scheduling algorithm before the register allocation \nis done (at this stage there is an unbounded number of registers in the code), even though conceptually \nthere is no problem to activate the instruction scheduling after the register allocation is completed. \n More precisely, usually the three cycle delay between a fixed point compare and the respective branch \ninstruction is encountered only when the branch is taken. However, here for simplicity we assume that \nsuch delay exists whether the branch is taken or not. The only feature of the machine that was disabled \nin this example is that of keeping the iteration variable of the loop in a special counter register. \nKeeping the iteration variable in this register allows it to be decremented and tested for zero in a \nsingle instruction, effectively reducing the overhead for loop control instructions. ~ find the largest \nand the smal lest number in a given array / minmax(a,n) { int i,u,v,min,max,n,a[SIZE]; min=a[O]; max=min; \ni=l; /****************** LOOP STARTS ************* / while (i <n) { u=a[i]; v=a[i+l]; if (u>v) { if (u>max) \nmax=u; if (v<min) min=v; } else { if (v>max) max=v; if (u<min) min=u; } j= i+p. }  p*************** \n/ Loop ENDS *************** printf( min=%d max=%d\\n ,min,max); } Figure 1. A program computing the minimum \nand the maximum of an array Every instruction in the code ofFigure 2, except for branches, requires one \ncycle inthetixed point unit, while the branches take one cycle in the branch unit. There is a one cycle \ndelay between instruction 12and13, dueto the delayed load feature of the RS/6K. Notice the special form \nof a load with update instruction in 12: in addition to assigning to rO the value of the memory locational \naddress (r31) + 8, it also increments r31 by 8 (post-increment). Also, there is a three cycle delay between \neach compare instruction and the corresponding branch instruction. Taking into consideration that the \nfixed point unit and the branch unit run in parallel, we estimate that the code executes in 20, 21 or \n22 cycles, depending on if O, 1 or 2 updates of max and mi n variables (LR instructions) are done, respectively. \nmax is kept in r30 min is kept in r28 i is kept in r29 n is kept in r27 address of a[i] is kept in r31 \n. . . more instructions here . . . *************** LfjOfJ STARTS ******************* CL.0: (11) L r12=a(r31,4) \n1oad u (12) LU rO, r31=a(r31,8) load v and increment index (13) C cr7=r12, r0 U>v (14) BF CL.4, cr7,0x2/gt \n---------------------------------------END BL1 (15) C cr6=r12, r30 u > max (16) BF CL.6,cr6,0x2/gt \n---------------------------------------END BL2 (17) LR r30=r12 max = u ---------------------------------------END \nBL3 CL.6: (18) c cr7=r0,r28 v < min (19) BF CL.9,cr7,0xl/lt ---------------------------------------END \nBL4  (110) LR r28=r0 min = v (Ill) B CL.9 --------------------------------------- END BL5 CL.4: (112) \nC cr6=r0,r30 v > max (113) BF CL.ll,cr6,0x2/gt ---------------------------------------END BL6 (114) LR \nr30=r0 max = v ~--------------------------------------END BL7 CL. 11: (115) C cr7=r12,r28 u < min (116) \nBF CL.9,cr7,0xl/lt ---------------------------------------END BL8 (117) LR r28=r12 min = u ---------------------------------------END \nBL9 CL.9: (118) AI r29=r29,2 i =i+2 (119) C cr4=r29,r27 i<n (120) BT CL.0,cr4,0xl/lt ---------------------------------------END \nBL1O *************** LOfjp ENDS ********************** . . . more instructions here . . . Figure 2. The \nRS/6Kpseudo-code forthe program of Figure 1 4. The Program Dependence Graph The program dependence graph \nis a convenient way to summarize both the control dependence and data dependence among the code instructions, \nWhile the concept of data dependence, that carries the basic idea of one instruction computing a data \nvalue and another instruction using this value, was employed in compilers a long time ago, the notion \nof control dependence was introduced quite recently [FOW87]. In what follows we discuss the notions of \ncontrol and data dependence separately. 4.1. Control dependence We describe the idea of control dependence \nusing the program example of Figure 1. In Figure 3 the control flow graph of the loop of Figure 2 is \ndescribed, where each node corresponds to a single basic block in the loop. The numbers inside the circles \ndenote the indices of the ten basic blocks BL1-BL1O. We augment the graph of Figure 3 with unique ENTRY \nand EXIT nodes for convenience. Throughout this discussion we assume a single entry node in the control \nflow graph, i.e., there is a single node (in our case BL1) which is connected to ENTRY. However several \nexit nodes that have the edges leading to EXIT may exist. In our case BL1O is a (single) exit node. For \nthe strongIy connected regions (that represent loops in this context), the assumption of a control flow \ngraph having a single entry corresponds to the assumption that the control flow graph is reducible. The \nmeaning of an edge from a node A to a node B in a control flow graph is that the control of the program \nmay flow from the basic block A to the basic block B. (UsuaUy, edges are annotated with the conditions \nthat control the flow of the program from one basic block to another.) From the graph of Figure 3 however, \nit is not apparent which basic block will be executed under which condition. The control subgraph of \nthe PDG (CSPDG) of the loop of Figure 2 is shown in Figure 4. As in Figure 3, each node of the graph \ncorresponds to a basic ENTRY Figure 3. The control flow graph of the loop of Figure 2 block of the program. \nHere, a solid edge from a node A to a node B has the following meaning: 1. there is a condition COND \nin the end of A that is evaluated to either TRUE or FALSE, and 2. if COND is evaluated to TRUE, B will \ndefinitely be executed, othenvise B will not be executed.   The control dependence edges are annotated \nwith the corresponding conditions as for the control flow graph. In Figure 4 solid edges designate control \ndependence edges, while dashed edges will be discussed below. For example, in Figure 4 the edges emanating \nfrom BL 1 indicate that BL2 and BL4 will be executed if the condition at the end of BLl will be evaluated \nto TRUE, while BL6 and BL8 will be executed while the same condition is FALSE. 1 _  --@ T F TF 2 --+4 \n6 -\u00ad 8 T T T T 3 79 *O Figure 4. The forward control subgraph of the PDG of the loop of Figure 2 As \nwas mentioned in the introduction, currently we schedule instructions within a single iteration of a \nloop. So, for the purposes of this type of instruction scheduling, we follow [CHH89] and build the for-war-d \ncontrol dependence graph only, i.e. we do not compute the control dependence that result from or propagate \nthrough the back edges in the control flow graph. The CSPDG of Figure 4 is a forward control dependence \ngraph. In the following we discuss forward control dependence graphs only. Notice that forward control \ndependence graphs are acyclic. The usefulness of the control subgraph of PDG stems from the fact that \nbasic blocks that have the same set of control dependence (like BL 1 and BL1O, or BL2 and BL4, or BL6 \nand BL8 in Figure 4) can be executed in parallel up to the existing data dependence. For our purposes, \nthe instructions of such basic blocks can be scheduled together. Now let us introduce several deffitions \nthat are required to understand our scheduling framework. Let A and B be two nodes of a control flow \ngraph such that B is reachable from A, i.e., there is a path in the control flow graph from A to B. Definition \n1. A dominates B if and only if A appears on every path fi-om ENTRY to B. Definition 2. B postdominates \nA if and only if B appears on every path from A to EXIT. Definition 3. A and B are equivalent if and \nonly if A dominates B and B postdorninates A. Definition 4. We say that moving an instruction from B \nto A is useful if and only if.4 and B are equivalent. Definition 5. We say that moving an instruction \nfrom B to A is speculative if B does not postdorninate A. Definition 6. We say that moving an instruction \nfrom B to A requires duplication if A does not dominate B. It turns out that CSPDGS are helpful while \ndoing useful scheduling. To fmd equivalent nodes, we search a CSPDG for nodes that are identically control \ndependent, i.e. they depend of the same set of nodes under the same conditions. For example, in Figure \n4, BL 1 and B L 10 are equivalent, since they do not depend on any node. Also, BL2 and BL4 are equivalent, \nsince both of them depend on BL1 under the TRUE condition. In Figure 4 we mark the equivalent nodes with \ndashed edges, the diiection of these edges provides the dominance relation between the nodes. For example, \nfor equivalent nodes BL 1 and BLI O, we conclude that BLI dominates BL1O. CSPDG is useful also for speculative \nscheduling. It provides the degree of speculativeness for moving instructions from one block to another. \nWhen scheduling a speculative instruction, we always gamble on the outcome of one or more branches; only \nwhen we guess the direction of these branches correctly, the moved instruction becomes profitable. CSPDG \nprovides for every pair of nodes the number of branches we gamble on (in case of speculative scheduling). \nFor example, when moving instructions from BL8 to BL 1, we gamble on the outcome of a single branch, \nsince when moving from BL8 to BL1 in Figure 4, we cross a single edge. (This is not obvious from the \ncontrol flow graph of Figure 3.) Similarly, moving from BL5 to BL 1 gambles on the outcome of two branches, \nsince we cross two edges of Figure 4. Definition % We say that moving instructions from B to A is n-branch \nspecula~ive if there exists a path in CSPDG from A to B of length n. Notice that useful scheduling is \nO-branch speculative. 4.2. Data dependence While control dependence are computed at a basic block level, \ndata dependencies are computed on an instruction by instruction basis. We compute both intrablock and \ninterlock data dependencies. A data dependence may be caused by the usage of registers or by accessing \nmemory locations. Let a and b be two instructions in the code. A data dependence edge from a to b is \ninserted into PDG in one of the following cases: A register defined in a is used in b ( jlow dependence); \n A register used in a is defined in b (anti-dependence); A register defined in a is defined in b (output \ndependence);  Both a and b we instructions that touch memory (loads, stores, calls to subroutines) and \nit is not proven that they address different locations (memory di.rarnbiguation). Ordy the data dependence \nedges leading from a deftition of a register to its use carry a (potentially non-zero) delay, which is \na characteristic of the underlying machine, as was mentioned in Section 2. The rest of the data dependence \nedges carry zero delays. To minimize the number of anti and output data dependence, which may unnecessarily \nconstrain the scheduling process, the XL compiler does certain renaming of registers, which is si.rnih \nto the effect of the static single assignment form [CFRWZ]. To compute all the data dependence in a basic \nblock, essentially every pair of instructions there has to be considered. However, to reduce the compilation \ntime, we take advantage of the following observation. Let a, b and c be three instructions in the code. \nThen, if we discover that there is a data dependence edge from a to b and from b to c, there is no need \nto compute the edge from a to c. To use this observation, the basic block instructions are traversed \nin an order such that when we come to determine the dependency between a and c, we have already considered \nthe pairs (a,b) and (b,c), for every possible b in a basic block. (Actually, we compute the transitive \nclosure for the data dependence relation in a basic block.) Next for each pair A and B of basic blocks \nsuch that B is reachable from A in the control flow graph, the intrablock data dependence are computed. \nThe observation in the previous paragraph helps to reduce the number of pairs of instructions that are \nconsidered during the computation of the intrablock data dependence as well. Let us demonstrate the computation \nof data dependence for BL1; we will reference the instructions by their numbers from Figure 2. There \nis an anti-dependence from (I 1) to (12), since (I 1) uses r31 and (12) defines a new value for r31. \nThere is a flow data dependence from both (I 1) and (12) to (13), since (13) uses r12 and rO defined \nin (I 1) and (12), respectively. The edge ((12),(13)) carries a one cycle delay, since (12) is a load \ninstruction (delayed  load), while ((I 1),(13)) is not computed since it is Instructions are never moved \nout or into a transitive. There is a flow data dependence edge region.  from (13) to (14), since (13) \nsets cr7 which is used in All the instructions are moved in the upward (14). This edge has a three cycle \ndelay, since (13) is duection, i.e, they are moved against the a compare instruction and (14) is the \ncorresponding direction of the control flow edges. branch instruction. Finally, both of ((I 1),(14)) \nand The original order of branches in the program is  ((12),(14)) are transitive edges. preserved. Also, \nthere are several limitations that characterize It is important to notice that, since both the control \nthe current status of our implementation for globaland data dependence we compute are acyclic, the scheduling. \nThis includes: resultant PDG is acyclic as well. This facilitates convenient scheduling of instructions \nwhich is NO duplication of code is allowed (see discussed next. Deftition 6 in Section 4.1). Only l-branch \nspeculative instructions are 5. The scheduling framework supported (see Deftition 7 in Section 4.1). \nThe global scheduling framework consists of the s No new basic blocks are created in the control top-level \nprocess, which tries to schedule flow graph during the scheduling process. instructions cycle by cycle, \nand of a set of heuristics These limitations will be removed in future work. which decide what instruction \nwill be scheduled next, in case there is a choice. While the top-level We schedule instructions in a \nregion by processing process is suitable for a range of machines dkcussed its basic blocks one at a time. \nThe basic blocks arehere, it is suggested that the set of heuristics and visited in the topological order, \ni.e., if there is a path their relative ordering should be tuned for a specflc in the control flow graph \nfrom A to B, A ismachine at hand. We present the top-level process processed before B. in Section S,1, \nwhile the heuristics are discussed in Section 5.2. LetA be the basic block to be scheduled next, and \n 5.1. The top-level process let EQUZV(A) be the set of blocks that are We schedule instructions in the \nprogram on a equivalent to A and are dominated by A (see region by region basis. In our terminology a \nregion Deftition 3). We maintain a set C(A) of candidate represents either a strongly connected component \nblocks for A, i.e., a set of basic blocks which can that corresponds to a loop (which has at least one \ncontribute instructions to A. Currently there are back edge) or a body of a subroutine without the two \nlevels of scheduling: enclosed loops (which has no back edges at all). 1. Useful instructions only: C(A)= \nEQUIV(A); Since currently we do not overlap the execution of 2. l-branch speculative: C(A) includes the \ndifkent iterations of a loop, there is no difference following blocks: in the process of scheduling the \nbody of a loop and a. the blocks of EQUIV(A); the body of a subroutine. b. AU the immediate successors \nof A in CSPDG; Innermost regions are scheduled f~st. There are a c. All the immediate successors of blocks \nin few principles that govern our scheduling process: EQUW(A) in CSPDG. Once we initialize the set of \ncandidate blocks, we compute the set of candidate instructions for A. An instruction I is a candidate \nfor scheduling in block A if it belongs to one of the following categories: 1 belonged to A in the fust \nplace. ~ 1 belongs to one of the blocks in C(A) and: 1. f belongs to one of the blocks in EQUIV(A) and \nit may be moved beyond its basic block boundaries. (There are instructions that are never moved beyond \nbasic block boundaries, like calls to subroutines.) 2. 1 does not belong to one of the blocks in EQUIV(XI) \nand it is allowed to schedule it speculatively. (There are instructions that are never scheduled speculatively, \nlike store to memory instructions.)  During the scheduling process we maintain a list of ready instructions, \ni.e., candidate instructions whose data dependence are fulfilled. Every cycle we pick from the ready \nlist as many instructions to be scheduled next as required by the machine architecture, by consulting \nthe parametric machine description. If there are too many ready instructions, we choose the %est ones \nbased on priority criteria. Once an instruction is picked up to be scheduled, it is moved to the proper \nplace in the code, and its data dependence to the following instructions are marked as fulfilled, potentiality \nenabling new instructions to become ready. Once all the instructions of A are scheduled, we move to the \nnext basic block. The net result is that the instructions in A are reordered and there might be instructions \nexternal to A that are physically moved into A. It turns out that the global scheduler does not always \ncreate the best schedule for each individual basic block. It is mainly due to the two following reasons: \n9 The parametric machine description of Section 2 does not cover all the secondary features of the machine; \n The global decisions are not necessarily optimal in a local context. To solve this problem, the basic \nblock scheduler is applied to every single basic block of a program after the global scheduling is completed. \nThe basic block scheduler has a more detailed model of the machine which allows more precise decisions \nfor reordering the instructions within the basic blocks. 5.2. Scheduling heuristics The heart of the \nscheduling scheme is a set of heuristics that provide the relative priority of an instruction to be scheduled \nnext. There are two integer-valued functions that are computed locally (within a basic block) for every \ninstruction in the code, these functions are used to set the priority of instructions in the program. \nLet 1 be an instruction in a block B. The fust function D(l), called delay heuristic, provides a measure \nof how many delay slots may occur on a path from 1 to the end of B. Initially, D(l) is set to O for every \nK in B. Assume that .Jl,JZ, ... are the immediate data dependence successors of Z in B, and let the delays \non those edges be d(lJ1), d(l,Jz), .... Then, by visiting I after visiting its data dependence successors, \nD(I) is computed as follows: D(l) = max((D(J1) + d(Z,J1)),(D(JJ + d(l,JJ), ... ) The second function \nCP(l), called critical path heuristic, provides a measure of how long it will take to complete the execution \nof instructions that depend on 1 in B, including 1 itself, and assuming an unbounded number of computational \nunits. Let E(l) be the execution time of 1. First, CP(Z) is initialized to E(l) for every 1 in B. Then, \nagain by visiting 1 after visiting its data dependence successors, CP(l) is computed as follows: CT(l) \n= max((CP(J1) i-d(l,J1)), (CP(J2) + d(l,JJ), ... ) + l?(f) During the decision process, we schedule useful \ninstructions before speculative ones. For the same class of instructions (useful or speculative) we pick \nan instruction with has the. biggest delay heuristic ( D). For the instructions of the same class and \ndelay we pick one that has a biggest critical path heuristic (CP). Finally, we try to preserve the original \norder of instructions. To make it formally, let A be a block that is currently scheduled, and let 1 and \nJ be two instructions that (should be executed by a functional unit of the same type and) are ready at \nthe same time in the scheduling process, and one of them has to be scheduled next. Also, let U(A) = A \nlJ EQUIV(A), and let B(l) and B(J) be the basic blocks to which 1 and J belong. Then, the decision is \nmade in the following orden 1. If B(l) e U(A) and B(J)# U(A), then pick ~ 2. If B(J) e U(A) and 13(1)# \nU(A), then pick J 3. If D(Z)> D(J), then pick ~ 4, If D(J)> D(f), then pick<  5. If CP(l) > CP(J), \nthen pick L 6. If CP(J) > Cl (l), then pick % 7. Pick an instruction that occurred in the code frost, \n  Notice that the current ordering of the heuristic functions is tuned towards a machine with a small \nnumber of resources. This is the reason for always preferring to schedule a useful instmction before \na speculative one, even though a speculative instruction may cause longer delay. In any case, experimentation \nand tuning are needed for better results. 5.3, Speculative scheduling In the global scheduling framework, \nwhile doing non-speculative scheduling, to preserve the correctness of the program it is sufficient to \nrespect the data dependence as they were defined in Section 4.2. It turns out that for speculative scheduling \nthis is not true, and a new type of information has to be maintained. Examine the following excerpt of \na C program: ... i f (cond) x=5; else x=3; print. f( x=%d , x); ... The control flow graph of this piece \nof code looks as follows: Instruction x=5 belongs to B2, while x=3 belongs to B3. Each of them can be \n(speculatively) moved into B 1, but it is apparent that both of them are not allowed to move there, since \na wrong value may be printed in B4. Data dependence do not prevent the movement of these instructions \ninto B 1. To solve this problem, we maintain the information about the (symbolic) registers that are \nIbe on exit from a basic block. If an instruction that is being considered to be moved speculatively \nto a block B computes a new value for a register that is live on exit from B, such speculative movement \nis dka.llowed. Notice that this type of information has to be updated dynamically, i.e., after each speculative \nmotion this information has to be updated. Thus, let us say, x=5 is fwst moved to B 1. Then, x (or actually \na symbolic register that . . . more instructions here . . . ********** LOOp STARTS ************ CL.0: \n(11) L r12=a(r31,4) (12) LU r0,r31=a(r31,8)  (118) AI r29=r29,2 (13) C cr7=r12,r0 (119) C cr4=r29,r27 \n(14) ;F CL.4,cr7,0x2/gt (15) cr6=r12,r30 (18) c cr7=r0,r28 (16) BF CL.6,cr6,0x2/gt (17) LR r30=r12 \n CL.6: (19) BF CL.9,cr7,0xl/lt (110) LR r28=r0 (Ill) B CL.9 CL.4: (112) C cr6=r0,r30 (115) C cr7=r12,r28 \n(113) BF CL.ll,cr6,0x2/gt (114) LR r30=r0 CL.11: (116) BF CL.9,cr7,0xl/lt (117) LR r28=r12 CL.9: (120) \nBT CL.0,cr4,(3xl/lt ********** Loop ENDS ************** . . . more instructions here . . . . . . more \ninstructions here . . . *********** Loop STARTS ************* CL.0: (11) L r12=a(r31,4) (12) LU r0,r31=a(r31,8) \n (118) AI r29=r29,2 (13) C cr7=r12,r13 (119) C cr4=r29,r27 (15) C cr6=r12,r30 (112) C cr5=r0,r3Cl (14) \nBF CL.4,cr7,0x2/gt (18) c cr7=r0,r28 (16) BF CL.6,cr6,EJx2/gt (17) LR r30=r12 CL.6:  (19) BF CL.9,cr7,(3xl/lt \n (110) LR r28=r0 (Ill) B CL.9 CL.4: (115) C cr7=r12,r28 (113) BF CL.ll,cr5,(3x2/gt (114) LR r30=r0 \nCL.11: (116) BF CL.9,cr7,0xl/lt (117) LR r28=r12 CL.9: (120) BT CL.13,cr4,0xl/lt ********** Loop ENDS \n*************** . . . more instructions here . . . Figure 5. The results of applying the useful scheduling \nFigure 6. The results of applying the useful and specula\u00adto the program of Figure 2 tive schedulingto \nthe programof Figure2 correspondsto x) becomes live onexitfrom B 1, and the movement ofx=3to B1 will \nbe prevented. More detailed description ofthe speculative scheduling and its relationship to the PDG-based \nglobal scheduling is out of the scope of this paper. 5.4. Scheduling examples Letus demonstrate the effect \nof useful and speculative scheduling on the example of Figure 2. The result ofscheduling useful instructions \nonlyto this program is presented in Figure 5. During the scheduling ofBLl, the ordyinstmctions that were \nconsidered tobe moved there were those ofBLIO, since only BLIO~EQUIV(BLl). Theresultisthat two instructions \nofBL10(118 and 119) were moved Figure 2 was executing in 20-22 cycles per iteration. into BL1, ftiginthe \ndelay slots of the instructions there. Similarly, 18was moved from BL4toBL2,andI 15wasmovedfrom BL8toBL6, \nTheresultantprogram inFigure5takes 12-13 cycles per iteration, while the original program of Figure \n6 shows the result ofapplying both the useful a.ndthe (l-branch) speculative schedulingto the same program. \nIn addition to the motions that were described above, two additional instructions (15 and 112) were moved \nspeculatively to BL1, to ffl in the three cycle delay between 13 and 14. Interestingly enough, since \n15and 112 belong to basic blocks that are never executed together in any single execution of the program, \nonly one of these Next we describe how the global scheduling scheme two instructions will carry a useful \nresult. All in all, was cor@ured so as to exploit the trade-off of the the program in Figure 6 takes \n11-12 cycles per compile-time overhead and the run-time iteration, a one cycle improvement over the improvement \nto a maximum extent. The following program in Figure 5. design decisions characterize the current status \nof the global scheduling prototype:  6. Performance results 9 Only two inner levels of regions are scheduled. \nA preliminary evaluation of the global scheduling So, we distinguish between inner regions (i.e., scheme \nwas done on the IBM RS/6K machine regions that do not include other regions) andwhose abstract model \nis presented in Section 2.1. outer regions (i.e. regions that include onlyFor experimentation purposes, \nthe global inner regions). scheduling has been embedded into the IBM XL Only small reducible regions \nare scheduled. family of compilers. These compilers support Small regions are those that have at most \n64several high-level languages, like C, Fortran, Pascal, basic blocks and 256 instructions. etc.; however, \nwe concentrate only on the C In a preparation step, before the globalprograms. scheduling is applied, \nthe inner regions that represent loops with up to 4 basic blocks are The evaluation was done on the four \nC programs in unrolled once (i.e., after unrolling they include the SPEC benchmark suite [SS9]. In the \nfollowing two iterations of a loop instead of one). discussion LI denotes the Lisp Interpreter After \nthe global scheduling is applied to the benchmark, GCC stands for the GNU C Compiler, . inner regions, \nsuch regions that represent loopswhile EQNTOTT and ESPRESSO are two with up to 4 basic blocks are rotated, \nby programs that are related to minimization and copying their fust basic block after the end of manipulation \nof Boolean functions and equations. the loop. By applying the global scheduling the second time to the \nrotated inner loops, we The basis for all the following comparisons achieve the partial effect of the \nsoftware (denoted by BASE in the sequel) is the performance pipelining, i.e., some of the instructions \nof the results of the same IBM XL C compiler in which next iteration of the loop are executed within \nthe global scheduling was disabled. Please notice the body of the previous iteration. that the base compiler \nincludes two types of The general flow of the global scheduling is as instruction scheduling on its own \n(aside of all the . follows: possible machine independent and peephole 1. certain inner loops are unrolled;optimization) \nas follows: 2. the global scheduling is applied the fust a sophisticated basic block scheduler similar \nto time to the inner regions only; that of [W90], and 3. certain inner loops are rotated;  a set of \ncode replication techniques that solve 4. the global scheduling is applied the second certain loop-closing \ndelay problems [GR90]. time to the rotated inner loops and the outer regions. So, in some sense certain \nimprovements due to the global scheduling overlap those of the scheduling The compile-time overhead of \nthe above described techniques that were already part of the base scheme is shown in Figure 7. The column \nmarked compiler. BASE gives the compilation times of the programs in seconds as measured on the IBM RS/6K \nmachine, model 530 whose cycle time is 40ns. The column marked CTO (Compile-Time Overhead) provides the \nincrease in the compilation times in percents. This increase in the compilation time includes the time \nrequired to perform all of the above mentioned steps (including loop unrolling, loop rotation, etc.). \nPROGRAM BASE CTO LI 206 13% EQNTOTT 78 17% ESPRESSO 465 12% GCC 2457 13% Figure 7. Compile-time overheads \nfor the global sched\u00aduling There are two levels of scheduling that we distinguish at the moment, namely \nusefid only and useful and speculative scheduling. The run-time improvement (RTI) for both types of scheduling \nis presented in Figure 8 in percents relative to the running time of the code compiled with the base \ncompiler which is shown in seconds. The accuracy of the measurements is about 0.5 0/0 -10/0. PROGRAM \nBASE RTI USEFUL SPECULATIVE 2.0% EQNTOTT 45 7.1% 7.3% ESPRESSO 1(36 -0.5% 0% GCC 76 -1.5% (3% LI 312 \n6.9% Figure 8. Run-time improvements for the global sched\u00aduling We notice in Figure 8 that for EQNTOTT \nmost of the improvement comes from the useful scheduling only, while for LI, the speculative scheduling \nis dominant. On the other hand, for both ESPRESSO and GCC, no improvement is observed. (Actually, there \nis a slight degradation in performance for both benchmarks, when the global scheduling is restricted \nto useful scheduling only.) To summarize our short experience with the global scheduling, we notice that \nthe achieved improvement in run-time is modest due to the fact that the base compiler has already been \noptimized towards the existing architecture. We may expect even bigger payoffs in machines with a larger \nnumber of computational units. As for the compile-time overhead, we consider it as reasonable, especially \nsince no major steps were taken to reduce it except of the control over the size of the regions that \nare being scheduled. 7. Summary The proposed scheme allows the global scheduling of instructions by \nan opt imizing compiler for better utilization of machine resources for a range of superscalar processors, \nIt is based on a data structure proposed for parallel/parallelizing compilers (PDG), a parametric machine \ndescription and a flexible scheduling framework that employs a set of useful heuristics. The results \nof evaluating the scheduling scheme on the IBM RS/6K machine are quite encouraging. We are going to extend \nour work by supporting more aggressive speculative scheduling, and scheduling with duplication of code. \nAcknowledgements.We would like to thank Kemal Ebcioglu, Hugo Krawczyk and Ron Y. Pinter for many helpful \ndiscussions, and Irit Boldo and Vladimir Rainish for their help in the implementation. References Transactions \non Prog. Lang. and [BG89] [BRG89] [BJR89j [BEH89] [CHH89] [CFRWZ] [E88] [EN89] [E851 [FOW87] Bernstein, \nD., and Gertner, I., Scheduling expressions on a pipelined processor with a maximal delay of one cycle \n, ACM Transactions on Prog. Lang. and Systems, Vol. 11, Num. 1 (Jan. 1989), 57-66, Bernstein, D., Rodeh, \nM., and Gertner, I., Approximation algorithms for scheduling arithmetic expressions on pipelined machines \n, Journa[ of AZgorit/vns, 10 (Mar. 1989), 120-139. Bernstein, D., Jaffe, J. M., and Rodeh, M,, Scheduling \narithmetic and load operations in parallel with no spilling , SIAM Journa[ of Computing, (Dec. 1989), \n1098-1127. Bradlee, D. G., Eggers, S.J., and Henry, R. R., Integrating register allocation and instruction \nscheduling for RISCS , to appear in Proc. of the Fourth ASPLOS Conference, (April 199 1). Cytron, R., \nHind, M., and Wilson, H., Automatic generation of DAG parallelism , Proc. of the SIGPLAN Annual Symposium, \n(June 1989), 54-68, Cytron, R., Ferrante, J., Rosen, B. K., Wegman, M. N., and Zadeck, F. K., An efficient \nmethod for computing static single assignment form , Proc, of the Annual ACM Symposium on Principles \nof Programming Languages, (Jan. 1989), 25-35. Ebcioglu, K., Some design ideas for a VLIW architecture \nfor sequential-natured software , Proc. of the IFIP Conference on Paral!el Processing, (April 1988), \nItaly. Ebcioglu, K., and Nakatani, T., A new compilation technique for paralleliziig regions with unpredictable \nbranches on a VLIW architecture , Proc. of the Workshop on Languages and Compilers fm-bm-aalle[ Computing, \n(August 1989), Ellis, J. R., Bulldog: A compiler for VLIW architectures , Ph.D. thesis, Yale U/DCS/RR-364, \nYale University, Feb. 1985. Ferrante, J., Ottenstein, K.J., and Warren, J. D., The program dependence \ngraph and its use in optimization , ACM  [F81] [GM$6] [GR90] [G089] [HG83] [JW89] [L881 [P851 [SLH90] \n [s89] p-v!xy Systems, Vol. 9, Nurn. 3 (July 1987), 319-349. Fisher, J., Trace scheduling: A technique \nfor global microcode compaction , IEEE Trans. on Computers, C-30, No. 7 (July 1981), 478-490. Gibbons, \nP.B. and Muchnick, S.S., Efficient instruction scheduling for a pipelined architecture , Proc. of the \nSIGPLAN Annual Symposium, (June 1986), 11-16. Golumblc, lM.C. and Rainish, V., Instruction scheduling \nbeyond basic blocks , IBM J, Res. Dev.,(Jan. 1990), 93-98. Groves, R. D., and Oehler, R., An IBM second \ngeneration RISC processor architecture , Proc. of the IEEE Conference on Computer Design, (October 1989), \n134-137. Hennessy, J,L. and Gross, T., Postpass code optimization of pipeline constraints , ACM Trans. \non Programming Languages and Systems 5 (July 1983), 422-448. Jouppi, N. P., and Wall, D.W., Available \ninstruction-level parallelism for superscalar and superpipelined machines , Proc. of the Third A SPLOS \nConference, (April 1989), 272-282. Lam M, Software Pipelining: An effective scheduling technique for \nVLIW machines , Proc. of the SIGPLAN Annual Symposium, (June 1988), 318-328. Patterson, D. A., Reduced \ninstruction set computers , Comm. of A CM, (Jan. 1985), 8-21. Smith, M.D, Lam M. S., and Horowitz M.A., \nBoosting beyond static scheduling in a superscalar processor , Proc. of the Computer Architecture Conference, \n(May 1990), 344-354. SPEC Newsletter , Systems Performance Evaluation Cooperative, Vol. 1, Issue 1, (Sep. \n1989). Warren, H., Instruction scheduling for the IBM RISC System/6K processor , IBit4 J. Res. Z)W., \n(J~. 1990), 85-92.  \n\t\t\t", "proc_id": "113445", "abstract": "", "authors": [{"name": "David Bernstein", "author_profile_id": "81100134885", "affiliation": "IBM Israel Scientific Center, Technion City, Haifa 32000, Israel", "person_id": "PP31028815", "email_address": "", "orcid_id": ""}, {"name": "Michael Rodeh", "author_profile_id": "81100123457", "affiliation": "IBM Israel Scientific Center, Technion City, Haifa 32000, Israel", "person_id": "P198604", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/113445.113466", "year": "1991", "article_id": "113466", "conference": "PLDI", "title": "Global instruction scheduling for superscalar machines", "url": "http://dl.acm.org/citation.cfm?id=113466"}