{"article_publication_date": "05-01-1991", "fulltext": "\n A Methodology for Managing in CLP Systems Hard Constraints IBM Joxan T. J. Watson Jaffar Research G%. \nSpiro Michaylov School of Computer Science IBM Roland H.C. Yap T. J. Watson Research fXr. P. O. Box \n704 Carnegie Mellon University P. O. Box 704 Yorktown Heights, NY 10598 Pittsburgh, PA 15213 Yorktown \nHeights, NY 10598 (joxan@ibm.tom) (spiro@cs. cmu.edu) (rhc@ibm.tom) Abstract In constraint logic programming \n(CLP) systems, the standard technique for dealing with hard con\u00adstraints is to delay solving them until \nadditional constraints reduce them to a simpler form. For example, the CLP (7?) system delays the solving \nof nonlinear equations until they become linear, when certain variables become ground. In a naive imple\u00adment \nation, the overhead of delaying and awakening constraints could render a CLP system impractical. In this \npaper, a framework is developed for the specification of wakeup degrees which indicate how far a hard \nconstraint is from being awoken. This framework is then used to specify a runtime struc\u00adture for the \ndelaying and awakening of hard con\u00adstraints. The primary implementation problem is the timely awakening \nof delayed constraints in the context of temporal backtracking, which requires changes to internal data \nstructures be reversible. This problem is resolved efficiently in our struc\u00adt ure, Permission to copy \nwithout fee all or part of this material is granted provided that the copies are not made or distributed \nfor direct commercial advantage, the ACM copyright notice end the title of the publication and its date \nappear, and notice is given that copying is by permission of the Association for Computing Machinery. \nTo copy otherwise, or to rewblish, reauires a fee andlor snecific r)ermission. @ l 991 ACM O-89791 -4i8-7/$jl \n/0005 /0306 ...$1 .50 1[ I Proceedings of the ACM SIGPLAN 91 Conference cm I Programming Language Design \nand Implementation. Toronto, Ontario, Canada, June 26-28, 1991. 1 Introcluction The Constraint Logic \nProgramming scheme [7] pre\u00adscribes the use of a constraint solver, over a spe\u00adcific structure, for determining \nthe solvability of constraints. In practice, it is difficult to construct efficient solvers for most \nuseful structures. A st an\u00addard compromise approach has been to design a partial solver, that is, one \nthat solves only a sub\u00adclass of constraints, the directly solvable ones. The remaining constraints, the \nhard ones, are simply delayed from consideration when they are first en\u00ad countered; a hard constraint \nis reconsidered only when the constraint store cent ains sufficient infor\u00ad mation to reduce it into a \ndirectly solvable form. In the real-arithmetic-based CLP(R) system [8, 9], for example, nonlinear arithmetic \nconstraints are classified as hard constraints, and they are delayed until they become linear. The key \nimplement ation issue is how to effi\u00adciently process just those delayed constraints that are affected \nas a result of a new input constraint, Specifically, the cost of processing a change to the current collection \nof delayed constraints should be related to the delayed constraints affected by the change, and not to \nall the delayed constraints. The following two items seem necessary to achieve this end. First is a notion \nwhich indicates how far a delayed constraint is from being awoken. For example, it is useful to distinguish \nthe delayed CLP(%?) constraint X = rna~(Y, Z), which awaits the grounding of Y and Z, from the constraint \nX = maz(5, Z), which awaits the grounding of Z. This is because, in general, a delayed constraint is \nawoken by not one but a conjunction of several input constraints. When a subset of such input constraints \nhas already been encountered, the run\u00adtime structure should relate the delayed constraint to just the \nremaining kind of constraints which will awaken it, The other item is some data structure, call it the \naccess structure, which allows immediate access to just the delayed constraints affected as the result \nof a new input constraint. The main challenge is how to maintain such a structure in the presence of \nbacktracking. For example, if changes to the struc\u00adture were trailed using some adaptation of PRO-LOG \ntechniques [14], then a cost proportional to the number of entries can be incurred even though no delayed \nconstraints are affected. There are two main elements in this paper. First is a framework for the specification \nof wakeup de\u00adgrees which indicate how far a hard constraint is from being awoken. Such a formalism makes \nex\u00adplicit the various steps a CLP system takes in reducing a hard constraint into a directly solv\u00adable \none. The second element is a runtime struc\u00adture which involves a global stack representing the delayed \nconstraints. This stack also contains all changes made to each delayed constraint when a new input constraint \nmakes progress towards the awakening of the delayed constraint. A secondary data structure is the access \nstructure. Dealing with backtracking is straightforward in the case of the global structure simply because \nit is a stack. For the access structure, no trailing/saving of en\u00ad tries is performed; instead, they \nare reconstructed upon backtracking. Such reconstruction requires a significant amount of interconnection \nbetween the global stack and access structure. In this runtime structure, the overhead cost of managing \nan op\u00aderation on the delayed constraints is proportional to the size of the delayed constraints aflected \nby the operation, as opposed to all the delayed con\u00adstraints. 2 Elackground and Related Work In this \nsection we first review some early ideas of dataflow and local propagation, and the notion of flexible \natom selection rules in logic programming systems. We then briefly review the basics of CLP, discuss \nthe issue of delaying constraints in C!LP, and mention some delay mechanisms in various CLP systems. \n2.1 Data Flow and Local Propagation The idea of dataflow computation, see e.g. [1], is perhaps the simplest \nform of a delay mechanism since program operations can be seen as directional constraints with fixed \ninputs and outputs. In its pure form, a dat aflow graph is a specification of the data dependencies required \nby such an oper\u00adation before it can proceed. Extensions, such as the I-structures of [2], are used to \nprovide a de\u00adlay mechanism for lazy functions and complex data structures such as arrays in the context \nof dat aflow. In local propagation, see e.g. [11], the solving of a constraint is delayed until enough \nof its variables have known values in order that the remaining val\u00adues can be directly computed. Solving \na constraint can then cause other constraints to have their val\u00adues locally propagated, etc. The concept \nand its implement ation are logical extensions of data flow in essence, a data flow graph is one possible \nlocal propagation path through a constraint network. In other words, directionality is eliminated. 2.2 \nDelay Mechanisms in PROLOG In PROLOG, the notion of delaying has been mainly applied to goals (procedure \ncalls), and im\u00adplemented by the use of a dynamically changeable atom selection rule. The main uses of \ndelaying were to handle safe negation [10], and also to attempt to regain some of the completeness lost \ndue to PRO-LOG S depth first search. There are similarities between implementing delay in PROLOG and \nim\u00adplementing a data flow system, except in one fun\u00addamental aspect: temporal backtracking. Further complications \nare related to the saving of machine states while a woken goal is being executed. Some PRO LOGs allow \nthe user to specify delay annotations. One kind is used on subgoals. For example, the annotation freeze \n(X, G) in PROLOG\u00ad11 [4] defers the execution of the goal G until X is instantiated. Another kind of annotation \nis applied to relations. For example, the wait declarations of MU-PROLOG [10] and the when declarations \nof NU-PROLOG [13] cause all calls to a procedure to delay until some instantiation condition is met. \nCarlsson [3] describes an implementation tech\u00adnique for freeze (X, G). While an implementation of ~ree.ze \ncan be used to implement more sophisti\u00adcated annotations like wait and when, this will be at the expense \nof efficiency. This is mainly because the annotations can involve complicated conjunc\u00adtive and disjunctive \nconditions. Since freeze takes just one variable as an argument, it is used in a complicated manner in \norder to simulate the be\u00adhavior of a more complex wakeup condition, In general, the present implementation \ntech\u00adniques used for PROLOG systems have some com\u00admon feat ures: The delay mechanism relies on a modification \nof the unification algorithm [3]. This entails a minimal change to the underlying PROLOG engine. In CLP \nsystems, this approach is not directly applicable since there is, in general, no notion of unification. \n  Goals are woken by variable bindings. Each binding is easily detectable (during unifica\u00adtion). In \nCLP systems, however, detecting when a delayed constraint should awaken is far more complicated in general. \nIn this pa\u00adper, this problem is addressed using wakeup degrees, described in the next section.  1Committed \nchoice logic programming languages [12] use delaying for process synchronization. However there is no \nbacktracking here. The number of delayed goals is not large in general. This can render acceptable, imple\u00admentations \nin which the cost of awakening a goal is related to the number of delayed goals [3], as opposed to the \nnumber of awakened goals. In a CLP system, the number of delayed constraints can be very large, and so \nsuch a cost is unacceptable.  2.3 Delaying Hard Constraints in CLP Before describing the notion of delaying \ncon\u00adstraints, we briefly recall some main elements of CLP. At the heart of a CLP language is a structure \nD which specifies the underlying domain of compu\u00adt ation, the constant, function and constraint sym\u00adbols, \nand the corresponding constants, functions and constraints. Terms are constructed using the constant \nand function symbols, and a constraint is constructed using a constraint symbol whose argu\u00adments are \nterms. An atom is a term of the form t ) where p is a predicate symbol and the P(h>... ? n tiare terms. \nA CLP program is a finite collection of rules, each of which is of the form Ao: cq, az, . . ..a~ where \neach ai, 1 < i < k, is either a constraint or an atom, that is, a term of the form p(tl, . . . . tn) \nwhere p is a user-defined predicate symbol and the ti are terms. The language CLP(T?) for example, involves \narithmetic terms, e.g. X + 3 * Y + Z and constraints, e.g. X + 3*Y + Z ~ O. The essence of the CLP operational \nmodel is that it starts with an empty collection CSO of constraints as its constraint store, and successively \naugments this store with a new input constraint. That is, each primitive step in the operational model \nob\u00adtains a new store CSi+l by adding an input con\u00adstraint to the previous store CSi, i > 0. The con\u00adjunction \nof the constraints in each store is satis\u00adfiable. If every attempt to generate a collection CSi+l from \nCSi results in an unsatisfiable collec\u00ad tion, then the store may be reset to some previous store CSj, \nj < i, that is, backtracking occurs. The full details of the operational model, not needed in this paper, \ncan be obtained from [7]. In principle, a CLP system requires a decision procedure for determining whether \na satisfiable constraint store can be augmented with an input constraint such that the resulting store \nis also sat\u00adisfiable. In practice, this procedure can be pro\u00adhibitively expensive. An incomplete system, \nbut which is often still very useful, can be obtained by partitioning the class of constraints into the \ndirectly solvable ones, and the hard ones. Upon encounter\u00ading a hard constraint, the system simply defers \nthe consideration of this constraint until the store con\u00adtains enough information to reduce the hard \ncon\u00adstraint into a directly solvable form2. There are a number of CLP systems with delayed constraints. \nOne is PROLOG-II [4] where the hard constraints are disequations over terms, and these constraints awaken \nwhen their arguments become sufficiently instantiated. In CLP(73) , the hard constraints are the nonlinear \narithmetic ones, and these delay until they become linear. In CHIP [6], hard constraints include those \nover natural num\u00adbers, and these awaken when both an upper and lower bound is known for at least all \nbut one of the variables. In PROLOG-III [5], some hard con\u00adstraints are word equations and these awaken \nwhen the lengths of all but the rightmost variables in the two constituent expressions become known. \ns Wakeup Systems Presented here is a conceptual framework for the specification of operations for the \ndelaying and awakening of constraints. We note that the formal\u00adism below is not designed just for logic \nprogram\u00adming systems. Let the rneta-constants be a new class of sym\u00adbols, and hereafter, these symbols \nare denoted by a and /3. A meta-constant is used as a template for 21t is po~~ible that hard constraints \nremained indefinitely deferred. a (regular) constant. Define that a meta-constraint is just like a constraint \nexcept that meta-constants may be written in place of constants. A met a\u00adconstraint is used as a template \nfor a (regular) con\u00adstraint. To indicate how far a hard constraint is from be\u00ading awoken, associate with \neach constraint symbol V a finite number of wakeup degrees. Such a de\u00adgree D is a template representing \na collection of V-constraints3. It is defined4 to be either the spe\u00adcial symbol woken, or a pair (t, \nC) where t is a term of the form V(tl,..., t~) where each tiis either a variable, constant or met a\u00adconstant, \nand  C, is a conjunction of meta-constraints which contain no variables and whose meta\u00adconstants, if \nany, appear in t.  Let d be a mapping from variables into variables and met a-constants into constants. \nAn instance of a wakeup degree D = (t,C) is the constraint ob\u00adtained by applying to t such a mapping \n0 which evaluates C into true. The instance is denoted ve. In CLP(7?) for example, a subset of the constraints \ninvolving the constraint symbol pow (where pow(X, Y, Z) means X = Yz) may be represented by the degree \npow(A, B, a), a # O. This subset contains all the constraints of the form pow(X, Y, c) where X and Y \nare not necessarily distinct variables and c is a nonzero real number. Associated with each wakeup degree \nD is a col\u00adlection of pairs, each of which contains a generic wakeup condition and a wakeup degree called \nthe new degree. Each wakeup condition is a con\u00adjunction of meta-constraints all of whose meta\u00adconstants \nappear in D. Any variable in a wakeup condition which does not appear the in associated degree is called \nexistential. An instance WO of a generic wakeup condition W is the constraint 3These are constraints \nwritten using the symbol Y/. 4ThM specific definition is but one way to represent a set of expressions. \nIt may be adapted without affecting what follows. obtained by applying the mapping 6 which maps  S \ncent tins the special degree called woken non-exist ential variables into variables and met a\u00ad which \nrepresents a subset of all the directly constants into constants. solvable V-constraints. Like wakeup \ndegrees, a wakeup condition rep\u00adresents a collection of constraints. Intuitively, a wakeup condition \nspecifies when a hard constraint changes degree to the new degree. More precisely, suppose that D is \na wakeup degree and that W is one of its wakeup conditions with the new degree D . Let C be a hard constraint \nin D, that is, C is an instance DO of D. Further suppose that the constraint store implies the corresponding \ninstance of W, that is, the store implies 3X1 . . .x.(w) where the Xi denote the existential variables \nof W. Let C denote a constraint equivalent to C A 3X1 . . . X.(WO). We then say the constraint C re\u00adduces \nto the constraint C via W. Consider once again the CLP(7?) constraints in\u00advolving pow. These constraints \nmay be parti\u00adtioned into classes represented by the wakeup de\u00adgrees pow(A, B, C), pow(~, B, C), POW(A, \ncr, C), pow(A, l?, cr) and woken. For the degree pow(A, 1?, C), which represents constraints of the form \npow(X, Y, Z) where X, Y and Z are variables, an example wakeup condition is C = a. This in\u00addicates that \nwhen a constraint, e.g. Z = 4, is en\u00adtailed by the constraint store, a delayed constraint such as pow(X, \nY, Z) is reduced to pow(X, Y, 4). This reduced constraint may have the new degree pow(A, l?, a). Another \nexample wakeup condition is A = 1, indicating that when a constraint such as X = 1 is entailed, a delayed \nconstraint of the form pow(X, Y, Z) can be reduced to pow(l, Y, Z). This reduced constraint, which is \nin fact equivalent to the directly solvable constraint Y = 1 V (Y # OA Z = O), may be in the degree woken. \nWe exem\u00adplify some other uses of wakeup conditions below. A wakeup system for a constraint symbol V is \na finite collection S of wakeup degrees for W satisfy\u00ading: No two degrees in S contain the same V\u00adconstraint. \n Let the !l-constraint C have a degree D which has a wakeup condition W and new degree D . Then every \nreduced constraint C of C via W is cent ained in D!.  The illustration in figure 1 cent ains an example \nwakeup system for the CLP(X) constraint symbol pow. A wakeup degree is represented by a node, a wakeup \ncondition is represented by an edge label, and the new degree of a reduced constraint is repre\u00adsented \nby the target node of the edge labelled with the wakeup condition which caused the reduction. Generic \nwakeup conditions can be used to spec\u00adify the operation of many existing systems which delay constraints. \nIn PROLOG-like systems whose constraints are over terms, awaiting the instantia\u00adtion of a variable X \nto a ground term can be rep\u00adresent ed by the wakeup condition X = a. Await\u00ading the instantiation of X \nto a term of the form ~(.. .), on the other hand, can be represented by X = ~(Y) where Y is an existential \nvariable. We now give some examples on arithmetic constraints. In PROLOG-III, for example, the wakeup \ncondi\u00adtion X ~ a could specify that the length of word must be bounded from above before further pro\u00adcessing \nof the word equation at hand. For CHIP, where combinatorial problems are the primary ap\u00adplication, an \nexample wakeup condition could be a< XAX~~Afl-a ~4whlch requires that X be bounded within a small range. \nFor CLP(%i) , an example wakeup condition could be X = a * Y + /3, which requires that a linear relationship \nhold be\u00adtween X and Y. Summarizing, each constraint symbol in a CLP system which gives rise to hard constraints \ncan be associated with a finite collection of wakeup degrees each of which indicate how far the constituent \ncon\u00ad straints are from being awoken, These degrees can be organized into a wakeup system, that is, a \ngraph WI DO: pow A, B,C) D2: pow/i, a, C), cr#O/la#l [ W1:A=O W2:A=1Legend: W4:B=0 W5:B=1 W7; C=0 wa:C=l \nwlo:B=a Wll:c= cl! w13:c=a w14:A=a Figure 1: Wakeup whose nodes represent degrees and whose edges are \nrepresent the wakeup condition/new degree rela\u00adtion between two degrees. Such a wakeup system can be \nviewed as a deterministic transition system, and can be used to specify the organization of a constraint \nsolver: the degrees discriminate among constraints so that the solver is able to treat them differently, \nwhile the wakeup conditions specify de\u00adgree transitions of hard constraints with respect to new. input \nconstraints. An important design criterion is that the entailed constraints corresponding to the wakeup \nconditions be efficiently recognizable by the constraint solver. This problem, being dependent on a specific \nsolver, is not addressed in this paper. It is difficult in genera15. In CLP(7?) , for example, it is \nrelatively inexpensive to perform a check if an equation like 51n PROLOG, this problem reduces to the \neasy check of whether a variable is bound.  IW 3\\ w14\\ W15 Dl: powa, B, C,a#OAcx#l D3: pow A, B,cY, \ncY#OAc Y#l [) w3:A=a Aa#OAa#l w6:B=a Aa#o Aa#l w9:c=a Aa+o Aa#l wM:A=a w15; B=a  degrees for pow/3 \nX = 5 is entailed whenever the constraint store is changed. The situation for an inequalit y like X ~ \n5 is quite different. A The Runtime Structure Here we present an implementational framework in the context \nof a given wakeup system. There are three major operations with hard constraints which correspond to \nthe actions of delaying, awakening and backtracking: 1. adding a hard constraint to the collection of \ndelayed constraints; 2. awakening delayed constraints as the result of inputting a directly solvable \nconstraint, and  3. restoring the entire runtime structure to a pre\u00advious stat e, that is, restoring \nthe collection of delayed constraints to some earlier collection, and restoring all auxiliary structures \naccord\u00adingly. The first of our two major structures is a stack6 cent aining the delayed constraints. \nThus imple\u00admenting operation 1, delaying a hard constraint, simply requires a push on this stack. Additionally, \nthe stack contains hard constraints which are re\u00adduced forms of constraints deeper in the stack. For \nexample, if the hard constraint POW(X, Y, Z) were in the stack, and if the input constraint Y = 3 were \nencountered, then the new hard constraint pow(X, 3, Z) would be pushed, together with a pointer from \nthe latter constraint to the former. In general, the collection of delayed constraints cent ained in \nthe system is described by the sub\u00ad collection of stacked constraints which have no in\u00adbound pointers. \nFigure 2 illustrates the stack after storing the hard constraint POW(X, Y, Z), then storing pow(Y, X, \nY), and then encountering the entailed constraint X = 5. Note that this one equation caused the pushing \nof two more elements, these being the reduced forms of the original two. The top two constraints now \nrepresent the current col\u00adlection of delayed constraints. The stack operations can be more precisely \nde\u00adscribed in terms of the degrees of the hard con\u00adstraint at hand. This description is given during \nthe definition of the access structure below. Now consider operation 2. In order to implement this efficiently, \nit is necessary to have some access structure mapping an entailed constraint C to just those delayed \nconstraints affected by C. Since there are in general an infinite number of entailed con\u00adstraints, a \nfinite classification of them is required. We define this classification below, but we assume that the \nconstraint solver, having detected an en\u00adtailed constraint, can provide access to precisely the classes \nof delayed constraints which change de\u00ad sHereafter, the term stack refers to this structure.  pow(5,Y,z) \nt 1 I pc9w(Y,5,Y) 1 I t I I pow(Y,x,Y) -i I I ~---- Figure 2: The stack gree. A dynamic wakeup condition \nis an instance of a generic wakeup condition W obtained by (a) re\u00adnaming all the non-existential variables \nin W into runtime variables7, and (b) instantiating any num\u00adber of the meta-constants in W into constants. \nAn instance of a d ynamic wakeup condition is obtained by mapping all its meta-constants into constants. \nA dynamic wakeup condition is used as a template for describing the collection of entailed constraints \n(its instances) which affect the same sub-collection of delayed constraints. For exam\u00adple, suppose that \nthe only delayed constraint is pow(5, Y, Z) whose degree is pow(a, 1?, C) with generic wakeup conditions \nB = a and C = cr. Then only two dynamic wakeup conditions need be con\u00adsidered: Y = a and Z = O. In general, \nonly the dynamic wakeup conditions whose non-existential variables appear in the stack need be considered. \nWe now specify an access structure which maps a dynamic wakeup condition into a doubly linked list of \nnodes. Each node contains a pointer to TThe~e are the variables the CLP system may encounter, a stack \nelement containing a delayed constraint. Corresponding to each occurrence node is a re\u00adverse point er \nfrom the stack element to the occur\u00adrence node. Call the list associated with a dynamic wakeup condition \nD W a D W-list, and call each node in the list a DW-occurrence node. Initially the access structure is \nempty. The fol\u00adlowing specifies what is done for the basic oper\u00adations. It is assumed, without loss of \ngeneral\u00adity, that the variables in the wakeup system are disjoint from runtime variables, and that no \nexis\u00adtential variable appears in more than one generic wakeup condition. 4.1 Delaying a new hard constraint \nTo delay a new hard constraint C, first push a new stack element for C. Let D denote its wakeup de\u00adgree \nand WI, . . . . W. denote the generic wakeup conditions of D. Thus C is ZXl for some 9. Then: For each \nWi, compute the dynamic wakeup condition D Wi corresponding to C and Wi, that is, DWi is W#.  For each \nDWi, insert into the DW~-list of the access structure a new occurrence node point\u00ading to the stack element \nC.  o Set up reverse pointers from C to the new oc\u00adcurrence nodes. 4.2 Processing an Entailed Constraint \nSuppose there is anew entailed constraint, say X = 5. Then: Obtain the dynamic wakeup conditions in the \naccess structure whose instances are implied by X = 5. If no such conditions exist (i.e. no delayed constraint \nis affected by X = 5 being entailed), not hing more needs to be done. 8The total number of occurrence \nnodes is generally larger than the number of delayed constraints. Consider the lists L associated with \nthe above conditions. Then consider in turn each delayed constraint pointed to by the occurrence nodes \nin L. For each such constraint C, perform the following. o Delete all occurrence nodes pointed to by \n c. o Construct the reduced form C) of C by re\u00adplacing all occurrences of X by 5. (Recall that in general, \nC is obtained by conjoin\u00ading C with the entailed constraint. ) Now push C onto the stack, set up a pointer \nfrom C to C, and then perform the mod\u00adifications to the access structure as de\u00adscribed above when a new \ndelayed con\u00adstraint is pushed.  Figure 3 illustrates the entire runtime structure after the two hard \nconstraints pow(X, Y, Z) and pow(Y, X, Y) were stored, in this order. Figure 4 il\u00adlustrates the structure \nafter a new input constraint makes X = 5 entailed.  4.3 Backtracking Restoring the stack during backtracking \nis easy be\u00adcause it only requires a series of pops. Restoring the access structure, however, is not so \nstraightfor\u00adward because no trailing/saving of the changes was performed. In more detail, the primitive \noperation of backtracking is the following: Pop the stack, and let C denote the constraint just popped. \nb Delete all occurrence nodes pointed to by C. If there is no pointer from C (and so it was a hard constraint \nthat was newly delayed) to another constraint deeper in the stack, then nothing more need be done.  \nIf there is a pointer from C to another con\u00adstraint C (and so C is the reduced form of  Al A2M A4A5A6A7Ml \nA9 C2 pow(Y,x,Y) c1 pow(x,Y,z) Al:x=o A2:X=1 Legend: &#38;l ; AS:Y=l :-AB:Z=l { Figure3: The C ), then \nperform the modifications to the ac\u00adcess structure as though C were being pushed onto the stack. These \nmodifications, described above, involve computing the dynamic wakeup conditions pertinent to C , inserting \noccur\u00adrence nodes, and setting up reverse pointers. Note that the access structure obtained in back\u00adtracking \nmay not be structurally the same as that of the previous state. What is important, however, is that it \ndepicts the same logical structure as that of the previous state. 4.4 Optimization Additional efficiency \ncan be obtained by not creat\u00ading a new stack element for a reduced constraint if there is no choice point \n(backtrack point) between the changed degrees in question. This saves space, saves pops, and makes updates \nto the access struc\u00adture more efficient. Another optimization is to save the sublist of oc\u00adcurrence nodes \ndeleted as a result of changing the degree of a constraint. Upon backtracking, such sublists can be inserted \ninto the access structure in constant time. This optimization, however, sacri\u00adfices space for time. A3:X=CYACY#OACY#l \nAtj:Y =ff Acr#OAcY#l A~:Z=CKACY#OAa#l access structure A third optimization is to merge ZVV-lists. Let \nthere be lists corresponding to the dynamic wakeup conditions Z)W1, . . . . DWn. These lists can be merged \ninto one list with the condition DW if the push of any delayed constraint C results in either (a) no \nchange in any of then lists, or (b) every list has a new occurrence node pointing to c; for every constraint \nC and for every mapping 6 of met a-const ants into constants, C implies DW6 iff C implies DW# for some \n1< i < n. In the example of figure 3, the three lists involving X can be merged into one list which \nis associated wit h the dynamic wakeup conditions X = cr. Sim\u00ad ilarly for Y and Z.  4.5 Summary of \nthe Runtime Structure A stack is used to store delayed constraints and their reduced forms. An access \nstructure maps a finite number of dynamic wakeup constraints to lists of delayed constraints. The constraint \nsolver is assumed to identify those conditions for which an entailed constraint is an instance. The basic \noperations are then implemented as follows. A4A5A6A7 A8 A9 L C4 pow(5,Y,z) t, I C3 pow(Y,5,Y) I 1- C2 \n1 pow(Y,x,Y) I cl pow(x,Y,z) I I AA:Y=O AF):Y=l A6:Y=cx AcY#OA~#l Legend: A7:Z=0 As:Z=l Ag:Z=a A(X#OA~#l \n{ Figure 4: The new access structure 1, Adding a new constraint C simply involves a upgrading the degree \nof one delayed constraint, in\u00adpush on the stack, creating new occurrence eluding awakening the constraint, \nand undoing the nodes corresponding to C and the setting of delay/upgrade of one hard constraint) is \nbounded pointers between the new stack and occurrence by the (fixed) size of the underlying wakeup system. \nnodes. The cost here is bounded by the num-The total cost of an operation (delaying a new hard ber of \ngeneric wakeup conditions associated constraint, processing an entailed constraint, back\u00adwith (the degree \nof) C. tracking) on delayed constraints is proportional to the size of the delayed constraints affected \nby the 2. Changing the degree of a constraint C involves operation. a push of a new constraint C , deleting \nand in\u00adserting a number of occurrence nodes. Since the occurrence nodes are doubly-linked, each such \ninsertion and deletion can be done in con-S Concluding Discussion stant time. Therefore the total cost \nhere is bounded by the number of generic wakeup con- A framework of wakeup degrees is developed to ditions \nassociated with C and C . specify the organization of a constraint solver. 3. Similarly, the cost in \nbacktracking of popping These degrees represent the various different cases a node C, which may be the \nreduced form of of a delayed constraint which should be treated another constraint C , involves deleting \nand in-differently for efficiency reasons. Associated with serting a number of occurrence nodes. The \neach degree is a number of wakeup conditions which cost here is again bounded by the number of specify \nwhen an input constraint changes the de\u00adgeneric wakeup conditions associated with C gree of a hard constraint. \nWhat is intended is that and C . the wakeup conditions represent all the situations in which the constraint \nsolver can efficiently up\u00addate its knowledge about how far each delayed con- In short, the cost of one \nprimitive operation on de\u00adst raint is from being fully awoken. layed constraints (delaying a new hard \nconstraint, The second part of this paper described a run\u00adtime structure for managing delayed constraints. \nA stack is used to represent the current collection of delayed constraints. It is organized so that it \nalso records the chronological order of all changes made to this collection. These changes appear in \nthe form of inserting a new delayed constraint, as well as changing the degree of an existing de\u00adlayed \nconstraint. An access structure is designed to quickly locate all delayed constraints affected by an \nentailed constraint. By an appropriate in\u00adterconnection of pointers between the stack and the table, \nthere is no need to save/trail changes made in the structure. Inst cad, a simple process of inserting \nor deleting nodes, and of redirecting pointers, is all that is required in the event of back\u00adtracking. \nBy adopting this technique of performing minimal trailing, the speed of forward execution is enhanced, \nand space is saved, at the expense of some reconstruction in the event of backtracking. Even so, the \noverall overhead cost of the runtime structure for managing an operation to the delayed constraints is, \nin some sense, minimal. Finally we remark that the implement a\u00adtion technique described has been used \nin the CLP( R) system for delaying hard constraints such as multiply and pow. Acknowledgement. We thank \nNevin Heintze and Michael Maher for their comments. References [1] Arvind and D.E. Culler, Dataflow \nArchitec\u00adtures , in Annual Reviews in Computer Science, Vol. 1, Annual Reviews Inc., Palo Alto (1986), \npp 225-253. [2] Arvind, R.S. Nikhil and K.K. Pingali, I-Structures: Data Structures for Parallel Comput\u00ading \n, ACM Transactions on Programming Lan\u00adguages and Systems, 11(4) (October 1989) pp 598-632. [3] M. Carlsson, \nFreeze, Indexing and other Imple\u00admentation Issues in the WAM , Proceedings 4th International Conference \non Logic Programming, MIT Press (June 1987), 40-58. [4] A. Colmerauer, PROLOG II Reference Manual &#38; \nTheoretical Model, Internal Report, Groupe Intelligence Artificielle, University Aix -Marseille II (October \n1982). [5] A. Colmerauer, PROLOG-III Reference and Users Manual, Version 1.1 , PrologIA, Marseilles (1990), \n[See also Opening the PROLOG-III Uni\u00ad verse , BYTE Magazine (August 1987).] [6] M. Dincbas, P. Van Hentenryck, \nH. Simonis and A. Aggoun, The Constraint Logic Programming Language CHIP , Proceedings of the 2nd Interna\u00adtional \nConference on Fifth Generation Computer Systems, Tokyo (November 1988), pp 249-264. [7] J. Jaffar and \nJ-L. Lassez, Constraint Logic Pro\u00adth A (JM symposium gramming , Proceedings 14 on Principles of Programming \nLanguages, Mu\u00adnich (January 1987), pp 11 1 1 19. [Full version: Technical Report 86/73, Dept. of Computer \nSci\u00adence, Monash University, June 1986] [8] J. Jaffar and S. Michaylov, Methodology and Implementation \nof a CLP System , Proceedings 4th International Conference on Logic Program\u00ad ming, MIT Press (June 1987), \npp 196 218. [9] J. Jaffar, S. Michaylov, P.J. Stuckey and R.H.C. Yap, The CLP(7?) Language and System \n, IBM Research Report, RC 16292 (#72336), (Novem\u00ad ber 1990). [10] L, Naish, Negation and Control in Prolog \n, Technical Report 85/12, Department of Com\u00ad puter Science, University of Melbourne (1985). [11] G.L. \nSteele, The Implementation and Definition of a Computer Programming Language Based on Constraints , Ph.D. \nDissertation (MIT-AI TR 595), Dept. of Electrical Engineering and Com\u00ad puter Science, M.I.T. [12] E.Y. \nShapiro, The Family of Concurrent Logic Programming Languages , ACM Computing Sur\u00ad veys, 21(3) (September \n1989), pp 412 510. [13] J.A. Thorn and J. Zobel (Eds), NU-PROLOG Reference Manual -Version 1.3 Technical \nRe\u00adport 86/10, Dept. of Computer Science, Univer\u00adsity of Melbourne (1986) [revised in 1988]. [14] D.H.D. \nWarren, An Abstract PROLOG Instruc\u00ad tion Set , Technical note 309, AI Center, SRI In\u00adternational, Menlo \nPark (October 1983).  \n\t\t\t", "proc_id": "113445", "abstract": "", "authors": [{"name": "Joxan Jaffar", "author_profile_id": "81100285914", "affiliation": "IBM T. J. Watson Research Ctr., P. O. Box 704, Yorktown Heights, NY", "person_id": "PP39035997", "email_address": "", "orcid_id": ""}, {"name": "Spiro Michaylov", "author_profile_id": "81100197077", "affiliation": "School of Computer Science, Carnegie Mellon University, Pittsburgh, PA", "person_id": "P266380", "email_address": "", "orcid_id": ""}, {"name": "Roland H. C. Yap", "author_profile_id": "81100576891", "affiliation": "IBM T. J. Watson Research Ctr. P. O. Box 704, Yorktown Heights, NY", "person_id": "PP15036693", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/113445.113471", "year": "1991", "article_id": "113471", "conference": "PLDI", "title": "A methodology for managing hard constraints in CLP systems", "url": "http://dl.acm.org/citation.cfm?id=113471"}