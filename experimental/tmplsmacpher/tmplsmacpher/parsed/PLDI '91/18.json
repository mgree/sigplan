{"article_publication_date": "05-01-1991", "fulltext": "\n The Marion System for Retargetable Instruction Scheduling * David G. Bradlee Robert R. Henry t Susan \nDepartment of Computer Science and Engineering, University of Washington, Seattle, Washington J. Eggers \nFR-35 98195 Abstract Marion is a retargetable code generator system de\u00adsigned specifically for RISCS. \nEach code generator is built from a machine description that includes code selection and code scheduling \ninformation in a concise and readable format. The description language is designed to be easy to use, \nyet rich enough to support a broad range of RISCS. We have used Marion to produce code generators contain\u00ading \ngood instruction schedulers for the Motorola 88000, the MIPS R2000 and the Intel i860. Given this enabling \ntechnology, we have experimented with alternative archi\u00adtectures and different strategies for instruction \nscheduling and register allocation. This paper describes the Marion system and machine description language, \nwith particular emphasis on instruction scheduling, 1 Introduction Retargetable code generators developed \nin the last decade have focused on instruction selection for com\u00adplex instruction set computer (CISC) \narchitectures, such as the VAX. Since these architectures implement the most common operations many different \nways and hide pipelining details and other feature costs, their companion code generators concentrated \non machine specifications that allow instructions to be selected by pattern matching [AGH+84]. These \ncode genera\u00adtors could ignore instruction scheduling and, in many cases, did not perform global register \nallocation. Re\u00adduced instruction set computer (RISC) architectures, however, implement most operations \nonly one way and expose pipeline and functional unit cost to the code generator. Retargetability remains \nimportant *This research was supported in part by NSF Research Grant number CGR-88-01 S06, NSF PYI Award \nnumber MIP\u00ad9058-439 and IBM Contract number 18830046. t Author s current address: Tera Computer, 400 \nN 34th Street, Seattle, WA 98103 Permission to copy without fee all or part of this material is granted \nprovided that the copies are not made or distributed for direct commercial advantage, the ACM copyright \nnotice and the title of the publication and its date appear, and notice is given that copying is by permission \nof the Aaaociation for Computing Machinery. To copy otherwise, or to republish, requires a fee andlor \nspecific permission. @ 1991 ACM 0-89791 -428 -7/91 /0005 /0229 . ..$1 !ifI . ---.- -... . ---- Proceedings \nof the ACM SIGPLAN 91 Conference on Programming Language Design and Implemantation. Toronto, Ontario, \nCanada, June 26-28, 1991. 229 for RISC code generators, but the emphasis must be on instruction scheduling \nand register allocation, rather than instruction selection. The Marion Code Generator Construction Sys\u00adtem \nis a retargetable code generator system de\u00adsigned specifically for uniprocessor RISCS that con\u00adtain multiple \nfunctional units and multi-cycle oper\u00adat ions. Each code generator is constructed from a natural machine \ndescription and performs instruction selection, instruction scheduling and global register allocation. \nThe description language is easy to use, yet provides enough constructs to support a broad range of RISCS. \nMarion has been used to produce code genera\u00adtors for the Motorola 88000 [Mot88], the MIPS R2000 [Kan87] \nand the Intel i860 [Int89]. The 88000 and the R2000 are traditional RISCS that are more similar than \ndifferent. In contrast the i860 can issue two in\u00adstructions per cycle and has explicitly advanced float\u00ading \npoint pipelines, making it a challenging target. Marion is probably the first retargetable code gener\u00adator \nsystem that includes quality instruction schedul\u00ading and can model machines in the i860 s class. Our \nresearch with Marion shows that instruction scheduling can be performed by a retargetable com\u00adpiler without \nsacrificing code efficiency. Nevertheless, many RISCS have idiosyncrasies that require special handling; \nthis induced us to include an escape mech\u00adanism to produce instruction sequences and a con\u00adstruct to \noverride normal operation latencies. Ad\u00additionally, we added two complicated mechanisms to take advantage \nof the most important features of the i860)s floating point pipeline. The Marion system separates the \nmajor por\u00adtions of the code generator, so that different sched\u00adulers, register allocators and strategies \nfor commu\u00adnicantion bet ween scheduling and register allocation can be coupled with different architectures. \nWe have implemented three strategies and compared them on various architectures. DetaiIs appear elsewhere \n[BEH91b, BEH91a, Bra91]. This paper describes the Marion system and de\u00adscription language. Section 2 \ndiscusses the Marion system, along with previous work. Section 3 explores the important features of the \nmachine description lan\u00adguage. Section 4 describes instruction scheduling. Section 5 evaluates the system. \nSection 6 concludes. 2 The Marion System Marion comprises three components: the code generator generator, \nthe compiler front end and com\u00adpiler back end. The input to the code generator gen\u00aderator is a machine \ndescription that describes the tar\u00adget s registers, functional units, pipeline stages and instructions, \nincluding scheduling properties; the out\u00adput is a set of tables and routines. The Lcc front end [FH90] \nconsumes ANSI-C and generates an interme\u00addiate language (IL) of directed acyclic graphs built from typed \nlow-level operators. The back end trans\u00adforms the IL, according to transformations specified in the machine \ndescription, performs instruction se\u00adlection by pattern matching and then gives control to the code generation \nstrategy. The code generation strategy directs the invo\u00adcation of and level of communication between \nin\u00adstruction scheduling and global register allocation. The scheduling algorithm and heuristics are part \nof the strategy; the register allocator, code DAG builder and scheduling support (which handles the low-level \nscheduling details), are strategy-and target\u00adindependent and provide access to target-dependent data. \nThe separation of the strategy from the rest of the code generator allows strategies to be replaced quickly, \nwithout changing the other components or the target-dependentlindependent interface. We have implemented \nthree strategies, The first, Postpass [GM86], performs global register allocation followed by instruction \nscheduling. The second, In\u00adtegrated Prepass Scheduling (IPS) [G H88], schedules with a limit on local \nregister use, performs global reg\u00adister allocation and then schedules again. The third, Register Allocation \nwith Schedule Estimates (RASE) [BEH91b], invokes the scheduler to gather schedule cost estimates, performs \nglobal register allocation and then does final scheduling. Marion makes several assumptions about the \ntar\u00adget architecture. First, the architecture must have general purpose register sets for integer, addressing \nand floating point operations. Second, the schedul\u00ading requirements for each instruction, including which \nfunctional units and pipeline stages are used on each cycle, must be known at compile time. Third, only \nload and store operations may access memory. RISCS generally comply with these assumptions. Machines \nwith hidden pipelines, such as the VAX, could be modeled using Marion by supplying null scheduling information \nand using a null scheduler. Marion employs simple algorithms whenever pos\u00adsible, particularly in areas \nthat are well understood. For example, we implemented most algorithms with lists. Although this slows \ncompilation time, it has permitted us to concentrate our efforts on that which was necessary to produce \ngood code for RISCS. 2.1 Instruction Selection Marion performs instruction selection before con\u00adtrol \nis given to the code generation strategy. The se\u00adlector matches the IL with a recursive-descent brute\u00adforce \ntree pattern matcher. Patterns are derived from the machine description by the code generator gen\u00aderat \nor. The matcher examines the patterns in the order given, selecting the first one that matches and then \nattempts to match the subtrees. If unsuccess\u00adful, it proceeds to the next pattern. After matching, Marion \ngenerates the code stream with a left-to-right bottom-up walk. During code selection pseudo-registers \nare created for all expression temporaries. User variables that may reside in registers and local common \nsubex\u00adpressions are also represented by pseudo-registers. Pseudo-registers are later mapped to physical \nregis\u00adters by the register allocator. Local pseudo-registers refer to registers that are live in only \none basic block. Registers live in more than one block are known as globs! pseudo-registers. A simple \ncode selector that is separate from other code generation phases can yield inefficient code for CISCS. \nThis is because arithmetic and logical oper\u00adations may be performed on memory operands and because many \nIL constructs can be mapped to more than one instruction sequence. On RISCS, however, operands may not \nreside in memory and most IL con\u00adstructs can be mapped to only one instruction se\u00adquence. Therefore, \nsince the code selector s choices are limited, a simple scheme that is separate from reg\u00adist er allocation \ncauses few inefficiencies, 1 Addressing expressions do require the selector to make choices, but the \nchoices are simple enough to be managed with an ordered pattern list. To accommodate local common subexpressions \nwithin a naive tree pattern matcher, an IL node with more than one parent is forced into a register, \nunless it is a constant that can be subsumed by an address\u00ading mode or an immediate operand. 10n some \nRISCS, notably the i860, there is a greater poten\u00adtial for inefficiency, because the code selector has \nmore instruc\u00adtion sequence choices, Nevertheless, there are still significantly fewer than on most CISCS. \n 2.2 Global Register Allocation Marion uses a graph coloring global register al\u00adlocator based on the \nwork of Chaitin [Cha82] and Briggs et aL [BCKT89]. Graph coloring models the register allocation problem \nas an interference graph in which nodes represent pseudo-registers and edges rep\u00adresent interferences \nbetween them. Pseudo-registers that interfere cannot share the same register. By as\u00adsigning different \ncolors to interfering nodes, coloring the graph is analogous to assigning physical regis\u00adters to pseudo-registers. \nThe register allocator de\u00adtermines interference using the instruction order pre\u00adsented to it. Chaitin \ns approach is widely used, pro\u00adduces good results and is general enough to support different code generation \nstrategies. Its disadvantage is that a pseudo-register is either assigned a physical register or spilled \nfor its entire lifetime. A method that splits a pseudo-register s lifetime [CH84] may be a profitable \nalternative. All three strategies that we have implemented em\u00adploy the same coloring algorithm. RASE \ndiffers from the others in that local pseudo-registers are replaced in the interference graph by pseudo-registers \nthat rep\u00adresent local register usage within each basic block. 2.3 Previous Work We designed yet another \nretargetable code gen\u00aderator because no existing system has a machine de\u00adscription language that incorporates \nscheduling con\u00adstraints or builds schedulers from the description, The ISP processor description language \n[BN71] contains more imperative detail than necessary and does not directly give the scheduling properties \nof each instruction. Henry s CODEGEN system [Hen87] and Davidson and Fraser s PO system [DF80] (and successors) \ntackle the code selection problem for CISCS, but incorporate no scheduling information. Bird s B# system \n[Bir87] merges instruction selec\u00adtion and scheduling into an L-attributed Graham\u00adGlanville-style code \ngenerator. Bird s model takes only trees of limited width, not DAGs, and assumes that all instructions \nuse the pipeline identically. Wall and Powell s Mahler system [WP87] uses scheduling information but \ndoes not have an explicit machine description language. Mahler uses an assembly lan\u00adguage for a simple \nvirtual machine with an infinite number of registers and no pipeline. The schedul\u00ading details are contained \nin the Mahler translator; changing the target architecture requires changing the translator. Hwu and \nChang [HC88] built a sys\u00adtem to perform architectural studies, which focused primarily on varying instruction \nissue rates, latencies and functional unit structures. They did not give in\u00adformation on the machine \ndescriptions. The GNU C compiler [Sta89] is a successful retar\u00adgetable compiler that uses interpreted \nmachine de\u00adscriptions resembling Davidson and Fraser s PO. Al\u00adthough the system has been targeted to \nRISCS, the machine description contains no scheduling informa\u00adtion; prototype schedulers have been written \nby hand. Landskov et al. [LDSM80] describe a machine model to support microcode compaction. In their \nmodel, each micro-operation is described by a 6-tuple indicating the name, inputs, output, function, \nre\u00adsources needs, clock phase and instruction fields for khe operation. The model is flexible enough \nto handle a number of compaction problems, but is not directly applicable to our task, because it assumes \neach opera\u00adtion has single-cycle latency and lacks the information necessary for code selection. 3 l!he \nMachine Description Language Marion incorporates ideas from ISP, CODEGEN, the PO derivatives and the \nmicrocode work. The ma\u00adchine description language, Maril, requires processor attributes to be declared \nexplicitly, in a flavor sim\u00adilar to ISP7S. Like PO, each machine instruction in a Maril description is \nlisted along with an expression that is used to match the IL. Like the tree trans\u00adformation system in \nCODEGEN, Marion employs a transformation phase, prior to code selection, to facil\u00aditate the IL-to-target-machine \nmapping. Marion also borrows from the microcode compaction model in its specification of resource requirements \nand restrictions on instruction word packing.2 A machine description comprises three sections: Declare \nspecifies features of the architecture, such as registers, pipeline stages and functional units; Cwvm \n(Compiler Writer s Virtual Machine) describes a sim\u00adple runtime model, mostly for parameter binding; \nIn\u00adstm lists each machine instruction with its scheduling properties, and includes IL transformations. \nMost aspects of Maril are straight-forward, De\u00adclare, Cwvm and instruction directives follow di\u00adrectly \nfrom an architectural description, such as a programmer s reference manual. Most scheduling properties \ncan be expressed simply, as operation la\u00adtencies and pipeline requirements associated with in\u00adstructions. \nMaril s innovation is in the handling of architectural details: the special latency specifica\u00adtions, \nthe escape functions, the instruction packing restrictions and the support for explicitly advanced pipelines. \nThe significance of Maril is that a user can write a basic machine description in a short time and then \nconcentrate on tuning the description with Maril s special constructs to produce efficient code. z~~truction \n~~~d paCking refers to the process of fitting individual operations into an instruction word. declare \n{ %reg r[O:7] (int); / *Integer reg.*/ %reg d[O:3] (double); / *Double float regs / %equiv r[O] d[O]; \n/*d regs overlap r regs */ %resource IF; ID; IE;IA;IW; / *fetch; decode; execute; access mern; writeback \n*/ %resource F1;F2;F3;F4;F5; / Floating add pipe*/ %def const16 [ 32768:32767]; / *signed immediate*/ \n%Iabel rlab [ 32768:32767] +relative ; / Branch oflset*/ %memory m[O:2147483647]; } Figure 1: TOYP declarations \nTo illustrate Maril, we use a toy processor (TOYP) that supports five operations: load, store, add, compare \nand branch. TOYP has eight 32-bit general purpose registers that can be used in 64-bit pairs to support \ndouble precision floating point. It contains a 5-stage instruction pipeline and a 5-stage floating point \nadd pipeline. We assume that the source language is similarly limited. 3.1 Declarations The Declare \nsection (Figure 1) describes the tar\u00adget s registers, memory and resources and defines im\u00admediate operands. \nItems referenced in subsequent sections are declared here. Each %reg declaration describes an array of \nregis\u00adters, along with the datatypes that can reside therein. Maril supports the signed C Language native \ntypes. The size of each register is inferred from the size of the largest type it may hold. For TOYP, \nthe r regis\u00adters hold integer values and the d registers hold dou\u00adble float values. The %equiv directive \nindicates that the d registers overlay the r registers; one d register overlays two r registers. A Yoresource \ndeclaration specifies processor re\u00adsources. A resource is a pipeline stage, data bus or other similar \nhardware. TOYP s resources include the floating point add and instruction pipeline stages. A Yodef declaration \nspecifies an integer constant range for use as an instruction s immediate operand. On TOYP the range \neonstl 6 is used by load, store and add instructions. A yolabel declaration is similar to a ~odef, but \nis used for branch offsets. Optional flags may appear on both of these declarations. For example, +relative \nindicates that rlab is a relative branch offset. Other flags permit %odefs to match all or part of relocatable \naddresses. Memory banks may be declared using the Yomemory directive. An instruction indicates it ref\u00aderences \nmemory by using the memory object as an cwvm { %general (int ) r; /*r gpr for int*/ %general (double) \nd; / d gpr for double j %allocable r[l:5]; / *register allocator*/ %calleesave r[4:7]; / *saved by callee*/ \n%SP r[7] +down; / *stack pointer*/ %fp 1[6] +down; I *frame pointer*/ %retaddr r[l]; / *return adclress+l \n%hard r[O] O; I *r[O] always 0 / %arg (int ) r[2] 1; I *lst int arg in r[2]*/ %arg (int ) r[3] 2; I *2nd \nint arg in r[3]*/ %arg (double) d[l] 1; / *lst double arg in d[l]*f %result r[2] (int ); / *Int result \nin r[2]*/ %result d[l] (double); [ *Double result in d[l]*/ } Figure 2: TOYP Compiler Writer s Virtual \nMachine array indexed by the effective address. 3.2 Runtime Model The Cwvm section (Figure 2) specifies \nthe run\u00adtime model to which the generated code must con\u00adform. Directives specify which register sets \nare for general purpose use ( %general), which are available for allocation by the global register allocator \n(910a1\u00adlocable), which are preserved across function calls ( %calleesave) and which registers are used \nfor the stack pointer, frame pointer and return address. On TOYP, the r and d register sets are general \npurpose. Either two integer parameters or one double float pa\u00adrameter may be passed in registers ( %arg); \nthe result is returned in r[2], if integer, or d[lj, if double (%re\u00adsult). Registers rfl] through r[5] \nmay be allocated by the global register allocator. Marion requires the user to specify two registers \nto be the stack and frame pointers, Optionally a third register may be declared as a global data pointer. \nMarion supports only relatively simple CWVMS. More complex CWVMS, such as those that use a vir\u00adtual frame \npointer, those with specialized calling con\u00adventions or those that support register windows, can\u00adnot \nbe handled with the primitives that we have im\u00adplemented. The Cwvm section is a bridge to allow us to \npursue the primary focus of our work. Thus, we kept the stack model simple to retain flexibility in register \nspilling and support for a variety of code generation strategies. 3.3 Instructions The Instr section \n(Figure 3) lists machine instruc\u00adtions, special instructions and glue transformations. The description \nfor each instruction indicates its pur\u00adpose and scheduling requirements. Glue transforma\u00ad %instr add \nr, r, r (int ) {$1 = $2+ $3;} [IF; ID; IE; IA; IW;] (1,1,0) %instr add r, r[O], #const16 (int ) {$1 = \n$3;} [IF; ID; IE; IA; IW;] (1,1,0) %instr cmp r, r, r (int ) {$1 = $2:: $3;} [IF; ID; IE; IA; lW;] (1,1,0) \n %instr fadd.d d, d, d {$1 = $2+ $3;] [IF; ID; F1,ID; F~;F2;F3;F4;F5; IW,F5: IW:l (1,6,0) %instr beqO \nr, #rlab {if ($1 == O) goto $2;} [IF; ID; IE;] (1,2,1) %instr ld r, r, #const16 {$1 = m[$2+$3];} [IF; \nID; IE; IA; IW;] (1,3,0) %instr st r, r, #const16 {m[$2+$3] = $1;} [IF; ID; IE; IA; IW;] (1,1,0) / *single \nreg move, referenced by moud*/ %move [s.movs] add r, r, r[O] {$1 = $2;} [IF; ID; IE; IA; IW;] (1,1,0) \n/~ *func escape: double reg moue (~-in.strs) */ %move *movd d, d {$1 = $2;} [1 (0,0,0) / *auxiliary \nlatency for instruction pair*/ %aux fadd.d : st.d (1.$1 ==2.$1) (7) /*glue transformation for ~ompare \n*/ %glue r, r {($1 === $2) ==> (($1 :: $2) == o);} Figure 3: TOYP instructions, %aux overrides the \nnor\u00ad mal latency for an edge, if the first operands of both instructions are the same. The glue transformation \nex\u00adpands the == into a generic compare : : and an == . %move indicates how to move within a register \nset, tions help complete the mapping between the IL and the target s instruction set. Each %instr directive \ndescribes a single machine instruction in five parts. The first is the instruction mnemonic and operands. \nThe second (in parenthe\u00adses) is an optional type constraint that is used during instruction selection. \nThe third part (in braces) is a single assignment C expression (excluding side effects and conditional \noperators) that indicates what oper\u00adation the instruction performs. Expression operands are integer constants \nor references to the instruct ion s operands, e.g. $1. The patterns used by the matcher are derived from \nthese expressions. The fourth part of a directive (in brackets) speci\u00adties the hardware resources needed \nby the instruction on each cycle after it is issued. Semicolons separate cycles; commas separate resources \nwithin a cycle. For example, in Figure 3, the add instruction uses the in\u00ad struction pipeline, one stage \nper cycle; the fadd. d in\u00ad struction requires the instruction decode stage (ID) and the first floating \npoint adder stage (Fl) each for two cycles, using both on the third cycle. Finally, the instruction directive \nincludes a triple (cost, !atency,s!ots). Cost is used only to distinguish actual instructions from zero \ncost dummy instruc\u00ad tions, which are useful for some type conversions. La\u00ad tency k the number of cycles \nbefore the instruction s result can be used by another instruction. Usually the latency is the number \nof cycles an instruction spends in execution stages, i.e. stages other than fetch, de\u00ad code or writeback. \nThe presence or absence of by\u00ad pass hardware, however, can affect this relationship. Therefore, we require \nthe user to indicate the latency directly, at the risk of some redundancy. Slots speci\u00ad fies the number \nof delay slots following the instruction that must be filled by the scheduler. It is typically used for \ncall and branch instructions, but can also be used for other instructions, such as a delayed load whose \ndelay slot must be filled. A positive slots value indicates that instructions in the delay slots are \nal\u00ad ways executed. A negative value indicates that they are executed only if the branch is taken. A zero \nvalue means that they are executed only if the branch is not taken, which is the same as having no slots \nat all, Sometimes operation latency depends on the in\u00adteraction between two instructions. For these situa\u00adtions \nthe user employs an auxiliary latency directive (%amr), which overrides the normal latency associ\u00adated \nwith the first instruction in the pair. For exam\u00adple, on TOYP an fadd. d usually has a 6-cycle latency. \nHowever, if the result is a stored to memory then the latency is 7 cycles. Maril includes several built-in \nfunctions for use in instruction expressions and transformations. The built-in functions high and low \nare typically used to split a 32-bit immediate into 16-bit halves. Evai eval\u00aduates a constant expression, \ntypically to negate or decrement an immediate; it may appear only within a glue transformation. Datatypes \nare used as built\u00ad ins to match type conversions in the IL. 3.4 Mapping the IL A target-independent \nIL rarely maps directly onto the machine s instruction set. Marion supports two mechanisms that enable \nthe user to complete this mapping, First, the *func escape mechanism allows the user to write a C function \nto produce a sequence of individually schedulable instructions. The user\u00adwritten functions comprise only \ncalls to routines that are exported by Marion; these routines create and manipulate operands and generate \ninstructions. For example, on TOYP a move between d registers maps into two moves between r registers. \nThe user-written function for *movd (not shown) creates operands to represent the two halves of each \nd register and then generates the two single move instructions. The op\u00adtional label [s.movs] on the single \nmove instruction di\u00adrective (see Figure 3) enables the user function to ref\u00aderence that instruction. \n~uncs are particularly useful for generating in-line code for machines without di\u00advide or double load, \nstore or move instructions. The second mechanism is the glue transformation, a tree-to-tree transformation \nthat is applied to the IL prior to code selection. For example, to compare and branch on TOYP requires \na generic compare instruc\u00adtion followed by a condit ional branch. The glue trans\u00adformation shown in Figure \n3 transforms the == op\u00aderator into a generic compare operator : : followed by an == operator. Of the \ntwo constructs, the glue transformation is easier to use and handles most cases. It is generally employed \nwhen an IL operator can be mapped to two or three machine instructions without the complica\u00adtion of half-register \nreferences. Typical uses are for comparisons, conversions or 32-bit immediate. The junc mechanism handles \ntwo more complicated map\u00adping situations: transformations that must manipu\u00adlate register halves and transformations \nthat contain common sub expressions. A glue transformation is in\u00adappropriate for these cases, because \nit is applied be\u00adfore pseudo-registers are created and because it is a tree-to-tree, not a tree-to-DAG \ntransformation. The benefit of the *junc escape mechanism is that it allows the user to map complicated \nIL constructs without sacrificing opportunities for efficient generated code. 4 Instruction Scheduling \n 4.1 Code DAG The primary data structure that supports scheduling is the code DA G, in which nodes repre\u00adsent \ninstructions and directed labeled edges repre\u00adsent dependence between instructions. The DAG is threaded \nby a code thread, which forms a topolog\u00adical sort and represents the initial order of the ba\u00adsic block \ns instructions. The roots of the DAG are instructions that are dependent on no other instruc\u00adtions. An \nedge (x, y) with label i means that y cannot be scheduled fewer than i cycles after x without caus\u00ading \na data hazard, a pipeline delay caused when data is not yet available, or violating the program s seman\u00adtics. \nEdges are of three types.3 A type 1 edge (x, y) represents a true dependence and is introduced if one \nof y s source operands is ~ s result. (The edge label is z s latency.) Type 2 edges ensure that memory \nreferences are ordered properly. Type 3 edges, which are anti-dependences, are used by some code gener\u00adation \nstrategies to ensure that separate uses of the same physical register do not overlap. The strategy controls \nthe inclusion of each edge type, permitting Marion to support strategies that do not require all types, \nsuch as Hennessy and Gross s method [HG83]. Although the IL DAG provides an initial template for the \ncode DAG, Marion separates code DAG con\u00adstruction from code selection, because (1) only type 1 edges \nappear in the IL DAG and (2) the code DAG is difficult to keep up-to-date when spill code is inserted \nby the register allocator. 4.2 Scheduling Algorithms The most common approach to instruction scheduling \nis list scheduling [Tho67, LDSM80, HG83, FERN84, GM86, GH88]. Given a code DAG, the scheduler keeps a \nlist of instructions that are ready to be scheduled without causing a delay. On each it\u00aderation it uses \na heuristic to select a ready instruction to schedule and then updates the list. In the general case, \nthis approach has worst-case running time of O(e), where e is the number of edges in the DAG, but the \nheuristic can increase the complexity. A list scheduler typically selects the highest pri\u00adority node \nin the ready list. A common heuristic for assigning priority is the maximum dist ante along any path \nfrom the instruction node to a leaf in the code DAG, The philosophy is that the node farthest from completion \nis the most critical; less important nodes can be scheduled later. The code generation strategies we \nhave imple\u00admented all use list scheduling algorithms with the maximum distance heuristic. With minor \nadditions to the scheduling support routines, other scheduling algorithms, such as critical path and \nbranch and bound [LDSM80] could be employed. 4.3 Structural Hazards A structural hazard occurs when \ntwo instructions need the same resource on the same cycle. For ex\u00adample, two instructions executing in \nseparate func\u00adtional units may require a register write-back bus on the same cycle, or an instruction \nmay need a pipeline stage for more than one cycle, preventing its use by a subsequent instruction. Avoiding \nthese hazards can yield better code. 3Edge types are not distinguished by the schedulers. %instr adds \nr, r, r {$3 = $1 + $2;} [ALU; WB] (1,1,0) %move fmov.s f, f (float ) {$2 = $1;} [FALU; FWB] (1,1,0) \nFigure 4: Maril directives for i860 integer add and iloat\u00ad ing point move instructions As discussed in \nSection 3.3, the machine descrip\u00adtion indicates each instruction s resource needs. From this information \nMarion constructs a resource vector for each instruction. Each element of the resource vector contains \nall resources needed on a particular cycle. To avoid structural hazards the scheduler com\u00adpares a candidate \ninstruction s resource vector with a resource vector that is the composite of the resources required \nby all currently executing instructions; if the intersection is non-empty, the candidate is not sched\u00aduled. \nThis scheme is computationally simple and easy to implement, but it restricts the scheduler s knowl\u00adedge \nof structural hazards to the current cycle. The information is available to check future hazards, but \nwe decided that the additional processing would not warrant the payback. Resources can be used to control \nmultiple instruc\u00adtion issue for superscalars. The scheduler will sched\u00adule as many instructions on a \ncycle as possible as long as they cause no structural hazards. For example, the i860 allows an integer \nand a floating point instruction to be issued simultaneously. By using a different set of resources for \neach of the two instruction types, the user can model two instructions per cycle. Figure 4 shows integer \nadd and floating point moue instruc\u00adtion directives.4 Since the resources they use differ, one of each \ncan be scheduled per cycle. The fetch and decode stages can be ignored if subsequent resources correctly \ncontrol instruction issue. 4.4 Control Hazards A control hazard is a pipeline delay introduced by a \nbranch instruction when the branch target address is not known at the time the target instruction needs \nto be fetched. Most RISCS attempt to avoid these hazards by using a branch delay mechanism. Instruc\u00adtions \nin delay slots are executed before the branch tar\u00adget instruction, In Maril, the number of delay slots \nis specified in the instruction directive. Marion always fills branch delay slots with nops. Gross and \nHen\u00adnessy s algorithm for filling delay slots [GH82] could 4The actual resources we use in the i860 description \ndiffer from what is shown, because of explicitly advanced pipelines, discussed in Section 4.5. be included \nin Marion as a separate intra-procedural pass after instruction scheduling. 4.5 Classes and Clocks The \ncode DAG and resource vectors provide enough information to schedule instructions for many RISCS, including \nthe R2000 and 88000. In addition, multiple instruction issue can be modeled using ma\u00ad chine resources. \nSome architectures, however, restrict multiple instruction issue or instruction word pack\u00ad ing, or have \nfeatures that cannot be expressed using the Maril constructs introduced so far. The i860 has several \nsuch features. First, it sup\u00adports instructions to perform floating point multiply and add together, \nand has options to feed the results of one floating point pipeline into another, but not all combinations \nof the primitives are legal. Second, the floating point add and multiply pipelines must be explicitly \nadvanced. An explicitly advanced pipeline (EAP) is a pipeline that retains its state until one of a particular \nset of instructions is executed. An ex\u00adample is a multiply pipeline that advances only when a multiply \ninstruction is issued; after a multiply has been launched, the pipeline remains in a suspended state \nuntil the next multiply is issued. To model the i860 floating point unit, we view it as a long instruction \nword in which each field corresponds to one of three multiplier stages called Ml, M2 and M3, three adder \nstages called Al, A2, A3, or the floating point write-back bus F WB. We declare as instructions the individual \npipestage sub\u00adoperations that compose a full operation, as shown in Figure 5(b). For example, to perform \nd6 e d4 * d5, the code selector produces the sequence Ml d4, d5; M2; M3; FWB d6 Ml launches the multiply, \nM2 and M3 advance the pipeline and FWB catches the result. These sub\u00adoperations need not be scheduled \non consecutive cy\u00adcles, because the pipeline is explicitly advanced. In addition, since each sub-operation \nuses only the re\u00adsource corresponding to one field, it may be packed with others to form a long instruction \nword. Two Maril features support the scheduling of sub\u00adoperations. First, to handle irregular packing \nre\u00adstrictions, the user can associate a class with a sub\u00adoperation. The class is the set of all long \ninstruc\u00adtion words in which that sub-operation may appear. The long instruction words are class elements. \nTwo sub-operations may be packed if the intersection of their associated classes is non-empty. The scheduler \nchecks for legality as each sub-operation is packed, For example, the i860 floating point multiply instruc\u00adtion \npfmul is a class element. Classes associated with %clock clk_rn YOreg ml (double; elk-m) +temporal; %reg \nm2 (double; clk.rn) +temporal; %reg m3 (double; clk.m) +temporal; (a) %instr Ml d, d (double; clk_m) \n{ml = $1 * $2;} [Ml] (1,1,0) %instr M2 (double; elk-m) {m2 = ml;} [M2] (1,1,0) %instr M3 (double; elk-m) \n{m3 = m2;} [M3] (1,1,0) %instr FWB d (double; elk-m) {$1 = m3;} [FWB] (1,1,0) (b) Figure 5: (a) Declarations \nfor a multiply clock and its as\u00ad sociated temporal registers. (b) Instruction directives for i860 floating \npoint multiply. Each multiply sub-operation affects cikm. Ml, M2, M3 and FWB all contain pfmzd. Therefore, \nMl and M3, for example, may be packed together, yielding a pfmut long instruction. Second, to allow EAP \nsub-operations to be sep\u00adarated in the schedule, the user defines a clock to keep track of time in a \nparticular EAP. A temporal register based on clock k is a register whose value changes when clock k ticks. \nTemporal registers are typically latches between EAP pipestages. Instruc\u00adtions that advance an EAP, and \ntherefore change the values in its latches, are declared to aflect the EAP s clock. As shown in Figure \n5, the i860 multiply pipeline has three temporal registers, ml, m2 and m3, based on clock elk-m; each \nmultiply sub-operation af\u00adfects c[k-m. Pipelines that advance on every cycle need not be modeled at the \nlatch level, because the sub-operations performed in the stages may not be temporally sep\u00adarated. Within \nan EAP, however, the sub-operations may be separated. Doing so sometimes enables an operation to begin \nearlier than would otherwise be possible, but this requires the scheduler to keep track of the sub-operation \nresults in temporal registers, a process we call temporal scheduling.  4.6 Temporal Scheduling The presence \nof temporal registers can cause dead\u00adlock in a non-backtracking scheduler. To avoid this, Marion modifies \nthe code DAG before scheduling. If a code DAG edge represents a true dependence via a temporal register \nbased on clock k, then Marion marks it as a temporal edge based on k. A sequence of nodes connected by \ntemporal edges is called a tem\u00adporal sequence. When the source sub-operation of a temporal edge has been \nscheduled, but the destina\u00adtion has not, the scheduler is said to be scheduling across a tempora[ edge. \n The temporal edge indicates that the value it rep\u00adresents is ephemeral; to prevent the loss of the value \nthe scheduler adheres to the following rule: Rule 1: If there is a temporal edge (z, y) based on clock \nk, and x has been sched\u00aduled, then an instruction z # y that af\u00adfects k may not be scheduled before y, \nbut may be packed with y. If z were scheduled before y, the value held in the tem\u00adporal register would \nbe lost, because z affects clock k. For example, Ml d4, d5; M2; M3; FWB d6 forms a temporal sequence \nbased on c[k-m. After Ml d4, d5\u00b0 is scheduled, the initial sub-operation in a similar sequence (e.g. \nMl d7, d8\u00b0 ) may not be scheduled before M2, but may be packed with it, since their resources do not \noverlap. After the two sub-operations are packed together and scheduled, the scheduler will be scheduling \nacross two temporal edges. To avoid violating Rule 1, all sub-operations that are destinations of temporal \nedges based on the same clock are bundled into a temporal group. Each temporal group is treated as a \nsingle instruction to be scheduled. In effect, tempo\u00adral grouping pre-packs all sub-operations that must \nbe packed together. Other sub-operations may then be packed with a temporal group. Without any code DAG \nmodification, Rule 1 can cause the scheduler to deadlock. Figure 6 gives an example. (q, r) is a temporal \nedge based on clock k, (p, ? ) is not a temporal edge. If q is scheduled be\u00adfore p, the scheduler will \ndeadlock, This is because p must precede r in the schedule, but p affects clock k. Scheduling p would \nviolate Rule 1. To force a correct ordering, and thereby avoid deadlock, Marion adds the extra code DAG \nedge (p, q). In the previous example, (p, r) is an alternate en\u00adtry. An alternate entry into a temporal \nsequence T is an edge (y, x) whose destination x is in T, but is not the head of T. Because of alternate \nentries, tempo\u00adral sequences must be protected to avoid scheduling deadlock, Intuitively, to protect \na temporal sequence T based on clock k, whenever an alternate entry into T is found, Marion searches \neach path backward from C fragment: a = (x + b)+ (a * z); return(y + z); Pq q affects k p affects k (q,r) \nis based on k r r affects k V Figure6: Scheduling deadlock. Without thedashed edge (p, q), q maybe scheduled \nbefore p, causing deadlock. the alternate entry. If an instruction that affects k is found, an edge is \nadded from that instruction (or a member of its temporal sequence) to the head of T. This ensures that \nall ancestors of any node in a temporal sequence are scheduled before the head of the sequence. The actual \nalgorithm differs somewhat and has a worst case behavior of O(ne), where n is the number of nodes in \nthe DAG and e is the number of edges. (Details appear elsewhere [Bra91].) Chaining occurs when a pipeline \nsends its result directly to itself or to another pipeline without using a general purpose register. \nMarion models chaining by introducing sub-operations that explicitly feed val\u00adues from one pipeline to \nanother. The code selector produces these sub-operations by matching patterns in the order specified \nby the user. The algorithm to protect temporal sequences takes chaining into con\u00adsideration. In addition, \nMarion prevents each pair of chained sequences from being reordered. More detail and a proof of correctness \nappear elsewhere [Bra9 1]. Using classes and temporal scheduling, the Mar\u00adion i860 code generator produces \ndual-operation floating point instructions (add and multiply to\u00adgether) and schedules integer and floating \npoint in\u00adstructions on the same cycle. Our model also sup\u00adports chaining between the floating point pipelines \nand uses the special T register [Int89] to hold a value between the multiply and add pipelines. Figure \n7 shows a sample code fragment produced by the Mar\u00adion i860 Postpass compiler. Why do we support EAPs \nwith temporal schedul\u00ading? We could treat an EAP as a normal pipeline, so that once an operation is launched, \nit advances through the pipeline on every cycle. However, this reduces scheduling opportunities, because \nsub\u00adoperations can be scheduled where complete opera\u00adtions cannot. In addition, without separating into \nsub-operations, operations in different EAPs are dif\u00adficult to overlap. Registt use: f2:a, f3:b, fi :x \nf8:y, f9:return value Cycle i860 instruction remarks o pfmul.s f2,f7,f0 ml+j2*j7 1 ratlp2.s f7,f3, f0 \nal+~7+~3 m2* ml 2 ratlp2.s f8,f7,f0 U2 t al m3* m2 al+~8+~7 4 pfadd.s fO,fO,fO a3 + a2 a2+ al 5 m12apm.s \nfO,fO,fO al+a3+m3 a3h a2 6 pfadd.s f0,f0,f9 f9 + a3 a2t al 7 pfadd.s fO,fO,fO as * a2 R pfadd.s f0,f0,f4 \nf2 -a3 Figure 7: Code produced by Marion i860 Postpass com\u00adpiler. The remarks column shows the operations \nthat each instruction performs. Temporal registers al-a5 and ml-m3 represent latches in the floating \npoint add and multiply pipelines, respectively. On cycle 5 the add pipe takes inputs from the multiply \nand add pipe outputs. Given sub-operations, some form of temporal scheduling is necessary to keep track \nof intermedi\u00adate values in the pipeline. Our approach is to pro\u00adtect temporal sequences before scheduling, \nso that deadlock cannot occur. This avoids backtracking and reliance on the register allocator, and removes \nthe burden of temporal scheduling from the scheduling algorithm and code generation strategy. An alternative \nto temporal scheduling with chain\u00ading is to allow the scheduler to chain on the fly whenever possible. \nThe register that represents the link in the chain can be freed and reused. Touzeau uses this approach \nin the FPS-164 Fortran Compiler [Tou84], but because it requires the scheduler to ma\u00adnipulate chains \nand to perform local register alloca\u00adtion, it is not flexible enough for Marion. 5 Evaluation Marion \nevolved in the same way as most other retargetable code generators. Although we initially attempted to \ndesign a broad description language, we had to extend and modify Maril as each signifi\u00adcant target was \nimplemented. The extensions for the R2000 and 88OOOwere minor, but because we wanted to exploit most \nof its unique features, the i860 forced us to rethink our model and led us to add classes and temporal \nscheduling to Marion. It may seem that building a retargetable code Table 1: Maril machine description \nstatistics. Each col\u00adumn gives the section size (in lines) and number of items of a particular kind. \n 88ooO R2000 i860 Section size Declare lines 16 17 251 I Cwvrn lines I 14 16 21 I Clocks o 0 4 Elements \n0 0 140 Classes 0 0 67 Aux lats 6 0 12 Glue xforms 29 18 27 funcs 1 2 7 func C lines 17 30 399  generator \nfor RISCS would be easier than one for CISCS. RISCS have simplified some aspects of code generation, \nparticularly code selection, but instruc\u00adtion scheduling and its interaction with register al\u00adlocation \nhave added new complications. Support for register pairs and the pipeline timing and intercon\u00adnect anomalies \nfound on some RISCS increase the complexity. The challenge we faced was to design a practical language \nthat could encapsulate as many of an architecture s scheduling and register details as possible, and \nbuild a system that could maintain and use that information to make judicious schedul\u00ading and register \nallocation choices. We think Marion meets this challenge. The description language is rich enough to \nen\u00adcapsulate most scheduling information for RISCS. We have used it to build code generators for three \ncommercially-available RISCS and for numerous ar\u00adchitectural variations. Although Maril is not as suc\u00adcinct \nas possible, the descriptions for the R2000 and 88000 were easy to write. Modeling the i860 was more \ndifficult, due to the complexity of the machine. Ta\u00adble 1 shows the size and composure of the machine \nde\u00adscriptions for these architectures. %instr directives constitute the bulk of the descriptions and \nmuch of the information is replication, since most instructions scheduling properties fall into one of \na few categories. Marion was designed to support different schedul\u00ading algorithms and code generation \nstrategies. The scheduling infrastructure can support most list scheduling algorithms. Other algorithm \ntypes could be added with minor additions: We irn-plemented three significantly different code generation \nstrate-Table 2: Marion system source code size (in lines of C code). The figures for TSI do not include \nthe front end and each TD portion is the output of CGG. Phase Lines Code Generator Generator (CGG) 4991 \nTarget\u00ad and strategy-independent (TSI) 10877 Target-dependent (TD), 88OOO 6864 Target-dependent (TD), \nR2000 5512 Target-dependent (TD), i860 8492 Strategy-dependent (SD), Postpass 151 Strategy-dependent \n(SD), IPS 1269 Strategy-dependent (SD), RASE 3750 gies, Postpass, IPS and RASE. IPS was the last to \nbe implemented, and took only one (expert) person\u00adweek. Table 2 shows the size of the Marion system source \ncode. The target-dependent portion that is output by the code generator is large, but roughly 75~o is \ncode to construct pattern trees. The target\u00adand strategy-independent portion includes the glue transformer, \ncode selector, global register allocator, code DAG builder and scheduling support. Marion compilers are \nnot fast. We emphasize, however, that Marion is a prototype system designed to show that retargetable \ninstruction scheduling is feasible; we have not concentrated on making Mar\u00adion efficient, preferring \nthe ibest simple implemen\u00adtation whenever possible. Table 3 shows the execu\u00adtion times of the front end \nand Marion-built R2000 and i860 back ends when compiling a program suite on a DECstation 5000. The program \nsuite includes the NAS Kernel benchmark (Nasker), a photon trans\u00adport problem solver (SPHOT) from the \nRice Com\u00adpiler Evaluation Suite, a supersonic reentry simu\u00adlation (ARC2D) from the PERFECT Benchmarks \nand the C front end .of our compilers (Lee). Dila\u00adtion is the ratio of instructions executed to instruc\u00adtions \ngenerated. IPS takes longer than Postpass, be\u00adcause it schedules each block twice and its scheduler is \nmore complicated than Postpass s. RASE takes even longer; in effect it schedules four times and its scheduler \nis more complicated than IPS s. Because the i860 requires extensive use of temporal registers and classes, \nand because floating point operations are split into sub-operations, compiling for the i860 takes roughly \ntwice as long as for the R2000. We have previously compared the performance of the three code generation \nstrategies and found that RASE and IPS both produce code that is 12$Z0faster than that produced by Postpass, \non a computation\u00adintensive workload [B EH91b]. The performance data for that comparison was derived by \ncombining ba\u00adTable 3: Time spent in front end, Marion back ends and MIPS cc when compiling the program \nsuite for the R2000 and the i860. MIPS cc performs local optimizations. I Module User time (sees) Dilation \n(x104) r R2000 Lcc front end 31 0.95 I Marion, Marion, Marion, mips-cc as Postpass IPS RASE -01 989 1846 \n5969 54 32 28.49 58.16 192.60 1.66 0.99 r i860 sic block execution costs computed by each scheduler \nwith execution frequencies computed by a separate profiling tool. Therefore, cache misses were not con\u00adsidered. \nTable 4 compares the estimated execution cycles, computed by Marion schedulers, to the actual execution \ncycles for the first fourteen Livermore Loop kernels. The actual cycles were computed by timing the execution \non a DECstation 5000 and multiplying by the clock rate, which is 25MHz. The ratio of actual time to estimated \ntime varies, but is consistent across strategies for each loop. For the Livermore Loops RASE-generated \ncode was 26 %ofaster than code pro\u00adduced by mips -01, which performs only local opti\u00admization; code produced \nby mips -02, which per\u00adforms global register allocation and global optimiza\u00adtion, was 42% faster than \nRASE code. This makes sense, because Marion compilers perform global reg\u00adister allocation, but no global \noptimizations. We examined the feasibility of retargeting Mar\u00adion to other architectures. Marion should \nbe able to model multiple instruction issue on the IBM RS/6000 [War90] by giving each functional unit \na separate set of resources. Since instructions using different func\u00adtional units will cause no structural \nhazards, they could be scheduled on the same cycle. Marion can also model the Sun SPARC [Sun87], except \nfor the register renaming that occurs on function calls be\u00adcause of register windows. To handle this, \nwe could extend Maril s C wvm to specify parameters and ar\u00adguments separately. Marion does not support \nmulti\u00adple identical functional units, which limits the super\u00adscalars that can be modeled, but introducing \narrays of resources would be a natural extension. Unfortunately, many architectures have features Table \n4: Execution time (in seconds) and ratio of actual to estimated execution time of Marion-generated R2000 \ncode. The mean is arithmetic for execution times and harmonic for ratios. Ker Exec time (sees) Postp \nIPS RASE -i\u00ad 2.22 2.22 2.22 %=%# 2 2.40 1.98 1.98 1.11 1.02 1.02 3 1.82 1.82 1.82 1.11 1.10 1.10 4 1.32 \n1.32 1.32 0.99 0.99 0.99 5 2.09 2.08 2.09 1.05 1.05 1.05 6 1.31 1.32 1.31 1.04 1.05 1.05 7 2.71 2.73 \n2.68 1.06 1.13 1.12 8 4.24 4.21 4.25 1.09 1.13 1.13 9 3.19 3.12 3.19 1.13 1.13 1.15 10 4.73 4.76 4.76 \n1.09 1.10 1.10 11 1.72 1.72 1.73 1.01 1.01 1.01 12 1.83 1.83 1.83 1.01 1.01 1<01 13 2.97 2.93 2.93 1.02 \n1.02 1.02 14 3.20 3.14 3.13 1.04 1.03 1.03 K 2.38 2.35 2.35 1.06 1.06 1.06 that are still difficult \nto handle. For example, the 88000 uses a priority scheme for its write-back bus. Adding Maril language \nfeatures to accommodate this scheme would not be difficult, but it would be expen\u00adsive to check the priorities \nduring scheduling. Instead, we give priority to the instruction scheduled first. 6 Conclusion Marion \nis a retargetable code generation system specifically designed for pipelined architectures. The Maril \nmachine description language has primitives to describe most scheduling requirements and can model a \nbroad range of RISCS, including some superscalars. As a proof of concept, we have built code generators \nfor the MIPS R2000, the Motorola 88000, the Intel i860 and a number of variations of those architectures, \nusing all three code generation strategies. Marion demonstrates that retargetable code gen\u00aderators with \ngood instruction schedulers can be con\u00adstructed for RISCS. Most scheduling requirements can be described \nusing a straight-forward description language, but even RISCS have idiosyncrasies that require an escape \nmechanism. Marion cannot han\u00addle all details perfectly. Nevertheless, it can model most of the important \nfeatures required to produce good schedulers, including features found in com\u00adplex RISCS, such as explicitly \nadvanced pipelines and irregular packing restrictions. Acknowledgements [GH82] T. R. Gross and J. L. \nHennessy. Optimiz\u00ading delayed branches. In Proceedings of IEEE We would like to thank Chris Fraser and \nPeter Dam\u00ad ron for their provided an References [AGH+84] [BCKT89] [BEH91a] [BEH91b] [Bir87] [BN71] [Bra91] \n[CH84] [Cha82] [DF80] [FERN84] [FH90] help in reviewing this paper. David Hanson easy-to-use robust C \nfront end. P. Aigrain, S. L. Graham, R. R. Henry, M. K. McKusick, and E. Pelegri-Llopart. Experi\u00adence \nwith a Graham-Glanville style code gen\u00aderator. ACM SIGPLAN Syrnp. on Compiler Construction, June 1984. \nP. Briggs, K. D. Cooper, K. Kennedy, and L. Torczon. Coloring heuristics for register al\u00adlocation. ACM \nSIGPLAN Conf. on Pr-ogram\u00adming Language Design and Implementation, July 1989. D. G. Bradlee, S. J. Eggers, \nand R. R. Henry. The effect on RISC performance of register set size and structure versus code generation \nstrategy. Intl. Symp. on Computer Architec\u00adture, May 1991. D. G. Bradlee, S. J. Eggers, and R. R. Henry. \nIntegrating register allocation and instruction scheduling for RISCS. Intl. Conf. on Architec\u00adtural Support \nfor Programming Languages and Operating Systems, April 1991. P. H. L. Bird. Code Generation and Instruc\u00adtion \nScheduling for Pipelined SISD Machines. PhD thesis, Univ. of Michigan, 1987. C. G. Bell and A. Newell. \nComputer Struc\u00adtures: Readings and Examples. McGraw Hill, New York, NY, 1971. D. G. Bradlee. Retargetable \nInstruction Scheduling for Pipelined Processors. PhD the\u00adsis, Dept. of Comp. Sci. &#38; Eng., Univ. of \nWash., 1991. (in preparation). F. C. Chow and J. L. Hennessy. Register allo\u00adcation by priority based \ncoloring. ACM SIG-PLAN Symp. on Compiler Construction, June 1984. G. J. Chaitin. Register allocation \nand spilling via graph coloring. ACM SIGPLA N Symp. on Compiler Construction, June 1982. J. W. Davidson \nand C. W. Fraser. The design and application of a retargetable peephole op\u00adtimizer. ACM Trans. on Prog. \nLang. and Sys., 2(2), April 1980. J. A. Fisher, J. R. Ellis, J. C. Ruttenberg, and A. Nicolau. Parallel \nprocessing: A smart com\u00adpiler and a dumb machine. ACM SIGPLAN Symp. on Compiler Construction, June 1984. \nC. W. Fraser and D. R. Hanson. A code gener\u00adation interface for ANSI C. Tech. rep. CS-TR\u00ad270-90, Dept. \nof Comp. Sci., Princeton Univ., July 1990. [GH88] [GM86] [HC88] [Hen87] [HG83] [Int89] [Kan87] [LDSM80] \n[Mot88] [Sta89] [Sun87] [Tho67] [TOU84] [War90] [WP87] MICRO-1 5, October 1982. J. R. Goodman and W.-C. \nHsu. Code schedul\u00ading and register allocation in large basic blocks. In lntl. Conf. on Supercomputing, \nJuly 1988. P. B. Gibbons and S. S. Muchnick. Efficient instruction scheduling for a pipelined archi\u00adtecture. \nACM SIGPLAN Symp. on Compiler Construction, July 1986. W. W. Hwu and P. P. Chang. Exploiting par\u00adallel \nmicroprocessor microarchitectures with a compiler code generator. Intl. Symp. on Com\u00adputer Architecture, \nJune 1988. R. R. Henry. The CODEGEN user s manual Computer Science Dept. Tech. rep. 87-08-04, Univ. of \nWash., August 1987. J. L. Hennessy and T. R. Gross. Postpass code optimization of pipeline constraints. \nACM Trans. on Prog. Lang. and Sys., 5(3), July 1983. Intel Corp., Santa Clara, CA. i8606J-bit Mi\u00adcroprocessor \nProgrammer s Reference Manual, 1989. G. Kane. MIPS R2 000 RISC Architecture. Prentice Hall, Englewood \nCliffs, NJ, 1987. D. Landskov, S. Davidson, B. Shriver, and P. W. Mallett. Local microcode compaction \ntechniques. ACM Computing Surveys, 12(3), September 1980. Motorola, Inc. MC88100 RISC Microproces\u00adsor \nUser s Manual, 1988. R. M. Stallman. Using and Porting GNU CC. Free Software Foundation, Inc., Cambridge, \nMA, 1989. Sun Microsystems, Inc., Mountain View, CA. The SPARC Architecture Manual, 1987. J. F. Thorlin. \nCode generation for PIE (par\u00ad allel instruction execution) computers. In A FIPS Conference Proceedings, \nvolume 30, April 1967. R. F. Touzeau. A Fortran compiler for the FPS-164 scientific computer. ACM SIGPLAN \nSymp. on Compiler Construction, June 1984, H. S. Warren, Jr. Instruction scheduling for the IBM RISC \nSystem/6000 processor. IBM Journal of Research and Development, 34(l), January 1990. D. W. Wall and M. \nL. Powell. The Mahler experience: Using an intermediate language as the machine description. Intl. Conf. \non Archi\u00adtectural Support for Programming Language~ and operating Systems, October 1987,  \n\t\t\t", "proc_id": "113445", "abstract": "", "authors": [{"name": "David G. Bradlee", "author_profile_id": "81100301081", "affiliation": "Department of Computer Science and Engineering, FR-35, University of Washington, Seattle, Washington", "person_id": "P60718", "email_address": "", "orcid_id": ""}, {"name": "Robert R. Henry", "author_profile_id": "81100018281", "affiliation": "Tera Computer, 400 N 34th Street, Seattle, WA and Department of Computer Science and Engineering, FR-35, University of Washington, Seattle, Washington", "person_id": "PP39073048", "email_address": "", "orcid_id": ""}, {"name": "Susan J. Eggers", "author_profile_id": "81100262930", "affiliation": "Department of Computer Science and Engineering, FR-35, University of Washington, Seattle, Washington", "person_id": "PP15027685", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/113445.113465", "year": "1991", "article_id": "113465", "conference": "PLDI", "title": "The Marion system for retargetable instruction scheduling", "url": "http://dl.acm.org/citation.cfm?id=113465"}