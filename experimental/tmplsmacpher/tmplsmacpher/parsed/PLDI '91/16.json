{"article_publication_date": "05-01-1991", "fulltext": "\n A Timed Petri-Net Model for Fine-Grain Loop Scheduling Guang R. Gao Yue-Bong Wong Qi Ning School of \nComputer Science McGill University 9480 University Montr&#38;al, Qui5bec H3A 2A7 Abstract Efficient execution \nof loops is one of the most important obstacles facing high-performance computer arcKltectures. Loop \nscheduling involves handling a partially ordered set of operations which are to be performed repetitively \nover a number of iterations. In thu paper we use Petri nets to study loop schedul\u00ading, due to their unique \npower for modeling both partial orders and cycles, The behavior of loops can be modeled by constructing, \nat compile time, a Petri-net behavior graph w~lch exKltdts a repetitive firing sequence known as a cyclic \nfrustum. The main contributions of thk paper include: The development of a Petri-net loop model called \nan SDSP-PN. Loops are first translated into a class of static dataflow graphs known as a static dataj70w \nsoft\u00adware pipeline (SDSP) and then the SDSP is translated into an SDSP-PN. When an SDSP-PN is executed \nac\u00adcording to the earliest jiring rule, a cyclic frustum ap\u00adpears in the behavior graph within a bounded \nnumber of steps. We show that (1) in an SD SP-PN having one critical cycle, a polynomial bound can be \nestablished for the cyclic frustum to occur (for all nodes in the loop) under the earliest firing rule; \nin an SDSP-PN having multiple critical cycles, a polynomial bound can be established for the cyclic frustum \nto occur only for nodes on the critical cycles; (2) from a cyclic frus\u00adtum, a time-optimal schedule for \nthe corresponding loop can be derived.  A methodology for integrating resource limitations into our \nmodel. We demonstrate how a timed Petri\u00adnet model known as an SDSP-SCP-PN can be con\u00adstructed to model \nthe execution of an SDSP on dataflow arcldtectures having a single clean execution pipeline (SCP).  \nThe mechanism of detecting cvclic frustums has been  implement ed in a prototype compiler t estbed. \nSimula- Permission to copy without fee all or part of this material is granted provided that the copies \nare not made or distributed for direct commercial advantage, the ACM copyright notice snd the title of \nthe publication snd its dste sppesr, snd notice is given that copying is by permission of the Association \nfor Computing Machine~. To copy otherwise, or to republish, requires a fee snd/or specific permission. \n@ 1991 ACM 0-89791 -428 -7/91 /0005 /0204 ...$1 .50 Proceedings of the ACM SIGPLAN 91 Conference on Programming \nLanguage Design and Implementation. Toronto, Ontario, Canada, June 26-28, 1991. ~ tion results on a \nnumber of Livermore loops, both with and without loop-carried dependence, have demon\u00adstrated that the \ncyclic frustum for both the SDSP-PN and the SDSP-SCP-PN can be determined at compile\u00adtime in O(n) time, \nwhere n is the number of instruc\u00adtions in the loop body. This demonstrates the fea\u00adsibility of determining \nthe cyclic frustum at compile time. We also describe how to determine the minimum amount of storage needed \nby a loop to maintain its optimal computa\u00adtion rate., 1 Introduction The exploitation of fine-main ~arallelism \nis a maior chal\u00adlenge ii the design of co~piler~ for high-performa~ce com\u00adputers. With the advent of \nsecond generation RISC archL tecturea, such as IBM RISG-6000 and Intel i-860 [26, 28], the challenge \nhas become more demanding, especially since code generators for these newer VLSI architectures must now \nexploit extensive instruction-level parallelism. With ULSI (Ultra-Large Scale Integration) multi-miMon \ntransis\u00adtor processor cKips under way within tlds decade, we can ex\u00adpect even more fine-grain parallelism \ndue to deep instruction pipelining and multiple function units (particularly multiple pipelined function \nunits). Fine-grain loop scheduling involves the exploitation of parallelism from a partially ordered \naet of operations of a loop body wldch are performed repetitively over a number of iterations, With loop-carried \ndependence, the depen\u00addence relation between operations is a dlgraph with cycles, presenting a serious \nobstacle to classical methods of loop scheduling. Many code scheduling problems are provably NP-hard \n[11, 25, 43]. Recently software pipelining has been proposed as a promising approach in the dhection \nof better loop schedul\u00ading. Software pipelining specifies a static parallel sched\u00adule which overlaps \ninstructions from different iterationa of a loop body. The advantage is that software pipelining provides \na direct way of exploiting parallelism acroas loop iterations without loop unrollhtg. Based on thk method, \nefficient code can be generated for pipelined processor ar\u00adclitect.ures, as well as super-scalar and \nVLIW archltecturea [1, 2,3,17,18,29,32,38, 42]. Although the work described in thk paper originated from \ndatafiow sojtware pipelining [22, 23], the results are dkectly applicable for finding a static schedule \nto exploit fine-grain parallelism in loops for machine models other than dataflow, machines such sw tightly-coupled \nsynchronous par\u00adallel machines (superscalar or VLIW) and various pipelined architectures. We use dataflow \ngraphs as our program representation. One reason is that control dependence is represented di\u00adrectly \nin a dataflow graph as data dependence and therefore can be treated uniformly. The advantage of dataflow \ngraphs as an intermediate form for compiler optimization has been recognized by various researchers and \nwidely used in the compilation of functional languages [20, 31, 33]. It is inter\u00adesting to note that \nimpressive advances have been made in methods of translating imperative programs into dataflow graphs \n[6, 7]. In thb paper we use Petri nets to study loop scheduling, due to their unique power for modeling \nboth partial orders and cycles. The behavior of a loop represented as a Petri net can be modeled by constructing, \nat compile time, its behau\u00adior graph which contains a repetitive firing sequence known as a cyclic jrustuna. \nOur algorithm for detecting cyclic frus\u00adtums has been implemented in a prototype compiler. Simu\u00adlation \nresults on a number of Livermore loops, both with and without loop-carried dependence, have demonstrated \nthat the cyclic frustum for both the SDSP-PN and SDSP-SCP-PN models can be determined at compile-time \nin O(n) time, where n is the number of instructions in the loop body. We also describe how to determine \nthe minimum storage needed by a loop to maintain its optimal computation rate and pro\u00advide an example. \n1.1 Synopsis Section 2 provides a brief example illustrating the main no\u00adtions presented in th~ paper. \nSection 3 defines a class of loops known as a static dataflow software pipeline (SDSP). Thk class includes \nloops both with and without loop-carried dependence. In this section we also describe how to obtain a \ncorrespondhg Petri-net loop representation, SDSP-PN. We then dkuss the techniques to obtain steady-state \nbehav\u00adior of the SDSP-PN operated under the earliest firing rule and define the parallel schedule derived \nfrom the SDSP-PN steady state. In Section 4 we show that steady-state behav\u00adior can always be reached \nin a polynomial number of steps for an SDSP-PN having one critical cycle. In Section 5 we provide experimental \nevidence that the cyclic frustum for both an SDSP-PN and its extension with resource con\u00adstraints can \nbe quickly reached, using examples such as the Livermore loops. In Section 6 we &#38;cuss issues in storage \noptimization under time-optimal scheduling for the SDSP-PN. A comparison with related work and directions \nfor fu\u00adture work are included in Section 7. Our conclusion is in Section 8. Appendm A provides the necessary \nbackground on timed Petri-nets to make thk paper self-contained. 2 An Example In this section we illustrate \nthe primary notions contained within thb paper using, as an example, the loop in Fig\u00ad ure l(a). The reader \nshould understand the intuition behind our analysis before proceeding to the theoretical aspects de\u00ad \nveloped in later portions of the paper. Some concepts and terminologies are used informally in the illustration; \nwhereas formal treatment appears in subsequent sections. Loop Ll, shown in Figure l(a), is an example \nof a DOALL loop-a loop that has no loop-carried dependence#.l The corresponding dataflow graph is given \nin Figure l(b)? Each node (or actor) in the graph represents a single in\u00adstruction. Note that the graph \nis acyclic due to the absence of data dependence between iterations. Code of this type is said to be \ndatajiow $oftware pipelined in the sense that suc\u00adcessive waves of elements from input arrays W, X, Y, \nand Z are fetched and fed into the graph of the loop body so that computation is able to proceed in a \npipelined fashion [21]. Basic dataflow concepts and architectures are described in [4, 14, 15]. Software \npipelining derives a static parallel schedule, known as a scheduling pattern by determtilng how a loop \nunravels when instruction execution is constrained only by data dependence. Once thii pattern is found, \nthe compiler uses it to overlap operations from successive iterations of the 100P body, The example shows \nhow a Petri-net can be used to de\u00adrive a loop scheduling pat t em. Figure 1(c) shows the static dataflow \ngraph for L1. Assume that enabled nodes are fired (or executed) under an earliest firing rule, i.e., \nthat all en\u00adabled nodes are executed as soon as they have been enabled. In addition, assume that the \nexecution time of each node is one cycle. Figure l(d) shows the direct transformation of Fig\u00adure 1(c) \ninto a Petri net, where the circles and bars re\u00adspectively denote places and transitions in the net. \nAs can be seen, the set of transitions represent nodes in the cor\u00adresponding dataflow graph whale the \nset of places, together with the tokens distributed among them, determine the state of the net. As stated \nearlier, a Petri net and its behavior graph can assist with loop scheduling since they can be used to \npor\u00adtray both partial orders used for fine-grain scheduling and cycles inherent in loops. Figure l(e) \nshows the behavior graph of the Petri net in Figure l(d), created through ap\u00adplication of the earliest \nfiring rule. In Figure l(e), the set of marked places labeled the initird and the terminal instan\u00adtaneous \nstates are merely repeated execution states. The portion of the firing sequence embedded between these \ntwo states is known as a cyclic frustum. Instead of cxtendhig the behavior graph indefinitely, we extract \nthe cyclic frustum and coalesce the initial and termi\u00adnal instantaneous states to form another strongly-connected \nPetri net, a so-called steadptate equivalent net consisting of the repetitive pattern mentioned earlier, \nas shown in Fig\u00adure l(f). From this repetitive pattern it is straightforward to statically derive a time-optimal \nloop schedule as illustrated in Figure l(g). Tlds approach iz similar to the one described in [2], for \ngenerating a schedule from pat t ems. Using results from Petri-net theory, we can prove that a scheduling \npattern can be found efficiently and, hence, is practical for compiler use. Simulation results of our \ntech\u00adnique are described in Section 5. 1Loop L1 should not be confused with Livermore Loop #1 used in \n the simulation results presented in Section 5. 2A direct translation of L1 contains control nodes, such \nas switch and merge nodes. For simplicity, we omit these nodes from the dataflow graph. From now on \nwe will use simplified graphs unless stated otherwise. re2v4rd data \u00adare -o.ladgwnt d X[i]5 A *ra A + \n%,,,[% C) PIP* ~mp~ Y[i] + A[i] 2[1] (fi B (A ~)B + + C(A~, c doall i from 1to n ++ A[i] := X[i] + \n5; rl[i] C[i] P5P* P? Pe B[i] := Y[i] + A[i]; : .<) \\ C[i] := A[i] + Z[i]; Wtil + D D[i] := B[i] + C[i]; \nD[l] (B, C, E) d5;:\u00ad ) P9 e Plo E[i] := W[i] + D[i]; + endall  v? E[i] +i(D)/ P E (a) LOOP L1 (b) Dataflow \nGraph (C)static Dataflow (d) SDSP-PN Graph for L1 P2 P4p6 P Plo A initial PI3 e imtantallaolls 8tit0 \nmarkingBc AD P2 P5P4 P @ tamlnal instantaneousA D state marking BCE PI p3p6 p8 p9 a B cE P2 P5P4 P7 \nPlo K (e) Behavior Graph (f) Steady-State (g) optimal Equivalerk Net Schedule Figure 1: 3 A Timed Petri-Net \nModel for Loops SDSP- SDSP, a token is assigned to the correspondhg place in the SDSP-PN. Two important \nproperties of an SDSP-PN arePN Model (1) that the initial marking is live and safe and (2) that the SDSP-PN \nis a marked graph. As an example, Figure 2 shows 3.1 Timed Petri Nets the dataflow graph and corresponding \nPetri-net representa- Appendw A defines the timed Petri-net model as well as tions for loop L2 with loop-carried \ndependence. related terminology and notations, and it summarizes basic In a dataflow graph a conditional \nexpression is repre\u00ad results for a time-optimal computation rate of timed marked sented by a so-called \nwell-formed conditional datajlow sub\u00adgraphs, the class of nets used in thk paper. Since published graph. \nSwitch and merge nodes are used in the implementa\u00ad works on Petri-net theory use diverse conventions \nand no-tion. These nodes have special firing rules wNlch can present tations, thk appendm is necessary \nto make thk document problems when considered in a Petri-net model. To over\u00ad self-cent ained. come these \nproblems, the firing rules of the switch node and the merge node are altered to produce and consume dummy \ntokens on their unselected branches. A detailed dis\u00ad3.2 Static Dataflow Software Pipeline (SDSP) cussion \ncan be found in [24]. Under thk treatment, switch In this paper dataflow graphs are used as a program \nrep-and merge nodes have the same firing rules as regular nodes; resentation for loops. The class of \nloops that are of inter-hence, a conditional dataflow graph can be treated as an or\u00adest here are non-nested \nloops, For nested loops, our tech\u00ad dinary SDSP. nique applies to the innermost loop where the execution \nunit spends the most execution time. Conditional constructs are 3.3 Behavior Graphsallowed as long as \nthe overall structure of the loop remains a well-formed dataflow graph [13, 16]. In addition, we as-The \nconstruction of a behavior graph provides an alternative sume that loop-carried dependence are from one \niteration way of describhg the behavior of a Petri net, besides reach\u00adto the next. Taking loop-carried \ndependence into account, ability y trees [35]. Behavior graphs are particularly useful for a dataflow \nprogram graph for a loop can contain both for-describing concurrency and cyclic firing patterns of a \nPetri ward data arcs and feedback data arcs. net. From a different standpoint, a behavior graph is a \ntrace A graph G is a SDSP dataflow graph under the above generated while executing an SDSP-PN under the \nearliest assumptions, formally expressed as a tuple (V, E, l?, 1 , F ), firing rule. At each time step \nin the execution, the behavior where V is the set of nodes in G, sets E and E contain graph records the \nnewly marked places and sets of enabled forward data arcs and feedback arcs, respectively, and F transitions \nto be fired at that step. In addition, directed and F are sets of acknowledgement arcs for E and E . \narcs are introduced among the places and the transitions To convert a SDSP to a Petri net, we insert \na place on to denote the token flow relation from place to transition each arc in the SDSP. The resulting \nPetri net is called an (token consumption) and transition to place (token produc-SDSP-PN. For any arc \nthat initially holds a token in the tion). The algorithm for constructing the behavior graph is Forward \ndata arc Acknowledgement .....+ arc X[il 5 Eeedback arc data ~ do ifrom 1to n A i] := X[i] + 5; B -ii \n:= Y[il + A[il; c := A[-il + E[i ll; D i] := B[i] + Cti]; E i] := W[i] + D[i];   end t[i] T (a) Loop \nL2 (b) Dataflow (c) Graph Figure given inour full paper [24]. Figure Illustrates an exam. ple of the \nbehavior graph constructed for the marked graph (resulting from the SDSP-PN for Ll), shown in Figure \nl(d), where the execution time of all transitions is assumed to be eoual. . Ascan be seen, construction \nofa behavior graph can continue indefinitely, and the behavior graph itself can be infinitely extended. \nA key observation is that the behavior graph exhibits arepetitive pattern after some initial period. \nThis effect is formalized by the following lemmas, including the concept of instantaneous state (proofs \nare omitted): Lemma 3.3.l A behavior graph is unique for an Sl)SP-PN. Lemma 3.3.2 There exists an indantaneous \nstate which appearu repeatedly in the beham or graph of an SDSP-PN. From Lemmas 3.3.1 and 3.3.2, we see \nthat once the in. stantaneous state repeats, it will do so forever. As a result, the region of the behavior \ngraph between two repeated in\u00adstantaneous states represents the steady-state behavior of the SDSP-PN, \nunder the earliest firing rule. Therefore, we have the following definition: Detinition3.3.l A Cyclic \nFruatum ofabehaviorgraph B is the portion of B between two consecutive occurrences of some repeated instantaneous \nstate. In addition, the two in\u00addantaneous dates that surround the frustum are termed the initial instantaneous \nstate and the terminal instantaneous state, respectively. The marking portion of both the initial and \nterminal in\u00adstantaneous state found in the behavior graph for L1 are marked in Figure l(e), where the \ntwo associated residual firing time vectors are simply vectors comprised of all zero entries. Notice \nthat cyclic frustum is actually a cyclic fir\u00ad ing sequence since it fires each transition at least once \nand brings the net back to the initial state. Once the behav\u00adior graph reaches its frustum, it repeats \nitself. Tlds sug\u00adgests the way of capturing the repetitive behavior. Instead of extending the graph indefinitely, \nwe extract the cyclic frustum and coalesce the initial and terminal instantaneous B P5 D P12 P9 Static \nDataflow (d)SDSP-PN Graph for L2 stat es together to form a strongly-connected Petri net, or Stead~-State \nEquivalent Net. Figure l(f) shows the steady\u00adstate equivalent net derived from the behavior graph in \nFig\u00adure l(e). 4 A Polynomial Bound for Finding the Cyclic Fmrstum As can be seen, the cyclic frustum \nfor SDSP-PN is guaran\u00adteed to appear. A question one might ask is: Can the cyclic frustum always be found \nwithin a bounded number of steps? ThkJ section provides the answer to thk question. The work described \nnext benefited both from Chretienne s thesis on Petri-net theory [9] and from Alken and Nicolau s work \n[2]. Chretienne proved that, after an initial period, succes\u00adsive firings of a transition will be cyclic, \nwith the cycle time being related to critical cycles. However, he dld not estab. lish a bound for thk \ncyclic pattern to occur. The establish\u00ading of a polynomial bound is important for practical rew sons, \nin order to perform pattern detection at compile time. Alken and Nicolau stated an O(rt$ ) bound of time \nsteps (or equivalently, U(n2 ) iterations) for finding a loop scheduling pattern [1, 2]. In the single \ncritical cycle case, their proof implicitly assumes that the computation rate of the nodes not on the \ncritical cycle equals the rate of the nodes on the critical cycle. In our proof, we give a justification \nto such an assumption and show that, as a result, the bound should be 0(n4 ) time steps (or equivrdently, \n0(n3 ) iterations). For the multiple critical cycle case, again the same implicit as\u00adsumption is made \nin [2] for the nodes not on the critical cycles. We are not able to justify such assumptions. In. stead, \nwe have established the bounds of 0(n3 ) time steps (or equivalently, 0(n2) iterations) for nodes on \nthe critical cycles to enter periodical firing pattern, and indicated that there is no such bound known \nto us for nodes not on the critical cycles, Hence the problem remains open for these nodes. Also, we \nare not aware of any experiments based on Aiken and Nicolau s work that reveals how fast a cyclic pattern \ncan be detected in real programs. We will show in Sections 4.1 and 4.2 that there is a poly\u00ad nomial \nbound on the time required for the cyclic pattern to appear in the case of a single critical cycle. Thk \nbound is O(TZ3) for nodes which are on the critical cycle and 0(n4 ) for all nodes, including any that \nare not on the critical cycle. In the case of multiple critical cycles, we have established 0(n3 ) as \nthe bound for nodes residing within critical cycles. We have found through experimentation, shown in \nSection 5, that patterns can be found in O(n) for examples such as the Livermore loops. Before beginning \nour proof, we define the notations which we will use: o Let G denote a SDSP-PN having n transitions, \nand let X: denote the time at which transition ticommences its h+l firing. We assume that the execution \ntime ~i of each transition ta is one time unit,. In gener~, however, the following results can be extended \nto cases in which transitions have different execution times. If P is a path in G, then AI(P), the token \nsum, denotes the sum of the tokens on each place in P.3 The token in one place is taken in the sum os \nmany times as the place is run through in P. Similarly, ii(P), the value sum, denotes the sum of ~i of \neach transition t; in P. The ~i of transition ti is taken in the sum as many times as the transition \nis run through in P. Let p~(ti, tj ) denote the set of possible paths in G from t; to tj having exactly \nh tokens along the path, and let ah(ti, tj) denote the value sum of the maximum value path in Ph(t;j \ntj). We also use the notation p~(til tj ) to denote the subset y of Ph(ti, tj) and the notation a~(ti, \ntj) to denote the maximum path value of subset P~(ti} tj). Since each transition has a self-loop with \none token on it (Assumption A.6.1), Pk(ti, tj ) # 0, for h ~ hO where hO is a positive integer. A simple \ncycle C in G is critical if the ratio of the value sum to the token sum is maximal, i.e., if ~(c) , ~(ci) \nM(C) -M(Ci) where CJ denotes the other simple cycles in G. Let ~i denote the cycle time of the simple \ncycle Ci in G, that is, ~i = fl(Ci)/M(Ci). 4.1 SDSP-PN Having One Critical Cycle In thk section we consider \nthe case in which an SDSP-PN has only one critical cycle. Let C* denote the only critical cycle in G, \nand let a*=tl(C*)/M(C* ) denote its cycle time. We first introduce several important lemmas. The first \n is Lemma 4.1.1 (due to Chretienne and others [8, 9, 10]) which relates the time at which transition \ntj starts its h+l firing to the computation of ak(ti, tj), the value sum of the maximal value path in \nPk(ti, tj ). Lemma 4.1.2 establishes the criterion (a polynomial bound) for any maximum value path passing \nthrough the critical cycle. Lemma 4.1.3 is due to Aiken and Nlcolau [2], and Lemma 4.1.4 is an inequality \nbased on the fact that the value-per-token ratio on a critical cycle is always larger than any ratio \nalong any non-critical cycle. Lemma 4.1.1 For any G executed under the earliest jiring rule, the time \nX: at which transition t j starts its h+ 1 firing equals max ah(t;,tj), ti c set oj enabled transitions \nat time O. t~ 3Note that a cycle is allowed along a path. Lemma 4.1.2 For h > 0(n3 ), the maximum value \npath in Ph(ti, tj) in G must pass through the critical cycle C*. Let V be a path in Ph(ti$ tj) which \ndoes not touch C*. Let Pa be a path in Ph(tij tj)which passes through C*. For any given h z hO, we choose \nPa = p(C*)m v, where ho is an integer, p and v are respectively the directed path from t~ to tyand the \ndirected path from tyto tj, ty is a transition on C*, and m is the number of times that C* is consecutively \niterated. The value m and the paths p and v can respectively be computed and constructed as follows. \n= l $@ ~ J Let ml = (M(Pa) 2n) mod M(C*) + n; p is a path from tj to tywith ml tokens while v is a path \nfrom tyto tj with n tokens. Notice that under such construction M(p) + M(v) < 3n. (1) Let s assume \nSm = M(p) +M(v)+m x M(C*) for the given h. Thus by definition both W and Pa belong to PSm (t;, tj). To \nprove this lemma, we first construct an upper bound on the value sum fl(SJ). Then we show that ~(P.) \nis always greater than the upper bound of Q(V) for m > 0(n2 ) or h > O(rls). Construction of the upper \nbound for fl(W) Let {C; ;i = 1, ..., a} denote the set of non-critical simple cycles in G, and let c \nand Ei be defined as follows: ~i = Q* ai, fi, l<i<a and E = min {Ci} i=l,..., a >0 Recall that a* and \n~i are respectively the cycle time for the critical cycle C* and the simple cycle Ci. Then, c is the \ncycle time difference of the critical cycle and the simple cycle that has the second largest cycle time \nin G. Notice that path V can be decomposed into a simple path q running from t, to t,and a set of non-critical \nsimple cycles, where for each C; we associate an inte\u00adger qi >0 to denote the number of times Ci is iterated \nin II. The value sum of Z is computed as follows: il(~) = ii(q)+ ~ Tli$l(C i) i-l  fl(q) + ~ ?j iC4iM(C; \n) i= 1 . Q(q) + ~ ~i(~* &#38;*)M(Ci) i=l < ii(q) + ~ %(a ~)M(ci) i=l Cl(q) + (CY* -&#38;)~q~Af(Ci) \ni=l < tl(q) + (cr* e)M(W) 4 Such paths must exist, because the token sum of any simple path (cycle-free) \nis bounded by n, and by Assumption A.6.1 P~(t., t,) # 0 provided h ~ n in a safe marked graph. Since \n9 6 Ps~(ti, tj)j M(W) can be expanded and El bounded as follows: Lemma 4.1.3 Given K integers 11,... \n, Ik, there is a aubaet fl(g) S of Ii such that ~ ~(q)+ (a -e) (M(p)+ M(v)+ m iW(C*)) = m (a c) M(C*)+b \n(2) where b = fl(q) + (a -e) (M(p) + M(v)) Q The evaluation of h so that fl(Pa) > fi(V) Note that the \npath Pa= N(C*)m u also belongs to Ps~(ti, tj), and f2(p(C *) P) equals fl(p) + fl(~) + m a iW(C*). As \nwe compare tl(p(C*)mv) and the upper bound of fl(fl?) (Equation 2), we see, for m > m. where m. is some \npositive integer, IT! can never be the maximum value path in Ps~ (ti, t j ). Moreover, this is true for \nall possible !$?c Ps~ (ii, t j ) that do not touch C*. m (a* -e) lkf(C*) +b < m a M(C*) + ~(~) + ~(v) \n(3) m. can be estimated by solving Equation 3 for m: m> b -(~(p)+ S_l(v)) (4) E M(C*) Next, we simplify \nthe right-hand side of Equation 4 and construct an upper bound. WM.bout loss of gen\u00aderality in i, assume \nthat Ci is the smallest value in {Cl,..., es}, i.e., e = ei = a* Q;: b (tl(~) + ~(v)) e it!f(c ) b \n< c a!f(c ) n(q) + (a* -.) (M(p)+ M(.)) = (~ -~i) AI(C*) = fl(~) + Cfi (.M(/J) + M(V)) (a -a,) M(C*) \nn(q) + * (M(p)+ M(v)) = (M-~) WC ) n(g) + * (M(P)+ J W~)) = (c )-w Af(Ci) n(~)+ ~(ci) (M(P) + (v)) (~)= \nfl(C*) M(Ci) !2(Ci) .M(C*) Note that G is a live-save marked graph composed of n transitions. The token \nsum for any simple path or any simple cycle is bounded by n. Similarly, the value sum of any simple path \nor any simple cycle is bounded by n. Note rdso that by Equation 1 M(p) + it-f(v) ~ 3n. Equation 5 can \nbe further reduced and bounded by rtxn+nx3n 1 < O(J) (6) As a result, for m ~ 0(n2 ), the maximum value \npath in PSm (ti, t j ) must pass through C*, where Sm = J!f(p) + A!f(v) + m x lkf(C*). Or equivalently, \nfor h ~ 0(n3 ), the maximum value path in ~h(ti, tj ) must pass through C*. (x)Ii mod k=O. \\IiES } In \nother words, the sum of all Ii E S is a multiple of k. Lemma 4.1.4 Let C be the critical cgcle in G, \nm x f2(C*) > ~ fl(C~) CiER wherell = {C.,...,cb I M(C@)+... +M(Cb) = mx ikf(C*)} and m is an integer \ngreater than O. Proof of Lemma 4.1.4 ~ ~(ci) = fl(Ca)+ --+ fi(cb) C{ER = dax~(ca)+---+~bx~(cb) < ~ x~(c.)+..$+~ \nx~(cb) = a x m x M(c ) = m x Cl(C*) Theorem 4.1.1 For any G with only one critical cycle C eoecuted \nunder the earliest firing rule and for h > 0(n3 ), the time constmint Xy+k -X/ = p is obeyed by all t; \n6 G, where k = M(C*) and p = Cl(C*). Proof of Theorem 4,1.1 By Lemma 4.1.1, thk theorem can be proven \nby showing that for h ~ 0(n3 ): yah+k(ti, ti) myywt(ti, tj) = P,vtj G G * where ti is a member of the \nset of initially enabled transitions at time O. Or equivalently, we can show that for h ~ 0(n3 ): where \nt;isa member of the set of initially enabled transitions at time 0, Notice that Pz(ti, tj)j the set of \npaths from tito tj with exactly z tokens, can be partitioned into three disjoint sub\u00adsets: P~(ti, tj \n), P~(ti, tj), and Pj(ti,where z >0. tj)j Sub\u00ad set P: (ti, ij ) denotes the set of paths that iterate \nthrough C* at least once. Subset P~(ti, tj) denotes the set of paths which only touch C*, that is, Cm \nis not embedded entirely in the path, and subset .P~(ti,paths that do tj)denotes not contain C* at all. \nWe would like to show that the maximum value path in Ph(ti, t j ) for h > 0(n3 ) can only be found in \nsubset Pf(tij i!j). By Lemma 4.1.2, we know that the maximum value path in Ph(ti, tj ) for h > 0(n3 ) \ncan never be found in subset P; (t~, t j ). For every path in subset P~(ti, tj ), there always exists \na corresponding path in subset If(ti, tj) Whkh has a higher value sum, provided h > (n+ l)k + n. For \nh > (n+ l)k +n, there exists at least k cycles along any possible path in ~k(ti, tj). By Lemma 4.1.3, \nthere exists a subset S of those cycles Ci such that ~C:e~ Af(Ci) is a multiple of k. Recall that k = \nikf(C*). Assume ~C$=~ M (Ci) = rn x ikf(C*), m ~ integer, and m >0 for any path P. c P~(t~, tj). Either \nS is composed of C m times, i.e., P= E P~(ti, tj); otherwise, P= could never have the maximum path value. \nThii is so because a path PM can be constructed from P= by replacing all Ci E S with exactly m C*. Pv \nmust dso exist in Ph(ti, tj), and by Lemma 4.1.4, it has a higher value sum. Therefore, the maximum value \npath of Pk(til tj ) for h ~ 0(n3) must be a member of subset Pf(ti, tj). In addition, notice that subset \nP{+ ~(ti, tj ) can be con\u00adstructed by having every path in subsets Pf(ti, tj ) and P~(ti, tj ) iterate \nthrough C* one more time. However, as was shown previously, subset P~(t; , tj ) does not contain the \nmaximum value path. Consequently, U+k(ti,tj) = af+k(ti, t3) = ~~(ti, t.j) + il(C*) = a;(t~,tj) +P = Uh(ti, \ntj) +p 1 Theorem 4.I.l states that all nodes in the loop (including both the nodes on or not on the \ncritical cycles) will enter a periodic firing pattern after 0(n3 ) iterations. sug. II&#38; gests the \nfact that we can simulate the loop execution at compile-time by constructing the behavior graph in O(ns \n) iterations to reach such pattern. Since each iteration has (?(n) firings in the simulation process, \nthe actual number of time steps to reach the pattern is @(n*), as shown by the following theorem. Theorem \n4.1.2 Under the eariiest firing de, the cyclic j+ustum of G hatiing one critical cycle can be found in \n0(n4 ) time steps. Proof of Theorem 4.1.2 By Theorem 4.1,1, the time constraint X~+k X: = p, Vti c G, \nk = M(C ) and p = t_l(C*), is satisfied when h z 0(n3 ). In other words, the cyclic frustum appears after \n0(n3 ) times of G are scheduled, Since G consists of n transitions, a total of O(n4 ) firings will be \nperformed. Note that 0(n4 ) firings can be done in at most 0(n4 ) time steps. Since p = t2(C ) is the \nvalue sum of the critical cycle, it is bounded by n. Therefore, under the earliest firing rule, the cyclic \nfrustum of G having one critical cycle emerges in O(n )+n time steps, or simply 0(n4 ), o Since the \nsteady-state equivalent net is an immediate result of the cyclic frustum, we have the following corollary: \nCorollary 4.1.1 Under the earliest firing rute, the steady\u00ad date equivalent net for a SDSP-PN having \nonly one critical cycle can be found in U(n4 ) time steps. 4.2 SDSP-PN Having Multiple Critical Cycles \nFor the case of multiple critical cycles in G, Chretienne proved that the firing of any transition tj \nin G with cy\u00adcle time a obeys the time constraint Xhtk X} = p un\u00adder the earliest firing rule, where \nk, in t 11 1s case, equals the least common multiple of all critical cycles token sum and p equals k \nx a, i.e., he showed that a cyclic frustum appears [9]. Since p depends directly on k, finding the cyclic \nfrustum in a bounded number of time steps involves the derivation of an upper bound for k. We are unaware \nof any proof that gives a polynomial bound for findhg such a bound; therefore, we believe that the problem \nremains open. We now show that X,ht k -X: = p is obeyed by all tjc C for h ~ 0(n2 ) under the earliest \nfiring rule, where k = M(C*), p = Cl(C*), and C* is any critical cycle in G, Let 1 denote the set of \ncritical cycles in G, and let ~, the complement of 1, denote the set of non-critical cycles in G. Note \nthat Lemmas 4.1.1 and 4.1.3 are still vrdid in the case of multiple critical cycles. In the inst ante \nof multiple critical cycles, Lemma 4,1,4 is revised to Lemma 4.2,1. Lemma 4.2.1 VCj cl~ G,m x f?(cj) \n~ ~ ~(ci) Ci ER where R = {Ca,..., cb I ~(ca)+. -.+~(cb) = mx M(Cj) and C.,..., Cb ~ Iu7}. Proof of \nLemma 4.2.1 Theorem 4.2.1 For any G executed under the earliest r-Pa ing schedule and for h ~ 0(n2 ), \nthe time constraint X,h+ - X? = p is obeyed by all tj G C*, where C* is a critical cycle in 1, k = iW(C*), \nandp = fl(C*). Proof of Theorem 4.2.1 With Lemma 4.1.1, this theorem is proven by showing that for h \n~ 0(n2 ), ma.xa~+k(i; ,tj) mFHab(t; ,tj) = p, Vtj 6 C* t; t~ where tiis a member of the set of initially \nenabled transition at time O. Or equivalently, we show that for h ~ 0(TZ2 ), ah+k(ti!tj) = ah(ti, tj) \n+p, Vtj ~ C* where ti is a member of the set of initially enabled transition at time O. Notice that \nPz(ti, tj), the set of paths from titotjwith exactly z tokens, can be partitioned into two disjoint subsets \nPf(tij tJ) and P~(ti, tj), where z >0. Subset Pf(ti,tj)de. notes the set of paths that iterate through \nC* at least once, whiie subset P~(ti, tj ) denotes the set of paths which only touch C*, i.e., C* is \nnot embedded entirely in the path. We show that for every path in subset P~(t~, tj ) there always exists \na corresponding path in subset Pf(ti, tj) which has a greater or equal value sum, provided h ~ (n+ l)k \n+ n. Consequently, the maximum value path in ~k(ti, tj) for h ~ O(n2 ) can always be found in subset \nPf(t; , tj). For h > (n+ l)k + n there exists at least k cycles along any possible paths in Ph(ti, tj). \nBy Lemma 4,1.3 there exists a subset S of those cycles Ci such that ~Ci ~~ M(Ci) is a multiple of k, \nwhere k = M(C*). Assume ~c,e~ M(ci) = ~ X kf(c*), wI E Loopll: First Sum integer, and m >0 for any path \nP. c Ph(tj, tj). Either S is composed of C* m times (i.e., Pm 6 P~(f,i, tj)) or P. may not have the maximum \npath value. This is so because a path P~ can be constructed from P= by replacing all Cj G S with exactly \nm C*. Pa must dso exist in Ph(tij tj), and by Lemma 4.2.1, it will have a greater or equal vrdue sum. \nTherefore, the maximum value path of Ph(ti, tj) for h ~ 0(n2) is always a member of subset P~(ti, tj). \nIn addition, notice that subset P$+ ~(ti} tj) can be con. strutted by having every path in the subsets \nP~(ti} t j ) and P~(t~, tj) iterate through C* one more time. However, aa was shown, the maximum value \npath can always be found in subset P~(ti, tj ). Consequently, ~h+k(ti, ~j) = ag+k(ti, tj) = ~f(ti, tj) \n+ O(cg) = 4f(ti, tj) +P = ah(ti, tj)+p o Theorem 4.2.2 For any G executed under the earlied jir\u00ad ing \nrule, the time constmint X3h+k -X; = p is obeyed by all tj G C* in O(n3) time steps, where C is a critical \ncycle in 1, k = M(C*), and p = O(C*). Proof of Theorem 4,2.2 By Theorem 4.2.1, the time constraint X,h+k \n X; = p is obeyed by all t j E C*, where C* is a critical cycle in 1, k = iW(C*), and p = fl(C*) after \nh ~ O(n2). In other words, it is satisfied after O(n2 ) iterations of G are scheduled. Since G consists \nof n transitions, a total of 0(n3 ) firings are required. Note that O(ns ) firings can be done in at \nmost O(n3 ) time steps. o Experimental Results This section provides simulation results which verify \nthat the cyclic frustum for an SDSP-PN can be found quickly for programs such as the Llvermore loops. \nIn Section 5.2 we add a single clean pipeline (SCP) to the SDSP-PN, producing a unified Petri-net model, \nSDSP-SCP-PN, which models the execution of SDSP loops on a class of dataflow architectures having a clean \nexecution pipeline consisting of 1 stages. We then examine the feasibdity of generating a scheduling \npat\u00adtern for our clean-pipeline machine by rerunning the same set of Livermore loops using this new model. \nListed below are the set of Livermore Loops chosen for study; all were written in SISAL [19, 31]: Loops \nwithout loop-carried dependence (LCD) -Loopl: Hydro Fragment Loop7: Equation of State Fragment  Loop12: \nFirst Difference  Loops with loop-carried dependence LOOP3: Inner Product -Loop5: Tri-Diagonal Elimination, \nBelow the Di\u00adagonal -Loop9: Integrate Predictors Our simulations were performed on a compiler/simulator \ntestbed developed at McGill University [23]. The testbed consists of a prototype SISAL compiler capable \nof produc\u00ading dat aflow code (known as A-Code) [40, 41]. For t hls par\u00adticular study, we modified the \nsimulator to permit analysis of cyclic frustums generated for both SDSP-PN and SDSP\u00adSCP.PN models. The \nsimulator accepts A-code as input and simulates the corresponding firing sequence of the code. 5.1 The \nSDSP-PN Model Table 1 shows the results of simulating an SDSP-PN. In the table, the gize of loop body \nreflects the number of nodes that are repeatedly executed. The start-up initia\u00adtion sequence of a loop \nis excluded. Start time and repeat time in~lcate the times when the initial and the terminal instantaneous \nstates are identified. Length of frustum is the time difference between the terminal and initial instanta\u00adneous \nstates. Transition count records the number of oc\u00adcurrences of a transition that appears in the cyclic \nfrustum, Computation rate is the average firing rate of each SDSP transition in the loop body and equrds \nTransition count Repeat Time Start Time  Finally, BD is a tight bound derived by observation and is \nintended only for comparison purposes. For the SDSP-PN model, we assumed an infinite number of clean \npipelines, each one consisting of a single stage. Note that in each example the repeated instantaneous \nstate is found within 2n time steps. 5.2 The SDSP-SCP-PN Model In thw section we describe the unified \ntimed Petri-net model SDSP-SCP-PN for fine-grhin loop scheduling having re\u00adsource constraints (for a \ncomplete discussion, see [24]). The unified model is constructed by including a single clean hard\u00adware \npipeline (SCP) of 1 stages into the SDSP-PN model. Execution consists of two steps: ~eries expansion \nand run place introduction. Notice that once an instruction enters the SCP, it runs through the pipeline \nwithout interference from other enabled instructions. This implies that the detailed structure of the \nSCP does not need to be explicit. Run-place introduction: We introduce a place p., known as the run place, \nto denote the SCP and mod\u00adify all transitions ti in the SDSP-PN to include p. as both the input and output \nplaces. Place p. is initially marked with a token representing the existence of one SCP. When a transition \nbecomes enabled, it competes for p, to get fired, Series expansion: To denote the fact that one traversal \nthrough SCP takes 1 time units, a series expansion procedure is performed which introduces a new tran\u00ad \nsition for each place in the SDSP-PN to account for time delay. We call the transitions originally appear\u00ad \ning in the SDSP-PN the SDSP transitions, and the ones newly introduced in series expansion, the dummy \n 5 Loop9 is a potential candidate for parallelizing as a DOALL loop; however, it requires subscript analysis \nto expose its parallelism. Here we have examined the loop both ways, with and without LCDS, to increase \nthe tilversity of our testing. Table 1: Experimental Results for the SDSP-PN Model Th Pr step 1 P, 2 \n3 P, 4 Pr 5 B (A, 6 P, 7 P, 8 ( 9 P. 10 Pr I adjacency o initial instantanaous 11 p, li*t stat* marking \n12 P, @ terminal inatantaneoua atsta ME* 13 P. (a) SDSP-PN of L1 (b) SDSP-SCP-PN of L1 (c) Behavior Graph \nafter Series Expansion after run place (p=) introduction Figure3: Construction of the SDSP-SCP-PN transitions. \nEvery SDSP transition is assigned an ex\u00adecution time of 1 whale every dummy transition is as\u00adsigned an \nexecution time of 1-1, where 1 denotes the length of the exeeution pipeline. When /=1, there are no dummy \ntransitions remaining in the final model. In the figure we dwtinguish dummy transitions by bars of a \ndifferent length. Figure 3(a) illustrates the outcome of L1 after a series ex\u00adpansion, and Figure 3(b) \nshows the result of introducing a run place, Theorem 5.2.1 An SDSP-SCP-PN with an initial mark\u00ading h-f; \nis live, safe, and persistent if the SDSP.PN with an initial marking M. is live, safe, and persistent. \nUsing Theorem 5.2.1 we construct the behavior graph for the combined model in a way similar to constructing \none for the SDSP-PN model described previously. With the existence of the run place as a structural contlict, \nchoices appear whenever more than one SDSP transition is enabled. Also, we make the following assumption: \nAssumption 5.2.1 The firing mechanism in the SCP ma\u00adchine will altuays choose one enabled transition \nto fire-it will never idle as long as there is at least one enabled node. The machine can break ties \nby giving priority to the nodes that simultaneously become enabled. The particular priority does not \nmatter; we assume only that the machine exhibits repeatable behavior, i.e., it always makes the same \nchoice given its rule for priority and machine condition, i.e., its instantaneous state. This assumption \nprovides the means for making the tran\u00adsition process of the instantaneous state unique. Thus, the behavior \ngraph will be unique aa long as a particular choice scheme is enforced. Figure 3(c) illustrates a behavior \ngraph derived from the example in Figure 3(b). In thk particular case, the choice resolution is done \nby a decision mechanism which employs a FIFO queue and an adjacency list repre\u00adsentation of the static \ndataflow graphs Similar to the behavior graph of an SDSP-PN, the be\u00adhavior graph of an SDSP-SCP-PN also \nexhibits repeti\u00adtive behavior. This behavior is described by the following Lemma 5.2.1, together with \nAssumption 5.2.1: Lemma 5.2.1 There ezists an instantaneous state in the behavior graph of an SDSP-SCP-PN \nwhich appeara repeat\u00ad edly. Once the machine returns to a previous instantaneous state the same firing \npriority is repeated. As an example, the two sets of highlighted markings shown in Figure 3(c) illustrate \nthe marking portion of the initial and terminal in\u00adstantaneous states. Their associated residual firing \ntime vec\u00adtors are zero vectors. The firing sequence in the steady-state is ADBCE. Finally, Theorem 5.2.2 \nimposes an upper bound on the execution rate of each node in the SDSP-SCP-PN. This upper bound is the \nresult of the resouree constraint im\u00adposed by the single clean pipeline and is independent of the approach \nused for conflkt resolution. When such a bound is achieved, the pipeline is 100~o utilized. 6 Adjacency \nli5t5 are a common representation for directed graphs. Node j is said to be adjacent to node i in a directed \ngraph G if the directed arc (i, j) exists in G. The adjacency list for node i is a list, in some order, \nof all nodes adjacent to i. Theorem 5.2.2 Let G be an SDSP-SCP-PN with n SDSP transitions. The computation \nrate of any SDSP transition in G can never be greater than l/n, i.e., ~ < lfn. Table 2 illustrates the \nfindings of our simulation of the SIXP-SCP-PN. The same set of measurements which were shown previously \nwere also examined here. In addition, we included processor usage to show the single clean pipeline utilization \nfor each loop. The value of BD is %3x 2 time steps, and 1 equals eight in th~ case. Again, the cyclic \nfrustum for all of these benchmarks can be found within O(n) time steps. The fast detection of the the \ncyclic frustums for both models gives indication of the feasib~lt y of using thw method for practical \ncompilers. 6 Minimum Storage Allocation in an SD SP-PN For an SDSP-PN PN, the optimal computation rate \ny is determined by the cycle time of the critical cycle in the net: kf(ck) 1 y = min Ti@J for d simple \ncycles ck in PN  fl(ti) { 1 Assume that all transitions have the same execution time of 1 cycles, \nLet G be the corresponding SDSP for PN. The computation rate for G is . M(ck) for ~ simple cycles Ck \nin G 7=m1n -m-x-T {} where ~k is the number of nodes in the cycle ck, and ~((?k) is the number of tokens \ninitially residing on cycle ck. For each cycle ck we call the ratio _ the balancing ratio of C~. Critical \ncycles will always have the smallest balancing ratio. Without changing the structure of the loop body, \nthe bal\u00adancing ratio of any cycle made entirely of data arcs (either forward or backward) cannot be altered; \nhence, its computw tion rate is fixed. The computation rate of the critical cycles put a hard upper bound \non the maximum computation rate of G. One immediate problem is determining the minimum amount of storage \nto maintain the same computation rate under the performance bound. Observe that one storage location \nis allocated for each pair of forward/feedback data and acknowledgement arcs in the graph. A token on \nthe acknowledgement arc implies that the storage of the corresponding forward/feedback data arc is empty, \nwhale a token on the forward/feedback data arc implies that the location is occupied. The total amount \nof storage allocated to the loop equals the total number of pairs of forward/feedback data arcs and acknowledgement \narcs. For any cycle ck in an SDSP other than the critical cy\u00adcle, we. can reduce storage without decreasing \nthe overall computation rate. We illustrate this in Example L2. Note that in Figure 2 the critical cycle \nC* in L2 is CDEC which contains the feedback data arc. The maximum computation rate is l/3i, i.e., the \nbalancing ratio is 1/3. Obviously, any change in the balancing ratio of C* will affect the computa\u00adtion \nrate of the entire graph. At the same time, the cycles CZ (ABA) and C, (B.DB) both have a higher balancing \nratio, 1/2. One storage location is allocated to Cl and the other 7 In a dynamic dataflow machine (such \nas Monscmn) this Corre\u00ad sponds to a frame of storage being allocated to each loop instance during loop \nunraveling [34]. Table 2: Single Clean Pipeline with Eight stages ?orw*d data ~ Ua Acknowledgement ......* \nUa P.a.dback &#38;t* ~ we Figure 4: Minimize Storage Allocation to Ca. Since Cl and Ca are not critical \ncycles, we can reduce their storage allocation as shown in Figure 4. Now, the new cycle CS (ABDA) has \na balancing ratio of 1/3, and only one location needs to be allocated. Compare Figure 4 with Figure 2, \nthe optimal computation rate remains unchanged, but the total storage allocation of the loop is reduced \nby 1/6. Related Work In this section we compare our method with related work in software pipelining. \nBased upon this comparison, we iden\u00adtifv directions for future research. -Our work is directly applicable \nto conventional software pipelining which performs loop scheduling by computing a static parallel schedule \nto overlap instructions of a loop body from different iterations. One advantage of software pipelin\u00ading \nis that it provides a direct way of exploiting parallelism across all iterations of a loop. This is acldeved \nwithout ex\u00adplicit use of loop unrolling and results in highly compact object codes. Software pipelining \nhas been proposed for syn\u00adchronous parallel machines as well aa for pipelined machines [1, 2,3,17,18, \n29]. In this paper we examined a class of loops (SDSP) where an implicit upper bound is imposed on the \nnumber of concur\u00adrent iterations at any point in time, that is, there could be no more than k active \niterations concurrently, with k being the number of nodes along the longest dependence path in the loop \nbody. We demonstrated that with an ideal machine our loop scheduling method finds a time-optimal schedul\u00ading \npattern in polynomial time, for SDSP loops either with or without conditional branches in their loop \nbodies. New contributions in establishing the bounds were discussed in Section 4 and compared with related \nwork. When limited resources are available, the general prob\u00adlem of finding a time-optimal schedule for \nsoftware pipelin\u00ading becomes NP-complete [25]. There have been two main approaches to solve th~ problem: \nCompiler-based methods In [17] a scheduling algorithm was proposed in which constraints are in the form \nof the number of instruc\u00adtions that can be executed in parallel. In [29] a method was proposed for software \npipelining with re\u00adsource constraints using the Warp systolic architec. ture, This method allows general \nresource constraints and achieves near optimal schedules using heuristics based upon knowledge of the \ntarget architecture. In [3] a bounded resource-constraint algorithm waa pre\u00adsented which achieves asymptotic \ntime-optimality un\u00adder the constraint that no more than k iterations are available for scheduling during \nany time step, where k is a constant. The authors, however, do not indlcat e how long the algorithm takes \nto find this schedule, Special hardware support In the ESL polycyclic machine [38] and the Cydra-5 [39], \nspecial hardware was used to simplify comPile\u00adtime scheduling for software pipelining. The draw\u00adbacks \nare the extra hardware cost and restrictions, e.g., the polycyclic machine can handle only acyclic graphs. \n In thk paper resource constraints were considered in a case study in which the machine employed a single \nclean pipeline. The absence of structural hazards in the pipeline substantially simplified the resource \nconstraints and made it possible to consider both precedence and resource con\u00adstraints using a single \nunified model, The feasibility of the combined model aa a baais for compile-time loop scheduling was \ndemonstrated by simulation results presented in the last section. Although a clean pipeline requires \nhardware sup\u00adport, we believe that such support is justified. Furthermore, the cost and complexity of \nadditional hardware should be less than that required in the polycyclic and Cydra-5 archi\u00adtectures. To \nour knowledge, most other work in this area has been based upon flow graphs as program representation. \nIn con\u00adtrast, we use dataflow graphs. Dataflow graphs can be easily modeled by Petri nets, allowing the \nuse of powerful modeling facilities for concurrency and performance measurement. The following extensions \nto our method are being pur\u00adsued: o Application of the method to dataflow models other than static dataflow, \nto study bounded time-optimal scheduling. Two such models are the tagged-token dataflow model [5] and \nthe FIFO-queued dataflow model [27]. Both models have eliminated the one-token-per-arc restriction assumed \nin the static dataflow model. The tagged-token model allows a pool of tokens on a single arc and distinguishes \ntokens by their colors. For the FIFO-queued model, each arc is a FIFO queue capable of holdlng multiple \ntokens, Storage optimization. The results from using our model suggest that critical cycles in a program \nde\u00adtermine the achievable performance of a software pipelined loop. This opens up new opportunities for \nstorage optimization resulting from time-optimal scheduling. For example, storage minimization prob\u00adlems \nof various dataflow graph models might be for\u00admulated and studied with thu insight.  Application to \nother machine models. The scheduling method described might be applied to other machine models to verify \nits effectiveness.  8 Conclusions In this paper we outlined a new scheme for fine-grain loop scheduling \nbased upon Petri-net models. An algorithm to derive a time-optimal loop schedule in polynomial time was \npresented for machines supporting an earliest firing sched\u00adule. We also described, through a case study, \nhow to inte\u00adgrate resource limitations into our basic model. The algo\u00adrithm we used for detecting a cyclic \nfrustum has been imple\u00admented in an experimental compiler, and preliminary results have demonstrated \nthe feasibfity of the proposed method\u00ad ology in practical examples, using several of the Livermore benchmark \nloops. 9 Acknowledgment We thank the Natural Science and Engineering Research Council (NSERC) for a grant \nin supporting thk work. Whale undergoing the development of this paper, we enjoyed sup\u00adport from the \nmembers of the ACAPS (Advanced Computer Architecture and Program Structures) Group at McGill Uni\u00adversity. \nIn particukr, we thank Herbert Hum and Jean-Marc Monti for their valuable ducussions. We also thank Russell \nOlsen for hk scrupulously proof reading of the final draft and suggestions for its improvement. Finally, \nwe thank Vivek Sarkar for many constructive suggestions which help significantly in both enhancing the \ntechnical contents and improving the organization of tlds paper. References [1] A. Alken. Compaction-based \nparallelization. (PhD the\u00adsis), Technical Report 88-922, Cornell University, 1988. [2] A, Alken and A. \nNlcolau, Optimal loop parallelization. In Proceedings of the 1988A CM SIGPLAN Conference on Programming \nLanguages Resign and Implementa\u00adtion, June 1988. [3] A. Aiken and A. Nlcolau. A realistic resource\u00adconstrained \nsoftware pipelining algorithm. In Proceed. ings of the Third Workshop on Programming Languages and Compilers \nfor Parallel Computing, Irvine, CA, Au\u00adgust 1990. [4] Arvind and D. E. Cul.ler. Dataflow architectures. \nAn. nui.d Reuiewa in Computer Science, 1:225-253, 1986. [5] Arvind and K. P. Gostelow. Some relationships \nbetween asynchronous interpreters of a data flow language. In E. J. Neuhold, editor, Formal Description \nof Program\u00adming Concepts, pages 95-119. North-Holland, 1978. [6] Robert A. Ballance, Arthur B. Maccabe, \nand Karl J, Ottenstein. The program dependence web: A repre\u00adsentation supporting control-, and demand-driven \nin\u00adterpretation of imperative languages. In ACM SIG-PLAN 90 Conference on Programming Language De\u00ad$ign \nand Implementation, White Plains, NY, June 20\u00ad22, 1990, page 257. ACM, 1990. [7] M. Beck and K. Plngrdi. \nFrom control flow to dataflow. Technical Report TR-89-105O, Department of Com\u00adputer Science, Cornell \nUniversity, Ithaca, NY, 1989. [8] J. Carlier, P. Chretienne, and C. Glrault. Modeling scheduling problems \nwith timed Petri nets. In G. Goos and J. Hartmanis, editors, Advances in Petri Nets, LIVCS .?~O, pages \n62-82. Springer-Verlag, Berlin, Hei\u00addelberg, NY, 1984. [9] P. Chretienne, Les Reseaux de Petri Temporiads \n(These d Ltat). PhD thesis, Institut de programmation, Uni\u00adversit6 P. et M. CURIE, C. N.R.S.-E.R.A. 592, \nSeptem\u00adber 1984. [10] P. Chretienne. Timed event graphs: A complete study of their controlled executions. \nIn International Work\u00adshop on Timed Petri Nets, pages 47 54, Torino, Italy, July 1985. IEEE Computer \nSociety Press. [11] E. G. Coffman. Computer and Job-Shop Scheduling Theory. John Wdey and Sons, New York, \n1976. [12] F. Commoner, A. W, Holt, S. Even, and A. Pnueli. Marked chrected graphs. Journal of Computer \nand Sy#\u00adtem Sciences, 5:511-523, 1971. [13] J. B. Dennis. Packet communication architecture. In Proceedings \nof the 1975 Sagamore Computer Confer\u00adence on Parallel Processing, 1975. [14] J. B. Dennis. Data flow \nsupercomputers. IEEE Com\u00adputer, 13(11):48 56, November 1980. [15] J. B. Dennis. Data flow for supercomputers. \nIn Pro. ceedings of the 1984 CornpCon, March 1984. [16] J. B. Dennis, J. B. Fosseen, and J. P. Linderman. \nData flow schemas. In International Sympo$ium on Theoret\u00adical Programming, LNCS 5, pages 187-215. Springer-Verlag, \nBerlin, Heidelberg, NY, 1972. [17] K. Ebciojjlu. A compilation technique for software pipelining of loops \nwith concMional jumps. In Proceed\u00adings of the 20th Annual Work#hop on Microprogram\u00adming, December 1987. \n[18] K. Ebcioj$u and A, Nlcolau. A global resource\u00adconstrained paralle~lzation technique. In Proceedings \nof the ACM SIGARCH International Conference on Su\u00adpercomputing, June 1989. [19] J. T. Fee. An analysis \nof the computational and parallel complexity of the Livermore loops. Parallel Computer, 8(7):163-185, \nJuly 1988. [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] [30] [31] [32] [33] [34] [35] J, Ferrante, \nK. J. Ottenstein, and J. D. Warren. The program dependence graph and its use in optimiz\u00adtion. ACM Tran8action8 \non Programming Language8 and Sy8tem8, 9(3):319 349, July 1987. G. R. Gao. A pipelined code mapping scheme \nfor static dataflow computers, Technical Report TR-371, Labo\u00adratory for Computer Science, MIT, 1986. \nG. R. Gao. A Code Mapping Scheme for Dataflow Soft\u00adware Pipe lining. Kluwer Academic Publishers, Best \non, December 1990. G. R. Gao and Z. Paraskevas. Compiling for dataflow soft ware pipelining. In D. Gelernt \ner, A. Nicolau, and D. Padua, editors, Language8 and Compiler8 for Par\u00adallel Computing, pages 275-303. \nMIT Press, 1990. G. R. Gao, Y. B, Wong, and Qi Nlng. A Petri-Net model for fine-grain loop scheduling. \nACAPS Tech\u00adnical Memo 18, School of Computer Science, McGill University, Montreal, January 1991. M. R. \nGarey and D. S. Johnson. Computer8 and Intractability: A Guide to the Theory of NP-Completeness. W. H. \nFreeman and Company, New York, 1979. IBM Corporation. IBM RISC Sy8tem/6000 Technology, 1990. G. Kahn. \nThe semantics of a simple language for par\u00adallel processing. In Information Processing 74, pages 471 \n475, 1974. L. Kohn and N. Margulis. Introducing the Intel i860 64\u00adblt microprocessor. IEEE Micro, pages \n15 30, August 1989. Monica Lam. Software pipelining: An effective schedul\u00ading technique for VLI W machines. \nIn Proceeding of the 1988 ACM SIGPLAN Conference on Programming Language8 De8ign and Implementation, \npages 318-328, Atlanta, GA, June 1988. J. Magott. Performance evaluation of concurrent sys\u00adtems using \nPetri nets. Information Processing Letter8, North-Holland, 18:7-13, January 1984. J. R. McGraw and et \nal. SISAL: Streams and iter\u00adation in a single assignment language language ref\u00aderence manual version \n1.2. Technical Report M-146, Lawrence Livermore National Laboratory, 1985. A. Nicolau, K. Pingali, and \nA. Aiken. Fine-grain com\u00adpilation for pipelined machines. Technical Report TR\u00ad88-934, Department of Computer \nScience, Cornell Uni\u00adversit y, Ithaca, NY, 1988. R. S. Nikhil. Id (Version 88.o) reference manual. Com\u00adputation \nStructures Group Memo 284, Laboratory for Computer Science, MIT, March 1988. G. M. Papadopoulos and D. \nE. Culler. Monsoon: An explicit token-store architect ure. In Proceedings of the Seventeenth Annual International \nSymposium of Com\u00adputer Architecture, Seattle, Wu8hington, pages 82-91, 1990. J. L. Peterson. Petri Net \nTheory and the Modeling of System8. Prentice-Hall, Inc., Englewood Cliffs, NJ, 1981. [36] C. V. Ramamoorthy \nand G. S. Ho. Performance eval\u00aduation of asynchronous concurrent systems using Petri Nets. IEEE Transaction \non Computers, pages 440\u00ad448, September 1980. [37] C. Ramchandani. Analysis of asynchronous concur\u00adrent \nsystems. Technical Report TR-120, Laboratory for Computer Science, MIT, 1974. [38] B. R. Rau and C. D. \nGlaeser. Some scheduling tech\u00adniques and an easily schedulable horizontal architecture for high performance \nscientific computing. In Proceed\u00adings of the 14th Annual Work8hop on Microprogram\u00adming, pages 183 198, \n1981. [39] B. R. Rau, D. Yen, W. Yen, and R. A. Towle. The Cy\u00addro 5 departmental supercomputer. IEEE \nComputer, 22(1):12-35, January 1989. [40] R, Tie. The A-code assembly language reference man\u00adual. ACAPS \nDesign Note 02, School of Computer Sci\u00adence, McGill University, Montreal, July 1988. [41] R. Tie. DASM: \nThe A-code data-driven assembler pro\u00adgram reference manual. ACAPS Design Note 03, School of Computer \nScience, McGill University, Montreal, July 1988. [42] R. F. Touzeau. A FORTRAN compiler for the FPS\u00ad164 \nscientific computer. In Proceedings of the ACM SIGPLAN 84 Symposium on Compiler Construction, pages 48-57, \nJune 1984. [43] J. D. Unman. NP-complete scheduling problems. .lour\u00adnal of Computer Sy8tem Science, 10:384-393, \n1975. A The Petri Net Model Since the original dissertation of C. A. Petri, Petri-net the\u00adory has emerged \nas an important tool for system analysis and modeling of a wide range of applications. Petri-net the\u00adory \nallows a system to be modeled by a Petri net, a math\u00adematical representation. Analysis can reveal important \nin\u00adformation about the structure and dynamic behavior of the modeled system, information which can be \nthen used to sug\u00adgest system improvements. A.1 The Model and Notations A Petri net PN is a three-tuple \n(P, 2 , A), where P is a non\u00adempty set of places denoted by {PI, pz, . . ., pn}) T is a non\u00adempty set \nof transitions denoted by {tl,i2, ..., k}, and A is a non-empty set of directed arcs such that P # 0, \nT # 0, PnT=O, A~Px TuTx P. Pictorially, P, T,and A are represented by circles, bars, and directed arcs, \nrespectively. By convention, dot notation has been employed as a means of simplifying the representation \nfor a set of places or a set of transitions. Shown below is the list of possible usages of dot notation, \nwhere PI and T1 denote the subset of P and T in PN. In addition, (t, p) denotes the directed arc from \ntto p while (p, t)denotes the directed arc from p to t. .p = {t I (t, p) E A} (the set of input transitions), \n@ P. = {t I (P1 ~) E -4} (the set of output transitions), 4 = {p I (p, t) E A} (the set of input places), \no t. = {p I (t,p) G A} (the set of output places), +, Tl = UVt< CT1 b TI = tin) lJv~ieT1 [ .s{ and Is-I \ndenote the number of elements in the set .s and s. respectively, where s can be a place/transition or \nset of places/transitions.  A.2 Marking and Firing Rules A marking of a net is a function M : P -+ I, \nwhere I is the set of non-negative integers, The non-negative integer associated with a place p, denoted \nby &#38;f(p), represents the number of tokens on the place. A Petri net with a marking is always referred \nas a marked Petri net. Marking MO is always referred to as the initial marking of a net. A transition \ntin Petri net PN is said to be enabled by the marking M, denoted by M ~, if and only if VP E d, M(p) \n>0. An enabled transition can be fired. The firing of an enabled transition t is done by removing one \ntoken from each of the input places p c .tand depositing one token on each of its output places p c t..Assuming \nthe marking wNlch enables tbe Ikf and the marking which is obtained by firing tbe M , firing can be expressed \nas M $ M . A marking M is said to be reachable from M if M can be obtained by firing an enabled transition \nt, M ~ M , or by firing a sequence of transitions r = tatb . . . t;, M % Ma ~ . . . ~ M . In the latter \ncase, ~ = tata... ti is termed the firing sequence. A firing sequence -r is called cyclic firing sequence \nif, for any marking M, M ~ M and z is not empty. Let u be a firing sequence. Then ~(u) is called the \nfiring vector of u, where .f(~)ij denoting the i-th element in the vector, is the number of occurrences \nof transition ti in u. The ~orward marking clam fi of a marking M is the set of markings that are reachable \nfrom M. Conceptually, each distinct marking of a Petri net represents a distinct state in the modeled \nsystem. Similarly, the forward marking class I&#38; of the marking M. represents the set of reachable \nstates of the modeled system. A.3 Properties Liveness, Boundness, and Persis\u00adtence A marking M is live \nfor a transition tif and only if for every marking Mi in the forward marking class A? there exists a \nfiring sequence which fires t.A marking M is live for a Petri net PN if and only if it is live for every \ntransition in the net. If PN represents a model of a system, the liveness property of PN implies that \nthe modeled system will never deadlock. A marking M is bounded for a place p if and only if there exists \nan integer IV such that for every marking Mi E ~, Mi(p) < N. If N = 1, the marking AZ is called safe \nfor p. A marking M is bounded (or safe) for the Petri net PN if and only if kf is bounded (or safe) for \nevery place in the net. Note that if PN is bounded, the set of reachable states of the modeled system \nmust be finite. A Petri net is persistent if and only if for all tl, tzE 2 ,tl# ta and any reachable \nmarking M, M 3 and J4 % imply M ~ (the firing of transition tzafter the firing of transition tl);i.e., \nif tland tzare enabled at a reachable marking, the firing of one cannot disable the other; other\u00adwise, \nit is said to have choice. A.4 Some Special Structures Self-loop is a transition which has both input \nand out\u00adput from the same place.  A Petri net PN is said to have structural confZict if there exists \na place p in PN such that Ip. I >1. The existence of structural contlicts is a necessary condi\u00adtion for \nsituations where choice might occur.  A Petri net model is said to be consistent if and only if there \nexists a non-zero integer assignment to each transition in the net (where each arc is assumed to carry \nthe integer of its at t ached transition), such that, at each place, the sum of the integers assigned \nto each of its input arc equals to the sum of the integers as\u00adsigned to each of its output arc. The assignment \nen\u00adsures the existence of repeatable behavior in the model so that it is meaningful to talk about c~cle \ntime. Here are the two known theorems [37] on consistency:  Theorem A.4.I A Petri net P is consistent \nif and only if there exists an initial marking M for which there exists a cyclic firing sequence. -Theorem \nIL4.2 A Petri net P which has a live and bound marking is consistent. A corollary of the above theorem \nis that, if a Petri net has a live and bound marking, there exists a cyclic firing sequence. A.5 Marked \nGraphs A class of Petri nets which is important to this work is the marked graphs. Definition A.5.1 A \nPetri net PN = (P, T, A) is called a marked graph if and only if Vp c P, I.p! = Ip.I = 1. Apparently, \nmarked graph must be persistent because, for each place in the graph, there is only one output tran\u00ad \nsition associated with it. Here are some known results for marked graphs (for proofs, see [12]): Theorem \nA.5.I A marking is live if and only if the token count of every simple c~cle is positives Theorem A.5.2 \nA live marking is safe if and only if every edge in the graph is in a simple cycle with token count 1. \nTheorem A.5.3 If r is a cyclic firing sequence such that M $ M, all transitions have been fired an equal \nnumber of times. A.6 Timed Petri Nets Adding the notion of time to the basic Petri-net model en\u00adables \nthe characterization of system performance. In this paper we consider a deterministic time, expressed \nby a non\u00adnegative integer number, can be assigned to each transition in the basic Petri-net model. The \nmodel described below is made up of the original timed Petri-net model introduced by 8A directed path \nP.i?jpk. . .t,p~ such that all places and transi\u00adtions are different except p; and pm. Ramchandani [37], \nand the concept of instantaneous state subsequently developed by Chretienne [10]. Formally, a timed Petri \nnet is defined with a pair (PN, 0), where PN is the basic Petri-net tuple (P, T, A) and S2 is a function \nthat assigns an integer, non-negative number ri to each transition ti in the net (i.e., fl : T + R, where \nR is the set of non-negative integer numbers). The value Ti denotes the ezecution time (or the firing \ntime) taken by transition t;. The state of the timed Petri net at time u is no longer de\u00adscribed only \nby the current marking at time u (ilfti ) because some transitions might still be processing at time \nu. A new concept of residual jiring time vector, R, is introduced to keep track of on-going executions \nat each time step. l?~(t~) stores the remaining execution time of transition tiat time u. Accordingly, \nAG and Ru together define the instanta\u00adneous state of a timed Petri net. We have also made the following \ntwo assumptions re\u00adgarding the firing rule of the enabled transitions: Assumption A.6.1 Two distinct \nfirings of the same tran\u00ad sitions cannot overlap. To formallg enforce thiu rule, each transition in the \nnet is assigned a distinct self-loop of its own with only one token in it. Though we do not draw them \nexplicitly, they are implicitly as$umed, Assumption A.6.2 Transitions are fired as soon as they are enabled, \nwhich has been termed as the earliest firing rule. A.? Optimal Computation Rate Timed Petri nets have \nbeen applied in the study of concur\u00ad rent systems to determine the cycie time or equivalently the computation \nrate. Next we review the method for obtaining the cycle time of a marked graph. Definition A.7.1 The \ncycle time of transition t; is dejined a8 ~m Xn &#38;  7b-m n where X? is the time at which transition \ntiinitiates itsn-th ezecution. Here are some known results for timed marked graph from [36]: The number \nof tokens in a simple cycle remains the same after any firing sequence.g c All transitions in a marked \ngraph have the same cycle time. o The cycle time is computed by (ck) ~(ti)  m w {} where k = 1,2 t...~gandtieff; \n~(ck) = Et .cCk ~(~i) = sum of the execution times of the t ransit;on in simple cycle ck; M(pi) = total \nnumber of tokens in (ck) = zpi~ck Simple CyCle Ck; A directed path ~itipj . . .t~p~ such that all places \nand transi. tions are different, except pi and pm. q = number of simple cycles in the net excludlng \nthe self-loop implicitly assumed for each transition; the cycle time of each self-loop is reflected by \nil(ti), Vti ET. The computation rate y of a transition is the average number of firings of that transition \nin unit time and is computed by the reciprocal of the cycle time. M(k) 1 ~ = min w qt;) { } where k=l,2, \n. . ..gandti CT The simple cycle C* which gives the maximum cycle time or equivalently the minimum computation \nrate is known as the critical cgcle. The cycle time of a timed marked graph can be obtained by enumerating \nevery simple cycle in the graph; however, the time complexity of the enumerating process can turn out \nto be exponential because there exists a marked graph with an exponential number of simple cycles [30]. \nA more efficient approach is given in [30] where the cycle time prob\u00adlem is formulated into a linear \nprogramming problem, and the problem has a theoretical polynomial bound. The above computation rate y \nis optimal or time-optimal in the sense that it is the maximum achievable computation rate under any \nmachine models [36, 37]. It can be achieved when a machine model has enough parallelism to execute all \nenabled transitions as soon as they become enabled. \n\t\t\t", "proc_id": "113445", "abstract": "", "authors": [{"name": "Guang R. Gao", "author_profile_id": "81100134147", "affiliation": "School of Computer Science, McGill University, 3480 University, Montr&#233;al, Qu&#233;bec H3A 2A7", "person_id": "P100128", "email_address": "", "orcid_id": ""}, {"name": "Yue-Bong Wong", "author_profile_id": "81100188876", "affiliation": "School of Computer Science, McGill University, 3480 University, Montr&#233;al, Qu&#233;bec H3A 2A7", "person_id": "PP31100775", "email_address": "", "orcid_id": ""}, {"name": "Qi Ning", "author_profile_id": "81332518612", "affiliation": "School of Computer Science, McGill University, 3480 University, Montr&#233;al, Qu&#233;bec H3A 2A7", "person_id": "PP39025495", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/113445.113463", "year": "1991", "article_id": "113463", "conference": "PLDI", "title": "A timed Petri-net model for fine-grain loop scheduling", "url": "http://dl.acm.org/citation.cfm?id=113463"}