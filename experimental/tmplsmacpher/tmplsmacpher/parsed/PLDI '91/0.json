{"article_publication_date": "05-01-1991", "fulltext": "\n Efficient and Exact Data Dependence Analysis Dror E. Maydan, John L. Hennessy and Monica S. Lam Computer \nSystems Laboratory Stanford University, CA 94305 Abstract fori=lto lOdo Data dependence testing is the \nbasic step in detecting loop level parallelism in numerieal programs. The prob\u00adlem is equivalent to integer \nlinear programming and thus in general cannot be solved efficiently. Current methods in use employ inexact \nmethods that sacrifice potential parallelism in order to improve compiler efficiency. This paper shows \nthat in practice, data dependence can be computed exactly and efficiently. There are three major ideas \nthat lead to this result. First, we have devel\u00adoped and assembled a small set of efficient algorithms, \neach one exact for special case inputs. Combined with a moderately expensive backup test, they are exact \nfor all the cases we have seen in practice. Second, we intro\u00adduce a memorization technique to save results \nof previous tests, thus avoiding calling the data dependence routines multiple times on the same input. \nThird, we show that this approach can both be extended to compute distance and direction vectors and \nto use unknowns ymbolic terms without any loss of accuracy or efficiency, We have im\u00adplemented our algorithm \nin the SUIF system, a general purpose compiler system developwl at Stanford. We ran the algorithm on \nthe PERFECT Club Benchmarks and our data dependence analyzer gave an exact solution in all cases efficiently, \n1 Introduction Work on parallelizing programs written in Fortran-hke languages has concentrated on parallelizing \nloops in the source code. To do this, one must be able to analyze array reference patterns. For example: \nThis research was supported in perr by a fellowship from AT&#38;T Betl Laboratories and by DARPA contract \nNOO014-S7-K-0S28. Permission to copy without fee all or part of this material ie granted providad that \nthe copies are not made or distributed for direct commercial advantage, the ACM copyright notice and \nthe title of the publication and its date appear, and notice is given that copying is by permission of \nthe Association for Computing Mschinery. To copy otherwise, or to republish, requires a fee andlor specific \npermission. @l 1991 ACM 0-89791-428-7/91/0005/0001 ...$1.50 I1 Proceedings of the ACM SIGPLAN 91 Conference \non Programming Language Design and Implementation. Toronto, Ontarior Canada, June 26-28, 1991. a[i] = \na[i+10]+3 end for fori=lto lOdo a[i+l] = a[i]+3 end for In the first loop all the iterations can be \nexecuted con\u00adcurrently since the locations being written do not over\u00adlap those being read. In the second \ncase, however, each read refers to the value written in the previous iteration, forcing sequential execution. \nData dependence testing answers the question of whether two array references can refer to the same location \nacross iterations. Since there are no integers i, i such that 1< i, i <10 and i = i + 10, we say that \nthe two references in the first ex\u00adample are independent, Since there are integers i, i such that 1< \ni, i <10 and i + 1 = i , the two references in the second example are dependent. Ideally, one would always \nlike to know definitively whether or not the references are dependent. Assuming independence when the \nreferences are in fact &#38;pendent can lead to incorrect results. Assuming dependence can inhibit parallelism. \nIn the worst case, though, the price of exactness is an unacceptably high computation cost for the compiler. \nAs we will show, data dependence testing is equivalent to integer programming, and the most efficient \ninteger programming algorithms known either depend on the value of the loop bounds or are order O(n (n)) \nwhere n is the number of loop variables [8] [9] [13]. Even for the small cases encountered in practice, \nthese algorithms are too expensive. Many algorithms have been proposed for this prob\u00adlem, each one selecting \ndifferent tradeoffs between ac\u00adcuracy and efficiency. Traditional algorithms attempt to prove independence, \nassuming dependence if they fail [2][41[181[191. If such an algorithm returns dependent, we do not know \nif an approximation was made. Some work has been done on algorithms which are guaranteed to be exact \nfor special case inputs. Simple loop residue [15] and Li and Yew s work [10] fall into this category. \nLittle work has been done to analyze either the accuracy or the efficiency of these algorithms in practice. \nNone of these algorithms, as far as we know, have been defini\u00adtively shown to be both accurate enough \nand efficient enough . In fact, Shen, Li and Yew found that cases such as coupled subscripts appear frequently \nand cannot be analyzed accurately using traditional algorithms [141. Our approach is to use a series \nof special case exact tests. If the input is not of the appropriate form for an algorithm, then we try \nthe next one, Using a series of tests allows us to be exact for a wider range of inputs. We evaluated \nour algorithms on the PERFIXX Club [71, a set of 13 scientific benchmarks, and found the algorithms to \nbe exact in every case. Cascading exact tests can also be much more efficient than cascading inexact \nones. By attempting our most applicable and least expensive test tirst, in most cases we can return a \ndefinitive answer using just one exact test, even the dependent ones. Even in the other cases, we only \nneed to check the applicability of multiple tests. We never have to apply more than one. In contrast, \ncascading inexact algorithms would require using all the tests on at least alt the truly dependent cases. \nAs we will show, most cases encountered in practice are in fact dependent. In fact, most direction vectors \ntested are dependent as well. Next we turn towards increasing the efficiency of our approach. Our basic \ntests range from inexpensive to moderately expensive. It has been said that dependence testing is performing \na large number of tests on rela\u00adtively small inputs [11]. We show that in actualiiy it is performing \na small number of unique tests on small in\u00adputs repeatedly. There is little variation in array reference \npatterns found in real programs, and most bounds tend to go from 1 to n where n is the same throughout \nthe pro\u00adgram. Thus, one can save much computation by using memoization,l remembering the results of previous \ntests. We introduce a hashing scheme to do this efficiently. We extend our approach to distance and direction \nvec\u00adtors. We lirst use the standard hierarchical approach based on Burke and Cytron s work [5]. While \nour al\u00adgorithms remain exact, we find that this approach leads to a large increase in computational cost. \nSimple pruning methods bring these costs back down. Finally, we extend our approach to handle symbolic \n terms. This preserves the exactness property of the algo\u00ad rithms with very little increase in cost. \n Problem Definition We first give the standard definition for the general prob\u00adlem of data dependence \ntesting. In the general case we allow multi-dimensional arrays and nested trapezoidal 1Maoimti~ is a \nteckique to remember the results of previ~s ccinputations, previously used to implement catl-by-need \narguments in LISP compilers.[1]. loops, We restrict ourselves to cases where loop bounds are integral \nlinear functions of more outwmdly nested loop variables and where all array references are integral linear \nfunctions of the loop variables. These condition do not necessarily have to be met in the source program, \nWe use optimization techniques (constant propagation, induction variable and forward substitution [16]) \nm in\u00adcrease the applicability of these conditions. Given the following general normalized (we normalize \nthe step size to 1) loop: for il =L1to U1do for i2 = L2(i1) to U2(i1) do ... for in = .L~(il,... ,i~_I) \nto U~(il,... ,i~_l) do a[~l (i )] [f2(31... Um(?)l = . . . ... -a[f((Ol[f;(@.. [%($ end for end for end \nfor such that all the L, U, f, f are known linear functions. The two references are dependent iff 3 \ninteger il, . . . ,i~ ,i~,... ,i~ such that fl (~ = f;(:) , . . . . fm(~=f~(:) L1 <ii, if <Ul ... Ln(il,... \n,il-l ) < in,i~ S Until,..., il)-l) In matrix form this is equivalent to where integer matrices Al \nand Az and vectors b; and b; are given. By replacing any equality az = b in (1) by the two inequalities \naz < b and az < b we see that this is equivalent to 3 integral i? such that AZ < ; (2) 2.1 Equivalence \nof Data Dependence to In\u00adteger ~ogramming Ideally one would like the data dependence test to be exact. \nUnfortunately, data dependence in general is ex\u00adactly equivalent to integer programming, a well studied \nproblem. The standard integer programming problem is [13] find the max i? such that AZ< ;, 2 integral \n(3) It is clear that (2) is a special case of (3) so data depen\u00addence can be reduced to integer programming. \nAnother polynomially equivalent version of integer programming is [13] 3 E such that Ai? = ;, i ~ O, \n2 integral (4) one cart reduce (4) to data dependence testing by con\u00adstructing the following program \nfor xl = O to unknown do ... for Xn = O to unknown do a[A1,lzl +... A1,~zJ...[A~,l, zl + . ..] = . . \n. . . . = a[ A1,~+lzl +... A1,2~X~].. .[...] end for end for All integer programming algorithms that \nwe know of are tcmexpensive in the worst case. IP is NP-Complete, any existing algorithm either depends \nexponentially on the number of variables and constraints or depends linearly on the size of the coefficients. \nTypical program have very few array dimensions and do not have deeply nested loops, Therefore being exponential \nin the number of variables and constraints could be acceptable, The size of the coefficients, though, \ncould conceivably be very large. The complexity of most common algorithms (branch and bound, cutting \nplane) depend on the size of the co\u00adefficients in the worst case. Lenstra [9] and Kannan [8] have developed \nalgorithms that do not depend on the co\u00adefficients, but in the worst case, Kannrtn s algorithm is O(nO[ \nJ) where n is the number of variables. Unfor\u00adtunately, this is much too expensive to use in general data \ndependence testing, Therefore, we do not believe it is possible to develop a practical test that will \napply to every conceivable case. Nonetheless, as we next show, special case algorithms which are efficient, \napply to all the examples we have found in our benchmarks.  3 Data Dependence Tests In this section, \nwe describe the individual tests used in our approach. 3.1 Extended GCD Test We use Banerjee s Extended \nGCD test [4] as a prepro\u00adcessing step for our other tests. While the test itself is not exact, it allows \nus to transform our problem into a simpler and smaller form, increasing the applicability of our other \ntests. This test solves the simpler questiom Ignoring the bounds, is there an integral solution to the \nset of equa\u00adtions. From equation (1) we see that this is equivalent to does there exist an integer vector \nF such that A; = b. If this system of equations is independent then we know that the original system \nis also independent since the loop bounds merely introduce additional constraints. If it is dependent, \nthe total system may be either independent or dependent. In this case, though, we are able to use there\u00adsults \nof the extended GCD test to make a change of vari\u00adables which simplifies the original problem. The origi\u00adnal \nGCD test is derived from number theory. The single quation alzl + a2x2 + . . . + anxn = b has an integer \nsolution iff gcd(a J divides b. This gives us an exact test for single-dimensional arrays ignoring bounds. \nBanerjee shows how this can be extended to multi-dimensional arrays. The system of equations 2A = Z can \nalways be factored into a unimodular2 integer matrix U and an echelon3 matrix D (with dll > O) such that \n1.7A = D. The factoring is done with an extension to Gaussian elim\u00adination. Then, the system 2A = Z has \nan integer solution F iff there exists an integer vector ; such that ;D = c. Since D is an echelon matrix \nthis back substitution can be done very simply. If no such ; exists, then the total system is independent. \nOtherwise, ignoring the bounds, there is a dependence. We then add in the bounds and continue. If such \na ; exists then the solution 7 is given by 2 = ; . If the system is not of full rank, and it usually \nis not, then i? will have some degrees of freedom. For example: i = (l, tl) U = ~ ~ then E = (I, tl) \nwhere tl (and therefore Zz)[Jcan e on any integral value. Wolfe showed that the bounds constraints on \n2 can be expressed as constraints on ~ [19]. For exam\u00adple: fori=lto lOdo a[i+lO] = a[i] end for  The \ninitial dependence problem is to find integers i,? such that i+lO=i and l<i, i ~lo  The extended GCD \ntest tells us that (i, i )= (tl, tl + 10). Transforming the constraints to be in terms of t 1 gives us: \ndoes there exist integer tl such that l<tl <lOand l<tl+lo <lo 2a unimedular matrix is a matrix whose \ndeterminant is +1. 3~ ~h~~ matrix is an m * n matrix such that if the first non zero element in row \ni is in column j, then the first non xero element in rowi+l isincolmnnk>j -2 J This transformation is \nvaluable for several reasons. First, we have reduced the number of variables. In general, each independent \nequation will eliminate one variable. Second, we have reduced the number of constraints. Be\u00adfore, each \nlower and upper loop bound generated one constraint each, while each dimension of the array gen\u00aderated \none equality constraint. The equality constraint a; = b had to be converted into the two inequality constraints \na~ ~ b and a~ >= b, Therefore we had 2*/+2*d constraints (where 1is the number of enclosing loops, and \nd is the number of array dimensions). Now all the equality constraints are folded into the bounds constraints. \nThus we are left with only 2 * i constraints. The complexity of most integer programming algo\u00adrithms \ndepends on the number of constraints and the number of variables. Thus, in general GCD prepro\u00adcessing \nshould make the other algorithms more efficient, but more importantly, the form of the new constraints \nis typically simpler than the original. We have eliminated equality constraints, a necessity for our \nAcyclic Test dis\u00adcussed below, and we have cut down on the number of variables per constraint. In the \nprevious example, some constraints (the equality ones) contained two variables, while after the tmnsformation \nall constraints contain one variable. As we shall see, our first exact test, the Single Variable Per \nConstraint Test, only applies in cases whe~ each constraint has at most one variable. 3.2 Single Variable \nPer Constraint Test Banerjee shows that if the solution to the generalized GCD test has at most 1 free \nvariable then one can solve the exact problem easily [4]. Each constraint is merely an upper or lower \nbound for the free variable. One merely goes through each constraint calculating the ap propriate lower \nor upper bound and storing the best ones found. If after going through all the constraints, the lower \nbound is greater than the upper bound, then the test returns independent . Otherwise the equations are \ndependent. Banerjee notes that this can be extended to the case where there are an arbitrary number of \nvari\u00adables in the system but at most one variable per con\u00ad straint. Each constraint is merely an upper \nor lower bound for one of the free variables. One goes through each constraint and remembers the tightest \nbound for each variable. If after going through all the constraints, /bi > ubi for any variable ti,the \nsystem is indepen\u00ad dent. Otherwise it is dependent. This test is a supcr\u00ad set of the wetl known single \nloop, single dimension ex\u00ad act test [3]. It is quite clear that with one loop and single-dimensional \narrays one cannot get constraints with more than one free vaxiable. This test, though, also applies to \nmany common multi-dimensional cases, in\u00ad cluding those with coupled subscripts, as shown below. for il \n=L1toU1do for i2 = L2 to /72do a[il ][iz] = a[il + constl ][iz + constz] end for end for for il =L1toWIdo \nfor iz=L2toU2do a[il ] [iz] = a[iz + constl ][il + C074] end for end for  To demonstrate the algorithm, \nwe cover the following example in detail. for iI = 1to 10do for iz = 1to 10do a[il][iz] = a[iz + 10] \n[il +9] end for end for  The GCD test will set il = tl, i{ = t2,i2 = t2+9andi~=tl -10. Expressing the \ncon\u00adstraints in terms of the t variables we get the follow\u00ading: l<tl~lo l<tz~lo l<tz+9<lo l~tl lo~lo \n The tirst constraint sets the lower bound oft 1 to 1 and its upper bound to 10. The second does the \nsame for tz. The third constraint resets tz s upper bound to 1. Finally, the last constraint resets tl \ns lower bound to 11. Since the lower bound on t 1 is greater than its upper bound, the system is independent. \nThis algorithm is very efficient, It requires O(num.constraints + num.vars) steps with very few operations \nper step. Even if certain constraints have more than one variable, applying this test to the applicable \nones will eliminate constraints for the proceeding algorithms. 3.3 Acyclic Test We have develo~d the \nAcyclic Test for cases where at least one constmint has more than one variable. We use the constraints \ninvolving more than one variable to create a directed graph. We add hvo nodes to the graph for each variable \nt~; one labeled i and one labeled i. We examine each pair of variables, ti~d tj, occurring in a single \nconstraint. We first express the constraint as a~ti<...+ CIjt j.If both ai and aj are greater than zero, \nwe add an edge to our graph from node i to node j. We then express the constraint as ajtj<... aiti ~d \nadd an edge from node j to node i. If a~ is less than zero, we would use node i for the tirst edge \nand node i for the second. Similarly, if ~j is less than zero, we would use node j for the first edge \nand node j for the second, llvo nodes are needed for each variable to distinguish thecasetj+tj +... <O \nfromthe caseii-tj+... <O. Looking at an example, let us assume that we have one constraint tl + 2t2 \nt3 <0. We create a graph with six nodes. The constraint implies that t1 s 2t2 + t3. We add an edge to \nthe graph from node 1 to node 2 and another from node 1 to node 3. Similarly, the constraint can k expressed \nas 2i z < t!l + ts so we add edges from node 2 to nodes 1 and 3. Finally, expressing the constraint \nin terms of ts we add edges from node 3 to nodes -1 and -2. If we had multiple constraints, we would \nrepeat this step for each one. If the resulting graph has no cycles, then we can solve the system exactly \nusing a simple substitution method. If there is no cycle, there exists a node i (there is no loss of \ngenerality in assuming that i > O) such that there are no edges entering node i (this node is a leaf \nin the depth\u00adfirst search tree of the graph), From the method used to construct the graph, this implies \nthat there are no con\u00adstraints of the form ajtj< ~iti+..,where ai ~ O. Thus all constraints involving \nt ~ are of the following form: where ~(f) is a linear function involving any of the vari\u00adables except \nti~d where all ai,k >0, 1 ~ k < n. Thus tiis only constrained in one direction, to be smaller than some \nfunction of the other variables. Thus we can set tito Li (the lower bound that the Single Variable Per \nConstraint Test have previously calculated for ti). It is possible that L~ = co if there is no lower \nbound fort i. If there is a solution with ti= Li then that solution is a solution to the original system. \nIf there is a solution to the original system with ti > Li then clearly setting ti to the lower value \nof Li will not violate any constraints. Thus there is a solution with ti= Li iff there is a solution \nto the original system. If our initial node was i rather than i, then all con\u00ad straints on t~ would be \nin the other direction, i.e. all a~,k<0, 1 < k < n, and we would set t~ to Ui. Once we set ti, we continue \non our depth-first search and set artother variable. We continue until we reach a contradiction, a lower \nbound larger than an upper one, or until no variables are left. If we eliminate all the variables without \nfinding a contradiction, then the system is dependent. Otherwise it is independent. Below we show an \nexample of the algorithm. t 1 is constrained in both directions, but we can set t z to Lz = 1. This \nleaves us with Now either t 1 or tscanbeset. If wesettl to L1 = 1 we are left with t3 can be set to any \nvalue between O and 4. There are no contradictions so the system is dependent. Even if there is a cycle \nin the graph, this algorithm can solve for all variables which are not in the cycle. This simplifies \nthe system for the next stages. It is not absolutely necessary to create the graph. One can instead simply \nsearch for variables which are only constrained in one direction and then set them. This approach has \na higher complexity than creating a depth\u00adfirst search tree, but it is easier to implement. This algorithm \nrequires GCD preprocessing to elimi\u00adnate equality constraints. The constraint il = i2 would otherwise \nbe represented as the two constraint il < iz and il z iz. These two constraints alone create a cycle \nin the graph (il < i2 S i). 3.4 Simple Loop Residue Test If there is a cycle in our graph, we attempt \nthe Simple Loop Residue Test. Pratt developed a simple algorithm for data dependence testing which works \nwhen all con\u00adstraints are of the form t i < tj + c [121. One creates a graph with a node for each variable. \nFor this inequality, we place a directed arc with value c from node tito node tj. Assume we have another \nconstraint tj < t~ + d BY transitivity, this implies that t i < tk + c + d. We define the value of a \npath in the graph to be equal to the sum of the values of the edges on the path. In the above ex\u00adample, \nthe value of the path from node z to node k is c + d. So the value of the path constrains its endpointa \nin the same way that an edge does. Thus, if there is a path from node nl to nz with vatue v, we know \nthat nl s n2 + v. Constraints with only one vfiable m ~SO acceptable. We create a special node, ~. The \nconstraint ti < c is represented with an edge from i to no with value c. Similarly, the constraint t \ni ~ c is represented with an edge from nO to i with value c. A cycle in the graph represents a constraint \nof the form i i < c or O < c. We check every cycle in the graph. If any cycle has a negative vatue, \nthe system is independent. Otherwise it is dependent. Shostak [15] extends this algorithm Iirst to deal \nwith inequalities of the form at i < btj + c and then to handle cases with more than two variables. Unfortunately \nthese extensions make the algorithm inexact. However, the al\u00adgorithm can be extended to the case uti \n< atj +C without losing exactness. This case is equivalent to a(ti tj ) < c. Let d be the largest integer \nsuch that d < c and d is a mnttiple of a. We can replace the inequality with ti tj< d/u where d/u is \nan integer. As an example of the Loop Residue Test, assume we have the following constraints: Figure \n1 shows the graph after converting the last con\u00adstraint to tl < t3 4. Figure 1: Example graph for Loop \nResidue Test There is a cycle from tlto tato tOto tiwith value 4 + 4 1 = 1. Therefore the system is \nindependent.  3.5 Fourier-Motzkin Our last algorithm, Fourier-Motzkin [6], is a backup inex\u00adact test. \nIt solves the general non-integer linear program\u00adming case exactly. If it returns independent, we know \nthat the integer case is also independent. If it returns de\u00adpendent, it also returns a sample solution. \nIf this sample solution is integral, then the integral case is dependent. Otherwise we are not sure. \nWe extend this algorithm to deal with a few cases where there is a real solution but no integral one. \nEven if there is a sample non-integer solution, we can sometimes prove independence. In the few cases \nwhere our first four algorithms did not ap\u00adply, and we were required to call Fourier-Motzkin, the algorithm \nwas always exact (i.e., it either returned inde\u00adpendent or the sample solution was integral). The cost \nof this algorithm is a matter of debate. Theoretically it can be exponential. Experimentally, Triolet \nhas implemented this approach and seems to be satisfied with its efficiency [171, but Li, Yew and Zhu \nconsider Triolet s numbers to be too expensive [11]. Nonetheless, we are required to call this algorithm \nso few times that its accrued expense is very reasonable. In the first step, we eliminate the first variable, \nz 1, from the set of constraints. All the constraints are first normalized so that their coefficient \nfor z 1 is O, 1 or -1. The set of constraints is then partitioned into three sets depending on the value \nof the coefficient, xl ~ Ill (&#38;,,.,,n) ,..., Z1 > Dp(iz,.,., n) q < El(zZ,...,n) ,. ... xl < .Eq(i?2,...,n) \n0< F1 (:2,,,+) >-..>0 s ~r(~2,...,n) This system has a solution iff 3F2,...,~ such that ~i(~2,...,n) \n< J%(~2,...,n), ~ = (1,... ,P), J = (1,... ,q) 0< Fk(;2,,n), k = (1,..., r) A proof can be found in [6]. \nThus, one can eliminate one variable at a time until there are none left. At each step, the number of \nconstraints grows by WI p q. While this can lead to exponential behavior in the worst case, when p \nand q are small, each step might actually eliminate constraints. If there is a solution ;2,...,~ then \nthe originat system will be satisfied with any z 1 such tit max(U~2,...,n)) < a < min(-?l(%,..,,n)). \nThus one can back substitute to find sample solutions for all the x variables. We would like the sample \nsolution to be inte\u00adgral. As a heuristic, at each step of the back substitution, we set Zj to be the \ninteger at the middle of the allowed range. At each step i in the back substitution, we know that there \nexists a real solution for z ~ such that max(D(~i+l,...,~)) < Xi < min(E(Zi+l,..., ~)), where we have \nalready substituted in values for i?i+l ,,n. Hav\u00ading no integer in the allowable range for z; does not \nnecessarily imply that there is no integer solution. If we had substituted a different sample value for \nsome z~, i + 1 < k < n, we might have found an integral solution. One special case, though, does turn \nout to be useful. Suppose there is no allowable integer for z ~. Since we have not constrained ourselves \nby selecting a value for any other Xj, we can be assured that there is no integer solution. In generat, \nif the sample solution is not integral, one can use branch and bound techniques. Say for example that \nz~ = 35, Then one sets up two companion sys\u00adtems. One with the added constraint that xi < 3 and another \nwith the constraint z ~ ~ 4. If neither system has a solution then the original system is independent. \nIt is possible that after this step one is still left with a non-integral solution. One can then repeat \nthe branch and bound step. Conceivably, one might be required to branch and bound many times (proportional \nto the size of the region). In such a case, one might have to cut off the process after an arbitmry number \nof steps and assume dependence. We have not found any cases which require us to use explicit branch and \nbound. In four cases, we are required to use it implicitly (see Section 6),  4 Effectiveness of Data \nDependence Tests We have implemented these algorithms in the SUIF sys\u00adtem [16], a general purpose compiler \nsystem developed at Stanford, We then ran them on the PERFECT Club benchmarks. These are a set of 13 \nscientific Fortran programs ranging in size from 500 to 18,000 lines col\u00adlected at the University of \nItlinois at Urbana-Champaign [7], We feel that these are a fair set of benchmarks. Non-scientific codes \ndo not exhibit much loop level par\u00adallelism, and we feel that library routines tend to be sim\u00adpler than \nfull programs. Table 1 shows how many times each dependence test was called per program. The first column \nrepresents ar\u00adray constants, for example a[3] versus a[4]. These cases are handled without dependence \ntesting. We merely in\u00adclude them to show that their frequency can skew statis\u00adtics if we apply general \ndependence routines to them. The second column represents the cases where GCD re\u00adturns independent. For \nthese cases, we do not need to call the exact routines, The other columns correspond to the number of \nsuccessful applications of the tests, As one can see, the vast majority of cases are handled with the \nSingle Variable Per Constraint Test. 5 Memorization To Improve Effi\u00adciency We have shown that algorithms \nam exact in every in\u00adstance of our benchmarks. Now we consider efficiency. Many references are very simple \nand thus tend to be repeated frequently. Bounds are frequently of the form i= 1 to n where n is either \na constant or unknown. While n may vary across programs, in the same program n might always be the same. \nThere is no need to call the dependence routines multiple times on the same data. By saving previous \nresults, we can eliminate most calls. The use of a hash table atlows us to find a duplicate call very \nquickly. A simple memorization scheme does not repeat a test if the input exactly matches a previous \none. We can improve the effectiveness of the algorithm by eliminating the loop lmund constraints on unused \nloop indices. Both of the following programs (a) for i= 1to 10do forj=lto lOdo a[i+lO] = a[i]+3 end for \nend for (b) fori=lto lOdo forj=lto lOdo alj+lO] = ati]-t3 end for end for  collapse to this one fori=lto \nlOdo a[i+lO] = a[i]+3 end for  TWo cases which before appeared to be different, now are identical. In \nTable 2 we show the results of both our sim\u00adple scheme and our improved one applied to the PER-FECT Club. \nWe use two hash tables; one using loop bounds and one not. The GCD test does not make use of bounds. \nThus, if a particular reference matches ignoring the bounds, we are not required to repeat the GCD test. \nWe use a simple-minded open table hashing scheme. It performs well enough for our purposes. If necessary, \nwe are certain that a more optimal one could be found. Treating the input data, army reference equations \nand loop bounds, as one long vector, Z, of integers, our hash\u00ading function is h(F) = size(i?) + ~i 2izi \nThis function was chosen so that symmetrical or partially symmetrical references would not collide. Because \nof the low num\u00adber of unique cases, random collisions are not much of a problem. In Table 3 we show how \nmemorization improves the results of Table 1. The Total Cases column gives the total number of exact \ntests from Table 1. The remaining columns show how many of each exact test is left after memorization, \nMemorization reduces the total from 5,679 to 332 tCStS! Further optimizations are possible. For example, \none can eliminate symmetrical cases. Assume there are two references in a loop; rl and rz. We wish to \nknow if this case is equivalent to a pair of references in our table; r; and r~, Assume the bounds are \nthe same. Our simple scheme would say the two cases are equivalent iff rl = r; and rZ = rj, but the cases \nare actually also equivalent ifrl =r~andrz =r\\. For example comparing a[i] to a[i-1] is the same as comparing \na[i-1] to a[i]. This can be taken farther. a[ilti] versus a[i+l] lj+l] is equivalent to alj][i] versus \nati+l][i+l]. One other possible improvement is to store the hash table across compilations. This will \neliminate the data Dependence Tc Frequen Y Program 7ziRT Constant GCD SVPC Acyclic Loop Residue Fourier-Motzkin \nAP 6,104 229 91 613 o o o Cs 18,520 50 0 127 15 0 0 LG 2,327 6,961 0 73 0 0 0 LW 1,237 54 0 34 43 0 0 \nMT 3,785 49 0 326 0 0 0 NA 3,976 45 0 679 202 1 2 Oc 2,739 2 7 36 0 0 0 SD 7,607 949 0 526 17 5 12 SM \n2,759 1,004 98 264 0 0 0 SR 3,970 1,679 0 1,290 0 0 0 TF 2,020 801 6 826 0 0 0 1-l 484 0 0 4 42 0 0 Ws \n3,884 36 182 378 4 0 160 TOTAL 59,412 11,859 384 5,176 323 6 174 Table 1: Number of times each test \ncalled for each program in the PERFECT Club Percentage of Uniq ; Cases Program With ut Bounds (GCD) Vith \nBounds Total Unictue Total UniqueSimple ~mproved siiii@-Improved AP 704 7.0% 4.4% 613 6.4% 4.4% Cs 142 \n7.7% 7.0% 142 16.2% 14.170 LG 73 32.9% 13.7% 73 47,9% 31.5% LW 77 11.7% 10.4% 77 23.4% 22.1% MT 326 3,4% \n2.5% 326 6.4% 4.3% NA 884 4,2% 3.4% 884 7,9% 6,9% Oc 43 27,9% 20.9% 36 19.4% 13.9% SD 560 6.6% 6.1% 560 \n9.5% 8.8% SM 362 5.5% 3.6% 264 4.970 3.0% SR 1,290 1.1% 0.9% 1,290 1.6% 1.1% TF 832 2.2% 1.7% 826 2.9% \n2.4% TI 46 30.4% 19.670 46 34.8?Z0 23.9% Ws 724 11.9% 11.0% 542 14.2% 11.6% TOT 6,063 5.7% 4.4% 5,679 \n7.3% 5.8% Table 2 Percentage of unique cases for memorization scheme with simple scheme and with unused \nvariables eliminated Dependence Test Frequency For Unique Cases Program #Lines Total Cases SW(2 I Acyclic \nLoop Residue Fourier-Motzkin AP 6,104 613 270 0 0 Cs 18,520 14214 6 0 0 LG 2,327 73230 0 0 LW 1,237 77152 \n0 0 MT 3,785 32614 0 0 0 NA 3,976 88448 11 1 1 w 2,739 365 0 0 0 SD 7,607 56036 6 3 4 SM2,759 26480 0 \n0 SR 3,970 1,290 14 0 0 0 TF 2,020 82620 0 0 0 TI484 4638 0 0 Ws 3,884 54235 1 0 27 TOTAL 59,412 5,679 \n262 34 4 32 Table 3: Number of times each test was called looking only at unique cases dependence cost \nof incremental compilation. In addition, if there is similarity across programs, one could use a set \nof benchmarks to set up a standard table which would be used by atl programs. 6 Direction and Distance \nVectors Typically, we are not just interested in knowing if two references are independent [19]. In case \nof dependence, we frequently also want to know the relationship of the dependence. For example: fori=lto \nlOdo a[i+l] = a[i]+7 end for fori=lto lOdo a[i] = a[i]+7 end for In both cases, the two references are \ndependent, but while the second loop can run in parallel, the first can\u00adnot. The amount of information \nto exactly describe the dependence is large. We need to enumemte every pair of iterations (i, i ) for \nwhich there is a dependence. In the first case, there is a dependence when (i, i ) is (1,2), (2,3),..., \n(10, 11). In the second case, there is a dependence when (i, i ) is (1, 1), (2,2),... , (10, 10). Differentiating \nthe two examples does not require us to use alt the information. The difference between the two examples \nis that in the first case i < i while in the second i = i . Direction vectors are a commonly used technique \nto summarize this relationship between the loop variables ~and their corresponding t;. We define a direction \n@to be one of < , = , > . There is a dependence with di\u00adrection < if there is a dependence such that \ni < i . We allow combinations such as ~ to imply that the direc\u00adtion is < or is = , We use * to represent \nany possible direction. A direction vector ~ is a vector of directions @l>@2, -., @n. We say that two \nreferences with loop in\u00addex variables ~and ~ are dependent with direction vector ~ iff ,,. in @ni;  \nlMo references can be dependent with more than one direction vector. fori=Oto lOdo forj=Oto lOdo a[i]fi] \n= a[2i] lj]+7 end for end for The two references are dependent with both (<, =) and (=, ). The dependence \ntest should therefore return all the direction vectors with which the two references are dependent. Another, \nmore detailed, form of summarizing the de\u00adpendence information is distance vectors. TJVOreferences are \ndependent with distance ~ if ~ Z7= d. fori=Oto lOdo a[i] = a[i-3]+7 end for  In this example, we know \nmore than just i < i , We know that i it = 3. The extended GCD test provides us with an easy way to \ncompute distance vectors. In the above example, GCD tells us that i = tl and i = -tl +3. We merely subtract \nthese two expressions. Since GCD does not use bounds, this method does not work for cases where the distance \nis only constant because of the bounds. For example fori=lto8do for j = 1to 10do a[lOi+j] = a[10(i+2)+j]+7 \nend for end for We will not discover that the distance vector is (2,0). Short of enumerating all possible \ndependence, we know of no way to compute distance vectors in every case. Nonetheless, GCD should work \nfor the common constant-distance cases. On the other hand, direction vectors can be computed in all cases. \nA simple method is to enumerate all possible direction vectors4 and ask if the references are indepen\u00addent \nsubject to the current vector. Each direction vector is a set of simple linear constraints on the loop \nvariables. We simply add these constraints to our system and solve as before. The standard approach, \nbased on Burke and Cytron [5], uses a hierarchical system to prune. Rather than testing each possible \nvector it first tests (*, *,...,*) If this returns independent, we know there are no direc\u00adtion vectors \nfor which the references are dependent. If it returns dependent, we then preform the tests with each \nof the following vectors (<, *, ..., *), (=, *,.. ., *) and (>, *,..., *). If, for example, the first \nvector returns in\u00addependent we know that there is no dependence with any vector whose tirst component \nis <. We can prune any such vector. If any vector returns dependent, we continue to expand its * s. The \naddition of direction vector constraints can con\u00adceivably limit the applicability of our tests, For exam\u00adple: \nto use Fourier-Motzkin due to the addition of the extra constraints. A more serious problem is that direction \nvectors re\u00adquire the dependence tests to be applied multiple times for a pair of references, The number \nof possible vec\u00adtors is potentially exponential in the loop nesting. Even using hierarchical pruning, \nwithout further optimization we still have problems in practice, In Table 4, we repeat Table 3 with direction \nvectors, counting every direction tested. This overestimates the cost since certain fixed costs such \nas the GCD test and the overhead of trans\u00adforming the bounds to be in terms of the t variables do not \ndepend on how many vectors are tried per test. Even allowing for a generous overestimate, calculat\u00ading \ndirection vectors has greatly increased the number of tests performed. Before, the compiler called 332, \nmostly SVPC, tests. Now, it needs to call about 12,500, mostly Acyclic and Loop Residue tests. The number \nof times Fourier-Motzkin is applied has gone from 32 to 157 (note that this is solely due to checking \nmultiple vectors for the same references). Some simple pruning methods can bring these costs back down \ndramatically. In discussing memorization, we mentioned that we need not include unused variables. For \nexampkx fori=lto lOdo forj=lto lOdo afi] = a(j+l] end for end for Since i does not appear in either \nthe array expression nor in a loop bound, we know that direction for i i is * . Thus we run the tests \nfor j and then prepend a * to the resultant direction vectors. Calculating distance vectors can also \nhelp. If, for ex\u00adample, we have for i=... a[i+l] = a[i] end for can be solved with the Acyclic Test. \nA direction vec\u00adtor may add the constraint t2 < tl. The Acyclic Test is no longer applicable and we must \nuse the Loop Residue Test. Similarly, there could be cases where direction vectors force us to use the \nFourier-Motzkin Test. In practice, we have observed a greater need for Acyclic and Loop Residue, but \nin no case have we been forced 4Note ~ith dismnce vectors we would have had to enumerate au possible \ndependenees. Whh direction vectors we only need to emrmer\u00adate all possible directions, a large but much \nsmaller number. we know from the GCD test that i i = 1. We therefore know that i < i and need not try \nout arty other directions for i. Table 5 shows our results with unused variables elim\u00adinated and with \ndistance vector pruning. We now have to call the tests only about 900 times. If we ntwl better resutts, \nBurke and Cytron suggest as an optimization that nice cases can be treated on a dimension by dimension \nbasis rather than as a system. For exam\u00adple: Dependence Test Frequent For Direction Vectors Program \nI #Lines II SVPC I Acyclic Loop Residue Fourier-Motzkin AP I 6,104II 363I 104 100 o Cs 18,520 127 48 \n34 0 LG 2,327 1,067 1,138 4,619 0 LW 1,237 132 73 59 0 MT 3,785 120 32 16 0 NA 3,976 295 124 172 23 Oc \n2,73937 8 4 0 SD 7,607 309 106 120 28 SM 2,759 355 110 169 0 SR 3,970 130 30 18 0 TF 2,020 169 16 11 \n0 TI 484 780 267 703 0 Ws 3,884 303 105 52 106 TOTAL 59,412 4,187 2,161 6,077 157 Table 4 Number of times \neach test was called only looking at unique cases and computing direction vectors Unused Variables and \nDistance Vector Pruning Program #Lines SVPE Acyclic Loop Residue Fourier-Mot.zkin AP 6,104 27 6 6 0 Cs \n18,520 14 16 14 0 LG 2,327 44 6 6 0 LW 1,237 15 12 5 0 MT 3,785 14 0 0 0 NA 3,976 48 59 118 7 Oc 2,739 \n5 0 0 0 SD 7,607 54 20 55 28 SM 2,759 8 0 0 0 SR 3,970 14 0 0 0 TF 2,020 23 0 0 0 TI 484 3 38 72 0 Ws \n3,884 35 15 0 106 TOTAL 59,412 304 172 276 141 Table 5: Number times eacl test was called using distance \nvector pruning and pruning away all unused variables for i= 1to 10do a[i + l][j] = a[i][j] end for \ni and j are not interrelated and we can compute each component of the direction vector independentty. \nDirection vectors also introduce an implicit branch\u00adand-bound step. It is possible for the tests to return \nunknown when not calculating direction vectors but to return independent for every possible direction \nvec\u00adtor. In these cases, we can clearly set the references to independent, This occurs four times in \nour test suite. In each case, there is a real dependence with distance greater than zero but less than \none.  7 Discussion We have shown that the algorithms can be exact in prac\u00adtice. We have not shown how \nbeing exact compares with other, inexact, approaches. Looking at pairs of ref\u00aderences does not give the \nentire picture. In a loop with a thousand independent pairs, being inexact in just one test could have \na devastating effect on the amount of par\u00adallelism discovered. Ideally, one would like a standard model \nto measure the parallelism found. Then one could say how much faster a program ran due to exact data \ndependence. Unfortunately no such system yet exists. Nonetheless, to give some comparison, we imple\u00admented \nthe simple GCD test (atgorithm 5.4.1 in [4]) and the Trapezoidat Banerjee Test (algorithm 4.3.1 in [4]). \nNot computing direction vectors, these algorithms found 415 out of 482 independent pairs, missing 16%. \nFor di\u00adrection vectors, we used the simple GCD test followed by Wolfe s extension to Banerjee s rectangular \ntest (2.5.2 in [19]). We eliminated unused variables so that a[i] versus a[i-1] would return the one \ndirection vector (* <) and not for example the three direction vectors ((< <) (= <) (> <)) which would \nbe returned if these references were also enclosed by a second, unused outer loop. These al\u00adgorithms \nreturned 8,314 direction vectors which is 22% more than the exact answer of 6,828, We do not believe \nthat our particular choice of tests is very important. The key concept is the use of a suite of special \ncase exact tests. It is quite possible that other tests could be added and some eliminated without significantly \nchanging our results. Extended GCD was chosen because it increases the applicability of the other tests. \nThe other tests were cho\u00adsen for several reasons: They all expect their data in the same form: A&#38; \n~ b. Thus there is no need to con\u00advert data from one form to another. Some tests, like the lambda test \n[11] expect their data in a different form. All the tests succeed in finding independent refer\u00adences \nin practice. We checked how many times each test returned independent (counting each direction vector \ntested) for the tests in Table 5. SVF C returned inde\u00adpendent in 40 out of 308 cases, Acyclic in 14 out \nof 172 cases, Loop Residue in 131 out of 276 cases and Fourier-Motzkin in 82 out of 141 cases. The ordering \nof the tests is by cost. Timing them on a MIPS R2000 based machine (a 12 MIPS machine), SVPC averaged \nabout 0.1 msec/test, Acyclic about 0.5 msec/test, Loop Residue about 0.9 msec/test and Fourier-Motzkin \nabout 3 msec/test. Finally, in Table 6 we timed our dependence tests and compared them to standard scalar \noptimizing compilers (f77 -03). We wish to show that being exact adds very little cost to compilation \ntime. The timings do not in\u00adclude the set up time required for dependence testing, for example expressing \na reference in terms of the loop variables. This setup time, while possibly significant, is equivalent \nfor all methods. The timings therefore shootd be looked upon as an upper bound for the extra time required \nto use our approach, The standard compilation time used full scalar optimization. Our approach added \nonly about 3% on average to the compile time.  8 Extension to Symbolic Testing Our tests expect all \nreferences and bounds to be linear functions of the induction variables. We mentioned be\u00adfore that we \nuse optimization techniques (constant prop\u00adagation, induction variable and forward substitution) to increase \nthe applicability of these conditions. For exam\u00adple: n = 100 iz=(t ... fori=lto lOdo iz = iz+2 a[iz+n] \n= a[iz+2n+l]+3 end for wilt be converted by our optimizer to: fori=lto lOdo a[2i+100] = a[2i+201]+3 end \nfor which meets our conditions for analysis. Nonetheless, there are cases where the unknown vari\u00adables \ncan not be expressed as functions of the induction variables. read(n) ... for i = 1to 10do a[i+n] = a[i+2n+l]+3 \nend for Dependence Program Dep. Test Cost AP Cs LG LW MT NA Oc SD SM SR TF TI Ws Table 6: Total Cost \nof Dependence As long as we know that n does not vary inside the loop, we can add it to our system as \nif it were an induction variable without bounds. For this ex\u00adample, our system would ask the following \nquestion. does there exist integers i, i and n such that l<i, i <lOand i+n=i +2*n+l  Table 7 shows the \nresults of adding symbolic testing to our system. Our tests are now called about 1,060 times compared \nwith about 900 times before. This should add little to our total cost. We speculate that the low cost \nis because our prepass optimization are quite powerful.  Conclusion Data dependence analysis is a fundamental \ncomponent in any parallelizing compiler. Previous techniques have required approximations. We have presented \na defini\u00ad tive solution to the problem by using a combination of simple, easy to implement techniques; \ncascading special case exact tests, memorization and better direction vector pruning, In practice, our \ntests have found independent references which could not be found with currently used techniques. Running \nlarge benchmarks, we demonstrate empirically that our method is both exact and inexpen\u00adsive. References \n[1] H. Abelson, G. J. Sussman, and J. Sussman. Struc\u00adture and Interpretation of Computer Programs. The \nMIT Press, 1985. Testing Cost (in sees) t77 -03 (in sees) 2.2 151.4 * 485.0 4.0 65.4 1.1 33.0 1.0 45.0 \n3.6 136.3 0.3 38.2 2.7 62.1 3.5 102.5  3.8 118.5 2.6 116.6 0,7 12.6 3,6 110.0 Testing. * too smrdl \nto measure [2] R. Allen and K. Kennedy. Automatic translation of FORTRAN programs to vector form. ACM \nTrans\u00adactions on Programming Languages and Systems, 9(4):491-542, October 1987. [31 U. Banerjee. Speedup \nof Ordinary Programs. PhD thesis, University of Illinois at Urbana-Champaign, October 1979. [4] U. Banerjee. \nDependence Analysis for Supercom\u00adputing. Kluwer Academic, 1988. [5] M. Burke and R. Cytron. Interprocedural \ndepen\u00addence analysis and parallelization. In Proceed\u00adings of the SIGPL4N 1986 Symposium on Compiler Construction, \npages 162-175, 1986. [6] G. Darttzig and B. C. Eaves. Fourier-motzkin elim\u00adimtion and its dual, Journal \nof Combinatorial The\u00ad0~, A(14):288-297, 1973. [7] M. Berry et al. The PERFECT Club benchmarks: effective \nperformance evaluation of supercomput\u00aders. Technical Report UIUCSRD Rep. No. 827, University of Illinois \nUrbana-Champaign, 1989. [8] R. Kannan. Minkowski s convex body theorem and integer programming. Mathematics \nof Operations Research, 12(3):415440, August 1987. [9] H.W. Lenstra. Integer programming with a fixed \nnumber of variables. Mathematics of Operations Research, 8(4):538 548, 1983. [10] Z. Li and P. Yew. Practical \nmethods for exact data dependency analysis. In Proceedings of the Second Workshop on Languages and Compilers \nfor Parallel Computing, 1989. Symbolic Testing Program #Lines SVPE Acyclic LQOp Residue Fourier-Motzkin \nAP 6,104 33 22 6 0 Cs 18,520 20 24 19 0 LG 2,327 48 6 6 0 LW 1,237 15 12 5 0 MT 3,785 19 0 0 0 NA 3,976 \n55 149 101 7 Oc 2,739 5 1 0 0 SD 7,607 54 20 55 28 SM 2,759 8 0 0 0 SR 3,970 21 1 2 0 TF 2,020 43 0 0 \n0 TI 484 3 38 72 0 Ws 3,884 35 19 0 106 TOTAL 59,412 359 292 266 141 Table 7: Number of times each test \nwas called computing direction vectors and adding symbolic constraints [11] Z. Li, P. Yew, and C. Zhu. \nAn efficient data depen\u00addence analysis for parallelizing compilers. IEEE Transactions on Parallel and \nDistributed Systems, 1(1):26-34, Jan 1990. [12] V.R. Pratt. Two easy theories whose combination is hard. \nTechnical report, Mass Institue of Technology, Sept. 1977. [13] A. Schrijver. Theory of Linear and Integer \nPro\u00adgramming, John Wiley &#38; Sons, 1986. [14] Z. Shen, Z. Li, and P. Yew. An empirical study on array \nsubscripts and data dependencies. In Proceed\u00adings of 1989 International Conference on Parallel Processing, \npages 11 145 to 11 152, 1989. [15] R. Shostak. Deciding linear inequalities by comput\u00ading loop residues. \nACM Journal, 28(4):769-779, Ott 1981. [16] S. Tjiang, M, Wolf, M.S. Lam, K. PiePer, and J.L. Hennessy. \nAn overview of the SUIF compiler sys\u00adtem. 1990. [17] R. Triolet. Interprocedural analysis for program \nrestructuring with parafrase. Technical Report CSRD Rep. No. 538, University of Illinois Urbana-Champaign, \nDec. 1985. [18] D. R. Wallace. Dependence of multi-dimensional array references. In Proceedings of 1988 \nInter\u00adnational Conference on Parallel Processing, pages 418-428, 1988. [19] M. Wolfe. Optimizing Supercompilers \nfor Super\u00adcomputers. The MIT Press, 1989. 14 \n\t\t\t", "proc_id": "113445", "abstract": "", "authors": [{"name": "Dror E. Maydan", "author_profile_id": "81100138889", "affiliation": "Computer Systems Laboratory, Stanford University, CA", "person_id": "P70229", "email_address": "", "orcid_id": ""}, {"name": "John L. Hennessy", "author_profile_id": "81100207767", "affiliation": "Computer Systems Laboratory, Stanford University, CA", "person_id": "P144244", "email_address": "", "orcid_id": ""}, {"name": "Monica S. Lam", "author_profile_id": "81100237956", "affiliation": "Computer Systems Laboratory, Stanford University, CA", "person_id": "PP14092336", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/113445.113447", "year": "1991", "article_id": "113447", "conference": "PLDI", "title": "Efficient and exact data dependence analysis", "url": "http://dl.acm.org/citation.cfm?id=113447"}