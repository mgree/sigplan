{"article_publication_date": "05-01-1991", "fulltext": "\n Fortran at Ten Gigaflops: The Connection Machine Convolution Compiler Mark Bromley, Steven Heller, \nTim McNerney, and Guy L. Steele Jr. Thinking Machines Corporation 245 First Street Cambridge, Massachusetts \n02142 Abstract We have implemented a prototype of a specialized compiler module and associated run-time \nsupport that allows a Fortran user to achieve sustained floating\u00ad point performance of over 10 gigaflops \non the Connec\u00ad tion Machine Model CM-2. This improves substantially over the previous record of 5.6 \ngigaflops, which was achieved by means of hand-crafted low-level coding techniques (on which this work \nhas been based). Indeed, the same code that ran at 5.6 gigaflops in 1989 now runs at over 14 gigaflops. \nThe compiler module processes array assignment statements of a certain form the right-hand side must \nbe a sum of products and the products must use the Fortran 90 array-shifting intrinsic in a certain stylized \npattern. Such assignment statements are suitable for expressing array convolution (stencil) computations \nand can be executed efficiently by chained multiply\u00adadd operations. Our implementation strategy divides \nresponsibility for the computation into four parts. A run-time library provides the outer loop structure \nfor strip-mining and for handling multidimensional arrays. Anew primitive speeds up nearest-neighbor \ngrid communication. Hand-written CM-2 sequencer microcode directs the pipelined execution of the floating-point \nunits. The compiler module performs a kind of loop unrolling in order to optimize the use of registers \nand determines, at compile time, the microcode to be used at run time. 1 Introduction The 1989 Gordon \nBell Prize for raw performance was won by the team of Doug McCowen and Irshad Mufti (of Mobil Oil) and \nMark Bromley, AlIan Edelrnan, Bob Lordi, Alex Vasilevsky, and Jacek Myczkowski (of Thinking Machines \nCorporation). Their code for a Permission to copy without fee all or part of this material is granted \nprovidad that the copies are not made or distributed for direct commercial edvantage, the ACM copyright \nnotice and the title of the publication and its date appear, and notica ia given that copying ia by permission \nof tha Association for Computing Machinery. To coPy otharwise, or to republish, requiraa a fee andlor \nspecific parmisaion. Q 1991 ACM 0-89791 -428 -7/91 /0005 /0145 ...$1 .50 Proceedings of the ACM SIGPLAN \n91 Conference on Programming Language Design and Implementation Toronto, Ontario, Canada, June 26-28, \n1991. ~ finite-difference seismic model achieved an overall sustained performance rate of 5.6 gigaflops. \nMost of the code was written in Connection Machine Fortran, but the inner loops of the computation were \nhandled by library routines that were carefully coded at a low level. These library routines carry out \ncertain convolu\u00adtion operations on la~e arrays. They are general enough to be used by many users, but \neach library rou\u00adtine performs a fixed pattern of computation. We have built on this work in two ways. \nFirst, we have generalized and improved the techniques for per\u00adforming stencil computations on the CM-2, \npushing the performance rates above 10 gigaflops. Second, we have made these techniques available at \nthe level of Fortran code. A user can express the computation as ordinary Fortran 90 array assignment \nstatements and therefore is not limited to choosing from a preselected set of library routines. In 1990, \nthe team of Doug McCowen and Irshad Mufti (of Mobil Oil) and Mark Bmmley, Steven Heller, Cliff Lasser, \nBob Lordi, Tlm McNerney, Jacek Mycz\u00adkowski, Guy Steele, and Alex Vasilevsky (of Thinking Machines Corporation) \nwon a 1990 Gordon Bell Prize honorable mention for raw performance. The same code that ran at 5.6 gigaflops \nin 1989 ran at 14.182 giga\u00adflops in 1990. The only difference was that the hand\u00adcoded library routine \nused in 1990 was replaced by a subroutine written in Fortran and compiled by the methods described here. \n2 Stencils Consider the following ment: R = Cl * CSHIFT &#38; + C2 * CSHIFT &#38; +C3*X &#38; + C4 * \nCSHIFT &#38; + C5 * CSHIFT For every position in computes a value that sponding element of neighboring \npositions. Fortran 90 assignment state\u00ad (X, DIM=l, SHIFT= -1) (X, DIM=2, SHIFT= -1) (X, DIM=2, SHIFT=+l) \n(Xr DIM=l, SHTFT=+l) the result array R, this statement depends not only on the corre\u00adx but also on \nelements of x at For example, the result value for position (4, 3) depends on the elements of X at posi- \nNeighbors need not be in the same row or column as tions (4, 2), (4, 4), (3, 3), and (5, 3) as well as \n(4, 3). We the center of the pattern; the stencil represent the computation pictorially in this way. \n The grid represents positions in both the source array x .. and the result array R. The bullet indicates \na position into which the result is to be stored and shaded squares indicate positions of x that contribute \nto the computa\u00adtion. Of course, a similar computation is being performed for all other positions of R \nat the same time. Here we show just a few of them (Notice the wraparound effect that occurs because the \nshifts are circular.) Because the same pattern of references to neighbor\u00ading elements of X occurs at \nevery position, we can sum\u00admarize such a computation by a picture of the pattern alone, without bothering \nto show it in the context of a grid: .,.,. ,,,, M ~:, ,:; ,..,, ,,,., @ Such a pattern is often called \na sferzcil. Another com\u00admon stencil pattern is ,,, J ,,,., ....... ,,,,,,, ;,;< ~, ,* :.:.:. :;;: ,,, \n..,,,,,, ,,,,, + ,,, which represents the computation R = Cl * CSHIFT (X, DIM=l, SHIFT=-2 ) &#38; + \nC2 * CSHIFT (X, DIM=l, SHIFT= -1) &#38; + C3 * CSHIFT (Xr DIM=2, SHIFT= -2) &#38; + C4 * CSHIFT (X, DIM=2, \nSHIFT= -1) &#38; +C5*X &#38; + C6 * CSHIFT (X, DIM=2, SHIFT=+2 ) &#38; + C7 * CSHIFT (X, DIM=2, SHIFT=+l) \n&#38; + C8 * CSHIFT (X, DIM=l, SHIFT=+l) &#38; + C9 * CSHIFT (X, DIM=l, SHIFT= +2) represents the computation \nR = Cl * CSHIFT(csHIFT (X, 1,-1) ,2, -I.) &#38; + C2 * CSHIFT(X, l, -1) &#38; + C3 * CSHIFT(CSHIFT (Xrl, \n-l) ,2,+1) &#38; + C4 * CSHIFT (X,2,-1) &#38; +C5*X &#38; + C6 * CSHIFT (X,2,+1) &#38; + C7 * CSHIFT \n(CSHIFT (x, 1,+1) ,2, -I) &#38; + C8 * CSHIFT(x, l,+l) &#38; + C9 * CSHIFT(csHIFT (X, l,+l) ,2, +I) \nMoreover, there is no need for the stencil to be symm\u00adetrical or particularly cente~d around its center \n: :# CJ :;.<: .,:,.,. ,.,. .W ., ,., q represents the computation R= C1*X &#38; + C2 * C.5H1FT (X,2,+1) \n&#38; + C3 * CSHIFT(CSI-IIFT (X, l,+l) ,2,-1) &#38; + C4 * CSHIFT (X, 1,+1) &#38; + C5 * CSHIFT (X,1,+2) \nThe Connection Machine Convolution Compiler processes single arithmetic assignment statements of the \nform R= T+ T+. ..+T where R is the name of an array (that is, a Fortran 90 whole\u00adarray reference) and \neach term T is of one of the following forms: ::= I-J * s(x) I s(x) * c I s (x) Ic  where c is an array \nname (a whole-array reference) and s (x) is a shifting of an array name represented by x, that is, an \nexpression having the following recursively defined form: T s(x) ::= x I CSHIFT (S (x) ,DIM=k, SHIFT=m) \nCSHIFT (S (X) ,Jc, m) I EOSHIFT (S (X) , DIM=k, SHIFT=m) EOSHIFT (S (X) , k, m) The current implementation \nof the Connection Machine Convolution Compiler requires that all shift\u00adings within a given assignment \nstatement must shift the same variable name. This general form covers all of the examples shown above. \nTechnical Background The Connection Machine Model CM-2 contains thou\u00adsands of bit-serial processors \n(a full-size CM-2 has 65,536 = 216 such processors). The processors commu\u00adnicate through a router mechanism \nthat forwards mes\u00adsages through a network that is logically structured as a 16-dimensional boolean hypercube. \nThe processors are physically packaged 16 to a chip; this same chip also contains router circuitry. ACM-2 \nwith 65,536 pro\u00adcessors therefore contains 4,096 = 212 chips connected by a physical 12-dimensional boolean \nhypercube. A CM-2 maybe equipped with optional floating\u00adpoint hardware. In this configuration, there \nis one floating-point ALU chip for every 32 processors. (This ratio was chosen to match memory bandwidth, \nfor the floating-point ALU has a 32-bit-wide data path to memory.) Thus there is one floating-point ALU \nfor every pair of processor chips. We will call this group\u00ading of two processor chips, a floating-point \nALU, and their associated memory a node. A CM-2 with 65,536 processors therefore contains 2,048 = 211 \nnodes. Because every node contains two processor chips, the nodes may be regarded as forming an 1ldimensional \nhypercube where each edge of the hypercube has two communications wires along it (or, if you will, one \nwire with twice the bandwidth). While the memory bandwidths of the processors and the floating-point \nALU are easily matched, it is a bit trickier to match data formats. The bit-serial proces\u00adsors are designed \nto operate each on its own data, one bit at a time; therefore a 32-bit floating-point number is ordinarily \nstored entirely within the memory associ\u00adated with a single processor. In a single memory cycle every \nprocessor can fetch one bit of a floating-point datum; for every processor to inspect its entire datum \nrequires 32 cycles, but the 32 processors in each node can process 32 floating-point numbers in parallel. \nThis bit-serial, word-parallel arrangement is sometimes called the processorwise storage format. The \nfloating-point ALU, on the other hand, is an off\u00ad the-shelf chip that expects to receive each 32-bit \nfloat\u00ad ing-point datum all at once, one per clock cycle; it is designed to accept and process floating-point \nnumbers in a bit-parallel, word-serial format. Each CM-2 node contains an interface chip that mediates \namong proces\u00ad sors, floating-point ALU, and memory; it converts between data formats as necessary [?]. \nThe most recent release of the CM Fortran compiler eliminates the need for data format conversion by \nstor\u00ad ing floating-point (and 32-bit integer) data in memory in a so-called slicewise format in which \nthe 32 bits of a floating-point number are stored one bit per bit-serial processor, occupying a slice \nthrough memory that can be accessed in a single memory cycle. Data stored in this format can be read \ninto the interface chip and from there directly into the floating-point chip without need of transposing. \nThis gives much greater flexibility in the use of the floating-point ALU because it elimi\u00adnates the need \nto process data in batches of size 32. The CM Fortran compiler in fact usually processes slice\u00adwise data \nin batches of size 4; this batch size is large enough to exploit the ALU pipeline effectively and yet \nsmall enough to allow several such batches to be kept in the internal registers of the floating-point \nALU chip. In effect, the CM Fortran compiler treats each node as a vector unit, using vectors of size \n4 and regarding the 32 internal registers as seven vector registers and four sca\u00adlar registers. This \nnew target machine model for the CM-2 routinely allows Fortran users to achieve execu\u00adtion rates of around \n4 gigaflops (4x109 floating-point operations per second). The purpose of the work reported here was \nto boost significantly the performance of a reasonably general class of applications by adding a special-purpose \noptimizer to this general Fortran-with-slicewise-data framework. 4 Overview of Our Approach The primary \ngoal is to maximize the rate at which use\u00ad ful floating-point operations are executed. The four bottlenecks \nthat might obstruct this goal are interpro\u00ad cessor communication, the floating-point unit, the instruction \nsequencer, and the memory interface. 4.1 Interprocessor Communication The SIMD nature of the CM-2 architecture \nmakes it practically impossible to overlap computation with the necessary interpmcessor communication. \nCommunicat\u00adion therefore constitutes pure overhead that will degrade the flop rate. Fortunately, for \ntwo-dimensional grids on fixed hardware, the cost of communication grows as the square root of the number \nof flops to be perfomned, so for sufficiently large problems the com\u00admunications overhead will be a relatively \nsmall frac\u00adtion of the total work. Nevertheless, we have found that it pays to put some effort into minimizing \nthe time spent communicating. To make best use of the communication bandwidth, we microcode a new grid \ncommunication primitive. Previous CM-2 grid primitives were designed to orga\u00adnize the bit-serial processors \ninto a grid and to allow every processor in parallel to pass a single datum to a single neighbor, all \nin the same direction (West, say). This grid is embedded within the hypmube topology in such a way that \ngrid neighbors are hypercube neigh\u00adbors, tlhereby making effective use of the network. The new primitive \norganizes nodes, not processors, into a twodimensional grid, and allows each node to pass data to all \nfour neighbors simultaneously. 4.2 The Floating-Point Unit The floating-point unit operates at the basic \nCM-2 clock rate of about 8 MHz. (It must be remembered that the CM-2 hardware was designed in 1986 and \nthat, for the sake of reliability and robustness, the design was fairly conservative by the standard \nof that year in its use of technology.) Two versions of the floating-point unit are offered, based on \nthe Weitek WTL3132 or WTL3164 chip. The work reported here is specifically targeted to the WTL3164. Like \nmost floating-point units, it has separate adder and multiplier circuits. It performs only one operation \nper cycle except for a few special instruc\u00adtions that permit an addition and a multiplication to be performed \nsimultaneously on one of a few operand patterns. We chose to tackle the class of stencil problems because \nthey are generally useful and lend themselves to exploitation of multiply-add operations. To make best \nuse of the floating-point unit, we use chained mul\u00adtiply-add operations exclusively, allowing two float\u00ading-point \noperations to occur per clock cycle. Not all of these operations will be useful, however. As we explain \nbelow, the compiler must make a special effort to dis\u00adcard the results of useless multiply-add cycles. \nThis is complicated by the pipelining of the floating-point unit: a multiplication started on cycle k \nwill become an operand of the addition started on cycle k +2; the result of that addition will be stored \ninto the destina\u00adtion register on cycle k +4. An interesting further con\u00adstraint is that one of the operands \nfor each multiplication must come from off-chip, that is, from memory, rather than from an internal register. \nOur general approach is therefore to preload the internal registers with elements of the array that are \nsubject to shifting arid then to stream coefficient values from memory during the useful multiply-add \noperations.  4.3 The Instruction Sequencer The CM-2 instruction sequencer is, like most micro\u00adcode instruction \nexecution engines, a highly idiosyn\u00adcratic collection of specialized functional units commanded by a \nvery wide instruction word. The principal structures include an AMD2901-style central ALU, an AMD2910-style \nmicroinstruction sequencer, a microinstruction memory, and a scratch data memory. The scratch data memory \nis addressed by a counter that makes it possible to address consecutive locations without assistance \nfrom the sequencer ALU, but changing the counter to a new value ties up the ALU for one cycle. Memory \naddresses for the parallel mem\u00adory in the processor nodes are generated by the sequencer ALU; indeed, \ngenerating memory addresses is the principal purpose of the sequencer ALU. Instructions to the floating-point \nunits are divided into two pieces, which we call the static parf and the dynamic part, The static part \ncontains the operation codes and the dynamic part contains the Ioad/store control and addresses for internal \nregisters. Only one of these two parts can be issued in a given cycle. When the sequencer issues the \nstatic part, it is latched on the processor boards and the floating-point units perform no operation. \nWhen the sequencer issues the dynamic part, it is executed by the floating-point units in con\u00adjunction \nwith the previously latched static part. (This is the result of an engineering trade-off that reduces \nthe number of distinct signals that must pass along the backplane from the sequencer board to the processor \nboards. This trade-off was quite reasonable in view of the original purpose of the floating-point units: \nto per\u00adform arithmetic operations in batches of 32. One extra cycle to latch part of the instruction, \nto be followed by 32 wgister access patterns, was considered an accept\u00adable (3 %o) overhead. Moreover, \nthe overhead for the static part can be amortized over multiple batches.) A useful strategy is to keep \nthe dynamic parts of floating-point instructions in the scratch data memory of the sequencer and feed \nthem cycle by cycle to the floating-point units. This avoids embedding register access patterns in the \nmicrocode, allowing them to be determined by a compiler and loaded into the scratch data memory at run \ntime. A peculiar constraint is that the microinstruction fields that control this feature overlap the \nmicroinstruction-next-address field; there\u00adfore one cannot perform a simple conditional branch within \nthe microcode on the same cycle that one is issuing a dynamic floating-point instruction part. (This \nwas a result of a typical engineering trade-off that reduces the width of the microinstruction word by \nusing the same bits for multiple purposes, thereby sav\u00ading a great deal of space on the sequencer board.) \nTo make best use of the instruction sequencer, we have carefully designed the best possible floating-point \nALU sequences and have written hand-tailored sequencer microcode to support these instruction sequences. \nThis microcode issues a single static instruc\u00adtion part to instruct the floating-point units to perform \nmultiply-add operations; it then computes and issues many memory addresses (hundreds or thousands) based \non parameters passed at run time. However, the pattern of references to the floating-point registers \nis not wired into the microcode. The compiler is responsi\u00adble for selecting the appropriate hand-crafted \ninstruc\u00adtion sequence and then generating the register references, which at run time are loaded into \nthe scratch data memory and thus made available to the fixed microcode. In this way a fixed set of microcode \nroutines can support a wide variety of stencil patterns. IA (1:64,1:64) I rE!EEl IA (65:128,65:128) \nII A (65:128,129:192) I I A (129:192,65:128) I I A (129:192,129:192) I Figure 1. Divisionofa I A (193:256,165:128 \n256x256 arrayamong II 16nodes A(193:256,129:192) I I A (193:256,193:256) I 4.4 MemoryBandwidth Topiler \npoint makebestuse ofmemorybandwidth, endeavors to exploit the registers unit; the idea is to use a quantity \nthecom\u00adof the floating\u00adas many times as possible once it has been loaded into a register. Three standard \noptimization techniques are used: strip minin~ pipelining, and loop unrolling. These are dis\u00adcussed further \nin the next section. 5 Details of Our Approach Our solution to the problem of compiling stencil pat\u00adterns \nfor the CM-2 is to divide the work among three software modules: compiler, run-time library, and microcode. \nThe compiler is responsible for register allocation and the choice of particular microcode rou\u00adtines; \nthis provides the flexibility to handle arbitra~ stencil patterns. The run-time library takes care of \nallo\u00adcating temporary memory space, performing interpro\u00adcessor communication, and providing the outer \nlevels of iteration. The microcode provides the inner looping structure that is critical to performance. \nAll the arrays involved in the stencil computation source, result, and coefficient+--are of the same \nsize and shape. They are expected to be divided up among the nodes in the same manner. The nodes themselves \nare arranged in a two-dimensional grid; each node con\u00adtains a two-dimensional subgrid of each array. \nFor example, if there were only 16 nodes, they would be arranged as a 4 x 4 grid. If the arrays were \n256x256, then each node would contain a 64x 64 subgrid. (See Figure 1.) 5.1 Communication What data \nmust be communicated among nodes? This question can be answered by examining the sten\u00adcil pattern. The \namount by which it extends in each direction from its center we will call the border r.oidfh for that \npattern in that direction. For example, the stencil pattern East border width = 1 ,.,::::: North border \n,:... ... ,$$:.jj,; .:.:.:.. ,,..., width =2 [~ ~ $::p] i ~:: -South border width = O West border wid~3 \n happens to have border widths that are all different. If we examine one subgrid in the context of an \nentire array and consider what data must be obtained from other subgrids, we see a simple pattern, formed \nby the superposition of many copies of the stencil: m ,,, ;;., ,:,:::, ,., . ..\u00ad ,.,. V.   IEBEE This \npattern can be simplified by padding it with a few extraneous data points so as to make it rectangula~ \nEbkkttH I 1 I r I I t  j : { u Then we can say that a node needs to fetch from its West neighbor \na number of columns equal to the West border width of the stencil pattern, and similarly for the other \ndirections. For our particular implementation, we simplify this even further. Because we happen to have \na communi\u00adcation primitive that exchanges data with four neigh\u00adbors at once, it costs no more to pad \nthe subgrid on all four sides by the largest of the four border widths: There is a cost in temporary \nmemory space, but in practice most stencils have fourfold symmetry anyway. Moreover, for large problems \nthe required amount of temporary storage is relatively small, and for small problems the required amount \nof temporary storage is absolutely small. Therefore this simplification usually doesn t hurt at all. \nInterprocessor communication for an entire stencil computation is performed at the beginning all at once. \nFirst, temporary storage is allocated to hold data from neighboring subgrids: temporary storage (Although \nthe temporary storage is of course allocated in a separate region of of the node memory, in these diagrams \nwe have broken it into pieces and arranged them around the subgrid to suggest intended geomet\u00adrical relationships. \nNote that the subgrids need not be square, but the communications time will be propor\u00ad tional to the \nlength of the longer side.) Second, data is exchanged with all four neighbors. The data in the subgrid \nfalls into nine sections. Corner sections must be copied to two neighbors (and, ulti\u00admately, to a diagonal \nneighbor as well); edge sections must be copied to one neighbor; and the central section is not needed \nby any neighboring node The overall pattern of exchange thus looks like this  zl=tE2trhHtt As a result, \neach node contains data like this The third step is to exchange data for the corners When this is complete, \nevery node contains all the data required to carry out the complete convolution without further need \nof communication (For some common stencil patterns, such as the third step may be omitted, as there is \nno need of corner data from diagonal neighbors. This saves only a very small amount of time for very \nlarge arrays, but the test is very easy and quick and does save a noticeable amount of time for smaller \narray s.) 5.2 Strip Mining Once the necessary data has been been brought into each node from its neighboring \nnodes, the subgrid for that node is logically partitioned into strips of width w. (The width is determined \nby the register allocator as described below.) The strips am then further divided in half; the basic \nmicrocode loop processes one half-strip, working from the edge of the subgrid to the center. The use \nof half-strips is the result of a trade-off. If the microcode loop were to process an entire strip, it \nmust handle boundary conditions at both the start and the end of the strip. These boundary conditions \nhave to do with addressing the data in the temporary storage area. In processing a half-strip, only one \nboundary con\u00addition needs to be handled. In this way a great deal of complexity is avoided in the microcode; \nthis in turn conserves microcode instruction memory, which is a scarce resource. The price of this is \nadditional overhead for having to startup the microcode loop twice as many times; this overhead is relatively \nsmall when operating on medium to la~e arrays. 5.3 Pipelining and Avoiding Redundant Loads A half-strip \nof width w is processed line by line, where each line consists of w array positions rI1 I II One iteration \nof the microcode loop loads some data elements into the floating-point registers, performs pipelined \nmultiply-add operations, and then stores a line of w results. Suppose that w = 8 and the stencil pattern \nis To compute one result for this pattern requires access to five elements of the data array, A naive \ncomputation would thus perform 40 loads to compute eight results. But placing eight copies of the pattern \nwith their centers side by side shows the total set of data array elements actually needed to compute \neight results: We call this composite pattern a nndtistencil. It spans ordy 26 array positions; therefore \nonly 26 data elements need be loaded in order to compute eight results at once. This is a significant \nsaving in memory band\u00adwidth. The key is to use a data element many times once it has been loaded into \nan internal register. In this example, the central six elements will be used three times each, and two \nother elements will be used twice each. The 32 internal registers of the floating-point unit can comfortably \naccommodate 26 data elements. But where shall the results be accumulated? One might suggest computing \nonly one or two results at a time, once the data elements needed for eight results have been loaded, \nso that only one or two more registers will be needed for accumulation. But that will not do. The floating-point \nunit requires that the coefficient val\u00adues come from memory during the multiply-add oper\u00adations; therefore \nwe cannot overlap the storing of results with computations for other results. Moreover, the presence \nof the interface chip between the floating\u00adpoint unit and memory introduces a cycle of latency. This \nlatency is overcome by pipelining, but there is a penalty every time the direction of this pipe is reversed. \nTherefore we wish to avoid interleaving stores with computation; it is more efficient to compute all \neight results and then store all eight consecutively. The trick here is to observe that if the results \nare pro\u00adcessed in a certain order, say from left to right, then certain registers will become free just \nin time to receive results. To see this, let us begin again with the basic stencil pattern. Choose any \nrow and label the left\u00admost position of the row within the pattern. (In prac\u00adtice we always choose the \nbottommost row, for reasons discussed below.) A,,   ,,.* ... T Now make a multistencil from this tagged \nstencil: Consider some occurrence of the tagged stencil pattern within the multistencil. It is easy to \nsee that because a le~fmost position of the stencil was tagged, no result to the right can possibly require \nthe use of that data ele\u00ad ment; therefore it can be used to accumulate the result for that stencil occurrence \nif we process stencil occur\u00ad rences from left to right. In the actual implementation we compute the results \nin pairs in order to exploit the timing of the WTL3164 chip; two chained multiply-add threads are interleaved. \nThe left result of each pair might well require the data element held in the register about to be used \nto accumulate the right result of the pair, but the details of the pipeline just barely allow use of \nthat data element before it is first written. How wide should the strips be? The compiler makes the multistencil \nas wide as possible subject to the size of the internal register set. Recall that the division of floating-point \ncommands in the CM-2 into static and dynamic parts, as described above, makes it desirable to execute \nonly multiply-add operations. To make this work, one register is reserved to contain the value zero, \nThe result of the first multiplication for a given result is added to this zero to begin the accumulation, \nThis is faster than initializing an accumulator register to zero, and moreover makes possible the just-in-time \nre-use of registers. During cycles where no computation is desired (when loading data elements or storing \nresults), multiply-add computations are performed nevertheless; the compiler must carefully arrange to \nmultiply zero by zero, add zero, and store the result into the zero register (there is no way not to \nstore the result!). Sometimes a second register must be reserved to contain the value 1. O; this occurs \nwhen the arith\u00admetic expression contains a term of the form c (just a coefficient value to be added in \nwithout being multip\u00adied by a data element). The compiler the~fore has 31 or 30 registers into which \nto load data elements. In principle the compiler can construct multistencils of any width. We have found \nit practical for the com\u00adpiler to attempt to construct multistencils of width 8,4, 2, and 1; it is all \nright if some of these don t work. The idea is that the run-time library routine can handle a subgrid \nof any size or shape simply by shaving off, at each step, the widest strip for which the compiler man\u00adaged \nto construct a workable multistencil, subject of course to the constraint that it cannot process a strip \nwider than remains to be processed. Thus a subgrid one of whose axes is of length 21 might be processed \nas two strips of width 8, one strip of width 4, and one strip of width 1 (each strip in turn being processed \nas two half-strips, of course). If the stencil pattern were such that a width-8 multistencil would consume \ntoo many registers, then the compiler would simply not generate code for the width-8 case, and the run-time \nlibrary routine would process the subgrid as five strips of width 4 and a strip of width 1. One example \nof this is the 13-point diamond: : . .:,. ,/, ,,, , , ,.. ;, *,.,.,, ,, , ,,. ,, . ... ... @ A width-8 \nmultistencil would require 48 registers, but the width-4 multistencil requires only 28 registers and \ntherefore works just fine: 5.4 Sweeps: Avoiding Yet More Redundant Loads Suppose that we have just computed \nand stored a set of eight results for this width-8 multistencil: We now wish to compute the next set \nof w results in the half-strip, the line just above this one. What data elements must be loaded? We can \nsee that there is considerable overlap with the elements loaded for the last line. Moreover, we can now \nsee the wisdom of choosing the ~oftotntnosf row for the registers to be re-used as accumulators: that \nrow contained data elements that cannot possibly be needed for the next line of results. Therefore, once \nthe results for the current line have been stored, the regis\u00adters containing them (and possibly other \nregisters whose contents are no longer needed) maybe recycled for the next set of data elements to be \nloaded. Indeed, a simple strategy is to split the multistencil into rows and allocate registers accordingly \nThen to compute the next line of results one need only load one new row of data elements. We avoid the \nshuf\u00adfling of registers by loading this new row into the row of registers just vacated by the storing \nof results. As a result the pattern in the xegisters rotates: The compiler must therefore unroll the \nloop ; the microcode loop is not unrolled, but the pattern of regis\u00adter access is unrolled so as to make \nthree copies (because them are three rows in the multistencil). The unrolling factor is passed as a parameter \nto the micro\u00adcode at run time. Even this is wasteful because it is too naive. Look again at the 13-point \ndiamond stencik The width-4 multistencil for this pattern contains only 28 positions, but dividing it \ninto five equal rows of eight positions each would require 40 registers: The solution is to treat separately \neach column of the multistencil. Instead of having a ring buffer of five rows, each row containing eight \nregisters, the compiler treats each column as a separate ring buffer. In this example the first and last \ncolumns require only a single register; the second and seventh columns require ring buffers of three \nregisters apiece; and the middle four columns require five registers apiece. As each line of a half-strip \nis processed, the compiler arranges to load, not simply a single row of data elements, but a leuding \nedgeof the multistencih Each loaded element is placed in the next available position in the ring buffer \nof registers for its column. The ring buffers rotate at different rates, of course, being of different \nsizes. The compiler must unroll the Ioop of register access patterns 15 times in this exam\u00adple, because \n15 is the LCM (least common multiple) of the ring buffers sizes 5,3, and 1, The sets of registers loaded \non successive iterations look like this: There is a cost (in consumption of sequencer scratch data memory) \nto this unrolling, so the compiler attempts to minimize it. The strategy is to try to keep each ring \nbuffer equal in size to the maximum column size, except for columns of height 1, because reducing a ring \nbuffer to size 1 always saves registers and never makes the LCM larger. If this uses too many registers, \nthen the compiler slowly compresses the columns, from smallest to largest, from their too-large size \nto their natural size. This approach tends to minimize the LCM, at least for the column heights typically \nencoun\u00adtered (less than 10). In the general case even more clever strategies may be required. Implerrwimtation \nStatus We have implemented two versions of this technique and are working on a third version for production \nuse. The first version was prototype and tested entirely in Lucid Common Lisp in order to take advantage \nof the excellent programming and debugging environment for both Lisp code and CM-2 microcode. We tested \nthe microcode loops thoroughly and timed them. This ver\u00adsion processes definitions such as (defstencil \ncross (r x cl C2 C3 C4 c5) (sin91e-float single-float) (:=r (+ (* cl (cshift x 1 -l)) (* C2 (cshift \nx 2 -1) ) (* C3x) (* C4 (cshift x 2 +1) ) (* C5 (cshift x 1 +1))))  The result is an ordinary Lisp function \nnamed cross thattakes Connection Machine arrays as arguments and performs the indicated computation. \nThe second version was constructed by translating the run-time library from Lisp to C (using a standard \nsemiautomatic tool developed at Thinking Machines Corporation) and providing front-end and back-end interfaces \nto the Lisp-coded compilation algorithms. The front end parses Fortran subroutines of a particu\u00adlar form, \nproducing a Lisp data structure; the result of the compilation is translated by the back end from Lisp \ninto C. The resulting C code can then be compiled by an ordinary C compiler and linked with other Fortran \ncode. This version therefore processes user code writ\u00adten entirely in Fortran, but requires the assignment \nstatement for a stencil computation to be isolated in a subroutine of its own SUBROUTINE CROSS (Rr X, \nCl, C2, C3, C4, C5) REAL, ARRAY( :, : ) :: R,x, cI, C2, C3, C4, C5 R = Cl * CSHIFT (X, 1, -1) &#38; \n+ C2 * CSHIFT (X, 2, -1) &#38; +C3*X &#38; + C4 * CSHIFT (X, 2, +1) &#38; + C5 * CSHIFT (X, 1, +1) END \n The microcode is the same as for the first version. The third version, now under construction, will \nbe fully integrated into the CM Fortran compiler, The microcode and run-time library will be the same \nas for the second version, but the compilation algorithms will be translated from Lisp to C and become \na module of the CM Fortran compiler. The need for isolated subrou\u00adtines will be eliminated. We plan to \nallow the user to flag stencil assignment statements with a directive in the form of a structwecl comment; \nwhile the compiler can easily recognize candidate assignment statements, the presence of a directive \njustifies the compiler in pro\u00adviding feedback to the user, such as a warning if the statement could not \nbe processed by this technique after all (for lack of registers, for example). 7 Results We have timed \ncode using the first and second ver\u00adsions of our implementation. Our preliminary timings were conducted \non small 16-node single-board machines that aw used within Thinking Machines Cor\u00adporation for softwaw \ntesting. The table below indicates the actually measumd flop rates and also shows the extrapolated flop \nrate for a full-size Connection Machine with 65,536 bit processors (2,048 nodes). Stencil Subgrid Number \nof Number of Elapsed Date Measured Extrapolated Pattern Size Nodes Iterations Tme Measured Speed to 2048 \nNodes 64X 128 16 250 4.54 seconds 21 Nov 90 44.6 Mflops 5.31 Gflops 128 X 256 16 100 6.78 seconds 21 \nNov 90 69.5 Mflops 8.90 Gflops 256 X 256 16 100 13.00 seconds 21 Nov 90 72.8 Mflops 9.29 Gflops 64x64 \n16 500 8.10 seconds 21 Nov 90 68.8 Mflops 8.80 Gflops 64X 128 16 250 6.07 seconds 21 Nov 90 91.7 Mflops \n11.74 Gflops 128 X 128 16 250 12.40 seconds 21 Nov 90 89.8 Mflops 11.50 Gflops 128 X 256 16 100 10.26 \nseconds 21 Nov 90 86.7 Mflops 11.10 Gflops 256 X 256 16 100 20.12 seconds 21 Nov 90 88.6 Mflops 11.34 \nGflops 64x64 16 500 9.81 seconds 21 Nov 90 56.8 Mflops 7.27 Gflops 64X 128 16 250 8.19 seconds 21 Nov \n90 68.0 Mflops 8.70 GflopS 128 X 128 16 250 15.30 seconds 21 Nov 90 72.9 Mflops 9.34 Gflops 128 X 256 \n16 100 10.44 seconds 21 Nov 90 S5.3 Mflops 10.92 Gflops 256 X 256 16 100 20.80 seconds 21 Nov 90 85.6 \nMflops 10.95 Gflops &#38;:;.;.:..,. ,!!.:. ;., :,.,,,,,,,.,,,, ~,.: m:* Yj J!:,:,,.,. ,:,:,::,,&#38;~ \n64x64 64X 128 128 X 128 128 X 256 16 16 16 16 500 250 250 100 11.40 seconds 9.98 seconds 18.70 seconds \n15.30 seconds 21 Nov 21 Nov 21 Nov 21 Nov 90 90 90 90 71.6 Mflops 82.0 Mflops 87.7 Mflops 85.6 Mflops \n9.16 Gflops 10.50 Gflops 11.23 GflopS 10.95 Gflops WI 256 X 256 16 100 30.51 seconds 21 Nov 90 85.9 Mflops \n11.00 Gflops 16 16 128 X 256 256 X 256 Siiiiin,::y::::,::.:!Y ,,. ,. 64X 128 :;; ,/,, :, ,,:::~!:?.$64X \n128 2048 2048 100 100 35000 38001 12.30 seconds 22.43 seconds 1919.41 seconds 1643.79 seconds 7 Dec 90 \n7 Dec 90 21 Nov 90 21 Nov 90 106.6 Mflops 116.8 Mflops 11.62 14.73 13.65 Gflops 14.95 Gflops Gflops Gflops \n64X 128 2048 38001 1627.59 seconds 21 Nov 90 14.88 Gflom  (Experience at Thinking Machines Corporation \nhas We found that the timings were quite sensitive to shown that such extrapolations are quite reliable. \nThis small changes in the run-time library, because the is because the CM-2 is a completely synchronous \nSIMD microcode loops are so fast that the front end computer machine; the time required for computation \nand grid is hard pressed to keep up. Careful recoding of the run\u00adcommunication does not change as the \nnumber of time support routines, including strength reduction to nodes is increased.) avoid integer multiplications \nin the inner front-end As indicated in the table, all measurements are loops, wsulted in fuirther improvements. \nbased on sustained executions of at least 100 iterations The computation in the code that won the Gordon \nof the stencil computation. All measurements are based Bell prize consisted of a nine-point cross stencil \nplus an on elapsed (wall clock) time. Only useful floating-additional term from two times steps before \nthe current point operations are counted; for example, computa-one. This tenth term was added in separately. \n(Future tion of one result for the pattern versions of the compiler should be able to handle all ,,.,. \nten terms as one stencil pattern.) We measured two ver\u00ad,,::,,,, sions of the code: one where the main \nloop consisted of ~ a stencil pattern, adding in the tenth term, and then . performing two assignment \nstatements to shift the is counted as 9 floating-point operations (5 multiplies time-step data into the \ncorrect variables for the next and 4 adds), despite the fact that it is executed on the iteration. This \nloop ran at a sustained 11.62 gigaflops. CM-2 as 5 multiply-add steps, because one of the adds The other \nversion unrolled this main loop by a factor of is not really useful (it merely adds a product to zero). \nthree so that the three variables could exchange roles In all cases the clock rate of the Connection \nMachine without any need to copy data from place to place. In system was 7 MHz. All measurements are \nfor single\u00adone trial this loop ran at a sustained 14.88 gigaflops. precision (that is, 32-bit) floating-point \noperations. Initialization and 1/0 overhead was figured in for pur\u00adposes of the prize competition, resulting \nin an overall sustained application performance of 14.18 gigaflops. While we are very pleased with this \nfigure, we realize that not all applications will achieve quite such high performance. We are confident, \nhowever, that a large number of stencil-based applications will run faster than 10 gigaflops with this \ntechnology. Related Work The sorts of computations for which we are optimiz\u00ading performance are certainly \nnot new. Moreover, many of the techniques that we have used are standard notions from the compiler literature: \npattern matching of the assignment statements, strip-mining, and loop unrolling to avoid register shuffling, \nSee the list of ref\u00aderences for some conventional citations. The tech\u00adniques that are new have primarily \nto do with overcoming or circumventing the constraints and restrictions of the floating-point chip or \nthe CM-2 architecture. A number of other researchers cited have examined the question of optimizing communications \nbetween processors for stencil-based applications. Closely related is research on discovering good tilings \nof arrays for distributing data or tasks among processors so that particular loops will be efficient \nor avoid dependency problems; here we avoid the general problem by restricting the domain of applicability \nWork on getting good cache performance in multicomputer systems is also related, but we use compile-time \ntechniques to manage a register set explicitly rather than using gen\u00aderal cache mechanism. 9 Conclusions \nand Future Work The class of stencil patterns is so large that we believe it is more effective to allow \nusers to express them as pro\u00adgram fragments than to provide a large selection of library routines, We \nbelieve that this class of computa\u00adtions can be further generalized. We are studying the question of \nwhich generalizations will produce the best performance improvements, both generally speaking and in \nparticular with reference to the Qmnection Machine Model CM-2. 10 References Allen, Frances E. and Cocke, \nJohn. A Catalogue of Optimizing Transformations. In Rustin, Randall (cd.), Design and Optimization of \nCompilers (Proceedings of the Courant Computer Science Symposium 5, March 1971). Prentice-Hall (Englewood \nCliffs, N.J,, 1972), 1-30. Baker, Lawrence J, Hypercube Performance for 2-D Seismic Finite-Difference \nModeling. I?roc. Third Con\u00adference on Hypercube Concurrent Computers and Applications. ACM (Pasadena, \n1988), 1146-1156. Gallivan, Kyle; Jalby, William; and Gannon, Dennis, On the Problem of Optimizing Data \nTransfers for Com\u00adplex Memory Systems. Pmt. 1988 International Confer\u00adence on Supercomputing. ACM (St. \nMalo, France, July 1988), 238-253. Gannon, Dennis; Jalby, WIIliam; and Gallivan, Kyle. Strategies for \nCache and Local Memory Management by Global Program Transformations, Proc. 1987 Inter\u00adnational Conf, \non Supercomputing (Athens, 1987). Hillis, W. Daniel. The Connection Machine, MIT Press (Cambridge, Massachusetts, \n1985). Hudak, David E., and Abraham, Santosh G. Com\u00ad piler Techniques for Data Partitioning of Sequentially \nIterated Parallel Loops. Proc. 1990 International Con\u00ad ference on Supercomputing. ACM SIGARCH (June 1990), \n187-200. The 1990 Gordon Bell Prize Winners. IEEE Software 8,2 (March 1991), 4-5. Ikudawa, K.; Fox, G.C.; \nKolawa, A.; and Flower, J,W, An Automatic and Symbolic Parallelization System for Distributed Memory \nParatlel Computers. Proc. Fifth Distributed Memory Computing Conference. IEEE Computer Society (Charleston, \nApril 1990), 1105-1114. Koelbel, Charles; Mahrotra, Piyush; and Van Rosen\u00addale, John. Supporting Shared \nData Structures on Dis\u00adtributed Memory Architectures. Proc. Second ACM SIGPLAN Symp. on Principles and \nParctice of Parallel Programming (PPOPP) (Seattle, March 1990), 177-186. Koelbel, Charles; Mahmtra, Piyush; \nSIatz, Joel; and Berryman, Harry, Parallel Loops on Distributed Machines. Proc. Fifth Distributed Memory \nComputing Conference. IEEE Computer Society (Charleston, April 1990), 1017 1027. %cha, David G. An Approach \nto Compiling Single-Point Iterative Programs for Distributed Memory Com\u00adputers. Proc. Fifth Distributed \nMemory Computing Conference. IEEE Computer Society (Charleston, April 1990), 1017-1027. Society for Industrial \nand Applied Mathematics. Both Gordon Bell Prize Winners Tackle Oil Industry Problems. Siam News 23,3 \n(May 1990). %lchenbach, Karl. Parallel CFD Algorithms on SUPRENUM. Pmc. Third International Conference \non Supercomputing. International Supercomputing Insti\u00adtute, Inc. (1988), 313-322. Thinking Machines Corporation. \nConnection Machine Model CM-2 Technical Summay. Technical report HA87\u00ad4 (Cambridge, Massachusetts, April \n1987). Wolfe, Michael. More Iteration Space Tiling. Proc. Supercomputing 89. IEEE Computer Society and \nACM SIGARCH (Reno, Nov. 1989), 655--664. Wolfe, Michael. Optimizing Supercompilers for Super\u00adcompufers. \nMIT Press (Cambridge, Mass., 1989).  \n\t\t\t", "proc_id": "113445", "abstract": "", "authors": [{"name": "Mark Bromley", "author_profile_id": "81100181438", "affiliation": "Thinking Machines Corporation, 245 First Street, Cambridge, Massachusetts", "person_id": "P189975", "email_address": "", "orcid_id": ""}, {"name": "Steven Heller", "author_profile_id": "81100257548", "affiliation": "Thinking Machines Corporation, 245 First Street, Cambridge, Massachusetts", "person_id": "PP31079277", "email_address": "", "orcid_id": ""}, {"name": "Tim McNerney", "author_profile_id": "81100532936", "affiliation": "Thinking Machines Corporation, 245 First Street, Cambridge, Massachusetts", "person_id": "P282359", "email_address": "", "orcid_id": ""}, {"name": "Guy L. Steele", "author_profile_id": "81100586340", "affiliation": "Thinking Machines Corporation, 245 First Street, Cambridge, Massachusetts", "person_id": "P100946", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/113445.113458", "year": "1991", "article_id": "113458", "conference": "PLDI", "title": "Fortran at ten gigaflops: the connection machine convolution compiler", "url": "http://dl.acm.org/citation.cfm?id=113458"}