{"article_publication_date": "05-01-1991", "fulltext": "\n Practical Dependence Testing * Gina Goff Ken Kennedy Cllau-Wen Tseng Department of Computer Science \nRice University Houston, TX Abstract Precise and efficient dependence tests are essential to theeffectivermss \nofaparallelizing compiler. This paper proposes a dependence testing scheme based on classi\u00adfyingpairs \nofsubscripted variable references. Exact yet fast dependence tests are presented for certain classes \nofarray references, as well as empirical results showing that these references dominate scientific Fortran \ncodes. These dependence tests are being implemented at Rice University in both PFC, aparallelizing compiler, \nand ParaScope, a parallel programming environment, Introduction In the past decade, high performance \ncomputing has become vital for scientists and engineers alike. Much progress has been made in developing \nlarge-scale paral\u00adlel architectures composed of powerful commodity mi\u00adcroprocessors. To exploit parallelism \nand the memory hierarchy effectively for these machines, compilers must be able to analyze data dependence \nprecisely for array references in loop nests, Even for a single micropro\u00adcessor, optimizations utilizing \ndependence information can result in integer factor speedups for scientific codes [II]. However, because \nof its expense, few if any scalar compilers perform dependence analysis. Parallelizing compilers have \ntraditionally relied on two dependence tests to detect data dependence be\u00adtween pairs of array references: \nBanerjee s inequalities and the GCD test [8, 55]. However, these tests are usu\u00adally more general than \nnecessary. This paper presents empirical results showing that most array references in scientific Fortran \nprograms are fairly simple. For these simple references, we demonstrate a suite of highly ex\u00adact yet \nefficient dependence tests. We feel that these tests will significantly reduce the cost of performing \n * lMs research was supported by the Center for Research on Parallel Computation, a National Science \nFoundation Science and Technology Center, by IBM Corporation, and by the Cray Research Foundation. Permission \nto copy without fee all or part of this material is granted provided that the copies ere not made or \ndistributed for direct commercial advantage, the ACM copyright notice and the title of the publication \nand its date appear, and notice is given that copying is by permission of the Asaocietion for Computing \nMachinery. To copy otherwise, or to republish, requires a fee andlor specific permission. @ 1991 ACM \n0.89791-428-7/91/0005/001 5.+.$1 ,50 Proceedings of the ACM SIGPLAN 91 Conference on Programming Language \nDesign and Implementation. Toronto, Ontario, Canada, June 26-28, 1991. 77251-1892 dependence analysis, \nmaking it more practical for all compilers. We begin with some definitions. 1.1 Data Dependence The theory \nof data dependence, originally developed for automatic vectorizers, has proved applicable to a wide range \nof optimization problems. We say that a data dependence exists between two statements SI and Sz if there \nis a path from SI to SZ and both statements access the same location in memory. There are four types \nof data dependence [32, 33]: True (flow) dependence occurs when S1 writes a memory location that S2 later \nreads. Anti dependence occurs when S1 reads a memory location that S2 later writes. Output dependence \noccurs when S1 writes a memory location that Sz later writes. Input dependence occurs when S1 reads a \nmemory location that Sz later reads.  Dependence analysis is the process of computing all such dependence \nin a program, 1.2 Dependence Testing Calculating data dependence for arrays is complicated by the fact \nthat two array references may not always access the same memory location. Dependence testing is the method \nused to determine whether dependence exist between two subscripted references to the same array in a \nloop nest. For the purposes of this explica\u00adtion, we will ignore any control flow except for the loops \nthemselves. Suppose that we wish to test whether or not there exists a dependence from statement S1 to \nSZ in the following model loop nest: DO il =L1, UI DO i2 =L2, U2 ... DO in =Ln, Un SI A(~l(il, . . ..in) \n. . ..~(il. (.i~))., i~)) =.-. S2 . ..= A(gl(il, . . ..i~). g~(il,(,i~))., i~)) ENDDO ... ENDDO ENDDO \n Let a and P be vectors of n integer indices within the ranges of the upper and lower bounds of the n \nloops in the example. There is a dependence from SI to Sz if and only if there exist a and ~ such that \ntr is lexi\u00ad 15 cographically less than or equal to ,8 and the following system of dependence equations \nis satisfied: f~(cl) = gi(p) di, 1< i < m Otherwise the two references are independent. 1.3 Distance \nand Direction Vectors Data dependence may be characterized by their access pattern between loop iterations \nusing distance and di\u00adrection vectors. Suppose that there exists a data de\u00adpendence for a = (al, . . \n.,a~) and @ = (Bl, . . .,~n). Then the distance vector D = (DI, ..., D~) is defined as @ a, The direction \nvector d = (cZ1, . . . . ct~) of the dependence is defined by the equation: < ifai<~i d~ = = if~i=Pi \n> ifa~>&#38; { The elements are always displayed in order left to right, from the outermost to the innermost \nloop in the nest. For example, consider the following loop nest: DO 10,i DO10 j DO10 k 10 A(i+l, j, k-1) \n=A(i, j, k) +C The distance and direction vectors for the dependence between the definition and use of \narray A are (1, O, 1) and (<, =, >), respectively. Since several different val\u00ad ues of a and /3 may \nsatisfy the dependence equations, a set of distance and direction vectors may be needed to completely \ndescribe the dependence. Direction vectors, first introduced by Wolfe [53], are useful for calculating \nthe level of loop-carried depen\u00ad dence [1, 4, 25]. A dependence is carried by the outer\u00ad most loop for \nwhich the direction in the direction vector is not = . For instance, the direction vector (<, =,>) for \nthe dependence above shows the dependence is car\u00ad ried on the i loop. Carried dependence are important \nbecause they de\u00ad termine which loops cannot be executed in parallel without synchronization. Direction \nvectors are also useful in determining whether loop interchange is le\u00ad gal and profitable [4, 25, 53]. \nDistance vectors, first used by Kuck and Muraoka [34, 42], are more precise versions of direction vectors \nthat specify the actual distance in loop iterations be\u00ad tween two accesses to the same memory location, \nThey may be used to guide optimization to exploit par\u00ad allelism [23, 27, 36, 51, 54] or the memory hierarchy \n[11, 19, 43]. Dependence testing thus has two goals. First, it tries to disprove dependence between pairs \nof subscripted references to the same array variable. If dependence may exist, it tries to characterize \nthem in some man\u00ad ner, usually as a minimal complete set of distance and direction vectors. Dependence \ntesting must also be conservative and assume the existence of any depen\u00ad dence it cannot disprove. Otherwise \nthe validity of any optimizations based on dependence information is not guaranteed. 1.4 Exact Tests \nWhen array subscripts are linear expressions of the loop index variables, dependence testing is equivalent \nto the problem of finding integer solutions to systems of lin\u00adear Diophantine equations, an NP-complete \nproblem [15, 17]. In practice most dependence tests, such as Banerjee s inequalities [8], seek efficient \napproximate solutions. Exact tests, on the other hand, are depen\u00ad dence tests that will detect dependence \nif and only if they exist. 1.5 Indices and Subscripts In this paper we will use the term indez to mean \nthe index variable for some loop surrounding both of the references. We assume that all auxiliary induction \nvari\u00ad ables have been detected and replaced by linear func\u00ad tions of the loop indices [2, 3, 5, 52]. \nIn addition, we will use the term subscrzpt to refer to one of the subscripted positions in a pair of \narray references; z. e., the pair of subscripts in some dimension of the two array references. Dependence \ntests always consider a pair of array references, but for brevity we refer to a subscript pair simply \nas a subscript. For example, in the pair of references to array A in the following loop nest, DO10 i \nDO10 j DO10 k 10 A(i, j) =A(i, k) +C we say that index i occurs in the first subscript and indices j \nand k occur in the second subscript. 2 Classification In this section we present two orthogonal criteria \nfor classifying subscripts in a pair of array references. The first criterion, complexity, refers to \nthe number of in\u00addices appearing within the subscript. The second crite\u00adrion, separability, describes \nwhether a given subscript interacts with other subscripts for the purpose of de\u00adpendence testing. 2.1 \nComplexity When testing for dependence, we classify subscript po\u00ad sitions by the total number of distinct \nloop indices they contain. A subscript is said to be ZIV (zero index vari\u00ad able) if the subscript position \ncontains no index in ei\u00ad ther reference. A subscript is said to be SIV (single in\u00ad dex variable) if only \none index occurs in that position. Any subscript with more than one index is said to be MIV (multiple \nindex variable). For inst ante, consider the following loop: DO10 i DO10 j DO10 k 10 A(5, 1+1, j) =A(N, \ni, k) +C When testing for a true dependence between the two references to A in the code below, the first \nsubscript is ZIV, the second is SIV, and the third is MIV. For the sake of simplicity, we will ignore \noutput dependence in this and all future examples.  2.2 Separability y When testing multidimensional \narrays, we say that a subscript position is separable if its indices do not oc\u00adcur in the other subscripts \n[1, 10]. If two different sub\u00adscripts contain the same index, we say they are coupled [38]. For example, \nin the loop below, DO10 i DO10 j Do 10k 10 A(i, j, j) =A(i, j, k) +C the first subscript is separable, \nbut the second and third are coupled because they both contain the index j, ZIV subscripts are vacuously \nseparable because they con\u00adtain no indices. Separability is important because multidimensional array \nreferences can cause imprecision in dependence testing. One suggested approach, called subscripi-by\u00adsubscript \ntesting, is to test each subscript separately and intersect the resulting sets of direction vectors [53]. \nHowever, this method provides a conservative approxi\u00admation to the set of directions within a coupled \ngroup it may yield direction vectors that do not exist. For inst ante, consider the following loop: DO10 \ni 10 A(i+l, i+2) = A(i, i) + C A subscript-by-subscript test would yield the single di\u00adrection vector \n(<). But a careful examination of the statement reveals that this direction vector is invalid since no \ndependence exists! On the other hand, if all subscripts are separable, we may compute the direction vector \nfor each subscript independently, and merge the direction vectors on a positional basis with full precision. \nFor example, in the loop nest below, DO10 i DO10 j DO10 k 10 A(i+l, j, k-1) = A(i., j, k) + C the leftmost \ndirection in the direction vector is deter\u00admined by testing the first subscript, the middle direc\u00adtion \nby testing the second subscript and the rightmost direction by testing the third subscript. The resulting \ndirection vector, (<,=,>), is precise. The same ap\u00adproach applied to distances allow us to calculate \nthe exact distance vector (1, O, l). We know from linear algebra that systems of equa\u00adtions with distinct \nvariables may be solved indepen\u00addently, and their solutions merged to form an exact solution set. Previous \ntests have used this property for array references consisting of only separable SIV subscripts [1, 23, \n34, 36, 42]. More recently, Li et ai. formalized and applied this method in the A-test to array references \nalso containing MIV or coupled sub\u00adscripts [38]. Our treatment of constraint propagation in Section 5.3 \nwas inspired by their work. 3 Dependence Testing The goal of dependence testing in this paper is to con\u00adstruct \nthe complete set of distance and direction vec\u00ad tors representing potential dependence between an ar\u00adbitrary \npair of subscripted references to the same array variable. Since distance vectors may be treated as pre\u00adcise \ndirection vectors, we will simply refer to direction vectors for the rest of the paper. For the sake \nof sim\u00adplicity we will also assume that all loops have a step of 1. Non-unit step values may be normalized \non the fly as needed. 3.1 Partition-Based Algorithm The classifications presented in the previous section \nmay be used naturally in a partition-based dependence testing algorithm as follows: 1. Partition the \nsubscripts into separable and mini\u00admal coupled groups. 2. Label each subscript as ZIV, SIV or MIV. \n3. For each separable subscript, apply the appropri\u00adate single subscript test (ZIV, SIV, MIV) based on \nthe complexity of the subscript. This will produce independence or direction vectors for the indices \noccurring in that subscript.  4 For each coupled group, apply a multiple subscript test to produce a \nset of direction vectors for the in\u00addices occurring within that group. 5. If any test yields independence, \nno dependence exist, 6a Otherwise merge all the direction vectors com\u00adputed in the p~evious steps into \na single set of direction vectors for the two references. This algorithm is implemented in both PFC, \nan au\u00adtomatic vectorizing and parallelizing compiler [3, 4], and ParaScope, a parallel programming environment \n[12, 27, 28]. Our dependence testing algorithm takes advantage of separability by classifying all subscripts \nin a pair of array references as separable or part of some minimal coupled group. A coupled group is \nminimal if it can\u00adnot be partitioned into two non-empty subgroups with distinct sets of indices. Once \na partition is achieved, each separable subscript and each coupled group have completely disjoint sets \nof indices. Each partition may then be tested in isolation and the resulting distance or direction vectors \nmerged without any loss of precision. Subscripts may be partitioned using the algorithm in Figure 1. \nAn alternative algorithm based on UNION/FIND is implemented in PFC. The dependence testing algorithm \nmay halt and return independence as soon as the test for any separable subscript or couple~ group yields \nindependence, since no simultaneous sol~\u00ad tions are possible once we prove no solutions exist fx some \nsubset of the entire system. 4 Single Subscript Tests INPUT: A pair of m-dimensional array references \ncontaining subscripts S1 . . . S~ enclosed in n loops with indices 11 . . .-I~ OUTPUT: A set of partitions \nPI . . . P~J, n S n, each containing a separable or minimal coupled group foreachi, l<i<ndo Pi -{s;} \nendfor for each index Ii, 1~ i ~ n do k e (none) for each remaining partition Pj do if 3 S1 ~ Pj such \nthat S~ contains Ii then if k = (none) then k+--j else PkLPkuPj discard Pj  endif endif endfor endfor \nFigure 1: Subscript Partition Algorithm 3.1.1 Merge The merge operation described in the test algorithm \nmerits more explanation. Since each separable and coupled subscript group contains a unique subset of \nin\u00addices, merge may be thought of as Cartesian product. In this loop nest, DO10 i DO10 j 10 A(i+l, j) \n=A(i, j) +C the first position yields the direction vector (<) for the i loop. The second position yields \nthe direction vector (=) for the j loop. The resulting Cartesian product is the single vector (<, =). \nA more complex example is shown below: DO10 i DO10 j 10 A(l+l, 5) =A(i, N) +C The first subscript yields \nthe direction vector (<) for the i loop. Since j does not appear in any subscript, we must assume the \nfull set of direction vectors { (<), (=), (>)} for the j loop. The merge thus yields the following set \nof direction vectors: { (<, <), (<,=),(<,>) ) Dependence test results for ZIV subscripts are treated \nspecially. If a ZIV subscript proves independence, the d~pendence test algorithm halts immediately. If \ninde\u00adpe~dence is not proved, the ZIV test does not produce diriction vectors, so no merge is necessary. \nWe first consider dependence tests for single separable subscripts. All tests presented in this paper \nassume that the subscript being tested contains expressions that are linear in the loop index variables. \nA subscript expression is linear if it has the form: alil+aziz +.. .+anil -e-e where ik is the index \nfor the loop at nesting level k; all a~, 1 s k < n, are integer constants; and e is an expression possibly \ncontaining loop-invariant symbolic expressions. We assume in PFC that all direction vec\u00adtors are possible \nfor nonlinear subscripts. 4.1 ZIV Test The ZIV test is a dependence test that takes two loop\u00ad invariant \nexpressions. If the system determines that the two expressions cannot be equal, it has proved inde\u00ad pendence. \nOtherwise the subscript does not contribute any direction vectors and may be ignored. The ZIV test can \nbe easily extended for symbolic expressions. Simply form the expression representing the difference between \nthe two subscript expressions. If the differ\u00ad ence simplifies to a non-zero constant, we have proved \nindependence. 4.2 SIV Tests A number of authors, notably Banerjee, Cohagan, and Wolfe [8, 14, 55], have \npublished a Single-Index exact test for linear SIV subscripts based on finding all solu\u00adtions to a simple \nDiophantine equation in two variables. Here we present a new exact test based on the idea of treating \nthe most commonly occurring SIV subscripts as special cases. It provides greater efficiency and is easily \nextended to handle symbolics and coupled sub\u00adscripts. We begin by separating SIV subscripts into two \ncategories: strong SIV and weak SIV subscripts. 4.2.1 Strong SIV Subscripts An SIV subscript for index \ni is said to be strong if it has the form (ai + c1, ai + C2); i.e., if it is linear and the coefficients \nof the two occurrences of the index i are constant and equal [1, 10]. For strong SIV subscripts, we define \nthe dependence distance as: d=i i=\u00ad a Then a dependence exists if and only if d is an integer and Id \nI s U L, where U and L are the loop upper and lower bounds. For dependence that do exist, the dependence \ndirection is given by: < ifd>O direction = = ifd=o > ifd<O { The strong SIV test is thus an exact test \nthat can be implemented very efficiently in a few operations. Since we calculate distance vectors in \nany case, we get the test for almost no additional cost. Another advantage of the strong SIV test is \nthat it can be easily extended to handle loop-invariant symbolic DO10 i =1,4 DO10 i =1,4 DO10 i =1,4 \n10 A(i)=A(i-1) 10 A(i)=A(l) 10 A(i)=A(5-i) o 0 0 ..a 00 ..0     u ,.~a o 0 ,.,@ o 0 o .1 0 /0 \n o 0 0 / L ..,@ e o O 0 /0 o 0 8 p -o .. L Lu i 0000 Bounded Iteration Space Strong SIV Figure2: \nGeometric expressions. The trick is to first evaluate the depen\u00addence distance, d, symbolically. If \nthe result is a con\u00adst ant, then the test maybe performed w above. Other\u00adwise calculate the difference \nbetween the loop bounds and compare the result with d symbolically. For in\u00adst ante, consider the following \nloop: DO IOi=l, I 1 10 A(i+2N) = A(i+N) + C The strong SIV test can evaluate the dependence dis\u00adtance, \nd, as 2N N, which simplifies to N. This is compared with the loop bounds symbolically, proving independence \nsince N > N 1. 4.2.2 Weak SIV Subscripts A weak SIV subscript has the form (al i + CI, az~ i-c2), where \nthe coefficients of the two occurrences of index i have different constant values. As stated previously, \nweak SIV subscripts may be solved using the Single-Index exact test. However, we also find it helpful \nto view the problem geometrically, where the dependence equation: ali+ q =azi +q describes a line in \nthe two dimensional plane with i and i as the axes [10]. The weak SIV test can then be formulated as \ndetermining whether the line derived from the dependence equation intersects with any in\u00ad teger points \nin the space bounded by the loop upper and lower bounds, m shown in Figure 2. In particular, we find \nit advantageous to identify the following two special cases. Weak-zero SIV Subscripts. We call the case \nwhere al = O or az = O a weak-zero SW subscript. If az is equal to zero, the dependence equation reduces \nto: C2 c1 i= al 19 0 0 y  o 0 ,.G o /0 0 o -,.@ o ..0 0 0 o /0 - o g,. o o - o o 0 ... . \\ 1> mm \n Weak-Zero SIV Weak-Crossing SIV View of SIV Tests We simply need to check that the resulting value \nfor i is an integer and within the loop bounds. A similar check applies when a 1 is zero. The weak-zero \nSIV test finds dependence caused by a particular iteration i. In scientific codes, i is usually the first \nor last iteration of the loop, eliminating one possible direction vector for the dependence. More im\u00adportantly, \nweak-zero dependence caused by the first or last loop iteration may be eliminated by applying the /oop \npeeiing transformation [28]. For instance, consider the following simplified loop in the program iomcatv \nfrom the SPEC benchmark suite [49]: DOIO1=I, N 10 Y(i, N) = Y(I, N) + Y(lJ, M) The weak-zero SIV test \ncan determine that the use of Y (1, N) causes a loop-carried true dependence from the first iteration \nto all other iterations. Similarly, with aid from symbolic analysis the weak-zero SIV test can discover \nthat the use of Y (N, N) causes a loop-carried anti dependence from all iterations to the last iteration. \nBy identifying the first and last iterations as the only cause of dependence, the weak-zero SIV test \nadvises the user or compiler to peel the first and last iterations of the loop, resulting in the following \nparallel loop: Y(1, N) = Y(1, N) + Y(N, N) DO10 i =2, H 1 10 Y(i, N) = Y(1, N) + Y(N, N) Y(N, N) = Y(I, \nN) + Y(N, N) Weak-crossing SIV Subscripts. We label as weak\u00ad crossing SIV all subscripts where a2 = \nal; these sub\u00adscripts typically occur as part of Cholesky decompo\u00adsition. In these cases we set i = i \nand derive the dependence equation: C2 c1 ~ = 2al This corresponds to the intersection of the dependence \nequation with the line i = i , To determine whether dependence exist, we simply need to check that the \nresulting value i is within the loop bounds, and is either an integer or has a non-integer part equal \nto 1/2. Weak-crossing SIV subscripts cause crossing depen\u00addence, loop-carried dependence whose endpoints \nall cross iteration i [1, 4]. These dependence may be elim\u00adinated using the loop splitting transformation \n[28]. For inst ante, consider the following loop from the Callahan\u00ad Dongarra-Levine vector test suite \n[13]: DO IOi. =1, N 10 A(i) = A(N-i+l) + C  The weak-crossing SIV test determines that depen\u00addence exist \nbetween the definition and use of A, and that they all cross iteration (IV + 1)/2. Splitting the loop \nat that iteration results in two parallel loops: DO 10 i = l,(N+l)/2 10 A(i) = A(N-1+1) + C DO20 i = \n(N+l)/2 + 1, N 20 A(i) = A(N-i+i) + C  Both forms of weak SIV tests are also useful for testing coupled \nsubscripts, described in Section 5. We rely on the Single-Index exact test to handle the general case. \n4.3 Complex Iteration Spaces SIV tests can be extended to handle complex itera\u00adtion spaces, where loop \nbounds may be functions of other loop indices; for example, triangular or trape\u00adzoidal loops. We need \nto compute the minimum and maximum loop bounds for each loop index. Starting at the outermost loop nest \nand working inwards, we replace each index in a loop upper bound with its max\u00adimum value (or minimal \nvalue if it is a negative term). We do the opposite in the lower bound, replacing each index with its \nminimal value (or maximal if it is a neg\u00ad ative term). We evaluate the resulting expressions to calculate \nthe minimal and maximal values for the loop index, then repeat for the next inner loop. This al\u00ad gorithm \nreturns the maximal range for each index, all that is needed for SIV tests. 4,4 MIV Tests The Banerjee-GCD \ntest [4, 8, 25, 55] may be employed to construct all legal direction vectors for linear sub\u00ad scripts \ncontaining multiple indices. In most cases the test can also determine the minimal dependence dis\u00ad tance \nfor the carrier loop. Since the literature in this area is extensive, we will not discuss it further \nhere. PFC employs a special version of the Banerjee-GCD test enhanced for triangular loop nests [8, 26]. \nWe note a special case of MIV subscripts called RDIV (Restricted Double Index Variable) subscripts that \nhave form (al i + c1, azj + CZ). They are similar to SIV subscripts, except that i and j are distinct \nin\u00ad dices. By observing different loop bounds for i and j, SIV tests may also be extended to exactly \ntest RDIV subscripts [55].  4.5 Symbolic Tests As we have pointed out in the text, we can perform de\u00adpendence \ntesting in a natural way for subscripts with loop-invariant symbolic additive constants. The basic idea \nis that C2 c1, the difference between the constant terms of each subscript expression, maybe formed \nsym\u00adbolically and simplified. The result may then be used like a constant. In this section we describe \na special test for indepen\u00addence between references to a subscripted variable that are contained in two \ndifferent loops at the nesting level of the SIV index. In the pair of loops below, DO IOi=l, N1 10 A(ali \n+ cl) = ... D020j=l, N2 20 . ..= A(azj + CZ) we can use the following general test. Assume for the \nsake of simplicity that al is greater or equal to zero. A dependence exists if the following dependence \nequation is satisfied: ali azj =scz c1 for some value of i, 1< i~ IVl and j, 1~ j ~ Nz. There are two \ncases to consider. First, al and az may have the same sign. In this case, ali a2j assumes its maximum \nvalue for i = IVl and j = 1 and its minimum value for i = 1 and j = N2 (remember, al and az are non-negative). \nHence, there is a dependence only if al azNz ~ C2 c1 s alNl az If either inequality is violated, the \ndependence cannot exist. In the second case, al and a2 have different signs. In this case, al i a2j \nassumes its maximum for i = N1 and j = N2, so there is a dependence only if al a2 ~ C2 C1 ~ alN1 a2N2 \n If either inequality is violated, the dependence cannot exist. It should be noted that these inequalities \nare just special cases of the Banerjee inequality. However, when they are stated in this form, it is \nobvious that they can be formulated for symbolic values of c1, C2, N1 and N2. Furthermore, this test \nmay also be used to test for de\u00ad pendence in the same loop, with N1 = N2. Our empirical study in Section \n6 shows that sym\u00ad bolic testing techniques significantly enhance the effec\u00ad tiveness of dependence tests \nin PFC. Any symbolic expressions that remain at the end of dependence test\u00ad ing may also be used as a \nuser query in an interactive system, or as a condition to break the dependence at run-time. 5 Delta Test \nThe tests used for separable subscripts can also be used on each subscript of a coupled group if any \ntest proves independence, then no dependence exists. However, we have already seen that subscript-by-subscript \ntesting in INPUT: coupled SIV and/or MIV subscripts OUTPUT: hybrid distance/direction vector, constrained \nMIV subscripts initialize elements of constraint vector @ to (none) while 3 untested SIV subscripts do \napply SIV test to all untested SIV subscripts, return independence or derive new constraint vector @ \n@+@~@ if C = 0then return independence else if~#@then ~+@  propagate constraint (? into MIV subscripts, \npossibly creating new ZIV or SIV subscripts apply ZIV test to untested ZIV subscripts, return independence \nor continue endif endwhile while 3 untested RDIV subscripts do test and propagate RDIV constraints endwhile \ntest remaining MIV subscripts, then intersect resulting direction vectors with ~ return distance/direction \nvectors from ~ Figure 3: Delta Test Algorithm a coupled group may yield false dependence. Some re\u00adcent \nresearch has focused on overcoming this deficiency (38, 50, 56]. In this section we present the Delta \ntest, a multiple subscript test designed to be exact yet efficient for common coupled subscripts. Figure \n3 presents an overview of the Delta test algorithm. The main insight behind the Delta test is that con\u00adstraints \nderived from SIV subscripts maybe propagated into other subscripts in the same coupled group effi\u00adciently, \nusually without any loss of precision. Since most coupled subscripts in scientific Fortran codes are \nsimple, in practice the Delta test is an exact yet fast multiple subscript test. The Delta test can detect \nindependence if any of its component ZIV or SIV tests determine indepen\u00addence. Otherwise it converts \nall SIV subscripts into constraints, propagating them into MIV subscripts where possible. It repeats \nuntil no new constraints are found, then propagates constraints for coupled RDIV subscripts. Remaining \nMIV subscripts are tested; the results are intersected with existing constraints. We describe the Delta \ntest algorithm in greater detail in the following sections. 5.1 Constraints Constraints are assertions \non indices derived from sub\u00adscripts. For instance, the subscript (al i + c1, azi + C2) generates the \nconstraint al i a2i1 = C2 c1 for index i. A dependence distance is an example of a simple constraint. \nThe constraint vector ~ = (61, 62, . . . . &#38;n) is a vector with one constraint for each of the n \nindices in the coupled subscript group. It is used in the Delta test to store constraints generated from \nSIV tests, and can be easily converted to distance or direction vectors. A constraint 6 may have the \nfollowing form: dependence line a line (a% + by = c) represent\u00ading the dependence equation  dependence \ndistance the value (d) of the depen\u00addence distance; it is equivalent to the dependence line (z y= d) \n dependence point a point (x, y) representing de\u00adpendence from iteration z to y  Dependence distances \nand lines derive directly from the strong and weak SIV tests. Dependence points result from intersecting \nconstraints, as described in the next section. 5.2 Intersect ing Constraints Since dependence equations \nfrom all subscripts must be solved simultaneously for dependence to exist, in\u00adtersecting constraints \nfrom each subscript results in greater precision. If the result of the intersection is the empty set, \nno dependence is possible, Constraint inter\u00adsection has been employed for both direction vectors [53] \nand coupled SIV subscripts [1, 10]. The version employed by the Delta test is equivalent to an exact \nmultiple subscript SIV test. Dependence distances are the easiest to intersect; a simple comparison suffices. \nIf all dist antes are not equal, then no dependence exist. For example, recon\u00adsider the following loop \nnest from Section 2.2: DO10 i 10 A(i.+1, i+2) = A(i., i) + C  Applying the strong SIV test to the first \nsubscript de\u00adrives a dependence dist ante of 1. Doing the same for the second subscript derives a distance \nof 2. To inter\u00adsect the two constraints we perform a comparison. This results in the empty set, proving \nindependence. It turns out that even complex constraints from SIV subscripts may be intersected exactly. \nRecall that each dependence equation from a SIV subscript may be viewed as a line in a two-dimensional \nplane. Inter\u00adsecting constraints from multiple SIV subscripts then corresponds to calculating the point(s) \nof intersection for lines in a plane. No dependence exists if the lines do not intersect at a common \npoint within the loop bounds, or if the coordinates of this point do not have integer values. If all \ndependence equations intersect at a single dependence point, its coordinates are the only two iterations \nthat actually cause dependence. DO10 i 10 A(i, i) =A(1, i-l) +C  For instance, in this example loop \ntesting the first and INPUT: constraints 61,62 and loop bounds U, L second subscripts in the pair of \nreferences to A derives OUTPUT: new constraint 6 or 0 the dependence lines (i = 1) and (i = i  1), respec\u00ad \ntively. These dependence lines intersect at the depen\u00ad {*either 61 or 6, = (none) *} dence point (1, \n2), indicating that the only dependence if til = (none) then is from the first to the second iteration. \nSince calculat\u00ad return &#38; ing the intersection of lines in a plane can be performed else if 82 = (none) \nthen precisely, constraint intersection is exact. return &#38; The full constraint intersection algorithm \nis shown in {* both 61 and C5zare dependence distances *} Figure 4. Note that for simplicity dependence \ndistances else if 61 = (dl) and 62 = (d2) then are also treated as lines at places in the algorithm. \nif dl = d2 then 5.3 Propagating Constraints return (dl) else 5.3.1 SIV Constraints return 0 A major contribution \nof the Delta test is its ability endif to propagate constraints derived from SIV subscripts {* both else \nif 61 and 61 = (*I, 62 are dependence y,) and C$Z= (22, points*} YQ) then into coupled MIV subscripts, \nusually without precision. The resulting constrained subscript be tested with greater efficiency and \nprecision. loss of can then Figure 5 if 81 = X2 and V1 = YQ then shows the constraint propagation algorithm. \nIts goal return (Zl, yl) is to utilize SIV constraints for each index to eliminate else instances of \nthat index in the target MIV subscript. We return 0 demonstrate the algorithm in the following example: \nendif {* both 61 and 62 are dependence lines/distances *} DO 10 i else if 81 = (alx+bly= 62 = (a2x + \nbzy = cl) and C2) then 10 DO 10 A(i+l, j i+j) = A(i, i+j) {* lines are parallel if slopes are equal*} \nApplying the strong SIV test to the first subscript of if alibl = a2/b2 then array A derives a dependence \ndist ante of (1) for index if cl/lq = c2/b2 then i. We can propagate this constraint into the second \nreturn (alz + bly = cl) subscript to eliminate both occurrences of i, resulting else in the constrained \nSW subscript (j 1, j). We then return 0 apply the strong SIV test to derive a distance of 1 on endif \nloop j. All subscripts have been tested, so the Delta {* lines must intersect if not parallel*} test \nis finished. We merge the elements of the con\u00ad else straint vector to determine that a dependence exists \nwith distance vector (1, 1), ($1, if L Y1) -~ xl intersection s U and al of 61 and 62 is an integer \nand Constraint cause we were propagation in able to eliminate this example is both instances eract be\u00adof \nindex L s return yl < U and (%1, yl ) yl is an integer then i in the Section constrained 6 shows that \nsubscript. Our this is frequently empirical the case study in for scien\u00ad else tific codes, In general \nthe algorithm may only eliminate return 0 one occurrence of an index. This results in improved endif \nprecision when testing coupled groups, but is not exact. endif If desired, additional precision maybe \ngained by utiliz\u00ad {* either&#38;is a dependence line/distance *} ing the constraint to reduce the range \nof the remaining {* and 62 is a dependence point, or vice versa *} index, as in Fourier-Motzkin Elimination \n[44]. {* without loss of generality, assume the former*} The constraint propagation algorithm is an incre\u00ad \nelse 61 = (alz+bly= if alzl + blyl = c1 return ($1, yl ) else cl) then and 62 = (rl, yl) mental adaptation \nof the A-test heuristic for selecting linear combinations of subscript expressions. It has also been \nextended to efficiently handle constraints from SIV tests and linearly dependent subscripts [38]. Below \nwe present some more examples of the Delta test. return 0 endif Multiple Passes The Delta test algorithm \niterates endif if MIV subscripts are reduced to SIV subscripts, since Figure 4: Constraint Intersection \nthey nest may produce demonstrates new this: constraints. The following loop 22 INPUT: MIV subscript \nwith form (alil + . ..+anin -Fe. a~i\\+. ..+a~i~+ e ), . and constraint vector C = (61, 62, . . . . d~ \n)  OUTPUT: constrained ZIV, SIV, or MIV subscript for each index i~ with nonzero a~ or a; do if b~ = \ndependence distance (d) then eee akd; ak~o;a~ba~ ak else if 6k = dependence line (ax + f?~ = c) then \nif a= Othen e4-e a~c/~ ; a~ +-O else if ~ = Othen ete+a~c/a; Ukeo else if a = @then e&#38;e+CJ~C/a;ak4--O;a~ \n+CJi+ak else {* multiply terms of subscript by a to *} {* retain integer coefficients in result *} foreach \n~~{al,..., a~, aj, a~, e,e, }do }do Tf-o!?\u00ad endfor endif else if bk = dependence point (~, y) then e~e+ak~-a~~ \nakeo;a~+o endif endfor Figure 5: Constraint Propagation DO10 i DO10 j DO 10k 10 A(j-i, i+i, j+k) = A(j-i, \ni, j+k) In the first pass of the Delta t~st, the second subscript is tested, producing a dependence \ndistance of (1) on the i loop. This constraint can be propagated into the first subscript, resulting \nin the subscript (j+ 1, j). Since a new SIV subscript has been created, the algo\u00ad rithm repeats. On the \nsecond pass, the new subscript is tested to produce a distance of (1) on the j loop. This constraint \nis then propagated into the third sub\u00ad script to derive the subscript (k 1, k). The new SIV subscript \ncauses another pass that discovers a distance of 1 on the k loop. Since all SIV subscripts have been \ntested, the Delta test halts at this point, returning the distance vector (1, 1, l). Improved Precision \nThe Delta test may also im\u00adprove the precision of other dependence tests on any remaining constrained \nMIV subscripts. DO10 i =1,100 DO10 j = 1,100 10 A(i-1, 21) = A(i, i.+j+llo) When applied to each subscript \nin this example loop, Banerjee s inequalities show possible dependence for both subscripts. The Delta \ntest can improve this by converting the first subscript into a dependence distance of (1) and propagating \nit into the second subscript to produce the constrained MIV subscript (2, j i + 110). Banerjee s inequalities \ncan now detect independence for the constrained subscript. DO10 i DO10 j 10 A(i, 2j+i) = A(i, 2j-i+5) \n Similarly, in this example 100P the GCD test shows in\u00adteger solutions for both subscripts. However, \npropagat\u00ading the distance constraint (0) for i from the first sub\u00adscript into the second subscript yields \nthe constrained MIV subscript (2j, 2j 2i + 5). The GCD test can now detect independence since the GCD \nof the coefficients of all the indices is 2, which does not divide evenly into the constant term 5. Distance \nVectors The Delta test is particularly use\u00adful for analyzing dependence in skewed loops [27, 36, 54], \nincluding upper triangular loops skewed by loop normalization [3, 53]. Consider the following simplified \nkernel from the Livermore Loops [41]: DO IOi=l, N DO IOj=l, N 10 A(i, j) = A(i-l, j) + A(i, j-1) + A(i+i, \nj) + A(i, j+l) Since all subscripts are separable, the strong SIV test can be applied to calculate distance \nvectors of (1, O) and (O, 1) for the dependence in the loop nest. This dependence information can be \nused to skew the inner loop to expose parallelism, resulting in the following loop nest: DO IOi=l, N \nDO10 j =I+j., ~+i 10 A(i, j-i-) = A(i-l, j-i) + A(i, j-i-1) + A(i+l, j-i) + A(i, j-.i+l) At this point, \nmost dependence tests are unable to cal\u00ad culate distance vectors due to the presence of MIV subscripts. \nHowever, the Delta test can easily prop\u00ad agate distance constraints for i from the first subscript into \nthe second subscript to derive the distance vectors (1, 1) and (O, 1). This dependence information may \n then be used to guide further optimizations such as loop interchange, loop blocking, or scalar replacement \n[11, 51, 55]. 5.3.2 Restricted DIV Constraints In the previous section we showed how SIV constraints \nmay be propagated. Propagating MIV constraints is expensive in the general case. However, we present \na method to handle an important special case consisting of coupled RDIV subscripts (discussed in Section \n4.4). For simplicity, we consider array references with the following form: DO10 i DO10 j 10 A(il+cl, \ni2-tc2) = A(i3+c3, ia+ cl) When il, i2 are instances of index i, and i3, i4 are in\u00adstances of index \nj, a constraint between i and j is de\u00adrived from the first subscript that may be propagated into the \nsecond subscript employing the algorithms for SIV subscripts discussed previously. The only addi\u00ad tional \ndiffer. More i2, i3 set of consideration commonly, are instances dependence is that il, iq are of index \nequations: bounds instances j. This yields for i of the and j may index i, and following i+cl=j +c3 j+c2Ei \n+c4 Each dependence equation may be tested separately without loss of precision when checking for dependence. \nHowever, both equations must be considered simulta\u00adneously when determining which distance or direction \nvectors are possible. We can propagate constraints for these coupled RDIV subscripts by considering instances \nof index i in the second reference as i + Ai, where Ai is the depen\u00addence distance between the two occurrences \nof i, We do the same for index j to produce the following set of dependence equations: i+c1=j+Aj+c3 j+ \nC2G i+A~+-C4 It is clear that these the first two equations may be combined to result in the equation: \n&#38;~Aj=cl+c2, cs cq We can then check this dependence equation when test\u00ading for a specific distance \nor direction vector. Array Transpose We show how RDIV constraints may be used in this array transpose \nexample: DO10 i. DO10 j 10 A(l, j) =A(j, i) +C Propagating RDIV constraints results in the depen\u00addence \nequation Ai + Aj = O. As a result, distance vectors must have the form (d, d), and the only valid direction \nvectors are (<, >) and (=,=). The direction vector (>, <) may be ignored since it is equivalent to a \nreversed dependence with direction vector (<, >) [9]. All dependence are thus carried on the outer loop; \nthe inner loop may be executed in parallel.  5.4 Precision and Complexity The precision of the Delta \ntest depends on the nature of the coupled subscripts being tested. The SIV tests applied in the first \nphase are exact. The constraint intersection algorithm is also exact, since we can calcu\u00adlate the intersection \nof any number of lines in a plane precisely. The Delta test is thus exact for any number of coupled SIV \nsubscripts. In the constraint propagation phase, weak-zero SIV constraints and dependence points may \nalways be ap\u00adplied exactly, since they assign values to occurrences of an index in a subscript. Dependence \ndistances (from strong SIV subscripts) may also be propagated into MIV subscripts without loss of precision \nwhen the co\u00adefficients of the corresponding index are equal. Fortu\u00adnately, this is frequently the case \nin scientific codes. When constraints can be propagated exactly and all subscripts uncoupled by eliminating \nshared indices, the Delta test prevents loss of precision due to multiple sub\u00adscripts. At its conclusion, \nif the Delta test has tested all subscripts using ZIV and SIV tests, the answer is ex\u00adact. If only separable \nMIV subscripts remain, the Delta test is limited by the precision of the single subscript tests applied \nto each subscript. Research has shown that the Banerjee-GCD test is usually exact for single subscripts \n[6, 30, 37], so the Delta test is also likely to be exact for these cases. There are three sources of \nimprecision for the Delta test. First, constraint propagation of dependence lines and distances may be \nimprecise if an index cannot be completely eliminated from both references in the tar\u00adget subscripts. \nSecond, complex iteration spaces such as triangular loops may impose constraints between subscripts not \nutilized by the Delta test. Finally, the Delta test does not propagate constraints from general MIV subscripts. \nAs a result, coupled MIV subscripts may remain at the end of the Delta test. More general but expensive \nmultiple subscript depen\u00addence tests such as the A or Power tests may be used in these cases [38, 56]. \nSince each subscript in the coupled group is tested at most once, the complexity of the Delta test is \nlinear in the number of subscripts. However, constraints may be propagated into subscripts multiple times, \n6 Empirical Results In this section we present empirical results to demon\u00adstrate that our dependence \ntests are applicable for sci\u00adentific Fortran codes, PFC currently performs the fol\u00adlowing dependence \ntests: o subscript classification and partitioning o ZIV test (symbolic)  * strong SIV test (symbolic) \nweak SIV test (including special cases) e MIV tests (GCD, triangular Banerjee) o Delta test (constraint \nintersection, propagation of distance constraints only) For this study we measured the number times \neach dependence test was applied by PFC when processing four groups of Fortran programs: RiCEPS (Rice \nCom\u00adpiler Evaluation Program Suite), the Perfect and SPEC benchmark suites [16, 49], and two math libraries, \neus\u00adpack and linpack. Explanation Table 1 provides the number of lines and subroutines for each program, \na histogram of the number of array dimensions for each pair of array ref\u00aderences tested, as well as the \nnumber of separable, cou\u00adpled, and nonlinear subscripts pairs found. array pairs test subscript ,irs \nnonlin program type lines subr ID 2D 3D 4D total sep coup total subs RiCEPS baro Shallow Water Atmosphere \n1002 7 34 360 0 0 394 754 0 754 o euler ID Unsteady Euler 1200 14 106 294 0 0 400 694 0 694 21 heat2d \nHeat Conduction System 336 2 352 251 0 0 603 822 32 854 149 linpackd L]near Algebra Benchmark 797 11 \n523600 88 74 50 124 29 mhd2d 2D MHD Equations 927 14 280 196 21 0 497 711 24 735 232 onedim Eigenfunction/Eigenenergies \n1016 16 297 209 0 0 506 567 148 715 47 shear 3D Turbulence 915 15 787 0 828 0 1615 2415 856 3271 368 \nsimple 2D Hydrodynamics 1892 18 174 163 0 0 337 500 0 500 1 sphot Particle Transport 1143 6 282 77 0 \n0 359 436 0 436 21 vortex Vortex Simulation 709 20 174 42 0 0 216 258 0 258 0 PeTfect adm Pseudospectral \nAir Pollution 6105 97 647 160 521 0 1328 2506 24 2530 19 arc2d 2D Fluid Flow Solver 3965 39 168 405 4396 \n0 4969 13172 994 14166 9 bdna Molecular Dynamics of DNA 3980 43 1787 269 0 0 2056 2085 240 2325 562 dyfesm \nStructural Dynamics 7608 78 268 866 7 7 1148 1897 152 2049 0 flo52 Transonic Inviscid Flow 1986 28 54 \n256 1624 0 1934 5284 154 5438 0 mdg Molecular Dynamics of Water 1238 16 928500 933 934 4 938 296 mg3d \nDepth Migration 2812 28 749 52 133 40 974 1412 0 1412 494 ocean 2D Ocean Simulation 4343 36 439 29 0 \n0 468 497 0 497 303 qcd Quantum Chromodynamics 2327 34 7774 171 3 0 7948 8115 10 8125 0 spec77 Weather \nSimulation 3885 65 1187 477 54 0 1718 2255 48 2303 122 spice Circuit Simulation 18521 128 470 51 0 0 \n521 546 26 572 50 track Missile Tracking 3735 34 242 115 0 0 357 464 8 472 7 trfd 2 Electron Integral \nTransform 485 7 396400 103 135 32 167 6 SPEC doduc Thermohydra;\\i;~ Modelization 5334 41 656 895 0 0 \n1551 2446 0 2446 11 fpppp 2 Electron Integral Derivative 2718 38 997 108 0 0 1105 1213 0 1213 3 matrix300 \nMatrix Multiplications 439 6 0800 8 14 2 16 0 nasa7 NASA Ames Fortran Kernels 1105 17 155 106 329 347 \n937 2267 475 2742 24 tomcatv Mesh Generation 195 1 0 207 0 0 207 272 142 414 0 Math Libraries eispack \nEigensystems Library 11519 75 2995 10295 0 0 13290 10823 12762 23585 2052 linpack Linear Algebra Library \n7427 51 3224 869 0 0 4093 4228 734 4962 290 Table 1: Program Characteristics ZIV strong weak-zero weak-cross \nother I Delta symbolics used program ~ AS- I AS I AS I AS II AS I ASIP S1 baro 294 294 0 000 000 0001000 \n0000 17 17 euler 267 162 240 236 3 000 400 000 0 00 0000 114 2 heat 2d 450 127 127 127 0 43 43 0 000 \n000 20 16 0 11305 10 linpackd 13 0 57 57 0 000 0 00 000 2 20 231602 10 0 mhd2d 55 28 384 384 1 12 12 \n0 400 000 0 00 12000 41 onedim 39 11 448 448 0 29 29 4 440 6200 32 31 25 315214 97 34 shear 179 72 1327 \n1327 151 346 346 168 400 4000 100 32 32 233 76 40 0 40 simple 112 44 358 357 0 550 000 000 2 20 0000 \n373 44 sphot 280 191 134 134 0 000 000 100 0 00 0000 11 0 vortex 42 24 216 216 0 000 000 0001000 0000 \n18 0 adm 632 210 1341 1327 142 17 17 0 Lll 22 12 52 52 01 13 12 12 0000 142 21 arc2d 9501 3628 2821 2807 \n0 111 111 12 000 3800 000 107 38 38 0 3024 454 bdna 85 21 1393 1249 14 69 69 0 000 000 76 33 0 119 28 \n12 1 454 14 dyfesm 1073 629 514 514 0 19 19 0 000 3600 26 14 1 69 32 27 6 50 0 f1052 2372 1043 2599 2599 \n6 87 87 0 220 3680 000 77 14 14 0 47 10 mdg 464 415 177 177 0 000 000 000 000 2200 20 0 mg3d 362 20 456 \n456 54 220 000 36 36 0 000 0000 51 ocean 20 2 108 107 8 110 400 840 34 34 0 0000 24 1 qcd 7945 6725 149 \n107 0 15 15 0 000 700 110 5550 27 1 spec77 385 62 1650 1631 273 24 24 0 12 12 12 2700 2790 24770 286 \n201 spice 258 102 229 210 8 110 000 1000 881 9913 46 8 track trfd doduc. 230 60 795 95 424 181 71 1638 \n181 71 1627 5 0 0 330 18 18 220 0 000 000 000 W--44 4220 16 0000 0 0 0 15 12 58 0 0 0 fpppp 902 11 232 \n232 110 000 1 00 000 15 15 0 0000 40 natrix300 30 12 12 0 000 000 000 000 1110 00 nasa7 832 410 947 947 \n0 184 184 0 6 10 1210 28 26 13 101 11 1 3 503 12 tomcatv 32 13 204 204 0 78 78 1 26 26 0 0001000 71 0 \n0 0 00 eispack 4215 471 6732 6228 2 726 726 92 ?93 225 0 [842 O 011671915407 569826451179314 61161191 \nlinpack 2754 227 1348 1271 88 15 15 0 4560 55 0 01 118 118 2 337 99 8 28 537 95 ZIV SIV MIV Delta symbolics \ncat egory strong weak-zero weak-cross other used summed oveT all pTogmms % of all tests applied 46.32 \n35.29 2.41 0.69 3.03 2.98 9.28 % of all successful tests 32,24 54.00 3.82 1.18 0.21 2.77 6.33 25.42 % \nof all proven independence 83.58 4.74 1.52 0.13 0 2.70 7.33 11.55 % of applications that were successful \n43.95 96.64 100.00 57.75 4.45 58.67 43.06 % of applications that proved independence 43.95 3.27 15.33 \n4.65 0 22.05 19.24 averaged over all pTograms % of all tests applied 38.24 49.40 2.81 0.64 1.43 3.03 \n4.45 % of all successful tests 21.91 67.54 3.87 0.84 0.43 3.31 2.50 14.94 % of all proven independence \n70.34 16.65 2.06 0.24 0 3.30 7.42 14.96 % of applications that were successful 34.74 97.70 100.00 40.51 \n17.53 74.61 41.75 % of applications that proved independence 34.74 4.02 3.96 8.52 0 15.49 22.04 Table \n3: Comparison of Dependence Tests Table 2 describes the usage and success frequencies the 8449 coupled \ngroups found were of size two; 19 cou\u00adof the dependence tests for each program. For each pled groups \nof size three were encountered in n asa 7. test, the table shows the number of times the test was The \nDelta test constraint intersection algorithm (A) applied, (S) succeeded in eliminating at least one tested \n6950 coupled groups exactly (8270). Propagation direction vector, and (I) proved independence. Note of \ndistance constraints was applied in 376 cases (4.4 ?ZO), that the S and I columns are combined for the \nZIV test converting MIV subscripts into SIV form in all but 28 because they are always identical. cases. \nThe Delta test thus managed to test 7298 cou- The A, S, and I columns for the Delta test reflect fre-pled \ngroups exactly (86~o), using only constraint inter\u00ad quencies measured for constraint intersection only. \nA section and propagation of dependence distances. We separate column (P) indicates the number of times \ndis-expect this percentage to improve once we implement tance constraints were propagated into MIV subscripts. \nfull constraint propagation, including propagation of Results for dependence tests applied on the constrained \nRDIV constraints. subscripts are credited to the test invoked. The last two Our results show that the \nSIV and Delta tests pre\u00ad columns in Table 2 show the number of times symbolic sented in this paper tested \nmost subscripts exactly. additive constants were manipulated in tests that (S) MIV tests such as the \nBanerjee-GCD test are only succeeded in eliminating direction vectors or (I) proved needed for a small \nfraction of all subscripts (3~0), independence. though they are important for certain programs. Many \nTable 3 summarizes the effectiveness of each depen-of the successful tests required PFC S ability to \nmanipu\u00addence test relative to other tests by presenting the per-late symbolic additive constants (23910). \nThis indicates centage contribution of each test to the total number of the import ante of symbolic analysis \nand dependence applications, successes, and independence. Also dis-testing. played is the absolute effectiveness \nof each test; i. e., 7 Related Workthe percentage of applications of each test that proved independence \nor was successful in eliminating one or In this section, we discuss the large body of work in the more \ndirection vectors. field of dependence testing. The suite of tests presented In order to limit bias toward \neither large or small pro-in this paper are distinguished by the fact that they grams, two groups of \nresults are presented. In the first combine high precision and efficiency by targeting a group, percentages \nare calculated after summing results simple yet common subset of all possible subscripts. over all programs. \nIn the second group, percentages are 7.1 Integer and Linear Programming calculating for each program \nand then averaged. Since testing linear subscript functions for dependence Analysis PFC applied dependence \ntests 74889 times is equivalent to finding simultaneous integer solutions (88% of all subscript pairs). \nSubscript pairs were not within loop limits, one approach is to employ integer tested if they were nonlinear \n(670), or if tests on other programming methods [18, 44]. Linear programming subscripts in the same multidimensional \narray have al\u00ad techniques such as Shost ak s loop residue [46] or Kar\u00adready proven independence. Over \nall array reference markar s method [24] are also applicable, though inte\u00adpairs tested, most subscript \npairs were ZIV (59%) or ger solutions are not guaranteed. Unfortunately, while strong SW (37~o). Few \nof the subscripts tested were integer and linear programming techniques are suitable MIV (3%). The ZIV \nand strong SIV tests combined for solving large systems of equations, their high ini\u00adfor most of the \nsuccessful tests (86%). The ZIV test tialization costs and implementation complexity make accounted for \nalmost all reference pairs proven inde\u00ad them less desirable for dependence testing. pendent (94%). 7.2 \nSingle Subscript Tests Most subscripts were separable. Coupled subscripts (20% overaH) were concentrated \nin a few programs, no-The earliest work on dependence tests concentrated on tably eispack (75% of all \ncoupled subscripts). Most of deriving distance vectors from strong SIV subscripts [34, 36, 42]. Cohagan \n[14] described a test that an\u00adalyzes general SIV subscripts symbolically. Banerjee and Wolfe [7, 53] \ndeveloped the current form of the Single-Index exact test. For MIV subscripts, the GCD test may be used \nto check unconstrained integer solutions [6, 25]. Baner\u00adjee s inequalities provide a useful general-purpose \nsin\u00adgle subscript test for constrained real solutions [7]. It has also been adapted to provide many different \ntypes of dependence information [4, 8, 9, 25, 26, 53]. Re\u00adsearch has shown that Banerjee s inequalities \nare exact in many common cases [6, 30, 37], though results have not yet been extended for direction vectors \nor complex it erat ion spaces. The I-test developed by Kong et d. integrates the GCD and Banerjee tests \nand can usually prove integer solutions [31]. Gross and Steenkiste propose an efficient interval analysis \nmethod for calculating dependence for arrays [21]. Unfortunately their method does not handle coupled \nsubscripts, and is unsuitable for most loop transformations since distance and direction vec\u00adtors are \nnot calculated. Lichnewsky and Thomasset describe symbolic dependence testing in the VATIL vec\u00ad torizer \n[39]. Haghighat and Polychronopoulos propose a flow analysis framework to aid symbolic tests [22]. Execution \nconditions may also be used to refine de\u00ad pendence tests. Wolfe s All-Equals test checks for loop\u00ad independent \ndependence invalidated by control flow within the loop [53]. Lu and Chen s subdomain test in\u00ad corporates \ninformation about indices from conditionals within the loop body [40], Klappholz and Kong have extended \nBanerjee s inequalities to do the same [29]. 7.3 Multiple Subscript Tests Early approaches to impose \nsimultaneity in testing multidimensional arrays include intersecting direction vectors from each dimension \n[53] and linearization [9, 20]; they proved inaccurate in many cases. True multiple subscript tests provide \nprecision at the ex\u00adpense of efficiency by considering all subscripts simul\u00ad taneously. In comparison, \nthe Delta test propagates constraints increment ally as needed. l?ourier-Motzkin Elimination Many of \nthe earliest multiple subscript tests utilized Fourier-Motzkin elim\u00adination, a linear programming method \nbased on pair\u00adwise comparison of linear inequalities. Kuhn [35] and Triolet et al. [48] represent array \naccesses in convex ~egions that may be intersected using Fourier-Motzkin elimination. Regions may also \nbe used to summarize memory accesses for entire segments of the program. These techniques are flexible \nbut expensive. Triolet found that using Fourier-Motzkin elimination for de\u00adpendence testing takes from \n22 to 28 times longer than conventional dependence tests [47]. Constraint-Matrix The Constraint-Matrix \ntest de\u00ad veloped by Wallace is a simplex algorithm modified for integer programming [50]. Its precision \nand expense are difficult to ascertain since it halts after an arbi\u00ad trary number of iterations to avoid \ncycling. The sim\u00adplex algorithm has worst case exponential complexity, but takes only linear time for \nmost linear programming problems. However, Schrijver states that in combinato\u00adrial problems where coefficients \ntend to be 1, 0, or 1, the simplex algorithm is slow and will cycle for certain pivot rules [44]. A-test \nLi et al. present the A-test, a multidimensional version of Banerjee s inequalities that checks for simul\u00adtaneous \nconstrained real-valued solutions [38]. The A test forms linear combinations of subscripts that elim\u00adinate \none or more instances of indices, then tests the result using Banerjee s inequalities. Simultaneous real\u00advalued \nsolutions exist if and only if Banerjee s inequal\u00adities finds solutions in all the linear combinations \ngen\u00aderated, The A-test can test direction vectors and triangular loops. Its precision may be enhanced \nby also applying the GCD or Single-Index exact tests to the pseudosub\u00adscripts generated. However, there \nis no obvious method to extend the A-test to prove the existence of simulta\u00adneous integer solutions. \nThe A-test is exact for two dimensions if unconstrained integer solutions exist and the coefficients \nof index variables are all 1,0 or -1 [37]. However, even with these restrictions it is not exact for \nthree or more coupled dimensions. The Delta test may be viewed as a restricted form of the A-test that \ntrades generality for greater efficiency and precision. Multidimensional GCD Banerjee s multidimen\u00adsional \nGCD test checks for simultaneous unconstrained integer solutions in multidimensional arrays [8]. It ap\u00adplies \nGaussian elimination modified for integers to cre\u00adate a compact system where all integer points provide \ninteger solutions to the original dependence system. It can also be extended to provide an exact test \nfor dis\u00adtance vectors [56]. Power Test Wolfe and Tseng s Power test gains great precision by applying \nloop bounds using Fourier-Motzkin elimination to the dense system resulting from the multidimensional \nGCD test [56]. The Power test is expensive, but is also flexible and well-suited for pro\u00adviding precise \ndependence information such as direction vectors in imperfectly nested loops, loops with complex bounds, \nand non-direction vector constraints. Both the Constraint-Matrix and A-tests require that a pretest \nbe used to eliminate linearly dependent sub\u00ad scripts. In comparison, the Power and Delta tests can detect \nand discard linearly dependent subscripts as part of their basic algorithm. 7.4 Empirical Studies Li \net cd. showed that for coupled subscripts, multiple subscript tests may detect independence in up to \n3670 more cases than subscript-by-subscript tests in libraries such as eispack [38]. Our results for \neispack demon\u00ad strate that the Delta test is as effective in testing cou\u00ad pled subscripts. A comprehensive \nempirical study of array subscripts and conventional dependence tests was performed by Shen et al. [45]. \nOur stucly focuses on the complexity of subscripted references and the effective\u00adness of our partition-based \ndependence tests. We also provide some data on the efficacy of symbolic depen\u00addence tests. 8 Conclusions \nThis paper presents a strategy for dependence testing based on the thesis that array references in real \ncodes have simple subscripts. Our empirical results show that in practice the dependence tests described \nin this paper are extremely precise, fast, and applicable to the vast majority of all subscripts in scientific \ncodes. In the few cases where our tests are inapplicable, we can afford applying more expensive tests \nsince their cost may be effectively amortized. Experience has shown that dependence analysis can be highly \nuseful for both scalar and parallel compilers [2, 11, 33]. We feel that the dependence tests described \nin this paper make dependence analysis more efficient and hence practical for every compiler. 9 Acknowledgements \nAs with most research, the suite of dependence tests we have described in this paper owes much to the \ncontributions of others. The first version of PFC [3, 25] employed subscript-by-subscript testing using \nthe Banerj ee-GCD test extended to calculate the level, minimum distance, and interchange information \nfor each dependence. This strategy proved very useful for PFC S layered vectorization algorithm. In the \nmid-eighties, Randy Allen and others imple\u00ad mented the strong SIV test [1] that was applied to each \nsubscript if all the subscripts were SIV and separable this change dramatically improved the efficiency \nof de\u00ad pendence testing. Both the strong SW and Banerjee tests were enhanced to deal with symbolic additive \ncon\u00ad stants. Some simultaneity was also added for multidi\u00ad mensional arrays by comparing dependence distances \nfor each index. David Callahan later extended PFC to handle weak SIV subscripts and general constraint \nintersection [10]. In addition, he proposed the RDIV constraint propa\u00ad gation algorithm and conducted \nan initial study on the complexity of array subscripts. Paul Havlak extended PFC S ability to test subscripts \ncontaining symbolic ex\u00ad pressions, and assisted in developing the symbolic test described in Section \n4.5. Unfortunately, dependence testing in PFC was hin\u00ad dered due to an oversight in the original algorithm \nthat employed SIV tests only if al! subscripts were SIV sub\u00ad scripts. The presence of a single MIV subscript \nin a multidimensional array would cause PFC to fall back on subscript-by-subscript testing with the Banerjee- \nGCD tests. The introduction of the A-test [38] mo\u00adtivated us to reexamine PFC S test strategy, exposing \nthe obvious mistake. Some additional work led to the current form of the Delta test. We are grateful \nto all the people named above for their contributions in the development of the depen\u00addence tests described \nin this paper, and to the PFC and ParaScope research groups, especially Paul Havlak, for their help in \nconducting our empirical study. We also wish to thank Kathryn McKinley, Doug Moore, and Vicky Dean for \ntheir assistance on this paper. References [1]J. R. Allen. Dependence Analysis for Subscripted Variables \nand Its Application to Program Transformations. PM3 the\u00adsis, I%ce University, April 1983. [2] J. R. Allen. \nUnifying vectorization, parallelization, and opti\u00admization: The Ardent compiler. In L. Kartashev and \nS. Kar\u00adtashev, editors, Proceedings of the Third International Con\u00adference on Supevcomputing, 1988. [3] \nJ. R. Allen and K. Kennedy. PFC: A program to convert Fortran to parallel form. In Supercomputers: Design \nand Applications, pages 186 205. IEEE Computer Society Press, Silver Spring, MD, 1984. [4] J. R. Allen \nand K. Kennedy. Automatic translation of For\u00adtran programs to vector form. ACM Tmnsuctions on Pro\u00adgramming \nLanguages and Systems, 9(4):491 542, October 1987. [5] Z. Ammarguellat and W. Harrision. Automatic recognition \nof induction variables and recurrence relations by abstract interpretation. In Proceedings o.f the ACM \nSIGPLAiY 9 O Conference on Program Language Design and Implementa\u00adtion, White Plains, NY, June 1990. \n[6] U. Banerjee. Data dependence in ordinary programs. Mas\u00adter s thesis, Dept. of Computer Science, University \nof Illinois at Urban&#38;Champaign, November 1976. Report No. 76-837. [7] U. Banerjee. Speedup of ordinary \nprograms. PhD thesis, Dept. of Computer Science, University of Illinois at Urbana Champaign, October \n1979. Report No. 79-989. [8] U. Banerjee. Dependence Analysis for Supercomputing. Kluwer Academic Publishers, \nBoston, MA, 1988. [9] M. Bnrke and R. Cytron. Interprocedural dependence anal\u00adysis and parallelization. \nIn Proceedings of the SIGPLAN 86 Symposium on Compiler C onstmction, Palo Alto, CA, Jtdy 1986. [10] D. \nCallahan. Dependence testing in PFC: Weak separability. Supercomputer Software Newsletter 2, Dept. of \nComputer Science, Rice University, August 1986.  [11] D. Callahan, S. Cam, and K. Kennedy. Improving \nregis\u00adter allocation for subscripted variables. In Proceedings of the ACM SIGPLAN 90 Conference on ProgTam \nLanguage Design and Implementation, White Plains, NY, June 1990. [12] D. Callahan, K. Cooper, R. Hood, \nK. Kennedy, and L. Torc\u00adzon. ParaScope: A parallel progr amming environment. The International Joumtal \nof Supemomputer Applications, 2(4), Winter 1988. [13] D. Callahan, J. Dongarra, and D. Levine. Vectorizing \ncom\u00adpilers: A test suite and results. In Proceedings of Supevcom\u00adptiting 88, Orlando, FL, November 1988. \n [14] W. Cohagan. Vector optimization for the ASC. In Pro\u00adceedings of the Seventh Annual Princeton Conference \non Information Sciences and Systems, Princeton, NJ, March 1973. [15] S. Cook. The complexity of theorem-proving \nprocedures. In Proceedings of ThiTd Annual ACM Symposium on Thewy of Computing, New York, NY, 1971. [16] \nG. Cybenko, L. Kipp, L. Pointer, and D. Kuck. Super\u00adcomputer performance evaluation and the Perfect bench\u00admarks. \nIn Proceedings of the 1990 ACM Inte rnationa[ Con\u00adference on SupeTcompxting, Amsterdam, The Netherlands, \nJnne 1990. [17] G. Dantzig. Linear Programming and Extensions. Prince\u00adton University Press, Princeton, \nNJ, 1963. [18] P. Feautrier. Parametric integer programming. Opera\u00adtionnelle/Operations Research, 22(3):243 \n268, September 1988. [19] D. Gannon, W. Jalby, and K. Gallivan. Strategies for cache and local memory \nmanagement by global program transfor\u00admations. In Proceedings o.f ih e First Int emational Con~er\u00adence \non Supercomputing. Springer-Verlag, Athens, Greece, June 1987. [20] M. Girkar and C. Polycbronoponlos. \nCompiling issues for supercomputers. hl Proceedings of Supercomputing 988, (h lando, FL, November 1988. \n[21] T. Gross and P. Steenkiste. Structured datafiow analysis for arrays and its use in an optimizing \ncompiler. Software %actiee and Experience, 20(2):133 155, February 1990, [22] M. Haghighat and C. Polychronopoulos. \nSymbolic depen\u00addence analysis for high perfomnce parallelizing compil\u00aders. In Proceedings of the Third \nWoykshop on Languages and Compilers for Parallel Computing, Irvine, CA, August 1990. [23] R. Henft and \nW. Little. Improved time and parallel proces\u00ad sor bounds for Fortran-like loops. IEEE Transactions on \nComputers, C-31(1):78 81, January 1982. [24] N. K armarkar. A new polynomial-time algorithm for lin\u00adear \nprograrmrn rig. in Proceedings o.f the 16th Annual ACM Symposi%m on the TheoTy of Computing, 1984. [25] \nK. Kennedy. Automatic translation of Fortran programs to vector form. Technical Report 476-029-4, Dept. \nof Mathe\u00admatical Sciences, Rice University, October 1980. [26] K. Kennedy. Triangular Banerjee inequality. \nSupercom\u00adputer Software Newsletter 8, Dept. of Computer Science, Rice University, October 1986. [27] \nK. Kennedy, K. S. McKinley, and C. Tseng. Analysis and transformation in the ParaScope Editor. In Proceedings \nof the 1991 ACM International Conference on Supercomput\u00ading, Cologne, Germany, June 1991. [28] K. Kennedy, \nK. S. McKinley, and C. Tseng. Interactive parallel programming g using the ParaScope Editor. IEEE Transactions \non PaTallel and Distributed Systems, 2(3), July 1991. [29] D. Klappholz and X. Kong. Extending the Banerjee-Wolfe \ntest to handle execution conditions. Technical Report 9101, Dept. of EE/CS, Stevens Institute of Technology, \n1991. [30] D. Klappholz, K. Psarris, and X. Kong. On the perfect accuracy of an approximate e subscript \nanalysis test. In PTO \u00adceedings of the 1990 ACM International Conference on Su \u00adpeycomputing, Amsterdam, \nThe Netherlands, June 1990. [31] X. Kong, D. Klappholz, and K. Psarris. The I test: A new test for subscript \ndata dependence. In Proceedings oj the 1990 International Conference on payalle~ Processing, St. Charles, \nIL, August 1990. [32] D. Kuck. The Structure of Computem and Computations, Volume 1. John Wiley and Sons, \nNew York, NY, 1978. [33] D, Kuck, R. Kuhn, D. Padua, B. Leasnre, and M. J. Wolfe. Dependence graphs and \ncompiler optimization. In Confer\u00adence R.ecoTd of the Eighth ACM Symposium on the Prin\u00adciples oj Programming \nLanguages, Williamsburg, VA, Jan\u00aduary 1981. [34] D, Kuck, Y. Muraoka, and S. Chen. On the number of op\u00aderations \nsimultaneously executable in Fortran-like programs and their resulting speedup. IEEE Transactions on \nCom\u00adputers, C-21( 12):1293-131O, December 1972. [35] R. Kuhn. Optimization and Interconnection Complexity \nfor: PaTa!lel pTocessom, Single-Stage Networks, and De\u00adcision Trees. PhD thesis, Dept. of Computer Science, \nUN\u00adversit y of Illinois at Urbana-Champaign, February 1980. [36] L. Lamport. The parallel execution of \nDO loops. Commu\u00adnications of the A CM, 17(2):83 93, February 1974. [37] Z. Li and P. Yew. Some results \non exact data dependence analysis. In D. Geleruter, A, Nicolau, and D. Padua, editors, Languages and \nCompilers for PaTallel Computing. The MIT Press, 1990. [38] Z. Li, P. Yew, and C. Zhu. Data dependence \nanalysis on multi-dimensional array references. In Proceedings of the 1989 ACM International Con.feTence \non SupeTcomputing, Crete, Greece, June 1989. [39] A. Lichnewsky and F. Thomasset. Introducing symbolic \nproblem solving technique in the dependence testing phases of a vectorizer. In Proceedings of the Second \nInterna\u00adtional Conference on Supercomputing, St. Malo, France, Jdy 1988. [40] L. Lu and M. Chen. Subdomain \ndependence test for mas\u00adsive par~elism. h Proceedings of Supe rco reputing 9 O, New York, NY, November \n1990. [41] F. McMahon. The Livermore Fortran Kernels: A com\u00adputer test of the numerical performance range. \nTechnical Report UCRL-53745, Lawrence Livermore National Labo\u00adratory, 1986. [42] Y. Mnraoka. Parallelism \nEzposuTe and Exploitation in PTo\u00adgrams. PhD thesis, Dept. of Computer Science, University of Ilhnois \nat Urbana-Champaign, February 1971. Report No. 71-424. [43] A. Portertield. Software Methods foT Improvement \noj Cache ~erfo~mance. PhD thesis, Rice University, May 1989. [44] A. Schrijver. Theory of Linear and \nInteger Programming. John Wiley and Sons, Chlchester, Great Britain, 1986. [45] Z. Shen, Z. Li, and P. \nYew. An empirical study of Fortran programs for parallelizing compilers. IEEE Transactions on Parallel \nand Distributed Systems, 1 (3):356-364, July 1990. [46] R. Shostak. Deciding linear inequalities by computing \nloop residues. Journal of the A CM, 28(4):769 779, October 1981. [47] R. Triolet. Interprocedural analysis \nfor program restructur\u00ading with Parafrase. CSRD Rpt. No. 538, Dept. of Com\u00adputer Science, University \nof Illinois at Urbana-Champaign, December 1985. [48] R. Triolet, F. higoin, and P. Feautrier. Direct \nparallelization of CALL statements. In Proceedings of the SIGPLA N 86 Symposium on Compiler Construction, \nPalo Alto, CA, July 1986. [49] J. Uniejew.ki. SPEC Benchmark Suite: designed for today s advanced systems. \nSPEC Newsletter Volume 1, Issue 1, SPEC, Fall 1989. [50] D. Wallace. Dependence of multi-dimensional \narray refer\u00adences. in Proceedings of the Second International Confer\u00adence on Supe?computing, St. Male, \nFrance, July 1988. [51] M. E. Wolf and M. Lam. Maximizing parallelism via loop transformations. In Proceedings \nof the Third Workshop on Languages and Compi[em foT PaTal/e/ computing, Irvine, CA, August 1990. [52] \nM. J, Wolfe. Techniques for improving the inherent par\u00adallelism in programs. Master s thesis, Dept. of \nComputer Science, University of Illinois at Urbana-Champaign, July 1978. [53] M. J. Wolfe. Optimizing \nSupeTcompilers for SupeTcomput\u00aders. PhD thesis, Dept, of Computer Science, University of Illinois at \nUrbana-Champaign, October 1982. [54] M. J. Wolfe. Loop skewing: The wavefront method revisited. International \nJouvnal of Parallel Programming, 15:4:279\u00ad293, August 1986. [55] M. J. Wolfe. Optimizing Supercompilers \nfo~ Supevcomput\u00adeTs. The MIT Press, Cambridge, MA, 1989. [56] M. J. Wolfe and C. Tseng. The Power test \nfor data depen\u00addence. Technical Report CS/E 90-015, Dept. of Computer Science and Engineering, Oregon \nGraduate Institute, Au\u00adgust 1990. To appear in IEEE Transactions on Parallel and Distributed Systems. \n29  \n\t\t\t", "proc_id": "113445", "abstract": "", "authors": [{"name": "Gina Goff", "author_profile_id": "81339501905", "affiliation": "Department of Computer Science, Rice University, Houston, TX", "person_id": "P97766", "email_address": "", "orcid_id": ""}, {"name": "Ken Kennedy", "author_profile_id": "81100453545", "affiliation": "Department of Computer Science, Rice University, Houston, TX", "person_id": "PP40027435", "email_address": "", "orcid_id": ""}, {"name": "Chau-Wen Tseng", "author_profile_id": "81410592010", "affiliation": "Department of Computer Science, Rice University, Houston, TX", "person_id": "PP77028537", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/113445.113448", "year": "1991", "article_id": "113448", "conference": "PLDI", "title": "Practical dependence testing", "url": "http://dl.acm.org/citation.cfm?id=113448"}