{"article_publication_date": "05-01-1991", "fulltext": "\n Automatic Generation of Global Optimizers* Deborah Whitfield and Mary Lou Soffa Department of Computer \nScience University of Pittsburgh Pittsburgh, PA 15260 ABSTRACT useful when experimenting with optimizations \nand when This research has developed an optimizer generator that automatically produces optimizers from \nspecifications. Code optimizations are expressed using a specification language designed for both traditional \nand parallelizing optimization, which require global dependence condi\u00adtions. Numerous optimizers have \nbeen produced from a prototype implementation of the generator. The quality of code produced using the \ngenerated optimizers compares favorably with that produced by hand coded optimizers. The generator can \nbe used as a phase in a compiler or as an experimental tool to determine the effects of various optimization \nand to tailor optimization. Experiments indicate that optimization interact in practice and that dif\u00adferent \norderings of optimization are needed for different code segments of the same program. Experiments found \nthat the cost-benefit ratio of some optimizations is quite large and in some cases can be reduced by \ncareful specifications of the optimiza tions or different implemen\u00ad tations. 1. Introduction Although \ntraditional global optimizations have long been applied to program code, the cost of implementing these \noptimizers remains high. We have reduced the cost of implementing optimizers by providing a General Optimization \nSpecification Language, (GOSpeL) and an optimizer generator (GENesis) that allows users to gen\u00aderate \na wide variety of global optimizers from compact, declarative specifications. The optimizers thus produced \nare useful in conventional compilers but are particularly * This work was partialty supported by tfre \nNational Science Foun\u00addation under Grant CCR-8801 104 to the University of Pittsburgh. Permission to \ncopy without fee all or part of this material is granted provided that the copies are not made or distributed \nfor direct commercial advantage, the ACM copyright notice and the title of the publication and its date \nappear, and notice IS given that copying is by permission of the Association for Computing Machinery. \nTo copy otherwise, or to republish, requires a fee andlor specific permission. @ 1991 ACM 0-89791 -428 \n-7/91 /0005 /0120 . ..$1.50 Proceedings of the ACM SIGPLAN 91 Conference on Programming Language Design \nand Implementation Toronto, Ontario, Canada, June 26-28, 1991. compiling for parallel machines, where \nit maybe unclear which transformations to use and how to order them. To use GENesis, the optimization \nunder considera\u00adtion for application on program code are expressed in GOSpeL by the user. For each optimization, \nGOSpeL requires the specification of the preconditions and the actions to optimize the code. In addition, \nthe interactive capability that the user desires with the optimizer can be specified. The preconditions \nconsist of the code patterns and the global dependence information needed for the optimization. The code \npatterns express the format of the code, and the global information describes the control and data dependence \nthat are required for the specified optimization. The actions take the form of primitive operations that \noccur when applying code transforma\u00adtions. The primitives are combined to express the total effect of \napplying a specific optimization. For generality, GENesis assumes a high level intermediate representation \nthat retains the loop structures from the source program. The user can specify whether the optimization \nshould be applied automatically (e.g., traditional optimization) or should be applied at the user s direction \n(e.g., paralleliz\u00ading transformations). The higher level intermediate code allows the user to interact \nat the source level for loop transformations, typically applied for parallel systems. With an optimizer \ngenerated by GENesis, the user can experimentally investigate the performance of the optimization on \nprogram code for the system under con\u00adsideration, The cost and expected benefit of various optimizations \ncan be compared on production code. Optimizations that are not effective can be removed and other optimization \ncan be added by simply changing the specifications and rerunning GENesis, producing a new optimizer. \nThe decision as to the order in which these optimization should be applied can be easily investi\u00adgated. \nNew optimization can be created or existing optimization tailored to the system and easily incor\u00adporated \ninto an optimizer. ~ Currently, we have used GOSpeL to specify approx\u00adimately twenty optimizations found \nin the literature and have been successful in specifying all optimization attempted. In order to test \nthe viability and robustness of this approach, we implemented a prototype for GENesis and have produced \na number of optimizers. Using these optimizers, the impact of ordering optimization as well as the cost \nand benefit of the optimizations have been inves\u00adtigated experimentally. In the next section, examples \nof the specifications of optimization in GOSpeL are given as well as an overview of the language. Section \n3 describes the design and implementation of GENesis. Experimental results are presented in Section 4. \nThe paper concludes with a section on related work. 2. Description of GOSpeL GOSpeL permits the uniform \nspecification of both traditional, sequential optimizations and parallelizing transformations by using \ncommon constructs. Control, anti, flow and output data dependence are used in this work, as these dependence \nare needed to specify parallel\u00adizing optimizations and can also be used to express tradi\u00adtional data \nflow for sequential optimizations. Thus the use of a common data dependence notation in the specification \nis a step toward unification of parallelizing and traditional optimizations. A flow dependence (Si 8 \nSj) exists between a state\u00adment Si that defines a variable and each statement Sithat uses the definition \nfrom S&#38; An anti-dependence (Si 8 Sj) occurs between statement Si that uses a variable that is then \ndefined in statement Sj. An output dependence (Si 8\u00b0 Sj) exists between a statement Si that defines or \nwrites a variable that is later defined or written by Sj. A control dependence (S, 8 bj) exists between \na control statement Si and all of the statements Sj under its control. In other words, if Si is an IF \ncondition then all of the statements within the THEN and the ELSE are control dependent on Si. The concept \nof data direction vectors for both forward and backward loop-carried dependence of array elements is \nalso used in this work. When determining the data dependence within loops, it is necessary to examine \nthe direction of the dependencel . Each element of the data dependence vector consists of either a forward, \nbackward, or equivalent direction represented with <, >, or=, respec\u00adtively. An * is used when any of \nthe three directions can apply (omitting the direction vector produces the same result). The number of \nelements in the direction vector corresponds to the loop nesting level of the statements involved in \nthe dependence, We first present examples of specifications and then highlight the features of the language \nby considering the syntax and semantics of the components of an optimiza\u00adtion specification. A BNF grammar \nwas developed for GOSpeL, and a subset appears in the Appendix of this paper. The grammar is used to \nconstruct well-formed specifications and also to implement an optimizer through GENesis. The specification \nof an optimization has three sec\u00adtions: type, precondition and action sections, where the precondition \nis sub-divided into two parts. The type sec\u00adtion specifies the required code element types. The precondition \nsection includes both the code format specification and the dependence that are needed. The data dependence \nspecification involves the description of statement and operand dependence, direction vectors, and any \nnecessary membership qualification. The action section specifies the primitive actions that perform the \ncode transformations. The general independence of the specification from the underlying implementation \nallows for the implementation of GOSpeL at any level, including source level. However, the general form \nthat a statement may take is needed to delimit statement components. The assumed representation of an \nassignment state\u00adment for this implementation of GOSpeL is: opr_l := opr_2 opc opr_3 The number of operands \ncould be modified to reflect other program representations. Figure 1 demonstrates a GOS\u00adpeL specification \nof Constant Propagation (CTP). The Code_Pattem keyword in the PRECOND sec\u00adtion of Figure 1 specifies \nan occurrence of a statement that assigns a constant, represented by the second operand. The data dependence \nconditions given in the Depend sec\u00adtion of PRECOND expresses the conditions that must exist before applying \nthe optimization. For CTP, the con\u00addition is to locate a use of the defining variable of Si (i.e., opr_ \nl) at statement Sj (in operand position pos) and ensure that there are no other definitions that reach \nthe use. If such a statement is found then the actions expressed in the action section are performed. \nThe action is to modify the use at Sj to be the constant found as the second operand of Si. Modify is \njust one of the primitive actions allowed in GOSpeL. Next consider the specification of the parallelizing \noptimization Loop Interchanging (INX) found in Figure 2. In the Code_Pattem section, any specifies an \noccurrence of two tightly nested loops L1 and L2. Two loops are tightly nested if one surrounds the other \nwithout any state\u00adments between them14. The data dependence condition in the Depend section expresses \ntwo conditions. First, it ensures that the loop headers are invariant with respect to each other by checking \nfor a flow dependence. Also, the Depend section expresses that there are no pairs of state\u00adments in the \nloop with a flow dependence and a (<,>) direction vector. If no such statements are found then the Heads \nand Ends of the two loops are interchanged. TYPE SUnr Si, Sj, S1; PRECOND Code_Pattem I* Find a constant \ndefinition / any Si: Si.opc == assign AND type(Si.opr_2) == cons~ Depend ~ Use of Si whh no other definitions \n*/ any (Sj,pos): flow_dep(Si, Sj,(=)); no (Sl,pos): flow_dep(Sl, Sj,(=)) AND (Si != S1) AND operand(Sj,pos) \n!= operand(Sl,pos); ACTION P Change use of Si in Sj to be constant*/ modify (operand (Sj,pos), Si.opr_2); \n Figure 1. GOSpeL specification of Constant Propagation TYPE Stmc Sn, Sm; Tight Loops: &#38;l, L2); \nPRECOND Code_Pattem / Find two nested loops *J any(Ll, L2); Depend /* Ensure invariant loop headers*/ \n~ No flow_dep statement and direction of (<,>) */ no L1.head flow_dep(L1.head, L2.head) no Sm, Sn: mem(Sm, \nL2) AND mem(Sn, L2), flow_dep(Sn, Sm, (e>)); ACTION p Interchange heads and tails*/ move(Ll .Head, L2.Head); \nmove(Ll .End, L2.End.prev); Figtue 2. GOSpeL specification of Loop Interchange Statement types have \npre-defined attributes indicating the 2.1. Declaration Section first, second and third operand and the \noperation. The pre-defined attributes of type loop include the loop body, Variables are defined to be \none of the following which identifies all the statements in the loop, loop controltypes: Statement, Loop, \nNested Loops, Tight Loops, or variable, initial value, final value, head of the loop, andAdjacent Loops. \nThe domain of these types are the par\u00adend of the loop. Tight Loops restrict Nested Loops byticular code \nelements in the intermediate code representa\u00adensuring that there are no statements between the twotion. \nA variable of type Statement can have as its value loops. Variables receive their values as a result \nof various any of the intermediate code statements in the program. operations performed by operators \nsuch as any, all, and All types have pre-defined attributes denoting the next or previous code element \nof that type. no. In the declaration section, variables are defined fol\u00adlowing the keyword TYPE using \nthe forma~ type: id_list; The id_list for Statement and Loop is simply a list, but Nested Loops, Tight \nLoops, and Adjacent Loops require parenthesized pairs of identifiers. 2.2. Pre-condition Section In order \nto specify a code transformation and condi\u00adtions under which it can be applied safely, the pattern of \ncode (e.g., constant operands) and the data and control dependence conditions (if any) that are needed \nmust be given. These two similar components constitute the precondition section of a specification. The \nkeyword PRECOND is followed by the keyword Code Pattern, which precedes the code pattern specificat~ons \nand Depend which precedes the dependence specification, forcing the user to order the code pattern specifications \nprior to the dependence specifications. This ordering is enforced for ease of converting the specifications \nto exe\u00adcutable code. The code pattern section specifies the format needed for the statements and loops \ninvolved in the optim\u00adization, The code pattern specification consists of a quantifier followed by the \nelements needed and the required format of the elements. quantifier element_list: offormat elements; \nThe quantifier operators any and all return an element or all the elements, respectively, of the requested \ntypes if a match is successful. The no operator returns null and warns the user that no statement has \nbeen specified for pattern matching. The second part of the code pattern specification describes the \nformat of the type of elements required. If Statement is the element type, then the for\u00admat typically \nrestricts the statement s operands and opera\u00adtor, Thus, if constants are required as operands or if loops \nare required to start at iteration 1, this requirement is specified in the forma_of elements. Expressions \ncan be constructed in format_of ~lements using the AND and OR conjunctive with the= usual meaning. The \nsecond component of the precondition section is the specification of data or control dependence that \nare required. The dependence are specified using the names of the code elements listed in element list. \nThe depen\u00addence specification consists of express~ons that return a boolean value and the set of elements \nthat meet the condi\u00adtions. The general form of the dependence specification is element_quantifier element: \nsets. of elements, dependence_conditions; The description of the sets of_elements is specified before \nthe dependence condit~ns only for ease in automatically converting the specifications to executable code. \nThe element quantifier may be any, no, or all. Any specifies a condition for one statement, all specifies \nall statements that satisfy the condition, and no specifies that no statement exists with the condition. \nThe dependence condition returns two objects: the collected set and a truth value. Optionally, the user \nmay request the position of the dependence within the statement (Sj,pos) to also be returned for each \nelement in the collected set, as shown in Figure 1, The sefs of elemenrs component permits the author \nof the specific~tio~ to define set membership of elements. The mem(Element, Set) operation specifies \nthat Element is a member of the defined Set, Set may be described using predefine sets, the name of a \nspecific set, or an expression involving set operations and set functions. An example of a predefine \nset is path (ID, ID ) that has as its values the set of statements along a path designated by ID and \nID . The sets_of_elements items maybe sepamted by an AND or OR operator. The dependence_conditions describe \nthe data and control dependence of the code elements and take the form: type_of_dependence(StmtId, StmtId, \nDirection); The dependence type can be either flow dependent (flow_dep), anti-dependent (anti_dep), output \ndependent (out--dep), or control dependent (ctrl_dep). Direction is a description of the direction vector, \nwhere each element of the vector consists of either a forward, backward or equivalent direction (represented \nwith c, >, =, respec\u00adtively), or any which allows any of the three directions. This representation is \nneeded to specify loop-carried dependence of array elements for parallelizing transfor\u00admations. This \ndirection vector may be omitted if loop\u00adcarried dependence are not relevant. Semantically, the dependence \nspecification has two roles. First of all, variables are assigned vaIues using any, all, and no. The \ndependence conditions place further res\u00adtrictions on the components of the computed sets. Secondly, the \nlist of dependence specifications are evaluated for a truth value. If all dependence specifications and \ncode pattern specifications are true, then the precondition evaluates to true. As an example, the following \nspecification is for one element named Si that is an element of Loop 1 such that there is a Sj, an element \nof Loop 2, and there is either a flow dependence or an anti-dependence between Si and Sj. Depend any \nSi: mem(Si, Ll) AND mem(Sj, L2), flow_dep(Si, Sj,(=)) OR anti_dep(Si, Sj, (=)); 2.3. Action Section \nThe actions of applying transformations can be decomposed into a sequence of the following five prim\u00aditive \noperations. The semantics of each are indicated below. These operations are overloaded in that they can \napply to different types of code elements. In the follow\u00ading descriptions, a, b and c refer to any type \nof code ele\u00adment. The five actions are Delete (a): delete a, Copy (a, b, c): copy a, place it following \nb, and name it c. Move (a, b): remove a from its original position and place it following b. Add(a, Element_description, \nb): add an element described by Element_description, place it following a, and call it b. Modify (Operand(S,i), \nNew_operand): modify Operand i of statement S to be New_operand, These actions are combined to fully \ndescribe the optimization. It may be necessary to repeat some actions for all statements found in the \nprecondition. Hence, a list of actions may be preceded by forall and an expression describing the elements \nto which the actions should be applied. The flow of control in a specification is implicit with the exception \nof the forall construct available in the action section. In other words, the ACTION keyword acts as a \nguard that does not permit entrance into this sec\u00adtion unless all conditions have been met. The initial \ndesign of the GOSpeL language was fine-tuned by having other researchers, some very familiar with optimizations \nand some not, write optimizations in GOSpeL. These users were able to specify known optimi\u00adzation without \nany help. One of the changes suggested by these users was to change an original quantifier one to any \nfor ease of understanding. 3. Description of GENesis The GENesis tool analyzes a GOSpeL specification \nand generates C code to perform the appropriate pattern matching, check for the required data dependence, \nand call the necessary primitive routines to apply the optimi\u00adzation. Figure 3 presents a pictorial view \nof GENesis and its use. A source program that is to be optimized is con\u00adverted to an intermediate representation \n(usually as part of the compilation process) and data dependence are com\u00adputed. The intermediate code \nand the data dependence are input into the generated optimizer (OPT), and optim\u00adized intermediate code \nis produced.  pEm6~z OPTIMIZER (OPT) -r n ICI Source Code\\ q&#38;; user, Pz!zGl c) options Figure \n3. Overview of GENesis There are three parts to GENesis: a generator, a library, and a cons&#38;uctor. \nThe generator produces code for the specified optimization, which utilize the pre\u00addefine routines in \nthe optimizer library. The constructor packages all of the produced code and the library routines within \nan interface, which prompts interaction with the user. GENesis analyzes the GOSpeL specifications using \nLEX and YACC, producing the data structures and code for each of the three sections of a GOSpeL specification. \nThe generator producer first establishes the data structures for the code elements in the specifications. \nCode is then generated to find elements of the required format in the intermediate code. Code to verify \nthe required data dependence is next generated. Finally, code is generated for the action statements. \nThe algorithm used in GENesis is given in Figure 4. Step 1: Input the GOSpeL specifications to GENesis \nStep 2: Analyze the GOSWL specifications using LEX and YACC and generate code to a. setup the data structures \ndefined in the type section b. search for the patterns specified in code_pattem section -call the necessary \npattern matching routines to find the specified types (e.g., find_nested-loops, find_statement) c. check \ndata dependence for those elements specified in the precondition section of Depend d. perform actions \nby calling pre-defined library rou\u00adtines for primitive actions  Step 3: Construct the optimizer by: \na. Packaging the produced code for all optimization and library routines b, Creating the interface from \na template to: i. Read the source code ii. Convert source to intermediate representation iii. Allow \nintemction with the user 1. Select optimization(s) to perform 2. Select application points 3. Override \ndependence restrictions  iv. Compute the data dependence v, Perform the optimization at user s request \nvi. Return to iii. until user quits session Figure 4. The GENesis Algorithm The generated code relies \non a set of predefine routines found in the optimizer library. These routines are optimization independent \nand only represent routines typi\u00adcally needed to perform optimization. The library con\u00adtains pattern \nmatching routines, data dependence verification procedures, and code transformation routines. The pattern \nmatching routines search for loops and state\u00adments. Once a possible pattern is found, the generated code \nis called for verification of such items as operands, opcodes, initiat and finat values of loop control \nvariables. When a possible application point is found in the intermediate code, the data dependence must \nbe verified. Data dependence verification may include a check for the non-existence of a particular data \ndependence, a search for all dependence, or a search for one dependence within a loop or set. The generated \ncode may simply be an if to ensure a dependence does not exist or may be a more complex conglomeration \nof tests and loops. For example, if all statements dependent on Sit need to be examined, then code is \ngenerated to collect the statements. The required direction vectors associated with each dependence in \nthe specification are matched against the direction vectors of the dependence that exist in the source \nprogram. If the dependence are verified then the action is executed. Routines consisting of the actions \nspecified in the ACTION section of the specification are generated for the appropriate code elements. \nThe constructor compiles the optimizer library and the generated code to produce the optimizer (OPT). \nThe constructor also generates an interface to execute the vari\u00adous optimization. The interface to an \noptimizer reads the source code, generates the intermediate code and com\u00adputes the data dependence. The \ninterface also queries the user for interactive options. This interactive capability permits the user \nto execute any number of optimization in any order. The user may elect to perform an optimiza\u00adtion at \none application point (possibly overriding depen\u00addence constraints) or at all possible points in the \nprogram. The interface permits the user to decide if the data depen\u00addence should be re-calculated between \nexecution of each optimization. 3.1. Prototype Implementation In order to test the robustness of the \nGENesis sys\u00adtem, a prototype implementation was developed. The pro\u00adtotype was used to generate optimizers \nfor a number of optimizations. For any optimization specified (e.g., xxx), the generator produces four \nprocedures: set_up_xxx, match_xxx, pre_xxx, and act_xxx. These procedures correspond to the sections \nin the specifications. In our implementation, an optimizer consists of a driver that calls the routines \nthat have been generated specifically for that optimizer. The format of the driver is the same for any \noptimizer generated. The driver calls procedures in the generated call interface for the specific optimization. \nThe call interface in turn calls the generated procedures that implement the optimization. The standard \ndriver is given in Figure 5 using pseudocode. Notice that the driver calls four procedures (set_up_OPT, \nmatch_OPT, pre_OPT, and act_OP7) that are found in the call interface for the specific optimization. \nThe call interface code simply calls the generated optimization specific code. In other words for CTP, \nthe set_up_OPT procedure consists of a single call to set_up_CTP. The driver requires a successful pattern \nmatch from match_CTP and pre_CTP in order to continue. Thus, the match OPT and pre OPT of the call interface \nprocedures return; boolean val~e. The C code that was actually gen\u00aderated to implement the pattern matching, \ndependence checking, and actions for CTP is given in Figure 6. Driver Call set_up_OPT to initialize \nstlp structure. pat_suc:= True WHILE (pat_suc AND NOT Done) DO pat_suc := match_elements(stlp) IF @at_suc) \nTHEN DO match_suc:= match_OPT IF (match.sue) THEN DO pre_suc:= pre_OPT IF (pre.sue) THEN DO act_opt \nDone := True; ENDIF ENDLF ENDIF ENDWHILE END Figure 5. The Driver Algorithm The generated set_up procedure \nconsists of code that initializes data structures for each element specified using any or all in the \nPRECOND section. The stip data structure contains identifying information about each statement or loop \nvariable specified in the TYPE section. For type Statement, an entry is initialized with the type and \ncorresponding identifier. If a loop type variable is specified, additional flags for nested or adjacent \nloops are set in the stlp entry. These entries are filled in as the information relevant to the element \nis found. For the CTP example, an stlp entry is initialized to type Statement and identifier Sit when \nthe optimizer executes procedure set_up_CTP, for a search for this statement is required ini\u00adtially. \nAfter the set_up_OPT procedure terminates, the driver initiates the search for the statement recorded \nin the stlp table by calling match_OPT. In the example, the driver would call match_CTP. In the match_CTP \nprocedure, the pattern matching routine jind searches the intermediate code for a quad (statement Si \n). that has opcode of ASSGN and a constant operand. If the source program s statement does not match, \nthen the optimizer driver re-starts the search for a new statement. set_up_CTP() { F setup stlp for \none elemenfistatement, Si / stlp[l] kind= Statemeng strcpy (stlp[l].desc.stmt.id, Si } num_elems = 1; \nreturn(l); 1 match_CTP() { if (quad[tind( Si ,O)o pcpkindnd != ASSGN) return(0); /* if quad s opcode \nisn t ASSGN, fail*/ if (quad[find( Si ,O)] .opra.kind != const) return(o); /* if quad s operand_a isn \nt constant, fail*/ return(l); /* match was successful for Si */ ) pre_CTP() { ins_stmt(-1, Sj ); /* \ninsem Statement, Sj */ /*If flow dependent Sj exists, assign its quad number / if((stlp[num_elems] .desc.stmt.stmt_num \n= dep(LSTILOW,find( S i ,O),O,O,lZQ))); else return(-l); ins_stmt(-1, N ); /* insert Statement, S1 \n*/ /*If suitable S1 exists, assign its quad number*/ while((stlp[num_elems] .desc.stmt.stmt_num = dep(LSTJ?LOW,O,find( \nSj ,O),O,l J3Q))) P compare quad_numbers and operand involved in dependence *I if (find( Si ,O) !=tindSl \n,0) ,O) &#38;&#38; dep_opr(find( Sj ,O))==dep_opr(findSl Sl ,0))) return(-l);  return(l); ) act_CTP() \n{ p modify one of quad Sj s operands */ /* repl compares AND replaces operand*/ /* involved in dependence \nif it matches */ modify(&#38;quad[find( Sj ,O)] .opra, repl(&#38;quad[find('' Si'',O)].oprc,&#38;quad[find(''Sj'',O)] \n.opra, &#38;quad[find( Sit ,O)] ,opra,&#38;quad[find( Sj ,0)].opra),-l); modify (&#38;quad[find( Sj ,O)] \n,oprb, repl(&#38;quad[find( Si ,O)] .oprc,&#38;quad[find( Sj ,O)oprb,b, &#38;quad[find( Si ,O)] .opra,&#38;quad[find( \nSj ,0)].oprb),-l); return(l); 1 ) Figure 6. The Generated Code for CTP procedure dep, given in Figure \n7, is called to find the first Function dep; InpuC 1, TYPE of search -LST or IF 2. KIND of dependence \n(anti, flow, output, ctrl) 3. Statements involved  TYPE == IF both starting and terminating state\u00adments \nof dependence TYPE == LST either the starting or terminating statement 4. FLG signaling the number of \ndep call 5. NUMber of elements in direction vector 6. DIRection vector to be matched  Outplm O -no \ndependence found 1-dependence for IF found value -statement number of dependence Si = emanating quad \nnumber Sj = terminating quad number if (TYPE = IF) then begin if (dependence from Si to Sj = KIND) and \n(DIR matches) retum( 1); else return(0); else begin if (Si known) then begin Sj = first terminating statement \nwith ( dependence= KIND) and (DIR matches); save[flg] = Sj; retum(Sj); else begin Si = first emanating \nstatement with (dependence = KIND) and (DIR matches); save[flg] = St retum(Si); endifi endifi end dep. \nFigure 7, The dep algorithm The next routine called is pre_OPT to check for data dependence. For CTP, \nthe pre_CTP prccedure inserts an element into the stlp structure for each depen\u00addence condition statement. \nSj is inserted into stlp and the statement that is flow dependent on Si. If one is not found then the \ncondition fails. S1 is inserted and the procedure dep is called again. Each S1 such that S1 is flow dependent \non Sj is examined to determine if the operand of S1 caus\u00ading the dependence is the same variable involved \nin the dependence from Si to Sj. If such an S1 is found then the condition fails. The last procedure \nto be called is act_OPT, which translates to act_CTP for CTP. Procedure act_CTP sim\u00adply modifies the \noperand collected in Sj. The call to repl compares the first and second parameters. Thus, the first call \nto modz~y considers operand a of Sj for replacement and the second call considers operand b for replace\u00adment, \neffectively implementing the pattern matching needed for determining the operand position of a depen\u00addence. \nact_CTP is called by the driver only if match_CTP and pre_CTP have terminated successfully. There are \nthree modules of C code involved in the generation of an optimizer by GENesis: the generator, the generated \ncode for an optimizer, and the non-optimization specific library. The generator consists of 1,735 lines \nof code (including LEX and YACC specifications). An optimization consists of 99 lines on the average, \nwhere the call interface consists of 29 lines of code, and the four generated procedures consist of 70 \nlines on the average. The non-optimization specific code in library is 1,873 lines. These lines of code \ndo not include the routines needed to convert the source to intermediate code or the data flow routines. \nThe existing GENesis prototype can be expanded in various aspects to permit user flexibility. Such implemen\u00adtation \nexpansions include a gmphical user interface to guide the user in the application of transformations. \nThe current implementation only permits the user to provide a suggested application point by inputing \nthe intermediate code location. Not all of the features in GOSpeL have been implemented in the prototyp~ \nhowever, the imple\u00admentation of these features would not pose any problems. Example restrictions include \na step by one in loop incre\u00adments and no expressions are included as code elements in the fomll construct \nof the ACTION section. Because of our interests, the optimizers currently implement only one optimization. \nFor a sequence of optimization to be applied to program code, the various optimizers are called in the \ndesired sequence. However, it is fairly easy to change the implementation to have the driver sequence \nthrough a number of optimization. The data flow analyzer may have to be called after each appli\u00adcation. \n 4. Experimentation was not disabled. Thus, users should be aware that apply- We are performing experiments \nusing optimizers produced by GENesis to determine the application cost and quality of code produced by \nthe optimizers and pro\u00adperties of optimizations. In this section, we discuss some of the results obtained \nso far. In all, optimizers were pro\u00adduced for ten optimizations including both traditional and parallelizing \noptimizations. The optimization are Copy Propagation (CPP), Constant Propagation (CTP), Dead Code Elimination \n(DCE), Invariant Code Motion (ICM), Loop Interchanging (INX), Loop Circulation (CRC), Bumping (BMP), \nParallelization (PAR), Loop Unrolling (LUR), and Loop Fusion (FUS). Experimentation was per\u00adformed using \nprograms found in the HOMPACK test suite and in a numerical analysis test suite3. HOMPACK con\u00adsists of \nFORTRAN programs to solve non-linear equa\u00adtions by the homotopy method. The numerical analysis test suite \nincluded programs such as the Fast Fourier Transform and solving non-linear equations using Newton s \nmethod. A total of ten programs were used in the experimentation. We first compared the quality of code \nproduced by our optimizers with that produced by hand-crafted optim\u00adizers. Our optimizers found the same \napplication points and the resulting code was comparable to that produced by the hand-crafted optimizers. \nThere were no extraneous statements, and the optimizations were correctly per\u00adformed. In the test programs, \nCTP was the most frequently applicable optimization (often enabled) while no applica\u00adtion points for \nICM were found. It should be noted that the intermediate code did not include address calculations for \narray accesses, which may introduce opportunities for ICM. CTP was also found to create opportunities \nto apply a number of other optimizations, which is to be expected. Of the total 97 application points \nfor CTP, 13 of these enabled DCE, 5 enabled CFO and 41 enabled LUR (assuming that constant bounds are \nneeded to unroll the loop). CPP occurred in only two programs and did not create opportunities for further \noptimization. To investigate the ordering of optimizations, we considered the optimizations FUS, INX \nand LUR which have been found to theoretically enable and disable one another 13, In one program, FUS, \nINX, and LUR were all applicable and heavily interacted with one another by creating and destroying opportunities \nfor further optimiza\u00adtion, For example, applying FUS disabled INX and applying LUR disabled FUS. Different \norderings pro\u00adduced different optimized programs. The optimizations also interacted when all three optimizations \nwere applied; when applying only FUS and INX, one instance of FUS in the program destroyed an opportunity \nto apply INX. How\u00adever, when LUR was applied before FUS and INX, INX ing an optimization at some point \nin the program may prevent another optimization from being applicable. To further complicate the prmess \nof determining the most beneficial ordering, different parts of the program responded differently to \nthe orderings. In one segment of the program INX disabled FUS, while in another segment INX enabled FUS. \nThus, there is not a right order of application. The context of the application point is needed. Using \nthe theoretical results of interactions from the formal specifications of optimizations13 as a guide, \nthe user may need multiple passes to discover the series of optimizations that would be most fruitful \nfor a given sys\u00adtem. Another set of experiments evaluated the cost and benefit of applying optimizations. \nThe cost of applying an optimization was estimated using the number of checks to determine preconditions \nand the number of operations to apply the code transformation. As an optimization was actually applied, \nthis value was computed by using code that GENesis produced. These cost values were validated by running \nthe optimizers and timing their execution. We found that the estimated times very closely reflect the \nactuaI times. The expected benefit of applying an optimi\u00adzation was computed by estimating the impact \nthe optimi\u00adzation has on execution time, taking into account code that was parallelized and code that \nwas eliminated. Dif\u00adferent architectural characteristics were considered, including vectorization and \nmulti-processing. These costs can be used in a number of ways. The costs can be used in determining whether \nan optimization should be included in a production optimizer. As an example, INX was found to be a relatively \ninexpensive operation with large benefits. CTP is inexpensive to apply, and it also enables many parallelizing \noptimizations. FUS was found to apply in only one test case and is a fairly expensive optimization to \napply with little expected benefit unless various types of memory hierarchies are part of the paral\u00adlel \nsystem, We have yet to experiment with this type of architectural consideration. If the cost of an optimization \nis very high, then alternative methods of specification should be attempted. In applying the optimizations, \nit was found that different specifications will produce different implementations of the optimization, \nwhich have an impact on the cost. For example, if the specification of LUR requires that both the upper \nand lower limits are constant, LUR is less costly to apply if the upper limit is checked before the lower \nbound. Our experimentation showed that it is more likely for the upper limit to be variable than the \nlower limit, thus discarding a non-application point earlier. The costs were also used to determine a \nbetter way to implement optimization, A number of optimizations involved the determination of membership \nwhen checking for preconditions. Two straightforward ways of imple\u00admenting the checking are (1) to determine \nstatements that are members and then check for the desired dependence, and (2) to consider the dependence \nof one statement and check the corresponding dependent statements for membership. We found that the cost \nof implementing the optimizations using these approaches varies tremendously and is not consistently \nbetter for one method over the other. Using heuristics, GENesis was changed to select the least expensive \nmethod on a case by case basis. In the tests performed, we found that the heuristic correctly selected \nthe best implementation. 5. Related Work Although numerous optimizing compilers and optimization systems, \nsuch as Parafrase-212, ParaScope2, and PTRANl have been designed and developed, this research focuses \non the automatic generation of optimiza\u00adtion systems. Techniques for the automatic generation of various \npeephole optimizers have been reported4-6 8. These optimizers apply localized optimizations found by \npattern matching on assembly or machine-level code. They have no facilities for handling global information, \nwhich is needed to perform the optimizations of interest in this research. GENesis works at a higher \nlevel program representation and can handle various types of program code structures. It also incorporates \nglobal information in the form of data and control dependence. However, it should be noted that GENesis \ncould also be used to pro\u00adduce peephole optimizers. A recently developed technique presents a language \nfor specifying optimizations on assembly language and an implementation of the language in Prolog7, There \nare no provisions for incorporating global information, although some simple data flow analysis is performed. \nThe machine-level nature of the implementation does not allow for easy recognition of array structures \nor source\u00adlevel constructs such as tightly nested loops. There are other optimization systems with goals \nthat differ from those in this research, which is to optimize program code. Some of these optimization \nsystems attempt to optimize high-level specifications into more concrete specifications using various \noptimizations and the user s help. Partsch and Steinbruggenl 1 give an excellent overview of these general \noptimization systems. The need for an optimization system permitting the specification of a sequence \nof transformations, and the automatic generation of the transformations, has long been recognized. Such \na system enables a user to create and easily implement novel optimizations which may be of particular \nbenefit to the system in hand. GENesis is such an optimization system. It permits the designer to create \nand specify the optimizations and control the application order. In addition, GENesis uses a well defined \nspecification technique for specifying optimiza\u00adtion. Acknowledgement The authors thank Christopher Fraser \nfor his many helpful comments and suggestions on this work. APPENDIX --BNF for Depend Section Pre = quant \nstmtlst: Elemlst condlst; quant a ANY INO I ALL Elemlst = mem ( ID, setexp ) Elmore I e Elmore -, I AND \nElemlst 10R Elemlst setexp a stxp [ comp ( stxp, setexp ) stxp -ID IPATH(ID, ID ) mem =$ MEM I NMEM comp \na INTER I UNION condlst = clist I NOT ( clist ) clist + term I condlst OR term term + conds I term AND \nconds conds + type ( Stmt.Id, StmtId direct) I ( condlst ) I ( StmtId relop StmtId ) I op_fn relop op_fn \nop_fn + operand ( StmtId ) type = FLOW 10UT I CTRL I ANTI direct -, ( sub more) I e sub + relop IANY \nI&#38; more + ,sub more IE relop-< l>l=lc=l>=l!= l== StmtId a ID cent I (ID, POS ) cent = . NXT cent \n1. PREV cent 1. HEAD cent I.END cent I.LABEL I.FINAL I.INIT I, BODY I, LCVIC References 1. F. E. Allen, \nM. Burke, R. Cytron, J. Ferrante, W. Hseh, and V. Sarkar, A Framework for Determin\u00ading Useful Parallelism, \nProceedings of (he 1988 International Conference on Supercomputing, pp. 207-215, St. Malo, France, February, \n1988. 2. Vasanth Balasundaram, Ken Kennedy, Ulrich Kre\u00admer, Kathryn McKinley, and Jaspal Subhlok, The \nParaScope Editoc An Interactive Parallel Program\u00adming Tool, Proceedings of Supercomputing 89, pp. 540-549, \nReno, Nevada. 3. Richard Burden and J. Douglas Faires, in Numerical  Analysis, Prindle, Weber &#38; \nSchmidt, Boston, MA, 1989.  \n\t\t\t", "proc_id": "113445", "abstract": "", "authors": [{"name": "Deborah Whitfield", "author_profile_id": "81100323561", "affiliation": "Department of Computer Science, University of Pittsburgh, Pittsburgh, PA", "person_id": "P65117", "email_address": "", "orcid_id": ""}, {"name": "Mary Lou Soffa", "author_profile_id": "81452611636", "affiliation": "Department of Computer Science, University of Pittsburgh, Pittsburgh, PA", "person_id": "PP39032771", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/113445.113456", "year": "1991", "article_id": "113456", "conference": "PLDI", "title": "Automatic generation of global optimizers", "url": "http://dl.acm.org/citation.cfm?id=113456"}