{"article_publication_date": "05-01-1991", "fulltext": "\n Circular Scheduling: A new Technique to Perform Software Pipelining Suneel Jain MIPS Computer Systems, \nInc. 928 Arques Avenue Sunnyvale, CA 94086 email: suneel@nlips.com ABSTRACT With the advent of cleeply \npipelined RISC proces\u00adsors, static instruction scheduling by the compiler has become extremely important \nto obtain high processor performance. This is especially true for floating point code, since in general \nfloating point operations have longer Mencies compared to integer operations. This paper suggests using \na new algorithm called circular scheduling, to per\u00adform software pipelining. .Software pipelining has \npreviously been investigated mostly for VLIW architectures. The algorithm clescribecl in this paper is \nshown to be quite effective for a scalar architecture. Register renaming, an idea that ori\u00adginates from \ndynamic instruction scheduling, is used in conjunction with this algorithm to aug\u00ad ment its performance. \nThe techniques described here have been implemented as part of a com\u00ad mercial, production quality optimizing \ncompiler for a RISC architecture. rhc resulting perfor\u00admance improvement has verifiecl the feasibility \nand practicality of the techniques. Permission to copy without fee all or part of this material ia granted \nprovided that the copies are not made or distributed for direct commercial advantage, the ACM copyright \nnotice and the title of the publication and its date appear. and notice is given that copying is by permiaaion \nof the Association for Computing Machinery. To COPY otherwisa, or to republish, requires a fee andkw \nspacific permission. e 1991 ACM 0-89791-428-7191/00051021 9...$1.50 Proceedings of the ACM SIGPLAN 91 \nConference on Programming Language Design and Implementation. Toronto, Ontario, Canada, June 26-28, 1991. \n1. Introduction Basic block scheduling [GrHe 83] [GiMu 86] has been used very successfully to schedule \ncode for scalar R1.SC processors. This paper describes a new technique to improve the perfor\u00admance beyond \nthis level by performing software pipelining. Soj ware Pipelining [ HePa 90] is an optinliz:i\u00adtion to \nreorganize loops such that each iteration of the software pipelinecl loop conttains instruc\u00adtions from \ndifferent iterations of the original loop. Instructions from a new loop iteration are ini\u00adtiated before \nthe preceding iterations are con-, plete. During the normal execution of a loop, the processor resources \ntypically are not fully utilized for the entire duration of the loop. For each itera\u00adtion, there is a \nramp up at the start and a ramp down at the end of the loop. This is caascd by values being loaded at \nthe start of the loop and stored back at the end of the loop. .Software pipe-Iining reduces these periods \nto once before the loop and once after the loop. I%esc portions of the cocle represent the prolog anti \nthe epilog for the loop created after software pipelining. In our illlplellleJlt:iti~>ll, software pipelining \nis accomplished by selectively moving instruc\u00adtions from the top of the loop body to the bottom of the \nloop body. rhe instructions can be thought of as having been moved out from the loop body and the same \ninstrue(ions from the next iteration moved up into the loop body. The loop body can be visualized as \na circular list of instructions, instead of a linear list. Instructions can be moved freely along this \ncircular list. This leads to the name circular .$ched.ding for this algorithm, Any node on the circle \ncan be treated as the start of the loop body and the previous node its end. The modified block of instructions \nis scheduled using the existing basic-block scheduler. This pro cess of moving instructions is repeated \nbased on certain heuristics, till the best possible schedule is obtained. In this implementation, circular \nscheduling is done after register allocation, as part of the normal scheduling process. Register allocation \nintroduces dependencies in the dag that did not exist in the code before, resulting in unnecessary constraints \nfor software pipelining. An aigorithm to remove these dependencies is afso described in this paper. The \nalgorithm performs register renaming during the process of instruction sch eduliug. This allows the scheduler \nto aiter decisions made by the register allocator ancl thus effectively utilize the limited number of \nfloating point registers in the architecture. 2. Related Work Software pipelining for scalar architectures \nhas not been studied extensively. S. Weiss anCI J.E. Smith [ WeSm 87] compare loopunt-oiling ancl software \npipelining as two afternative tech\u00adniques to improve scalar code for the Clay-1S. Their approach to software \npipelining is similar to this implementation. However, their aigorithm is too limited. They aflmv only \none class of instruc\u00adtions, either loads, execution instructions, or stores to be moved out of the loop. \nl%ey also assume the existence of 256 registers and thus do not handle problems causecl by Iimitecl number \nof registers. The work clescrib&#38;l here does software pipelining in a much more general way and a]lows \nany instruction to be moved out of the loop basecl on heuristics. lt aiso works with the 16 fioating\u00adpoint \nregisters available in the MIPS architecture. Better results are also obtainecl by combining loop ~lnrolling \nand software pipeliniug [ He Pa 90\\. Other work in software pipelining ([BoCh 90], [Char 81], [DHB 89], \n[Lam 88], [RaGl 81], [SDX 86], [TOLIZ 84]) has been done mostly for VLIW architectures. The algorithms \nused in most cases are very complex and not suitable for doing software pipelining for scalar architectures, \nIn a VLIW architecture, multiple instructions can be issued in each cycle. Touzeau [Touz 84] and Lam \n[Lam 88] find the minimum initiation inter\u00adval for which a schedule can be found. These methods are similar \nto solving a packing problem for the given instructions. The scheduling COIV straints are divided into \nresource constraints and dependence constraints, and are speciafly handled to perform softwvu-e pipelining. \nTouzeau and Lam also assume that the target machine has a large number of registers. Software pipelining \nis done before register allocation. Touzeau handles running out of registers by introducing spill code \nwhich makes the schedule strung out ant] not optimal. Lam uses simpler techniques for scheduling such \nloops, which serializes the ex ecl~\u00adtion of loop iterations. The circular scheduling algorithm described \nhere basically moves instructions aroLInci the loop iterations. It makes use of the existing basic-block \nscheduler to do the actuai scheduling. All the constraints, incluciing depen(ience constraints i>etween \ninstructions of different iterations are hanciled by eciges in the ciag. In contrast to the algorithms \nfor the VLIW architectures, our algt~\u00adrithm does not find jhe optimal scheciu]e. It attempts to improve \nthe scheduling using itera\u00adtive steps. This makes it adaptable to more con\u00adplicateci scheduling mies. \nIn our implementation, software pipe iining is done after register allocation. Register renam\u00ading is \nperform eci 10 reciuce ciependencies inlr~J\u00adciuced by this. F.arlier work [Ferr 87], [DHB 89 I has also \ninciicated that potential parallelism in a cocie segment is generaily increased if renaming is useci \nto eiiminate multiple definitions of a vari\u00adable. AL the enci of our software pipeiining algo\u00adrithm. \nal i register references are fuilY resolveci. rhere is no neeci for the scheciu]er to introciuce spill \nc(xic. In tile event of running OU1 of regis\u00ad ters, the scheduler compromises by doing less aggressive \ncode movement, thus cutting down on the number of registers med. 3. Backgratnd The scheduling techniques \ndescribed in this paper have been developed as part of the com\u00admon backend for all the MIPS optimizing \nconl\u00adpilers [(X-IX W 86]. The instruction scheduler for the MIPS compilers is implemented at the assem\u00adbler \nlevel. The scheduler is invoked after register allocation has been done. An important objective was to \nleverage the work performed by the exist\u00ading scheduler. The existing scheduler already computes global \ndatailow information for the gen\u00aderal and floating-point registers. This information can also be used \nfor register renaming and software pipelining. The MIPS architecture [Kane 87] provides 16 floating-point \nregisters (FPR). The FPRs can hold either single precision or double precision values. The 16 registers \nbecome a constraining factor when performing aggressive optimization like loopunrolling and software \npipelining and need to be factored into the algorithms. There are several i~llplelllelltati(}lls of the \nMIPS floating point architecture. The R301O and the R6010 are the current implementations in CMOS and \nECL respectively. I Latencv in clock cvcles . operation R301O R601O SPDP SP 2.3 LoAD 2 n.a. 1. 2 STORE \n1 n .a. 9 ADD .23 SUB 223 MULT 45 j 19 J31V 12 17 Table 1: Latency of floating-point operations 1 he \nIatencies for the most common floating-point operations, both single precision and ckmblc precision, \nare given above in Table 1. Both the floating-point chips permi~ loads and stores to execute concurrently \nwith floating point operations, provided they do not nlotlfy or use the result registers of the executing \nopera\u00adtion. Fixed-point operations may also execute concurrently wit h fi eating-point operations. The \nfloaling-point operations can be overlapped with each other in certain circumstances. The exact rules \nare quite complex. The conditions for over\u00adlap differ for the two floating-point chips. 4. Circular Scheduling \nIn this section, we give the motivation for circular scheduling and the intuitive reasoning behind it. \nWe then give a more detailed cfescrip\u00ad tion of our soliware pipelining algorithm. 4.1. Motivation A \nloop bocly is a block of code that gets repeatedly executed. The first instruction in the loop executes \nafter the last instruction in the loop bo(iy for ali iterations except the ftrst one. The first instruction \ncan thus he vieweci as a successor (o the iast instruction. This ieads to the iciea of treating the instructions \nin a loop body as a circu\u00adiar list of instructions, rather than as a iincar list. instructions in a loop \nbody that are the roots of its dag are iogically the instructions at the top of (he loop. f hesc instructions \ncan be move(i from the (op of the loop to the bottom of the loop. An instruction move(i in this manner \nhas circled once arouuci the ioop, and we tail it a cir\u00adckd instruction. Circleci instructions in a ioop \nim(iy correspon{i to the next iteration. A copy of each circicci instruction constitutes the proiog for \nthe software pipeiineci ioop. f%e remaining instructions arc copied to form the loop epilog. If the originai \nioop ha(i N iterations, the software pipelineci ioop is execute[i N -1 times anti the proiog an(i cpiiog \ncom bineci for-m the remaining iterfiti(m. 11 a circlc{i instruction is also a root in the ciag for the \nl(x}p, it can be moved again. Such an in slructi(m is said to have been circle{i twice. [t bec(mles an \ninstruction for t hc next 10 ncx I itera\u00adIi(m ( two ilcrali(ms at tcr lile currenl iteration of the loop), \nThis process of circling can be contin\u00adued to cause M many different iterations execut\u00ading at the same \ntime in the loop body as needed. For a loop that has been circled k times, there are k prologs and k \nepilogs. The loop body is executed N -k times. Having too many iterations executing simultaneously causes \ncode expansion in the prolog and epilog and thus should be done only when justified by the resulting \nperformance gains. How does moving an instruction from the top to the bottom of the loop help? It helps \nif the scheduler is able to generate a better schedule for the modified block of code. The basic premise \nof software pipelining is that this is indeed the case. The benefits are twofold: 1. Instructions close \nto the top of the loop (the successors of the roots of the dag) now become roots of the dag of the modified \nloop. They can thus be scheduled earlier, eliminating stalls at the start of the loop body . 2. f he \nmoveci instruction can be scheduled before some of the instructions at the end of the loop body if there \nare no dependen\u00adcies between them. This effectively utilizes stalled cycles that are usually present \ntowards the end of the loop.  As more instructions are movecl, the number of stalls will decrease to \na minimum level. At a certain point, moving more instruc\u00adtions will cause the schedule to get worse. \nAfter all, if all instructions in the loop are moved, we get back the original loop body anti thus the \norigi\u00adnal schedule. Keeping track of inter-ircration dependem ties is fairly simple with this method \nof doing software pipe lining. rhe iteration number of each instruction is the number of times it has \nbeen cir\u00adcled. Whether an edge needs to be inserted in the dag for any pair of instructions from different \niterations can be determined by analyzing the code. Sometimes data dependency information is needecl, \nand [he algorithm relies on such infor\u00admation being provided by an earlier pms ol the compiler. 4.2. \nExample Ihe concepts described in the previous sec\u00adtion are best illustrated with a simple example. The \nexample is a simple loop to add a constant to all the elements of an array. The source code is given \nbelow: for(i=O; i< N;i= i+ 1) x[i] = x[i] + c; The object code generated with the normal scheduling \nalgorithm, for the R6000/R6010, is shown below in Figure 1. There is a 2 cycle interlock caused by the \n4 cycle latency of a ciou\u00adble precision add for the R601O. Thus each itera\u00adtion of the loop takes 7 cycles. \nLOOP: ldcl $f4,0(r3) addiu r3,r3,8 adcl.ci $f6,$f4,$f12 hue r3 ,r2 ,LOOP < ~ q,cle ij~ferl<~k > s(icl \n$f6,-8(r3) Figure 1: Object code with normal scheduling The i];stmctions immediately following the branch \ninstructions (beq, brie) are the delay slot of the branch and logically execute before the branch. The \ninstruction scheduler can CIOthe fol\u00adlowing change: addiu tO,tO ,i => lW tl ,i+ j(tO) hv tl,j(to) addiu \ntO,tO,i This allows loads and stores to be freely movecl above or below the increment of the base register. \nWithout this capability, the benefits of software pipelining would be very limited. The object code for \nthe same example, when compiled witl~ software pipehning is sh(m n in Figure 2. Two instructions, adtiu \nand ldcl are movecl from the top to the bottom of the loop (i .e circled once). I%ey are also copied \ninto the prolog. f h e loop body now executes N -1 times. There are no interlocks left in the loop body. \nEach iteration thus takes .5 cycles. The cpilog of the software pipelined loop c(mtains th(m instructions \nthal have not been copied over to the prolog. There is a 3 cycle interlock which takes effect only once \nper loop, in the last iteration. LOOP : addiu r3,r3,8 bcq L3,r2 ,LEND prdog Idcl $f18,-8(r3) I.BEG: add.d \n$f16,$f18,$f12 ldcl $f18,0(r3) adcliu r3,r3,8 loop body bne r3 ,r2, LBF.CJ Sdcl $f16,-16(r3) LEND: add-d \n$f16,$f18,$f12 < .? ~ycle interlock > epilog Sdc 1 $f16,-8(r3) Figure 2: Ob.jeet code after software \npipelining Loop unrolling w<w disabled while creating this example. This was done to get short, concise \nobject code. In normal operation, the loop will be unrolled in most cases before it is software pipelined. \n 4.3. Algorithm The software pipelining algorithm described below relics on the misting basic-block scheduler \nto do most of the work. f.oop unrolling is done by [he global optimizer in an earlier phase of the compiler. \nrhis ensures that the scheduler has enough instructions to rearrange during software pipe] ining. he \nscheduler dots not attempt to perform loop unrolling. The current version of the algorithm only moves \ninstructions once across the loop. This is adequate because the input is unrolled loops. .Software pipe \niining is currently done only for single basic block loops with no procedure calls. (llly loops wi( h \na loop index that is incremented once inside the loop are crm\u00adsidered. Ill is allows coast ructi(m of \na prok>g such that the software pipelincd lcmp bocly is executed exactly N -1 times. A key issue in the \nalgorithm is how to select instructions to move oul of the loop. Only instructions that can be scheduled \nlirsl arc eligible to be moved out. f his is the same as the list of candidate instructions available \nfor scheduling (the roots of the dag). Selecting an instruction out of this list is breed on the following \nheuris\u00adtics: a) Move instructions that are on the longest pth in the dag, since they are most likely \nto cause stalls towards the end of the loop. b) Identify resources that cause bottlenecks at some places \nin the loop. Move instructions that use those resources. For example, if most of the stalls are due to \ninstructions waiting for the floating-point multiply unit to be free, give higher priority to moving \nmultiply instructions. The algorithm to perform circular schedul\u00ading is summarized below: 1. Apply the \nbasic-block scheciuling algorithm and check if there arc any stalls. If there are no stalls, use the \nschedule. 9e. If there are any stalls, check if the basic block is a loop that is software pipelinable. \nJlle constraints to be satisfied are listed ear\u00adlier in this section. 3. Choose some instructions that \nare suitable candidates to move from the top of the loop. Move the instructions to the bottom of the \nloop.  4. Rebuild the dag for the modified loop and app]v the scheduling algorithm as before. Count \nthe number of stalls in this schedule.  y. If there are no stalls, use the current schedule. [f (here \nare still some stalls, repeat steps 3 and 4. 6. 1 erminate this process if there are no more instructions \nthat can be moved or the gem eraicd schedule is a Iol worse than earlier schedules. The detmnination \nof Id wor.w is done using some heuristics. If one of the earlier schech:les was better than (he current \none, it is selcctcd for use. 7. ( keatc the prolog and epilog basic blocks and schedule them. Alter \nthe main loop body so t hal it is executed f(w one less itera\u00ad  tion than before. The prolog an(i cpilog \ncombined, cx ecut e one full it crat ion. 4.4. Extensions The current algorithm does not usc data dependency \ninformation that is being compu(ui by an earlier phase of the compiler. The use of this information is \nbeing implemented and will reduce the number of inter-iteration dependen\u00adcies. The software pipelining \naigorithm described above can easily be extmicci to handle loops with multiple basic blocks also. As \ninstructions are moved OUI of the loop, all the basic blocks in the loop will need 10 be rescheduled. \nThe algorithm can be ex tcndccl to aflow more than 2 iterations of the loop to be executed within the \nsof(~vare pipelined loop. ~his is done by nloling instructions thtit are afrcady circled. This wili k.wd \nto the creation of 2 prologs and 2 epilogs. Since the input (o the scheduler is u nrolkd loops. it has \nII(N been neccssn~! to implement this. In the prcsencc of operations with extrcmcli long Mcncies, [his \nwould be more important. I%e alg(wit hm dcscribcd abm e can be u SC(I \\vit]] :1 VI. IW architcc((lrc \nalso, 111(hill C;ISCi{ ui]] be ncccss:in to m(nc instructions fr(m) nlorc Ih:In (me iteratifm {MI( of \nthe basil block. It would bc interesting tl) c(mpare the pcrf(mnance of our algorithm \\vith olhcr algorithms \npreviously inlplc\u00admentcxl. 5. Register Renaming The SIWCSS of the circuiar scheduling algo\u00adrithm dcscrihcd \nai){wc is tied I() minin]izing the dcpendcncics in the l(x)p. ~his afkmrs grealcr frccd(ml in moving \ninstrucli(ms around. SIncc the scheduling is chmc at tcr register allocation. SOIUC dcpcncicncics arc \nintroduced that did not exist in the code bcl(we. l () minimize these dependencies. rcgislcr renaming \nis dImc in lhe schcduier. 1.ile term rcgislcr rcnwning Comes from (Iylul?li( j 1 (,,,,aIhc .s(hc{lulIug67]. \nwhcreb) hardware rearranges the instruction execution to reduce stalls. To reduce data dcpcndences caused \nby the definition and use of the same register, the hardware can logically rcnarnc the register for a \nsubsequent definition. This allows the instructions defining the sarnc register to proceed con\u00adcurrently. \nF@re 3. illustrates how renaming the registers eliminates some of the edges in the dag, thus reducing \ndependencies,   use rl b  Figw-e 3: Effect 1# register renaming em the dag Other i]llplcIl]cl]tatio]~s \n.hil~~ done sof{~vare pipelining before register allocation. l-hey avoid the problcm of dcpendcncics \nintroduced by regis\u00adter allocation, but introctucc thu problem of not being able 10 idlOCilte rqislcrs \nfully. ] his ]cads to spilling of some registers which UI IUm CaII i\\d\\ ~rsel) affcc[ the SAAI le. F \ntw archit cc(u res wi!h fwv registers. this has a big impact, Rcgis[cr renaming allows 11s 10 reg:iin \nm~)sl of the bcnclils 01 doing instrucliim schc(ltlliil: Iwf(wc register allocation. It is performed \nat the same time instruction scheduling. 5.1. Algorithm The algorithm to do register renaming described \nbelow: Do global dat aflow analysis to compute the set of registers live at the end of each basic block. \nIdentify registers that are not live at the beginning ancl at the end of the basic block. This includes \nregisters that are used as. tem\u00ad poraries within the basic block as well as registers that are unused. \nThis forms the pool of registers available for register renaming. [dentify the live ranges for the temporary \nregisters within the basic block. This is clone while building the dag for the basic block. While building \nthe dag, ignore the depen\u00addencies between different live ranges. In other words, do not add edges from \nthe uses of a live range to the definition of the next live range. Also remove the edge from the definition \nof a live range to the definition of the next live range, Pick an instruction to schedule based on certain \nheuristics. If the instruction uses a temporary register, [hat register is replaced by the new register \nallocated for its live range. If the use is the last use in the live range, the new register is put back \nin lhc pool of available registers. [f the instructi~m being scheciuieci (iefines a temix)rary register, \na new register is Cilosen Ior it from the p(mi of avaiiabic regist u-s. ilis new register is now assigne~i \nfor [hc lile range containing dlc definition. If tilere are no more registers available for renanl \u00ading, \nthe sci]eduiing aigorithm is abtx-ted. We use an eariier schcciuie that was the bcsI SC) far. 8. Repeat \nsteps 5, 6 and 7 till ail instructions in the basic block are scheduleci. To reduce the likelihood of \nrunning out of registers to do renaming, register usage is used as one of the heuristics while picking \nthe next instruction to schedule. Given hvo candidate instructions for scheduling next, the one that \ndoes not neeci a new register or frees up a register is given higher priority. The register renaming \nalgorithm has been found to be very usefui even for basic blocks that are not loops to be software pipelined. \nIt is thus applied whenever there are stails in a basic block, even if the block is not software pipeiim \nai>le. 6. Experimental Results and Analysis The performance improvements obtaineci using the software \npipelining afgorithm was evaiuate(i using several benchmarks. The bench\u00admarks were run with full optimization, \nwith and without software pipelining anti register renam\u00ading. The tables beiow list the percentage inlprove\u00adment \nin each case. The tests were run on two sys\u00adtems with different floating point implen~enta\u00adticms. The \ntwo systems are lhe M/2000 (R3000,R3010) and the RC;6280 (R600(),R601 O). 6.1. Livermore Loops The Livermore \nLoops [ MCMA 72] bench\u00admark measures the performance of 24 FOR-TRAN kerneis. These kernels are ex cerpls \nfrom large FORrRAN programs tilat h:we been judged 10 prov idc a goo(i mewwre ol scien tilic application \nperformance. l he MFIJY/s rate for each kernei \\Yas measureci with anti w i~bout software pipeiim ing. \n1 ile MF1,OP/s rates \\\\ erc oi>taine~i i>y averaging the ciata for tell runs. rhc mble gives tile percentage \nim inxwemen ( in th c MFI .{ W/s rate f~m each kcrnci, for ix)[h [hc R30 i O anti ti~c R6010 i>aseci \nsystems. ,Some of the kerneis silou cci no impr(wc \u00adment because they were aireaciy being optim aiiy \nsci~eci~~ic(i (i.e. no intm-iocks or stalis). Kernels Is. 16 :ul(i i 7 have con(iiti~)nai statements \nanti ucrc not soltw a~-c pipe iine(i. Kcrnei 22 rails tile EXP library routine and did not improve. For \na few kernels, there is a slight degradation in per\u00adformance, even though cycle counts are either the \nsame or better. This is caused by scheduling of several loads together. This exposes some clata cache \nmisses that were hidden behind some float\u00ading point operations earlier. This regression is being investigated \nfurther. The performance for the Livermore loops is presented below in Table Percentage Improvement Kernel \nR3010 R6010 1 30 38 9 . 1 o 3 3 9 . 4 0 17 3 -1 9-\u00ad 6 7- \u00ad -5 7 36 34 8 17 15 9 43 53 10 0 0 11 0 \n6 1 2 0 23 13 0 9-L 14 9 13 Lj 1 0 16 I 0 17 0 0 18 30 17 19 7 . -~ ~o 3 0 21 -1 . ~~ )9. . 0 0 23 33 \n19 24 1 -1 . Table 2: Performance of Livermom Loops (DP) 6.2. other Benchmarks Performance was analyzed \nfor some other benchmarks also. For these benchmarks, cycle times were measured instead of the actual \nrun tim cs. The cycle times were obtainecl using the MU S instruction tracing facility pixie. f his tool \nmeasures exactly the number of machine cycles needed to execute a given program. This nleas\u00adures perform \nante inclepen clcnt of other hardware parameters like cache misses and pge faults. Performance improvement \nfor some common benchmarks is summarized in Table3. below: % Improv ement Benchmark R301O R601O Iinpack \no 23 la400 8.6 28 tomcatv (SPEC) 17.3 18.2 nasa7 (SPEC) 5 A 12.5 ctoduc (SPEC) 3.1 ~ .2 fpppp (SPEC) \n2.4 2.3 Table 3: Performance of common benchmarks For the linpack benchmark, the existing scheduler \nalready generates an optimal schedule for the R30 10. The improvements are ch[e to both software pipelining \nand register renaming. The compile time for the above benchmarks increased between 50/0 anti 250/% for \nthe programs analyze(i. The increase in compiie time is gem eraliy proportional to the amount of benefit \ndcriveci. For programs with fewer floating-point cocie anti loops, the degm(iation in compile time is \nmuch less. (X course, the performance improvement is aiso smail for those programs. For ail the benchmarks, \nloop unrolling is performeci before software pipelining. Since unrolling removes a large number of the \nFP interlocks, the gains from software pipeiining are not as large as they woui(i have been if imp unrolling \nwas nor cione. However, the resuits of applying both optimizatio~ls m-e better th;m :lPPI},. ing either \n(>f them alone. 7. Concision i%e aigoritbn)s presenteci in the paper aiiow scaiar processors to take \na[ivantage ot software pipelining, ~he algorithms provide fairly simple anti higiliy i easible ways to \nextend the capabilities of a basic-block scileciuier, 1 he optimization per\u00adformeci yielci reasonable \nimprovemeul over :iireaciy optimized code iiom a produc[i(m quaiity compiler, without adding substantially \nto the compilation time, The benefit is larger in most benchmarks for the R601 O as comparecl to the \nR3010. This is because of [he longer lalencies in the R601O that provide better opportunities for the \nsoftware pipe] ining algorithm. Better results are expected with future illlplelllel~tatic~lls of the \narchitecture that offer either deeper pipelining or more instruction level partaflelism. 8. Acknowledgements \nThe author gratefully acknowledges significant contributions by Sun Chan, Fred Chow, Earl Killian, Sin \nLew and Alex Wu in developing the ideas described in the paper. Fred also provided me with invaluable \nhelp in writing ancl eciiting the paper. 9. References [BoCh 90] F. Bodin, F. Charot, Loop optimization \nfor IIorizontal Microcode Machines , Proc. of International Conference on Supercomputing, June 1990, \n[Char 81] A.E. Charleswort h, AI] Approach to .Scientific Array Pro\u00adcessing: The Architecture Design \nof the AP-120 B/FPS 164 Fanl\u00adily , Computer, December 1981. [CHKW 86] F. Chow, M. Himelstein, E. Kil-Iian, \nL. Weber, Engineering a RI.W; Compiler System , Proceedings COMPCON, IEEE, March 1986. [r)HB 89] J.C. \nDehnert, P. Y.-T. Hw, J.P. Bratl, OV u-lapped [mop Suppor[ in [he Cydra 5 , Proc. 3rd Inter\u00adnational \nConference on Architec\u00adttwal support f or Programming Languages and Operating Svs\u00adtems, April 19S9. P. \nB. Ciiblmns, S.S. Muchnick. Flficieni In st mction Scheduling l or a Pipelined Architecture , Proc. of \nthe SIGP 1.AN Sympo\u00ad [GrHc 83] [Ferr 87] [HePa 90] [Kane 87] [Lam 88] [ MCMA 721 [RaGl 81]  [SDX \n86] sium on Compiler Construction, June 1986. T4R. Gross, J .1,. Hennessy, Postpass Code Optimization \nof Pipeline Constraint s , ACM Transactions on Programming Languages and Systems, July 1983. J. Ferrante, \nWhat s in a Name, Or the Value of Renaming for Parallelism Deteclion and Storage Allocation , f ethnical \nReport # 12157, IBM 1 homas J, Watson Research Center, Janu\u00adary 1987. J. I,. Hennessy, D.A. Patterson, \n Conlputer Architecture, A Qaanlitative Approach , Morgan Kaufmann Publishers, IIlc . (1990) Gerry Kane, \n MIPS RISC Archi\u00ad tecture , Prentice-Hall, Inc. ( 1987) M. Lam, software Pipelining: An effective scheduling \ntech\u00adnique for VLIW machines , Proc. of the ACM SIGPLAN (:onf . 011 Programming Languages Design and \nInlple\u00admentalion. June 1988. F. H. McMahon, FORTRAN CPU Performance Analysis , I.awrence Liv enn ore \nLabora\u00adtories, 1972. RR. Rau, C.D. Glaeser, %ome ,Scheduting Tcchniqoes and an Easily ,Schedulable Horizontal \n.h-chitecturc for I-ligh\u00adperfonnance ,Seientific Conlpat\u00ading , M1(;R(> l-i, October 1981. B. Su, S. Ding. \n.1. Xia, UIWR An Ex tcnsion of U RCR for .SOftw.are t ipelining , Proc. 19th Annual Workshop of Microprc)\u00adgramming, \n(ktobcr 1986, 227 [Toma 67] R.M. Tomasulo, An Efiicient Aigorithm for Fxploring Multiple Arithmetic \nUnits , IBM Journal of Research ancl Development, January 1967. [TOUZ 84] R. F. Touzeau, A FORTRAN Compiler \nfor the FP.S-164 Scientific Computer , Proc. of the ACM SIGPLAN Symposium on Compiler Construction, June \n1984. [ WeSm 87] S. Weiss, J.E. Smith, A Study of Scalar Compilation Tech\u00adniques for Pipelined Supercom\u00adputers \n, Proc. 2 nfi fn t em ati on al Conference on Architectural Support for Programming Languages anti Operating \nSys\u00adtems, October 1987. A patent application has been fileci by MIPS Computer Systems, lnc. on some of \nthe algo\u00ad rithms ciescribecl in this paper. \n\t\t\t", "proc_id": "113445", "abstract": "", "authors": [{"name": "Suneel Jain", "author_profile_id": "81100111313", "affiliation": "MIPS Computer Systems, Inc., 928 Arques Avenue, Sunnyvale, CA", "person_id": "PP14049180", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/113445.113464", "year": "1991", "article_id": "113464", "conference": "PLDI", "title": "Circular scheduling: a new technique to perform software pipelining", "url": "http://dl.acm.org/citation.cfm?id=113464"}