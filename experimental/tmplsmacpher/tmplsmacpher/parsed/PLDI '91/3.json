{"article_publication_date": "05-01-1991", "fulltext": "\n C(IG: A Prototype Coagulating Code Generator* MJ.G. Morris Software Options, Inc. 22 Hilliard St. Cambridge, \nMA 02138 1. Build a control flow graph for the program using Abstract intermediate code instructions \n(or flow blocks) as nodes. This paper describes the design and performance of CCG, a prototype code generator \nbased on couguiation. 2. Use a profile for the program to label each arc in the Coagulation orders code \ngeneration using a run-time flow graph with its expected frequency in a typical profile for the program \nbeing compiled. By treating execution. Mark each arc as uncompiled . busy parts of a program first and \nusing the strategy of 3. Select instructions and allocate registers for each local optimality, CCG maximizes \nthe benefit of careful node (considered in isolation) at minimal cost. instruction selection, register \nallocation and interpro- Each node now presents boundary conditions con\u00ad cedural optimization while \navoiding unnecessary data cerning the location and type of data it uses and movement in busy sections. \nCoagulation radically al\u00ad supplies. ters standard techniques for code generation, achieving highly efficient \ncode without graph-coloring register al\u00ad 4. Select an uncompiled arc with the highest expected location \nor peephole optimization. Experimental results frequency. If the boundary conditions on the arc s showing \nan average 25% improvement over the GNU C entry and exit nodes do not agree, make minimal compiler suggest, \nthat compilation order is crucial and cost repairs (such as inserting copy instructions that coagulation \ncan outperform current code generator or revising storage allocation) to bring them into technology. \nagreement. Otherwise mark the arc compiled and merge the compiled regions connected by the arc. 1 Introduction \n5. Repeat step 4 until all arcs are compiled . Phasing instruction selection, register allocation and \nA region is a connected subgraph of the flow graph, peephole optimization during code generation is widely \nand a compiled region is one consisting of compiled recognized as problematic [8], since resources freed \nat nodes and arcs. Karr called his technique coagulation a later phase cannot be considered by an earlier \nphase. because compiled regions begin in isolation as individ-Karr [9] proposed a new model for code \ngeneration in\u00ad ual nodes and clump together as the arcs between them tended to ameliorate the phasing \nproblem and to make are compiled. Coagulation s heuristic value is that it detailed use of profile information. \nAssuming detailed treats busy parts of the program first when registers knowledge of instruction costs \non the target machine, are abundant and repairs are relatively unlikely. Poten\u00adhis code generation recipe \nis: tially expensive reconsideration of compiled regions can be restricted to the early, and most profitable, \nstages of This work was supported by tile Defense Advanced Research compilation. In effect, register \nallocation and instruc-Projects Agency under contract NOOO14-85-C-O71O. tion selection are done simultaneously, \neach decision be-Permission to copy without fee all or part of this material is granted ing made to minimize \nthe cost. The table in section 4.4 provided that the copies are not made or distributed for direct commercial \nillustrates the penalty of not compiling the highest fre\u00ad advantage, the ACM copyright notice and the \ntitle of the publication and its date appear, and notice ia given that copying ia by permission of the \nquency arcs first. Aaaociation for Computing Machinary. To copy otherwise, or to republish, requires \na fee and/or specific permission. A Prototype Implementation We have imple\u00ad0 1991 ACM 0-89791 -428 -7/91 \n/0005 /0045 ...$1 .50 mented a prototype coagulating code generator, CCG. Proceeding of the ACM SIGPLAN \n91 Conference on Programming Language Design and Implementation. Toronto, Ontario, Canada, June 26-28, \n1991. ~ Written in Common Lisp, it rUnS On a variety of Work\u00adstations. CCG is retargetable; its current \ntargets are Sun 3 and MicroVAX assembly code, and targeting to the MIPS R3000 is under way. CCG is implemented \nas a back-end to E-L, a new wide-spectrum language system [1]. Preliminary results with CCG suggest that \ncoagula\u00adtion can produce very fast code. CCG gcc CCG/gcc Benchmark size time time (time) BUBBLE 326 118 \n126 0.94 INTMM 702 182 178 1.02 PERM 212 100 128 0.78 PUZZLE 2610 100 112 0.89 QUEENS 540 108 124 0.87 \nQUICK 340 102 118 0.86 SIEVE 240 104 138 0.75 TAK 124 130 268 0.49 TOWERS 726 180 248 0.73 Geometric \nh tan 0.80 The table shows results from a suite of standard benchmarks: the integer benchmarks (not requiring \ndy\u00adnamic allocation) from the Stanford benchmark suite, a heavily recursive Gabriel benchmark [6] (TAJi), \nand the ubiquitous SIEVE. Timings are in 60t,h s of a second on a Sun 3/280 and code sizes are in bytes. \nWe compiled C versions of the benchmarks using the GNU C compiler (gee version 1.37.1), widely regarded \nas one of the best optimizing C compilers for the Sun 3.1 The CCG tim\u00adings are for E-L versions of the \nbenchmarks, obtained as direct hand translations from distributed versions. This set of benchmarks gives \nCCG an average speeclup of 25% over gcc .2 Source listings and detailed analysis of the results are available \n[1 1].3 The remainder of this paper outlines the design of CCG, elaborates additional experimental results, \nand proposes future work. Design A compiler based on coagulation has three basic phases. The front end \nhandles the tasks of lexical analysis, pars\u00ading and symbol table construction. It also constructs a control \nflow graph, performs data flow analysis and may perform global optimizing transformations. The remaining \ntwo phases are embedded in the following top-level function defining CCG: Function CCG(FG : flow-graph) \n-> () 1 To get the best gc c timings we considered all combi\u00ad nations of the -0, -fstrength-reduce, -fomit-frsme-po \ninter, -finline-functions and -fcombine-registers flags. 2 We have omitted the VAX benchmark timings, \nwhich are uni\u00adformly even more favorable toward CCC;. 3Section 4.1 discusses the results for INTL4L4. \nFor each node in FG do Kernel .Compile(node) CompileJ1l-Arcs (FG) Assign-Registers (FG) AssignJlemory(FG) \nEmitAssembler(FG) The For loop and CompileAllArcs embody the second phase, coagulation, and the remaining \nthree functions are the third phase. Ass ign~egist ers and Ass ignllemory do not allocate storage, but \nmerely as\u00adsign already allocated virtual storage classes (sets of data values allocated to the same storage) \nto actual stor\u00adage.  2.1 Overview This section gives a general overview of CCG. Front End We designed \nCCG as a general-purpose code generator suitable for imperative languages such as Ada, C, etc. The current \nprototype is part of the E-L programming environment [14]. E-L tools reduce source code (written in surface \nE-L ) to an internal representation, base E-L , which amounts to a slightly augmented lambda calculus. \nE-L supports program transformation on the internal representation, and also makes an interpeter available \nfor profiling and debug\u00adging. When it comes time to generate code, E-L for\u00adwards a control flow graph \nand a symbol table to CCG. At present the E-L programmer may select two pro\u00adgram transformations that \neffect the code produced by CCG. One performs finite differencing of addressing ex\u00adpressions and the \nother does partial beta and delta re\u00adduction (constant propagation), Work is underway to eliminate the \nneed for programmer advice at this stage of compilation. E-L also performs limited algebraic transformations. \nThe finite differencing transformation is a generaliza\u00adtion of classical reduction in strength [2]. Other \nstan\u00addard transformations on intermediate forms, e.g. in\u00adduction variable elimination and common subexpres\u00adsion \nelimination, are planned but not yet implemented. These transformations are essentially independent of \nco\u00adagulation, and preliminary investigations suggest that the experimental results presented here would \nbe im\u00adproved were they available. CCG s flow graph is unusual because procedure calls are directly represented \nby flow arcs. Data flow analysis is therefore automatically inter-procedural. It includes live/dead analysis, \ndead code removal, and conserva\u00adtive pointer analysis, but is generally no more extensive than the analysis \nperformed by many optimizing com\u00adpilers. CCG makes substantial use of available expres\u00adsions involving \nindirect addressing, although they are not strictly required. Profiling There are potentially at least \nfour ways to provide frequencies for the arcs in CCG s control flow graph. The environment could provide \nan interpreter or debugging compiler to gather execution statistics, the programmer could provide the \ndata manually, C(X could use heuristics (e.g. loop weighting) to estimate arc frequencies, or CCG could \nsupport post-compilation profiling and recompilation. We have implemented a flow graph interpreter in \nLisp which we used to gather profiles for the examples in this paper. Also, work is underway to forward \ndata gathered by E-L s profiling interpreter (which operates on base E-L). Future versions of CCG will \nsupport post\u00adcompilation profiling. To support experimentation, CCG provides a nzap\u00ad ping tool which \ndetermines a cycle basis for a flow graph. Given frequencies for the cycle basis CCG automatically extends \nthem to the entire graph, The programmer can alter the map frequen ties and experiment wi tlh different \nprofiles without gathering execution statistics directly. Target Information Architectural tables hold \nall in\u00adformation about the target machine s instruction set and registers. A set of production tables \ndrives the in\u00adstruction compiler. This is a single function that clients within CCG apply repeatedly \nto generate code and to obtain cost estimates. The instruction compiler chooses the least expensive code \nsatisfying the semantics of the intermediate code, subject to boundary conditions concerning the storage \nclasses of available and supplied data, the physical po\u00adsition of subsequent code (e.g., whether or not \na jump is required), and the data flow information available at the node. It also takes into account \nregister usage in\u00adformation supplied by its caller. If there is ambiguity in the boundary conditions, \nfor example if the storage classes are unspecified, then the instruction compiler assumes values that \ngive the least expensive machine code. Sometimes this results in no code at all being generated for a \nnode, as in the case of assignment statements (see section 2.3). The cost of a (non-branching) machine \ncode instruc\u00adtion, m, is given by the formula C(m) = CSa(m) + CtT(m), where Ca and Ct are constant space \nand time cost co\u00adefficients that can be specified by the user at compile time, u(m) is the space occupied \nby m and r(m) is the instruction s time in machine cycles. Values for a and ~ come from the target s \narchitectural tables. Coagulation CCG S main phase, called coagulation, consists of two passes over the \nflow graph. The initial pass calls the instruction compiler on each flow graph node with no boundary \nconstraints. We refer to this pass as kernel compilation, because each node becomes a (separate) kernel \nregion, the smallest compiled region possible. Kernel compilation tries to find the least ex\u00adpensive \nalternative that satisfies the semantics of the node s intermediate code. Unlike conventional code gen\u00aderators \nthat may predetermine storage allocation (e.g. stack allocation of procedure locals), it sets the stor\u00adage \nallocation and other attributes of the instruction as a consequence of selecting the cheapest machine \ncode. The attributes set by kernel compilation comprise each kernel region s boundary conditions for \ncorrect execu\u00adtion. Once each node is compiled in isolation, it remains to correct inconsistencies among \nnodes whose boundary conditions are in conflict. We say that a flow arc s en\u00adtry (exit) node lies in \nthe arc s supply (respectively, de\u00adman d) subregion. Coagulation s second pass, called arc compilation, \napplies the function compile~rc to each arc in order of decreasing frequency (as determined by the program \ns profile). CompileArc guarantees that the boundary conditions of each of the arc s subregions align. \nThe storage classes of supplied data must be con\u00adsistent and properly aligned with the demanded storage \nclasses, The region formed by combining the supply and demand subregions also must not allocate more \nregisters than are available. Finally, CompileArc arranges a lin\u00adear order of the supply and demand region \nwith respect to each other. The number of compiled regions starts out equaling the number of flow graph \nnodes and steadily decreases as Compile Arc merges regions. At any point a com\u00adpiled region is essentially \na self-contained program in the sense that, were data supplied at its entry arcs ac\u00adcording to its demanded \nboundary conditions, it would compute according to its intermediate code semantics and provide results \non its exit arcs consistent with its supply boundary conditions. This strongly contrasts with conventional \ncompilers that use a de facto template to determine the entire program s structure in advance and generate \ncode to fit into that template. Final Stages CCG S work is essentially complete af\u00adter coagulation. The \nmachine instructions, the alloca\u00adtion of registers and memory, and the code s linear order all have been \ndetermined. All that remains is to assign virtual registers (see the next section, below) to hard\u00ad ware \nregisters, assign specific offsets to data held in the global data area, and translate the internal representa\u00adtion \nof the target machine code into assembly. 2.2 Technical Background icode instructions in the obvious \nway. For example, if Before exploring the coagulation algorithms in detail we need to describe some of \ntheir basic data structures. Our account leads to a definition of the virtual regz.der match graph, an \nextension of the match graph defined in [9] that we need to handle register allocation. h40st of the \nwork of coagulation centers around this VR match graph. Occurrences Intermediate code contains occurrences \nof variables. We regard each occurrence as a separate reference to the value held by the variable at \nthat point in the flow graph. An occurrence may have several at\u00adtributes. For example, if it is the last \nreference to a variable before the variable is set or assigned, then it is a last use. An occurrence \nbound to storage whose con\u00adtents is changed by the instruction s machine code is called a generation. \nWe denote last uses by using prime notation (X is a last use of A ) and we use asterisks to denote generations \n(e.g. X*). A variable V is izve on an arc A if there is a directed path from A to an occurrence O representing \na use of V, and the path contains no other occurrences of V, We also say that O is live on A. There is \na dual notion: an occurrence O of V is supplied on A if V is live on A and there is a directed path from \nO to A containing no other occurrences of V. Just as in [9], flow analysis inserts extra occurrences \nat split and join nodes in the flow graph to guarantee the following invariant: MATCH: If V is live on \nA then there is ex\u00adactly one occurrence of V live on A and exactly one occurrence of V supplied on A. \nCohabitation A cohabitation class is a set of occur\u00adrences the compiler has allocated to the same storage \nbecause of data flow or assignment. Suppose A is an arc connecting region R$ to Rd. If V is live on A \nand mentioned in both regions, then the MATCH invariant guarantees that there is a unique occurrence \nof V up\u00adstream of A in R~ supplying the value of V, and a unique matching occurrence downstream of A \nin Rd demanding that value. Barring insertion of an instruction affect,iug V on A, the result of compiling \nA will be that these two occurrences cohabit, i.e. the arc compiler merges their former cohabitation \nclasses into a single class. The co\u00adhabitation classes in a flowgraph grow in size and de\u00adcrease in number \nas coagulation merges classes. By definition all of the occurrences in a cohabitation class lie in the \nsame (compiled) region. The lzve range of a cohabitation class is the maximal subgraph in the region \ncontaining all the occurrences of the class such that all paths in the subgraph terminate at an occur\u00adrence \nin the class. The notion of path is extended inside X is a last use in the following figure, then the \npath terminating at X does not intersect the path beginning at Z. Two cohabitation classes conflict if \ntheir live ranges intersect and at least one contains a generation. The in\u00adtuition is that one is live \nwhen the other changes value. Two such classes cannot be merged without creating a cohabitation inconsistency. \nThe resolution of such potential inconsistencies is one of coagulation s major tasks, and we discuss \nit further in section 3.4 Virtual Registers A program typically contains a large number of cohabitation \nclasses that do not conflict and potentially could be allocated to the same storage location. With registers \nin finite supply it is particularly important to pack cohabitation classes that happen to be allocated \nto registers. We therefore define a virtual register (VR) to be a set of cohabitation classes. Unlike \ncohabitation classes, VRS can be broken up without vi\u00adolating the compiled program s semantics, and in \nthis sense the partition of cohabitation classes into VRS is at the compiler s convenience. A VR is tnconststent \nif it contains two conflicting co\u00adhabitation classes. Each cohabitation class starts out in its own VR. \nWhen we merge two cohabitation classes we also merge their VRS. Two VR,S conflict if a cohab\u00aditation \nclass in one conflicts with a cohabitation class in the other. Thus merging conflicting VRS creates an \ninconsistent VR. Storage Classes A storage class corresponds (roughly) to a set of hardware storage locations \non the target machine. For example, many instructions on the MC68020 allow data to be in either address \nor data reg\u00adisters. To avoid premature specialization we employ a storage class RN which represents the \nunion of available address and data registers. To avoid considering all possible subsets of storage locations, \nand to capture the notion of one storage class being more general than another, we represent storage \nclasses as a lattice whose maximal element represents any storage location and whose minimal element \nrepre\u00adsents a failure to allocate. Part of the storage lattice for the MCXM020 looks like 4our defini \nkion of ~ohabi tat ion conflict is equivalent tO that in [9], and we use Karr s methods to represent \ncohabitation classes, regions ancl to compute conflict. ANY If A is binary then it is possible for V \nto be live on A and appear in one of the subregions but not in the other. AVAILABLE COMPUTED In this \ncase the lone boundary occurrence is called a I RN MEMORY CONS1 ANT A 2N x AO Al DO DI ~~\\ \\ ~ NULL \n The join of any two storage classes is the smallest stor\u00adage class containing both, and the meet of \nany two is the largest consistent with both. For example, the join of AN and D1 is RN, and the meet of \nRN and CONSTANT is NULL. We define the storage class of a set of objects (e.g. a VR, which is a set of \ncohabitation classes, or a cohabitation class, which is a set of occurrences) as the common meet of its \nelements. If the storage class of an object is NULL then it cannot be allocated to machine storage. A \ngeneral strategy in CCG is to allocate a VR to the most general storage class consistent with the code \nthat uses it. (Terminology notwithstanding, a VR S storage class may or may not be a register. ) An occurrence, \nand therefore its VR, may be allocated to COMPUTED if there is an expression available for its computation. \nIn this case no actual storage is used. As a special case an occurrence may be allocated to CONSTANT \nif its value is a compile-time const ant.5 Any storage chss not lying below AVAILABLE in the lattice \nis regarded as unallocated storage, that is, there is no actual machine location reserved for its use. \nThis is the default storage for extra occurrences that are prop\u00adagated through the flow graph along with \navailable ex\u00adpressions. Compile_Arc may choose to re-allocate such occurrences to actual storage if it \nneeds their values, say, in the computation of an addressing expression. The VR Match Graph Let R, and \nRd refer to the compiled supply and demand regions adjacent to an un\u00adcompiled arc A. If R~ # Rd we say \nthat A is binary and the region formed by subsequently compiling A is a bi\u00adnary region. If R~ = R~ then \nA and its associated region are unary. Suppose V is a variable live on an A. The MATCH invariant has \nan important consequence: if V has an occurrence anywhere in R$ then there is a unique occur\u00adrence O, \nof V in R. supplied on A. Similarly, if V occurs anywhere in Rd then there is a unique occurrence od \nof V in Rd live on A. 0$ is the supply boundary occurrence of V and od is V s demand boundary occurrence. \n5Since it is sometimes cheaper to keep known constants in registers, CCG allows botb possibilities. brush \noccurrence because its value brushes across the opposing region. Unary arcs obviously can t have brush \noccurrences on their boundaries. The boundary conditions referred to in the Introduc\u00ad tion are carried \non boundary occurrences. These con\u00ad ditions include the occurrence s storage class and allo\u00ad cation to \nvirtual registers. If there are two boundary occurrences of V they are said to match. We extend the notion \nof matching to cohabitation classes and VRS in the obvious way: two cohabit ation classes match across \nA if one contains an occurrence that matches an occur\u00ad rence in the other, and two VRS match across A \nif they contain matching cohabitation classes. There are two important differences in the way that VRS \nmay appear on the boundary of an arc. Binary arcs may have brush VRS whose only boundary occurrences \nare brush occurrences. A brush VR does not match a VR in the opposing subregion, yet its value is live \nthroughout that region. The second difference is that it is possible for a VR to appear on both boundaries \nof a unary arc, but obviously not so for a binary arc. We call such VRS double-sided. The VR match graph \nfor A is a directed graph whose nodes are the VRS live at A with edges connecting sup\u00adply VRS to matching \ndemand VRS. The connected com\u00adponents represent VRS that will be merged (unless there are repairs on \nA) when A is compiled. Karr [9] defines the match graph only for cohabitation classes, but we extend \nit to VRS to accommodate register allocation. For example, the following picture shows a VR match graph \nfor a binary region. The letters indicate cohab\u00aditation classes and dashed boxes surround VRS. Only one \nVR, {X, W}, contains more than one class. There is one match component, {{w, x}, {x}} and one brush component \n{{Y}}. The VRS {U} and {Z} are interior to their respective subregions because they are not live on A. \nThe work of CompileArc centers around the compo\u00adnents of the VR match graph. We say a match compo\u00adnent \nis cohabitation inconsistent if it contains at least two cohabitation classes which are in conflict. \nA com\u00adponent is storage inconsistent if its storage class is NULL. The VRS of a consistent component \nmaybe merged into a consistent VR, and it is Compile Arc s job to make sure all components of the VR \nmatch graph are consis\u00adtent. We elaborate its methods in section 3.  2.3 Local O@imality Since CCG does \nnot sharply distinguish optimization from code generation, the benefits that accrue during coagulation \ncome from applying the strategy of local optirnaiity. This rule of thumb goes roughly as follows: When \nfaced with undetermined boundary conditions, assume those that produce the cheapest code. The next few \nsections describe some of the novel optimization techniques employed by CCG, many of which are guided \nby local optimality. Assignment As Cohabitation Local optimality suggests that assignment statements \nsuch as ASSIGN A -> B should be compiled into no code, but with the boundary condition that A and B cohabit. \nDuring ker\u00adnel compilation CCG therefore merges the assignment source and target occurrences into the \nsame cohabita\u00adtion class, As a consequence cohabitation classes may contain occurrences of more than \none variable, and may have more than one occurrence live on an arc. Allowing assignment to induce cohabitation \nis the sole source of cohabitation inconsistencies. However, our ex\u00adperience bears out Karr s speculation \nthat coagulation often resolves cohabitation inconsistencies by placing copy instructions on arcs of \nlower frequency than the original assignment, thus improving on the strategy of compiling assignments \nas copies. Flexible Procedure Calling The rigid calling con\u00adventions in common use impose a heavy tax \non proce\u00addures and recursion. Procedure calling penalties encour\u00adage unclear coding practices and can \neven influence pro\u00adgramming language choices. CCG substantially reduces the overhead of procedure calls \nby several methods: . No registers are reserved for special purposes such as holding temporaries, passing \nparameters, or re\u00adturning results. This allows CCG great flexibility in using registers for tasks that \nbest reduce execu\u00adtion cost. . CCG implements parameter passing by assigning an actual parameter to its \nassociated formal, 13e\u00adcause CCG also treats assignment as cohabitation, at busy procedure call sites \nit automatically aligns the formals and actuals in the same register. (Re\u00adturned values are treated similarly.) \nSome param\u00adeters may end up being passed in memory, or there may be some register-to-register copies \ninvolving parameters, but all of these cases arise as a con\u00adsequence of the more general cost-driven \nregister allocation and storage alignment algorithms, CCG does not generate a stack frame unless the \nprocedure dynamically allocates local data. CCG allocates all static data in a segment offset from a \nglobal base register. The only statically allocated values that cannot be so t rested are those that \nare live across a procedure call to their declarer. We call these recursive values. (Note that the value \nof a variable may be recursive at one call site and not another.) CCG copies a recursive value to the \nstack between its last definition and the call site and pops it off the stack in time for its use on \nreturn. The precise location of stack copies depends on the con\u00adtext. Often a procedure s only stack \nactivity will be to save recursive values. 0 CCG fully (inlines procedures with unambiguous static continuations. \n Routines stored in external libraries are called using standard target operating system conventions. \nHow\u00adever, CCG s flexible treatment greatly reduces overhead for those procedures whose flow graphs are \navailable at compile time. Register Allocation CCG implements a very effec\u00adtive interprocedural register \nallocation scheme that nat\u00adurally identifies spill locations according to dynamic cost estimates, a feat \nknown to be difficult for graph coloring register allocators [3, 4]. Following the strategy of local \noptimality, kernel com\u00adpilation effectively places all data (initially) in registers. Intermediate code \ninstructions accessing arrays or deref\u00aderencing pointers are compiled as empty code, in the expectation \nthat they may be deferred or incorporated into addressing mode access. Coagulation insures that the first \nregions compiled will be a program s busy loops. Frequently all data in such regions reside in registers, \nwith loads and stores occurring outside the loops. Addressing Modes The current targets for CCG are CISC \narchitectures, the MC68020 and VAX-11. One difficulty in compiling for these targets is to effectively \nuse the wide variety of addressing modes. Local opti\u00admization dictates the initial compilation of arithmetic \ninstructions into registers, an action that may later be modified if the data turns out to be provided, \nfor exam\u00adple, by array access. A storage class z%consisiency arises on an uncompiled arc when data supplied \nby the entry region is in a stor\u00adage class incompatible with that demanded by the arc s exit region. \nTypical cases include data needed for or supplied by computation in one region, but saved in memory (e.g. \nin an array) in another region. Another typical case is data spilled because of register pressure. The \narc compiler has the option of moving data (by in\u00adserting a copy instruction on the current arc) or revising \nthe arithmetic instruction to use a memory addressing mode. Such decisions are based on cost computations \nbased on the program s profile.  Coagulation In this section we elaborate on coagulation, the most \nnovel phase of CCG. Recall that coagulation begins with kernel compilation, selecting machine code instructions \nfor each graph node considered in isolation. Kernel compilation amounts simply to applying the instruct\u00adion \ncompiler to each node assuming no boundary con\u00additions, and we do not discuss it further. The bulk of \ncoagulation s work, arc compilation, is embodied in the following function: Function Compi.leAllArcs \n(FG : flow-graph) -> () Let arc-stack be Arcs.in_.Order(FG) While not empty? (arc-stack) do Let currentarc \nbe Pop(Arc3tack) Let repair?, repair-arc be CompileArc(currentarc) When repair? Push currentarc onto \narc-stack When repair-arc Push repair_arc onto arc-stack Compi.leAllArcs begins by stacking the flow \narcs in order ofdecreasing frequency. It then successively pops each arc from the stack, making any repairs \nnecessary to align thearc s boundary conditions. Karr [9] imple\u00admented repairs by inserting instructions \non the current arc or on arcs in the adjacent regions, Our algorithms only insert nodes on the current \narc, thus allowing one of three results from CompileArc: The boolean repair? is false. There were no \nre\u00ad pairs and the region(s) adjacent to to current_arc were merged into a single, consistent, compiled \nre\u00adgion.  repair? is true but repair-arc is null. Only current=rc is pushed back on the stack. It must \nbe recompiled due to changes in its adjacent regions  that do not involve insertion of new instructions \n(e.g. changes in storage allocation). CompileArc inserted a new node in the flowgraph adjacent to current \niirc, creating repairarc. The latter is pushed last and therefore compiled before currentarc is recompiled. \n Repair algorithms for unary and binary regions differ in subtle ways, so we consider them separately: \nFunction CompileArc(arc : FlowArc) -> list (FlowArc) If Unary? (arc) then Compile-Unary..Arc (arc) else \nCompile.. Binary Arc(arc) A proof that CompileAll_Arcs terminates is beyond the scope of this paper, \nbut its central intuition is sim\u00adple. Each repair (and subsequent compilation of a repair arc, if there \nis one) removes at leaet one inconsistency from a finite stock on the current arc, but does not add any. \nIts correctness relies on the recursive assumption that the subregions involved are correctly compiled, \ni.e. there are no cohabitation or storage inconsistencies and no more VRS are allocated to registers \nthan there are registers available. 3.1 Basic Strategy for Compiling Arcs Although unary and binary arc \ncompilation differ in im\u00adportant ways, they share a common outline of five steps: Match Graph Obtain \nthe boundary occurrences, compute their virtual registers, and construct the VR match graph. If the region \nis binary, identify any brush VRS allocated to registers. Resolve Cohabitation If there is a match component \nwith a cohabitation inconsistency, insert a MOVE in\u00adstruction to resolve the inconsistency and compile \nthe new arc adjacent to the move. Resolve Storage Consistency Resolve any storage\u00adinconsistent components \nby inserting MOVE instruc\u00adtions and recompiling adjacent arcs, or by relocat\u00ading VRS in one of the subregions. \nAllocate Registers The register allocation strategy differs substantially between unary and binary re\u00adgions. \nWe give more details in sections 3.2 and 3.3, below. Resolve Code Order Each subregion contains infor\u00admation \nabout the linear order of its machine code. For binary regions, use cost comparisons to decide the best \nordering, taking into account the length of offsets and their effect on branch instructions. Change the \nsense of comparisons if this reduces the estimated cost. The next two sections discuss some details of \nthe al\u00adgorithms. Because storage consistency and code order resolution are relatively straightforward, \nwe concentrate on cohabitation resolution and register allocation. 3.2 Binary Regions Cohabitation Consistency \nThe following simple example illustrates the main action of CCG s algorithm for resolving binary cohabitation \ninconsistencies. For simplicity in this and subsequent examples, all VRS will consist of exactly one \ncohabitation class, and we abuse notation by identifying the class with its VR. 1: GEN X* 2: Y<-x 3: \nUSEX A 4: GEN Y* 5: USEY 6: USEX Assume that all arcs but A (2 + 5) have been compiled. The supply region \nfor A consists of instructions 1 and 2, and the demand region contains instructions 3-6. In\u00adstruction \n2 was compiled as a cohabitation. The match graph on A has the component,  /cl\\ G? (2 3 where (2 1 \n= {X;, X2, Y2}, C2 = {Y;, Y5} and C3 == {X3, X6}. Because the live range of C3 overlaps the gen\u00aderation \nin C2 at node 4, these two cohabitation classes are inconsistent. This component is cohabitation incon\u00adsistent \nbecause merging it would entail merging C2 with C3. The solution is to insert a move on A (to either \nCz or C3), producing the graph i : GEN X* 2: Y<-x A t 3: USE X7: MOVE X ->X* I 4: GEN Y* 6:USE X Now, \nif we compile the new arc A , the subsequent match graph on A becomes /c \\  2 4 c; where C4 n {X;} \nand C: n (73 U {X;}. Since X; is a last use, its live range (just node 7) does not intersect the generation \nY~, so C2 does not conflict with C4 and the component is now consistent. Notice that the class Cj created \nby merging C3 with {.Y; } on A is interior and does not enter into the matching. In all there are four \nkinds of binary cohabitation in\u00adconsistency, depending on whether a conflicting VR is found on the demand \nor supply side, and whether or not it contains a brush occurrence. The example above illustrates a demand \nmaich inconsistency (no brush oc\u00adcurrence), by far the most common of the four. It shows how insertion \nof a MOVE instruction on the current arc eliminates the component s inconsistency essentially by splitting \nthe component in two. Register Allocation Once the VR match graph is consistent with respect to cohabitation \nand storage, we extend the register allocation of the two subregions to an allocation for the entire \nregion. A simple example illustrates the central technique. In the figure below the two X VRS match across \nbinary arc A. U and W are interior VRS in the supply region R~, and Z is interior to the demand region \nRd. Y is a demand brush VR. R. A xY z&#38; Allocating all of the illustrated VRS to registers requires \n3 registers in each subregion, and without further inter\u00advention the composite region would require 5 \nregisters (the two X VRS would be merged). There are two ways to reduce register pressure back to 3 for \nthe composite region. Inferior merging relies on the observation that interior VRS in disjoint regions \ncannot conflict. We can, for example, merge U and Z to reduce register pressure to 4, but no further \ninterior merging is possible in the example. Because Y is live throughout R. and may conflict with every \nVR, we can t merge it with anything. However, the fact that it is not mentioned in R, means that its \nvalue can safely reside in memory there. Inserting a memory-to-register move of Y on A we get the following \npicture, in which both A and the new arc A are yet to be compiled. + A Y A Y* Y MOVE z Rd The MOVE \ninstruction (in memory) and the piling A yields contains destination two VRS, the source Y Y* (in register). \nCom\u00ad 53 Uw x R, A Rd in which Y is the new VR formed by merging the MOVE destination and the former \nbrush VR in Rd. (We don t show the Y VR in Rd because, being allocated to mem\u00adory, it is now irrelevant.) \nNow Rd has two interior VW, each of which may be merged with a counterpart in Rs to reduce register pressure \nto 3. This technique is the coagulation version of spilling, though in this example it would be more \naccurate to call it restoring . A counting argument shows that if p, and pd are, respectively, the number \nof registers consumed by the supply and demand subregions of a binary region, then it is always possible \nby interior merging and spilling to reduce the register pressure of the composite region to max(p,, ~d). \nTherefore we can always obtain an alb\u00adcation for a binary region even if each of its subregions already \nuses all available registers. We only spill/restore when we run out of registers, but we have used a \nvariety of heuristics to guide interior merging. 3.3 Unary Regions Cohabitation Unary VR match graphs \ncontain no brush VRS but may contain double-sided VRs. Consider a program that reads values into variables \nX and Y and conditionally exchanges the values via a temporary variable Z. A simplified flow graph looks \nlike 1: 2: X <-READ Y* <-READ A I \\ / 3: 4: z x y <\u00ad<\u00ad \u00ad x Y 6: 7: PRINT PRINT X Y) Suppose all the \narcs except A (2 + 5) are compiled, making A unary. Local optimality has caused the m\u00adsignments in nodes \n3, 4 and 5 to induce cohabitation, giving us the classes c1 = {X71,x3, 23,Z5,Y5, Y7} c? = {Y2, Y4, X4,X13}. \n In this example each cohabitation class is in its own singleton VR, so we don t distinguish the two. \nThe VR match graph is c1 1+  x 11Y Since the live ranges of Cl and Cz overlap, and each contains a \ngeneration, the two classes are cohabitation inconsistent and cannot merge. The cohabitation resolver \ninserts MOVE instructions and compiles the newly created arcs until the match graph for A has no more \ninconsistencies. This example requires three moves. Just before the final compilation of A the graph \nlooks like 1: X<-READ 2: Y* <-READ + 8: MOVE, X ->X* \\ Al 3: z<-x 4: x<-Y 9: MOV; Y ->Y* 5: Y<-z IO: \nMOVE X ->X*  + / 6: PRINT X} 7: PRINT Y After A is compiled the (consistent) classes are c; = CIU{X8, \nY;} c; = C2u{Kj, Xfo} C3 = {x:, x{,} Register Allocation Since a unary region is formed from a single \nsubregion by the compilation of an arc, un\u00adless there is a repair, it inherits its instructions and reg\u00adister \nallocation from the single subregion. Move instruc\u00adtions inserted by cohabitation or storage consistency \nal\u00adgorithms, however, may cause additional registers to be allocated. In the example above, the cohabitation \nclass C s is entirely new to the region. We suspend register allocation for VRS created by moves on unary \narcs until A is consistent. Then we note all newly created cohabitation classes such as C3. The default \ncompilation of instructions 8, 9 and 10 causes C3 S VR to be tentatively allocated to a register. If \nsuch a class causes registers to overflow, it is relocated to memory. In the example, the MOVEinstructions \nwould be revised to read 8: PUSH X ->X* 9: MOVE Y >Y* IO: POP X ->X* essentially implementing a swap \nusing the stack as tem\u00ad porary storage. 1 here are no brush VRS to consider, so such relocations are \nall the unary register allocator needs to handle. 4 Results and Discussion CCG was intended as a proof-of-concept \nfor code gen\u00aderation by coagulation. During the implementation we extended the work in [9] by providing \nregister allocation, code ordering, general storage allocation, and by sub\u00adstantially simplifying cohabitation \ninconsistency resolu\u00adtion. Having constructed the prototype we explored its behavior through a series \nof experiments on the bench\u00admark suite described in section 1. The next few sections discuss the results \nof our experiments, 4.1 Code Quality Code inspection suggests that the average 25% speedup over gcc obtained \nby CCG (shown in the table in sec\u00adtion 1) generally follows from superior register allocation and simplified \nhandling of procedure calls. Instruction selection seems to be roughly equivalent between CCG and gcc. \nFinite differencing and simplification transfor\u00admations provided by the E-L environment simply keep pace \nwith similar optimization done by gcc. On the three non-recursive benchmarks (BUBBLE, INTMMand SIEVE)CCG \nachieves an average speedup of 1l~o, with significantly superior performance only on SIEVE.The comparatively \npoor performance of CCG on INTMMis a consequence of an extra memory access in the innermost loop. Efforts \nare underway to implement a general algorithm for treating pointer expressions that should cause CCG \nto eliminate two memory accesses in INTMM Sinner loop, a change that would gain a speedup of about 8% \nover gee. As one would hope, Compile Arc s code ordering al\u00adgorithm places tests at the bottom of (most) \nloops. But because procedure calls are implemented as flow arcs, the algorithm also produces some surprising \nintermin\u00adgling of procedure bodies, particular in heavily recursive call loops. 4.2 Procedure Calling \nThe table in section 1 gives an average speedup on recursion-dominated programs in the benchmark suite \nof 32%, compared with the overall average speedup of 2570. This shows that CCG produces very efficient \ncode for recursion, a confirmation of our treatment of proce\u00addures in general. For example, source for \nthe TAK benchmark looks like: Function tak(x:integer, y:integer, z:integer) -> (integer) If yge xthen \nz else tak(tak(x-l,y,z),tak(y-l,z,x) ,tak(z-l,x,y)) ... tak(21,13,6) ... CCG found sufficient registers \nin which to pass all of the parameters, requiring only a single register-to\u00adregister move for alignment \nbefore the last two inner\u00admost calls to tak. CCG uses a caller saves conven\u00adtion to push only recursive \nvalues (see section 2.3) on the stack. In particular, at the last innermost recursive call (tak(z-1 ,x,y))only \nthe result softhe two previous calls to tak are saved, but not the values ofx, y or z. There is no overhead \nfor stack frames other than there\u00adturn address saved bya jumps ubroutiue instruction. Even without tail-recursion \nelimination, CCG produces very efficient code for TAK. 4.3 Profile Variation It s natural to ask how \nrobust our results are in the face of variation of the profile. As one would expect, wildly wrong profiles \ncause CCG to produce very slow object code. A more reasonable question to ask is, what is the likely \neffect of small perturbations from average behavior? Since the benchmarks mentioned here take no input, \ntheir execution profile is constant. But using CCG S cycle basis map we varied the base frequencies of \nseven of the programs 6 by random factors ~20910. The following results are typical: Benchmark Standard \nPerturbed AT BUBBLE 118 118 o% INTMM 182 178 -2% PERM 100 100 o% QUEENS 108 108 o% QUICK 102 106 +4Y0 \nSIEVE 104 106 +2% TAK 130 130 o% The table shows that CCG is likely to be quite stable under statistical \nfluctuation in profile data. Of course, programs for which there is no reasonable typical data probably \nwon t gain much benefit from use of a profile (though other aspects of CCG may still be advanta\u00adgeous). \nBut experiments to date confirm our intuitions: GFluctuations in cycle basis freclueneies for PUZZLE \nand TOWERS produces graphs with some arcs receiving negative fre\u00adquencies, so we omitted them. coagulation \nresponds to reasonably fluctuating profiles without serious degradation. 4.4 Compilation Order One of \nthe main premises of coagulation is that the or\u00adder in which arcs are compiled is important. To test \nthis hypothesis we examined three arc orders. Stan\u00ad dard order is the usual one for CCG, compiling arcs \nin decreasing order of execution frequency based on the program s execution profile. Random order shuffles \nthe arcs so that frequency does not influence compilation order. Perverse order uses the same arc frequencies \nas standard order, but compiles the arcs from the least to the most frequent. The next table shows running \ntimes for benchmarks compiled by CCG under three compilation orders, stan\u00addard, random and perverse, \nValues are normalized to the gee-compiled running time. Benchmark Standard Random Perverse BUBBLE 0.94 \n0.94 1.59 INTMM 1.02 1.16 2.00 PERM 0.78 0.89 0.91 QUEENS 0.87 1.11 2.08 QUICK 0.86 0.98 1.25 SIEVE 0.75 \n0.86 1.17 TAK 0.49 0.51 0.56 TOWERS 0.73 0.97 1.65 gee. mean 0.79 I 0.90 I 1.26 Here we see an expected \ncorrelation: standard order produces the best results, perverse order produces the worst, and random \norder falls somewhere in between. Since the frequency labeling is the same across the three orderings, \ncost calculations are unchanged. However, in programs large enough to have significant competition for \nregisters, reversing the arc order causes spilling in\u00adside busy sections.  4.5 Minimizing Size Since \nCCG currently employs no optimization that are profligate in their use of space (e.g. procedure inlining \nor loop unrolling), nor does it choose data structures or allocation based on space costs,7 space savings \nare generally limited to gains made by using compact in\u00adstructions, The next table shows the results \nof running CCG on the benchmarks, setting time costs to zero so that only space costs are considered \nduring instruction selection. 7These optimization fit well into coagulation, and we hope to include them \nin the future, Benchmark Time Size TAK 140 122   %2-i-+% BUBBLE 118 304 o% -7% PERM 102 214 +2% +1% \nSIEVE 134 234 +29% -3% QUICK 108 316 +6% 7% QUEENS 114 544 +6% +1% INTMM 194 678 +7% 3% TOWERS 198 706 \n+10% 396 PUZZLE 130 2474 +30% 5% I L 1 : columns giving percentag change are a compz i\u00adson to the table \nin section 1 in which space was free and only time costs were considerecl. It appears to be difficult \nto reduce code size by more than a few percent from the size required for the fastest code. On modern \nworkstations execution time is much more precious than space[7], so these gains appear not to be worthwhile. \nFor smaller tests the results are somewhat unreliable. In one case (PERM)both execution times and code \nsize increase! This is not surprising given the local nature of CCG S coagulation algorithms. It is no \nmore important to conserve space in inner loops than it is elsewhere in the program, yet doing so may \npenalize execution speed. Worse yet, decisions to minimize space in inner loops may require CCG to insert \nadditional MOVEinstructions elsewhere to resolve inconsistencies, thus defeating the inner loop savings. \nThe anomalous result for BUBBLE,improving space usage at no cost to speed, is a consequence of the nearly \ntotal dominance of a single block performing memory\u00adto-memory swap. On the MC68020 the instructions that \nconserve space also conserve time in this loop, while space savings in dynamically unimportant code result \nin an overall space improvement. Despite these minor anomalies, the table shows that CCG delivers a modest \ntime/space tradeoff, just as one would expect if coagulation is working according to the\u00adory. 4.6 Coxmpiler \nRun Time We have not completed a formal analysis of CK3C; s compile-time performance, but we can sketch \nsome es\u00adtimates. Let N be the number of nodes and A be the number of arcs in the flow graph at the beginning \nof arc compilation. Let Va be the number of variables live on arc a, and let V = max(V. ) over all the \narcs in the program. The number of repairs on arc a is proportional to V.. Since each repair could create \nanother node, hence another arc, the number of newly created arcs could be as high as ~ Va. (Repair arcs \ndo not themselves require repair. ) Thus, the total number of arcs to be compiled is no more than the \nnumber of original arcs plus the new arcs: A+~Vo SA+AV=A(l+V)-JAV. The cost of all but one of the repairs \nmade by Compile Arc is proportional to V., The one exception involves the computation of conflict among \npairs of co\u00adhabitation classes in the regions adjacent to the arc. MThile this computation could involve \nroughly O(N2) comparisons, the implementation suggested in [9] and employed in CCG in practice requires \ntime proportional to V.. Thus the repair cost for a is bounded by V. < V. During kernel compilation CCG \nmakes one pass over each flow graph node at essentially a constant cost per node. Since the compilation \ncost for each arc is about V and there are at most AV arcs, we see that the total cost C for coagulation \nis given by C-N+ AV2. Observing that the number of arcs in a flowgraph is in practice no more than about \n10 ?ZOthan the number more of nodes, we have, finally, C-NV2. The relationship between the number of \ninstructions (N) in a program and the number of variables live at any one point (V) is not entirely clear, \nbut intuition suggests that typical coding practices impose a limit on V relatively independent of N. \nThis is in keeping with the observed behavior of CCG. We did not code the prototype with compilation \nspeed in mind, and effort is being made to improve CCG S running time, These estimates suggest such work \nwill not be in vain.  4.7 Related Work Traditional optimizing code generators tend to estimate the cost \nof various optimization decisions a priori, at the time the compiler is constructed [2]. However, us\u00ading \na program s run-time profile to sharpen estimates is becoming increasingly popular in optimizing compilers. \nRecent work on register allocation (e.g. [15]) and code positioning [12] use profile information at link \ntime to guide optimization choices. Some graph coloring heuris\u00ad tics employ execution profiles during \na separate register allocation phase [13]. The strategy of compiling assignment as cohabitation subsumes \nthe classical optimization of copy propagation [2], and the cohabitation classes maintained by CCG resemble \nwebs [13]. CCG s flexible procedure calling mechanisms share the philosophy, if not the detailed implementation, \nof the Orbit compiler [10]. The general strategy of coagulation recalls trace scheduling [5], a technique \nfor obtaining parallelism in microcode. A trace is a most-frequently executed (lin\u00adear) path through \na flow graph. CCC, implicitly op\u00aderates on traces, although its treatment of loops dif\u00adfers from [5]. \nHowever, CCG makes all decisions about instruction selection, storage allocation, and code or\u00addering, \nwhereas trace scheduling is concerned primarily with microcode reordering to maximize parallelism. Al\u00adthough \nthe current CCG algorithms share very little de\u00adtail with trace scheduling, we believe that some variant \nof trace scheduling is probably quite suitable as the ba\u00adsis for a coagulating code-reordering algorithm \nto take advantage of pipelining and coprocessor parallelism in RISC architectures.  4.8 Future Work \n The results described in this paper were achieved en\u00adtirely without peephole optimization and using \nonly three classical intermediate-code techniques (reduction in strength, constant propagation and partial \nbeta re\u00adduction). Significant omissions include common subex\u00adpression and tail-recursion elimination, \nboth of which are entirely compatible with coagulation and which we hope to implement within the E-L \nenviromnent. We also intend to extend CCG to support langu?~ges that generate run-time closures (including \nCommon Lisp and E-L). RISC Targets Although the current targets of CCG are both CISC architectures, we \nbelieve that coagula\u00adtion is ideally suited to code generation for RISC ma\u00adchines. CCG can make excellent \nuse of large, regular register sets, and cost estimation is much easier in a RISC environment. Since \na load/store architecture sub\u00adstantially reduces the number of cases that need to be considered when \naligning storage, we expect the runniug time of the compiler to be substantially reducecl without compromising \nthe performance of compiled code. Much of the complexity of the current CCG is reclu i red to handle \nirregular register sets (MC68020) and extensive addressing modes (MC68020 and VAX-11), and could be eliminated \nfor RISC targets. We also hope to exploit CCG S highly local, cost-driven control over instruction selection \nand code ordering to maximize the use of in\u00adstruction caching. 4.9 Conclusion We developed CCG as a feasibility \nstudy for the code\u00adgeneration technique of coagulation. Our results bear out the hypothesis that coagulation \nmakes very good use of profile information. It also makes good use of regis\u00adters, particularly in busy \nloops, regardless of whether the loop is the consequence of an iterative or recur\u00adsive construct in the \nsource language. The isolation of target-specific routines to the instruction compiler, operating independently \nfrom the main coagulation al\u00adgorithm, makes the task of retargeting CCG relatively easy. Despite exponential \ngrowth in the speed and capac\u00adity of serial computers, the demand for their perfor\u00admance continues to \nexceed its supply, Especially with the increasing popularity of RISC, we need to refine our understanding \nof compilation technique to extract the maximum benefit from existing and future hard\u00adware. Our experience \nwith CCG suggests that coagula\u00adtion may be a profitable route toward that understand\u00adiug(  Acknowledgments \nThe author would like to thank Gretchen Ostheimer and Alex Zatsman for their invaluable contributions \nto the design and implementation. Steve Rozen and Judy Townley provided much valuable support and insight. \nAnd, of course, thanks to Mike Karr for his original idea and ready advice. References [1] E-1 definition. \nTechnical Report S01-03-86, Soft\u00adware Options, Inc., April 1986. [2] Alfred V. Aho, Ravi Sethi, and Jeffrey \nD. Ull\u00admanr Compilers, Principles, Techniques, and Tools. Addison-Wesley, 1986. [3] Preston Briggs, Keith \nD. Cooper, Ken Kennedy, and Linda Torczon. Coloring heuristics for regis\u00adter allocation. ACh!l SIGPLAN \nNotices, 24(7):275\u00ad284, July 1989. [4] Frederick Chow and John Hennessy. Register al\u00adlocation by priority-based \ncoloring. In Proceedings of the 1984 Syrnposiunt on Compiler Construction, pages 222-32. ACM, June 1984. \n[5] J.A. Fisher. Trace scheduling: A technique for global microcode compaction. IEEE Trans. On Computers, \n7( C-30):478-490, July 1981. [6] R.P. Gabriel, Performance and Evacuation of Lisp Systems. hHT Press, \nCambridge, Mass., 1985. [7] Jim Gray. The 5 minute rule for trading memory for disc accesses and the \n10 byte rule for trading Inemory for CPU time. In Proceedings of the ACM SIG on Management of Data 1987 \nAnnual Confer\u00adence, pages 395 398, 1987. [8] John Hennessy and Mahadevan Ganapathi. Ad\u00advances in compiler \ntechnology. Annual Review of Computer Sctence, 1:83-106, 1986. [9] Michael Karr. Code generation by \ncoagulation, In Proceedings of the SIGPLA N 84 Symposium on Compiler Construction, pages 1-12. ACM, June \n1984. [10] David Kranz} Richard Kelsey, Jonathan Rees, Paul Hudak, James Philbin, and Norman Adams. \nOrbit: An optimizing compiler for Scheme. In Proceedings of the Symposium on Compiler Construction, pages \n219-233. ACM, July 1986. [11] W.G. Morris and Steve Rozen. Interim report on E-L/CCG benchmarks. Technical \nReport SOI-Ol\u00ad90, Software Options, Inc., 1990. [12] Karl Pettis and Robert C. Hansen. Profile guided \ncode positioning, In Proceedings of the ACM SIG-PLAN 90 Conference on Programming Language Design and \nImplementation, volume 25, pages 16\u00ad 27. ACM, June 1990, [13] Vatsa Santhanam and Daryl Odnert. Register \nallo\u00adcation across procedure and module boundaries, In Proceedings of the ACM SIGPLA N 90 Conference \non Programming Language Design and Inlplemen\u00adtation, volume 25, pages 28 39. ACM, June 1990. [14] Judy \nG, Townley. E-L user s manual. Technical report, Software Options, Inc., 1990. [15] David W. Wall. Register \nwindows vs. register allo\u00adcation. ACM SIGPLAN Notices, 23(7):67-78, July 1988.  \n\t\t\t", "proc_id": "113445", "abstract": "", "authors": [{"name": "W. G. Morris", "author_profile_id": "81100134940", "affiliation": "Software Options, Inc., 22 Hilliard St., Cambridge, MA", "person_id": "P294610", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/113445.113450", "year": "1991", "article_id": "113450", "conference": "PLDI", "title": "CCG: a prototype coagulating code generator", "url": "http://dl.acm.org/citation.cfm?id=113450"}