{"article_publication_date": "05-01-1991", "fulltext": "\n Linear-time, Optimal Code Scheduling for Delayed-Load Architectures Todd A. Proebsting and Charles N. \nFischer* University of Wisconsin-Madisont Abstract Fast, optimal algorithms, however, can be devised \nfor simpler, yet realistic architectures. Our results A fast, optimal code scheduling algorithm for proces\u00ad \nshow that for a restricted set of pipeline constraints sors with a delayed lo,ad of 1 instruction cycle \nis de\u00ad and a simple RISC load/store architecture, optimal scribed. The algorithm minimizes both execution \ntime code can be generated in linear time for expressions and register use and runs in time proportional \nto the without operand sharing. Our delayed-load scheduling size of the expression tree. Extensions that \nspill regis\u00ad algorithm, DLS, efficiently combines instruction selec\u00ad ters when too few are available \nare also presented. The tion, instruction scheduling and register allocation. It algorithm also performs \nvery well for delayed loads of is restricted to handling expression trees in which all greater than 1 \ninstruction cycle. For machines with leaf nodes are direct memory references. We propose load delays \ngreater than 1, bounds are given for the DLS as an attractive, simple, fast and effective alter\u00ad minimal \nnumber of registers needed for optimally eval\u00ad native to more complicated, slower heuristic solutions. \nuating an expression tree. 2 l?r~vious Work 1 Introduction Code scheduling algorithms and heuristics \nfor The problem of optimally scheduling instructions pipelined architectures have been ext,emively studied \nunder arbitrary pipeline constraints is NP-complete ([GJ79], [LLM+87], [HG82], and [PS90]). Many heuristics \nhave been proposed for scheduling pipelined code; all assume, however, that pipeline constraints can \noccur after any instruction, and that operators may share common subexpressions. The intractability of \nfinding an optimal schedule holds even if an unlim\u00adited number of registers is available. Optimal register \nallocation in itself is also NP-complete in the presence of common subexpressions [GJ79]. Such negative \nre\u00adsults have led to the belief that generating good qual\u00adity code for RISC machines with pipeline constraints \nis too difficult to do well except in complex optimizing compilers. in recent years. Most of the attention \nto code scheduli\u00adng has been directed at scheduling expressions rep\u00adresented by directed acyclic graphs \n(DAGs) for archi\u00adtectures with pipeline constraints after both loacls and operations.1 Heuristic attacks \non this general prob- Iem can be found in [HG82], [HG83], [G M86], [War90], [LLM+87], and [PS90]. Heuristic \nsolutions treat regis\u00adter allocation as a separate issue that occurs either be\u00adfore or after scheduling. \nUnlike DLS, these algorithms fail to integrate code scheduling and register allocation, and therefore \nsuffer from phase-ordering problems, In addition, whereas DLS runs in 0(72) time (\\vhere n is the number \nof nodes in the expression), these algo\u00adrithms run in 0(n2) time, and must have an a,dclitional *This \nwork was supported by NSF grant CCR 8908355. register allocation phase. t Authors) ~ddre~~: Dept. of \nComputer Sciences, An adaptation of HLL S algorithm [Hu61] gives an 1210 \\V. Dayton {todd,fischer}@cs.wise.edu \nSt., Madison, WI 53706. Email: optimal solution to scheduling a tree-structured task system on multiple \nidentical processors if each task Permission to copy without provided that the copies are fee all not \nmade or part of this material or distributed for direct is granted commercial has unit execution time \n[Cof76], but the algorithm advantage, the ACM copyright notice and the title of the publication and does \nnot handle register allocation constraints. For its date appear, and notice is given that copying is \nby permission of the an architecture with 2 functional units, one for loads Association for Computing \nMachinery. To copy otherwise, or to republish, requires a fee andlor specific permission. 1 We will use \noperations to denote nomload instruc~ions. @ 1991 ACM 0-89791 -428 -7/91 /0005/0256 . ..$1 .50 i I I \nProceedings of the ACM SICiPl_AN 91 Conference on I Programming Language Design and Implementation. \nToronto, Ontario, Canadar June 26-28, 1991. 256 reg +--memory load memory, reg regi + regj op regk op \nreg3, reg~, regt memory + veg store reg, memory Figure 1: DLS Machine Model and one for operations, with \nidentical pipeline con\u00adstraints, Bernstein et. al. have investigated code scheduling with register allocation \nfor trees ([ BPR,84] and [BJR89]). Although applicable to a much different machine, Bernstein s results \nand algorithms are similar to ours both minimize pipeline interlocks and register usage, and both run \nin O(n) time. 3 Delayed-Load Architecture \\fre restrict our attention to a simple class of architectures-RISC \nload/store architectures with de\u00adlayed loads. All instructions require a single instruc\u00adtion cycle to \nissue, and only loads are pipelined. Our rnaclline s instruction set is given in Figure 1. This architecture \nis an approximation of the integer func\u00adtional units of many modern RISC processors such as the SPARC \nand MIPS R3000 [PH90]. A delayed load requires that the destination of a load not be accessed by subsequent \ninstructions for some number of instruction cycles, although other, unrelated instructions may execute. \nDelay will be used to refer to the number of cycles that must elapse before the destination register \nis ready to be used, An attempt to use a destination register prior to the elapsing of Delay cycles forces \na pipeline interlock that blocks processor execution until the register has finished loading. Figure \n2 shows two possible evaluations of an exam\u00adple expression tree. It is assumed that Delay=l. The (naively \nproduced) left sequence wastes cycles due to pipeline interlocks at times 3 and 7 asterisks (*) de\u00adnote \nthe registers with which the delays are associated. The righf sequence incurs no delays,  4 Register \nAllocation TYade-Offs Register allocation and instruction scheduling interact because the order of instructions \ndetermines the regis\u00adter needs for computing a given expression. Likewise, register allocation can limit \nor expand the possibilities for re-ordering code to limit pipeline interlocks. If register allocation \nprecedes instruction scheduling, the ability to schedule the code can be severely limited by constraints \ninduced not by data dependence, but by constraints introduced by potential register interfer\u00adence. If \nregister allocation follows instruction schedul\u00ading, a given schedule may require unnecessarily many \nregisters, thus limiting the effectiveness of a global op\u00adtimizer and possibly requiring spill code. \nThis well\u00adknown phase-ordering problem is accepted in practice, but can lead to sub-optimal register \nuse because the instruction schedulers minimize interlocks w?thout tal{\u00ading into account the possibility \nthat increased register demands could lead to costly register spilling. The DLS algorithm avoids this \nphase-ordeling prob\u00adlem by scheduling code and allocating registers in tan\u00addem, DLS optimally schedules \ninstruc(iom to avoid interlocks and minimize register usage for expression trees when Delay=l. When Delay> \n1 or ~vhen DAGs are transformed into trees, DLS serves as an excel\u00adlent heuristic while retaining its \nconcept ual sunpl lci ty, guaranteed linear performance, and integrated register allocation. 4.1 Canonical \nForm Generating code and allocating registers is IIIUCh sinl\u00ad pler for expression trees than for arbitrary \nD/l Gs. Once a preliminary schedule for ~he code has Ilecn gcm erated for a tree, and the register neecls \ndetermined, it is possible to reschedule the code and re-assign the reg\u00adisters to obtain a code sequence \nin a callol}ica] form. This canonical form has three important characteris\u00adtics: the relative order of \nthe operators remains un\u00adchanged, the relative order of the loads rclnail]s un\u00adchanged, and the number \nof registers neec]ed 1emai ns unchanged. For a given number of registers and spe\u00adcific operation and \nload orders, the ;anonic.al order will minimize pipeline interlocks for a clelayed-load ma\u00adchine. The \ncanonical schedule is produced by nlollng ]oa[ls as early as possible in the instruction sequence. Shift\u00ading \nthe loads will move a load away from its parent in the tree and therefore increase the number of instruc\u00adtions \nbetween the load and its dependent operation. To produce the canonical ordering of an instruc\u00adtion sequence \nusing R registers which has L loads and (L-1) operations,2 create an ordering that consists of R loads \nfollowed by an alternating sequence of 1,-11 (op,load) pairs, followed by the remaining R-1 oper\u00adations. \nLoads are moved before operations that they had previously followed-this does not affect data dc\u00adpendences \nsince all operations depend on registers find all loads depend on melilory. The movement of the loads \nrelative to the operations will came the neces\u00adsary register assignments to change; if done systcn) a\u00ad \n2Tbere are (L 1) operations in a b]nary tree N itll L 10W1S. 257 Cycle# With Interlocks Without Interloclis \n1. load ml, rl load ml, rl 2. load m2, r2 load m2, r2 /+\\ 3, 4. add rl, r2*, r2 load load m3, m4, r3 \nr4 5. load m3, rl 6. load m4, r3   / / /+\\ 7. ml m2m3 m4 8. add rl, r3*, r3 add r2~ r3, r3 u-&#38;.-._ \nadd rl, r2, r2 add r3, r4, r4 add r2, r4, r4 Figure 2: Sample Expression Tree and Two Evaluation Sequences \nically this will not cause the register needs to increase. Since loads increase the number of registers \nin use by one, and operations decrease the number of registers in use by one, the number of registers \nin use at any point in the evaluation is equal to the number of loads performed minus the number of operations \nperformed. A canonical order evaluation, therefore, ensures that the number of registers in use will \nnever exceed R. Figure 3 gives an example expression with a stan\u00addard Sethi-Ullman (SU) instruction schedule \n[SU70], a canonical order with 3 registers, and a canonical order assuming 4 registers, (The Sethi-Ullman \norder, which is optimal with respect to register usage, orders in\u00adstructions by scheduling sub-trees \nseparately so that the sub-tree with the greatest register needs is sched\u00aduled first. ) Simply putting \nthe SU-generated instruc\u00adtions into canonical form without additional registers removes 1 pipeline interlock. \nAdding the extra register eliminates all interlocks. Note that the relative orders of loads and the relative \norder of operations is the same in all three sequences. 4,2 Adding Registers Helps As is seen in Figure \n3, adding registers sometimes helps, This follows from the observation that loads can often be shifled \nbackwards (i.e. earlier) in the in\u00adstruction sequence without affecting the outcome of the computation. \nThis shifting does not change the relative ordering of the loads with respect to one an\u00adother, or the \nrelative ordering of the operations with respect to one another-it simply shifts the loads far\u00adther from \nthe operations that use them. Shifting a load farther away allows its delay slot to be filled with an \nintervening load or operation. Minimizing the number of registers needed to eval\u00aduate an expression without \nload delays is an essential consideration. If the operations in the expression tree in Figure 3 were \nevaluated from left to right, it would be necessary to use 5 registers rather than 4 to produce a canonical \nevaluation without interlocks. It is there\u00adfore necessary to treat the problem of opt,i 1nal code generation \nas one of minimizing pipeline interlocl{s and register usage through code scheduling. 5 Optimal Algorithm \nfor Delay=l Optimal instruction scheduling and register allocation for an expression tree when Delay= \n1 can be done in time proportional to the size of the expression tree. Our DLS algorithm is a variation \nof the Sethi-Ulhnan algorithm adapted to our machine model. Both the SU algorithm and the DLS algorithm \nare driven by nlini\u00admizing the register needs for evaluating an expression. These needs are denoted as \nthe minReg of a node and refer to the minimal number of registers needed for computing the sub-tree rooted \nat that nocle without spilling. The mlnReg value of a node is simply the standard SU number, adapted \nto our load/store arclli\u00adtecture, It is calculated by the following rule. if (isLeaf(node)) { node .minReg \n= 1 ; } else { if (node left .minReg == node, right .minReg) { node .minReg = node . left .minReg+i ; \n} else { node .minReg = MAX(node left .minReg, node .right. minReg); } } The order of operations of an \nexpression tree de\u00adtermines the optimal order of the loads the loads will appear in the same relative \norder as their parents. This follows because forcing two (load,op) pairs, (li, Opi ) and (lk, opk) (assuming \nthat Opi comes before Opk), out of order would force the separation between (Ii, Opi) to be less than \nit was originally and less than the original (ik, op~) distance. This decrease in separation could cause \npipeline interlocks. The increase in (iL, op~ ) sep\u00ad # = Sethi-Ullman(3) Canonical(3) Canonical(4) 1. \nload m3, rl load m3, rl load m3, rl 2. load Im4, r2 load m4, r2 load m4, r2 3. load m5, r3 load m5, \nr3   / \\ + 4. add rl, r2*, r2 add rl, r2, r2 load m6, r4 5. load ~5, rl load m6, rl add rl, r2, r2 \n 6. load m6, r3 load ml, rl 7, add r3, rl , rl add r3, r4, r4  8. add rl, r3*, r3 load ml, r3 load m2, \nr3 + 2 /+\\ 9. add r2, r3, r3 add r2, rl, rl add r2, r4, r4 10, load ml, rl load m2, r2 add rl, r3, \nr3 /+\\ /+\\ 11. load m2, r2 add r4, r3, r3 m3 m4m5 m6 12, 13. add rl, r2*, r2 14. add r3, r2, r2  L1-I-!!E-E \nFigure 3: Expression Tree and Canonical Instruction Sequences aration may avoid some interlocks, but \nthe net effect cannot be advantageous. The goal of finding the op\u00adtimal instruction schedule and register \nusage therefore reduces to finding the optimal operation schedule and register usage. 5.1 Exceptional \nCases for Delay==l When Delay=l, exactly two trees in the given model have schedules that must always \nincur pipeline inter\u00adlocks: the tree consisting of a single node, and the tree consisting of a single \noperator and two leaf (mem\u00adory) nodes. It is trivial to verify that these must incur pipeline interlocks, \nand that the register needs for these trees are 1 and 2, respectively. 5,2 Algorithm The DLS algorithm \npresented in Figure 4 finds an in\u00adstruction schedule and register assignment that is op\u00adtimal for a given \nexpression tree. For all trees with the exception of the two just mentioned, the DLS sched\u00adule will have \nno pipeline interlocks and will use the minimal number of registers for any schedule without interlocks. \nThe number of registers needed for such a schedule is exactly one more than the minimal number of registers \nneeded to evaluate the expression without any spills (i.e. the SU minReg value of the root of the expression). \nThe DLS algorithm is a simple three-pass algorithm for finding the optimal instruction sequence and reg\u00adister \nallocation. The procedure label ( ) (left out for brevity) labels the nodes with their SU minReg values. \nProcedure order( ) finds the operation and load or\u00ad clers. Order ( ) is similar to the original Sethi-ullman \nalgorithm. Schedule ( ) then emits the instructions in canonical order. 5.3 Outline of Optimality Proof \nThe argument that the DLS algorithm creates au opti\u00admal instruction schedule and register allocation \nfollows from two observations: the number of registers to avoid interlocks must be at least minReg+ 1, \nand the canom ical order generated by the algorithm using minReg+ 1 registers does not incur pipeline \ninterlocks. To incur no load delays requires at least minReg+ 1 registers. The evaluation cannot take \nfewer than mlnReg registers by definition. If only mlnReg regis\u00adters were available, there must be a \npoint at which a just loaded register must be used in the next instruc\u00adtion, which would result in a \nload delay. This follo~vs from the fact that only loads can increase the number of registers in use, \nand thus at sonle point, a loacl must put minReg registers in use. Because only minReg reg\u00adisters are \navailable, this load must be followed by an operation on the just loaded register (if another oper\u00adation \ncould have been scheduled, it would have been to keep the number of registers in use at a minimulm). \nTherefore, more than minReg registers are needecl to avoid interlocks. Because Delay=l, it is only necessary \nto find a sin\u00adgle instruction to fill every load delay slot. l hese can be other loads or (unrelated) \noperations. Adding an\u00adother register, but keeping a Sethi-Ulln~an ordering for the operations and a canonical \norderilig for the entire instruction stream assures that there is at least one more register iive than \nthere is in an SU order. This register must have been made live by a load since the SU algorithm would \nhave at most mlnReg registers live procedure order(root : ExprNode; var opSched, loadSched : NodeList) \n{ // Sethi-Ullman Ordering if (not isLeaf(root)) { if (root.left.minReg < root .right.minReg) { order(root.right, \nopSched, loadSched); order(root.left, opSched, loadSched); } else { order(root.left, opSched, loadSched); \norder(root.right, opSched, loadSched);  } append(root, opSched); } else { append(root, loadSched); } \n} procedure schedule(opSched, loadSched : NodeList; Regs : integer) { // Canonical Ordering initialLoads \n: integer = MIN(Regs, length(loadSched)); for i = 1 to initialLoads do { // Loads First ld = popHead(loadSched); \nld.reg = getRego; gen(Load, ld.name, ld.reg);  3 while (not Empty(loadSched)) { // (Operation,Load) \nPairs = popHead(opSched); op.reg = op.right.reg; gen(op.op, op.left.reg, op.right.reg, op.reg); ld = \npopHead(loadSched); ld.reg = op.left.reg; gen(Load, ld.name, ld.reg); 1 while (not Empty(opschecl)) { \n// Remaining Operations OP OP = popHead(opSched); op.reg = op.right.reg; gen(op.op, op.left.reg, op.right.reg, \nop.reg); freeReg(op. left..reg) :  } } procedure generate(root : ExprNode; Delay : integer) { // Schedule \nInstructions label(root); // Calculate minReg values opSched = loadSched = emptyListo; // Initialize \norder(root, opSched, loadSched); // Find Load and Operation Order schedule(opSched, loadSched, root.minReg+Delay) \n; // Emit Canonical Order } Figure4: Optimal Delay Load Scheduling (DLS)Algorithnl 260 at any point. \nIn other words, the loads are at least one step ahead of the operations that can use them. Since the \ndelay slot is exactly one cycle long, and loads are at least one cycle ahead, the loads must always be \nfollowed by an unrelated instruction. This means no pipeline interlocks occur for any expression tree \nwith at least 3 operands (i.e. except for the two previously mentioned exceptions). Our DLS algorithm \ncreates an operation order that can be evaluated using exactly minReg registers be\u00adcause it creates a \nSethi-Ullman ordering for the opera\u00adtions and loads. Because it schedules the instructions in canonical \norder using a single extra register, it must satisfy the necessary conditions specified in the previ\u00ad \nous paragraph. Therefore, given a single extra register, the algorithm creates an instruction sequence \noptimal in evaluation time and register usage.  Optimal Spilling For the DLS algorithm to be practical, \nit must also be able to produce good schedules when fewer than minReg+ 1 registers are available for \nallocation. Sup\u00adpose, for example, that exactly minReg registers are available, Should the algorithm \nintroduce spill code so that sub-trees may be computed without interlocks? If so, where should the spills \nbe introduced? If not, lvill the computation incur excessively many pipeline interlocks? The best solution \ndepends on the form of tile expression tree. The tree in Figure 5 can be best handled by allowing the \ncanonical execution order (with minReg=4 regis\u00adters) and incurring a single load delay of 1 cycle. Hav\u00ading \nno interlocks would have cost 2 instructions, a store and load. The tree in Figure 6 will incur 3 delay \ncy\u00adcles, but would incur only the cost of a single load and store if a spill were introduced. Ordering \ncode so that spill/delay costs will be min\u00adimized requires extending the original DLS algorithm. Given \nan expression with two sub-trees that have iden\u00adtical minReg values, the algorithm orders the sub-trees \nsuch that the sub-tree exerting the minimal register pressure is scheduled last. The register pressure \nof a sub-tree is relative to the min~eg of that sub-tree; it is simply a count of the times that minReg \nregisters will be live in a, normal SU evaluation order of the sub\u00adtree. Register pressure is calculated \nby the routine in Figure 7. The sub-trees of the tree in Figure 8 are labeled with their register pressures. \nThe register pressure of the left sub-tree is 3 because its mi.nReg value is 2, and in its evaluation \n2 registers will be live 3 times after loading m3 (or m4), m2, and ml. The register pressure of the root \nnode is only 1, however, because minReg=3 and the left sub-tree will be evaluated first (without ever \nhaving 3 registers live in a normal SU order), and the right sub-tree will be evaluated second, reaching \n3 live registers only once after m5 (or m6). Therefore, the optimal evaluation of the tree (given 3 registers) \nwill incur only one load delay by scheduling the left sub-tree before the right. Had the right been scheduled \nfirst, 3 load delays would have occurred. The decision to spill a node is made when calcu\u00adlating the \nminReg and pressure values. A node is spilled if its minReg value is equal to the number of available \nregisters and its pressure is greater than 2 (the cost of a store and load). If a node has a minReg value \ngreater than the number of available registers, then its child with the greater pressure should be spilled. \nSpilling information is calculated bottolu-up ill the tree while calculating mi.nReg and pressure, lhc \nalgorithm avoids spilling until absolu t,e]y necessary OT until it is advantageous, if ((node .minReg \n== Registers and node .pressure > 2) or node .minReg > Registers) { if (node .left. pressure > node .right \npressure) { // Spill node .left ; // Make node .left a Leaf temporary; // Set node .left pressure = 1 \n; // Set node .left .minReg = 1 ; // Set node. pressure = node. right pressure; // Set node .minReg = \nnode .right .minReg; } else { // Spill node. right, etc. } }  Loads introduced by spills will not cause \npipe]inc in\u00adterlocks because they will occur only at a nocle whose sibling has a minReg value of at least \nRegisters-i. This ensures that the load will be part of a tree Jvhose root has minReg of at least Registers-1. \nSince this new leaf (spill) node has a mi.nReg of 1 and a register pressure of 1, it cannot increase \nthe minReg or regis\u00adter pressure of the entire tree. The cost o; the s~]ill is restricted to the cost \nof the store/loacl, and the in\u00adterlocks associated with evaluating the sub-tree bclofv the spilled node \n(which cannot be greater than 2). T DAGs and Forests l$%en code is generated or scheduled for an entire \nbasic block it is natural to treat the IR as a DAG becal)se of common sub-expressions. Since optimal \ncocle schedul\u00ading for DAGs is NP-complete, we prefer to schedule trees. By splzttzng nodes of a DAG, \nit is possible to treat a DAG as an ordered forest of trees. A DA(1 is split by computing shared internal \nnodes to temporary storage prior to computing ancestor nodes. Proceecl\u00ad # = No Spill Interlock Spill \nNo Interlock 1. load ml, rl load ml, rl 2. load m2, r2 load m2, r2 3. load m3, r3 load m3, r3 4. load \nm4, r4 load m4, r4 5. add rl, r2, r2 add rl, r2, r2 6. load m5, rl load m5, rl 7. add r3, r4, r4 add \nr3, r4, r4 8. load m6, r3 load m6, r3 9. add r2, r4, r4 add r2, r4, r4 10. load m7, r2 load m7, r2 11, \nadd rl, r3, r3 store r4, TEMP 12. load m8, rl load m8, r4 13. add rl, r3, r3 14. add r2, rl , r] load \nTEMP, rl 15. add r3, rl, rl add r2, r4, r4 16. add r4, rl, rl add r3, r4, r4 17, add rl, r4, r4 than \nInterlocks Exanlple with 4 Registers  / \\ + /+\\ /+\\ rn7 m8 ml rnzm3 m4m5 m6 Figure 5: Spilling May \nbe More Expensive /+\\ /+\\ /+\\  / \\m /+\\ 2/ \\m /+\\ m7 m8m3 m4 #_ No Spill Interlock Spill No Interlock \n1. load m3, rl load n13, rl 2. load m4, r2 load n14, r2 3. load m2, r3 load m2, r3 4. add rl, r2, r2 \nadd rl, r2, ?2 5. load ml, rl load ml, rl 6. add r3, r2, r2 add r3, r2, r2 7. load m7, r3 load m7, r3 \n8. add rl, r2, r2 add rl, r2, r2 9. load m8, rl load m8, rl 10. store r2, TEMP 11. add r3, rl , rl load \nm6, r2 12. load m6, r3 add r3, rl, rl 13. load m5, r3 14. add rl, r3*, r3 add r2, rl, rl 15. load m5, \nrl load TEMP, r2 16. add r3, z1, rl 17. add r3, rl , rl add r2, rl, rl 18. add r2, rl, rl Figure 6: \nSpilling May Save Cycles Example with 3 Registers if (isLeaf(node)) { node.pressure = 1; } else{ if \n(node.left.minReg == node.right.minReg) { // Assumes that node with minimal pressure is scheduled last. \nnode.pressure = MIN(node,left .pressure, node.right .pressure) ; } else if (node. lef t .minReg > node \n.right. .minlteg) { // node.left will be scheduled first --Following standard SU rule. if (node.left.minReg \n== node.right .mifieg+l) { // both subtrees contribute register pressure node.pressure = node.left.pressure \n+ node,right.pressure; } else { // only the first subtree contributes register pressure node.pressure \n= node .left.pressure; } el!e{ if (node.right.minReg == node.left.minReg+i) { node.pressure = node.right.pressure \n+ node. left,pressure; } else { node.pressure = node.right.pressure; } ) } Figure 7: Computation of \nRegister Pressure /+Y /+Y /+Y ml (1) +(2) m5 (1) m6 (1) /\\ m2(1) + (1) /\\ m3 (1) m4 (1) Figure 8: Tree \nwith Register Pressure Labels 263 ing in a bottom-up fashion, evaluating a DAG then reduces to evaluating \na sequence of trees (a forest), The idea, of a canonical order for trees can be ex\u00adtended to forests \nwith corresponding benefits. If two trees are executed in sequence and have no data de\u00adpendence between \nthem, it is possible to order their operators and loads separately, and then schedule both sets of operators \nand loads together. This is done by concatenating the operator lists together and the load lists together, \nand then scheduling these lists such that ioads from the second expression are interspersed with the \noperators of the first expression, The operations of the first tree and the loads of the second tree \nare moved away from their respective loads and operations, pos\u00adsibly reducing register needs and pipeline \ninterlocks. Entire basic blocks can also be handled in this fashion. Data dependence between stores and \nloads in a basic block will limit the ability to shift, loads earlier in the schedule. The definition \nof a canonical order for a basic block requires that loads not be shifted prematurely, Treating basic \nblocks as forests of trees in this way can produce code with substantially fewer interlocks than if the \ntrees were handled separately, After splitting DAGs into trees, DLS can schedule the resulting forests \nand allocate registers efilciently. It is not necessary to resort to expensive O(nz) DAG algorithms for \nboth instruction scheduling and register allocation. 8 Behavior for Delay>l The optimality results for \nDelay=l do not directly ex\u00adtend to greater Delay values. DLS is, however, an exce~lent heuristic for \nlarger Delay s, retaining its sim\u00adplicity and linear running time. As a heuristic for cocle generation \nwith Delay>l, it is sometimes neces\u00adsary to use more than one additional register beyond the minReg count \nfor the expression. The same ar\u00adgument made for the optimality of the case Delay=l shows that the number \nof registers needed will never be greater than minReg+Delay, and that it too must be at least minReg+i, \nSo, a reasonable heuristic ap\u00adproximation for the number of registers needed for a DLS-generated operation \norder is minReg+Delay. 8.1 Non-Contiguous Operand Order\u00ad ing The Setlli-Ullman algorithm generates code \nthat is contiguous. That is, instructions generated for one sub-tree do not mix with the instructions \nfor a sib\u00adling sub-tree. The DLS algorithm does not possess this property because the loads from one \nsub-tree may be mixed with the operations from another, The al\u00adgorithm does, however, produce schedules \nthat exhibit some contiguity: the loads taken alone, and the op\u00aderations taken alone do have contiguous \norders. It is precisely this property that allows a (divicle ancl con\u00adquer approach that treats each \nsub-tree separately. It is not always possible to generate code with this contiguous operation/load property \nand still have the code be optimal with respect to pipeline interlocks ancl register usage if Delay is \ngreater than 1. Figure 9 is the snacdiest example of a tree that does not have an optimal schedule in \nwhich the operations/loads are ordered contiguously for Delay =2 the tree is labeled with the optimal \nevaluation order. Lacking the contiguous property for optimal results, it is unlikely that a linear time \nalgorithm exists for optimally scheduling trees with Delay> 1. \\Vhet,her the optimal algorithm is a polynomial-time \nor exponential\u00adtime, it will be much more expensive to run than DLS (and in practice not all that more \neffective). 8.2 Anolnaly Using DLS for Delay =2, an interesting (and surpris\u00ading) counter-intuitive result \nhas been found It is pos\u00adsible for an expression tree to have a sub-tree whose optimal (delay-free) evaluation \nrequires m ore registers than the entire tree s optimal evaluation. Figure 10 can be evaluated optimally \nwith 5 registers, ho\\vever the right sub-tree taken alone requires 6 registers for a delay-free evaluation. \n(No delay-free evaluation exists using fewer than 6 registers, and DLS will find this optimal evaluation.) \nNote also that the full t,ree has minReg=4, and Delay =2, yet it needs only 5 registers for a spill-free, \ninterlocl<.free evaluation. As Figure 10 also demonstrates, an optimal evalu\u00adation does not always need \nminReg+Delay registers It is possible to find the minimal number of registers needed for a particular \ncombination of operation and load orders in linear time. After the orders of the loads and operations \nhas been determined, comparing the lo\u00adcation of a load within its sequence with the location of its parent \noperation within the operation sequence can be used to determine how many registers would be necessary \nto avoid an interlock on that loacl. The max\u00adimum value over all the loads is the number of registers \nnecessary for an interlock-free schedule. TVith t)his in\u00adexpensive extra step, it is possible to further \nfine-tune DLS to use even fewer registers in many cases when Delay>]. /+7  + 24) + 23) / /\\ m5 (21) \n\\ +1) L+I /+\\ (lx (15) (17) (19) + 10) + 16) /\\ /\\  / Y) / Y) /+Y )/+Y ) m7 m8 m9 m10 mll m12 m13 (Y \n(2) (3) (4) (5) (7) (9) (11) Figure 9: Non-Contiguous Optimal Evaluation for Delay =2. Y +\\ 3)m 7+\\7+\\ \nY+\\ ?+\\ 7+\\ ?+\\ (7) +  }+\\ 8)+\\ 7+\\ 7+\\ / \\ )+\\ 17+\\ 7)+\\ m2 m3m4 m5m6 m7 m8 m9 m2 m3m4 m5m6 m7 m8 \nm9 (1) (2) (3) (4) (5) (7) (9) (11) (1) (2) (3) (4) (5) (6) (8) (lo) Figure 10: Anomaly for Delay =2, \nEntire Tree Needs 5 Registers -Right Sub-tree Alone Needs 6. (The respective optimal evaluation orders \nare given.) 8.3 Heuristic Empirical Results $) Future Work: Register Vari- The DLS algorithm works extremely \nwell as a heuris-ables tic for D clay values greater than 1. By enumerating The present DLS algorithm \ncannot handle register all possible expression trees of 25 or fewer nodes, and variables (i.e. leaf nodes \nwhich do not represent loads testing the algorithm against an exhaustive search al\u00adfrom memory). DLS \ntakes full advantage of the fact gorithm, the effectiveness of the algorithm as a heuris\u00adthat (without \nregister variables) all leaf nodes have an tic can be easily verified. We ran trials for Delay s of 2 \nimplicit delay-slot and that they require that an addi\u00ad through 9 to obtain the results given in Figure \n11.3 A tional register will be in use. DLS also relies on the schedule produced by DLS is considered \nsub-optimal fact that all operations decrease the number of regis\u00ad if it uses R registers but contains \nno interlocks when ters in use by one. These assumptions allow for the there exists a non-interlocking \nschedule needing fewer simple Sethi-Ullman ordering combined with a canon\u00ad than R registers, or if it \nuses R registers and contains ical ordering to have the desired optimality properties. I interlocks when \nthere exists a schedule using R reg-With register variables, it is not always the case that isters that \ncontains fewer than I interlocks. DLS never leaf nodes require an additional register or that oper\u00ad uses \nmore than mlnReg+Delay registers and produces ations will decrease the number of registers in use. An \na non-interlocking schedule for all but a finite number operation that has two register variables for \nchildren of trees for any given Delay. will increase the number of registers in use when eval\u00ad 3 These \nresults assume the linear-time postpass mentioned in uated. gS.Z to use fewer thau minReg+Delay registers \nwhenever possible. The simple expression tree in Figure 12 denlon\u00ad n DLS DLS % DLS Delay Optimal Sub-Optimal \nTotal Optimal 3 2 1,015,481 3 1,015,481 4 1,007,509 5 1,007,509 6 1,007,511 7 1,007,535 8 1,007,703 u \n9 I 1,008,631 Figure 11: Heuristic   # +\\ ++ meml mem2 regN regM 17,930 17,930 25,902 25,902 25,900 \n25,876 25,708 24,780 Results for All # With 1. load II2. Iload 1,033,411 1,033,411 1,033,411 1,033,411 \n1,033,411 1,033,411 1,033,411 1,033,411 1 98.3 98.3 97,5 97.5 97<5 97,5 97.5 97.6 TYees of 25 or Fewer \nNodes Interlocks Without Interlocks meml, memz, rl rz load I load meml, memz, rl rz II  Figure 12: Evaluation \nOrders with Register Variables strates that a more powerful algorithm will be needed to handle register \nvariables. Normal Sethi-Ullman evaluation would label the left sub-tree with an SU\u00adnumber of 2 and evaluate \nit before the right sub-tree with a SU-number of 1. The entire tree would there\u00adfore have a SU-number \nof 2. DLS would seem to indi\u00adcate that 3 registers would be needed for a delay-free evaluation, and that \nthe left sub-tree s operator would be scheduled before the right s (a Sethi-Ullman order. ing). Putting \nthe operators and loads into a DLS-like canonical order gives an evalution order in which an interlock \nwill occur. Had the right sub-tree s opera\u00adtor been scheduled before the left s, no such interlock would \nhave occurred. 10 Conclusion The DLS algorithm presented performs optimal code scheduling and optimal \nregister allocation in linear time for expression trees with load delays and instruc\u00adt ion cycles of \n1. The algorithm can be modified to predict optimal locations for register spilling. Unlike other code \nscheduling algorithms, it does not suffer from phase-ordering problems with register allocation. It performs \nas an excellent heuristic for load delays greater than 1 and can be readily extended to han\u00addle tree \nforests derived from DAGs. As a heuristic it retains its coordination of register allocation and code \nscheduling without sacrificing its run-time efficiency or conceptual simplicity.   References [BJR89] \nDavid Bernstein, Jeffrey l! I Jaffe, ancl Michael Rodeh. Scheduling arithmetic and load operations in \nparallel with no spilling, SIAM Journal on Computing, 18(6): 1098\u00ad1127, December 1989, [BPR84] David \nBernstein, Ron Y. Pinter, and Michael Rodeh. Optimal scheduling of arithmetic operations in parallel \nwith mem\u00adory access. In Proceedings of the 12?tlt Am nua[ Sympos2um on Principles of Prograna\u00adming Languages, \npages 325 333, January 1984. [Cof76] E. G. Coffman, Jr., editor. Computer a~td Job-Shop Scheduling Theorg. \nJohn Wiley and Sons, 1976. [GJ79] M. R. Garey and D. S. Johnson. Computers and Iwtractabiliiy: A Guzde \nio the Tlzeor~ of NP-Completeness. W. H. Freeman and Company, 1979. [GM86] Phillip B. Gibbons and Steven \nS. Mucll\u00adnick. Efficient instruction scheduling for a pipelined architecture. In Proceed~ngs of 266 [HG82] \n[HG83] [Hu61] [LLM+87] [PH90] [PS90] [SU70] [Wai-90] the SIGPLAN 86 Symposium on Compiler Construction, \npages 11-16, 1986. John L. Hennessy and Thomas R. Gross. Code generation and reorganization in the presence \nof pipeline constraints. In Proceed\u00adings of the 9th Annual Symposium on Prin\u00adciples of Programming Languages, \npages 120-127, 1982. John L. Hennessy and Thomas R. Gross. Postpass code optimization of pipeline con\u00adst \nraints. ACM Transactions on Program\u00adming Languages and Systems, 5(3):422 448, July 1983. T. C. Hu. Parallel \nsequencing and assem\u00adbly line problems. Operations Research, 9(6):841-848, 1961. Eugene Lawler, Jan Karel \nLenstra, Charles Martel, Barbara Simons, and Larry Stock\u00admeyer. Pipeline scheduling: A survey. Computer \nscience research report, IBM Re\u00adsearch Division, 1987. David A. Patterson and John L. Hen\u00adnessy. Computer \nArchitecture: A Quanti\u00adtative Approach. Morgan Kaufmann Pub\u00adlishers, Palo Alto, California, 1990. Krishna \nPalem and Barbara Simons. Scheduling time-critical instructions on RISC machines. In Proceedings of the \n17fh Annual Symposium on Principles of Pro\u00adgramming Languages, pages 270-280, 1990. Ravi Sethi and J. \nD. Unman. The genera\u00adtion of optimal code for arithmetic expres\u00adsions. Journal of the ACM, 17(4):715 \n728, October 1970. H. S. Warren, Jr. Instruction scheduling for the IBM RISC system/6000 processor. IBM \nJournal of Research and Development, 34(1):85-92, 1990.  \n\t\t\t", "proc_id": "113445", "abstract": "", "authors": [{"name": "Todd A. Proebsting", "author_profile_id": "81100592757", "affiliation": "University of Wisconsin-Madison, Dept. of Computer Sciences, 1210  W. Dayton St., Madison, WI", "person_id": "P283229", "email_address": "", "orcid_id": ""}, {"name": "Charles N. Fischer", "author_profile_id": "81100312451", "affiliation": "University of Wisconsin-Madison, Dept. of Computer Sciences, 1210  W. Dayton St., Madison, WI", "person_id": "P43394", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/113445.113467", "year": "1991", "article_id": "113467", "conference": "PLDI", "title": "Linear-time, optimal code scheduling for delayed-load architectures", "url": "http://dl.acm.org/citation.cfm?id=113467"}