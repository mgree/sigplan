{"article_publication_date": "05-01-1991", "fulltext": "\n A Data Locality Optimizing Algorithm Michael E. Wolf and Monica S. Lam Computer Systems Laboratory Stanford \nUniversity, CA 94305 Abstract This paper proposes an algorithm that improves the local\u00adity of a loop \nnest by transforming the code via interchange, reversal, skewing and tiling. The loop transformation \nrrl\u00adgorithm is based on two concepts: a mathematical for\u00admulation of reuse and locality, and a loop transformation \ntheory that unifies the various transforms as unimodular matrix tmnsfonnations. The algorithm haa been \nimplemented in the SUIF (Stan\u00adford University Intermediate Format) compiler, and is suc\u00adcessful in optimizing \ncodes such as matrix multiplica\u00adtion, successive over-relaxation (SOR), LU decomposition without pivoting, \nand Givens QR factorization. Perfor\u00admance evaluation indicates that locatity optimization is es\u00adpecially \ncrucial for scaling up the performance of parallel code.  Introduction As processor speed continues \nto increase faster than me\u00admory speed, optimization to use the memory hierarchy efficiently become ever \nmore important. Blocking [9] or tiling [18] is a well-known technique that improves the data locality \nof numerical algorithms [1, 6, 7, 12, 13]. Tiling can be used for different levels of memory hierarchy \nsuch as physical memory, caches and registers; multi-level tiling can be used to achieve locality in \nmultiple levels of the memory hierarchy simultaneously. To illustrate the importance of tiling, consider \nthe ex\u00adample of matrix muhiplicatiorx for 11 :=1 ton for 12 :=1 ton for 13 :=1 ton ~-is research was \nsupported in part by DARPA contract NOOO14-87-K\u00ad 0828. Permission to copy without fee all or part of \nthis material is granted provided that the copies are not made or distributed for direct commercial advsntoge, \nthe ACM copyright notice and the title of the publication and its date appear, and notice is given that \ncopying is by permission of the Association for Computing Machinery. To copy otherwise, or to reoublish, \nreauires a fee and/or s~ecific i)ermission. @ l991 ACM ()-89791 -4~8-7/91 /0005/0030$1... .50 Proceedings \nof the ACM SIGPLAN 91 Conference on Programming Language Design and Implementation. Toronto, Ontario, \nCanada, June 26-28, 1991 C[11,13] += A[~I,~z] * B[~2,~I+]; In this code, although the same row of C and \nB are reused in the next iteration of the middle and outer loop, respec\u00adtively, the large volume of data \nused in the intervening iterations may replace the data from the register file or the cache before it \ncan be reused. Tiling reordem the execu\u00adtion sequence such that iterations from loops of the outer dimensions \nare exeeuted before completing all the itera\u00adtions of the inner loop. The tiled matrix multiplication \nis for 112 :=ltonbys for 113 :=ltonbys for II :=1 ton for 12 := 112 to rain(llz+s-l, n) for 13 := 113 \nto min(lls+s 1, n) C[11,~3] += A[~l,~z] * B[~z,~3]; Tiling reduces the number of intervening iterations \nand thus data fetched Wsveen data reuses. This allows reused data to still be in the cache or register \nfile, and hence reduces memory accesses. The tile size s can be chosen to allow the maximum reuse for \na specitlc level of memory hierarchy. The improvement obtained from tiling can be far greater than from \ntraditional compiler optimization, Figure 1 shows the performance of 500 x 500 matrix multiplica\u00adtion \non an SGI 4D/380 machine, The SGI 4DD80 has eight MIPS/R3000 processors running at 33 Mhz. Each processor \nhm a 64 KB direet-mapped tirst-level cache and a 256 KB direct-mapped seeond-level cache. We ran four \ndifferent experiments: without tiling, tiling to reuse data in caches, tiling to reuse data in registers \n[5], and tiling for both register and caches. For cache tiling, the data me copied into consecutive locations \nto avoid cache interfer\u00adence [12]. Tiling improves the performance on a single processor by a factor \nof 2.75. The effect of tiling on multiple pro\u00adcessors is even more significant since it not only reduces \nthe average data access latency but also the required me\u00admory bandwidth. Without cache tiling, contention \nover the 1I 60. - both tiling 55. - A o oachetiling register tiling 50 - + no tiling 45 .\u00ad 40 .\u00ad 35 \n.\u00ad 30 - 25 .\u00ad 20 .\u00ad 15 .\u00ad 10 .\u00ad 5 \u00ad o~ 012 Processors x 64 iterations and register tiles are 4 x 2. \n Figure 1: Performance of 500x 500 double precision ma\u00adtrix multiplication on the SGI 4D/380. Cache tiles \nare 64 memory bus limits the speedup to about 4.5 times. Cache tiling permits speedups of over seven \nfor eight processors, achieving an impressive speed of 64 MFLOPS when com\u00adbined with register tiling. \n1.1 The Problem The problem addressed in this paper is the use of loop tmnsformations such as interchange, \nskewing and reversal to improve the locality of a loop nest. Matrix multipli\u00adcation is a particularly \nsimple example because it is both legal and advantageous.! to tile the entire nest. In general, it is \nnot atways possible to tile the entire loop nest. Some loop nests may not be tilable. Sometimes it is \nn~essary to apply transformations such as interchange, skewing and reversal to produce a set of loops \nthat are both tilable and advantageous to tile. For example, consider the example of an abstraction of \nhyperbolic PDE code in Figure 2(a). Suppose the array in this example is larger than the memory hierarchy \nlevel of interesq the entire array must be fetched anew for each iteration of the outermost loop. Due \nto dependence, the loops must lirst be skewed before they can be tiled. This is also equivalent to finding \nnon-rectangular tiles. Figure 2 contains the entire derivation of the tiled code, which we will use to \nillustrate our locality algorithm in the rest of the paper. There are two major representations used \nin loop trans\u00adformations: distance vectors and direction vectors [2, 17]. Loops whose dependence can \nbe summarized by distance vectors are special in that it is advantageous, possible and easy to tile all \nloops [10, 15]. General loop nests, whose dependence are represented by direction vectors, may not be \ntilable in their entirety. The data locality problem ad\u00addressed in this paper is to find the best combination \nof loop interchanges, skewing, reversal and tiling that max\u00adimizes the data locality within loop nests, \nsubject to the constraints of direction and distance vectors. Research has been performed on both the \nlegality and the desirability of loop transformations with respect to data locality. Early research on \noptimizing loops with direction vectors concentrated on the legality of pairwise transfor\u00admations, such \nas when it is legal to interchange a pair of loops. However, in general, it is necessary to apply a series \nof primitive transformations to achieve goals such as par\u00adallelism and data locality. This has led to \nwork on combi\u00adnations of primitive transforms. For example, Wolfe [18] shows how to determine when a \nlmp nest can be tild, two-dimensional tiling can be achieved via a pair of trans\u00adformations known as \nstrip-mine and interchange [14] or unroll and jam [51. Wolfe also shows that skewing can make a pair \nof loops tilable. Banerjee discusses gen\u00aderal unimodular transforms for two-deep loop nests [4]. A technique \nused in practice to handle general n-dimensional loop nests is to determine a priori the sequence of \nloop transforms to attempt. This technique is inadequate be\u00adcause certain transformations, such as loop \nskewing, may not improve code, but may enable other optimization that do so. Which of these to perform \nwill depend on which other optimization will be enabled: the desirability of a transformation cannot \nbe evaluated locally. Furthermore, the correct ordering of optimization are highly program dependent. \nOn the desirability of tiling, previous work concentrated on how to determine the cache performance \nand tune the loop parameters jlv a given loop nest. Portertield gives an algorithm for estimating the \nhit rate of a fully-associative LRU (least recently used replacement policy) cache of a given size [14]. \nGannon et al. uses reference windows to determine the minimum memory locations necessary to maximize \nreuse in a loop nest [8]. These evaluation func\u00ad tions are useful for comparing the locatity performance \naf\u00ad ter applying transformations, but do not suggest the trans\u00ad formations to apply when a series of \ntransformations may first need to be applied before tiling becomes feasible and useful. If we were to \nuse these evaluation functions to find the suitable transformations, we would need to search the transformation \nspace exhaustively. The previously pro\u00ad posed method of enumerating the all possible combina\u00ad tions of \nlegal tmnsformations is expensive and not even possible if there are infinitely many combinations, as \nis the case when we include skewing. (a): Extract dependence information 1,+ -1 for 11 :=oto5do for \n12 := Oto6do A[12 +1] := 1/3 * (A[&#38;] + A[12 +11 + A[~2 +21); D = {(0,1),(1,0),(1, 1)}. (b): Extract \nlocality information Uniformly generated set= {A IIzI A[12 reuse category reuse veetor space self-temporal \nSpan{ (l, o)} self-spatial Span{ (l, o), (o,l)} group Span{ (l, o), (o,l)} Loops carrying reuse = {11, \n12}. + 11 A[12 potential +21 }. reuse factor s 1 3 12 (c): Search transformation localized space Span{ \n(o, 1)} Span{ (l, o)} Span{(l, o), (o,l)} space transformation 10 = 01 not possible T= ;: sources of \nlocality self-spatial, group self-temporal, self-spatial, group accesses per iteration 1/1 1/(1s) The \nbest legal choice is to tile both 11 and 12. (d): Skew to make inner loop nest fully permutable ~, + \nT=:;[1 D = TD = {(0,1),(1,1),(1,0)} (e): Final code Figure 2: Example of the locality optimization algorithm \non a hyperbolic PDE-style loop nest. 32 1.2 An Overview the space of transformed code into equivalence \nclasses, This paper focuses on maximizing data locality at the cache level. Although the basic principles \nin memory hi\u00aderarchy optimization are similar for all levels, each level has slightly different characteristics, \nrequiring slightly dif\u00adferent considerations. Caches usually have small set as\u00adsociativity, so cache \ndata conflicts can cause desired data to be replaced. We have found that the performance of tiled cork? \nfluctuates dramatically with the size of the data matrix, due to cache interference [12]. We show that \nthis effect can be mitigated by copying reused data to consecu\u00adtive locations before the computation, \nor choosing the tile size according to the matrix size. Both of these optimiza\u00adtion can be performed \nafter code transformation, and thus cache interference need not be considered at code trans\u00adformation \ntime. Another major difference between caches and registers is their capacity. To fit all the data used \nin a tile into the faster level of memory hierarchy, transformations that increase the dimensionality \nof the tile may reduce the tile size. However, as we will show, the reduction of memory accesses can \nbe a factor of Sd when d-dimensional tiles of lengths s are used. Thus for typicat cache sizes and loop \ndepths found in practice, increasing the dimensionality of the tile will reduce the total number of memory \naccess, even though the length of a tile side may be smaller for larger dimensional tiles. Thus the choice \nof tile size can be postponed until after the application of optimization to increase the dimensionality \nof locality. The problem addressed in this paper is the choice of loop transforms to increase data locality. \nWe describe a lo\u00adcality optimization algorithm that applies a combination of loop interchanges, skewing, \nreversal and tiling to improve the data locality of loop nests. The analysis is applica\u00adble to array \nreferences whose indices are afiine functions of the loop indices. The transformations are applicable \nto loops with not just distance dependence vectors, but also direction vectors as well. Our locality \noptimization is based on two results: a new transformation theory and a mathematical formulation of data \nlocality. Our loop transformation theory unifies common loop transforms including interchange, skewing, \nreversal and their combinations, as unimodular matrix transforms. This unification reduces the legality \nof all compound transfor\u00admations to satisfying the same simple constraints. In this way, a loop transformation \nproblem can be formulated as solving for the transformation matrix that maximizes an objective function, \nsubjected to a set of constraints. This matrix model has previously been applied only to distance vectors. \nWe have extended the framework to handle di\u00adrection vectors as well. The second result is a formulation \nof an objective func\u00adtion for data locality. We introduce the concepts of a reuse vector space to capture \nthe potential of data locality opti\u00admization for a given loop nest. This formulation collapses hence \nallowing pruning of the seatch for the best transfor\u00admation. Our locality algorithm uses the evaluation \nfunction and the legality constraints to reduce the search space of the transforms. Unfortunately, finding \nthe optimal transforma\u00adtion still requires a complex algorithm that is exponential in the loop nest depth. \nWe have devised a heuristic algo\u00adrithm that works well for common cases found in practice, We have implemented \nthe algorithm in the SUIF (Stan\u00adford University Intermediate Format) compiler, Our al\u00adgorithm applies \nunimodular and tiling transforms to loop nests, handles non-rectangular loop bounds, and generates non-uniform \ntiles to handle non-perfectly loop nests, It is successful in tiling numerical algorithms such as ma\u00adtrix \nmultiplication, successive over-relaxation (SOR), LU decomposition without pivoting, and Givens QR factor\u00adization. \nFor simplicity, we assume here that atl loops are perfectly nested. fltat is, all computation is nested \nin the innermost loop. In Section 2, we discuss our dependence representation and the basics of unimodular \ntransformations. How tiling takes advantage of reuse in an algorithm is discussed in Section 3. In Section \n4, we describe how to identify and evaluate reuse in a loop nest. We use those results to formulate an \nalgorithm to improve locality. Finally, we present some experimental data on tiling for a cache.  2 \nA Loop Transformation Theory While individual loop transformations are well understood, ad hoc techniques \nhave typicatly been used in combining them to achieve a particular goal. Our loop transformation theory \noffers a foundation for deriving compound transftx \u00admations efficiently [16]. We have previously shown \nthe use of this theory to maximizing the degree of parallelism in a loop nesu we will demonstrate its \napplicability to data locality in this paper. 2.1 The Iteration Space In this model, a loop nest of depth \nn corresponds to a finite convex polyhedron of iteration space 2 , bounded by the loop bounds. Each iteration \nin the loop corresponds to a node in the polyhedron, and is identified by its index vector F=(P1, PZ,... \n,% ); pi is the loop index of the i loop in the nest, counting from the outermost to innermost loop. \nThe iterations are therefore executed in lexicographic order of their index vectors. That is, if p; is \nlexicographicatly greater than p;, written Z > p;, iteration p; executes after iteration p;. Our dependence \nrepresentation is a generalization of distance and direction vectors. A de~~dence vector in an n-nested \nloop is denoted by a vector d = (dl, dz, . . . , dn). Each component di is a possibly infinite range \nof integers, represented by [~, ~], where &#38; e2U{-oa}, ~GZU{cm}and d~~~. A single dependence vector \nthe~fore represents a set of distance vectors, called its distance vector sefi S(J) ={(el,..., efi)lei \nG ii? and c@ ~ ei ~ ~} . Each of the distance vector defines a set of edges on pairs of nodes in the \niteration space. Iteration pl depends on iteration p;, and thus must execute after p;, if for some distance \nvector ;, fi = p; + Z By definition, since p; ~ p;, Z must therefore be lexicographically greater than \nO, or simply, lexicographically~sitive. The dependence vector d is also a distance vector if each of \nits components is a degenerate range consisting of a singleton value, that is, d~fi = ~. For short, we \nsimply denote such a range with the value itself. There are three common ranges found in practicw [1, \ncm] denoted by + , [ co, 11 denoted by , and [ cm, cm] denoted by + . They correspond to the previously \ndefined directions of < , > , and * , respectively [17]. We have extended the definition of vector operations \nto allow for ranges in each of the component. In this way, we can manipulate a combination of distances \nand directions simply as vectors. This is needed to support the matrix transform model, discussed below. \n Our model differs from the previously used model in that all our dependence vectors are represented \nas lexi\u00adcographically positive vectors. In particular, consider a strictly sequential pair of loops such \nas the one below: for 11 := Otondo for 12 := Otondo b := g(b); The dependence of this program would \npreviously be represented as ( * , * ). In our model, we repre\u00adsent them as a pair of lexicographically \npositive vectors, (O, + ), ( + , + ). The requirement that all dependence are lexicographically greatly \nsimplifies the legality tests for loop transformations. The dependence vectors define a partial order \non the nodes in the iteration space, and any topological ordering on the graph is a legal execution order, \nas atl dependence in the loop are satisfied. 2.2 Unimodttlar Loop Transformations With dependence represented \nas vectors in the iteration space, loop transformations such as interchange, skewing and reversal, can \nbe represented as matrix transformations. Let us illustrate the concept with the simple example of an \ninterchange on a loop with distances. A loop inter\u00ad change transformation maps iteration (pl, ~) to iteration \n(m,l ). In matrix notation, we can write this as 01 The elementary permutation matrix ~ o thus per- \nM  forms the loop interchange transforma~o; on &#38;e iteration space. Since a matrix tmnsformation \nT is a linear transfor\u00admation on the iteration space, T~2 T~l = T(fi ~1). Therefore, if ~ is a distance \nvector in the origirtal iteration space, then T~ is a distance vector in the transformed it\u00aderation space. \nThus in loop interchange, the dependence vector (dl, dz) is mapped into  [::1[:1=[%1 in the transformed \nspace. Therefore if the transformed dependence vector remains lexicographically positive, the interchange \nis legal. The loop reversal and skewing transform can similarly be represented as matrices [3, 4, 16]. \n(An example of skewing is shown in Figure 2(d).) These matrices are uni\u00admodular matrices, that is, they \nare square matrices with integrat components and a determimnt of one or nega\u00adtive one. Because of these \nproperties, the product of two unimodular matrices is unimodukw, and the inveme of a unimodukr matrix \nis unimodular, so that combinations of unimodular loop transformations and inverses of unimodu\u00adlar loop \ntransformations are also tmimodular loop transfor\u00admations. Under this formulation, there is a simple \nlegality test for all transforms. Theorem 2.1 . LetD be the set of distance vectors of a loop nest: A \nunimo$ular transformation T is legal if and only if dd ED: Td +6. The elegance of this theory helps reduce \nthe complex\u00adity of the implementation. Once the dependence are ex\u00adtracted, the derivation of the compound \ntransform simply consists of matrix and vector operations, After the trans\u00adformation is detemnined, a \nstraightforward algorithm ap\u00adplies the transformation to the loop bounds and derives the final code. \n3 The Localized Vector Space It is important to distinguish between reuse and localip. We say that a \ndata item is reused if the same data is used in multiple iterations in a loop nest. Thus reuse is a measure \nthat is inherent in the computation and not depmdent on the particular way the loops are written. This \nreuse may not lead to saving a memory access if intervening iterations flush the data out of the cache \nbetween uses of the data. For example, reference A [12] in Figure 3 touches dif\u00adferent data within the \ninnermost loop, but reuses the same elements across the outer loop. More precisely, the same data A [12] \nis used in iterations (11, 12),1 ~ 11 ~ n. There is reuse, but the reuse is separated by accesses to \nn 1  [::1[:1=[:1 for 11 :=1 tondo for 12 :=1 tondo f(A[ll], A[12]); Figure 3: A simple example. other \ndata, When n is large, the data is removed from the cache before it can be reused, and there is no locality, \nTherefore, a reuse does not guarantee locality, Specifically, if the innermost loop contains a large \nnum\u00adber of iterations and touches a large number of data, only the reuse within the innermost loop can \nbe exploited. We can apply a unimodular transformation to improve the amount of data reused in the innermost \nloop. However, as shown in this example, reuse can sometime occur along multiple dimensions of the iteration \nspace. To exploit multi-dimensional reuse, unimodular transformations must be coupled with tiling. 3.1 \nTiling In general, tiling transforms an n-deep loop nest into a 2n-deep loop nest where the inner n loops \nexecute a compiler-determined number of iterations. Figure 4 shows the code after tiling the example \nin Figure 2(a), using a tile size of 2 x 2. The two innermost loops execute the iterations within each \ntile, represented as 2 x 2 squares in the figure. The two outer loops, represented by the two axes in \nthe figure, execute the 12 tiles. As the outer loop nests of tiled code controls the execution of the \ntiles, we will refer to them as the controlling loops. When we say tiling, we refer to the partitioning \nof the iteration space into rectangular blocks. Non-rectangular blccks are obtained by first applying \nunimodular transformations to the iteration space and then applying tiling. Like all transformations, \nit is not always possible to tile. Loops Ii through Ij in a loop nest can be tiled if they ure fully \npermutable [11, 16]. Loops Ii through Ij in a loop nest are fully permutable if and only if all dependence \nvectors are lexicographically positive and for each de\u00adpendence vector, either (dl,... , di -1) is lexicographically \npositive, or the ith through jth components of ~ are all non-negative. For example, the components of \ndependen\u00adce in Figure 2(b) are all non-negative, and the two loops are therefore fully permutable and \ntilable. Full permutabil\u00adity is also very useful for improving parallelism [16], so parallelism and locality \nare often compatible goals. After tiling, both groups of loops, the loops within a tile and the loops \ncontrolling the tiles, remain fully per\u00admutable. For example, in Figure 4, loops 11( and 11~ can be interchanged, \nand so can 1[ and l;. By interchanging Ii and 1$, the loops II; and 14 can be trivially coalesced to \nproduce the code in Figure 2(e). This transformation has previously been known as strip-mine and interchange \n. Unroll and jam is yet another equivalent form to tiling. Since all loops within a tile are fully permutable, \nany loop in an n-dimensional tile can be chosen to be the coalesced loop. 3.2 Tiling for Locality When \nwe tile two innermost loops, we execute only a finite number of iterations in the innermost loop before \nexecuting iterations from the next loop. The tiled code for the example in Figure 3 is: for 112 :=ltonbysdo \nfor II :=1 tondo for 12 := 112 to max(n,112+s 1) do f(AIIll, A[121); We choose the tile size such that \nthe data used within the tile can be held within the cache, In this example, as long as s is smaller \nthan the cache size (in words), A [121 will still be present in the cache when it is reused. Thus tiling \nincreases the number of dimensions in which reuse can be exploited. We call the iterations that can exploit \nreuse the localized iteration space. In the example above, reuse is exploited for loops 11 and 12 only, \nso the localized iteration space includes only those loops. The depth of loop nesting and the number \nof variables accessed within a loop body are small compared to typical cache sizes. Therefore we should \nalways be able to choose suitable tile sizes such that the reused data can be stored in a cache. Since \nwe do not need the tile size to determine if reuse is possible, we abstract away the loop bounds of the \nlocalized iteration space, and characterize the localized iteration space as a localized vector space. \nThus we say that tiling the loop nest of Figure 3 results in a localized vector space of span{ (O, 1), \n(1, O)}. In general, if n is the first loop with a large bound, counting from innermost to outermost, \nthen reuse occur\u00adring within the inner n loops can be exploited. Therefore the localized vector space \nof a tiled loop is simply that of the innermost tile, whether the boundary between the controlling loops \nand the loops within the tile he coalesced or not. 4 Evaluating Reuse and Locality Since unimodular \ntransformations and tiling can modify the localized vector space, knowing where there is reuse in the \niteration space can help guide the seamh for the transformation that delivers the best locality. Also, \nto choose between alternate transformations that exploit dif\u00adferent reuses in a loop nest, we need a \nmetric to quantify locality for a specific localized iteration space. Figure 4: Iteration space and dependence \nof tiled code from Figure 2(a). 4.1 Types of Reuse the 12 loop. Reuse occurs when a reference within \na loop accesses the same data location in different iterations. We call this self-temporal reuse. Likewise, \nif a reference accesses data on the same cache line in different iterations, it is said to possess self-spatial \nreuse. Furthermore, different refer\u00adences may access the same locations. We say that there is group-temporal \nreuse if the references refer to the same location, and group-spatial reuse if they refer to the same \ncache line. Examples of each type of reuse are given be\u00adlow. Let us first consider reuse within a single \nreference. In Figure 3, the reference A [11] has self-temporal reuse in the innermost loop because it \naccesses the same element for all iterations (11, 12), where 1 < 12 ~ n. Similarly, A [12] has temporal \nreuse in the outermost loop. In both cases, the same data is reused n times; that is, the memory accesses \nare reduced to 1/n th of the original if the reuse is exploited. Besides self-temporal reuse, A [12] \nalso has self-spatial reuse in the innermost loop, since each cache line is reused 1 times, where 1 is \nthe cache line size. Likewise, A [~ ] has self-spatial locality in the outermost loop. Altogether, each \nreference has temporal reuse of a factor of n, and an additional spatial reuse of a factor of 1. Therefore, \nin either case each cache line can be reused ni times if the localized space encompasses all the reuse. \nBy definition, temporal reuse is a subset of spatial reuse; reusing the same location is trivially reusing \nthe same cache line. That is, loops carrying temporal reuse also carry spatial reuse. While the same \ndata can be reused arbitrarily many times depending on the program, the ad\u00additional factor of improvement \nfrom spatial reuse is limited to a factor of 1, where 1 is the cache line size. We now consider group \nreuse, reuse among different references. Trivially, identical references within the same loop nest will \nresult in the same cache behavior as if there had just been one reference. Now consider, for example, \nthe references A [12], A [12+1 ] and A [12+2] in Figure 2. In addition to any self reuse these references \nmight have, it is easy to see that they have a factor of three reuse in In contrast, consider the example \nin Figure 3. In this 2\u00addimensional iteration space, the iterations that use the same data between the \ntwo groups are the kth column and the kth row. As it is, only iterations near the diagonal of the space \ncan exploit locality. Furthermore, no unimodular or tiling transformation can place uses of the same \ndata close to each other. Thus, multiple references to the same array do not necessmily result in significant \nlocality. The difference betv$tx?n the A [ 12], A [12+1 ], A [ 12+2] references and the A [II], A [12] \nreferences is that the former set of references has similar array index functions, differing only in \nthe constant term. Such references are known as unijiormly generated references, The concept of uniformly \ngenerated references is also used by Gannon et al. [8] in estimating their reference windows. Definition \n4.1 Let n be the depth of a loop nest, ~nd d be the dimensions of an array A, Two references A[ f (7)] \nand A[j(i)j, where ~ and ~ are indexing functions Z -Zd, are called uniformly generated if 7(7) = Hi \n+ G j and F(i) = Hi +;~ where H is a linear tran~ormatwn and ;f and Z~ are constant vectors. Since little \nexploitable reuse exists between non\u00aduniformly generated references, we partition references in a loop \nnest into equivalence classes of references that op\u00aderate on the same array and have the same H. We call \nthese equivalence classes uniformly generated sets. In the degenemte case where a uniformly generated \nset consists of only one element, we have only self reuse for that ref\u00aderence. In the example in Figure \n2, the indexing functions of thereferences A[~z], A[~z+l] and A[1z+2] canbe written as . -l  [0 ll[; \n]+[017 [01] [;]+[ 1], [0 l][; ]+[2] respectively. These references belong to a single uniformly generated \nset with an If of [ O 1 ]. 4.2 Quantifying Reuse and Locality So far, we have discussed intuitively where \nreuse takes place. We now show how to identify and quantify reuse within an iteration space. We use vector \nspaces to repre\u00adsent the directions in which reuse is found, these are the directions we wish to include \nin the localized space. We evaluate the locality available within a loop nest using the metric of memory \naccesses per iteration of the innermost loop. A reference with no locality will result in one access \nper iteration. 4.2.1 Self-Temporal Consider the self-temporal reuse for a reference A [177+ a, Iterations \n?1 and ~ reference the same data eleme~t whenever Hz+? = Hfi +?, that is, when H(ti Z) = O. We say that \nthere is reuse in direction F when HP== 6. That is, reuse is exploited if F is included in the localized \nvector space. The solution to this equation is ker H, a vector space in 7? . We call this the selj-temporaf \nreuse vector space, R.sT. Thus, R.sT = ker H. For example, the reference C [II, J3] in the matrix mul\u00adtriplication \nin Section 1 produces: Rs~ = ker H = span{ (O, 1,0)}. Informally, we say there is self-temporal reuse \nin loop 12. Since loop 12 has n iterations, there are n reuses of C in this loop nest. Similar analysis \nshows that A [11, 121 has self-temporal reuse in the Is direction and B [12, IS ] has reuse in the 11 \ndirection. In this example nest, every reuse vector space is one\u00addimensional. In general, the reuse vector \nspace can have zero or more dimensions. If the dimensionality is zero, then R,sT = 0 and there is no \nself-temporal reuse. An ex\u00adample of a two-dimensional reuse vector space is the refer\u00adence A[ll] within \na three-deep nest with loops (11, 12, Is ). In general, if the number of iterations executed along each \ndimension of reuse is s, then each element is reused Sd~(~ST ) times. As discussed in Section 3, a reuse \nis exploited only if it occurs within the localized vector space. Thus, a reference has self-temporal \nlocality if its self-temporal reuse space R,ST and the localized space L in the code have a non\u00adnull \ninters~tion. The dimensionality of R ST fl L indicates the quantity of self-temporal reuse utilized: \nthe number of memory accesses is, simply, l/sd~@ T ~), where s is the number of iterations in each dimension. \nConsider again the matrix multiplication code. The only localized direction in the untiled code is the \ninner\u00admost loop, which is span{ (O, O, 1)}. It coincides exactly with RST (A [ 11, 12] ), resulting in \nlocality for that refer\u00adence. Similarly, the empty intersection with the reuse vec\u00adtor space of references \nB [12, IS] and C [11, 13] indicates no temporal locality for these references. In contrast, the localized \nvector space of the tiled matrix multiplication spans all the three loop directions. Trivially, there \nis a non-empty intersection with each of the reuse spaces, so self-temporal reuses are exploited for \nall references. All references within a single uniformly generated set have the same H, and thus the \nsame self-temporal reuse vector space. Therefore the derivation of the reuse vector spaces and their \nintersections with the localized space need only be performed once per uniformly generated set, not once \nper reference. The total number of memory accesses is the sum over that of each reference. For typical \nval\u00adues of tile sizes chosen for caches, the number of memory accesses will be dominated by the one with \nthe smallest values of dim(RST n L). Thus the goal of loop transforms is to maximize the dimensionality \nof reusq the exact value of the tile size s does not affect the choice of the transfor\u00admation. 4.2.2 \nSelf-Spatial Without loss of generality, we assume that data is stored in row major order. Spatial reuse \ncan occur only if accesses are made to the same row. Furthermore, the difference in the row index expression \nhas to be within the cache line size. For a reference such as A [J, 121, the memory accesses in the 12 \nloop will be reduced by a factor of 1, where 1 is the line size. However, there is no reuse for A [II, \n10*12] if the cache line is less than 10 words. For any stride k: 1 s k s 1, the potential reuse factor \nis l/k. All but the row index must be identical for a reference A [Hi + ?] to possess self-spatial reuse. \nWe let HS be H with all elements of the last row replaced by O. The self-spatial reuse vector space is \nsimply RSS = ker Hs. As discussed above, tempcmd locality is a special case of spatial locality. This \nis reflected in our mathematical formulation since kerH c kerH5. If RSS n L = RST rl L, the reuse occurs \non the same word, the spatial reuse is also temporal, and there is no additional gain due to the prefetching \nof cache lines. If RSS n L # R,sT rl L, however, different elements in the same row are reused. If the \ntransformed array reference is of stride one, all the data in the cache line is also reused, resulting \nin a total of l/(lsdk(RsT Lj) memory accesses per iteration. If the stride is k <1, the number of accesses \nper iteration is k/(lSdti(RSTnLJ ). As an example, the locality for the original and fully tiled matrix \nmultiplication is tabulated in Table 1.  4.2.3 Group-Temporal To illustrate the analysis on group-temporal \nreuse, we use the code for a single SOR relaxation step: for II :=1 tondo for 12 :=1 tondo A[ll,lz] := \n0.2* (A[ll,lz I+A[ll+l,lz I +AIII 1,12 ]+A[11,12 +1] +A[ll, lz l]); There are five distinct references \nin this innermost loop, all to the same array. The reuse between these references, can potentially reduce \nthe number of accesses by a factor of 5. If all temporal and spatial reuses are exploited, the total \nnumber of memory accesses per iteration is reduced from 5 to 1/1, where 1 is the cache line size. With \nthe way the nest is currently written, the localized space con\u00ad sists of only the 12 direction. The reference \nA [11,12 1] uses the same data as A [11, 12] from the previous itera\u00ad tion, and A [11, 12+1] from the \nsecond previous iteration. However, A [11 1, 12] and A [11 + 1,121 must neces\u00ad sarily each access a \ndifferent set of data. The number of accesses per iteration is thus 3/1. We now show how to mathematically \ncalculate and factor in this group-temporal reuse. As discussed above, group-temporal reuse need only \nbe calculated for elements within the same uniformly generated set. Two distinct references A [Hi+ &#38; \n] and A [Hi + E..] have group temporal reuse within a localized space L if and only if 3?7EL:HF=Z1-Z2. \nTo determine whether such an Fexists, we solve the system of equations HP= Z1 ?2 to get a particular \nsolution 7P, if one exists. The general solution is ker H + FP, and so there exists a F satisfying the \nabove equation if and only if (span{ F,} +ker H) flL #ker Hfl L. In the SOR example above, H is the identity \nmatrix and ker H = 0. Thus there is group reuse between two references in L if and only if rj = 131 132E \nL. When the inner loop is 12, there is reuse between A [11, 12 1] and A[llrlz], since (0,-1) -(0,0) E \nspar{(o, l)}. Reuse does not exist between A [11,12 11 and A [11 + 1, 12], since (0,-1) -(1,0) # Spax{(o,l)}. \nIn fact, the references fall into three equivalence classes: {A[1,,12-11, A[lI,lZI,A[lI,lZ +lI} {A[lI+l,lz]} \n{A[lI,lz]} Reuse exists and only exists between all pairs of references within each class. Thus, the \neffective number of memory references is simply the number of equivalence classes. In this case, there \nare three references instead of five, as expected. In general, the vector space in which there is any \ngroup\u00adtemporal reuse RGT for a uniformly generated set with references {A [H;+ cl] ,.. . A [Hi + cl 1 \n} is defined to be RGT = span{ i!z,... ,P~} +ker H where fork =2,..., g, ;~ is a particular solution \nof H?=~l ~k. For a particular localized space L, we get an additional benefit due to group reuse if and \nonly if RGT (l L # R.sT n L. To determine the benefit, we must partition the references into equivalence \nclasses as shown above. Denoting the number of equivalence classes by gZ , the number of memory references \nfor the uniformly generated set per iteration is 92 , instead of g.  4.2.4 Group-Spatial Finally, there \nmay also be group-spatial reuse. For exam\u00ad ple, in the following loop nest for II :=1 tondo ~(A[ll, 01, \nA[ll,ll); the two references refer to the same cache line in each iteration. Following a similar analysis \nas above, the group-spatial vector space RGS for a uniformly generated set with ref\u00ad erences {A [ H7+ \nc;] ,...3[H~+ &#38;l}isdefied @be RG,s = span{ ~z,... ,?~} + ker.HS where HS is H with all elements of \nthe last row replaced by Oandfork =2,..., n, 7~ is a particular solution of H$= ~s,~ &#38;# where ~S,i \ndenotes ~i with the last row set to O. The relationships between the various reuse vector spaces are \ntherefore RST C Rs,s, RGT c RGS Reference Reuse Untiled (L = span{Z3 j) Tiled (L = span{.?l, Z ,&#38;}) \nRST Rss RST fl L RSS fI L cost R,STfl L RSS n L cost A[11,12] span{z3 } Span{Fz, l? } span{z3 } span{F3 \n} 1/s span{e?3} Sp~{.?Z, Z 3} 1/(1s) B[lZ,13] Span{zl } span{ zl, E3} 0 span {Z3 } 1/1 Span{? 1 } span{ \nFl, z3} 1/(1s) C[11,13] span{? } Sp~{;Z,F3} 0 Sprm{a} 1/1 Span{?z } sp~{~z,~s} 1/(1s) Table 1: Self-temporal \nand self-spatial locality in tiled and untiled matrix multiplication. We use the vectors .?1, ?2 and \n73 to represent (1, O, O), (O, 1, O) and (O, O, 1) respectively. Two references with index functions \nHi+ &#38; and H;+ c; belong to the same group-spatial equivalence class if and only if 3~E L ~Hs?= ~5,i \n 25,j. We denote the number of equivalence sets by gs. Thus, g~ ~ gT ~ g, where g is the number of elements \nin the uniformly generated set. Using a probabilistic argument, the effective number of accesses per \niteration for stride one accesses, with a line size 1, is 9S + (9T 9S)/~.  4.2.5 Combining Reuses The \nunion of the group-spatial reuse vector spaces of each uniformly generated set captures the entire space \nin which there is reuse within a loop nest. If we can find a trans\u00adformation that can generate a localized \nspace that encom\u00adpasses these spaces, then all the reuses will be exploited. Unfortunately, due to dependence \nconstraints, this may not be possible. In that case, the locality optimization problem is to find the \ntransform that delivers the fewest memory accesses per iteration. From the discussion above, the memory \naccesses per iteration for a particular transformed nest can be calculated as follows. The total number \nof memory accesses is the sum of the accesses for each uniformly generated set. The general formula for \nthe number of accesses per iteration for a uniformly generated set, given a localized space L and line \nsize 1, is 9S t (9T !7S)/~ /e~dim(R,snL) where O RSTnL=RSSIIL e= 1 otherwise. { We now study the full \nexample in Figure 2(a) to il\u00adlustrate the reuse and locality analysis. The reuse vector spaces of the \ncode are summarized in Figure 2(b). For the uniformly generated set in the example, H=[O l], H.s= [() \no]. The reuse vector spaces are R,sT = ker H = span{ (l, O)} RSS = ker Hs = span{ (O, 1), (1,0)} RGT \n= span{ (O, 1)} +ker H = span{ (o, 1), (1,0)} RGS = span{ (O, 1)} +kerHS = span{ (O, 1), (1,0)}. When \nthe localized space L is span{ (O, l)}, RST17L = 0 RSS n L = span{ (O,1)} # RST RGT (l L = span{ (O, \n1)} &#38;S f_IL = span{ (O, 1)} = RGT. Since g~ = 1, the total number of memory accesses per it.mtion \nis 1/1. Similar derivation shows the overall num\u00adber of accesses per iteration for the localized space \nof Span{ (l, o), (o, 1)} to be 1/(1s). The data locality optimization problem can now be for\u00admulated \nas follows: Definition 4.2 For a given iteration space with 1. a set of dependence vectors, and 2. unformly \ngenerated reference sets  the data locality optimization problem is to fmd the uni\u00admodular andlor tiling \ntransform, subject to data depen\u00addence, that minimizes the number of memory accesses per iteration. \n 5 An Algorithm Our analysis of locality shows that differently transformed loops can differ in locality \nonly if they have different lo\u00adcalized vector spaces. That is, all transformations that generate code \nwith the same Iodized vector space can be put in an equivalence class and need not be examined individually. \nThe only feature of interest is the innermost til~ the outer loops can be executed in any legal order \nor orientation. Similarly, reordering or skewing the tiled loops themselves does not affect the vector \nspace and thus need not be considered. 39 From the reuse analysis, we can identify a subspace that is \ndesirable to make into the innermost tile. The question of whether there exists a unimodular transformation \nthat is legal and creates such an innermost subspace is a difficult one. An existing atgorithm that attempts \nto find such a transform is exponential in the number of loops [151. The general question of finding \na legal transformation that min\u00adimizes the number of memory accesses as determined by the intersection \nof the localized and reused vector spaces is even harder. Although tie problem is theoretically difficult, \nloop nests found in practice are generally simple, Using charac\u00adteristics of programs as a guide, we \nsimplify this problem by (1) reducing the set of equivalent classes, and (2) using a heuristic algorithm \nfor finding transforms. 5.1 Loops Carrying Reuse Although reuse vector spaces can theoretically be spanned \nby arbitrary vectors, in practice they are typically spanned by a subset of the elementary basis of the \niteration space, that is, by a a subset of the loop axes. In the code below, the array A has self-temporal \nreuse in the vector space spanned by (O, O, 1); we say that the loop 13 carries reuse. On the other hand, \nthe B array has a self-temporal reuse in the (1, 1, O) direction of the iteration space, which dees \nnot correspond to either loop 11 or 12 but rather a combi\u00adnation of them. This latter situation is not \nas common. Instead of using an arbitrary reuse vector space directly to guide the transformation process, \nwe use its smallest enclosing space spanned by the elementary vectors of the iteration space. For example, \nthe reuse vector space of B[ll+lz,~g] is spanned by (l, O,O)and(O,l, O), thedi\u00adrections of lcmps 11 and \n12 respectively. If we succeed in making the innermost tile include both of these directions, we will \nexploit the self-temporal reuse of the reference. In\u00adformally, this procedure reduces the arbitrary vector \nspaces to a set of loops that carry reuse. With this simplification, a leQp in the source program either \ncarries reuse or it does not. We partition all trans\u00adformations into equivalence classes according to \nthe set of source loop directions included in the localized vector space. For example, both loQps in \nFigure 2(a) carry reuse. Transformations are classified depending on whether the transformed innermost \ntile contains only the first loop, the second or both. 5.2 An Algorithm to Improve Locality We first \nprune the search by finding those loop directions that need not or cannot be included in the localized \nvector space. These loops include those that carry no reuse and can be placed outermost legatly, and \nthose that carry reuse but must be placed outermost due to legality reasons. For example, if a three-deep \nloop has dependence {(1, + , + ), (0,1, + ),(0,0, + )} and carry locality in all three loops, there \nis no need to try all the different transformations when clearly only one loop ordering is legal. On \nthe remaining loops Z, we examine every subset 1 of Z that contains at least some reuse. For each set \n1, we try to find a legal tmnsformation such that the loops in 1 are tiled innermost. Among all the legal \ntransformations, we select the one with the minimal memory accesses per iteration. Because the power \nset of Z is explored, this al\u00adgorithm is exponential in the number of loops in Z. How\u00adever, Z, a subset \nof the origimd imps, is typically smalt in practice. There are two steps in finding a transformation \nthat makes loops 1 innermost. We tirst attempt to order the outer loops the loops in Z but not in 1. \nAny trans\u00adformation applied to the loops in T -1 that result in these loops being outermost and no dependence \nbeing violated by these loops is sufficient [16]. If that step suc\u00adceeds, then we attempt to tile the \n1 loops innermost, which means finding a transformation that turns these loops into a fully permutable \nloop nest, given the outer nest. Solving these problems exactly is still exponential in the number of \ndependence. We have develo~d a heuristic compound transformation, known as the SRP transform, which is \nuse\u00adful in both steps of the transformation algorithm. The SRP transformation attempts to make a set \nof loops fully permutable by applying combinations of permutation, skewing and reversal [16]. If it cannot \nplace all loops in a single fully permutable nest, it simply finds the outermost nest, and returns all \nremaining loops and dependence left to be made lexicographically positive. The atgorithm is based upon \nthe observations in Theorem 5.1 and Corolla ry 5.2. Theorem 5.1 Let N = {11,... , In } be a loop nest \nwith le~icographically positive dependence ~ E D, and Di = {d EDl(dl,..., dj_l ) ~ 6}. Loop Ij can be \nmade into a fully permutable nest with loop Ii, where i < j, via reversal andlor skewing, ZJ Proofi All \ndependence vectors for which (dI,..., di_l) + 6 do not prevent loops Ii and Ij from being fully permutable \nand can be ignored. If then we can skew loop Ij by a factor of ~ with respect to loop Ii where to make \nloop Ij fully permutable with loop Ii. If instead the condition holds, then we can reverse loop lj and \nproceed as above. D Corollary 5.2 If loop $ has dependence such that 32 c D :d$h = cm and Zldc D :~ = \nco then the outer\u00admost fully permutable nest consists only of a combination of loops not including Ik. \nThe SRP algorithm takes as input the loops N that have not been placed outside this nest, and the set \nof dependen\u00adce D that have not been satisfied by loops outside this loop nest. It first removes from \nN those serializing loops as defined by the 1~s of Corollary 5.2. It then uses an iter\u00adative step to \nbuildup the fully permutable loop nest F. In each step, it tries to find a loop from the remaining loops \nin N that can be made fully permutable with F via possi\u00adbly multiple applications of Theorem 5.1. If \nit succeeds in finding such a loop, it permutes the loop to be next outer\u00admost in the fully permutable \nnest, adding the loop to F and removing it from N. Then it repeats, searching through the remaining loops \nin N for another loop to place in F. This algorithm is known as SRP because the unimodular transformation \nit performs can be expressed as the product of a skew transformation (S), a reversal transformation@) \nand a permutation transformation (P). We use SRP in both steps of finding a transformation that makes \na loop nest 1 the innermost tile. We first ap\u00adply SRP iteratively to those loops not in 1. Each step \nthrough the SRP algorithm attempts to find the next out\u00adermost fully permutable lcmp nest, returning \nall remaining loops that cannot be made fully permutable and returning the unsatisfied dependence. We \nrepeatedly call SRP on the remaining loops until (1) SRP fails to find a single loop to place outermost, \nin which case the algorithm fails to find a legal ordering for this target innermost tile 1, or (2) there \nare no remaining loops, in which case the algo\u00adrithm succeeds. If this step succeeds, we then call SRP \nwith loops 1, and all remaining dependence to lx satis\u00adfied. In this case, we succeed only if SRP makes \nall the loops fully permutable. Let us illustrate SRP algorithm using the example in Figure 2. Suppose \nwe are trying to tile loops 11 and 12. First an outer loop must be chosen. 11 can be the outer loop, \nbecause its dependence components are all non\u00adnegative. Now loop 12 has a dependence component that is \nnegative, but it can be made non-negative by skewing with respect to 11 (Figure 2D). Loop 12 is now placed \nin the same fully permutable nest as 11; the loop nest is tilable (Figure 2(e)). In general, SRP can \nbe applied to loop nests of arbi\u00adtrarily depth where the dependence can include distances and directions. \nIn the important spex.iat case where atl the dependence in the loop nest to be ordered are lexi\u00adcographically \npositive distance vectors, the algorithm can place all the loops into a single fully permutable loop \nnest. The algorithm is 0(n2d), where n is the loop nest depth and d is the number of dependence vectors. \nWith the ex\u00adtension of a simple 2D time-cone solver[16], it becomes 0(n3 d) but can find a transformation \nthat makes any two loops fully permutable, and therefore tilable, if some such transformation exists. \nIf there is little reuse, or if data dependence constrain the legal ordering possibilities, the atgorithm \nis fast since Z is small. The atgorithm is only slow when there are many carrying reuse loops and few \ndependence. This algorithm can be further improved by ordering the search through the power set of Z \nusing a branch and bound approach.  6 Tiling Experiments We have implemented the algorithm described \nin this pa\u00adper in our SUIF compiler. The compiler currentty gener\u00adates working tiled code for one and \nmultiple processors of the SGI 4D/380. However, the scalar optimizer in our compiler has not yet been \ncompleted, and the numbers ob\u00adtained with our generated code would not reflect the true effect of tiling \nwhen the code is optimized. Fortunately, our SUIF compiler also includes a C backend; we can use the \ncompiler to generate restructured C code, which can then be compiled by a commercial scalar optimizing \ncompiler. The numbers reported below are generated as follows. We used the compiler to generate tiled \nC code for a sin\u00adgle processor. We then performed, by hand, optimization such as register allocation \nof array elements[5], moving loop-invariant address calculation code out of the inner\u00admost loop, and \nunrolling the innermost loop. We then compiled the code using the SGI S optimizing compiler. To run the \ncode on multiple processors, we adopt the model of executing the tiles in a DO-ACROSS manner[16]. This \ncode has the same structure as the sequential code. We needed to add only a few lines to the sequential \ncode to create multiple threads, and initialize, check and increment a smiitl number of counters within \nthe outer loop. 6.1 LU Decomposition The original code for LU-decomposition is: for 11 :=1 tondo for \n12:= 11+1 to ndo  25 ~- D 64x64 tiling A 32x32 tiling Q no tiling 20\u00ad- 15 .\u00ad 10 \u00ad 5 .\u00ad oo~ 12345678 \nProcessors Figure 5: Performance of 500 x 500 double precision LU factorization without pivoting on the \nSGI 4D/380. No register tiling was performed. A[lz, ll] /= A[~lr~l]; for 13:= 11+1 to ndo A[Jz,13] -= \nA[lzrll]*A[ll,13]; For comparison, we measured the performance of the orig\u00adinal untiled LU code on one, \nfour and eight processors. For this Code, we allOCated the element A [ &#38;, 11] to a register in the \ndumtion of the innermost loop, We parallelized the middle loop, and ran it self-scheduled, with 15 itemtions \nper task. (The results for larger and smaller granuku-ities are virtually identical.) Figure 5 shows \nthat we only get a speedup of approximately 2 on eight processors, even though there is plenty of available \nparallelism. This is because the memory bandwidth of the machine is not suf\u00adficient to support eight \nprocessors that are each reusing so little of the data in their respective caches. This LU loop nest \nhas a reuse vector space that spans all three loops, so our compiler tiles all the loops. The LU code \nis not perfectly nested, since the division is outside of the innermost loop. The generated code is thus \nmore complicated for112=ltonbysdo for113=ltonbysdo for 11 =1 tondo for 12 = max(~l +1,11~) to min(n,112+s \n1) do if113~11+ l~.f13+ s-l then A[lz,ll] /= A[ll,I1]; for 13 = max(ll +1,113) to min(nr113+s 1) do A[Iz,13] \n-= A[lz,ll]*A[Il,13] ; As with matrix multiplication, while tiling for cache reuse is important for one \nprocessor, it is crucial for mul\u00adtiple processors. Tiling improves the uniprocessor exe\u00adcution by about \n20Yo; more significantly, the speedup is much closer to linear in the number of processors when tiling \nis used. We ran experiments with tile sizes 32x 32 and 64x 64. The results coincide with our analysis \nthat the marginal performance gain due to increases in the tite size is low. In this example, a 32 x \n32 tile, which holds 1/4 the data of a 64 x 64 tile, performed virtually as well as the larger on a single \nprocessor. It actwlly outperformed the larger tile in the eight processor experiment, mostly due to load \nbalancing.  6.2 SOR We also ran experiments with the SOR code in Figure 7(a), which is a two-dimensional \nequivalent of the example in Figure 2(a). Figure 6 shows the performance results for three versions of \nthis nest, where t is 30 and n + 1, the size of the matrix, is 500. None of the original loops is parallelizable, \nThe tirst verson, labeled DOALL , is transformed via wavefronting so that the middle loop is a DOALL \nlwp [16] (Figure 7(b)). This transformation unfortunately destroys the original locality within the code. \nThus, performance is abysmal on a single processor, and speedup for multiple processors is equally abysmal \neven though again there is plenty of available parallelism in the 1$ loop, 401\u00ad 3D tile A 2D tile o \nDOALL middle oo~ 5678 Processors Figure 6: Behavior of 30 iterations of a 500 x 500 double precision \nSOR step on the SGI 4D/380. The tile sizes are 64 x 64 iterations. No register tiling was performed. \nThe code for the second version, labeled 2D tile , is shown in Figure 7(c). This version tiles only the \ninner\u00ad for 11 :=1 totdo for IZ := Iton ldo for 13 := lton ldo A[IZ,13] := 0,2*(A[&#38;,~3] + A[~z+l,~3] \n+ A[&#38;-l, ~3] + A[~z,~3+l] + A[~,13 ll); (b): WOALL SOR nest for 1{ := 5 to 2N 2+3t do doall I; := \nmax(l, (l[ t 2N +2)/2) to min(t, (l~ 3)/2) do for Ii : = max(l+~~,~{-2~~-n+l) to min(n-l+l~rl~-l zl~) \ndo A[14 14,1; 24 -14] := 0.2* (A[4 l; ,l; 214 14] + A[lj l~+ l,l; -21~-ljl + A[lj l~-1,1( 21~ lj] + A[lj \nl~,l( 21j 1:+1] + A[lj l~,l; 21~ lj ll); (c): 2-D Tile SOR nest for I{ :=1 totdo for II; := Iton lbysdo \nfor I; := lton ldo forIj := IIj to mh(n-l, IJ3+s -1) do AII~,Ij] := 0.2* (A[4] [I~l+AII; +l,ljl+AII; \n-l, Ijl+AII; ,Ij+ll+A[l; ,lj-ll); (d): 3-D Tile SOR nest for 11{ := 2 ton-l+ibysdo for 11~ :=2ton l+tbysdo \nfor I; :=1 totdo for I; := II. to min(n-l+t, II~+s -1) do for Ij := 11~ to min(n l+t, IIj+s 1) do A[~;,~;] \n:= 0.2* (A[4-l{][l; lf] + A[~~-l(+l,lj lf] + A[~~ ~~ 1,~~ ~;] + A[l. 1;,~j ~;+l] + A[l~ l{,lj l{ l]); \nFigure 7: Code for the different SOR versions. most two loops, The uniprocessor performance is signif-reuse \nand locality, and a matrix-based loop transformation icantly better than even the eight processor wavefronted \ntheory. performance. Thus although wavefronting may increase While previous work on evaluating locality \nestimates parallelism, it may reduce locality so much that it is better the numlxx of memory accesses \ndirectly for a given trans\u00adto use only one processor. However, the speedup of the formed code, we break \nthe evaluation into three parts, We 2D tile method is still limited because self-temporal reuse use a \nreuse vector space to capture the inherent reuse is not being exploited. within a loop nest we use a \nlocalized vector space to To exploit all the available reuse, all three lwps must capture a compound \ntransform s potential to exploit local\u00adbe included in the innermost tile. The SRP algorithm first ity; \nfinally we evaluate the locality of a transformed code skews 12 and 13 with respect to 11 to make tiling \nlegal, and by intersecting the reuse vector space with the localized then tiles to produce the code in \nFigure 7(d). This 3D tile vector space. version of the loop nest is best for a single processor, and \nThere are four reuse vector spaces: self-temporal, self\u00adalso has the best speedup in the multiprocessor \nversion. spatial, group-temporal, and group-spatial. These reuse vector spaces need to be calculated \nonly once for a given loop nest. We show that while unimochdar transformations  7 Conclusions can alter \nthe orientation of the localized vector space, tiling can increase the dimensionality of this space. \nIn this paper, we propose a complete approach to the prob\u00adlem of improving the cache performance for \nloop nests. The reuse and localized vector spaces can be used to The approach is to first transform the \ncode via interchange, prune the search for the best compound transformation, reversal, skewing and tiling, \nthen determine the tile size, and not just for evaluating the locality of a given code, taking into account \ndata conflicts due to the set associativ-First, all transforms with identical localized vector space \nity of the cache [12]. The loop transformation algorithm are equivalent with respect to locatity. In \naddition, trans\u00adis based on two concepts: a mathematical formulation of forms with different localized \nvector spaces may also be equivalent if the intersection between the localized and the reuse vector spaces \nis identical. A loop transformation algorithm need only to compare between transformations that give \ndifferent localized and reuse vector space inter\u00adsections. Unlike the stepwise transformation approach \nused in ex\u00adisting compilers, our loop transformer solves for the com\u00adpound transformation directly. This \nis made possible by our theory that unifies loop interchanges, skews and rever\u00adsals as unimodukir matrix \ntransformations on dependence vectors with either direction or distance components. The algorithm extncts \nthe dependence vectors, determines the best compound transform using locality objectives to prune the \nsearch, then transforms the loops and their loop bounds once and for all. This theory makes the implementation \nof the algorithm simple and straightforward. References [1] W. Abu-Sufah. Improving the Pe~ormance of \nVir\u00adtual Memory Computers. PhD thesis, University of Illinois at Urbana-Champaign, Nov 1978. [2] U. Banerjee. \nData dependence in ordinary programs. Technical Report 76-837, University of Illinios at Urbana-Champaign, \nNov 1976. [3] U. Banerjee. Dependence Analysis for Supercomput\u00ading. Kluwer Academic, 1988. [4] U. Banerjee. \nUnimodular transformations of double loops. In 3rd Workshop on Languages and Compilers for Parallel Computing, \nAug 1990, [5] D. Callahan, S, Carr, and K. Kennedy, Improving register allocation for subscripted variables, \nIn Pro\u00adceedings of the ACM SIGPLAN 90 Co#erence on Programming Language Design and Implementation, June \n1990. [6] J. Dongarra, J. Du Croz, S. Hammarling, and I. Duff, A set of level 3 basic linear algebra \nsubprograms. ACM Transactions on Mathematical Software, pages 1 17> March 1990. [7] K. Gallivan, W. Jrdby, \nU. Meier, and A. Sameh. The impact of hierarchical memory systems on linear al\u00adgebra algorithm design. \nTechnical repofi University of Illinios, 1987. [8] D. Gannon, W. Jalby, and K. Gallivan, Strategies for \ncache and local memory management by global program transformation. Journal of Parallel and Dis\u00adtributed \nComputing, 5:587-616, 1988. [9] G. H. Golub and C. F. Van Loan. Matrix Computa\u00adtions. Johns Hopkins University \nPress, 1989. [10] F. Irigoin and R. Triolet. Computing dependence direction vectors and dependence cones. \nTechnical Report E94, Centre lYAutomatique et Informatique, 1988. [11] F. Irigoin and R. Triolet. Supemode \npartitioning. In Proc. 15th Annual ACM SIGACT-SIGPLAN Sympo\u00adsium on Principles of Programming Languages, \nJan-Uary 1988. [12] M. S. Lam, E. E. Rothberg, and M. E. Wolf. The cache performance and optimization \nof blocked al\u00adgorithms. In Proceedings of the Sixth International Conference on Architectural Support \nfor Program\u00adming Languages and Operating Systems, April 1991. [13] A. C. McKeller and E. G. Coffman. \nThe organization of matrices and matrix operations in a paged muM\u00adprogramming environment. CACM, 12(3):153-165, \n1969. [14] A. Porterlield. Software Methods for Improvement of Cache Pe~ormance on Supercomputer Applications. \nPhD thesis, Rice University, May 1989. [15] R. Schreiber and J. Dongama. Automatic blocking of nested \nloops. 1990. [16] M. E. Wolf and M. S. Lam, A loop transforma\u00adtion theory and an algorithm to maximize \nparallelism. IEEE Transactions on Parallel and Distributed Sys\u00adtems, July 1991. [17] M. J. Wolfe. Techniques \nfor improving the in\u00adherent parallelism in programs. Technical Report UIUCDCS-R-78-929, University of \nIllinois, 1978. [18] M. J. Wolfe. More iteration space tiling. In Super\u00adcomputing 89, Nov 1989.  \n\t\t\t", "proc_id": "113445", "abstract": "", "authors": [{"name": "Michael E. Wolf", "author_profile_id": "81100334934", "affiliation": "Computer Systems Laboratory, Stanford University, CA", "person_id": "PP31100648", "email_address": "", "orcid_id": ""}, {"name": "Monica S. Lam", "author_profile_id": "81100237956", "affiliation": "Computer Systems Laboratory, Stanford University, CA", "person_id": "PP14092336", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/113445.113449", "year": "1991", "article_id": "113449", "conference": "PLDI", "title": "A data locality optimizing algorithm", "url": "http://dl.acm.org/citation.cfm?id=113449"}