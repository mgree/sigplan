{"article_publication_date": "02-01-1992", "fulltext": "\n Recognizing Substrings of LR(k) Languages in Linear Time Joseph Bates and Alon Lavie School of Computer \nScience Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, PA 15213 Abstract LR parsing techniques \nhave long been studied as efficient and powerful methods for process\u00ading cent ext free languages. A linear \ntime algo\u00adrit hm for recognizing languages representable by LR(k) grammars has long been known. Recog\u00adnizing \nsubstrings of a context-free language is at least as hard as recognizing full strings of the lan\u00adguage, \nas the latter problem easily reduces to the former. In this paper we present a linear time al\u00adgorithm \nfor recognizing substrings of LR(k) lan\u00adguages, thus showing that the substring recogni\u00adtion problem \nfor these languages is no harder than the full string recognition problem. An interest\u00ading data structure, \nthe Forest Structured Stack, allows the algorithm to tra,ck all possible parses of a substring without \nloosing the efficiency of the original LR parser. We present the algorithm, prove its correctness, analyze \nits complexity, and mention several applications that have been con\u00adstructed. 1 Introduction The problem \nof recognizing substrings of context\u00adfree languages has emerged in many practical ap\u00adplications, in the \nareas of both formal and nat u\u00adral languages. Given a string z, we wish to know whether there exists \nsome string w, such that x is a substring of w, and w is in the language of Permission to copy without \nfee all or part of tlds rnatertial is granted provided that the copies are not made or distributed for \ndirect commercial advantage, the ACM copyright notice and the title of the publication and its date appear, \nand notice is given that the copying is by permission of the Association for Computing Machinery. To \ncopy other\u00adwise, or to republish, requires a fee arrd/or specific permission. 01992 ACM 089791453-8/92/0001/0235 \n$1.50 235 a given context-free grammar G. The ability to recognize that a given string is not a substring \nof any sentence in the language allows the early and local detection of syntax errors, without the need \nto complete a full parse or compilation. Substring recognizes have been considered in several works on \nrecovery from syntax errors. Richter [Ric85] develops a formal method for reporting syntax errors, without \nattempting to correct them. His method requires a substring recognize, although no such recognize is \ndescribed in his paper. Cor\u00admack [Cor89] describes a method for construct\u00ading an LR parser that recognizes \nall substrings of a context-free grammar G. This is done by a more complicated construction of the LR \nparsing tables, appropriate for dealing with substrings. Cormack s construction provides a deterministic \nparser (free of table conflicts) for only the hounded contezt class of grammars, which is a class smaller \nthan LR( 1). Rekers and Koorn [RK91] propose a substring parsing algorithm for arbitrary context\u00ad free \ngrammars based on Tomita s generalized LR parsing algorithm [Tom86]. Although their al\u00ad gorithm has some \nsimilarities to the one pro\u00ad posed here, it is not linear, and it s correctness and complexity are not \naddressed in their paper. Substring recognizes appear to be also useful in the context of parallel and \nincremental parsing [AD83] [Ce178]. The substring recognition problem can easily be shown to be at least \nas hard as the full-string recognition problem, ily reducible to the space. Also, since context-free \nlanguage guage, the general strings is not harder as the latter problem is eas\u00adfirst in constant time \nand the set of all substrings of a is itself a context-free lan\u00adproblem of recognizing sub\u00adthan that \nof recognizing full\u00ad strings. However, the set of all substrings of an LR(k) language is not necessarily \nitself an LR(k) language, therefore a linear time bound for recog\u00ad nizing substrings of LR(k) languages \nis not trivial. In this paper we show that the substring recog\u00ad nition problem for LR(k) grammars is \nnot any harder than the full-string recognition problem. We present an algorithm for the LR(k) substring \nrecognition problem that runs in linear time, which is similar to that of the original LR parsing algorithm \n[AU72]. While previous sub string pars\u00ading algorithms such as Cormack s [Cor89] modi\u00adfied the LR parsing \ntables to accommodate for substring recognition, our algorithm modifies the parsing algorithm itself, \nwhile leaving the origi\u00adnal LR parsing tables intact. We introduce a data structure, the Forest Structured \nStack (FSS ), that keeps track of all possible parses of the substring, while preserving the efficiency \nof the original LR parsing algorithm. The SLR, canonical LR(l) and LALR parser variants differ only in \nthe algo\u00adrithms that produce the parsing tables from the grammar, and share a common LR parsing algo\u00adrit \nhm that is controlled by these tables. Since our substring algorithm replaces this run-time pars\u00ading \nalgorithm while using the parsing tables as is ), it is equally applicable to all of the above LR variants. \nThe parsing algorithm for canoni\u00adcal LR(k) grammars (k > 2) differs slightly from the other variants, \nin order to account for the ex\u00adtended lookahead into the input. Thus, a slightly different version of \nour substring algorithm han\u00addles canonical LR(k) grammars 1 Section 2 describes the FSS data structure \nand summarizes the substring recognition algorithm for LR(l) grammars. In section 3 we sketch the correctness \nof the algorithm. Section 4 analyzes the time complexity of our algorithm. An anlor\u00adtized analysis is \nused to prove that the algori :hm does indeed run in linear time. Finally, some ap\u00adplications of the \nalgorithm and our conclusions are present ed in section 5. Throughout this paper we touch only on the \nkey points of our work. In particular, only sketches of proofs are presented. An extended paper including \nthe complete proof details will be published elsewhere, and is avail\u00adable from the authors in the form \nof a technical 1Due to space limitations, the substring algorithm for canonical LR(k) grammars is not \ndescribed in this paper. For a full description of the algorithm see [BL91]. report [BL91].  2 The Algorithm \nThe substring recognition algorithm we describe in this section is denoted by SSR. It is a vari\u00adation \nof the conventional LR parsing algorithm, denoted by LRP. 2.1 The Forest Structured Stack The Forest \nStructured Stack (FSS) is a graph, consisting of a set of trees, representing a possi\u00adbly infinite set \nof stacks of LRP. The nodes of the graph are labeled by states of the LR ma\u00adchine. The edges that connect \nthe state nodes are labeled by grammar symbols. Each path from a root to a leaf corresponds to the top \nportion of an LRP stack, in which the node at the root of the path represents the state at the top of \nthe stack. The algorithm simulates the behavior of LRP on all the staclis represented in the FSS, adding \nnodes in correspondence with actions that push items on the stack (shifts), and removing nodes in correspondence \nwith stack reductions. The tree representation avoids the duplication of stacks which have an identical \ntop part but which differ in content deeper down. 2.2 An Informal Description of the Al\u00adgorithm The idea \nbehind SSR is to effectively simulate the behavior of LRP on all possible strings of which the input \nis a suffix. When parsing a string w, of which our input string z = 01Z2 . . . Xn is a suffix, LRP is \nin some state (at the top of the stack) upon shifting xl, the first symbol of z. We are interested in \nall such states and thus we initial\u00adize SSR by building a FSS with a distinct single node tree for each \nstate that can be the result of shifting ml according to the pre-compiled action table. Since each single \nnode tree represents all stacks with that state at the top, the initial FSS represents the set of all \npossible stacks after the shifting of Z1. From here on we continue the parsing of z ac\u00adcording to each \nof the FS S trees. SSR performs a series of alternating Reduce and Shift phases, one pair of phases for \neach input symbol. During a Reduce Phase, reductions are per\u00adformed on all trees whose top state indicates \nthat a reduction is to be performed. In LR parsing, reductions remove nodes from the stack. When performed \non a tree, they are done on all paths in the tree, starting at the root, to a depth corre\u00adsponding to \nthe number of symbols on the right\u00adhand side of the rule being reduced. Reductions are a problem only \nwhen they wish to remove nodes deeper than the length of some path in the FSS. This corresponds to a \nreduc\u00adtion that includes symbols derived from parsing the part of the full string that is prior to x. \nIn our algorithm, we refer to such reductions as long reductions, and treat them in a manner somewhat \nsimilar to our initialization, A reduction normally removes the right-hand side of the rule being reduced, \nand then shifts the non-terminal symbol A of the left-hand side of the rule. The new state at the top \nof the stack is determined from the goto tabie, and depends on A and on the state revealed at the top \nof the stack by the reduction. With long reductions, since only a partial stack exists, this state is \nnot known. Our algorithm determines all such possi\u00adble states by a lookup in the long reduction goto \ntable. This supplemental table specifies for each possible reduction from a state at the top of the stack, \nthe set of states that may be reached as a result of the shifting of the left-hand side non\u00adterminal \nof the rule being reduced. The table is easily constructed from the parsing tables prior to run-time. \nEach of the determined goto states corresponds to at least one full string, the pars\u00ading of which would \nhave resulted in that state being at the stack top at this point in the pars\u00ading process. It is sufficient \nat this point to add these states to the FSS as single node trees. Long reductions are performed at most \nonce per state in a Reduce Phase, since a second long reduction from the same top state would produce \nthe same new trees, and thus would be redundant 2. When the action defined by the table on the root node \nof a tree is error, the entire tree is dis\u00adcarded, These are trees that correspond to prefix We believe \nthat we can manage without the long re. duction goto table, and simply consider all states that are a \nresult of shifting A. This leads to a somewhat simpler implementation, but the proof of correctness \nappears to be more difficult in this case. strings of z that cannot be completed to strings in the language. \nA Reduce Phase terminates when the action indicated by the table, on each of the tree root nodes, is \nto shift the next input symbol, All the shift operations are done in the consequent Shift Phase of the \nalgorithm. Upon reaching the end of the input x, if the FSS is not empty, we can safely assume that there \nexists a prefix string y such that the parsing of the string yz by the LR parser would not have caused \na parsing error by this point. Properties of LRP guarantee the existence of a suffix z, such that w = \nyx,z is accepted. Thus x is confirmed to be a valid substring. To increase the efficiency of the algorithm, \ntwo operations, SUBSUME and CONTRA CT, are performed on the FSS structure at appropriate times. When \na single node tree is added to the FSS, and the state of the node is identical to that of some other \ntree root node in the FSS, the larger tree may be deleted from the FSS, since the sin\u00adgle node tree represents \nall stacks of LRP that have that particular state at the top of the stack. This set of stacks necessarily \nincludes all stacks that were represented by the larger tree rooted at a node of the same state. The \nSUBSUME op\u00aderation detects such conditions and deletes the larger tree. Long reductions frequently create \nsin\u00adgle node trees that subsume other trees in the FSS, The CONTRACT operation merges two trees, the \nroots of which are of the same state, return\u00ading a single tree as a result. The merging is done recursively \ndown the two trees, to ensure that no immediate sibling nocles in the FS S are labeled by the same state. \nThis in turn guarantees that at all times, the branching degree of every node in the FSS is bounded by \nthe number of states in the parsing table, a property essential for maintaining a linear bound on the \nrunning time of the algo\u00adrithm. Two trees may end up having the same top state as a result of either \na shift operation or a reduction. In the shift case, since prior to the shift the trees necessarily had \ndifferent top states, they may be simply merged at the top node level, and no deeper tree contraction \nis needed. How\u00adever, in the case of a recluction, if the result of the reduction is a top state which \nis the same of that of another existing tree in the FSS, a full CONTRACT operation is performed. l.E~E \n+ T 2.E+T 3.T~T * F 4.T+F 5. F+(E) 6. F~id  Figure 1: A simple grammar for arithmetic ex\u00adpressions \nThe RECLAIM operation is responsible for freeing the dynamically allocated storage for those nodes and \ntrees that are discarded in the course of the algorithm.  2.3 An Example To further clarify how the \nalgorithm works, we present a simple example. Figure 1 contains a simple arithmetic expression grammar, \ntaken from [AS U86], page 218. Table 1 cent ains the SLR parsing table for this grammar, as it ap\u00adpears \nin Figure 4.31 of [ASU86] (page 219). Ta\u00adble 2 shows the long reduction goto table for this parsing table. \nFor each state, the long reduction goto table contains the list of states into which the parser may shift \nafter a reduction from that state3 . Figure 2 shows the contents of the FSS along the various stages \nof the execution of the algorithm on the input {* id )  . Let us follow a trace of this execution. \nThe initialization stage of the algorithm results in en\u00adtering a single node of state 7 into the FSS, \nsince this is the only state that is the result of shift\u00ading the first input symbol * . Thus, after \nthe initialization, the FSS contains the the sin\u00adgle node tree shown in Figure 2a. State 7 wishes to \nshift the next input symbol  id , thus the first Reduce Phase is empty, and the shifting of id occurs \nin the Shift Phase, resulting in the 3Note that in the general LR(k) case, the action table may indicate \nreductions by several different rules from a particular state for different lookaheads. Thus, strictly \nspeaking, the long reduction goto table specifies a partial function from top states and lookaheads to \nsets of states However, in our simple example, the grammar is LR(0) and a reduction by at most a single \nrule is possible from each state. We have therefore simplified the table for this example by omitting \nthe lookaheads.  Top state Goto states after reduction o 1 2 18 3 29 4 5 3 10 6 7 8\u00ad 9 1 10 2 11 3 \nTable 2: Long reduction goto table for the parsing table in Table 1 tree in Figure 2b. The next Reduce \nPhase in\u00adcludes several reductions. Stat e 5 on input  )  indicates a reduction by rule 6. This is \na nor\u00admal reduction, and results in the tree in Figure 2c. State 10 on input  )  indicates a reduction \nby rule 3. This is a long reduction. According to the long reduction goto table, this long reduc\u00adtion \nresults in the single node tree of state 2, as depicted in Figure 2d. State 2 on input  ) indicates \na reduction by rule 2. This again is a long reduction, and according to the long Yeduc\u00adtion goto table, \nit results in two single node trees, of states 1 and 8 respectively, as can be seen in Figure 2e. The \nReduce Phase terminates at this point, since neither state 1 nor state 8 indicate a reduction on input \n ) . The following Shift Phase discards the node of state 1, since the pars\u00ading table indicates an \nerror for state 1 on input ) . State 8 indicates a shift of  )  into state 11, resulting in the tree \nshown in Figure 2f. This completes the Shift Phase. The consequent termination test discovers that we \nhave reached the end of the input. Since the FSS is not empty, the input is a valid substring (of an \narithmetic ex\u00adpression in the language of our grammar), and the algorithm terminates. Note that due to \nthe sim\u00adplicity of the chosen example, no CONTRACT or SUBSUME operations occurred in the execution outlined \nabove. r Action Goto n State id + * () $ETF o sh5 sh4 123 1 sh6 acc 2 r2 sh7 r2 r2 3 r4r4 r4 r4 4 sh5 \nsh4 823 5 r6r6 r6 r6 6 sh5 sh4 93 7 sh5 sh4 10 8 s116 shll 9 rl sh7 rl rl 10 r3r3 r3 r3 11 r5r5 r5 r5 \nI Table 1: SLR parsing table for grammar in Figure 1 07 510 id F ? o7 7 8  Fig. 2a ==> Fig. 2b > Fig. \n2C ==> 2 11 o9 ) o8 > => Fig. 2d > Fig. 2e Fig. 2f Figure 2: Structure of the FSS throughout the execution \nof algorithm SSR on the example  3 Correctness Now we sketch the correctness of SSR. The reader is referred \nto Aho and Unman [AU72] for a com\u00adprehensive proof of correctness of the original LR parsing algorithm \nLRP. In our proof, we rely on the correctness of LRP, namely that given an LR grammar G, and an input \nstring x, LRP accepts z if and only if z ~ L(G). We therefore concen\u00adtrate on proving the following theorem \n: Theorem: Let G be an LR(l) grammar and z be an input string. SSR accepts x if and only if there exist \nstrings y, z such that w = y . z . z is accepted by LRP. We show that SSR simulates the parsing of x \nby LRP for all possible prefix strings y. If upon shifting Xm, the last input symbol of x, SSR has not \nreject ed z, there exists at least one such prefix string y, for which LRP has not rejected the input \ny . z after the shifting of Zn. The existence of a suffix string z, for which w = y . x . z is accepted \nby LRP is assured by the fact that LR parsers reject inputs as early as possible [AU72]. We now provide \na sketch of how the above outline may be formalized. A stack configuration c is a triple (s, z, i), where \ns = [Stl, stz, ...> st~] is a stack of states (with stk at the top), x is the input string of length \nn, and O < i < n is a position within the in\u00adput string. The set of stack configurations ~ep\u00adresented \nat any point of SSR includes a configu\u00adration for each path from a root node to a leaf in the FSS. The \nLRP stack configurations are those particular configurations that correspond to stacks manipulated by \nLRP. A stack configura\u00adtion c = (s , w, j) is an LRP stack configuration if after some number of steps \nof LRP on input w, s represents the LRP stack and j is the parser s po\u00adsition within the input string. \nIn particular, the stack representation s of an LRP stack configu\u00adration c always has the LR machine \ns start state at the bottom of the stack. To formally prove that SSR simulates the pars\u00ading of the input \nstring x by LRP for all possi\u00adble prefix strings g, we define a meaning function M, mapping general stack \nconfigurations to their corresponding LRP st ack configurations. A stack configuration c = (s, x, i) \nof the FSS is mapped by ikl to the (possibly imfinite) set of all LRP stack configurations with s as \nthe top portion of the stack. Formally, let S* denote the set of all state stacks, and LRC denote the \nset of all LRP stack configurations. Then : AI((S, Z, i)) = ={(?-..s, z,z, yl+i)ELRC l~6S*&#38;y EZ*} \n where r . s denotes the concatenation of the state stacks. We extend the domain of Al to the sets of \nconfigurations in the natural way, namely M({c;}) = Ui Al(c;). In the following analysis we assume that \nall states of the LRP parsing table are reachable from the start state. If in fact this property does \nnot hold, we may easily (in constant time and space) modify the table to include only such reachable \nstates, and use our modified table instead of the original one. Lemma 1: Let c1 = (s, x,i) and Cz = (r. \ns,a, i) be two stack configurations. Then Lf(cz) ~ il!f(c~). Proof : Straightforward from the definitions \nof LRP stack configurations and the configuration mapping function 111. Both the SUBSUME and CONTRACT \noper\u00adations of SSR remove paths from the FSS when there exist other paths in the FSS that are suffixes \nof the paths being removed. Lemma 1 implies that the removal of such paths from the FSS does not alter \nthe set of LRP configurations denoted. To formalize the effect of the parsing opera\u00adtions of algorithm \nSSR on the FSS, we define the function ne~t, from stack configurations to sets of stack configurations. \nFor a given configuration c = ([stl, stz, ..., Stl], z,j), ne~t(c) is the set of configurations c that \nare the result of a single SSR parsing step from c. Thus, as in the SSR algorithm, ne~t(c) is defined \naccording to the ac\u00adtion ACT(st[, Xj+l ) indicated in the LR action table. In the case of a shift or \na normal reduc\u00adtion, neot(c) is a set containing the single result\u00ading new configuration. In case of \na long reduction, nezt(c) is the set of all stack configurations con\u00adsisting of single state stacks, \nthe states of which one can reach after shifting the left-hand side non\u00adterminal of the rule being reduced, \nas determined by the long reduction gotm table, If the action is accept 4 or the end of string is reached, \nwe define nezt(c) = {c}, and if it is reject (a parse error), then ne~t(c) = @. To formalize the effect \nof the Reduce and Shift phases, we define the extension of next to sets of stack configurations in the \nfollowing way. Let C = C l U C 2 be a set of stack configurations such that C l contains exactly the \nstack configurations of C whose top state indicates that the next ac\u00adtion is a reduction, and C2 is the \nrest of C. If Cl # @ then nezt(C) = {c c next(c) [ c c Cl} U C2. If Cl = ~ then nezt(C) = {c ~ nezt(c) \n\\ c 6 C2}. Thus, reductions have precedence over other ac\u00adtions. Based on this extended. definition of \nnext we define for every n >0 the function nextn, which is the result of n successive applications of \nnext. Note that a Reduce phase corresponds to some finite number of applications of next and that a Shift \nphase corresponds to a single application of next. Lemma 2: The Simulation Lemma : Let C be a set of \nstack configurations. Then : A!f(next(C)) = nem@f(C)) Proof : We prove this by case analysis on the parsing \nactions that occur on each c ~ C. The cases of Shift, Accept, Reject and normal reduc\u00adtions are straightforward, \nas next is identical to the equivalent action of LRP. Long reductions are more subtle, and in this case \nthe result follows from the definitions of ill and next. Lemma 3: The Generalized Simulation Lemma : \nWe generalize Lemma 2 to any finite num\u00adber of applications of next. Let C be a set of stack configurations. \nFor every n ~ O : M(neO.tn(C)) = ner.W(M(C)) Proof : By a straightforward induction on n us\u00ading Lemma \n2. Lemma 4: When parsing an input string %= X1X2.. . Xn, let Cl be the set of initial stack configurations \nof the FSS, and let Ci denote the set of stack configurations represented by the 4 Notice that in practice \nthe action will never be accept, since the algorithm will have terminated upon reaching the end of the \ninput string. However, we include this case for the sake of completeness. FSS after the ith Shift Phase. \nThe following two properties are maintained for each of the C i (l<i<n): 1. Soundness : if c C C; then \nill(c) # ~ 2. Completeness ; for all LRP stack configura\u00adtions c = (r . S,W, Iyl + i), such that the \nlast operation of the parser is a shift of xi, there exists a c = (.s, z, i) G C i such that c c M(c). \n Proof : By induction on i. (5 I has both prop\u00aderties due to the way it is constructed. The induction \nstep is proven by the following argu\u00adments. Since the nezt function is a formal mod\u00adeling of the Reduce \nand Shift phases of the algo\u00adrithm (excluding the process of possibly discard\u00ading some configurations \nby SUBSUME and CON-TRACT operations), it follows that for some n, C; G nextn (Ci_l) (with the missing \nconfigu\u00adrations being those discarded by the SUBSUME and CONTRACT operations) and since SUB-SUME and \nCONTRACT have no effect on the set of configurations represented by Al, iVf(C~) = M(nextn(C;-~)). The \nnext function has the property that if M(c) # @ and next(c) # ~, then M(next(c)) # @, which extends to \nneztn and thus guarantees soundness. By Lemma A !f(C2) = lkf(next (C~-1)) = nezt (M(C;_l)), which guarantees \ncompleteness. Corollary: If Cn is the set of stack configurations represented by the FSS after the nth \nShift Phase, where n = Ix], then Cn # ~ if and only if there exists an LRP configuration c = (s , yx, \nIyl + [z I). Note that the existence of such an LRP configu\u00adration c implies the existence of a string \nw = yz, such that w is not rejected by LRP by the time Zn was shifted. The soundness property of Lemma \n4 guarantees that if C. # 4, such an LRP stack configuration c exists. The completeness property guarantees \nthat if such a configuration c exists, Cn # ~. Since LRP has the property that an input is rejected at \nthe first possible oppor\u00adtunity on a left to right scan of the input string [AU72], this implies that \nthere exists a string z such that w = w . z = y. x .z is accepted by LRP, completing the correctness \nof SSR.  Complexity Analysis We will now prove that SSR runs in linear time for grammars free of epsilon \nrules. In [BL91] we demonstrate that SSR maintains a linear running time even in the presence of such \nrules. After the initialization of the FSS, the algo\u00adrithm enters a loop that consists of a termination \ntest for end of input, examining the next input symbol, a Reduce Phase and a Shift Phase. This loop can \nbe executed up to n 1 times, until the end of string is reached. The initialization of the FSS that \nprecedes the loop requires only constant time. It involves scanning a column of the LR action table, \nand the creation of a constant num\u00adber of root nodes. The termination check also takes constant time. \nSince there are only a con\u00adst ant number of root nodes (see Lemma 5 below), each Shift Phase involves \nonly a constant number of shift operations and thus takes constant time. However the time cost of each \nReduce Phase is not uniform, and varies from one run through the loop to the next. Each Reduce Phase \ninvolves some number of Tree Reductions, which are re\u00adductions on all paths of an FSS tree to a constant \ndepth. We will show that each such Tree Reduc\u00adtion is completed in constant time and then use an amortized \ncost evaluation to obtain a linear bound on the total number of Tree Reductions. Finally, we will argue \nthat the total time cost of all SUBSUME, CONTRACT and RECLAIM op\u00aderations also is at most linear in the \nlength of the input. In the following analysis, S denotes the set of states of the parser, and IS] is \nthe size of this set. We distinguish between root nodes of the FSS and internal nodes. Lemma 5 : At any \ntime there is at most a single root node of any given state. Proof: The claim holds after the initialization \nof the algorithm, and throughout Reduce and Shift phases SSR explicitly checks for root nodes of identical \nstate, and when detected, merges the appropriate e trees, using SUBSUME and CON- TRACT as necessary. \nLemma 6 : The total number of nodes that be\u00adcome internal in the course of execution of the algorithm \non a string z of length n is O(n). Proof : In the case that the grammar is free of epsilon rules, root \nnodes become internal only as a result of shift operations. Once a node be\u00adcomes internal, it never again \nbecomes a root node. Thus, the Lemma is a direct result of the fact that the number of root nodes at \nthe start of any Shift Phase is bounded by IS[, and there are at most n Shift Phases. Thus the total \nnumber of shift operations is O(n). Lemma 7 : No node in the FSS ever has more than [S\\ children. Proof \n: CONTRACT operations are performed whenever necessary so as to maintain this prop\u00ad erty. We now concentrate \non analyzing the time complexity of Reduce phases. A normal reduction on a single path of nodes in the \nFS S is identical to an LRP reduction, and takes constant time. Long reductions are very similar to normal \nreductions. However, they involve accessing the long reduc\u00adtion goto table in order to determine the \npossible states that may result from the shifting of the left\u00adhand side non-terminal of the rule being \nreduced. This table access is done in constant time. New root nodes are created for the resulting states \nof this process, and each new node added may re\u00adquire a SUBSUME operation, if there already ex\u00adists a \nroot node of the same state. This condition can be detected in constant time by a linear scan of the \nset of root no ales, and need be done only a constant number of times per long reduction, since at most \nIS I new root nodes may be added. We account for the time spent on the SUBSUME operations separately. \nTherefore, excluding the time spent on all SUBSUME operations, a long reduction on a single path requires \nonly constant time, Thus, any reduction, normal or long, on a single path requires only constant time. \nA Reduce Phase reduction in SSR operates on a FSS stack tree, and performs the reduction on all paths \nin the tree that originate at the root node to a depth equivalent to the number of symbols on the right-hand \nside of the rule being reduced. Since this is a constant depth, and the fan-out degree of FSS tree nodes \nis also bounded by a constant, each such Tree Reduction involves only a constant number of reductions \n(one for each path), each taking constant time. Thus in order to complete the time analysis of Reduce \nphases, we need only demonstrate that O(n) Tree Reduc\u00adtions are performed in the course of the algorithm. \n For the purpose of the analysis, we separate the rules of our grammar into two groups. Gram\u00admar rules \nwith a single symbol on the right-hand side are grouped together as non-genemtive rules and their corresponding \nreductions are referred to as non-genemtiue reductions. All other rules will be called generative rules \nand their corre\u00adsponding reductions genemtive reductions. We will show that the cost of performing a \ngenera\u00adtive reduction can be charged to internal nodes of the FSS that are discarded by the reduction, \nand that only a constant number of consecutive non-generative reductions may occur between the generative \nones. Thus, the non-generative reduc\u00adtions may be charged to the generative ones, and they in turn can \nbe charged to the nodes. Lemma 8 : In the course of the execution of algorithm SSR, only a constant number \nof con\u00ad secutive non-generative Tree Reductions n-my be performed. Proof : Since long reductions are \nperformed at most once per state in a Reduce Phase, we need only consider the normal reductions. Non\u00adgenerative \nreductions do not remove internal nodes from the FSS. By a counting argument it can be seen that after \na constant number of such reductions on FSS trees, such a reduction is re\u00adpeated. If this were to occur \nthe non-generative rules that correspond to this series of reductions would form a cycle, in contrast \nwith the fact that any LR grammar must be non-cyclic. Lemma 9 : In the course of an execution of algorithm \nSSR, there are only O(n) generative Tree Reductions. Proof : We recall that at most one long reduc\u00adtion \ncan occur for each state per Reduce Phase. Therefore, at most O(n) such reductions may oc\u00adcur in all \nReduce Phases combined. Any other generative reductions are performed on trees with internal nodes. Such \na Tree Reduction will re\u00admove all internal nodes to a depth corresponding to the number of symbols on \nthe right-hand side of the rule. We therefore account for these re\u00adductions by charging a unit of cost \nto each inter\u00adnal node removed by the Tree Reduction. Since the node is removed from the FSS by the Tree \nReduction, it may only be charged once. Also, for each generative Tree Reduction performed, at least \none internal node is charged. Thus the to\u00adtal number of internal nodes charged is an upper bound on the \ntotal number of generative Tree Re\u00adductions. By Lemma 6 there are only O(n) nodes that become internal \nin the course of the execu\u00adtion of SSR. Thus, only O(n) internal nodes may be charged for Tree Reductions \nand we obtain an O(n) bound on the total number of generative Tree reductions. Lemma 10 : The Total number \nof Tree Reduc\u00adtions is O(n). Proof: We look at the non generative reductions as groups of consecutive \nreductions that occur be\u00adfore, between and after the generative reductions. Due to the O(n) bound on \nthe number of genera\u00adtive reductions, we obtain a similar bound on the number of such groups of non generative \nreduc\u00adtions. We therefore get an O(n) bound on the total number of non generative Tree Reductions. This \nin turn provides us with a bound of O(n) on the total number of all Tree reductions. We complete the \ntime complexity analysis of our algorithm by showing that all CONTRACT, RECLAIM and SUBSUME operations \ntogether require only O(n) time. First we consider the CONTRACT operations. The CONTRACT operation merges \ntwo FSS trees that have root nodes of the same state. The contraction itself is done by comparing the \nstates of the children of the first root node with those of the second root node. Lemma 7 guarantees \nat most ISI 2 comparisons. If a child of the first root node has a state identical to that of a child \nof the second root node, the two subtrees are contracted by a recursive call to CONTRACT. All other chil\u00addren \n(and their appropriate subtrees) are added as children of the first root node, and the sec\u00adond root node \nis deleted. Thus, the top level CONTRACT operation requires constant time. Note that any recursive call \nto CONTRACT will necessarily result in the elimination of an inter\u00adnal node. We may thus charge a unit \nof cost to the node deleted as a result of each recursive call to CONTRACT, and since the node is deleted \nfrom the FSS by the this operation, it may be charged only once. Since CONTRACT is invoked only aft er \nreductions, there are at most O(n) top level calls to CONTRACT. Lemma 6 guarantees that at most O(n) \ninternal nodes will be charged, therefore implying at most O(n) recursive calls to CONTRACT. This provides \nus with an O(n) bound on the total number of CONTRACT calls and a similar bound on the total time complexity \nof all CONTRACT operations. Next, we consider the RECLAIM operations. These operations delete entire \nsubtrees from the FSS, when these become obsolete. We assume the cost of such an operation is directly \npropor\u00adtional to the number of nodes in the subtree, i.e. constant time per node being deleted. The dele\u00adtion \nof internal nodes can be charged a unit of cost to the node being deleted, and only O(n) root nodes are \nreclaimed in all Shift Phases and Reduce Phases combined, since at most one root node is reclaimed per \noperation. Finally, we observe that we have already ac\u00adcount ed for the SUBSUME operations. SUB-SUME \nsearches for a root node of a state identi\u00adcal to that of a new single node tree created by a long reduction. \nThis requires constant time. If found, the tree is the reclaimed by the RECLAIM operation, the time for \nwhich we have already ac\u00adcounted for. This completes the time complexity analysis of our algorithm, under \nthe assumption that the grammar contains no epsilon rules. Our analysis has shown that the total time \ncost of all opera\u00adtions in an execution of the algorithm on an input string of length n is O(n).  Conclusions \nThe original algorithm, while in fact not al\u00adways linear, was used as the basis for a syntax checking \nmodification to the IBM VM/370 edi\u00adtor XEDIT. That modification enabled the IBM editor to check COBOL \nsource code for syntax errors, when users modified lines, screens or files. For instance, when the cursor \nwas moved off a modified line, the editor would beep and display an unobtrusive error message if the \nline was not a substring of any COBOL program. Though COBOL has a large grammar, this modification had \nno apparent effect on the speed of XEDIT on machines of the early 19 S0 s. The algorithm was also used \nto check Pascal programs on an IBM PC editor, and this too had no apparent effect on the speed of the \neditor, Thus, the original algorithm appeared to be adequately fast in practice. We have implemented \nour revised algorithm and have tried it on several test grammars, No precise measurements have been performed \nto compare the actual running time of our substring algorithm with that of the original LR parser. However, \nin practice, the revised implementation continues to run as fast as before. Acknowledgements Alan Demers \nprovided helpful discussion during development of the original algorithm and proof sliet ches in 1979. \nMerricli Furst was helpful in the development of some of the revised ideas de\u00adscribed here. We also thank \nSteve Guattery for his comments.  References We have presented and proved a linear time a.l\u00adgorithm \nfor recognizing substrings of LR(k) lan\u00adguages. The original version of this algorithm was ini\u00adtially \ndeveloped by the first author in 1980. It did not include the CONTRACT operation for merging trees of \nthe FSS. Tree cent ractions are crucial to retaining a linear bound on the run\u00adning time of the algorithm. \nIn the process of try\u00ading to prove the linear time bound we discovered this deficiency, and the proper \nmodifications were consequently made. [ADS3] R. Agrawal and K.D. Detro. An eff\u00adcient incremental LR parser \nfor gram\u00admars with epsilon productions. Acts In\u00adformatica, 19:369-376, 19S3. [ASU86] A. V. Aho, R. Sethi, \nand J. D. Unman. Compilers: Principles, Techniques and Tools. Addisom Wesley, 19S6. [AU72] A. V. Aho \nand J. D. Unman. The The\u00adorg of Parsing, Translation and Com\u00adpiling, Vol. I: Parsing. Prentice-Hall, \nEnglewood Cliffs, N. J., 1972. [BL91] J. Bates and A. Lavie. Recognizing substrings of LR(k) ear time. \nTechnical 91-188, 1991. languages in lin-Report CMU-CS\u00ad [Ce178] A. Celentano. Acts Informatica, Incremental \nLR 10:307-321, parsers. 1978. [Cor89] G. V, Cormack. An LR substring parser for noncorrecting syntax \nerror recov\u00adery. In Proceedings of the SIGPLAN 89 Conference on Programming Language Design and Implementation, \npages 161\u00ad169, 1989. [Ric85] H. Richter. ror recovery. Programming 7(3):478-489, Noncorrecting syntax \ner-ACM Transactions on Languages and Systems, July 198.5. [RK91] J. Rekers and W. Koorn. Substring parsing \nfor arbitrary context-free grarm mars. In Proceedings of Second Interna\u00adtional Workshop on Parsing Technolo\u00adgies, \npages 218 224, Cancun, Mexico, 1991. [Tom86] M. Tomita. Eficient ural Language. Kluwer lishers, Hingham, \nMa., Parsing for Academic 1986. Nat-Pub\u00ad  \n\t\t\t", "proc_id": "143165", "abstract": "<p>LR parsing techniques have long been studied as efficient and powerful methods for processing context free languages. A linear time algorithm for recognizing languages representable by LR(<italic>k</italic>) grammars has long been known. Recognizing substrings of a context-free language is at least as hard as recognizing full strings of the language, as the latter problem easily reduces to the former. In this paper we present a linear time algorithm for recognizing substrings of LR(<italic>k</italic>) languages, thus showing that the substring recognition problem for these languages is no harder than the full string recognition problem. An interesting data structure, the Forest Structured Stack, allows the algorithm to track all possible parses of a substring without loosing the efficiency of the original LR parser. We present the algorithm, prove its correctness, analyze its complexity, and mention several applications that have been constructed.</p>", "authors": [{"name": "Joseph Bates", "author_profile_id": "81100271349", "affiliation": "", "person_id": "PP39073647", "email_address": "", "orcid_id": ""}, {"name": "Alon Lavie", "author_profile_id": "81100546052", "affiliation": "", "person_id": "PP39079359", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143165.143213", "year": "1992", "article_id": "143213", "conference": "POPL", "title": "Recognizing substrings of LR(k) languages in linear time", "url": "http://dl.acm.org/citation.cfm?id=143213"}