{"article_publication_date": "02-01-1992", "fulltext": "\n Semantic Foundations of Jade Martin C. Rinard and Monica S. Lam Computer Systems Laboratory Stanford \nUniversity, CA 94305 Abstract Jade is a language designed to support coarse-graia paral\u00adlelism on both \nshared and distributed address-space ma\u00adchines. Jade is data-oriented a Jade programmer simply augments \na sequential imperative progmm with declara\u00adtions specifying how the program accesses data. A Jade implementation \ndynamically interprets the access spec\u00adification to execute the program concurrently while en\u00adfoming \nthe program s data dependence constraints, thus preserving the sequential semantics. This paper describes \nthe Jade constructs and defines both a serial and a parallel formal operational semantics for Jade. The \npaper proves that the two semantics are equivalent. Introduction Over the last decade, research in \nparallel architectures has led to many new parallel systems. These systems range from multiprocessors \nwith shared address spaces, multi-computers with distributed address spaces, to net\u00adworks of high-performance \nworkstations. Furthermore, the development of high-speed interconnection networks makes it possible to \ncomect the systems together, form\u00ading a tremendous computational resource. An effective way to use these \nmachines is to partition a computation into coarse-grain tasks. The current language support for this \ncomputing environment is, however, rather primi\u00adtive programmers must explicitly manage the hardware \nresources using low level communication and synchro\u00adnization primitives. This paper presents Jade, a \nnew lan\u00adguage designed to simplify the expression of coarse-grain parallelism. Instead of using explicitly \nparallel constructs to create and synchronize concurrent tasks, Jade programmers use Thk research was \nsupported in part by DARPA contract NOO039.91-C-013S. Permission to copy without fee alt or part of this \nmaterial is granted provided that tbe copies are not made or distributed for direct commercial advantage, \nthe ACM copyright notice and the title of the publication and ks date appear, and notice k given that \ncopying k by permission of the Association for Computing Machinery. To copy other\u00adwise, or to republish, \nrequires a fee and/or specific permission. declarative constructs to specify how parts of sequential \nprogram access data. The Jade implementation dynam\u00adically interprets these access specifications to determine \nwhich operations can execute concurrently without vi\u00adolating the program s sequential semantics. This \ndata\u00adoriented approach simplifies the programming process by preserving the familiar sequential, imperative \nprogram\u00adming paradigm. Jade programmers need not struggle with phenomena such as data races, deadlock \nand non\u00ad deterministic program behavior. Jade is a set of extensions to existing sequential lan\u00adguages. \nProgrammers can therefore parallelize large ex\u00adisting applications simply by analyzing how the program \nuses data and augmenting the source code with Jade ex\u00adtensions. Because Jade hides the low-level coordination \nof parallel activity fkom the progranimer, these applica\u00adtions are portable across different parallel \narchitectures. We introduced the basic concepts of the data-oriented approach to concurrency in a previous \npaper [6]. The previous version of Jade was designed for machines with shared address spaces. We have \nimplemented Jade as an extension to C, C++ and FORTRAN on the Encore Mul\u00adtirnax, the Silicon Graphics \nIRIS 4D/240S, and Stanford DASH multiprocessor [7J. We have found it possible tcr parallelize sequential \nprograms with a reasonable pro\u00adgramming effort. Implemented applications include a parallel sparse Cholesky \nfactorization algorithm due to Rothberg and Gupta [11], the Perfect Club benchmark MIX [1], LocusRoute, \na VLSI routing system due to Rose [10], a parallel make program, and a program sim\u00adulating the flow of \nsmog in the Los Angeles basin. We have revised the definition of Jade so that it can now be implemented \non machines with separate address spaces. The same Jade program could be executed, for example, on both \na shared address space multiprocessor and a network of workstations. This revision also makes it possible \nfor the Jade implementation to dynamically verify the correctness of the access specifications. If the \nprogram does not correctly declare how it will access data, the Jade implementation will signal an error. \nThis verification guarantees that the parallel and serial execu\u00ad tions of a Jade progmm compute the same \nresult. This paper presents the revised Jade language, and es\u00ad tablishes the semantic foundations for \nJade. We first in\u00ad formally present the Jade constructs, and explain how a @ 1992 ACM 089791453-8/92/0001/0105 \n$1.50 programmer uses the Jade access declaration statements to specify how parts of the program access \ndata. As we present the Jade constructs, we describe the concur\u00adrency patterns that the data usage information \ngenerates. We then formally present both a sequential and a par\u00adallel operational semantics for Jade. \nBecause Jade is a declarative language, it is not immediately obvious how the Jade implementation generates \nthe parallel execution. The parallel operational semantics therefore provides in\u00adsight into how to actually \nimplement Jade. Finally, we prove that the sequential and parallel semantics are equiv\u00ad alent. 2 Jade \nProgramming Paradigm A Jade programmer provides the program knowledge re\u00ad quired for efficient parallelization; \nthe implementation combines its machine knowledge with this information to map the computation efficiently \nonto the underlying hardware. Here are the Jade programmer s responsibili\u00ad ties: . Task Decomposition: \nThe programmer stats with a serial program and uses Jade constructs to identify the program s task decomposition. \n Data Decomposition: The programmer determines the granularity at which tasks will access the data, and \nallocates data at that granularity.  Access Specification: The programmer provides a dynamically determined \nspecification of the data each task accesses.  The Jade implementation performs the following ac\u00adtivities: \n Constraint Extraction: The implementation uses the program s serial execution order and the tasks ac\u00adcess \nspecifications to extract the dynamic inter-task dependence constraints that the parallel execution must \nobey.  Synchronized Parallel Execution: The implemen\u00adtation maps the tasks efficiently onto the hard\u00adware \nwhile enforcing the extracted dependence con\u00adstraints.  Data Distribution: On machines with multiple \nad\u00ad  dress spaces, the implementation generates the mes\u00adsages required to move data between processors. \n 2.1 Jade Data Model Each Jade program has a shared memory that all tasks can access; objects (dynamically \nor statically) allocated in this memory are called shared objects, Each task also has a private memory \nconsisting of a stack for proce\u00addme parameters and local variables and a heap for dy\u00adnamically allocated \nobjects accessed only by that task. Objects allocated in private memory are called privare objects. The \nimplementation enforces the restriction that no shared object can contain a reference to a private object. \nThis, along with the restriction that no task be directly given a reference to another task s private \nobject, ensures that no task can access any other task s private objects. Each task has an access specijcation \nwhich specities how the task will access shared objects. The programmer defines a task s access specification \nusing access decla\u00adration statements. For example, the rd (read) statement declares that the task may \nread the given object, while the wr (write) statement declares that the task may write the given object. \nAccesses conjlict if, to preserve the serial semantics, they must execute in the underlying sequential \nexecu\u00adtion order. Accesses to different objects do not conflict. Writes to the same object confiict, \nwhile reads do not conflict. A read and a write to the same object also conflict. The Jade implementation \nexploits concurrency by rdaxing the sequential execution order between tasks that dedare no conflicting \naccesses. Since the parrdlelization is based on the access specifi\u00adcation, it is important that the access \nspecification be ac\u00adcurate. An undeclared access could introduce a data race and make a parallel execution \nof the program compute an erroneous result. The Jade implementation precludes this possibility by dynamically \nchecking each task s ac\u00adcesses to shared objects. If a task attempts to perform an undeclared access, \nthe implementation will generate a run-time error. The implementation serializes tasks that dectare con\u00adflicting \naccesses to an object even though the tasks may actually access dkjoint regions of the object. The pro\u00adgrammer \nmust therefore atlocate objects at a fine enough granularity to expose the desired amount of concurrency \nin the program.  2.2 Basic Concurrency Jade programmers use the wit honl y-do construct to identify \na task and to specify how that task will access data. Here is the general syntactic form of the construcc \nwithonly { access declaration } do (pararaeters for task body) { task body } The task body section contains \nthe serial code ex\u00ad ecuted when the task runs. The parameters section contains a list of variables from \nthe enclosing environ\u00ad ment. When the task is created, the implementation copies the values of these \nvariables into a new envi\u00adronment the task will ,execute in this environment. To ensure that no task \ncan ~ference another task s private objtxm, no variable in the parameters section can refer to a private \nobject. When a task is created, the Jade implementation exe\u00adcutes the access declaration section to generate \nthe task s access specification. This section is an arbitrary piece of code containing access declaration \nstatements. Each such statement declares how the task will access a given shawl objecc the task s access \nspecification is the union of all such declarations. This section may con\u00adtain dynamically resolved variable \nreferences and control flow constructs such as conditionals, loops and function calls. The programmer \nmay therefore use information available only at run time when generating a task s ac\u00adcess spwfication. \nThe Jade implementation uses the access specification information to execute the program concurrently \nwhile preserving the program s sequential semantics. We illus\u00adtrate this concept by tracing the execution \nof the follow\u00ading Jade program. x := sh (0); Y := sh (l); if (g(0) > O) { x := y; }; withonly { wr (x) \n; } do (x) { *x := f(l); }; withonly { rd(y) ; wr (y) ; I do (Y) { *Y := *y + f(2); } This program first \nuses the sh construct to allocate two objects in the shared heap. x and y refer to these shared objects. \nThe program then computes g (O), making x and y refer to the same object if g ( O) > 0. The pro\u00adgram \nthen executes the two withonly-do constructs. Each wi-thonly-do construct creates a task and gen\u00aderates \nthat task s access specification. If x and y refer to the same object, then the tasks accesses may conflict \nbecause both specifications declare that the tasks will write the same object. In this case the implementation \npreserves the serial semantics by executing the first task before the second task. The program therefore \ngenerates the following sequential task graph: := *y +f(2) 3  If x and y refer to different objects, \nthe two tasks access specifications declare that the tasks wiiil access disjoint sets of objects. Therefore, \nthe two tasks ac\u00adcesses do not conflict. The implementation can execute the tasks concurrently without \nviolating the program s serial semantics. In this case the program generates the following parallel task \ngraph m- Conceptually, the Jade implementation dynamically generates and executes a task graph. As the \nimplemen\u00adtation creates tasks, it inserts each new task into the task graph. To preserve the serial semantics \nthe implemen\u00adtation inserts precedence arcs between tasks whose ac\u00adcesses may conflict. Each such arc \ngoes from the earlier task in the underlying sequential execution order to the later task. When a processor \nbecomes idle it executes one of the initial tasks (a task with no incoming precedence arcs) in the current \ntask graph. When the task completes it is removed from the task graph. By building the task graph incrementally \nat run time, the Jade implementation can detect and exploit dymmic, &#38;tadependent concur\u00adrency available \nonly as the program runs. The programmer controls the amount of exploited con\u00adcurrency by choosing the \nappropriate task granularity. Because the statements in a task execute sequentially, two pieces of code \ncan execute concurrently only if they are in different tasks. The programmer must therefore make the \ntask decomposition fine enough to expose the desired amount of concurrency. 2.3 Advanced Concurrency \nln the model of parallel computation presented in section 2.2, a task s access specification is determined \nonce and for all when the task is created. Two tasks may either execute concurrently (if none of their \naccesses conflict) or sequentially (if their accesses may conflict). Therefore, all synchronization takes \nplace at task boundaries. The following example demonstrates how synchronizing only at task boundaries \ncan waste concurrency. x. := sh (o); Y := sh (l); withonly { wr (x) ; } do (x) { *X := f(l); }; withonly \n{ rd (y) ;rd (x) ;wr (x) ; } do (y, x) { s := g(*y); *x := h(*x, S); }; wi.thonly t wr (y) ; } do (Y) \n{ *Y := f(2); } This program generates three tasks. The tasks must ex\u00adecute sequentially to preserve \nthe serial semantics. How\u00adever, the second task does not access x until it finishes the statement s : \n= g ( *y). Therefore, the fist task should be able to execute concurrently with the state\u00adments := g \n(*Y) from the second task. similarly, the second task no longer accesses y after the statement s := g \n(*y) finishes. The statement *x := h (*x, s) from the second task should be able to execute concur\u00adrently \nwith the third task. This example illustrates how information about when tasks access shared objects \ncan expose concurrency. To allow programmers to express when a task will ac\u00adcess shared objects, Jade \nprovides both a new construct, wit h-cent, and new access declaration statements df.rd, df-.wr, no.rd \nand no-wr. The wi.th-cent construct allows the programmer to update a task s ac\u00adcess specification as \nthe task executes. This construc~ in combination with the new access declaration statements, allows the \nprogrammer to exploit the kind of inter-task concurrency described above. Here is the general syntactic \nform of the with-cent construcc with { access declaration } cent; As in the withonly-do construct, the \naccess declaration section is an arbitrary piece of code con\u00adtaining access declaration statements. These \nstatements change the task s access specification so that it more pre\u00adcisely reflects how the rest (or \ncontinuation, as the cent keyword suggests) of the task will access shared objects. The w i.t h-cent \nconstruct combines the current ac\u00ad cess specification with the declarations in the access declaration \nsection to generate a new access specifi\u00ad cation. Unless the access declaration explicitly de\u00ad clares \notherwise, the new access specification has the same declarations as the old access specification. The \nwit honl y-do construc4 on the other hand, builds its access specification from scratch. Unless it explicitly \nde\u00ad clares an access, the access specification does not contain that de&#38;ration. The keywords in the \nconstructs (with vs. wit honl y) reflect this difference in the treatment of undeclared accesses. 2.4 \nDeferred Accesses The df_rd and df_wr statements declare a deferred ac\u00adcess to the shared object. That \nis, they specify that the task may eventually read or write the object, but that it witl not do so immediately. \nBefore the task can access the object, it must execute a with-cent construct that uses the rd or w r \naccess declaration statements to con\u00advert the defened declaration to an immediate declaration. Therefore, \na task that initially declares a deferred access to a shared object does not have the right to access \nthat object. It does, however, have the right to convert the deferred declaration to an immediate declaration. \nThis immediate declaration then gives the task the right to access the object. Deferred declarations \naltow a task to defer its syn\u00adchronization for a shared object until just before it ac\u00adtually accesses \nthe object. The following modification to our example illustrates how deferred declarations can increase \nthe amount of exploitable concurrency in a Jade program: x := sh (0); := sh (l); wi.thonly { wr (x) ; \n} do (x) { x := f(l); ); withonly { rd (y) ; df_rd (x) ; df_wr (x) ; } Y do (y,X) { s := 9(* Y); with \n{ rd (x) ; wr (x) ; }Cent; *X := h(*xr S); }; withonly { wr (y) ; } do (Y) { := f(2); *Y } Because the \nsecond task declares a deferred read and a deferred write access on x, it cannot access x until it con\u00adverts \nthe deferred declarations to immediate declarations, The second task can therefore start to execute while \nthe tirst task is still running. The with-cent statement in the second task converts the deferred declarations \nto immediate declarations. Because tie immediate declara\u00adtions give the second task the right to access \nx, it must wait untit the first task completes before it can proceed. This example demonstrates how deferred \ndeclarations al\u00adlow the Jade implementation to execute an initial segment of one task concurrently with \nanother task even though the second task may eventually carry out an access that conflicts with one of \nthe first task s accesses. 2.5 Completed Accesses Jade programmers use the no_wr and no-rd access declaration \nstak?ments to explicitly remove a declaration from a task s access specification. These statements al\u00adlow \nthe programmer to indicate when a task has com\u00adpleted a specified access. The following example illus\u00adtrates \nhow the no-wr and no.rd statements can increase the amount of exploitable concurrency x := sh (0); Y \n:= sh (l); withonly { wr (x) ; ) do (x) { *x := f(l); ); withonly { rd (y) ; df_rd(x) ; df_wr(x) ; } \ndo (Y, x) f s := 9(* Y); with { no_rd (y) ; rd (x) ; wr (x) ; ) cent; *x := h(*x, S); 1; withonly { \nwr (y); } do (Y) { *Y := f(2); } After the second task executes the wit h-cent state\u00adment, it no longer \ndeclares that it will read y. At this point the two tasks cannot perform conflicting accesses, so the \nrest of the second task can execute concurrently with the third task. This example illustrates how programmers \ncan use the with-cent construct and the no.wr and noxci access declaration statements to eliminate conflicts \nbetween the enclosing task and tasks occurring later in the underlying sequential execution order. This \ncontlict elimination may allow later tasks to execute as soon as the enclosing task executes the wit \nh-cent statement. In the albsenceof the with-cent statement the tasks would have had to wait until the \nenclosing task terminated. 2.6 Summary Access specifications give the Jade implementation all the information \nit needs to correctly execute a program in parallel. Programmers generate a task s initial access specification \nwhen it is created, and can update the spec\u00adification as the task runs. At any time, the current access \nspecification must accurately reflect how the nest of the task and its future sub-tasks will access data. \nIt is this a priori restriction -the guarantee that neither the task nor any of its sub-tasks will ever \naccess certain shared objects -that allows the Jade implementation m exploit concurrency between tasks. \nEach declamation enables a task and its sub-tasks to access a shared object in a certain way. For example, \nan immediate read declaration enables the task to read the data; it also enables the task s sub-tasks \nto declare a read access and then read the &#38;ta. Conversely, if a task has not declared a read access \non a given object, a sub-task camot declare a read access and thus cannot read the object. A declaration \ntherefore allows a sub-task to access certain data by enabling the sub-task to declare certain accesses. \nTable 1 summarizes the actions that read declarations enable. The table for writes is similar. When the \nimplementation exeeutes tasks, it must en\u00adsure that the paratlel execution performs conflicting ac\u00adcesses \nin the same order as the serial execution. There\u00adfore, a task cannot execute if any of its enabled accesses \nconflict with an immediate or deferred access declared by an earlier task.  3 Operational Semantics \nSection 2 informally described the Jade language and the concurrent behavior of Jade programs. In this \nsextion we Table 1: Enabled Actions Declaration Enabled Enabled Access Declarations rd rd read df.rd \nno-rd rd df-rd none df-rd no-rd no-rd I none I none develop both a serial and a parallel operational \nsemantics for Jade, and show that the parallel and serial semantics are equivalent, Because Jade is a \nset of extensions to serial, imperative languages, we base our Jade semantics on a semantics for such \na language. In this section we use the language Simple defined in Appendix A as our base language. We \nmade Simple simple for purposes of exposition, but it is powerful enough to illustrate the basic concepts \nbe\u00adhind the Jade semantics. Although Simple only supports integers and references, our semantic treatment \ntrivially generalizes to languages with more elaborate data struc\u00adtures. Simple also has no sophisticated \nflow of control constructs such as first class continuations or closures. Again, the semantic treatment \npresented in this section trivially generalizes to languages with such constructs. Because Jade only \ndeals with a program s dynamic mem\u00adory accesses, it does not matter what part of the program happens \nto generate these accesses. We fnxt define an operational semantics for Simple. This semantics consists \nof the definition of expression evaluation and the definition of a transition relation on Simple program \nstates. This transition relation is in effect an interpreter for Simple. We use the transition relation \nfor Simple to define a serial operational semantics for Jade programs. This semantics executes Jade programs \nin the standard serial execution order and dynamically checks the correspondence between each task s \ndeclared and actual accesses to shared objects. We also use the transition relation for Simple to define \nthe parallel operational semantics for Jade. The parallel and seriat semantics use the same mechanism \nto check that tasks perform no undeclared accesses to shared ob\u00adjects. The parallel semantics, however, \nruns tasks con\u00adcurrently if they can perform no conflicting accesses. The parallel semantics maintains \na set of active tasks and a set of suspended tasks. Active tasks can execute concur\u00adrently; suspended \ntasks wait for active tasks to complete conflicting accesses. This semantics uses the standard interleaving \napproach to modelling concurrency in that it models the parallel execution of tasks by interleaving tlheir \natomic transitions. The parallel semantics synchronizes tasks by maintain\u00ading, for each shared object, \na queue of the declarations of tasks that may access that object. When all of a task s immediate declarations \nreach the front of their queues, the task is activated and can run. When a task termi\u00adnates, it removes \nall of its declarations from the queues, potentially activating other tasks. When a task is created, \nthe implementation inserts each of its declarations into the appropriate queue just before its parent \ns declarations. When a task creates several sub\u00adtasks, the sub-tasks declarations will appear in the \nqueue in their task creation order. This task creation order is also the standard serial execution order. \nBecause a sub\u00adtask s declarations app?ar before its parent s declarations, the sub-task takes precedence \nover its parent, Therefore, tasks with conflicting declarations will get activated and execute in the \nstandard serial excxmtion order. The main theoretical result of this paper is a theorem which establishes \nthe correspondence between the serial semantics and the parallel semantics. The theorem states that a \nparallel execution of a Jade program will success\u00adfully halt if and only if the serial program successfully \nhalts. Also, all such parallel and serial executions gen\u00aderate the same result. The Appendix contains \nthe complete set of axioms that define the operational semantics. In the paper we illus\u00adtrate how the \noperational semantics work by reproducing representative axioms fmm the Appendix. 3.1 Access Specifications \n In this section we formally define the access speci\u00ad fications that the semantics uses to check the \ncorre\u00ad spondence between a task s declared and actual ac\u00ad cesses. Each access specification is a set \ns of dec\u00ad larations. A declaration is a tuple of the form (di 6 {df, i.m, no}, rw E {rd, wr}, l). The \nfirst field of the tuple determines whether the tuple represents a de\u00ad ferred (df), an immediate (ire) \nor no (no) access decla\u00ad ration. The second field of the tuple determines whether the tuple represents \na read (rd) or a write (wr) decla\u00ad ration, while the third field identifies the shared object to which \nthe declaration refers. We say that a task with access specifications declares a deferred write access \non a shared object I if (df, wr, 1) G s, an immediate write access if (ire, wr, 1) Es, etc. A task with \naccess specifications can read (or write) a shared object I only if (ire, rd, 1) 6 s (or (ire, wr, 1) \nc s). A task can declare an access (or no access) on an object in a withonly-do or with-cent construct \nonly if s declares either an immediate or a deferred access on that object. We formalize the second concept \nwith the following definition: Definition 1 s 1-(di, rw, l) ifl(df, rw, 1) E s or (ire, rw, l) ~ s, \nThe Jade implementation vefies that a task s access specification is accurate by dynamically checking \nevery access to a shared object that the task declares or per\u00adforms. If the task attempw to perform or \ndeclare an access that its access specification does not enable, the implementation detects the violation \nat run time and sig\u00adnals an error, 3.2 Expression Evaluation In this section we define how Jade programs \nevaluate expressions. We assume an intinite set of shared memory locations ShLoc and an infinite set \nof private memory locations PrLoc. Each location holds a shared or private object. In Simple, shared \nlocations can hold integers and references to shared objects. Private locations can hold integers and \nreferences to either shared or private objects, A memory is a partial function from a finite number of \nactive memory locations to objezts: I E Loc = ShLoc U PrLoc v E ShObj = ShLoc U 2 v G PrObj = ShObj U \nPrLoc m E ShMem = ShLoc z ShObj n c PrMem = PrLoc @ PrObj Programmers use identifiers (id e I@ to refer \nto vari\u00ad ables. An environment e (c Env = Id ~ PrObj) gives the correspondence between variables and \nvalues. Expressions are evaluated in a context containing an environment, a shared and a private memory, \nand an ac\u00adcess specifications. As described in section 3.1, s deter\u00admines which shared objects a task \nhas the right to read and/or write. The notation exp h (e, m, n,s) = v should be read as: expression \nexp in context (e, m, n,s) evalu\u00adates to v . When a task evaluates an expression, it may read a shared \nobject. At each such read, the semantics checks that the task declares an immediate read access on that \nobject.. This check takes the form of a precondition on the axiom that evaluates reads to shared objects. \nThis axiom (reproduced below from Appendix B) requires that if a task reads a shared object, it must \ndeclare the access: c?xpin (e, m,n, s) = leDom m,(im, rd, l) cs *expin (e, m, n,s} = m(1) Appendix B \ncontains the complete set of axioms that define expression evaluation.  3.3 Simple Semantics We now \ndefine the operational semantics for Simple pro\u00adgrams. This semantics takes the form of a transition \nrelation ~. Intuitively, each transition starts with a se\u00adquence of statements and a statement evaluation \ncontext 110 and executes the first statement in the sequence. The result is a new statement sequence \nand a new context. Statement evaluations take place in the same contexts as expression evaluations. A \nSimple statement may write a shared object. In this case the operational semantics must check that the \ntask declares an immediate write access on that object. We reproduce that axiom here to show how the \nsemantics uses a precondhion to perform the check expl in (e, m,n, s) = I 6 Dom m, (ire, wr, l) Es, ex~ \nin (e, m,n, s) = v c ShObj *expl := ex~;c in (e, m, n, s)+ cin (e, m[l *v], n,s) If the task that generates \nthe write does not declare the access, the Jade implementation must generate an error. Formally, the \ntask takes a transition to the special state error. The semantics uses the foltowing axiom to generate \nthis transition: expl in (e, m, n,s) = I c Dom m, (ire, wr, 1) @s *expl := ex~;c in (e, m, n, s)-+error \n Finally, we demonstrate that when a task allocates a shared object, it acquires a deferred read and \na deferred write declaration on the new object. The following axiom executes a sh statement, which allocates \na new objecc I c ShLoc \\Dom m,expin (e, m,n, s) = v c !5hObj, s = sU{(df, rd,l), (df, wr,l)} id:= sh(exp);cin \n(e, m, n, s)+ cin (e[id= I], m[l w v], n,sl) This axiom arbitmrily chooses a new location for the atlocated \nobject. Therefore, different executions of the serial program may dtifer in their choice of which lo\u00adcations \nhold which objects. Such executions represent equivalent computations, but the actual program states \nare different. To capture this equivalence we define what it means for two contexts to be equivatenc \nDefinition 2 (el, ml, nl, s1) =b (e2, m2, n2, s2) Zti Dom el = Dom e2, and there exist bijectims bn : \nDomnl + Domnz,I&#38; :Domml + Dommzand bs : S1 + S2such that b= bnUbmUZ (where Z is the identity function \non .2?U {e rro r}) and 1. VIE Dom nl.b(nl(l)) = nz(b(l)), 2. VI G Dom ml .b(ml(l)) = m2(b(l)), 3. Vide \nDom el .b(el(id,)) = e2(id), 4. V(di, rw, 1) E sl.bs((di, rw, l)) = (di, rw, b(l)).  Lemma 1 If (el, \nml, nl, sl) -b (ez, nb, nz, %) then Qcxp E J!?xP. b(expin (el, ml, nl, sl)) = exp in (ez, m2, nz, $2). \nAppendix C contains the rest of the axioms that define the opam.ionrd semantics for Simple. 3,,4 Jade \nStatement Semantics This section defines the transition relation + j for Jade statements. This transition \nrelation extends + to the access declaration sections of Jade constructs. These tmnsitions take place \nin Jade contexts; a Jade context is a tuple (e, m,n, s,r ). These contexts are the same as statement \nevaluation contexts, except that they contain au additional set of declarations r. The axioms accumu\u00adlate \ndeclarations from the access declaration sections of Jade constructs into this set. When the semantics \nhas finished executing the access declaration section of a w,ithonly-do construct, r becomes the access \nspec\u00adification of the new task. For a with-cent construct, the semantics uses r to update the current \ntask s access specification. We first reproduce an axiom that demonstrates how access declaration sections \ncan contain arbitrary code. Tlhe following axiom makes all of the Simple transitions valid in the access \ndeclaration section of a wit h-cent construct c1 in (e, m,n, s)+ in (e , m , n , s ) with {cl}cont;q \nin (e, m, n,s, r)-+j with {dl}cont;cz in (e , m , n , s , r) We next reproduce an axiom dealing with \nthe accu\u00admulation of declarations into the access specification set r. To legally declare that a new \ntask will access a given shared object, the parent task s access specification must enable the declaration. \nThe semantics enforces this con\u00adstraint with a precondition on the axiom which constructs the new task \ns access speeiticatiom c = withonly {di.rw(exp);c l}do(ids){cz};cs, expin(e, m,n, s)=l CDomm, r = ru{(di, \nrw, l)}, sl-(di, rw,l) cin (e, m, n,s, r)-+j withonly {cl }do(ids){cz};q in (e, m, n,s, r ) Appendix \nD contains the rest of the axioms which define the +j transition relation, and the definition of the \nequivalence relation ~~ for Jade contexts. 3.5 Serial Jade Semantics We now define the transition relation \n+, for serial Jade program states. A serial Jade program state (m, ts) = S! G SerState = Sh M em x (Task*) \nis a pair consisting of a shared store and a stack of tasks. Each task is a tuple t = (c, e, n,s, r) \nc Task containing the code c for the task. The following axiom defines the execution of a wit hon 1y-do \nstatement. The precondition first checks that none of the task parameters refers to a private ob\u00adject. \nThe new task then becomes the first task in the sequence, with the parent task second. Therefore, the \n111 program s statements execute in the standard sequential, depth-tirst execution ordec Definition 3 \ns ~r = {(di, rw, l) c s I~3(di , rw, l) c r}U {(ire, rw,l) G r} U{(df, rw,l) c r}. c = wi.thonly {~}do(kll,... \n, @){cl };Q, Vi < n.id G Dom e and e(id~) g PrLoc, e = [idl w e(idl )] . . . [idn * e(idn)], (m, (cje, \nn,s, r) ots)+, (m, (cI, e ,0,0 ~ r,O)o (c2, e,n,s,0)ots) When a task completes, the computation continues \nwith the rest of its parent task. The next axiom removes completed tasks from the top of the stack of \ntasks. The completed task s parent is the new first task, so the pro\u00adgram s execution continues with \nthe parent task (m, (c, e,n, s, r) ots)+$(m, ts) When a program successfully hatts, it takes a transition \nto the integer that is the program s result c= result (exp), expin (e, m,n, s) = v E Z (m, (c, e, n,s, \nr))+$v The rest of the axioms that define +. appear in Ap\u00ad pendix E. In particular, there is an axiom \nthat takes the serial program state to error if there is an error in the execution of one of the tasks. \n We now define the notion of equivalence for serial Jade program states, and state a theorem that allows \nus to treat +, as a transition function betwexm equivalence classes of serial Jade program states. Definition \n4 b J s is the restriction of b to s. That is, Domb~s= Dombrlsandb Js(v)= b(v). Definition 5 (m, tl o \n. ..otn) =$ (m , t{ o.. .ot~) zfi bm : Dom m --+ Dom m is a bijection and Vi < n. ift~ = (c, e,n, s,r), \nt~ = (C , e , n , s , r ) then c = d and 3b.b J IMmbm = bm, (e, m,n, s,r) -~ (e , m,n , s , r ). Theorem \n1 If stl %, stz then We can now view +8 as a program execution function. The value of -+s is the unique \nequivalence class of pro\u00ad gram states obtained by executing the next step of the program. Our serial \nsemantics is therefore deterministic. We now define the notion of observation for the serial execution \nof Jade programs. The basic idea is that we start the program in a start state and run it until it can \nprogress no further. If the program halts with an inte\u00adger result, we observe the result. If the program \nhalts in error or could only partially execute we observe error. If the program runs forever we observe \nl-: Definition 6 sst(c) = (0, (c, 0, k!,k?J,0)). SObs(c) = v if sst(c)+$ . ..+$VC2 error if sst(c)+$ \n. . .+~error or Sst(c).-+$ . . .+~st+~,st C SerState J-if sst(c)+$ ...+$ ... { 3.6 Parallel Jade Semantics \nWe now define the transition relation + ~ for parallel Jade program states. A parallel program state \npt = (m, A, S, E ) c ParState consists of a shared memory m, a set A of active tasks, a set S of suspended \ntasks and an ordering relation E on the declarations of the parallel program state. 3.6.1 Object Queues \nThe following definitions impose some consistency re\u00adquirements on the structure of C: Definition 7 Given \na set T of tasti, decl(T) = kJ(C,e,rl,s,r)~ q s. Definition 8 We say that c is consistent for A U S ijjf \n1. EC decl(A uS) x decl(A uS), 2.d1Cd2and d2cd3+d1rd3. 3. (di, rw,l) c (di , rw , l ) + I = l . 4. (di, \nrw, 1) @ (di , rw , I) and (di , rw , 1) @ (di, rw, 1)  #3(c, e,n, s,r)6 AUS. (di, rw,l) G s and (di \n, rw ,1) Es. Given this definition, c represents a set of queues, one for each shared object. Each declaration \n(di, rw, 1) ap\u00adpears in the queue for L Declarations appear in a queue in their tasks underly\u00ading sequential \nexecution order. So, if task t 1 would exe\u00adcute before task tz if the program executed sequentially, \nthen tl s declarations appear before the declarations of tz. The operational semantics uses these queues \nto determine when tasks can execute concurrently. As soon as all of a task s immediate declarations reach \nthe front of their queues, that task can execute. Therefore, if the declara\u00adtions of two tasks are simultaneously \nat the front of their respective queues, the two tasks access specifications do not conflict and the \ntasks can execute concurrently. We formalize the notion of ftont of a queue with the following definitions. \nf ((di, rw, 1), C) is true just when (di, rw, 1) is at the front of its queue. 112 Definition 9 SUCC(S, \nc) = {d I 3d G s.d C d }. pred(s, C) = {d I 3d c s.d c d}. f((di, wr, l), C) i~pred({(di, wr, l)}, \nC)= 0. f((di, rd, l), C) iff V(di , rw , l ) 6 pred({(di, rd, l)}, C).rw = rd. As the definition of \nf shows, a write declaration is at the front of its queue when there are no declarations before it in \nthe queu% a read declaration is at the front of its queue when there are only other read declarations \nbefore it in the queue. This reflects the fact that several tasks can concurrently read a shared object \nbecause reads do not change the object s state. Writes, of course, must execute in the underlying sequential \nexecution order. The definition off demonstrates that a deferred decla\u00adration can prevent an immediate \ndeclaration from being at the front of its queue. In this case the deferred dec\u00adlaration prevents the \nimmediate declaration s task from executing. This reflects the fact that a deferred declara\u00adtion represents \na potential access. The immediate decla\u00adration s task cannot proceed until there is no possibility that \nan earlier task can perform an access that conflicts with any of its accesses. We now discuss the definition \nof + ~, the tmnsition relation for parallel Jade program states. We model the parallel execution of a \nJade program by interleaving the atomic transitions of that program s parallel tasks. + ~ therefore arbitrarily \npicks one of the active tasks and executes the next step of that task s computation. The tasks themselves \nmay change their specifications, create new tasks, or complete their execution. Each of these events \nchanges the program state s set of specifications, The transition relation must therefore modify E to \nreflect these changes. l%e~ is a function to perform each kind of mod\u00adification to E. When the semantics \nexecutes a wi,t honl y-do construct, it uses the ins function to insert the new task s declamations into \nthe queues just before its parent s declarations. When the task com\u00adpletes, the semantics uses the rem \nfunction @ remove its declarations from the queues. When the semantics executes a with-cent construct, \nit uses the upd func\u00adtion to perform the queue modifications that correspond to the changes in the task \ns access specification, Definition 10 s@l = {(di, rw, 1) G s}. rpl(s, r, C) = (pred(st@l, C) xr@l)U(r@l \nxsucc(s@l, c)). (di, rw,l)cr ins(r, s, E) =E UrPl(s~ r) E) (di, rw, I)er r@l x @l. rem(s, E) =C \\{(d, \nd ) Id 6 sor d c s}. upd(s, r, E) = rem(s, C) U rpl(s, r, C). 3.6.2 Transition Relation We now present \nthe axioms that define +P. We tirst present the axiom that executes a wi.thonl y-do state\u00adment when it \ncreates a new task. This axiom suspends both the new task and its parent. The pwent task maybe unable \nto mn because its access specification may con\u00adflict with the new task s access specification. The new \ntask may be unable to run because its access specification may contiict with those of previously created \ntasks. c= withonly {e}do(ki l,... ,ioL){cl};cz, t=(c,e, n,s, r)~A, Vi < n.iol c Dom e and e(iol) $? PrLoc \ne = [idl ++ e(idl )] ...[idn H e(i&#38; )], t = (cl, e ,0,0~ r, O), t = (c2, e,n, s,0) (m, A, S, C)+P \n(m, A\\ {t}, SkJ {t , t }, ins(0 T r,s, C)) When all of a suspended task s immediate declarations reach \nthe front of their respective queues, the semantics must transfer the task to the set of active tasks \nso that it can execute. The following axiom activates such sus\u00adpended tasks: t=(c, e,n, s,r)6S, V(im, \nrw, 1) E s.f((im, rw, 1), E) (m, A,$E )+P(m, A&#38; {t}, S\\ {t}, E ) When a task completes, it must \nremove its declara\u00adtions from the queues. The semantics can then activate tasks whose accesses conflicted \nwith the completed task s accesses. t=(e, e,n, s,r)~A, (m, A, $ c )+P(m, A\\ {t}, $ rem(s, c )) The rest \nof the axioms that define +P are in Appendix F. In particular, there is an axiom that takes a program \ntc~error if the program violates some of the execution constraints, and an axiom that computes the program \ns result. We next define how to observe the parallel execution of a Jade program. If the program successfully \nhalts, we observe the result. If the program has an error, or gets into a state from which it cannot \nprogress, or has one of its active tasks get into a state from which it cannot progress, we observe error. \nIf the program runs forever, we observe L The parallel observation function PObs observes every parallel \nexecution and takes the union of the resulting observations. In this definition, PObs makes no fairness \nassumptions about the parallel execution. Definition 11 pst(c) = (0, {(C, 0,0, 0,0)},0, 0). hung((m, \nA, S, c )) ifl (m, A,S, C )+, or ~t E A.(m, {t}, O, c )+,, PObs(c) = {v I Pst(c)+fl . ..+PVE2} U {error} \nif pst(c)+P . .+Perror or pst(c)+p . . .+Ppt, hung(pt) u {J-} i~pst(c)+P . . .-+p. We next define a notion \nof consistency for parallel pro\u00adgram states, and prove that +P preserves consistency. We use lemma 2 \nextensively in the proof of correspon\u00addence between the serial and parallel semantics. Definition 12 \nA parallel program state (m, A, S, c ) is consistent iff c is consistent for A US and V(c, e, n,s, r) \nG AuS 1. V(di, rw,l) E r.1 cDom m,s} (di, rw,l) 2. V(di, rw,l) E s.I E Dom m, di G {df, ire}, 3. (c, \ne,n, s,r)6 Aa V(im, rw, i) E s.f((im, rw, 1), C).  Lemma 2 If pt ~ ParState is consistent, pt+Ppt and \npt c ParState then pt is consistent. Proof Sketch: The key aspect of the proof is to show that +P preserves \nproperty 3 of definition 12. To show this, we must show that the legal queue updates and insertions do \nnot cause the program state to violate this property. We tirst consider the queue insertions caused by \na par\u00ad ent task spawning a new task. For each queue, the new task s declarations appear just before the \nparent task s ddarations. Both the parent task and the new task are suspended in the new state. All active \ntasks other than the parent task remain active in the new state. We now show that any immediate declaration \nof an active task in the new state remains at the front of its queue. If there is no declaration from \nthe parent task in the active task s declaration s queue, then by property 1 of definition 12 there is \nno declaration from the new task in that queue. Otherwise, the active task s declaration must have appeared \neither before or after the parent s declamations in the old state. If the declaration appeared before \nthose of the parent task, then it will appear lxfore those of the new task in the new state. If the declara\u00ad \ntion appeared after those of the parent task, then all of the declarations in question must be read declarations. \nBy property 1 of definition 12 the new task inserted no write declarations in the queue, so the active \ntask s read declaration is still at the front of the queue. We next consider an update to the queue due \nto the execution of a wit h-con~ statement. In the new state, the updating task s declarations appear \nin the same place in the queues as the task s declarations from the old state. We can use a case analysis \nsimilar to that for the insertion case to determine that the transition preserves property 3 of definition \n12. 3.7 Semantic Correspondence In this section we present the proof of correspondence between the parallel \nand serial semantics. We first setup an equivalence between serial program states and parallel program \nstates. We use this definition to show that the serial execution of a Jade program is also one of the \nlegal parallel executions. This is the first step towards prov\u00ad ing the correspondence between the parallel \nand serial semantics. Definition 13 (m, ts) -,P (m, A, S, c ) ijjf there exists an ordering (c+, e{, \nn{, S;, r{ ). . . . 0 (&#38;)%l)&#38;)sA>rA) on A uS such that t~ts = (cl, el, nl, sl, rl) o .-o (en, \nen, nn, sn, rn )thenm=nand Vi<n 1. c~ =~,e~ =ej, n~ =n~,s~ =sj, ri = rj, 2. V(di, rw, 1) c s~.f((di, \nrw, l), rem(wj<~ sj, C)).  Lemma 3 If st~,p pt, pt is consistent and st+,st then 3pt .pt+p ---+P pt \nand either st ~sP pt or st = pt . Proof Sketch: We perform a case analysis of all the tran\u00adsitions that \na serial program state can take. We then iden\u00adtify a corresponding transition that an equivalent parallel \nprogram state can take, and show that the new serial and parallel program states are equivalent. Theorem \n2 SObs(c) E PObs(c). Proof Skelch: A simple induction using lemmas 2 and 3. We next define the notion \nof equivalence for parallel program states. This definition is again intended to cap\u00adture precisely how \ntwo program states that differ only in their choice of allocated locations are equivalent. Definition \n14 (ml, Al, SI, Cl ) -p (mz, Az, S2, E2 ) W there exist bisections b~ : AI + Az, b~ : S1 + S2, b Dom \nml + Dom mz and bd : decl(A1 US1) + d~cl~A2 U S2) such that if b.. = b. U b, then d c1 d + b~(d) E2 h(d \n), and for all t= (cl, el, nl, sl, rl) E Al US1, z~ba$(t) = (cz, ez, n~, SZ,rz) then 1. cl = C2, 2. \n3b. (cl, ml, nl, sl, rl) =; (ez, mz, nz, s2, rz) and b$Domb~=b~, 3. V(di, rw,l) E .q. bd((di, rw, !)) \n= (di, rw, b~(l)) E s2.  We now examine the possibilities when two equivalent program states take tmnsitions. \nWe tirst show that if one state takes a transition, then so does the other. Lemma 4 If ptl is consistent, \npt2 is consistent, ptl-p pt2 and ptl +Ppt{ then 3ptj .pt2+P ptj. 114 Proof Sketch: A case analysis of \nthe axiom that generated ptl +Ppt~ reveals that ptz can always take a correspond\u00ading transition generated \nby the same axiom. We now characterize what can happen when equiva\u00adlent states both take a transition. \nThey key result is that two different transitions from equivalent paratlel program states commute. Lemma \n5 If pt ~ is consistent, ptz is consistent, PtI 5P Ptz> 1% +Pptj and pt2+p p~ then either 2. pt~= pt$or \n 3. pt{ = error, pt~+perror or 4. p% = error, pt{dperror or  Proof Sketch: Equivalent states have isomorphic \nsets of active tasks. If ptl and ptz tcmk transitions from iso\u00ad morphic tasks, the two program states \nptj and pt$ are equivalent. If one of the tasks genemted a transition to error, then the isomorphic task \ncan also generate that transition. If the two states both generated an integer resul~ then the definition \nof equivalence ensures that the results are the same. For case 5, the transitions came tim non-isomorphic \ntasks. An inspection of the axioms that define +P reveals that all of the active tasks in pt ~ and ptz \n(with the possible exception of the tasks that generated the transitions) are still active in pt~ and \npt~. Therefore, there is an active task in pt{ that is isomorphic to the task that generated ptz+ppt~ \nand vice-versa. These active tasks generate wsitions ptj +ppt~ and pt~+Ppt~. We go through a case analysis \nof the possible pairs of transitions to show that pt y SP pty. In effect, we must show that the two transitions \npt 1+P pt~ and pt~ -+p pt~ commute. The proof of commutativity relies on the def\u00ad initions of consistency \nand equivalence of parallel pro\u00ad gram states. We tirst show that the accesses to shared objeets of any \ntwo active tasks do not conflict. For an active task to write a shared objeet, it must deelare an immediate \nwrite on that shard object. By property 3 of definition 12 that declamation must be at the front of its \nqueue. Therefore, no other task s read or write declaration can be at the front of the queue, and again \nby property 3 of definition 12 all other tasks that deelare art immediate access on that object must \nbe suspended. No other active task em access the object. The effects of the two transitions on the shared \nmemory do not interfere, and therefore commute. We must also verify that queue operations carried out \nby two distinct tasks commute. Each task changes some subset of the program state s queues. If the tasks \nchange disjoint subsets, then their queue operations obviously commute. If the tasks change some of the \nsame queues, then their ag~gate queue operations commute if the op\u00aderations commute for every queue. \nWe therefore show that for any one queue, the two operations commute. If the two operations both modify \nthe same queue, then the two tasks both have sets of declarations in the queue. By definition 8 the sets \nare disjoint and one set comes before the other with respect to C. Queue removes and updates affect only \neaeh task s declarations; because the two se~ of declarations are disjoint the operations com\u00admute. Inserts \nput a new set of declarations into the queue just before the parent task s declaration se~ again because \nthe two tasks declarations sets are disjoint the operations colmmute. We next show that if any parallel \nexecution of a Jade program terminates without an error, then all paratlel ex\u00adecutions terminate and \nyield the same result. Theorem 3 Jjv c PObs(c) and v c Z, then PObs(c) = {v}. Proof Sketch: An induction \non the length of the reduction sequence using lemmas 2, 4 and 5. Together theorems 2 and 3 establish \nthe comx.pon\u00ad dence between the serial and parallel semantics. Theo\u00ad rem 2 says that if the serial execution \nof the program successfully hatts, then at least one of the parallel exe\u00ad cutions successfully halts \nwith the same result. Theorem 3 says that if one of the parallel executions successfully halts, then \nall of pamllel executions successfully halt with the same result. So, a parallel execution of a Jade \npro\u00ad gram will successfully halt if and only if the serial pro\u00ad gram successfully halts, and all such \nparallel and serial executions generate the same resuk The difference between the paratlel semantics \nand the serial semantics is that the parallel semantics may termi\u00ad nate in the error state when the serial \nsemantics does not terminate, and vice-versa. This ean happen if the program has two independent tasks \nt 1 and t2 such that tl runs forever and tz has an error. The serial semart\u00ad tics will always execute \none of the two tasks before the otlher, and will therefore always get the same result. The paratlel semantics, \nhowever, cart execute the two tasks in parallel, producing either an infinite loop or an error depending \non how the tasks transitions interleave. 4 Comparison with Other Work Explicitly parallel programming \nlanguages such as C!3P [4], Ada [9], Linda [2], OeCam [5] and Concur\u00adrentSmalltalk [12] force the programmer \nto manage con\u00adcurrency using low-level operations to synchronize parat\u00adlell tasks. IMs explicitly parallel \napproach often leads to complicated, nondeterministic programs that are difficult to debug and maintain. \nJade, on the other hand, adopts an implicitly parallel approach that maintains the program\u00adming advantages \nof serial languages. In [6] we present a detailed analysis of the differences between Jade and such explicitly \nparallel languages. FX-87 [8] is similar to Jade in that it contains con\u00adstructs that allow the programmer \nto express how the program accesses data. In FX-87, memory locations are partitioned into a finite, statically \ndetermined set of re\u00adgions. The programmer declares the regions of memory that a function touches as \npart of the function s type. The FX-87 compiler &#38;n then verify the correspon&#38;nce be\u00adtween the \ndeclared and actual data accesses, scheduling conflict-free pieces of the program for concurrent execu\u00adtion. \nMaking regions a static concept severely limits the amount of concurrency the implementation can ex\u00adtract \n[3]. At run time, multiple dynamic objects must be mapped to the same static region. Therefore, the com\u00adpiler \ncannot exploit concurrency available between parts of the program that access disjoint sets of objects \nfrom the same region. The other major difference between Jade and FX-87 is that FX-87 has no constricts \nfor identifying task bound\u00ad aries and synchronization points. It is the FX-87 com\u00ad piler s responsibility \nto partition the program into tasks. We are aware of no generally applicable algorithm for successfully \npartitioning programs containing side ef\u00ad fects. conclusion Jade programmers implicitly express parallelism \nby spec\u00ad ifying how a serial, imperative program uses data. Such an approach simplifies parallel programming \nand makes programs more portable across different parallel archi\u00ad tectures. The access specification \nis the key interface between the Jade programmer and the Jade implementation. The programmer uses Jade \nconstructs to generate an access specification for every task. It is the programmer s m\u00ad responsibility \nto ensare that the access specification de\u00ad clares atl of the task s accesses. Access specifications \ntherefore restrict how parts of the program can access data, Based on these restrictions, the Jade implemen\u00ad \ntation identities tasks whose accesses do not conflict and executes these tasks concurrently. Finally, \nto de\u00ad tect data races caused by incorrect access specifications, the Jade implementation dynamically \nchecks that each access specification is correct. In this paper we present both a sequential and a paral\u00ad \nlel operational semantics; these semantics formally define the meaning of Jade programs. The proof of \ncorrespon\u00ad dence between the sequential and parallel operatirmat se\u00ad mantics demonstrates that a Jade \nprogrammer can reason about parallel programs using a simple sequential pro\u00adgramming model. References \n[1] M. Berry et al. The perfect club benchmarks: Effec\u00adtive performance evaluation of supercomputers. \nIn\u00adternational Journal of Supercomputer Applications, 3(3):540, 1989. [2] N. Carriero and D. Gelernter, \nHow to Write Parallel Programs: A Guide to the Perplexed. ACM Com\u00adputing Surveys, 21(3):323 357, September \n1989. [3] R. T. Hamrnel and D. K. Gifford. FX-87 Perfor\u00admance Measurements: Datafiow Implementation. \nTechnical Report MIT/LCS/TR-421, MIT, Novem\u00adber 1988. [4] C. A. R. EIoare. Communicating Sequential Pro\u00adcesses. \nPrentice-Hall, Englewood Cliffs, N.J., 1985. [5] Inmos Ltd. Occam Programming Manuul. Prentice-Hall, \nEnglewood Cliffs, N.J., 1984. [61 M. S. Lam and M. C. Rinard. Coarse-grain par\u00adallel programming in Jade. \nIn Proceedings of the Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, \npages 94-105, April 1991. [71 D. Lenoski, J. Laudon, K. Ghmachorloo, A. Gupta, and J. L. Henrtessy. The \ndirectory-based cache co\u00adherence protocol for the DASH multiprocessor. In Proceedings of the 17th Annual \nInternational Sym\u00adposium on Computer Architecture, pages 94-105, May 1990. [8] J. M. Lucassen. *S and \nEffects: Towards the *Integration of Functional and Imperative Program\u00adming. Technical Report MIT&#38;CS/l_ \nR-408, MIT, August 1987. [91 United States Department of Defense. Reference Manual for the Ada programming \nlanguage. DoD, Washington, D.C., January 1983. ANSI/MIL-STD\u00ad1815A. [10] J. S. Rose. LocusRoutex A Parallel \nGlobal Router for Standard Cells. In Proceedings of the 25th De\u00adsign Automation Conference, pages 189 \n195, June 1988. [11] E. Rothberg and A. Gupta. Efficient sparse ma\u00adtrix factorization on high-performance \nworkstations -exploiting the memory hierarchy. To appear in ACM Transactions on Mathematical Software. \n [12] Y. Yokote and M. Tokoro. Concurrent Program\u00adming in ConcurrentSmalltalk. In A. Yonezawa and M. \nTokoro, editors, Object-Oriented Concurrent Programming, pages 129-158. MIT Press, Cam\u00adbridge, MA, 1987. \n A Abstract Syntax This section contains the abstract syntax for the sim\u00adple sequential imperative language \nSimple. For pur\u00adposes of exposition we use the access specification operations im-rd and inwr instead \nof rd and wr. i~,l? idG Id c E Bog ::= CO@result (Exp) c E Code ::= Stm4C ode / Js4Code ~c St E Stint \n;:= &#38;xz(ExP) I Id := EXP ~ Id:= pr(l?xp) I ~d~= sh(l?xp) ~ *Exp:= Exp I while (Exp) {CO&#38;-} if \n(Exp) {Code} else {Code} I jst 6 Jst ::= with {Code}cont I withonly {Code} do(Id,. . . , Id){ Code) \nSp c SpcZ ::= imrd I imwr I df-rd I df.wr [ no.rd I no-wr exp E Exp ::= Z ~Id~ Exp@IExp/ *EXP ~ is-pr(Exp) \n[ i s.sh(Exp) Op E Op ::=+l-IXI=I <I> B Expression Evaluation We define expression evaluation with the \nfollowing ax\u00adioms. iG2 iin(e, rn, n,s)=i idcDom e idin (e, m,n, s) = e(id) expin(e, m,n, s)=l EDomn \n*expin (e, m,n, s) = n(1) expin (e, m,n, s) = I EDom m, (ire, rd, 1) e s *expin (e, m,n, s) = m(1) expl \nin (e,m,n, s) = VI C 2, ex~ in (e,m,n, s)= vzEZ expl op ex~ in (e, m,n, s) = VI opv~, expin (e, m, n,s \n)=1 CDomn is-pr(exp) in (e, m, n,s) = 1, is-sh(exp) in (e, m, n,s) = 0 expin(e, m,n, s)=16Domrn is_pr(exp) \nin (e, m,n, s) = O, is_sh(exp) in (e, m, n,s) = 1 is-pr(exp) in (e, m, n,s) = O, is-sh(exp) in (e, m, \nn,s) = O These axioms may leave an expression s value unde\u00adfined if the expression or one of its subexpressions \nfails to satisfy the preconditions of one of the axioms. In this case the expression evaluates to the \nspecial value error indicating an evaluation error. C Simple Transition Relation + is defined to be the \nsmallest relation satisfying the following axioms. expin (e, m, n,s) = v c PrObj id:= expyin (e, m, n,s)+cin \n(e[id* v], m, n,s) expl in(e, m,n, s) = lcDom n, ex~ in (e, m,n, s) = v G PrObj *expl := ex~;cin (e, \nm, n,s)+cin (e, m, n[l I+ v], s) expl in (e, m,n, s) = IGDom m, (ire, wr, l) ~ s, ex~ in (e, m,n, s) \n= v c ShObj *expl := ex~;cin (e, m, n,s)+cin (e, m[l H v], n,s) expl in (e, m,n, s) = I e Dom m, (ire, \nwr, l) $?s *expl := ex~;cin (e, m, n,s)+error I G PrLoc\\Dom n,expin (e, m,n, s) = v E PrObj id:= pr(exp);cin \n(e, m, n,s)+ cin (e[id+ I], m,n[l I+ v], s) I G ShLoc \\Dom m,expin (e, m,n, s) = v G ShObj, s = SU {(df, \nrd,l), (df, wr, !)} id:= sh(exp);c in (e, m, n, s)+ cin (e[id~ I], m[l I+ v], n,s ) expin (e, m,n, s) \n= 1 if (exp) {cl} else {cz};e3 in (e, m, n,s)+ cl;q in (e, m, n,s) expin (e, m,n, s) = O if (exp) {cl} \nelse {c2};e3 in (e, m, n,++ c2;La in (e, m,n, s) expin (e, m,n, s) = 1 while (exp) {cl };CZ in (e, m, \nn, s)+\u00adcl;while (exp) {c1};cz in (e, m, n,s)  expin (e, m,n, s) = O while (exp) {Cl};CZ in (e, m, n,s)+cz \nin (e, m, n,s) I)efinition 15 subexp(exp, st) is true if exp appears in an expression of sl. exp in \n(e, m, n,s) = error, subexp(exp, st) s~e2 in (e, m, n,s)-+error D Jade Transition Relation F ParaIIeI \nTransition Relation +-P is defined tQ be the smallest relation satisfying the following axioms. following \naxioms. +j is defined to be the smallest relation satisfying tie cin (e, m, n,s)+ti in (e , m , n , \ns ) t=(c, e,n, s,r) CA, cin (e, m,n, s,r)+jd in (e , m , n , s , r) cin (e, m, n,s, r)+.jd in (e , m \n, n , s , r ) (m,&#38;s, C)-+p Gin (e, m,n. s\\+error (m , A\\{t}U{(d, e , n , s , r )}, S,C) cin (e, m, \nn,sjr)+jerror t = (c, e,n, s, r) c A,cin (e, m, n,s, r)+jerror q in (e, m,n, s)+~ in (e , m , n , s ) \n(m, A, S, c )+Perrorwithonly {Cl}do(kis){cz};cs h3 (e, m, n,s, r)+j withonly {dl}do(ids){c2};c3 in (e \n, m , n , s , r) t = (with {e}cont;c, e, n,s, r) G A, c1 in (e, m,n, s)+dl in (e , m , n , s ) t =(c, \ne,n, s~r, O) with {Cl}cont;cz in (e, m, n,s, r)+j (m, A, S,C)+P with {dl}cont;q in (e/, m , n , s , r) \n(m, A\\ {t}, S W{t }, upd(s, s T r, c)) c = withonly {di.rw(exp);cl }do(ki${cz};cs, c = withonly {c}do(idl, \n. . . , i&#38;){cl };c2, expin (e, m,n, s) = I EDom m, t=(c, e,n,s, r)~A, r = ru{(di, rw,l)}, s!-(di, \nrw,l) Vi < n.ida G Dom e and e(id) $4 PrLoc cin (e, m, n,s, r)+~ e = [idl w e(idl)] . . . [kin w e(ik)], \nwithonly {cl }do(iis){cz};cs in (e, m, n,s, r ) t = (cl, e , O,O t r, O), t = (c2, e,n, s,0) (m,&#38;$E)+P \n(m, A\\ {t}, S w {t , t }, ins(0 I r,s, C)) expin (e, m,n, s) = I eDom m,sl-(di, rw, l) with {di-rw(exp);cl \n}cont;cz in (e, m, n,s, r)+j with {c1}cont;c2 in (e, m, n,s, r U {(di, rw, i)}) t=(e, e,n, s,r)e A, \n(m, A, S, c )+P(m, A \\ {t}, S, rem(s, c )) (m, (di-rw(exp);c, e, n,s, r) o ts)+jerror , t=(c, e,n, s,r)CS, \n Definition 16 (cl, ml, nl, s1, rl) s; (ez, mz, nz, s2, rz) z~ V(im, rw, 1) C s.f((im, rw, 1), C) (eljm, \nw,sl) =b (e2, m2, n2, s2) ind(el, m,m,h) =b (m, A,S, C )+P(m, AW {t}, S\\ {t}, E ) (e2, m2, n2, r2) c \n= result (exp), expin(e, m,n, s)=v CZ E Serial Transition Relation (m, {(C, e,n, s, r)}, O,C )+Pv +S \nis defined to be the smallest relation satisfying the following axioms. cin (e, m, in (e , m , n , s \n, r ) 11, s,r)=+jd (m, (c, e, n,s, r) ots)+.(rn , (d, e , n , s , r ) ots) cin (e, m,n, s,r)--+jerror \n(m, (c, e, n,s, r) O ts)+$error = with {c}cont;c (m, (c)e, ~,s, r))+s(m, (c?, e, n,s T r,O, )) c = withonly \n{e}do(kl, . . . ,id. ){cl};cz, Vi ~ n.i@ c Dom e and e(id~) @ PrLoc, e = [idl t+ e(idl )] . . . [idn \nI+ e(idn )] (m, (c, e, n,s, r) o ts)-+, (m, (c~, e ,0,0 T r,O) o (c2, e,n,s,0) Ots) (m, (e, e,n, s,r) \nots)+. (m, ts) c = result (exp), expin (e, m,n, s) = v e 2 (m, (c, e,n, s,r))-+,~   \n\t\t\t", "proc_id": "143165", "abstract": "<p>Jade is a language designed to support coarse-grain parallelism on both shared and distributed address-space machines. Jade is data-oriented: a Jade programmer simply augments a sequential imperative program with declarations specifying how the program accesses data. A Jade implementation dynamically interprets the access specification to execute the program concurrently while enforcing the program's data dependence constraints, thus preserving the sequential semantics.</p><p>This paper describes the Jade constructs and defines both a serial and a parallel formal operational semantics for Jade. The paper proves that the two semantics are equivalent.</p>", "authors": [{"name": "Martin C. Rinard", "author_profile_id": "81100087275", "affiliation": "", "person_id": "P192533", "email_address": "", "orcid_id": ""}, {"name": "Monica S. Lam", "author_profile_id": "81100237956", "affiliation": "", "person_id": "PP14092336", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143165.143189", "year": "1992", "article_id": "143189", "conference": "POPL", "title": "Semantic foundations of Jade", "url": "http://dl.acm.org/citation.cfm?id=143189"}