{"article_publication_date": "02-01-1992", "fulltext": "\n Abstract Semantics for a Higher-Order Functional Language with Logic Variables Radha Jagadeesan Imperial \nCollege, London, UK SW7 2BZ. Abstract Although there is considerable experience in using lan\u00ad guages \nthat combine the functional and logic program\u00adming paradigms, the problem of providing an adequate se\u00admantic \nfoundation for such languages has remained open. In an earlier paper, we solved this problem for first-order \nlanguages by reducing the problem to that of solving si\u00admultaneous fixpoint equations involving closure \noperators over a Scott domain and showing that the resulting se\u00admantics was fully abstract with respect \nto the operational semantics [4]. These results showed that the first-order fragment could be viewed \nas a language of incremental definition of data structures through constraint intersec\u00adtion. The problem \nfor higher-order languages remained open, in part because higher-order functions can interact with logic \nvariables in complicated ways to give rise to behavior reminiscent of own variables in Algol-60. We solve \nthis problem in this paper. We show that in the presence of logic variables, higher-order functions maybe \nmodeled extensionally as closure operators on function graphs ordered in a way reminiscent of the ordering \non extensible records in studies of inheritance [1]. We then extend the equation solving semantics of \nthe first-order subset to the full language, and prove the usual sound\u00adness and adequacy theorems for \nthis semantics. These results show that a higher-order functional language with logic variables can be \nviewed as a language of incremental definition of functions.  Introduction The benefits of combining \nthe functional and logic pro\u00adgramming paradigms are manifold; for example, the pro\u00adgrammer gets the power \nof incremental definition of data structures, which goes a long way towards solving the This research \nwas performed at Cornell University un\u00adder an NSF Presidential Young Investigator award (NSF grant CCR \n8958543), NSF grant CCR-9008526, and a grant from the Hewlett-Packard Corporation. Corre\u00adspondence regarding \nthis paper should be sent to pin\u00adgali@cs.cornell. edu. Permission to copy without fee all or part of \ntlds rnsterial Keshav Pingali Cornell University, Ithaca, NY 14853. copy overhead of pure functional \ndata structure construc\u00ad tion [7,9,13,11,2]. However, it has proved difficult to find a suitable semantic \nfoundation for such hybrid languages, which is ironic since pure functional and logic programs can be \ngiven simple abstract semantics as functions and relations over values. In previous work, we had provided \nsuch a foundation for the first-order case by reducing the problem to that of solving simultaneous fixpoint \nequations involving clo\u00adsure operators over a Scott domain [4]. Using this device, we were able to provide \na denotational semantics that is fully abstract with respect to the operational one. These results showed \nthat a first-order functional language with logic variables can be viewed as a language in which data \nstructures are defined through constraint intersection. For a number of reasons, the problem of giving \nsuch a se\u00admantics to a higher-order functional language with logic variables seemed intractable. As we \nshow in Section 2, higher-order functions can interact with logic variables in very complicated ways \nto give rise to behavior remi\u00adniscent of own variables in Algol-60. Furthermore, these languages are \ninherently parallel in the sense that any correct interpreter must either be parallel or must sim\u00adulate \nparallelism. Logic variable instantiation is like a globally visible side-effect and modeling the combination \nof concurrency and side-effects usually requires complex notions like powerdomains. Inspite of these \napparent dif\u00adficulties, we show here that in the presence of logic vari\u00adables, higher-order functions \nmay be modeled extension\u00adally as closure operators on function graphs with an or\u00addering reminiscent of \nthe ordering on extensible records in studies of inheritance [1]. Using this tool, we are able to construct \na pleasing equation solving semantics for these languages and prove the usual soundness and ade\u00adquacy \ntheorems. Our results extend the equation-solving paradigm that underlies Kahn semantics for dataflow \nnet\u00adworks [6] to a more expressive setting with higher order constructs and shared memory; this allows \nthe communi\u00adcation abilities of processes to change dynamically, unlike the Kahn model of dataflow in \nwhich the channel struc\u00ad 1Although we did not consider non-determinism, it has been shown recently that \nour results extend to a first-order language with committed choice non-determinism [12]. is granted provided \nthat the copies are not made or distributed for direct commercial advantage, the ACM copyright notice \nand the title of the publication and its date appear, and notice is given that copying is by permission \nof the Aaaoeiation for Computing Machinery. To copy otherwise, or to republish, n@res a fee red/or sprcitic \npermission. @ 1992 ACM 089791453-8192/0001/0355 $1.50 ture of networks is fixed and cannot be altered \nduring runtime. The rest of the paper is organized as follows. In Sec\u00adtion 2, we discuss two programs \nthat serve to introduce the main issues and shed light on some of the difficulties in giving an abstract \nsemantics for a functional language with logic variables. These programs are written in Id, a dataflow \nlanguage that will serve as a concrete language in this paper. Section 3 gives a formal state transition \nsemantics for Id programs. The abstract semantics is de\u00adfined in Section 4. The correspondence between \nthe op\u00aderational and denotational semantics is described in Sec\u00adtion 5. For lack of space, we omit proofs \nand detailed dis\u00adcussions from this paper and refer the interested reader to a companion technical report \nfor details [5]. 2 Informal Introduction to the Language This section introduces Id [9] and its operational \nseman\u00adtics informally through a number of programming exam\u00adples. The core of the language is functional \nand logic variables are introduced through an array construct [3]. An array with uninitialized logic \nvariables as its elements is allocated by the expression array(e) where e is an integer-valued expression \nspecifying the size of the array. Array updating is performed by a definition of the form A [i] = v. \nThe value v is unified with the value contained in A [i] and the resulting value is stored into A [i]. \nThus, if A [i] was undefined (i.e., it was an uninitialized logic variable), the execution of this definition \nresults in the value v being stored in A [i]. If unification fails, the en\u00adtire program is considered \nto be in error. An element of an array may be selected by A [i]. We permit an unini\u00adtialized variable \nto be returned as the result of executing a program. Here is a simple Id program: {A = array (lO) ; A[l] \n= 2; fill-even(A,5) ; fill-odd(A,4) ; in A] def fill-even (X, h) = {for i from 1 to h do X[2*1] = X[2*i-1]*2 \nod} def fill-odd(X, h) = {for i from 1 to h do X[2*i+l] = X[2*i]*2 od} interleaved. Fortunately, the \nviewpoint of constraints pro\u00advides a nice way to mask this operational complexity. For example, the definition \nA = array (10) can be viewed as a constraint that is satisfied by any array A of size 10 (and by an overdefined \nelement, T, which trivially satisfies all constraints). We can think of fill-even and fill-odd as constraining \nthe even and odd elements of the array A, with A being produced by the intersection of these con\u00adstraints \nwith the constraints A = array ( 10) and A [11 = 2. The denotational semantics formalizes this viewpoint \nof constraints. Higher order functions and logic variables This example illustrates the interaction between \nhigher\u00adorder functions and logic variables. Consider the pro\u00ad gram: {A = array(2); g=f A; ti=gi; t2=g2; \nin A] deff Xi= {X[i] = i in 0} The result of this program is the array [1,2]. In this pro\u00adgram, f is \na curried function which takes its arguments one at a time; the first argument must be an array and the \nsecond, an integer. When this function is applied to an array, it returns a function that can be applied \nto an integer; if this new function is applied to the integer i, element i of the array gets updated \nto i. In other words, g, the result of applying f to A, has the array A embedded inside it, and this \narray gets updated each time g is ap\u00adplied. This is reminiscent of the behavior of own variables in a \nlanguage like Algol-60. Furthermore, the applications of g need not be in the same scope as its introduction: \nwe can pass g to another function and apply it inside that function. Notice also that the right hand \nside of the definition of A (that is, array(2)) cannot be substituted for A ev\u00aderywhere in the program \nwithout altering the meaning of the program. Unlike in pure functional languages, object identity is \nimportant; in the operational semantics of Sec\u00adtion 3, the definition of A will be allowed to take part \nin constraint solving only after the right hand side has been reduced to an array of two logic variables \nof the form [L1,L2]. When executed on a dataflow simulator, this program produces an array of length \n10 in which the i th element is 2i. Procedure fill-even fills in the even elements of ar\u00adray A by reading \nthe odd elements and multiplying them by 2 and procedure fill-odd works similarly, Notice that this program \ncannot be executed sequentially (that is, like a PASCAL or FOR TRAN program); instead, compu\u00ad tations \nin the calls to fill-even and fill-odd must be Syntax For the purpose of this paper, we define a core \nlanguage whose syntax is shown in Figure 1. To avoid getting over\u00adwhelmed by subscripts and ellipsis, \nwe have made this lan\u00adguage very simple while retaining all essential constructs. The main differences \nbetween Id , as presented earlier in the examples, and the core language are as follows. The program \n::= exp clef-list ::= def lde~def-list def ::= id = exp exp ::= const I id I expl op exp2 [ if expl then \nexp2 else exp3 I array(exp) I expl[exp2] \\ expl exp2 [ (Az. exp) I clef-list in exp Figure 1: Syntax \nof Id loop construct is eliminated since a loop can be replaced by a tail recursive function. To simplify \nnotation, we will require that all functions return a result. It is convenient to assume that the left-hand \nside of a definition is an iden\u00adtifier; a definition in Id of the form el [e2] = e3 can be replaced by \ntwo definitions x = e I [e2] ; x = e3 where x is a new identifier. We will assume that all local vari\u00adables \nhave been made into parameters so that the body of a function does not introduce any new names. We assume \nthat the language is simply typed, and that the expressions are typed correctly in the usual sense. Arrays, \nbooleans and integers are considered to be of base type. For definitions of the form x= e, x must have \nthe same type as e. In the rest of this paper, we will ignore the details of typing. Since we do not \nperform unification of A-abstractions, we impose syntactic restrictions to ensure that there are no multiple \ndefinitions of functions: if x in the abstraction Ax.exp is of higher-order type, then x cannot occur \nby itself on the left hand side or the right hand side of a definition. We refer the interested reader \nto the companion technical report [5] for details. 3 Operational Semantics of Id In this section, we \ngive an operational semantics for Id us\u00ading Plotkin-style [10] state transition rules. The state of the \ncomputation is represented by a conjuration where a configuration is a quintuple < D, e, pr, p, FL >. \nD con\u00adtains definitions whose right-hand sides have not yet been completely reduced to an identifier, \nconstant, array, or an abstraction of the form Az.exp. The expression e in the configuration is the expression \nwhose value is to be pro\u00adduced as the result of the program. Configurations are rewritten by reduction \nand by constraint solving. Once the right-hand side of a definition in D has been reduced completely, \nthe definition can participate in constraint solving. Configurations have two components named pF and \np which keep track of such definitions. When the right hand side of a definition in D reduces to a A-abstraction, \nit is moved into pp, the function environment. Since ~\u00adabstractions are not unified, an identifier bound \nto a A-Abstraction by a definition cannot occur on the left hand side of any other definition; hence, \npF is simply a list of identifier/A-abstraction pairs. The second component, p, called the environment, \nkeeps track of bindings between identifiers and base values (identifiers, constants and ar\u00adrays) and \nhas a more complex structure to permit uni\u00adfication it consists of a (possibly empty) set of alias\u00adseis \nwhere an alias-set is an equivalence class of base val\u00adues. For example, {x, y, z}, {x, y, 4} and {z, \nY, [Ll, L2]} are alias-sets. If unification fails, the configuration is rewritten to Error and computation \naborts. The transition rules for configurations are specified in terms of a bhary relation ~ on the set \nof configura\u00adtions. In any program P, let ezpp be the expression to be evaluated. The initial configuration \nfor program P is < ~, expp, ~, ~, Id >. We define some syntactic cate\u00ad gories required for the operational \nsemantics. The nota\u00adtion [z1,..., Zn] for arrays represents a sequence of one or more identifiers. Ce \nConfigurations ::= < D,e, pF, p,FL > I Error Dc Defs ::= #ldefl, . . . . def~ et expression pFcFunction.env \n::= ~1{~1 = kl.el, . . . . .fn = ~x~.e~} pc Environment ::= @l{ Al,...,&#38;} A6 Alias-set ::= {Bl, . \n. . . l?n} Bc Base-value ::= xlclAr x, Lc Id = set of identifiers Are Array ::= [X,,..., xn] FLc Free-list \n= T(Id) The unification algorithm we use is similar to the one in Qute [13]. No occurs-check is performed; \ninfinite data structures are considered to be legitimate objects of com\u00adputation. The unification algorithm \nis defined in terms of a binary relation -on environments. Definition 1 * is a binary relation on environments \nde\u00adjined as foilows: 1. If Al and A2 are members of an environment p, and Al and A2 have an identifier \nin common, then p * (p -{Al} -{A2}) U {Al U A2}. .2. If {[xl,...,%], [w, . . ..wJ} C Acp then p --+ p \nu {{x,, y,},..., {zn, yn}}. Intuitively, these transformations leave the meaning of an environment unchanged. \nIf pl -PZ and PI *P2, then PI is said to be reducible; otherwise, it is irreducible. Let Abe the reflexive \nand transit ive closure of -. It can be shown that for every environment p, there is a unique, irreducible \nenvironment pl such that &#38;pl [13]. If p is a syntactic environment and A is an alias-set, let U(p, \nA) denote the unique, irreducible environment such that (pU {A})-%4(P, A). We will need an operation \nthat is similar to environ\u00ad ment look-up in functional languages. In a functional language, an environment \nis considered to be a function from identifiers to values. In our system, the function environment pF \ncan be interpreted the same way. The rewrite rules have been designed so that in any configu\u00adration that \nis not Error, the environment p is irreducible. This means that every identifier that is not in the free-list \nis an element of exactly one alias-set. Definition 2 Let < D, e, ~F, p, FL > be a configuration and x \nbe an identifier not a member of FL. Let p be consistent. The function V(z) is dejined by cases on the \ntype of x: 1. x is a variable of base type: Let A be the (unique) alias-set that contains x. V(x) is \ndejined by cases depending on A: All the elements of A are identifiers. In this case, V(x) is undejined. \n At least one element of A is a constant c, The elements of A are either identifiers or the con\u00adstant \nc. We define V(z) to be c.  At least one element of A is an array. The el\u00ad  ements of A are either \nidentifiers or arrays of the same length. V(x) could be defined to be any one of these arrays. To be \nprecise, place a lexi\u00adcographical ordering on identifiers and let V(x) be the array whose first element \nis the least in this ordering. 2. x is a variable of a function type: In this case, V(x) is L where x \n= L is the unique definition of x in pF. The operational semantics for Id is given in Figures 4 and \n5. The first rule replaces free occurrences of a first order variable x by V(x) in any context, if V(x) \nis defined. Arbitrary contexts are denoted by Cfl in this rule. Most of the other clauses in this semantics \nare self-explanatory. The two sides of a conditional expression play no role in the computation until \nthe predicate has been evaluated to true or false. Unlike in functional languages, function application \ncannot be implemented by a copy of the body of the function in which occurrences of the formal param\u00adeter \nare substituted by copies of the actual parameter. Instead, a definition is created for the actual parameter \nand the actual parameter is substituted for the formal pa\u00adrameter only when it haa been completely reduced \nto a base value or function. 4 Abstract Semantics This section describes the abstract semantics for \nId. First, we give an informal overview of our approach. We discuss the first-order semantics which views \ndata structure con\u00adstruction as constraint intersection, and we relate com\u00adputing with constraints to \nthe solution of systems of si\u00admultaneous equations involving closure operators. This part of the paper \nis a summary of results reported in an earlier paper [4]. Then, we show how the higher-order case fits \ninto this picture, Next, we give a formal ac\u00adcount of the construction of various domains needed for \nthe formal semantic account. Finally, we present the for\u00admal semantics. 4.1 Informal Introduction 4.1.1 \nFirst-order Language Consider the following Id program: {A = array(3); A[i] = 2; A[2] = 1; A[3] = 3; \nin A} The definition A = array(3) is viewed as a constraint that gives partial information about A -any \narray of length 3 satisfies this constraint. Similarly, A [11 = 2 is a constraint sat isfied by any array \nwhose first element is 2. How should we describe equational constraints for\u00ad mally? The usual powerdomain \nconstructions are of no help here. For example, the Smythe powerdomain [15], consisting of upward closed \nsets, is designed to describe sets of values satisfying constraints of the form z P a. The set of values \nin a domain satisfying an equational constraint is not, in general, an element of the Smythe powerdomain. \nConsider the constraint x = y. What sets of pairs satisfy this constraint? Certainly not an upward closed \nset because, for example, (1, 1) satisfies the con\u00adstraint but (2, l-) does not satisfy it. To motivate \nthe formal model of constraints, note that the basic mechanism by which constraints get imposed in Id \nis through unification. Each time unification is per\u00adformed, new constraints are imposed on some variables \nand this adds to the information content of the vari\u00adables. Such functions are obviously extensive functions. \nImposing a constraint twice is no different from imposing it once; therefore, functions modeling imposition \nof con\u00adstraints should be idempotent. Finally, we want the func\u00adtions to be monotonic and continuous \nsince the process of generating constraints is supposed to be computable. First, we formalize the notion \nof information content . If B is the domain of elementary values such as integers and booleans, consider \nthe domain of both basic values and arrays, which can be described informally by the do\u00admain equation: \nW= B+ W+ WXW+WXWX W+... In the infinite sum, the component B represents elemen\u00adtary values, the component \nW represents arrays of length 1, the component W x W represents arrays of length 2, etc. Notice that \narray elements come from the domain W itselfi therefore, array elements can be arrays themselves, and \nthe domain includes infinitely nested arrays. To this domain, we add an element labeled T which is a \nspe\u00adcial value that models error, the result of (contradictory) L The domain V 1\u00ad -L The closure operator \nfor array (3) Figure 2: The Domain V and a Closure Operator definitions. A pictorial representation \nof the resulting do\u00admain, which we call V, is shown in Figure 2. Arrays of different lengths are incomparable. \nIf al and a2 are two arrays of the same length, we say that al ~ a2 if a2 can be obtained by replacing \noccurrences of 1 in al by other values from W. For example, the least defined array of length 3 is [1,1, \n-L] and it is below [2, &#38; J-] etc. The error element T is above all other values in V. This do\u00admain \nis constructed formally in the companion technical report [5]. We can model constraints using closure \noperators [14]. Definition 3 A closure operator, f, on a domain V is a continuous function satisfying, \n(i) Vz ~ V. x ~ f(x), (ii) fof=f. As an example, consider the definition x = array(3). The elements of \nV that satisfy the constraint on x are easily seen to be solutions of the equation x = (Au.u U [L, J-, \nJ-])z. Note that ~U.U u [J-, L, L] is a closure operator. A pictorial representation of this func\u00adtion \nis shown in Figure 2 it maps 1 to [1,1, 1], the least defined array of length 3, it maps T and all arrays \nof length 3 to themselves, and it maps all other values in V (such as basic values and arrays of length \nother than 3)to T. Now that we can model constraints as closure opera\u00adtors, we need to understand how \nto model simultaneous imposition of constraints. The following lemma provides the answer. Lemma 1 1~~ \n: V + V and g : V ~ V are c{osure op\u00ad erators, any solution to the system of simu ltaneous equa\u00ad tions \n z = f(z) z = g(z) is a solution of the equation x = f(g(x)) and vice versa. The least common solution \nof the system of equations is the limit of the sequence J-, f(g(l)), f(g(f(g(l)))), ... This lemma lets \nus talk meaningfully about the least solution of a set of fixpoint equations. One interpretation of this \nlemma is that U(f o g)n is the smallest closure operator above f and g; hence, simultaneous imposition \nof constraints can be modeled using least upper bounds of closure operators. The abstract semantics of \nthe first-order language mod\u00adels definitions as closure operators on environments where environments \nare functions from identifiers to V. The in\u00adterpretation of expressions is more subtle. From our pre\u00advious \ndiscussion, the expression array (3) can be inter\u00adpreted as the function Au.u u [1., 1, 1]. Thus, array(3) \nis a closure operator of type V ~ V. In general, we have to give meaning to an expression of the form \narray(e) where e can impose constraints on the environment; so, the meaning of an expression is a closure \noperator of type (v x Eivv) + (v x Eivv). 4.1.2 Informal discussion of higher-order seman\u00adtics Consider \nthe following version of the example discussed in Section 2: deff Xi={ X[i]=i inO] {A = array(2) ; ----(5)g=f \nA; ----(6) ti =g 1; --__(7) t2 =g 2; in Al Function g, the result of applying f to A, has the array \nA embedded inside it, and this array gets updated each time g is called. The result of the program is \nthe array [1,2]. In a pure functional language, higher-order functions are modeled by currying first-order \nfunctions. It is worth understanding why currying is inadequate for model\u00ading the higher-order part of \nId. Consider the function F = A (x, y) .e[z, y] which represents a function that ac\u00adcepts as input a \npair, say of type D1 x D2, and returns an element of type D3, If v is of type D1, the function G = ((curry \nF) v) is of type Dz ~ Da. This type does A 9 {<1,1 >--< IJ. >} {<2,1 >+<2J >} <2,1>+D.40>,<2,()> -<2,0>} \n1 Figure 3: Dataflow graph for example not model the behavior of functions in the presence of logic variables \nsince it does not reflect the fact that v can get updated when the function G is applied, as in the ex\u00adample \nabove. In a pure functional language, the value of v does not depend on what happens to G and the func\u00adtion \nG is determined entirely by F and v. This is not the case once logic variables are introduced: in our \nexample, the value attained by array A depends on the arguments that g has been applied to. TO capture \nthis behavior, we extend the constraint point of view developed for the first-order semantics to functions. \nIn the higher-order semantics, function sym\u00adbols like f and g are given meanings as graphs of input\u00adoutput \npairs and lambda abstractions are given mean\u00adings as closure operators on these graphs. For exam\u00adple, \nthe graph of g will be a set of elements of the form (u, v) a (u , v ) where the u s and v s are integers. \nThe intuition is that each such pair represents a piece of in\u00adformation about g : given an approximation \nu to the ar\u00adgument and v to the result, g refines the argument to u and the result to v . Function graphs \nget refined through application and this refinement occurs in two ways the domain of the graph can increase \nor a particular element (u, v) ~ (u , v ) gets refined to (u, v) -+ (u , v ), where (u , v )~(u , v ). \nAs an example, consider Figure 3 which shows a dataflow-like representation of the example. Ap\u00adplication \nnodes are made explicit as App, and the term AX. Ai. X [i] =i in O is denoted by L. Initially, the graphs \nof f and g are { } and all other variables have the value 1. The two applications of g examine their \narguments and results and add the elements (1, 1) -+ (1, J-) ancl (2,1) -+ (2, l-) to graph of g. Also, \nthe node array (2) makes its output array [1, J-]: the array of two elements, both of which are undefined. \nThese values are shown at the top in Figure 3. The application node corresponding to g = f A col\u00adlects \nthe information about the graph of g and [1,1] and passes it up to the node labelled L. Note that the \nuse of graphs allows us to keep track of the arguments that the function has been applied to. The graph \npassed to f is ([1,1], {(1,1) + (1,1), (2,1)+ (2,1)}) -+ ([1,1], {(1,1) + (1,1), (2,1)+ (2,.L)}). This \nis refined by the node L to yield the graph ([L, L], {(1,1)+ (1,1), (2,1) -+ (2,1)})+ ([1,2], {(1,1) \n+ (1,0), (2,1)+ (2, o)}) This graph is passed down to the application of f. This application node in \nturn passes down a refined version of the graph of g, namely {(1, -L) + (1, O), (2,1) ~ (2, O)}. Furthermore, \nit refines the value on the edge connected to the node array (2) to [1, 2]. The new value of the graph \nof g is used to update values at the application sites of g. For example, the application node corresponding \nto the statement t2 =g 2 can now update t2 to O. The graphs at this stage are shown at the bottom in \nFigure 3. Re\u00adpeating these steps again does not alter any values. Note that the final result agrees with \nthe answer that the op\u00aderational semantics specifies. The domain of graphs and the notion of application \nfor graphs is specified formally in Section 4.2. As in the first-order case, definitions in the full \nlanguage are inter\u00adpreted as closure operators on environments. The type of expressions is also analogous \nto the first order case: an expression that produces a value of higher-order type (say al a Oz) will \nbe interpreted as a closure operator on the domain Zlcl _ ~z x ENV where Dwl _ g2 is the do\u00admain of graphs \nof type al ~ a2. This domain is specified more formally next.  4.2 The Domain of Function Graphs The \ndomains that arise in the semantic description are complete algebraic lattices. We denote the finite \nelements of a domain D by B(D). Given a set of ordered pairs S, define Dom(S) = {x I (3) (z, y) c S}. \n Let DI, D2 be two domains. We first define graphs of functions from D1 to D2. Informally, an element \nof Graphs(D1 a D2) can be thought of as the partial input-output relation of a continuous function from \nD1 to D2. Definition 4 The set of gruphs of functions from D1 to D2, denoted by Graphs(D1 ~ D2), is dejined \nas follows. Members of this set are sets S of elements of the form (x, c ), where z E B(D1), x ~ It(Dz), \nsatisfying: 1. Function: {(z, z ), (z, z )} ~ S+ (%, x UZ ) E S. 2. Monotonicity: [{(x, z ), (y, y )} \n~ S A I/~Z A z ~y ]+ (%, y ) E s. 3. Dom(S) is downward closed.  The first requirement ensures that \nwe can view graphs as encoding functions given an element in the domain of the graph, the corresponding \noutput is the most defined element associated with that element by the graph. Tak\u00ading advantage of this, \nwe will sometimes write Z1 ~ Z2 when the pair (xl, *2) occurs in a graph. The second re\u00adquirement ensures \nthat more input guarantees more out\u00adput. The final requirement clarifies the nature of the par\u00adtiality \n: when an element appears in the domain of the graph, all elements less than it also appear in the do\u00admain; \nthis is justified from the operational intuition that if we apply a function to an argument, we have \nin effect applied it to all values less defined than the argument. Note that there are elements S G Graphs(D1 \na D2) such that Dom(S) is not all of D1. Thus, elements of Graphs(D1 -D2) are to be distinguished from \nthe full input-output relation of a continuous function from D1 to D2 . For the semantics, we need graphs \nof closure operators. The following definition picks out the graphs that corre\u00adspond to closure operators \nby imposing the conditions of extensivity and idempotence. Definition 5 Let D be a domain. Then, the \ndomain of graphs of closure operators on D, denoted CG(D), is defined as follows. Elements of this domain \nare sets S c Graphs(D) that satisfy: 1. Estensivity: (z, z ) E S+-[it@ A d ~ Dom(S)] 2. Idempotence: \n{(c, z ), (z , z? )} ~ S=(Z, z ) E S  The ordering on elements of C~(D) is subset inclusion. Suppose \nthat we are given an element i in the domain of the graph: the first condition ensures that the cor\u00adresponding \noutput o is more defined than the input el\u00adement. It also ensures that the graph contains o in its domain. \nNow, using the second condition, we can deduce that the output for input o is no greater than the output \nfor i, thereby enforcing idempotence. Thus, an element of C~(D) can be thought of as the partialn input-output \nrelation of some closure operator on D. As before, the domains of elements of Cg(D) are not required \nto encom\u00ad pass the whole of D. In this light, the ordering S1 ZS2 among elements of C~(D) implies two \nflavors of inform~ tion: firstly, the domain of the graph S1 is contained in the domain of S2, and secondly, \non every input in the domain of S1 the graph S2 yields more refined output, Given a set S of pairs of \nelements from B(D), let ~ denote the closure of S under the requirements placed on function graphs; that \nis, it is the smallest element of C~(D) containing S. If S is a singleton set {x}, we will sometimes \nwrite T instead of ~. It is easy to check that C~(D) is a complete, algebraic lattice, with the empty \ngraph as the least element; least upper bounds given by ~1,S2 E C~(D)+S1US2 = ~; and B(C~(D)) = {Sjin \n}, where S f in is any finite set of pairs of elements from B(D). We can now define the domains required \nfor the semantics. Let V be the domain of base values defined earlier. The domains at various types are \ndefined inductively: Base: DO = V. Product spaces: Do, ~ OZ = Da, x Dcz Function spaces: Dal ~ ~2 = C~(DO, \nx D., ) Thus, elements of Dal ~ U2 are sets of elements of the form (z, V) ~ (~ , y ), where z, # ~ B(DOI), \nV, y E B(D~, ), satisfying the requirements of Definition 5. All of these domains are complete, algebraic \nlattices. Next, we define two useful auxiliary functions on the domains of graphs. The first function \nis an extension of the operator that performs closure under the requirements on func\u00adtion graphs. Let \nu E B(DO), v E B(DT). Let u E DO, vi E D, be such that UQU and v~v . Then, de\u00adnote by (u, v) -(u , v \n), the element of Da ~ r defined as follows: {(u, v)+ (~f, Yf)luGzfLu , vGYfGv} The second function, \nApp, defines the notion of appli\u00adcation for graphs. It takes three inputs and refines them to yield three \noutputs: view the first input as the graph of the function, the second input as the argument and the \nthird input as the result. The argument and result co\u00adordinates are updated in the natural way. Furthermore, \napplying the graph of a function changes the graph of the function itselfi it is updated to recordn the \nresults of this application. This is exactly the behavior we required in our informal discussion in Section \n4.1.2. We encourage the reader to check that the definition matches the be\u00adhavior of the application \nnode described in our informal discussion in Section 4.1.2. Let s E DU1~uq, t E D.,, u c D.,. Then, App(s, \nt, u) = (s , t , u ), where t = sU{($~,Yj) e (zt, Yj)l ~j~t AYjZU} (tt, ~:) = U{(z , y )1 (z, y)+ (z \n, yt) ~ s , x~t, ~~u} 4.3 The Semantic Clauses quences using the basic fact that a single reduction \nstep Figure 6 describes the denotations of definitions. The environment in which all identifiers are \nmapped to T is called envT. Some of the constraints are inequalities of the form a G z, where a is a \nconstant and z is being con\u00adstrained. These can be rewritten as z = (kc.a u x)x in which the lambda-abstraction \nis obviously a closure op\u00aderator. The notation lcs in front of a set of simultaneous equations involving \nclosure operators stands for the least common solution of that set of equations. Figures 7 and 8 describe \nthe denotations of all expressions except lambda abstraction. In the meaning of constants, the function \nK maps syntactic constants to their abstract equivalents. In the rule for conditionals, ez and e3 play \nno role if el is undefined. Function application is tricky since applica\u00adtion may cause the graph of \nthe function to change. To understand this rule write the application el (ez) using a prefix Apply operator. \nApp, the closure operator that is the meaning of Apply, was defined in Section 4.2 and enforces constraints \nbetween el, e2 and the output. Figure 9 describes the denotation of lambda abstrac\u00adtion. By checking \nfor a non-empty argument graph, the denotational semantics captures the fact that the body of a lambda \nexpression is accessed only when it is applied to an argument. If the argument graph is non-em~t y, we \nfirst compute the updated environment using the function UpdateEnv which essentially evaluates the body \nof the lambda expression in each environment obtained by bind\u00ading the formal parameter to an actual parameter \nobtained from a, the approximation to the graph. The new envi\u00adronment is used to compute the new value \nof the graph. The case of recursion is handled implicitly by the defini\u00adtion of the denotation of equations. \nThis is analogous to the handling of feedback loops by a fixpoint iteration in static determinate Kahn \ndataflow. The fixpoint iteration in this case is performed in the computation of the least common solution. \n  5 Relating the Semantic Defini\u00adtions In this section we outline the proof that the denotational semantics \nis correct for reasoning about the operational semantics. The interested reader is referred to a compan\u00ad \nion technical report [5] for full details. The proof extends extant proofs [4] for the first order language \nto a higher order setting. Reduction preserves meaning As a prelude to the main adequacy result, we \nshow that re\u00adduction preserves meaning. Once this is in hand, we prove that the results obtained operationally \nare indeed those predicted by the denotational semantics. These proofs proceed by induction on the length \nof computation se\u00adpreserves meaning. In order to show that one-step reduction preserves meaning we need \nto associate meanings with the basic entities used in the operational semantics, i.e. with con\u00adfigurations. \nThe semantic function M assigns to config\u00adurations a closure operator over the domain V x ENV. We use \nthe semantic functions&#38; and C defined previously. We define M[(D, e, p~, p, FL)] (a, env) (a, env) \n~ (b, env ) = lCS env = C[D U p U pF] env (b, env ) = $[ej (b, env ) { in (b, env ) The function M represents \nthe effect of the complete com\u00adputation on a configuration. We prove that as we rewrite a configuration, \nthe first order component of the result given by M does not alter. In particular, we show that ~-reduction \ndoes not alter the closure operator corre\u00adsponding to M. Thus the denotational semantics at\u00adtains the \nfirst order results predicted by the operational semant its. The Adequacy Theorem The hardest part of \nthe proof of full abstraction is the converse to what is outlined in the previous subsection; namely, \nthat every value predicted by the denotational se\u00ad mantics is attained by the operational semantics. \nStrictly speaking,we show that for every finite approzimant to the first-order results predicted by the \ndenotational se\u00admantics, there is a computation sequence that produces a more refined value at a finite \nstage. We first define a relationship ~ between first order syntactic expressions, e, and closure operators, \nf, on V x ENV. Intuitively, S[e] ~ e means that given any finite approximant to the result predicted \nby g[e], there is a finite sequence of reductions evaluating e in a suit\u00adable syntactic environment, \nthat produces a more refined value. In particular, if the result predicted by &#38;[e] is T, evaluating \ne in a suitable syntactic environment results in error. The proof that S[e] ~ e, for all first order \nexpressions e proceeds by structural induction on the expressions, and its details may be found in our \nearlier paper [4]. The sub\u00ad tle case is when one has parallel imposition of constraints. We make use \nof the fact that the semantic prescription for determining the least common fixed point of a pair of \nclo\u00ad sure operators suggests an interleaving of the reduction sequences of the subterms. More precisely, \nsuppose that gl and g2 are two closure operators that correspond to the imposition of two constraints \ngiven as sets of equa\u00ad tions El and E2. Suppose that we know how to construct reduction sequences corresponding \nto El and E2 individ\u00ad ually. Then, since we know that the least common fixed point of gl and g2 is the \nleast fixed point of (gl o gz), we can construct an interleaved reduction sequence of El and E2 corresponding \nto the computing the iterates of (910gz). In other words, thespecial form of the fixed point iteration \nprovides guidance about how to construct the interleaved reduction sequence. [5] The first order result \ncan be extended to the full higher order language and the details are given in the accom\u00adpanying technical \nreport [5]. This proof uses the idea of logical relations used inproofs of adequacy unfunctional languages. \nPrevious work [4] showed that the semantics [6] for the first order fragment was fully abstract. We be\u00adlieve \nthat with suitable restrictions on the graphs in the environment, full-abstraction for the full language \ncan be achieved. [7] 6 Conclusions and Related Work [8] We have given formal operational and denotational \nse\u00admantics for a higher order functional language with logic variables and shown that the denotational \nsemantics is [9]adequate with respect to the operational semantics. The closest work along these lines \nis that of Mantha, Lindstrom and George who have given a semantics for a lazy functional language with \nlogic variables [8]. How\u00ad [10] ever, this semantics encodes operational notions like sus\u00adpensions and \nin that sense, is somewhat less abstract than our semantics which is phrased purely in terms of func\u00adtions \nover value domains. It is possible that such opera-[11] tional notions are needed to model laziness, \nwhich is not required for the data-driven execution semantics of our language. Acknowledgments: We have \nhad stimulating discussions [12]with Gary Lindstrom and Surya Mantha on functional languages with logic \nvariables. We would like to thank Richard Huff and Wei Li for reading the paper carefully and correcting \nerrors in the text. [13]  References [1] H. Ait-Kaci. A Lattice theoritic approach to compu\u00ad[14] tation \nbased on a calculus of partially ordered type structures. PhD thesis, University of Pennsylvania, [15]1984. \n [2] H. Ait-Kaci, R. Boyer, P. Lincoln, and R. Nasr. Ef\u00ad ficient implementation of lattice operations, \nACM Transactions on Programming Languages and Sys\u00adtems, 11(1):115 146, January 1989.  [3] Arvind, R. \nNikhll, and K. Pingali. I-structures Data structures for parallel computing. ACM Transactions on Programming \nLanguages and Systems, 11, Octo\u00adber 1989. [4] R. Jagadeesan, P. Panangaden, and K. Pingali. A fully \nabstract semantics for a functional language with logic variables. In Proc. of the 1989 Logic in C omputer \nScience Conference, 1989. To appear in ACM Transactions on Programming Languages and Systems. R. Jagadeesan \nand K. Pingali. An abstract seman\u00adtics for a higher-order functional language with logic variables. Technical \nReport TR 91-1220, Cornell Uni\u00adversit y, 1991. G. Kahn. The semantics of a simple language for parallel \nprogramming. In Proc. of the IFIP Congress 74, pages 471-475, 1974. G, Lindstrom. Functional programming \nand the log\u00adical variable. In Proc. of the l% h ACM Symposium on Principles of Programming Languages, \n1985. S. Mantha, G. Lindstrom, and L. George. A seman\u00adtic framework for functional programming with con\u00adstraints. \nUnpublished Technical Report. R. Nikhil, K. Pingali, and Arvind. Id Nouveau. Tech\u00adnical Report CSG Memo \n265, M.I.T. Laboratory for Computer Science, 1986. Gordon D. Plotkin. A structural approach to oper\u00adational \nsemantics. Technical Report DAIMI FN-19, Aarhus University, 1981. U. Reddy. Logic Programming Functions, \nRela\u00adtions and Equations, chapter On the relationship be\u00adtween logic and funtional languages. Prentice-Hall, \n1986. V. Saraswat, M. Rh-tard, and P. Panangaden. Seman\u00adtic foundations of concurrent constraint program\u00adming. \nIn Proc. of the conference on Principles of Programming Languages, 1991. M. Sato and T. Sakurai. QUTE: \na functional lan\u00adguage based on unification. In Logic Programming: functions, relations and equations, \n1986. D. Scott. Data types as lattices. S1.4ii4 Journal of Computing, 1976. M. B. Smythe. Powerdomains. \nJournal of Computer and System Sciences, 16:23 36, 1978. Ident: 1. < D, C[x], pF, p, FL > e < D, c[v(~)/z], \np~, p, FL > if V(x) is defined Ops: 1. < D,e10pe2, pF, p, FL> d < D*, x10px2, pF, p*, FL* > where {XI, \nX2} ~ FL, FL* = FL -{Z1, X2}, p* = pU {{x1}, (z2}} D*= DU{zl=el, x2=e2} 2. <D, mopn, pF, p, FL> -<D, \nr,pF, p, FL> ifr=mopn Cond: 1. < D,cond(e~, ez, es), pF, p,FL > -i < D*, cond(zl, e2, es), pF, p*, FL* \n> where xlcFL, FL* =FL {xl}, p*=pU{{zl}}, D*=DU{X1=el} 2. < D, cond(true, e2, es), pF, p, FL > ~ < D, \ne2, ~F, p, FL > 3. < D, cond(false, e2, es), pF, p, FL > -+ < D, e3, pF, p, FL >  Arrays: 1. < D,array(e), \npF, p,FL > ~ < DU {x = e}, array (x), pr, p*, FL* > where xEFL, FL* = FL {x}, p* = pU {{z}} 2. < D,ar \nray(n), pF, p,FL > ~ < D, [Ll, . . .. Ln]. pF, p*, FL* > where Ll,..., Ln c FL, p* = p U {{ Ll},...{Ln}}, \nFL* = FL {Ll, . ..Ln} 3. < D,e1[e2], pF, p, FL > + < D*, x1[x2], pF, p*, FL* > where {X1,X2} ~ FL, \nFL* = FL {X1,X2}, p* = pU {{xl}, {zz}} D*= DU{xl=el, x2=e2} 4. < D, [Ll,..., Ln][i], pF, p,FL > -< \nD, Li, pF, p,FL > where 1~ i< n.  Function: 1.< D,e1(e2), pF, p,FL > a < DU {Xl = el, x2 = e2}, x1(x2 \n),p~, p, FL* > where {xl, X2} ~ FL, FL* = FL {xl, Z2} 2. < D,(Ax.e1)e2, p~, p,FL > ~ < D,y= e2 ine~, \np~, p*, FL* > where yEFL, FL = FL {y} where e; = el[y/x], P* = PU {{Y}} 3. <D, $=ezinel, pF, p, FL> \n+ <D*, el, pF, p, FL> D*= DU{z=e2}  Figure 4: Structured Operational Semantics of Id: Expressions < \nD,e, pF, p,FL >-< D*, e*, p~, p*, FL > Defs: 1. < DU {z =e}, el, pF, p,FL >+< D* U{x = e*}, el, p~, \np*, FL* > 2. < DU {Z = y}, e,~~, p, FL >+< D,e, pF, ~(p, {z, y}), FL > if z, y first order, Z4(p, {x, \ny}) is consistent.  < D U {x = y}, e,pF, p,FL >+ Error if z, y first order, 24(P, {z, y}) inconsistent. \n 3. < DU {z = c}, e,pF, p,FL >+< D,e, pF, ~(p, {z, c}), FL > if z first order, Z4(p, {z, c}) is consistent \n  < D U {x = c}, e,pF, p,FL >+ Error if x first order, U(p, {x, c}) inconsistent. 4. <Du{x=[L1 ,..., \nLn]}, e,p~, p, FL >+< D,e, pF, U(p, {z, [Ll, .... Ln]}), FL > (if Z4(p, {x, [Ll, . . . . Ln]}) is consistent) \n<Du{z=[L1 ,..., Ln]}, e, PF, p, FL >+ Error (otherwise) 5. < D U {F = h.el}, e,~F, p, FL >-+< D,e, pF \nU {F = Ax.el}, p,FL > Figure 5: Structured Operational Semantics of Id: Definitions ( env ~ env C[z = \ne] env = Ics (b, env ) = S[e] (b, env ) env [z] = b in env env ~ envl C[defl ; defz] env = lcs env = \nC[defl] env env = C[defz] env { in end Figure 6: Denotational Semantics of Id: Definitions (a, env) ~ \n(b, env ) S[const] (a, env) = lcs K(const)Q { in (b, env ) (a, env) ~ (b, env ) ~[z],(a, env) = lcs env \n[z] = b { in (b, env ) env~ env S[cond(el, ez, es)] (a, env) = lcs (b, env ) = g[e~] (b, env ) { in \ncase b of -b (a, env ) true : S[ez] (a, env } false : S[es] (a, env ) otherwise :< T, enUT > endcase \n(a, env) ~ (b, env ) (bl, env ) = t[el] (b,, env )S[el op ez] (a, env) = lcs (b2, env ) = $[e2] (b2, \nenv ) bl opb2~ b [ in (b, env ) (a, env) ~ (b, env ) (a,,a~~,, b) = @p(~,, a~~~, b)  &#38;[e~(ez)] (a, \nenv) = lcs (a~,~, env ) = ~[ez] (a=,,, end) (at, env ) = ~[e~] (at, env ) { in (b, env ) Figure 7: Denotational \nsemantics of Id: Expressions 365 Figure 9: Denotational Semantics of Id: Lambda terms (a,env) ~ (b, end) \n~[arra~(e)] (a, env) = ICS (b~, env ) == S[e] (b, env ) { AwatJ(b~)~ b in (b, end) S~[LI . . . Ln]~ (a, \nenv) = lcs { (a, env) ~ (b, env ) b[i] = ent.7 [Li]; i == 1.. .n in (~, end) in (b, env ) Figure 8: Denotational \nSemantics of Id: Array expressions S[k.e] (a, enti) if (a = 0) then (0, env) else let env = U@ateEnve(a)(env) \nin {U@ateGraphe(env ) (u), env } UpdateEnve((u, W) a (u , o )) A env. env[z t-+ ~] c env) Ics ~~ b { \n(b, env ) = ~~ej (b, env ) in env [z H env[x]] UpdateEnve({91 . . . 9.}) A env. env~env lCS { envt = \nUpdateEnve(~), i = 1.. .n U@uteEnue(S) UpdateGraphe(envl) ((u, v) ~ (u , v }) U@ateGraph (env ) ({gl \n. . . gn}]i = 1... n}) UpdateGraphe(env ) (S) 366 \n\t\t\t", "proc_id": "143165", "abstract": "", "authors": [{"name": "Radha Jagadeesan", "author_profile_id": "81100214384", "affiliation": "", "person_id": "P237380", "email_address": "", "orcid_id": ""}, {"name": "Keshav Pingali", "author_profile_id": "81100554731", "affiliation": "", "person_id": "PP39048331", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143165.143236", "year": "1992", "article_id": "143236", "conference": "POPL", "title": "Abstract semantics for a higher-order functional language with logic variables", "url": "http://dl.acm.org/citation.cfm?id=143236"}