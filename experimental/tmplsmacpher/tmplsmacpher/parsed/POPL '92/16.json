{"article_publication_date": "02-01-1992", "fulltext": "\n Principal Signatures for Higher-order Program Modules Mads Tofte Department of Computer Science Copenhagen \nUniversity (DIKU) Abstract Under the Damas-Milner type discipline for functional languages, every expression \nhas a principal type, if it elaborates at all. In the type discipline for ML Mod\u00adules, a signature expression \nhas a principal signature, if it elaborates at all. However, while functions can be higher-order in ML, \nparameterised modules in ML are first-order only. We present a type discipline for a skeletal higher-order \nmodule language which has prin\u00adcipal signatures. Sharing and multiple wiews of struc\u00adtures are handled \nin a manner which is compatible with the semantics of the first-order ML modules. Introduction Several \nprogramming languages have a notion of pro\u00adgram module. Typically, a module is either basic or parameterised. \nA basic modulle is called a package in ADA and a structure in Standard ML[5]; a parame\u00adterised module \nis called a generic package in ADA and a ~unctor in ML. In these two languages, a parame\u00adterised module \ncan at most take a basic module as parameter and yield a basic module as result, i.e. pa\u00adrameterised \nmodules are first-order. Our proposal is to admit higher-order modules, i.e. parameterised mod\u00adules whose \narguments and results might themselves be parameterised, To see the need for such a concept, consider \nthe modules in Figure 1. Standard ML syntax is used. First a signature (i.e. a structure type or inter\u00adface \n) called MONOID is declared. Then we define the functor Prod, which maps monoids N and M to their product \nmonoid, and use Prod in another func\u00ad tor Square which we finally a~pply to get a structure Plane. Because \nML is statically scoped, Square has no meaning unless Prod has already been declared. Con\u00adsider Figure \n2, however, which contains the declara- Perrnission to copy without fee all or /partof thk material is \ngranted provided that the copies are not made or distributed for direct commercial advantage, the ACM \ncopyright notice and the title of the publication and its date appear, and notice is given that copying \nis by permission of the Association for Computing Machbrery. To copy other\u00adwise, or to republish, requires \na fee an~d/or specific permission. tion of a functor signature PROD and a higher-order functor Sqriare. \nNow Square makes sense without ref\u00aderence to any particular functor; Square is thus easier to read and \nin addition it can be compiled separately regardless of whether Prod has been written or not. In short, \nwe propose to admit functors in struc\u00adtures and signatures. Functors are still mappinga from structures \nto structures, but since structures can con\u00adtain functors, functors now are higher-order. signature MONOID \n= s ig type t vale: t val plus: t*t->t end; f unct or Prod ( structure M: MONOID structure N: MONOID \n): MONOID= Struct type t =M.t * N,t val e = (Me, N.e) fun pius((zl ,z2), (yl,ly2))= (M. PhM(z~ ,YI) ,N.PIus(z2, \nY2)) end; f unctor Square (X: MONOID 1: MONOID= Prod(structure M = ~ structure N = X); structure Plane \n= Square ( Struct type t= real val e=0.0 fun plus<i,j) :real = i+j end) ; Plane .plus(Plane.e, (7.4,,5.4) \n); Figure 1: First-order functors The main theoretical challenge posed by higher\u00adorder modules is understanding \ntheir static semantics. @ 1992 ACM 089791-453-8/92/0001/0189 $1.50 2 Elaboration of Modules signature \nPROD = (structure M: MONOID structure N: MONOID 1: MONOID functor Square ( structure X: MONOID functor \nPros : PROD): MONOID= Prod(structure M = X structure N = X); Figure 2: A Higher-order Functor Harper \net aL[3] propose a very elegant type-theoretic module concept which allows higher-order modules. Unfortunately, \ntheir approach does not address shar\u00ading and multiple wiews of structures, both of which are important \nin ML. In the present paper we do treat sharing and multiple views of structures, and the treatment is \ncompatible with ML. Just as the static semantics of the Core of ML (i.e. the non-modules part) depends \ncritically on the ex\u00adistence of principal types, so the static semantics of ML modules relies critically \non the existence of prin\u00adcipal signatures, as explained below. The aim of this paper is to address what \nwe think is the most interest\u00ading theoretical problem posed by adding higher-order functors to ML: can \none extend the modules type disci\u00adpline to the higher-order language while maintaining that principal \nsignatures exist? The contribution of this paper is that we can answer this question in the affirmative \nfor a skeletal language. The semantic theory presented in this paper is in\u00adtended to be the simplest \npossible generalisation of the first-order theory. In one respect, the theory in its present form is \nrestrictive: it requires that when\u00adever a functor j with functor signature @l is matched against a functor \nsignature 02) we have 01 = 02. The relaxation of this restriction will be discussed below. Although the \nsignificance of principal type schemes in the Core is widely recognised, the fact that principal signatures \nare as crucial to the Modules as the prin\u00adcipal type schemes are to the Core is less well known. We shall \ntherefore briefly explain what principal signa\u00adtures are in Section 2. We then proceed to present the \nstatic semantics, leading to the statement of the prin\u00adcipality theorem in Section 3. In Section 4 we \nexplain admissification, the Modules equivalent of unification, and in Section 5 we present the signature \ninference algorithm. Finally, in Section 6 we explain why the restriction of our type discipline to the \ncase of first\u00adorder modules is not completely identical to the ML discipline. The word elaboration means \nthe static part of ex\u00adecution ; the term type inference is often used for the same thing, but the latter \nterm is not good for Modules, since other semantic objects than types are inferred. In this section we \nreview as much of the elab\u00adoration of Modules as is required to explain informally what principal signatures \nare. ML is a statically typed language, so the definition of equality of types is important. In languages \nwith a fixed signature of types, the issue of when two types are equal might be trivial, but ML allows \nuser-defined types, type abbreviations and type specifications, so equality of types is not a trivial \nissue. Equality of types cannot simply be identity of type constructors (i.e. program identifiers) for \nthe type construction allows one to have multiple identifiers for the same type. Therefore, ML has a \ndistinction between type constructors, which are program identi\u00adfiers, and type names, which are unique \nnames . The latter are internal to the semantics; testing of equal\u00adity of types tests equality of type \nnames, not equality of type constructors. Throughout this paper, name means unique name, not identifier. \nThe basic rule for introducing new type names is simple: dat at ype declarations introduce new type names, \ntype declarations do not. Now let us consider type specifications, for example the specification of the \ntwo types M.t and N.t in func\u00adtor Prod in Figure 1. We wish to elaborate the body of the functor under \nsome assumptions about M and N (the actual structures may not yet be available and in any case they could \nvary). Should we assume that M.t and N.t are equal or not? A moment s reflection reveals that they should \nnot be treated as identical, for one can easily produce actual structures corresponding to M and N which \nhave different ttypes. In general, we should only deem the body of a functor type cor\u00adrect if it would \nalso be type correct were we to use any real structures that match the parameter specifi\u00adcations instead \nof the formal structures. In particular, M and N should only have as much equality of types as any real \nstructures that match the parameter speci\u00adfications must have. The way to force equality between specified \ntypes is by the sharing specification. Fig\u00adure 3 shows a functor which would not be type correct without \na sharing constraint. A functor with multiple structure arguments is just a derived form of a functor \nwith a single structure argument which has substructures. (Figure 4 shows the signature of the argument \nof the functor from Fig\u00adure 3.) It therefore suffices to consider functors of the simple form functor \nF(X:sigexp) = body sigexp ::= f unct or structure F ( M: MONOID sig sigid spec end generative signature \nidentifier structure N: MONOID sharing Struct type M.i = N.t)= funsigexp (strid: ::= sigexpl ) : sigexp2 \nfunctor signature val x = M.plus(M. e, N.e) end; Figure 3: Example c~f type sharing SIGNATURB ARG= sig \nstructure M: MONOID structure N: MONOID sharing type M.t = N.t end; Figure 4: Signature with structure \nspecifications where sigexp is a signature expression. If we can elab\u00adorate sigexp to a formal structure \nS which has pre\u00adcisely the components and sharing that any structure which matches sigexp must have then \nwe can elaborate body assuming that X is bound to S without making assumptions that are not certain to \nhold when the functor is applied. If in addition S is uniquely deter\u00admined by sigexp then the elaboration \nof the body can be done without worrying about which S to choose for X in order to get the body tc, elaborate. \nML is designed in such a way that whenever sigexp elaborates at all then it elaborates to a structure \nS which has precisely the components and the sharing that any structure which satisfies sigexp must have. \nMoreover, S is unique, up to a certain kind of re\u00adnaming, as explained below. This crucial property of \nsignature elaboration is expressed formally in the principality theorem[4, App A] which states, roughly \nspeaking, that whenever sigexp can be elaborated at all, then it can be elaborated to a unique principal \nsignature. 3 A Skeletal Language We shall now define elaboration of higher-order func\u00adtors using structural \noperaticmal semantics. Most of the notation and terminology is exactly as in [5] and [4] but is repeated \nhere for expository reasons. The grammar of our skeletal language appears in Figure 5. We are primarily \ninterested signature ex\u00adpressions (sigezp). These can contain specifications (spec). Compared with Standard \nML, the one novelty Figure 5: Grammar spec ::= structure stri d: sigexp structure functor funid: funsigexp \nfunctor sharing longstn dl = longstridz sharing empty specl (;) spec2 sequential in Figure 5 is that \nwe allow specification of functors in our language. Functor specifications contain func\u00adtor signature \nexpressions, (funsigezp). For simplicity, all specifications but structure and functor specifica\u00adtions \nhave been omitted. Also notice that our skeletal language does not include structure expressions and \ndeclarations; in particular, there are no productions for functor declaration and functor application. \nApart from the fact that we wcmld allow the declaration of functors inside structures, there would be \nlittle of in\u00adterest in spelling out the details here, especially since our main aim is the principality \ntheorem, which con\u00adcerns signature expressions only. As in ML, we assume three disjoint classes of identi\u00adfiers, \nnamely SigId (signature identifiers), FunId (func\u00adtor identifiers) and StrId (structure identifiers), \nranged over by sigid, funid and strid, respectively. Substruc\u00adtures are referenced by long identifiers, \nfor example A. B is the B substructure of the structure A. The (;) in the last production means that \nthe semicolon is op\u00adtional. In Standard ML, one can specify sharing between structures and this implicitly \nimplies sharing of all type and structure components that are specified in both structures. Just as types \nhave type names, so structures have structure names and we say that two structures share if they have \nthe same name. The elaboration of a struct. . end expression produces a structure with a fresh name. \nStructure names are only used for representing sharing and they play no part in the dynamic semantics. \n3.1 Semantic Objects We shall now define the semantic objects, i.e. the objects that occur in the inference \nrules which will be presented below. The semantic objects are defined in Figure 6. m~ StrName Ne NameSet \n= Fin(StrName) GE SigEnv = SigId @ Sig Fe FunEnv = FunId ~ FunSig SE E StrEnv = StrId ~ Str S or (m, \nE) C Str = StrName x Env E or (F, SE) E Env = FunEnv x StrEnv Z or (N)S E Sig = NameSet x Str @ or (N)(S, \n(N ) S ) c FunSig = NameSet x (Str x Sig) Ae Asmb = Str U Asmb x Asmb Bor N, G,EG Basis = NameSet x SigEnv \nx Env Figure 6: Semantic Objects structure name (m2, ( functor environment {}7 structure environment \n{}, {t I+ real* real} type environment {e ~ real* real, variable environment plus l-+ a})) Figure 7: \nThe structure Plane A structure environment SE is a finite map from structure identifiers to structures; \nwe write Dom SE for the domain of SE. Similarly for signature en vi\u00adronments G and functor environments \nF. A (static) structure S is a pair (m, E), where m is the name of the structure and E is an environment, \nwhich gives the static information about the components of the structure. To term such an object static \nstruc\u00adture (ss opposed to signature) might seem a bit odd, since one informally thinks of a signature \nas being a structure type, but as we shall see shortly, signa\u00adtures and static structures are different. \nSince the only structure components we allow in this paper are structures and functors, an environment \nE is a pair of just two environments (F , SE). Figure 7 shows a structure which P/ane from Figure 1 could \nelabo\u00adrate to, if we assume that environments also contain type and variable environments. In the figure, \na is (real * real)* (real* real) + real* real. We shall often need to select parts of semantic ob\u00adjects \n for example the name of a structure, In such cases we rely on variable names to indicate which part \nis selected. For instance m of S means the struc\u00adture name of S. Moreover, when a semantic object contains \na finite map we shall (apply the object to an argument, re\u00adlying on the syntactic class of the argument \nto de\u00adtermine the relevant function. For instance S(strid) means (SE of (E of S))(strid). Names that \nare specified in a signature expression and are not shared with already declared types or structures \nstand for unknown types and structures. In other words, such names are generic or flexible, as opposed \nto names of declared types and structures, which we call rigid. A signature is an object of the form \n(N)S, where S is a structure and N is a finite set of names. The prefix (N) is a binding operator which \nbinds the members of N throughout S . The names in N are the flexible names of the signature whereas \nnames that are free in (N)S are rigid. Se\u00admantic objects that can be obtained from each other by renaming \nof bound names are considered equal. For all semantic objects A, we write names(A) to mean the set of \nnames that occur free in A Notice the resemblance between a signature 2 = (N)S and a type scheme u = \nVaf j.~ in the Damas-Milner type discipline. Instantiation of bound names in Z takes place when a structure \nis matched against z. A functor signature @ is an object of the form (N)(S, (N )S ). Here (N)S is the \n(principal) signature for the parameter signature expression of the functor and S is the body structure \nof the functor; the names bound by (N ) are the names in S which have to be generated afresh upon each \nfunctor application, typi\u00adcally because of struct.. end expressions in the body. As an example, the functor \nsignature of the functor Square in Figure 1 is (N)(S, (N )S ), where N = {ml, tl} S = (ml, {t +tl}) N \n= {m2} S = (m2, {t =+ tl*tl}) (Here we have temporarily admitted types into the language, but ignored \nvariable environments and empty environments.) Notice that the meaning of the t type of the result depends \non the t type specified in the argument via the name t I which is bound outer\u00admost in the functor signature. \nA basis B is a triple N, G, E, where N is the set of rigid names generated so far, G is a signature en\u00advironment \nand E is an environment. The G and E components are used for looking up identifiers during elaboration. \nAn assembly A records the structures that have been generated so far. Assemblies and bases appear together \nin pairs (A, B). Not all structures in A need occur in B, for B contains only those semantic objects \nwhich are currently in scope. on the other hand, ev\u00adery structure S which occurs somewhere in B will \nalso occur somewhere in A, or at least there will be some structure occurring somewhere in A of which \nS is a cut-down version, as defined below. Throughout this paper, occurrence is hereditary; for example, \nS occurs in (N)(S, (N ) S ) and E occurs in (m, E). As one sees from the definition in Figure 6, an as\u00adsembly \ncan be thought of as a binary tree of struc\u00ad 3.3 Cover signature PLUS = sig type t val plus: t*t->t \nend; structure Planel : PL US= Plane; Figure 8: A signature constraint tures; this is not strictly \nnecessary, although practical in proofs. The assembly is just a store of structures, and other representations \nwould work well too. Consistency Once a structure has been created, it is possible to have restricted \nviews of it. This can happen in two ways. The first is by a signature constraint. (Figure 8 shows a signature \nconstraint which has the effect of creating a view of the Plane structure which hides the e component.) \nThe other way is to apply a functor with signature (l V)(S, (N ) S ) to an actual structure S+ which \nhas more components than required by S. In this case a cut-down view of S is created. Both of these ways \nmay hide components, but they never hide sharing. Thus one can easily have two structures S1 = (ml, El) \nand S2 = (m2, E2) where ml = m2 (i.e. the two structures share) and El # E2. In this case, however, names \nin El and Ez should at least be used consistently in those components that are present in both views. \nTh~is leads to the definition of consistency A semantic object A or assembly A of objects is said to \nbe consistent if (after changing bound names to make all nameset prefixes in A disjoint) for all S1 and \nS2 occurring in A and for every strid and every funid, ifmofSI=mofS2,then 1. If Sl(strid) and S2(strid) \nexist, then m of Sl(sttid) == m of S2(strid) 2. If SI (funid) and S2(~unid) exist, then Sl(ju nid) = \nS2(@nid)  Note that consistency of functor components is equal\u00adity; for structures, only name equality \nis required. It is essentially item 2 which has the undesirable effect of demanding complete agreement \nbetween specified and declared functors. Perhaps it is possible to drop this item altogether without \nupsetting the rest of the theory, but the consequences of this change are not yet fully understood. As \nhinted in Section 3.1 once a structure S = (m, E) has been created, S must have at least the compo\u00adnents \nof any subsequent view S-of S. In particular, S-should be (m, E-), where E-is an environment satisfying \nDom E-~ Dom E. Of course we would also expect some relationship between the objects in the range of E-and \nthe objects in the range of E, but that is part of consistency (see Section 3.2). The definition of cover \ngiven below is a slight generalisa\u00adtion of the has at least the components of relation. Moreover, during \nelaboration one sometimes wants to extend the current assembly with new structures with\u00adout adding components \ntc, structures in the current as\u00adsembly. This leads to the notion of conservative cover, a strengthening \nof ordinary cover: Let Al and A2 be semantic objects or assemblies of objects, let N be a nalme set and \nlet id range over structure and functor identifiers. We say that A2 cov\u00aders Al on N if whenever (m, El \n) occurs in Al with m free and m c N, then m c names(A2) and for all id 6 Dom El there exists an E2 such \nthat (m, E2) oc\u00adcurs in A2 with m free and id ~ Dom E2. We say that A2 covers Al if A2 covers Al on names(Al). \nWe say that A2 is a conservative cover of Al if A2 covers Al and Al covers A2 on names(Al ). 3.4 Admissibility \nBesides demanding that semantic objects be consis\u00adtent, the definition of ML requires cycJe-freedom and \nwell-formedness. The former prevents a structure from being specified as a proper substructure of itself \nand the latter ensures that for every name prefix (N) and every structure S = (m, E) in the scope of \n(N), if m is not bound by (N) then no name in E is bound by (N) either. We refer to [5, Sec 5.3] and \n[5, Sec 5.4] for details. We now collect these constraints as follows: An object or assembly A is admissible \nif it is cycle\u00adfree, consistent and well-formed. We say that A2 is an admissible cover of Al, written \nAl ~ A2, if (Al, A2) is admissible and A2 covers Al. We say that A2 is a conservative admissible cover \nof Al, written Al ~ A2, if (Al, A2) is admissible and Az is a conservative cover of Al. Both ~ and ~ \nare preorders. 3.5 Realisation A realisation is a function p : StrName --+ StrName. The support Supp \np of a realisation p is the set of names n for which p(n) # n. Realisations p are extended to apply to \nall semantic objects; their effect is to replace each name n by ~(n). In applying p to an object with \nbound names, such as a signature (N)S, first bound names must be changed to avoid name capture. 3.6 \nSignature Instantiation When matching a structure S against a signature (N)S, S may have more components \nthan present in S. However, for the components that do appear in S, it must be possible to obtain consistent \nnaming in S and S by instantiation of the members of N: A struct ure Sz is an instance of a signature \n(NI )S1, written (N1)S1 > S2, if there exists a realisation q such that p(S1) = S2 and Supp p ~ N1. Note \nthat there is at most one such p. 3.7 Principal Signatures Let 1? be a basis, A an assembly and let \nsigexp be a signature expression. In Section 3.8 we shall define what it is for sigexp to elaborate to \na structure S in (A, B), written A, B t sigexp + S. This is used in the definition of principal signature: \nWe say that a signature (N)S is principal for sigexp in (A, B) if A ~ B and, choosing N such that N n \nnames(A, B) = 0 1. There exists an A such that A ~ A and A , B 1-sigexp ~ S 2. ForallA and S , ifA~A \nand A , B 1-sigexp ~ S then (N)S > S The reason for introducing A in the above definition will be given \nin Section 3.8. For now, simply note that a signature Z = (N)S is principal for sigexp in (A, B) only \nif all structures that sigezp elaborates to can be obtained from Z by instantiation of the flexible names, \ni.e. the names in N. This is similar to the notion of principal type scheme in the Damas-Milner type \ndiscipline [2], where the flexible names play the r61e of generic type variables. 3.8 Inference Rules \nThe inference rules for signature expressions, specifica\u00ad tions and functor signature expressions appear \nbelow. All the conclusions of the rules are of the form A,B E phrase 3 P (1) which is read: in the basis \nB and assembly A, the phrase phrase elaborates to the object P. Here phrase is one of the phrase forms \ndefined in Figure 5 and P is either a structure, a signature, a functor signature or an environment. \nAt top level, A is the assembly of all structures de\u00adclared so far. The phrase classes we consider do \nnot declare new structures (recall that we are concerned with signature expressions only), so elaboration \ndoes not produce an assembly. Thus we have A on the left\u00adhand side of the E, but not on the right-hand \nside of the %-. If we were to write down rules for structure ex\u00adpressions, functor declaration and functor \napplication, elaboration would gradually increase the top-level as\u00adsembly, so we would have assemblies \non the right-hand side of the ~ in those rules. An important invariant to keep in mind when read\u00ading \nthe rules is that one can infer (1) only if A is admissible and covers both B and P. Applying this invariant \nto the case where phrase is a top-level signa\u00adture expression and P is a signature Z, we see that Z cannot \nadd components to top-level structures. The rules appear in the Appendix. There is one rule for each \nproduction in the grammar, plus a rule (4) for principal signatures. We shall now discuss the most important \nrules. The assembly is only expanded in two rules. The first is rule 4. Let us first read the rule bottom-up. \nAssume the binding operation is non-trivial (i.e. that N # 0). Then there are structures that occur free \nin S but are not free in (N)S. Therefore we need to ex\u00adpand the assembly to cover the elaboration of \nsigexp to S. Conversely, reading the rule top-down, it is nat\u00adural that when we bind structures that \noccur in S by (N) then we simultaneously discharge the correspond\u00ading structures from the assembly. That \nway renaming of bound names and expansion of the assembly go to\u00adgether. The expansion of A to A is not \nsupposed to add components to structures that are already in A, for the expansion is solely there to \nintroduce new structures that are used locally in the elaboration of sa gexp, so we require A ~ A . The \nsecond rule which expands the assembly is rule 5. We first elaborate sigezp ~ to a signature (N)S. This \nmust happen via rule 4, so (N)S is principal for sigexp in (A, B). We then choose N in a way that avoids \naccidental sharing (names (A, B) n N = 0). We now form the assembly A = (A, S) which is used for the \nelaboration of sigezp2 to Z, the functor result sig\u00adnature. By the invariant, every structure (m, E) \nthat occurs free in X and satisfies m c N must be covered by A , i.e. by S. The + operation on environments \nis defined aa fol\u00ad lows: E + E is the environment defined by (E + E )(id) = E (id), if id c Dom E , \nand (E+ E )(id) G E(id) otherwise; The + operation is extended to se\u00admantic objects containing environments \nin the obvious way. The + N in rule 5 is explained in Section 4. We can now express the invariant as \na formal constraint on elaboration: A sentence of the form A, B t phrase &#38; P is ad\u00admissible if (A, \nB, P) is admissible and A covers (B, P). A tree V is called an inference tree if it is formed by applying \nthe inference rules and satisfies the side\u00adconditions on the rules; V is said to be a proof if every \nsentence occurring in it is admissible; inference trees that are not proofs are banned from elaboration. \nstructure P = Struct structure Q = struct end end; s ig(3) structure P : sig end functor F: (X: sig(4J \nstructure PI : sig structure Q: sig end end sharing P = PI end(4)): sig end sharing P = P end(3) . signature \nSIG = * Figure 9: Embedded functor signature We can now state the principality theorem, Theorem 3.1 \n(Principal Signatures) Lei B be a basis, A an assembly and let A, B 1-sigexp ~ S, for some S. Then there \nexists a principal signature for sigezp in (A, B). This theorem follows from a stronger theorem, which \nwe state in Section 5. The elaboration of the program in Figure 9 is sum\u00admarised in Figure 10. It illustrates \nmost features of the inference system. Some of the occurrences of s ig and end have been decorated with \noccurrence num\u00adbers. Let sigezpi be the signature expression from sig(i) to end(i), i = 3,4. The specifications \nof P and F are referred to as specp, and spec=, respectively. We assume that the assembly and the basis \nare empty at the outset. After the elaboration of the structure declaration we have B = {P H Sp} and \nA = SP, where Sp = (ml, {Q ~ (m2, {})}), The elaboration then proceeds as outlined in Figure 10. (4) \nA , B1 !!-sigexp4 * (N4)5 4 . L# I At, B ~ specp, w EPI A , .B1 t SpecF ~ EF I(lo) P =P * {} A , B \n1-sigex ~ ~ S3 r  S4 = (m4, {P * SP}) IV4 = {m4} EP, = {P H (ml, {})} B1=B+EP1 @ = (N4)(S4, ({m5})(m5, \n{})) E,={F H@} B2=B1+EF S3= (m3,EPl + EF) N3 = {m3} A = Sp, A = (A)S3) and A = (A , S4) B={ PsSP} SP \n= (ml, {Q I+ (m2, {})}) Figure 10: An elaboration tree Admissificatian Principal signatures can be found \nby an algorithm which we will present in Section 5. The algorithm cru\u00adcially relies on the existence \nof most general (or prin\u00adcipal) admissifiers, just as the algorithm W[2] relies on the existence of most \ngeneral unifiers in first-order term unification[7]. Admissification is invoked dur\u00ading elaboration when \na sharing specification is encoun\u00adtered. If the two structures that are specified to share do not have \nthe same name, admissification seeks to identify the names. Because the resulting assembly has to be \nadmissible, this might lead to further iden\u00ad tification of names of structures that occur below the two \nstructures. For example, consider the program in Figure 9. The algorithm traverses the program from left \nto right, corresponding to a depth-first, left-to-right traversal of the elaboration tree. The algorithm \nmakes as few identifications of names as possible, in order that the result be principal. The first sharing \nconstraint has the effect of identifying the names of P and P . The second sharing constraint identifies \nthis name with ml, the name of P, and this forces identification of the name of P1l.Q with the name of \nP.Q, i.e. m2, even though P ! is not mentioned in the sharing equation. The reason for this is that we \nhave to stay within a consistent assembly. Not any pair of names can be identified. Names of declared \nstructures (as opposed to specified struc\u00adtures) are always supposed to be unique, so two such names \ncannot be identified. Also, if (N)S is the result of elaborating a parameter signature expression of \na functor specification, then the names of N are to be treated as distinct within the result signature. \nOperationally, what it means for a name to be rigid is that it cannot be replaced by another name by \nad\u00admissification. Formally, the set of rigid names is the names in the N-component of the basis. This \nis why we add (i.e. union) N with the N-component of the basis in rule 5. Finally, the side-condition \nN n names(A, l?) = 0 in rule 5 constrains possible identifications of names. The way we check whether \nthis side-condition is being violated is by assuming that all names have a rank, which is a natural number. \nAt top level, names have rank O. Rank is increased by one locally when one starts elaboration of the \nresult signature of a functor signature expression. The details are as follows: Let A be an assembly, \np a realisation and N a name set. The rank of A, written rank(A), is the maximum rank of any name that \noccurs free in A. A is stratified if whenever (m, E) occurs in A with m free then rank(E) < rank m. (Intuitively, \n(m, E) contains free names that will later be bound further out by name prefixes nested within each other; \nby in\u00adsisting that structures be stratified we can keep these name prefixes disjoint and avoid capture \nof free names throughout elaboration.) We say that p is decreas\u00ading if rank(ym) < rank(n), for all names \nn; p is said tobefixed onNifp(n) =n,forallncN. (In\u00adtuitively, realisation is supposed to be fixed on \nrigid names, i.e. the names in N of B; on other names, re\u00adalisation should not increase rank the binding \nof a name may be moved further out, but never further in, as this could lead to capture of free names \nwhen forming signatures.) Moreover, p is said to be an ad\u00admissifier for A under N if p is decreasing \nand fixed on N, A covers PA on N and PA is admissible and stratified. An admissifier p for A under N \nis said to be most general or principal if, for every admissifier p for A under N, there exists a realisation \n@ such that p is decreasing, fixed on N and p (p*A) = PA. The following theorem corresponds to Theorem \n10.3 in [4] and the proof is similar. Theorem 4.1 (Admissification) For any assembly A and name set \nN, if there exists some admissijerjor A under N then there exists a principal admissijier for A under \nN. The proof of this theorem is similar to the proof of Theorem 10.3 in [4]; Essentially, the proof \ngives an algorithm .4dmissify, such that for all assemblies A and name sets N, either Adrnissijy(A, N) \nreturns a principal admissifier p for A under N, or it fails, in case no admissifier exists. Admissification \nis reminiscent of Remy s record unification[6] and Ait Kaci s type unification[l]. The main extra subtlety \nwe have to deal with is unification under nested name prefixes. 5 An Inference Algorithm In this section \nwe state a stronger version of Theo\u00adrem 3.1 which is suitable for inductive proof. Also, we present the \nconstructive part of the proof as an inference algorithm. Let K be the category defined as follows. An \nobject O of K is a pair (A, B), where A is an assembly and B = (N, G, E) is a basis satisfying that A \nis admissible and stratified and A ~ B (by which we mean A ~ (G, E) and names(A) z N). The set of objects \nof K is denoted Obj. For all objects 01 = (Al, Bl) = (Al, (Nl, Gl, El)) and 02 = (A2, B2), and for every \np and rank k, there is a morphism 01 =OZ ifp is decreasing and fixed on N1, pB1 = B2, pA1 ~ A2, Al covers \nA2 on N1 and rank(Ol, 02) < k. The condition Al covers A2 on NI\u00b0 prevents realisation from adding components \nto rigid structures. Notice that 01 =02 implies q(Bl) = P(N1, GI, El) = (N1, pG1, pE1) = B2. Morphisms \n01 A 02 and 01 k bOz between 01 and 02 are equal if k = k; and p(n) = ~ (n), for all n c names 01. Composition \nin K is the natural extension of composition of realisations. For all O = (A, 1?) we write O 1-phrase \ns Q as a shorthand for O c Obj and A, B 1-phrase * Q. Theorem 5.1 (Realisation and Principality y) Let \nphrase be one of sigexp, spec or funsigexp. Then (1) and (2). (1) For all O, O , Q, p and k, if O 1-phrase \n3 Q andO k O then O t-phrase ~ p(Q) (2) For all O, O , Q , p and k, if  O AO and O 1-phrase a Q then \nthere exist O*, @ and Q* depending only on O, phrase and k, such that Moreover, there exists a ~ such \nthat the diagram o p , k p,k /\\ *.,k O*- o commutes and @Q* = Q .  Theorem 3.1 follows from part (2) \nof the above theo\u00ad rem if we let A be the assembly of the structures that occur inB,letO=O =(.A,B), Q \n=S ,p=Id, k = O, phrase = sigezp and Q = S*. The proof is by induction cm the depth of inference. It \nis too long to be included here, but the construc\u00ad tion of O*, p and Q in part (2) happens accord\u00ad ing \nto the algorithm W in Figure 11. The algorithm is named after the Damas-M ilner algorithm with the same \nname [2] because of th(e great similarity. Given an expression e and a type assignment A, a call (S, \nT)= W (A, e) of their algorithm either returns (S, ~), where S is a substitution and ~ (after suitable \nbinding of type vari\u00adables) is the principal type for e in S(A), or fails, in case the expression does \nnot elaborate at all. Simi\u00adlarly, a call (0 , 9,S) = WS:,,WP(O, Sigf=w) of our algorithm either returns \n(0 , p, S), where v is a realisation and S (after suitable binding of names) is the principal signature \nfor siglexp in O , or fails, in case the signature expression does not elaborate at all. The analogy \nis strengthened by the fact that 0 is roughly P(O), except that O in fact might be an non-trivial admissible \ncover of yY(0)., for reasons explained in the conclusion. The realisation p returned by the algorithm \nis the composition of realisations obtained for the subexpres\u00adsions of sigezp. The only place non-trivial \nrealisations arise is when a sharing specification is processed. In this case, the admissifica,tion algorithm \nmentioned in Section 4 is invoked. Apart from failing because of identifiers that are not found in the \nbasis, the only way our algorithm W can fail is indirectly because Admissify fails. Again, this is similar \nto the behaviour or the Damss-Milner algorithm. Our W algorithm actually consists of four mu\u00adtually recursive \nfunctions, one for each group of inference rules in !3ection 3.8. In the last of these, WSPeC, we use \nthe following notation. For all O = (A, B) = (A, (N, G,E)), we write Admissify(O, longstridl, longstridz) \nto mean Admissify(A , N), where (m, {X H B(longstridl)}), At =(Al (m, {X H l?(longstrid2)})) where m \nis new and X is an arbitrary structure iden\u00ad tifier. Notation used in Figure 11: CIOSAS means (N)S, \nwhere N = names S \\ names A Below(A, A ) is a minimal (with respect to ~) assem\u00adbly A consistent with \nA satisfying that when\u00adever S = (m, E) occurs in A with m free and m E names(A , A ), then S Q A Oas(A, \nB) introduces a variable O and simultaneously introduces variables for the components of O B of O means \nthe B component of object O; similarly for Nof Band Eof B O M P is the object ((A, P), B), where (A, \nB) = O and P is any semantic object. (O M P concerns the assembly component of O, whereas O + P always \nconcerns the basis component of O.) 6 Conclusion Our type discipline is almost, but not completely, a \nconservative extension c)f the Standard ML modules discipline. The difference, aa far as first-order \nmod\u00adules is concerned, haa to do with the notion of cover and the rtde of assemblies. Assemblies do not \nappear in the inference rules for Standard ML, although they play a prominent part in the proof of the \nprincipality theorem[4, Theorem A.2]. The reason why we have them in our inference rules haa to do with \ncover. In ML, cover is only required at one point: ML allows the conclusion B + sigexp ~ (N)S, only when \nB cov\u00aders Below(S, 1?) [5, Sec 5.13]. Were we to copy this restriction to the higher-order language, \nprincipality would be lost. To see this, consider Figure 9 again W,ige3p(0 as (A, l?), sigexp, k) : Obj \nx Rea x Str = and imagine that the last sharing constraint is deleted case sigexp of and that B is a \nbasis which contains two structures longsigid => that have different names ml and m2 but both have a \nlet (N*)S* = B(longsz gid), where all names in Q substructure. One now has the following unpleas- N* \nare chosen to be fresh and have rank k ant options when elaborating SIG in B. Either one in (OW S*, ld, \nS*) identifies the name of P! with ml or m2 in order to sat- I sig spec end=> isfy the restriction when \ninferring a principal signature let (O*, p , E*) = W ,P=C(O, spec, k) for sigezp4, in which case one \ndoes not get a princi- S* = (m*, E* ), where m is new pal signature for sigezp3, since the choice between \nml in (O* WS*, p*, S*) and m2 is arbitrary. Or one gives P a fresh name, in which case the principal \nsignature for sigezp4 violates WPtin$~,~P(O as (A, l?), sigezp, k) : Obj x Rea x Sig = the requirement, \nbecause P has no Q component. let (O* as (A*, B*), p , S*) = W,ig~~P(O, sigeq, k) A! = Below(A*, P*A) \n For the higher-order language, we therefore need a way to allow components in S that are not covered \nin (( A~, B*), p*, X*) by B when we infer a principal signature (N)S in B. This is achieved by introducing \nassemblies into the Wftin,~gecP(O, funsigexp, k) : Obj x Rea x FunSig = elaboration trees and by modifying \nthe definition of let principality so that principality becomes relative to ~ = ClosA; S* both B and \nA. O = (A, B) (sttid: sigexpl ): sigexpz = funsigexp (O~,~;, Z!)= Wprinsig.q(oj szgezpl, ~) (N: )S; \n= X;, where all names in N; are structure Empty = struct end; chosen to he fresh and have rank k + 1 \nOz = O~w S;+ Nf+{strid~SJ}  signature SIG1 = sig (O;,PJ, x;)= WPtin,igezP(02, si9e~p2, k + 1) structure \nE: sig A~=AofO~ structure Dangle: sig end A* = Below(A~, names(A~) \\ N;) end P*=% ; OP? sharing E= \nEmpty O* = (Nf)(p~Sf, Z;) end in (( A*, p~(Bof 0~)), q*,@*) signature SIG2 = sig W.P..(O, spec, k) \n: Obj x Rea x Env = local case spec of structure E: sig empty => (O, Id, {}), where Id is the identity \nstructure Dangle: sig end I sharing longstrid ~=longstrid2 => end let @ = Admissify(O, longstridl, longstrid~) \nsharing E= Empty in (p* O,q*, {}) in I specl (; )specz => end let (o; as (A~, B;), 91, El) = W,P40, specl, \nk) end; (O:, p;, E;) = W,p~4(AY, B! + E;), spec~, k) in ((A of Oj,pjB~),@j OPl,PjEl +Ej) I structure \nstrid: sigexp => Figure 12: Two pathological signatures let (O*, ~ , S*) = WSiJaZP(O, sigeq, k) in (O*, \np*, {sir-id * S*}) The consequences of these modifications are illus\u00adfunctor funid:funsigexp => I trated \nby the example in Figure 12. In ML, SIG1 is let (O*, p , O*) = W~un.igeZP(O, funsigezp, k) illegal, while \nSIG2 is legal. (To see this, notice that in (O*, p , {funid * @*]) SIG1 elaborates to a structure containing \na compo\u00adnent (Dangle) which is not covered by the basis; in Figure 11: The Inference Algorithm SIG2, \nthe dangling structure is not in the resulting signature because of the local specification, so SIG2 \nis legal.) In our scheme they would both be illegal. In all other respects, our discipline is consistent \nwith the ML discipline. In the view of the author, sign% Appendix: Inference Rules tures like the ones \nin Figure 12 are pathological and differences in the treatment of them are unlikely to Signature Expressions \n ~ have any practical significance whatsoever. A, B1-spec~E (2) A, B 1-sig spec end+ (m}E) Acknowledgements \nThis work is heavily based on the work on the seman- B(sigid) ~ S (3) tics of ML and hence uses ideas \ndue to Robin Milner A, B t-sigid a S and Robert Harper. I am also grateful to Robin Mil\u00adner, David MacQueen, \nDavid. Turner, Don Sarmella, Maria-Virginia Aponte and Jc)hn Hannan for interest\u00ading discussions and \nvery valuable feedback. Also, I A~A A , B i-sigexp ~ S wish to thank the SERC Standard ML project and \n(N)S principal for sigexp in A, B (4)ESPRIT Semantique project for their financial sup- A, B 1-sigezp \na (N)S port. Functor Signature Expressions References [1] Hassan Ait Kaci. An algebraic semantics ap- \nA, B 1-sigezpl a (N)S IV n names(A, B) = 0 proach to the effective resolution of type equa\u00ad (A, S), B \n+ N + {strid w S} 1-sigezpz +-X tions. Theoretical Computer Science, 45(3):293- A, B 1-(strid: sigezpl \n): sigezpz ~ (N)(S, X) 351, 1986. (5) [2] L. Damas and R. Milner. Principal type schemes for functional \nprograms. In Proc. 9th Annual A CM Specifications A, Bkspec~E Symp. on Principles of Programming Languages, \nA,B t sigexp ~ S pages 207-212, Jan. 1982. (6) A, B 1-structure strid: sigezp ~ {strid w S} [3] R. \nHarper, J. Mitchell, and E. Moggi. Higher\u00adorder modules and the phase distinction. In Proc. A, B 1-funsigezp \n~ @ A Cikl Symp. on Principles of Programming Lan- A, B i-functor funid: funsigezp ~ {funid R 0} guages, \npages 341-354, Jan. 1990. (7) [4] Robin Milner and Mads Tofte, Commentary on m of B(longstridl) := m \nof B(Jongstrid2)Standard ML. MIT Press, 1991. (8) A, B i-sharing longstridl = longstrid2 * {} [5] Robin \nMilner, Mads Tofite, and Robert Harper. The Definition of Standard ML. MIT Press, 1990. (9) A,BE * {} \n [6] D. Remy. Typechecking records and variants in a natural extension of ml. In Proc. 16th Annual A,B \n1-specl ~ El A, B + El 1-spec2 d E.J ACM Symp. on Principles of Programming Lan\u00ad A, B>specl (;) specz \n~ El + E2guages, pages 77-88, ACM, Jan. 1989. (lo) [7] J. Robinson. A machine-oriented logic based \non the resolution principle. J. Assoc. Comput. Mach., 12(1):2341, 1965. \n\t\t\t", "proc_id": "143165", "abstract": "<p>Under the Damas-Milner type discipline for functional languages, every expression has principal type, if it elaborates at all. In the type discipline for ML Modules, a signature expression has a principal signature, if it elaborates at all. However, while functions can be higher-order in ML, parameterised modules in ML are first-order only. We present a type discipline for a skeletal higher-order module language which has principal signatures. Sharing and multiple views of structures are handled in a manner which is compatible with the semantics of the first-order ML modules.</p>", "authors": [{"name": "Mads Tofte", "author_profile_id": "81100142765", "affiliation": "", "person_id": "PP39080224", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143165.143206", "year": "1992", "article_id": "143206", "conference": "POPL", "title": "Principal signatures for higher-order program modules", "url": "http://dl.acm.org/citation.cfm?id=143206"}