{"article_publication_date": "02-01-1992", "fulltext": "\n A semantics for ML concurrency primitives Dave Berry, Robin Milner and David N. Turner db@dcs. ed. ac.uk, \nrrn@dcs.ed. ac. uk, dnt@dcs. ed. ac. uk Laboratory for l?oundations of Computer Science University of \nEdinburgh. * Abstract We present a set of concurrency primitives for Stan\u00addard ML. We define these by \ngiving the transitional semantics of a simple language. We prove that our se\u00admantics preserves the expected \nbehaviour of sequential programs. We also show that we can define stores as processes, such that the \nrepresentation has the same behaviour as a direct definition. These proofs are the first steps towards \nintegrating our semantics with the full definition of Standard ML. 1 Background and Motivation There \nhave beerr several attempts to add concurrency primitives to Standard ML (SML) and related lan\u00adguages \n[H0183, Mat91, Rep91a, CM90, Ber89]. How\u00adever, when we began this work none of these imple\u00admentations \nhad a published formal definition. The formal definition of SML is an integral part of the development \nof the language. If we are to add concur\u00adrency to the language, it is essential that we have a formal \nsemantics for the new constructs that is com\u00adpatible with the existing definition. In this paper we present \nour work towards such a semantics. The first choice we faced was that of which primi\u00adtives to use. The \nstory of our choice shows an inter\u00adesting convergence between theory and practice. We began by giving \na semantics for the primitives used by Matthews [Mat91], because Matthews is working with us on a distributed \nimplementation of SML. The resulting rules were similar to those that we present here, but were complicated \nby the addition of a mu\u00adt ual exclusion relation between processes. This was because Matthews primitives \nallow arguments of the *This work is supported by several grants from the SERC and (for Turner) support \nfrom Harlequin Ltd. Permission to copy without fee all or part of this material is granted provided that \nthe w.pies are met made or distributed for direct commercial advantage, the ACM copyright notice and \nthe title of the publicationsadkadateappear,and notice is given that copying is by permission of the \nAssociation for Computing Machinery. To copy other\u00adwise, or to republish, requires a fee and/or specific \npermission. @ 1992 ACM 089791453-8/92/0001/0119 $1.50 119 choice operator to be arbitrary expressions \nthat could themselves create new sub-processes. We then restricted the choice operator to take com\u00admunications \nas arguments. We added the type a com to enforce this restriction, following PFL[H0183] and CML[Rep91a]. \nThis gave us a cleaner semantics, but required the addition of an operator to coerce a value cbf type \n a com to one of type a. Given the need for this operator, we decided to adopt the extra func\u00adtionality \nthat Reppy gives it in CML. As a result, we now have a semantics for the basic primitives of CML. Thus \nfor semantic reasons we have arrived at the same result as Reppy s purely pragmatic reasoning. Our semantics \nhas been influenced by Facile [GMP89] and the Chemical Abstract Machine [BB90]. However, we believe that \nour approach is simpler. For example, Facile can create a new process with either a behaviour expression \nor a fork function, and this re\u00adsults in some transitions being labelled with behaviour expressions. \nWe avoid the need for such complex la\u00adbels by eliminating behaviour expressions. We give the semantics \nof a small language. This al\u00ad lows us to concentrate on the key features of the lan\u00adguage, and to prove \nsome simple properties without having to consider an unreasonable number of cases. we aim to incorporate \nthe semantics of this language with the semantics of SML. We present two steps along this path. The first \nshows that our semantics pre\u00ad serves the expected behaviour of sequential programs, which suggests that \nour primitives can be added to SML without disturbing its functional features. The second shows that \nwe can represent stores ds processes with the desired behaviour. This goes a long way to\u00adwards recovering \nthe non-functional part of SML. Reppy has independently given a semantics for CML [Rep91b]. He uses the \nstyle developed by Wright and Felleisen [WF91], but the result is very similar to our definition. We \ngive some proofs of properties that we would like to hold for our language, which Reppy doesn t. On the \nother hand, Reppy gives a seman\u00ad tics for all the constructs in the current definition of CML, whereas \nwe only deal with the basic operators. We don t foresee any problems in adding the other operators to \nour definition. The Concurrency Primitives And Their Semantics signature Concurrency = sig type a channel \nval channel: unit -> .a channel type a com val send: a channel * a -> a c Oln val receive: a channel \n-> a com val choose: a com * a com -> a com val wrap: a com * ( a -> b) -> b com val noevent: a com val \nfork: (unit -> a) -> unit val sync: a com -> a end Figurel: The signature ofthe concurrency primitives \nFigure 1 shows how our primitives might appearif they were available in full SML. The type a com is the \ntype of suspended communications. Suspended communications are actually performed by applying sync to \nthem. This allows the type systemto specify thepossible arguments tothe choice operator. It also allows \na programming style based on the abstraction of synchronisation from the events being synchronised, as \nproposed by Reppy (who calls this type a event). Furthermore, it enables us to split our semantic rules \ninto two groups, one for evaluation rules and one for communication rules. The syntax of our simple language \nis: 1=x I () ]1~ lZ I fnz=>i ]rec~(z)=>l I (lI,lz) where 1,11 and 12 are lexical phrases and z and ~ \nare alphabetic identifiers, The concurrent behaviour of our language is defined by the constructors and \nbasic values shown in Figures 1 and 2. They have no special syntax beyond their existence as identifiers \nand values. We use parentheses to show grouping. The semantic objects are summarised in Figures 2 and \n3. The set of expressions is a superset of both the set of values and the set of lexical phrases. The \nset of identifiers includes all possible alphabetic identifiers, including the constructors and basic \nvalues. Each singleton processSet is called a process, and is written ~ : e]. The notation P~ : e] denotes \nthe processSet P U {~ : e]}. The union of two process-Sets is only defined if their domains are disjoint. \nWe usually omit the parentheses around configurations. The notation f +g and f g denotes the usual op\u00aderations \non functions. The notation K1 K2 is also used to denote set difference. The notation e{v/z} denotes \nthe substitution of v for x in e, with the usual renaming to avoid the capture of free variables. We \ndivide the semantic rules into two groups. The rules in the first group defines the evaluation of ex\u00adpressions. \nThey are given in the transitional style, in which each sentence defines one step of evaluation. The \nrules in the second group define communication between two processes. They are given in the rela\u00adtional \nstyle, in which a sentence gives the result of the communication. The sentences in the evaluation rules \nare transitions between configurations. Each configuration specifies the channels and processes that \nhave been created by the computation so far. A transition specifies some computation involving one or \ntwo processes, called the selected processes. Each transition is Iabelled with a set of the processIds \nof its selected processes. It is important to realise that these labels are not related to the communication \nlabels of CCS or Facile. They are mainly used to state and prove properties about the language. Indeed, \nwe often omit labels from sentences when the information is irrelevant. The sentences in the communication \nrules are re\u00adlations between a pair of suspended communications and the expressions that they return \nwhen they com\u00admunicate with each other. We use a double arrow to mark the fact that the communications \nhappen at one go, rather than as a series of transitions. There aren t any rules for noevent. noevent \ncan never synchronise with another communication. Definition 2.1 The set of all channels in P is de\u00adnoted \nthan P. A configuration K, P is well-formed ifl (than P) ~ K. A reduction sequence is well-formed if \nall the configurations in the sequence are well-formed. Lemma 2.1 (Preservation Lemma) In a transi\u00ad tion \nh K, P % K , P , the following all hold: 1. dom(P) ~ dom(P ). 2. K ~K .  $. If K, P is well-formed \nthen so is K , P . If k @ K , then E Ku{k}, P ~ K U{k}, P . 4. 5. If k c K and k @than(P), then 1-K \n{k}, P ~ K {k}, P . 6. Ifp @ P and chan(~ : e]) G K, then F K,P~ :e]&#38; K , P ~ :e].  7. If P = P \n~ : e] and p # S, then there ex\u00ad ists P such that 1- K, P ! z K , P[) and P = P ~ : e]. Proofl The \nproof is a simple induction on the depth of in\u00adference of k K, P &#38; K , P . 120 k E ChannelId P) \nq E Processldl Z,y, z,f E Identifier  () C Unit = {(1} c E Constructor= {send, receive, choose, wrap, \nnoevent} b c Basic Value = {channel, fork, sync} Figure 2: Basic Semantic Objects K ChannelIdSet = Fin(ChannelId) \nv Value = Unit U ChannelId. U Constructor U Constructed Value U Basic Value U Closure U ValPair (C, v) \nConstructed Value = Constructor x Value (v,, Zq) ValPair = Value x Value in x =>e Closure = Identifier \nx Expression e Expression = Value U Application U Identifier U ExpPair U RecExp el ez Application = Expression \nx Expression (e~,ez) ExpPair = Expression x Expression rec f(z) =>e RecExp = Identifier x Identifier \nx Expression P ProcessSet := ProcessId ~~ Expression ProcessIdSet = Fin(ProcessId) (K, P; Configuration \n= ChannelIdSet x ProcessSet Figure 3: Compound Semantic Objects Corollary 2.2 The same properties hold \nof a reduc\u00ade-e tion sequence t-K, P - K , P . (16) ve--+ve 3 Conservation of Sequential Be-(fnz =>e) \nv -e{v/z} (17) haviour Cv+(c, v) (18) One property that we wish to hold for our language is that a sequential \nexpression will produce the same el ---+ ej result in our semantics as it would in the usual sequen\u00ad \n(19) (e~,ez) + (e\\, eZ) tial semantics. We show that this is the case by giving the usual transitional \nrules for the sequential part of e-e (20) our language, and showing that the full language pro-(v, e) \n--+ (v, e ) duces the same behaviour for sequential expressions. This result can be extended to a relational \nsemantics rec f(z) +e -fn z=>e{rec ~(x) =>e/j} (21) of the sequential language using a standard proof \nof Lemma 3.1 A transition 1-K, P + K , P has ei\u00ad equivalence between the transitional and relational \nse\u00adther one or two selected processes. mantics, such as the one given by Hennessy [Hen90]. Definition \n3.1 A transition with two selected pro-The following rules give the usual transitional se-cesses is called \nan interaction. mantics of the sequential part of our language. Only the rules for sync and fork give \nrise to in\u00adel + e{ teractions. This is because they are the only opera\u00ad (15) el ez -e; ez tions that \ncan transfer values between processes. sync K, .P~ : (fn z =>e) v] Q .K,P~ : e{v/z}] K,fqp : cv]3K, P~ \n: (C, zf)] K,P~ : e] ~ K ,P ~ : e ] K,P~ : (w, e)] --5 K , P ~ : (v, e )] k#K K,P~ : channelo] ~ KU{k}, \nP~ : k] q g do?n(qu{p} K,P~ : fork fnz+e] u} K,P~ : ()][q : e] coml, comz =+ el, ez K, P~ : sync(coml)][q \n: sync(com2)] u} K, F ~ : el][q : e2] Figure 4: Evaluation Rules c~, com ==+ el, ez i c {1,2} (choose, \n(C1,C2)), com s e,, e2 (send, (k, v)), (receive, k) % v, v coml, comz =+ el, ez (wrap, (com~,e3)), com2 \n==+ e3 e~, e2 com2, coml * e2, el coml, com2 =$-el, e2 Figure 5: Communication Rules 122 (1) (2) (3) \n (4) (5) (6) (7) (8) (9)  (lo) 11) 12) (13) (14)  transfers values explicitly, fork can transfer \nvalues in the body of the function passed to the process that it crest es. Definition 3.2 Given a reduction \nsequence T, intT is the set of the processId pairs that label the interac\u00adtions in T. We define =2 to \nbe the transitive-reflexive closure of intT. The T-effect of a set S of gwocessIds is the set of all \nprocessIds that are equivalent (~T) to any member of S. If P is a process$et such that dom P = S, then \nthe transitions in T that are labelled with processIds in the T-eflect of S are called the P\u00adaffected \ntransitions of T. It follows that if T has the form: t-K,P + K , P then dom P is the T-effect of dom \nP. Definition 3.3 Let T1 and T2 be the well-formed reduction sequences F K1, P1 +* K~,P~ and t-K2, P2 \n-* K;, P;. Let the length of T2 be n. Then T2 is a projection of T] if P2 ~ PI, K2 ~ K1, T1 contains \nexactly n P2 -a#ected transitions, and for i E {1, .... n}, the selected processes in the ifh transi\u00adtion \nin T2 are identical to those in the ith Pz -afleeted transition in T1. Definition 3.4 Two processSets \nPI and P2 are inde\u00adpendent, written PI II P2, iff than P] and than Pt are disjoint. An expression is \nsequential if it doesn t contain any constructors or basic values. Lemma 3.2 (Projection Lemma) Let T \nbe the reduction sequence F K,P d K , P , and P = PIuP2, where PI II P2. Let P; ~~ P and dom P: be the \nT-effect of dom Pi (i = 1, 2), so that P = P~uP~. Then there is a projection T of T of the form F K, \nPI + K , P;, for some K . Proofi The proof uses induction on the length of T. Let n be the number of \nPi-affected transi\u00adtions in T. Let the first transition t in T be 1-K, PI UP2 % K , P~UP(, where dom \n.P~ and dom P; are the t-effects of dom PI and dom P2. Let T1 be the rest of T. By induction, there is \na projection T: of T1 with the form 1-K , P; d K , P;. If S ~ P; , then T1 and T; each contain n 1 Pi-affected \ntransitions. It follows that the reduction sequence formed by prefixing T{ with t-K, PI z K , P; is a \nprojection of T, as required. If S q P;, then T1 and T{ each contain n Pi-affected transitions. Furthermore \nP: = PI. If we also have K = K, then T; is a projection of T with the desired form. If K # K, then \nthere exists k such that K = Ku {k}. But k @ chanP1, so by the Preservation Lemma we have F K -{k},P~ \n- K -{k},P~, which is a projec\u00adtion of T with the desired form. Corollary 3.3 If T is the reduction sequence \n1-K, P~ : e] + K , P ~ : e ] and e is sequential, then there exists a projection of T that has the form \nhK, ~:e]+*K, ~: e ]. We can now state our first result. This theorem :shows that sequential expressions \nproduce the same result in our language as they do in a conventional sequential language. It suggests \nthat the functional behaviour of SML is preserved when our primitives are added. Theorem 3.4 (Conservative \nEztension Theo\u00adrem) If e is sequential, then for all K, P and p, t-e+ v ifl 2K , P such that h K,P~ : \ne] ~ K , P ~ : v] Proofi Only if case: If we can show that: If 1-e + e then 1P such that 1-K,P~ : e] \n+ K,P ~ : e ]. then the result follows by a simple induction on the length of F e + v. We can show that \nthis property holds by using induction on the depth of inference of F e + e and considering cases of \ne. Case 1. e -fnz =>el v. Then e = el{v/z}, from Rule 17. Rule 3 is the only rule in the parallel language \nspecification that matches e. It yields the configuration K, .P~ : el {v/z}], and the result follows \ndirectly. Case 2. es cv. Case 3. e -rec f(%) Sel. These cases are similar to Case 1. Case 4. e = el ez \n(e~ @ Value). Then 1-e + e must have been inferred by Rule 15. Therefore i-el + ej. By induction, there \nexists P such that t-K, P~ : el] + K , P ~ : e!]. The result follows from Rule 1. Case 5. e-v el. Case \n6. e a (el, e2) (el @ Value). Case 7. e E (v, et). These cases are similar to Case 4. If case: By the \ncorollary of the Projection Lemma, there exists a projection of h K, P~ : e] +* K , P ~ : v] with the \nform 1-K, ~ : e] +* K, ~ : v]. The result follows from a simple recasting of the only if case. 4 Modelling \nStores as Processes We also want to integrate our primitives with the non\u00ad functional features of SML. \nOne approach is to define these features in terms of processes. In this section we show how stores can \nbe defined in terms of our simple language. We prove that these definitions produce the desired behaviour, \nusing similar techniques to those of the previous section. We begin by giving a transitional semantics \nfor a sequential language with stores. These rules require two new sets of semantic objects: a c Address \ns E Store = Address &#38; Value In addition, we extend the set of basic values with ref, assign and deref, \nand the set of values with the set of addresses. Sentences are extended with stores in the obvious way. \nThe first seven rules are the rules of the sequential language, extended with stores. / e,sl-e, sz (23) \nve, sl-ve , sz fnx=>e v, s + e{v/z}, s (24) C u, S--+( c,v), s (25) rec ~(x) =>e, s + fn z =>e{rec ~(z) \n=>e/f}, s (26) el, SI --+e~, sz (27) (e~,e2), SI -(ej, e2), S2 e, S1+ e , sz (28) (v, e), s, + (v, e \n), sz In addition, there are three new rules that define the creation, updating and dereferencing of \nstore cells: afldoms (29) ref v, s -----+ a, s+(a, v) assign (a, -o), s --+ (), s+(a, v) (30) deref a, \ns + s(a), s (31) The following lemma will be needed in the proof of the result: Lemma 4.1 (Reordering \nLemma) If FK,P~ :e]L K , P ~ :e]Q K ,P ~ :e ] where p @ S and K, P~ : e] is well-formed, then E K, P~ \n:e]@ K , P~ :e ] ~ K ,P ~ :e ] where K = K -(K K) . 124 Proof: We know that t-K , P ~ : e] ~ K , P \n~ : e ] and that K K and than ~ : e] are disjoint. There\u00adfore t-K,P ~ :e]~ K , P ~ :e ]byCase5of the \nPreservation Lemma. Then Case 7 of that lemma shows that h K, P~ :e]3 K , P~ :e ]. We also know that \n1-K,P~ : e] ~ K , P ~ : e] and that K  K and K are disjoint. Therefore t-K ,P~ : e] ~ K ,P ~ : e] by \nCase 4 of the Preservation Lemma. Furthermore, since p @ S and chan~ : e ] z K , we can deduce that 1-K \n, P~ : e ] 3 K , P ~ : e ] by Cases 6 and 7 of the Preservation Lemma. We now define a representation \nof a store as a set of processes. First, we define a map from basic values in the sequential language \nwith stores to expressions in the concurrent Ianguagel. This map extends to values and expressions in \nthe obvious way. rep(ref )= fn x => let addr = channel () in let f = rec cell(y) => cell (sync <choose, \n( <receive, addr>, <send, (addr, y)> )>) in fork (fn z=> f x); addr end end rep(deref ) = fn a => sync \n<receive, a> rep(assign) = fn x => sync <send, x>; () rep(b) == b, b @ {ref, deref, assign}. In our proof \nof the result we frequently make implicit use of the fact that the representation of an expression is \ncomposed of the representation of its sub-expressions. For example, rep (el e2) = (rep e,) (rep ez). \nNext, we define the set of addresses to be a subset of the set of channels. We also introduce a subset \nof pro\u00adcessIds, called storeIds, and an injection stzd : a H pa from addresses to storeIds. We define \nthe following abbreviations: cell~ef(a) = rec cell(y) => cell (sync <choose, ( <receive, a>, <send, (a, \ny)> )>) 1For clarity, we use the derived forms let x = eI in ez end and e3; e4 to mean (fn x => e2) el \nand (fn y => eq) es respectively (where y is not free in es). cellfi(a) = fn y => celldef (a) (sync \n<choose, ( <receive, a>, <send, (a, y)> )>) cell(a, v) = celijn(a) (sync <choose, ( <receive, a>, <send, \n(a, v)> )>) These abbreviations record the recursive evaluation ofa cell, When a cell cell(a, v) communicates \nwith an\u00adother process, it evaluates to cellfn(a) v , which evalu\u00adatesin two steps to cell(a, v). When \nacellis created, celldef(a) evaluates to celtjn(a). Now we can define the representation of an element \nof a store. This definition extends to stores in the obvious way: rep(a, v)=~a : celj(a, v)] Now we \ncan state our second result. This shows that our representation of stores produces the same results as \nthe augmented sequential language, when only one process can access the store. The theorem has two parts, \none showing that our language cannot produce results that the sequential language cannot, and one showing \nthat our language can produce every result that the sequential language can. We are unable to state the \ntheorem using iff because the side-conditions on the store are part of the implied statement in both \ncases, instead of being linked to one of the evaluations. Theorem 4.2 (Store Theorem) Part A. Let K,P~ \n: rep e] be well-formed and {~ : rep e]} [1 P. Lets be a store with stid(dom s ) disjoint from (dom P) \nU{p}. If i-e,{} + v, s then there is a reduction sequence T of the form: F K, l?~ : rep e] --+ K U (dom \ns ), P U (rep s )~ : rep v] where P [1 (rep s ) ~ : rep v] and the T-e~ect of {p} is stid(dom s ) U \n{p}. Part B. Let K, P~ : rep e] be well-formed and let {~ : rep e]} [1 P. Let T be a reduction sequence \nwith the form t-K,P~ : rep e] + K , P ~ : rep v]. Then there exists a store s such that: F e,{} --+ v, \ns where P II (rep s ) ~ ; rep v], the !f -eflect of {P} % stid(dom s ) U {P}, (dom s) ~ K and rep s \n~S P . Proofi In each part we prove a stronger statement, from which the result follows trivially. We \nbegin each eval\u00aduation with a store s (or representation thereof) in\u00adstead of the empty store, and add \nthe condition that f II (rep s)~ : rep e]. As a result, stid(dom s )U{p} is now required to be the T-effect \nof {p} U (rep s). (This strengthening of the statement is why we use the variable s instead of s in the \noriginal.) l?art A: If we can show that the statement holds for a single transition 1-e,s + e , s , then \nthe result follows by a simple induction on the length of t-e,s +* v, s!. We can show that this property \nholds by using induction cm the depth of inference of t-e,s ---i* v, s and considering cases of e: Case \n1. e S fnz=>el v. Then e = el{v/z}and s = s, from Rule 24. Rule 3 is the only rule in the parallel language \nspecification that matches e. It yields the configuration: KU(dom s), PU(rep s)~ : rep(el{v/z})] amd \nthe result follows directly. Case 2. eE cv. Case 3. e ~ rec f(z) +e~. These cases are similar to Case \n1. Case 4. e S e~ ez (e~ @ Value). Then F e,s + e , s must have been inferred by Rule 22. Therefore E \nel, s + e{, s . By induction, there exists a reduction sequence T with the form: II-KU(dom s), PU(rep \ns)~ : rep el] -* K U(dom s ), P U(rep s )~ : rep ej] where P II (rep s ) (p : rep e;] and the T -effect \nof stid(dom s)U{p} is stid(dom s )U{p}. From (repeated applications of) Rule 1, there is a reduction \nsequence T/ of the form: 1-KU(dom s), PU(rep s)~ : rep e] + K U(dom s ), P U(rep s )~ : rep e ] where \nthe T -effect of stid( dom s) U {p} is stid(dom s )U{p}. Also, we know that P II ~ : rep e2], and so \nwe have P II (rep s )~ : rep e ] as desired. Case 5. eE v e~. Case 6. e -(el, ez) (el @ Value). Case \n7. e S (v, e~). These cases are similar to Case 4. Case 8. eE ref v Then e = a, and s = s+ (a, v), where \na @dom s, by Rule 29. Also, repe=fn x=> let addr = channel () in let f = Gd~def (cmidr ) in fork (fn \nz => f x); addr end end v This can be evaluated by the reduction sequence shown in Figure 6, which produces \nthe configuration: KU(dorn s) U{a}, PU(rep s)~ : a]~. : cell(a, v)] as desired. Case 9. e S assign(a, \nv) Then e = () and s = s+(a, v), from Rule 30. Also, rep e = (fn x => sync <send, x>; ()) (a, v). The \nevaluation of rep e begins with the following tran\u00adsit ion: E KU(dom s), PU(rep s)~ : rep e] _ KU(dom \ns), ~U(rep s)~ : sync <send, (a, v)>; ()] Let rep(a, v) = ~~ : celi(a, v)]. Then the evaluation continues \nwith the transition shown in Figure 7. This produces the configuration: ~dom s), 11.kep(s-(a,v))~~ : \ncelljn(a) v]k : v; ()] which evaluates in two steps to: ~dom s),llh-ep(s-(a, v))~~ : cell(a,v)]k : V; \n()] and then to: KU(dom s), .?Wrep(s-(a, v))ba : cell(a, v)]~ : ()] as desired. Case 10. e E deref a \nThen e = s(a) and s = s, from Rule 31. Also, rep e = fn x => sync <receive, x> a. The evaluation of rep \ne begins with the following tran\u00adsition: t-KU(dom s), PU(rep s)~ : rep e] ----+ KU(dom s),.PU(rep s)~ \n: sync <receive, a>] Let rep(a, v) = ~a : cell (a, v)]. Then the evaluation continues with the transition \nshown in Figure 8. This produces the configuration: KU(dorn s), PUrep(s (a, v))k~ : cell~n (a) v] ~ : \nv] which evaluates in two steps to: KU(dom s), PUrep(s (a, v))~a : cell(a, v)]~ : v] as desired. Part \nB: By Corollary 3.3 there exists a projection T of the reduction sequence that we start from, with the \nform: E KU(dom s), (rep s)~ : rep e] + K U(dom s ), (rep s )~ : rep v] Each process in rep s has the \nform ~a : cell (a, v)], where a is unique. The only rule that matches one of these processes is Rule \n10. This rule requires an\u00adother process to perform a sync operation at the same time, and the communication \nrules require that the two communications must share a channel. Therefore p must be a selected process \nof the first transition of T. Also, each process in rep s can only interact when it has the form ~a : \ncell (a, v)]. After interacting and just after creation, it has a different form, and can t interact \nagain until it returns to this form. Therefore we can use the Reordering Lemma to produce a reduc\u00adtion \nsequence T which is identical to T except that all transitions that return a process in rep s to the \nform cell(a, v)] occur immediately after the relevant a: I.P interaction or creation. We proceed by induction \non the length of T , con\u00adsidering cases of e. Case l.e=fnx=>elv By Rule 3, the first transition of T \nis: k KU(dom s), (rep s)~ : rep e] \u00ad .KU(dom s), (rep s)~ : rep(el{v/z})] Now, Rule 24 is the only sequential \nrule that matches e, and it yields the configuration el {v/z},s. By induction, there exists a reduction \nsequence k el{v/z}, s +* v, s . The result follows, Case 2. eS cv. Case 3. e S rec j(z)~e~. These cases \nare similar to Case 1. Case 4. e -e~ e~ (e~ @ Value). T must begin with a sequence of transitions which \neach have an instance of Rule 1 as the final inference of the transition. The premises of these instances \nform an evaluation of the form: E KU(dom s), (rep s)~ : rep el] + K U (dorn s ), (rep s )~ : rep Vl] \nSince this is a smaller evaluation than T , we can use induction to show that F el, s ---i* VI, s . Repeated \napplications of Rule 22 give us t-e,s --+* vl e2, s . The remainder of T has the form: h K U(dom s ), \n(rep s )~ : rep(vl ez)] +* K U(dom s ), (rep s )~ : rep v] By induction, 1-VI e2, s +* v, s . The result \nfol\u00adlows directly. Case 5. e=vel. Case 6. e s (el, e2) (el @ Value). Case 7. e S (v, e~). These cases \nare similar to Case 4. Case 8. e= ref v We know that rep(e) =fn x => let addr = channel () in let f = \nCdzdef (add? ) in fork (fn z => f x); addr end end v Therefore T must begin with a permutation of the \nsequence of transitions given in Case 8 of Part A. This yields the configuration: KU(dom s) U{a}, PU(rep \ns)~ : a]~. : cell(a, v)] The only sequential rule that applies is Rule 29. This gives the configuration \na,s+ (a, v) as desired. Case 9. e -assign (a, v) We know that rep e = (fn x => sync <send, x>; ()) (a, \nv) Therefore T must begin with a permutation of the sequence of transitions given in Case 9 of Part A. \nThis yields the configuration: 126 KU(o?om s), Fl_J(rep s)~ : rep e] ----+ KU(dom s), -? W(? ep s)~ \n: let addr = channel () in . . . end] + KU(dom s) U{a}, RJ(rep s)~ : let addr = a in . . . end] + (a@ \nKU(dom s)) KU(dorn s) U{a},.F U(? ep s)~ : ].et f => cellde~(a) in . . . end] + KU(dom s) U{a},.PU(rep \ns)~ : let f => celi~n (u) in . . . end] - KU(dorn s) U{a}, PU(rep s)~ : -fork (fn z => celj~(a) v); a] \n-+ KU(dom s) U{a}, PU(rep s)~ : (); a]~a : cellf,, (a) v] which evaluates in two steps to: .KU(dom s) \nU{a}, PU(rep s)~ : (); a]~a : cell (a, v)] which evaluates in one step to: KU(dom s) U{a}, PU(rep s)~ \n: a]~a : celz(a, v)] Figure 6: The sequence of transitions for Case 8 of the proof of the Store Theorem \n(omitting premises). <receive, a>, <send, (a, v)> <cLoose, . . . >, <:send, (a, v)> Ku(dom s), PUrep(s \n(a, v~)~a : sync <c:hoose, . . . >]~ : sync <send, (a, v)>] K u(dom s), ~urep(s (a, v))~a : sync <chc~ose, \n. . . >]~ : sync <send, (a, v)>; ()] KU(dom s),lll~ep(s-(a,v ))~. : cell(a,v)]~ : sync <send, (a, v)>; \n()] ?), v w, v + KU(@n s), PUrep(i (a, v))~a : v]~ : v] KU(dom s), PUrep(s--(a, v))~a : v]~ : v; ()] \nKU(dom s), PUTep(s (a, v))~a : cei$n(a) v]~ : v; ()]  Figure 7: The key transition of Case 9 of the \nproof of the Store Theorem. KUdom s, PUrep(s-(a, v))~a : cell(a,~)]~ : ()] The only sequential rule \nthat applies is Rule 30. This gives the configuration ( ),s+ (a, v) as desired. Case 10. e = deref a \nNow, rep e = fn x => sync <receive, x> a. Therefore T must begin with the sequence of tran\u00adsitions given \nin Case 10 of Part A. This yields the configuration: KUdom s, PUrep(s (a, v))~a : cell(a, v)]~ : v] The \nonly sequential rule that applies is Rule 31. This gives the configuration v,s as desired. Further Work \n There are three main avenues that we wish to follow with this work in the future. The first is to see \nhow other communication constructs can be expressed in our semantics. Many constructs can be expressed \nas library functions using the primitives presented here, as Reppy has shown [Rep89]. We hope to define \na wide range of features in this way, possibly including those of LINDA [CG89]. For example, we could \ndefine an asynchronous send operator as a function that forks a new process to send the value. We could \nuse techniques similar to those used in the Store Theorem to show that our definition behaved as desired. \nA compiler could include an im\u00adplementation of this function that was more efficient than the formal \ndefinition. This would continue an established tradition in the ML world, typified by the current definition \nof arrays in terms of lists [Ber91a]. Some constructs probably can t be expressed in terms of the ones \ngiven here. We hope to extend our semantics to cover these constructs explicitly. For ex\u00adample, in Reppy \ns latest paper on CML he includes some new primitive operators, guard and wrap~bort [Rep91a]. guard takes \na function argument of type () -> a corn. When sync is applied to it, it gener\u00adates a communication value \nby applying the function. wrapAbort pairs a communication value with a func\u00adtion that is called if the \nvalue is part of a choice and <send, (a, a)>, <recei,ve, a> <choose, . . . >, <receive, a> KU(&#38;Im \ns), ~Urep(s (a, v))~a ~ sync <choose, ...>]~ : sync <receive, a>] KU(dom s), PUrep(s (a, v))(pa : cell \n(a, v)]~ : sync <receive, a>] v> v v, v + KU(do?72 s), PUrep(S (a, v))(pa : v]~ : v] KU(dorn s), PUrep(s-(a, \nv))~a : cezlfn(a) v]~ : v] Figure 8: The key transition for Case 10 of the proof of the Store Theorem. \nthe choice selects another communication. Reppy s semantics for CML includes definitions for these op\u00aderators, \nand we don t foresee any problems defining them in our semantics. The second avenue that we wish to follow \nis that of incorporating our semantics with the Definition of Standard ML [MTH90]. The Definition uses \nthe re\u00adlational style of operational semantics (which we also use for our communication rules). It seems \nto be im\u00adpossible to use this style to define concurrent systems, because there is no way to specify \npotentially infinite interleaved evaluations. We need a way of relating the two styles of opera\u00adtional \nsemantics. One approach is to follow the ideas in Berry s thesis [Ber91b], and define a syntactic ex\u00adpansion \nof relational rules into corresponding transi\u00adtional rules. Another approach, not necessarily dis\u00adjoint, \nis to define several features of SML in terms of our primitives, and then to show an equivalence be\u00adtween \nan appropriately reduced version of the defini\u00adtion and our simple language. The theorems presented here \nare steps along this path. The third avenue that we wish to follow is that of implementation. CML has \nalready been implemented on uniprocessors and on the Mach operating system. Matthews has implemented \nhis primitives on unipro\u00adcessors and on a shared memory multiprocessor (the DEC Firefly). He is working \nwith us on an imple\u00admentation in which the persistent store of Poly/ML is distributed across a network \nof workstations, thus allowing processes to be created remotely. It is trivial to define our primitives \nin terms of his, so we should soon be able to test our primitives in a true distributed implementation. \nWe are also working on a proof of correctness for a protocol that implements our communication op\u00aderations \nin a distributed environment. Most concur\u00adrent languages don t allow choices between both send and receive \nconstructs; our language does, and this complicates the implementation. Some extensions to CSP have tackled \nthis problem [BS83], but these use process-to-process communication instead of commu\u00adnication via channels. \nThe only work that we know of in this area is Knabe s implementation of Facile [Kna] and Mit chell s \nimplementation of PFL [Mit86]. 6 Conclusion We have presented a semantics for a simple concurrent language. \nThe features of this language are the same as those of CML, and are similar to other concurrent extensions \nof SML. As in CML, the functions send, receive, etc. build suspended communications of the type a corn. \nThe sync operator must be applied to a value of this type to make the communication actually happen. \nWe use this system because it gives a simple semantics. By contrast, Reppy uses it for its practical \nutility. This agreement of the theoretical and practical suggests that this set of primitives is both \nnatural and desirable. We have shown that our semantics preserves the desired behaviour of sequential \nexpressions. We have also defined stores in terms of our primitives, and have shown that these behave \nas desired. The proofs of these theorems use some lemmas that should be useful in proofs of similar statements. \nOur theorems are steps on the way to incorporating our semantics with the Definition of Standard ML. \nWe are investigating other constructs for concurrency with respect to our semantics, and are implementing \nour primitives in a distributed version of SML.   References [BB90] G. Berry and G. Boudol. The chemical \nab\u00adstract machine. In Proceedings of the Seven\u00adteenth ACM Symposium on Principles of Pro\u00adgramming Languages, \n1990, 128 [Ber89] B. Berthomieu. Implementing CCS, the LCS experiment. Technical Report 89425, LAAS- \nCNRS, 1989. [Ber91a] Dave Berry. The Edinburgh SML library. LFCS Report Series ECS-LFCS-91-148, Lab\u00adoratory \nfor Foundations of Computer Science, University of Edinburgh, 1991. [Ber91bl Dave Berry. Generating Program \nAnimators from Programming Language Semantics. PhD thesis, University of Edinburgh, 1991. [BS83] G.N. \nBuckley and A. Silberschatz. An effec\u00adtive implementation for the generalized input\u00adoutput construct \nof CSP. ACM Transac\u00adtions on Programming Languages and Sys\u00adtems, 5(2):234 238, Apr 1983. [CG89] Nicholas \nCarriero and David Gelernter. How to write parallel programs: A guide to the per\u00adplexed. ACM Computing \nSurveys, 21(3):323\u00ad357, September 1989. [CM90] Eric C. Cooper and J. Gregory Morrisett. Adding threads \nto Standard ML. Technical Report CMU-CS-90-186, School of Computer Science, Carnegie Mellon University, \n1990. [GMP89] Alessandro Giacalone, Prateek Mishra, and Sanjiva Prasad. Facile: A symmetric inte\u00adgration \nof concurrent and functional program\u00adming. International Journal of Parallel Pro\u00adgramming, 18(2):121-160, \nApril 1989. [Hen90] Matthew Hennessy. The Semantics of Pro\u00adgramming Languages. Wiley, 1990. [H0183] Soren \nHolstrom. PFL: A functional. language for parallel programming and its implementat\u00adion. Report 83.03 \nR, Department of Com\u00adputer Science, Chalmers University of Tech\u00adnology, 1983. [Kna] F. Knabe. A distributed \nprotocol for channel\u00adbased communication with choice. ECRC, Munich. In Preparation. [Mat91] David Matthews. \nA distributed concurrent implementation of Standard ML. In EurOpen Autumn 1991 Conference, 1991. To appear. \n[Mit86] Kevin Mitchell. Implementations of Process Synchronisation and their Analysis. PhD the\u00adsis, Department \nof Computer Science, Univer\u00adsity of Edinburgh, Jul 1986. [MTH90] Robin Milner, Mads Tofte, and Robert \nHarper. The Definition of Standard ML. MIT, 1990. [Rep89] J. H. Reppy. First-class synchronous opera\u00adtions \nin Standard ML. Technical Report TR 89-1068, Dept. of Computer Science, Cornell University, 1989. [Rep91a] \nJ. H. Reppy. CML: A higher-order con\u00adcurrent language. In ACM SIGPLAN 91 Conference on Programming Language \nDe\u00adsign and Implementation, SIGPLAN Notices 26(6), pages 294-305, 1991. [Rep91b] J. H. Reppy. An operational \nsemantics of first-class synchronous operations. Technical Report TR 91-1232, Dept. of Computer Sci\u00adence, \nCornell University, Aug 1991. [WF91] A. Wright and M. Felleisen, A syntactic ap\u00adproach to type soundness. \nTechnical Report TR91-160, Dept. of Computer Science, Rice University, Apr 1991. \n\t\t\t", "proc_id": "143165", "abstract": "<p>We present a set of concurrency primitives for Standard ML. We define these by giving the transitional semantics of a simple language. We prove that our semantics preserves the expected behaviour of sequential programs. We also show that we can define stores as processes, such that the representation has the same behaviour as a direct definition. These proofs are the first steps towards integrating our semantics with the full definition of Standard ML.</p>", "authors": [{"name": "Dave Berry", "author_profile_id": "81361592517", "affiliation": "", "person_id": "PP45030168", "email_address": "", "orcid_id": ""}, {"name": "Robin Milner", "author_profile_id": "81332515695", "affiliation": "", "person_id": "PP37038567", "email_address": "", "orcid_id": ""}, {"name": "David N. Turner", "author_profile_id": "81100229526", "affiliation": "", "person_id": "PP31097951", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143165.143191", "year": "1992", "article_id": "143191", "conference": "POPL", "title": "A semantics for ML concurrency primitives", "url": "http://dl.acm.org/citation.cfm?id=143191"}