{"article_publication_date": "02-01-1992", "fulltext": "\n Generating a Compiler for a Lazy Language by Partial Evaluation Jesper J@rgensen * DIKU, Department \nof Corn uter science EUniversity of Copen agen Universitetsparken~LnDm~~~lOO Copenhagen 0 e-mail: knudQdiku.dk \n Abstract Compiler generation is often emphasized as being the most important application of partial \nevaluation. But most of the larger practical applications have, to the best of our knowledge, been outside \nthis field. Especially, no one has generated compilers for langnages other that small lan\u00adguages. Thk \npaper describes a large application of partial evaluation where a realistic compiler was generated for \na strongly typed lazy functional language. The language, that was called BAWL, was modeled after the \nlangnage in Bird and Wadler [BW88] and is a combinator language with pattern matching, gnarded alternatives, \nlocal defini\u00adtions and list comprehensions. The paper describes the most important techniques nsed, especially \nthe binding time improvements needed in order to get small and ef\u00adficient target programs. Finally, the \nperformance of the comDi~er is comrtared with two comDilers for similar lan\u00adgua~es: Mirand; and LML. \nKeywords Compiler generation, partial evaluation, binding time im\u00adprovements, lazy functional languages. \nIntroduction This paper describes a large application of partial evalua\u00adtion. Our aim was to try to write \na prototype compiler for a strongly typed lazy functional language. We wanted to see how far one could \nget by using partial evaluation as a com\u00adpiler generation tool and what problems this might involve. \nThe language chosen for this purpose was called BAWL and was modeled after the language in Bird and Wadler \n[BW88], a langnage in the class of languages that also in\u00adcludes KRC [Tnr82], Orwell [Wad85], Mirandal \n[Tur86] and Haskell [HW90]. A parser was generated by using the standard UNIX2 facility Yacc [Joh75]. \nNo type checker ' This work wss supported by ESPRIT Basic Research Actions project 3124 T5em antique \n1Miranda is a trademark of Research Software Ltd, 2 UNIX is a trademark of Bell Laboratories, Permission \nto copy without fee all or part of this material is granted provided that the copies are not made or \ndistributed for direct commercialadvantzge,the ACM copyright notice and the title of the publication \nand ita date appear, and notice is given that copying is by permission of the Association for Computing \nMachinery. To copy other\u00adwise, or to republish, requires a fee and/or specific permission, @ 1992 ACM \n089791453-8/92/0001/0258 $1.50 has been written, but its existence is supposed such that the code generation \npart could assume that it was working on well typed programs. The code generating part of the compiler, \nwhich is the one described in this paper, was gen\u00aderated by using partial evaluation. The partial evaluator \nused was Similix [BD9 I] [Bongo], an ardoprojector (self\u00ad applicable partiaf evaluator) for a higher \norder snbset of Scheme [RC86], including side effects on global variables. This means that the target \nlangnage for the compiler is Scheme. Since efficient compilers exist for Scheme we can translate our \nsource programs into sufficiently low level code by first translating them into Scheme code and then \ncompiling the Scheme code. 1.1 Outline of the paper The paper is organized as follows. After a short \nsection on notation, we introduce the notion of compiler generation. Then in section-3 we introduce the \nlanguage and in section 4 we describe the interpreter and describe how the com\u00adpiler was generated. In \nsection 5 we describes the binding time improvements used to improve the performance of the compiler. \nIn section 6 we show an example of a source program and its corresponding target code. In section 7 we \nillustrate the performance of the compiler and in sec\u00adtion 8 we discuss further improvements of the compiler \nand partial evaluation as a compiler generation tool. Section 9 con t ains a conclusion. 1.2 Notation \nWe will use denotationaf semantics in the style of Schmidt [Sch86] when describing the semantics of programs \nand program constructs. The notation (L P d) denotes the result of rnnning the L-program P on data d, \ne. g. (Scheme P d) denotes the result of running the Scheme-program P on input d. 1.3 Prerequisites \n [t is assumed that the reader has a basic knowledge of func\u00adtional programming, partial evaluation and \ncompiler gen\u00aderation by partial evaluation, e.g. as presented in [JSS85] or [JSS89]. 2 Compiler generators \n A compiler generator is a program (or system) that given some machine readable formaf description of \na program\u00adming language produces a compiler for that language. The formal description can be of many \nforms: a denotational semantics, an attribute grammar, a combinator based se\u00admantics, some form of an \noperational semantics, etc. 2.1. Compiler generation by partial evaluation Partial evaluators are not \ncompiler generators, but one of the primary examples of their uses has certainly been com\u00adpiler generation. \nThis is also supported by the fact that one of the main motivations for making partial evaluators self-applicable \nis that it makes compiler generation possi\u00adble. Today self-applicable partial evaluators are available \nfor many types of languages, e.g. imperative languages (a flowchart language [GJ89]) and functional languages \n(Mixwell [JSS85] and Scheme [Bon91b] [Con88]). The way one generates a compiler for a given language \nby partial evaluation is by writing an interpreter Int for the language. From this interpreter the compiler \nis generated by specializing the partial evaluator Mix with respect to the interpreter: Compiler = L \nMix (Mix,Int) One gets a compiler generator by specializing the partial evaluator with respect to itself: \nCogen = L Mix (Mix,Mix) This means that the specification is the interpreter and the specification language \nis the source language of the partial evaluator. It also means that the choice of partial evalu\u00adator \nwill fix the specification language. We have used an approach that uses a specification close to a denotational \none and therefore a partial evaluator for a higher order functional language is the most suitable. For \nthis reason and because of its in-house availability, Similix is an ap\u00adpropriate choice of partial evaluator. \n2.2 Similix This section describe the features of Similix that are im\u00adportant for understanding the ideas \nof this paper. As men\u00adtioned before, Similix is a self-applicable partial evaluator for a higher order \nsubset of Scheme including side effects on global variables. When we specialize a program we have to \nspecify some static input and a name of a function in the program, called the goal junction. The goal \nfunction is the function that we want to specialize with respect to the static input. Similix uses rnonovariant \nbinding time analysis which has the effect that residual functions are only generated for one set of \nbhding time values for the arguments. Similix does not handle partially static data structures This means \nthat Similix does not perform simple reduction like: (car (cons esprl exprz )) * ezprl The fundamental \nreason for this is that it is hard to handle such transformations in a semantically safe way in a strict \nlanguage (however, a solution is outlined in [Bon92]). If evaluation of ezpr2 may not terminate then \nthis implies that the left hand side of the rule may not terminate, but the same does not hold for the \nright hand side (this gets even worse if ezprz cent ains side effects). 3 The BAWL language We will not \ngive a formal description of the syntax of the BAWL language, but one can be found in [Jor91a] or in \nAppendix B. A program is called a script as in Orwell or Miranda and a script consists of definitions \nof types, type alb.ses, functions and conformals (constants defined via patterns, e.g. (x,y)=(l+2,fib \n20) or primes in the exam\u00adple below; for the understanding of this paper, just think of conformals as \nzero arit y functions). Here is a small ex\u00adample of a script: fac O=l fac n = n*f ac(n-1) fib n=1, if \nn<=l = fib(n-l)+fib(n-2) , other~ise primes = sieve [2. .] where sieve (p:x) = p:sieve[nln<-x; n mod \np>O] fac is the factorial function, fib a naive version of the fibonacci function and primes a conformal \ndefined to be the infinite list of primes. As can be seen from the ex\u00adample, scripts can contain Iiterals \n(constants), variables, constructors, operators, applications, list comprehensions and arithmetic sequences. \nThe languages also supports tu\u00adpies. Patterns may include literals, variables, constructor patterns and \ntuples. Some of the important features that the language does not include are input facilities and mod\u00adules. \nThe boolean constructors True and False and the list constructors : and [] are the only constructors \npredefine in the initial constructor environment. 4 Generating the compiler The compiler was generated \nin the following way. First a denotational semantics was written for the language and by a simple transliteration \nof this into Scheme, a naive inter\u00adpreter was obtained. This interpreter was then rewritten by a series \nof binding time improvements (see section 5) to yield the final interpreter. All of this process was \ndone by hand. The compiler was then generated by machine from the final version of the interpreter by \npartial evaluation. 4.1 Semantics oft he language Usnally when we assign meaning to a program we do this \nby specifying the function that the program computes. This is done under the assumption that the program \nhas some sort of main function that is the meaning of the en\u00adtire program. But often one also wants to \nrun programs in an interpreted environment (for development and debug\u00adging), and in this case it seems \nbetter to assign another meaning to a program. The meaning of a BAWL program is therefore an environment \nmapping function names to their meaning. We can express this a little more formally by using a denotational \nstyle semantics: S [script] = fix(A#.[fj + Au... At)nj .Euezprjn [~j,i + ~i]md](h) where S is the function \nassigning meaning to programs. fj ranges over all functions defined in the script, nj is the arity of \nfj, do an initial function environment and PO an initial variable environment. The function E that assigns \nmeanings to expressions is shown in Appendix C. We have left out the semantics of constructors, pattern \nmatching, guards and local definitions to make the description sim\u00adpler. A detailed description of these \naspects can be found in [Jor91a]. Now it is simple to assign meaning to expres\u00adsions in an interpreted \nenvironment: I~script] uexprn = E~ezprn p. (S~scriptn )  4.2 The structure of the interpreter The overall \nstructure of the interpreter follows the seman\u00adtics. It takes a script and an expression expr as input \nand evaluates exprin the scope of the environment created by the functions. Eistheevaluation function \nfor expressions, print is the print routine driving the evaluation. (define (I script expr) (print (E \nexpr init-venv (S script)))) s takes a script and returns a function environment, (define (S script) \n(fix (lambda (fenv) (lambda (fun) (if (defined-in-script fun script) (let* ([clef (lookup-clef fun script)] \n[expr (lookup-expr clef)] [formals (lookup-formals clef)]) (make-function formals expr init-venv fenv)) \n(init-fenv fun)))))) (define (make-function formals expr venv fenv) (if (null? formals) (E expr init-venv \nfenv) (lambda (v) (make-function (cdr formals) expr (upd (car formals) v venv) fenv)))) where the function \nfix is the standard applicative fixed point operator defined as: (define (fix f) (lambda (x) ((f (fix \nf)) x))) That fix actually produces the right fixed point is beyond the scope of this paper, for details \nsee [Jor91a]. Appendix D shows the E function as it looks in the first version of the interpreter. 4.3 \nGenerating the compiler If we want to generate a compiler from the interpreter I, all we have to do is \napply the compiler generator cogen to I and we are done. But there is a problem with this ap\u00adproach, \nsince expr in I is dynamic (we only know the script when compiling) while expr in make-funct ion is static. \nThis will make the first argument to E dynamic since the bind\u00ading time analysis of Similix is monovariant. \nThis spoils the whole idea of compiler generation since we want expr in make-function to be static to \nget some specialization of expressions in the script done. We will therefore choose a slightly different \napproach: we will only apply cogen to a program without the function I and make s our goal func\u00adtion. \nThis will give us a compiler that given a script scr produces a residual program with a specialized version \nS-O of s: That is, the compiled script is a Scheme program with a function S-O that when called returns \na function environ\u00adment. It might seem strange that we want to partial evrduate S, when s haa only static \ninput. One might think that we could just apply s to the script instead. There are two reasons that this \ndoes not give the desired result. Firstly, applying s to a script will not generate a piece of code that \ncan be saved on a file. Instead it will return an internal represention of a function. Secondly, standard \nevaluation of lambdas in Scheme is to weak head normal form and thus does not evaluate under lambdas. \nOn the other hand, a partial evaJuator does perform static reduction under lambdas (controlled by the \npartial evaJuator s particular strategy), so applying (s-o) to a value will be more efficient than applying \n(S scr) to a value. Now that we have removed the interpreter function from the program we can define \na slightly different version that will be used when running compiled scripts: (define (I fenv expr) (print \n(E expr i.nit-venv fenv) )) This takes a function environment fenv instead of a script as before. This \nfunction environment can be the result of a compilation or can be produced by the function S. We can \nnow define a function run that takes an expression expr and the name of a file which holds a compiled \nscript and evaluates the expression with respect to the script: (define (run expr script-file-name) (begin \n(load script-file-name) (I (S-O) expr))) The description we have given so far in this section is somewhat \nsimplified, since s also returns a constructor en\u00advironment and some tables, but we have left out these \ndetails, since they are not important for the general ideas. 4.4 Lazy evaluation When translating into \nScheme, functions of our source lan\u00adguage functions become Scheme functions and are there\u00adfore strict. \nBut we want our implementation to be lazy. Often call by need is called lazy evaluation, since it post\u00adpones \nevaluation of arguments until these become needed. But we want more than call by need, that is, we want \nour implementation to be lazy in more than evaluation of ar\u00adguments. Assume that we write the following \nversion of the fibonacci function: = fl!n fib n uhere fl = I:l:[fl!n + fl!(n+l)ln<-[O. .]] Here +1 is \nnot an argument but a conformal, and we surely do not want to calculate the conformal fl every time it \nis used. To summarize: we want our implementation to be lazy in two ways, evaluation of arguments and \nevaluation of conformais.  4.5 Achieving call by need Let us first see how we can achieve call by need. \nCall by (lambda (fun-O) need can be viewed as an optimization of call by name and . ..)) call by name \ncan easily be simulated in Scheme. This is (define (S-O) done by suspending the evaluation of an argument \nexpres\u00adsion ezpr by putting a lambda abstraction around it: (lambda () ezpr) and we say that we have \nsuspended the evaluation of expr. Then, when the value of the argument is needed, instead of just using \nthe argument a we have to apply it first: (a) That is, we force the evaluation of the expression in \na. In this way an argument is evaluated each time it is needed. There are several methods to avoid this, \nbut the one pre\u00adsented here (taken from [RC86])3 is both simple and very efficient to execute4. For delaying \nan argument we now use: (save (lambda () e)) where save is defined as: (define (save s) (let ([v 01 \n[tag #f]) (lambda () (begin (unless tag (set! v (s)) (set! tag *t)) v)))) In this way the argument expression \nis only evaluated if the tag is true and the first time the argument is forced the tag is set to true. \nThis ensures that arguments are evaluated at most once. The expression evaluation function E then handles \napplication like this: (cond ... ( (EJIPPIY? expr) ((E (EApply->El expr) venv fenv) (save (lambda () \n(E (EAPPIY-JE2 expr) venv f env) ) ) ) ) ,,. ) where venv is the variable environment and fenv is the \nfunction environment. 4.6 Ivfaking evaluation of conformals lazy To handle lazy evaluation of conformals \nwe had to use a slightly different approach than used to achieve call by need. The save operation of \nsection 4.5 introduces a new memory cell (the v and tag variables) where the result of evaluating the \nargument is going to be saved the first time it is evaluated. This works because the saved evaluation \nis unique, i.e. it is the argument to a specific application. But evaluation of a conformal can be initiated \nmany places in a program (reexamine the example in section 4.4) so in order to get evaluation of the \nconformal shared, we have to save its value in a global memory cell. This is done in the following way: \nBefore doing the fix point iteration in s we create new memory cells for all the conformals in the script \n(top level only ) and the evaluation of the con formals are then saved in these using a slightly different \nversion of save called sava-at: 3Compiling lazy languages by partial evaluation seems to have been done \nfor the first time in [Bon91a]. 4 I have compared several different versions that people at DIKU have \ncome up with and this one performed best (define (save-at a s) (lambda () (unless (cdr a) (set-car! a \n(s)) (set. -cdr! a *t)) (car a))) save-at saves the result of evaluating s in the cell pointed toby \na. 5 Binding time improvements of the in\u00adterpreter A common experience among people working with partial \nevaluation is that not all programs are equally well suited for partial evaluation and that a certain \namount of rewrit\u00ading of programs are necessary to make them partial eval\u00aduate well . This means that \nmuch of what is presented in this section is in no way particular to partial evalua\u00adtion of interpreters, \nbut rather to the method of partial evaluation used. The rewritings are semantics preserving transformations \nthat aim to make more parts of a program static such that the partial evaluator can do more work, hence \nthe name binding time improvements . For fur\u00adther discussions of binding time improvements see [H H91], \n[HG91] or [Bon91b]. Here is a list of the important binding time improvements we have used: ' Using static \ninformation about dynamic data. ' Transformations into continuation passing style. ' Simulating partially \nstatic data structures to achieve variable spMting [Ses85]. ' Removal of bindings from environments, \n The last point is not really a binding time improvement, but it does improve the size and performance \nof the tar\u00adget programs. We will now explain these improvements in detail. 5.1 Using static information \nabout dynamic data A key point in the rewriting process is to identify static information that is not \nvisible to the specialize and make this visible. Let us for instance assume that we have some dynamic \nvariable x somewhere in a program. Since x is dynamic the specialize can not do any computation de\u00adpending \non x. Now it might be possible from the context in which x occurs to detect some information about the \nvalue of x depending only on static values in the program and then rewrite the program such that the \nspecialize can utilize this information. This idea waa one of the main ones introduced in the KMP example \nby Consel and Danvy [CD89]. Let us look at an example that illustrates this idea. The example shows a \nrewriting that is classic (it was already used in the original MIX project [JSS85]) and it is abso\u00adlutely \nessential to be able to get a reasonable result. If we look at our definition of s of section 4.2 we \ndefinitely want fun to be static, since if it is not, then clef, expr and formals all becomes dynamic \nand we can hardly expect the specialize to do any serious work under these conditions. But fun is not \nstatic, because it can clearly be bound to function names from the dynamic expression of the I func\u00adtion. \nThe observation that saves the day is that we know which values that fun can possibly be bound to, namely \nthe function names defined in script. If fun is bound to something else it is either a name defined in \nthe initial function environment or an error. So if we rewrite S into (define (S script) (let ([funs \n(collect-function-names script)] [fenv (fix . ..)]) (lambda (fun) (S1 fun funs fenv)))) (define (S1 fun \nfuns fenv) (if (null? funs) (init-fenv fun) (let ([funl (car funs)]) (if (equal? fun f uni ) (fenv funi \n) (S1 fun (cdr funs) fenv) )) ) ) where (fix . . . ) is the original body ofs (cf. section 4.2), then \nthe generated function environment fenv is now only applied to the static funi in S1 rather than the \ndynamic fun. The idea is used extensively in rewriting the interpreter and is essential to make pattern \nmatching compile into efficient code. This is described in detail in [Jor91b].  5.2 Continuation passing \nstyle The most important rewriting is transformation into con\u00adtinuation passing style (in some cases \njust tail recursive form). The importance of this has also been shown in [Dan91]. The cases that can \noften successfully be handled by this kind of transformation are those where static values gets caught \nin a dynamic context. A typical example can be a function returning several results (packed together \nin e.g. a list) of which some are static and some dynamic, The result will then be dynamic and the static \nvalues will then be inaccessible to the specialize. By a transformation into continuation passing style \none can pass the arguments separately to a continuation which can now use the static ones of these. Another \nsituation in which transformation into contin\u00aduation passing style may improve binding times is when \nfunctions that represent specialization points (i.e. become residual functions) return higher order values. \nLet us look at an example (a trivial one, but it serves the purpose of demonstrating the idea). Consider \nan application ((f X) 42) where f is defined by: (define (f x) (if ~ some test involving x > (lambda \n(z) (subl z)) (lambda (z) (addl z)))) Assume that x is dynamic, then calls to f will be special\u00ad ization \npoints (with Similix s current strategy) and Similix will therefore make (f x) dynamic. Hence the application \nof (f x) to 42 will not be performed. Let us rewrite the application into: (fc x (lambda (c) (c 42))) \nand the definition of f into: (define (fc x c) (if ~ some test involving x > (c (lambda (z) (subi z))) \n(c (lambda (z) (addl z))))) Then application of the lambda abstractions to 42 will be performed, since \nthe continuation is applied directly to the closure values that the original function f would have re\u00adturned. \n 5.3 Partially static data structures As stated in section 2.2 Similix does not handle partially static \ndata structures, but in many cases these can be sim\u00adulated by using higher order functions. The simplest \nex\u00adample is partially static lists which can be simulated by using the following definitions (this special \nversion is due to Torben Mogensen) instead of (), cons and list -ref: (define (snil) (lambda (n c) (n))) \n(define (scons a d) (lambda (n c) (c a d))) (define (slist-ref 1 index) (1 (lambda () error) (lambda \n(a d) (if (= O index) a (slist-ref d (subi index)))))) The effect gained by this is normally called variable \nsplit\u00adting or arity raising [Rom88]. The reason that this method works is that Similix treat higher order \nfunctions, called closures, as a kind of partially static structures in that they can contain both static \nand dynamic subparts. When a closure is applied to its arguments during specialization, the body of the \nclosure is specialized and the subparts may emerge again. PartiaJ evaluators that directly handle par\u00adtially \nstatic data structures will of course not need this transformation. Another way to do this improvement \nis to replace the data structures by environments. This idea is explained in detail in [Jor9 lb].  5.4 \nRemoval of bindings from environments This transformation is very dependent on the fact that we are working \nwith an interpreter and will probably be very hard to automate. Assume that we have a definition of the \nform fxy=. ..(fac x)... uhere fac O=l fac n = n*fac(n-1) in a source program. Then a compiler generated \nby Similix from a simple interpreter gives a result of form: (define (fat-O x-O y-O) (lambda (n-O) (... \n(fat-O x-O Y-O)) ) ) The presense of the variables X-O and Y-o reflect the fact that the values of x \nand y are in fact accessible inside the definition of fa.. The specialize is unaware of the fact that \nthe definition of fac does not use these values and therefore thinks that a call to fat-o can depend \non these. This problem is also mentioned by Bondorf in [Bon90], but no solution was given there. A way \nto solve the problem is to do a live variable analy\u00adsis of fat s definition. This is in fact very simple \nsince what it boils down to is just finding free variables. The result of this analysis is then used \nto reduce the set of variables that are bound in environments to the set of live variables. This reduction \nthen has to take place before evaluating fat s right hand side. This method has been used with good results \nto reduce the size of the target code produced by the compiler. 6 An example In this section we give \na small example of what a target function generated by the compiler may look like. The target code is \nshown exactly as produced by the compiler. The example is the factorial function fac from section 3 and \nthe compiled version of this function looks like: (define (make-fun-cl-O-3 sc_O) (lambda (v.1) (if (struct-equal? \nO (vJ ) ) 1 (* (v-i) ((make-f un-cl-O-3 SC-O) (save (lambda () (-(v-l) l)))))))) where v-I is a delayed \nvalue and the code (V-I) corresponds to forcing v-i. The code is of course not optimal, but we would \nneed to do strictness analysis in order to improve further on the result. This will be discussed in more \ndetail in section 8. Appendix A shows the entire target program form which the function above is taken. \n7 Performance We will illustrate the performance of the compiler in two ways. First we will show the \nspeedup gained by partial evaluation, that is, by compilation. Second we will com\u00adpare our compiler with \ntwo similar products: Miranda ver\u00adsion 2 from Research Software Ltd. and the LML compiler of Chalmers \nUniversity. 7.1 NIeasuring run times All the tests were run on Spare station I/Sun OS 4.1. The Scheme \nsystem used was Chez Scheme Version 3.2 and the run times in Scheme were measured using the time function. \nThe Miranda system used was Miranda version 2 system from Research Software Ltd. and the run times were \nmeasured using the count option. The version of LML was .99.3 and the run times were measured using the \nS option. None of the run times include garbage collection. 7.2 Speedup We will not measure the speedup \ngained by partial evalua\u00adtion in the usual way as the ratio between the run time of the original program \n(iu our case the interpreter) and the run time of the residual program (in our case the target program), \nsince this way of measuring is not entirely fair in our case. This is because our interpreter is biased \ntowards partial evaluation 5, which means that it may run several times slower than one that is not. \nInstead we define the speedup as the ratio between the run time of the first in\u00adterpreter (before the \nbinding time improvements) running our source program and the run time of the target program produced \nby our compiler. The problems involved in per\u00adformance memmrements will be described in more detail in \n[Jor91a]. Figure 1 shows the run times and the speedups for two small examples. +ac and prinws have already \nbeen defined and take is a predefine function which takes the first n elements of a list. T1~t is the \ntime that the interpreter used to run the program, while T~a,gc~ is the time it took the target program \nto do the same task. Expression Tr~t/s. T~a,,et/s. Speedup take 100 primes 46.3 0.67 69.1 fac 100 0.82 \n0.015 54.7 Figure 1: Run-times and speedups  7.3 Comparing BAWL with Miranda and LML We have compared \nBAWL to Miranda and LML on a number of examples all taken from the Miranda manual, though some have been \nmodified slightly to stay syntac\u00adtically within LML and our language. A few results from this test are \nshown figure 2 and 3. A complete description of the test can be found in [Jor91a]. Expression Run times/see \nMiranda BAWL LML fac 100 0.048 0.015 .040 fib 20 4.43 0.96 .20 hd (drop 99 primes) 0.87 0.61 .11 Figure \n2: Compared run times Expression Storage/kbytes Miranda BAWL LML fac 100 40.3 9.97 11.3 fib 20 2050 1230 \n449 hd (drop 99 primes) 379 410 102 Figure 3: Compared storage usage We can in general state that our \nimplementation is as feat or a little faster than the Miranda system and uses about the same amount of \nstorage. We can on the other hand not compete with the graph reduction based LML system, neither with \nrespect to speed nor use of storage. 7.4 The compiler The compiler has 220 functions and its size is \naround 23.OK cells (the number of cons cells needed to represent it in memory) which is about twice as \nbig as the compiler generator of Similix. It is hard to get a fair measure of the performance of the \ncompiler itself since it performs no type checking. To give an idea, the compile time of the example \nof section 3 was .32 seconds. We have tried to compile larger examples (a pattern matching compiler and \na binding time analysis) with good results. 8 Discussion 8.1 Further improvements of the compiler There \nis still room for improvement of the compiler, both by further rewriting of the interpreter or by transform \ntions of either the source programs or the target programs. As described in section 4.6 all target functions \nget extra parameters to allow them to access the values of confor\u00admals, and this happens even if the \nfunctions never actually need to access any of these values. There are also other reasons for extra parameters \nthat we will not mention here, but a simple liveness analysis of the source programs might help to eliminate \nsome of these superfluous parameters. In the Scheme code generated by the compiler many functions are \ncurried, because these in a way inherit their structure from the source language. Uncurrying some of \nthese functions could improve performance considerably because tupled application is much faster in Scheme \n(fewer closures have to be built). Which functions to uncurry could be decided by a simple analysis of \nthe source program and the result of this could be used by the interpreter, and this would then make \nthese target functions appear in uncurried form. Strictness analysis of the source program could also \nbe used to improve the target programs. Using strictness in\u00adformation would in the case of the factorial \nfunction give a speedup of the generated code of about 3 times. It seems that strictness optimizations \ncan be combined with what corresponds to a local evaluation order analysis. This can be done by keeping \ntrack of which variables have been forced and which have not. Then instead of forcing a de\u00adlayed value \nevery time it is used we should only force it the first time, then record the forced value in the environ\u00adment \nand then subsequently use the value recorded in the environment. Using this optimization on the example \nof section 6 we can obtain, (define (make-fun-cl-O-3) (lambda (v-l) (let ([fv-i (v-i)]) (if (equal? O \nfv-1) i (* fv-i ((make-fun-cl-O-3) (save (lambda () (-fv-1 l))))))))) even without a strictness analysis. \nExperiments also show that lambda lifting the source programs can yield good results, but this is a field \nthat needs more investigation. If a type-checker were written, the result of the type\u00adchecking could \nbe used to optimize the target code, e.g. replace polymorphic equality by specific instances. Since Scheme \nis dynamically typed, Scheme implementa\u00adtions spend a lot of time doing type checking when running programs \nand this time is wasted if we know that a pro\u00adgram is well typed. Since the target programs produced \nby our compiler are all well typed, it should be possible to run these without the dynamic type checking \nand thereby improve further on their performance. One solution could be to generate code for a different \nstrict language like Stan\u00addard ML [MTH90] and this might be possible by rewriting the back end part of \nSimilix that generates code. Another solution would be to have a Scheme compiler in which one could turn \noff the run time type checking. 8.2 Partial evaluation as a compiler generation tool For a compiler generation \nsystem to be of any use it should either provide us with a faster way to obtain compilers than by traditional \nhand performed methods or it should give us a safer method to obtain correct compilers. If writing a \nspecification is just as hard as writing the ac\u00adtual compiler by hand then nothing is gained. It is hard \nto estimate the time used to write our code since the methods used were developed during the process. \nThe program be\u00ading specialized is 587 lines long and auxiliary files (called adt-files in the Similix \nsystem) containing functions op\u00aderating on the representation of value, standard environ\u00adment, the interpreter, \netc., contains 876 lines (in both cases without counting comments). By the correctness of the compiler \nwe mean with respect to the the original semantics. This correctness depends on the correctness of Similix, \non the correctness of the binding time improvements and the transformation of the semantics into the \ninterpreter. We are presently working on proving the correctness of the last two criteria. Some of the \nadvantages of using partial evaluation as a compiler generation tool are: It is generally easier to read \nand write an interpreter than a compiler. This makes it easier to change and maintain the compiler. Debugging \nan interpreter is easier than debugging a compiler. One can trace evaluation and one can look at both \ncompile time data structures and run time data structures at the same time. When debugging a compiler \neither the compiler or the target program may fail to work; there is no such separation when debugging \nan interpreter. We say that there is no separation of binding time. You get an interpreter thrown in \nto boot, that is, we get both an interpreter and a compiler at the expense of writing an interpreter \nonly. This can be useful for many purposes, especially one might instrument the interpreter with operations \ntracing the execution or doing statistics. This will the result in a compiler doing these operations \ntoo. Code generation is handled by the partial evaluator, i.e. one does uot have to think about things \nlike label or variable name generation, backpatching etc. Target and specification language is the same \n(in our caae Scheme). One less language to think about makes life easier and one may assume that specification \nlan\u00adguage is well known to the user of partial evaluation. Some of the disadvantages are: c One has less \ncontrol of the code generation since this is done by the partial evaluator. This means that one can only \ngenerate code that the partial evaluator is able to produce. Also one may not be able to obtain the sharing \nof target code that may be possible by con ventional compilers. ' Target and specification languages \nare the same. This is also a disadvantage since it prevent us from choosing different target languages. \nIn general one can say that we get the advantages at the price of some freedom in the way we can generate \ntarget code. 9 Conclusion and future work We have shown that it is possible by partial evaluation to \ngenerate compilers for languages of a realistic size and that the compilers generated can in fact produce \nreasonably fast target code even when compared to handwritten compilers. It remains to be shown whether \nit is possible to automate some or all of the binding time improvements discussed in section 5. Binding \ntime improvement is certainly an in\u00adteresting field and presently some research is under way in this \nfield. We are presently looking into the problems of formalizing and proving correct the translation \nfrom deno\u00adtational semantics into Scheme interpreters. Acknowledgements Many people have contributed \nin various ways; but I would especially like to thank Anders Boudorf, Carsten Gomard, John Hannan, Fritz \nHenglein, Neil D, Jones, John Launch\u00adbury, Lars Mathiesen, Torben Mogensen and Peter Sestoft. I would \nalso like to thank the members of the TOPPS group at DIKU, References [BD91] Anders Bondorf and Olivier \nDanvy. Automatic autoprojection of recursive equations with global variables and abstract data types. \nScience oj Computer Programming, 16, 1991. To appear. [Bon90] Anders Bondorf. Automatic autoprojection \nof higher order recursive equations. In Neil D. Jones, editor, ESOP 9o, Copenhagen, Den\u00admark. Lecture \nNotes in Computer Science J3.2, pages 70-87, Springer-Verlag, May 1990. [Bon91a] Anders Bondorf. Compiling \nlaziness evaluation. In [JHH91], pages 9 22, by partial 1991. [Bon91b] Anders Bondorf. 4.0. Included \nin 1991. Similix Similix manual, system version distribution, September [Bon92] Anders Bondorf. Improving \nout explicit cps-conversion. binding times with\u00ad1992. Forthcoming. [BW88] Richard Bird to Functional \nPrentice-Hall, and Philip Wadler. Introduction Programming. Computer Science, 1988. [cD89] Charles Consel \nand Olivier Danvy. Partial eval\u00aduation of pattern matching in strings. Iraforma\u00adtion Processing Letters, \n30(2):79-86, 1989. [cor188] Charles Consel. New insights into par\u00adtial evaluation: the schism experiment, \nIn Harald Ganzinger, editor, ESOP 88, Nancyl France. Lecture Notes in Computer Science ,5 00, pages 236-247, \nSpringer-Verlag, March 1988. [Dan91] Olivier Danvy. Semantics-directed of nonlinear patterns. Information \nLetters, 37(6):315-322, 1991. compilation Processing [GJ89] Carsten K. Gomard and Neil D. Jones. Compiler \ngeneration by partial evaluation: a case study. In Proceedings of the Twelfth IFIP World Com\u00adputer Congress, \n1989. [HG91] Carsten Kehler Hoist and Carsten K. Gomard. Partial evaluation is fuller laziness. In Sym\u00adposium \non Partial Evaluation and Semantics-Based Program Manipulation, Yale University, New Haven, Connecticut. \nSIGPLAN Notices, vo/ume 26, 9, pages 223 233, ACM Press, June 1991. [HH91] Carsten Kehler Hoist and binding-time \nimprovement pages 83 100, 1991. John for Hughes. free. In Towards [JHH91], [HW90] Paul Hudak and Philip \nWadler. Report on the programming language Haskell. Technical Re\u00adport, Yale University and Glasgow University, \nApril 1990. [JHH91] Simon L. Peyton Jones, Graham Hutton, Carsten Kehler Hoist, editors. Functional \ngramming, Glasgow 1990. Workshops in puting, Springer-Verlag, August 1991. and Pro-Com\u00ad [Joh75] S. C. \nJohnson. YACC: Yet another compiler. Technical Report 32, Bell ries, Murray Hill, New Jersey, 1975. compiler \nlaborato\u00ad [Jor91a] Jesper Jdrgensen. Compiler Generation tial Evaluation. Master s thesis, DIKU, sity \nof Copenhagen, Denmark, student October 1991. Forthcoming. by Par-Univer\u00adreport, [Jor91b] Jesper Jorgensen. \nGenerating a pattern ing compiler by partial evaluation. In pages 177-195, 1991. match\u00ad[J HH91], [JSS85] \nNeil D. Jones, Peter Sestoft, and Harald S@n\u00addergaard. An experiment in partial evaluation: the generation \nof a compiler generator. In J.-P. Jouannaud, editor, Rewriting Techniques and Applicationsj Dijon, France. \nLecture Notes in Computer Science 202, pages 124-140, Springer-Verlag, 1985. [JSS89] Neil D. Jones, Peter \nSestoft, gaard. Mix: a self-applicable for experiments in compiler and Symbolic Computation, and Harald \nS@nder\u00adpartial evaluator generation. LISP 2(1):9 50, 1989. [MTH90] Robin Milner, The definition 1990. \nMads Tofte, of Standard and Robert ML. MIT Harper. Press, [RC86] Jonathan Rees and William Clinger. Revised \nreport3 on the algorithmic language scheme. Sig\u00adplan Notices, 21(12):37-79, December 1986. [Rom88] Sergei \nA. Romanenko. A compiler generator produced by a self-applicable specialiser can have a surprisingly \nnaturaJ and understandable structure. In Dines Bj@rner, Andrei P. Ershov, and Neil D. Jones, editors, \nPartial Evaluation and Mixed Computation, pages 445 463, North-Holland, 1988. [Sch86] David A. Schmidt. \nDenotational Semanticsj a Methodology for Language Development. Allyn and Bacon, Boston, 1986. [Ses85] \nPeter Sestoft. The structure of a self-applicable partiaJ evaluator. In Harald Ganzinger and Neil D. \nJones, editors, Programs as Data Ob\u00adjects, Copenhagen, Denmark. Lecture Notes in Computer Science 217, \npages 236 256, Springer-Verlag, October 1985. [Tur82] David Turner. Recursion equations as a pro\u00adgramming \nlanguage. In Darlington et al., editor, Functional Programming and Its Applications., pages 1-28, Cambridge \nUniversity Press, 1982. [Tur86] David Turner. An overview of Miranda. Sigp/an Notices, 21(12):158 166, \nDecember 1986. [Wad85] Philip Wadler. Introduction to Orwell. Technical Report, Programming Research \nGroup, Univer\u00adsity of Oxford, 1985. A Example of a complete target program This appendix contains a \ncomplete target program exactly as produced by the compiler. The source program was: facl=O fac n = n* \nfac(n-1) primes = sieve [2. .] where sieve (p:x) = p:sieve[nln<-x; n mod p-=O]  and here is the target \nprogram: (loadt (string-append **similix-library** scheme .adt )) (loadt thunk. adt ) (loadt bawl .adt \n) (define (target-O fenvi.O cenvi.1) (let ([cfst-2 (make-cfst 1)1) (vector (lambda (fn-3) (cond [(equal? \nfn-3 fat) (make-fun-c l-O-3 cfst-2)] [(equal? fn_3 primes) (save-at (conf-store-ref cfst-2 O) (lambda \n()  ((make-fun-cl-O-IO) (save (Lambda () (iterate+l 2)))) ))1 [else (fenvi-O fn-3)] ) ) (lambda (en-4) \n(cenvi-1 en-4)) ~ (fat) ) (primes) ())))  (define (make-fun-cl-O-IO) (lambda (v-O) (if (v:? (v-o)) \n(let* ( [d-2 (cdr (v-O) )1 [d-3 (car (v-O) )1 ) (inv: d-3 (save (lambda () ( (make-fun-cl-O-10) (save \n(lambda () (leg-O-15 (d-2) d-3)))))))) (error fd MO matching equations for function: s sieve)))) (clef \nine (leg-O-i5 v-O venv-1 ) (if (vnil? v-O) nil (let ([all-2 (car v-O)])  (if (not-equal? (modulo (all-2) \n(venv.1)) O) (inv: di-2 (save (lambda () (leg-O-15 ((cdr v-O)) venv-1) ))) (leg-O-15 ((cdr v-O)) venv-1))))) \n (define (make-fun-cl-O-3 SC-O) (lambda (v-l ) (if (struct-equal? (v-l) O) 1 (* (v-i)  ((make-fun-cl-O-3 \nSC-O) (save (lambda () (-(v-i) i)))))))) 266 B Syntax of BAWL Figure 4 shows the syntax of the language \nBAWL that the compiler is in a form that should be short and readable. Some parentheses may omitted in \ncase there is only one alternative, and in the last alternative definitions and alternatives can separated \nby newlines instead of ; s and of uhere s. Local definitions may not contain type definitions. There \nchar and bool. .9Cr def ::= ::= I I I clef{ ;def} fun eg pat = rhs tname {tvar} tname {tvar} ::= cdef \n== tezpr { I cdef } cdef ::= con {atezpr} texpr ::= atezpr I tname {atezpr} I tezpr -> tezpr atexpr \n::= tname I tvar I (tezprl,...,tezprn) [tezprl ,...,tezprnl ::= {pat} rhs rhs ::= alt{; alt} {where \nalt ::= = expT, if guard =expr, otherwise eq I guaTd ::= expT pat ::= lit vaT I (pat, : pat2 ) I [patl,...,patnl \n(con {pat} ) I (patl,...,patn) expT ::= lit uar ~ fun I con (ezprl Op [ezpr2]) I (OP [ezpr]) (ezprl \n: expT2) [ezpTl,...,ezpTnl I I (ezpTl,...,ezpTn) (expTl ezpr2) scr} [ezpr~ [,erprz] .. [expT2] 1 I I \n[ezpr I qual { ;gual}]  gual ::= expT I pat< -expT (script) (function (conformal able to handle. The \ndescription is deliberately kept be omitted e.g. in applications, the guard may be if True may be replaced \nby otherwise. In scripts, offset rules like those of Miranda hold for the scope are some predefine type \nnames like: num, string, definition) definition)  (type declaration) (type alias) (constructor definition) \n(type expression) (type construction) (function type) (type name) (variable) (tuples, n=O or n~2) (lists, \nn~O) (equations) (right hand side) (alternatives) (last alternative) (guard) (literal) );:y) (lists, \nn>O) (constructor pattern) (tuples, n=O or n~2) (literal) (variable) (fnnction or conformal) (constructor) \n(operator application or sections) (pair) (lists, n~O) (tuples, n=O or n>2) (application) (arithmetic \nsequences) (list comprehensions) (filter) (generator) OP ;;= +9\u00ad,*, l,dlv, mod,-,=,-= ,>, <,<=,>=,<>, \n-,&#38;, \\/,#, !,++,--,. Figure 4: Syntax of language 267 C Semantics of expressions Semantic algebra: \nDomain: Value = (Basic + List + Tuples + Constructs + Functional Basic = Integer + Float + Char + ... \nList = ... Tuples = ... Constructs = ... Function = Value ~ Value p E VEnv = Variable -Value (variable \nenvironment) 4 c FEnv = FunctionName -Value (function environment) y E CEnv = ConstructorName -+ Value \n(constructor environment) Valuation functions: E: Expression e VEnv ~ FEnv + CEnv -Value Euhtll ~~y = \nL~ZZt] E~var]pq$y = p[varl Eufun~p@y = q+[j%nn Euconj p~~ = y~C07Ln E~asn p~y = AS ~asl p~y, where as \nis an arithmetic sequences Eutc] p~y = LC~Zcl p~~, where lC is a list comprehension L: Literal -+ Value \n(omitted) AS: Expression -+ VEnv ~ FEnv --i CEnv -t Value (omitted) LC: Expression ~ VEnv ~ FEnv ~ CEnv \na Value (omitted) Figure 5: Semantics of expressions D Interpretation of expressions This appendix shows \nthe part of the interpreter that corresponds to the valuation function defined in Appendix C. (define \n(E (cond ( (ELit? ( (EVar? ( (EFun? ( (Econf? ( (ECon? ( (EEil? ((E:? ( (ETuple? ( (EApplY? ( (EAS? ( \n(ELC? (else expr venv fenv cenv) expr) (L (ELit->lit expr) )) expr) (my-force (venv (EVar->var expr) \n) ) ) expr) (fenv (EFun->fun expr) ) ) expr) (my-force (fenv (EFun->fun expr) ) ) ) expr) (cenv [EC6n-Mon \nexpr) ) ) expr) (inVMil) ) expr) (inV: (EL (E:->EI expr) venv fenv cenv) (EL (E:->E2 expr) (Tuple (ETuple->exp* \nexpr) venv fenv cenv) ) expr) ((E (EAPPIY->E1 expr) venv fenv cenv) (EL (EApplY->E2 expr) (AS expr venv \nfenv cenv) ) expr) (LC expr venv fenv cenv) ) (error E Syntax error in expression: s expr) )) ) expr) \nexpr) venv fenv cenv) )) venv f env cenv) ) ) (define (save (EL expr (lambda venv () (E fenv expr cenv) \nvenv fenv cenv)) ) ) (clef ine (my-force delayed-value) (delayed-value) )    \n\t\t\t", "proc_id": "143165", "abstract": "<p>Compiler generation is often emphasized as being the most important application of partial evaluation. But most of the larger practical applications have, to the best of our knowledge, been outside this field. Expecially, no one has generated compilers for languages other than small languages. This paper describes a large application of partial evaluation where a realistic compiler was generated for a strongly typed lazy functional language. The language, that was called BAWL, was modeled after the language in Bird and Wadler [BW88] and is a combinator language with pattern matching, guarded alternatives, local definitions and list comprehensions. The paper describes the most important techniques used, especially the binding time improvements needed in order to get small and efficient target programs. Finally, the performance of the compiler is compared with two compilers for similar languages: Miranda and LML.</p>", "authors": [{"name": "Jesper J&#248;rgensen", "author_profile_id": "81100019874", "affiliation": "", "person_id": "PP31083237", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143165.143220", "year": "1992", "article_id": "143220", "conference": "POPL", "title": "Generating a compiler for a lazy language by partial evaluation", "url": "http://dl.acm.org/citation.cfm?id=143220"}