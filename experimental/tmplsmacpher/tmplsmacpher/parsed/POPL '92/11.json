{"article_publication_date": "02-01-1992", "fulltext": "\n Compile-time Analysis of Parallel Programs that Share Memory Jyh-Herng Chow Williams Ludwell Harrison \nIII Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign Abstract \nTraditional optimization techniques for sequential pro\u00ad grams are not directly applicable to parallel \nprograms where concurrent activities may interfere with each other through shared variables. New compiler \ntechniques must be developed to accommodate features found in parallel languages. In this paper, we use \nabstract interpretation to obtain useful properties of programs, e.g., side effects, data dependence, \nobject lifetime and concurrent expressions, for a language that supports first-class functions, point\u00aders, \ndynamic allocations and explicit parallelism through cobegin, These anrdyses may facilitate many applica\u00adtions, \nsuch as program optimization, parallelization, re\u00adstructuring, memory management, and detecting access \nanomalies. Our semantics is based on a labeled transition system and is instrumented with procedure strings \nto record the proce\u00addural/concurrency movement along the program interpre\u00adtation. We develop analyses \nin both concrete domains and abstract domains, and prove the correctness and termina\u00adtion of the abstract \ninterpretation. Introduction Today most languages for parallel processing provide some kind of concurrent \nconstructs (or directives) for users to annotate explicit parallelism. These concurrent constructs greatly \ncomplicate the semantics of programs, especially if data sharing among concurrent components and nonde\u00adterministic \nresults are allowed. Consequently, traditional optimization techniques for sequential programs are no \nlonger directly applicable to parallel programs. Midkiff and Padua [MP90] present eleven examples to \nillustrate how traditional optimization or parallelization methods without modifications may fail to \ngenerate correct codes for a language with a cobegin coustruct. Figure 1 shows one of the examples. Note, \nthe explicit synchronization statements, wait and set, can be replaced by busy wait\u00ad ing and assignments \non the variable s, which in general *This work is supported by the Department of Energy under Grant No. \nDOE DE-FG02-85ER25001, with additional support from NASA Grant No. NCC 2-599. Permission to copy without \nfee sfl or part of tids material is granted provided that the copies are not made or distributed for \ndirect commercial advantage, the ACM copyright notice and the title of the publication and its date appear, \nand notice is given that copying is by permission of tire Association for Computing Machinery. To copy \nother\u00adwise , or to republish, requires a fee andjor specific permission. will be hard to recognize. By \nanalyzing each thread in iso\u00adlation, the compiler might come up with the wrong code at the right side, \nsince no data dependence exists in each thread. In order to apply safe optimizations, possible in\u00adteractions \namong concurrent threads must be considered altogether. Yet, to date, little research has been done on \nthe problem, and most compilers simply avoid the risks by inhibiting data sharing between concurrent \nthreads (which restricts the set of algorithms that can be programmed), or not optimizing the parallel \ncodes at all (which ignores any possible improvement on programs from additional con\u00ad currency ot traditional \noptimizations) [M P90]. Most current static analysis of parallel programs at\u00adtempts to detect access \nanorna2ies (data races) [McD89] [MH89], or verify program properties, like termination and deadlock freedom \n[Rat90]. Cytron [Cyt86] advocates that new compiler techniques for program optimization must be developed \nto accommodate features found in parallel lan\u00adguages. However, no methods are proposed. Shasha and Snir \n[SS88] investigate how to insert minimum number of delays to enforce sequential consistency in executing \npar\u00adallel programs, and re-caat it to the problem of code mo\u00adtion in compilers. They address only the \ncase of straight line code. The idea is further extended by Midkiff et al. [MPC90, MP90] to deal with \nloops and array accesses. Data flow analysis for message passing systems appears in [RS90], but they \nassume no interference between processes by shared variables. This work attempts to provide a general \nframework to the problem. In this paper, we present methods for obtain\u00ading various properties about parallel \nprograms by using ab\u00adst ract interpret ations [C C77]. The language we use, which is untyped, supports \nfirst-class functions, pointers, d y\u00adnamic allocations and explicit parallelism through cobe\u00adgin. The \nproperties we are interested in are: side eflects and data dependen ces which are the key to program \nopti\u00admization and restructuring, object lifetimes which can as\u00adsist memory allocation or deallocation \nproblems, and con\u00ad cw-r-ent expressions which allow us to detect access anoma\u00ad lies and to decide when \nunfair (nonpreemptive) scheduling is permissible. This work, to our knowledge, is the first to apply \nabstract interpretations to share-memory parallel programs for obtaining such information. Abstract interpretation \nhas been widely used for obtain\u00ading properties, such as strictness analysis [BHA86, Nie87], sharing and \nlifetime analysis [Hud87, Deu90, Har89], com\u00adplexity analysis [Ros89], supporting sets [NPD87], and data \ndependence [HPR89, Har89]. However, they all work on sequential languages. Recently [Mer91] uses it to \nana\u00adlyze communication patterns in CSP-like programs where @ 1992 ACM 089791-453-8/92/0001/0130 $1.50 \n cobegin cobegin cobegin TI.: . . . T1 : Sl: a=b S1: a ; b // S2: wait (s) S2, wait(s) S3: C=d * // // \nS3: cad T2: d=e coend S4: set(s) // coend T2: d=e S4: set(s) coend  Figure 1: An Incorrect concurrentization \nfrom [MP90] he assumes no nested concurrency, no shared memory and no dynamic process creation. There \nare advantages to using abstract interpretation. First, due to the complexity of analyzing parallel pro\u00adgrams, \nit is necessary to combine related states to reduce the cost (e.g. [Tay83] or virtual states in [McIX9]). \nThis combining mechanism is in fact a form of abstraction (e.g., the concept of clan in [McD89]). By \nabstract interpreta\u00adtion, there are many alternatives available for designing abstract semantics and \neach of them defines a different combining mechanism. These choices are not easily visi\u00adble and obtainable \nfrom traditional perspectives. Second, becauee the abstract interpreter does in some sense ex\u00adecute the \nprogram, procedure boundaries are erased, so that intraprocedural and interprocedural analysis can be \ntreated in the same way. Third, any optimization should preserve the semantics of source programs. Since \nabstract interpretation is based on a formal mathematic/semantic model, the correctness of analysis, \ni.e. its consistency with the language semantics, can be proved formally and easily if we follow some \nexisting frameworks (e.g. [Bur87]). Thie paper is organized as follows. Section 2 describes the language. \nSection 3 describes a device called procedure strings that capture the procedural/concurrency move\u00adments \nalong a program interpretation. Section 4 describes the instrumented semantics based on a labeled transition \nsystem. We shall call it the concrete semantics. Interleav\u00ading semantics is used for the cobegin construct. \nSection 5 provides the analysis of side effects, data dependence, object lifetime and concurrent expressions \nunder this con\u00adcrete semantics. Section 6 gives the abstract semantics. We prove the correctness and \ntermination of the abstract interpreter. In section 7, the analyses are re-cast into the abstract domain. \nSection 8 concludes the paper. 2 The Language The syntax of the language is shown in Figure 2. The language \nis based on MIL, an intermediate language used in the Miprac multilingual (Fortran, Scheme, C, etc. ) \npar\u00adallelizing compiler [Har91, HA89], with the addition of cobegin construct. Here for simplicity, we \nomit data structures and assume lambda s have only 1 argument. 1 1&#38;.~tr=y data structures and procedures \nof unrestricted type are treated in Miprac. See [Har91]. ( G Exp ::= (begin ( <v ) I (cobeginv ( (v \n) I (if <vi (V2 ~ 3)v I (read (VI )V I (write &#38;v (V )v I (create Id )v I (call .fU1 &#38; )V I Idv \nI cons+ I (lambda. Id&#38;l ) I (+ W C )v Figure 2: The language There are three kinds of values in \nthis language: in\u00adtegers, locations and closures. The expression at the position ( 1 specifies the location \nto be read/written in read/write, or the closure to be called in call. cre\u00adate creates a new location. \n(write f f ) returns the value of .$ 2. The expression ld returns the location of the variable Id, i.e., \nLvalne. Loops are formed by tail\u00adrecursive functions. The identifiers in lambda and cre\u00adate are assumed \ndistinct (simple renaming will do). For simplicity, we do not have explicit synchronization con\u00adstructs; \nhowever, they can be done through shared vari\u00adables. Nondeterministic results are poesible; e.g., (2,1), \n(2,2) and (1,1) are possible values for (z, g) in the program (X4 ; y=x) II X=2 (cobegin (begin (write \nz 1) (write u (read z))) (write c 2)). To employ a transition system, expressions are labeled (v G Elab). \nLabels are denoted by superscripts. Succ(v) ~ves the expression label to be executed following the ex\u00adecution \nof ~ in a thread. The labeling also defines the atomicitv of an evaluation, which is very important for \nthe interlea~lng eemantics. The labeling mechanism results in fine atomicit y; for example, the labeling \nfor y := x, or (write yl (read Z2)3)4 indicates that it consists of 4 atomic evaluations! Although finer \natomicity results in larger number of interleavings, in practice, most of the interleavings are redundant \n(produce the same etate) and can be easily eliminated, e.g., by uirtwal coarsening [Va189]. We will assume \nthat auxiliary expressions endif, return and endthread are added by the compiler to denote the end of \nan if, the return of a function, and the end of a thread; that is (if &#38;l ~, <3)+ (if cl (endif .$2) \n(endif &#38;)) (lambda Id ~1) ~ (lambda Zd (return ~1)) (cobegin Cl g,) ~ (cobegin (endthread -$1) (endthread \n~,)) 3 Procedure Strings Each lambda or cobegin branch is uniquely named by its expression label (a E \nA or q c I ; A, r C 131ab). Procedure strings are composed by these two kinds of labels to de\u00ad note procedural \nand concurrency movements along a pro\u00ad gram execution. This device for capturing runtime prop erties \nis first introduced in [Har89] for analyzing sequential Scheme programs (only procedurzd movements exist \n-thus the name procedure string). .5 t Figure 3: An example of procedure stings Definition 1 (Procedure \nString) A procedure string is either: (1) e: the empty string, (2) the term ad: denoting a call to procedure \ncr (downward movement), (3) the term au: a return of procedure m (upward movement), (~) the term qd: \nentering a cobegin branch, (5) the term q : exiting from a cobegin branch, (6) (pi lpz ): the procedure \nstring resulting from the execution of a cobegin where pi is the procedure string for branch i, or (7 \n) pl pz: the concatenation of procedure strings PI and PZ (e is often omitted).z Figure 3 illustrates \nhow procedure strings are con\u00adstructed. The cobegin has two branches labeled by ql and q2. At the beginning, \nthe procedure string is e; it becomes Of when entering the first branch, becomes @f d during the call \nto ~, and becomes q~~dj after the call; the procedure strings at branch qz are constructed similarly, \nFinally, the two are combined after the cobegin. Procedure strings will also be used to differentiate \nin\u00adst antes of an identifier or inst antes of function applica\u00adtions. For example, each application of \na lambda closure creates a new location for its argument, which can be dis\u00adtinguished by the procedure \nstring at that moment. The procedure string pb when a location L is created is called the birthdate of \nthe location (or L is born at pb). Let pl and p2 be two procedure strings. Define PI < pZ if pl is a \npre-execwh on of pz, or formally speaking, PI and PZ are comparable if (P1 ~ pz ) v (P2 ~ Pi). Net(p) \nreturns the net movements of p; i.e., it deletes every crdw and (cIc) from p, repeatedly; e.g., ~et(ad(q~q~lq~qj)~d~u) \n= CYd. p is called balanced if Net(p) = e. -1 p deletes the longest balanced suffix of p; e.g., + (CYd~d~dy \n/? adCY ) = ad. p2 pl returns the difference of pz and pI (defined only when pl s p2); if p2 = (rl lr2) \nand pl < ri for some i, it returns ri -p,. For exam\u00adple, CYd(/3d~Ul~d~U)cr crdyd = ~ a . Common(pl ,p2) \nreturns the longest procedure string that is comparable to pl and pz. Examples are: 2We define (PI IP2) \n(P21P1). 4 Concrete Semantics The concrete semantics is an operational semantics instru\u00admented with \nprocedure strings. The semantics will be baaed on a labeled transition system. Definition 2 (Labeled \nTransition System) A labeled transition system is a tuple (II, X, T, IIJo), where (~ c)* is the set of \nconfigurations, (a c)X is the set of events, T : 1 x X + W is the transition function, and Jo~ J is the \nset of initial configurations. We define a configuration to consist of a set of processes and a store, \ni.e., Conf = E (Process) X Store. We use *I 802 to denote a transition. An event u is enabled in $1, \nif 342, IJI 2 ~z. Two events al and az (al # a2) arc concurrent, denoted by al IIaz, if ~qh~z~,, @1~@2A$I~yj~. \nA process contains the information: process id (Pid), status (Stat), program counter (Pc), procedure \nstring (Ps), temporary locations (TmpLoc), environment (Env), restoration function (Res), and parent \nid. All the processes in a configuration share the same store. The number of processes in a configuration \nmay increase due to the exe\u00ad cution of a cobegin expression. A distinguished element # c Pc denotes the \ntermination of a process. The set of events, X, can be specified by Pid x Pc; tbus, an event (q, v) is \nenabled in @ if q has a running state in @ and u # #. A configuration wit bout any enabled event is ter\u00ad \nminated if every process in it is terminated. Our operational semantics is baaed on this transition \nsystem. Basically, given a configuration ~, each enabled event a; in @ will result in a new configuration \n$:. Our semantic function T : Conf ~ lP(Conf) takes @ and re\u00ad turns the set (J {~j} of configurations. \nThe configurations W. that result from executing a program in J/o is the least fixpoint of E, where S \n= AW. let W = U{7@l@ ~ m} if I@= W then J else &#38;q that satisfies q. = E*o = &#38;iPn, Figure 4 shows \nthe semantic domains. The irreflexivity of the domains not only simplifies the termination problem but \nalso allows simple and efficient implementations of the abstract interpretation. We mentioned before \nthat instances of a variable named x can be differentiated by procedure strings (their birth\u00ad rates). \nThis property is used to define the domain Lot, Store, TmpLoc and Env. For example, a location is rep \nresented by (z, pb), where z is an identifier in create or a lambda argument and Oh is the birthdate \nof the loca\u00ad tion, The domain TmpLoc is used to record the values of subexpressions so that, taking (write \n~ ~ &#38;2) for exam\u00ad ple, the values of operands are available when the write is being evaluated after \na sequence of transitions from evalu\u00ad at ing ~ ~ and 62. A process id is simply the procedure string \nwhen the process is crested. For example, in Figure 3, the ids of the two new processes due to the cobegin \nare q: and q;. Stat is the status of a process, where a value O means the process is in running state \nand a value i > 0 means the process is waiting for i processes, which occurs when it waits for the completion \nof cobegin branches. v G Pc Figure 4: Semantic Domains Pos8C0nj = lP(C onf) Conj = Processes X Store \nProcesses d IP(PTOcess) Process =Pidx Statx Pcx Psx TmpLoc x Env x Res x Pid stoTe =Idx Ps-+Val P;d = \nPs Stat = <0,1,2}L Pc = Ehzb U {#} TmpLoc = Elab x Ps ~ Val Env =Id-+Ps Res =Ps~Pcx Env Val = Loc +-Closure \n-I-Int Loc =Idx Ps Closuve =Ax Env Pa procedure string A procedure label (c Elab) r cobegin branch label \n(C Elab) Id identifier Int integer  may point to an expression label, or # (termination). The restoration \nfunction Res is used to retrieve the program counter and environment of the caller after a function call. \nA closure is denoted by a lambda label and an environ\u00adment. The concrete semantics is shown in the Appendix \nA. We think of PossConf and Processes as Hoare powerdo\u00admains (that is, aa containing only downward-closed \nsub\u00adsets), even t bough to do so adds configurations to a pro\u00adgram s meaning, that cannot occur in reality. \nFor exam\u00adple, non-termination will appear to be a possible outcome of every program. This does not matter \nfor our purposes, because the abstract domains we use represent downward closed subsets of their concrete \ncounterparts, in any event. Concrete Analysis This section describes the analyses in the concrete domain, \nwhich mostly extend the analyses in [Har89] to the case of parallel programs. Throughout the paper, objects \nwill refer to location objects , and we say ~ makes a reference to an object iff the evaluation of ~ \nreads or writes the object. We shall develop the analyses considering a single execu\u00ad tion path. Let \n(V,, WI, Qz , #, ) be the sequence of possible configurations resulting from the evaluation of &#38;!i?o. \nAn execution path, E, is a transition sequence of configura\u00ad tions $, ~) VI q=) +Z !!) . . .. where \n@i 6 ~i and (~i, ~i), q~ G Pid A v; E Pc, are events; intuitively, it corresponds to one particular interleaving \namong all pos\u00adsibilities, if there are multiple enabled events in any of the configurations. Lemma 1 \nIn an execution path, suppose pr is the proce. dure stving when ati object L with birthdate p~ &#38; \nrefer\u00adenced. Let 6 = common(p., pb). Then, either (1) pb = 8, or (2) pr and pb are incomparable, occurring \nat different branches of a cobegin, and $ is the procedure string when the cobegin is encountered. bc \n4 4 d ~ e{I o~ 4 A denotes a procedural a location born here movement Figure 5: An example of side effects \n Proofi If pb and p, appear at two different branches of a cobegin, say qh and q,, respectively, and \n~ is the procedure string when the cobegin is encountered. Then pb = $Tl<wb and pT = ~q~wr. We have common(pv, \npb) = ~ = 8. Otherwise, pb and p, are comparable, and pb = g. 5.1 Side Effect Side effect analysis is \nto determine all visible reads and writes that occur during a procedure call, i.e., those to locations \nknown outside the call. Reads are deliberately treated as side effects, because later this side effect \nanalysis is used to determine data dependence. Definition 3 (Side Effect) A frmction f has a side ef\u00adfect \non an object L if f makes a reference to L and L is not created by the evaluation off. 3 Example 1 Consider \nFigure 5 where points a, b and c are cobegins, and an object L is created at point $. Some function calls/returns \nare denoted by small triangles. Our analysis should be able to tell us which functions result in side \neffects at each reference to L. For example, a reference at point d (which occurs during the call of \nfunction ,&#38; and al) should tell us ~1 and ~1 have side effects (because they access L which is not \ncreated by their evaluation), and a reference at point e should tell us CY2has a side effect, but not \n~2 because the reference is internal to P2. Theorem 1 In an execution path, suppose p, is the proce\u00addure \nstring when an object L with birthdatepb is referenced. Let 8 = common(p,, pb). Then, Net(pp t?) contains \na term CYd iff the instance &#38; of a function a corresponding to this term has a side effect on L. \nProofi we first consider the case 8 # pb, i.e., pb and P. occur at different branches of a cobegin, \n~: Suppose &#38; has a side effect on L. &#38; is active at the time of reference; thus Net(pr) contains \nthat ad 3In [Har@ it is proved that this definition is the minimal one, if it is to be the basis of \na correct dependence graph. term. By Lemma 1, 8 is the procedure string when the cobegin is encountered. \nIf Net(t?) contains that ad term, which means the &#38; is active during the whole evaluation of the \ncobegin and L is created by its evaluation, then by Definition 3, d will not contribute a side effect \non L. Therefore, Net(@) can not contain that ad term. So IVet(p, -0) contains the term ad. +-: .-Nei(m.- \n0) contsins the term ad. Suppose The instance &#38; is called at the thread where P. resides. Since 0 \n# pb, the object L is created by other con\u00adcurrent thread; in particular, L is not created by the evaluation \nof &#38;. Thus by Definition 3, &#38; has a side effect on L. The proof for the case @= pb is similar. \nl Example 2 Consider Figure 5 again. The procedure strings at points a, c, d, e, f are pa = 7d, pc = \n7drI~@, pd = YdV?A%$CY?,Pe = ~dvif%iff;,pf = Ydv;E%f. At the reference point d, Net(pd -cornmon(p~, p~)) \n= Net(pd Pa) = q?/3fv$cYf, which says functions PI and al have a side effect, but not y. Similarly, the \nreference at point e only causes a side effect of the function crz, since dd~ Net(pe -COWWTIOT+Ie,pf)) \n= Net(pe -p.) = 7140 2.  5.2 Object Lifetime The purpose of the object lifetime analysis is to determine \nif an object outlives the function (and the enclosing func\u00adtions) that creates it. If function ~ creates \nan object L and L is referenced after function ,8 is deactivated, then L should not be allocated in the \nstack frame of /3. In the case of a hierarchical memory, if we know an object L will be referenced by \nanother concurrent thread, then it should be allocated in the memory accessible to both threads. Suppose \nan object L with blrthdate ~b is referenced at Pr, and suppose process Po invokes F I, PI invokes Pz, \n.... and pk creates ~. (Note, PC-I is inactive, waiting for the completion of PI, 1 < z < k.) Let ~: \nbe the procedure string of P: at this moment. Then, p: l < p:, and pb = pk. N OW if a reference to L \noccurs at p,, processes Pj+I, .... pk may have terminated and Pi resumes execution. Let p: be the procedure \nstring of Pi at this time. Then, pi = p;, O < i < j l, andpj < p;, j+l < i < k. Define c~rr(~f.,pb) to \nbe this p;. Note, pb= pk < pi < P~. Theorem 2 Let ,# be the procedure that creates L with birthdate pb. \nIn an execution path, suppose pr is the pro. cedure string when L is referenced. Let q$ = corr(p,, pb). \nThen, Net(q$ -pb) contains a term /3 ifl the instance of /3 corresponding to this term is deactivated \nbefore L is ref\u00aderenced. Sketch of proofi The access at pr can be thought of aa an access at #, for we \nare only interested in when a reference is made. Since pb < $$, Net(~ pb) is well defined. Intuitively, \nthe deactivation of ~ appends the term ,!3 to the portion of procedure string since L is created. This \ntheorem tells us whether objects created by create expressions can be allocated on the stack. More precisely, \nbecause we know the extent of objects, the information allows us to associate each function exit with \na deallocation list of objects, as proposed in [Har89]. cobegin cobegin .91: (write (create bl) 1) S2: \n(write y M) S3: (read (read y)) // 54: s5: [::;:: :C;;; e ) )d: i% ~s s? s6: (read (read X)) s7: (read \n(read y)) coend w I // $8: (read (read y)) coend Figure 6: Object allocation in hierarchical memories \nExample 3 In a sequential program, com (p., pb) s p,. Suppose function a calls /3 which calls ~, and \ninside ~ an object L is created; then return from ~, return from /3, and now L is referenced. Namely, \n...cl Pd ~d ~ v W PU ~e f(L). We have pb=ffd~d~d, p,=ad~dyd~ ~ , and Net(pr pb) = 7UPU. Thus, L can \nnot be allocated in the stack frame of 7 or ~. However, we can free the space for L when a is returned. \nIn a system with hierarchical memories, suppose each cobegin thread is executed in a different processor. \nSince accessing global memory is more expeusive than accessing local memory, we would like to allocate \nobjects as locally as possible. The following shows how a compiler can auto\u00admatically determine where \nto allocate memory for objects shared by multiple processors. Observation 1 If O = common(p~, pb) # pb, \nthen the ob\u00adject should be allocated at the level of (or above) the mem\u00ador~ that is accessible to the \nthreads corresponding to 6 . Example 4 Consider Figure 6. Assume x and y are bound before this program \nfragment (not quite written in our language). Object bl is created at S1 and its location is assigned \nto y at s2. Object b2 is created at S4 and its location is assigned to x at s5. By considering each ref\u00aderence \nto the objects, the read at S3 might suggest that bl be allocated locally. However, the access at S7 \nsuggests it be allocated at a higher level memory, and the possible access at S8 forces the object to \nbe allocated even higher. But we are sure that b2 can be allocated locally.  5.3 Data Dependence This \nsection describes how to find data dependence by the information obtained from side effect analysis. \nIn par\u00adticular, we want to know the object read/written in a read/ write or the objects accessed during \na call, The no\u00adtation cl N .fZ means the evaluation of .$l is not part of the evaluation of (Z (e.g., \n.$l w (z is not true if (l is a subexpression of (z), and vice versa. Definition 4 (Data Dependence) \nLet tfti M <v. There is a data dependence between <U and <., denoted by 6. ~ <U, iff [u and <u both make \na reference to an object L. 4Again, for a particular execution path or interleaving. By the side effect \nanalysis, whenever a read or write is encountered during the interpretation of a program, we look into \nthe procedure string component p~ of that loca\u00adtion. Theorem 1 says if there is a downward movement of \nprocedure a in Net(pr -cornnzon(p,, p~)), then @ has a side effect on this location. In this way, we \ncan determine a set of read/write side effects made in each procedure call, and by intersecting these \nsets, the data dependence relation between expressions can be found. However, consider fU 2 ~v where \nL is born in ,$U. Ac_ cording to side effect analysis, L does not belong to the side effect sets of ~U, \ntherefore side effect analysis alone does not reveal this data dependence. In fact, the analysis alone \ncan discover only those data dependence ~ti A tjo, where L is created neither in &#38; ti nor in <0. \nIt is the next theorem that allows us to establish this data dependence, and give us confidence to rely \non Theorem 1 for building a correct dependence graph. Lemma 2 Let $U w <.. Suppose that (u creates L \nand L occurs as a value during the evaluation of .$V. Then, .$U writes L to some variable x, <v reads \nsome variable y to get L, and either (1) x = y and (U ~ <v or (.% ) there is a chain (u % ... ~t. - \nProofi omitted. See Theorem 6 of [Har89] for proof of a nearly identical result. Lemma 3 In the above \nLemma, (u $ (v implies {u ~ <v, andtux~ ... ~ (V implies (u 3 .fW1 Z ... Z (v, fOr some objects .q, .... \nz~, where z~ is not created in &#38;Wi_l or (U,. (let (~ = ( 0 and c$V= (Wr) Remark: Note that xi may \nnot be in the intersection of the side effects of <U and ~V, but zi is in the intersection of side effects \nof fWi-l and fWj. Proofi Let F denote a read whose return value is L, and that satisfies S,$l .$2, fl \nM .f2, such that (2 creates L and (c) F occurs in the evaluation of &#38;l, The proof is by induction \non i, the number of reads satisfying condition (C), up to and including l?, from the beginning of execution. \nBy Lemma 2, we must have: ~U writes L to x (call this event W), and ~V reads y (CSU this event R), for \nsome z and y. (base case:) Consider i = 1. We must have z = y, otherwise F will not be the first reference \nsatisfying (C). Now, if z is created by either (m or .fV, then there must exist an earlier reference \nsatisfying (C) (that returns z), a contradiction. Therefore, a is created elsewhere, and we have .$U \nL .$2 Assume the lemma holds when i < d. Now consider the dth reference F satisfying (C). if the case \n(1) of Lemma 2 is true. Depending on where z is created: x is born in ~U. Then the read that returns \nthe value z in the event R (in ~V) is an earlier ref\u00aderence satisfying (C). By induction, the lemma holds. \n x is born in fV. Then the write of z in the event W is an earlier reference satisfying (C). The lemma \nholds since by induction it holds for (.:<..  z is born in some other (W, Then <U ~ (v,  if the case \n(2) of Lemma 2 is true: by the previous argument, each ~Wi_~ ~ .$Wi can be replaced by .$~o~ b2 ( *.. \n. ~ <q,, where bi is not created in ~,i-, or t:. u Proof: If L is created in ~U or ~V, the theorem follows \nfrom Lemma 3, Since each object z~ is not created in ~Wi_, or ~Wi, both fWi-, and <W, have a side effect \non it, thus the data de\u00adpendence can be established. Example 5 Consider the following expressions: (1 \n: (write z (write z (create Y))) ~, : (Write (Write (read z) (create L)) 1) t, : (read (read (read z))) \nNamely, ~1 creates Y, and writes it to x and z; (z creates L, reads z (gets Y), writes L to Y, and assigns \n1 to L; &#38; reads z, Y, and L. Consider (begin ~1 &#38;2 &#38;). We have G A 63 * G ~ (3, where Y is \nnot created in ez or g,. Now consider (cobegin &#38;2 f,), where ~, = (begin cl ~3), with an execution \npath of the evaluation order fl -+ (2 + C3. Then we have i.f2 ~ (4, <4 ~ (2 and &#38; ~ &#38;2, where \nonly the last one is revealed by the side effect analysis. This example also explains why the dependence \n-is not directed in Definition 4.5 5.4 Concurrent Expressions and Unordered Accesses It is desirable \nto identify which expressions are possibly concurrent and may result in unordered accesses to an ob\u00adject. \nThe information of concurrent expressions is simply a by-product of our transition system: expressions \nthat are possibly concurrent are explicitly displayed in each con\u00adfiguration, namely, the program counters \nof the enabled events. Definition 5 (Concurrent Expressions) VI and W2 are concurrent, denoted by VII \nIV2, ifl there exists a con\u00adfiguration $ in an execution path, such that 3rl 71Z E 411, (TI # m) and \n(IJl = 7r142) A (WZ = 7r~2) A (O c mjl)A(O c m.Jl). Therefore, if two concurrent expressions are accessing \n the same location and at least one is a write access, then the result may be nondeterminate and we can \nsignal a possible access anomaly if necessary. 5Note also that undirected dependence are natural in the \nset ting of concurrency; see conflict edges in [SS88]. POssCOnf = Pcs + Con.f Conj = Processes X Store \nPTOcesses Process StOTe Fz %z PC. K TmpLoc Env Res =A~~xEnv i% =( A+r)+2A w = Loc x Closure x % Loc =Exfi \n Figure 7: Abstract Semantic Domains 6 Abstract Semantics Abstractions to the concrete domains are necessary \nto make the interpretation computable. We will use the notation F to denote abstract entities. A pair \nof maps (AbsD, ConcD), AbsD : D-m (abstraction map) and Conc~ : ~~lP(D) (concretization map), is defined \nfor each concrete domain D, in order to create the connection between concrete semantics and abstract \nsemantics, where the concretization map ConcE is defined by (1) namely, a downward closed, upward complete \nsubset of D. For an operation f : D1 ~D2 + . . . *Dk, a safe abstraction f : D1-+~2-. . . b~k needs \nto satisfy fd] ...dkcconc~k(~~l...~k) (2) where J, = Abs Didl. The abstract semantic domains are shown \nin Figure 7 and explained in the following. 6.1 Abstract Procedure Strings First we want to abstract \nthe domain of procedure strings, so that it becomes finite. The abstraction map in [Har89] is adopted: \nPs=(A+ r)+ 2A where A = {e, d, old+, U, UU+, u+d+}: c for no net move\u00adment, d(u) for single downward \n(upward) movement, dd+ (uu+ ) for multiple downward (upward) movement, and u+ d+ for one or more upward \nmovements followed by one or more downward movements. Given a label, the abstract procedure string returns \nthe net movement of the label, summarized by A; e.g., if p = ctd8d7d7u~d, ~= {a + {d},~+ {ddt},y+ {6}}. \n There are two levels of information loss in this abstrac\u00adtion. First, the relative sequencing of movements \nis erased, e.g., we can not tell if a movement of a function happens before or after a movement of another \nfunction. Second, the net movement is coarsely described by the six values, so we do not know exactly, \nfor example, how many down\u00adward movements there are in dd+. 6.2 Abstract Configurations We would like \nto abstract configurations so that for ill e PossConf, ~ E PossConf becomes a single element, in\u00adstead \nof a set of elements, so that powerdomain const ruc\u00adtions can be avoided. The approach we will take is \nto combine all configura\u00adtions that have the same set of program counters into a single abstract configuration, \nThat is, Observation 2 The effect of this abstraction is that the number of configurations in evaluating \n(cobegin (l ~z) is bounded by (ml + 1) x (mz + 1), where m, is the ntim\u00adber of expressions in <i, and \nthe number of the result\u00adconfigurations after executing a cobegin is always 1. That is, the only result-configuration \nwill summarize all the possible results of evaluating the cobegin, and serves as a single initial configuration \nfor the rest of computation. The combining mechanism suggested by this abstraction greatly reduces the \ncomplexity of the configuration-space.  6.3 Abstract Processes Again we want to abstract the domain \nof sets of processes, lP(Process), to avoid powerdomain constructions. Since each v e Pid is distinct, \nthe domain of Processes is equiv\u00adalent to Pid + Stat xPc xPs xTmpLoc xEnv xRes xPid. (3) The abstraction \nmap we will take is to combine all the processes created by the same cobegin branch into a single abstract \nprocess. Namely, where given a procedure string p, Thread(p) is the laat branch label in p. Observation \n3 The abstraction loses no information (from Processes to Processes) if everu cobegin is ex\u00adecuted at \nmost once. If a cobegin is executed more than once, an abstract process wilt denote all those (concrete) \nprocesses that are originated fvom the same branch. Since processes originated from the same cobegin \nbranch execute the same program fragment, a transition made by the abstract process takes these (concrete) \npro\u00adcesses to their next states altogether. 6.4 Other Abstract Domains Sketch of proofi Let @, F) = ~V~, \nand (II, a) = $. There are several ways to abstract domains consisting of the domain of procedure strings. \nAs we see before, the Pid component in Processes is in fact abstracted into I ; i.e., q G I represents \nall processes originating from the cobegin branch q. The Ps component in Res is abstracted into A, and \nthe component is completely omitted in TrnpLoc in which the values of all instances of an expression \nwill be joined. A similar idea is applied to the domains ~ and Store [Har89]. The basic domains Stat, \nPc, A, Id and Int are flat so they are abstracted into sets with subset-inclusion as par\u00ad tial orderings \nwithout worrying about complex powerdo\u00ad mains. The abstraction of domains Val, Loc and Closure are straight \nforward.  6.5 Termination The abstract semantics is shown in Appendix B. [t is nec\u00adessary to prove the \nabstract interpretation always termi\u00adnates. The subscript in ConcE, AbsD, ~, etc, will be omitted, which \ncan be inferred from the argument. Theorem 4 ~ is monotonic. Sketch of proofi Namely, to prove 1. TCl@2 \n* 7T1V z TW2V, w e Pcs Suppose TI ~~z. Subscripts are used to distinguish the two instances. _ For an~ \ngi~n V, (ml, 51 ) = ~lV ~ (fiz,FZ) = UJZV, so IIIGIIz and F1~77z. We show for any given q c I that @l \nq~@2q (0 is defined inside the semantics) for each case of f , Here we take begin as an example,, Let \n(3i,Di,Fi,~i,~i,~i>Toi ) = ~i(q), i = 1,2. Since ~1 (q)~~z(q), we have % ~%, VI LV2, PI Q-2, ~1~72, ?l~~z, \nF1~F2, and ~Ol~V02, If O @ 31, then @1~c@2~; otherwise we need to compare @iv for each v ~ i71 (@Iv~@zv \nis clearly true for v ~ i7z Z,). By comparing each chang~, ~~ have V; GP4, 7;@ (since iI(JZ)G72(VZ))i \n111G112, and V; = V;. So, ~1 [(~~, ~)//V~]~Il!Z [(~~, F)//VJ], and therefore, @lvg&#38;v. 2. Vlgvz * \nTTvl ~ TTV2, VT E PosscorJf Clear, since domain Pcs is flat. Theorem 5 ~ terminates, V ~ E PossConf. \nProof: ~ is the only recursively defined function in the abstract semantics, ~ is monotonic since ~ is \nmonotonic, and all ascending chains in %sCon.f have finite length (in fact all the ~bstract_semantic \ndomains are finite),-so t~ terminates, for any V e ~Z. O 6.6 Correctness We need to prove our abstract \nsemantics is a safe approx\u00ad imation to the concrete semantics. By assumption, II ~ Conc(II) and u c Cone(F). \nFor any enabled process K c II, we prove that the configuration resulting from one transition made by \nthis process is an element in Conc(~V4 ). Here we show for the case of begin form. Let (q,s, v,p, t, \ne, T, %) = m and (3, V, F, ?,~, F, ljo) = ~(~ ), where n = Thread(q). Since II E Conc(II), we have s \n6 Cone(F), v g Conc(ti),p G Conc(~), t ~ Conc(~, e G Cone(?), T e Cone(F) and q. ~ CO~C(To). The configuration \nresulting from this transition is W = (II T + {(q,s, Sticc(v), P,t , e, T, no)}, a) in T, andis v = \nT[(E[(3, i7 , F, ?, F, F, To)/rl], =)//v ] in ~. By (l), it is sufficient to show that AIM(W ), or equivalently \n(dne to (3)) Abs(lI[(s, Sticc(v), p, t , e, r, qO)/v]) (4) is less than n[(3, ti, p, ?, z, F, ljo)/?? \n]. (5) The second and fourth components are the only changes in (4)_and (5). Let ml=(4). We show that \n(~1 q )J.2 = V and (II1q )J4 L?. Let {q,ql,.,.,q~ } = COnc(q ), and {v, vi,,,. ,v~} and {t,tl,...,tk} \nare the corresponding components in If. After the transition, we have (~1 q )J2 = {Succ(v), vi,..., vk}=z. \nBy the definition of T and ~, t = t[t(vz, + p)/v, + p] and ? = 7P(V2 )//v]. Since t ~ Conc(~ and t(vz,+ \np) E Conc(z(m)), we have t c Cone(?) or equivalently Abs({)@. We kno~ ~ = Abs(t) U Abs(tl) U o.. U Abu(tb) \nand t~t ;thus, (III v )J4 = Abs(t ) U Abs(tl) u .,, U Abs(t~)@ Uzl-1 . . . U~&#38;. 0 Theorem 7 Let WO,IJl,.. \n. be the sequence of possible configurations resulting from the evaluation of &#38;WO. Then, 10 E Conc~O \n* Vi, *i G Conc(ZllO). Proofi by induction on i. By Theorem 5, suppose ~0 terminates after n iterations. \nWe have mo = ~(wouwl) = S(UJOUTIUQ2) =.. . = t(wouwlu. . .Uv.) =TOUTIU. .. UT. (base case:) w~en i \n= O, !410G Conc(&#38;Vo) is obvious, since 10 E ConcIJo. Assume the theorem holds for Vi. Then, Ui E \nConc(Z%O ) = Conc(Zn). NOW for any @j c Wi, @j c Conc(~nVj), where ~ = U{7rJ3 I r G H, II = @Jl}. By \nTheorem 6, T@j G Conc(TVn~ ), or equivalently, Abs(?_@j)Z7Q.V~, for a31 j. We have Therefore, Wi+l = \n(J{T@j } 6 Conc(~n). 7 Abstract Analysk In this section, we rephrase the analyses in the abstract world, \nRecall the operations  and common on procedure strings are used in developing our concrete analyses. \nThey must also be abstracted so that the abstract versions sat\u00adisfy (2). One way to define an abstract \nversion is by defin\u00ading a table, for example, where Corn is defined by Table 1. For the following discussion, \nwe will let (WO,VI, U2 .0. ) be the sequence of possible configurations resulting from the evaluation \nof SVO. 7.1 Abstract Side Effect Theorem 8 Let ~n = ~0 and 10 G Conc(~O). h can execution path @o ~ \n~~ . . . . where @i G U?;, Vi, if there is an instance of&#38; that has a side eflect on an object L, \nthen +,),7),V,V,, (Pr=COrrZnZOr@.,n {4 old+, U+d+} # Pb))Cr {} Proofi Suppose the object L with birthdate \npb is created in @b and the reference at pr occurs in ~~. By Theorem 7, Vi. V: ~ Conc(~n), s~ there must_exist \nvb and V,, such_that ~b < ~nc(+b) = Conc(VnVb) and +. G Cone(@r) = Conc(VnV, ), which in turn say there \nmust exist_qb and q,, such that m C conc(~b~~) and ~r g Conch@, q, ), and therefore pb C co~c(~b) and \np. e Cofzc(pr). According to Theorem 1, Net(p, -comrnon(p,, pb)) contains a term of the form ad. By (2) \nand the fact AbsP(Net(p)) = AbsPp, we have Abs(p, common(p,, pb))~(~,=common(~r, ~b)). Thus, (~rncommon(p,, \nPb))a n {d, old+, u+d+ } # {}.   7.2 Abstract Object Lifetime Theorem 9 Let ~. = ~0 and W. &#38; Conc(~O). \nIn an execution path +0 + tJI . . . . where +i c Vi, ii, if there is an instance of a that hag a side \neflect on an object L, then Proof: Similar to the previous proof. 7.3 Abstract Concurrent Expressions \nTheorem 10 Let ~n = ~. and V. G Conc(~O). If two eqmawions v] and v2, q IIv2, then there exists a V c \nPeg, and ~1, ~2 E r, such that (VI,V26 v) A (0 E (nmll)) A (06 (Ev41)) where R = (~. V)Jl. (ql and 72 \ncould be the same. ) Proofi By Definition 5, let @ be the configuration where w and vz are two program \ncounters of enabled events, @ c Iil~, for some i. Since 11~ c Cone(@n), there exists a V such that @ \nE Conc(iQ. V) = Cone@. We have II = $11 c Conc~ = Conc(~Jl). The rest of the proof is obvious.  7.4 \nSharpen the Abstract Analyses Unfortunately. . . the use of ~ and common makes the ab\u00adstract analyses \ndull. For inst ante, since c appears at ev\u00adery entry in the table of Corn, according to Theorem 8, any \nfunction that has a downward movement in p, will have a side effect! To sharpen the analyses, the solution \nin [Har89] is adopted, where it is the age, instead of birthdate, recorded in location ob iects. The \nidea is to give up using the operations z and common, and instead record incrementally the net move\u00adments \nof an object after it is born. Assume we have main\u00adtained a function D so that U(V, ~) gives us the current \nnet movement of object ~ at the thread V. u needs to be up\u00addated whenever there is a procedural or concurrency \nmove\u00adment. Then, when we make a reference of ~ at a t bread V,, ~(q~, ~) tells us which functions have \ndownward move\u00adments since the object was born, and thus those functions might have a side effect on ~. \nFurthermore, if we know where ~ was born, say Vb, then an upward movement of a function in ~(q~, ~), \n~b ~ ~b, tells us ~ is accessed after that function has been deactivated. However, the above change greatly \ncomplicates our pre\u00adsentation and we choose not to include it in this paper. This development is carried \nout in detail for the sequential case in [Har89]. 8 Conclusion The presence of concurrent constructs \ngreatly complicate program analysis problems and challenge the current com\u00ad piler techniques. To apply \na safe optimization, for exam\u00ad ple, possible interactions among concurrent activities must be considered \naltogether. We believe once people are used to writing parallel programs, the demand to develop var\u00ad \nious static analyses for improving these parallel programs will grow. This paper provides a general framework \nto the problem, and presents practical techniques for obtaining information about parallel programs, \nsuch as side effects, object lifetime, data dependence and concurrent expres\u00ad sions. Our analyses may \nfacilitate many applications: Q Further parallelization of programs. The com\u00adpiler can discover more \nparallelism in programs with explicit parallelism, and rest ruct ure them. For ex\u00adample, the techniques \nin [SS88, MP90] can be easily extended to procedure calls. Traditional optimization, such as code motions, \nin the presence of the explicit concurrent constructs.  Non-preemption scheduling of concurrent threads. \nIf two concurrent threads have no interaction, then it is desirable that they can run to completion without \npreemption.  Table 1: Corn 61 62, 61,62 G A m~ldldd+]uluu  mm{= ++ Memory management. Allocate objects \nin a more efficient way to reduce average memory access time (in the presence of hierarchical memory) \nand speedup memory recycling (in the presence of dynamic aUoca\u00adtion). Load balancing among concurrent \nthreads; for ex\u00adample, program restructuring can also be applied to achieve load balancing among concurrent \nthreads.  Nondeterminism and access anomaly detec\u00adtion. Compilers can indicate which variables are in\u00advolved \nand report such information to users.  References [BHA861 G.L. Burn, C.L. Hankin, and S. Abramsky. \nThe Theory and Practice of Strictness Analysis for Higher Order Functions. In Programs as Data Objectsl \nLecture Notes in Computer Science .217, pages 43-62. Springer-Verlag, 1986. [Bur87] G. L. Burn. Abstract \ninterpretation and the Par\u00adallel Evaluation of Functional Languages. PhD thesis, University of London, \n1987. [CC77] Patrick Cousot and Radhia Cousot. Abstract In\u00adterpretation: a Unified Lattice Model for \nStatic Analysis of Program by Construction or Approx\u00adimation of Fixpoints. In ACM 4th Symposium on Principles \nof Programming Languages, pages 238\u00ad252, 1977. [Cyt86] Ron Cytron. On the Implications of Parallel Languages \nfor Compilers. Technical Report RC\u00ad11723, IBM T. J. Watson Research Center, April 1986. [Deu90] Alain \nDeutsch, On Determining Lifetime and Aliasing of Dynamically Allocated Data in Higher-order Functional \nSpecifications, In ACM 1 7th Symposium on Principles of Progmmming Languages, pages 157-168, 1990. [HA89] \nWilliams L. Harrison 11[1and Zahira Ammarguel\u00adlat. The Design of Automatic Parallelizers for Symbolic \nand Numeric Programs. In T. Ito and R.H. Halstead, editors, Parallel Lisp: Languages and Systems, Lecture \nNotes in Computer Science 441, pages 235-253. Springer-Verlag, 1989. [Har89] Williams Ludwell Harrison \nIII. The Interproce\u00addural Analysis and Automatic Parallelization of Scheme Programs. Lisp and Symboh \nc Computa\u00adtion: an International Journal, 2(3/4):179 396, 1989. [Har91] Williams Ludwell Harrison 111. \nSemantic Analysis of Symbolic Programs for Automatic Paralleliza\u00adtion. book in preparation, 1991. [HPR89] \nSusan Horwitz, Phil Pfeiffer, and Thomas Reps. Dependence Analysis for Pointer Variables. In ACM SIGPLAN \nConf. on Programming Language Design and Implementation, pages 28-40, 1989. [Hud87] Paul Hudak. A Semantic \nModel of Reference Counting and its Abstraction. In S. Abramsky and C. Hankin, editors, Abstract Interpretation \nof Declarative Languages. Ellis Horwood Limited, 1987. [McD891 Charles E. McDowell. A PracticrJ Algorithm \nfor Static Analysis of Parallel Programs. Journal of Parallel and Distributed Computing, 6(3):515\u00ad 536, \nJune 1989. [Mer91] N. Mercouroff. An Algorithm for Analyzing Com\u00ad municating Processes. In Mathematical \nFounda\u00adtions of Programming Semantics, Lecture Notes in Computer Science. Springer-Verlag, 1991. [MH89] \nCharles E. McDowell and David P. Helmbold. De\u00adbugging Concurrent Programs. ACM Computing Surveys, 21(4):593-622, \nDecember 1989. [MP90] S.P. Midkiff and D.A. Padua. Issues in the Op timization of Parallel Programs. \nIn International Conference on Parallel Processing -Vol II Soft\u00adware, pages 105 113, 1990. [MPC901 S.P. \nMidkiff, David Padua, and R.G. Cytron. Compiling Programs with User Parallelism. In David Gelernter, \nAlexandru Nicolau, and David Padua, editors, Languages and Compilers for Par\u00adallel Computing, Research \nMonographs in Parallel &#38; Distributed Computing, pages 402-422. MIT Press, 1990. [Nie87] Flemming \nNielson. Strictness Analysis and Deno\u00adt ationid Abstract Interpretation. In ACM l~th Symposium on Principles \nof Programming Lan\u00adguages, pages 120-131, 1987. [NPD87] Anne Neirynck, Prakaah Panangaden, and Alan J. \nDemers. Computation of Ali~es and Sup port Sets. In ACM 14th Symposium on Principles of Programming Languages, \npages 274 283, 1987, [Rat90] C. Rattray, editor. Specification and Verification of Concrmrent S9stems, \nWorkshops in Computing. Springer-Verlag, 1990. [Ros89] Mads Rosendahl. Automatic Complexity Anal\u00adysis. \nIn Proc. the Fourth International Conf. on Functional Programming Languages and Com\u00adputer Architecture, \npages 144 156. ACM Press, 1989. [RS90] John H. Reih and Scott A. Smolka. Data Flow Analysis of Distributed \nCommunicating Pro\u00adcesses. International Journal of Parallel Program\u00adming, 19(1):1 30, 1990. [SS88] Dennis \nShaahaand Marc Snir. Efficient and Cor\u00adrect Execution of Parallel Programs that Share Memory. ACM Transaction \non Programming Language and System, 10(2):283-312, April 1988. [Tay83] Richard N. Taylor. A General-Purpose \nAlgorithm for Analyzing Concurrent Programs. Communic\u00adation of ACM, 26(5):362-376, May 1983. [Va189] \nAntti Valmari. Eliminating Redundant Interleav\u00ading During Concurrent Program Verification. In PARLE 89, \nParallel Architectures and Languages Europe, Volumn II: Parallel Languages, Lecture Notes in Computer \nScience 366, pages 89-103. Springer-Verlag, 1989. Notation and Auxiliary Functions. j[z/v] = Az. if \nz = y then z else f(z). j[z//v] = f[(jv u z)/y]. j[m//Y] = f[Lz//t/l] . . . [z//yk], Y = {yl . . .Yk}. \nFirst(v): label of the first expr to be executed in evaluating v. Pararn(a): the identifier of a s parameter. \nBinder(a): the binder of identifier z, m + P2= if m = P and P2 = w, for some p,q 6 l s, then pi(g) else \nif PI = p(q) and p2 = p(q ), for some p, q, q G Ps then p(qlq ) else 1. Instance (x, ~) = P(llinder(z)). \n~(Z)= {S lls>OASGF}UF. Appendix A. The Concrete Semantics [(endthread~O @ )]: let (qP, sp, vP, pP, tP, \nep, FP, qOp) = ~P ~ 11, where qo = 7P pt=pev: P;= PP+P 9 =Sp l t?= t[t(w, -i p)/zJ, i p] t~= if S; = \nO then tp[t(vl, + p)/vo, + Pp] let v= t(q, + p) v = if v b Z # O then First(w) else First(m) in (H + \n{(q, s,v , p,t, e,r, vo )}!0) [(endif~O V )]: let t t[t(q,-1 p][t(q,-i p] = 1 p)/v, +p)/vo, in (H + {(q,s, \nSum(u), p, t , e, ~, no)}1 4 ~(read Cv )]: let (z, p~) = t(u~, + p) v = O(z, p,) t = +p] t[?J /v, in \n(II + {(q,s, Succ(v), p, t , e, ~, ~o)}~~) K(write cv tv )]: kt(Z,Pb)t(~l, = +p) U2= qzq,i p) C7 = \nfJb] C7[?J2/Z,t = t[v2/v, + p] in (H + {(q,s, Succ(v), p, t , e, ~, qO)},rJ ) [(create x)]: let v = \n(c)p) t = t[w/v, i p] in (H -t {(q, s, S74cc(u), p, t , e, ~, no)}!0) [(call &#38;v f )]: let (a, ea) \n= t(v~, + p) V2 = t(vz, +p) x = Param(a) P =p tad e = e~ /zJ J,= T[(v, e)/p ] == 4V2 /x, p ] v = First(a) \n in (II + {(v, s,~ , p,t, e , r , qo)}, a ) [(returnO &#38;v )]: let (u , e ) = T(+ p) t = t[t(vl, + \np)/v, + p] pt=ptff t = t[t(vl, -i p)/P , + p ] in (II + {(q,s, Succ(J), p , t ,e ,r,%)},0) [(+f:y )1: \n-t(V:, ip),i= 1,2 v;~ifvlc Zandv2EZ then q + 02elseL t =@#/v, ip] in (H + {(q, s, Sum(u), p, t , e, T, \nqo)},fJ) [(lambda~ z 6 )]: let t = t[(a, e)/u, + p] in (fI + {(n, s, s~cc(~)$ p, t , e, ~, qo)}ta) \nfzl: let t = t[(z, e(z)) /v, + p] in (II + {(v,s, Succ(u), p, t ,T,)}, e,?70CT) [d: let t = t[c/v, \n1 p] in (f IJ + {(v,s, Succ(u), p, t , e, ~, q. )},0) end Appendix B. The Abstract Semantics . in \nW[(H , ~~j//v ] [(create z)]: let 6 = (({o}, F), J-E, J-z) in u~{iqlq e rj 7 , = q5//v] = ii\u00ad{I/}+ {s74..(.)} \nwhere @ = An. v = v {v}+ {SW.(V)} ~ = ~[(~!~ !P,~ $~!~!no)/nl in W[(II , = )//V ] [(call &#38; &#38;v \n)]: let Vi = Z(V1),l < i <2 @, -&#38;) = z, in U~QCXla G z], where. k= let z = Pamzm[a) 1 rI = n[(s,;;F \n,7!zL, F ,fio)/nl in T[(ii , E )//V ] [(returna e )]: let (ii , F ) = F(cI) n = 7(U1) ? = 7[T//7][i7//v] \n~ = jXAbspc# v = D  {u} + Swc(z ) v = 1 II = v {.}+ n[(F,7 ,5 ,: , SUCC(D ) F !F,70)/nl in T[(TF, \n5) //V l [z]: let i = ({z}, F(Bh4e7 (~)))r Z ~ 7[(7, Lz, L=)//V] v = D -{v}+ { %CC(U)} v = v -{u}+ {Succ(v)} \n/ II =, II[(F, V , p, ?,E, F, Tl,J/?l] . in W[(ll , F)//V ] E.]: let 7 = 7[(1~, LF, c)//ul ii = v {v} \n+ {Svcc(v)} v = v {V} + <Succ(v)} ii = rI[(F,D ,p,i ,F, F,?iQ)/71] in W[(fi , F)//V ] end  \n\t\t\t", "proc_id": "143165", "abstract": "<p>Traditional optimization techniques for sequential programs are not directly applicable to parallel programs where concurrent activities may interfere with each other through shared variables. New compiler techniques must be developed to accommodate features found in parallel languages. In this paper, we use <italic>abstract interpretation</italic> to obtain useful properties of programs, e.g., side effects, data dependences, object lifetime and concurrent expressions, for a language that supports first-class functions, pointers, dynamic allocations and explicit parallelism through cobegin. These analyses may facilitate many applications, such as program optimization, parallelization, restructuring, memory management, and detecting access anomalies.</p><p>Our semantics is based on a <italic>labeled transition system</italic> and is instrumented with <italic>procedure strings</italic> to record the procedural/concurrency movement along the program interpretation. We develop analyses in both concrete domains and abstract domains, and prove the correctness and termination of the abstract interpretation.</p>", "authors": [{"name": "Jyh-Herng Chow", "author_profile_id": "81100327979", "affiliation": "", "person_id": "PP48024110", "email_address": "", "orcid_id": ""}, {"name": "William Ludwell Harrison", "author_profile_id": "81406595478", "affiliation": "", "person_id": "P299146", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143165.143194", "year": "1992", "article_id": "143194", "conference": "POPL", "title": "Compile-time analysis of parallel programs that share memory", "url": "http://dl.acm.org/citation.cfm?id=143194"}