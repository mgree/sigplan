{"article_publication_date": "02-01-1992", "fulltext": "\n Unboxed objects and polymorphic typing Xavier Leroy Ecole Normale Sup&#38;ieure and INRIA* Abstract \nThis paper presents a program transformation that al\u00adlows languages with polymorphic typing (e.g. ML) \nto be implemented with unboxed, multi-word data repre\u00adsentations, more efficient than the conventional \nboxed representations. The transformation introduces coer\u00adcions between various representations, based \non a typ\u00ading derivation. A prototype MIL compiler utilizing this transformation demonstrates important \nspeedups. 1 Introduction It is common saying that statically-typed programs can be compiled more efficiently \nthan dynamically-typed programs. A number of run-time type tests become unnecessary, for instance. In \nthis paper, we study some compilation techniques that rely on the availabil\u00adity of typing information \nat compile-time. These tech\u00adniques are connected to the da,ta representation prob\u00adlem: how high-level \nobjects manipulated by the lan\u00adguage are mapped onto machine-level objects, 1.1 Static typing and data \nrepresenta\u00adtion There are many ways in which knowing type informa\u00adtion at compile-time can help in selecting \nbetter rep\u00adresentations for data. First of all, a compiler needs to know the size (required amount of \nmemory) of all ob\u00adjects manipulated by the program, in order to allocate enough space for variable valu,es \nand intermediate re\u00adsults, and to move the right number of bits from one Iocation to another when performing \nbindings or as\u00adsignments. Without static typing, a default size (usu\u00adally one word) must be assumed for \nall objects in the program, and all represent ations must fit in this size. Objects that do not fit naturally \nin one word, such as *Author s address: INRIA Rocquencourt, projet Formel, B.P. 105,78153 Le Chesnay, \nFrance. E-mail: Xavier .Leroy@inria. f r. Permission to copy without fee afl or part of this material \nis granted provided that the copies are not made or distributed for direct commercial advantage, the \nACM copyright notice and the title of the publication and its date appear, and notice is given that copying \nis by permission of dte Association for Computing Machinery. To copy other\u00adwise, or to republish, requires \na fee and/or specific permission. records and 64-bit floating-point numbers, have to be boxed (allocated \nin the heap and handled through a pointer). With static typing, objects belonging to dif\u00adferent types \ncan have different sizes, as long as all ob\u00adjects of the same type have the same size. The compiler infers \nthe size of an object from its type, and therefore knows how to allocate a variable of that type, or \nmove a value of that type. This makes it possible to have unallocated objects larger than one word: unallocated \n64-bit floating-point numbers, and unallocated records, in particular. The form,er are crucial for numerical\u00adintensive \nprograms. The latter are crucial for languages based on the A-calculus, such as ML, where functions have \nonly one argument and one result, and functions with several arguments or several results are encoded \nas functions taking or returning records. A compiler may also us,estatic typing information to indicate \nwhich register class is best suited for a given object. Most architectures distinguish floating-point \nregisters from general-purpose registers; floating-point operations can only be performed between floating\u00adpoint \nregisters. When an object is statically known to have type float, it is possible to keep this object \nin a floating-point register, instead of a general-purpose reg\u00adister, so that computation on this object \nwill not require register moves. The typical example of this type-based targeting is the use of different \ncalling conventions for functions of different types: a function with result type float can be compiled \nto return its result in a given floating-point register, ready to be used by the caller; a function that \nreturns an object of type int will use a given integer register instead. Static typing guaran\u00adtees that \nthe caller and the callee agree on the types of the arguments and the results, and therefore on their \nlocations. 1.2 The problem with polymorphism Type-directed compilation, as exemplified above, re\u00adquires \nthat each object manipulated by the program have one, unique, statica,lly-known type. This holds with \nsimple, monomorphic type systems such as Pas\u00adcal. But this is not the ciwe with more advanced type systems, \nnotably those providing type abstraction or polymorphism [7]. With type abstraction, the concrete type \nof an object can remi~in unknown at compile-time. @ 1992 ACM 089791 -453 -8/91!/0001/0177 $1,50 With \npolymorphism, an object can belong to several different types at the same time; again, its actual type \nis not available at compile-time. This paper focuses on polymorphism, and, more specifically, on the \npoly\u00admorphic type discipline of the ML language [12]. For instance, the ML identity function function \nx -i x belongs to type a --+ a for all type expressions a. This type allows the function to be applied \nto objects of any type. Therefore, when compiling this function, we know neither the size of the argument \nnor the correct calling convention. Several solutions to this issue have been considered. (See [13] for \na survey.) The first one is to defer the com\u00adpilation of a polymorphic object until it is actually used. \nAt that time, we can compile a version of this object specialized to the type it is used with. This technique \nis often used for generics in Ada. It supports efficient data representations, but results in code duplication \nand loss of separate compilation. A simpler solution is to assume a default size, com\u00admon to all objects, \nand default calling conventions, common to all functions, just as if the language was not statically \ntyped. Most existing ML implementations have taken this approach: they use one-word represen\u00adtations \nand uniform calling conventional [6, 9, 4, 3]. This approach solves the problem of polymorphism, but \nresults in a serious loss of efficiency. For instance, tuples are always heap-allocated, making passing \nseveral argu\u00adments to a function quite expensive. This efficiency loss is unfortunate, especially when \nlarge parts of a program are monomorphic (types are known at compile-time), as it is the case with most \nrealistic ML programs.  1.3 Mixed representations In this paper, we propose an alternate solution, mixed \ndata representations, that relies on using different rep\u00adresentations, boxed as well as unboxed, for \nthe same high-level object, depending on the amount of type in\u00adformation available at compile-time. This \nsolution is both efficient (monomorphic pieces of code are com\u00adpiled with optimal data representations) \nand practical (polymorphic functions are compiled once and for all). It relies on introducing coercions \nbetween various data representations, based on a typing derivation for the program. Some recent papers \nalso consider mixing boxed and unboxed representations in the implementation of ML\u00adlike languages. Peyton-Jones \n[14] expresses many opti\u00admization on boxing and unboxing as source-to-source transformations in this \nsetting. To cope with polymor\u00adphism, a simple typing restriction is proposed: type 1The New Jersey compiler \nutilizes better calling conventions when applying a known function [3]. However, this optimization does \nnot work when calling functions that are passed as argu\u00adments, or defined in snot her compilation unit. \nvariables in a polymorphic type can only be instantiated by boxed types (types whose values are boxed). \nThe work presented here is complementary: our program transformation can be viewed as a translation from \nML (unrestricted polymorphism) into Peyton-Jones lan\u00adguage (restricted polymorphism). Morrison et al. \nhave also used coercions between uni\u00adform and specialized representations in the implemen\u00adtation of their \nNapier88 language [13]. The coercion mechanism we use is similar to theirs, but offers one distinct advantage: \nwhereas coercions in Napier88 in\u00advolve interpreting type tags at run-time, ours may be entirely compiled, \neliminating type information passing at run-time. We also provide a more formal framework to reason about \ncoercion-based compilation techniques, and prove their correctness. The remainder of this paper is organized \nas follows: section 2 informally explains mixed data representa\u00adtions and demonstrates the use of coercions. \nSection 3 formalizes these ideas, as a translation from the core ML language to the core ML language \nwith restricted polymorphism and explicit coercions. We prove that the translation preserves the type \nand the semantics of the original program. Section 4 shows how the results above extend to concrete data \ntypes. Section 5 reports on an implementation of this technique in the Gallium high-performance ML compiler. \nWe give a few conclud\u00ading remarks in section 6. 2 Presentation The approach taken in this paper is to \nmix two styles of data representation: specialized representations (multi\u00adword objects and special calling \nconventions) when the static types are monomorphic; and uniform representa\u00adtions (one-word objects and \ndefault calling conventions) when the static types are polymorphic. Coercions be\u00adtween the two representation \nstyles are performed when a polymorphic object is used with a more specific type. In the case of a polymorphic \nfunction, for instance, co\u00adercions take place just before the function call and just after the function \nreturn. Polymorphic terms are compiled with the assumption that all terms whose static type is a type \nvariable will be represented at run-time with a uniform representation. Hence the compiler knows their \nsize (one word) and their calling protocol (the default one). Consider the following polymorphic function: \nlet make-pair = Ax. (x,x) Its type is VG. a + a x a. Since x has static type a (a type variable), the \ncompiler assumes the value of z fits in one word, and is passed in the default location: typically the \nfirst integer register. The returned value has static type a x cY. The compiler knows it is a pair, hence \nit produces code that returns an unallocated pair of one-word, uniformly represented values in the first \ntwo integer registers. Consider now the application nmke.pair(3,14), Here, function make-pair is used \nwith the more specific type float + float x float. The compiled code for this application evaluates the \nargument 3.14 with the spe\u00ad cialized representation for objects of type f lost: an un\u00adallocated, two-word \nfloating-point number in a floating\u00adpoint register. Similarly, we expect as a result two un\u00adallocated \nfloating-point numbers in two floating-point registers. These choices are not compatible with the hy\u00adpotheses \nmade when compiling make.pair. Therefore, it is not possible to call the code for function make_pair \ndirectly. The argument 3.14 must be first coerced to the uniform representation for floating-point numbers: \nthe number is boxed, and a pointer to the box is passed to function make.pair. The value returned, an \nunallo\u00adcated pair of two boxed floating-point numbers, must be coerced back to the specialized representation \nfor pairs of floating-point number~, by unboxing the two components of the pair. To express this translation \nmore formally, we intro\u00adduce two operators: wrap(r), the coercion from the spe\u00adcialized representation \nfor objects of type T to the uni\u00adform representation; and unwrap(~), the reverse coer\u00adcion, from the \nuniform representation to the specialized representation. Often, wrap(7) will be implemented by boxing, \nand unwrap(~) by unboxing. Better implemen\u00adtations can be considered for certain types T, however; hence \nwe stick to the more general terminology wrap\u00adunwrap. We will often say that an object is in the wrapped \nstate , or in the unwrapped state , to in\u00addicate how it is represented. With this notation, the compilation \nof make_pair(3.14) can be expressed as a translation to an expression with explicit coercions: let x \n= make-pair(wrap(f loat)(3.14)) in (ummap(f loat)(f st(x)), unwrap(f loat)(snd(x))) followed by a conventional \nPascal-like compilation that infers size and calling convention information from the types. The next \nexample involves h[gher-order functions: let map_pair = ~f. Ax. (f (f :St(x)), f (snd(x))) in map-pair(imt-of \n-float) (3.14, 2.718) The map-pair functional has type Va, /3. (a + ~) + a x a + $1x /3. As explained \nabove, it was compiled with the assumption that its pi~rameter f is a function whose argument and result \nare in the wrapped state. The int-of -float primitive has type float+ int, and therefore operates on \nunwrapped integer and floating\u00adpoint numbers. Hence, the map-pair function cannot be applied directly \nto the int-of _float function: it must be given a version of int ..of float that operates on wrapped \nrepresentations. This version is obtained by composing int-of-float with the right coercions: Ax. wrap(int)(int-of \n_float(unwrap(f lost)(x))) This function is a suitablle argument to the map-pair functional. The rest \nof tlhe translation proceeds as in the previous example, resulting in: let y = map-pair (Ax, wrap(int)(int.of-float(unwrap(float)(x)))) \n(wrap(float)(3.14), wrap(float)(2.718)) in (unwrap(int)(f st (y)), unwrap(int)(snd(y))) The important \npoint is thid higher-order functions may require their functional arguments to be transformed to accommodate \nuniform representations instead of spe\u00adcialized representations. This transformation does not require \nrecompilation of the functional, nor of the func\u00adtional argument. It suffices to put some stub code around \nthe functional argument, to perform the re\u00adquired coercions. 3 Formalizaticm In this section, we formally \ndefine the translation out\u00adlined above, in the context of the core ML language. We show that the translated \n,program, evaluated with spe\u00adcialized data representations, computes the same thing as the original program, \nevaluated with uniform data represent at ions. 3.1 The languages The source language is the core ML language: \nA-calculus with constants and the let construct. The only data structures are pairs. The target language \nis core ML extended with the two constructs wrap(r) and nnwrap(~). The syntax for source terms (ranged \nover by a), target terms (a ), type expressions (r), and type schemes (a) is as follows: a ::= il~lzlkc. \nalletz= alin a21al(a2) [ (al,az) / f st(a) I snd(a) a ::= ilflxlk.a lletz=a~ inajla~(a~) I (a{, a~) \nI fst(a ) I snd(a ) I wrap I unwrap(~)(a ) 7-::= alintlfloatl~l+~zl~lx T2 u ::= Val ...an. 7- Here and \nelsewhere, we write x for an identifier, i for an integer constant, f for a flloating-point constant, \nand a for a type variable. Primitives are presented as prede\u00adfine identifiers such as add-float. To the \nsource language we apply Milner s type disci\u00adpline [10, 16]. We recall the typing rules below. They define \nthe familiar predicake E 1-a : T ( under assump\u00adtions E, term a has type ~ ). Here, E is a finite mapp\u00ading \nfrom identifiers z to type schemes a. E(z) = vcq . ..a~. r Do+) g {al . ..%} E+z:p(7-) Eki:int E 1-f \n: float E+z:71*a:Tz E1-Ax. a:rl-r2 Eta2:r1+r2 Etal:rl E t-a2(al) :72 Et-al:rl Eka2:r2 EF (al,az) :~1 \nxT2 E1-a:~1x~2 Eka:rl Xr2~a Et-fst(a) : -rl E E snd(a) : ~2 E1-al:~l E+x:Gen(~l, E)ha2:r2 Ekletx=alinaz: \nrz In the last rule above, we write Gen(-r, E) for the type scheme generalizing r in environment E. It \nis defined by Gen(~, E) = Val ...an. T where al . . . an are the type variables free in T but not in \nE.  3.2 The translation The translation of a term from the source language to the target language is \nbased on the types given to the term and to its subterms. More precisely, we define the translation on \na typing derivation for the given term. (In an actual compiler, this derivation would be the principal \ntyping derivation for the term.) The transla\u00adtion is presented as the predicate E l--a : ~ a a , where \nE k a : T is the typing predicate defined above, and the fourth component a (a term of the target calculus) \nis the translation for a. This proposition is defined by a set of inference rules, with the same structure \nas the typing rules. The rules are given in figure 1. Most rules propagate the translated terms in the \nob\u00advious way. The hard work is performed by the SP func\u00adtion (p is a substitution of types for type variables) \nin the rule for variable specialization. Given a target term a represented according to type r, function \nSP is responsible for inserting the right urap and unwrap coercions to transform it into a term represented \nac\u00adcording to type p(~). The S transformation is defined as follows: SP(a : a) = unwrap(p(a))(a ) SP(a \n: int) = a SP(a : float) = a SP(a :T~x T2) = let x = a in (sP(f st(x) : -rI), S ,(snd(x) : ~2)) SP(a \n: TI +T2) = kc. SP(a (GP(z : -rl)) : 7-z) where x is not free in a We also need to define the dual transformation \nG: G,(a : a) = wrap(p(a))(a ) GP(a : int) = a GP(a : float) = a GP(a : TI x TZ) = letx=a in (G,(fst(x) \n: TI), Gp(snd(x) : T2)) GP(a : T~+ Tz) = Ax. GP(a (SP(z : TI)) : TZ) where x is not free in a The term \na given to the S transformation has been compiled assuming uniform representations for all data of static \ntype ~, for each type variable cr. When a is considered with type p(T), the context expects these data \nof static type a to have the same (specialized) representations as data of static type p(a). Therefore, \nthe goal of transformation S is to locate all data of static type a in term a , and apply the unwrap(p(a)) \ncoercion to them. The transformation proceeds recursively over T, the principal type for a . When T is \na type variable a, it simply applies unwrap(p(a)) to a . When T is an atomic type nothing needs to be \ndone, since p(r) = T. When T is a product type T1 x T2, the two components of a , f st (a ) and snd(a \n), are recursively transformed, and the two results are paired together. Finally, when T is a function \ntype T1 + 72, the transformation re\u00adturns a function that translates its argument y with type T1, applies \na to it, and translates the result with type 72. The processing of the argument requires a dif\u00adferent \ntransformation, G, instead of S, because of the contravariance of the arrow type constructor. In other \nwords, the translated function SP(a : T1 + T2) should be applicable to data of type p(T1 ), using the \nspecial\u00adized representation for components of type a; before applying a to it, it is necessary to switch \nto uniform representation for these components. This is performed by the dual transformation G, defined \nexactly as S, ex\u00adcept that in case T = a, the coercion wrap(p(a)) is used instead of unwrap(p(a)). Working \nout the example make.pair(3. 14) above, we get the following derivation, where E is the typing en\u00advironment \nmake-pair +-Va. a --+ a x a and p is the substitution a + float. E t-make-pair : float + float x float \n+ SP(make_pair : a + a x a) E 1-3.14: float + 3.14 E 1-make.pair(3.14) : float x float + SP(make-pair \n: a + a x a)(3.14) By definition of S, we have: E(Z) = VCII . . . f2!n. T ~07Tt(f2) ~ {@ . ..12n} E1-i:int*i \nE1-f:floataj Et-Z:p(r)*SP(Z:T) Ei-al:rl+a~ E+x:Gen(rl, E)ha2:T2>aj E1-letx=alinaz:rz+ letx=a~ina~ Figure \n1: The translation rules SP(make.pair : a + a x a) = Ax. SP(make.pair(GP(x : float)) : float x float) \n= )x. let y = make.pair(wrap(f loat)(x)) in (unwrap(float)(fst(y)), unwrap(f loat)(snd(y))) After performing \nthe @reduction x = 3.14 at compile\u00adtime, we get the intuitive translation given in section 2. The translation \noften introduces many redexes that can be reduced at compile-time. 3.3 Type correct ness of the translation \nIn this section, we show thak the translation defined above does not introduce type errors: the resulting \ntar\u00adget term is well-typed. The target language is equipped with a variant of the type yystem for the \nsource lan\u00adguage. (We write ~ for the typing judgments of the target language, instead of }-.) There \nare two differ\u00adences. The first one is the explicit mention of wrapping and unwrapping at the level of \ntypes: we introduce a new kind of type expression, [7-], that represents the type of all wrapped values \nof type T. Conversely, other kinds of type expressions, such as float or T1 x T2, now stand for unwrapped \nvafues of these types, We write T! for the extended type expressions: T ::= a Iint IflOat [7-; + T; IT; \nX T; I [T ] Then, the typing rules for tlhe wrap and unwrap con\u00adstructs are, obviously: E #a :T E pa \n [d E ~wrap :[~] E +unwrap : T The second difference is the restriction of polymor\u00adphism. In the source \nlangua,ge, a type variable a uni\u00adversally quantified in a type scheme can be substituted by any type \nexpression. In the target language, we only allow substitution by a wrapped type , that is, a type of \nthe form [T]. This restriction reflects the fact that, at compile-time, objects whose type is a type \nvariable are assumed to be in the wrapped state. If type vari\u00adables can only be instantiated by wrapped \ntypes, then the assumption above holds for all well-typed target terms. To implement this restriction \non polymorphism, we change the typing rude for variables to: E (z) = Val . . .an. T ~OWL(f3) ~ {al . \n..an} E ~z : [P](T) Here, the substitution [p] is the substitution defined by [P](a) = [p(~)] for all@ \nc Dom(p). Any substitution of wrapped types for type variables has the the form [p] for some P. We can \nnow state the correctness of the translation with respect to the type systems. Proposition 1 ~~E ~ a: \nT a a , then E ~a : T. Proofi the proof requires the following lemma. Lemma 1 1. ~~ E ~a : [p](T ), I!hen \nE #SP(a : r ) : p(T ). ~. U E f a : P(T ), then -E ~ GAu : T ): [PI(T ). Proofi by induction over r \n. In case r = a, (1) is the typing rule for unwrap, and (2) is the typing rule for wrap. 0 Proposition \n1 follows from a simple inductive argu\u00adment on the translation derivation. The only interesting case \nis a = z. Then, the translation is: E(z) = Vcq . . . fin. T ~O?7+) ~ {al... an} E 1-m!\\)(T) + S,(Z ,7) \n In the type system for the target language, we have E # z : [p](T), By case 1 of lemma 1, we conclude \nthat E # SP(Z : T) : p(T), as expected. 0 Figure 2: Operational semantics (left: uniform 3.4 Operational \nsemantics In this section, we give operational semantics for the source language and for the target language, \nin prepa\u00adration for a proof of the semantic correctness of the translation. We define two evaluation \npredicates, map\u00adping terms to values, in the style of [12, 16], The syntax for values is: v ::= ~ I t \nI (v) I vljvz I clos(k,~~.a)e) I OP Op ::= add l... A value is either a constant; a pointer (v) to a \nheap cell cent aining value v; an (unallocated) pair of two values VI, V2; a primitive operation op; \nor a closure C1OS(k, Am.a, e) of function kc. a by evaluation environ\u00adment e, with k being the expected \nsize of the function argument (see below). Evaluation environments e are finite mappings from variables \nto values. This defini\u00adtion for values makes the boxing steps explicit (they are usually left implicit \nin this kind of semantics), and pro\u00advides for the fact that values may have different sizes, To be more \nspecific, we assume the following typical size assignment: Ilill = 1 II13J1112=Ilfll = 2 Il(v)ll = 1 \nrepresentations; right: mixed representations) IIV,,V211 = Ilv,ll + IIV211 Ilclos(k, Az. a,e)ll = 2. \n(We consider closures as two pointers, one to the code part, one to the environment part). We also associate \na size to type expressions accordingly: The semantics are given in figure 2. The semantics for the source \nlanguage uses uniform data representa\u00adtions: all terms are mapped onto values of size one. For instance, \nthe floating-point number f is represented as (f); the pair of two terms, the first evaluating to Vl, \nthe other to V2, is represented as (VI, V2); and closures are boxed, too. For the target language, we \nuse specialized represen\u00adtations: floating-point numbers, pairs, and closures are left unallocated. Notice \nthe appearance of a new kind of run-time type errors: applying a closure to a value of the wrong size. \nCoercions wrap(~) and unwrap(~) are implemented as boxing and unboxing for types of size greater than \none, and as no-ops for types of size one.  3.5 Semantic correctness of the transla\u00adtion In this section, \nwe show that the translation preserves semantics: the translated program (evaluated with mixed representations) \ncomputes the same thing as the original program (evaluated with uniform representa\u00ad tions). There is \na slight difficulty here: the semantics may assign different values to the two programs, because one \nobject may have different representations in the two semantics. For instance, in the case of a term with \ntype float, the translation is correct if and only if whenever the term evaluates to (,f), then its translation \nevaluates to f. Hence we need to define a notion of equivalence be\u00adtween two values, one corresponding \nto uniform rep\u00adresentations, the other corresponding to mixed repre\u00adsentations. Actually, the equivalence \nis defined be\u00adtween typed values (value-type pairs). We write it P+V:T s d : T . Types are needed hereto \ncorrectly interpret the values, and to ensure the well-foundedness of the definition. The environment \nI provides an interpretation for type variables in T and T . Legal interpretations for type variables \nare non-empty sets V of pairs of values, such that for all (v, w ) c V, we have llw \\/ = 1. This re\u00adstriction \nover the size of u captures the fact that when\u00adever values are considered with, type a, they must use \ndefault representations. The definition of the semantic equivalence is mod\u00adeled after the semantic typing \nrelations used in some proofs of soundness for type systems [16].    Clos(k, kc .u , e ) : T~ -+ \nTj iff [[T{II = k, and for all vallues VI, V2, vi such that r+vl:TIRv{ : T; and e+z+vl ~ a ~ V2, there \nexists a value vj such that e + z + vj 1-a ~ v; andF~vz:Tzzsv$ : T;. The equivalence relation extends \nto type schemes, and to environments: r*v:v~l...~~.~ = v :Val... a#iffforor all legal interpretations \nVI . . . V~ for type variables al ...an, we have r+altvl +... +-CIn+Vn&#38;V:TXV :T I k e :E w e :E if \nthedomains ofe,E,e , E are the same, and fcm all z G Dom(e), we have r + e(x): E(x) w e (z): E (z). \nWe can now state the semantic correctness of the trans\u00ad lation: Proposition 2 Assume that: E1-a:T~ a \n, I ~e:ERe :E, e~a~v. Then, there exists a value v such that: e kar Zv and r~V:TNV :T. Proofi the proof \nmakes use of the results below. Lemma 2 Let p be the substitution {al + T1 . . . an - Tn}. ~efhe vi = \n{(V, V ) I r ~ TJ: T~ -V : [T~]}. Then, the Vi are legal interpretations for the cw. And the following \ntwo results are equivalent: r+altvl+ . ..+an+_Vn+V. TN V :T (1) r *V: p(T) N V : [p](T) (2) Proofi by \ninduction over ~. 1 Lemma 3 1. Ife 1-a ~ v and 1? ~ v : P(T) w v : [P](T), then there exists V t such \nthat e ~ SP(a : T) ~ v and r ~V :p(T) N V : p(T).  2. If e t-a U v andr + v:p(T) %v :p(T), then  there \nexists v such t!hat e &#38; GP(a : T) z v and r ~ V :p(T) N V : [p](T). Proofi by induction over T . \n0 The proof of proposition 2 itself is a simple inductive argument on the translation derivation. The \nonly inter\u00adesting case is a = x. Then, the translation derivation is: E(z) = Val . .. CKn.T ~O?7L(~) \n~ {al . . . an} E!-z :P(T) ~ S,(Z :~) The only evaluation possibility is v = e(z). By hypoth\u00adesis, I \n>e(z):V~l . . . ~n. T = e (z) :Val . . . cln . T. By definition of ~ on type schemes, and by lemma 2, \nwe have: r ~ e(Z): p(T) = e (z): [/J](T). Then, the expected result follows from lemma 3 (case 1), taking \na = z and v = e (z). 0 4 Concrete data types Until now, we have only dealt with simple data struc\u00ad tures \nsuch as tuples and records. This section discusses more complex data structures: ML concrete data types. \nExcept in degenerate cases, values belonging to con\u00ad crete data types are best kept boxed at all times, \nin the unwrapped state as well as in the wrapped state. This conclusion can be drawn separately from \ntwo features of the concrete data types: they are sum types; and they can be recursive. Since data types \nare sum types, we do not know the exact size for values of these types, only an upper bound. Keeping \nthese values unallocated would waste resources (e.g. registers). Since data types can be recursive, values \nof these types cannot be allo\u00ad cated completely flatly: sub-components of the same type must be handled \nthrough pointers. Therefore, data types are represented by a heap block containing the constructor tag \nand the argument to the constructor, as usual. However, specialized rep\u00adresentations result in a layout \nof the constructor ar\u00adgument that is flatter than usual, and therefore more space-and time-efficient. \nFor instance, assuming the constructor declaration C of float x float, the value C(3.1415, 2.718) is \nrepresented by the 5-word block: ~ C \\ 3.1415 I 2.~18 This optimized layout is natural when we use mixed \nrep\u00adresentations. We statically know that the constructor argument is of type f lost x f lost. Hence \nthe construc\u00adtor argument is evaluated as an unallocated pair of two unallocated numbers. When applying \nconstructor C to this argument, the components of the argument are not boxed yet, and we are free to \nchoose the most compact memory layout for them. ML data types can be parameterized by other types, as \nthe familiar list type: datatype a list = Nil I C!ns of Q x a list This raises a subtle issue. If we \nnaively follow the approach above, a list of floating-point numbers Cns(3.1415, Ni.1), with static type \nfloat list, is rep\u00adresented with the numbers unboxed, as follows. Cns/ 3,1415 I 1 J El However, generic \nfunctions over lists, such as the length function, are compiled without knowing the ex\u00adact type of the \nlist elements, and therefore they assume wrapped representations for the list elements, Hence, before \nbeing passed to a generic function over lists, the list above must be coerced to: m That is, to coerce \na T list to an Q list, we would have to apply coercion wrap(~) to each list element. More generally, \ntransformation S would be defined on lists as: SP(a : T list) = map (Az. SP(Z : ~)) a . This operation \nrequires time and space proportional to the length of the list, making this approach clearly im\u00adpractical. \nTo avoid copying, we must require that list elements are in the wrapped state at all times, even if their \ntype is statically known. In other words, all list cells must share the same layout, with only one word \nallocated for the list element (last format above). This layout is determined once and for all when the \nlist type is defined, assuming wrapped representations for the components of type a (the type parameter). \nThen, nothing needs to be done when specializing or general\u00adizing a list: Sp(a : T list) = a . Instead, \nsome wrapping and unwrapping steps are re\u00adquired when constructing or accedsing lists. To insert them \ncorrectly, it suffices to consider constructors and accessors as polymorphic functions that are used \nwith more specific types: the translation given above inserts the right coercions, To correctly handle \nlists and other generic data struc\u00adtures, it is not enough to keep list components in the wrapped state. \nVVe must also impose some additional compatibility conditions on the wrap and unwrap trans\u00adformations. \nNamely, we must ensure that the com\u00adponents of a wrapped object are themselves in the wrapped state. \nThis is what we call recursive wrap\u00adping . Consider the list 1 = Cns((3.1415, 2.718), Nil), with type \n(float x float) list. If we don t perform recursive wrapping, the natural representation for list 1 is: \nThis object is not a suitable argument to a function f with type Va. (Q x a) list -+ . . . Such a function \nassumes its list argument to be of the format: And no coercion will take place on 1 before it is passed \nto f, since 1 is a list. Hence, the correct representation for 1 is: \\Cns I I Unwrapped integers are \nunboxed, 32-bit wide. Even  I 3-- E@l though they fit in one word, they have to be boxed in t F -A \nm [ZiIIl This means that the correct wrapped representation for a pair of floating-point numbers is \na boxed pair of two boxed floating-point numbers. Similarly, the wrapped represent ation for a function \nwith type f lost -t float is not a boxed closure of the original function on un\u00adwrapped numbers, but \na boxed closure of the corre\u00adsponding function on wrapped numbers. More gener\u00adally, the wrapped representaticm \nfor an object of type T must be compatible with the wrapped representation for objects of type ~ , for \nall types # more general than T. To this end, we redefine the wrap and unwrap coercions on product types \nand on function types in terms of the S and G translations: wrap(~l x ~Z) (a ) = wrap(x)(G~~+71,p-.7Z] \n( :axp)) a wrap(~l -+ ~z)(a ) = wrap(~)(G{~+rl,p.-7z] ( :a.+@)) a unwrap(Tl x ~z) (a ) = wwrap(x)(sia+rl,~~+rz} \n( :axp)) a unwrap(Tl A ~Z)(a ) = ( :a+p)) ~wrap(+)(s{&#38;+Tl,@ tT2} a Here, we have introduced primitive \ncoercions wrap and unwrap that are attached to the type constructors x and * themselves, and no mcme \nto product types and to function types. These coercions can be implemented arbitrarily, for instance \nby boxing and unboxing. 5 Application to ML The author has implemented the ideas above in Gal\u00adlium, a \nprototype high-performance compiler for the ML language. 5.1 Representations for ML data types We first \ndescribe the exact data representations used. Tuples, records, and floating-point numbers are repre\u00adsented \nas described above: unboxed in the unwrapped state, boxed in the wrapped state. Since moving around a \nlarge object is expensive, it would be wise to limit the size of an unboxed tuple. We could decide that \nany tu\u00adple requiring more than 4 words, for instance, is always boxed. This can be determined from the \ntype of the tuple. Concrete data type values are kept boxed at all times, as explained in section 4. \nthe wrapped state, for g~arbage collection reasons (see below). The wrap(int ) operation is therefore \nimple\u00admented as boxing, and unwrap (int) as unboxing. An alternative would be tag,ged, unboxed~ 31-bit \nwide in\u00adtegers both for the unwrapped state and the wrapped state; this would reduce heap allocation, \nbut arithmetic on tagged integers is slower, and interface with C func\u00adtions is complicated. The garbage \ncollector allows small, 8-bit wide inte\u00adgers to remain unboxed in the wrapped state. These small integers \nare used tcl encode booleans, as well as a built-in char type, The wrap and unwrap operations for these \ntypes are no-ops. As an easy generalization of the case for booleans, small integers could also represent \nenumerated data types (,concrete types with constant constructors only). The only value of type unit, \n(), is represented in the unwrapped state as the O-tuple the absence of any value, actually. Wrapped \nvalues of type unit are represented as a given one-word constant. The wrap(unit ) (a) operation consists \nin evaluating a and loading the constant; the unwrap (unit) (a) operation simply evaluates a and throws \nthe result away. Closures representing functional values are repre\u00adsented by two unallocated words. One \nword points to the code part of the function. The other word points to a heap block containing the environment \npart: the values for the free variables of the function. Alloca\u00adtion of the environment :part is not \nperformed if it fits in one word. This simple approach already eliminates some closure allocations, especially \nin the case of simple curried functions. It seems impossible to avoid boxing the environment part in \n:all cases: the type-based tech\u00adniques proposed for tuples do not apply here, since we do not know the \ntypes clf the values contained in the environment part. (The type of the function does not say anything \nabout these types.) Arrays are generic data structures that can be ar\u00adbitrarily large. This mii.kes coercion \nby copying im\u00adpractical on arrays. Actually, ML arrays can be physi\u00adcally modified, making copying semantically \nincorrect. Gallium uses a simple representation for arrays: it al\u00adways keeps array elements in the wrapped \nstate, as in the case of lists. Unwrapped array elements are desir\u00adable, however, since they lead to \nflat arrays, that are more compact and have better locality properties. One approach is to represent \narrays as a flat block of un\u00adwrapped objects, paired with functions to read or write an element, The \naccess functions coerce the array ele\u00ad ments to or from the wrapped representation as needed. References \nto an array with a known type would di\u00adrectly access the array; references to an array with an unknown \ntype would go through the access functions. Test Gallium GalliumO SML-NJ Carol cc -02 What is tested \n1 Takeushi 3.00 5.09 4.47 34.0 1.96 function calls (3 args), integer arithmetic 2 Integral 0.80 2.83 \n8.46 15.2 0.40 floating-point arithmetic, loops 3 List summation 3.60 3,45 5.12 7.90 list processing, \ninteger arithmetic 4 Sieve 1.00 0.94 2.31 5.74 list processing, functional, polymorphism 5 Boyer 1.80 \n2.76 3.60 14.6 term processing, function calls 6 Knuth-Bendix 0.90 0.98 1.11 12.4 0.86 term processing, \nfunctional, completion polymorphism 7 Church integers 6.58 2.40 2.90 16.1 functional, polymorphism 8 \nSolitaire 5.84 10.8 12.6 17.1 0.70 function calls, arrays, loops Figure 3: Experimental results  5.2 \nAn overview of the implementation The Gallium system compiles the Carol Light dialect of ML into assembly \ncode for the MIPS R3000 processor [11]. It combines the data representation technique pre\u00adsented here \nwith a conventional, non CPS-based back end, using some of the standard techniques from [I]. A compilation \ninvolves two passes that communicate through an intermediate language nicknamed C- . This is a simple \nexpression-based language, that manip\u00adulates unboxed tuples of integers, floating-point num\u00adbers, or \npointers (either code pointers or heap ad\u00addresses). C provides most of the operations and control structures \nof C, minus the operations on struct and union. In addition, C directly supports excep\u00adtions and garbage \ncollection. This intermediate language is weakly typed: to each expression is attached a machine-level \ntype expression. A machine-level type is simply a sequence of atomic types: either int, float, or address. \nMachine-level types contains just enough information for the back\u00adend to determine the sizes and the \ncalling conventions, and for the garbage collector to trace all pointers into the heap, The front-end \nperforms type inference, expands pattern-matching into decision trees, inserts the wrap and unwrap operations, \nand explicits closures. The front-end is entirely machine-independent. It embodies all the ML-specific \ntreatments in the compiler. By con\u00adtrast, the back-end is machine-dependent, but it knows almost nothing \nabout ML. It performs instruction se\u00adlection, reordering of computations, liveness analysis, register \nallocation by priority-based coloring of the in\u00adterference graph [8], and emission of MIPS assembly code. \nIn the run-time system, the main originality is the use of static typing information to supplement the \nlack of tagging on objects that are not pointers. Tradition\u00adally, garbage collectors rely on run-time \ntags to distin\u00adguish pointers into the heap from other objects. Tags are also used to implement certain \nprimitives such as generic equality. This solution is not adequate in our case, since we use native, \nunallocated 32-bit integers and floating-point numbers, that cannot be tagged. In\u00adstead, we make some \nof the static typing information available at run-time. Namely, each boxed object is adorned with a header \ngiving the machine-level type of the object; each stack frame is associated with a de\u00adscriptor giving \nthe locations of live objects of type ad\u00address in the corresponding function; a table contains the locations \nof all global variables of type address. This in\u00adformation allows the garbage collector to trace all \nvalid pointers into the heap. It has been pointed out that normally this approach does not work well \nin the presence of polymorphism [2], since an object with static type a can be either an ad\u00address into \nthe heap, or an unboxed integer. In our case, such an object is guaranteed to be in the wrapped state; \nand we have arranged for all wrapped representations to be valid pointers, by boxing wrapped integers. \nHence we can assume that all objects of type a are valid point\u00aders. This fact allows the use of a simple, \nfast copying collector at the cost of allocating wrapped integers. The alternative is to keep integers \nunboxed at all times, and revert to a collector with ambiguous pointers [5], which is slower and more \ncomplex. 5.3 Benchmarks corresponding to these types. Aggressive compile-time Figure 5.1 gives some \nexperimental results obtained with the Gallium compiler. The tests were run on a DecStation 5000/200. \nAll tests were limited to 8 megabytes. The times given are user CPU times, in\u00ad cluding garbage collection \ntime. Gallium is the com\u00ad piler described above; GalliumO is a version of the Gallium compiler that shares \nthe same back-end and code generator, but uses conventional, boxed data rep\u00ad resentations; SML-NJ is \nStandard ML of New Jersey version 0.66, from Bell Labs and Princeton university; Carol is Carol version \n3.1, from INRIA; and cc is the Ultrix 4.1 C compiler, at optimization level 2. These figures indicates \nthat the data representation technique described here lead to important speedups on some programs; have \nlittle imlpact on other programs; and really slow down one (fairly contrived) test pro\u00adgram. The best \nresults are achieved on programs that per\u00adform mostly numerical computations (tests 1 and 2): unboxed, \nuntagged integer and floating-point numbers really pay off in this case. For these programs, the ex\u00adecution \ntimes for Gallium are comparable to the times for the C compiler. The author believes that this data \nrepresentation issue was the m~ain bottleneck that pre\u00advented C-like code written in ML from being compiled \nas efficiently as in C. Programs that perform mostly symbolic computa\u00adtion (tests 5, 6, and 8) also benefit \nfrom specialized data representations, although the speedups are less dramatic. This is somehow surprising, \nsince these pro\u00adgrams mostly manipulate values of concrete data types, that are always boxed. However, \nthey benefit from having unallocated tuples to (communicate with func\u00adtions with several arguments, and \nunallocated closures to communicate with higher-o:rder functions. The most interesting tests are those \nthat make heavy use of polymorphic data structures and polymorphic higher-order functions (tests 3, 4, \n6, and 7). Poly\u00admorphic higher-order functions tend to execute less effi\u00adciently with mixed data representations: \nthe stub code inserted around their functional arguments introduces extra function calls. In test 6, \nthis potential slowdown is overcome by the other benefits of mixed representa\u00adtions (unallocated tuples \nand closures). Tests 3 and 4 shows a slight slowdown; apparently, it could be avoided by performing some \ncompile-time reductions on a lo\u00adcal scale. Test 7, however, demonstrates a major slow\u00addown on a highly \npolymorphic program, The test con\u00adsists in mapping quad quad (Ax. x + 1) on a list of integers, where \nquad is double double, and double is Church s numeral number two: ~f. Ax. f (f x). In this example, the \nclosure for double gets considered in rapid succession with different types (a, a a a, and (a ~ a) ~ \n(CU-i a)). The compiled code spends a lot of time switching between thle various representations reductions \nare required to eliminate these unnecessary coercions. The author has not encountered this phe\u00adnomenon \nin more realistic examples than test 7, how\u00adever. 6 Concluding remarks The technique presented in this \npaper, while resulting in important speedups, is essentially local, and based solely on static typing \ninformation. This means it re\u00ad mains easy to prove correct, and easy to implement. No extra static analysis \nis required; such analyses are often quite expensive, to the point of being impractical. Higher-order \nfunctions ca,useno difficulties, while most other systems static analyses fail in this case. And sep\u00ad \narate compilation remains possible, since all we need are the types of external identifiers an information \nprovided by any module system. Mixed data representations not only speed programs up, but also make it \neasier to interface with libraries written in another language, such as C: it suffices to take unwrapped \nrepresentations compatible with the C data formats and calling conventions. Standard ML features type \nabstraction (at the level of modules) in addition to polymorphism (at the level of terms). From the standpoint \nof data representa\u00adtion, type abstraction raises the same issues as polymor\u00adphism. For instance, there \nis this nasty restriction in Modula-2 [17], that an abstract type can only be imple\u00admented by a pointer \ntype or an ordinal type, to ensure values of an abstract type fit in one word. Mixed data representations \nalso work well with type abstraction: we take vahes of an abstract type to be unwrapped in\u00adside the structure \nthat defines the abstract type, and wrapped outside, in the clients of the structure; the right coercions \nare introduced by applying the G trans\u00adformation from section 3.2 to the values exported by the structure. \nThe technique presented in this paper works bet\u00adter in conjunction with compile-time reductions such \nas function inlining (though the Gallium compiler cur\u00adrently performs no inlining). Reductions can be \nper\u00adformed before or after introducing the coercions. In the latter case, inlining a polymorphic function \ncre\u00adates redexes of the form wrap(T) (unvrap(~) (a )) or unwrap(~) (wrap(T) (a )), that can trivially \nbe replaced by a , saving one boxing ntep and one unboxing step. In the former case, a polymorphic function, \nonce inlined, has a more specific type, and therefore can be compiled more efficiently. When all polymorphic \nfunctions are systematically inlined, the program becomes completely monomor\u00adphic, and it can be compiled \nwith optimal data rep\u00adresentations. This essentially amounts to the Ada ap\u00adpreach referred to in the \nintroduction. The strength of our technique is that it is possible to stop compile\u00adtime reductions at \nany time (when the code becomes too large), and still get a correct program. This paper has only considered \nsimple coercions be\u00adtween the wrapped representations and the unwrapped representations. More elaborate \ncoercion schemes can certainly be found. (Thatte [15] gives interesting exam\u00adples of complex coercions. \n) In particular, all coercions considered here are strict; lazy coercions (coercions that would be performed \nonly on demand) could lead to a better utilization of unwrapped data structures inside generic data structures \nsuch as lists and arrays. A more axiomatic presentation of the translation proposed here, giving minimal \nsemantic conditions over the coercions, would certainly help in finding good sets of coercions. Acknowledgments \nMany thanks to Ian Jacobs, Damien Doligez and Luc Maranget for their comments. References [1] A. V. Aho, \nR. Sethi, and J. D. Unman. Compilers: principles, techniques, and tools. Addison-Wesley, 1986. [2] A. \nW. Appel. Run-time tags aren t necessary. Lisp and Symbolic Computation, 2(2), June 1989. [3] A. W. Appel. \nCompiling with continuations. Cam\u00adbridge University Press, 1991. [4] A. W. Appel and D. B. MacQueen. \nA Standard ML compiler. In Functional Programming Lan\u00adguages and Computer Architecture, volume 242 of \nLecture Notes in Computer Science. Springer- Verlag, 1987. [5] J. F. Bartlett. Compacting garbage collector \nwith ambiguous roots. Technical report, DEC Western Research Laboratory, 1988. [6] L. Cardelli. The functional \nabstract machine. Polymorphism, l(l),1983. {7] L. Cardelli and P. Wegner. On understanding types, data \nabstraction, and polymorphism. Com\u00ad puting surveys, 17(4), 1985. [8] F. Chow and J. Hennessy. Register \nallocation by priority-based coloring. SIGPLAN Notices, 19(6), 1984. [9] G. Cousineau, P.-L. Curien, \nand M. Mauny. The categorical abstract machine. Science of Computer Programming, 8(2), 1987. [10]L. \nDamas and R. Milner. Principal type-schemes for functional programs. In Proc. Symp. Principles of Programming \nLanguages, 1982. [11] G. Kane. MIPS RISC architecture. Prentice-Hall, 1990. [12] R. Milner, M. Tofte, \nand R. Harper. The definition of Standavd ML. The MIT Press, 1990. [13] R. Morrison, A. Dearle, R. C. \nH. Connor, and A. L. Brown. An ad hoc approach to the implementation of polymorphism. A Cikl Transactions \non Program\u00adming Languages and Systems, 13(3), 1991. [14] S. L. Peyton-Jones. Unboxed values as first-class \ncitizens. In Functional Programming Languages and Computer Architecture, volume 523 of Lecture Notes \nin Computer Science, 1991. [15] S. R. Thatte. Coercive type isomorphism. In Functional Programming Languages \nand Computer Architecture, volume 523 of Lecture Notes in Com\u00adputer Science, 1991. [16] M. Tofte. Type \ninference for polymorphic refer\u00adences. Information and Computation, 89( 1),1990. [17] N. Wirth. Programming \nin Modula-f. Springer-Verlag, 1983.  \n\t\t\t", "proc_id": "143165", "abstract": "<p>This paper presents a program transformation that allows languages with polymorphic typing (e.g. ML) to be implemented with unboxed, multi-word data representations. The transformation introduces coercions between various representations, based on a typing derivation. A prototype ML compiler utilizing this transformation demonstrates important speedups.</p>", "authors": [{"name": "Xavier Leroy", "author_profile_id": "81100078576", "affiliation": "", "person_id": "PP39079479", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143165.143205", "year": "1992", "article_id": "143205", "conference": "POPL", "title": "Unboxed objects and polymorphic typing", "url": "http://dl.acm.org/citation.cfm?id=143205"}