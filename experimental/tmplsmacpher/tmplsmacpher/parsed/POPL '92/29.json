{"article_publication_date": "02-01-1992", "fulltext": "\n Model Checking and Abstraction* Edmund M. Clarke Orna Grumberg David E. Long School of Computer Science \nComputer Science Department School of Computer Science The Technion Carnegie Mellon UniversityCarnegie \nMellon University Pittsburgh, PA 15213 Haifa 32000, Israel Pittsburgh, PA 15213 emc+~cs. cmu. edu orna~cs. \ntechnion. ac il long+@cs. cmu. edu October 16, 1991 Abstract system (or model) determined by the program \nand in the length of its specification. In the paper, it was used We describe a method for using abstraction \nto reduce to verify a simple version of the alternating bit protocol the complexity of temporal logic \nmodel checking. The with 20 states. basis of this method is a way of constructing an ab- In the nine \nyears that have passed since that paper stract model of a program without ever examining the was published, \nthe size of the programs that can be ver\u00adcorresponding unabstracted model. We show how this ified by \nthis means has increased dramatically. By de\u00adabstract model can be used to verify properties of the veloping \nspecial programming languages for describing original program. We have implemented a system based transition \nsystems, it became possible to check exam\u00adon these techniques, and we demonstrate their practi\u00ad ples \nwith several thousand states. This was sufficient cality using a number of examples, including a pipelined \nto find subtle errors in a number of nontrivial, although ALU circuit with over 101300 states. relatively \nsmall, protocols and circuit designs [1]. Use of boolean decision diagrams (BDDs) [2] led to an even \nIntroduction greater increase in size. Representing transition rela\u00ad tions implicitly using BDDs made \nit possible to verify Complicated finite state programs arise in many ap-examples that would have required \n1020 states with the plications of computing particularly in the design original version algorithm [4]. \nRefinements of the BDD\u00adof hardware controllers and communication protocols. based techniques [3] have \npushed the state count up over When the number of states is large, it may be very dif-10100 states. In \nthis paper, we show that by combining ficult to determine if such a program is correct. Tem-model checking \nwith abstraction, we are able to handle poral logic model checking [5, 15, 16, 17] is a method even larger \nsystems. In one example, we are able to for automatically deciding if a finite state program sat-verify \na pipelined ALU circuit with 64 registers, each isfies its specification. A model checking algorithm \nfor 64 bits wide, and more than 101300 reachable states. the propositional branching time temporal logic \nCTL Our paper consists of three main parts. In the first, was presented at the 1983 POPL conference [6]. \nThe we propose a method for obtaining abstract models of algorithm was linear in both the size of the \ntransition a program. In the second, we show how these abstract *This research was sponsored in part \nby the Avionics Labo-models can be used to verify properties of the program. rat ory, Wright Research \nand Development Center, Aeronautical Finally, we suggest a number of useful abstractions, and Systems \nDivision (AFSC), U.S. Air Force, Wright-Patterson AFB, we illustrate them via a series of examples. OK1O \n45433-6543 under Contract F33615-90-C-1465, ARPA Or\u00adder No. 7597 and in part by the National Science \nFoundation We model programs as transition systems in which under Contract No. CC R-9005992 and the U.S.-Israeli \nBinational the states are n-tuples of values. Each component of Science Foundation. a state represents \nthe value of some variable. If the The views and conclusions contained in this document are those ith \ncomponent ranges over the set Di, then the set of of the authors and should not be interpreted as representing \nthe official policies, either expressed or implied, of the U.S. govern-all program states is D1 x . . \n. x Dn. Abstractions will ment. be formed by giving subjections hl, . . . . hn which map each Di onto \na set D: of abstract values. The surjec- Permission to copy without fee all or part of this material \nis granted tionh=(hl,.. ., h.) then maps each program state to provided that the copies are not made \nor distributed for direct commercial advantage, the ACM copyright notice and the title of the a corresponding \nabstract state. This mapping may be publication and ita date appear, and notice is given that copying \nis by applied in a natural way to the initial states and the permission of the Association for Computing \nMachinery. To copy other\u00ad transitions of the program. The result is a transition wise , or to republish, \nrequires a fee and/or specific permission. system which we refer to as the canonical abstraction @ 1992 \nACM 089791453-81921000110343 $1.50 of the original program. If it is possible to construct this abstraction, \nwe can use it to verify properties of the program. However, if the state space of the tran\u00adsition system \nis infinite or very large, this may not be feasible. In the finite state case, it may be possible to \nrepresent the system using BDD-based methods, but the computational complexity of building the canonical \nabstraction may still be very high. To circumvent these problems, we show how to derive an approximation \nto the canonical abstraction. The approximation may be constructed directly from the text of the program \nwith\u00adout first building the original transition system. We show how this can be accomplished by symbolic \nexecu\u00adtion of the program over the abstract state space. The specification language that we use is a \nproposi\u00adtional temporal logic called CTL* [7]. This logic com\u00adbines both branching time operators and \nlinear time operators and is very expressive. Formulas are formed using the standard operators of linear \ntemporal logic and two path quantifiers, V and 3. The formula V(+) is true at a state whenever # holds \non all computa\u00adtion paths starting at the state. The formula 3(4) is true whenever 1#1holds for some \ncomputation path. The atomic state formulas in the logic are used to specify that a program variable \nhas a particular abstract value. Because of this, formulas of the logic maybe interpreted with respect \nto either the original transition system or its abstraction. Our goal is to check the truth value of \na formula in the abstract system, and conclude that it has the same truth value in the original system. \nWe prove that this approach is conservative if we restrict to a subset of the logic called VCTL* [12] \nin which only the V path quantifier is allowed. If a formula is true in the abstract system, we can conclude \nthat the formula is also true in the original system. However, if a for\u00admula is false in the abstract \nsystem, it may or may not be false in the original system. In addition, we show that if the equivalence \nrelations induced by the hi are congruences with respect to the operations used in the program, then \nthe method is exact for full CTL*. That is, a formula is true in the abstract system if and only if it \nis true of the original system. We suggest several different abstractions that are use\u00adful for reasoning \nabout programs. These abstractions include 1 congruence modulo an integer, for dealing with arithmetic \noperations; 2 single bit abstractions, for dealing with bitwise log\u00adical operations; 3. product abstractions, \nfor combining abstractions such as the above; and 4. symbolic abstractions. This is a powerful type \nof abstraction that allows us to verify an entire class of formulas simultaneously.  We demonstrate \nthe practicality of our methods by considering a number of examples, some of which are too complex to \nbe handled by the BDD-based methods alone. These examples include a 16 bit by 16 bit hard\u00adware multiplier \nand a pipelined ALU circuit with over 4000 state variables. Numerous other authors have considered the \nprob\u00adlem of reducing the complexity of verification by us\u00ading equivalences, preorders, etc. For example, \nGraf and Steffen [11] describe a method for generating a reduced version of the global state space given \na de\u00adscription of how the system is structured and specifica\u00adtions of how the components interact. Clarke, \nLong and McMillan [8] describe a related attempt. Grumberg and Long [12] propose a framework for compositional \nverifi\u00adcation based on VCTL*. Dill [10] has developed a trace theory for compositional design of asynchronous \ncircuit. But, these methods are mainly useful for abstracting away details of the control part of a system. \nThere has been relatively little work on applying model checking to systems which manipulate data in \na nontrivial way. Wolper [18] demonstrates how to do model checking for programs which are data inde\u00adpendent. \nThis class of programs, however, is fairly small. Our approach makes it possible to handle pro\u00adgrams \nwhich have some data dependent behavior. More recently, BDD-based model checking techniques [4, 9] have \nbeen used to handle circuits with data paths. These methods, while much more powerful than explicit state \nenumeration, are still unable to deal with some systems of realistic complexity. Some examples in sec\u00adtion \n9, for instance, could not be handled directly with these approaches. Our method works well in conjunc\u00adtion \nwith these techniques, however. Of the work on using abstraction to verify finite state systems, the \napproach described by Kurshan [14] is most closely related to ours. This approach has been auto\u00admated \nin the COSPAN system [13]. The basic notion of correctness is u-language cent ainment. The user may give \nabstract models of the system and specification in order to reduce the complexity of the test for contain\u00adment. \nTo ensure soundness, the user specifies homo\u00admorphisms between the actual and abstract processes. These \nhomomorphisms are checked automatically. Our work differs from Kurshan s in several important re\u00adspects. \n1.Our specifications are given in the temporal logic CTL* which can express both branching time and linear \ntime properties, Moreover, we are able to identify precisely a large class of temporal formu\u00adlas for \nwhich our verification methodology is sound. Not all properties are preserved in going from the reduced \nsystem to the original, so this is quite im\u00adportant. 2. Our abstractions correspond to language homo\u00admorphisms \ninduced by boolean algebra homomor\u00adphisms in Kurshan s work. For this type of abstrac\u00adtion, we show how \nto derive automatically an ap\u00adproximation to the abstracted state machine. This approximation is constructed \ndirectly from the pro\u00adgram, so that it is unnecessary to examine the state space of the unabstracted \nmachine. There is no need to check for a homomorphism between the ab\u00adstract and unabstracted systems, \nand it is possible to apply our technique to construct approximations for systems with infinite state \nspaces. 3. The particular abstraction mappings that we use also appear to be new. We demonstrate that \nthese abstractions are powerful enough and that the cor\u00adresponding approximations are accurate enough \nto allow us to verify interesting properties of complex systems. Our paper is organized as follows: the \nnext section is a brief introduction to BDDs and symbolic model checking. This is followed by a discussion \nof transition systems, homomorphisms, and the notion of abstraction that we use. Section 4 discusses \nthe compilation of pro\u00adgrams into transition systems. In the following section, we show how to construct \nthe approximation directly from a program without first building the original tran\u00adsition system. The \nconditions required for exactness are discussed in section 6. Section 7 is the heart of our paper; we \nrelate the theory developed in the previous sections to the temporal logic that we use for specifica\u00adtions. \nIn particular, we prove that our method is con\u00adservat ive in the case of VCTL* formulas. We also show \nthat if the approximation is exact, then all CTL* formu\u00adlas are preserved. Section 8 describes the programming \nlanguage that is used for specifying finite state systems. Section 9 explains some of the abstractions \nthat we have developed for reasoning about complex systems and il\u00adlustrates their use with examples. \nThe paper concludes wit h a discussion of some directions for future research. 2 Boolean decision diagrams \nBoolean decision diagrams (BDDs) are a canonical form representation for boolean formulas described by \nBryant [2]. They are often substantially more compact than traditional normal forms such as conjunctive \nnor\u00admal form and disjunct ive normal form, and they can be manipulated very efficiently. A BDD is similar \nto a boolean decision tree, except that its structure is a directed acyclic graph rather than a tree, \nand there is a strict total order placed on the occurrence of variables as one traverses the graph from \nroot to leaf. Consider, for example, the BDD of figure 1. It represents the formula (aAb)V(cAd), using \nthe variable ordering a < b < c < d. Given an assignment of boolean values to the variables a, b, c and \nd, one can decide whether the assignment \\ o 0 1 Figure 1: A BDD representing (a A b) V (c A d) makes \nthe formula true by traversing the graph begin\u00adning at the root and branching at each node based on the \nvalue assigned to the variable that labels the node. For example, the valuation { a = 1, b = O,c = 1, \nd = 1} leads to a leaf node labeled 1, hence the formula is true for this assignment. Bryant showed that \ngiven a variable ordering, there is a canonical BDD for every formula. He also gives al\u00adgorithms of linear \ncomplexity for computing the BDD represent ations of =~ and f V g given the BDDs for formulas f and g. \nQuantification over boolean vari\u00adables and substitution of a variable by a formula are also straightforward \nusing this represent ation. Given a finite state program, let V be its set of boolean state variables. \nWe identify a boolean formula over V with the set of valuations which make the for\u00admula true. A valuation \nof the variables corresp ends in a natural way to a state of the program; hence the for\u00ad mula may be \nthought of as representing a set of program stat es. The BDD for the formula is in practice a concise \nrepresentation for this set of states. In addition to rep\u00adresenting sets of st ates of a program, we \nmust represent the transitions that the program can make. To do this, we use a second set of variables \nV . A valuation for the variables in V and V can be viewed as designating a pair of states of the program. \nSuch a pair can be viewed as corresponding to a transition between the states of the pair. Thus, we can \nrepresent sets of transitions us\u00ading BDDs in much the same way as we represent sets of states. Many verification \nalgorithms such as temporal logic model checking and state machine comparison can make effective use \nof this representation. 3 Transition systems and abstractions We consider programs with a finite set \nof variables VI, 7)2, . . . . vn. If each variable vi ranges over a set Di of possible values, then the \nset of all possible program states is D1 x DZ x ... x Dn, which we denote by D. We represent the possible \nbehaviors of the program with a set oft ransitions between states. This notion is formal\u00adized in the \nfollowing definition. Definition 1 A transition system over D is a triple M = (S, I, R) where 1.S=D isasetofstates; \n2. I G S is a set of initial states; and 3. R c S x S is a transition relation.  Abstractions will \nbe formed by letting the program variables range over sets D; of abstract values. We will give mappings \nto specify the correspondence between unabstracted and abstracted values. Formally, we let hl, hz, ..., \nhn be subjections, with hi: Di ~ D{ for each i. These mappings induce a subjection h: D -D{ defined by \nh((dl,... ,dn)) = (hl(dl),..., hn(dn)). Alternatively, the relation between unabstracted and abstracted \nvalues can be specified by means of a set of equivalence relations. In particular, each hi corresponds \nto the equivalence relation Wi c Di x Di defined by di W~ e~ if and only if h~(d~) = h;(e~). The mapping \nh induces an equivalence relation * ~ D x D in the same manner: (all, . . ..cln) w (cl,...,en) if and \nonly if dl -1 elA. . .Adn Nn en, We will sometimes specify abstractions by mappings and sometimes specify \nthem by equivalence relations. The two methods are entirely equivalent. Fix a transition system M over \nD and a subjec\u00adtion h:D -+ D . By applying h to the components of M, we obtain an abstract version of \nM. Definition 2 The canonical abstraction of M induced by h is the transition system Mab, over D defined \nas follows. 1. Sab~ = D . 2. Iab,(d ) if and only if 3d (h(d) = d A I(d)). 3, R~b,(d , e ) if and only \nif  3d3e (h(d) = d A h(e) = e A R(d, e)). Definition 3 A homomorphism from a transition sys\u00adtem M \nover D to a transition system M over D is a subjection h: D --+ D such that: 1. I(d) implies I (h(d)); \nand 2. l?(d, e) imp~ies R (h(d), h(e)).  Proposition 1 The mapping h from M to Mab, is a homomorphism. \nAs we will show in section 7, an abstract transition system such as Mab~ may be used to deduce properties \nof M. Moreover, using an abstract transition system instead of M may greatly reduce the complexity of \nau\u00adtomatically verifying these properties. Unfortunately, it is often expensive or impossible to construct \nMab, di\u00adrectly because we must have a representation of M to do the abstraction. We may not be able to \nobtain such a representation if D is infinite or simply too large for our system to handle. In BDD-based \nsystems, even if we are able to represent M, the complexity of comput\u00ading the relational products in \nthe definition of Mab~ is often extremely high. In section 5, we discuss a method for circumventing these \nproblems. The basic idea will be to take advantage of structure in the transition sys\u00adtem ikl. Such structure \narises because M is typically given by a relatively concise program. We show how to compute an approximation \nto Mab~ that can be derived directly from the program text. Hence, it is never nec\u00adessary to construct \na representation of M. In addition, the approximation is often accurate enough to allow us to verify \ninteresting properties of the program. 4 Compilation The approximation to kfabs will be constructed \nby performing an abstract compilation of the program, Hence we begin by considering how programs are \ncom\u00adpiled into transition systems. At a conceptual level, the compilation may be viewed as a two step \nprocess. First, predicate logic formulaa 92 and 9 are constructed to rep\u00adresent the program s actions \nand initial states. These formulas are built from a set of primitive relations that represent the operations \nsuch as addition and compari\u00adson used in the program. Second, the formulas are inter\u00adpreted to derive \nthe actual transition system, By think\u00ading of the process at this level, we can avoid low-level details \nwhich would tie the discussion to a particular programming language. Below, we illustrate how for\u00ad mulas \nrepresenting the actions and initial states might be derived. The construction is similar to that used \nto give the relational semantics of an imperative program\u00adming language or to derive verification conditions \nin the inductive assertions method. First note that a precondition-postcondition seman\u00adtics is not sufficient \nfor our purposes since we are in\u00adterested in the temporal behavior of programs. For this reason, there \nmust be some convention about when time passes during the execution of the program. We assume that there \nare a finite set of control points in the pro\u00adgram (typically chosen by the user) and that executing \na sequence of statements between two consecutive control points requires exactly one time unit. To avoid \nhaving infinite sequences of statements take a finite amount of time, we assume that every path through \na loop body must cent ain at least one control point. Hence it is not necessary to deal with loops explicitly; \nthey are implicit in the sequencing of transitions between control points. We will assume that the intervals \nbetween consecutive control points are sequences of assignment statements and boolean conditions that \nare derived from the con\u00additional statements in the program. TO begin, we ex\u00adamine how formulas representing \nthese intervals can be derived. Consider an assignment statement, say vi := Vj. The formula i (vl,. ... \nvn, v~,.. ., v: ) for such a statement contains two parts. The first specifies that the value of vi after \nthe statement (referred to as vj) is equal to the value of vj before the statement. The second spec\u00adifies \nthat no other variables change. For this particular example, we obtain V~=Vjll Avk=vk. k+i When the \nstatement is more complex, we introduce ad\u00additional temporary variables to hold intermediate re\u00adsults. \nFor example, vi := vi + (Vj ) would be repre\u00ad Vk sented by the formula 3t(P-(vj, vk, t) AP+(vi, t,w[))A~v{=vt, \nl#i  where P and P+ are primitive relations representing subtraction and addition, respectively. For \nboolean conditions, such as vi > vj, T again con\u00adtains two parts, The first specifies that the condition \nevaluates to the boolean value true (represented by the primitive relation P~rUe). The second specifies \nthat no variables change. For this example, we obtain 3t (P~TUe(f) A P>(vi, Vj , t)) A ~ V~ = V~ . k \n To find the formula representing a sequence S1; S2, we first find the formulas T1 representing S1 and \nT2 representing S2. The formula for the sequence is formed by taking a relational product: 3V; . . .3v; \n(T1(vl,. ... vn, vf, v:), v:) AT2(vy, . . ..v., v~, v~)),~)), The logical formula for the entire program \nis formed by combining the formulas for its intervals. We intro\u00adduce an additional program variable p \nthat ranges over the set of the program s control points. We also assume that there are primitive relations \nrepresenting each of the individual control points. The formula for the entire program is a disjunction \nwith one disjunct per interval. Each of these disjuncts is of the form Cj(p)ATj~(W,,..., Wm,W~, -.-, \nvi) A Ck(p ), where Cj is the relation for control point j, ck is the relation for control point k, \nand Tj k is the formula for the interval between control points j and k. In an actual implementation, \nit is possible to avoid enumerating all intervals by treating the program graph as a DAG rather than \na tree. In addition, formulas rep\u00adresenting the transition relation and initial states of the program \nwill not actually be constructed; instead, the program is %yrnbolically executed) to derive the cor\u00adresponding \ntransition system. Starting from the initial states of the program, we simulate the execution of the \nprogram from each state. As we do the simulation, we record which states transitioned to which other \nstates. A key point is that this simulation is driven by knowing how the operations in the program behave, \ni.e., how the primitive relations are interpreted. 5 Computing approximations In the previous section, \nwe mentioned that the initial states and transition relation of a transition system M could be represented \nby formulas 9 and X. Now we examine the relationship between these formulas and similar formulas gabs \nand ~~b~ for ~ab~. By applying a certain transformation to these latter formulas, we obtain formulas \n9aPp and XaPP describing an approxi\u00admation kfaPP to h f~b~. Throughout this section and the next, we \nassume that # and ~ are relations built up from the primitive relations representing the operations in \nthe program. Recall that building ~aba requires evaluating two re\u00adlational products, both involving existential \nquantifica\u00adtion over the elements of D. For conciseness, we will denote this kind of existential abstraction \nusing an op\u00aderator [.]. That is [#(x l,. . . . am)] is an abbreviation for 3yl ...~ym (h(gl) = xl A ... \nAh(ym) = Zm Aq$(yl,. . . )Ym)). Note that [~(zl,. . . , Zm)] has the same free variables Sscj(xl, ..., \nZm). In the latter, the variables range over elements in the Di, while in the former they range over \nelements in the D;. Based on the definition of Mab~, we observe that if 9 and X are the formulas representing \n1 and R, then g~b~ = [9] and ~ab~ = [X] are formulas representing lab~ and Rab~. We now define a transformation \nT on formulas [~]. The idea of T will be to simplify the formulas to which [.] is applied. We assume \nthat ~ is given in negation normal form, i.e., negations are applied only to primitive relations. 1. \nIf P is a primitive relation T([P($l,..., %)]) = [P(xl,..., zm)] T([+ (z?l,...,%m)])= [+ (q, ...,$m)] \n 2.T([4A 0])= T([#l) A T([@l) . 3. T([4 v $]) = T(M) vml]) o 4. T([% +]) = 3ZT([+]). 5. T(W% 4]) = \nVZ T([#]).  In other words, T pushes the existential abstractions inwards. We note that applying T to \na formula results in a formula which is true more often. This is an impor\u00ad tant point since, as will \nbe seen in section 7, it ensures that our methodology will be conservative. Formally, we have the following \nresult. Theorem 1 [~] ~ T([#]). We will let MaPP be the transition system over D whose initial states \nand transition relation are repre\u00adsented by the formulas JaPP = ~(gab~) and ~aPP = T(&#38;,) respectively. \nProposition 2 The following relationships hold be\u00adtween the components of Mab. and the components of \nMapp . I. s~h, = Sapp ; 2. Iab, ~ Iapp; and 3. Ra~s C R.PP .  Observe that japp and %pp have essentiaHY \nthe same structure as 9 and R. The only difference is at the low\u00adest level; the latter formulas have \nprimitive relations and their negations, while the former have abstract versions of these same relations. \nThus, just as we can derive M by symbolically executing the program using the in\u00adterpretations of the \nprimitive relations, we can derive M .PP by symbolically executing the program using the abstracted interpretations \nof the primitive relations. We now consider the relation between M and Mapp. Recall that the mapping \nh is a homomorphism from M to &#38;fab$. We also have the following property of homo\u00admorphisms. Proposition \n3 Let h be a homomorphism from M to M , and let M be z transition system such that  1. s =s ; 9.I < \n1 ; and 3. R GR . Then h is a homomorphism from M to M . Using the above properties, we can conclude \nthat h is also a homomorphism from M to M,Pp. We will discuss the properties implied by this fact in \nsection 7. The relationship between M, kfah~ and MaPP is summarized by the following diagram; here T \nis a program. compilation .P~ M abstract h h compilation 1/ I 6 Exact approximations In the previous \nsections, we demonstrated the exis\u00adtence of homomorphisms from M to kf~b, and M~pp. These results will \nbe used to show that our verifica\u00adtion methodology is conservative. In this sectionj we consider additional \nproperties which suffice to make the method exact. Recall that each hi induces an equiva\u00adlence relation \n-i on Di. Definition 4 Let P(z1, . . . . Xm) be a relation with xj ranging over Dij. The equivalence \nrelations ~ij are a congruence with respect to P if v dle l... dmem (dl ~il el A . ..Adm ~im em +(qdl, \n. . . , dm) @ P(el ,.. ., em))). Theorem 2 If the -i are congruences with respect to the primitive reiations \nand ~ = T ([~]), then ~ ~ [~]. Corollary 1 If the -i are congruences with respect to the primitive relations, \nthen Mabs = MapP. Definition 5 An exact homomorphism from a transi\u00ad tion system M to a transition system \nMl is a homo\u00ad morphism h from M to M with the following additional properties. 1. I (h(d)) implies I(d); \nand 2. R (h(d), h(e)) implies R(d, e).  Theorem 3 If the ~i are congruences with respect to the primitive \nrelations, then h is an exact homomor\u00adphism from M to Mab, (and hence to MaPP). 7 Temporal logic The \nlogics that we will use for specifying properties will be subsets of the logic CTL*. CTL* is a powerful \ntem\u00adporal logic that can express both branching time and linear time properties. For convenience when \ndefining subsets of the logic, we will assume that all formulas are given in negation normal form. That \nis, negations only appear in atomic state formulas. Definition 6 The logic CTL * [7] is the set of state \nfor\u00admulas given by the following inductive de$nition. 1. true and false are atomic state formulas. If \nvi is a program variable and d; G D:, then vi z d; and Vi $ dj are atomic state formulas.  2. If ~ \nand @ are state formulas, then ~ A * and q$V @ are state formulas. 3. If 4 is a path formula, then Q(q5) \nand 3($) are state formulas. 4. If ~ is a state formula, then 4 is a path formula. 5. If 4 and @ are \npath formulas, then so are q5A~ and  #v@. 6. If ~ and ~ are path formulas, then so are x 4, ~ U $, \nand q5VIJ. We also use the following abbreviations: F ~ and G ~, where 4 is a path formula; denote (trueKJ4) \nand (faiseV 4) respectively. CTL is the subset of CTL* that is obtained by elim\u00adinating rules 3 through \n6 above and adding the rule. 3 . If 4 and ~ are state formulas, then so are VX ~, 3X4, V(~U@), 3(4U@), \nV(4V@), and 3(4V@). CTL is of interest because there is a very efficient model checking algorithm for \nit [7]. VCTL* and VCTL [12] are restricted subsets of CTL* and CTL respectively in which the only path \nquantifier allowed is V. These two logics are sufficient to express many of the properties that arise \nwhen verifying programs. As we will see, these logics will also be used when the conditions needed for \nexactness do not hold. We now define the semantics of CTL* for transition systems M over D or D . The \natomic state formulas will be interpreted slightly different depending on whether the state set is abstract \nor not. Definition 7 A path in M is an infinite sequence of states r = soslsz . . . such that for every \ni E N, R(W, %+1). The notation irn will denote the suffix of r which begins at Sn. Definition 8 Satisfaction \nof a state formu!a 4 by a state s (s ~ 4) and of a path formula ~ by a path ir (ir ~ I/J) is defined \ninductively as follows. 1. s \\ true, ands ~ false. Ifs= (cl,...,en) c D, then s \\ (vi ~ d;) if and only \nif ha(e;) = d:. If s=(e~, ... ,e~) ~ D , then s ~ (W -d:) if and only if e; = d;. In either case, s ~ \n(vi ~ d;) if and only if it is not the case that s ~ (IG ~ d~). 2. s~ q!IA4 if and only ifs ~ 4 ands \n1=$. s~ 4v+ if and only ifs&#38;@ ors~~. 3.  s 1= y(d) if and only if for euery path ~ starting at \ns, iT ~ 4. s ~ 3(4) if and ordy if there erists a path ~ starting at s such that r ~ 4. 4. T ~ 4, where \nq!I is a state formuia, if and only if the first state of r satisfies the state formula. 5.ir~~A ~ifando \nnlyifr~4and~ ~$.r~ 4V~ifandonlyif~~40rr~~. 6. r~X@ifandontyif Trl 1=~. ~~q5U+ifand only if there exists \nn 6 N such that N ~ + and for a!li <n,~i~@.ir~~V~ ifandonly iffor all n~N, if~i ~dforaili <n, thennn \n~~. The notation M > 4 indicates that every initial state of M satisfies the formula 4. We now turn \nto the main theorems. These results tell us when it is sound to use abstraction to verify proper\u00adties \nof a program. Theorem 4 Suppose h be a homomorphism from M to M and ~ is a VCTL * formula. Then Ml ~ \n$ implies M~4. Theorem 5 Suppose h is an exact homomorphism from M to M and 4 is a CTL*formula. Then \nM >4 if and only ifM ~4. 8 A simple language In this section, we briefly describe a language for spec\u00adifying \nreact ive programs. We will use this language in the examples that follow. The language is procedural \nand cent ains structured programming constructs, such as while loops and non-recursive procedures. It \nis also finite state: the user must specify a fixed number of bits for each input and output in a program. \nThe model of computation is a synchronous one. At the start of each time step, inputs to the program \nare obtained from the environment. All computation in a program is viewed as instantaneous. There is \none special statement, wait, which is used to indicate the passage of time. When a wait statement is \nencountered, changes to the pro\u00adgram s outputs become visible to the environment, and a new time step \nis initiated. Thus, computation pro\u00adceeds as follows: obtain inputs, compute until a wait is encountered, \nmake out put changes visible, obtain new inputs, etc. Aside from the wait statement, most of the language \nfeatures are self-explanatory. A program in the language may be compiled into a Moore machine for verification. \nSince the Moore ma\u00adchine for a program may have a large number of states (even after abstraction), it \nis important not to gener\u00adate an explicit-state representation of this machine. In\u00adstead, our compiler \ndirectly produces a BDD that rep\u00adresents the Moore machine. This BDD is used as the in\u00adput to a BDD-based \nmodel checking program. When a program is compiled, the user may specify abstractions for some of the \ninputs or outputs. By using the tech\u00adniques described earlier, the compiler directly generates an (approximate) \nabstract Moore machine. There are a input set [1] input start [8] output count [8] := O output alamn[l] \n:= 1 loop if sef= 1 count := start else if count >0 count := count 1 end if if count =O alarm := 1 else \nalarm := O enclif wait endloop Figure 2: An example program number of abstractions built into the compiler, \nand the user may define new abstractions by supplying proce\u00ad dures to build the BDDs representing them. \nAbstract versions of the primitive relations are computed auto\u00ad matically. Figure 2 is a small example \nprogram: a settable countdown timer. The timer has two inputs, set and start, which are one and eight \nbits wide respectively. There are also two outputs: count, which is eight bits wide and is initially \nzero; and alarm, which is one bit and initially one. At each time step, the operation of the counter \nis as follows. If set is one, then the counter is set to the value of start. Otherwise, if the counter \nis not zero, it is decremented. The alarm output is set to one when count is zero, and to zero if count \nis nonzero. 9 Example abstractions In this section, we discuss some abstractions which have proved useful \nin practice. Each is illustrated with a small example. The temporal logic formulas in this sec\u00adtion are \nwritten with some syntactic sugaring of the atomic propositions in order to make them easier to read. \n9.1 Congruence modulo an integer For verifying programs involving arithmetic operations, a useful abstraction \nis congruence modulo a specified integer m: h(i) = imod m. This abstraction is motivated by the following \nproper\u00adties of arithmetic modulo m. ((i mod m)+ (j mod m)) mod ms i+j (mod m) ((i mod m) -(j mod m)) \nmod m -i-j (mod m) ((i mod m)(j mod m)) mod m a ij (mod m) In other words, we can determine the value \nmodulo m of an expression involving addition, subtraction and mul\u00ad tiplication by working with the values \nmodulo m of the sub expressions. The abstraction may also be used to verify more com\u00ad plex relationships \nby applying the following result from element ary number theory. Theorem 6 (Chinese remainder theorem) \nlf ml, mz, . . . . mn are positive integers which are pairwise rel\u00ad.. atively prime, m = m1m2 . . .mn, \nand b, Z1, Z2, . ., Zn are integers, then there is a unique integer i such that forl~j~n, b~i<b+m and \ni-ij (mod mj). Suppose that we are able to verify that at a certain point in the execution of a program, \nthe value of the nonnegative integer variable x is equal to ij modulo mj for each of the relatively prime \nintegers ml, m2, . . . . mn. Further, suppose that the value of z is constrained to be less than mlmz \n. . . mn. Then using the above result, we can conclude that the value of r at that point in the program \nis uniquely determined. We illustrate this abstraction using a 16 bit by 16 bit unsigned multiplier \n(see figure 3). The program has inputs req, inl and in2. The last two inputs provide the factors to operate \non, and the first is a request sig\u00adnal which starts the multiplication. Some number of time units later, \nthe output ack will be set to true. At that point, either output gives the 16 bit result of the multiplication, \nor overfZow is one if the multiplication overflowed. The multiplier then waits for req to become zero \nbefore start ing snot her cycle. The mult iplicat ion itself is done with a series of shift-and-add steps. \nAt each step, the low order bit of the first factor is exam\u00adined; if it is one, then the second factor \nis added to the accumulating result. The first factor is then shifted right and the result is shifted \nleft in preparation for the next step.1 The specification we used for the multiplier was a series of \nformulas of the formz VG(ready AreqA(inl mod m = i) A(in2 mod m = j) 10ne feature of the language which \nthe program uses is the ability to extend an operand to a specified number of bits. For example, z: 5 \nextends z to be 5 bits wide by adding leading O bits. This facility is used to extend output and factor2 \nwhen adding and shifting so that overilow can be detected. The statement (overflow, output) := (output: \n17) + jactor2 sets output to the 16 bit sum of output and factor2, and oveTj70w to the carry from this \nsum. Also, z <<1 is x shifted left by one bit. Right shifts are indicated using >>, The break statement \nis used to exit the innermost loop. 2 This specification admits the possibility that the multiplier always \nsignals an overflow. We will verify that this is not the case using a different abstraction (see subsection \n9.2). input znl[16] input zn.2[16] input req output jactorl [16] := O output factor-2[16] := O output \nO @ Ut [16] := O output overflow := O output ack := O procedure waitfor(e) while Ye wait endwhile enclproc \n loop 1: waitfor(req) factorl := inl factori? := M output := o overflow := O wait loop if (factorl \n= O) V (overflow = 1) break end if if factorl [0] = 1 (overflow, output) := (output: 17) + factor2 endif \nfactorl := factorl >1 wait if (factorl = O) V (overflow = 1) break endif (otierflow,factorl?) := (factor2: \n17) <1 wait endloop ack:= 1 wait waitfor(=req) ack := O endloop Figure 3: A 16 bit multiplier ~ V(lack \nU ack A ok)) where ok s (overflow V (output mod m = ij mod m)). Here, i and j range from O through \nm 1, and ready is an atomic proposition which is true when execution is at the program statement labeled \n1. The input in2 and the outputs factor2 and output were all abstracted mod-U1O m. The output factorl \nwas not abstracted, since its entire bit pattern is used to control when fact orl? is added to output, \nWe performed the verification for m = 5, 7, 9, 11 and 32. These numbers are relatively prime, and their \nproduct, 110,880, is sufficient to cover all 216 possible values of output. The entire verification required \nslightly less than 30 minutes of CPU time on a Sun 4. We also note that because the BDDs needed to represent \nmultiplication grow exponentially with the size of the multiplier, it would not have been feasible to \nverify the multiplier directly. Further, even check\u00ading the above formulas on the unabstracted multiplier \nproved to be impractical. 9.2 Representation by logarithm When only the order of magnitude of a quantity \nis im\u00adportant, it is sometimes useful to represent the quantity by (a fixed precision approximation of) \nits logarithm. For example, suppose i ~ O. Define lgi = [log,(i + 1)1, i.e., lgi is O if i is O, and \nfor i > 0, lgi is the smallest number of bits needed to write i in binary. We take h(i) = lgi. As an \nillustration of this abstraction, consider again the multiplier of figure 3. Recall that a program which \nalways indicated an overflow would satisfy our previous specification. We note that if lg i+lg j < 16, \nthen lg ij < 16, and hence the multiplication of i and j should not overflow. Conversely, if lg i + lg \nj ~ 18, then lg ij 217, and the mult iplicat ion of i and j will overflow. When lg i + lg j = 17, we \ncannot say whether overflow should occur. These observations lead us to strengthen our specification \nto include the following two formulas. VG(ready A req A (lg inl + lg in2 < 16) ~ V(lack U ack A Yoverjlow)) \nVG(ready A req A (lg inl + lg in2 2 18) ~ V(=ack 13 ack A overflow)) We represented all the 16 bit variables \nin the program by their logarithms. Compiling the program with this abstraction and checking the above \nproperties required less than a minute of CPU time. 9.3 Single bit and product abstractions For programs \ninvolving bitwise logical operations, the following abstraction is often useful: h(i) = the jth bit of \ni, where j is some fixed number. If hl and hz are abstraction mappings, then h(i) = (h,(i), h,(i)) \nalso defines abstraction mapping. Using this abstrac\u00adtion, it may be possible to verify properties that \nit is not possible to verify with either hl or hz alone. input in [16] output pariiy[l] :=0 output b[16] \n:=0 output done[l] :=0 b := in wait While b#O parity := parity@ b [0] b:=b>l wait endwhile done :=1 Figure \n4: A parity computation program As an example of using these types of abstractions, consider the program \nshown in figure 4. This program reads an initial 16 bit input and computes the parity of it. The output \ndone is set to one when the computation is complete; at that point, parity has the result. Let #z be \ntrue if the parity of i is odd. One desired property of the program is the following. 1. The value assigned \nto b has the same parity as that of in; and 2. parity @ f b is invariant from that point onw ards. \n We can express the above with the following formula. ~#z n A VX(ljb A VG I(parity @ lb)) V #in A VX(#b \nA VG(parity @ [b)) To verify this property, we used a combined abstraction for in and b. Namely, we grouped \nthe possible values for these variables both by the value of their low order bit and by their parity. \nThe verification required only a few seconds.  9.4 Symbolic abstractions The use of a BDD-based compiler \ntogether with model checker makes it possible to use abstractions which de\u00adpend on symbolic values. This \nidea can greatly increase the power of a particular type of abstraction. For ex\u00adample, consider a simple \npartitioning: O, ifi <a; h(i)= ~ { ifi>a. where a is some fixed value. We might try to use such an abstraction \nwhen the program we are trying to verify involves comparisons. If two numbers are not equiva\u00ad lent according \nto this abstraction, we can find the truth value of a comparison between them. In most cases however, \nusing only this single abstraction would not imply much about the unabstracted program s behav\u00ad ior. \nMuch more information may be obtained by letting a be a symbolic constant. Using such an abstraction \nallows us to verify that a formula is true for all possible abstract programs obtainable by varying a. \nThus, we can effectively verify an entire class of properties for the unabstracted program.3 As an example \nof using this abstraction, consider the program of figure 5. This program represents a cell in a linear \nsorting array. There is one cell for each integer to be sorted, and the cells are numbered con\u00adsecutively \nfrom right to left. In the array, each cell s left and teftsorted inputs are connected to its left neigh\u00adbor \ns y and sorted outputs, and each cell s right input is connected to its right neighbor s z output. The \nvalues to be sorted are the values of the z outputs, The sort proceeds in cycles. During each cycle, \nexactly half the cells (either all the odd numbered cells or all the even numbered cells) will have their \ncomparing output equal to one, These cells compare their own z output with that of their right neighbor. \nThe smaller of these values is placed in y. In addition, if the values were swapped, the cell s sorted \noutput is set to zero. During the next clock period, the right neighbor s x and sorted values are copied \nfrom the first cell s y and sorted outputs. When the rightmost cell s sorted output becomes one, the \nsort is complete. In this example, we consider an array for sorting eight numbers.4 The properties which \nwe verified are: 1. for every a, eventually the values of the z outputs are such that all numbers which \nare less than a come before all numbers which are greater than or equal to a, and this condition holds \ninvariantly from that point on; and 2. for every a, the number of the x outputs which are less than a \nis invariant except when elements are being swapped. The first property implies that the array is eventually \nsorted. The second one implies that the final values of the z outputs form a permutation of the initial \nvalues. We performed the verification by abstracting all the 16 bit variables in the program as described \nabove. The temporal formulas corresponding to the two properties are VFVG((K[l] < aVz[2] ~ a) A.. A(z[7] \n< aVz[8] > a)) 3 In our compiler, non-symbolic abstractions are specified by giving a relation Ef(d, \nd ) which represents h(d) = d . For a sym\u00adbolic abstraction, this relation is extended with additional \npa\u00adrameters which are the symbolic constants it depends on. The BDD representation of the final Moore \nmachine will depend on these symbolic constants. The model checker simply treats the symbolic constants \nas additional state variables. 4 In this program, x and y may have any initial values. The comparing \noutput is set to zero or one depending on the cell s position in the array. The left and right ends of \nthe sorting array are durnrny cells for which z is 2] 6 1 and O respectively. The left cell s soTted \noutput is also fixed at 1. input /ejl[16] input lej%orted[l] output sorted [l] := O output comparing[l] \n:= O or 1 output swap[l] := O output z [16] output y[16] input right[16] loop if comparing = 1 swap \n:= (c < right) uait if swap =1 y:=$ 2 := right sorted := O else y :== right endi-f Wait else wait wait \nc := lefi sorted := lefts orted end if comparing := Tcomparing 1:wait endloop Figure 5: A sorting cell \nprogram and ~ v G (x~cl(x[i] < a) = n) v =stable . () Here, the summation denotes the number of formu\u00adlas \nz [i] < a which are true, and stable is an atomic proposition which is true when every cell is executing \nthe statement labeled 1.5 Verifying these properties re\u00adquired just under five minutes of CPU time. In \naddition, checking these properties on the unabstracted program was not feasible due to space limit at \nions. We also used symbolic abstractions to verify a simple pipeline circuit. This circuit is shown in \nfigure 6 and is described in detail elsewhere [3, 4]. It performs three\u00ad address arithmetic and logical \noperations on operands stored in a register file. We used two independent abstractions to perform the \nverification. First, the register addresses were ab\u00adstracted so that each address was either one of three \nsymbolic constants (ra, rb or rc) or some other value. 5We also verified the property VG VF stab le to \ncheck that the cells maintain loclcstep. Read ports Write pti Bypass circuitry Figure 6: Pipeline circuit \nblock diagram This abstraction made it possible to collapse the en\u00adtire register file down to only three \nregisters, one for each constant. The second abstraction involved the in\u00addividual registers in the system. \nIn order to verify an operation, say addition, we create symbolic constants ca and cb and allow each \nregister to be either ca, cb, ca + cb or some other value. As part of the sp ecificat ion, we verified \nthat the circuit s addition operation works correctly. This property is expressed by the temporal formula \nVG((srcl = ra) A (src,2 = rb) A (dest = rc) A ~stall -+ dXVX((regra = ca) A (regrb = cb) -+ VX(regrc \n= ca + cb))). This formula states that if the source address registers are ra and rb, the destination \naddress register is rc, and the pipeline is not st ailed, then the values in registers ~a and rb two \ncycles from now will sum to the value in register rc three cycles from now. The reason for using the \nvalues of registers ra and rb two cycles in the future is to account for the latency in the pipeline. \nThe largest pipeline example we tried had 64 regis\u00adters in the register file and, each register was 64 \nbits wide. This circuit has more than 4,000 state bits and nearly 101300 reachable states. The verification \nrequired slightly less than six and one half hours of CPU time. In addition the verification times scale \nlinearly in both the number of registers and the width of the registers. For comparison, the largest \ncircuit verified by Burch et al, [3] had 8 registers, each 32 bits, and the verification required about \nfour and one half hours of CPU time on a Sun 4. In addition the verification times there were growing \nquadratically in the register width and cubi\u00adcall y in the number of registers. Conclusion We have described \na simple but powerful method for us\u00ading abstraction to simplify the problem of model check\u00ading. There \nare two parts to this method. First, we have shown how to extract abstract finite state ma\u00adchines directly \nfrom finite or infinite state programs. The construction guarantees that the actual state ma\u00adchine for \nthe program is a refinement of the extracted state machine. Second, we have examined when sat\u00adisfaction \nof a formula by an abstract machine implies satisfaction by the actual machine. For formulas given in \nthe logic VCTL*, this is always the case. We have also implemented a symbolic verification system based \non these ideas and used it to verify a number of non\u00adtrivial examples. In the process of doing these \nexam\u00adples, we have found a number of useful abstractions. Our work on generating abstract systems could \nbe used with other verification methodologies, such as testing language cent ainment, as well. There \nare a number of possible directions for future work. One problem with using our current approach with \nlogics like CTL*, which can express the existence of a path, is in ensuring the strict exactness conditions. \nBy using a more complex finite state model such as AND/OR graphs, it should be possible to extend the \ntechniques and obtain a conservative model checking algorithm for such logics. We also wish to explore \nthor\u00adoughly the problem of generating abstractions for infi\u00adnite state systems. The important step in \ndoing this is determining abstract versions of the primitive rela\u00adtions. Some of the techniques and results \nfrom auto\u00ad mated theorem proving, term rewriting, and algebraic specification of abstract data types \nshould prove useful for this problem. Similar techniques would be useful for studying the flow of data \nin a system. Data items might be represented as terms in the Herbrand universe and functional transformations \non the data would cor\u00adrespond to building new terms from the input terms. Given an equivalence relation \nof finite index on terms, we would derive abstract primitive relations for the op\u00aderations and use these \nto produce an abstract version of the system. References [1] M. C. Browne, E. M. Clarke, D. L. Dill, \nand B. Mishra. Automatic verification of sequential circuits using tem\u00adporal logic. IEEE Trans. Cornput., \nC-35( 12):1035-1O44, 1986. [2] R. E. Bryant. Graph-based algorithms for boolean func\u00adtion manipulation. \nIEEE TrarJs. Cornput., C-35(8), 1986. [3] J. R. Burch, E. M. Clarke, and D. E. Long. Represent\u00ading circuits \nmore efficiently in symbolic model checking. In Proc. 28th ACM/IEEE Design Automation Conf. IEEE Comp. \nSot. Press, June 1991. [4] J. R. Burch, E. M. Clarke, K. L. McMillan, and D. L. Dill. Sequential circuit \nverification using symbolic model checking. In Pr-oc. 27th ACM/IEEE Design Au\u00adtomation Coqf. IEEE Comp. \nSot. Press, June 1990. [5] E. M. Clarke and E. A. Emerson. Synthesis of synchro\u00adnization skelet om for \nbranching time t emp oral logic. In Logic of Programs: Workshop, Yorktown Heights, NY, May 1981, volume \n131 of LNCS. Springer-Verlag, 1981. [6] E. M. Clarke, E. A. Emerson, and A. P, Sistla. Au\u00adtomatic verification \nof finite-state concurrent systems using temporal logic specifications. In Proc. 10th Ann. ACM Symp. \non Principles of Prog. Lang., Jan. 1983, [7] E. M. Clarke, E. A. Emerson, and A. P. Sistla. Au\u00adtomatic \nverification of finite-state concurrent systems using temporal logic specifications. ACM Trans. Prog. \nLang. Sysd., 8(2):244 263, 1986. [8] E. M. Clarke, D. E. Long, and K. L. McMillan. Com\u00adpositional model \nchecking. In Proc. ~th Ann. Symp. on Logic in Comput. Sci. IEEE Comp. Sot. Press, June 1989. [9] 0. Coudert \nand J. C. Madre. A unified framework for the formaJ verification of sequential circuits. In Proc. 1990 \nIEEE Inter. Conf. on Comput. -Aided De\u00adsign. IEEE Comp. Sot. Press, Nov. 1990. [10] D. L. Dill. Trace \nTheory for Automatic Hierarchical Verification of Speed-Indepxdent Circuits. ACM Dis\u00adtinguished Dissertations. \nMIT Press, 1989. [11] S. Graf and B. Steffen. Compositional minimization of finite state processes. \nIn R. P. Kurshan and E. M. Clarke, editors, Proc. 1990 Workshop on Comput. -Aided Verification, June \n1990, [12] 0. Grumberg and D. E. Long. Model checking and mod\u00adular verification. In J. C. M. Baeten and \nJ. F. Groote, editors, Proc. CONCUR 91: .%d Inter. Conf. on Con\u00adcurrency Theory, volume 527 of LNCS. \nSpringer-Verlag, Aug. 1991. [13] Z. Har El and R. P. Kurshan. The COSPAN user s guide. Technical Report \n11211 -871 OO9-21TM, AT&#38;T Bell Labs, 1987. [14] R. P. Kurshan. AnsJysis of discrete event coordination. \nIn J. W. de Bakker, W.-P, de Roever, and G. Rozen\u00adberg, editors, Proc. REX Workshop on Step wise Refine\u00adment \nof Distributed Systems, Models, Formalisms, Cor\u00adrectness, volume 430 of LNCS. Springer-Verlag, May 1989. \n[15] 0. Liechtenstein and A. Pnueli. Checking that finite stat e concurrent programs satisfy their linear \nsp edifica\u00adtion. In Proc. I,?th Ann. ACM Symp. on Principles of Prog. Lang., Jan. 1985. [16] J. Quielle \nand J. Sifakis. Specification and verification of concurrent systems in CESAR. In Proc. Fifth Inter. \nSymp. in Programming, 1981. [17] A. P. Sistla and E. Clarke. Complexity of propositional temporal logics. \n]. ACM, 32(3):733-749, July 1986. [18] P. Wolper. Expressing interesting properties of pro\u00adgrams in propositional \ntemporal logic. In Proc. Ifth Ann. ACM Symp. on Principles of Prog. Lang., Jan. 1986.  \n\t\t\t", "proc_id": "143165", "abstract": "<p>We describe a method for using abstraction to reduce the complexity of temporal logic model checking. The basis of this method is a way of constructing an abstract model of a program without ever examining the corresponding unabstracted model. We show how this abstract model can be used to verify properties of the original program. We have implemented a system based on these techniques, and we demonstrate their practicality using a number of examples, including a pipelined ALU circuit with over 10<supscrpt>1300</supscrpt> states.</p>", "authors": [{"name": "Edmund M. Clarke", "author_profile_id": "81100393517", "affiliation": "", "person_id": "P74801", "email_address": "", "orcid_id": ""}, {"name": "Orna Grumberg", "author_profile_id": "81100493474", "affiliation": "", "person_id": "PP39077247", "email_address": "", "orcid_id": ""}, {"name": "David E. Long", "author_profile_id": "81100467914", "affiliation": "", "person_id": "P62659", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143165.143235", "year": "1992", "article_id": "143235", "conference": "POPL", "title": "Model checking and abstraction", "url": "http://dl.acm.org/citation.cfm?id=143235"}