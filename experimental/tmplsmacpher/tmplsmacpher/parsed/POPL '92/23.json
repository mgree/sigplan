{"article_publication_date": "02-01-1992", "fulltext": "\n Partial Evaluation of High-Level Imperative Programming Languages, wit h applications in Hard Real-Time \nSystems- Vivek Nirkhe* Dept. of Computer Science Univ. of Maryland, College Park, MD 20742 vivek@cs. \nunrd. edu, (301)-405-2724 Introduction Partial evaluation or mixed computation specitilzes a program \nwith respect to information available or com\u00adputable at compile-time. This can involve constant propagation, \nloop unwinding and speciaHzing proce\u00addure calls. Partial evaluation is substantially more powerful than \nconstant propagation. For example, a file can be designated as being available at compile\u00adtime; during \npartial evaluation, the file would be read and the program specitilzed with respect to the con\u00adtents \nof the file. Consider a programming language interpreter that takes as input program text and in\u00adput \ndata for the program. If the interpreter is special\u00adized with respect to a program text P, the resulting \nspecialized code is a compiled version of P [Fut82]. In experiments [JGB+90], researchers have obtained \nspeed-ups of an order of magnitude in applications such aa specializing interpreters to a particular \npro\u00adgram. Most work on partial evaluation has been done for functional lan~uages, and much of the work \non par\u00adtial evaluation of imperative languages has addressed low level languages without scoping, local \nvariables, procedure calls, arrays or pointers. Previous works on partial evaluation of high-level imperative \nprogram\u00adming languages have dealt awkwardly with handling of access to and modification of non-local \nvariables that contain values known at compile-time. In this paper, we describe techniques for partial \nevaluation of a high-level imperative programming language. The contributions of this paper are: A description \nof the problems involved in per\u00adforming partial evaluation of high-level impera\u00ad *Supported in part by \ncontract DSAG60-87-C-O066 from the U.S. Army Strategic Defense Command. tSupported by NSF grants CCR-8908900 \nand CCR-9157384. Permissionto copy without fee all or part of this material is granted provided that \nthe copies are not made or distributed for direct commercialadvantage,the ACM copyright notice and the \ntitle of the publicationandits dateappear,andnoticeis giventhatcopyingis by Permissimrof theAssociationfor \nComputingMachinery.To copyotier\u00adwise, ortorepublish,requiresafeeand/orspecificpermission, William Pught \n Dept. of Computer Science and Institute for Advanced Computer Studies Univ. of Maryland, College Park, \nMD 20742 pughOc8. rmd ~edu, (~01)-405-2705 tive programming languages, and a discussion of our solutions. \nWe describe a solution to the prob\u00ad lem of combining recursion and access to non\u00ad locaJ variables in \nan imperative language [Jon88].  A formal description of a partial evaluator for an imperative language, \nalong with denotationa.1 se\u00admantics of the language and a sketch of a proof that partial evaluation leaves \nthe semantics of a program unaffected.  A discussion of how to extend the language to include features \nsuch as arrays, structures, heap\u00adallocated memory and pointers.  2 Motivation Our primary motivation \nfor applying partial evalua\u00adtion to high-level imperative programming languages is to allow the use of \nreusable software in hard real\u00adtime (HRT) systems. By hard real-time, we mean applications where it is \ncatastrophic to violate tim\u00ading constraints (such as factory automation, space missions, weapons control, \nor medical instrumenta\u00adtion). The scheduling and allocation algorithms of HRT operating systems ensure \nthat the HRT applica\u00adtions meet their deadlines by taking the timing and resource requirements and constraints \nof the appli\u00adcations into account and pre-scheduling the applica\u00adtion. Current techniques for obtaining \nexecution time bounds and pre-scheduling are unable to cope with high level language features such aa \nrecursive proce\u00addure calls, loops with non-constant bounds or dynamic memory allocation. Accordingly, \nproposed program\u00adming languages and environments for HRT systems [KS86, PK89, PS91] forbid these features. \nPrograms suitable for use in HRT systems must have predetermined execution times and behaviors, even \nif current automatic techniques are unable to predict them. Some examples of situations in which we could \nobtain predetermined execution behavior while using features such as recursion, loops with non-constant \nbounds and dynamic memory allocation include: @ 1992 ACM 089791453-8/92/0001/0269 $1.50 A recursive \nimplementation of mergesort could be used to scrt an array, assuming the size of the array is known at \ncompile-time (a quicksort algo\u00adrithm would not be appropriate).  Recursive procedures could be used \nto traverse a graph or other data structure describing part of the system configuration; the structure \nof the graph would need to be known at compile-time, but information known only at run-time could be \nstored at each node of the graph.  A table driven algorithm such as a DFA simulator or an LL(l) parser \ncan be incorporated in the system (assuming the table is known at compile\u00adtime).  In the above cases, \npartial evaluation can be used to produce a residual program that does not contain any of the <forbidden \nfeatures (a description of how these examples would be handled is described in Section 7). We believe \nthat this approach can substantially im\u00adprove programmer productivity and allow the devel\u00adopment of reusable \nsoftware for HRT systems that can be reused in different environments without substan\u00adtial changes to \nthe source programs. Our language and partial evaluation methods are structured so that residual programs \ndo not contain forbidden features and so that we can give infor\u00admative and concise error messages when \nwe cannot produce such residual programs. We believe that the constraints on our language and of partial \nevaluation, such as the division of variables into compile-time and run-time variables (Section 3.1.), \nare constraints that a programmer needs to obey in order to produce pro\u00adgrams with determinable execution \nbehavior. By for\u00admalizing these constraints, our approach may even be helpful in the design stages of \nHRT software. The lim\u00aditations we impose so that residual programs do not contain forbidden features \nare not inherent in our approach. In Section 6.5, we describe how to remove most of the limitations. \nThis research is motivated by Maruti, a system which supports HRT applications [LTCA89], wherein it will \nbe integrated. A detailed description of the application of partial evaluation in HRT systems along with \na number of examples can be found in [NP91, Nir91]. 3 Partial Evaluation of an Im\u00ad perative Programming \nLan\u00ad guage Several issues must be tackled in the design of a pro\u00adgramming language suitable for partizd \nevaluation. Here, we describe those issues and the rationale for our decisions. 3.1 Classifying calculations \nand values as compile-time or run-time As a part of partial evaluation, we must decide which calculations \ncan be performed at compile-time (i.e., partiaJ evaluation time) and which must be postponed until run-time. \nTo determine this, we must know which variables have values known at compile-time (i.e., during partial \nevaluation) and which will have values that will be known only at run-time. Variables can be designated \ncompile-time or run-time either on\u00adline (dynamically) or off-line (statically). Using on\u00adline designation, \nthe status of a variable (known or unknown at partial evaluation time) is computed dur\u00ading partial evaluation \nand changes according to the status of values assigned to it. Using off-line desig\u00adnations, each variable \nis designated as either compile\u00adtime or run-time for the entire life-time of the variable, and compile-time \nvariables always have a value known at compile-timel. Off-line designations can be sup\u00ad plied as annotations \nby the user, inferred via binding time analysis [JSS89, Con90, Lau91], or obtained via a combination \nof the two. Off-line designations can be thought of as an extension of the type system to include information \nabout the compile-time/run-time nature of variables. We have chosen to use off-line designations. In \nthe context of HRT systems, certain computations, such as loops and recursion, must be based on compile-time \nvalues. This requires the programmer to develop a mental model, comparable to her/his model for the type \nsystem of the program, of what is computable at compile-time. Using off-line designations, many errors \n(such as loops that are not guaranteed to terminate at run-time) would be detected and reported much \nas type errors are and would be easier to find and fix. Using on-line designations, such errors would \nstill be detected, but determining the cause of the error could require tracing through simulated execution, \nrequiring something akin to an interactive debugger for partial evaluation. In addition to the motivation \ncited above, we be\u00adlieve that off-line designations is a better choice even in contexts other than HRT \nsystems. On-line des\u00adignations can make partial evaluation inefficient and can lead to overspecialization \nof a program, leading to code explosion. Also, on-line designations make it very difficult to handle \nnon-local variables properly (see Sections 5.1 and 8). Jones [Jon88] argues in favor of off-line designations, \ncommenting that all efficient 1Alth o~gh it is possible to utilize off-line designations that change \nthrough the life of a variable we simplify our exposition by assuming that the designation of a variable \ndoes not change. self-applicable partial evaluators he is aware of have been based on static designations. \nDespite this, most works on partial evaluation of imperative languages [Bu184, E086, Mey91] have advocated \non-line desig\u00adnations for variables. We conjecture that the reasons for this are philosophical: since \nvariables in impera\u00adtive programming languages can hold different values and can be used for different \npurposes during their life\u00adtime, it might seem appropriate to allow their compile\u00adtime/run-time designation \nto change. Of course, the same rationale argues for dynamic typing, but many imperative languages use \nstatic typing. In Section 6.4 we describe how to implement a restricted form of on\u00adline designatio,ls \nwithin our semantics. Our current implementation requires user annota\u00adtions to designate variables as \ncompile-time or run\u00adtime. We plan to supplement this with binding-time analysis so that the status of \nmany intermediate and temporary variables can be inferred from the context in which they are used, reducing \nthe burden on the user. Even with binding-time analysis, user annota\u00ad tions are useful since they help \nto localize problems when binding time analysis cannot find an acceptable set of designations.  3.2 \nDynamic/Lexical scoping Most imperative programming languages use lexical scoping (also called static \nscoping), as opposed to dy\u00adnamic scoping. Unfortunately, the partial evaluation of procedure calls is \nrelated to inlining, and thus it is easier to describe partial evaluation of a language with dynamic \nscoping, We solve this dilemma by preprocessing the lan\u00adguage so that programs have the same semantics \nunder either lexical or dynamic scoping. This allows the user to write programs assuming lexical scoping, \nand allows us to use dynamic scoping in the definition of partial evaluation and the formal semantics. \nIn practice, the residual code can be compiled assuming lexical scop\u00ading. To perform this preprocessing \nstep, we annotate each variable declaration with a tag unique for the lex\u00adical block the variable is \ndefined in. We then annotate each variable use with the tag for the variable defi\u00adnition visible via \nlexical scoping. These tags become part of the variable name and are used when search\u00ading for a variable \nin the environment. This renaming is similar to a global application of the suitable system\u00adatic variable \nrenaming described in Sections 4.7.3.2 and 4.7.3,3 of the revised Algol 60 report [Nau63]. 3.3 Conditionals \nand the value undefrnt If the guard of an if-then-else statement cannot be computed at compile-time, \nthen and else clauses of the conditiomd are required to have identical effects on the compile-time store. \nIf a variable is used as a temporary in one or both clauses, the value undef,nt can be assigned to it \nat the end of both clauses to satisfy this requirement, 3.4 while loops In order to be able to guarantee \ntermination at run\u00adtime, while loops must either have a compile-time guard or have a maximum number of \niterations spec\u00adified as a compile-time expression. while loops with compile-time guard are similar to \nnormal while loops. These loops must modify the compile-time store (oth\u00aderwise the loop is flagged as \nnon-terminating), and are therefore always unwound completely (which may lead to non-termination of partial \nevaluation). The second version may use a run-time guard, but expects a maximum iteration count and a \nlist of vari\u00adables to be killed. If the body of the loop modifies compile-time variables, it must be \nunwound, and the maximum iteration count is used to determine the number of times the loop is unwound. \nAlternatively, if the loop does not modify compile-time variables we do not unwind the loop. However, \nthe maximum it\u00aderation count is required in order for the loop to be acceptable in the residual code \nfor a HRT system. If a loop hss a run-time guard, we do not know at compile-time how many iterations \nwill be executed at run-time. Therefore, after the partial evaluation of the loop, the values of compile-time \nvariables whose value depends upon the number of iterations executed will not be known. Hence, we enforce \na restriction that each compile-time variable that is live after the loop must have identical value after \neach iteration. The variables that do not meet this restriction should be dead upon exit. The variables \nspecified in the kill list are assigned the value undet ~nt after the partial ewduation of the loop terminates \nso that the loop can have a predictable effect on the compile-time store. The kill sets do not have to \nbe provided by the user. They could be automatically computed as the set of variables that might be modified \nwithin the while loop and are dead upon exit from the loop. Similar kill sets also could be a part of \nthe if state\u00adment. They are not needed there because the user can insert the necessary statements to \nkill variables. We avoid giving this in the semantics for conciseness. However, for convenience, kill \nsets should be computed automatically for if statements as well. 3.5 Denotational semantics For the \nmost part, the denotational semantics (see Fig\u00adures 1 and 2) are similar to those typically given for \nsimple imperative languages. All types are extended with a value J-. Run-time locations are also extended \nwith the value unknownRlocn such that for any run\u00adtime location 1, ~R1.c~ C unknownR&#38;. E 1. Loca\u00ad \nMeaning of expressions \u00ad&#38;, L, 7? : Ezp --+ Env --+ State + Ev x State Meaning of statements C : Stint \n+ Env + State + State Meaning of declarations D ; Decl -+ Env + State + Env x State Meaning of programs \n- P : Stint + Store Meaning of operators O : Op + (Int x Int) + Int Environments Env : Id 4 Dv Locations \n Locn : Clocn + Rloen Compile time locations Clocn Run time locations Rlocn Stores Store : Locn -+ \nSv Procedure abstractions Proc : Id x Stint Denotable values Dv : Locn + Proc Storable values Sv : \nInt Expressable Values Ev : Locn + Int Figure 1: Schema for denotational semantics tions are otherwise \na flat lattice. Integers are extended with a value unde~Int but as a flat lattice (i.e., for all integers \ni, i # undef Int + undef I.t @ i @ undef Int. We have refrained from giving explicit handling of co\u00adercions \nand most of the erroneous cases in order to simplify the semantics. The semantics allow expressions to \nhave side-effects, although the only operation that produces side-effects is auto-increment. The store \ncontains both a compile-time store and a run-time store, referenced by compile-time loca\u00adtions (C~ocn \ns) and run-time locations (IUocn s) re\u00adspectively. There are separate allocation routines for compile-time \nand run-time stores, which do not interfere with each other. The azlocc(store) and azZocR(store) functions \nreturn the first unallocated compile-time or run-time location in the store respec\u00adtively. In the residual \nprograms remaining after partial evaluation, there will not be any references to compile\u00adtime variables \nor store. However, to show that partial evaluation leaves the semantics unchanged, we must define the \nsemantics of a program to include opera\u00adtions on both compile-time and run-time variables. The erase \nfunction is used to garbage collect mem\u00adory after exit from a block. Without this feature, entering a \nblock with compile-time variables would have a permanent effect on the compile-time store. This would \ninterfere unpleasantly with the require\u00adment that both branches of a conditional have iden\u00adtical effects \non the compile-time store, placing unnec\u00adessary limitations on. the use of blocks and procedure calls \nin conditionals. The erase function is defined as: Meaning of expressions EP,.CP, RP : Exp -+ Env d State \nd EVP x State Meaning of statements CP : Stint -+ Env + State 4 Stint x State Meaning of declarations \nDP : Decl ~ Env h State * Decl x Env x State Meaning of programs P : Stint -Ans Expressible Values \nEVP : Locn + Ezp Runtime values Runtime : RLocn + NCExp (NCExp are non-constant expressions: anything \nexcept int) Figure 3: Schema for denotational semantics When a while loop with a run-time guard termi\u00adnates, \nthe variables in the kill list are assigned the value undef ~nt. We could perform this killing dy\u00adnamically: \nafter performing partial evaluation of the loop, any compile-time variables whose value cannot be determined \n(i.e. has different values after different iterations) could be set to undef ~nt. Unfortunately, this \nwould only happen if partial evaluation was per\u00adformed; if the program was executed without partial evaluation, \nno killing would take place. This would make it more difficult to formalize the theorem (given in Section \n4.1) that partial evaluation does not change the meaning of a program. To avoid this problem, we force \nthe kill sets to be specified explicitly and be killed during normal execution as well as during par\u00adtial \nevaluation. The kill function sets the variables in the kill list to undeflnt and requires that all locations \nnot killed be identical in the two stores provided to it as arguments. The kill function is defined as: \nkill idList env oldStore newStore = if empty (idList) then if oldStore = newStore then newStore else \nL else kill (tail idList) env oldStore[l + undeflnt] newStore[l t undeflnt] where 1 = env(head idList) \n  4 Partial Evaluation The types used in the formal definition of partial eval\u00aduation are given in \nFigure 3. Note that the set of ex\u00adpressible values has been extended to include Exp; the meaning of a \nstatement is both an updated compile\u00adtime store and a residual statement, and the meaning of a declaration \nis a new environment, an updated compile-time store, and a residual declaration. The definition of partial \nevaluation is given in Figure 4. erase oldStore newStore = X. if (oldStore 1) = 1 then L else (newStore \n1) &#38;[int]store= store) env (int, &#38;[idl env store= (env id, store) ~[ezpl op ezpz] env store= \n(O[op] (VI, v.z), store () where (q, store ) = R[ezpl] erw store and (V2,store ) = ~[erm] env store t[ezp \n+ +] erw store= (v, store [1 -v + 1]) where (/, store ) = C[erp] erw store and v = store / ~[ezp] env \nstore= if isLocn(v) then (store v, store ) else (v, store ) (force result to be an R-value) where (v, \nstore ) = t[erp] env store C[ezp] env store= if is~nteger(v) then J-else (v, store ) (force result to \nbe an L-value) where (v, sto~e ) =s [e*P] env store DISKlp] env store = (env, store) D[dec/l; cZec/2] \nenv sto~e = D[dec/2] end store where (end, store ) = D[decJ1] env store z)[runTime w idl en. store= (env[id \n-/], store[[ -L]) where I = mallocr(store) ~[compileTime var idl en. store= (env[id + /], .tore[/ t -L]) \nwhere / = mallocc(store) D[proc @ (tdj ),... idn(id: ) = stmtl, . . . . strntn] env store = (env , store) \nwhere end = env[idl t procl, ..., id. -procn] (allow recursive procedure calls) and procl = (id~, stmtl \n) and . . . and proctt = (id~, stmt~) (since semantics use dynamic scoping, we don t have to capture \nthe current environment) C[skip] env store = store C[strntl; stmtz] enw store = C[stfnt2] env store where \nstore = C[strntl] env store C[ezpl := ezpr] env store= store [l t r] where (r-,store ) = ~[ezp,] env \nstore and (z,store ) = ~[ezpl] env store C[id(ezp)] env store = C~roc.body] env~oc.id + w] store (use \ndynamic scoping in call) where pTOC = env id and (v, store ) = -CP[ezp] env store C-[declare decl begin \nstrnt end] env store = erase store (C.[stmt] env store ) where (env , store ) = D[dec/] env store C[if \nexp then stmtl else stmt2] env store = (if denotesTwe(v) then C[strntl] else C[strnt2] ) env store where \n(v, store ) = ??[ezp] emu store C[while ezp max c do stint killing ~] env store = if c <,0 V denotesFake(v) \nthen kill k env store store ) (terminate loop) else If denotesZ rue(v) then kill k env store f storetff \n(continue loop) else 1 (non-boolean guard) where (v, store ) = ~[ezp] env store and store = C[stmt] env \nstore and store = C[while exp max c 1 do stint killing k] env store C[while ezp do stint] env store= \nif denotesFa/se(v) then store (terminate loop) else if derwtesTrue(v) then C[while ezp do stint] enw \nstore (continue loop) else 1 where (v, store ) = 7i?[ezP] env store and store = C[stmt] env store P[stmt] \n= C[stmt] (Aid.1) (X.1) Figure 2: DenotationaJ semantics L!p[int]env store= (int, store) tP[idl env store \n= if (env id)?Ckxm then (env id, store) else (id, store) &#38;P [ezpl op ezpz] enw store = if q ?Runtime \nV 02? Runtime then (v1 OPv2, store ) else (OP [op] (VI, VZ), store ) where (v1, store ) = RP [ezpl] \nenv store and (W, store ) = RP[ezp2] env store &#38; [ezp + +] env store = if l? Clocn then (v, store \n[[ + v + 1]) else (v++, store ) where (2, store ) = LP [ezp] env store and v = store 1 RP[ezp] enu store \n= if isLocn(v) then (store v, store ) else (v, store ) where (v, store ) = &#38;P [e*p] env store Lp[ezp] \nenv store = if i.dnteger(v) then L else (v, store ) where (v, store ) = &#38;P [ecp] env store VP [skip] \nenv store = (skip, env, store) VP [compileTime var idl env store = (skip, env[id + /], store[l --L]) \nwhere 1 = malloc.(store) VP [runTime var idl env store = (runTime var id, env[id + UnknOIVnR~O~~], store) \nVP [decll; declz] env store = (decl~; decl~, env , store ) where (deci~, end, store ) = VP [decll] env \nstore and (deci~, env , store ) = VP [decbl .~v St- VP [proc idl (idj ),... id~(id~ ) = stmtl, . . . \n. stmt~] env store = (skip, env , store) where env = env[idl --PT-OC1, ..., idn -proc~] and procl = (id!, \nstmtl ) and . . . and proe~ = (id~, stmt~) Cp [skip] env store = (skip, store) CP [stmtl; stmt2] env \nstore = (stmtj; stmtj, store ) where (stmt~, storel) = CP[stmtl] env store and (strntj, store ) = Cp \n[stmtz] env store Cp [ezpl := ezp,] env store = if /? Clocn then if r? Runtime then J_ else (skip, store \n[[ -r]) else (/ := r, store ) where (r, storel~) = RP[expr] env store and (1, store ) = LP [ecpl] env \nstore Cp[id(ezp)] env store = CP~roc.body] env~roc.id + v] store (for a compile-time parameter) where \nproc = env id and (v, store ) = ZP[ezp] env store Cp [id(ezp)] env store = (declare proc t(prod.id) = \nstint begin t(v) end, store ) (for a run-time parameter) where proc = env id and (v, store~) = ,CP[ezp] \nenv store and t= new procedure name and (drnt ,= Cp fproc.body] env~roc.id -UfLk720WnRL..] store ) store \nCp [declare decl begin stint end] env store = (declare decl begin stint end, erase store store ) where \n(decl , env , store ) = VP [decq env store and (stint , store ) = CP[stint] end store Cp [if exp then \nstmtt else stint.] env store = if v? Runtime (run-time guard, evaluate both branches and force congruence) \nthen if storet = storee then (if v then t else e, store~) else J\u00adelse (if denotesZ rue(v) then Cp [stmt~] \nelse Cp [stmt~] ) env st~re where (v, store ) = RP [ezp] env store and (t, storet ) = CP [stmtt] env \nstore and (e, store. ) = Cp[stmt.] env store CP[while ezp max c do .dmt killing k] env store = if c \n<0 V denotesFa/se(v) then (skip, kill k env store store ) (time to exit loop) else if denotesTrue(v) \nthen (StmtR; [OOpR, kill k env store store ) (loop has a compile-time guard of true; unwind it) else \nif store = store then (while v max c do dTdR killing k, kill k env store store ) (loop doesn t modify \ncompile-time store, we don t have to unwind it) else (if v then stmt~; /OOpR else skip, kill k env store \nstore ) (runtime guard but loop body modifies compile-time store: unwind once, try again) where (v, store \n) = RP [ezp] env store and (StT71tR, store ) = Cp [stint] env store and (loopR, store ) = Cp[while ezp \nmax c 1 do stint killing k] env store CP [while exp do stint] env store = if v? C ompiletime then \nif denotes~alse(o) then (skip, stcwe~) else if store = store then nonterminating (unwinding would never \nterminate) else (stmt~; loop R, ~tore ) (denotesTrue(v)) else L (loop guard isn t a compile-time boolean \nvalue) where (v, store ) = RP[ezp] env store and (StmtR, store ) = C= [stint] env store and (ZoOP/R,storel(~ \n) = Cp [while ezp do stint] env store Pp [stint] = CP[stmt] (Jid.1) (N-l)) Figure 4: Formal definition \nof Partial evaluation 4.1 Semantics areunaltered by partial evaluation We claim that partial evaluation \ndoes not affect the semantics of programs. More specifically, if partial evaluation succeeds, execution \nof the residual program computes the same run-time store as the original pro\u00adgram would. Partial evaluation \ncan fail to terminate because of an infinite loop on compile-time values (induced ei\u00adther through ? while \nloop or recursion). This can occur even in a program that would always terminate at run-time, since the \ncode containing the infinite loop might never be executed. In the context of HRT sys\u00adtems, it is preferable \nto reject such programs, since we wish to generate residual code that can be guaranteed to be free of \ninfinite loops. Partial evaluation can also fail by raising an error indicating that a conditional or \nloop with a run-time guard does not have a predictable effect on the compile-time store. The formal definition \nof partial evaluation does not support cyclic calls (see Section 5.1). Cyclic calls are supported by \ncaching techniques outside the formal semantics. Extending the formal semantics to handle cyclic calls \nwould un\u00adnecessarily complicate them. Our claim is stated formally as: Theorem 1: (assuming that envc \nU enup and stmec U storer are defined) if (StTTJt&#38; stO? I?~)= cp[st~t]en% Sto? f?c and store; = \nc[stmt~] envr storer then store: U store; is defined and store: U store; = C[stmt] (env. U envr) (store. \nU storer) if (deC/R, env~, store~) = DP[decl] en?h Sto? ec and (env~, store:) = D[decl~] envr stoTe~ \nthen env~ U env~ and store: U store; are defined and (env~ U env~, store: U store~) = D[decl] (envC U \nenvr) (store. U storer) if (e~~&#38; sto~e~) = n~[e~~] envC storeC and (v, stoTe~) = ~[eZ~R] envr storer \nthen store: U store: is defined and (v, store: U store;)= 7?[exp] (env. U envr) (storec U storer) For \na run-time expression, exp if (ezp~, store~) = Zp[exp] envC storeC and (v, stoTe~) = ~[e$~R] envr stoTer \nthen store: U store; is defined and (v, stoTe: u stoTe;) = L[eqiJ (enffc U envr) (storec U storer) The \ncomplete proof of Theorem 1 is quite long, be\u00ad cause it includes an inductive subproof for each lan\u00adguage \nconstruct (The complete proof can be found in [Nir91]). Due to a careful construction of the seman\u00adtics \n(such as the choice of dynamic scoping), each sub\u00adproof is fairly straightforward. We give in Figure \n.5 a proof that the semantics of procedure calls with run\u00adtime parameters are unaffected by partial evaluation; \nsubproofs for other language constructs are similar. Each subproof consists of substituting the definitions \nfor the language construct in question, expanding the definitions, simplifying the results, and using \ninduc\u00adtion on the component parts. Compile-time environments map procedure identi\u00adfiers to their definitions, \nmap compile-time variables to the appropriate CLocn and map run-time variables to unknownRLOC.. During \nexecution of residual code, environments contain temporary definitions of pro\u00adcedures produced via procedural \nspecialization, map compile-time locations to ~CLOCn and map run-time locations to the appropriate RLocn. \nThe temporary procedure definitions are only referenced at the point where they are defined, so in the \nproofs we eliminate them from the environments produced while execut\u00ading the residual code. When original \nprogram is exe\u00adcuted, the environment maps procedure identifiers to their definitions and variable names \nto the appropriate locations (either CLocn s or RLocn s). During partial evaluation, the store maps compile\u00adtime \nlocations to the appropriate values and run-time locations to -Lint. During execution of residual code, \ncompile-time values are mapped to lint and run-time locations are mapped to the appropriate values. Dur\u00ading \nexecution of the original program, both types of locations are mapped to appropriate values. Throughout \nthe proof, we use a subscript R to designate residual code, a subscript c to designate values computed \nduring partial evaluation, and a sub\u00adscript T to designate values computed during the execution of the \nresidual program.  5 Memoizing Procedural Spe\u00adcialization When a procedure is called, the body of the \nprocedure is specitilzed according to the values of the compile\u00adtime parameters and the values of the \ncompile-time store that are read during the partial evaluation of the procedure. We might call a procedure \nmultiple times with the same compile-time parameters and store. To avoid code explosion, we would like \nto produce one residual procedure for all of these calls. Note that al\u00adthough the semantics for partial \nevaluation produces definitions of residual procedures throughout the pro\u00adgram, the preprocessing step \ndescribed in Section 3.2 means that this movement of procedure declaration SHOW if stint = id(exp)and \n(5tmtR, store:) = CP[stint] envc store. and store; = c[ShfR] env. store, then store: u store; = C[stmt] \n(env. u env~) (store, U store, ) PROOF: (procId, procBo@) = (env id) = (env. id) (procedures match exactly \nin both env and env=) (v, store ) = .C[ezp] env store (name the results of executing the original program) \n store = C[id(ezp)] env store = C~rocBody] env~rocId + v] store t = new procedure name (tisn t used anywhere \nelse) ShtR = declare proc i(prodd) = bOd~R begin t(VR) end (VR, do?%:)= &#38;[6!%p]e~ti.store. (bodyR, \nstore~) = CP~rocBodyJ envc ~rocId t UnknOWnH@] store; env~ = env, [t t (body~)] St07@ = c[ddR] env. \nSi!o?%r= (.?[t(VR)] e~v~ do?%  (takes into account the fact that a block that doesn t declare any variables \nhas no effect on the store) (V,, StOre~) = &#38;[VR] env~ StOrer = &#38;[VR] enV. StOre, (Since VR c~ \nt cOntZI.in references tO t) store: = (C[i!(VR)] env~ store, ) = C[bodyR] env~~rocId -v,] store: = C[bodyjz] \nenv, ~rocld + v,] store: (since there aren t any references tot in bod~R) v = W. A store = store; U store; \n(via induction on ezp) (restate what things have been expanded to) (bod~R, store;)= CP~rocBody] envC~rodId \n+ UnknOWnR,.Cn] store: store; = C [bodya] env, store: store = C [id(ezp)] envr ~rocIJ -v,] store = C \n~rocBody] env~rodd -v] store: U store: If v= v, and env = env. U envr then env~roc~d -v] = envc~rod~d \n+ UnknOWnR/oC. ] U envr~roc~d t v,] Therefore: store = store: U store: (via induction on procBody) 0 \nFigure 5: Proof of Theorem 1 for procedure calls with a run-time call-by-reference parameter has no effect, \nand we can treat all procedure special\u00adizations as if they were defined at the definition site of the \noriginal procedure. We do this by memoizing [Mic68, PT88, Pug88], or caching, the results of procedure \nspecialization. In order to properly memoize procedure specialization, we augment the computations of \npartial evaluation by computing read and write sets: lists of non-local memory locations that might be \nread or written (and the values read and written) from the time the pro\u00adcedure is entered until we exit \nthe procedure. When we exit the procedure, the read set of the procedure is used along with the values \nof compile-time in pa\u00adrameters as the key of the procedure in the cache. The write set of the procedure \nis stored along with values of the compile-time out parameters and the residual pro\u00adgram as the result \nof the procedure. The approaches taken by other researchers to the problems associated with procedural \nspecialization are discussed in Section 8.  5.1 Cyclic calls Suppose f is a procedure that takes a compile-time \ninteger as a parameter. While specializing f(7), we encounter a call to f(7). This may not involve an \nin\u00adfinite loop, since the inner call to f(7) may happen only under certain run-time conditions. We must \nre\u00adturn the least fixed point of this definition. For the residual code, this is easy. The residual code \nfor the inner call to f(7) is simply a call to the residual code that will be generated for the outer \ncall to f(7). The least fixed point for the read/write sets and compile-time out parameters of the call \nare somewhat harder to pin down, since they have multiple fixed points. For example, if we assume that \na write of the value 42 to the compile-time global variable a is in the write set of the inner call to \nf(7), a write of the value 42 to the compile-time global variable a is in the write set of the outer \ncall to f(7). Therefore, when we see a call that matches the in parameters and the reads we have performed \nso far of a proce\u00addure call we are currently specializing, we freeze the non-local compile-time reads \nand writes of the proce\u00addure call and fix the values of the compile-time out parameters. Any attempt \nto add additional reads or writes is an error. If no additional reads or writes occur, we have selected \nthe least fixed point as the result of the procedure specialization. Incorporating this restriction into \nbinding-time analysis will be the subject of future work. There areother cases that can be handled, such \nas allowing a later write of a value that is never read, or allowing additional reads if the write-set \nis empty; at the moment it is unclear what additional cases would be useful to handle. Note that a procedure \ncall must be put in the cache as soon as we start to specialize it, since we must be able to detect cyclic \ncalls. Such entries must be spe\u00adcially marked so that we use an up-to-date version of the read set of \nthe call and recognize the cache hit as a cyclic call. Freezing the compile-time read/writes may seem \noverly restrictive, but actually allows many interest\u00ading options. Perhaps most importantly, it allows \nus to handle properly reference compile-time data struc\u00ad tures that, once created, are read-only. It \nalso accom\u00ad modates tail recursive calls, allowing us to partially evaluate analyzer programs (described \nby [Bu184]).  5.2 Code explosion Many experiments with partial evaluation have run into problems with \ncode explosion: unnecessary loop unwinding and over-specialization of procedures pro\u00adducing an unmanageable \namount of code. The use of off-line compile-time/run-time designations of vari\u00adables and procedure parameters \nallows a programmer to control this problem: procedures are only special\u00adized according to compile-time \nparameters (and refer\u00adences to non-local compile-time memory). Even if a parameter can be computed at \ncompile-time, it can be passed as a run-time parameter, reducing the number of distinct specializations \nof the procedure.  6 Discussion of Additional Language Features Here we sketch directions for handling \nsome additional language features in the denotational semantics of the language and the partial evaluation. \nSome additional features, such as for loops, are straightforward. 6.1 Arrays and records Arrays and records \ncan be allowed in our scheme with minimal changes (For details see [Nir91]). Allowing an array of compile-time \nvalues to be indexed by a run\u00adtime index creates minor complications, because the array would need to \nbe maintained both at compile\u00adtime and at run-time. This suggests extending the designation of values \ninto compile-time and run-time with an additional class: perpetual compile-time val\u00adues -values known \nboth at compile-time and run-time. Note that we cannot update the value of a compile\u00adtime array indexed \nby a run-time subscript (doing so would make it impossible to know the values of the array at compile-time), \nRecords that contain both compile-time and run\u00adtime fields are transparently divided into two records, \none containing the compile-time fields and the other containing the run-time fields.  6.2 Pointers and \ndynamic memory al\u00adlocation We can allow pointers and dynamic memory, features that most real-time languages \nforbid since these fea\u00adtures make it difficult to estimate memory require\u00adment [KS86, PS91]. We allow \nheap memory to be al\u00adlocated only at compile-time. We maintain two heaps, one holding compile-time values \nand one holding run\u00adtime values. In order to allow run-time pointers to compile-time values, we would \nhave to allocate the val\u00adues on perpetual compile-time heap (i.e. the heap that is available both at \ncompile-time and at run-time). Note that allocating records that contain both run\u00adtime and compile-time \nvalues will require the alloca\u00adtion of memory in both heaps, and pointers to such locations must consist \nof two locations (one for each heap). The memory for the heap containing run-time values must be set-aside \nat run-time (but is statically allocated). 6.3 Other parameter passing schemes In the formal semantics, \nwe describe procedures with a single call-by-reference parameter (call-by-reference parameter passing \nwas described since it is generally the most difficult to handle). The semantics can be easily extended \nto include multiple parameters and in, out and inout parameters implemented via value passing and copy-back. \n 6.4 Dynamic status We can implement a limited form of on-line designa\u00adtion of variables as compile-time \nor run-time. Such variables are represented by a compile-time boolean flag (that tells us whether the \nvalue is currently known at compile-time) and compile-time and run-time ver\u00adsions of the variable. The \nflag is used to determine whether the current value of the variable is compile\u00adtime or run-time. There \nis a minor problem: loops and conditionals must have predictable effects on the flag that contains the \ncurrent designation of the variable (and on the compile-time version of the variable, if the flag in\u00ad \ndicates that the variable is compile-time). This can be achieved by inserting explicators [E086, Mey91] \nto force the variable to be run-time when we can t can t predict a single compile-time value. A slightly \nmore serious problem involves finding the fixed points of cyclic calls (see Section 5.1) cyclic calls \nwill freeze the designations of all non-loczd variables.  6.5 Application to non-real-time lan\u00adguages \n We can remove many restrictions that we need to im\u00adpose in order to support hard real-time programming. \nFor the usual programs, we can allow a while state\u00adment that expects a run-time guard and does not need \na maximum iteration count or a kill list. Since such while loops cannot be unwound, they must not modify \nthe compile-time store. We can also incorporate an\u00adother heap to hold run-time allocated memory holding \nrun-time values.  7 Examples For the examples described in Section 2, our partial evaluator would specialize \nthe programs as follows. consider the following specification of mergesort: exchangesort (runtirne int \narr [1 ; runtime int start, compiletime int sizel) { ... 1; merge (runt ime int arr ~] ; runt ime int \nstart, compiletime int sizel, size2) { ... 1; merge sort (runtime int arr [1 ; runt ime int start; compiletime \nint size) { if (size <= 8) then exchange sort (arr, start, size) else { compiletime int m = size/2; \nmerge sort (arr, 1, m) ; merge sort (arr, l+m, size-m) ; merge (arr, 1, m,size-m) ; 1; 1; A call merge \nsort (A, 1, 128) would be trans\u00adlated into a call to merge sort. 128 (A, 1), where merge sort. 128 is \na specialized routine for sorting runs of length 128, as shown below. mergesort. 128(runtime int arr \n[1 ; runt ime int start) { mergesort .64(arr, 1) ;  merge sort. 64(arr, 1+64) ; merge .64.64 (arr, 1); \n3; Partial evaluation would also produce resid\u00ad ual code for merge sort.64, merge sort.32, merge sort. \n16, mergesort. 8, merge. 64.64, merge .32.32, merge .16.16, merge .8.8, and exchangesort. 8. Automatic \ninlining would merge some of these procedures.  Recursive traversal of a graph would produce spe\u00adcialized \nprocedures for each node of the graph. If the graph was a tree, inlining would transform the speciahzed \nprocedures into an expanded re\u00adcursive decent traversal of the tree. Otherwise, tail-recursion optimization \nwould transform the code into a set of code fragments connected via jumps (in a structure isomorphic \nto the graph).  An LL parser would be expanded into a recursive decent parser, and a DFA simulator would \nbe ex\u00adpanded into a set of code fragments connected via jumps (one fragment for each state).  8 Related \nWork The initial work on partial evaluation and self ap\u00adplicable partial evaluator can be attributed \nto Futa\u00admura [Fut82]. These ideas were later developed and applied for functional languages and for autoprojec\u00adtion \nby Neil Jones and his colleagues [JSS89] and many others [JS86, Con88, CD91, BEE88], and for imperative \nlanguages by Ershov and his colleagues [Ers82, Bu184, E086]. In the original strict partial evaluation \n[Ers82], con\u00additional and loops with run-time guards are not par\u00adtially evaluated. Ershov and Ostrovski \ndescribed po@\u00advariant mixed computation [E086], a method for re\u00admoving this limitation. This involves \nthe use of ex\u00adplicator statements (which change compile-time vari\u00adables into run-time variables) for \nvariables whose value cannot be predicted because we cannot predict during partial evaluation which branch \nof a condition will be executed. Ershov and Ostrovski discuss specialization of procedure calls and handle \nglobal variables by mak\u00ading them run-time and creating explicators for them. Meyer s approach [Mey91] \nis essentially similar to that of Ershov and Ostrovski, although it is much clearer. Meyer performs more \npreprocessing to reduce the number of global variables transformed into run\u00adtime variables, but the post-processing \nin Ershov and Ostrovski s scheme appears to achieve a similar effect. An unfortunate property of the \nschemes described in both [E086] and [Mey91] is that compile-time global variables can be unnecessarily \nand permanently con\u00adverted into run-time variables as a side-effect of mak\u00ading a procedure call. We believe \nthis is due to their de\u00adcision to allow on-line designation of variables. Given the on-line designations, \nit appears difficult or im\u00adpossible to decide appropriate least fixed-points for cyclic calls. In addition, \nallowing access to non\u00adlocal compile-time variables forces the computation of read/write sets for procedure \nspecializations (an issue not addressed in by [E086] or [Mey91]). Nei\u00adther [E086] nor [Mey91] address \nthe issues of lexi\u00adcal/dynamic scoping, compile-time reference parame\u00adters, or compile-time non-local \ndata structures. Bulyonkov proposed another polyvariant mixed computation scheme [Bu184] wherein a statement \nis partially evaluated for all possible states in which known data can be, to produce multiple residual \nstate\u00adments. Though it produces redundant residual code, there are no explicator statements and it leaves \nless for execution at run-time. Bulyonkov s scheme is very susceptible to code-explosion unless the state \nspace is quite small, and he does not address issues such as procedure calls and non-local variables. \nIn [NP91], we motivate the application of partial evaluation to hard real-time systems and describe many \nexamples, but do not provide a technical discus\u00adsion of techniques for partial evaluation of imperative \nlanguages. Conclusion Partial evaluation is an important technique that provides significant advantages \nfor hard real-time programming. It allows specialization of reusable HRT programs with high level languages \nfeatures and thereby tailor them for a particular environment. These specialized programs are amenable \nto estima\u00adtion of execution time, an important aspect for HRT programs. With the tight estimation of \nexecution time for the residual programs, it will be feasible to use high level features for real-time \nprogramming and thereby increasing programmer productivity. This technique can change the way real-time \nprograms are written as the programmers are not restricted from using recur\u00adsion, dynamic memory, and \npointers. In this paper, we have given the formal semantics of a language and a partial evaluator and \nshown that the partial evaluator maintains the semantics of the pro\u00adgrams. We have described method for \nhandling access to non-local variables in the presence of recursion that does not convert compile-time \nvariables into run-time variables. This is a significant result for partial evalu\u00adation of imperative \nlanguages. The static distinction between run-time and compile-time variables helps in simplifying partial \nevaluation, controlling code explo\u00ad [BEE88] [Bu184] [CD91] [Con88] [Con90] [E086] [Ers82] [Fut82] [JGB+90] \n[Jon88] sion, properly handling non-local compile-time vari\u00adables, and providing concise and useful error \nmessages when we cannot produce residual programs with the desired features. 10 Bibliography D. Bjorner, \nA. P. Ershov, and N. D. Jones (Eds.). Partial Evaluation and Mixed Com\u00adputation. North-Holland, 1988. \nM. A. Bulyonkov. F olyvariant Mixed Com\u00adputation for Analyzer Programs. Acts In\u00adformatica, 21:473-484, \n1984. Charles Consel and Olivier Danvy. Static and Dynamic Semantics Processing. In Principles of Programming \nLanguages 91, January 1991. Charles Consel. New Insights into Partial Evaluation: the SCHISM Experiment. \nIn European Symposium on Programming 88, March 1988. Charles Consel. Binding time analysis for higher-order \nuntyped functional languages. In Lisp and Functional Programming 90, 1990. A. P. Ershov and B. N. Ostrovski. \nCon\u00adrolled Mixed Computation and its Ap\u00adplications to Systematic Development of Language-oriented Parsers. \nIn L. G.L.T. Meertens, editor, IFIP TC2/WG 2.1 Working Conference on Program Specifi\u00adcation and Transformation, \npages 31 48. North Holland, 1986. A.P. Ershov. Mixed Computation: Poten\u00adtial Applications and Problems \nfor Study. Theoretical Computer Science, 18:41 67, 1982. Y. Futamura. Partial Computation of Pro\u00adgrams. \nIn RIMS Symposia on Software Sci\u00adence and Engineering, pages 1 35. LNCS 147, 1982. N. D. Jones, C. K. \nGomand, A. Bon\u00addorf, O. Danvey, and T. Mongensen. A self-applicable evaluator for the lambda\u00adcalculas. \nIn IEEE Conference on Computer Languages, pages 49-58, 1990.  N, D, Jones. Challenging Problems in Par\u00adtial \nEvaluation and Mixed Computation. In IFIP TC2 Workshop on Partial Evalu\u00adation and Mixed Computation, \npages 225 282. North Holland, 1988. JSS89] KS86] [Lau91] [LTCA89] [Mey91] [Mic68] [Nau63] [Nir91] [NP91] \n[PK89] [PS91] Ulrik Jdrring and William Scherlis. Com\u00adpilers and Staging Transformations. In Principles \nof Programming Languages 86, January 1986. N. D. Jones, Peter Sestoft, and Harald S@ndergaard. Mix a \nSelf-applicable Par\u00adtial Em.luator for Experiments in Compiler Generation. LISP and Symbolic Computa\u00adtion, \npages 9 50, 1989. E. Kligerman and A.D. Stoyenko. Real-Time Euclid: a Language for Reliable Real-Time \nSystems, IEEE Transactions on Software Engineering, pages 941-949, Sep. 1986. John Launchbury. Strictness \nand Binding-Time Analysis: Two for the price of One. In Programming Language Design and Im\u00adplementation \n91, June 1991. S. T. Levi, Satish K. Tripathi, Scott Carson, and Ashok K. Agrawala. The MARUTI Hard Real-Time \nOperating Sys\u00adtem. ACM SIGOPS Operating Systems Re\u00adview, 23(3), July 1989. Uwe Meyer. Techniques for \nPartial Evalu\u00adation of Imperative Languages. In Sympo\u00adsium on Partial Evaluation and Semantics-Based \nProgram Manipulation (PEPM- 91), June 1991. Donald Michie. Memo Functions and Machine Learning. Nature, \n218:19-22, April 1968. Peter Naur (Ed,). Revised report on the al\u00adgorithmic language algol 60. Communicat\u00adions \nof the ACM, 6(1):1 18, January 1963. Vlvek Nirkhe. Application of Partial Eval\u00aduation to Real-Time Programming. \nPhD thesis, Department of Computer Science, University of Maryland, College Park, 1991. Forthcoming. \nVivek Nirkhe and William Pugh. A Partial Evaluator for the Maruti Hard Real-Time System. In 12th IEEE \nReal-Time Systems Symposium, Dec. 1991. P. Puschner and Ch. Koza. Calculating the Maximum Execution Times \nof Real-Time Programs. The Journal of Real-Time &#38;ys\u00adtems, 1(2):159-176, Sep. 1989. ChangYun Park \nand Alan C. Shaw. Ex\u00adperimenting With A Program Timing Tool Based On Source-Level Timing Schema. IEEE \nComputer Magazine, pages 48-57, May 1991. [PT88] William Pugh and Tim Teitelbaum. Incremental Computation \nvia Function Caching. In Principles of Programming Languages 88, January 1988. [Pug88] William and the \nPugh. Incremental Computation Incremental Evaluation of Func\u00ad tional Programs. versity, 1988. PhD thesis, \nCornell Uni\u00ad  \n\t\t\t", "proc_id": "143165", "abstract": "", "authors": [{"name": "Vivek Nirkhe", "author_profile_id": "81100555468", "affiliation": "", "person_id": "P291860", "email_address": "", "orcid_id": ""}, {"name": "William Pugh", "author_profile_id": "81100057068", "affiliation": "", "person_id": "PP15020758", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143165.143223", "year": "1992", "article_id": "143223", "conference": "POPL", "title": "Partial evaluation of high-level imperative programming languages with applications in hard real-time systems", "url": "http://dl.acm.org/citation.cfm?id=143223"}