{"article_publication_date": "02-01-1992", "fulltext": "\n Garbage Collecting the World Bernard Lang* Christian Queinnect .Jos6 Piquer~ INRIA Rocquencourt Ecole \nPolytechnique Universidad de Chile &#38; INRIA Rocquencourt Abstract Distributed symbolic computations \ninvolve the existence of remote references allowing an object, local to a processor, to designate another \nobject located on another processor. To reclaim inaccessible objects is the non trivial task of a distributed \nGarbage Collector (GC). We present in this paper a new distributed GC algorithm which (i) is fault\u00adtolerant, \n(ii) is largely independent of how a processor garbage collects its own data space, (iii) does not need \ncentralized control nor global stop-the-world synchroniza\u00adtion, (iv) allows for multiple concurrent active \nGCS, (v) does not require to migrate objects from processor to pro\u00ad cessor and (vi) eventually reclaims \nall inaccessible objects including distributed cycles. These resnlts are mainly obtained through the \nconcept of a group of processors (or processes). Processors of a same group cooperate ltogether to a \nGC inside this group; this GC is conservative with respect to the outside of the group. A processor contributes \nto the global GC of all groups to which it belongs. Garbage collection on small groups reclaims quickly \nlocally distributed garbage clus\u00adters, while garbage collection on large groups ultimately reclaims widely \ndistributed garbage clusters, albeit more slowly. Groups can be reorganized dynamically, in partic\u00adular \nto tolerate failures of some member processors. These properties make the algorithm usable on very large \nand evolving networks of processors. Other than distributed symbolic computations, possible applications \ninclude for example distributed file or database systems. *IN RIA-Rocquencourt, Domain. de Voluceau, \nBP 105, 78153 Le Chesnay Cedex, France hug@margaux. inria .f r. This work has been partially supported \nby the Eureka Software Factory project. tLH (URA 1439), ficole Polytechnique, 91128 palaiseau Cedex, \nFrance queinnec@polyt echnique. f r. This work has been partially funded by Greco de Programma\u00adtion. \ni Departamento de la Computaci6n, Universidad de Chile, Casilla 2777, Santiago, Chile jpiquer@dcc. dcc \n.uchile. cl. Permission to copy without fee all or part of this material is granted provided that the \ncopies are not made or distributed for direct commercial advantage, the ACM copyright notice and the \ntitle of the publication and ita date appear, and notice is given that copying is by permission of the \nAssociation for Computing Machinery. To copy other\u00adwise, or to republish, requires a fee and/or specific \npermission. 1 Introduction Computations performed by collections of processors are more and more common \ntoday. Shared memory systems only allow for a limited number of proces\u00adsors. Some problems instead are \nclearly parallel and would benefit from greater and greater numbers of cooperating processors. This paper \npresents a new distributed Garbage Collection (GC) algorithm well suited for very large nets of possibly \nheterogeneous processors, even for a world-wide net, Our proposal can be roughly sketched as follows. \nAny processor manages its own data space with a lo\u00adcal GC. Remotely referenced objects have a reference \ncounter so that a large part of these objects can be easily deallocated whlen becoming inaccessible. \nPro\u00adcessors are organized into groups, The processors of a group cooperate to partial GCS global to the \ngroup: the aim of a group is therefore to discover and re\u00adclaim all unreachable distributed cycles of \nobjects in the group by means (of a concurrent mark-and-sweep collector. Multiple overlapping group GCS \ncan be si\u00admultaneously active. When a processor or a communi\u00adcation link fails to cooperate, the groups \nwithin which it lies are reorganized and continue their work. Even\u00ad tually all distributed clusters of \nunreachable objects will belong to a group that reclaims these clusters when it finishes its associated \nGC. Our algorithm has some other interesting proper\u00ad ties:  it supports failure of processors but is \nstill able to reclaim unused cells when located on working processors only. It also supports the addition \nof new processors, or changes in the network topol\u00adogy.  it does not need a centralized control: for \nin\u00adstance, a net may split into two disconnected sub\u00adnets each of which will continue to scavenge its \nown space. Multiple group GCS can be simulta\u00adneously active, each of which is focusing on a par\u00adticular \nsubnet where garbage must be reclaimed.  it can use any kind of tracing GC (mark-and\u00adsweep, copy, etc. \n) for local collections, provided this local GC transmits the marks used by the group GC from remote \nentry references to remote  @ 1992 ACM 089791-453-8192/000110039 $1.50 exit references. However no special \nbits for the group GC are required during the local collec\u00adtions. the detection and/or reclamation of \nunused cells does not require the migration of objects from processor coprocessor: the algorithm respect \nthe locality of objects as decided by the mutator. Our algorithm is robust and we think that it can \nbe used not only in the run-time library of distributed symbolic computation languages, but also by dis\u00adtributed \nfile systems or distributed database systems to reclaim unused files or objects. It is particularly attractive \nfor these systems since it makes weak as\u00adsumptions on their local GCS. We first establish the context \nand our terminology in section 2, and then describes in section 3 the work\u00ading of algorithm for a single \ngroup of network nodes, The problem of node failure and of possible recovery strategies from the point \nof view of both the GC algo\u00adrithm and the application program are then discussed in section 4. The next \nsection discuss ways of perform\u00ading cheaply several group collection at the same time. Some comparison \nwith related work and a conclusion end the paper. Terminology We consider a collection of nodes organized \ninto a net\u00adwork and communicating by exchange of messages. We call node a processor or a process on a \nproces\u00adsor able to manage its own memory space. Nodes may contain processes called mutators performing \nindepen\u00addent computations and allocating chunks of memory called cells, either for their own need or \nto serve other nodes. Cells may contain references to cells in the same or other nodes. Each node also \ncontains roots which are references to cells it considers useful. In par\u00adticular, all cell references \nknown by a mutator (e.g. in registers or in an execution stack) are roots. Cells ref\u00aderenced by a root \ndirectly or indirectly through other cells are said to be reachable or Jive. Other cells are said to \nbe unreachable or dead, and they constitute the garbage memory to be reclaimed by garbage collection \n(GC). A reference to a cell in the same node (as that where the reference is found) is said to be local, \nA ref\u00ad erence to a cell on another node is said to be remote. Garbage collection within a node on the \nbasis of local roots and local references is called a 10CU1 GC. A remote reference to a cell -y is represented \nby a reference to an exit item on the same node, which references an entry item on another node, which \nit\u00adself references locally the cell -y (see figure 1). To fetch the value of a remet e reference, three \nindirec\u00adtion and some communication time are required; it is of course hoped that the number of remote \nreferences is far outnumbered by the number of local references. Exit (resp. entry) items are immutable \nwith respect to the cells they refer to, and thus they can be safely shared: each node has only one exit \nitem for all remote references to a given cell, and only one entry item for each remet ely referenced \nlocal cell. A group GC is a non-local GC, i.e. a GC which involves more than one node. A group GC operates \non a group of nodes, and it reclaims any cell of the group that it can prove inaccessible from any root \nof any node. Note that entry or exit items are not part of the set of roots. The creation of a remote \nreference involves the synchronized creation of the associated entry and exit items. Node P 1 @p exit \nitem Node Q I remote edge Figure 1: Remote reference Entry items have a reference counter that is equal \nto the number of exit items referencing them (up to messages in transit). When an exit item is reclaimed, \na decrement message is sent to the counter of the en\u00adtry item it was referencing. If this decrement mes\u00adsage \nbrings its counter down to zero, the entry item is reclaimed too. This is the only available mecha\u00adnism \nto reclaim entry items; it is safe since non coop\u00aderative nodes (or nodes that are down) do not send \ndecrement messages and thus the cells they refer to cannot be reclaimed at all (without an external in\u00adtervention). \nReference counters along with increment or decrement messages do pose some problems in a distributed \nenvironment where no global time order exists. Nevertheless weighted reference count [Bev87], generational \nreference count [G0189] or indirect refer\u00adence count [Piq91] can, for instance, be safely used to maintain \nthese counters. The problem with reference counters is that they cannot reclaim dead cycles of cells \nsp arming several nodes. On each node, a local collector reclaims unreachable cells while the computation \nproper is done by a local mutatm-. The mutator and the collector can inter\u00adleave their work with a granularity \nranging from the usual stop-and-collect mode to the concurrent mode described in [DLM+ 78]. We restrict \nlocal GCS to be\u00adlong to the tracing family, following the terminology of [LD87]. Several nodes can form \na group in order to perform a GC global to this group. Such a group GC will reclaim unreachable cells \nand, in particular, the unreachable cycles that span nodes within the group. A group GC is partial unless \nthe group contains all possible nodes. Groups can overlap or form hierarchies and therefore can cent \nain smaller groups. Hierarchies are useful when gathering nodes that exhibit some sort of locality. This \nlocality may be for instance topological or geographic: a group can be defined to contain the different \nprocesses of a processor, the processors of a local area network, the networks of a country etc. The \nlocality may also be a logical one, for example based on the ratio of mutual remote references. Though \ngroups can be dynamically created to per\u00adform group GCS, we expect that preferred hierarchies of groups \ncan be foreseen, for example based on phys\u00adical neighbourhood (as above) to minimize communi\u00adcation problems. \nGroups are intended to gather nodes or groups that will cooperate without failure during a group GC. \nWe assume a fail-st op mode where a failing node (or link) ceases to communicate and does not fool other \nnodes in a byzantine way. If a node fails or, more gener\u00adally if a node does not want to cooperate, then \nthe groups to which it belongs can exclude it so that the work can be pursued on smaller groups without \nlos\u00ading what is already done. Any cycle passing through a non-cooperative node cannot be reclaimed at \nall, but dead cycles spanning only cooperative nodes in a same group will still be reclaimed by the group \nGC performed on this group, Some groups must be large enough so that long cycles can be recovered, but \nthe larger they are the longer the group GC takes, and the higher the probability that some node in the \ncy\u00adcles fails before the end of the group GC, However, if groups can be defined to exactly cover the \ncycles that are expected to be reclaimable, a global GC can thus be achieved on all nodes through independent \npartial group GCS. 3 The basic algorithm We consider a network of nodes containing cells, some of which \nare linked by remet e references. We first describe how a group maybe created, and how a global GC is \nperformed on the group. In section 5.2 we shall discuss in more detail the simultaneous execution of \nthis algorithm on several groups or on all of them. The steps listed here constitute the single group \nal\u00ad gorithm. They are further developed in the remainder of this section. 1. group negotiation: A node \nwanting to partici\u00adpate to a group GC sets up a group within which it will be performed. 2. initial \nmarking: All the entry items of nodes within the group are marked with respect to the group. The marks \non entry items depend on whether they are referenced from inside or from outside the group. 3. local \npropagation: Local GCS propagate the marks of the entry items towards the exit items. 4. global propagation: \nThe group GC propagates the marks of the exit items towards the entry items they reference, when within \nthe group. 5. stabilization: The preceding two steps are re\u00adpeated until marks of entry or exit items \nof the group no longer evolve. 6. dead cycles removal: The group GC breaks the unreachable cycles in \nthe group. 7. group disbanding: The work is now finished and the group may-be disbanded.  Recall that, \nas announced above, several group GC may actually take place simultaneously, for groups of varying sizes, \npossibly overlapping. The single group algorithm can be viewed as the cooperation of a variety of traditional \nGCS at various levels: local GCS can be any kind of tracing GCS, en\u00adtry or exit items are managed by \nmeans of reference count ers, and any dead cycle through these items is eventually broken by a global \nconcurrent mark-and\u00adsweep GC on a group encompassing the cycle. 3.1 Group negotiation When a node decides \nto participate to a new group GC, it inspects the other nodes to determine in co\u00adoperation with them \nwhat group can be set up for that purpose. There are multiple reasons to be in\u00advolved in a group lGC: \nthe node can be idle, or its entry items have not been accessed for a long time, or it is not currently \ninvolved in any group GC of some given size range, etc. The node is free to choose the kind of group \nit wants. It can either choose to form a small group with very close nodes or to undertake a major group \nGC with all nodes it is aware of. Let us suppose for now that the newly created group is composed of \nnodes only (we will explain later how to manage groups containing groups). The group is a set of nodes \nwilling to cooperate together until the end of the associated group GC. Once the group is created, there \nis no such thing as a leader of the group but all nodes of the group are aware of their co-members. The \ntechnique actually used for group formation is not essential to our algorithm. One simple way to im\u00ad \nplement group negckiation is to predefine a hierarchy of groups, as shown. in figure 2, based for instance \non neighbourhoo d criteria. Small dead clusters of cells Levelz Levell A*3 N%deA NodeB NodeC NodeD Figure \n2: Group hierarchy are quickly reclaimed by GCS on small groups, while larger ones are ultimately reclaimed \nby the slower GCS on large groups. If groups are predefine then group negotiation is an instance of a \ndistributed consensus algorithm [LS76, Gra78]. In order to distinguish messages relevant to a spe\u00adcific \ngroup, or a specific group GC cycle, a unique identifier is associated to each group GC cycle and is \nmade known to the nodes in the group. This can be done concurrently with the group creation and the initialization \nof marks for this group GC (cf. next section). This unique identifier is necessary to keep track of dynamic \nreconfiguration of groups during a cycle (for example because of a node failure), or sim\u00adply for the \nproper management of messages. Its role is actually dependent on the variant of our algorithm one chooses \nto implement, and on the need to handle some fault tolerance problems (e.g. late or disordered messages). \n 3.2 Initial marking Within each group, entry and exit items have a mark. A key point of our algorithm \nis that marks are local to groups i.e. are only meaningful with respect to a particular group GC. Different \ngroups may give different marks to a same (entry or exit) item. With respect to a group, an entry item \nmay be marked soft or hard, and an exit item may be marked none, SON or hard. These marks are strictly \nexclusive and can only be increased from none to hard during the local GC for exit items, and from soft \nto hard during the group GC for entry items. Initially, a hard mark for an entry item means that it is \nneeded outside the group , while a sojl mark means that the entry item is only referenced from in\u00ad side \nthe group (if at all). Later, a hard mark may also mean accessible from a root of a node in the group \n. The unused cycles that will be reclaimed in the end only include sojl marked items (i.e. items that \nare un\u00adreachable both from outside the group and from the roots in the group). The initial marks of the \nentry items of a group can be determined locally to this group by means of the reference counters: this \ntechnique was inspired by Christopher [Chr84]. Christopher s algorithm allows, given a group G and after \na computation local to G, to know precisely which (and how many times) en\u00adtry items are referenced from \noutside G. The initial marking protocol can thus be done in four successive steps: 1.on every node of \nthe group and for every entry item, a copy of the reference counter is created for this group. 2. on \nevery node of the group and for every exit item, if it references an entry item belonging to a node in \nthe same group, a decrement message is sent that will decrement the copy of the reference counter of \nthat entry item. 3. when all decrement messages generated during the previous step are received then \non every node of the group, any entry item that has a strictly positive copy of the reference counter \nis marked hard. Otherwise the copy of the reference counter is zero, the entry item is therefore only \nreferenced from inside the group and it is accordingly marked Sojl . 4. the space for the copies of \nthe reference counters is reclaimed (though we shall see that it may be useful to keep these copies). \n In practice, many groups may coexist and a node may belong to several groups. Marks and copies of refer\u00adence \ncount ers must be separate for each group. An entry or exit item data structure can contain these additional \nfields indexed by the group identifier. A classical termination detection algorithm has to detect that \n(z) no decrement message for this initial\u00adization is still in transit, and ( ii,) all nodes in the group \nhave sent their decrement messages. Group negoti\u00adation and initial marking can be partly combined. A \nrequest to a node for cooperation in a group GC may be accompanied by a bunch of decrement messages. \nOnce marks have been initialized, every node starts propagating them locally and globally as described \nby the next two sections. 3.3 Local propagation A group GC needs cooperation from local GCS. Local GCS \nare not time-constrained. A group GC simply waits for the local garbage collector to start a new local \ncycle, and thereby contribute to the group GC. Each local GC cyclle is followed by extra steps that propagate \nits results for the benefit of group GCS, as explained in section 3.4. A local GC cycle may be ini\u00adtiated \nby local computational need, or may be urged from an external source such as another node notic\u00ading it \nhas not contributed for a long time to some group GC. Local GCS are not required to be marking or copying \nGCS, they are only required to propagate the marks from the entry items to the exit items they locally \nreference, directly or indirectly. After a local GC on a node, any exit item locally accessible from \nan entry item (of the same node) is marked at least as hard as this entry item was at the beginning of \nthe local GC. A node is said to be stable w.r.t. a group GC when the propagation of its entry marks \nto the exit items by a local GC would not change the previously found marks on its exit items, and thus \nwould not contribute any new data to the group GC (cf. section 3.5). One simple way to achieve the propagation \nof entry marks is to have a two-phase mar-king for the local GC (see figure 3) Initially, all marks on \nexit items are reset to none. If the exit marks resulting from a previous local GC are to be kept (cf. \nsection 3.4), a copy is made in an appropriate location. then a first tracing is performed from the hard \nentry items and the internal roots. Any exit item reached by this tracing is marked hard. finally, a \nsecond tracing is performed starting from the sofi entry items. This second tracing completes the first, \ni.e., for example in the case of a marking algorithm, the marks of the first trac\u00ading are not removed. \nAny exit item reached by this second tracing is marked sojl, if it is not yet marked hard. After such \na locall GC cycle, the following is known: Tracing from sojl entry items is conservative since it is \nnot yet known whether the entry items are useful or not w.r.t. the whole network. Cells that have not \nbeen visited at all are not reachable ei\u00adther from outside the node (trough entry items) or from the \nlocal roots. They can thus be reclaimed safely (this is implicit in case of a copying algo\u00adrithm). The \nsame is true of exit items that are still marked none. They can be safely reclaimed, though the entries \nthey reference must be informed (see be\u00adlow).  Exit items mar ked hard are reachable either from a hard \nmarked entry or from a local root. Hence they are either reachable from a root within the group or referenced \nfrom outside the group (up to floating garlbage, i.e. the reference may have disappeared sinlce the beginning \nof the group GC cycle). The same is of course true of the entry items referenced by these exit items. \n Exit items marked soft carry no new mark\u00ading/usefulness information w .r.t. the group GC, since the \nentries they reference are at least already marked SOD.  Only a single bit mi>rk is needed for marking \nthe cells within the node, to indicate whether they are (conser\u00advatively) live according to the local \nGC. The distinc\u00adtion between soft and hard is done by separating the two phases, so that it can be propagated \nfrom entry items to exit items, where the two kind of marks are physically distinct. soft Hard \\ I\\Ii \nI \\ / ROOLS Node \\l &#38;w None soft Had Figure 3: Two-phase marking in a local GC When an exit item \nis reclaimed, a decrement mes\u00adsage is sent to its associated entry item, which is then reclaimed in turn \nif its reference count reaches zero, even if it is marked hard. In summary, note that (i) the two-phase \nmarking may be performed by any kind of tracing GC , (ii) the work required by the two-phase marking \nis equivalent to that of traditional tracing GC except for the order of exploration of cells, 1The tracing \nGC must actually trace all the local cells in order to properly prop,agat e marks. This is not done by \ngen\u00aderational GCS [LH83]. In their case, a simple approach tO ~\u00adderstanding the problem is to consider \ngenerations as distinct nodes, and then to adapt the techniques presented here. In the case of a strictly \nhieramhical organization of groups (see sec\u00adtion 5.2), it may be cc,nvenient to privilege generational \nprox\u00adimit y over geographical proximity for organizing the groups (e.g. by grouping together young generations \nof difTerent nodes). Thk has not been pursued in depth by the authors. (iii) no extra bits are required \non local cells to record the global marks, only entry or exit items differential e between them. Indeed, \nthe two-phase marking may be performed by a copying collector that does not use marks at all. It is possible \nto use a concurrent local GC [Wad76, KS77, DLM+78) Bak78, Yua90], but not a page\u00adoriented one as in [BDS91]. \nThe only difference is that the mutator has to cooperate with the collector by marking some of the cells \nit touches. The trick is to ask the mutator to always do hard marking. During the hard phase, this works \nlike an incremental collector. At the end of the hard marking phase, all cells that the mutator can access \nin the future (except for cells it will create and mark hard) are already marked hard. Hence, during \nthe soft phase, continued cooperation from the mutator (still a hard marker) will only mark already hard \nmarked cells, and this will be totally in\u00adnocuous though with some time overhead due to this useless \nmarking. Extra floating garbage [Wad76] may also be produced because the exploration order of the two-phase \nmarking may not be optimal. Note however that the working of the algorithm with concurrent col\u00adlectors \nis actually very complex to analyze because of incoming messages and interactions with mutators on other \nnodes.  3.4 Global propagation When an exit item is known to be hard i.e. ac\u00adcessible from a local \nroot or referenced by at least one exit item not belonging to the group, its mark has to be propagated \nto the entry item it references whenever it belongs to the group under consideration. It is not mandatory \nto send this information immedi\u00adately, it may be batched with other messages to lessen transmission cost. \nConversely, it is not necessary to wait till the end of the local collection before propa\u00adgating a hard \nmark, and early propagation may speed up the termination of the group GC. The propagation of a hard mark \nfrom an exit item can only harden the mark of the associated entry item. Note that once an exit item \nhas been marked hard with respect to a group, remembering this mark will avoid sending fur\u00adther hardening \nmessage from that exit item and with respect to that group. Hence it may be useful to pre\u00adserve the marks \npreviously found for exit items w .r. t. each group GC. A local GC may work for one or several group \nGC at a time. When it finishes a local cycle, it is free to start a new one, provided the hard marks \non exit items have been propagated or preserved for later propagation. This new cycle may be for purely \nlocal use, or also for the same group(s) and/or other groups. When a new remote reference is created, \nthe asso\u00adciated entry item is marked hard since it is (will be) necessarily accessible from a root of \nthe node for which it is created, and to which this remote reference will be sentz.  3.5 Stabilization \nGlobal propagation of marks is finished when all en\u00adtry items of the nodes participating in the group \nGC are marked hard whenever they are reachable from a cell outside the group or from a root belonging \nto the group. Note that is a conservative requirement: some hard marks may be due to past reachability \nthat has disappeared since the beginning of the group GC cycle (floating garbage). Such a situation, \ncalled group stability is reached when All nodes are stable, i.e. they have no new data that could justify \nhardening more entry items lo\u00adcally or elsewhere within the group. e There are no messages in transit \nthat request the hardening of some e-ntry item. A node is stable if it has propagated all the (relevant) \nhard marking data it is aware of to the other nodes in the group. This is normally done after a local \nGC. New hard marking data comes mainly as requests to mark hard an entry item previously marked sojl \nw.r.t. to the current group GC. When this occurs, the node reverts to (or remains in) a non stable state \nuntil the marking has been propagated to the exit items by a local GC and (new) hard marks on exit items \nhave been propagated to the other nodes of the group. The stability of a node may also be lost when a \nnew entry item is created on this node. Though the cell referenced by the new entry item must have been \nlive, this fact may not yet have been discovered by the group GC, and there is the possibility that the \npath that formerly kept it alive will be severed before hard marking may be propagated to it. Finally, \nstability may be lost when a cell immigrates from another node. Though this cell is obviously live, the \nknowledge of this liveness may not have been yet propagated to the local cells or the local exit items \nit references. Assuming that all messages arrive3, group stabil\u00ad ity must be reached since marks can \nonly increase from none to hard and the total number of entry or 2The creation of a remote reference \nrequires some care to keep a consistent reference count on the new entry item, while its intended recipient \nhas not yet received (and acknowledged) a reference to it. This problem, as well as other problems related \nto protocols to preserve the consistency of distributed reference counts have been discussed in other \nproposals. They are not considered further in this paper. 3Fault-tolerance w .r, t. message behaviour \nis not discussed in this paper. exit items is bounded by the total number of possi\u00adble cells that can \nbe allocated in the nodes of the group. Moreover any entry item created during the group GC is marked \nhard. Group stability can be detected by any distributed termination detection al\u00adgorithm when no node \nfailure occurs. Examples of such algorithms in the area of garbage collection may be found in [HK82, \nAug87, Der90]. The handling of node failure is discussed in section 4. 3.6 Dead cycles removal After \nstabilization, all entry items that are directly or indirectly accessible from a root or from a node \nout\u00adside the group are marked hard. Entry items marked sofi can only be part of inaccessible cycles local \nto the group and can thus be safely reclaimed. SOY entry items can be independently reclaimed by each \nnode of the group without group synchronization. This is gracefully achieved by relying on the reference \ncount\u00ading mechanisms. When group stability is detected, each node in the group modifies its soft entry \nitems to now reference nil rather than a local cell. This mutation is safe since these entry items are \ndead. The former offsprings of these entry items, not otherwise accessible, will be re\u00adclaimed by the \nnext local GC. Similarly the exit items that were kept alive exclusively by these entries will be reclaimed \nby the next local GC. The reclamation of such an exit item causes the sending of a decrement message \nto the entry item it references. In the case of dead loops, (dead) entry items on the loop eventually \nreceive decrement messages from all the (dead) exit items that reference them. Hence their reference \ncoun\u00adters decrease to O and they are eventually reclaimed by the normal reference counting mechanism. \nThis protocol achieves a delayed reclamation instead of a synchronized deletion of useless cells. The \nlatter is difficult, since we cannot brutally reclaim the useless exit items which are still referenced \nfrom local cells, and since we cannot either reclaim the useless entry items because the y are still \nreferenced by exit items. Observe that our algorithm is independent of the nature of inaccessible cell \nclusters: they can be simple cycles or more complex cycles entangled with subcy\u00adcles, etc. Occasional \ny the y may even be non cyclic structures, t bough these are often reclaimed earlier through the reclamation \nof exit items marked none after a local GC. All inaccessible entry items are re\u00adclaimed and no heuristics \nis needed to identify pot en\u00adtial cycles. Also note that the deallocation mechanism of entry items is \nunique and only based on reference counters. Similarly the deallocation of exit items is only a consequence \nof local GCS. The removal of dead cycles is effectively obtained by cutting all the remote edges of these \ncycles. 3.7 Group disbanding When a group GC is finished, its associated group may be disbanded, though \nit is often convenient to keep the same groups fcm further group GC. Marks (and possibly other data structures) \nrelative to this group can then be reclaimed. If the delayed protocol of the previous step is used, this \nreclamation can take place as soon as it finishes mutating to nil its SOB entry items.  4 Failure If \na node fails, i.e. ceases to cooperate, then we assume that this will be detected by some of the other \nnodes of the group, for example those nodes which precisely re\u00adquire cooperation. For that detection, \nmessages with acknowledgements and time-out can be used. A node which detects such a failure may choose \nbetween sev\u00aderal non-exclusive options: It can decide that this failure is probably only temporary and \nwaits for the failed node to wake up. This only slows down the work of some groups to which the silent \nnode belongs.  It can reorganize the group i.e. create a new group excluding the failing node (and all \nother nodes with which communication is now impossible if, for example, relayed by the failing node). \n  The simplest method to reorganize the group is to build a new group, subgroup of the failed group, \nand to restart from scratch a new group GC on that sub\u00adgroup. However the existing information already \ngath\u00adered by the failed group can be used to initialize the new group. In this case the hard-marks are \nkept and transferred as hard-marks for the new group. The marking initialization procedure of section \n3.2 is used only to promote fro,m soft to hard all entry items ac\u00adcessible from nodes that have been \nexcluded from the new group. The group GC on the new subgroup is then resumed and since it starts from \nmore advanced marking informaticm, it can stabilize more quickly. However to start from marks inherited \nfrom the failed group may increase the ratio of hard-marked floating garbage. The failed group may be \naborted immediately. It may also be preserved for some time in the hope that the failure is temporary \nand the group GC can be resumed later, or until the failure is definitely known as non-recoverable. A \ntransmission link may fail and divide a group into disconnected subgroups. These subgroups will then \nreorganize themselves independently, and they will re\u00adsume independent parts of the group GC that will \nre\u00adclaim all cycles that are local to each of the reorganized subgroup. Of course, cycles spanning through \nmissing links will not be reclaimed by these partial group GCS. Note that, in all cases, the cells that \nwere reachable from the failed node will never be reclaimed, since the reference counts of the entry \nitems referenced from the failed node can no longer decrease to O. Hence, even if the group GC is aborted, \nno remote data used by the failing node will have disappeared when it resumes operations. When a node \nhas a non-recov erable failure, three types of problems may have to be considered: 1. what happens to \ncells referenced by the failed node? 2. what is to be done of remote references to cells in the failed \nnode? 3. what is to be done of cells in the failed node that can be recovered by external means (e.g. \nlocal op\u00aderator intervention, or known replication on dif\u00adferent nodes)?  Answers to the second question \nare essentially the re\u00adsponsibility of the application. However, if some cells can be recovered in the \nfailed node together with their network identity (the corresponding entry item), it may be useful to \nhave a mean to migrate these cells to another node which is made known to the network so that corresponding \nexit items may be updated. A similar action may be first taken when the death of the node has to be decided \nby an external agent4. The first question is the more interesting one for our algorithm. References from \nthe failed node may be determined by Christopher s technique, provided a reference count of references \ninternal to the group has been kept up-to-date on all entry items of the group5. Let G be the original \ngroup, N the failing node, and G the group G less the node N. We can run Christo\u00adpher s algorithm on \nG to obtain for the entries in G the count of references from within G . By taking the difference with \nthe reference counts w.r.t. group G, we can determine which entries in G were being refer\u00adenced by the \nfailing node N (or any number of failing nodes taken together). It is then possible for a net\u00adwork or \napplication administrator (either a program or a human being) to take appropriate action with re\u00adspect \nto these entry items and the cells they access. Decrement messages may be sent to the concerned en\u00adtries \nto consummate the death of the references from the failing node6. Alternatively the administrate or may \n4 Migration with reference counter is taken into account by [Piq91]. 5 Maintaining group based reference \ncounts is also useful to initialize the marking of entry items when a new GC cycle is decided for the \nsame group. Then Christopher s algoritlun need be used only when creating a new group. 6 This must be \nremembered to inform the failing node, when there is a chance of later partial or total recovery, so \nas to avoid decide that the cells that were accessed by the failing node contain important data that \nshould not be al\u00adlowed to die. In that case he or it may decide to create new live references to these \ncells. Note that the recovery facility described above re\u00adquires keeping up-to-date the internal reference \ncounts for each entry w.r.t. each group it belongs to. This en\u00adtails both a space overhead on each entry, \nand a time overhead since several reference counts have to be up\u00addated whenever a remote reference is \nlost or created. The hierarchical scheme proposed in the next section will answer both these concerns. \n 5 Simultaneous group collec\u00adt ions 5.1 Contention between group GCS Our algorithm makes use of local \nGCS to perform parts of a group GC. In other words, a group GC delegates to a local GC the propagation \nof the marks belonging to the group GC. This situation could be replicated at other levels, and in particular \na subgroup G of a group G could be considered as a single node from the point of view of the collection \nin group G. While this could possibly be useful in some cases (e.g. large variations in network connectivity \nand/or communi\u00adcation speed), it seems not to be generally advisable. A first reason is that entry items \nw.r.t. a subgroup G are still to be located on nodes. Though they can be the same objects as node entry \nitems, not all node entry items are subgroup entry items since some re\u00admote references may be fully local \nto the subgroup. This entails additional management overhead for en\u00adtries. Similarly, exit items have \nto be created for the subgroup G and kept till the end of the group GC for G , i.e. much longer than \nthe duration of a local GC on a single node. It is instead much simpler and more efficient to con\u00adsider \nthat several group GCS take place simultane\u00adously, and that a local GC on a node can contribute to several \nof them. Entry and exit items can then be common to all group GCS, though they must keep separate marks \n(and possibly separate group reference counts cf. section 4) for each group. If we consider as above \na group G and a subgroup G of G, a hard mark w.r.t. G on an entry item means that it is accessible from \nsome root in G or from cell outside G. Hence this entry item must also be marked hard w.r.t. G since \nit must be also accessible either from a root in G or from outside G (because G is included in G). Hence, \nany local collection that con\u00ad dangling references. Such a mechanism does not properly be\u00adlong to the \ngarbage collection algorithm, but it must be available to allow the application programs to handle this \nsituation, tributes to the group GC of G can also contribute to the group GC of G by propagating a hard \nmark w.r.t. G to every entry item for which it does it w.r.t. G. However the hard marking phase of a \nlocal GC w.r.t. G is not a complete one for G since some entry items may be marked hard w.r.t. G{ and \nsoft w.r.t. G. Thus if a local GC works for for the group GC of G, this may slow down the progression \nof the group GC for G . Conversely, if a local GC works for for the group GC of G , its hard marking \ncannot be used at all for the group GC of G, which is even more slowed down. Contention between two embedded \ngroups is far from a clear-cut issue. Each slows the other down, but not to the same extent. Smaller \ngroups can termi\u00adnate faster, with less floating garbage, but they recover less storage and never the \nlarge dead cycles, and they do not contribute to the group GCS of larger embed\u00adding groups. Conversely, \nlarger groups may take much longer to terminate, thus leave more floating garbage, but they usually recover \nmore storage and only they can recover larger dead cycles of cells. Additionally a large group GC contributes \nto some extent to the GCS of smaller embedded groups, and when a group GC on a larger group finishes \nits work, it reclaims all its dead internal storage, thereby also completing the work of all its subgroups \n(up to floating garbage). The situation is simply worse when a node belongs to two overlapping groups \n(i.e. groups with a non triv\u00adial common intersection). Then the speed of the two group GCS is halved \non the average since the local GC can work only for either group, as there is no simple relation between \nthe marks for the two group GCS. This discussion hints first at the necessity of having a strictly hierarchical \nembedding of groups (no par\u00ad tial overlap) to avoid group contention over the ser\u00ad vices of the local \nGCS. We show in section 5.2 how to deal with the contention problem in the case of embedded groups, and \nthus have each local GC of a node contribute to the group GCS of all groups to which the node belongs. \nHence all GCS can terminate faster, dead cells are recovered earlier, and less floating garbage is produced. \n 5.2 Hierarchical cooperation of group GCS We now consider that all groups are organized in a strictly \nhierarchical order by inclusion. If two group overlap, then one cent ains the other. We further as\u00ad sume \nthat there is one group cent aining all the node in the network, which we call the universal group. Each \ngroup can then be assigned a levei index which is the number of groups it is strictly embedded in. Hence \nthe universal group has level O. Note that the level of a group may change as groups embedding it may \nbe destroyed or newly crested. This problem must be taken in consideration when implementing the algorithm \nbelow since it uses level related data. At any given time, on a given node, group levels uniquely identify \nthe groups that contain the node. Each node will keep a table rel ating the unique identifier of each \ncontaining group to its group level. Our objective is to have each local GC contribute precisely to \nthe marking of entries w.r.t. to the group GCS of all groups containing the node. We have seen in the \nprevious section that when an entry is marked hard w.r.t. some group G, it can also be marked hard w.r.t. \nany subgroup G of G. Hence, for an entry x in a node IV, we can de-fine a marking level Mark$ (N) as \nthe least level (i.e. the level of the largest group) such that the entry z is marked hard. Similar marking \nlevels can be used cm exit items. Remember that a iower marking level actually means more hard marks \nsince level indexes are in reverse inclusion order. Now, instead of propagating an ordered binary marking \n(hard or Sojft) from entry items to exit items with a two phase tracing algorithm, the local GC must \npropagate an integer marking by means of a multi\u00adphase tracing algorithm (or any equivalent algorithm). \nMulti-phase marking is performed by propagating the Marko of entries in increasing order, so that each \nexit item get the lowest marking level of all entries that can reach it. The tracing from the roots is \ndone with the O-level trace and thus marks exit items hard w.r.t. all groups. This multiphase tracing \nis very similar to the time-stamp propagation used in Hughes algo\u00adrithm [Hug85]. Some care has to be \nexercised with the multiphase tracing (and with the 2-phase trac\u00ading as well) because the marking level \nof some entries could decrease while it is taking place, and these en\u00adtries should not be skipped. Since \nlevels are meaningful only locally to a node, the marking levels on exit items are changed to the unique \nidentifier of the corresponding group before they are sent to the node they reference. This unique identifier \ncan be turned back into a local marking level upon arrival. Stability is also dstected by means of group \nlevels. A node is stable w,r.t. level 1 when it has not re\u00adceived any data that could justify decreasing \nthe for\u00admer marking level of some exit item down to or below ~ (cf. section 3.5). When global group stability \nis known at level / on a node, all entry items with a level marking higher than 1 may be reset to nil \n(cf. section 3.6). Recall that this may not be true at level 1 for all nodes, since their level 1 group \nmaybe different. level). For the next GC cycle in this group, marking levels of entry items must be reinitialized. \nWhenever, for a given entry, the count of references external to the group is non-zero, the marking level \nof that entry item is set to the group level, unless it is already smaller. Multiphase tracing can be \nextended to concurrent node iV for the level 1 group (i.e. the number of ref\u00adlocal collectors (as it \ncan be in the case of Hughes erences from inside that group). However this entails collector). As explained \nin section 3.3, the mutator a significant overhead, both in space for keeping the can just act as a O-level \nmarker. Actually it just con-counts, and in time for updating all of them. tributes to the tracing in \nthe usual way, of parallel Instead we propose to keep the usual global refer\u00adcollectors. This contribution \nis naturally taken as O-ence count for the whole network, and for each of the level one during the O-level \nphase (the first phase), and other groups a difference count which is defined as fol\u00adis useless but innocuous \nduring the later phases. lows: Vi ~ 1, Difl~[i, ~] = CounfN[i 1,x] CountN[i, x] However, there is one \ndrawback to hierarchical coop\u00aderat ion of GCS as described above. The assertion that This has several \nadvantages: an entry item that is marked hard w.r.t a group G has to be hard w.r.t. any subgroup G of \nG is Difference counts are very convenient for initial\u00adnot quite correct because of the existence of \nfloating izing marking levels. The initial marking level is garbage. The entry items on a dead cycle \nmay have a the smallest i such that DzflN [i, x]# O, or O if they low marking level because they were \nformerly reach\u00ad are all zero. However, during the reinitialization able in some large group G. This reachability \nmay of marking levels for the next GC cycle of a group, have disappeared, but the marking remains low \nuntil it may be that the marking level of entry item z the end of the GC cycle for group G, which make \ntake is already smaller, in which case it is not changed a long time since the group G is large. During \nall that (cf. section 5.2). time, the GC of any subgroup G containing the cycle It requires less updates: \nwhen counts have to be will have to assume that this dead cycle is live, be\u00adupdated (incremented or \ndecrement ed) only the cause the marking level mechanism will keep the low global reference count and \none difference count marking level of G on the entry items of the cycle. have to be modified. Hence, \nif completing GC cycles on large groups takes much longer than on small ones, it may be Difference counts \nare usually much smaller, often advisable to occasionally suspend the work on the equal to O. Hence it \nis easier to use storage-saving larger groups, and perform hierarchical GC only for techniques such as \n1 or 2 bits reference counts the smaller groups to avoid the above effect 7. After\u00ad [DB76, WF77]. wards, \nhierarchical GC on all groups can be resumed from its saved state. Remember though that this prob\u00adlem \nconcerns only dead cycles since other garbage is 6 Related works always reclaimed by the reference count \nmechanism. Our algorithm, like many others, is based on the con\u00adcept of multi-area collection which was \npioneered by 5.3 Keeping group reference counts Bishop [Bis77]. Distributed [Hug85, LL86, Rud86, Bev87, \nG0189, Der90, Piq91] or fault-tolerant [Ves87,Keeping reference counts up-to-date w.r.t. each group Sch89, \nSGP90] or real-time [Bak78, QBQ89, Yua90, can be useful for two reason: Bak91] or concurrent GCS [KS77, \nDLM+78, HK82, vdS87] (among otherss) have been studied for long. When groups do not change, it allows \na faster To mix increment and decrement messages through initialization of the marks on entry items (thus \nasynchronous communicant ion links to maintain refer\u00ad avoiding the cost of Christopher s algorithm). \nence count ers raises some difficulties. However elegant It is necessary to enable the recovery procedures \nsolutions [Bev87, Go189, Piq91] have been proposed described in section 4 based on variants of reference \ncounters that avoid the need of increment messages used by traditional ref-One obvious solution is just \nto keep them all in the erence count ers. Their schemes have various proper\u00adentry items, and update them \nwhen the number of ties but the generational reference counting scheme references change. In the case \nof a strictly hierar-of Goldberg allows a node to simply obtain the total chical organization of groups, \none can use an array numb er of references on entry items. This is useful CountN [i, x] giving the reference \ncount of entry x of when initializing the marks of a group. Liskov and Ladin [LL86] is one of the first \npublished 7 Actually new GC cycles may be started on the full hierarchy fault-tolerant distributed GC. \nThe graph of remote with no additional cost. However, for the larger groups, these references is reconstructed \non a single node and scav- G C cycles should not be terminated. Their partial results, i.e. the markings \nobtained, should simply be merged with those of enged there. The results are sent back to all partici\u00adthe \nsuspended G Cs of the corresponding groups when the latter pating nodes which can then erase useless \nitems. This are resumed. The merging on each entry is done by taking the minimum of the marking levels \nattained by both GCS. 8Some of these algorithms belong to more than one category. induces a logical \nsynchronization of all nodes; this syn\u00adchronization is made safe through assumptions on de\u00adlays and global \ntime but is not scalable to large num\u00adbers of nodes. Fault-tolerance is only with respect to the failure \nof the central service but does not seem to account for failures of normal nodes involved in the group \nGC. The algorithm makes very strong assump\u00adtions on the ability of the local GCS to detect con\u00adnectivity \ny between entry and exit items without giving references to an actual algorithm satisfying these con\u00adstraints. \nHudak and Keller proposed a real-time distributed GC in [HK82]. Their algorithm performs a group GC on \nthe whole space, operates in real-time and is fur\u00adthermore able to reclaim irrelevant t asks. On the \nother hand, it is not fault-tolerant, imposes extra fields on every cell, and requires local mut at ors \nto strongly co\u00adoperate. Except for the reclamation of tasks, our al\u00adgorithm can simulate theirs: a single \ngroup exists con\u00adt aining all objects, and any cell is considered to forma node by itself. Since there \nis a single group, reference count ers are no longer useful and can be abandoned as well as the initial \nmarking which is useless since there is nothing outside the group. Rudalics presented a real-time distributed \nGC in [Rud86]. It is strongly based on a copying local GC and tolerates out-of-order message transmission. \nIt operates on the whole space and is not fault-tolerant, on the other hand it propagates global marks \nfaster than ours. In [Hug85], Hughes proposed an algorithm to achieve a global GC. His algorithm is not \nfault-tolerant but separates well local and global GCS. His basic al\u00adgorithm is similar to ours, but \nwith a single group en\u00adcompassing the whole network. On the other hand, his use of time-stamps allows \nhim to achieve at low cost simultaneous global collections shifted in time. This keeps a continuous flow \nof freed memory, and does not let garbage float for very long. The drawback is that completing a GC cycle \non a very large network still takes very long, and that is the time needed to actu\u00adally reclaim garbage \ncells (it takes roughly half a GC cycle on the average). Our hierarchical algorithm also uses strictly \nordered stamps, but they indicate spa\u00adtial multiplexing rather than temporal multiplexing of global collectors. \nThough we lose on the frequency of network-global GCS, and hence on the freeing of very large cyclic \nclusters, we have faster and more fre\u00adquent GCS for small groups, and can reclaim quickly clusters with \ngood locality. In addition, this spatial multiplexing is the basis for the ability of our algo\u00ad rithm \nto tolerate and recover from failures. Hence we believe our algorithm to be more scalable to very large \nnetworks. we were unable to compare our algorithm with that of [Sch89] which we do not understand enough \nfrom his paper. Shapiro, Gruber and Plainfoss6 [SGP90] presented an extension of Vestal [Ves87] with \nmany improve\u00adments and refined prckocols to deal with various types of failures. They considers a single \nglobal group, and reference counters of their entry items are represented by the list of referencing \nnodes (for better tolerance to data transmission faults). At the end of a local col\u00adlection, sofl entry \nitelms as well as their sofi local off\u00adspring are migrated towards other nodes so that cycles will end \nin a single node and will be reclaimed there (our algorithm does not need to migrate objects). The ability \nof their algorithm to handle various types of communication failures can be used in the reference count \npart of our algorithm. Its tolerance to dupli\u00adcated messages would in particular increase the re\u00adsilience \nof our algorithm against nodes where it could be erroneously progri~mmed, for example in the case of \nerroneous sending of decrement messages.   7 Conclusions This paper gives the principle of a distributed \ngarbage collector that can collect all garbage, and in particular cyclic garbage with an efficiency proportional \nto the locality of the structures to be reclaimed. It has min\u00adimal interaction with the computing processes \n(rout a\u00adtors) and uses little synchronization. Its hierarchical structure makes it potentially usable \nfor very large distributed systems. Though we implicitly rely on many techniques that have been adequately \ndeveloped in the context of other algorithms, it is clear that many points still need to be made more \nprecise, m~ore than was possible within the limits of this paper, In particular, several options and \ndesign choices have been left open. Further choices and refinements should depend on the characteristics \nof the network and (most importantly) of message transmission, the type of application programs con\u00adsidered \n(e.g. importimce and size of cycles, ratio of remet e references, size of cells, etc.), and on exper\u00adiment \nal results and measurements. Several aspects also need further analysis and/or proof, for example the \nuse of concurrent collectors in local collections. Bibliography [Aug87] Lex Augusteijn. Garbage collection \nin a dis\u00adtributed environment. In PARLE 87 -Parallel Architectures and Languages Europe, pages 75-93. \nLecture Notes in Computer Science 259, Springer-Verlag, June 198 7. [Bak78] Henry G Baker. List processing \nin real time on a seriaJ computer. Communications of the A CM, 21(4):280-294, April 1978. [Bak91] Henry \nG Baker. Cantabile immobile real-time SIGPLAN 87- Symposium on Interpreters and In\u00ad garbage collection \nwithout motion sickness. @1991 terpretive Techniques, pages 253 263, Saint Paul, Nimble Computer Corporation, \n1991. MA, 1987. [BDS91] Hans J Boehm, Alan Demers, and Scott Shenker. [LH83] Henry Lieberman and Carl \nHewitt. A real-time Mostly parallel garbage collection. In PLDI 91 garbage collector based on the lifetimes \nof objects. -A CM SIGPLAN Programming Languages Design Communications of the ACM, 26(6):419-429, June \nand Implementation, pages 157 164, Toronto (On-1983. tario, Canada), June 1991. SIGPLAN Notices Vol \n[LL86] Barbara Liskov and Rivka Ladin. Highly-available 26 N 6. distributed services and fault-tolerant \ndistributed [Bev87] D I Bevan. Distributed garbage collection using garbage collection. In PODC 86-Proceedings \nof reference counting. In PARLE 87 Parallel Ar-the 5th Symposium on the Principles of Distributed chitectures \nand Languages Europe, pages 176 187. Computing, pages 29-39, Vancouver (Canada)j Au-Lecture Notes in \nComputer Science 2591 Springer-gust 1986. Verlag, June 1987. [LS76] B Lampson and H Sturgis. Crash recovery \nin a [Bis77] Peter Bishop. Computer Systems With a Very distributed data storage system. Technical report, \nLarge Address Space and Garbage Collection. PhD Palo Alto Research Center, Xerox, Palo Alto, CA, thesis, \nMIT, May 1977. 1976. [Chr84] Thomas W Christopher. Reference count [Piq91] JOS6 Miguel Piquer. Indirect \nreference count\u00adgarbage collection. Soflware-Practice and Experi-ing: A distributed garbage collection \nalgorithm. In ence, 14(6):503 507, June 1984. PARLE 91 -Parallel Architectures and Languages Europe, \npages 150 165. Lecture Notes in Computer[DB76] Peter L. Deutsch and Daniel G. Bobrow. An effi-Science \n505, Springer-Verlag, June 1991. cient incremental automatic garbage collector. Com\u00admunications of the \nACM, 19(9):522 526, September [QBQ89] Christian Queinnec, Barbara Beaudoing, and 1976. Jean-Pierre Queille. \nMark DURING sweep rather than mark THEN sweep. In PARLE 89 Par\u00ad[Der90] Margaret H Derbyshire. Mark scan \ngarbage col\u00adallel Architectures and Languages Europe. Lecture lection on a distributed architecture. \nLisp and Sym-Notes in Computer Science 365, Springer-Verlag,bolic Computation, 3(2):135-170, April 1990. \n June 1989. [DLM+ 78] Edsger W Dijkstra, Leslie Lamport, A J Mar\u00ad[Rud86] Martin Rudalics. Distributed \ncopying garbagetins, C S Scholten, and E F M Steffens. On-the-fly collection. In ACM Symposium on Lisp \nand Func\u00adgarbage collection: An exercise in cooperation. Com\u00adtional Programming, pages 364-372, Cambridge,munications \nof the ACM, 21(11):966 975, November MA, 1986. 1978. [Sch89] Marcel Schelvis. Incremental distribution \nof [G0189] Benjamin Goldberg. Generational reference timestamp packets: A new approach to distributed \ncounting: A reduced-communication distributed garbage collection. In Object-Oriented Programmingstorage \nreclamation scheme. In ACM SIGPLAN Systems and LAnguages, pages 37 48, October 1989. Programming Languages \nDesign and Implementa\u00adtion, pages 313-321, Portland (OR), June 1989. [SGP90] Marc Shapiro, Olivier Gruber, \nand David Plain\u00adfoss&#38; A garbage detection protocol for a realistic [Gra78] J N Gray. Notes on database \noperating systems. distributed object-support system. Research ReportIn R Bayer et al., editor, Operating \nSystems: An 1320, INRIA Rocquencourt, November 1990. Advanced Course, pages 394 481. Lecture Notes in \nComputer Science 60, Springer-Verlag, 1978. [vdS87] Jan L A van de Snepscheut. (algorithms for on\u00adthe-fly \ngarbage collection revisited. Information [HK82] Paul Hudak and Robert Keller. Garbage col-Processing \nLetters, 24:211-216, March 1987. lection and task deletion in distributed applicative processing systems. \nIn ACM Symposium on Lisp [Ves87] Stephen C Vestal. Garbage Collection: An Ex\u00adand Functional Programming, \npages 168 178, Au-ercise in Distributed, Fault-Tolerant Programming. gust 1982. PhD thesis, Washington \nUniversity, January 1987. [Hug85] John Hughes. A distributed garbage collection. [Wad76] Phil L. Wadler. \nAnalysis of an algorithm for In Functional Programming and Computer Architec-real time garbage collection. \nCommunications of the ture, pages 256 272. Lecture Notes in Computer Sci-ACM, 19(9):491-500, September \n1976. ence 201, Springer-Verlag, September 1985. [WF77] David S. Wise and Daniel P. Friedman. The one\u00ad[KS77] \nH T Kung and S W Song. An efficient garbage bit reference count. BIT, 17(3):351-359, 1977. collection \nsystem and its correctness proof. In Pro\u00ad ~ua90] Taiichi Yuasa. Real-time garbage collection on ceedings \nof IEEE 18th Symposium on Foundation of general-purpose machines. Journal of System Soft- Computer Science, \npages 120-131, Providence (RI), ware, 11:181 198, 1990. October 1977. [LD87] Bernard Lang and Francis \nDupont. Incremen\u00adtal incrementally compacting garbage collection. In \n\t\t\t", "proc_id": "143165", "abstract": "<p>Distributed symbolic computations involve the existence of <italic>remote references</italic> allowing an object, local to a processor, to designate another object located on another processor. To reclaim inaccessible objects is the non trivial task of a distributed Garbage Collector (GC). We present in this paper a new distributed GC algorithm which <italic>(i)</italic> is fault-tolerant, <italic>(ii)</italic> is largely independent of how a processor garbage collects its own data space, <italic>(iii)</italic> does not need centralized control nor global stop-the-world synchronization, <italic>(iv)</italic> allows for multiple concurrent active GCs, <italic>(v)</italic> does not require to migrate objects from processor to processor and <italic>(vi)</italic> eventually reclaims all  inaccessible objects including distributed cycles.</p><p>These results are mainly obtained through the concept of a <italic>group</italic> of processors (or processes). Processors of a same group cooperate together to a GC inside this group; this GC is conservative with respect to the outside of the group. A processor contributes to the global GC of all groups to which it belongs. Garbage collection on small groups reclaims quickly locally distributed garbage clusters, while garbage collection on large groups ultimately reclaims widely distributed garbage clusters, albeit more slowly. Groups can be reorganized dynamically, in particular to tolerate failures of some member processors. These properties make the algorithm usable on very large and evolving networks of processors. Other  than distributed symbolic computations, possible applications include for example distributed file or database systems.</p>", "authors": [{"name": "Bernard Lang", "author_profile_id": "81100409202", "affiliation": "", "person_id": "PP31086011", "email_address": "", "orcid_id": ""}, {"name": "Christian Queinnec", "author_profile_id": "81100373159", "affiliation": "", "person_id": "PP14132326", "email_address": "", "orcid_id": ""}, {"name": "Jos&#233; Piquer", "author_profile_id": "81100290956", "affiliation": "", "person_id": "P148342", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/143165.143176", "year": "1992", "article_id": "143176", "conference": "POPL", "title": "Garbage collecting the world", "url": "http://dl.acm.org/citation.cfm?id=143176"}