{"article_publication_date": "01-28-1980", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; \n1980 ACM 0-89791-011-7 $5.00 complicated facet of the design process involving much optitnizatimr in \nthe reduction of its effects. But the third derives from plain human fallibility, and it is here that \none senses an opportunity for improving the quality of designs. Even if careful analysis were cxentually \nto reveal that the majority of the shortcomitlgs in designs m-ise from unsatisfactory specifications, \nit would remain the case that designers make mistakes, often very costly ones The development of methods \nfor catching those mistakes is a worthwhile enterprise. Computers being better suited than humans to \nintricate computation, it would seem that designing could be made more accurate through au(onra[ion, \nby involving the computer more in the design process Totfil elimination of humans would presumably reduce \nenormously errors arising from inattention and limitations of memory and computation speed. Yet the design \nprocess requires a substantial element of crea[ivily, and it seems unlikely that such creativity could \nbe supplied entirely by machines in the near futus-e Automatic programming and automatic design in general \nare awe-inspiring goals. Research sho~dd of course continue in this are~, as the consequei]ces of success \nwill be correspondingly awe-inspiring. However its sponsors will need to be more patient than those supporting \nless ambitious projects. There do exist places today for accurate but not-so-creative computatioll in \ncle$ign. Computer Aided Design (CAD) has been a popular topic for many years, and its successful application \nin industry has left little doubt of the potential for computers to help in the design process. The verification \nof designs by computer represents a more ambitious role for computers in design thnn ordin:iry CAD. The \ncomputer is still not being asked to contribilte the magic of cre:itivity, but it does need to know more \n{ibout the objectives of the design than does a typical CAD prograin. One can form a better idea of the \nrole of the computer here by considering the verification function independerltly of the question of \nhow a coitiputer might perform that function. ~n the design of software such verification is carried \nout in practice with hutnans, sometimes in what is Ciilled the ww/k[hroug/I. The programmer expkiins \nhis program (the progr:ilimrer s design ) to fello~v prograininers who serve as p;itient critics. His \njob is to persuade the critics thtit his progratn does what he s{iys it does. The critics are not called \non for innovative contril>ntions, merely for painstakingly accurate checking of cktims, This is not to \nsay that the critics might not SUpplY innovation. Indeed ri very pm=ceptive critic maygo so far as to \ndetect flaws in the specifications thetnselves, by virtue of his insight into the n:iture of the appiic:ition. \nHowever the prinuiry function of the critics is to c:itch errors in the ckiims of the designer, and a \nthorough and accurate presentation by the programmer shotild turn out to be a pailifully boring experience \nfor any critic without sowie bookkeeping blood in his family tree. This view of the w:ilkthrough is inteilded \nto suggest a division of/abor in the walkthrough while progriimining (and more generally designing), \nwith its demands on crea[ivi[y, is better left to the human, the accuracy demanded by verificatioil suggests \nthat machines should take over the critics job, Viewed in this way, tiutonratic verification cannot \nseriously be disinissed (is useless. The question then arises, is automatic verificatioil impossible? \nT/re purpose of thispoper is[os}tow how verifiers maybe specificdin a wqv that Ieovcs no doubl about \n[he possibilityof btti[ding [hem, n}tilc}lolding ottttnore promi.re than previous approaches for the \npossibility of successftilly using thcnr. We make no cl:iim as to the eventual economic feasibility \nof our approach. However we assert with considerable confidence that if our approach to verification \ncannot be lmade at Ie:ist reasonably useful, then neither can any of the other :ipproaches that have \nbeen spelled out in the literature to diit~ The IVature of Verij7c~(iotl When a progr:~mmer discusses \nhis program with the critics in a walkthrough, the discussiorl centers oil the behavior of his program \nalong with facts relevant to the data dom:iin(s) and control structnrcs of the prograin. In non-progr:itnming \nsitu:itions, the :in:dogous discussions would center on whatever objects were being designed, whether \nbuildings, pkines, toys, or whatever. In Or(fer to autoinnte any of the discussants, it would seein necessary \nto iinpiement expertise in the relevant areas. Viewed in this light, automatic verification would seem \nto pose a inajor challeilge to artificial inteliigencers. The kind of approach we describe below stands \nsomewhere in the middle between timbitioas artificial intelligence :ind classic:il inatheln;ttic:il logic. \nIt defines the problem more in te,rms of the topic being discussed with the verifier and less in terins \nof axioins and rules of inference. (A logician would characterize this as being model theoretic in orientation. \n) Yet it involves formal flOtiOitS of proof, proof step, :Ind s~ulldness, ! making it sound very like \n:i classical proof theoretic account of verification. In fact wh!iit this paper clescribes is a notion \nof proof that is {much niore sensitive to the issue of the topic of discussion and less io the classical \nnotions of axiom and inference rule, which we shall argue can be dispensed with in favor of a more model-theoretic \nor semantic notion of rigorous argument. Let us now consider the essential attributes of the sort of \nexplanation a verifier is supposed to cope with. We identify three problems here: what is an appropriate \n/anguuge for giving an explanation to a computer, how is its meaning to be defined, and how does one \nper,wode a computer of the truth of claims nuide in that language? Given the linguistic capabilities \nof today s computers, the natural I?nguage used in walkthroughs will need to be replaced by something \nmore formal for the automatic verifier. This immediately presents an obstacle to the programmer, who \nmust now present his explanations fortnally. Is this an insuperable obstacle? Consider the language used \nby the programmer to convey pt-ocedl{res tothe computer, whether Fortran, Cobol, PL/1, Lisp, or whatever, \nThe programming language is certainly formal, yet the programmer has managed not only to learn it but \nto produce large amounts of software with it. Thus formality alone is not an obstacle to programmin~ \nwe feel that neither should it be an obstacle to explaining. The sort of formal language we have in mind \nis exactly that used by mathematical logicians, to within uninteresting details of syntax. An assertion \nsuch as for el,ery x there exists y > x such that y is prime is a firte ex:imple. This would be considered \na syntactic variant of Vx3y[y>x A prime(y)] We would assume that a sophisticated verifier would make \nprovision for such variations in taste in the syntax of formal languages, much as some LISP systems today \noffer the alternatives of parenthesized Polish notation or an Algol-like notation, in each case with \nthe same underlying semantics. Another assertion within the scope of what we have in tnind is if x=3 \nthen after setting y to x+1 and then setting x to y+ 1, x=5. This is not a statement normally encountered \nin ordinary logic, but has been catered for satisfactorily by recent developments in formal Iogics of \nprograms, see eg, [5J On the other hand we would probably stop short of metastatetnents such as saying \nx= 3 does not help here, or this sentence is false. The one criterion we would adhere to in determining \nthe language would be whether each construct was sufficiently precisely defined as to determine unambiguously \nthe correctness of each assertion in the language. Exactly what constitutes correctness is left undefined \nhere However the logicixn reader will not be led astray by reading valid for correc~ while the non-logician \ncan assume that the notion coincides with his intuitions about the correctness of assertions about well-defined \nmodels of any given domain. In the sequel correctness is the only property of the language we shall make \nuse of. The definitions of the language constructs would in general be in terms of the models about which \nthe language t~lked. There would be models whose domain was the natural numbers, models consisting of \nstates for progratns to travel between and for propositions to be true or false of (this being one possible \nmodel for talking precisely :rbout what programs do), models consisting of abstract representations of \ncontinuous functions (for when these arise in an explanation), snd so on. For all of these models the \nmeanings of the language constructs would be either standard or at least spelled out for the benefit \nof the user. There is much controversy in the philosophical and the artificial intelligence literature \nabout how to model reality. From the point of view of our application for models there are two key properties \none should look for in a fortnal model. The first property is simplicity of definition. This enables \nthe model to be explsined to the verifier designer; the person that specifies the program for reasoning \nabout the model, the one who defines the proof sys(em that we talk about below. It also pcrtnits the \nuser of the verifier to reiate the model to his own private model of whatever he is talking about. The \nsecond property is sufficient precision of definition to determine unambiguously the correctness of every \nassertion the computer will be called on to deal with. This permits the verifier designer to clefine \nwhat constitutes an acceptable argument and to specify algorithms for following such arguments. Whether \nthe model is a first-order relational structure, or contains a type hierarchy, or e~en claims to have \nno relation to any structure thus far contemplated by philosophers or mathematicians, is not as important \nas whether it meets the above objectives. It is easy but not very productive to get carried away with \nquestions of universality of any given class of models. The important thing is to have understandable \nmodels. Beyond the model itself there is the question of the language used for talking about that model. \nFor example do -you want to use wtriables ranging over individuals in the model? Do you want to be able \nto form the disjunction of any set of assertions in the language, or the negation of an assertion? Can \nyou quantify over a variable? Do you want sotne way to refer to subsets of the domain, or to indeterminate \nfunctions or even functional? Or would you prefer to drive all those concepts into the model itself, \nthereby imbuing them with mathematical existence and reducing the language perhaps to no more than equations \nbetween terms involving free variables} as is done in the various approaches to algebraic logic? All \nthe above language considerations make sense A powerful verification system would be able to cater for \na wide range of such approaches to language, just as a student hm to be prepared to cope with the different \nmodes of explanation of his various te:ichers. Such power need not be developed all at onc~ a verifier \nshould be just as useful as its available models and languages permit it to be Researchers in search \nof the one true language will find our position on the choice of language something of a copout. The \nproblem of form~l language design is a good one for research, but there is as yet no real consensus in \nany research community that a universally useful language has been found. This author s vote would go \nto an extreme algebraic logic approach, which seems to offer unit y wit bout 10SSof descriptive power. \nHowever this is not a widely held position even in strongholds of logic, despite its energetic support \nfrom such visible mathematicians as Hahnos, Tarski, and more recently Quine, not to mention computer \nscience s own self-styled ADJ group. Under the circumstances, the let a hundred flowers bloom philosophy \nseems appropriate, but further developments in the area could still chnnge our position. Depending on \nthe model and the language permitted for talking about that model, there may be fast slow, or no algorithms \nfor deciding correctness of all assertions about the model. The approach we adopt to specifying verifiers \naddresses this problem, later in the paper. Thus we have taken care of the language ~ind its meaning, \nleaving the problem of how to persuade a computer that rr given assertion is correct. The Nature of Proof \nWe begin by defining a notion of proof at a level abstract enough th:tt it could serve as a starting \npoint for development of either a cli{ssic:]l treatment of formal proof or an account of informal reasoning. \nThe notion is independent of the question of effectiveness in proof checking. (lur account does however \nassume that a fixed standard of correctness is applied uniformly to all the assertions of a proof, This \nmakes it a Hilbert-type (or Frege-type) account of proofs rather than a natural-deduction one. lNatural \ndeduction permits variations in the stmd:ird of correctness in a proof, under the control of hypotheses \nthat may be assumed locally and later discharged, usually for the purposes of exploring cases individually \nor for reo ucfio ad absurdutn arguments. The connections between the two styles are easily made and \npresent no serious computational obstacles [21 essentially what happens is that the hypotheses can be \nincorporated into each of the imertkms within their scope, St little cost. Our arguments can therefore \nbe applied equally well to a natural-deduction framework, an exercise we do not carry out in this paper. \nThis is not to say that the exercise should not be performed; a practical verifier should certainly permit \nnatural deduction. Proof Correctness A proof is a set of assertions. A proof is correct when all of its \n:issertions are correct. A basic rule of persuasion is that one s proofs be correct. The reader in any \ndoubt as to whether this applies to informal explarrrrtion should try to imagine a programmer getting \naway with incorrect claims about his program in the course of explaining it. Any critic astute enough \nto spot such an inaccuracy will cert[iinly protest, regardless of whether the incorrect cjaim fojlows \nlogically from other claims. (If it does then presumably one of those other claims is also incorrect. \n) Another basic ruleof persuasion is that it should be possible to follow a proof. Howevtr we put correctness \nof proofs ahead of this requirement. One could call this the accr(racy-over-coherence principle for proofs \nin this context there is a striking quotation on page 288 of [91 attributed to Lord Marrsfielcl in advice \nto the newly appointed governor of a West India island: There is no difficulty in deciding a case-only \nhear both sides patiently, then consider what you think justice requires, and cjecide accordingly but \nnever give your reasons, for your judgement will probably be right but your reasons will certairdy be \nwrong. In other words it is easier to be right than to convince people that you m-e right. It should \nbe pointed out that lMansfield s advice assumes the governor to be a competent judge. In applying the \naccuracy-over-coherence principle we shall assume that the programmer or designer doing the explaining \nhas sufficient competence to be right about his program or design when discussing it, even if he has \ntrouble fitting his correct assertions together to form a believable proof. Proof Sotlndness It is of \ncourse in the nature of assertions that they are sometimes difficult to test for correctness, and this \nis where proofs help The essence of a proof is in its structure, in the way some of its assertions help \nin seeing the correctness of other assertions. The classic account of the structure of a proof, and the \none e.spousedhere, assigns to every assertion q in a proof those assertions of the proof which are the \npremises of q, which intuitively are those assertions that are intended to help directly in seeing the \ncorrectness of q. Assertions without any premises are usually singled out as axioms, but this serves \nno essential technical purpose and is not done here (For those that find m:ithematica] formalism helpful \nwe may make this precise by postulating binary relation ~, the is-,oremise-o~ relation, on the assertions \nof the proof -prrq means that p is a premise of q.) Every assertion q of a proof is associated with a \ns(ep of the proof having conclusion q and premises the, premises of q. (Formally a stepis the pair (q,{plimq}), \none pair for each q in the proof. ) We define a sourrds[ep to be one with either a correct conclusion \nor an incorrect premise A soutlri proo~is one with only sound steps. As a practical matter we shall assume \nthat proofs are always finite. A proof is ocyclic when the proof has no cycles in its is-premise-of relation \n(i.e. m+, the transitive closure of r, is irreflexive). The following proposition is intuitively obvious, \nthough not often stated explicitly. Proposition. An acyclic proof is sound if and only if it is correct. \nFor correctness clearly implies soundness. Conversely, suppose we have a SOUIKi but incorrect proof. \nThen some assertion is incorrect, so the step of which it is the conclusion has an incorrect premise, \nand so cm, yielding an infinite series of incorrect assertions (since the proof is acyclic), contradicting \nthe finiteness of the proof. (Those who care about such things will of course be aware that finiteness \nof the proof is not needed to prove this result; it suffices that ~ be we//-founded in the conclusion-to-premise \ndirection. Much work in proof theory depends on this observation. Hmve\\ er our account is meant to be \napplied directly to real-world proofs, so we lose nothing by requiring all proofs to be finite.) We assume \nfrom here on that proofs are acyclic. (The reader should not however assume that acyclicity is an essential \nfeature of proofs. The author has been involved recently in work on cyclic proofs where the above proposition \nholds, as a proof-shortening alternative to the use of the induction principle.) Proof Systems A proof \nsys(etn is a set of steps, iav?riably infinite in practice. A .rorfnd proof s,w(em has only sound steps. \nA proof is in a proof svstem when it is a subset of the system; thus if a ixoof is in a sound proof system \nit is sound and so correct by the above proposition. A system is cotnp/cte when every correct assertion \nis the conclusion of some step of some proof in the system. The function of a proof systetn is to provide \na standard of proof. Given a proof system, a proof-checker s job is to test whether the proof is in the \nsystem. Effectirwress The ideal proof system from the user s viewpoint is one that contains all sound \nsteps. However this would include all premise-free steps with correct conclusions, amounting to one-step \nproofs of the form It s obvious for all correct assertions. For any model and language whose set of correct \nassertions is not easily tested for membership a practical proof system will have to settle for less \nthan all sound steps. A rectirsivc proof systctn is one whose set of steps is a recursi~e set, that is, \none for which there is an algorithm to tell whether a given step is in the proof system. Clearly a proof-checker \ncan always be implemented for a recursive proof system. A less precise but more useful notion is that \nof a [raclob/c proof system, in which steps can not only be tested for membership but can be so ?ested \nin a remooable amount of time, say a few seconds. Tract.ahiiity is imprecise not only because of uncertainty \nabout the exact time limit but because of the dependency of the complexity of such tests on the choice \nof computer on which the tests are implemented. Even when these factors are settled on there still remains \nthe difficulty of determining the inherent complexity of a given set. For example we still do not know \nhow quickly propositional tautologies can be recognized. The best algorithms take time G(2n) in the worst \ncase, but for all we know there is an O(n) algorithm that does the job. (h the other hand, although the \nboundary of tractability may be imprecisr, it is nevertheless possible for a proof system to lie well \nwithin that boundary. For example, if membership in the system is decidable in a second on a $1000 computer \nthis author at least would not hesitate to call such a system tractable. Yet the potential for a system \nas tractable as this to reduce the labor of proof is very high as we shall see later. The classical kind \nof proof system is exceedingly tractable-It consists essentially of the substitution instances of a finite \nset of schemti, with occasional restrictions on the possible substitutions, (We do not distinguish this \nfrom a system which makes substitution an explicit inference rule.) For example, the axioms p4q-p) and \n(p4q+r)) + (p+q -p+r), together with modus ponens (from p,p+q infer q), constitute a proof system every \nstep of which has either no premises (an instance of one of the axioms) or two premises (an instance \nof modus ponens). Such a proof system is very tractable because steps can be tested in milliseconds since \nonly simple-minded and efficient pattern matching need be applied, in time independent of the size of \nthe assertions if careful attention is paid to their representation. The problem with this pattern-matching \napproach is that where a substantial amount of computation is required to verify the truth of a claim, \nthere is no alternative but to have the proof of the claim explicitly contain all the steps of the computation. \nIn effect the author and the checker of the proof have to go through the entire computation together \nstep by step. Putting this the other way around, if the assertion to be proved has a short proof then \nthat proof embodies a short computation demonstrating the assertion s truth. When the inference rules \nlook M much like the atomic steps of some abstract computer as do the rules of classical proof systems, \nit is perfectly reasonable to view a proof as a completely spelled out computation. Such a formal proof \nhas no more use than does the complete trace of execution of the calculations involved in say computing \nthe inverse of a matrix. Our first step towards a more usable proof system then is to propose the adoption \nof larger systems than the classical ones, but not as /roe/afdq yet still tractable enough to be usable \n(and therefore still a proper subset of the set of all sound steps), By larger we mean containing many \nsteps not in the classical system, In this way we may hope for the shortening of many proofs, as well \nas the introduction of new proofs where a correct assertion had no proof in the old system (as can happen \nwith incomplete systems). Usability In the end, a verification system stmrds or falls on whether it can \nbe used in practice. The novelty in our proposal lies primarily in its treatment of usability. Enkirging \nthe proof system m;iy permit shorter proofs, but brevity in proofs is not everything. The user should \nalso know what the systelrr is, or proof development will become a hit-or-miss proposition where the \nuser produces a step and then tests whether it is in the system. Presumably such tests require computer \nhelp when tractable permits machine computations in} olving millions of steps. This is all very unsatisfactory \nbecause the liser has no basis for efficiently guiding the development of his proof. Classical systems \nperinit the user to tell fairly easily whether a given step is in the system, which is very helpful in \ndeveloping the proof. Enlarging classical systems without preserving this property is a risky business. \nIn the approach we describe below we play it safe and improve this property at the same time as we substantially \nenlarge the system. Decision methods for fragnrents of logic supply precisely the mech;inism we need \nto address this issue Typical decidable fragments of logic resemble classical inference rules to the \nextent that they tend to be easily characterized. For example one may talk about the fragment whose language \nis restricted to propositional connective, variables:, +, and =, t;iking correctness to mean validity. \nSuch a characterization is in fact simpler in character than the sort of characterization of a typical \ninference rule, dealing purely with the choice of vocabulary and not with matching patterns. It is about \nthe simplest sort of syntactic characterization one could have On the other hand fraginents differ from \ninference rules by not en.wring soundness but by defining a domain in which soundness is decidable. (By \ndecidable fragment we shall mean that step soundness is decidable in that fragment. ) So just being in \nii decid;ible fragment would not seem to be a good criterion for a step to belong to a proof. After all, \nthe step may be unsound, and if it is going to take a million-step computation to test soundness then \nthe user is no better off than before. These objections notwithstanding, let us take for a proof system \nthe set of all sound steps each of which is in some tractable fragment of logic, that is, an easily decidable \nfragment. Now we have already remarked on the accuracy-over-coherence principle, and claimed that it \nis easier to stick to the truth than to connect truths together to form a coherent proof. If the user \nrminages to stick to the truth in his proof and so produce only correct proofs, then such proofs will \ncontain only sound ,. steps, as was pointed out earher. If in acldition the user keeps each step in one \nof the frsgments catered for by the proof system, it is then guaranteed that the proof is in the system. \nThis is the key to our approach, The user now has two things to worry about, correctness and tractability, \ninstead of one as in the classical approach, namely just matching some rule. However, we maintain that \neach of these is easier for the user to deal with than pattern matching. Correctness is handled by miigic, \nso to speak. That is, the user employs whatever it is that makes him so sure of his facts, without any \nreference to the methods used by the verifier, Tractability is a little more difficult, involving strtificial \nconstraints that must be learnt, just as n programming language must be learnt. However, these constraints \ncan be arranged to be less demanding than the equ;illy artificial constraints of a classical proof system. \nThe use of decision methods in verification is by no means novel. Indeed the earliest formal verifier \n[6] depended heavily on the existence of decision methods for certain fragments of Floyd s programming \nlogic. However the use described here uses a new (and still untested) principle, and should not be confused \nwith these other use~ Note that, unlike soundness, tractability is a concept dependent on the availal>ility \nof decision methods. As such it tnay be considered an ariifact of the state of the art crf deciding soundness \ntogether with the state of the art of computer hardw~re, AS we learn more, more domains will become tractable, \nbut no amount of learning will ch?nge which steps are sound, And for as long as computer hardwilre continues \nto bcw out Church s thesis the set of theorems of an implementable proof system will always be recursively \nenumerable. (The converse holds also -if a computer is built that transcends Church s thesis we can expect \nto see some effectively checkable systems with non-r. e. theories. r his point is obvious to recursion \ntheorists, but seems to be a stumbling block for some less cornputationally oriented logicians, who have \na terrible time getting straight the applicability of G6del s incompleteness theorem,) The advantages \nof our approach are (a) Much larger proof systems (M measured by the number of permissible steps of a \ngiven length) are possible with this approach than with either the classical approach or with any method \nbased on accumulating a library of theorems and deri~ ed rules of inference. Even if one uses decision \nmethods recluiring exponential time, and insists on answers within a few seconds, a single decision method \ncan ha~e the effect of haying millions of inference rules. For exnmple if a method takes 211 microseconds, \nn being the num[>er of words in the step, then in 4 seconds the method can handle steps of size up to \nn=22. With say an 8-word dictionary for the domain in question this means 822 possible steps, of which \nperhaps 1(E4 are well-formed, and of those perhaps 10L are sound. (That a sizable and fixed fraction \nof well-formed steps must be sound follows from the existence of short sound steps together with the \npossibility of padding those steps with irrelevancies, or de;id rats as they are sometimes called. For \nexami]ie, if p is correct so is pvq, and if x=y is correct so is x=y+().) The proof system would then \nbe characterized as including just those sound steix in the given fragment of length at most 22. The \nlength constraint is not difficuit for the user to take into account. 22n A decision method ;)l\\vil~s \nrccluiring time micrmseconrfs is less satisfactory, giving n=4 as the largest step size it can han(ilc \nThis wouid i>ermit oniy 84 = 4096 steps, of which such a smdi number would be weli-formed and sound that \none might as weil just write them all down, and perhaps add a few ionger ones to spite the algorithm. \nIf 2211 was the worst-case time but there were many ini>uts of size 5 m more thiit could be handled in \na practicai amount of time one might feel more favorably inclined towards such an algorithm. Unfortunately \nwe would no longer have a good characterization of the system; which of those steps longer th~n 4 does \none acimit? Admitting just those that are deait with in under 4 seconds, say, is no better than the hit-m-miss \nai)proach we decried at the outset, since then the user has no way to develop his proof but to try out \nsteps on the computer. Thus there is a threshold somewhere between one and two exponential in the worst-case \nanalysis, beyond which algorithms have nothing to offer that a classical system cannot offer. (b) The \nverifier specifications are well defined. TO build it, it suffices to implement the necessary decision \nmethods. (c) The user need not keei> track of the cmnimtation carried out by the verifier. In fact the \nuser need not even have a general metho(i for computing soundness in any of the domains known about by \nthe proof system. It suffices for him to keep his ixoof correct, with whatever methods he would use in \nkeeping iill his assertions correct in the course of a ~v;ilkthro~igh with hilinan critics, and to keep \neach step tractabie. It should be clear that these two coilditions girnrailtee that ail steps of his \nproof reinain in the proof system.  How people stick to the truth during an explanation (to within ~i[iiiltentional \nslips) is a inystery that we do not feel obliged to exi>iain -we just observe that that s wh[it seems \nto hai>i)rn between humans. Keepin/ steps tractable is another issue; here we rely on tractability being \nan e:isiiy recognized property of steps, What tract:ible doinzilis exist? To begin with, the classicai \nkii]ds of :ixioms :inrl inference rilles siIppiy tractabie, :iibeit rather ii]ii]ovcrished, doinains. \nSuch doinains are defined in a rei:itively coinplicated and artificial way so that every step in the \ndomain is sound; still they are very tractable, There are a number of decidable logical theories already \nin existence that meet all our criteria. The domains they each treat are e:isily recognized simplyon \nthe basis of the vocabulary used. And their complexities are at most one exponential or a little more. \nThese are more s:itisfactory than the classical axioms and rules \u00adtractability is easier to drcide, \nyet they cater for vastly more steps, The work done by Nelson and Oppen [8] on combining decision methocls \nis extremely useful in this context. They show that methods for quantifier-free theories can be combined \nto deill with the deductive clpsure of the union of those theories in time the maximum of the slowest \nmethod and B*l, the ri-th Bell number, n being the length of the problem. The author has considcrrd the \nproblem of extending this result to cope with program Iogics, (It had been hoped thrrt this work could \nhave bwn presented at this conference, but the paper fell into the unfortunate majority of the conference \nsubmissions. ) Consider the usual constructs of programmirrg kingu:iges (procedure calls with call-by-vahle \nparameters, begin-end, if-then-else, while-do, even nondeterministic choice) along with the commoner \nconstructs of programming logics (p{a}q for partial correctness [51 halts(:i) for tcrmin:ition, and a=b \nfor progrtirn qtlil,alc[lcc). Include with these everything covered by the .Nelsorl-Oppen theory. Then \nthe entire resulting logic is still of complexity at most Bn. The following is a valid formula of this \nlogic, as the reader may wish to verify, halts( whi/e x<y A Z=X+1 do begin A(x); B; A(z-l); B emf) \n* halts( whi/e y=x+2 A +y<z v ZSX) do begitr A(z-1); fl endJ Of course we have left out something from \nprogram logic, since otherwise the probicrn would be quite undecidable. What we have omitted :ire the \nthree main kinds of binding mechanisms, nanre]y quaatificatiorr, assignment, and procedllre definition. \nJntrodllcirlg an}, one . of these would make the fragment undecida[>le Quantification would give :!II \nof first order logic, rissignrnent would make termination of a single program undecidable, and procedure \ndefinition would permit recursion, which would introduce undccidal>iiity through well-known undecidability \nresults about testing inclusion between context-free languages. Thus this logic is trra.~ivra/ for decidable \nii fragment of what one tnight consider b:isic sequential program logic. That such a rniiximal fragment \nis so 2211 tractable (Bn, :is opposed to s:iy 2 or worse) is perhaps surprising. This fragment would \nbe oidy one of many such fraginents in which other concepts silch asqwurtification and integer multiplicatiorl \nwere c:itered for. To deal with quantific:ition one woidd develop decidabIe fragments of logic that covered \nall existing rules for first-order predicate calc~lhls. To begin with there :ire the fragments studied \nby Goedel,, Ackermcmn, and Hilbert-Bernays, and analyzed for their complexity by Lewis [7j. These provide \nexamples of fragments th:it are not defined purely Iexically but have simple structural constraints liiniting \nthe number of alternations of qiliintifiers in various ways These coulld be extended by say ii fragment \ncont:iining one qumrtified variable x pcr step (hilt with many occurrences of vx :ind 3x for that variable \nx in the step) together with rn:iny S5 rnoclai operators commuting with vx and 3x and playing the role \nof quantifiers the identity of whose vari;ibles had been suppressed except for the f:ict that they were \n:ill different from x. We conjecttire that this fragmcntj or something very close to it, is decid:ib]e \nin exponenti:il time. We also conjecture that the proof system it defines is complete for first order \npredictive logic (all theorems of that logic can be proved in th:it system). Other Approaches Here are \nsome other approaches that have been proposed for prograin vt=rification. (a) Direct proc,f checking, \nA verifier checks that a complete formal proof uses only legal axioms and irrference rules. This has \nthe adv:intages that the specifications for the verifier are well defined, ant] that the verifier is \nfast. The disadvantage is that forinal proofs are tedioits in the extreme to prodilce. (b) Proof hints. \nHere one supplies fragments of a proof find lets the verifier silpply the rest. This approach avoids \nthe tedimn of approach (:i) by letting the comp~iter do more of the work, often with the help of decision \nmethods for fr:igments of logic, However it is unclear what the specifications for silch a verifier should \nbe (c) Derived rujes of inference. This :ippro;ich is a vari~ition of (ii). The icje:i is to permit \nthe user to derive :idditional rides of iilfererrce beyond those silpplied by the proof system, as a \nmeans of permitting shorter formal proofs. With this :ipproach verifiers rernaiil well specified. (d) \nProof gener:ition under prograin controL Here the user writes a program to produce a proof. This is the \nappro~ich proposed by ,Mihrer for LCF 2, his Logic of Computable Functions proof-checker. As described \nby Milner the user s program amounts to heuristic control of proof discovery strategies. A certain :imourrt \nof experimentation is required in order to get the proof right, and no convenient way is provided for \npredicting the outcome of a particular program other than by trying it to see what happens. Topdown Proof \nDevelopment Our basic notion of proof says nothing about how a proof might be developed, and obtiiously \nany order of development Ieading to the same prorrfshcruld be equally satisfactory. Still, it is worth \npointing out that there is a particularly natural way of developing a proof recursively, We may consider \na proof to be a .i>roof of one of its assertions (presumably one not af)penring as a premise in any step \nof the proof) from some subset of the premise-free assertions of the proof. In fact what is being proved \nhas itself the form of a step, Then the proof can be considered the filling-in of intermediate assertions \n(possibly including some additional premise-free assertions). The user develops his proof by selecting \nany step that is not in the proof system and trying to explain it. The assertions of the explanation \nare added to the proof, along with the appropriate is-premise-of edges. The process is then repeated \nfor [inother step not yet in the system, presumably one that was just introduced, When all steps are \nin the system the proof is complete. This approach can be considered a top-down proof development technique. \nIt strikes us as a very natural one to use in practice. System Manual The system manual that goes with \nan implementation of our approach takes the place of the traditional axiom system. It should contain \nthe following a definition of the language and its semantics (thus defining correctness and so soundness), \nand an algorithm for membership in some fragment that the user can perform, preferably in his head in \na matter of seconds. Its user verifies that the language and its semantics agree with his intuition, \nand then proceeds to use the algorithm for tractability in the course of developing proofs. In general \ntractability will be decomposed into a not-necessarily-disjoint union of predicates, analogously to the \nway classical proof systems have a finite set of axioms and inference rules. Implementation Implementation \nof swch a system requires little more than the implementation of a set of decision methods, together \nwith a procedure for detecting cycles in proofs. This permits verifiers to be wsembled as a completely \nuncoupled set of modules, and allows for their growth by permitting new methods m be added without compromising \nthe existing methods. Must Verifiers be Per~ecf? In this section we pursue ano{her issue that arises \nin connection with our semantically oriented point of view. It is often assumed that a verifier with \nbugs is no better than no verifier at all. However, if this were true there would be no point in having \nwalkthroughs, since humans are notoriously unreliable whether acting as programmers or critics. Yet humans \nare not altogether useless io this capacity as they do manage to spot bugs. The raison d e/reef a verifier \nis essenti:illy thiit of a compiler -it does a large amount of tedious computation automatically, drastiurl]y \nreducing the number of errors one may expect in such crwnput:ition when performed by humans. In the case \nof the compiler the computation involves translating and optimizing code; in the case of the verifier \nit involves checking the correctness of ckiims. In both cases a lot of clerical work is involved; also \nin both cases it is possible for errors in the compiler or verifier to completely invalidate the outcome. \nExperience has shown that compilers can be effective without being entirely reliable, and we see no reason \nwhy the siinre should not be true of verifiers. In fact we would expect verifiers to undergo a settling \nin period in the same way a compiler does, during which time feedbzck from users with compiler-induced \nor verifier-induced bugs results in eliminating those bugs that cause frequent problems, leaving behind \njust those bL]gs that very occasionally take an unfortunate user unawares. One may think of this process \nas the gradual cit i/ization of a compiler or a verifier. From this point of view there always remains \nthe possibility of a fatal bug in a supposedly verified program or other design. However, when the probability \nhas been driven below a certnin level, the probability of error from other sources must start to dominate, \nsuch as incorrect specifications, machine errors in the case of programs, defects in material and workmanship \nin the case of production of physical objects such as cars] and so on. Beyond this point further impro~ement \nin the reliability of the verifier is not worth the effort, any more than it is wcrrth the effort of \nthe provers of the four-color theorem to prove correct the compiler they used, as part of the overall \nproof. The Truculent Trio Recently De Millo, Liptonj and Per[is wrote a delightfully tongue-in-cheek \npaper on difficulties confronting verification [3} We got x kick out of rending it, but were disappointed \nby the reviews and replies [10], both for and is.gainst, which we felt all managed somehow to overlook \na grain or two of salt Being competitive by nature we were tempted to review the paper in the same spirit \nin which it was surely conceived. In a weak moment we yielded to temptation. There are eight main poin{s \nmade in [I] that we could discern. They are as follows. 1. Real proofs are vague, and only believed with \ntime and experience. 2. Theorems should be simple.  .,3Long formal proofs arc unconvincing. 4. Lcrng \nctslculations iire unconvincing. 5. Real software is fuzzily specified. 6. There is no evidence th:~t \nsmall program changes will require only correspondingly small proof changes 7. Sotne theorems have long \nproofs. 8. Perfection is an impossible &#38;JOiil.  We too are for motherhood :ind apple pie, and heartily \nendorse all of these statements except 4. Now why don t these people trust long computations? One day \nbefore breakfast we believed the result of five and a half hours of computation in connection with a \ncomputer search for 120-digit twin primes using Rabin s probabilistic prime tester. (We re not just cribbing \nfrom Lewis Carroll, it really was before breakfast, the computer had been left running overnight.) If \nthe trio doubt the results of Iong computation they should have expressed their doubts about the recent \nproof of the four-color theorem, which involved more than 300 hours of computation. That s about a dozen \nbreakfasts. Points 1 and 5 leave us with a sensation that might be akin to what missionaries feel when \nthey see cannibals eating their buddies. Hands up all those who d hate to see all this untidiness in \nmathematics and software brought to an end. Didn t the trio s parents tell them that neatness counts? \nAnyone who thinks the status quo in mathematical rigor is what we should all be aiming for in this business \nis probably one of those nostnlgia buffs we keep reading about. On garage walls one reads those little \nnotices headed How to Kill Ideas and ]earnssuch tricks as We always did it that way. We deposited Point \n6 in our There is no evidence for file, which is well-nigh full of dead rats overdosed on saccharin and \nother injurious substances that the Surgeon-General must have been feeding them. Not collecting evidence \nseems to us the perfect academic alternative to mattress testing. Point 7 puzzled us somewhat. }$ e \\;,ollJd \nhave gone for the stronger remark Goedel madeexactiy half a century ago, that some theorems have no proofs. \nlMaybe the authors are showinig off by trying to make a point with one hand tied behind their collective \nb:ick, confining their attention to decidable theories, but they don t let on that this is what they \nre up to. Anyway, why they would want to restrict Point 7 to decidable theories is a total mystery to \nus. The writing style was very relaxing, and we were looking forward to further light bedti~rre reading \nfrom this team, maybe something a little more controversial. Their reply to [10] howel:er overdid the \ncontroversy bit. Their closing paragraph reads as follows. Finally, pervading sever:il of the letters \nis the sense that if only we did things this way or that way or if we drastically shifted our activities, \nthen program proving would work. Perhaps, but we are troubled by Thoreau s advic~ 13eware of any enterprise \nthat requires new clothes. Surely there has not been a more nihilistic statement in any research journal \nin this decade. Right there in black and white it says it don t get involved in resexrch areas that seem \nto need new methods. Watch out when the NSF turnis to Thoreau for its light bedtime reading. JNOW De \nMillo, Lipton, and Perlis wouldn t be pulling our le~ would they... ? We are terribly troubled by people \nwho tell us they are troubled, and trebly troubled by troubled trios. Conclusions We have tried to show \nhow decision methods can take over the role generally reserved for axiom systems in automatic verification \ntechnology. It is clear that the technology has beenl moving in this direction for some tim~ even the \nearly verifier of J. King [6], under the influence of R. Floyd [4], demrmstrated a strong reliance on \nefficient decision methods. And the verifiers of Constable [1] and Olppen [8] make heavy use of decision \nmethods as opposed to the more heuristic methods that are employed in many other verifiers. Yet no cohereat \nframework has emerged to date within which it is cllear just how decision methods can uniform/y replace \nmore traditional rules of inference . while preserving the precision with which a traditional proof system \nis specified. Indeed many people working in verification methodology today are still tied to the stereotypes \nthat some mathematical logic texts regrettably do much to encourage, ewr while they rnak.e use of the \nlatest decision methods. The approach we described offered such a framework, in which a proof system \ncould be defined entirely by identifying tractable fragments of logic, yet in a way that permitted proofs \nto be developed without help from the computer. To avoid ending on too optimistic a note we point out \nsome shortcomings. We have already observed the hazard inherent in any attempt to formalize the walkthrough, \nnamely thnt formality may cramp the style of the programtner defending his program. One might take as \nan objective of natural language research the softening of such formal environments to improve productivity, \nwhich we regard as a worthwhile but very difficult problem. An issue unresolved here is the extent to \nwhich the replacement of traditional rules by decision methods improves the rate of convergence to a \ntractable proof. We are inclined to optimism as the reader will have noticed, but c~early a fair amount \nof experimenting will have to be done in order to vindicate this optimism. Another problem is whit to \ndo for the user who wants to refer to a function for which no decision method presently caters, e.g. \ngcd or factorial. A long-term view would be that eventually decision methods will be developed for fragments \nincorporating all the more familiar functions. A more fort-holcling and pmb:ibly quite satisf~ictory \napproach is to let the user proceed m he would have in a traditional axiom system, ttiking as the permitted \nsteps the substitution instances of those axioms that are appropriate for those functions. The nice thing \nabout axioms is that they implicitly define a procedure, namely the one that attempts to match an axiom \nag:iinst an instance of its use Thus even though the system is built out of decision methocls, the user \nneed not he a programmer to add to the system s capabilities, all he needs is to be able to write axioms. \nThe advantage that the system programmer has is that he can augment the system in much more powerful \nways by adding more comprehensive decision methods. Thus an axiom can be viewed as the poor man s decision \nmethod. Bibliography [1] Constable, R.L. and M. O Donnell, A Progrutnming Logic, Winthrop Press, 1978. \n [2] Cook, S. A., and R.A. Reckhow, The Relative Efficiency of Propositional Proof Systems, J. Symbolic \nLogic, 44, 1, 36-50, March, 1979. [q De Millo, R, A., R.J. Lipton, and A,J, Perlis, Social Processes \nand Proofs of Theorems xnd Programs, Comm. ACM 22, 5, 271-280, May 1979. [4] Floyd, R. W., Assigning \nMeanings to Programs, In Mathemiz[ical A.~pec!.r of Cotnp~iter Science (ed. J.T. Schwartz), 19-32, 1967. \n [5] Hoare, C. A. R., An Axiomatic Basis for Computer Programming, CACM 22, 576-5$0, 1969. [6] King, \nJ. C., A program wrificr, Proc. IFIP Cong. 71, North-Holland, Amsterdam, 1971, 23 S-249. (Also Ph.D. \nThesis, Carnegie-Mellon University, Pittsburgh, Pa, 1969. ) [7] Lewis, H., Complexity of Solvable Cases \nof the Decision Problem for the Predicate Calculus, 19th Annual Symposium on Foundations of Computer \nScience, Ann Arbor, Michigan, Oct., 1978. [8] lNelson, G. and DC. Oppen., A Simplifier B:ised on Efficient \nDecision Algorithms, 5th Ann. ACM Symp. on Principles of Programming L@uages, 141-15-. J:in. 1978. [9] \nNewmim, J. R., The Wor/dof JMhetna!ics, Simon and Schuster, NY, 1956, [10] Pomeraat< A. G., et al, Eleven \nletters to the AC,M Forum section on [3], Comm. ACM 22, 11, 621-629, Nov., 1979. Not to mention lots \nof letters to Software Engineering Notes, SIGPLAN Notices, etc. Even Dijkstra wrote in. [11] Pratt, \nV. R., Axioms or Algorithms, Proceedings of the Eighth Symposium on Mathematical Foundations of Computer \nScience, Olomouc, Czcchoslo\\akia, Sept., 1979.\n\t\t\t", "proc_id": "567446", "abstract": "The goal of automatic program verification is to prove programs correct formally. We argue that the existing notions of formal proof are too syntactic and as such too intimately bound up with details of low-level computation. We propose a more semantic notion of formal proof which nevertheless pays due respect to the problem of effectiveness in proof checking. Such a notion supplies a more practical basis for the specification of verifiers than do extant approaches. In particular the problem of constructing verifiers according to our approach is reduced <i>entirely</i> to routine development and implementation of decision methods, while permitting shorter proofs and yet remaining easy to develop proofs with.", "authors": [{"name": "V. R. Pratt", "author_profile_id": "81100298352", "affiliation": "M.I.T.", "person_id": "PP39036584", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/567446.567457", "year": "1980", "article_id": "567457", "conference": "POPL", "title": "On specifying verifiers", "url": "http://dl.acm.org/citation.cfm?id=567457"}