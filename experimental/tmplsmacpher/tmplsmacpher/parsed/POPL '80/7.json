{"article_publication_date": "01-28-1980", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; \n1980 ACM 0-89791-011-7 $5.00 Section 2 reviews the algebraic framework presupposed here. Section3 considers \nqualified problems. Associatedwkh each control flow arc there is a relation on a finite set Q. The paths \nthat gualifyto be considered are (essentially) those for which the composition of the relations encountered \nis nonempty. For example, Q might be the set of all relevant values for COLOR in Figure 1.1. The relation \non Q associated with the YES arc from TEST/3 k NEED_RED = { (RED, RED) }. The relation associated with \nthe arc from ASSIGN/2 is GET_BLUE with only pairs of the form (q, BLUE). The composition (GET_BLUE)(NEED_RED) \nis the empty relation, which shows that the path from ASSIGN/2 to TEST/3 to ASSIGN/3 is spurious. Section \n4 introduces our first technique, called context tuplirrg. Implementation issues are considered in Section \n5. The call string approach to interprocedural data flow analysis proposed by Sharir and Pnueli [SP78] \nis essentially a special case of context tupling but is less robust. Call strings are discussed in Section \n6. An alternative technique called oha flow n-acing k considered in Section 7. An incidental benefit \nto the study of data flow tracing is that some of the results in [We75 ] are refined and simplified. \nThe choice between context tupling and data flow tracing in qualified analysis seems to depend on essentially \nthe same factors governing the choice between elimination and iteration in ordinary analysis. Finally, \nSection 8 assesses the practical significance of our methodology, including feedback effects on the delicate \ntask of passing from programs to meaningful data flow analysis problems. Data from preliminary experiments \nare presented at appropriate places in Sections 5, 7, and 8. The results are encouraging. At least for \nthe small sample of real programs that has been studied, quafified analysis is clearly feasible and is \nsubstantially more informative than ordinary analysis. This paper is a condensation of a technical report \navailable from the second author. For the sake of brevity we have only sketched the theory behhd qualified \nanalysis and the techniques for tuning an ordinary algorithm to run well on problems constructed by our \nmethodology. To facilitate consultation of the full report, we have retained the original numbering (except \nfor some figures). Gaps in the numbering here are byproducts of condensation, not typographical errors. \nWe thank Ron Frank for encouragement and many stimulating discussions during the early stages of this \nwork. We thank Ashok Chandra for his insightful comments, especially regarding the optimization of the \ngeneral data flow tracing procedure for solving qualified data flow problems. We thank Micha Sharir for \na copy of [SP78] and comments on its relation to this work. Finally, we thank Janet Fabri and Mark Wegman \nfor comments on the presentation. 2. ALGEBRAIC FRAMEWORK Though the intuition about data flow analysis \nis fairly well standardized, we cannot simply cite a few widely read papers and say that basic ideas \nare as in this one or that one. Why not? This paper is not biased toward any one or two analysis algorithms. \nIt shows how to solve a problem ~ by constructing a problem @ and then solving S@ by any of a host of \nknown methods. It is therefore important that our notion of what it is to be a problem be free of extraneous \nassumptions. In constructing @ we should not waste time obtaining conditions reqnired by only some (or \nsometimes even none) of the known algorithms. A careful formulation of the important presuppositions \nof data flow analysis will soon be available in [Ro80a, Sec. 1]. Meanwhile, our algebraic framework is \nsummarized below. To avoid excessive parentheses, the value of a function f at an argument x is fx rather \nthan f(x). If fx is itself a function then (fx)y is the result of applying fx to y. The usual s and z \nsymbols are used for arbitrary partial orders as well as for the usual order among integers. A function \nfrom a partially ordered set (posef) to a poset is isotone iff x < y implies fx < fy. (Isotone maps are \nsometimes called monotonic in the literature.) A meet semi lattice is a poset with a binary operation \nA such that x A y is the greatest lower bound of the set {x, y}. A meet semilattice wherein every subset \nhas a greatest lower bound is complete. (Such a semilattice is also a complete lattice, a fact that is \ninteresting but does not happen to be used in this paper.) The greatest lower bound of a set X will be \ndenoted A X. In particular, the empty subset has a greatest lower bound A @ = T~ in L, and T~ is then \nthe maximum element in L. When L is clear from context we will write just T here. Note the distinction \nbetween our meet semilattices and the join semilattices of [We75], where least upper bounds are considered \ninstead of greatest lower bounds. For us, strong assertions about a program s data flow are high in the \nsemilattice. This is somewhat more common than the use of join semilattices in the literature. The algebraic \ncontext of a data flow problem is a pair (L, M) consisting of a complete meet semilattice L and a set \nM of isotone maps U : L ~ L. (Intuitively, maps in M can be conveniently represented in implementing \ndata flow algorithms, even if no convenient representation for arbitrary isotone maps on L is available.) \nBecause this paper is not biased toward any one algorithm or family of algorithms for solving data flow \nproblems, we do not restrict L or M any further, except as explicitly stated in the results. In particular, \nwe do not assume that M includes the identity map and the constant with value T=. We do not assume that \nM k closed under composition and meet. Contexts that satisfy these assumptions are said to be closed \n(as in [R078]). The framework here is as in [R078] except that the fundamental definitions [R078, Defs. \n2.4 2.6] do not presuppose a closed context. (For a graph G with arc set AG , the obvious extension \nof a map f : AG ~ M from arcs in G to paths in G may now take values outside M, but its values are still \nisotone maps from L to L.) For a given algebraic context (L, M), a probfem Y is a quadruple (G, f, E, \nE) with f : AG ~ M (the local information), E ~ NG (the entry nodes), and E : E ~ L (the entry information). \nA problem ~ is sohed by any map I : NG ~ L asserting no more at any node than can be propagated along \npaths from entry nodes, transforming entry information by the local information associated with the arcs \n along the paths. Like an acceptable assignment [GW76, p. 177], a good solution I dominates any fixpoint. \nTo illustrate the framework we return to Figure 1.1 and the question of availability of A + B. To indicate \nwhether A + B is available we use the semilattice L = { O, 1, 2 } with the usual ordering. We need 2 \nin addition to the honest bit values O and 1 in order to avoid a misleading coincidence. Ordinary availability \nanalysis deals with several expressions simultaneously by manipulating a vector of bits. It is unusual \nfor all the expressions of interest in a program to be available at any one point, so it is unusual for \nT ~ to be assigned to a node when the analysis is complete. Because the simple example deals with only \none expression, we need { O, 1, 2 } to avoid having 1 in L play two roles: the starting point T ~ for \niteration and the assertion that A + B is available. Having chosen L, we need a set M of isotone maps \nto complete the algebraic context. To solve the example availability problem by iteration it will suffice \nto use the set M = { 1, y, T ~ ], where 1 is the identity map on L and y summarizes the effects of instructions \nthat generate the availability of A + B because they compute A + B without later changing A or B. Values \nof y are as follows: yO=land yl=landy2 =2. The constant TM with value TL = 2 is not needed yet, but \nwe put it into M now for later convenience. (A more elaborate example would require a member of M for \ninstructions that kill the availability of A + B by changing A or B.) The global flow problem .9 is shown \nin Figure 2.1. There is just one ordinary solution 10rd, with Iordn = O for all n. The spurious path \nfrom ASSIGN/2 to TEST/3 to ASSIGN/3 is responsible for the O bit at ASSIGN/3, where a 1 bit could safely \nbe used. The next section will formalize this intuition, but first we must review the two major kinds \nof algorithm used for ordinary analysis. In many examples L is well-founded: there are no infinite descending \nchains. What have come to be called iterative algorithms can be used in any algebraic context where L \nis well-founded. The only operation on M used by these algorithms is eval : hl x L ~ L defined by eval(U, \nx) = Ux. It is well-known [Ki73, Thin. 2] that these algorithms find the maximum solution whenever L \nis well-founded and the context k distributive [KU76, p. 160]: (VU EM)(V X, y E L)(u(x A y) = UX A Uy). \n(2.4) Instead of assuming that L is well-founded, what have come to be called elimination algorithms \nassume that (L, M) is a closed context and use the composition and pointwise meet operations on M as \nwell as eval : M x L ~ L. Examples include [AC76; GW76; R078; Ta75; U173], most (if not all) of which \nneed only assume as much about (L, M) as is assumed in [R078]. To deal with loops we need to assume more \nthan jnst a closed context. Somehow the effects of infinitely many paths must be summarized in finitely \nmany steps. The weakest assumption that is known to suffice is rapidi?y, a concept whose general definition \navoids assuming distributivity. Under a strong form of distributivity, rapidity k more succinctly characterized \nas follows. An algebraic context is strongly distributive iff (V U E M)(V nonempty X = L) (u(Ax)=A{uxlxe \nx]). (2.5) Note that (2.4) is equivalent to (2.5) with X restricted to firrite nonempty subsets of L. \nThe general definition [R078, Def. 2.2; Ro80a, Def. 1.6] of rapidity implies the following lemma. LEMMA \n2.6. Let (L, M) be a strongly distributive closed algebraic context. Then (L, M) is rapid iff, for each \nU in M,themap U*= A{Uk lk=O,l, 2,... }is alsoin M and can be computed from U in at most t* steps, where \nt* is independent of U and any composition or meet operation on M is counted as a single step. Our t* \ncorresponds to t@ -1 in the more general definition. Instead of assuming that L is well-founded, elimination \nalgorithms assume that (L, M) is a rapid closed context. Distributivity is often assumed, but it is easy \nto reformulate many algorithms to avoid it as soon as one has distinguished good solutions from maximum \nsolutions. Because (2.5) is equivalent to (2.4) in the common situation where L is well-founded, we are \nnot really assuming more than is usually assumed in distributive contexts when we assume strong distributivity. \n3. QUALIFYING PATHS Let there be given a finite set Q whose members will be called qualific~ions. We \nmay think of Q as a set of mutually exclusive and exhaustive assertions. At any moment during the actual \nexecution of a program, just one qualification holds. Now suppose that a program has been represented \nas a graph. When control is at a node n and a qualification q holds, flow along some ontarcs of n may \nnot be possible. If flow along is possible, then a qualification r will hold when control reaches the \ntarget of c. Thus we have a relation PC on Q, with q PC r iff control might flow along c when q holds, \nwith the result that r holds. The case where flow along c is impossible under q is covered by the condition \n(q pc ?) = @, where, for any relation p on Q, (qp?)={r ~Qlqpr}and(?pr)={q eQlqpr}. (3.1) Associated \nwith each entrance node m is a set EQm of qualifications that might hold on entrance, with EQm = Q the \nsafe assumption in the absence of particular information on thk point. A path c qualifies for consideration \nif, so far as can be determined from this partial information about the program, it seems possible. Specifically, \na path c = (cl, .... cK) (Starting frOttI a node m in E) qualifies if there are qualifications qk for \nk = O, .... K such that q. is in EQm and qk 1 Pck qk for k = 1, .... K. This is equivalent to saying \n(EQm PC ?) # @, where PC is the composition of the relations encountered along the path (the identity \nrelation on Q in the special case K = O of null paths) and the notation (3.1) is extended from members \nof Q to subsets of Q by (up?) ={rc Ql(3qcu)(qpr)} and (?pv)={ q~Ql(3r<v)(qpr)}. (3.2) We have explained \nthe notations and sketched the motivations for the following definition. DEFINITION 3.3. A qualified \nglobal data flow problem is a problem 9 = (G, f, E, E) together with additional information: a finite \nset Q, a relation PC on Q for each arc c in G, and a subset EQm of Q for each node m in E. A path c in \nG from a node m in E to a node n is said to qualify iff (EQm PC ?) # 0. A qualfied solution for .9 is \nany I : NG + L such that, whenever m is in E and c : m -+ n is a qualifying path in G, In s (fc)Em. For \nexample, the problem 9 from Figure 2.1 can be qualifed by the scheme shown in Figure 3.1. The set Q is \n{ RED, BLUE, WHA }, where WHA stands for whatever values COLOR might have other than RED and BLUE. For \nm = TEST/1 we have EQm = Q in the absence of information about the context of TEST/1 in a larger program. \nThe relations PC on Q assigned to arcs here are (identity) = { (RED, RED), (BLUE, BLUE), (WHA, WHA) }; \nNEED_RED = { (RED, RED) ]; NEED_OTHER = { (BLUE, BLUE), (WHA, WHA) }; GET_BLUE = { (RED, BLUE), (BLUE, \nBLUE), (WHA, BLUE) }. We now have a qualifed problem with a qualified solution Iqual that is different \nfrom the ordinary solution Iord only for n = ASSIGN/3, with I ~ualn = 1 in this case. To obtain sharp \ndata flow information, we would like to solve problems in a way that considers only qualifying paths. \nOne approach would be to take a paper developing an analysis method and work through the entire development, \nreplacing each reference to all paths c by a reference to all qualifying paths c and checking that everything \nis still meaningful and correct. This would be tedious (hence error-prone) and would have to be repeated \nfor each analysis method to be considered. A more promising approach is to use old methods in new ways. \nGiven a qualified problem, we construct an ordinary problem with the same graph but a different algebraic \nxmtext in Section 4. This is context tripling. Alternatively, we construct an ordinary problem with the \nsame algebraic context but a different graph in Section 7. This is an application of symbofic execution \ncalled abta flow tracing. The basic result is the same in both cases. Any good solution to the new problem \n(with all paths considered) defines a good qualified solution to the original qualified problem. This \nsection concludes with some auxiliary considerations common to both techniques. DEFINITION 3.4. Let 9 \n= (G, f, E, E) be quafified by Q, PC for each arc c, and EQm for each entrance m. The auxiliary algebraic \ncontext consists of a semilattice Laux and a closed monoid Maux of isotone maps on Laux, where Laux is \nthe set of all subsets of Q with the partial ordering u < v iff v G u and where f : Laux + Laux is in \nMaux iff there is a relation pon Qsuch thatfu=(up?)for allusQ. It is easy to check that Maux really is \na closed monoid of isotone maps. We use containment rather than inclusion for < because larger sets \nare less informative. Thus @ is T ~ux while Q itself is L ~ux. Some easily checked properties of the \nauxiliary context will be important here: Laux is finite, MaUx is strongly distributive, and UT aux = \nT ~ux for all U in MauX. DEFINITION 3.5. As in Def. 3.4, the auxiliary flow problem Yaux = (G, faux, \nE, Eaux) inherits G and E from .9 but has the auxiliary algebraic context and (V c c AG)(V u ~ LauX)[(faUXc)u \n= (u PC 7)]; (V m E E)(Eauxm = EQm). The member Q[n] of Laux assigned to a node n in the maximum solution \nfor the auxiliary problem is said to be the set of relevant qualifications at n. Any of the usual data \nflow analysis algorithms can be used to find the relevant qualifications at each node. The utility of \nthis information is more apparent when we relate Q[n] back to the original situation, with a qualified \ndata flow problem: Q[n]={r<Ql (3 m .S E)(3 q c EQm)(3c : m+ n)(q PCr) }. (3.6) Thus Q[n] tells what \ncan hold at n when reached by a qualifying path. The size of Q[n], averaged over all the nodes n in the \ngraph, will be more important to the efficiency of our techniques than is the size of Q itself. The pair \n(G, E) is called a flow scheme in [R078; Ro80a]; here it will be convenient to call the quintuple (G, \nE, Q, (PC I c c AG), (EQm I m ~ E)) a qualzfied f70w scheme, where by (EQm \\ m c E) we mean the function \nwith domain E whose value at m is EQm, and so on, Note that ,9aux (and therefore the relevant qualifications \nat each node) depends only on the qualified flow scheme associated with 9 . If several qualified problems \nhave the same qualified scheme, then only one auxiliary problem needs to be considered, DEFINITION 3.7. \nAs in Def. 3.5, a qualified fixpoint is anyJ:NG+ Lsuchthat, for allminEwith EQm # @and all c in AG with \nPC n (Q[sc] x Q[tc]) # O, Jm < Em and Jtc s (fc)&#38;tsc. A good qualified solution is any qualified \nsolution I such that, for each qualified fixpoint J and each n in NG, In z Jn. Comparing this definition \nwith [R078, Def. 2.6], we see that every ordinary fixpoint is also a qualified fixpoint, but not conversely. \nQualified fixpoints are only constrained for relevant choices of m and c. Etecause there are new and \npossibly larger fixpoints to be dominated, an ordinary good solution may fail to be a good qualified \nsolution even though it is trivially a qualified solution. This may sound strange, but it is consistent \nwith the nontechnical usage of words like good . For example, a programmer is a person, but a good programmer \nmay fail to be a good person, From (3.6) it follows that the relation pc n (Q[sc] x Q[tc]) could also \nbe described by the similar expression with Q in place of Q[tc]. This subrelation of PC will be mentioned \nfrequently, so we abbreviate: [PC] = PC n (Q[sc] x Q[tc]) = PC n (Q[sc] x Q). (3.8) 4. CONTEXT TUPLING \nIntuitively, we may think of the original context (L, M) as a family L u M of scalars subject to addition \nand multiplication operations that are only defined for certain pairs of scalars. The sum of x and y \nis x A y, provided x and y are in L. The product of U and x is Ux, provided U is in M and x is in L. \nSome caution will be needed because the usual laws of numerical arithmetic do not hold, but potential \ntechnical difficulties may be ignored in this intuitive sketch. For qualified analysis we use a new algebraic \ncontext (LQ, MQ). Members of LQ are vectors whose components are scalars from L. For k = I Q [ the vectors \nhave length k because we want to be able to associate various members of L with each graph node, one \nmember of L for each qualification in Q. (As will be seen in Section 5, doing this is not as costly as \nit seems at first glance. ) What set MQ : LQ + LQ of isotone maps should be used? Here we rely on the \nanalogy with linear algebra. A simple way to transform vectors to vectors is to apply linear operators, \nand linear operators from k-vectors to k-vectors correspond to k-by-k matrices. Each map $ in MQ will \nbe represented by a matrix Q. When vectors $, q have +.$ = q, the r-th component qr can be found by summing \nthe products @rq&#38;q, as q ranges over the indices. Because our sums and products do not obey the laws \nof numerical arithmetic, very few of the theorems of linear algebra can be applied here. In particular, \nthe matrix representation of an operator is not unique here. Some caution is needed in writing up the \nvector/matrix idea as the precise context tupling construction with the algebraic structures actually \navailable, but everything does work out nicely. Given a map @ with domain Q x Q, it will be convenient \nto denote the value @(q, r) at the argument (q, r) by @,q, reversing the order of q and r when we put \nthem in subscript position. This reversal is a little trick to maintain a close analogy with ordinary \nvector/matrix notation despite the unfortunate fact that composition of functions is backwards when compared \nto composition of relations: pos Ax Cfor P~Ax Band o~Bx C; VU= VOU:A+Cfor U:A+ Band V: B-C. Let (L, M) \nbe an algebraic context. Let Q be any finite set and let LQ _be the set of all f : C! + L (with Eq rather \nthan our usual .$q for the value of ( at q, by analogy with vector notation), so that LQ is partially \nordered by 6 < n if f fq s qq for all q. Let MQ be the set of all @ : LQ + LQ such that, for some @:Q \nx Q+ M with value @rqat (q, r), (Vgc LQ)(vr~Q) [(o&#38;), =A{@rqfq lq~Q1l. [Lemma 4.1(1)] Then (LQ, MQ) \nis an algebraic context, and LQ is well-founded if L is well-founded. 72 THEOREM 4.2. Let (L, M) be \nthe algebraic context of a problem 9 = (G, f, E, E) that is qualified by Q, PC for each arc c in G, and \nEQm for each m c E. Suppose M includes the constant T ~ with value TL. Let @ be the problem (G, fk, E, \nE#) with algebraic context (LQ, MQ), where G and E are inherited from.9 and f#c corresponds to @ with \n@rq = (if q [PC] r then fc else TM); (E#m)q = (if q 6 EQm then Em else T~). Suppose Is is a good solution \nfor @. Then a good qualified solution for 9 is obtained by setting, for each node n in G, In=A {(I%)r \nIr6Q[n] }. In the running example from Figure 1.1, Figure 2.1, and Figure 3.1, the problem @ and a good \nsolution 1# are as in Figure 4.1. For n = ASSIGN/3 the good qualified solution Iqual determined by I* \nhas Iqualn = 1 A 2 A 2 = 1. The path from ASSIGN/2 to TEST/3 to ASSIGN/3 does not trick qualified analysis \ninto undue pessimism. Here @ and even @ could be solved by inspection. If Theorem 4.2 is to be of more \ngeneral utility, we need some assurance that @ is not hopelessly more difficult to solve than 9. The \nnew problem @ can be solved in various ways. The available algorithms differ in what they presuppose \nabout their inputs. As was noted in Section 2, iterative algorithms presuppose a well-founded semilattice \nbut need no operations on maps other than evaluating them at specific arguments. Theorem 4.2 has the \nfollowing direct corollary. COROLLARY 4.3. As in Theorem 4.2, any iterative algorithm that can solve \n&#38; alone can also solve .9 with qualifications by first solving@. Practical global flow problems almost \nalways involve well-founded semilattices, but the descending chains can be impractically long. This is \none reason for considering elimination algorithms, even in distributive contexts where the solutions \nthey find are no better than those found by iterative algorithms. As was noted in Section 2, elimination \nalgorithms presuppose a rapid closed context. The next lemma shows that (LQ, MQ) is indeed closed under \ncertain conditions. Parenthetical conclusions are only asserted under the corresponding parenthetical \nhypotheses. LEMMA 4.4. Let (L, M) be a (strongly) distributive closed algebraic eontext (such that each \nU E M has UT = T). Let Q be any finite set. Then (LQ, MQ) is a (strongly) distributive closed algebraic \ncontext (such that each p 6 MQ has@T = T). Elimination algorithms can represent members of MQ with I \nQ I by I Q I matrices of representations of members of M. If each U in M has UT = T then the representation \n@ is determined by @ in Lemma 4.1(1). If not, then several matrices may represent the same member of \nMQ. (For example, suppose M includes a constant map U with value w # T. Many matrices represent the constant \nmap in MQ with vaIue (w, w, w, ...) in LQ.) The rules for representing the basic operations on MQ are \ngiven by analogs of the rules in linear algegra that represent operations on linear operators by operations \non their matrix representations. Given representations of @ and V, we can find representations of ++ \nand + A $. Of course members of LQ can be represented by I Q I -vectors of representations of members \nof L. At first glance this seems costly. If it costs FL to store a member of L and PM to store a member \nof M, then for k = I Q I it costs kyL to store a member of LQ and k2pM to store a member of MQ in the \nobvious way. An unobvious way is therefore considered in Section 5. For the present all that matters \nis computability: if we can somehow represent members of L and M then we can also represent members of \nLQ and MQ. For rapidity we use some ideas from [Ro80a, Sec. 2], simplified for strongly distributive \ncontexts. If (L, M) is a strongly distributive rapid closed context, then (LQ, MQ) can be shown to be \nrapid as well as (by Lemma 4.4) strongly distributive and closed. At present we do not know how to obtain \nrapidity without strong distributivity. COROLLARY 4.6. As in Theorem 4.2, any elimination algorithm that \ncan solve 9 alone can also solve 9 with qualifications by first solving .@, provided that the original \nalgebraic context is strongly distributive. 5. EPFICIENT IMPLEMENTATION OF CONTEXT TUPLING The obvious \nvector/matrix implementation of Theorem 4.2 would multiply costs by k or k2 for k = I Q 1. The example \nfrom Figure 4.1 illustrates the properties that a slightly more clever implementation can exploit. Each \nrelation [PC] is spume: itcontains only about k pairs rather than the up to k2 pairs possible in a relation \non Q. Therefore each f#c matrix in Theorem 4.2 is spmxe it contains only about k nontrivial entries, \nwhere T ~ iS Irivia[. In numerical computation a trivial entry is normally 0.0; here it is TM. The principle \nis the same: instead of storing @ : Q x Q + M as an array, one can store some means for testing whether \nIPrq is trivial and some means for retrieving @rq when the test says that it is nontrivial. In practice \none may wish to use a triviality test that is fast and simple even if it is slightly pessimistic. If \nthe test occasionally reports that I#?rq is nontrivial when retrieval actually yields @rq = TM! then \nthe pairs (q, r) that produce this behavior are processed inefficiently but correctly. The costs of occasional \npessimism may be less than the costs of applying an exact triviality test to all pairs (q, r). Two sparse \nmatrix representations can be recommerided, one for each of the broad data flow analysis strategies. \nFor iteration, each Q : Q x Q + M tO be represented is a matrix for fHc in Theorem 4.2. Testing whether \nq [PC] r is a good way to test whether (f#c)rq is nontrivial. If q [PC] r then (f#c)rq is found by finding \nfc. (This scheme is pessimistic for the occasional arc c with fc = TM. ) For elimination, one may encounter \nmatrices @ derived from the simpler @c matrices by the O, A, * operations on MQ. One could use a hash \ntable. (See [AU77, pp. 340-349] for an introduction to hashing.) If (q, r) hashs to a table entry, then \n@rq is nontrivial and can be found by looking in the table. Moreover, the table can store pointers to \nmembers of M rather than members of M dkectly. When @ corresponds to fzc in Theorem 4.2, every (q, r) \nwith @,q nontrivial has the same member of M as every other (q, r) with @rq nontrivial. In general, we \nexpect the matrices encountered by any elimination algorithm to be quite sparse and to have distinctly \nfewer nontrivial entry values than they have nontrivial entries. Hashhg and indirection are therefore \nrecommended for representing members of MQ, even though they do not improve the worst-case time bounds. \nTrivial vector entries .$q = TL are not so common. We do have Eq = TL whenever .$ is associated with \na node n that has q in Q -Q[n]. The average value of I Q[n] \\ in our example is 2.4 and most $q values \nare nontrivial. Moreover, even iterative algorithms encounter distinct nontrivial entry values, as happens \nat four nodes in Figure 4.1. So far we have discussed vector/matrix representations that exploit sparseness. \nOne could also turre a given flow analysis algorithm to the characteristics of problems ~~ as constructed \nin Theorem 4.2. This is not the place to report on a polished implementation of context tupfing with \na fast but subtle basic algorithm. No such implementation is available. The experience of building and \nusing an ambitious data flow analysis package would deserve a paper in its own right. To encourage use \nof qualified analysis in such experimentation, we will remark on some of the possibilities for tuning \na particularly simple algorithm. The algorithm is essentially that of [Ki73 ], restated in current notation \nwith isotone maps assigned to arcs rather than nodes. (At this point the full report states the basic \nalgorithm PROPAGATE and discusses ways to tune it so as to run well when the algebraic context has the \nform (LQ, MQ). Here we present the bare minimum needed to understand the experimental results. The tuned \nversion Q UALA GA TE is much longer to write down, but at least it is derived from PROPAGATE by fairly \nsimple output-preserving transformations. ) Both PROPAGATE and QOALAGATE use a worklist W of arcs c that \nneed attention because the current guessed information at the target of c might not be dominated by what \ncan be propagated along c from the current guessed information at the source of c. Each execution of \nthe statement WORK takes an arc c from W and does whatever is necessary, which may entail adding arcs \nto w . each program, a semilattice L of bit vectors is considered, with bit vector positions corresponding \nto pairs (Reg, n). where Reg is a register and n is a node corresponding to a basic block of program \ntext that assigns to Reg. A 1 bit in the position corresponding to (Reg, n) indicates that Reg received \nits present value at node n. Each qualification is an assignment of label values to each of several registers \nthat are used by the program in order to branch to whatever label is indicated by a certain register. \n(This kind of qualification is discussed more fully in Section 6.) For the present, all that matters \nis the size of Q and the average size of I Q[nl I (averaged over the nodes of the graph). Table 5.1 summarizes \nthe basic characteristics of the four programs. The last column shows what percentage of the arcs c have \n[PC] s AQ, where AQ={(q, q)lq~Q} Table 5.2 compares the costs of ordinary and qualified analysis. The \nordinary WORK column in Table 5.2 tells how often WORK was executed by PROPAGATE in solving the ordinary \nproblem $ , with qualifications ignored. The qualified WORK column tells how often WORK was executed \nby QUALAGATE in solving the problem $ # constructed by Theorem 4.2. In general qualified analysis converges \nmore slowly, though not by much. The program GRF is very intricate and has so many spurious paths that \nconvergence is faster with qualified analysis. Of course the number of executions of WORK is not the \nwhole story: each execution of WORK in QUALAGATE costs more than a similar execution in PROPAGATE. We \ntherefore consider a statement COMP in QUALAGATE that computes a member of L and has a cost comparable \nto the cost of WORK in PROPAGATE. The (C OMP) column counts executions of COMP after an intermediate \ndegree of tuning, while the COMP column counts executions of COMP after all the tuning done in the full \nreport. Comparing this column with ordinary WORK gives a rough idea of the cost expansion involved in \nqualified analysis (under our working assumption that calls on eval and on A are much more expensive \nthan housekeeping). Of course this is only the expansion involved when the basic algorithm is PROPAGATE. \nFor a rough prediction of the costs of QUALAGATE, one can multiply the average I Q[n] I by the number \nof WORK executions in PROPAGATE. The last column in Table 5.2 lists the predictions, which are rather \npessimistic, These results do not reflect several other tuning possibilities, such as exploitation of \nthe high percentage of arcs that have [PC] ~ AQ. The preliminary experiment indicates that quafified \nanalysis will be feasible for real programs, despite the factors of k and k2 for k = \\ Q I that enter \ninto worst-case bounds on the costs of steps in data flow analysis. The first author has compared the \ncosts of ordinary and The preliminary experiments implemented W by arranging qualified analysis with \n4 programs called EXT, PAG, VSI, and the arcs in a circular list and associating a bit with each arc \nto GRF. They are modules from a large operating system. For indicate whether the arc was currently in \nW. The operation take c from W moved a cursor along the list, stopping as soon as an arc with a 1 bit \nwas found. Then the bit was set to O and the arc became the value of c. For fnture take operations the \ncursor was left where it stopped. In the simplest implementation, only take would move the cursor. In \nthe experiments, however, give d to W compared d with the current cursor position and moved the cursor \nback to d when d was earlier in the list. (This slight complication of the housekeeping should hasten \nconvergence.) The arcs were ordered by an ordering of their sources, with arbitrary tie-breaking when \ntwo arcs had the same source. The node ordering was rPostorder as used in [HU75; KU76]. The net effect \nof implementing W with rPostorder is much like the net effect of beginning with the iteration in [HU75; \nKU76] and then optimizing in a natural way to avoid recomputing members of L when revisiting nodes whose \npredecessors have not acquired new members of L since the last visit. This optimization has been noticed \nbefore [HU75. p. 531], but it seems not to have been noticed that one gets to essentially the same place \nby an easier route: start with PROPAGATE from [Ki73] and implement W with rPostorder rather than the \nimplementations considered in [Ki73 ]. Two other natural implementations are as a queue (the first thing \nin is the first thing out) and as a stack (the last thing in is the first thing out). The full report \nshows that FIFO and LIFO were inferior to rPostorder in their performance on the programs analyzed in \nthe preliminary experiments. 6. LINKAGE ADDRESSES AND CALL STRINGS Assembly languages commonly have \nsome kind of load address instruction such as LDA(Reg, Lab), where Reg is a register and Lab is an instruction \nlabel. Performing this instruction puts the address indicated by Lab into Reg. A branch register instruction \nsuch as BRR(Reg) resets the instruction counter to whatever address is held in Reg. In ordhtary data \nflow analysis BRR(Reg) passes control to any instruction whose address might be held in Reg. There is \nusually more than one such address, since programmers rarely use branch register instructions when direct \nbranch instructions BRA(Lab) will do the same job. If Reg can hold either Al or A2 then a block ending \nwith BRR(Reg) will have two outarcs in the usual graph representation: one to the block beginning at \nAl and the other to the block beginning at AZ Along some control flow paths LDA(Reg, Labl) will put Al \ninto Reg and eventually BRR(Reg) will pass control to A2 even though Reg has not been changed since the \nload address instruction. These spurious paths are much like the ones in the example program from Section \n1. The preliminary experiments discussed in Section 5 use qualified analysis to avoid spurious paths \nthat branch the wrong way at BRR(Reg). Each member of the set Q = Q3 is a map from certain registers \nthat figure in BRR instructions to certain address values that figure in LDA instructions. Of course \nit is possible in assembly language to treat the result of an arbitrary computation as an address, but \nreal programs tend to very conservative in manipulating the register values they may later branch to. \nIf a register Reg appears in a BRR(Reg) instruction, then there are few relevant instructions that can \nchange Reg. The instructions that can change Reg with an effect on BRR(Reg) are usually of two forms: \nLDA(Reg, Lab) or a memory fetch FET(Reg, Mere) that puts the contents of memory cell Mem into Reg. Such \nfetches can restore Reg to hold an address that was saved by a memory store instruction STO(Reg, Mere) \njust before a computation that used Reg for some other purpose. The instruction types just reviewed (load \naddress, branch register, branch, memory fetch, and memory store) are enough to illustrate how qualified \nanalysis may be applied to assembly language programs. With a slight modification they will also illustrate \nthe relation of qualified analysis to interprocedural analysis. Consider a simple way to implement nonrecursive \nlinkage. A single register Reg will suffice, provided we also use a memory cell Mem for each procedure. \nThe code for a call on a procedure with a cell Mem for holding return addresses will arrange for parameter \ntransmission and then continue in the following stereotyped way, where Lab indicates the entry point \nof the called procedure, [ STO(Reg, Mere) / LDA(Reg, Lab) / BRA(Lab ) / Lab : FET(Reg, Mere) ]. (6.3.1) \n The called procedure can return simply by executing BRR(Reg). (6.3.2) Of course (6.3) is not the only \nway to code the control aspects of procedure linkage, but it is typical enough and easy to write down. \nThe same information contained in the choice Q = Q3 of qualifications in the preliminary experiments \ncould be encoded by strings of addresses: each LDA(Reg, Lab) in (6.3. 1) adds the address indicated by \nLab to the end of the current string, and each FET(Reg, Mere) removes the last address from the current \nstring. The length of address strings is bounded by the bound on lengths of call chains in a nonrecursive \nprogram. For such programs the call strings approach to interprocedural analysis in [SP78, Sec. 4] is \nessentially the same as context tupling with Q taken to be this encoding Q3 of Q3. For example, our qualifying \npaths with Q = Q3 correspond to interproceduralti valid Parbs [Sp78, Sec. 3], as is apparent from [SP78, \nLemma 4.1]. The theorems in [SP78] are restricted to strongly distributive algebraic contexts (L, M). \nIn such contexts [SP78, Thin. 4.6] is a corollary of Theorem 4.2 when nonrecursive programs are considered. \nBecause [SP78] considers recursive programs too, it is important to consider what happens when our theory \nis generalized to avoid the assumption that Q is finite. With infinite Q all cases of [SP78, Thin. 4.6] \nwould come under = TL. The following theorem refines and simplifies results Theorem 4.2. implicit in \n[We75, Sec. 3B]. With Q allowed to be infinite Section 3 is still meaningful, but the relevant qualifications \nat each node can no longer be found by straightforward iteration in finitely many steps. Theorem 4.2 \nis still true and even has the same proof. Like [SP78, Thin. 4.6], Theorem 4.2 does not immediately yield \nalgorithms when Q is infinite. At least when Q = Q3 and L is finite, Section 5 of [SP78] shows how to \nrestrict attention to a finite subset of Q without loss of information. The combinatorics are such that \nit is difficult to envision circumstances under which call strings would be preferable to the functional \napproach in [SP78, Sec. 3]. However, call strings are of interest here for two reasons. First, they yield \na simple definition of the goals of interprocedural analysis as a special case of qualified analysis, \neven though one may prefer the functional approach for doing the work. Second, the ideas in [SP78, Sees. \n5, 6] may generalize to other situations with infinite Q but finite sets EQm, at least for finite L. \nReturning to the case of nonrecursive procedures and finite Q = Q3, we note that context tupling is more \nrobust than use of call strings. Sections 4 6 of [SP78] can only be applied rigorously to programs with \na strict procedure-calling discipline, expressed either by explicit calls or by low-level code that obviously \nsimulates them, as in (6.3). The slightest departure from procedure calling will reopen all the questions \nabout call strings. Proceeding from the general to the particular, we proved Theorem 4.2 and its corollaries \nlong before considering (6.3). Context tupling is rigorously applicable to low-level code that uses instructions \nlike BRR mostly in the manner of (6.3) but occasionally in some other way, perhaps because of error exits. \nThe worst-case assumption PC = Q x Q is available for the very rare occasions when a program treats the \nresult of a nontrivial computation as a linkage address. 7. DATA FLOW TRACING A special kind of symbolic \nexecution has been called valuation [Si72] or expansion [We75]. We will use a longer but more mnemonic \nphrase: dara fZow tracing. Given a global data flow problem with a finite semilattice, we construct a \nnew graph H with some useful properties. The nodes of H are pairs (n, x), where n is a node from the \noriginal graph G and x is in the semilattice. There is an arc Cx from (n,x)to(p,y)ifthere isanarccin \nGfrom ntopwith (fc)x = y. In the full report H is constructed in the natural way by a program GENTRACE \nappropriate for any algebraic context with a fkrite semilattice. In the common special case where UT \n~ = T~ for all U in M, a smaller graph H is just as good because nodes of the form (p, TL) are unnecessary. \nFor this special case we can replace GENTRACE with SPECTRA (2?I, a program that leaves out CHand (p, \ny) when y THEOREM 7.1. Let $ = (G, f, E, E) be a global data flow problem with L finite and let H be \nthe output from GENTRACE (or SPECTRACE in the special case) with input 9 . Then I: No+ Ldefined by In= \nA{xl(n, x)c NH} is the maximum solution for .9. Data flow tracing is of interest for several reasons. \nWhen L is finite and M is not distributive, it provides a possibly slow but definitely simple way to \nfind the maximum solution for a problem. When fragments of program text are associated with the nodes \nin G, the expanded graph H can be used to specify an expanded program that is more amenable to optimization. \nOur interest in data flow tracing comes from the case where 9 is ~aux from Def. 3.5 for a qualified flow \nproblem. Optimizing SPECTRACE for this application to qualified data flow problems, we let the nodes \nof H be pairs (n, q) with q < Q rather than pairs (n, u) with u s Q. The following program results. QUALTRACE \n: begin dcl (G,E,Q, (PCIceAG), (EQm Im~E)) qualified flow scheme; dcl W subset of NG x Q; dcl H graph; \ndcln,pnodes inG; dclcarc inG; dclq,rinQ; get G, E, Q,(pclc6AG), (EQm lm~E); W+{(m, q)lm<Eandq<EQm }; \nH + (Graph with node set W and arc set @); while W # @do [ take (n, q) from W; for aft c in sG l(n) do \n[ p + tGc; for all r in (q PC ?) do TRACE_ARC : [ if ~ (p, r) 6 NHthen [ (Add node (p, r) to H); give \n(p, r) toW]; (Add arc Cx from (n, q) to (p, r) in H) 1 1 1; put H end 76 THEOREM 7.2. Let @ = (G, f, \nE, E) be qualified by Q, PCfor each arc c in G, and EQm for each m E E. Let G# be the output of C)UALTRACE \nwith input G, E, and so on. Let @ be the problem (G*, f#, E#, E#), where Fcx = fcfor c the arc in G that \nmakes TRACE_ARC add c# to G*; E#={(m, q)lmc E&#38;q6EQm ]; E#(m, q) = Em. Suppose 1# is a good solution \nfor @. Then a good qualified solution for Y is obtained by setting, for each node n in G, In = A {I#(n, \nq) I (n, q) isanode in Gs ]. COROLLARY 7.3. As in Theorem 7.2, any iterative algorithm that can solve \n$ alone can also solve @ with qualifications by first solving d. COROLLARY 7.4. As in Theorem 7.2, any \nelimination aIgorithm that can solve @ alone can also solve @ with qualifications by first solving @, \nprovided that the algorithm can cope with arbitrary graphs. Two questions are important in studying the \nefficiency of data flow tracing for qualified problems. First, is G# much larger than G? The ratio I \nNG, 1/ ] NG ] is precisely the average of I Q[n] I over all n in NG, and the ratio I Ao, I / I AG I is \nlikely to be a little less. One can arrange to have nearly I Q I nodes in G# for each node in G, but \nthis may only happen in programs written to establish worst-case bounds on the size of G#. In any case, \nthe implementation strategy outlined in Section 5 can be applied to data flow tracing also. Second, is \nGi qualitatively less tractable for analysis than is G? For example, consider the class of reducible \n[HU74] graphs. In practice the graph G is usually reducible [KZ77; Kn71 ], but G* is another mattter. \nThe frill report illustrates how easy it is to lose reducibility when G# is constructed. Thus we are \nfaced with the unappealing prospect of applying algorithms with quadratic worst-case bounds to graphs \nsubstantially larger than the original graphs. Worst-case bounds are not the whole story, however. The \npractical experience with iteration has been much better than worst-case bounds would suggest, as evidenced \nby [MR79, p. 98] as well as our experience. When W is managed with rPostorder, the number of executions \nof WORK is usually less than 3 I AG I and often less than 2 I AG I , at least for relatively simple traditional \nproblems like available expressions. Data flow tracing was applied to 3 of the 4 programs from Section \n5, with GRF being too large for the experimental implementation. Table 7.2 compares computational effort \nunder the two techniques: context tupling with QUALAGATE and data flow tracing with PROPAGATE applied \nto the graph G# The qualifed WORK and COMP columns come from Table 5.2 while the WORK after tracing \ncolumn counts executions of WORK by PROPAGATE with input graph G* prepared by QUALTRACE. The preliminary \nexperiments suggest that context tupling is somewhat more efficient than data flow tracing when iteration \nis used for ordinary analysis, but the difference is small and might well be cancelled out by housekeeping \ncosts. With a modest amount of tuning, data flow tracing becomes essentially as fast as context tupling \nunder the conditions of the experiment, The programming effort required to implement context tupling \nis greater. To code QUALAGATE is more work than to code QUALTRACE, code PROPAGATE, and finally code a \nsimple linkage from output of QUALTRACE to input of PROPAGATE. The modularity of qualified analysis with \ndata flow tracing is a major advantage. On the other hand, some applications may have more repetitions \nof nontrivial vector components: (In)q = (In)r # TL for q # r. For such applications the more elaborate \ntuning in QUALAGATE may be worthwhile. It must be remembered that the comparison here deals with low-level \nprograms and simple traditional bit vectors. It must also be remembered that we have dealt with exhaustive \nanalysis of entire programs, as is usual in the literature on data flow analysis. The practical cost \nconsiderations are very different (and largely unexplored) when one deals with high-level programs, with \nmore ambitious analysis (e.g. static type checking or subscript range analysis [Ha77]), or with demand \nanalysis [BJ78] of large programs that sometimes undergo small changes. In short, our comparison favoring \ndata flow tracing over context tupling has been under the same conditions that favor iteration over elimination. \nThe only general data flow algorithm designed for conversion to demand analysis of high-level programs \nis the elimination algorithm of [Ro80a], and elimination algorithms are generally more adaptable for \nthese purposes than iterative algorithms. Further discussion of the choice between iteration and elimination \nis in [Ro80b, Sec. 6] and the references cited there. The choice between context tupling and data flow \ntracing seems to reduce to the choice between elimination and iteration for ordinary analysis. Having \nchosen elimination (or iteration), one should choose context tupling (or data flow tracing) for qualified \nanalysis. 8. PRACTICAL SIGNIFICANCE The narrowest kind of assessment is suggested by the corollaries \nin Sections 4 and 7. Given a qualified problem, we might ignore the qualifications and find an ordinary \ngood solution Iord. Most of the algorithms that could be used for this purpose could instead be used \nto find a good qualified at greater cost. We can ask whether the solution lqual additional information \nis worth what it costs. The answer wiIl depend on the program from which the probIem is abstracted. A \nprogram that has a long lifetime and is expected to be unusually reliable or efficient will deserve a \nthorough analysis. The most extreme examples of thk kind of program are operating systems. The first \nauthor has applied qualified analysis to 4 modules from a large operating system, with costs that have \nbeen discussed in Sections 5 and 7. Table 8.1 shows the increase in 1 bits (averaged over the nodes in \nthe graph) when Iqual is compared with Iord. The last column shows the increase in 1 bits when the average \nover q in Q[n] of the number of 1 bits in (Ixn)q is compared with Iordn (and then averaged over the nodes). \nTo obtain Iqual we first obtain 1~ with values in LQ, and these Q-tuples can be saved for future reference. \nIf ~ = I% has nontrivial .$q # f,, then I% tells us more than Iqual. Merely by accessing tuples, we can \nobtain a sharp answer to a question of the form Suppose control reaches thk node with that qualification. \nWhat is sure to be true? With nothing but Idrd or even Iqual to consult, we would either have to reanalyze \nthe program or reply with only what is sure to be true regardless of the qualification in the question. \nFor the four programs analyzed in Table 8.1, the last column shows that I* can be expected to be 20 or \n30 percent more informative than Iord, while a few gnusually intricate programs like GRF will yield dramatic \nincreases. To determine whether the information is worth what it would cost to obtain with a polished \nimplementation of qualified analysis, we would need to perform an elaborate experiment that estimates \nthe costs of making do without the information as well as the costs of obtaining it. A broader view \nshould be taken. Precisely because the mathematical theory gains elegance by considering data flow problems \nabstracted from the concrete programs that pose them, it is important to remember that practical data \nflow analysis has a program, not a problem, as the ultimate input. Passing from a program to a problem \nis a delicate task if one wants meaningful information about the program. For example, consider availability \nof expressions involving subscripted variables. Because i and j can have the same value, assignments \nto A[i, k] must be taken as rendering A[j, k] + B unavailable. On the other hand, because i and j can \nhave different values, computations of A[i, k] + B must not be taken as making A[j, k] + B available. \nTo insure meaningful availability information, whoever specifies f : AG e M is obliged to assume precisely \nthe opposite of whatever is desirable, and neither i = j nor i # j is desirable throughout. When one \nobserves that a program instruction obliging the assumption i = j is followed by one obliging i # j without \nany intervening changes to i or j, one begins to feel that there must be a better way to proceed. There \nis. Let Q={i =j,i#j}andconsider howthe truth of either assumption is affected as control flows along \neach arc in the graph chosen to represent control flow in the program. We get a qualified flow scheme \nand an auxiliary problem as in Def. 3.5. The relevant qualifications at each node can be found. Now suppose \nc is an arc from n to p. If q in Q[n] holds at n and r in Q[p] holds at p, then it is relatively easy \nto specify a member of M that transforms information about the availability of A[i, k] + B correctly \nfor this particular pair (q, r). Doing so, we get @,q in M. Considering all pairs (q, r) and using T~ \nfor irrelevant pairs. we get a matrix @ that represents a map + in MQ. Forming @ would be a prudent first \nstep in trying to choose a safe fc in M to cover all possibilities, since we could then take fc to be \nA { @,q I q, r e Q ]. Rather than choose fc in this way and then solve a qualified problem, we should \nsolve the ordinary problem @ with fxc = @. Unlike the problem @ from Theorem 4.2, constructed from a \nqualified problem with no reference to the actual program, this problem 9@ is constructed from the program \nin a way suggested by context tupling. Except for the fact that even iteration algorithms will sometimes \nencounter distinct nontrivial entries in the same matrix, the implementation considerations from Section \n5 apply. Solving .@ need not be as costly as a glance at I Q I would suggest. (In this simple example \nI Q I is only 2 anyway. ) Because the matrix Q is much more informative than /l {@,qIq,rcQ],solving @ \nwill bemuch more informative than applying Theorem 4.2. Without being applied in any strict logical sense, \ncontext tupling has a beneficial effect on the way we formulate problems in the first place. Context \ntupling may be more useful because of its feedback effect on problem formulation than because of results \nlike Theorem 4.2. In this setting a further gain in the sharpness of data flow information can be had \nby allowing the entry information Ex : E ~ LQ to have nontrivial &#38;q # fr. With ourexample Q={i=j,i#j}thismaynotbe \nimportant, but consider call strings as discussed in Section 6. A compiler could analyze a routine in \nisolation from the routines that call it, but without failing to utilize any fact abstracted from the \ncallers. For example, there might be two expressions such that one is available on entry when the caller \nis Alphonse while the other is available on entry when the caller is Gaston. The routine can be analyzed \nwith (E m)q varying according to whether q indicates that the caller is Alphonse or Gaston. Without a \ntheory of the relationship between programs and problems, we cannot summarize the feedback benefits of \nour methodology in crisp mathematical theorems. For the present we make do with telling examples. REFERENCES \nAU77. Aho, A. V., and Unman, J.D. Princ@Les of Compikr Design. Addison-Wesley, Reading MA, 1977. AC76. \nAllen, F. E., and Cocke, J. A program data flow analysis procedure. Comm. ACM 19 (1976), 137-147. BJ78. \nBabich, W, A., and Jazayeri, M. The method of attributes for data flow analysis (Part II. Demand analysis). \nActs Infoimatica 10 (1978), 265-272. GW76. Graham, S.L., and Wegman, M. A fast and usually linear algorithm \nfor global flow analysis. J, ACM 23 (1976), 172-202. Ha77. Harrison, W.H. Compiler anaIysis of the value \nranges for variables. IEEE Trans. on Software Engineering 3 ( 1977), 243-250. HU75. Hecht, M. S., and \nUnman, J.D. A simple algorithm for global data flow analysis problems. SIAM J. Computing 4 (1975), 519-532. \nJM80. Jones, N, D., and Muchnick, S.S. (Eds. ) Program FkJ w Analysis: Theory and Applications. Prentice-Hall, \nEnglewood Cliffs NJ, 1980(?), to appear. KU76. Kam, J. B., and Unman, J.D. Global data flow analysis \nand iterative algorithms. J. ACM 23 (1976), 158-171. KZ77. Kennedy, K. W., and Zucconi, L. Applications \nof a graph grammar for program control flow analysis. proc. 4th ACM Symp. on Principles of Programming \nLanguages (January 1977), 72-85. Ki73. Kildall, G.A. A unified approach to global program optimization. \nProc. ACM Symp. on Principles of Programming Languages (October 1973), 194-206. Kn71. Knuth, D.E. An \nempirical study of FORTRAN programs. Software Practice and Experience 1 (1971), 105-134. MR79. Morel, \nE., and Renvoise, C. Global optimization by suppression of partial redundancies. Comm. ACM 22 (1979), \n96-103. R078. Rosen, B.K. Monoids for rapid data flow analysis. Proc. 5th Ann. ACM Symp. on Principles \nof Programming Languages (January 1978), 47-59. Ro80a. Rosen, B.K. Monoids for rapid data flow analysis. \nSIAM J. Computing 9 (1980), to appear. (An earlier version with different numbering appears as [R078].) \nRo80b. Rosen, B.K. Degrees of availability as an introduction to the general theory of data flow analysis. \n(To appear in [JM80].) SP78. Sharir, M., and Pnueli, A. Two approaches to interprocedural data flow analysis. \nReport 002, Computer Science Department, New York University, New York, September 1978. (Revision to \nappear in [JM80].) Si72. Sintzoff, M. Calculating properties of programs by valuations on specific models. \nProc. ACM Symp. on Proving Assertions about Programs, SIGPLAN Notices 7, 1 (January 1972), 203-207. Ta75. \nTarjan, R.E. Solving path problems on directed graphs. Rept. STAN-6-75-528, Computer Sci. Dept., Stanford \nU., November 1975. U173. Unman, J.D. Fast algorithms for the elimination of common subexpressions. Ac?a \nZnfornratica 2 (1973), 191-213. We75. Wegbreit, B. Property extraction in well-founded property sets. \nIEEE Trans. on So@WrE Engineering 1 (1975), 270-285. FIGURES AND TABLES next three pages m YES ASSIGN/1 \nX+A+B + AsslGN/2 COLOR +-BLUE YES AsslGN/3 )(-+(A+B)+Y COLOR = RED? I Figure 1.1 Thevalue of A+ Bisavailable \nwhenever ASSIGN/3 isreached byapossible path from TEST/l. o 0 0 Figure 2.1. Bits telling whether A + \nBisavailable appear at each of the nodes. Ordinary andqualified analysis put different bitsat ASSIGN/3 \nbut the same bits at other nodes. The entry information is Oat TEST/l. All but two arcs chavefc = 1. \nFor cthe arc from ASSIGN/ 1 to TEST/2 and for c the arc from ASSIGN/3 to NEXT, fc = y. Q NEED-OTHER NEED.RED \nv ASSIGN/1 v NEED.OTHER AsslGN/2 GET-BLUE AsslGN/3 Figure 3.1. Qualified flow scheme for the example \nfrom Figure 1.1. Unmarked arcs c have pc the identity relation on Q (000) v [ (022) I (loo) (122) Figure \n4.1. Each tuple f is displayed (f RED~BLuEfwHA). For c the arc from ASSIGN/1 to TEST/2 and for c the \narc from ASSIGN/3 to NEXT, f% corresponds to the matrix Q with Q,q = (if r = RED = q then Y else TM). \nOther arcs c have f*c 2 1 in MQ, with each matrix entry being 1~ or T ~. bit vector average percentage \nprogram NG I IAGI IQ I length I Q[nl I [PC] s AO RX-I 144 203 120 8 2.09 85 PAG 232 347 269 19 2.63 82 \nVS1 347 537 255 18 2.67 78 GRF 743 1102 655 60 2.80 72 Table 5.1. The average number of relevant qualifications \nat a node n is less than 3. More than 70 percent of the arcs c have only pairs (q, r) with q = r in their \nrelevant relations. average ordinary qualified program (COMP) COMP prediction I Q[nl I WORK WORK EXT \n2.09 261 346 963 570 545 PAG 2.63 866 1022 2756 1842 2278 VSI 2.67 682 693 1617 1613 1821 GRF 2.80 2247 \n1964 6897 3076 6292 Table 5.2. Cost comparison between ordinary and qualified analysis. qualified WORK \nprogram COMP WORK after tracing EXT 346 570 587 PAG 1022 1842 2625 VSI 693 1613 1658 Table 7.2. Cost \ncomparison between context tupling and data flow tracing. bit vector average 1# IOrd lqual lord program \nlength I Q[nl I Iard Iord EXT 120 2.09 4.4 20.5 PAG 269 2.63 4.6 29.1 VSI 255 2.67 7.3 29.0 GRF 655 2.80 \n199.0 262.5 Table 8.1. Percentage increases in number of 1 bits. The number of 1 bits in I qualn is compared \nwith the number of 1 bits in IO,dn) and the percentage is averaged over all nodes n. The average over \nq in Q[n] of the number of 1 bits in (I%)q is compared with the number of 1 bits in Iordn, and the percentage \nis averaged over all nodes n. \n\t\t\t", "proc_id": "567446", "abstract": "It is known that not all paths are possible in the run time control flow of many programs. It is also known that data flow analysis cannot restrict attention to exactly those paths that are possible. It is therefore usual for analytic methods to consider all paths. Sharper information can be obtained by considering a recursive set of paths that is large enough to include all possible paths but small enough to exclude many of the impossible ones. This paper presents a simple uniform methodology for sharpening data flow information by considering certain recursive path sets of practical importance. Associated with each control flow arc there is a relation on a finite set Q. The paths that <i>qualify</i> to be considered are (essentially) those for which the composition of the relations encountered is nonempty. For example, Q might be the set of all assignments of values to each of several bit variables used by a program to remember some facts about the past and branch accordingly in the future. Given any data flow problem together with qualifying relations on Q associated with the control flow arcs, we construct a new problem. Considering all paths in the new problem is equivalent to considering only qualifying paths in the old one. Preliminary experiments (with a small set of real programs) indicate that qualified analysis is feasible and substantially more informative than ordinary analysis. The methodology also has a beneficial feedback effect on the delicate task of passing from programs to meaningful data flow analysis problems. Even when all paths qualify, unusually sharp information can be obtained by passing from programs to problems in ways suggested by our theorems.", "authors": [{"name": "L. Howard Holley", "author_profile_id": "81339504931", "affiliation": "IBM Cambridge Scientific Center, Cambridge, MA", "person_id": "P383055", "email_address": "", "orcid_id": ""}, {"name": "Barry K. Rosen", "author_profile_id": "81100316668", "affiliation": "IBM Thomas J. Watson Research Center, Yorktown Heights, NY", "person_id": "P28116", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/567446.567454", "year": "1980", "article_id": "567454", "conference": "POPL", "title": "Qualified data flow problems", "url": "http://dl.acm.org/citation.cfm?id=567454"}