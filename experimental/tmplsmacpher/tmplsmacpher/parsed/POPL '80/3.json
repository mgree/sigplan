{"article_publication_date": "01-28-1980", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; \n1980 ACM 0-89791-011-7 $5.00 professional programmers and computer scientists. They had a good display-oriented \ntext editor, but no text\u00adoriented language like SNOBOL [Griswold] or any UNIX facility like AWK [Aho] \nor LEX [Lesk]. It has received most of its use from people with a clerical task that is regular eIIOLlgh \nto be tedious, but not recurrent enough to justify a big progratnming effort in a more conventional language. \nA typical comment has been: In a couple of hOUH I was able to learn Poplar and use it to solve a problem \nthat would have taken much longer otherwise. A few people wrote more serious programs: a report generation \nsystem for software projects, a family budget maintainer, a correspondence management system for academic \njournal editors, a purchase order management system. Large portions of some of these projects have been \nwritten applicatively, Basics of Poplar Values and functions Strings are primitive values and are written \nin quotes; e.g. A string and , Concatenation of strings is denoted by juxtaposition: aaa bbb = aaabbb \nAs in SNOBOL, a number is simply a string of digits. The quotes can be omitted: 123 = 123. Addition and \nsubtraction can be written as infix operations. The special primitive value fail plays the role normally \nplayed by Boolean values in Algol; conditional expressions test their parameters for being fail or not, \nrather than true or false: if p then x else y = if p=fail then x else y Non-primitive values are either \nlists or functions, Lists are written like [ A , list ] and []. Lists may be subscripted: ~ A , B ]2 \n= B . ([n reality, subscription is written as L/i rather than Lt) A negative subscript -i yields the \nlist with its first i elements removed. [ A , b , C , d ]-2 = [ c , d ]. Lists can be concatenated with \nthe inf~ operator ,, . The familiar Cons(x, y) operation of LISP can be accomplished with the idiom [x] \n,, y which places x in a list of length one and concatenates it with y. Functions are denoted by lambda \nexpressions except that instead of Ax. one writes x: . An expression like ([x,y]: x + y) is an abbreviation \nfor (z: Z1+ zJ. The application of fhnctions to parameters is writtf n in post-fix notation using the \noperator 6/ . 3+9 / (t: ttt) = 121212 There is a standard assignment statement x ~ e; it is used mostly \nfor defining functions at the top level. The precedence of : is such that one can conveniently use post-fixed \nfunctions as a sort of assignment statement, L/x: X+X = L/(x: X+X) and is equivalent to (x ~ L; x+x). \nEquality Assertions To make programs readable there is a checked comment facility. Any function definition \ncan be decorated with a set of assertions which constitute a test evaluation of the function. For example, \ngiven the fhnction x: [x,x]/Cone/Reverse one can adci equality assertions to produce x: = foo [x,x]/Cone \n= foofoo /Reverse = oofoof which says: Ifthe input is foo the valueof [x,x]/Cone }till be foofoo and \nthe final value will be oofoof . This idea has worked out well: it is much easier to grasp what a program \nis doing if a well-chosen exalnple is interleaved with it. The fact that the example is machine\u00adchecked \nmakes it more credible than a normal comment, In practice, one needs mechanical aids to generate examples \nbecause of all the details (e.g., How many spaces are in ?) which escape the reader, but not the checker. \nPost-fix syntax and built-in iteration Since applicative programming has been employed mostly by meta-programmers \nrather than programmers many of the syntactic creature comforts, like for-loops, are absent from applicative \nlanguages, The applicative style usually requires the use of many recursive function definitions, one \nfor every loop. To remedy this situation Poplar supplies several built-in iterative operators. String \nconcatenation and the arithmetic operations extend to lists of strings. Three iterative functional are \ninfix operators: LISP s Maplist, APL s reduction operator, and an operator similar to the p operator \nof recursive fMction theory. Like function application these three operators are written with the function \nsecond rather than first, [a, b, c]//f = [a/f, b/f, c/fl (Maplist) [a, b, c]///f = [[a,b]/f, c]/f (Reduce) \nx%f = if x/f then (x/f)%f else x (p-operator) A sequence of numbers can be generated by the notation \n4--7 = [4, .5,6, 7] A list of eqilal length of lists may be transposed, [[a, b, c], [d, e, fj]/Transpose \n= [[a, d], [b, e], [c, fl] Transpose is important because it allows one to generalize a non-unary fhnction, \nf, to work on lists via the idiom [Listl, List2]/Transpose//f The combination of built-in iterators and \npost-fix notation was very successful; succinct applicative programs to do complicated things could be \nwritten easily without using recursion. Furthermore, writing such programs became a simple, natural process, \nrather than a challenge to the intellect. As an example, consider the key-word-in-context problem discussed \nby [Parnas]: given a list of book titles like Green Sleeves Time Was Lost generate an alphabetized list, \nuseful for looking up specific key words: <Green> Sleeves Time Was <LosD Green <Sleeves> <Time> Was \nLost Time <Was> Lost The procedure is as follows: Break the text up into lines. Break each line up into \nwords. For each line: Generate a list of pairs, one for each word, consisting of the word, and a reconstruction \nof the line with brackets around the woi-d. Merge all these lists into one big one, Sort the list by \nthe words. Discard the words. Concatenate all the lines to form tie final text. 34 Figure 2. The Program \nAnnotated with Equality AssertionsFigure 1 shows the Poplar program to do this, and Figure 2 shows the \nsame program decorated with equality KWIC @ (S: = Green Sleeves~Time Was Lost~ assertions. The major \nsteps correspond to the informal steps above. The character ~ stands for carriage-return. The functions \nLines and Words are patterns, to be discussed later, that split text into lines and words, respectively. \nAppend concatenates pairs of lists; Cone concatenates pairs of strings, The phrase //2 is a shorthand \nfor //(x:x2) . Besides using a non-trivial recursion, the function G makes heavy uses of the implicit \niteration of concatenation. The subexpression ( w.,) puts a space at the beginning of eLLchword before \nthey are concatenated by Cone. The subexpression (wl  (w.l/G)) prefixes the current word WI to every \nstring in the list that the recursive call of G returns. Figure 1. A Poplar Program for Key-Word-in-Context \nKWIC ~ (S: s/Lines //Words //(WList: [WList, WList/G]/Transpose) ///Append /sort //2 ///Cone); G f (w: \nif w/isnull then n else [ < WI > ( w.l///Conc J )] >, (WI (wI/G))); Notice that the informal description \nof this procedure consists of quite imperative statements while the program itself is entirely applicative! \nThis is the advantage of post\u00adfix syntax. The key to this is that at any point in the program there is \nonly one thing being dealt with, and it plays the role normally played by the state of the machine in \nan impemtive program. Many programs have been written in this style, often interactively. The system \nallows one to type something like /F as a command, and function F is applied to the last thing printed \nout, and then the result is printed. A transcript is kept, and the user may edit this transcript to produce \na program. s/Lines = ~ Green Sleeves , Time Was Lost ] //Words = [[ Green , Sleeves ], [ Time , Was , \nLost ]] //(WList: ~List, WList/G]/Transpose) =[[[ Green , <Green> SleevesJ ], [ Sleeves , Green <Sleeves> \n~ ]], [[ Time , <Time> Was Lost~ ], [ Was , Time <Was> Lost) ], [ Lost , Time Was <Lost> ~ ]]] ///Append \n=[[ Green , <Green> S1eeves~ ], [ Sleeves , Green <Sleeves>~ ], [ Time , <Time> Was LostJ ], [ Was , \n Time <Was> Lost~ ], [ Lost , Time Was <Lost>~ ]] /Sort =[[ Green , <Green> Sleeves) ], [ Lost , Time \nWas <Lost>~ ], [ Sleeves , Green <Sleeves>~ ], [ Time , <Time> Was Lost) ], [ Was , Time <Was> Lost~ \n]] //2 = [ <Green> Sleeves~ , Time Was <Lost> ~ , Green <Sleeves>) , <Time> Was Lost~ , Time <Was> LOst~ \n] ///Cone = <Green> Sleeves Time Was <Lost> Green <Sleeves> <Time> Was Lost Time <Was> Lost ,,); G ~ \n(w:= [ Time , Was , Lint ] if w/isnull then g else [ < WI > ( w-l///Cone) J ] >> (Wl  (w-l/G = [ \n<Was> Lostj , Was <Lost>) ]) ) = ~ <Time> Was Lost~ , Time <Was> Lost~ , Time Was <Lost> ~ ]); 35 This \nprogram is rather inscrutable, but we believe that translating it to a more conventional notation makes \nit worse. In Figure 3 the program appears written in an Algol/LISP style of syntax, i.e., changed to \na prefix notation with all the maplist and reduce operations explicit. To make the nesting tolerable, \nwe introduced many assignment statements; imagine how the program would look if we eliminated them by \nback-substituting! Of course the assignment statements give one the opportunity to int~ociuce a mnemonic \nidentifier to describe the intermediate result. Thus the opaqueness of the program is due to the style \nof expression rather than W syntax of the language. Figure 3. The KWIC Program Written in Algol/LISP \nprocedure KWIC(S) begin ListofLines ~ Lines(s); ChoppedLines ~ maplist(Listofhes, Words); ListofListsofPairs \n~ maplist(ChoppedLines, AWList.T ranspose(WList, G(WList))); ListofPairs f reduce(ListofListsofPairs, \nAppend); OrderedList ~ Sort(ListofPairs); ListotStrings + maplist(OrderedList, Xx.x2); return reduce(ListofStrings, \nCone) end procedure G(w) begin if isnull(w) then return []; Fint ~ < W1 > reduce(maplist(w.l, Ax.  \nx), Cone) ~ ; Rest + maplist(G(wl), Ax. W1  x); return Cons(First, Rest) end Problems with the syntax \nPoplar s users and potential users had mixed feelings about th~ syntax, Even aspects we consider successful \nwere not un%ersally appreciated. No one was ever sure what the precedence rules were or should be. Postfm \nsyntax, even if one likes it, has problems: for functions that were binary, one has an urge to place \none of the arguments after the function name. This syntactic style has evolved in Smalltalk [Kay]. If \none wrote a function\u00adproducing function like F ~ (x: y: x+ y), a call of the function, Y/(X/F), did not \nlook right. It was not obvious how to indent programs; in something like ~ugeExpressionl, HugeExpression2] \n/TinyFunction, the function name would get lost. Instead of providing if-then-else expressions (as this \npaper suggests) we used two binary operations > and I with the following definitions: x > y = if x= fail \nthen fail else y xIy = ifx=fail then yelsex This allows one to write things like (BigExpression ] u) \nrather than tie more cumbersome t @ BigExpression; if t= fail then L1 else t The conventional if p then \nx else y could almost be achieved by (p > x I y). In retrospect, this syntax caused more confusion than \nit was worth. A better set of operators Although the buiit-in iterators were successful in general, we \nnow have a better idea of what they should be. The maplist operator had the feature that if a value in \nthe output list was fail it was omitted. This was handy, but occasionally it tended to bury errors one \nwould like to discover. There should be a separate operator to accomplish this, perhaps Split defined \nas follows: [1, p]/Split = [sheep, goats] where sheep is the list of items on list 1 for which p is true \n(i.e. not fail) and goats is a list of all the others. 36 The reduce operator /// was confusing to use \nwhen the Clever Evaluation is essential function was not associative. The following definition The KWIC \nprogram is inefficient by contemporary would have been more useful: standards. Every line seems to create \na large new structure [x,y,z]///F = [X, [y, [Z, U]/F]/F ]/F which the following line consumes. Great \nimprovements in It processes the list from right to left and includes the this algorithm s performance \ncan be made by a little empty list in the enumeration. This would allow us easily cleverness in the evaluation \nstrategy. We recently changed to solve the bothersome problem of eliminating adjacent the implementation \nto use the lazy evaluation strategy repetitions from a list. described in [Henderson&#38;Morris] so these \nmulti-pass operations are merged. The essence of the technique is that [1,1,3,4,1,2,2] nothing is evaluated \nuntil it absolutely must be. Under this ///([x,y]: if (-y/isnull and x= Yl) then Y else [x],,Y) regime \nlists often behave like streams because their tails = [1,3,4,1,2] remain unevaluated until they are needed. \nIn the case of Of course, this redUCe would have not have worked for KWIC the first operator that forces \nevaluation is Sort which demands that it receive a list of lists each of whose tirst functions like Plus \nand Cone which didn t expect to see the components is a fully evaluated string. This causes the Append \noperation to be completed, but the second null list as an argument; but such ftmctions could be extended \nto take lists of parameters as a matter of course component of each pair remains unevaluated until the \nfinaJ making their use with reduce unnecessary. reduction using Cone. Thus, in principle, this program \nrequires only enough space to create a list of all the The general iteration operator, %, was not very \nuseful. individual words and does not require space proportional to Perhaps we should have built in the \nlist iterator described its output which approximates the square of the input. by [Burge] which is something \nlike Notice that the revised definition of the reduce operator Lit fghax=if ~x)thena eIsegx(Litfgha works \nmuch better under lazy evaluation. For example, the beginning of the value of L///Append can emerge before \n(hx)) The need was felt for ways other than the sequence L has been completely traversed. operator to \ngenerate lists from whole cloth. For example, the following function might be useful: [a, fl/GenList \n= if a= fail then U else Since lists are never fully evalu-.ted one can even deal with [a] ,, ([a/f, \nfl/GenList) infinite lis~. The Fibonacci numbers may be described by the recursively defined list Fib. \nAn infix functional composition operator, e.g. Fib + [1,1] ,, (Fib + Fib.l) fog == (x: x/f/g) Suppose \none want to find the first Fibonacci number that is would have been used frequently divisible by 3. He \ncan say [Fib, div3]/Search where Search can be defined in terms of Split the obvious way. This will not \ninvolve computing any more elements than a more 37 conventional program would. In general, any while \nloop could be written in this way: s ~ a; while P(s) do s ~ F(s) can be simulated by [[a, F]/GenList, \nP]/Search Our implementation of lazy evaluation has not been a complete success for reasons which we \nshall discuss latter, but it has allowed us to be hopeful that this style of programming may someday \nbe more practical. Pattern Mitching There are two aspects to the design of pattern matching: the parsing \nof strings and the post-processing of successful parses. We devoted most of our effort to the second \nof these, on the theory that a great deal is known about the first. In essence, the matching sub-language \nis the language of regular expressions, A primitive pattern is either a string or the ellipsis .,. which \nmatches anything (like SNOBOL S ARB). Larger patterns may be constructed from smaller ones by using four \ncombination rules: if P and Q are patterns, then so are the following PQ concatenation PIQ alternation \nP~=PIPP]PPP etc. iteration p?=(p l ) optional The Kleene star pattern P* can be written as P~?. Every \npattern is enclosed in braces {} . Since patterns can be assigned to variables it is possible to create \nrecursive patterns. For example, E ~ {digit~ I ( E + E ) } A simple parsing algorithm causes problems \nRather than use a general parsing algorithm like [EarIey] s we chose an ad hoc matching algorithm of \nthe no-back-up variety. We felt it would be a lot of work to implement a general parsing algorithm that \nwould run as fast as an ad hoc one, Furthermore, it was not clear what to do with multiple parses. Some \nof the advantage of having a formally correct parser would be lost if the programmer had to understand \nthe matching algorithm in order to decide which parse would come out first. Nevertheless, in retrospect, \nwe feel that a better algorithm is called for because even the implementor found he made mistakes in \nwriting patterns. For example, he would occasionally write something like {( a I an )  Noun} even though \nthe manual stated that this would not work because the matcher would not back up to try the an alternative \nafter matching the a in a string like an owl . The troublesome ell@sis The ellipsis pattern, which was \nvery handy to LMe in practice, raises some problems we don t know how to solve, even with a fully general \nparser, because it gives rise to a considerable amount of ambiguity, The pattern {,.. x ...} can match \nthe string bxbxb in two different ways. We chose the shortest-match-first approach so that the string \nwould parse into b , x and bxb . However, in more complex situations things do not work out well no matter \nwhat rule one adopts. Consider the following description of text in which spaces and carriage-returns \nare used to describe the two-level that appears in the KWIC example. {( (.-. )7? ... J )j-} There are \nmany possible parses of the string AAA BBB~CCC DDDj and we cannot think of any consistent rule which \nwill 38 produce the parse one wants. It seems clear that in this context one intends ellipsis to mean \nany characters other than space and carriage-return. . SNOBOL has an expression, break( ) ), that means \nprecisely that; and now we appreciate it! In practice, this difficulty has been surmountable; we use \na two-step process described below: break up the text at all carriage-returns, then break the sub\u00adpieces \nat spaces. Applicative post-processing is workable How can one make pattern matching an applicative operation? \nSpecifically how does the language make the results of parsing available to post-processors without using \nside effects? For exam pie, the SNOBOL pattern P = ARB .X ; ARB .Y ; assigns the parts of the string \nwhich fall before the semicolons to the variables X and Y as a side effect of the matching process. This \nis unsatisfactory because a reader who sees only the name P in a matching operation cannot easily discover \nwhat variables, if any, will be changed. The basic idea in Poplar is that a pattern is a function which \ncan be applied to a string; the result can be fail or something derived from the string by a set of pattern \ncomposition rules. As the default, the matcher simply re\u00adconcatenates the pieces matched so that aa;bbc; \n/{... ; ... ; } = aa;bbc; However, by decorating the pattern appropriately one can arrange for different \nthings to happen: Suffixing a component with # causes what ever it matches to be discarded. aa;bbc; \n/{... ( ; #) ... ( ; #)} = aabbc One can replace pieces by suffixing the phrase > newpiece aa;bbc; /{... \n( ; > X ) ... ; } = aaXbbc;! One can make lists out of the pieces by inserting brackets and commas in \nthe pattern aa;bbc; /{ [... ; , ... ; ] } = ~ aa; , bbc; ] Conceptually, it is best to think of a two-phase \nprocess: first the string is parsed, then one computes the result from the parse tree using the various \nsignals attached to the pattern. Although it can be syntactically confusing to intertwine these two processes, \nit overcomes the fact that any division of the two phases can lead to them becoming inconsistent, Experience \nsuggests a slightly different design for post\u00adprocessing patterns might be better. First, one is always \nwriting # after string constants to indicate that they should be discarded; the default should be the \nother way around. Second, including names for the interesting sub-pieces witiin the pattern has great \nmnemonic value. Once there are more than two or three interesting parts of a pattern one begins to lose \ntrack of the order. The design alternative we now favor was the one chosen by [Wadler]: introduce Pascal \nrecords into the language and allow the result of a match to be a record. For example, the value of a \nmatch using {x:: ... ; Y:: ,,, ; ) would be a record with components X and Y. This retains the applicative \nnature of pattern matching while regaining the virtues of SNOBOL S conditional value assignment notation. \nA more significant probt em is associated with iterated patterns like {Pi_}. A SNOBOL programmer can \nnot use the equivalent pattern, ARBNO(P), if he wants to do anything with the result of the parse. If \nhe wants to apply the procedure F to each substring P matched he must write an explicit loop that chops \noff a prefix of the string matching P, applies F, and starts over. This is too bad: there is a nice construct \nthat can describe iterated. 39 structures, but one must resort to traditional programming to actually \nprocess them. The first solution to this problem is to introduce a new operator ~ that parses things \njust like ~ but produces a list of the items matched rather than re-concatenating them. Then the operation \nF can be applied to each element on the list using Maplist. Thus one says string/{P~}//F A second answer \nis given by a very general method for processing the outcome of a pattern match: attach a fhnctionto \napattern element andapply ittothe result of matching that element. One says and the result ofasuccessful \nmatch would becornputedby apply ing Fto each of the sub-strings which matched P and concatenating the \nresults. This method is applicable in more general cases. typified 5Y the recursive patterns. Without \nfunctional attachment such patterns are not useful if one wants to process the recursive structure, For \nexample, to parse an expression andcornpute its value one can write. E+{digit! I[ ( # E, + # E ) # ]\\ \nphls} which is succinct if nothing else. Functional attachment was used extensively to build powerful \npatterns which simultaneously matched and transformed their input. Multi-pass parsing is conceptually \neasiefi but needs help Experience has shown that the create-a-list-and-process-it method is usually easier \nto use than the function attachment method, It seems simpler to comprehend because it is less intricate. \nIn general, the APL style of processing aggregates seems just as appealing for parsing as for list processing. \nLet us now consider the problem of writing the two patterns Lines and Words that appeared in the KWIC \nexample. Lines is relatively easy Lines @ {(... ~ #)~} Notice that the carriage-returns are discarded. \nWords is harder because one has to get the piece immediately following the last space and cope with the \ncase in which there are no spaces. Words ~ {(,..# Letter~)~ ...JY3 where Letter is a pattern matching \nany letter. If lazy evaluation methods were extended to pattern matching this method would compete with \na left-to-right parser. Unfortunately, we found that the semantics we choose for pattern matching are \nnot quite right for lazy evaluation. For example, the value of s/{P$ z #} is fail if S does not end with \nZ . Thus one cannot begin to process a long file of P s for fear that the file will not end in Z . Because \nbreaking up text is a very common operation and our pattern-matching language doesn t seem to do it very \ngracefully, we contemplate adding it as a primitive. S / breakup{Separators} is defined as returning \ntwo lists, The first is the list of separated objects, and the second is the list of separators. For \nexample: 12,4,78 /breakup, }, } = [[ 12 , 4 , 78 1, ~ , , > 11 a12c3 /breakup{ digit} =[[ a , , c , \n ], [ 1 , 2 , 3 ]] abed /breakup {digit} = [[ abed ], u] Lines @ s: (s/breakup { J })1 Words ~ s: (s/breakup \n-0ettert})2 40 Notice that breakup always succeeds so it is amenable to lazy evaluation. Implementation \nNotes An interpreter for Poplar was implemented on the Alto Whacker], using the language Mesa [Mitchell]. \nIt is organized so that there is no distinction made between expressions and values. What one normally \nthinks of as a value is simply an expression that the evaluator will not reduce any further. An expression \nis represented by a Node and may be one of a variety of different types: A string, an empty list, or \nfail A list node with pointers to the first element and the rest of the list A specific operator with \none or two associated operands; e.g., Plus with a pointer to each summand, or Maplist with a pointer \nto the function and a pointer to the list. A A-expression A closure: a pointer to an environment list \nof variable-value pairs and a pointer to an expression The evaluator is a simplifier: passed an expression, \nit returns a new expression, which is a simplified version of the first. After normal evaluation, an \nexpression will be in one of the following three forms: A string or fhil. A closure of a A-expression. \nA list composed of the a50ve ~nd (recursively) lists. To convert the evaluator to be lazy in the manner \ndescribed in [Henderson&#38;Morris] we made two changes: Arguments of a function are not evaluated until \nneeded. Components of a list structure are not evaluated until needed. In each of these cases the expression \nis put in a closure wi$ the current environment. An outcome of this rule is that the final result of \nevaluation may be a list node whose components are closures (the suspensions of [Friedman&#38;Wise]). \nNeeded means that the value is to be printed or treated as the subject of a pattern match. We did not \nmake the concatenation of strings or pattern matching lazy, but have chosen a half-lazy representation \nof strings. Both arguments of a concatenation are fully evaluated; but, if the resulting string is more \nthan 100 characters long, the result is represented as a node with pointers to the two strings. Thus, \nin general, a string is represented by a binary tree of such nodes. The terminal nodes point at pieces \nof files which are paged in as needed. Immediately before printing or pattern matching, this tree is \nconverted to be right-linear;i .e. each left son is a terminal node. This scheme was arrived at after \nsome experimentation and seems to work well most of the time, Garbage Collection We implemented a scan/mark \ngarbage collector for both Nodes and strings. Temporary string storage was compacted, and files were \nclosed if garbage collection revealed that no string pointed to them. We set up strict programming conventions \nto avoid collection-related bugs. We made it our policy that each procedure would register the address \nof any local variable of type Node; it did not have to register parameters because they were the caller \ns responsibility. Registered locations were kept in a stack which grew and shrank in parallel with the \nMesa run-time stack. When garbage collection was necessary, only those nodes accessible from registered \nlocations were saved. We Used the scan/mark algorithm instead of reference counts because it gave us \nexplicit control over the memory, and no programming errors ever caused us to lose memory since we explicitly \nconfirmed its use every time a garbage 41 collection happened. If a procedure failed to register a value, \nthe subsequent garbage collection would destroy the values about to be used. BLlgs of this sort were \nnot too hard to find since the collector gave nodes on the free list a special type, and subsequent access \nusually checked the node s type. Luzy evaluation: surprises and problems As expected, lazy evaluation \nrequired a larger constant overhead than normal evaluation. A lot of time is spent savi~g contexts in \nthe form of closures and re-establishing them. We guessed that this would cause a slowdown by a factor \nof three in those computations where one must eventually evaluate everything completely. Happily, it \nappears that the factor was nearer to two. Another problem is that the saved closures can tie LIp a lot \nof space. To avoid this one can scan the expression part of a closure to determine what variables are \nfree in it, and include only these in the environment list for the closure. We don t know whether this \nwould be worth the bother. There was an unpleasant surprise in the lazy evaluator design. It sometimes \nrequires twice as deep an evaluation stack as the normal evaluator. Consider Factorial ~ ([x, fl: if \nx = O then f else [x-l, ~x]/Factorial) One s intuition suggests that this is efficient because the recursive \ncall can be replaced by a simple jump, an optimization that most compilers and some interpreters detect. \nUnfortunately, under lazy evaluation this program is somewhat less efficient. The problem is that the \nexpression Fx at each level remains unevaluated. Thus when the evaluator gets to the call at which x \n= O it begins to work on the expression f to produce a number. At this level f is bound to a closure \nwhose expression is Nx and whose environment binds x to 1 and f to a closure whose expression is Px etc. \nIn other words, to come LIP with a numerical value for f the evaluator is going to get into a recursion \nprecisely as deep as the one we thought we were avoiding! This second recursion is not in general avoidable \nbecause one doesn t know that * is associative and one is also required to overwrite all those closures \nwith the numerical values on the way back. Furthermore, if the evaluator cannot avoid the recursion in \nthe first place we will need twice as much stack as under a normal evaluator. In practice, this problem \nis not devastating because Poplar encourages a programming style with no recursion in it whatsoe~er. \nOne should write Factoria12 @ (x: I--x///Times) which is shorter, clearer and as efficient under lazy \nevaluation as Factorial is under normal evaluation, even if we defined l--x by a recursive procedure, \nA way of avoiding some of these difficulties has been suggested by [Turner]. His implementation avoids \nclosures and environment lists entirely by translating the expression into combinators. However, some \nhand simulations indicate that the size of his combinator expressions may grow large in the same situations \nthat generate many closures under our implementation. His implementation avoids checking each value to \nsee if it is evaluated. It also solves the problem of deeper nesting by expanding the functions in-line \nthe first time they are called. A more fundamental problem is that lazy evaluation is not as powerfLd \na method of improving performance as one might imagine. Consider the following function: AveragePayroll \n+-(Payroll: Payroll/breakup { J } //Entry/Salaries: [Salaries///Plus, Salaries/Length]/Divide) Evaluation \nof Salaries///Plus does not require the entire list Salaries to exist at any one time. Nor does evaluation \nthe pattern-matching language so that it works on lists. of Salaries/Length. But since both are to be \ncalculated the Now we are required to say what it is about the pattern entire list Salaries will materialize. \nEvaluating one forces matching language that makes it so nice other than that it is the list into existence, \nand it cannot be garbage collected just like regular expressions . One thing tiat makes it because the \nother still needs it. There is no mechanism to powerful is that it is basically a second order language \nlike synchronize the evaluation. In general, this problem may [Backus] s in that expressions in the language \ntend to occur whenever a list is generated that needs to be traversed denote functions mther than values. \nFor example, x y in by two different functions, Another example is the conventional language assumes \nthat x and y denote strings and the value is another string; in the pattern [List, P]/Split/[PL, NPL]: \n[PL///Plus, NPL///Plus] matching language x and y are functions and the result is Problems of this type \nwill often be associated with the another fLmction. reduction operator because it reduces a list to a \nsingle value, making greater space savings possible. Writing a few Poplar should have a powerful compiler \nspecial functions to handle reduction might solve some of these problems, For example, consider reduce: \nIf we are really going to write programs as profligate as the KWIC example, lazy evaluation is not powerful \nenough to L/([fl, f2, f3]/reduce) = [L///fl, L///f2, L///f3]. recover all the efficiency that is needed. \nThe approach Although its use is not completely natural, one could demonstrated by [Darlington&#38;Burstall] \nis more promising contemplate a compiler generating it. and is being studied by the third author who \nclaims that for any function written in a lazy programming style, there is an equivalent and equally \nefficient program that may be Reflections written in the normal style. One can imagine a pre-Lists and \nStrings should be unlj7ed processor that at compile time performs a source-to-source transformation that \nconverts a lazy program to its non-lazy It never occurred to us at first to unify the concepts of equivalent. \nThis would avoid the problems discussed strings and lists; we thought of strings as LISP atoms. above. \nFurthermore, the compile-time analysis could be However, it became clear that this division forced the \nused to detect type errors that are especially difficult to language into two pieces as in SNOBOL: the \npattern sub\u00ad cope with when things happen in an order the programmer language and the general list-processing \nlanguage, The doesn t expect. shortcomings of this became clear when someone wanted to precede a parsing \noperation by a lexical analysis that produced a list of strings. The pattern language could not Deep \nproblems about applicative programming be used on the list! This mistake was avoided in LISP70 A more \nserious bar to applicative programming is typified ~esler], by the following problem: S~ppose one wishes \nto process all the elements of a list, some of which may cause We now contemplate an alternate design \nin which the base data type is character, and a string is just a list all of whose elements are characters. \nThe puzzle is how to generalize exceptional conditions; One writes L // (x: if OK(x) then newwal(x) else \n(Exceptions ~ Cons(x, Exceptions); x)) The problem is that one wants to use a side channel to convey \nsome information which is ancillary to the, main computation. In general, if a process has multiple output \nstreams which receive data at very different, unpredictable rates it is difficult to retain an applicative \napproach. Beyond this technical problem there are basic, long\u00adstanding philosophical questions with \napplicative languages which our experience has brought to the surface: How should interaction with a \nuser be carried out? In our environment it is the norm to write programs that interact with a person \nthrough a keyboard, screen, and pointing device. To describe such things applicatively one can describe \neach program as a function that maps each input into its output response, or better an input stream into \nan output stream as [Friedman&#38;Wise] have done. This model doesn t fit very well with making random \nchanges to a two\u00addimensional display, however, How does one debug a program with a surprising evaluation \norder? Our attempts to debug programs submitted to the lazy implementation have been quite entertaining. \nThe ~nly thing in our experience to resemble it was debugging a , multi-programming system, but in this \ncase virtually every parameter to a procedure represents a new process. It was difficult to predict when \nsomething was going to happen; the best strategy seems to be to print out well-defined intermediate results, \nclearly labelled. How does one predict performance? Never mind that lazy evaluation, or any other clever \nstrategy, will make the program perform better than it would have otherwise-ultimately one depends upon \nhis understanding of the machine to design things so that they run reasonably: If the machine is clever \nit is probably harder to understand, especially if it employs various ad hoc heuristics, based upon expectations \nof what sort of programs people write. How does one arrange meaningful checkpoints? Even if one s computation \nhas no bugs and is non-interactive the order in which things are done can be relevant. When one s computation \ntakes a long time he would like to save intermediate states that have meaning to the programmer. For \nexample, in a correspondence management system we found it desirable to produce a letter and record the \nfact that it had been sent as an atomic action. Typically one might request the system to send many letters \nand expect that one or two requests would cause trouble for ,reasons ranging from hardware errors, to \nsoftware errors, to improper requests. Also, one occasionally wanted to interrupt the process to do something \nelse with the machine. Since there is no interdependence between these requests and because the operation \ntakes a non-trivial amount of time one would like all but the troublesome requests to be completed. We \nattempted to solve this problem through the use of explicit writes on files-a highly non-applicative \noperation, If one attempted to describe the operation as a whole, surrendering control of what happens \nto the system, any mishap forces one to start over entirely. To summarize, the potential practical benefit \nof an applicative language is that its implementation has much more running room in which to be clever \nsince the order in which operations are performed is constrained only by the data flow. Examples of such \ncleverness are lazy evaluation, compile-time loop integration, and parallel processing. On the other \nhand, computing is an activity which goes on in time and space. In situations where one cares about the \ntime and space aspects of an operation as mUCh as the qualitative result, applicative programming is \nless applicable(!). Furthermore, the personal, interactive mode 44 of computing tends to increase the \nfrequency of these situations. Acknowledgements Alan Perlis has relentlessly encouraged our exploration \nof this programming style. Dan Swinehart, Robert Kierr, and Marcello Siero have courageously written \nprograms that they depended upon in Poplar. Paul McJones has made many penetrating comments about the \nlangauge and this paper. References [McCarthy] John McCarthy, Recursive functions of symbolic expressions \nand their computation by machine, Comm. ACM 3, 4 (April 1960) 185-195, [Church] A. Church, The Calculi \nof Lamb&#38; Conversion, Annals of Mathematics Studies, No, 6, Princeton University Press+ Princeton, \nN. J., 1941. [Kleene] S. C. Kleene, Introduction to Metamathematics, D. Van Nostrand, Princeton, N. J. \n1950. [Strachey] Christopher Strachey, Towards a fom~ semantics, In Formal Language Description Languages \nfor Computer Programming, T.B. Steel, cd., North-Holland, Amsterdam, 1966, 198-220. [Lanolin] Landin, \nP.J. The next 700 programming languages. Comm, ACM 9, 3 (March 1966), 157-164. [Friedman&#38;Wise] Friedman, \nD.P. and, Wise, D.S. CONS should not evaluate its arguments. In Automata, Languages and Programming, \nMichelson and Milner, eds., Edinburgh University Press, 1976, 257-284. [Milner] M. Gordon, R. Milner, \nL. Morns, M. Newey, and C. Wadsworth, A metalanguage for interactive proof in LCF, in Proc. 5th annual \nACM SIGACT-SIGPLAN Symposium on Principles of Programming Languages, Tucson, Arizona, 1978. [Burge] William. \nH, Burge, Recursive Programming techniques, Addison-Wesley, Reading Mass., 1975. [Backus] John Backus. \nCan programming be liberated from the von Neumann style? A functional style and its algebra of programs. \nComm. ACM 21, 8 (Aug. 1978)> 613-641. [Morris&#38;Schmidt] J. Morris and E. Schmidt, Poplar Language \nManuaJ, Xerox PARC, internal memorandum, 1978. ~adler] Philip C, Wadler, Syntav directed data conversion, \nXerox PARC, internal memorandum, 1978. [Griswold] R,E. Griswold, J.F. Poage , and J.P. Polonsky, The \nSnobol-4 Programming Language, Prentice-Hall, 1971. [Aho] A. V. Aho, B. W. Kernighan, and P. J. Weinberger, \nAwk -A Pattern Scanning and Processing Language, Bell Laboratories Internal Memorandum, Murray Hill, \nN. J., 1978. [Lesk] M. E. Lesk, and E. Schmidt, Lex -A Lexical Analyzer Generator, Bjll Laboratories \nInternal Memorandum, Murray Hill, N. J., 1978. ~arnas] D. Parnas, On the criteria to be used in decomposing \nsystems into modules, Comm. ACM 15,12, (Dee 1972). [Kay] A. Goldberg and Alan Kay, Smalltalk-72 instruction \nManual, Xerox Palo Alto Research Center, Report SSL 76-6, 1976. [Henderson &#38; Morris] Peter Henderson \nand James H. Morris, A lazy evaluator. Third Symposium on Principles of Programming Languages, Atlanta, \n1976, 95-103. [Earley] J. Earley, An efficient context-free parsing algorithm, Comm. ACM 13,2, (Feb 1970). \nwhacker] C. P. Thacker, E. M. McCreight, B. W. Lampson, R. F. Sproull, and D. R. Boggs, Alto: A personal \ncomputer, in Computer Structures (second edition), Siewiorek, Bell, and Newell (eds.), McGraw- Hill, \nto appear. [Mitchell] J. Mitchell, W. Maybury, R. Sweet, Mesa Language Manual, Version 5.0, Xerox Palo \nAlto Research Center, report CSL-79-3, Uurner] D. A. Turner A new implementation technique for applicative \nlanguages. Software Practice and Experience 9, 1 (1979), 31-49. ~esler] L. Tesler, H. Enea, D. Smith, \nThe LISP70 pattern matching system, Proceedings of the International Joint Conference on Artificial Intelligence, \nSt,anford, 1973. [Darlin@on&#38;Burstall] J. DarlingIon and R. Burstall, A transformation system for \ndeveloping recursive programs, JACM 24, 1, (January 1977), pp 44-67.\n\t\t\t", "proc_id": "567446", "abstract": "Experience using and implementing the language Poplar is described. The major conclusions are: Applicative programming can be made more natural through the use of built-in iterative operators and post-fix notation. Clever evaluation strategies, such as lazy evaluation, can make applicative programming more computationally efficient. Pattern matching can be performed in an applicative framework. Many problems remain.", "authors": [{"name": "James H. Morris", "author_profile_id": "81332516966", "affiliation": "Xerox, Palo Alto Research Center", "person_id": "PP14058604", "email_address": "", "orcid_id": ""}, {"name": "Eric Schmidt", "author_profile_id": "81490666792", "affiliation": "University of California, Berkeley", "person_id": "PP14202959", "email_address": "", "orcid_id": ""}, {"name": "Philip Wadler", "author_profile_id": "81100173596", "affiliation": "Carnegie-Mellon University", "person_id": "PP39030941", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/567446.567450", "year": "1980", "article_id": "567450", "conference": "POPL", "title": "Experience with an applicative string processing language", "url": "http://dl.acm.org/citation.cfm?id=567450"}