{"article_publication_date": "01-28-1980", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; \n1980 ACM 0-89791-011-7 $5.00 completeness and its ramifications for language design. We then discuss \nsome common examples of incompleteness found in existing Algol-like languages. And we end by presenting \nthe type structure of the type-complete programming language Russell, which has been designed by the \nauthora. As we show, the flexibility provided in Russell by the combination of a small, rich type structure \nand type-completeness belie Backus~s assertion of inherent weakness of vonNeumann languages. L. 2&#38;Q.e-ComDletenesfi \nTO design a language with many changeable parts, it is necessary first to design a language framework \nthat specifies how the parts must behave and how they may be composed. The idea of type\u00ad completenees \nis to require that this framework consist of a type structure, which specifies the legal uses of names \n(or what kinds of parts can be later added) and a very few, uniformly applicable composition rules, which \nspecify the typee and meaningS of composite expressions in terms of the types and meanings of their components. \nWe will say that a language is ~-$omplete if and only if the following three conditions hold: 1. Each \nname (i.e., identifier or operator sym\u00adbol) and each expression in the language has a type; the type \nof any composite expression is compoeed from the types of its components. The type of a name or expression \nmay not depend on the context in which the name or expression appears. For example, the type of an identifier \nmay not depend on whether it appears on the left-or right-hand side of an assignment statement. 235 \n2. For each type in the type structure of the language, it ie poseible to write a expres\u00adsion in the \nlanguage having this type. 3. Any expression can be parameterized with respect to any free name having \nany type in the expression to yield a function of an eveu more complex type. Tbie implies that func\u00adtions \nmust be able to have parameters of any type and to produce results of any type. Probably the simplest \nexamp 1 e of a type\u00adcomplete language is tbe typed lambda calculus. The framework of the lambda calculus \nconsists of the type structure given by the following simple grammar: Type = BaseType I FunctionType \nBaseType = {N, B, ... ] FunctionType = Type + Type and the rulee for application and lambda abstrac\u00adtion, \nthe only composite expressions in the language. This framework covers the entire language: 1. Each lambda \nexpression has a type in the type structure given above, 2. the typee of composite expressions are com\u00adposed \nfrom the types of the components (e.g., the operator and operand of an application) without regard for \nsurrounding context, and 3. we can always produce a more complex expres\u00adsion of higher type by lambda \nabstraction of any free identifier.  In fact, this framework describes a family of languages that differ \nin the choice of base types and constants that are ?built in~?. Requiring a language to be type-complete \nensures two basic properties of its design. Firstly, it guaranteea that parameterization and application \nare universally applicable in the language. Any name can be a parameter, a parame\u00adter can be of any type \nand an argument can be con\u00adstructed that can be bound to any parameter. It is important not to place \nany constraints on the forms of parameters and arguments because parame\u00adtrization is the fundamental \ntool in any program\u00adming language for providing changeable parts in a program. If we want to use the \nsame program for many different values of X, we can do so by making X a parameter. Type-completeness \nguarantees that this can always be done, no matter how X happens to be used in the program. Secondly, \ntype-completeness allows us to define all forms of declarations in terms of the basic parameter/argument \nbinding semantics. Type-completeness ensures that declaration and parameterization are equivalent in \ntheir scope of usage; any name that can be declared can also be a parameter. Thus , by adopting Landints \nprinciple of correspondence?! [66] , which states that parame\u00ad terization and declaration are semantically \nequivalent, we can capture all the naming conven\u00ad tions of the language by a few basic notions, without \nregard for what is being named. The observation that type-completeness allows a major simplification \nin describing the naming rules of a language points out a serious fault in the DoD Steelman requirements. \nThe requirements clearly state that the language should not con\u00adtain unnecessary complexity;V8 yet type\u00adcompleteness \nis specifically ruled out by require\u00adment 5D, Procedures, functions, types, labels, exceptions conditions \nand statements shall not . . . be computable as values of expressions, or be usable as nongenetic parameters \nto procedures or functions.t! And, in Ada, which observes require\u00adment 5D exactly, one finds nine different \nforms of declaration, each with slightly different syntax and slightly different meaning. In Russell, \nby way of contrast, the syntax and semantics Of s%U declarations can be described in less than one page \nof text, with a single syntactic rule and semantic interpretation, An important point to be drawn here \nis that simplicity is sometimes better provided by gen\u00aderalizing, rather than by restricting, as has \nbeen done in Ada and Euclid. Placing bounds on the legal forms of parameters prevents us from giving \none uniform semantics for two obviously related mechanisms, parameterization and declaration. Moreover, \nas we discuss below, lack of type\u00adcompleteness also forces the introduction of spe\u00adcial mechanisms to \nhandle other cases where common combining forms would suffice. Type-completeness by itself does not guaran\u00adtee \na useful programming language. Even though the typed lambda calculus is type-complete, the simplicity \nof its type structure makes it unsuit\u00adable for many purposes. However, the idea of using the type structure \nof a language as its framework suggests a novel approach to language design. Instead of designing by \nadding new features, we should attempt to make systematic enrichments to the underlying type structure \nand to add new, general combining forms only when necessary. Below we discuss two obviously needed additions \nto the simple lambda calculus type structure given above. In both cases, we show that the simple measure \nof enriching the type structure allows a few general-purpose mechanisms to provide more facility than \nmany special-purpose language features. essar L. P-2 xI!z?x Structure Enrichments One of the most obvious, \nbut least under\u00adstood, properties of Algol-like languages is that programs refer to variables.! Names \nin a program can be bound not only to values, but also to ncon\u00adtainersft holding values. While the variable \nasso\u00adciated with a name cannot be changed, the value held in tbe variable can be changed during evalua\u00adtion \nof a program. As variables are fundamental to our understanding of Algol-like languages, it seems natural \nthat the type structure of such a language should incorporate the notion of vari\u00adable.w Indeed, all Algol-like \nlanguages partially incorporate variables into their type structures: they make distinctions between \nvariable and value parameters and between left-and right-hand sides of assignment statements. However, \nour notion of type-completeness requires more than this; in par\u00adticular, functions must be able to produce \nvari\u00adables as results, as well as being able to have variable arguments. This idea first strikes one \nas probably unnecessary (and possibly dangerous), but as Lan\u00adolin suggests there seems to be flno configuration \n[i.e., composition], however unpromising it might seem when judged cold, that will not turn up quite \nnaturally. [66, pg. 1581 And we find that several disparate features of existing languages turn out to \nbe nothing more than weak special cases of this more fundamental facility. The first of these was noticed \nby Landin [65, 66] in his discussion of Algol 60 name parameters. Name parameters in Algol 60 behave \nlike variable\u00adreturning, parameterless ~type procedures.tt How\u00adever, the analogy fails for two reasons: \n1. Variable-returning procedures are not part of the type etructure of Algol, i.e., they can\u00adnot be written \nby the user, and 2. Because of an unfortunate property of the Algol 60 syntax, parameterless procedures \nare not legal arguments to procedures (an occurrence of pn as an argument is taken as fpon) .  Were \nthe analogy exact, the ability to pasa Pro\u00adcedures as arguments in Algol 60 would obviate the need for \ncall-by-name. One of Backuata criticisms of existing languages is the introduction of spe\u00adcial parameter-passing \nmechanisms; our contention is that such mechanisms are artifacts of the language designers misunderstandings, \nnot of the basic properties of vonNeumann machines. Secondly, failing to incorporate the notion of variable \ninto the type structure makes it necessary to introduce special composite formst~ of names to provide \nfor common operations like field selection and subscription. If variables are part of the visible type \nstructure. i.e. , If it is possible to write variable-returning func\u00adtions, then no such special forms \nare needed. The baeic mechanism any language must provide to apply an operator to an operand suffices \nto handle this additional case. This is one of the unfortunate aspects of Algol 68, which allows ~?reference\u00adreturning \nfunctions but treats them differently from component selectors. The Revised Report [vanWijngaarden771 \ndoes not describe re of x as the application of the reference-returning func\u00adtion function re to the \nreference X,n but says instead that x must have a subnamew re. Aside from the unnecessary use of special \nsyntactic forms, the failure to allow variables as results of functions also forces the introduction \nof special features to handle cases where it is necessary to modify the means of accessing a variables \ncomponents, e.g., the array slice!t feature of Algol 68, PL/I or Ada. To illustrate this idea, consider \nthe following Russell program to assign zero to each element of an array: PJZ2Q [x n sub k!zb -i :=lQnfixsubi:=O \n~ m The parameters of the procedure include a variable fxn to be modified and an infix function ~~sublt \nwhich produces an Integer variable when applied to a T variable and an Integer index value. Within the \nprocedure x looks like a one\u00addimensional array; to access one of the components of x, we apply the sub \nfunction with a single Integer argument. However, an array argument actually provided to the procedure \ncould well be a multi-dimensional array, i.e., an array from which components would normally be extracted \nby using a subscription operator of a different type (the declaration of the parameter x says nothing \nabout what kind of an array it is, or even that it is an array) . For instance, we could use this procedure \nto zero only the diagonal elements of a two\u00addimensional array by passing to the procedure the\u00adfollowing \nsubscription operator M x {W T]; i {Y.@ Integer} 1 w Integer ~x sub [i,il ~ where we assume the existence \nof an operator named sub!? that takes a type T variable and two Integer indices as arguments and produces \nan Integer vari\u00adable as a result. Obviously, to make full use of this sort of flexibility, it is advantageous \nto make the array type itself a parameter of the pro\u00adcedure, so the use of the zeroing procedure is not \nlimited to arrays of a single type. This point is discussed further below. By allowing the user to apply \nany of the gen\u00aderal combining forms to compose variable-returning functions, a language can provide a \nsmaller but much richer set of selectors than would be allowed by any special ?~array slice?? mechanism. \nThus, by enriching the type structure of a language to include a type-complete treatment of variables,-we \ncan build a simple language with a wider range of flexibility than much larger languages with many special \nfeatures. And this simplicity benefits both the user and the language designer: the user has fewer new \nconstructs to master and the designer knows that only a few combining forms will suffice, thus lessening \nhis concern about what should or should not be added to the language. This idea of using variable-returning \nfunc\u00ad tions as the means of manipulating complex data structures appears in the typeless language GEDANKEN \n[Reynolda70]. As Reynolds notes, ~tthis approach ensures that any process which accepts some data structure \nwill accept any logically equivalent structure, regardless of its internal representation.tf [Pg. 3081 \nHowever, as noted above, to make full advantage of this functional data structurer~ approach for statically \ntyped languages, it seems necessary to allow types to be legal parameters. But our preceding discussion \nof type-completeness leada us to the same conclusion: if we can give a name to a type, then we must be \nable to make that name a parameter. ~.A. Data TvDea It has become commonplace to design languages with \ndata type definition facilities to simplify the creation and manipulation of complex data structures. \nRecent work in programming methodol\u00ad ogy (e.g., abstract data typean) has led to increased concern about \nthe design of such facili\u00adties. In the newest languagea, e.g., Euclid or Alphard, the data type definitions \nhave become rather complicated in form and meaning. However, looking at the data type definition facilities \nof most languages in terms of type\u00adcompleteness shows that they are not integral parts of their respective \nlanguage cores. Tko common characteristics of the treatment of data type definitions shows the unfortunate \nnature of the special status they are accorded. Firstly, most languages give a special rule for the interpretation \nof type declarations. In Algol 68 or Euclid, for example, a type identifier is taken as a synonym for \nits definition, i.e., type declarations are treated as a macro-like syn\u00ad tactic shorthand. But this is \nthe only sort of definition for which this rule applies. The Algol 68 Revised Report describes the action \nperformed and the result yielded for every form of declara\u00ad tion except for a mode (type) declaration, \nwhich involves no action, yielda no result.!? [pg. 21] Also, moat languages that allow type declara\u00adtions \ndo not allow type parameters. Thus, even though it is possible to introduce new type iden\u00adtifiers in \na program, it is not possible to parametrize a program with respect to one of these type identifiers. \nRecently, CLU, Alphard and Ada have attempted to remedy this oversight; however, their tpolymorphict~ \nfeatures fail to allow the generality allowed by type-completeness. In each case, type parameters are \nviewed as a ?compile-time ! feature, i.e., aa using the simple macro-like meaning of type declaration \nused in Pascal or Euclid. (This is most obvious in the explanation of the Ada tgeneric!~ feature; however, \nthe more ambiguous descriptions of the meanings of types in CLU [Liakov77] and Alphard [Wulf78] sug\u00adgest \nthe same treatment of type parameters.) Treating type parameters as macro names fails to generalize naturally; \nwe have no notion of the meaning of a construct that has a type parameter, only a meaning for the application \nof a particular data type argument to the construct. So we have difficulty interpreting how we could \nmake type\u00adparameterized constructs themselves parameters, as is demanded in a type-complete language. \nIn this case, type-completeness can serve as a means of judging the success of a language design effort. \nType-completeness demands that a data type definition facility should use the same rules for defining \nthe meaning of names as any other definition facilityn in the language. The benefits of ensuring the \nuniformity of treatment of data type names are exactly those cited above for type-complete languages: \nthe interchangeable parts of a program (or programming language) would now include even the data types \nof the language, and the same general combining forms used to pro\u00adduce other sorts of values could be \nused to pro\u00adduce types. Thus, for example, there would be no need for special forms of parametrized data \ntypes,n because the general form of functions that yield types would suffice (and most probably would \nprovide a richer form of expression). Below. we present the type structure of the type-complete programming \nlanguage Russell, which has been designed by the authors [Demers79]. In Russell, the type structure includes \nboth of the extensions we suggested above: variables and data types are both incorporated into the Russell \nnotion of typen and the language framework uses only a few basic combining forms. Even though the Russell \nframework is small, the language is amen\u00adable to many extensions and provides more expres\u00adsiveness than \nmany much larger Algol-like languages. k. ZhelQf&#38;JJ Framework am!ti Ramifications Russell is a type-complete \nAlgol-like pro\u00adgramming language with a simple but extremely rich framework. Each name in a Russell program \n(i.e., each operator symbol or identifier) has a type, or signatureW in Russell terminology, which is \nan element of the following grammar: Signature ::= ~ TypeDenotation I ~ TypeDenotation I OperationSignature \n Opera ionSignature ::=  PX.WI { Id.i.sLwaturel}~ 1 I -{ Id L Signature 1 }: 1 Signature , I m Id( \n{ Id l OperationSignature 1 l; ) where a simple signature specifies whether the name is a value or a \nvariable and gives its data type (a TypeDenotation is any denotation, i.e., expression, having a ~ signature); \na procedure and function signature specifies the names and signatures of the parameters (and the result \nsig\u00adnature of a function); a type signature specifies the names and signatures of the operations the \ntype comprises. The remainder of the Russell framework includes combining forms of: 1. application and \nabstraction (both to turn a denotation into a function and to turn a statement into a procedure), 2. \nconditional evaluation, 3. selection of a component of a data type, and 4. agglomeration, to turn a \nset of operations into a data type (as discussed below).  In each case, the meaning of a combining form \ncan be given independently of the signature of the result produced by evaluating the form or of the signature \nof any free identifier appearing in com\u00adponent denotations. Thus , the framework of Russell is really \nquite small. Nevertheless, the framework is exceedingly rich, due to some of the ideas we have discussed \nabove. Of primary importance in the Russell type structure is the fact that functions can have parameters \nof any signature and can produce results of any signature. Thus , field selectors in Russell are functions \nthat take variables as arguments and yield variables as results. Parameterized types in Russell are functions \nthat yield types as results (and may take types as arguments) . Also, the effect of lambda calculus call-by-name \nparameter-passing can be achieved by using nullary, value-producing functions as argu\u00adments, i.e., functions \nwith signature *[] ~ TypeDenotation . (Russell uses call-by-value as the parameter\u00ad passing mechanism \nof the language framework; this same parameter/argument binding mechanism is also used to describe the \nmeaning of all declarations in a Russell program.) The generality of the function mechanism also makes \npossible the treatment of data types used in Russell. A data type in Russell is a set of operations that \nprovides an interpretation of values; so the signature of a data type is simply the set of signatures \nof the operations the type comprises. The richness of functions in Russell allows us to specify the meaning \nof such primitive operations as taking the value of a variable of type Tn and Wallocating a new variable \nof tYPe T as functions that are components of the data type T (with signatures tix{wT)]2@.T and funcl_] \n~ T , respectively) . Additionally, the structured types of Russell include selector functions as part \nof their meanings, as was discussed above. Thus, the only new notion introduced by data types is that \nof agglomeration --grouping a set of operations together as a data type. The operations them\u00adselves are \nordinary in their signatures and are treated no differently from operations that happen not to be part \nof a data type (indeed, in many cases, the decision to make an operation part of a data type is purely \na matter of taste). The fact that data types in Russell are integrated into the signatUre structure of \nthe language in this straightforward manner has several important consequences. It makes the signature-matchingW \nrules, by which type\u00adcorrectness is determined, simple to present even though Russell places no restrictions \non the sig\u00adnature of any parameter of a procedure or func\u00adtion. This point is discussed in much greater \ndetail in the other paper by the authors in this Proceedings. Also, the unexceptional nature of data \ntypes makes it possible to leave out of Russell many features that are commonly built in to Algol-like \nlanguages; they can easily be provided by the user as he sees fit. lie give two simple examples. First, \nwe need not introduce multi-dimensional arrays in Russell because they can be easily com\u00adposed from one-dimensional \narrays. A two\u00addimensional array (that is, an array that is sub\u00ad scripted as A sub [i, j] in Russell) \ncan be produced by first composing applications of the Array type-producing function 241 provided, e.g., \nArray[ IndexType , Array[ In $ exType2, ComponentType I 1, and then by redefining the subscription operator \nof the resulting type to he the composition of the subscription operators of the array types that were \ncomposed. Thus, the user of the resulting type would write A sub [i, j] instead of (Asub i) sub j . This \nis not a matter of syntactic shorthand as it is in Pascal, hut of using the basic composi\u00adtion operators \nof the language to produce exactly the form of expression that the user desires. This same mechanism \nalso allows a user to define a two-dimensional subscription operator for a one\u00addimensional array, which \nallows a simple defini\u00adtion of structures 1 ike triangular and hand matrices. Secondly, unlike all other \nAlgol-like languages of which the authors are aware, Russell requires no special syntactic form or semantics \ninterpretation for variable declaration and allo\u00adcation. Each data type in the language provides a function \nNew with signature fi]wT that returns a %ew variablett of the type. So one declares new variables simply \nby binding a name to the value of an application of New. This function can be composed with any of the \ncombining forms to produce new useful functions that are not built in. For instance, the function XC \nn {YQ. Integer} 1 w Integer J@ x == Integer$New[] b ~ X:=n result x q performs allocation with initialization. \nNote that type-completeness demands the existence of functions with signatures like that of New --and \nthese functions have a natural interpretation. A more fundamental consequence of the unifor\u00ad mi ty of \nnaming conventions in Russell is that Russell provides a particularly ~~abstractt~ view of data types. \nIn a Russell program any two denota\u00adtions with the same signature and the same meaning can be used interchangeably \nin U contexts. Thus, any two type denotations that have the same signature and the same logical properties \ncan be uniformly substituted for one another in ~ pro\u00adgram, i.e., the meaning of any program can depend \nonly on the signature and meaning of a type and not on the typers identity (how the type was pro\u00adduced) \n. So a change to a type that does not affect its signature or logical properties is guaranteed not the \naffect the meaning of any pro\u00ad gram using the type. The same properties of Russell that provide this \nabstract treatment of data types allow us to blur the distinction between what is built into the language \nand what is later added by a user of the language. Each name in a Russell program has only a signature \nand a meaning. There is nothing one can do with a name in any Russell program that depends on how the \nsignature or meaning bound to the name were produced. Thus , the user of the type Color declared by \n242 Color ;= -( Red, white, Blue ) evers static type-checking is still has no way of knowing how this \ntype happened to be preserved. declared; many other types can be produced have the same signature and \nlogical properties this one. In fact, it makes no difference to user whether Color happened to be declared \nabove or was provided as part of the built types included in an implementation of Russell. that as the \nas inw 2. A more Pascal-like Russell excludes func\u00adtional data types and type variables (which necessitate \nretention in the implementation), uses a nonrecursive union type-producing function and introduces a \nreference data type to build the structures previously By guaranteeing that implementation-defined allowed \nby recursive unions. (The signature and programmer-composed values behave the same, we of this reference \ntype is given in provide the implementor of Russell with wide lati\u00ad [Demers79].) It is interesting to \nnote that tude in deciding what he wishes to include in the the signature of the union type-producing \nbuilt inn parts of an implementation. Something function in Russell is independent of whether may be \nbuilt in because it is necessary, e.g., the union is allowed to be recursive. Booleans and Integers, \nof the implementation space efficiencies that is necessary implementation-defined or because making it \npart provides certain time or unavailable to the user. All is that the particular parts are consistent \nwith 3. To make programming introduce word and f~coercionw Russell look more like a systems language \nit is necessary to a data type behaving like a machine to introduce a certain number of functions that \nallow us to obscure the Russell framework; this means that they must be given signatures and they must \nbe semantically consistent with the remainder of the language (for example, call-by-value must be the \nparameter\u00adpassing mechanism used by any new functions pro\u00ad the distinction for example. be performed \nfunctions of between words and integers. In Russell, such coercions through the signature like introduction \ncan of vided) . The particular conditions necessary to = w {uWord] 1 W Integer ensure semantic consistency \nare spelled out in that simply blessW the argument as being of [Demers79]. And the rich structure of \nthe Russell different type and return it. These func\u00ad framework ensures that a broad range of tions obviously \ncannot be written in Russell implementation-defined values can be added to the if they are not built \nin, but there is noth\u00ad language within the framework. For instance, ing in the framework that prevents \na particu\u00ad 1. The Russell Report describes a very leveln language that allows recursive types (as in \n[Hoare73b]) and functional type-valued data types. Type-valued ables may be declared in this language; \nhigh union and vari\u00adhow\u00ad lar implementation of the language from including them if they are needed. Simi\u00adlarly, \nthe handling of packed or machine\u00addependent records are matters that can be provided by a particular \nimplementation; from our remarks above about interchangeability. we see that a user?s program cannot \ntell, aside from time or space efficiencies, whether records or arrays are stored in any particular fashion. \nThus, we can reasonably regard Russell as a very expansive family of programming languages. ~. Conclusion% \nBackus raises some fundamental criticisms about the current structure of Algol-like program\u00adming languages. \nThese criticisms cannot be met by adding new featuresn to languages; this simply compounds the basic \nproblems of the size and com\u00adplexity of our languages. Instead, we must either accept Backus~s claim \nthat the situation is hope\u00adless or return to first principles and try to dis\u00adcern where past attempts \nhave failed. This paper develops one such principle of language design and shows that its application \nobviates many of Backusts criticisms . Type\u00adcomplete languages can have simple parameter\u00adpassing and \ndeclaration rules (while allowing wide flexibility in the behavior of parameters and declared names) \nand allow the designer to build families of languages with widely varying nchange\u00adable partsw. Moreover, \nthe focus on type\u00adcompletenees changes the fundamental role of the language designer from one whose responsibility \nis to put together a large number of nfeaturesn to one who must devise a rich but small type struc\u00ad ture \nand who is then forced to live within its constraints. This change should make programming languages \nmuch harder to design, but much easier to understand. 6. Re ferences [Backus78] Backus, J. Can Programming \nBe Liberated From the vonNeumann Style? A Functional Style and its Algebra of programa. QAQi 21(1978), \nPP. 613-641. EDemers79] Demers, A. and J. Donahue. Revised Report on Russell, Report TR79-389, Computer \nScience Department, Cornell University (1979). [Hoare73a] Hoare, C.A.R. Hints on Programming Language \nDesign. ACM Symposium on Principles of Pro\u00ad gramming Languages (1973), pp. l-~0. [Hoare73b] Hoar e, C.A.R. \nRecursive Data Structures. Report STAN-CS-73-400, Department of Computer Science, Stanford University \n(1973). [Lampson77] Lampson, Butler, James Horning, Ralph London, James Mitchell and G. PoDek. ReDort \non the &#38; Programming Language Euclid. SIGPLAN ~ 12:3 (1977). [Landin65] Landin, P.J. A Correspondence \nbetween Algol 60 and Churchts Lambda Notation. G&#38;M 8(1965), pp. 89-101, 158-165. [Landin66] Landin, \nP.J. The Next 700 Programming Languages. Q@ 9(1966). PP. 157-164. [Liskov77] Liskov, B., A. Snyder, R. \nAtkinson and C. Schaffert. Abstraction Mechanisms in CLU. Qi&#38;M 20(1977), pp. 564-576. [Reynolds70] \nReynolds, J. GEDANKEN --A Simple Typeless Language Based on the Principle of Complete\u00adness and the Reference \nConcept. w 13(1970), pp. 308-318. [vanWijngaarden77] vanWijngaarden, A., et al. Revised Report on the \nAlgorithmic Language Algo168. SIGPLAN Notices 12:5(1977), pp. 1-70. [Wulf781 Wulf, W., et al. (Preliminary) \nAn Informal Definition of Alphard. CMU-CS-78-105, Department of Computer Science, Carnegie-Mellon University, \n1978. \n\t\t\t", "proc_id": "567446", "abstract": "In his recent Turing Lecture, John Backus delivered a trenchant argument for the proposition that \"programming languages are in trouble.\" Backus claims this to be inevitable: the development of Algol-like languages must lead to this sorry state because it begins from faulty assumptions.A less radical interpretation of the difficulties of language design is that we still do not understand the fundamental principles that should guide our design efforts; the machines we use may limit our results, but our ignorance has far greater effect. As evidence to support this position, we can cite the difficulties of building successors to Algol 60 and Pascal. Both languages have been acclaimed as well-designed, but it has proved extremely difficult to capture just what they \"got right.\" Thus, Hoare has claimed that \"Algol 60 was not only a great improvement on its predecessors, but also on nearly all of its successors.\" [73a] The recent Ada design suggests that the same fate may befall Pascal.In this paper, we expand on an idea of Landin [66] to develop an important principle of programming languages: type-completeness. We argue that application of this principle is an effective tool in understanding problems of programming language design. In particular, type-completeness1. allows us to point out many of the flaws and inconsistencies in existing languages (so we can know what mistakes not to repeat) and2. provides a framework for the design of languages (or language families) that have wide variation in their changeable parts.Below, we present the principle of type-completeness and its ramifications for language design. We then discuss some common examples of incompleteness found in existing Algol-like languages. And we end by presenting the type structure of the type-complete programming language Russell, which has been designed by the authors. As we show, the flexibility provided in Russell by the combination of a small, rich type structure and type-completeness belie Backus's assertion of inherent weakness of vonNeumann languages.", "authors": [{"name": "Alan Demers", "author_profile_id": "81100529925", "affiliation": "Cornell University, Ithaca, New York", "person_id": "P12363", "email_address": "", "orcid_id": ""}, {"name": "James Donahue", "author_profile_id": "81100145919", "affiliation": "Cornell University, Ithaca, New York", "person_id": "PP43116770", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/567446.567469", "year": "1980", "article_id": "567469", "conference": "POPL", "title": "Type-completeness as a language principle", "url": "http://dl.acm.org/citation.cfm?id=567469"}