{"article_publication_date": "01-28-1980", "fulltext": "\n Permission to make digital or hard copies of part or all of this work or personal or classroom use is \ngranted without fee provided that copies are not made or distributed for profit or commercial advantage \nand that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, \nto post on servers, or to redistribute to lists, requires prior specific permission and/or a fee. &#38;#169; \n1980 ACM 0-89791-011-7 $5.00 parsing grammar, then the function represents a Reynolds cover. To be precise, \na (context-free) grammar is a four tuple (N, 1, P, S), where N and X are disjoint finite sets of nonterminals \nand terminals,, respec\u00adtively, the start symbol S is an element of N, and P, the set of productions, \nis a finite subset of NX(NU ~)*. Let G = (M, Z, P, S) be the semantic gramna.r and H = (N, Z, Q, T) be \nthe parsing grammar. Let f be a function from M to N. Let ~ be the homomorphic ext~nsion of f from (M \nU Z)* to (N ~ Z)* such that f is the identity on Z. Let Z(P) = {f(A)~~(X)l A+x 1S in P}. Then H Reynolds \ncovers G if f(S) = T and :(P) ~ Q. Note if the pars=ammar Reynolds covers the semantic grarmnar, then \nthe parsing grammar struc\u00adturally contains the semantic grammar. If H Reynolds covers G and in addition \n7(P) = Q, then ~ is said ta be a hctuonxxphiem fxom G onto H. In a compiler, each production in a ~vat~ \ntree of H (the parsing grammar) would be replaced by an inverse image production under ~ to obtain a \nderivation tree of G (the semantic grammar). Thus there is no po~nt in having Q contain productions that \nare not in T(P), and the two granunars can be related by an onto homomorphism. One impediment to the \nuse of a semaDbic grammar and a related parsing grammar is that it is computationally difficult to test \na pair of gram\u00admars to determine if the previously mentioned relations hold between them [HR,HRS]. For \ninstance the problem of determining, for two grammars, whether the first Reynolds covers the second is \nNP-complete. Furthermore this problem is NP\u00adcomplete even for regular grammars. Also, the set of grammar \npairs related by an onto homomorphism is NP-complete, even for regular grammars. The problems of testing \ntwo grammars for structural equivalence or for Structural containment are PSPACE-hard. For regular grammars, \nthese two prob\u00adlems are PSP.ACE-complete. The upshot of these complexity results is thati if one has \na specific semantic grarmnar and a specific parsing grammar, use of a general algorithm to test if any \nof the above mentioned relations holde between the two grammars may require exponential time. Further\u00admore \nthis exponential complexity occurs even for regular grammars. However, here, we show that the previously \nmentioned sitiilarity relations can be tested for in polynomial time when the parsing grammar has certain \nnatural properties that help make the parsing grammar easy to parse. The key point of this paper is that \nalthough these similarity relations are hard to test in general, they do have efficient algorithms for \nsome of the very cases of practical interest that motivate the def\u00adinitions of the similarity relations. \nHenceforth, we assume that all grammars are reduced, i.e. that each nonterminal occurs in some derivation \nof a terminal string, since this simpli\u00adfies some of the algorithms. The reader should note that there \nare efficient polynomial time algorithms for reducing a grammar [AU, LX]. For structural containment, \nif the parsing grammar is LL(k), uniquely invertible (i.e. no two productions have the same right hand \nside) , or bounded right context (m,n ), then there is a polynomial time algorithm for testing if the \nseman\u00adtic grammar is structurally contained by the pars\u00ading grammar. As corollaries, there are polynomial \ntime algorithms when the parsing grammar is simple precedence, uniquely invertible weak precedence, uniquely \ninvertible simple mixed strategy prece\u00addence, uniquely invertible extended precedence, or bounded context \n(mrn ). For Reynolds covering or onto homomorphism, if the parsing grammar is structurally unambiguous \n(i.e. each of the struc\u00adtures of the grammar hay exactly one derivation tree), then there is a polynomial \ntime algorithm for testing if the relation holds between the semantic Trarwm.r and the parsing grammar. \nThus , for any class r of unambiguous granunars, there is a POlynOmial time algorithm for testing if \nthe relation holds when the parsing grammar is a member of ~. Furthermore, structural unambiguity can \nitself be tested for in polynomial time. Related complexity results are also presented for several problems \nfor the regular grammars, program schemes, and monadic program schemes. For example, we show that there \nis a polynomial time algorithm Car testing, for two program schemes S and T, if the sets of (not necessarily \nexecutable) computation paths of S and T are equal. Finally, we follow [AU] for the definitions of the \ngra~. ciasses considered here; and we follow ~Ma] and [N@] for the definitions of p~ogram schemes. and..monadic \nrecursion schemes. We abbreviate. bounded context and bounded right con\u00ad text by BC and BRC, respectively. \n2. Testing for Structural Containment In this section we show that there are poly\u00adnomial time algorithms \nfor testing, for grammars G and H, if G is structurally contained by H, when H is an LL(k), uniquely \ninvertible, or BRC(m,n) grammar. Thus , there are polynomial time algor\u00adithms when H is a member of any \nsubset of these three grammar classes, e.g., the simple precedence, uniquely invertible weak precedence, \nuniquely invertible simple mixed strategy precedence, uniquely invertible extended precedence, OK BC(m,n) \ngramlnare. The first Theorem deals with the case when the parsing greumnar is LL(k). For each k, there \nis a polynomial time algorithm to determine, for grammar G and LL(k) grammar H, if G is structurally \ncontained by H. But the deqree of the polynomial that bounds the runtime of the algorithm grows linearly \nwith k. Note that a compiler using det\u00aderministic top down parsing is only likely to use an LL(l) , or \nperhaps an LL(2) , grammar. Theorem 2.1. For each k ~ 1, there is a polynomial ~gorithm to determine, \nfor gramr G and LL (k) grammar H, if G is structurally contained by H. n  We outline the algorithms \nand sketch the proof of their correctness. Let k 2 1. Let G = (M,Z,P,S), and let H = (N,Z,Q,T). The algorithm \nfor k involves processing a list of objects of the form (A,B,w) where A c M, B e N, and w c Z*/k. (Here \nL/k, for language L and integer k, denotes the set of strings obtained by truncating each member of L \nto its first k symboh3. ) Processing an object results in either the algorithm halting with output NO \n, or the addition of (possibly zero) new objects to the list. No object is ever deleted from the list. \nIf all ~jects on the list have been processed, the algo\u00adrithm halts with output YES . An object (A,B,w) \nrepresents the assertion that \u00ad for each derivation tree generated from A having a frontier whose first \nk symbols equal w, the derivation tree is structurally equivalent to some derivation tree generated from \nB. Algorithm for k. 1. Intialize the list with the set of all objects of the form (S,T.W) where w c \nL(G)\\k. Mark each of these objects unprocessed .  2. While an object is marked unprocessed , do tihe \nfollowing:  A. Choose an object, say (A,B,w) , from the list that is marked unprocessed . B. If no production \nB+< of Q exists such that w ~L(~)/k, then halt with output NO . c. Otherwiee, let B+@l.. .@n be this \nproduc\u00ad tion where each ~i e X U N. [Since H ie LL(k), at most one such production exists.] For each \nproduction A~al. ..an in P where each Cti e Z U M and such that w 6 L(al. ..ank,k, do the following. \n(i) Check that n = m and that, for each i, either ai and pi are both nonterminals or both are the same \nterminal. If ~, halt with output NO . (ii) Otherwise, for each O,i e M and each U c E*/k such that, there \nexist Xry c Z*/k, for which x e L(al...ai_lk/k u c L(ai)/k, y c L(cii+l...ank/k and w = x~/k, add (Ui,6 \niru ) to the list if it is not already-on the list. D. Mark the object (A,B,w) processed . 3. Halt \nwith output YES . Sketch of correctness. We claim that G is structurally contained by H if and only if \nthe algorithm halts with output YES . Case 1. Suppose G is structurally contained by H. By induction \non the number of steps taken by the algorithm to place an object on the list, each object placed on the \nlist represents a true asser\u00adtion. Moreover, the processing of an object repre\u00adsenting a true assertion \ncan not cause the algorithm to halt with output NO . case 2. suppose the alg6rithm halts with output \nYES . To see that G is structurally contained by H, consider a derivation tree generated by G. Each nonterminal \nnode of the tree is associated with a nonterminal A in M. Also, if the nonterminal node heade a subtree \nwhose frontier is x, we associate the string x/k with the node . A member of N can be associated with \neach nonterminal node of the tree so as to obtain a structurally equivalent derivation tree of E as follows. \n (1) The nonterminal T is associated with the root of the tree. (2) Let @ be a node associated with A \nc M, B c N, and w E Z*/k. LetcX . ..a n be the sequence of 1 labels Erom Z V M associated in left-to-right \norder with the immediate descendants of V. Then A+a .,.u is the production of G applied to V in bu~ldin~ \nthe derivation tree. By induction, it can be shown that (A,B,w) was en object on the list proc&#38;ssed \nby the algorikhm. Since khe algO\u00ad rithm halted with output YES , there is one and only one production \nB+ ~ in Q such that w G L(~,)/k ~<~ compatible with a . ..a The immediate 1 n. descendants of v .xce \nassociated in left-to-right order with the eymbols in ~. R Theorem 2.2. There is a polynomial time algorithm \nto determine, for grammar G and uniquely invertible grammar H,if G is structurally contained by H. D \nWe outline the algorithm andsketch the proof of its correctness. Let G = (M,~,P,S), and let H = (N,X,Q,T). \nAlgorithm. 1. Let $, S: and T be symbols not appearing in G or in H. Construct grammars~ = (M , ~ ,P \n,S ) and H = (N ,Z ,Q ,T ) where Z = zU{$], M =MU {S }, N =N U{T }, P =P~{S +S$}, and Q = QU{T + T$}. \n 2. The set MATCHED STRUCTURES is the set of pairs  (A,B) such that A ~ M , B e N , and there is a \ncommon structure generated by A and by B. MATCHED STRUCTURES is constructed as follows. Initially it \nis empty. The pair (A,B) is added to MATCHED STRUCTURES if P has a production 4ala2. ..ak and Q has a \nproduction B+~1~2...~k such that, for 1 S i 5 k, either ai and ~i are identical terminals, or a, and \nBi are both 1 nonterminals and the pair (ai,~i) has already been determined to belong to NATCHED STRUCTURES. \n 3. If there is a production A+ u a in P 1 2... ak and a string (31~2...~k such that, for 1 S i < k, \neither a, and ~i are identical terminals, or (ai,~i) is in MATCHED STRUCTURES, but there is no production \nin Q whose right side is q!62 . ..~k. halt with output NO . Otherwise, halt with output YES . To insure \na polynomial time bound for step 3, the alyrithm should halt as soon as a required production of Q is \ndiscovered to be missing. Sketch of correctness proof. We note that G is structurally contained by H \nif and only if G is structurally contained by H . The remson for executing step 1 is khat G is not structurally \ncontained by H if and only if th~ is a structure generated by some member of M that is not generated \nby ~ member of N . (This need not be true for G and H. One counterexample is when P = {S4aS,S+a} and \nQ = {T+aT,T+aA,A.+ a}.) We also note that H is uniquely invertible. By induction on the depths of structures, \nit can be shown that, for a uniquely invertible grammar, each structure generated by a nonterminal of \nthe grammar is generated by exactly one nonter\u00adminal of the gramnar. This imp-at if (A,B) is in MATCHED \nSTRUCTURES, then A generates some structure that is generated by B, but by no other  nonterminal in \nN . Suppose G is not structurally contained by H . Let o he a structure of minimum depth that is generated \nby some nonterminal of G r but is not generated by any nonterminal of Hr. SupposeTis generated from nonterminal \nA using the production . ..akin P . Since o is of minimum depth, A + Iaz for 1 5 i 5 k, either ai is \na terminal or else it is the root of a structure that is generated by a nonterminal in NT . For each \ni, let ~i be this terminal Or nonterminal.. Thus, whenever a i. is a nonterminal, the structure it generat=e \nis generated by @i, and, thue, by no other nonterminal in N . Moreover, if cl, is a no=e=l, then the \npair (o,i,~i) is in MAT/!HED STRUCTURES. Thus, u is generated by a nonterminal in N if and only if 2 \ncontains a production whose right side is 131B2... f3k. Since by assumption U is not generated by a nonterminal \nin N , there is ~ such production in Qv. Thus while executing step 3, the algorithm halts with output \nNO . ~ Theorem 2.3. For each m,n 2 1, there is a polyno\u00admial time algorithm to determine, for granunar \nG and BRC(m,n) grammar H, if G is structurally contained by H. 0 The algorithms of Theorem 2.3 are extensions \nof the algorithms of Theorem 2.2 and will not be presented here. 3. Testing for Reynolds Covering In \nthis section we show that thsre is a poly\u00adnomial time algorithm for tasting, for grammar G and structurally \nunambiguous grammar H, if G is Reynolds covered by H or if there is an onto homomorphism from G to H. \nMoreover, if such a Reynolds cover or onto homomorphism exists, our algorithms output appropriate functions \nfrom the nonterminals of G to the nonterminals of H. Since the class of structurally Unambigwus grammars \npro\u00adperly contains the class of unambiguous grammars, polynomial time algorithms exist when H e P for \nany class r of unambiguous grammars. Thus, poly\u00adnomial time algorithms exist when H is a member of most \nof the grammar classes corresponding to parsing methods used in compilers. First, we present two relevant \naspects of structural ambiguity. Proposition 3.1. There is a polynomial time algo\u00adrithm to determine \nif a grammar is structurally unambiguous. n Since every uniquely invertible grammar is structurally \nunambiguous and since there is a method for translating a grammar into a structurally equivalent uniquely \ninvertible grammar [MC], the following holds. Proposition 3.2. There is an exponential time algorithm \nto translate a grammar into a structurally equivalent gramr that is structurally unambigu\u00ad ~ ous . The \nmain result of this section is the following. Theorem 3.3. There is a polynomial time algorithm for testing, \nfor grammar G and structurally unam\u00adbiguous gramma H, if H Reynolds covers G. More\u00adover if H Reynolds \ncovers G, then this algorithm outputs an appropriate function from the nonterm\u00adinals of G to the nonterminals \nof H. D sketch of algorithm. Let G = (M,~,P,S), and H = (N,Z,Q,T). The idea is to find, for each A 6 \nM, a derivation tree generated by G containing A and a structurally equivalent derivation tree T generated \nby H. Since H is structurally unambiguous, if there is a Reynolds cover then A must map into the correspond\u00ad \ning node of -c. However, these derivation trees are not explicitly constructed since they may be expo~tial \nin the sizes of G and H. Ratherr each tree is ccnnpac~ly represented without producing it in entirety. \nThe algorithm consists of 4 eteps. 1. Step 1 of the algorithm consists of computing, for each A E M, \na set MATCH(A) defined in terme of a specific tree, called TREE(A) , that A generates. MATCH(A) = {B~B \nc N, and B gener\u00adates a tree that is structurally equivalent to TREE (A) }. The sets MATCH(A), for A \ne M, are ccmputed in a manner related to the test for aliveness in [LRS]. The trees TREE(A), for A E \nM, are nOt explicitly computed. If MATCH(A) has not already been computed and there is a production \nA+a1a2. ..ak in P such that each ai is either a terminal or a nonterminal for which mTCH(ai) has already \nbeen computed, then MATCH(A) = {BIB e N, and there is a production B+6162. ..bk in Q for which (3i equals \na. if ai is a terminal and 1 ~i e MATcH(ai), otherwise]. 2. Step 2 consists of oomputing, for each A \n6 M, an incomplete reachability tree for A. .ln incomplete reachability tre~ =e=ivation tree produced \nby G having S as its root and frontier nodes that are elements of Z U M. An incomplete reachability tree \nfor A is an incomplete reachability = ~w~ich A is one of the frontier nodes. A reachability tree for \neach nonterminal of G can be found in polynomial time in a manner analogous to the reachability test \nin [LRSI. The number of productions used in each reachability tree is bounded by [Ml. 3. Step 3 COnSiStSOf \ncomputing, for each A c M, a set CANDIDATE(A) for candidate images of A in N. Specifically, the set CANDIDATE(A) \nis a subset of MATCH(A) constructed as follows. For each B G MATCH(A), take the incomplete reachability \ntree for A, label one occurrence of A on its frontier with B, and label every other nonterminal node \na on the frontier with MATCH(a). Compute the label of each interior node of the incomplete reachability \ntree in a bottom-up manner. Each interior node ie labelled with the set of nonterminals C e N for which \n(i) there is a production C+@1~2.. .f3k in Q and(ii) the interior node hae k inunediate descendants such \nthat, for 1 s i s k, if the ith immediate descendant is a terminal then (3Ps the same terminal and if \nthe i= immediate descendant is a nonterminal then ,6 i is an element of the label of that descendant. \nIf the root of the node is labelled with a eet containing T, then B is included in CANDIDATE(A).  4. \nBecause H is structurally unambiguous, it can be shown that, for each A e M, the set CANDIDATE(A) hae \nat meet one member. If the Set CANDIDATE(A) ~ ~y for some A c M, then the algorithm halts with output \nNO . Other\u00adwise, CANDIDATE induces a map from M to N. The algorithm checks that this map is a Reynolds \ncover. If so, the algorithm halts with output lly~sl,. If not, the algorithm halts with output NO . \n We claim that the algorithm, given G and H as input, halts with output YES if G is Reynolds covered \nby H, and halts with output NO other\u00adwise. a An immediate corollary is the following. Corollary 3.4. \nThere ie a polynomial time algc\u00adrithm to determine if there exists an onto homo\u00ad morphism from a given \ngrammar G to a given structurally unambiguous grarmnar H. D Additional results obtained include the following. \nTheorem 3.5. There is a polynomial time -algor~thm that given a grammar G, outpute a grammar H such that \n(1) H is structurally Unambiguous; (2) H Reynolds covers G; and (3) For any structurally unambiguous \ngrammar H that Reynolds covers G, Hs alSO Reynolds covers H, so that L(H)sL(H ). c1  Theorem 3.6. [HR]. \nThere are polynomial time =hms to determine, for grammar G, if G is Reynolds covered by 1. for all k \n2 1, an LL(k) grammar, 2. for all k > 1, an LR(k) grammar, 3. a simple precedence grammar, 4. a uniquely \ninvertible weak precedence gram\u00admar, 5. a uniquely invertible simple mixed strategy precedence grammar, \n 6. a uniquely invertible extended precedence granunar, and  7. an operator precedence grammar. Moreover, \nwhen such a covering grammar H exists, these algorithms. produce one such H and a mapping from the symbols \nof G to its symbols. D  4. Some Additional Corollaries In this section several additional corollaries \nof the results in Sections 2 and 3 are presented. These corollaries show that there are polynomial time \nalgorithms for each of the following problems: 1. the structural equivalence and structural containment \nproblems for the grarmnar classes in Section 2; 2. for all k 2 1, the equivalence and contain\u00adment problems \nfor the regular LL(k) grammars; 3. the R-equivalence problem for program schemes  [K] (i.e., the problem \nof determining for program schemee S and T if the sets of all (not necessarily executable) computation \npaths of S and of T are equal); 4. the isomorphism problem for free program schemes [Mad (i.e., the problem \nof determin\u00ading for free program schemes S and T if, for all interpretatione I, the eequences of instructions \nexecuted during the computations of S and of T under I are the same except possibly for statement labels); \n 5. one possible version of the isomorphism problem for the free monadic recursion schemes; and  6. \nthe Reynolds covering and onto homomorphism probl&#38;s for the gra&#38;ar classes in -Section 3.  \nWe also mention otie difficulty with extending the results of Section 2 to the LR(k) grammars. Theorem \n4.1. Let r be a subclass of one of the ng grammar classes: 1. for all k 2 1, the LL(k) grammars; 2. \nthe uniquely invertible grammars; and 3. for all m,n 2 1, the BRC(m,n) granrnars. Then, there is a polynomial \ntime algorithm to determine, for grarmnars G and H in r, if G is structurally equivalent to H or if G \nis structur\u00adally contained by H. o  Proof. The proof is immediate from Theorems 2.1, 2.2, and 2.3 since \ntwo grammars G and H are structurally equivalent if and ord.y if each struc\u00adturally contains the other. \nM Grammar classes p satisfying the conditions of Theorem 4.1 include, for all k 2 1 the LL(k) and strong \nLL(k) grammars, the uniquely invertible gr ammars, the simple precedence grammars, the uniquely invertible \nweak precedence grammars, the uniquely invertible simple mixed strategy precedence grammars, the uniquely \ninvertible ex\u00ad tended precedence grammars, and for all m,n > 1 BRC!(m, n) and BC(m, n) grammars. since \nevery uniquely invertible gramnar is structurally unambiguous, a second corollary of Theorem 2.2 and \nProposition 3.2 is the following. Theorem 4.2. There is an exponential time algorithm to determine, 1. \nfor grammars G and H, if G is structurally con\u00adtained by H or if G is structurally equivalent to H; \n2. for parenthesis grammars [MC] G and H, if L(G) .= L(H) or if L(G) = L(H); 3. for nondeterministic \ntop-down tree automata [T] M and N, if L(M) .G L(N) or if L(M) = L(N); and 4. for nondeterministic bottom-up. \ntree automata [TI M and N, if L(M)= L(N) or if L(M) = L(N). D  Eaoh of these eight problems are equivalent \nunder deterministic polynomially time-bounded re\u00adducibility, and are PSPACE-hard [HS]. A third immediate \ncorollary of Theorems 2.1 and 4.1 is the following. Theorem 4.3 [H]. For all k 2 1, there is a poly\u00ad \nnomial time algorithm to determine, for regular LL(k) granunars G and H, if L(G) ~ L(H) or if L(G) = \nL(H). D Proof. For regular grammars G and H, rL(G) S L(H) if and only if G is structurally contained \nby H, and (ii) L(G) = L(H) if and only if G is structurally equivalent to H. n As shown in [H]., there \nare polynomial time algorithms to construct, for program scheme S, reg\u00adular LL(l) grammars G and GS, \n, respectively, such s that (i) L(GS) equals the set of all finite (not neces\u00adsarily executable) computation \npaths of S and (ii) L(GS,) equals the set of all marked finite  prefixes of computation sequences of \nS. An analogous polynomial time algorithm exists re\u00adlating monadic recursion schemes to LL(l) grammars \n[z]. These algorithms, together with Theorems 2.1 and 4.1, can be used to show that several decidable \nproblems for the program schemes, free program schemes, and free monadic recursion schemes have polynomial \nalgorithms. Definition 4.4,1.[Ma] A program scheme S is eaid to be free if and only if each of its computation \npathe is executable under some interpretation. 2. [ANPl A monadic recursion scheme S is said to be free \nif and only if, for no Herbrand interpre\u00adtation I of S, does the computation of S under I entail the \ntesting of some predicate twice on the same value. 3. Two monadic recursion schemes are said to be isomorphic \nif and only if, for all interpretations I, the sequences of defining equations executed during the computations \nof S and of T under I are identical except for labels. u  Theorem 4.5. There are polynomial time algorithms \nto determine, (1) for program schemes S and T, if S and T are R-equivalent;  (2) for free program schemes \nS and T, if S and T are isomorphic; and  (3) for free monadic recursion schemes S and T, if S and T \nare isomorphic. n An immediate corollary of Theorem 3.3 is the following. Theorem 4.6. Let ~ be a subclass \nof the struc\u00adturally unambiguous grammars. Then, there is a polynomial time algorithm to determine, for \ngram\u00admars G and H in f , if H Reynolds covers G or if there is an onto homomorphism from G to H. D Any \nclass r Of unambiguous grammars satisfies the condition of Theorem 4.6. Thus , there are polynomial time \nalgorithms to solve the Reynolds covering and onto homomorphism problems for most of the grammar classes \ncorresponding to parsing methods used in compilers. The proof of Theorem 3.3 also shows Ghat the homomorphism \nand onto homo\u00admorphism problems for the deterministic finite automata and for the unambiguous nondeterministic \nfinite automata are decidable in polynomial time. The homomorphism and onto homomorphism problems for \narbitrary nondeterministic finite automata are Nl+complete [HRI. Finally, we have been unable to extend \nthe results of Section 2 to the LR(k) grammars for any k. It turns out that the structural equivalence, \nstructural containment, and equivalence problems for the LR(0) grammars are at least as hard as the equivalence \nand containment problems for the unambiguous nondeterministic finite automata. Formally, the equivalence \nand containment problems for the unambiguous n-deterministic finite auto\u00admata are deterministic polynomial \ntime reducible to the structural equivalence, structural contain\u00adment, and equivalence problems for the \nregular LR(0) grammars. We know of no polynomial time algorithms to solve the equivalen~ or containment \nproblems for the unambiguous nondeterministic finite automata. 5. References [ml A.V. Aho, and J.D. Unman, \nParsing, Translation, and and 2, Prentice-Hall,Englewood 1972 and 1973. The Theory Compiling, Cliffs, \nof Vols. NJ, 1 [AMP] E. Ashcroft, A. Manna, and able properties of monadic JACM 20, 1973, pp. 489-499. \nA. Pnueli, functional Decid\u00adschemas, [GHI J.N. Grayr and M.A. Harrison, On the and reduction problems \nfor context-free grammars, JACM 19, 1972, pp. 675-698. covering [H] H.B. Hunt of regular JCSS . III, \nObservations on expression problems, the to complexity appear in [HRI H.B. Hunt III, and D.J. of grammatical \nsimilarity inary report, Proc. of etical Computer Science, 1967, PP.139-145. RosenkrantzrComplexity relatiOns--prelim\u00ada \nConference on Theor-Waterloo, Canada, [HS] H.B. Hunt complexity Proc. of Theory of 1975, pp. 111, and \nT.G. Szymanski, On the of grammar and related problems, Seventh Annual ACM Symposium on Computing, Albuquerque,New \nMexico 54-65.  [HRSI H.B. Hunt III, D.J. Rosenkrantz, and T.G. Szy -manaki, On the equivalence, containment, \nand covering problems for the regular and context\u00adfree languages, JCSS 12, 1976, pp.222-268. [K] D.M. \nKaplan, valence of 386. Regular programs, expressions and the equi-JCSS 3, 1969, pp. 361\u00ad [LRSI P.M. \nLewis, D.J. Compiler Design Reading, Mass., Rosenkrantz, and R.E. Theory, Addison-Wesley, 1976. Stearns, \n[Ma] Z. Manna, McGraw-Hill, Mathematical New York, Theory 1974. of Computation, [MC] R. McNaughton, Parenthesis \n1967, pp. 490-500. grammar.,JACM 14, [PU] M.C. Paull, valence of 1968, pp. and S.H. context-free 427-468. \nUnger, Structural grammars, JCSS equi\u00ad2, [RHj J.C. Reynolds, and R. Haskell, coverings, Unpublished manuscript, \nGrammatical 1970. [T] J.W. Thatcher, Tree automata: an survey, in A.V. Aho (Ed.), Currents Theory of \nComputing, Prentice-Hall, Englewood Cliffs, N.J., 1973, pp. informal in the 143-172. [Z] H.P. Zeigler, \nFormal models for some standard features of programming languages, Proc. of ACM Symp. on Theory of Computing, \nMarina Del Rey, California, 1969, pp. 211-216. \n\t\t\t", "proc_id": "567446", "abstract": "Efficient algorithms are presented for several grammar problems relevant to compiler construction. These problems include(i) testing, for a reduced context-free grammar G and an LL(k), uniquely invertible, or BRC(m,n) grammar H, if G is structurally contained by H, and(ii) testing, for a reduced context-free grammar G and a structurally unambiguous grammar H, if G is Reynolds covered by H or if there is an on to homomorphisem from G to H.Related complexity results are presented for several problems for the regular grammars, program schemes, and monadic program schemes.", "authors": [{"name": "H. B. Hunt", "author_profile_id": "81337489860", "affiliation": "SUNY at Albany, Albany, NY", "person_id": "PP42050708", "email_address": "", "orcid_id": ""}, {"name": "D. J. Rosenkrantz", "author_profile_id": "81100627499", "affiliation": "SUNY at Albany, Albany, NY", "person_id": "PP43124791", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/567446.567467", "year": "1980", "article_id": "567467", "conference": "POPL", "title": "Efficient algorithms for structural similarity of grammars", "url": "http://dl.acm.org/citation.cfm?id=567467"}