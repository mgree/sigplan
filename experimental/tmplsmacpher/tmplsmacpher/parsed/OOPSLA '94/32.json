{"article_publication_date": "10-01-1994", "fulltext": "\n A Status Report on the 007 OODBMS Benchmarking Effort* Michael J. Carey David J. Dewitt Chander Kant \nJeffrey F. Naughton Computer Sciences Department University of Wisconsin-Madison Abstract The 007 Benchmark \nwas first published in 1993, and has since found a home in the marketing litera-ture of various object-oriented \ndatabase management system (OODBMS) vendors. The 007 Benchmark (as published) was the initial result \nof an ongoing OODBMS performance evaluation effort at the Uni-versity of Wisconsin. This paper provides \nan update on the status of the effort on two fronts: single-user and multi-user. On the single-user front, \nwe review and critique the design of the initial 007 Benchmark. We discuss some of its faults, the reasons \nfor those faults, and things that might be done to correct them. On the multi-user front, we describe \nour current work on the development of a multi-user benchmark for OODBMSs. This effort includes changes \nand exten-sions to the 007 database and the design of a family of interesting multi-user workloads. \n Introduction The 007 benchmarking effort is an on-going research project that aims to evaluate the performance \nof OODBMSs. The first result of this effort, which built upon the foundation laid by earlier OODBMS benchmarking \nefforts [CS92, RKC87, AndSO, DD88], was the single-user 007 benchmark [CDN93]. This benchmark differed \nfrom its predecessors in that it attempted to provide a truly comprehensive test of *This work was funded \nby Digital Equipment Corporation. single-user OODBMS performance. Among the per-formance characteristics \ntested by the 007 Bench-mark are the speed of a given OODBMS on a wide va-riety of different pointer \ntraversals (e.g., over cached data and disk-resident data, including both sparse and dense traversals), \nupdates (including updates of both indexed and unindexed data, repeated updates, sparse updates, updates \nof cached data, and object creation and deletion), and simple object queries (in-cluding both exact-match \nand range queries and both pointer-based and value-based joins). To date, we have completed phase one \nof the 007 effort, in which we defined the single-user bench-mark and used it to profile the performance \nof a num- ber of commercial OODBMSs. A full report on this phase, giving a detailed benchmark description \nand measured performance results for four systems on a common client-server hardware base, is available \nvia anonymous ftp from ftp. cs . wise. edu in the 007 directory. The ftp directory also contains the \nsource code for the benchmark for five systems.l The dif-ference between the number of available implementa-tions \nand the number of systems for which we have published our measurements is a reflection of the fact that \nthe 007 Benchmark is not without controversy. The approach used to specify the benchmark, and to limit \nliberties that might otherwise be taken when implementing it, has been a point of some contention and \nconcern for several vendors who would have liked to participate (but under a different set of ground \nrules). Despite the information provided by 007 and other OODBMS benchmarking efforts, there is vir-tually \nno published information available about the Yeal performance of commercial OODBMSs. This is because \nreal applications usually involve multi-ple users, making the multi-user performance of OODBMSs critical \nto understand. Unfortunately, the 1 Also available on the World Wide Web at URL ftp://ftp.cs.wisc.edu/OO7. \nperformance studies to date provide little or no in-sight into what can be expected for multi-user per-formance. \nThis leaves a critical gap in the available knowledge for consumers of OODBMS technology. The 007 effort \nat Wisconsin is currently working to address this gap by developing a comprehensive multi-user OODBMS \nbenchmark. We have changed and extended the design of the 007 database to bet-ter accommodate multi-user \nworkloads, and we have developed a parameterized OODBMS workload that produces an interesting family \nof multi-user work-loads. We are in the process of trying out our bench-mark on several OODBMSs, and \nwe are also actively seeking input from OODBMS vendors and users re-garding our design. The remainder \nof this paper begins with a brief re-view of the design of the single-user 007 Benchmark. (Readers familiar \nwith the 007 Benchmark can skip this section of the paper.) The review is then followed by a retrospective \ncritique of the benchmark and the way in which it was administered; this section of the paper is largely \nnon-technical, presenting some im-pressions and lessons from our single-user benchmark-ing experience. \nWe then turn our attention to our current work on multi-user OODBMS benchmark-ing, discussing proposed \nchanges in the 007 database and the associated multi-user workload family that we propose to use in evaluating \nOODBMS perfor-mance. A major goal of this portion of the paper is to convey the status of our effort \nin order to publically solicit feedback from vendors and users of commer-cial OODBMS technology regarding \nwhat we re doing right, what we re doing wrong, and what we might be missing in our current multi-user \nbenchmark design. 2 Single-User 007 Benchmark As mentioned in the introduction, the original 007 Benchmark \n[CDN93] was designed to test OODBMSs on a wide variety of traversal, update, and query tasks in order \nto thoroughly explore their single-user per-formance characteristics. As a result, its database structure \nand operations are non-trivial. The 007 Benchmark is intended to be suggestive of many dif-ferent CAD/CAM/CASE \napplications, although in its details it does not model any specific application. It is important to \nrealize that the goal of the bench-mark is to focus on important aspects of system per-formance, not \nto model a specific application. Ac-cordingly, in the following when we draw analogies to applications, \nwe do so to provide intuition into the benchmark rather than to justify or motivate the benchmark. Parameter \nSmall Medium NumAtomicPerComp 20 200 NumConnPerAtomic WV9 3/6/g DocumentSize (bytes) 2000 20000 Manual \nSize (bytes) 1OOK 1M NumCompPerModule 500 500 NumAssmPerAssm 3 3 NumAssmLevels 7 7 NumCompPerAssm 3 3 \nNumPrivateModules 1 (per client) 1 (per client) Table 1: 007 Benchmark database parameters. 2.1 007 \nDatabase Description There are two official sizes of the 007 Benchmark database: small and medium. Table \n1 summarizes the parameters of the 007 Benchmark database; their meanings will become clear in a moment \nwhen we walk through the database design. In our five implementations of the benchmark, all of these \npa-rameters are controlled by a configuration file that is read by the database generation code. As indicated \nin the table, the benchmark was designed even ini-tially to scale in proportion to the number of clients \nby having one private module per client in both the small and medium databases (modules are described \nbelow), Our goal in setting the parameters for the small and medium cases in the way that we did was \nto arrange it so that for the small database, on high-locality workloads, each client s working set will \nfit in its cache, while for the medium database, the working set will not fit in the client cache; also, \nfor the small database, the entire database can fit in the server cache, whereas the medium database \nwill not fit in the server cache. We are generalizing here, of course, as exactly what will and won t \nfit depends upon (1) the characteristics (e.g., the pointer representation) of the specific OODBMS being \ntested and (2) the sizes of the client and server caches. Before describing the 007 Benchmark database \nfurther, we note that the full source code for our im-plementations in all the systems we tested is available \nby anonymous ftp from f tp . cs . wise . edu. 2.1.1 The Design Library A key component of the 007 Benchmark \ndatabase is a set of composite parts. Each composite part cor-responds to a design primitive such as \na register cell in a VLSI CAD application, or perhaps a procedure in a CASE application; the set of all \ncomposite parts forms what we refer to as the design library within the 007 database. In the design library, \nthe num-ber of composite parts associated with each module is controlled by the parameter NumCompPerModule, \nwhich is set to 500. Each composite part has a num- ber of attributes, including the integer attributes \nid and buildDate, and a small character array type. Associated with each composite part is a document \n object, which models a small amount of documenta- tion associated with the composite part. In addition \nto its scalar attributes and its as-sociation with a document object, each composite part has an associated \ngraph of atomic parts. In-tuitively, the atomic parts within a composite part are the units out of which \nthe composite part is constructed. In the small benchmark, each compos-ite part s graph contains 20 atomic \nparts, while in the medium benchmark, each composite part s graph contains 200 atomic parts. (This number \nis controlled by the parameter NumAtomicPerComp.) For exam-ple, if a composite part corresponds to a \nprocedure in a CASE application, each of the atomic parts in its as-sociated graph might correspond to \na variable, state-ment, or expression in the procedure. One atomic part in each composite part s graph \nis designated as the root part. Each atomic part has the integer attributes id, buildDate, x, y, and \ndocId, and the small character array type. The buildDate values in atomic parts are randomly chosen in \nthe range MinAtomicDate to MaxAtomicDate, which is currently 1000 to 1999. In addition to these attributes, \neach atomic part is con-nected via a bi-directional association to several other atomic parts, as controlled \nby the parameter Num-ConnPerAtomic; this parameter is first set to 3, then to 6, and then to 9 in the \nsingle-user benchmark. Our initial plan was to connect the atomic parts within each composite part in \na random fashion. However, random connections do not ensure complete connec-tivity (i.e., reachability \nof all of a composite part s atomic parts from its root part). To ensure complete connectivity, one connection \nis initially added to each atomic part to connect the parts in a ring; the remain-ing connections are \nthen added at random. The connections between atomic parts are imple-mented by interposing a connection \nobject between each pair of connected atomic parts. Here the intu-ition is that the connections themselves \ncontain data; the connection object is the repository for that data. A connection object contains the \ninteger field length and the short character array type. Figure 1 depicts a composite part, its associated \ndocument object, and its associated graph of atomic parts. One way to view this is that the union of \nall atomic parts corresponds to the object graph in the 001 benchmark [CS92]; however, in 007 this ob-ject \ngraph is broken up into semantic units of locality by the composite parts. Thus, the composite parts \nin 007 provide an opportunity to test how effective var-ious OODBMS products are at supporting complex \nobjects. 2.1.2 Assembling Complex Designs The design library, which contains the composite parts and \ntheir associated atomic parts (including the connection objects) and documents, accounts for the bulk \nof the 007 database. However, a set of compos- ite parts by itself is not sufficiently structured to \nsup-port all of the operations that we wished to include in the benchmark. Accordingly, we added the \nnotion of an assembly hierarchy to the database. Intuitively, the assembly objects correspond to higher-level \nde-sign constructs in the application being modeled in the database. For example, in a VLSI CAD appli-cation, \nan assembly might correspond to the design for a register file or an ALU. Each assembly is either made \nup of composite parts (in which case it is a base assembly) or it is made up of other assembly objects \n(in which case it is a complex assembly). The first (bottom) level of the assembly hierarchy consists \nof base assembly objects. Base assembly ob-jects have the integer attributes id and buildDate, and the \nshort character array type. Each base assem-bly has a bi-directional association with three com-posite \nparts. (The number of composite parts per base assembly is controlled by the parameter Num-CompPerAssm.) \nIn the single-user 007 Benchmark, each base assembly had associations with two kinds of composite parts, \nprivate and shared, although only the private associations were used; the shared associ-ations were designed \ninto the database for use in the multi-user benchmark design that we were anticipat-ing for our follow-on \nwork. Higher levels in the assembly hierarchy are made up of complex assemblies. Each complex assembly \nhas the usual integer attributes, id and buildDate, and the short character array type; additionally, \nit has a bi-directional association with three subassemblies (controlled by the parameter NumAssmPerAssm), \nwhich can either be base assemblies (if the complex assembly is at level two in the assembly hierarchy) \nor other complex assemblies (if the complex assembly is higher in the hierarchy). There are seven levels \nin the assembly hierarchy (controlled by the parameter NumAssmLevels). Each assembly hierarchy is called \na module. Mod-ules are intended to model the largest subunits of the Id = 248590 docld = 345 type = \ntypeNumber3 title = widget #27 dots 4 t buildDate = 3587341 documentation text = \"widget 827 doesn't \nreally do very much but we put one in the spec so here it is\" Figure 1: A Composite Part and its associated \nDocument object. database application, and are used extensively in the 2.2.1 Read-Only Traversal Operations \nmulti-user workloads; they are not used explicitly in The first category, read-only traversals, consists \nofthe small or medium single-user 007 databases, each four distinct traversal operations. The first two \nareof which consists of just a single module, but were based on traversing the assembly hierarchy. Theincluded \nfor their usefulness in our planned multi-traversals start from the top (i.e., at the module level)user \nfollow-on work. Modules have several scalar at-and proceed in a depth-first manner; as base assem-tributes \n-the integers id and buildDate, and the blies are reached, each referenced composite part isshort character \narray type. Each module also has an accessed. The first traversal is a dense traversal thatassociated \nManual object, which is a larger version of accesses the entire atomic parts graph of each encoun-a document. \nManuals are included for use in testing tered composite part (via a depth-first search). Thethe handling \nof very large (but simple) objects. second traversal is a sparse traversal, which accessesFigure 2 depicts \nthe full structure of the single-only the root part of each composite part, leaving the user 007 Benchmark \ndatabase. Note that the pic-remainder of the atomic parts graph untouched. The ture is somewhat misleading \nin terms of both shape third and fourth traversals focus on the manual ob-and scale; the actual assembly \nfanout used is 3, and ject associated with a module. The third traversalthere are only (37 -1)/2 = 1093 \nassemblies per mod-scans the entire text of the manual, while the fourthule in the small and medium databases, \ncompared to traversal touches only the first and last characters of10,000 atomic parts per module in \nthe small database the manual s text (to test random accesses to veryand 100,000 atomic parts per module \nin the medium large objects). database. Also, as mentioned earlier, the composite Each of the traversal \noperations is implemented us-parts associated with a given module are private to ing methods of the object \nclasses involved, as eachthe module in the sense that they are only referenced one is supposed to represent \nan operation that wasby base assemblies of that particular module. anticipated (unlike the operations \nin the query cate-gory) at application design time. For each traversal, 2.2 Single-User 007 Operations \nboth a cold time (based on starting with an empty cache) and an average hot time (based on reaccessing \nThe operations of the single-user 007 Benchmark exactly the same data repeatedly) are reported. In can \nbe roughly grouped into three categories: read-addition, for each hot time, two cases are reported only \ntraversals, updates, and queries. Each category, -with the traversals being performed either within in \nturn, contains a number of different operations in one transaction or as many transactions -to expose \norder to achieve broad OODBMS performance cover-caching performance both within and across transac-age. \ntion boundaries. Module i I ,..........__........ . . . . . . . . . _ . . . . . . . / :    r e-l \ncomplex assemblies base assemblies  Design Library of Composite Parts Figure 2: Structure of a module. \n2.2.2 Update Operations guages and implementations as of the initial 007 benchmarking exercise). Included \nare an exact-match The second category, updates, includes two update query that looks up one atomic part, \ntwo range traversals. Both of these execute the first (dense) queries that look up 1% and 10% of the \natomic parts, assembly hierarchy traversal operation but perform respectively, and a sequential scan \nquery that accesses updates along the way. The first update traver- every atomic part in the type extent \nfor atomic parts. sal changes the value of two unindexed attributes The three more complex queries in \nthe query cat-of either one, all (once), or all (repeated a total of egory are essentially join queries. \nThe first two four times) of the atomic parts that it encounters. are pointer joins, i.e., they are navigational \nin na-The second update traversal is similar, but instead ture. One is a three-level path query that \ngoes from changes the value of one indexed attribute of the a document title to the base assemblies that \nuse the atomic parts. The update category also includes an document s corresponding composite part; the \nother insert operation, which inserts five new composite is a two-way pointer join of two connected type \nex-parts into a module, and a corresponding delete op- tents (composite parts and base assemblies). The \neration, which deletes the same five composite parts last join query is an ad hoc (value-based) join \nbe-in order to restore the module to its original logical tween two other type extents (between documents \nstate. Together, the collection of update operations and atomic parts, based on matching the documents \nin the benchmark tests the efficiency of a number of ids and the document id field that atomic parts \nhave).update-related implementation issues, especially the Each of the 007 object queries are written \nas declar-recovery mechanism (usually based on writing log atively as possible using whatever facilities \nthe tested records or shadows) and the index manager. OODBMS has for expressing, or otherwise imple-menting, \nqueries.  2.2.3 Object Queries The final category, queries, is a set of seven object queries that range \nfrom simple to moderately com-plex (at least given the state of OODBMS query lan-  3 Single-User Lessons \nAs mentioned in the introduction, we implemented and ran the single-user 007 Benchmark on five dif-ferent \nOODBMSs (four commercial systems plus our own research prototype). We were eventually permit-ted to report \nresults for four of the five systems. This experience taught us quite a bit about the systems tested \nas well as the overall state of the commercial OODBMS industry. The various technical lessons that we \nlearned are reported in [CDN93]. Here we look back on some of the other, less technical issues and lessons \n(including a few design flaws) that re-sulted from the initial 007 work. 3.1 Specification Woes One \nissue that concerned us from the very beginning of our work was how to ensure that the 007 bench-mark \nimplementations for two different OODBMSs are really providing a fair, apples to apples com-parison of \nthe systems. Essentially, this question boils down to what really is the benchmark? (in terms of the \ndegrees of freedom in its implemen-tation). Perhaps less intuitively, we found that in OODBMS benchmarking \nthis also boils down to the question <what really is the system? Every system tested had a rather different \nset of features as well as a different set of potentially useful performance knobs (each requiring a \ndifferent level of wizardry and a dif- ferent degree of knowledge regarding specific details of the 007 \ndatabase and operations). An easy out would have been to simply specify the benchmark operations at \na high level (e.g., in English) and let each vendor implement them in the best way that they could using \nalgorithms of their choosing and every performance feature available in their sys-tem. However, we specifically \nwanted to avoid this, as it would have defeated our purpose. That approach would have turned the 007 \neffort into a program-ming contest for wizards rather than a comparison of OODBMS engines as they would \nlikely be used by intelligent, but not exceptional, application program-mers. Instead, we chose a different \napproach -all official 007 numbers to date have been obtained by us, running code that we either implemented \nperson-ally or else audited very carefully, and each system was tested on identical hardware at the University \nof Wisconsin. Was this really necessary? Unfortunately, yes. As just one example of why we conducted \nthe benchmark this way, most OODBMSs (all commer-cial systems that we tested, in fact) provide a set \nfacility. This provides a convenient way to represent the fact that a given object, A, has a one-to-many \nassociation with a set of other objects, B, C, and D. Using the set facility, you can store in object \nA a set of references to the objects B, C, and D. The manuals of the various systems, as well as the \nprovided ex- ample programs, do indeed implement one-to-many associations in this way. However, each \nOODBMS essentially provides the full power of C++, making it possible to bypass the system-provided set \nfacility. Instead, one could use C++ to code a special asso- ciation class tailored for the 007 benchmark. \nThis hand-coded association class might be much faster than the standard system-provided class. Should \ndo- ing so be an acceptable approach? Our answer was that system-provided classes for things like sets \nmust be used wherever they are ap- propriate in the benchmark -otherwise users who employ system-provided \nfacilities would be unable to duplicate the benchmark results. Moreover, one would hope that these system-provided \nfacilities, de- scribed in the manuals as the recommended ap- proach for modeling relationships, would \nbe reason- ably well implemented. However, in some cases we were criticized with the comment that sets \nare not part of the engine, so why are you benchmarking them? Similarly, we were criticized in some cases \nfor not using the fine-tuning features offered by a sys- tem when doing so meant hard-wiring details \nof the 007 database parameters (e.g., exact sizes of all ob-jects, exact cardinalities of all sets, and \nso on) into the benchmark code. Again, we resisted vendors urgings to do this, as we felt that typical \nOODBMS appli-cation programmers would not be able to use such features nearly as well as an expert programmer \nwho was implementing the 007 Benchmark with full, a priori knowledge of all of the database parameters. \nOur focus was on obtaining OODBMS performance information that would be useful to typical program-mers. \nWhen we ran the original tests there was no clear way to resolve this issue. We started by specify-ing \nthe benchmark in English, and we simply made public our own implementations for five systems to clarify \nthe intent of the English. Obviously, this needs to be tightened up. Luckily, now there is hope. The \nODMG committee, whose membership includes all major OODBMS vendors, recently pro-posed a standard data \nmodel for OODBMS applica-tions [Cat941 and embeddings for this model in C++ and Smalltalk. A clean way \nto specify the benchmark in the future will be to provide an implementation of the benchmark using this \nstandard, thus ensuring that all systems running the benchmark will run ex-actly the same code. A particularly \ninteresting (but time consuming) exercise would be to run the bench-mark both this way and using all \nof each vendor s bells and whistles (or at least those made available through optional extensions of \nthe ODMG standard) to see what each system s dynamic range might be.  3.2 More Specification Woes A \nrelated problem with the specification and imple-mentation of the initial 007 Benchmark -or per-haps \njust an extreme example of the issues discussed above -arose with respect to the queries in the benchmark. \nWhen we began the 007 project, the target OODBMSs varied wildly in their support for declarative query \nprocessing. Some systems had no declarative query language. Among those systems that did support a declarative \nquery language, the query languages had widely differing expressive pow-ers. For this reason, we implemented \neach of the 007 queries in the system-provided query language if pos- sible; otherwise we hand-coded \na reasonable eval-uation algorithm for the query in C++, using the same algorithm across all query-impaired \nsystems to avoid the programming contest problem. We have been justifiably criticized that this approach \ncould penalize a system for providing a query processor. Due to this problem, the query processing as-pect \nof the 007 Benchmark is an aspect of the benchmark that needs significant improvement. Once again, as \nthe OODBMS industry matures, however, there is hope for avoiding this situation. Specifically, the ODMG \ncommittee has also proposed a standard query language, called OQL, that ODMG-compliant OODBMSs will be \nrequired to support. In light of this proposal, we will revisit the query portion of the benchmark. For \nthe next major iteration of the single-user benchmark, we intend to express the queries using the OQL \nquery language standard. If a given system does not support OQL or an equiv-alently powerful declarative \nlanguage, it will not be permitted to participate in the query portion of the benchmark. 3.3 Auditing \nthe Auditors One legitimate concern with benchmarking is: who will audit the benchmark implementation \nand re-sults? As mentioned above, our initial solution was to do most of the implementation work and \nall of the benchmark-running work ourselves, very carefully au-diting (and toning down when necessary) \nthose im-plementations that were vendor-provided. Moreover, in the past, we (the UW 007 team) have set \nthe rules for the benchmark and have interpreted them and resolved all disputes. We do not enjoy playing \nall of these roles; ideally, some standards group like the TPC should audit benchmarks. Perhaps in the \nfuture 007 (or some derivative benchmark) will be adopted by ODMG or a related standards group who will \nput a mechanism in place to clarify the rules and audit the implementations and results. We think this \nis a natural progression for a benchmark -being pro-posed in its initial form by a small team of people, \nand then being revised, formalized, and administered by a standards body that both vendors and customers \ncan trust. This progression (which produced TPC-A through TPC-C [Gra93]) seems more efficient than the \nalternative of letting a large committee of vendors define a benchmark from scratch (a la the approach \ntaken in TPC-D [TPC94]), as it is difficult for a large committee with diverse goals and mutual fears \nto de-fine something as complex as a database benchmark. 3.4 Freedom of Information Specification and \nauditing issues aside, one overrid-ing problem that a would-be benchmarker must face is that many OODBMSs \n(like almost all relational DBMSs) have clauses in their software licenses that prohibit the release \nof performance results without explicit permission from the OODBMS vendor. That is, for some systems, \nit is actually a violation of the system s license agreement, and therefore a very real potential source \nof a law suit, to purchase an OODBMS, run a benchmark, and publish the results. Moreover, as we discovered, \nthis is not just idle jar-gon in the license agreement. If you do go ahead and benchmark such a system, \nyou will almost surely be hearing from the vendor s lawyers. While it is apparently perfectly legal for \nsoftware licenses to be written in this manner, one has to won-der why some vendors feel the need for \nsuch clauses -and what the impact is on the information systems in-dustry. If other industries followed \nthe DBMS indus-try s lead, Road &#38; Track could not publish its new car road tests, and Consumer Reports \ncould not evaluate dishwashers and stereos. Closer to the computer in-dustry, it would be prohibited \nto publish benchmark ratings for new microprocessors. Clearly, this clause has a very strong stifling \neffect on would-be bench-markers. It certainly made our job much harder; at each step of the way we had \nto negotiate with some of the companies involved to keep them from firing up their lawyers and dropping \nout of the benchmark. (At times it also kept our FAX machine humming with legal correspondence, leading \nus to duck and shout incoming! each time the FAX phone rang in our secretary s office.) Dealing with \nthis issue was very painful, both for us and for our technical colleagues who work for the var-ious OODBMS \nvendors. Having said that, we should also note that in general our contacts in the com-panies involved \nwere helpful and supportive in what was an extremely high-stress situation for everyone involved. 3.5 \nInterpreting the Results Given that one has managed to overcome (or ignore for the moment) the sticky \nissues above, producing a set of numbers, one is then faced with yet another issue: What do the benchmark \nresults mean? This is viewed by some as being a major problem with the 007 Benchmark, and many have urged \nus to make the 007 Benchmark results easier to use by providing a mechanism to condense the test results \ninto a single number for the purpose of ranking systems. We agree that the benchmark report currently \nin-cludes far too many numbers; we will talk in a mo-ment about our plans to condense the 007 Bench-mark \nto a more manageable size. However, we do not anticipate ever reducing the benchmark to a sin-gle number. \nThis is not due to bashfulness or fear of hurting people s feelings; rather, it is a result of (1) our \ndecision to favor a comprehensive benchmark rather than a concise one but narrow one, and (2) our belief \nthat for a knowledgeable customer, a multiple number benchmark is far more useful than any single number \nbenchmark could be. That is, to use the 007 results, we recommend that you determine the main performance \ndemands of your application and then look at the OODBMSs performance on the portion of 007 that most \nclosely matches those demands. To illustrate some of the difficulties with a single number condensation \nof the 007 results, here are three ways that have been suggested (e.g., in trade journal articles covering \n007 or marketing literature from OODBMS companies) for producing a single 007 number. 1. Number of first \nplaces on benchmark tests. In the benchmark report we present 105 timing results (tests) from each system.2 \nTo produce this one-number performance metric, one simply counts the number of times that each system \nhad the fastest time on one of the 105 tests. By this measure, the results as of this writing are: 2This \nis not quite as outrageous as it might seem, as the 007 technical report includes a number of graphs. \nThe total of 105 numbers comes from counting every point on every graph. Rank 1 System 1 Score 1 1 E/Exodus \n1 61  2. Weighted ranking of places. To produce this alternative one-number 007 performance metric, \none gives each system one point for a first place finish on a test, two points for a second place finish, \nthree points for third place, and four points for fourth place. These numbers are then added up for \neach system, so a lower overall number is better. By this measure, the results are: mi  3. Geometric \nmean. Yet another possible approach, which weights each number as being equally important (and therefore \npresumes that all key performance ar-eas have been equally well covered), is to use the geometric mean \nof the results as the one-number metric -taking the 105th root of the product of the 105 individual test \nresults. This is the ap-proach taken by the TPC in their new TPC-D benchmark. By this measure the results \nare: (We couldn t place Ontos in this ranking because as of this writing we do not have numbers for the \nmedium-9 007 database results for Ontos.) While we don t ever want a single-number bench-mark, 007 certainly \nerrs on the side of generat-ing too many numbers. This is because benchmark-ing is a learning process, \nand only through itera-tive refinement can one end up with a minimal yet somehow sufficient set of benchmark \noperations. We plan to fix this problem in 007 by further reduc-ing the set of tests involved in running \nthe bench-mark. This will be done by requiring one or a few numbers for each of a few interesting categories \nof workloads, e.g., hot traversals (how fast can an ap-plication program traverse in-memory data?), cold \nsparse and dense traversals (how fast can an applica-tion program traverse data not yet in memory?), \none or two update traversals, a smaller set of queries, etc. Some of the current 007 parameter variations \n(e.g., the 3/6/9 variation for the atomic parts graph, and the some/all/repeated update variation) have \ncome to show little or no new information as systems have matured and their early performance bugs have \ndis-appeared. These variations can now be eliminated.  3.6 Application Coverage One final issue that \nis becoming more and more wor-thy of discussion is the question of what sort of appli- cation(s) an OODBMS \nbenchmark like 007 should be loosely based on. When we began this work, the answer was quite clear, and \nas a result the current single-user 007 Benchmark has a strong CAD/CAM flavor. This reflects the fact \nthat CAD-type appli-cations were the initial target application for most OODBMSs. However, this is now \nchanging: we are seeing OODB systems applied in diverse application areas ranging from CAD to telecommunications \nto fi-nance, with some estimates putting the CAD portion of the OODBMS market in the 30-40X range and \ndropping. Clearly, emerging OODBMS application domains may have workload characteristics that are not \nwell covered by 007. We are actively seeking feedback about such applications to help us in future revisions \nof the benchmark, especially with respect to their multi-user characteristics. Such information would \nbe very helpful for our current multi-user 007 Benchmark effort, to which we now turn our atten-tion. \n 4 Towards a Multi-User 007 As an initial step towards filling the information gap that the introduction \nmentioned in the area of multi- user OODBMS performance characteristics, we are currently developing \na multi-user 007 OODBMS benchmark. There are five systems currently in-volved in this effort: E/Exodus \n[CDF+86, RCS93], 02 [DeuSl], Objectivity [Obj92], Ontos [Ont92], and Versant [Ver92]. Our experience \nso far is that designing a multi-user benchmark is a much more difficult problem than designing a single-user \nbenchmark. This is par- tially due to the fact that the number of dimen-sions along which the workload \ncan vary is greater in the multi-user case, and partially because multi-user workloads inherently involve \ncomplex interac-tions of multiple concurrent activities. Also, if there is no agreement about what constitutes \nthe canoni-cal single-user OODBMS workload, there is even less agreement in the multi-user arena. For \nthis reason we regard coming up with a monolithic workload that generates a single-number system evaluation \n(a la TPC-A or TPC-B) as nothing short of hopeless. Our response to date has been to develop a fully \npa-rameterized workload that is made up of primitives that can be combined to generate a range of work- \nloads with a wide variety of different characteristics. In this sense the multi-user 007 Benchmark, in \nits current form, is really a customizable benchmark gen- erator that we hope will be useful to sophisticated \nconsumers of OODBMSs. In the remainder of this paper we give a brief description of our ongoing work \non the multi-user database and its workloads. Our hope is that read-ing and hearing about this work will \nlead some of the more informed members of the OODBMS com-munity, like vendors and serious application \ndevelop-ers, to provide us with feedback based on this de-scription. Such feedback would enable us to \nimprove our design, helping us to produce a more relevant and therefore more useful benchmarking tool \nfor the OODBMS community to share. 4.1 The Multi-User 007 Database The multi-user 007 database is a scalable \nextension of the single-user 007 database described in Section 2.1. As mentioned there, scalability for \nmulti-user benchmarking was a goal even in the initial 007 database design. However, the process of designing \nthe current multi-user 007 database led us to make some minor changes to the initial single-user 007 \nBenchmark database, and to extend it slightly, as we did not correctly anticipate certain sharing and \ndata contention issues that quickly became clear when we began running some initial multi-user experiments. \nIn the initial 007 Benchmark database, as dis-cussed in Section 2.1, each base assembly in a given module \nhad associations with two kinds of compos-ite parts -private and shared. Both were references into the \ndesign library, with private composite part references referring only to the composite parts that were \nprivately associated with the given module; the shared composite part references were randomly as-signed \nto point anywhere within the entire design li-brary (i.e., to private parts of any module). The un-fortunate \naspect of this design was that there were no truly private composite parts; e.g., it provided no way \nto have a client transaction read from its private com-posite parts and update shared composite parts \nwith-out interfering with the private reads of other clients. We have therefore abandoned this design, \nremoving the shared composite part associations from the defi-nition of the benchmark s base assembly \nobject type. Instead, for the multi-user benchmark we have added one additional module, the shared module, \nto the database. One last change, alluded to earlier, is that we also eliminated the 3/6/g connection \ncount varia-tion scheme from the single-user benchmark; for the multi-user 007 database, the number of \noutgoing connections per atomic part is simply fixed at three. For the multi-user 007 Benchmark, an important \nissue that we faced was how to scale the database with the workload. As clients are added, should the \ndatabase remain the same, potentially leading to more and more contention? Or should the database grow \nwith each additional client, the idea being that each additional client represents a designer who will \nbe working on an additional piece of a CAD design? We chose the latter approach, which implies trying \nto keep contention more or less constant as the number of clients scales up. This, in turn, requires \ncarefully scaling the database in terms of both its private and shared modules. To scale the benchmark \ndatabase in proportion to the number of clients, we have one private module (as described in Section \n2.1) in the database for each client in both the small and medium databases. In addition, we grow the \nshared module in proportion to the number of clients. Roughly speaking, the shared module, which has \nthe same depth as the private modules in the benchmark database, can be thought of as a mega-module consisting \nof as many sub-modules as there are clients in the database. Each submodule of the shared module therefore \nhas one less level than than a private module, with the sub-modules being hooked together via the root \ncomplex assembly of the shared module. The shared mod-ule brings to the design library its own additional \nset of composite objects; each submodule of the shared module adds NumSharCompPerClient (currently set \nto 200) composite objects to the design library, and these are the composite objects that are referenced \nby the base assemblies of that submodule.  4.2 The Multiuser 007 Workload As mentioned at the beginning \nof this section, our ap-proach to multi-user workload generation has been to define a parameterized workload \nthat can be used to create workloads with a variety of different character-istics. By doing so, we are \nable to thoroughly explore the space of multi-user OODBMS performance. The multi-user 007 workload consists \nof a set of clients, each running a series of (parameterized) traversal beginlransaction; for RepeatCount \ndo if this is a shared transaction start at the root of the assembly hierarchy of the shared module; \nelse start at the root of the assembly hierarchy of module k ; Follow a single random path down the hierarchy \nto a base assembly; From the base assembly, perform some operation on a composite part; Sleep(SleepTime); \nend ; endTransaction; Figure 3: Generic multi-user 007 transaction. transactions that are themselves \nmade up of primi- tive operations. Different clients can be told to run traversal transactions with different \nparameters, thus allowing a wide range of different workloads to be generated (e.g., with varying degrees \nof inter-client data sharing). 4.2.1 Traversal Transactions Each client in the multi-user 007 Benchmark \nhas a distinct client number. Pseudo-code for a generic multi-user 007 traversal transaction for client \nlc is shown in Figure 3. The transaction repeatedly chooses a single path through the assembly hierar-chy, \nperforming some operation on a single composite part that it reaches via the chosen path. In particu-lar, \nif the parameter Repeatcount is set to one, the transaction will visit only one composite part. Each \ntime through the loop, there are two possibil-ities for what each assembly hierarchy traversal can do \nwhen it visits a composite part: 1. Do a read-only depth-first search traversal of the atomic part subgraph \nassociated with that com-posite part. 2. Do a read-write traversal. Specifically, do a depth-first search \nof the associated atomic part subgraph that swaps the X and Y coordinates of each atomic part as it is \nvisited.  We call these operations on composite parts ba-sic operations. Since each of these (read-only \nor read-write) operations can be done beginning with a traversal of either the client s private module \nor the (globally) shared module, there are a total of four possible basic operations. Given this description \nof the basic operations, we can now describe the param-eters of the traversal transactions used to generate \nthe 007 multi-user workload. We consider each pa-rameter below. . Percentage of Each Basic Operation. \nThe percentages of each of the four basic opera-tions is best described by a vector. For example, (100, \n0, 0, 0) specifies a workload in which each transaction contains 100% read-only operations on its private \nmodule. Similarly, (80, 10, 10, 0) specifies a workload in which each transaction contains 80% read-only \noperations on its private module, 10% read-only operations on the shared module, and 10% update operations \non its pri-vate module. In more detail, these percentages are interpreted to be probabilities for each \nop-eration: each time through the loop, when the transaction reaches a composite part, it flips a biased \ncoin (i.e., generates a random number) to decide which kind of basic operation to perform on it. On the \naverage, then, if the RepeatCount parameter of Figure 3 is set to 100, a transaction drawn from the (80, \n10, 10, 0) vector will contain about 80 private read-only operations, 10 shared read-only operations, \nand 10 private update op-erations . . Repeat Count. By varying the RepeatCount parameter, which determines \nhow many basic operations a trans-action contains, it is possible to generate trans-actions of arbitrary \nlength -ranging from short, traditional TP-ish transactions to longer, CAD-like transactions. . Sleep \nTime. The SleepTime parameter controls the inten-sity of the transactions. If this parameter is set to \nzero, the transaction is never idle (unless it is waiting for the OODBMS running the transac-tion); this \nis perhaps suggestive of a CAD pro-gram such as a design rule checker. By specifying a longer sleep time, \none can model a transaction of an interactive session that involves think times between the database \noperations. The runtime arguments to a given benchmarking run specify all three of these parameters \n-the per- centage vector for basic operations, RepeatCount, and SleepTime - in the form of command line \nar- guments .  4.2.2 Multiuser Workload Generation The parameters just described (operation percentage \nvector, operation repeat count, and sleep time) make it possible to generate a wide variety of multi-user \ntransaction workloads. In addition, a final very im-portant parameter of the multi-user 007 Benchmark \nis the number of clients, as mentioned in the preced-ing multi-user benchmark database description. By \nvarying the number of client workstations, and by varying the parameter set given to each client, many \ndifferent workloads can be experimented with. Of particular interest are the various data sharing patterns \nthat can be generated from the framework that the 007 multi-user benchmark parameters pro-vide. One class \nof workload that can be generated are symmetric workloads, where all clients behave simi-larly with respect \nto accessing their private data and the system-wide shared data. Such workloads can be generated by running \nevery client with the same set of input parameters. Potentially interest-ing examples include the private \nread-only workload mentioned above, or the largely private (80, 10, 10, 0) read-mainly workload. Similarly, \nif an operation vector of (0, 0, 100, 0) is given to each client, a pri-vate read-write workload can \nbe generated. In addi-tion, the transaction length parameter allows the dif-ferent sharing patterns to \nbe applied to either short transactions or long transactions, and the sleep time parameter allows both \ncompute-intensive and pause-intensive transactions to be explored. In addition to symmetric workloads, \nit is also pos-sible to use the 007 multi-user benchmark param-eters to generate interesting asymmetric \nworkloads. One such workload might be a producer/consumer workload, where one client generates information \nthat others read (such as stock price quotations). By run-ning a producer client with an operation vector \nof (O,O,O,lOO), one could have it be a provider of shared information. The rest of the clients, the consumers \n, could be run with operation vectors of (0, 100, 0, 0) so that each reads the shared information provided \nby the producer s updates. A somewhat less contentious version of the consumers could be generated by \nrun-ning them instead with an operation vector like (50, 50, 0, 0), causing them to spend only half of \ntheir time reading from the shared module (with the other half being spent on private reads). 4.2.3 \nOther Operations Under Consideration In addition to the family of multi-user workloads that can be generated \nusing the parameterized transac-tions described up to this point, we are currently considering (and experimenting \nwith) several addi-tional workload variations. These additional varia-tions are essentially a first attempt \nto make the 007 Benchmark somewhat more broad than just being a CAD-like benchmark. We briefly mention \nsome of our additional (in progress!) ideas along those lines here. Finer Granularity Traversals Each \ntime one of the aforementioned traversal trans-actions visits a composite part, it traverses the entire \natomic part subgraph that is associated with the com-posite part. Each such subgraph traversal touches \n80 objects in the small database (20 atomic parts plus 60 connection objects) and 800 objects in the \nmedium database (200 atomic parts plus 600 connection ob-jects.) While this seems fine for CAD-like transac-tions, \nhaving transactions access such a large num-ber of objects is too heavy-weight for modeling some varieties \nof transactions. To address this problem, in addition to the traversals described previously, we in-tend \nto add another type of transaction -very sim-ilar in structure to the original multi-user traversal transaction \n-that stops its traversal and performs its basic operations at the base assembly level. Our intent is \nto use this workload to characterize appli-cations that have finer-grained interactions between client \ntransactions. Set Update Operations One of the few items of feedback that we have received regarding \nmulti-user workloads is that concurrent set update (e.g., insert and delete) operations are impor-tant \nfor some types of OODBMS applications. To explore performance issues raised by multi-user set updates, \nwe have added an associated set object to every module (both shared and private). This new set will contain \nprimitive objects that are distinct from the other objects in the benchmark database. We are currently \nexploring these issues by testing the follow-ing operations: 1. Generate. Every client generates some \nnumber of new ob-jects and inserts them into the set of its pri-vate module. While there is no explicit \ncon-tention among the clients, depending upon how the system is implemented, clients are likely to contend \nfor both logical and physical resources (e.g., OIDs and database disk bandwidth) at the server. Migrate. \nEvery client migrates the objects in its private set into the set of the shared module. Here, clients \nare contending for this shared set due to concurrent insert operations. Migrate back. Every client iterates \nthrough the shared set, moving the objects that it put there back to its own private set. (This operation \nhas caused ter-rible data contention in our initial experiments, so it may need to be modified.) Destroy. \nEvery client destroys the objects in its private set and deletes them from the set. Your Favorite Operation \nGoes Here! As we have mentioned already, we are very interested in hearing from OODBMS vendors and customers \nre-garding the nature of their multi-user workloads. If we are successful at soliciting such information, \nper-haps in the form of critical comments on the prelimi-nary multi-user 007 Benchmark design that we \nhave described, we will likely extend (and/or modify) our design further in the future.   5 Conclusion \nThis paper has reported on the current status of the 007 OODBMS benchmarking effort at the Univer-sity \nof Wisconsin. The first half of the paper was de-voted to a review and retrospective evaluation of the \nsingle-user 007 Benchmark. We discussed a number of issues that arose concerning the design and admin-istration \nof the benchmark, explaining what we did, why we did it, and what we might do differently the next time \naround. We then described our in-progress work on multi-user OODBMS benchmarking, detail-ing a set of \nproposed changes to the 007 database and an associated multi-user workload family for eval- uating OODBMS \nperformance. A major goal of the latter part of the paper is to publically solicit feed-back from vendors \nand users of commercial OODBMS technology regarding, in their opinions, what we re doing right and what \nwe might be missing in our proposed multi-user benchmark. We are now in the process of running a variety \nof multi-user workloads against the multi-user 007 database; our parameter-ized workload approach has \nalready paid off, allowing us to explore the OODBMS performance space on five systems without a continual \nmodify/compile/debug cycle between every data point. We are hoping that, through extensive experimen-tation \nand feedback from the OODBMS community, we will be able to develop a set of workloads that strikes a \nreasonable balance between simplicity and completeness. However, all of the benchmark imple-mentations \nwill be configurable by varying the run-time parameters of the benchmark. Moreover, as our work progresses, \nwe intend to make our benchmark implementations freely available SO that others can independently and \nefficiently conduct their own tests on workloads that interest them. We expect this to be quite useful, \nas the results from the different data points we have explored so far in the multi-user workload space \nindicate that the benchmark will in-deed find significant variations in the multi-user per-formance characteristics \nof the systems being tested. (We intend to publish a full set of results once we are satisfied that we \nhave a solid multi-user benchmark design.) References [AndSO] T. Anderson et al. The HyperModel Benchmark. \nIn Proceedings of the EDBT Conference, Venice, Italy, March 1990. [Cat941 R. Cattell. The Object Database \nStandard: ODMG-93 (Release 1.1). Morgan Kauf-mann, San Mateo, CA, 1994. [CDF+86] Michael J. Carey, David \nJ. Dewitt, Daniel Frank, Goetz Graefe, M. Muralikr- ishna, Joel E. Richardson, and Eugene J. Shekita. \nThe architecture of the EXODUS Extensible DBMS. In Proceedings of the Twelfth International Conference \non Very Large Data Bases, pages 52-65, 1986. [CDN93] Michael J. Carey, David J. Dewitt, and Jeffrey \nF. Naughton. The 007 bench-mark. In Proceedings of the 1993 ACM- SIGMOD Conference on the Management \nof Data, Washington D.C., May 1993. [CS92] R. Cattell and J. Skeen. Object opera-tions benchmark. ACM \nTransactions on Database Systems, 17(l), March 1992. [DD88] J. Duhl and C. Damon. A perfor-mance comparison \nof object and relational [DeuSl] [Gra93] [Obj92] [Ont92] [RCS93] [RKC87] [TPC94] [Ver92] databases using \nthe sun benchmark. In Proceedings of the ACM OOPSLA Con-ference, San Diego, California, September 1988. \n0. Deux et al. The 02 system. Communi-cations of the ACM, 34( lo), October 1991. Jim Gray. The Benchmark \nHandbook. Morgan Kaufmann, San Mateo, CA, 1993. Objectivity, Inc. Objectivity reference manual. 1992. \nOntos, Inc. Ontos reference manual. 1992. Joel E. Richardson, Michael J. Carey, and Daniel T. Schuh. \nThe design of the E pro- gramming language. ACM Transactions on Programming Languages and Systems, 15(3), \nJuly 1993. W. Rubenstein, M. Kubicar, and R. Cat-tell. Benchmarking simple database oper-ations. In Proceedings \nof the ACM SIG-MOD Conference, San Francisco, Califor-nia, May 1987. TPC. TPC BenchmarkTM D (Decision \nSupport). Working draft 6.5, Transaction Processing Performance Council, Febru- ary 1994. Versant, Inc. \nVersant reference manual. 1992.  \n\t\t\t", "proc_id": "191080", "abstract": "<p>The OO7 Benchmark was first published in 1993, and has since found a home in the marketing literature of various object-oriented database management system (OODBMS) vendors. The OO7 Benchmark (as published) was the initial result of an ongoing OODBMS performance evaluation effort at the University of Wisconsin. This paper provides an update on the status of the effort on two fronts: single-user and multi-user. On the single-user front, we review and critique the design of the initial OO7 Benchmark. We discuss some of its faults, the reasons for those faults, and things that might be done to correct them. On the multi-user front, we describe our current work on the development of a multi-user benchmark for OODBMSs. This effort includes changes and extensions to the OO7 database and  the design of a family of interesting multi-user workloads.</p>", "authors": [{"name": "Michael J. Carey", "author_profile_id": "81100229036", "affiliation": "Computer Sciences Department, University of Wisconsin-Madison", "person_id": "PP14089385", "email_address": "", "orcid_id": ""}, {"name": "David J. DeWitt", "author_profile_id": "81100113216", "affiliation": "Computer Sciences Department, University of Wisconsin-Madison", "person_id": "PP15022460", "email_address": "", "orcid_id": ""}, {"name": "Chander Kant", "author_profile_id": "81100057149", "affiliation": "Computer Sciences Department, University of Wisconsin-Madison", "person_id": "P42704", "email_address": "", "orcid_id": ""}, {"name": "Jeffrey F. Naughton", "author_profile_id": "81100509752", "affiliation": "Computer Sciences Department, University of Wisconsin-Madison", "person_id": "P137881", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/191080.191147", "year": "1994", "article_id": "191147", "conference": "OOPSLA", "title": "A status report on the OO7 OODBMS benchmarking effort", "url": "http://dl.acm.org/citation.cfm?id=191147"}