{"article_publication_date": "01-07-2008", "fulltext": "\n Formal Veri.cation of Translation Validators A Case Study on Instruction Scheduling Optimizations Jean-Baptiste \nTristan INRIA Paris-Rocquencourt jean-baptiste.tristan@inria.fr Abstract Translation validation consists \nof transforming a program and a posteriori validating it in order to detect a modi.cation of its se\u00admantics. \nThis approach can be used in a veri.ed compiler, pro\u00advided that validation is formally proved to be correct. \nWe present two such validators and their Coq proofs of correctness. The val\u00adidators are designed for \ntwo instruction scheduling optimizations: list scheduling and trace scheduling. Categories and Subject \nDescriptors F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Pro\u00adgrams \n-Mechanical veri.cation; F.3.2 [Logics and Meanings of Programs]: Semantics of Programming Languages \n-Operational semantics; D.2.4 [Software Engineering]: Software/Program Ver\u00adi.cation -Correctness proofs; \nD.3.4 [Programming Languages]: Processors -Optimization General Terms Languages, Veri.cation, Algorithms \nKeywords Translation validation, scheduling optimizations, veri\u00ad.ed compilers, the Coq proof assistant \n1. Introduction Compilers, and especially optimizing compilers, are complex pieces of software that perform \ndelicate code transformations and static analyses over the programs that they compile. Despite heavy \ntesting, bugs in compilers (either in the algorithms used or in their concrete implementation) do happen \nand can cause incorrect object code to be generated from correct source programs. Such bugs are particularly \ndif.cult to track down because they are often misdiagnosed as errors in the source programs. Moreover, \nin the case of high-assurance software, compiler bugs can potentially invalidate the guarantees established \nby applying formal methods to the source code. Translation validation, as introduced by Pnueli et al. \n(1998b), is a way to detect such compiler bugs at compile-time, therefore preventing incorrect code from \nbeing generated by the compiler silently. In this approach, at every run of the compiler or of one of \nthe compiler passes, the input code and the generated code are fed to a validator (a piece of software \ndistinct from the compiler itself), which tries to establish a posteriori that the generated code behaves \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL \n08, January 7 12, 2008, San Francisco, California, USA. Copyright c &#38;#169; 2008 ACM 978-1-59593-689-9/08/0001. \n. . $5.00 Xavier Leroy INRIA Paris-Rocquencourt xavier.leroy@inria.fr as prescribed by the input code. \nThe validator can use a variety of techniques to do so, ranging from data.ow analyses (Huang et al. 2006) \nto symbolic execution (Necula 2000; Rival 2004) to the generation of a veri.cation condition followed \nby model checking or automatic theorem proving (Pnueli et al. 1998b; Zuck et al. 2003). If the validator \nsucceeds, compilation proceeds normally. If, however, the validator detects a discrepancy, or is unable \nto establish the desired semantic equivalence, compilation is aborted. Since the validator can be developed \nindependently from the compiler, and generally uses very different algorithms than those of the compiler, \ntranslation validation signi.cantly increases the user s con.dence in the compilation process. However, \nas unlikely as it may sound, it is possible that a compiler bug still goes un\u00adnoticed because of a matching \nbug in the validator. More prag\u00admatically, translation validators, just like type checkers and byte\u00adcode \nveri.ers, are dif.cult to test: while examples of correct code that should pass abound, building a comprehensive \nsuite of incor\u00adrect code that should be rejected is delicate (Sirer and Bershad 1999). The guarantees \nobtained by translation validation are there\u00adfore weaker than those obtained by formal compiler veri.cation: \nthe approach where program proof techniques are applied to the compiler itself in order to prove, once \nand for all, that the generated code is semantically equivalent to the source code. (For background on \ncompiler veri.cation, see the survey by Dave (2003) and the recent mechanized veri.cations of compilers \ndescribed by Klein and Nipkow (2006), Leroy (2006), Leinenbach et al. (2005) and Strecker (2005).) A \ncrucial observation that drives the work presented in this pa\u00adper is that translation validation can \nprovide formal correctness guarantees as strong as those obtained by compiler veri.cation, provided the \nvalidator itself is formally veri.ed. In other words, it suf.ces to model the validator as a function \nV : Source \u00d7 Target . boolean and prove that V (S, T )= true implies the desired semantic equivalence \nresult between the source code S and the compiled code T . The compiler or compiler pass itself does \nnot need to be proved correct and can use algorithms, heuristics and implementation techniques that do \nnot easily lend themselves to program proof. We claim that for many optimization passes, the approach \noutlined above translation validation a posteriori combined with formal veri.cation of the validator \n can be sig\u00adni.cantly less involved than formal veri.cation of the compilation pass, yet provide the \nsame level of assurance. In this paper, we investigate the usability of the veri.ed val\u00adidator approach \nin the case of two optimizations that schedule instructions to improve instruction-level parallelism: \nlist schedul\u00ading and trace scheduling. We develop simple validation algorithms for these optimizations, \nbased on symbolic execution of the orig\u00adinal and transformed codes at the level of basic blocks (for \nlist scheduling) and extended basic blocks after tail duplication (for trace scheduling). We then prove \nthe correctness of these validators against an operational semantics. The formalizations and proofs of \ncorrectness are entirely mechanized using the Coq proof assistant (Coq development team 1989 2007; Bertot \nand Cast\u00b4eran 2004). The formally veri.ed instruction scheduling optimizations thus ob\u00adtained integrate \nsmoothly within the Compcert veri.ed compiler described in (Leroy 2006; Leroy et al. 2003 2007). The \nremainder of this paper is organized as follows. Section 2 recalls basic notions about symbolic evaluation \nand its uses for translation validation. Section 3 presents the Mach intermediate language over which \nscheduling and validation are performed. Sec\u00adtions 4 and 5 present the validators for list scheduling \nand trace scheduling, respectively, along with their proofs of correctness. Section 6 discusses our Coq \nmechanization of these results. Sec\u00adtion 7 presents some experimental data and discusses algorithmic \nef.ciency issues. Related work is discussed in section 8, followed by concluding remarks in section 9. \n2. Translation validation by symbolic execution 2.1 Translation validation and compiler veri.cation We \nmodel a compiler or compiler pass as a function L1 . L2 + Error, where the Error result denotes a compile-time \nfailure, L1 is the source language and L2 is the target language for this pass. (In the case of instruction \nscheduling, L1 and L2 will be the same intermediate language, Mach, described in section 3.) Let = be \na relation between a program c1 . L1 and a program c2 . L2 that de.nes the desired semantic preservation \nproperty for the compiler pass. In this paper, we say that c1 = c2 if, when\u00adever c1 has well-de.ned semantics \nand terminates with observable result R, c2 also has well-de.ned semantics, also terminates, and produces \nthe same observable result R. We say that a compiler C : L1 . L2 + Error is formally veri.ed if we have \nproved that .c1 . L1,c2 . L2,C(c1)= c2 . c1 = c2 (1) In the translation validation approach, the compiler \npass is com\u00adplemented by a validator: a function L1 \u00d7 L2 . boolean. A val\u00adidator V is formally veri.ed \nif we have proved that .c1 . L1,c2 . L2,V (c1,c2)= true . c1 = c2 (2) Let C be a compiler and V a validator. \nThe following function CV de.nes a compiler from L1 to L2: CV (c1)= c2 if C(c1)= c2 and V (c1,c2)= true \nCV (c1)= Error if C(c1)= c2 and V (c1,c2)= false CV (c1)= Error if C(c1)= Error The line of work presented \nin this paper follows from the trivial theorem below. Theorem 1. If the validator V is formally veri.ed \nin the sense of (2), then the compiler CV is formally veri.ed in the sense of (1). In other terms, the \nveri.cation effort for the derived compiler CV reduces to the veri.cation of the validator V . The original \ncompiler C itself does not need to be veri.ed and can be treated as a black box. This fact has several \npractical bene.ts. First, programs that we need to verify formally must be written in a programming language \nthat is conducive to program proof. In the Compcert project, we used the functional subset of the speci.cation \nlanguage of the Coq theorem prover as our programming language. This makes it very easy to reason over \nprograms, but severely constrains our programming style: program written in Coq must be purely functional \n(no imperative features) and be proved to terminate. In our veri.ed validator approach, only the validator \nV is written in Coq. The compiler C can be written in any programming language, using updateable data \nstructures and other imperative features if necessary. There is also no need to ensure that C terminates. \nA second bene.t of translation validation is that the base compiler C can use heuristics or probabilistic \nalgorithms that are known to generate correct code with high probability, but not always. The rare instances \nwhere C generates wrong code will be caught by the validator. Finally, the same validator V can be used \nfor several optimizations or variants of the same optimization. The effort of formally verifying V can \ntherefore be amortized over several optimizations. Given two programs c1 and c2, it is in general undecidable \nwhether c1 = c2. Therefore, the validator is in general incom\u00adplete: the reverse implication . in de.nition \n(2) does not hold, potentially causing false alarms (a correct code transformation is rejected at validation \ntime). However, we can take advantage of our knowledge of the class of transformations performed by the \ncompiler pass C to develop a specially-adapted validator V that is complete for these transformations. \nFor instance, the validator of Huang et al. (2006) is claimed to be complete for register allocation \nand spilling. Likewise, the validators we present in this paper are specialized to the code transformations \nperformed by list schedul\u00ading and trace scheduling, namely reordering of instructions within a basic \nblock or an extended basic block, respectively.  2.2 Symbolic execution Following Necula (2000), we \nuse symbolic execution as our main tool to show semantic equivalence between code fragments. Sym\u00adbolic \nexecution of a basic block represents the values of variables at the end of the block as symbolic expressions \ninvolving the val\u00adues of the variables at the beginning of the block. For instance, the symbolic execution \nof z := x + y; t := z \u00d7 y is the following mapping of variables to expressions z . x 0 + y 0 t . (x 0 \n+ y 0) \u00d7 y 0 v . v 0 for all other variables v where v 0 symbolically denotes the initial value of variable \nv at the beginning of the block. Symbolic execution extends to memory operations if we con\u00adsider that \nthey operate over an implicit argument and result, Mem, representing the current memory state. For instance, \nthe symbolic execution of store(x, 12); y := load(x) is Mem . store(Mem0 , x 0 , 12) 00 0 y . load(store(Mem, \nx , 12), x ) v . v 0 for all other variables v The crucial observation is that two basic blocks that \nhave the same symbolic evaluation (identical variables are mapped to iden\u00adtical symbolic expressions) \nare semantically equivalent, in the fol\u00adlowing sense: if both blocks successfully execute from an ini\u00adtial \nstate S, leading to .nal states S1 and S2 respectively, then S1 =S2. Necula (2000) goes further and compares \nthe symbolic evalua\u00adtions of the two code fragments modulo equations such as compu\u00adtation of arithmetic \noperations (e.g. 1+2 = 3), algebraic prop\u00aderties of these operations (e.g. x + y = y + x or x \u00d7 4= x \n<< 2), and good variable properties for memory accesses (e.g. load(store(m, p, v),p)= v). This is necessary \nto validate trans\u00adformations such as constant propagation or instruction strength re\u00adduction. However, \nfor the instruction scheduling optimizations that we consider here, equations are not necessary and it \nsuf.ces to compare symbolic expressions by structure. The semantic equivalence result that we obtain \nbetween blocks having identical symbolic evaluations is too weak for our purposes: it does not guarantee \nthat the transformed block executes without run-time errors whenever the original block does. Consider: \nx := 1x := x / 0 x := 1 Both blocks have the same symbolic evaluation, namely x . 1 and v . v 0 if v= \nx. However, the rightmost block crashes at run\u00adtime on a division by 0, and is therefore not a valid \noptimization of the leftmost block, which does not crash. To address this issue, we enrich symbolic evaluation \nas follows: in addition to comput\u00ading a mapping from variables to expressions representing the .nal state, \nwe also maintain a set of all arithmetic operations and mem\u00adory accesses performed within the block, \nrepresented along with their arguments as expressions. Such expressions, meaning this computation is \nwell de.ned , are called constraints by lack of a better term. In the example above, the set of constraints \nis empty for the leftmost code, and equal to {x 0/0} for the rightmost code. To validate the transformation \nof a block b1 into a block b2, we now do the following: perform symbolic evaluation over b1, obtaining \na mapping m1 and a set of constraints s1; do the same for b2, obtaining m2,s2; check that m2 = m1 and \ns2 . s1. This will guarantee that b2 executes successfully whenever b1 does, and moreover the .nal states \nwill be identical. 3. The Mach intermediate language The language that we use to implement our scheduling \ntransforma\u00adtions is the Mach intermediate language outlined in (Leroy 2006). This is the lowest-level \nintermediate language in the Compcert compilation chain, just before generation of PowerPC assembly code. \nAt the Mach level, registers have been allocated, stack loca\u00adtions reserved for spilled temporaries, \nand most Mach instructions correspond exactly to single PowerPC instructions. This enables the scheduler \nto perform precise scheduling. On the other hand, the semantics of Mach is higher-level than that of \na machine lan\u00adguage, guaranteeing in particular that terminating function calls are guaranteed to return \nto the instruction following the call; this keeps proofs of semantic preservation more manageable than \nif they were conducted over PowerPC assembly code. 3.1 Syntax A Mach program is composed of a set of \nfunctions whose bodies are lists of instructions. Mach instructions: i ::= setstack(r, t, d) register \nto stack move | getstack(t, d, r) stack to register move | getparam(t, d, r) caller s stack to reg. move \n| op(op, rr, r) arithmetic operation | load(chunk, mode, rr, r) memory load | store(chunk, mode, rr, \nr) memory store | call(r | id) function call | label(l) branch target label | goto(l) unconditional branch \n| cond(cond, rr, ltrue) conditional branch | return function return Mach functions: f ::= fun id { stack \nn1; frame n2; code ri } Mach offers an assembly-level view of control .ow, using labels and branches \nto labels. In conditional branches cond, cond is the condition being tested and rr its arguments. Instructions \nare similar to those of the processor and include arithmetic and logical operations op, as well as memory \nload and stores; arguments and results of these instructions are processor registers r. op ranges over \nthe set of operations of the processor and mode over the set of addressing modes. In memory accesses, \nchunk indicates the kind, size and signedness of the memory datum being accessed. To access locations \nin activation records where temporaries are spilled and callee-save registers are saved, Mach offers \nspe\u00adci.c getstack and setstack instructions, distinct from load and store. The location in the activation \nrecord is identi.ed by a type t and a byte offset d. getparam reads from the activation record of the \ncalling function, where excess parameters to the call are stored. These special instructions enable the \nMach semantics to enforce useful separation properties between activation records and the rest of memory. \n 3.2 Dynamic semantics The operational semantics of Mach is given in a combination of small-step and \nbig-step styles, as three mutually inductive predi\u00adcates: S f ri, R, F, M . ri ',R',F ',M' one instruction \n* S f ri, R, F, M . ri ',R',F ',M' several instructions G f f , P, R, M . R',M' function call The .rst \npredicate de.nes one transition within the current function corresponding to the execution of the .rst \ninstruction in the list ri. R maps registers to values, F maps activation record locations to values, \nand M is the current memory state. The context S= (G, f , sp, P ) is a quadruple of the set G of global \nfunction and variable de.nitions, f the function being executed, sp the stack pointer, and P the activation \nrecord of the caller. The following excerpts should give the .avor of the semantics. v = eval op(op, \nR(rr)) S f op(op, rr, rd) :: c, R, F, M . c, R{rd . v}, F, M true = eval condition(cond,R(rr)) ' c= find \nlabel(ltrue , f .code) ' (G, f , sp, P ) f cond(cond, rr, ltrue ) :: c, R, F, M . c,R,F,M When a call \ninstruction is executed, the semantics transitions not to the .rst instruction in the callee, but to \nthe instruction fol\u00adlowing the call in the caller. This transition takes as premise an execution G f \nf , P, R, M . R',M' of the body of the called function, as shown by the following rules. G(R(rf )) = \nf G f f , F, R, M . R',M' (G, f , sp, P ) f call(rf ) :: c, R, F, M . c, R', F, M' alloc(M, 0, f .stack)=(sp, \nM1) init frame(f .frame)= F1 * ' G, f , sp, P f f .code, R, F1,M1 . return :: c,R',F2,M2 M' = free(M2, \nsp) G f f , P, R, M . R',M' 4. Validation of list scheduling List scheduling is the simplest instruction \nscheduling optimization. Like all optimizations of its kind, it reorders instructions in the pro\u00adgram \nto increase instruction-level parallelism, by taking advantage of pipelining and multiple functional \nunits. In order to preserve pro\u00adgram semantics, reorderings of instructions must respect the fol\u00adlowing \nrules, where . is a resource of the processor (e.g. a register, or the memory): Write-After-Read: a \nread from . must not be moved after a write to .;  Read-After-Write: a write to . must not be moved \nafter a read from .;  Write-After-Write: a write to . must not be moved after another write to ..  \nWe do not detail the implementation of list scheduling, which can be found in compiler textbooks (Appel \n1998; Muchnick 1997). One important feature of this transformation is that it is performed at the level \nof basic blocks: instructions are reordered within basic blocks, but never moved across branch instructions \nnor across labels. Therefore, the control-.ow graph of the original and scheduled codes are isomorphic, \nand translation validation for list scheduling can be performed by comparing matching blocks in the original \nand scheduled codes. In the remainder of the paper we will use the term block to denote the longest sequence \nof non-branching instructions between two branching instructions. The branching instructions in Mach \nare label, goto, cond and call. This is a change from the common view where a block includes its terminating \nbranching instruction. 4.1 Symbolic expressions As outlined in section 2.2, we will use symbolic execution \nto check that the scheduling of a Mach block preserves its semantics. The syntax of symbolic expressions \nthat we use is as follows: Resources: . ::= r | Mem | Frame Value expressions: t ::= r 0 initial value \nof register r | Getstack(t, d, tf ) | Getparam(t,d) | Op(op,rt) | Load(chunk, mode,t, tm) r Memory expressions: \ntm ::= Mem0 initial memory store | Store(chunk, mode,rt, tm,t) Frame expressions: tf ::= Frame0 initial \nframe | Setstack(t, t, d, tf ) Symbolic code: m ::= . . (t | tm | tf ) Constraints: s ::= {t, tm,tf ,...} \nThe resources we track are the processor registers (tracked indi\u00advidually), the memory state (tracked \nas a whole), and the frame for the current function (the part of its activation record that is treated \nas separate from the memory by the Mach semantics). The sym\u00adbolic code m obtained by symbolic evaluation \nis represented as a map from resources to symbolic expressions t, tf and tm of the appropriate kind. \nAdditionally, as explained in section 2.2, we also collect a set s of symbolic expressions that have \nwell-de.ned se\u00admantics. We now give a denotational semantics to symbolic codes, as transformers over \nconcrete states (R, F, M). We de.ne inductively the following four predicates: S f [ t] (R, F, M)= v \nValue expressions S f [ tf ] (R, F, M)= F ' Frame expressions S f [ tm] (R, F, M)= M ' Memory expressions \nS f [ m] (R, F, M)=(R ' ,F ' ,M ' ) Symbolic codes The de.nition of these predicates is straightforward. \nWe show two selected rules. v = eval op(op, rv)S f [ rt] (R, F, M)= rv S f [ op(op,rt)]](R, F, M)= v \n.r, S f [ m(r)]](R, F, M)= R ' (r) S f [ m(Frame)]](R, F, M)= F ' S f [ m(Mem)]](R, F, M)= M ' S f [ \nm] (R, F, M)=(R ' ,F ' ,M ' ) For constraints, we say that a symbolic expression t viewed as the constraint \nt has well-de.ned semantics is satis.ed in a concrete state (R, F, M), and we write S, (R, F, M) |= t, \nif there exists a value v such that S f [ t] (R, F, M)= v, and similarly for symbolic expressions tf \nand tm over frames and memory. For a set of constraints s, we write S, (R, F, M) |= s if every constraint \nin s is satis.ed in state (R, F, M).  4.2 Algorithm for symbolic evaluation We now give the algorithm \nthat, given a list ri of non\u00adbranching Mach instructions, computes its symbolic evaluation a(ri)=(m, \ns). We .rst de.ne the symbolic evaluation a(i, (m, s)) of one instruction i as a transformer from the \npair (m, s) of symbolic code and constraint before the execution of i to the pair (m ' ,s ' ) after the \nexecution of i. update(., t, (m, s)) =(m{. . t},s .{t}) a(setstack(r, t, d), (m, s)) = update(Frame, \nSetstack(m(r), t, d, m(Frame)), (m, s)) a(getstack(t, d, r), (m, s)) = update(r, Getstack(t, d, m(Frame)), \n(m, s)) a(getparam(t, d, r), (m, s)) = update(r, Getparam(t, d), (m, s)) a(op(op, r , r), (m, s)) = update(r, \nOp(op, m(r )), (m, s)) a(load(chunk, mode, r , r), (m, s)) = update(r, Load(chunk, mode,m(r ),m(Mem)), \n(m, s)) a(store(chunk, mode, r , r), (m, s)) = update(Mem, Store(chunk, mode,m(r ),m(Mem),m(r)), (m, \ns)) We then de.ne the symbolic evaluation of the block b = i1; ... ; in by iterating the one-instruction \nsymbolic evaluation function a, starting with the initial symbolic code e =(. . .0) and the empty set \nof constraints. a(b)= a(in,...a(i2,a(i1, (e, \u00d8))) ...) Note that all operations performed by the block \nare recorded in the constraint set s. It is possible to omit operations that cannot fail at run-time \n(such as load constant operators) from s; we elected not to do so for simplicity. The symbolic evaluation \nalgorithm has the following two prop\u00aderties that are used later in the proof of correctness for the validator. \nFirst, any concrete execution of a block b satis.es its symbolic ex\u00adecution a(b), in the following sense. \nLemma 1. Let b be a block and c an instruction list starting with * a branching instruction. If S f (b; \nc), R, F, M . c, R ' ,F ' ,M ' and a(b)=(m, s), then S f [ m] (R, F, M)=(R ' ,F ' ,M ' ) and S, (R, F, \nM) |= s. Second, if an initial state R, F, M satis.es the constraint part of a(b), it is possible to \nexecute b to completion from this initial state. Lemma 2. Let b be a block and c an instruction list \nstarting with a branching instruction. Let a(b)=(m, s). If S, (R, F, M) |= s, * then there exists R ' \n,F ' ,M ' such that S f (b; c), R, F, M . c, R ' ,F ' ,M ' .  4.3 Validation at the level of blocks \nBased on the symbolic evaluation algorithm above, we now de.ne a validator for transformations over blocks. \nThis is a function Vb taking two blocks (lists of non-branching instructions) b1,b2 and returning true \nif b2 is a correct scheduling of b1. Vb(b1,b2)= let (m1,s1)= a(b1) let (m2,s2)= a(b2) return m2 = m1 \n. s2 . s1 The correctness of this validator follows from the properties of symbolic evaluation. Lemma \n3. Let b1,b2 be two blocks and c1,c2 two instruction sequences starting with branching instructions. \nIf Vb(b1,b2)= * true and S f (b1; c1), R, F, M . c1,R ' ,F ' ,M ', then S f * (b2; c2), R, F, M . c2,R \n' ,F ' ,M ' . Proof. Let (m1,s1)= a(b1) and (m2,s2)= a(b2). By hypoth\u00adesis Vb(b1,b2)= true, we have m2 \n= m1 and s2 . s1. By * Lemma 1, the hypothesis S f (b1; c1), R, F, M . c1,R ' ,F ' ,M ' implies that \nS, (R, F, M) |= s1. Since s2 . s1, it follows that S, (R, F, M) |= s2. Therefore, by Lemma 2, there exists \n* R '''' '' '' '' ,F ,M such that S f (b2; c2), R, F, M . c2,R '' ,F ,M . Applying Lemma 1 to the evaluations \nof (b1; c1) and (b2; c2), we obtain that S f [ m1] (R, F, M)=(R ' ,F ' ,M ' ) and (R '''' '' S f [ m2] \n(R, F, M)= ,F ,M ). Since m2 = m1 and the denotation of a symbolic code is unique if it exists, it follows \nthat (R '''''' '' ,F ,M )=(R ' ,F ,M ). The expected result follows.  4.4 Validation at the level of \nfunction bodies Given two lists of instructions c1 and c2 corresponding to the body of a function before \nand after instruction scheduling, the following validator V checks that Vb(b1,b2)= true for each pair \nof matching blocks b1,b2, and that matching branching instructions are equal. (We require, without signi.cant \nloss of generality, that the external implementation of list scheduling preserves the order of basic \nblocks within the function code.) V (c1,c2)= if c1 and c2 are empty: return true if c1 and c2 start with \na branching instruction: decompose c1 as i1 :: c1 ' decompose c2 as i2 :: c2 ' return i1 = i2 . V (c1' \n,c2' ) if c1 and c2 start with a non-branching instruction: decompose c1 as b1; c1 ' decompose c2 as \nb2; c2 ' (where b1,b2 are maximal blocks) return Vb(b1,b2) . V (c1' ,c2' ) otherwise: return false To \nprove that V (c1,c2)= true implies a semantic preserva\u00adtion result between c1 and c2, the natural approach \nis to reason by induction on an execution derivation for c1. However, such an induction decomposes the \nexecution of c1 into executions of in\u00addividual instructions; this is a poor match for the structure of \nthe validation function V , which decomposes c1 into maximal blocks joined by branching instructions. \nTo bridge this gap, we de.ne an alternate, block-oriented operational semantics for Mach that de\u00adscribes \nexecutions as sequences of sub-executions of blocks and of branching instructions. Writing S for global \ncontexts and S, S ' for quadruples (c, R, F, M), the block-oriented semantics re.nes the * S f S . S \n' , S f S . S ' and G f f , P, R, M . R ' ,M ' predicates of the original semantics into the following \n5 predicates: S f S .nb S ' one non-branching instruction * S f S .nb S ' several non-branching instructions \nS f S .b S ' one branching instruction S f S S ' block-branch-block sequences G f f , P, R, M .blocks \nR ' ,M ' * The fourth predicate, written , represents sequences of .nb transitions separated by .b transitions: \n* S f S .nb S ' S f S S ' * S f S .nb S1 S f S1 .b S2 S f S2 S ' S f S S ' It is easy to show that the \nblock-oriented semantics is equiva\u00ad * lent to the original . semantics for executions of whole functions. \nR '' Lemma 4. G f f , P, R, M . ,M if and only if G f f , P, R, M .blocks R ' ,M ' . We are now in a \nposition to state and prove the correctness of the validator V . Let p be a program and p ' the corresponding \nprogram after list scheduling and validation: p ' is identical to p except for function bodies, and V \n(p(id).code,p ' (id).code)= true for all function names id . p. Theorem 2. Let G and G ' be the global \nenvironments associated with p and p ', respectively. If G f f , P, R, M . R ' ,M ' and ' '' V (f .code, \nf .code)= true, then G ' f f , P, R, M . R ' ,M . Proof. We show the analogous result using .blocks instead \nof . in the premise and conclusion by induction over the evaluation derivation, using Lemma 3 to deal \nwith execution of blocks. We conclude by Lemma 4. 5. Validation of trace scheduling Trace scheduling \n(Ellis 1986) is a generalization of list scheduling where instructions are allowed to move past a branch \nor before a join point, as long as this branch or joint point does not correspond to a back-edge. In \nthis work we restrict the instructions that can be moved to non-branching instructions, thus considering \na slightly weaker version of trace scheduling than the classical one. Moving instructions to different \nbasic blocks requires compen\u00adsating code to be inserted in the control-.ow graph, as depicted in .gure \n1. Consider an instruction i that is moved after a conditional instruction targeting a label l in case \nthe condition is true (left). Then, in order to preserve the semantics, we must ensure that if the condition \nis true during execution the instruction i is executed. We insert a stub , i.e. we hijack the control \nby making the conditional point to a new label l ' where the instruction i is executed before going back \nto the label l.  Figure 1. The two extra rules of trace scheduling. On the left, an example of move \nafter a condition. On the right, an example of move before a join point. On each example the code is \nshown before and after hijacking. Dually, consider an instruction i that is moved before a label l targeted \nby some instruction goto (l) (right part of .gure 1). To ensure semantics preservation, we must hijack \nthe control of the goto into a new stub that contains the instruction i. This way, i is executed even \nif we enter the trace by following the goto. In list scheduling, the extent of code modi.cations was \nlimited: an instruction can only move within the basic block that contains it. The unit of modi.cation \nwas therefore the block, i.e. the longest sequences of non-branching instructions between branches. Dur\u00ading \nvalidation, the branches can then be used as synchronization points at which we check that the semantics \nare preserved. What are the synchronization points for trace scheduling? The only in\u00adstructions that \nlimit code movement are the return instructions and the target of back-edges, i.e. in our setting, a \nsubset of the labels. We also .x the convention that call instructions cannot be crossed. Those instructions \nare our synchronization points. In conclusion, the unit of modi.cation for trace scheduling is the longest \nsequence of instructions between these synchronization points. As in the case of list scheduling, we \nwould like to build the validator in two steps: .rst, build a function that validates pairs of traces \nthat are expected to match; then, extend it to a validator for whole function bodies. The problem is \nthat a trace can con\u00adtain branching instructions. Our previous block validator does not handle this. \nMoreover, we must ensure that control .ows the same way in the two programs, which was obvious for the \nblock valida\u00adtor since states were equivalent before branching instructions, but is no longer true for \ntrace scheduling because of the insertion of compensating code along some control edges. A solution to \nthese problems is to consider another represen\u00adtation of the program where traces can be manipulated \nas easily as blocks were in the list of instructions representation. This represen\u00adtation is a graph \nof trees, each tree being a compact representation of all the traces eligible for scheduling that begin \nat cut points in the control-.ow graph. The cut points of interest, in our setting, are code as trace \nscheduling lists of (not veri.ed) instr. (section 5.4) code as validation Vg graphs (section 5.3) of \ntrees function entry points, calls, returns, and the labels that are targets of back-edges. The important \nproperty of these trees is that if an instruction has been moved then it must be within the boundaries \nof a tree. The validator for trace scheduling is built using this program representation. To complete \nthe validator we must transform our program given as a list of instructions into a semantically equiv\u00adalent \ncontrol-.ow graph of trees. The transformation to this new representation also requires some code annotation. \nThis leads to the architecture depicted in .gure 2 that we will detail in the remainder of this section. \nNote that the transformation from lists of instruc\u00adtions to graphs of trees needs to be proved semantics-preserving \nin both directions: if the list c is transformed to graph g, it must be the case that g executes from \nstate S to state S ' if and only if c executes from S to S ' . 5.1 A tree-based representation of control \nand its semantics Figure 3 illustrates our tree-based representation of the code of a function. In this \nsection, we formally de.ne its syntax and seman\u00adtics. Syntax The code of a function is represented as \na mapping from labels to trees. Each label corresponds to a cut point in the control\u00ad.ow graph of the \nfunction. A node of a tree is labeled either by a non-branching instruction, with one child representing \nits unique successor; or by a conditional instruction, with two childs for its two successors. The leaves \nof instruction trees are out(l) nodes, carrying the label l of the tree to which control is transferred. \nFinally, special one-element trees are introduced to represent call and return instructions. Instruction \ntrees: T ::= seq(i, T ) (i a non-branching instruction) | cond(cond, r , T1,T2) | out(l) Call trees: \nTc ::= call((r | id),l) Return trees: Tr ::= return Control-.ow graphs: g ::= l . (T | Tc | Tr) Functions: \nf ::= fun id { stack n1; frame n2; entry l; code g; }  Figure 2. Overview of trace scheduling and its \nvalidation. Solid Semantics The operational semantics of the tree-based represen\u00adarrows represent code \ntransformations and validations. Dashed ar-tation is a combination of small-step and big-step styles. \nWe de\u00adrows represent proofs of semantic preservation. scribe executions of instruction trees using a \nbig-step semantics label l0 op1 cond label op2 cond label . . . , l1 l2 . . . , l2 l1 l0l2l4 . . . seq(op1, \ncond(. . . , seq(op3, out(l4)), out(l2))) seq(op2, cond(. . . , seq(op3, out(l4)), out(l2))) return op3 \nlabel l4 ret Figure 3. A code represented as a list of instructions (left) and as a graph of instruction \ntrees (right) S f T, R, F, M . l, R ' ,F ' ,M ', meaning that the tree T , start\u00ading in state (R, F, \nM), terminates on a branch to label l in state (R ' ,F ' ,M ' ). Since the execution of a tree cannot \nloop in.nitely, this choice of semantics is adequate, and moreover is a good match for the validation \nalgorithm operating at the level of trees that we develop next. S f out(l), R, F, M . l, R, F, M v = \neval op(op, R(r )) S f T,R{rd . v}, F, M . l, R ' ,F ' ,M ' S f seq(op(op, r , r),T ), R, F, M . l, \nR ' ,F ' ,M ' true = eval condition(cond,R(r )) S f T1, R, F, M . l ' ,R ' ,F ' ,M ' S f cond(cond, r \n, T1,T2), R, F, M . l ' ,R ' ,F ' ,M ' * The predicate S f l, R, F, M . l ' ,R ' ,F ' ,M ', de.ned in \nsmall-step style, expresses the chained evaluation of zero, one or several trees, starting at label l \nand ending at label l ' . * S f l, R, F, M . l, R, F, M S f f .graph(l), R, F, M . l ' ,R ' ,F ' ,M ' \n'' . l '' '''' S f l ' ,R ' ,F ,M * ,R '' ,F ,M . l '' '''' S f l, R, F, M * ,R '' ,F ,M Finally, the \npredicate for evaluation of function calls, G f f , P, R, M . v, R ' ,M ', is re-de.ned in terms of trees \nin the obvious manner. alloc(M, 0, f .stack)=(sp, M1) init frame(f .frame)= F1 f .graph = g * G, f, \nsp, P f g(f .entry), R, M1 . l, R ' ,M2 g(l)= return M ' = free(M2, sp) G f f , P, R, M . R ' ,M '  \n5.2 Validation at the level of trees We .rst de.ne a validator Vt that checks semantic preservation between \ntwo instruction trees T1, T2. Vt(T1,T2, (m1,s1), (m2,s2)) = if T1 = seq(i1,T1' ): return Vt(T1' ,T2,a(i1, \n(m1,s1)), (m2,s2)) if T2 = seq(i2,T2' ): return Vt(T1,T2' , (m1,s1),a(i2, (m2,s2)) if T1 = cond(cond1,r \n1,T1' ,T1 '' ) and T2 = cond(cond2,r 2,T2' ,T2 '' ): return cond1 = cond2 . m1(r 1)= m2(r 2) . Vt(T1' \n,T2' , (m1,s1), (m2,s2)) '' '' . Vt(T1 ,T2 , (m1,s1), (m2,s2)) if T1 = out(l1) and T2 = out(l2): return \nl2 = l1 . m2 = m1 . s2 . s1 in all other cases: return false The validator traverses the two trees in \nparallel, performing symbolic evaluation of the non-branching instructions. We reuse the a(i, (m, s)) \nfunction of section 4.2. The (m1,s1) and (m2,s2) parameters are the current states of symbolic evaluation \nfor T1 and T2, respectively. We process non-branching instructions repeatedly in T1 or T2 until we reach \neither two cond nodes or two out leaves. When we reach cond nodes in both trees, we check that the conditions \nbeing tested and the symbolic evaluations of their arguments are identical, so that at run-time control \nwill .ow on the same side of the conditional in both codes. We then continue validation on the true subtrees \nand on the false subtrees. Finally, when two out leaves are reached, we check that they branch to the \nsame label and that the symbolic states agree (m2 = m1 and s2 . s1), as in the case of block veri.cation. \nAs expected, a successful run of Vt entails a semantic preserva\u00adtion result. Lemma 5. if Vt(T1,T2)= true \nand S f T1, R, F, M . '' '' l, R ' ,F ,M then S f T2, R, F, M . l, R ' ,F ,M 5.3 Validation at the level \nof function bodies We now extend the tree validator Vt to a validator that operates over two control-.ow \ngraphs of trees. We simply check that identically\u00adlabeled regular trees in both graphs are equivalent \naccording to Vt, and that call trees and return trees are identical in both graphs. Vg(g1,g2)= if Dom(g1) \n= Dom(g2), return false for each l . Dom(g1): if g1(l) and g2(l) are regular trees: if Vt(g1(l),g2(l), \n(e, \u00d8), (e, \u00d8)) = false, return false otherwise: if g1(l)= g2(l), return false end for each return true \nThis validator is correct in the following sense. Let p, p ' be two programs in the tree-based representation \nsuch that p ' is identical to p except for the function bodies, and Vg(p(id).graph,p ' (id).graph)= true \nfor all function names id . p. Theorem 3. Let G and G ' be the global environments associ\u00adated with p \nand p ', respectively. If G f f , P, R, M . R ' ,M ' and Vg(f .graph, f ' .graph)= true, then G ' f f \n' , P, R, M . R ' ,M ' .  5.4 Conversion to the graph-of-trees representation The validator developed \nin section 5.3 operates over functions whose code is represented as graphs of instruction trees. How\u00adever, \nthe unveri.ed trace scheduler, as well as the surrounding com\u00adpiler passes, consume and produce Mach \ncode represented as lists of instructions. Therefore, before invoking the validator Vg, we need to convert \nthe original and scheduled codes from the list-of\u00adinstructions representation to the graph-of-trees representation. \nTo prove the correctness of this algorithm, we need to show that the conversion preserves semantics in \nboth directions, or in other terms that each pair of a list of instructions and a graph of trees is seman\u00adtically \nequivalent. The conversion algorithm is conceptually simple, but not en\u00adtirely trivial. In particular, \nit involves the computation of back edges in order to determine the cut points. Instead of writing the \nconver\u00adsion algorithm in Coq and proving directly its correctness, we chose to use the translation validation \napproach one more time. In other terms, the conversion from lists of instructions to graphs of trees \nis written in unveri.ed Caml, and complemented with a validator, written and proved in Coq, which takes \na Mach function f (with its code represented as a list of instructions) and a graph of trees g and checks \nthat f .code and g are semantically equivalent. This check is written f .code ~ g. The full validator \nfor trace scheduling is therefore of the following form: V (f1, f2)= convert f1.code to a graph of trees \ng1 convert f2.code to a graph of trees g2 return f1.code ~ g1 . f2.code ~ g2 . Vg(g1,g2) To check that \nan instruction sequence C and a graph g are equivalent, written C ~ g, we enumerate the cut points l \n. Dom(g) and check that the list c of instructions starting at point l in the instruction sequence C \ncorresponds to the tree g(l). We write this check as a predicate C, Bf c ~ T , where B = Dom(g) is the \nset of cut points. The intuition behind this check is that every possible execution path in c should \ncorrespond to a path in T that executes the same instructions. In particular, if c starts with a non\u00adbranching \ninstruction, we have i non-branching C, Bf c ~ T C, Bf i :: c ~ seq(i, T ) Unconditional and conditional \nbranches appearing in c need special handling. If the target l of the branch is a cut point (l .B), this \nbranch terminates the current trace and enters a new trace; it must therefore corresponds to an out(l) \ntree. l .B C, Bf label(l) :: c ~ out(l) l .B C, Bf goto(l) :: c ~ out(l) ltrue .B C, Bf c ~ T C, Bf cond(cond, \nr , ltrue) :: c ~ cond(cond, r , out(ltrue ),T ) However, if l is not a cut point (l/.B), the branch \nor label in c is not materialized in the tree T and is just skipped. l/.B C, Bf c ~ T C, Bf label(l) \n:: c ~ T l/.B c ' = find label(l, C) C, Bf c ' ~ T C, Bf goto(l) :: c ~ T ltrue .B/c ' = find label(ltrue \n,C) C, Bf c ~ T C, Bf c ' ~ T ' C, Bf cond(cond, r , ltrue ) :: c ~ cond(cond, r , T ' ,T ) An interesting \nfact is that the predicate C, Bf c ~ T indirectly checks that B contains at least all the targets of \nback-edges in the code C. For if this were not the case, the code C would contain a loop that does not \ngo through any cut point, and we would have to apply one of the three skip rules above an in.nite number \nof times; therefore, the inductive predicate C, Bf c ~ T cannot hold. As discussed in section 6, the \nimplementation of the ~ check (shown in appendix A) uses a counter of instructions traversed to abort \nvalidation instead of diverging in the case where B incorrectly fails to account for all back-edges. \nThe equivalence check C ~ g de.ned above enjoys the desired semantic equivalence property: Lemma 6. Let \np be a Mach program and p ' a corresponding program where function bodies are represented as graphs of \ntrees. Assume that p(id).code ~ p ' (id).code for all function names id . p. Let G and G ' be the global \nenvironments associated with p and p ', respectively. If f .code ~ f ' .code, then G f f , P, R, M . \nR ' ,M ' in the original Mach semantics if and only ' R '' if G ' f f , P, R, M . ,M in the tree-based \nsemantics of section 5.1. The combination of Theorem 3 and Lemma 6 establishes the correctness of the \nvalidator for trace scheduling. 6. The Coq mechanization The algorithms presented in this paper have \nbeen formalized in their entirety and proved correct using the Coq proof assistant ver\u00adsion 8.1. The \nCoq mechanization is mostly straightforward. Oper\u00adational semantics are expressed as inductive predicates \nfollowing closely the inference rules shown in this paper. The main dif.culty was to express the algorithms \nas computable functions within Coq. Generally speaking, there are two ways to specify an algorithm in \nCoq: either as inductive predicates using inference rules, or as com\u00adputable functions de.ned by recursion \nand pattern-matching over tree-shaped data structures. We chose the second presentation be\u00adcause it enables \nthe automatic generation of executable Caml code from the speci.cations; this Caml code can then be linked \nwith the hand-written Caml implementations of the unveri.ed transforma\u00adtions. However, Coq is a logic \nof total functions, so the function de.nitions must be written in a so-called structurally recursive \nstyle where termination is obvious. All our validation functions are naturally structurally recursive, \nexcept validation between trees (function Vt in section 5.2) and validation of list-to-tree conversion \n(the function corresponding to the ~ predicate in section 5.4). For validation between trees, we used \nwell-founded recursion, using the sum of the heights of the two trees as the decreasing, positive measure. \nCoq 8.1 provides good support for this style of recursive function de.nitions (the Function mechanism \n(Barthe et al. 2006)) and for the corresponding inductive proof principles (the functional induction \ntactic). Validation of list-to-tree conversion could fail to terminate if the original, list-based code \ncontains a loop that does not cross any cut point. This indicates a bug in the external converter, since \nnormally cut points include all targets of back edges. To detect this situation and to make the recursive \nde.nition of the validator acceptable to Coq, we add a counter as parameter to the validation function, \ninitialized to the number of instructions in the original code and decremented every time we examine \nan instruction. If this counter drops to zero, validation stops on error. Appendix A shows the corresponding \nvalidation algorithm. The Coq development accounts for approximatively 11000 lines of code. It took one \nperson-year to design the validators, program them and prove their correctness. Figure 5 is a detailed \nline count showing, for each component of the validators, the size of the speci.cations (i.e. the algorithms \nand the semantics) and the size of the proofs. Speci.\u00adcations Proofs Total Symbolic evaluation 736 1079 \n1815 Block validation 348 1053 1401 Block semantics 190 150 340 Block scheduling validation 264 590 854 \nTrace validation 234 1045 1279 Tree semantics 986 2418 3404 Trace scheduling validation 285 352 637 Label \nmanipulation 306 458 764 Typing 114 149 263 Total 3463 7294 10757 Figure 5. Size of the development \n(in non-blank lines of code, without comments) It is interesting to note that the validation of trace \nscheduling is no larger and no more dif.cult than that of list scheduling. This is largely due to the \nuse of the graph-of-trees representation of the code. However, the part labeled tree semantics , which \nincludes the de.nition and semantics of trees plus the validation of the conversion from list-of-instructions \nto graph-of-trees, is the largest and most dif.cult part of this development. 7. Preliminary experimental \nevaluation and algorithmic issues Executable validators were extracted automatically from the Coq formalization \nof the algorithms presented in this paper and con\u00adnected to two implementations of scheduling optimizations \nwrit\u00adten in Caml: one for basic-block scheduling using the standard list scheduling algorithm, the other \nfor trace scheduling. The two veri\u00ad.ed compilation passes thus obtained were integrated in the Com\u00adpcert \nexperimental compiler (Leroy 2006; Leroy et al. 2003 2007), and tested on the test suite of this compiler \n(a dozen Cminor pro\u00adgrams in the 100-1000 l.o.c. range). This test suite is too small to draw any de.nitive \nconclusion. We nonetheless include the exper\u00adimental results because they point out potential algorithmic \ninef.\u00adciencies in our approach. All tests were successfully scheduled and validated after scheduling. \nManual inspection of the scheduled code reveals that the schedulers performed a fair number of instruction \nreorderings and, in the case of trace scheduling, insertion of stubs. Validation was effective from a \ncompiler engineering viewpoint: not only manual injection of errors in the schedulers were correctly \ncaught, but the validator also found one unintentional bug in our .rst implementation of trace scheduling. \nTo assess the compile-time overheads introduced by validation, we measured the execution times of the \ntwo scheduling transfor\u00admations and of the corresponding validators. Figure 4 presents the results. The \ntests were conducted on a Pentium 4 3.4 GHz Linux ma\u00adchine with 2 GB of RAM. Each pass was repeated as \nmany times as needed for the measured time to be above 1 second; the times reported are averages. On \nall tests except AES, the time spent in validation is com\u00adparable to that spent in the non-veri.ed scheduling \ntransformation. The total time (transformation + validation) of instruction schedul\u00ading is about 10% \nof the whole compilation time. The AES test (the optimized reference implementation of the AES encryption \nalgo\u00adrithm) demonstrates some inef.ciencies in our implementation of validation, which takes about 10 \ntimes longer than the correspond\u00ading transformation, both for list scheduling and for trace schedul\u00ading. \nThere are two potential sources of algorithmic inef.ciencies in the validation algorithms presented in \nthis paper. The .rst is the comparison between the symbolic codes and constraint sets generated by symbolic \nexecution. Viewed as a tree, the symbolic code for a block of length n can contain up to 2n nodes (consider \nfor instance the block r1 = r0 + r0; ... ; rn = rn-1 + rn-1). Viewed as a DAG, however, the symbolic \ncode has size linear in the length n of the block, and can be constructed in linear time. However, the \ncomparison function between symbolic codes that we de.ned in Coq compares symbolic codes as trees, ignoring \nsharing, and can therefore take O(2n) time. Using a hash-consed representation for symbolic expressions \nwould lead to much better performance: construction of the symbolic code would take time O(n log n) (the \nlog n accounts for the overhead of hash consing), comparison between symbolic codes could be done in \ntime O(1), and inclusion between sets of constraints in time O(n log n). We haven t been able to implement \nthis solution by lack of an existing Coq library for hash-consed data structures, so we leave it for \nfuture work. The second source of algorithmic inef.ciency is speci.c to trace scheduling. The tree-based \nrepresentation of code that we use for validation can be exponentially larger than the original code \nrepresented as a list of instructions, because of tail duplication of basic blocks. This potential explosion \ncaused by tail duplication can be avoided by adding more cut points: not just targets of back edges, \nbut also some other labels chosen heuristically to limit tail duplication. For instance, we can mark \nas cut points all the labels that do not belong to the traces that the scheduler chose to optimize. Such \nheuristic choices are performed entirely in unveri.ed code (the scheduler and the converter from list-to \ntree\u00adbased code representations) and have no impact on the validators and on their proofs of correctness. \n8. Related work The idea of translation validation appears in the work of Pnueli et al. (1998a,b). It \nwas initially conducted in the context of the compilation of a synchronous language. The principle of \nthe val\u00adidator is to generate veri.cation conditions that are solved by a model checker. The authors \nmention the possibility of generating a proof script during model checking, which generates additional \ncon.dence in the correctness of a run of validation, but is weaker than a full formal veri.cation of \nthe validator as in the present pa\u00adper. The case of an optimizing compiler for a conventional, impera\u00adtive \nlanguage has been addressed by Zuck et al. (2001, 2003) and Barret et al. (2005). They use a generalization \nof the Floyd method to generate veri.cation conditions that are sent to a theorem prover. Two validators \nhave been produced that implement this framework: voc-64 (Zuck et al. 2003) for the SGI pro-64 compiler \nand TVOC (Barret et al. 2005) for the ORC compiler. This work addresses ad\u00advanced compiler transformations, \nincluding non-structure preserv\u00ading transformations such as loop optimizations (Goldberg et al. 2005) \nand software pipelining (Leviathan and Pnueli 2006). Test program List scheduling Transformation Validation \nRatio V /T Trace scheduling Transformation Validation Ratio V /T fib 0.29 ms 0.47 ms 1.60 0.44 ms 0.58 \nms 1.32 integr 0.91 ms 0.87 ms 0.96 1.0 ms 1.2 ms 1.15 qsort 1.3 ms 1.5 ms 1.15 1.8 ms 3.3 ms 1.89 fft \n9.1 ms 18 ms 1.98 19 ms 62 ms 3.26 sha1 9.4 ms 6.7 ms 0.71 12 ms 24 ms 2.00 aes 56 ms 550 ms 9.76 67 \nms 830 ms 12.25 almabench 25 ms 16 ms 0.65 56 ms 200 ms 3.57 stopcopy 4.1 ms 4.1 ms 1.00 4.9 ms 6.1 ms \n1.25 marksweep 5.3 ms 6.3 ms 1.18 6.8 ms 11 ms 1.69 Figure 4. Compilation times and veri.cation times \nWhile the approach of Pnueli, Zuck et al. relies on veri.ca\u00adtion condition generators and theorem proving, \na different approach based on abstract interpretation and static analysis was initiated by Necula (2000). \nHe developed a validator for the GCC 2.7 com\u00adpiler, able to validate most of the optimisations implemented \nby this compiler. His approach relies on symbolic execution of RTL intermediate code and inspired the \npresent work. Necula s validator addresses a wider range of optimizations than ours, requiring him to \ncompare symbolic executions modulo arithmetic and memory\u00adrelated equations. Rival (2004) describes a \ntranslation validator for GCC 3.0 with\u00adout optimizations. While Necula s validator handles only transfor\u00admations \nover the RTL intermediate language, Rival s relates di\u00adrectly the C source code with the generated PowerPC \nassembly code. Rival s validator uses Symbolic Transfer Functions to rep\u00adresent the behaviour of code \nfragments. While Necula and Rival do not discuss instruction scheduling in detail, we believe that their \nvalidators can easily handle list scheduling (reordering of instruc\u00adtions within basic blocks), but we \ndo not know whether they can cope with the changes in the control-.ow graph introduced by trace scheduling. \nHuang et al. (2006) describe a translation validator specialized to the veri.cation of register allocation \nand spilling. Their algo\u00adrithm relies on data-.ow analyses: computation and correlation of webs of def-use \nsequences. By specializing the validator to the code transformations that a register allocator typically \nperforms, they claim to obtain a validator that is complete (no false alarms), and they can also produce \ndetailed explanations of errors. Compared with validators based on veri.cation condition gen\u00aderators, \nvalidators based on static analysis like Necula s, Rival s, Huang et al s and ours are arguably less \npowerful but algorithmi\u00adcally more ef.cient, making it realistic to perform validation at ev\u00adery compilation \nrun. Moreover, it seems easier to characterize the classes of transformations that can be validated. \nWhile several of the papers mentioned above come with on\u00adpaper proofs, none has been mechanically veri.ed. \nThere are, how\u00adever, several mechanized veri.cations of static analyzers, i.e. tools that establish properties \nof one piece of compiled code instead of relating two pieces of compiled code like translation validators \ndo, in particular the JVM bytecode veri.er (Klein and Nipkow 2003) and data .ow analyzers (Cachera et \nal. 2005). 9. Conclusions and further work We presented what we believe is the .rst fully mechanized \nveri.\u00adcation of translation validators. The two validators presented here were developed with list scheduling \nand trace scheduling in mind, but they seem applicable to a wider class of code transformations: those \nthat reorder, factor out or duplicate instructions within basic blocks or instruction trees (respectively), \nwithout taking advantage of non-aliasing information. For instance, this includes common subexpression \nelimination, as well as rematerialization. We believe (without any proof) that our validators are complete, \nthat is, raise no false alarms for this class of transformations. It is interesting to note that the \nvalidation algorithms proceed very differently from the code transformations that they validate. The \nvalidators uses notions such as symbolic execution and block\u00ador tree-based decompositions of program \nexecutions that have ob\u00advious semantic meanings. In contrast, the optimizations rely on notions such \nas RAW/WAR/WAW dependencies and back-edges whose semantic meaning is much less obvious. For this reason, \nwe believe (without experience to substantiate this claim) that it would be signi.cantly more dif.cult \nto prove directly the correctness of list scheduling or trace scheduling. A direct extension of the present \nwork is to prove semantic preservation not only for terminating executions, but also for di\u00adverging executions. \nThe main reason why our proofs are restricted to terminating evaluations is the use of big-step operational \nseman\u00adtics. A small-step (transition) semantics for Mach is in develop\u00adment, and should enable us to \nextend the proofs of semantic preser\u00advation to diverging executions. Another, more dif.cult extension \nis to take non-aliasing information into account in order to validate reorderings between independent \nloads and stores. More generally, there are many other optimizations for which it would be interesting \nto formally verify the corresponding valida\u00adtion algorithms. Most challenging are the optimizations that \nmove computations across loop boundaries, such as loop invariant hoist\u00ading and software pipelining. Acknowledgments \nJulien Forest helped us use the new Coq feature Function and improved its implementation at our request. \nWe thank Alain Frisch for discussions and feedback. References Andrew W. Appel. Modern Compiler Implementation \nin ML. Cambridge University Press, 1998. Clark W. Barret, Yi Fang, Benjamin Goldberg, Ying Hu, Amir Pnueli, \nand Lenore Zuck. TVOC: A translation validator for optimizing compilers. In Computer Aided Veri.cation, \n17th Int. Conf., CAV 2005, volume 3576 of Lecture Notes in Computer Science, pages 291 295. Springer, \n2005. Gilles Barthe, Julien Forest, David Pichardie, and Vlad Rusu. De.ning and reasoning about recursive \nfunctions: a practical tool for the Coq proof assistant. In Functional and Logic Programming, 8th Int. \nSymp., FLOPS 2006, volume 3945 of Lecture Notes in Computer Science, pages 114 129. Springer, 2006. Yves \nBertot and Pierre Cast\u00b4eran. Interactive Theorem Proving and Pro\u00ad gram Development Coq Art: The Calculus \nof Inductive Constructions. EATCS Texts in Theoretical Computer Science. Springer, 2004. David Cachera, \nThomas Jensen, David Pichardie, and Vlad Rusu. Extract\u00ading a Data Flow Analyser in Constructive Logic. \nTheoretical Computer Science, 342(1):56 78, 2005. Coq development team. The Coq proof assistant. Software \nand documen\u00adtation available at http://coq.inria.fr/, 1989 2007. Maulik A. Dave. Compiler veri.cation: \na bibliography. SIGSOFT Softw. Eng. Notes, 28(6):2 2, 2003. John R. Ellis. Bulldog: a compiler for VLSI \narchitectures. ACM Doctoral Dissertation Awards. The MIT Press, 1986. Benjamin Goldberg, Lenore Zuck, \nand Clark Barret. Into the loops: Prac\u00adtical issues in translation validation for optimizing compilers. \nIn Proc. Workshop Compiler Optimization Meets Compiler Veri.cation (COCV 2004), volume 132 of Electronic \nNotes in Theoretical Computer Science, pages 53 71. Elsevier, 2005. Yuqiang Huang, Bruce R. Childers, \nand Mary Lou Soffa. Catching and identifying bugs in register allocation. In Static Analysis, 13th Int. \nSymp., SAS 2006, volume 4134 of Lecture Notes in Computer Science, pages 281 300. Springer, 2006. Gerwin \nKlein and Tobias Nipkow. Veri.ed bytecode veri.ers. Theoretical Computer Science, 298(3):583 626, 2003. \nGerwin Klein and Tobias Nipkow. A machine-checked model for a Java\u00adlike language, virtual machine and \ncompiler. ACM Transactions on Programming Languages and Systems, 28(4):619 695, 2006. D. Leinenbach, \nW. Paul, and E. Petrova. Towards the formal veri.cation of a C0 compiler: Code generation and implementation \ncorrectness. In Int. Conf. on Software Engineering and Formal Methods (SEFM 2005), pages 2 11. IEEE Computer \nSociety Press, 2005. Xavier Leroy. Formal certi.cation of a compiler back-end, or: programming a compiler \nwith a proof assistant. In 33rd symposium Principles of Programming Languages, pages 42 54. ACM Press, \n2006. Xavier Leroy et al. The Compcert certi.ed compiler back-end. Development available at http://gallium.inria.fr/~xleroy/ \ncompcert-backend/, 2003 2007. Raya Leviathan and Amir Pnueli. Validating software pipelining optimiza\u00adtions. \nIn Int. Conf. On Compilers, Architecture, And Synthesis For Em\u00adbedded Systems (CASES 2002), pages 280 \n287. ACM Press, 2006. Steven S. Muchnick. Advanced compiler design and implementation. Mor\u00adgan Kaufmann, \n1997. George C. Necula. Translation validation for an optimizing compiler. In Programming Language Design \nand Implementation 2000, pages 83 95. ACM Press, 2000. Amir Pnueli, Ofer Shtrichman, and Michael Siegel. \nThe code validation tool (CVT) automatic veri.cation of a compilation process. International Journal \non Software Tools for Technology Transfer, 2:192 201, 1998a. Amir Pnueli, Michael Siegel, and Eli Singerman. \nTranslation validation. In Tools and Algorithms for Construction and Analysis of Systems, TACAS 98, volume \n1384 of Lecture Notes in Computer Science, pages 151 166. Springer, 1998b. Xavier Rival. Symbolic transfer \nfunction-based approaches to certi.ed compilation. In 31st symposium Principles of Programming Languages, \npages 1 13. ACM Press, 2004. Emin G\u00a8un Sirer and Brian N. Bershad. Testing Java virtual machines. In \nProc. Int. Conf. on Software Testing And Review, 1999. Martin Strecker. Compiler veri.cation for C0. \nTechnical report, Universit\u00b4e Paul Sabatier, Toulouse, April 2005. L. Zuck, A. Pnueli, and R. Leviathan. \nValidation of optimizing compilers. Technical Report MCS01-12, Weizmann institute of Science, 2001. Lenore \nZuck, Amir Pnueli, Yi Fang, and Benjamin Goldberg. VOC: A methodology for translation validation of optimizing \ncompilers. Journal of Universal Computer Science, 9(3):223 247, 2003. A. Appendix: the validation algorithm \nfor conversion from instruction lists to instruction trees We show here the algorithm that determines \nsemantic equivalence between a list of instructions and an instruction tree, corresponding to the predicate \nf , Bf c ~ T in section 5.4. 1 let rec skip control B func c counter = if counter = 0 3 then None else \n5 match c with | Mlabel lbl :: c . 7 if lbl in B then Some (Mlabel lbl :: c ) 9 else skip control B func \nc (counter - 1) | Mgoto lbl :: c . 11 match .nd label lbl func with | Some c . if lbl in B 13 then Some \n(Mgoto lbl :: c ) else skip control B func c (counter - 1) 15 | None .None | i :: c . Some (i :: c ) \n17 |. None 19 let test out sub lbl = match sub with 21 | out lbl . lbl =lbl |. false 23 let rec validTreeBase \nB f cur t = 25 let cur = skip control B (fn code f) cur (length (fn code f)) in match cur , t with 27 \n| Some(getstack(i,t,m):: l), getstack(i ,t ,m ,sub) . i =i . t =t . m = m . 29 validTreeBase B f l sub \n| Some(setstack(m,i,t) :: l), setstack(m ,i ,t ,sub) . 31 i=i . t =t . m = m . validTreeBase B f l sub \n33 | Some(getparam(i,t , m) :: l ), getparam(i , t , m ,sub) . i =i . t =t . m = m . 35 validTreeBase \nB f l sub | Some(op(op,lr, m) :: l ), op(op , lr , m ,sub) . 37 op=op . lr =lr . m = m . validTreeBase \nB f l sub 39 | Some(load(chk,addr, lr , m) :: l ), addr = addr . chk = chk . 41 validTreeBase B f l sub \n| Some(store(chk,addr,lr,m):: l), 43 addr = addr . chk = chk . validTreeBase B f l sub load(chk , addr \n, lr , m ,sub) . lr =lr . m = m . store (chk , addr , lr , m ,sub) . lr =lr . m = m . 45 | Some(cond(c,rl,lbl) \n:: l), cond(c ,rl ,sub1,sub2) . c =c . lr =lr . 47 validTreeBase B f l sub2 . ( if lbl in B 49 then test \nout sub1 lbl else match .nd label lbl (fn code f) with 51 | Some l . validTreeBase B f l sub1 | None \n.false ) 53 | Some(label(lbl ) :: l), out(lbl ) . lbl =lbl | Some(goto(lbl) :: l), out(lbl ) . lbl =lbl \n55 | , . false   \n\t\t\t", "proc_id": "1328438", "abstract": "<p>Translation validation consists of transforming a program and <i>a posteriori</i> validating it in order to detect a modification of itssemantics. This approach can be used in a verified compiler, provided that validation is formally proved to be correct. We present two such validators and their Coq proofs of correctness. The validators are designed for two instruction scheduling optimizations: list scheduling and trace scheduling.</p>", "authors": [{"name": "Jean-Baptiste Tristan", "author_profile_id": "81342514111", "affiliation": "INRIA Paris-Rocquencourt, Rocquencourt, France", "person_id": "P925378", "email_address": "", "orcid_id": ""}, {"name": "Xavier Leroy", "author_profile_id": "81100078576", "affiliation": "INRIA Paris-Rocquencourt, Rocquencourt, France", "person_id": "PP43115919", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1328438.1328444", "year": "2008", "article_id": "1328444", "conference": "POPL", "title": "Formal verification of translation validators: a case study on instruction scheduling optimizations", "url": "http://dl.acm.org/citation.cfm?id=1328444"}