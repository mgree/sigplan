{"article_publication_date": "01-07-2008", "fulltext": "\n Clowns to the Left of me, Jokers to the Right (Pearl) DissectingDataStructures Conor McBride University \nof Nottingham ctm@cs.nott.ac.uk Abstract 1. Introduction This paper introduces a small but useful generalisation \nto the derivative operation on datatypes underlying Huet s notion of zipper (Huet 1997; McBride 2001; \nAbbott et al. 2005b), giv\u00ading a concrete representation to one-hole contexts in data which is undergoing \ntransformation. This operator, dissection , turns a container-like functor into a bifunctor representing \na one-hole con\u00adtext in which elements to the left of the hole are distinguished in type from elements \nto its right. I present dissection here as a generic program, albeit for polyno\u00admial functors only. The \nnotion is certainly applicable more widely, but here I prefer to concentrate on its diverse applications. \nFor a start, map-like operations over the functor and fold-like operations over the recursive data structure \nit induces can be expressed by tail recursion alone. Further, the derivative is readily recovered from \nthe dissection. Indeed, it is the dissection structure which delivers Huet s operations for navigating \nzippers. The original motivation for dissection was to de.ne division , capturing the notion of leftmost \nhole, canonically distinguishing values with no elements from those with at least one. Division gives \nrise to an isomorphism corresponding to the remainder theorem in algebra. By way of a larger example, \ndivision and dissection are exploited to give a relatively ef.cient generic algorithm for abstracting \nall occurrences of one term from another in a .rst-order syntax. The source code for the paper is available \nonline1 and compiles with recent extensions to the Glasgow Haskell Compiler. Categories and Subject Descriptors \nD.1.1 [Programming Tech\u00adniques]: Applicative (Functional) Programming; I.1.1 [Symbolic and Algebraic \nManipulation]: Expressions and Their Representa\u00adtion General Terms Algorithms, Design, Languages, Theory \nKeywords Datatype, Differentiation, Dissection, Division, Generic Programming, Iteration, Polynomial, \nStack, Tail Recursion, Traver\u00adsal, Zipper 1 http://www.cs.nott.ac.uk/~ctm/CloJo/CJ.lhs Permission to \nmake digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL 08, January \n7 12, 2008, San Francisco, California, USA. Copyright c . 2008 ACM 978-1-59593-689-9/08/0001. . . $5.00 \nThere s an old Stealer s Wheel song with the memorable chorus: Clowns to the left of me, jokers to the \nright, Here I am, stuck in the middle with you. Joe Egan, Gerry Rafferty In this paper, I examine what \nit s like to be stuck in the middle of traversing and transforming a data structure. I ll show both you \nand the Glasgow Haskell Compiler how to calculate the datatype of a freezeframe in a map-or fold-like \noperation from the datatype being operated on. That is, I ll explain how to compute a .rst-class data \nrepresentation of the control structure underlying map and fold traversals, via an operator which I call \ndissection. Dissection turns out to generalise both the derivative operator underlying Huet s zippers \n(Huet 1997; McBride 2001) and the notion of division used to calculate the non-constant part of a polynomial. \nLet me take you on a journey into the algebra and differential calculus of datatypes, in search of functionality \nfrom structure. Here s an example traversal evaluating a very simple language of expressions: data Expr \n=Val Int | Add Expr Expr eval ::Expr . Int eval (Val i)=i eval (Add e1 e2)=eval e1 +eval e2 What happens \nif we freeze a traversal? Typically, we shall have one piece of data in focus and a hole in the expression \nwhere it belongs, with unprocessed data ahead of us and processed data behind. We should expect something \na bit like Huet s zipper rep\u00adresentation of a one-hole context (Huet 1997), a stack-like structure carrying \nposition information and cacheing all the out-of-focus data for each node between the hole and the root. \nHowever, now we need different sorts of stuff on either side of the hole. In the case of our evaluator, \nsuppose we proceed left-to-right. Whenever we face an Add, we start by going left into the .rst operand, \nrecording the second Expr to process later; once we have .nished with the former, we must go right into \nthe second operand, recording the Int returned from the .rst; as soon as we have both values, we can \nadd them. Correspondingly, a Stack of these direction-with-cache choices completely determines where \nwe are in the evaluation process. Let s make this structure explicit:2 type Stack =[Expr +Int] Now we \ncan implement an eval machine a tail-recursive pro\u00adgram, at each stage stuck in the middle with either \nan expression to decompose, in which case we load the stack and go left, or a value to return, in which \ncase we unload the stack and try to move right. 2 For brevity, I write \u00b7 + \u00b7 for Either, L for Left and \nR for Right eval ::Expr .Int eval e=load e[] load ::Expr .Stack .Int load (Val i) stk=unload istk load \n(Add e1 e2)stk=load e1 (L e2 :stk) unload ::Int .Stack .Int unload v [] =v unload v1 (L e2 :stk)=load \ne2 (R v1 :stk) unload v2 (R v1 :stk)=unload (v1 +v2)stk Each layer of this Stack structure is a dissection \nof Expr s recursion pattern. We have two ways to be stuck in the middle: we re either L e2, on the left \nwith an Expr waiting to the right of us, or R v1,on the right with an Int cached to the left of us. Danvy \nand colleagues have shown us how to calculate abstract machines from programs by transforming them into \na sequential, continuation-passing style, then defunctionalising the result (Ager et al. 2003). Here, \nhowever, the structure of the program comes from the structure of its data. Correspondingly, we can mechanize \nthe stack construction more directly, by working from the types. 2. Polynomial Functors and Bifunctors \nThis section brie.y recapitulates material which is quite standard. I hope to gain some generic leverage \nby exploiting the characterisa\u00adtion of recursive datatypes as .xpoints of polynomial functors. For more \ndepth and detail, I refer the reader to the excellent Algebra of Programming (Bird and de Moor 1997). \nIf we are to work in a generic way with data structures, we need to present them in a generic way. Rather \nthan giving an individual data declaration for each type we want, let us see how to build them from a \n.xed repertoire of components. I ll begin with the polynomial type constructors in one parameter. These \nare generated by constants, the identity, sum and product. I label them with a 1 subscript to distinguish \nthem their bifunctorial cousins. data K1 ax=K1 a --constant data Id x=Id x --element data (p+1 q)x=L1 \n(px)|R1 (qx) --choice data (p\u00d71 q)x=(px, qx)1 --pairing Allow me to abbreviate one of my favourite constant \nfunctors, at the same time bringing it into line with our algebraic style. type 11 =K1 () Some very basic \ncontainer type constructors can be expressed as polynomials, with the parameter giving the type of elements \n. For example, the Maybe type constructor gives a choice between Nothing , a constant, and Just , embedding \nan element. type Maybe =11 +1 Id Nothing =L1 (K1 ()) Just x =R1 (Id x) Whenever I reconstruct a datatype \nfrom this kit, I shall make a habit of de.ning its constructors linearly in terms of the kit con\u00adstructors. \nTo aid clarity, I use these pattern synonyms on either side of a functional equation, so that the coded \ntype acquires the same programming interface as the original. This is an old idea (Aitken and Reppy 1992), \nbut it is not standard Haskell: these de.nitions may readily be expanded to code which is fully compliant, \nif less readable. The kit approach allows us to establish properties of whole classes of datatype at \nonce, using Haskell s rather powerful facili\u00adties for ad hoc polymorphism (Wadler and Blott 1989). For \nexam\u00adple, the polynomials are all functorial: we can make the standard Functor class class Functor pwhere \nfmap ::(s.t).ps.pt respect the polynomial constructs. instance Functor (K1 a)where fmap f (K1 a)=K1 \na instance Functor Id where fmap f (Id s)=Id (fs) instance (Functor p, Functor q).Functor (p+1 q)where \nfmap f (L1 p)=L1 (fmap fp) fmap f (R1 q)=R1 (fmap fq) instance (Functor p, Functor q).Functor (p\u00d71 q)where \nfmap f (p, q)1 =(fmap fp, fmap fq)1 Our reconstructed Maybe is functorial without further ado. 2.1 Datatypes \nas Fixpoints of Polynomial Functors The Expr type is not itself a polynomial, but its branching structure \nis readily described by a polynomial. Think of each node of an Expr as a container whose elements are \nthe immediate sub-Exprs: type ExprP =K1 Int +1 Id \u00d71 Id ValP i =L1 (K1 i) AddP e1 e2 =R1 (Id e1, Id e2)1 \nCorrespondingly, we should hope to establish the isomorphism Expr ~=ExprP Expr but we cannot achieve \nthis just by writing type Expr =ExprP Expr for this creates an in.nite type expression, rather than an \nin.nite type. Rather, we must de.ne a recursive datatype which ties the knot : \u00b5 pinstantiates p s element \ntype with \u00b5 pitself. data \u00b5 p=In (p(\u00b5 p)) Now we may complete our reconstruction of Expr type Expr =\u00b5 \nExprP Val i =In (ValP i) Add e1 e2 =In (AddP e1 e2) The container-like quality of polynomials allows \nus to de.ne a fold-like recursion operator for them, sometimes called the iterator or the catamorphism.3 \nHow can we compute a value in v from recursive data in \u00b5 p? First, we can expand a \u00b5 ptree as a p(\u00b5 p) \ncontainer of subtrees; next, we can use p s fmap operator to deliver a pv, recursively computing the \nvalue for each subtree; .nally, we can post-process the pvvalue container to produce a .nal result in \nv. The behaviour of the recursion is thus uniquely determined by the p-algebra f ::pv.vwhich does the \npost-processing. (|\u00b7|)::Functor p.(pv.v).\u00b5 p.v (|f|)(In p)=f (fmap (|f|)p) For example, we can write \nour evaluator as a catamorphism, with an algebra which implements each construct of our language for \nvalues rather than expressions. The pattern synonyms for ExprP help us to see what is going on: eval \n::\u00b5 ExprP .Int eval =(|f|)where f (ValP i)=i f (AddP v1 v2)=v1 +v2 3 Terminology is a mine.eld here: \nsome people think of fold as threading a binary operator through the elements of a container, others \nas replacing the constructors with an alternative algebra. The confusion arises because the two coincide \nfor lists. There is no resolution in sight. A catamorphism may appear to have a complex higher-order \nrecursive structure, but we shall soon see how to turn it into a .rst\u00adorder tail-recursion whenever pis \npolynomial. We shall do this by dissecting p, distinguishing the clown elements left of a chosen position \nfrom the joker elements to the right. 2.2 Polynomial Bifunctors Before we can start dissecting, however, \nwe shall need to be able to manage two sorts of elements. To this end, we shall need to in\u00adtroduce the \npolynomial bifunctors, which are just like the functors, but with two parameters. data K2 a xy=K2 a data \nFst xy=Fst x data Snd xy=Snd y data (p+2 q)xy=L2 (pxy)| R2 (qxy) data (p\u00d72 q) xy=(pxy, qxy)2 type 12 \n=K2 () We have the analogous notion of mapping , except that we must supply one function for each parameter. \nclass Bifunctor pwhere bimap ::(s1 . t1). (s2 . t2). ps1 s2 . pt1 t2 instance Bifunctor (K2 a)where bimap \nfg(K2 a)=K2 a instance Bifunctor Fst where bimap fg(Fst x)=Fst (fx) instance Bifunctor Snd where bimap \nfg(Snd y)=Snd (gy) instance (Bifunctor p, Bifunctor q). Bifunctor (p+2 q)where bimap fg(L2 p)=L2 (bimap \nfgp) bimap fg(R2 q)=R2 (bimap fgq) instance (Bifunctor p, Bifunctor q). Bifunctor (p\u00d72 q)where bimap \nfg(p, q)2 =(bimap fgp, bimap fgq)2 It s certainly possible to take .xpoints of bifunctors to obtain \nre\u00adcursively constructed container-like data: one parameter stands for elements, the other for recursive \nsub-containers. These structures support both fmap and a suitable notion of catamorphism. I can recommend \nGibbons (2007) as a useful tutorial for this origami style of programming. 2.3 Nothing is Missing We \nare still short of one basic component: Nothing. We shall be constructing types which organise the ways \nto split at a position , but what if there are no ways to split at a position (because there are no positions)? \nWe need a datatype to represent impossibility and here it is: data Zero Elements of Zero are hard to \ncome by elements worth speak\u00ading of, that is. If you can reduce an element of Zero to a well\u00adde.ned value, \nyou can exchange it for anything you want! refute ::Zero . a refute x=x seq error \"we never get this \nfar\" I have used Haskell s seq operator to insist that refute evaluate its argument. This is necessarily \nunde.ned, hence the error clause can never be executed. In effect, refute rejects its input. We can use \npZero to represent ps with no elements . For ex\u00adample, the only inhabitant of [Zero]mentionable in polite \nsociety is []. Zero gives us a convenient way to get our hands on exactly the constants, common to every \ninstance of p. Accordingly, we should be able to embed these constants into any other instance:4 in.ate \n::Functor p. pZero . px in.ate =fmap refute Now that we have Zero, allow me to abbreviate type 01 =K1 \nZero type 02 =K2 Zero  3. Clowns, Jokers and Dissection Let us now develop this idea of dissecting \nfunctors with the help of some visual stimuli. If we consider functors parametrised by elements, depicted \n, we can draw a typical value in some px as a container of s: .- - - - - -. We shall need to relate these \nfunctors to bifunctors which re.ne the notion of element into two kinds: clowns (.) tothe left andjokers \n(.) to the right of some hole (.). More particularly, we shall need three operators which take polynomial \nfunctors to bifunctors, classifying their elements as clowns, jokers or the hole. Firstly, all clowns \n plifts puniformly to the bifunctor which uses its left parameter for the elements of p. .-.-.-.-.-.-. \nWe can de.ne this uniformly: pcj= (pc) instance Functor f. Bifunctor ( f)where bimap fg( pc)= (fmap \nfpc) data Note that Id ~ =Fst. Secondly, all jokers  pis the analogue for the right parameter. .-.-.-.-.-.-. \ndata pcj= (pj) instance Functor f. Bifunctor ( f)where bimap fg( pj)= (fmap gpj) Note that Id ~ =Snd. \nThirdly, dissection  ptakes pto the bifunctor which chooses a position in a p, storing clowns to the \nleft of it and jokers to the right. .-.-.-.-.-.-. We must clearly de.ne dissection case by case. Let \nus work in\u00adformally and think through what to do for each polynomial type constructor. Constants have \nno positions for elements, .-. so there is no way to dissect them: (K1 a)=02 The Id functor has just \none position, so there is just one way to dissect it, and no room for clowns or jokers, left or right. \n.- -. -. .-.-. Id =12 Dissecting a p+1 q, we get either a dissected por a dissected q. L1 .- - - -.-. \nL2 .-.-.-.-. R1 .- - - -.-. R2 .-.-.-.-. 4 If the compiler adopts a uniform representation for polymorphic \ndata, it may be pro.table to replace in.ate by an unsafe cast. (p +1 q)= q p +2  So far, these have \njust followed Leibniz s rules for the derivative, but for pairs p\u00d71 q we see the new twist. When dissecting \na pair, we choose to dissect either the left component (in which case the right component is all jokers) \nor the right component (in which case the left component is all clowns). j L2 (.-.-.-.-. , .-.-.-.-.)2(.- \n- - -. , .- - - -.)1 -. R2 (.-.-.-.-. , .-.-.-.-.)2 (p \u00d71 q)= q p \u00d72  q +2 p \u00d72 How can we implement \nthis? Fortunately, Haskell supports the overloading of functions involving classes of related types (Pey\u00adton \nJones et al. 1997). Instance declarations resemble logic pro\u00adgrams and constraint-solving proceeds by \ntype-directed backchain\u00ading, rather like Prolog but without backtracking. It pays to think of type classes \nas a programming language (Hallgren 2001; McBride 2002). Allow me to abuse notation very slightly, giving \ndissection constraints a slightly more functional notation, after the manner of Neubauer et al. (2001): \nclass (Functor p, Bifunctor p ) . p .p | p . . p where --methods to follow In ASCII, p . p p ,but . \np is rendered relationally as Diss the annotation |p . p is a functional dependency , indicating that \np determines p , so it is appropriate to think of \u00b7 as a functional operator, even if we can t quite \ntreat it as such in practice.5 I shall extend this de.nition and its instances with operations shortly, \nbut let s start by translating our informal program into type-class Prolog : instance (K1 a) ..02 instance \nId . . 12 instance ( p . .p, q .q) . . (p +1 q) .p +2 q . .p, q .q) . .  (p \u00d71 q) .p \u00d72  instance \n( p . . p \u00d72 q q +2  Before we move on, let s just check that we get the answer we expect for our \nexpression example. (K1 Int +1 Id \u00d71 Id) .+2 Id +2 Id \u00d72 12 . 02 12 \u00d72 A bit of simpli.cation tells \nus: ExprP Int Expr ~= Expr + Int Dissection (with values to the left and expressions to the right) has \ncalculated the type of layers of our stack! 4. How to Creep Gradually to the Right If we re serious \nabout representing the state of a traversal by a dissection, we had better make sure that we have some \nmeans to move from one position to the next. In this section, we ll develop a method for the p .p class \nwhich lets us move rightward one . position at a time. I encourage you to develop the leftward move for \nyourselves. What should be the type of this operation? Consider, .rstly, where our step might start. \nIf we follow the usual trajectory, we ll start at the far left and to our right, all jokers. ..-.-.-.-.-.-. \n5 Recent unsupported extensions of GHC support associated type families , a more functional approach \nto type-level programming. I am grateful to Brandon Moore for the news that the code in this paper ports \nseamlessly. Once we ve started our traversal, we ll be in a dissection. To be ready to move, we must \nhave a clown to put into the hole. . .-.-.-.-.-.-. Now, think about where our step might take us. If \nwe end up at the next position, out will pop the next joker, leaving the new hole. .-.-.-.-.-.-. . But \nif there are no more positions, we ll emerge at the far right, all clowns. .-.-.-.-.-.-. . Putting this \ntogether, we add to class p .p the method . right :: pj +( p cj , c) . (j ,p cj )+ pc Let me show you \nhow to implement the instances of right. I shall adopt the style of polytypic programming (Jansson and \nJeuring 1997), pretending to match on the polynomial parameter as if it were a special kind of argument. \nright{-p -} :: pj +( pcj , c) . (j , pcj )+ pc Sadly, these arguments are just Haskell comments, but \nthey serve a useful documentary purpose. In particular, they show in which instance each clause belongs. \nIf you paste each clause of right{-p-}into the corresponding p . . p instance, Haskell s type class mechanism \ncan interpret each appeal to right{-p-} by compile-time recursion on p. For constants, we jump all the \nway from far left to far right in one go; we cannot be in the middle, so we refute that case. right{-K1 \na -} x = case x of L (K1 a) . R (K1 a) R (K2 z, c) . refute z We can step into a single element, or \nstep out. right{-Id x -} x = case x of L (Id j ) . L (j , K2 ()) R (K2 (), c) . R (Id c) For sums, we \nmake use of the instance for whichever branch is appropriate, being careful to strip tags beforehand \nand replace them afterwards. right{-p +1 q -} x = case x of L (L1 pj ) . mindp (right{-p -} (L pj )) \nL (R1 qj ) . mindq (right{-q -} (L qj )) R (L2 pd, c) . mindp (right{-p -} (R (pd, c))) R (R2 qd, c) \n. mindq (right{-q -} (R (qd, c))) where mindp (L (j , pd)) = L (j , L2 pd) mindp (R pc)= R (L1 pc) mindq \n(L (j , qd)) = L (j , R2 qd) mindq (R qc)= R (R1 qc) For products, we must start at the left of the \n.rst component and end at the right of the second, but we also need to make things join up in the middle. \nWhen we reach the far right of the .rst component, we must continue from the far left of the second. \nright{-p \u00d71 q -} x = case x of L (pj , qj )1 . mindp (right{-p -} (L pj )) qj R (L2 (pd, qj )2, c) . \nmindp (right{-p -} (R (pd, c))) qj R (R2 ( pc, qd)2, c) . mindq pc (right{-q -} (R (qd, c))) where mindp \n(L (j , pd )) qj = L (j , L2 (pd, qj )2) mindp (R pc) qj = mindq pc (right{-q -} (L qj )) mindq pc (L \n(j , qd)) = L (j , R2 ( pc, qd)2) mindq pc (R qc)= R (pc, qc)1 Let s put this operation straight to work. \nIf we can dissect p, then we can make its fmap operation tail recursive. Here, the jokers are the source \nelements and the clowns are the target elements. tmap :: p .p . (s . t) . ps . pt . tmap fps = continue \n(right (L ps)) where continue (L (s, pd)) = continue (right (R (pd, fs))) continue (R pt)= pt These programs \nmay seem .ddly, but in fact they re remarkably easy to write because they re precisely typed and abstract. \nBy mak\u00ading clowns and jokers separate parameters, we distinguish them from other data and from each other. \nThe types attract us towards the programs which make sense, a virtuous tendency which could only be strengthened \nby improving access to live type information during the editing process. 4.1 Tail-Recursive Catamorphism \nIf we want to de.ne the catamorphism via dissection, we could just replace fmap by tmap in the de.nition \nof (|\u00b7|), but that would be cheating! The point, after all, is to turn a higher-order recursive program \ninto a tail-recursive machine. We need some kind of stack. Suppose we have a p-algebra, f::pv . v, and \nwe re traversing a \u00b5 p depth-.rst, left-to-right, in order to compute a value in v.At any given stage, \nwe ll be processing a given node, in the middle of traversing her mother, in the middle of traversing \nher grandmother, and so on in a maternal line back to the root. .-.v-.-.-\u00b7\u00b7\u00b7-. . . . . In .- -\u00b7 \u00b7 \u00b7- \n-. .-.v-.-.-\u00b7\u00b7\u00b7-. . | In .- -\u00b7 \u00b7 \u00b7- -. .-.v-.-.-\u00b7\u00b7\u00b7-. . | In .- -\u00b7 \u00b7 \u00b7- -. We ll have visited all the \nnodes left of this line and thus have computed vs for them; right of the line, each node will contain \na \u00b5 p waiting for her turn. Correspondingly, our stack is a list of dissections: [ pv (\u00b5 p)] We start, \nready to load a tree, with an empty stack. tcata :: p .p . (pv . v) . \u00b5 p . v . tcata f t = load f t \n[] To load a node, we unpack her container of children and step in from the far left. load :: p .p \n. (pv . v) . \u00b5 p . [ . p v (\u00b5 p)] . v load f (In pt) stk = next f (right (L pt)) stk After a step, we \nmight arrive at another child, in which case we had better load her, suspending our traversal of her \nmother by pushing the dissection on the stack. next :: p .p . (pv . v) . . (\u00b5 p,p v (\u00b5 p)) + pv . [ \np v (\u00b5 p)] . v next f (L (t, pd)) stk = load f t (pd : stk) next f (R pv) stk = unload f (f pv) stk \nAlternatively, our step might have taken us to the far right of a node, in which case we have all her \nchildren s values: we are ready to apply the algebra f to get her own value, and start unloading. Once \nwe have a child s value, we may resume the traversal of her mother, pushing the value into her place \nand moving on. unload :: p .p . (pv . v) . v . [ . p v (\u00b5 p)] . v unload f v (pd : stk)= next f (right \n(R (pd, v))) stk unload f v [] = v On the other hand, if the stack is empty, then we re holding the value \nfor the root node, so we re done! As we might expect: eval :: \u00b5 ExprP . Int eval = tcata f where f (ValP \ni)= i f (AddP v1 v2)= v1 + v2 By design, dissection captures the notion of state for the left-to\u00adright \ntransformation of a (necessarily .nite) container-like struc\u00adture. As a consequence, a stack of dissections \ncaptures the state of the natural recursion over .nite trees built from such containers, turning its \ncontrol structure into data. I can imagine a number of motivations for doing this, besides mathematical \ncuriosity. Firstly, you might be programming with recursive data struc\u00adtures in a resource-aware setting, \nsuch as that of Hofmann and Jost (2003). By turning the control structure into data, you eliminate stack \nin favour of heap, bringing the necessary resources under control of the type system. If you build the \nstack by consuming the input and the output by consuming the stack, you might arrive at a rationalised \nreconstruction of the pointer reversal technique for traversing trees without fear of stack over.ow. \nSecondly, you might want your traversal process to support sus\u00adpension and resumption. Dissection makes \nthe state of a traversal into .rst-class data, bringing the schedule of the computation under much .ner \ncontrol. Thirdly, and perhaps most interestingly, your program might bene.t from manipulating its control \nstructures more directly. For example, you might handle exceptional values by discarding a chunk of stack, \nrather than propagating them layer by layer. In effect, more possibilities open when your control structure \nis not only .rst-class, but also .rst-order. Filli atre s work on backtrack\u00ading iterators (2006), also \nclosely connected with zippers, shows interesting possibilities in this direction.  5. Derivative Derived \nby Diagonal Dissection If we re interested in the possibility to manipulate .rst-order rep\u00adresentations \nof contexts, it seems appropriate to revisit the zipper. In his seminal functional pearl, Huet (1997) \nnot only shows how to represent one-hole contexts as stack-like structures, but also how to navigate \nef.ciently around a tree decomposed as the pair of a zip\u00adper and a subtree in focus. In this section, \nI ll examine the way the derivative of a functor, now a special case of dissection, gives rise to the \nzipper datatype (McBride 2001; Abbott et al. 2005b). More\u00adover, I ll show how the dissection s explicit \nleft-to-right analysis delivers Huet-style navigation. The dissection of a functor is its bifunctor of \none-hole contexts distinguishing clown elements left of the hole from joker ele\u00adments to its right. As \nwe ve already seen, the rules for computing dissections just re.ne the centuries-old rules of the differential \ncal\u00adculus with this left-right distinction. We can undo this re.nement by taking the diagonal of the \ndissection, identifying clowns with jokers. .px = pxx Let us now develop the related operations. 5.1 \nPlugging In We can add another method to class p .p, . plug ::x . p xx . px saying, in effect, that \nif clowns and jokers coincide, we can .ll the hole directly, with no need to traverse and replace, all \nthe way to the end. . .- - -.- - -. ...- - - - - -. The implementation is straightforward. As with right,the \ncom\u00adments show you which instance declaration should receive each clause. plug{-K1 a -} x (K2 z)= refute \nz plug{-Id-} x (K2 ())=Id x plug{-p +1 q -} x (L2 pd)= L1 (plug{-p -} xpd) plug{-p +1 q -} x (R2 qd )= \nR1 (plug{-q -} xqd) plug{-p \u00d71 q -} x (L2 (pd, qx )2)=(plug{-p -} xpd,qx)1 plug{-p \u00d71 q -} x (R2 ( px \n,qd)2)=(px,plug{-q -} xqd)1 5.2 Zipping Around We now have almost all the equipment we need to reconstruct \nHuet s operations, navigating a tree of type \u00b5 p for some dissectable functor p. zUp,zDown,zLeft,zRight \n:: p .p. . (\u00b5 p,[ p(\u00b5 p)(\u00b5 p)]) . Maybe (\u00b5 p,[ p(\u00b5 p)(\u00b5 p)]) Ileave zLeft as an exercise, to follow \nyour implementation of the leftward step operation, but the other three are straightforward uses of plug \nand right. This implementation corresponds quite closely to the Generic Haskell version from Hinze et \nal. (2004), but requires a little less machinery. zUp (t,[]) = Nothing zUp (t,pd :pds)=Just (In (plug \ntpd),pds) zDown (In pt,pds)=case right (L pt)of L (t,pd ). Just (t,pd :pds) R . Nothing zRight (t,[]) \n= Nothing zRight (t ::\u00b5 p,pd :pds)= case right (R (pd,t)) of L (t. ,pd) . Just (t. ,pd. :pds) R ( ::p \n(\u00b5 p)). Nothing Notice that I had to give the typechecker a little help in the de.nition of zRight. The \ntrouble is that \u00b7 is not invertible.When we say right (R (pd,t)), the type of pd is givenbysome p which \ndoes not actually determine the corresponding p, and thence the appropriate instance of \u00b7.p. I ve forced \nthe issue by collecting . p from the type of the input tree and using it to .x the type of the all clowns \nfailure case emerging from the appeal to right, thus forcing the selection of the p . . p instance in \na less than perspicuous manner. I wish I didn t have to be this devious, but there is currently no direct \nnotation for me to be explicit about which instance I want it must be inferred!  6. Division: No Clowns! \nI originally stumbled into dissection whilst trying to to .nd an operator .\u00b7 (for leftmost ) on suitable \nfunctors p which would induce an isomorphism reminiscent of the remainder theorem in algebra. ~ px =(x,.px)+p \nZero This .px is the quotient of px on division by x,and it represents whatever can remain after the \nleftmost element in a px has been removed. Meanwhile, the remainder , p Zero,represents those ps with \nno elements at all. We can see this choice as follows: 8 > (x, < . .- -\u00b7 \u00b7 \u00b7- -. . .-. -\u00b7 \u00b7 \u00b7- -. .px) \n> : .-. p Zero Certainly, the .nitely-sized containers should give us this isomor\u00adphism, but what is \n.\u00b7? It s the context of the leftmost hole. It should not be possible to move any further left, so there \nshould be no clowns! We need .px = p Zero x For the polynomials, we shall certainly have divide :: \np .p. px . (x, . pZero x)+p Zero divide px =right (L px ) To compute the inverse, I could try waiting \nfor you to implement the leftward step: I know we are sure to reach the far left, for your only alternative \nis to produce a clown! However, an alternative is at the ready. I can turn a leftmost hole into any old \nhole if I have6 in.ateFst ::Bifunctor p . p Zero y . pxy in.ateFst =bimap refute id Now, we may invert \ndivide as follows: p .p. (x, . pZero x)+p Zero . px unite (L (x,pl))=plug x (in.ateFst pl) unite (R \npz)=in.ate pz unite :: It is straightforward to show that divide and unite are mutually inverse by induction \non polynomials. To see why dissection is a necessary precursor to division, think about dividing a composition.The \nleftmost x in a p (qx) might not be in a leftmost p position: there might be q-leaves to the left of \nthe q-node containing the .rst element. For example, (Id Nothing,Id (Just x))1 ::(Id \u00d71 Id)(Maybe x) \nhas its leftmost element in the second component. We need to be able to express the idea that we have \nonly q-leaves left of the p\u00adhole, which could be anywhere, but with different stuff to the left and to \nthe right. By generalising to dissection, we get the correct behaviour for composition the chain rule. \n (p .1 q)= q \u00d72 ( p).2 ( q; q) where data (p .2 (q;r)) cj =(p (qcj )(rcj )).2 (\u00b7;\u00b7) That is, we have \na dissected p, with clown-.lled qs left of the hole, joker-.lled qs right of the hole, and a dissected \nq in the hole. If you specialise this to division, you get ~ .(p .1 q)x =.qx \u00d7 p (q Zero)(qx) which \nexactly captures the leaves left of the hole intuition. Let us now put division and dissection to work! \n 7. Generic Generalisation By way of a .nale, let me present a more realistic use-case for dissection, \nwhere we exploit the .rst-order representation of the context by inspecting it in the course of a recursive \ncomputation. The task is to implement a generalisation mechanism, transforming an expression by replacing \nall occurrences of a given subexpression by a variable. This is a common technique in proof by induction: \ngeneralisation strengthens inductive hypotheses. The Coq proof assistant, for example, has a tactic for \ngeneralsation. Let us now 6 Again, in some systems in.ateFst can effectively be replaced by a cast. develop \nan ef.cient generalisation algorithm for a generic .rst\u00adorder syntax. 7.1 Free Monads and Substitution \nWhat is a generic .rst-order syntax ? A standard way to get hold of such a thing is to de.ne the free \nmonad p * of a (container-like) functor p (Barr and Wells 1984). data p * x =V x |C (p (p * x)) Theideais \nthat p represents the signature of constructors in our syntax, just as it represented the constructors \nof a datatype in the \u00b5 p representation. The difference here is that p * x also contains free variables \nchosen from the set x. The monadic structure of p * is that of substitution. instance Functor p .Monad \n(p * )where return x =V x V x > =s =s x C pt > =s =C (fmap (> =s)pt) Here > =is the simultaneous substitution \nfrom variables in one set to terms over another. However, it s easy to build substitution for a single \nvariable on top of this. Following Bird and Paterson (1999), we can use Maybe x to represent a variable \nset which distinguishes a new, bound variable Nothing from old, free variables Just x. Let us rename \nMaybe to S, successor , for this purpose. We may readily eliminate the bound variable by instantiating \nit with a term constructed over the free variables, as follows: type S =Maybe (.)::Functor p .p * (S \nx) .p * x .p * x t . s =t > =s where s Nothing =s s (Just x)=V x Generalisation can now be seen as the \ntask of computing the most abstract inverse to (.s). That is, for suitable p and x, we need some *** \n(.)::... .px .px .p (S x) such that (t . s). s = t, and moreover that fmap Just s occurs nowhere in t \n.s. In order to achieve this, we ve got to abstract every occurrence of s in t as V Nothing and apply \nJust to all the other variables. Taking t . s =fmap Just t is de.nitely wrong!  7.2 Indiscriminate Stop-and-Search \nThe obvious approach to computing t . s is to traverse t checking everywhere if we ve found s. *** (S \nx) (.)::(Functor p, PresEq p, Eq x).px .px .p t . s |t =s =V Nothing V x . s =V (Just x) C pt . s =C \n(fmap (.s) pt) Here, I m exploiting Haskell s Boolean guards to test for a match at the root. I write \n=for Haskell s Boolean equality test (== in ASCII), which is available for types in the Eq class. Only \nif the match fails do we fall through and try to search more deeply inside the term. How do we know we \ncan test equality of terms? We .rst must con.rm that our signature functor p preserves equality, i.e., \nthat we can lift equality eq on x to equality \u00b7.eq.\u00b7on px. instance (PresEq p, Eq x) .Eq (p * x) where \nV x =V y =x =y C ps =C pt =ps .=.pt = =False Lifting equality is quite mechanical. The only interesting \ncase is for sums, where structural difference is actually possible. class PresEq p where \u00b7.\u00b7.\u00b7::(x .x \n.Bool).px .px .Bool instance Eq a .PresEq (K1 a)where K1 a1 .eq.K1 a2 =a1 =a2 instance PresEq Id where \nId x1 .eq.Id x2 =eq x1 x2 instance (PresEq p, PresEq q).PresEq (p +1 q)where L1 p1 .eq.L1 p2 =p1 .eq.p2 \nR1 q1 .eq.R1 q2 =q1 .eq.q2 .eq. =False instance (PresEq p, PresEq q).PresEq (p \u00d71 q)where (p1,q1)1 .eq.(p2,q2)1 \n= p1 .eq.p2 .q1 .eq.q2 Our .rst attempt at generalisation is short and obviously correct, but it s rather \ninef.cient. If s is small and t is large, we shall repeatedly compare s with terms which are far too \nlarge to stand a chance of matching. We search for s s root everywhere, whether or not its leaves reach \nthe edge, as shown below. t s? It s rather like testing if a list xs has suf.x ys like this. hasSu.x \n::Eq x .[x ] .[x ].Bool hasSu.x xs ys |xs =ys =True hasSu.x [] ys =False hasSu.x (x :xs)ys =hasSu.x xs \nys If we ask hasSu.x \"xxxxxxxxxxxx\" \"xxx\", we shall test if x = x thirty times, not three. It s more \nef.cient to reverse both lists and check once for a pre.x. With fast reverse, this takes linear time. \nhasSu.x ::Eq x .[x ] .[x ].Bool hasSu.x xs ys = hasPre.x (reverse xs)(reverse ys) hasPre.x ::Eq x .[x \n] .[x ] .Bool hasPre.x xs [] = True hasPre.x (x :xs)(y :ys)|x =y =hasPre.x xs ys hasPre.x =False  7.3 \nHunting for a Needle in a Stack We can adapt the reversal idea to our more arboreal problem. The divide \nfunction tells us how to .nd the leftmost position in a polynomial container, if it has one. By iterating \ndivide, we can navigate our way down the left spine of a term to its leftmost leaf, stacking the contexts \nas we go. That s a way to reverse a tree! A leaf is either a variable or a constant. A term either is \na leaf or has a leftmost subterm. To see this, we just need to adapt divide for the possibility of variables. \ndata Leaf px =VL x |CL (p Zero) leftOrLeaf :: p .p .. ** * px .(px,p Zero (px))+Leaf px leftOrLeaf (V \nx)=R (VL x) leftOrLeaf (C pt)=fmap CL (divide pt) Now we can reverse the term we seek into the form \nof a needle the leftmost leaf with a straight spine of leftmost holes running all the way back to the \nroot, as shown schematically, and in detail: .-. -\u00b7\u00b7\u00b7- -. . . . .-. -\u00b7\u00b7\u00b7- -. . given by .-. -\u00b7\u00b7\u00b7- -. \n z }| { V x C .-. p .p .p * p Zero (p * x)]) . x .(Leaf px, [ needle t =grow t [] where grow tpls =case \nleftOrLeaf t of L (t. , pl).grow t. (pl :pls) R l .(l, pls) needle :: Given this needle representation \nof the search term, we can im\u00adplement the abstraction as a stack-driven traversal, hunt (below), which \ntries for a match only when it reaches a suitable leaf. As our point of focus corresponds to a leaf in \ns, it s now easy to rule out internal positions in t: t s? No! Moreover, we need only check for our \nneedle when we re stand\u00ading at the end of a left spine at least as long. tt  s?No! Let us therefore \nsplit our state into an inner left spine and an outer stack of dissections. (.)::( p .p, PresEq p, PresEq2 \n. p, Eq x) . *** px .px .p (S x) t . s =hunt t [][] where (neel, nees)=needle s hunt tspi stk =case \nleftOrLeaf t of L (t. , pl).hunt t (pl :spi) stk R l .check spi nees (l =neel) where check =\u00b7\u00b7\u00b7 Haskell \ns restricted technology for type annotations makes it hard for me to write hunt s type in the code. Informally, \nit s this: hunt ::p * x . --term in focus [.p (p * x)] . --local spine [ p (p * (S x))(p * x)] . --stack \nto root p * (S x) Now, check is rather like hasPre.x, except that I ve used a lazy accumulator to ensure \nthat the expensive equality tests for the rest of the term are evaluated only as soon as we know that \nthe spine is at least as long as the needle. check spi. [] True = next (V Nothing)(fmap in.ateFst spi. \n+ stk) check (spl :spi)(npl :nees ) b = check spi. nees . (b .spl .refute |=.npl) check =next (leafS \nl)(fmap in.ateFst spi + stk) where leafS (VL x)=V (Just x) leafS (CL pz )=C (in.ate pz ) For the equality \ntests we need \u00b7.\u00b7|\u00b7.\u00b7, the bifunctorial analogue of \u00b7.\u00b7.\u00b7, although as we re working with .p, we can \njust use refute to test equality of clowns. The same trick works for Leaf equality: instance (PresEq \np, Eq x).Eq (Leaf px)where VL x =VL y = x =y CL a =CL b = a .refute.b = = False Now, instead of returning \na Bool, check must explain how to move on. If our test succeeds, we must move on from our matching subterm \ns position, abstracting it: we throw away the matching pre.x of the spine and stitch its suf.x onto the \nstack to stitch, just in.ate the spine to a stack, then append. However, if the test fails, we must move \nright from the current leaf s position, injecting it into p * (S x)and stitching the original spine to \nthe stack. Correspondingly, next tries to move rightwards given a new term and a stack. If we can go \nright, we get the next old term along, so we start hunting again with an empty spine. next t. (pd :stk)=case \nright{-p -}(R (pd, t))of L (t, pd).hunt t [] (pd. :stk) R pt. .next (C pt)stk next t. [] = t. If we \nreach the far right of a p, we pack it up and pop on out. If we run out of stack, we re done!  8. Discussion \nThe story of dissection has barely started, but I hope I have com\u00admunicated the intuition behind it and \nsketched some of its poten\u00adtial applications. Dissection is the structure which supports nav\u00adigation \nof .rst-order, .rst-class contexts, for more .exible man\u00adagement of both data and control in a purely \nfunctional setting. In my other work implementing typecheckers for interactive pro\u00adgramming environments \ndissection-based control structures are invaluable in managing what is effectively a process of term \ntraver\u00adsal interruptable non-locally by fresh information at any time. On a more theoretical note, what \ns clearly missing here is a se\u00admantic characterisation of dissection, with respect to which the op\u00aderational \nrules for p may be justi.ed. It is certainly straightfor\u00adward to give a shapes-and-positions analysis \nof dissection in the categorical setting of containers (Abbott et al. 2005a), much as we did with the \nderivative (Abbott et al. 2005b). The basic point is that where the derivative requires element positions \nto have decidable equality ( am I in the hole? ), dissection requires a total order on positions with \ndecidable trichotomy ( am I in the hole, to the left, or to the right? ). The induced notion of division \ncan be used to calculate power series representations for data structures, establishing a signi.cant \nconnection with the notion of combinatorial species as studied by Joyal (1986) and others. The details, \nhowever, deserve a paper of their own. I have shown dissection for polynomials here, but it is clear \nthat we can go further. For example, the dissection of list gives a list of clowns and a list of jokers: \n []= []\u00d72 [] Moreover, if p has a dissection, it is an interesting exercise to con\u00adstruct the dissection \nof its free monad p * . However, if we want to address more complex phenomena, such as mutually recur\u00adsive \ndatatypes (requiring multiple parameters) or iterated dissec\u00adtion (representing multiple holes), we shall \nrapidly reach the limits of the Haskell techniques I ve shown here. But it s a delight that Haskell allows \nus to come even this close to implementing the general pattern, rather than its individual instances. \nIn principle, and in dependently typed practice, the whole de\u00advelopment extends readily to the multivariate \ncase. The general i dissects a mutli-sorted container at a hole of sort i, and splits all the sorts into \nclown-and joker-variants, doubling the arity of its parameter. The corresponding .i .nds the contexts \nin which an el\u00adement of sort i can stand leftmost in a container. This generalises Brzozowski s (1964) \nnotion of the partial derivative of a regular expression, with the set of sorts corresponding to the \nalphabet. But if there is a broader message for programmers and program\u00adming language designers here, \nit is this: the miserablist position that types exist only to police errors is thankfully no longer sustainable, \nonce we start writing programs like this. By permitting calculations of types and from types, we discover \nwhat programs we can have, just for the price of structuring our data. What joy!  Acknowledgments I \nwish to thank EPSRC for funding this research through grants EP/C512022/1 and EP/C511964/1. This work \nhas bene.ted con\u00adsiderably from the criticism and advice of many people. I should like to express my \nwarmest gratitude to Thorsten Altenkirch, Lucas Dixon, Peter Hancock, Dan Piponi, Tarmo Uustalu, Philip \nWadler, and the anonymous referees. References Michael Abbott, Thorsten Altenkirch, and Neil Ghani. \nContainers -constructing strictly positive types. Theoretical Computer Sci\u00adence, 342:3 27, September \n2005a. Applied Semantics: Selected Topics. Michael Abbott, Thorsten Altenkirch, Neil Ghani, and Conor \nMcBride. . for data: derivatives of data structures. Fundamenta Informaticae, 65(1&#38;2):1 28, 2005b. \nMads Sig Ager, Dariusz Biernacki, Olivier Danvy, and Jan Midt\u00ad gaard. A functional correspondence between \nevaluators and ab\u00ad stract machines. In PPDP, pages 8 19. ACM, 2003. William Aitken and John Reppy. Abstract \nvalue constructors. Technical Report TR 92-1290, Cornell University, 1992. Michael Barr and Charles Wells. \nToposes, Triples and Theories, chapter 9. Number 278 in Grundlehren der Mathematischen Wissenschaften. \nSpringer, New York, 1984. Richard Bird and Oege de Moor. Algebra of Programming.Pren\u00adtice Hall, 1997. \nRichard Bird and Ross Paterson. de Bruijn notation as a nested datatype. Journal of Functional Programming, \n9(1):77 92, 1999. Janusz Brzozowski. Derivatives of regular expressions. Journal of the ACM, 11(4):481 \n494, 1964. Jean-Christophe Filli atre. Backtracking iterators. Technical Report 1428, CNRS-LRI, January \n2006. Jeremy Gibbons. Datatype-generic programming. In Roland Back\u00adhouse, Jeremy Gibbons, Ralf Hinze, \nand Johan Jeuring, editors, Spring School on Datatype-Generic Programming, volume 4719 of Lecture Notes \nin Computer Science. Springer-Verlag, 2007. To appear. Thomas Hallgren. Fun with functional dependencies. \nIn Joint Winter Meeting of the Departments of Science and Computer Engineering, Chalmers University of \nTechnology and Goteborg University, Varberg, Sweden., January 2001. Ralf Hinze, Johan Jeuring, and Andres \nL\u00a8oh. Type-indexed data types. Science of Computer Programmming, 51:117 151, 2004. Martin Hofmann and \nSteffen Jost. Static prediction of heap space usage for .rst-order functional programs. In Proceedings \nof the 30th ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (POPL), pages 185 197. \nACM, 2003. G\u00b4erard Huet. The Zipper. Journal of Functional Programming,7 (5):549 554, 1997. Patrik Jansson \nand Johan Jeuring. PolyP a polytypic program\u00adming language extension. In Proceedings of POPL 97, pages \n470 482. ACM, 1997. Andr\u00b4e Joyal. Foncteurs analytiques et esp\u00b4eces de structures. In Combinatoire \u00b4enum\u00b4erative, \nnumber 1234 in LNM, pages 126 159. 1986. Conor McBride. The Derivative of a Regular Type is its Type \nof One-Hole Contexts. Available at http://www.cs.nott.ac. uk/~ctm/diff.pdf, 2001. Conor McBride. Faking \nIt (Simulating Dependent Types in Haskell). Journal of Functional Programming, 12(4&#38; 5):375 392, \n2002. Special Issue on Haskell. Matthias Neubauer, Peter Thiemann, Martin Gasbichler, and Michael Sperber. \nA Functional Notation for Functional De\u00adpendencies. In The 2001 ACM SIGPLAN Haskell Workshop, Firenze, \nItaly, September 2001. Simon Peyton Jones, Mark Jones, and Erik Meijer. Type classes: an exploration \nof the design space. In Proceedings of the Haskell Workshop, Amsterdam, The Netherlands, June 1997. Philip \nWadler and Stephen Blott. How to make ad-hoc polymor\u00adphism less ad hoc. In 16th ACM Symposium on Principles \nof Programming Languages, pages 60 76. ACM, January 1989.  \n\t\t\t", "proc_id": "1328438", "abstract": "<p>This paper introduces a small but useful generalisation to the 'derivative' operation on datatypes underlying Huet's notion of 'zipper', giving a concrete representation to one-hole contexts in data which is undergoing transformation. This operator, 'dissection', turns a container-like functor into a bifunctor representing a one-hole context in which elements to the left of the hole are distinguished in type from elements to its right.</p> <p>I present dissection here as a generic program, albeit for polynomial functors only. The notion is certainly applicable more widely, but here I prefer to concentrate on its diverse applications. For a start, map-like operations over the functor and fold-like operations over the recursive data structure it induces can be expressed by tail recursion alone. Further, the derivative is readily recovered from the dissection. Indeed, it is the dissection structure which delivers Huet's operations for navigating zippers.</p> <p>The original motivation for dissection was to define 'division', capturing the notion of leftmost hole, canonically distinguishing values with no elements from those with at least one. Division gives rise to an isomorphism corresponding to the remainder theorem in algebra. By way of a larger example, division and dissection are exploited to give a relatively efficient generic algorithm for abstracting all occurrences of one term from another in a first-order syntax. The source code for the paper is available online and compiles with recent extensions to the Glasgow Haskell Compiler.</p>", "authors": [{"name": "Conor McBride", "author_profile_id": "81100120358", "affiliation": "University of Nottingham, Nottingham, United Kingdom", "person_id": "PP43135912", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1328438.1328474", "year": "2008", "article_id": "1328474", "conference": "POPL", "title": "Clowns to the left of me, jokers to the right (pearl): dissecting data structures", "url": "http://dl.acm.org/citation.cfm?id=1328474"}