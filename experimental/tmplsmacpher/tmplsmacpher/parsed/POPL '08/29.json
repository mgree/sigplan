{"article_publication_date": "01-07-2008", "fulltext": "\n Focusing and Higher-Order Abstract Syntax Noam Zeilberger Carnegie Mellon University noam@cs.cmu.edu \nAbstract Focusing is a proof-search strategy, originating in linear logic, that elegantly eliminates \ninessential nondeterminism, with one byproduct being a correspondence between focusing proofs and programs \nwith explicit evaluation order. Higher-order abstract syn\u00adtax (HOAS) is a technique for representing \nhigher-order program\u00adming language constructs (e.g., . s) by higher-order terms at the meta-level , therebyavoiding \nsomeof thebureaucratic headaches of .rst-order representations (e.g., capture-avoiding substitution). \nThis paper begins with a fresh, judgmental analysis of focus\u00ading for intuitionistic logic (with a full \nsuite of propositional con\u00adnectives), recasting the derived rules of focusing as iterated in\u00adductive \nde.nitions. This leads to a uniform presentation, allowing concise, modular proofs of the identity and \ncut principles. Then we show how this formulation of focusing induces, through the Curry-Howard isomorphism, \na new kind of higher-order encoding of ab\u00adstract syntax: functions are encoded by maps from patterns \nto ex\u00adpressions. Dually, values are encoded as patterns together with ex\u00adplicit substitutions. This gives \nus pattern-matching for free , and lets us reason aboutarich type system with minimal syntacticover\u00adhead.We \ndescribehowto translatethe languageandproofoftype safety almost directly into Coq using HOAS, and .nally, \nshow how the system s modular design pays offin enabling a very simple ex\u00adtension with recursion and \nrecursive types. Categories and Subject Descriptors D.3.1[Programming Lan-guages]:Formal De.nitions and \nTheory; F.4.1[Theory of Com\u00adputation]: Mathematical Logic Lambda calculus and related sys\u00adtems General \nTerms Languages 1. Introduction The end result of this paper will be to show how so-called focus\u00ading \nproofs produce througha careful judgmental analysis and the Curry-Howard isomorphism an exceptionally \ncompact presenta\u00adtion of a call-by-value language with a full suite of types. In the process, we hope \nto convince the reader of an aphorism: abstract syntax should be even more abstract. The technique of \nfocusing was originally invented by Andreoli (1992) as a re.nement of bottom-up proof search in linear \nlogic, to reduce an otherwise intractable amount of nondeterminism. Soon afterwards, it was promoted \nby Girard (1993) as a conceptual tool Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage.To copyotherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. POPL 08 January 7 12, 2008, San Francisco, California, USA. Copyright c&#38;#169; \n2008ACM 978-1-59593-689-9/08/0001...$5.00 for .nding unity in logic, as it turned out that also the classical \nand intuitionistic connectives could be classi.ed by their focus\u00ading behavior, or polarity. Recently, \nfocusing and polarity have seen a surge in interest as more and more surprising properties of fo\u00adcusing \nproofs are discovered, including one important example: it is slowly becoming clear that focusing proofs \ncorrespond to pro\u00adgrams with explicit evaluation order (Herbelin 1995; Curien and Herbelin 2000; Selinger \n2001; Laurent 2002; Wadler 2003; Lau\u00adrent 2005; Dyckhoff and Lengrand 2006). In this paper we will demonstrate \nan additionalfascinatingfact about focusing proofs: they correspond to programs with pattern-matching. \nMoreover, it turns out that focusing canbegivena uniform, higher-order formu\u00adlation as an iterated inductive \nde.nition, and that this representation can be encoded naturally in Coq (Martin-L\u00a8of 1971; Coquand and \nPaulin-Mohring 1989). Combining thesefacts, we obtain the above aphorism: a new kind of higher-order \nabstract syntax that encodes pattern-matching for free . 2. Focusing intuitionistic logic 2.1 Background \nBefore diving into the compact presentation of focusing and its Curry-Howard interpretation ` alaHOAS,letus \nstarton morefamil\u00adiar ground with a standard intuitionistic sequent calculus, and de\u00adscribehowto obtaina \nsmall-step focusing system. Figure1gives the sequent calculus for intuitionistic logic in a slight variation \nof Kleene s G3i formulation (Kleene 1952;Troelstra and Schwicht-enberg1996).Formulas(P, Q, R)arebuilt \nout of conjunction(\u00d7) and disjunction(+)and their respectiveunits(1and0), implication (.), and logical \natoms(X, Y, Z). Every logical connective has a pair of a left rule and right rule(s) (we omit the rules \nfor the units to save space). The identity rule is restricted to atoms and there is no explicit cut rule, \nthough both cut (from G f P and G,P f Q conclude G f Q)and the general identity principle(P . G implies \nG f P )are admissible. Now, one way to conceive of the sequent calculus, as Gentzen (1935) originally \nsuggested, is as a proof search procedure. Each rule can be read bottom-up as a prescription, To prove \nthe con\u00adclusion, try proving the premises . Starting from a goal sequent G f P ,one attemptstobuildaproofbyinvokingleft-andright\u00adrules \nprovisionally to obtain a new set of goals until, hopefully, all goals can be discharged using rules \nwith no premises (i.e., id, 1R or 0L). Since there are only .nitely manyrules and each satis.es the subformula \nproperty (Troelstra and Schwichtenberg1996), it is not hard to see that (so long as one checks saturation \nconditions to avoid repeatedly applying left rules) the sequent calculus gives a naive decision procedure \nfor propositional intuitionistic logic. The reason this decision procedure is naive, though, is because \nthe order of application of rules is left entirely unspeci.ed. For example, the following are two equally \nlegitimate derivations of X \u00d7 Y f X \u00d7 Y , that differ only in the order of \u00d7L and \u00d7R: G ::= \u00b7 | G, P \nStable context G ::= \u00b7 | G, X | G, P .vQ Active context O ::= \u00b7 | P, O X . G G f X id G, P f Q G f P \n. Q .R G f P G f Q G f P G f Q X . G G; P f Q Context G f [P ] \u00d7R +R G f [P . P \u00d7 Q . GG, P, Q f R vffffG[P \n]G[Q]G[P ]G[Q] .ffPQ GG,P R G,Q R+ \u00d7L fG R f.f.f.G[PQ]G[PQ]G[PQ] v +L G; O f R G f R G,P . G f P \u00d7 Q \nG f P + Q G f P + Q G f [X] Q] Q;O f R G,X;O f R P . Q . GG f P G,Q f R .L G; X, O f R v G; P .G; P, \nQ, O f R G; P, O f R G; Q, O f R Figure 1. Intuitionistic sequent calculus G; P . Q, O f R G; P . Q, \nO f R G f R G; \u00b7f R id id X \u00d7 Y, X, Y f XX \u00d7 Y, X, Y f Y \u00d7R Q, O f R G f R G f R v P . X \u00d7 Y f X \u00d7 Y \nG f P G f R id id X \u00d7 Y, X, Y f XX \u00d7 Y, X, Y f Y Figure 2. Focused intuitionistic sequent calculus \u00d7L \n\u00d7L X \u00d7 Y f XX \u00d7 Y f Y \u00d7R X \u00d7 Y, X, Y f X \u00d7 Y Q . GG f [P ] G; Q f R G f [P ] \u00d7L X \u00d7 Y f X \u00d7 Y However,it \nis not the case that order of application is arbitrary. For example, to prove X + Y f X + Y ,one must \napply +L .rst (from the bottom): id id X + Y, X f XX + Y, Y f Y +R +R X + Y, X f X + YX + Y, Y f X + \nY +L X + Y f X + Y Applying either right-rule .rst will yieldafailed proof attempt. In these terms, focusing \ncan be seen as exploiting properties about the connectives to implement a smarter bottom-up proof search. \nFigure2presentsa focusing system for intuitionistic logic that implements the following strategy: 1. \nDecompose conjunctions and disjunctions greedily on the left, until the context contains only atoms and \nimplications. 2. Given a stable sequent (i.e., one with no undecomposed hy\u00ad  for implication.1Very \nsimilar focusing systems based on vwillbe. write .the same polarizations are presented in (Girard 2001, \n\u00a79.2.3) and (Dyckhoff and Lengrand 2006). Some examples of focusing sys\u00adtems derived from alternative \n(lazy, call-by-name) polarizations of intuitionistic logic are in (Herbelin 1995; Howe 1998; Miller and \nLiang 2007). Of course, from the point of view of proof-search, it is crucial that anyfocusing strategy \nbe complete, i.e., if a sequent is provable in the ordinary sequent calculus, then the focusing strategy \nwill succeed in .nding some derivation.Wewill notgiveacompleteness proof for this system here (the reader \ncould refer to (Dyckhoffand Lengrand 2006)), and instead move on to describe an alternative presentation \nof focusing.  2.2 A higher-order formulation Let us begin with some observations about derived rules \nin the focused system. These observations are not new (Andreoli 2001; Girard 2001) but the system we \nobtain from these observations potheses), focus on some proposition(G f [P ]), either the right side \nof the sequent or the antecedent of a hypothesis v Consider proving the proposition X .(Y . (P .Thederivationmustbegininoneofthefollowingtwoways, \nbefore Q)) in focus. P . Q. losing focus: 3. A proposition in focus remains in focus (forcing us to keep \n vv vcontrastwiththeconnectivesoflinearlogic,whichhave.xedpo-.fX GG; PQ..X G Y G vv G f [P .Q] G f [X]G \nf [Y . (Q .P )] G f [X]G f [Y . (P .Q)] G f [X . (Y . (Q .G f [X . (Y . (P . Note that this is not the \nonly possible focusing strategy for propo-Once in focus, atomic propositions can only be proven by assump\u00adsitional \nintuitionistic logic. Most of the intuitionistic connectives tion, while implications initiate a decomposition \nphase. The set of haveambiguous polarity, in Girard ssense (Girard 1993). This is in derived rules G \nf [Y ] applying right-rules) until either there are no more premises, or else we reach an implication, \nwhich blurs the sequent (and we go back to step 1). P ))] Q))] larity. So whereas there is essentially \nonly one way to focus linear logic, there are different possible strategies for intuitionistic logic, \ncorresponding to different polarizations. Our strategy treats con\u00ad v v G f [X . (Y . (P .G f [X . (Y \n. (P . 1Describing the polarity of call-by-value implication is actually a bit more Q))] Q)))] junction \nand disjunction as both positive, implication as negative, subtle. Technically, one can identify an underlying \nnegative implication which turns out to correspond, via Curry-Howard, to the strict, call- P -. N which \ntakes positive antecedent and negative consequent, and then by-value interpretation (Curien and Herbelin \n2000; Selinger 2001; vtationforpositiveconjunctionanddisjunction(,1,0),and conclusion)..., analyze P \n.either as P -. .Q (asa negativehypothesis) or .(P -. .Q) (as a positive Q with implicit polarity shifts \n(Girard 2001, \u00a73.3.2), i.e., Laurent 2005).To emphasize thisfact, we adopt linear logic no\u00ad ::= \u00b7| X, \n. | P . Similarly, consider decomposing X . (Y . (P .left of the sequent: X . XP .Q . P . v vv v Linear \ncontext is therefore complete, in the sense that it covers all possible deriva\u00ad . Q, . tions of the formula \nin right-focus. Q)) on the . . P Q G, X, P .G, X, P .Q; \u00b7f R vv Q f RG, X, Y f R G, X, Y ; \u00b7f R .1 . \nP .2 . Q \u00b7. 1.1, .2 . P . Q G,X; P .Q f R . . Q. . P G,X; Y . (P .Q) f R (no rule for0) . . P . Q G; \nX, Y . (P .Q) f R v v v G,X; Y f R . . P . Q .................................................................... \n G; X . (Y . (P . Again, the following derived rule is complete: v Q)) f R Stable context G ::= \u00b7| G, \n. G f [P ] G, X, P .G; X . (Y . (P .Q)) f R G f [P ] vv G, X, Y f R Q f R . . P G f . v In general \nfor a proposition P , we can give a complete set G; P f Q (possibly empty) of derived rules for establishing \nG f [P ], each .(. . P ): G, . f Q containing a set (possibly empty) of premises of the form X . G or \nG; Q f R. Likewise, we cangivea single, complete derived rule G; P f Q for establishing G; P f R, with \na set (possibly empty) of premises G f . of the form G, G' f R. X . GG f . G; P f Q G f . G f P . Both \nkinds of derived rules for a formula P can be generated G f\u00b7 G f X, . Q, . from a single description, \nwhich we can gloss as the possible recipes fora focused proof.To derive G f [P ], we must provide (using \nG)all of the ingredients forsome recipe.To derive G; P f G f R R, we must showhow to derive R given(G \nand) the ingredients for any of the recipes. As we are trying to suggest by using culinary language(Wadler \n1993), the method for constructing both kindsof derived rules can be expressed in terms of linear entailment. \nMore precisely, a list of ingredients . is a linear context of atoms and implications, and we write . \n. P when . exactly describes the focused premises in a possible focused proof of P . The rules for . \n. P (topof Figure3) arejustthe usual right-rules for the positive connectives of linear logic together \nwith axioms v P .G f P G f R G f [P ] G; Q f R Q . G G f [P ] Figure 3. Large-step focusing there is \nno circularity in treating it as an assumption here. Indeed, for any particular P built out of the connectives \nwe have consid\u00adered, there will only ever be .nitely manyderivations . . P , so vNotethatthejudgment \nobeysasubformulaproperty. .. P v X . X and P .Q . P . v X, Y . X.(Y .(P .Q)) and X, P .Q . X.(Y .(P . \nv v Q. By way of example, we have this rule will just have a .nite list of premises (as in the example \nQ)). above). However, we hope to make the case that this higher-order formulation should be taken at \nface value interpreted construc\u00adtively, it demands a mapping from derivations of . . P to unfo\u00adcused \nsequents G, . f Q. This idea will play a central role in our Proposition (Subformula property). If . \n. P , then . contains only subformulas of P . Curry-Howard interpretation. The generic instructions for \nproving a proposition in focus, The entire large-step focusing system is given in Figure 3, which we \ndescribed informally above, can nowbe written formally: with allof the rules for allof the connectives \n(including the units1 . . P G f . G f [P ] The judgment G f . is interpreted conjunctively:2 from thehy\u00adpotheses \nin G we must prove everything in .. Thus the rule asks for some choice of recipe (i.e., . . P ), and \na proof that we have all the ingredients (i.e., G f .). Note that although this rule leaves . unspeci.ed, \nit still obeys the usual subformula property, by the subformula property for . . P . Likewise, we can \nwrite the generic rule for decomposing a proposition on the left: .(. . P ): G, . f Q G; P f Q Here the \nrule quanti.es over all . such that . . P , showing that from anysuch . (together with G), Q is derivable. \nThis sort of quanti.cation over derivations might seem like a riskyform of de.nition,butitis simplyan \niterated inductive de.nition (Martin\u00adL\u00a8of 1971) since we already established what . . P means, 2And so \nis not like a multiple conclusion sequent in Gentzen s LK. v and 0). Observe that the only rules that \nexplicitly mention the pos\u00aditive connectives are those for the . . P judgment, and we can take the latter \nas literally de.ning the positive connectives. While the system is relatively sparse in rules, it is \nrich in judgments . The idea of the judgmental method (Martin-L\u00a8of 1996; Pfenning and Davies 2001) in \ngeneral is that by distinguishing between differ\u00adent kinds of reasoning as different judgments (and not \nmerely be\u00adtween different logical connectives or type constructors), one can clarify the structure of \nproofs. This becomes very vivid under a Curry-Howard interpretation, as the proofs of different judgments \nare internalized by different syntactic categories of a programming language.Wewill.ndthatthe.vejudgmentsoflarge-step \nfocus\u00ading all correspond to very natural programming constructs. First, though, let us see how the identity \nand cut principles work in this new logical setting. Because of the additional judgmental machin\u00adery, \nidentity is re.ned into three different principles. Principle (Identity). G; P f P Principle (Context \nidentity). If G . . then G f . If P . Principle (Arrow identity). Q . G then G; P f Q Proof. These three \nprinciples are proven simultaneously we give Proof of substitution, reduction and composition. Again, \nthe proof the proof .rst, and then explain its inductive structure. is simultaneous. (Identity) The following \nderivation reduces identity to context (Substitution) We examine the derivation of G, ., G ' f J. identity: \nAlmost all cases (there are seven total) are immediate, simply applying substitution (and possibly weakening) \nto the premises . . P G, . f . and reconstructing the derivation. The one interesting case is G, . f \n[P ] the following: .(. . P ): G, . f P G, ., G ' f [P ]G, ., G ' P .G, ., G ' f R Note the .rst premise \ncan be discharged since the derivation quanti.es over . such that . . P . By substitution on the premises \nwe have G, G ' f [P ] and v Q . . ; Q f R G; P f P (Context Identity)We applya side-induction on the \nlengthof G, G ' ; Q f R. Moreover G, G ' v f . and P .Q, . '. By arrow identity (by inversion) that \nG, G ' ; P f Q. We cut G, G ' Q . . imply P .we have G; P f Q, and by the side-induction we have G f \n. ' , G, G ' ; P f Q using reduction to obtain G, G ' f Q,and the latter letting usbuild the derivation: \nwith G, G ' ; Q f R using composition to obtain G, G ' f R. (Reduction)Byinversion on G f [P ],thereexists \nsome. . P G; P f Q G f . ' such that G v .. The interesting case is . [P ] and f = f ., and by inversion \non G; P f Q we have Q, . ' G f P . (Arrow Identity) Consider the following derivation: v G, . f Q. Hence \nG f Q by substitution. (Composition) We examine the derivation of G f P . If it was derived from G f \n[P ], we immediately apply reduction. P ..(. . P ): G, . f Q .(. . P2): G, . f P v Q . GG, . f [P ]G, \n.; Q f Q Otherwise the derivation must look like so: G; P f Q v P1 .Q . G is by assumption. The second \nG f P P2 . GG f [P1] G; P2 f P The .rst premise P .premise reduces (as in the proof of identity above) \nto context identity. The third premiseisby identity. The above argument can be seen to be well-founded \nso long as the relationship of being a proper subformula is well-founded. We reason as follows: The proof \nof identity appealed to context identity, which in turn appealed to arrow identity, and which .nally \nappealed back to both context identity and identity. The .rst cycle v P2 . . For any. . P2, we can weaken \nthe derivation G; P f Q to G, .; P f Q, and then apply composition to obtain G, . f Q. Thus G; P2 f Q, \nand we can reconstruct the derivation concluding G f Q. The above de.nes a cut-elimination procedure, \nwhich we can eas\u00adily see is terminating by a nested induction. First on the cut for\u00admula/context, then \non the second derivation for substitution, and v (id on P . context id on . . P . arrow id on P1 . . \nid on P2), takes P to a proper subformula P2.The secondcycle onthe .rstderivationfor composition.Again,this \nusesthefactthat v (context id on . . arrow id on P1 .. ' . P1), takes . to a proper subcontext . ' (i.e., \n. ' contains only proper subformulas of formulas in .). Both cycles cannot continue inde.nitely if the \nproper subformula relationship is well\u00adfounded as indeed it is for the propositional connectives. We \ncan also distinguish between three different kinds of principles that would ordinarily be called cuts \n. The .rst is where we have a derivation of G, . f J (in which J stands for an arbitrary concluding judgment, \ni.e., G, . f [P ] or G, .; P f Q or G, . f R or G, . f . '), and we want to substitute another derivation \nG f . for the hypotheses .. The second is where we have a coincidence between a right-focused derivation \nG f [P ], and a derivation G; P f Q, which we can transform into an unfocused derivation G f Q. In the \nthird, we combine an unfocused derivation G f P together with G; P f Q to obtain G f Q. We call the .rst \ncut principle substitution, the second reduction, and the third composition. In the usual proof-theoretic \nterminology, these correspond to right-commutative, principal, and left-commutative cuts, respectively. \nPrinciple (Substitution). If G, G ' f . and G, ., G ' f J then G, G ' f J Principle (Reduction). If G \nf [P ] and G; P f Q then G f Q Principle (Composition). If G f P and G; P f Q then G f Q To prove these \nwe need a weakening lemma, which is immediate. Proposition (Weakening). If G f J, then G, . f J. P2 . \n. . context id on the proper subformula relationship is well-founded. We gave the proofs of identity \nand cut in such explicit detail in part to emphasize that there actually isn t very much detail. For \nexample, we did not have to give the case of one typical positive connective and sweep the others under \nthe rug, because both proofs do not even mention particular positive connectives instead they reason \nmodularly about derivations of . . P . And modularity is a powerful tool: it gives us license to introduce \nnew types almost arbitrarily, so long as we de.ne them purely through the . . P judgment. 3. Focusing \nthe .-calculus In the previous section, we saw how combining the technique of focusing with a judgmental \nand higher-order analysis of derived rules led to a sequent calculus rich in judgments . Now, we will \nshow how these different judgments correspond precisely, through the Curry-Howard isomorphism, to natural \nprogramming language constructs. We start with a type system containing all the propo\u00adsitional connectives \ndescribed above, though for simplicity leav\u00ading out atomic types so the language will have strict products \nand sums, and call-by-value function spaces. After giving it an op\u00aderational semantics corresponding \nto cut-elimination and proving type safety, we will show how our informal use of higher-order abstract \nsyntax can be formalized in Coq. Finally, we will try to give a demonstration of the aforementioned modularity \nprinciple, by showing the ease with which recursion and recursive types can be added to the language. \nFocusing Typing Syntactic category . . P . . p : P patterns G f [P ] G f V : [P ] values G; P f Q G f \nF : P > Q (CBV) functions G f R G f E : R expressions G f . G f s : . substitutions Figure 4. The Curry-Howard \nisomorphism 3.1 Type system Let us begin by examining the . . P judgment, which lies at the heart of \nour formulation of focusing. Previously we described . . P as holding when the context . is an exact \nlist of focused premises needed for a focused proof of P .For a particular P , there need not be a unique \n. such that . . P , and indeed there might not be anysuch . (e.g., when P =0). What then do derivations \nof . . P look like? Abstractly, theydescribe the different shapes a focused proof of P can have, up to \nthe point where either the derivation ends or focus is lost. Thus for example we only have the as simply \ninternalizingatrivialfactorization lemma aboutvaluesin the ordinary sense.Forexample theMLvalue (fn x \n=> x*x, fn x => x-3) can be factored as the pattern (f,g) composed with a substitu\u00adtion [(fn x => x*x)/f, \n(fn x => x-3)/g]. As we shall see, the utilityof thisfactorizationis thatvalues aregivena uniform repre\u00adsentation. \nWhatabout functions?Again,letuslookatthe unannotatedrule for G; P f Q: .(. . P ): G, . f Q G; P f Q Recall \nthis is a higher-order rule, which can be interpreted con\u00adstructively as demanding a map from derivations \nof . . P to derivations of unfocused sequents G, . f Q. The former,we know, correspond to patterns with \ntypes for their free variables. The latter correspondto expressions (the precise senseof which willbeex\u00adplained \nbelow). Therefore, a function is a map from patterns to ex\u00ad pressions. In other words, functions are \nde.ned using higher-order axiomatic derivation P .Q . P .focused proof of P .Q is to immediately lose \nfocus. On the other v vv Q, because the .rst step in a abstract syntax (Pfenning and Elliott 1988). hand, \nthere are two rules for disjunction: Formally, we will assume the existence of partial mapsf, de\u00ad .ned \ninductively over patterns. Thus for anypattern p, f(p) is ei\u00ad . . P . . Q ther unde.ned or denotesauniqueexpression, \npossibly mentioning . . P . Q . . P . Q variables bound by p, and moreover this mapping respects renam\u00ading \nof pattern variables. Concretely, partial maps may be speci.ed because a focused proof of P . Q can continue \nby focusing on by a .nite list of branches: either P or Q. Now, let us label thehypothesesin . with \nvariables since we f ::= (p1 . E1 | ... | pn . En) are ignoring atomic hypotheses, there are only function \nvariable hypothesesf : P .typing judgment: v Q. Then we can annotate . . P asa pattern\u00ad f : P ..1 . p1 \n: P .2 . p2 : Q \u00b7. ():1 .1, .2 . (p1,p2): P . Q . . p : P . . p : Q (no rule for 0) . . inl p : P . Q \n. . inr p : P . Q v Aprogrammer might now get an intuition for whythe context. must be linear: it corresponds \nto the usual restriction that patterns v..QfP: Q Q . with the proviso that the pi do not overlap. In \nSection 3.3 we will v cannot bind a variable more than once. Likewise why P .P .Q is an axiom: it corresponds \nto a primitive pattern. v describehowto encodetheHOAS representationexplicitlyinCoq, using the function \nspace pat . exp. Now to build a function, we simply wrap a f with a .. The annotated rule for function-typing \nbecomes: .(. . p : P ): G, . f f(p): Q G f (.f): P >Q We should emphasize that we are still only de.ning \nthe abstract syntax of functions, not their evaluation semantics although the two aspects are indeed \nclosely related. For instance, the syntax forces a call-by-value interpretation, since functions are \nde.ned by pattern-matching over fully-expanded patterns.3 Moreover, a well\u00ad typed function (.f): P >Q \nis necessarily exhaustive (since the If . . P represents pattern-typing, what can we conclude about the \nother judgments of the focusing system? As we will de\u00adscribe, these correspond to typing judgments for \nvalues, functions, expressions,and substitutions (see Figure 4). Since these judgments are de.ned by \nmutual recursion, we will have to work our way through the system to convince ourselves that these names \nfor the typing rule forces f(p) to be de.ned for all p and . such that . . p : P )andnon-redundant (since \nf is de.ned as a map), in the usual sense of pattern-matching. Finally, the two rules for deriving unfocused \nsequents are now annotated as typing expressions: G f V :[P ] different syntactic categories were not \nchosen arbitrarily.Webegin with G f [P ], which will be annotated with a value V . Recall that the judgment \nis de.ned by a single rule: . . P G f . G f [P ] The .rst premise is now annotated . . p : P , giving \nus a pattern p binding some function variables with types given by .. The second premise is annotated \nwith a simultaneous substitution s =(F1/f1,...,Fn/fn),where f1,...,fn are thevariablesin . and F1,...,Fn \nare functions. Thus the annotated rule becomes: . . p : P G f s :. G f [s]p :[P ] What exactly is this \ncurious value [s]p, which combines a pattern togetherwithanexplicit substitution?Wecanthinkofthis notation \n v g : P .Q . GG f V :[P ]G f F : Q; R G f V : P G f F (g(V )) : R The .rst rule creates anexpression \ndirectly fromavalue, the second by feedingavaluetoanamed functionvariable,and composingthe result with \nanother function. From these two rules, we can intuit that expressions really do correspond closely to \nexpressions in the ML sense that is to computations (Moggi 1991). However,our expressions have a more \nrigid syntax, with an explicit sequencing of evaluation that resembles A-normal form (Flanagan et al. \n1993). 3Of course, Haskell has pattern-matching too, so the emphasis is on fully\u00adexpanded . In Haskell, \nthere is a semantic difference between a function de.ned using wildcard/variable patterns, and the one \nobtainedby replacing the wildcards/variables withexpanded patterns.Forexample \\x-> () and \\() -> () both \ncan be given type ()-> (),but the latter is strict. This difference does not exist in ML since all functions \nare strict. Indeed, as with A-normal form, we seem to encounter the problem that substitution requires \na re-normalization step: for how do we express the result of substituting G/g into F (g(V ))? Another \nway of looking at this is that the expression F (g(V )) corresponds to the one interesting case in the \nproof of the substi\u00adtution principle from Section 2.2, wherein we appealed back to the reduction and \ncomposition principles. Consequently, to make the language closed under ordinary substitution, we internalize \nthese principles as additional rules for forming expressions: G f V :[P ]G f F : P >Q G f E : P G f F \n: P >Q G f F (V ): Q G f F (E): Q We can likewise internalize identity principles to let us take short\u00adcuts \nwhenbuilding terms: v f : P .Q . G G f id : P >P G f f : P >Q id is the polymorphic identity function, \nwhile arrow identity allows us to treat a function variable directly as a function. The complete type \nsystem is summarized in Figure 5, de.ning this Curry-Howard interpretation for large-step focusing, which \nwe call focused .-calculus. The .gure visually quarantines identity and cut principles, to highlight \ntheir special status. The reader may wonder why we have not also internalized the substitution and context \nidentity principles. Such steps are possible for example, internalizing substitutionwouldgiveusacalculusinwhichexplicit \nsubstitutions are evaluated incrementally (Abadi et al. 1991) but we forgo them here, choosing instead \nto de.ne these as meta\u00adtheoretic operations. Substitution is de.ned in Section 3.2; we state context \nidentity here: Principle (Context identity). Suppose G . ., and thatf1,...,fn are the variables in .. \nThen G f (f1/f1,...,fn/fn):.. Proof. Trivial (now that we can directly appeal to arrow identity). Let \nus consider some examples but to make these more palat\u00adable, we .rstdevelop some syntactic sugar.Without \ndangerof am\u00adbiguity, we can write values in unfactorized form: V ::= F | () | (V1,V2) | inl(V ) | inr(V \n) Itisalways possibleto recovera uniquefactorization, i.e., s and p such that V =[s]p. As a special case, \nevery pattern p can also be seen as the value [(f1/f1,...,fn/fn)]p, where f1,...,fn are the variables \nbound by p. Because the syntax is higher-order, we can use meta-variables to build maps by quantifying \nover (all or some subset of) patterns. So for example p . () is a constant map which sends anypattern \nto (), while the map f . () is only de.ned on function variable patterns. When a function is de.ned by \na single pattern-branch, we use the more conventional notation .p.E instead of .(p . E). Finally, we \nlet 2=1 . 1 be the type of booleans, write t = inl(), f = inr() for boolean patterns, and use b as a \nmeta-variable quantifying over these two patterns. EXAMPLE 1.We de.ne boolean functionsand and not: and \n= .((t, t) . t | (t, f) . f | (f, t) . f | (f, f) . f) not = .(t . f | f . t) It is easy to check that \nand :2 . 2 > 2 and not :2 > 2. In this simple case there is a bijective correspondence between patterns \nandvalues, and so the syntax basically mimics the standard mathematical de.nitions. v Linear context \n. ::= \u00b7| f : P .Q, . Pattern p ::= f | () | (p1,p2) | inl p | inr p . . p : P vv f : P .Q . f : P .Q \n.1 . p1 : P .2 . p2 : Q \u00b7. ():1 .1, .2 . (p1,p2): P . Q . . p : P . . p : Q (no rule for0) . . inl p \n: P . Q . . inr p : P . Q .................................................................... Stable \ncontext G ::= \u00b7| G, . Value V ::= [s]p Function F ::= .f | id | f where f ::= (p1 . E1 | ... | pn . En) \nSubstitution s ::= \u00b7| (F/f, s) Expression E ::= V | F (g(V )) | F (V ) | F (E) G f V :[P ] . . p : P \nG f s :. G f [s]p :[P ] G f F : P >Q v f : P .Q . G .(. . p : P ): G, . f f(p): Q G f id : P >P G f f \n: P >Q G f (.f): P >Q G f s :. G f F : P >Q G f s :. v G f\u00b7 : \u00b7 G f (F/f, s):(f : P .Q, .) G f E : R \nG f V : [P ] g : P .v Q . G G f V : [P ] G f F : Q; R G f V : P G f F (g(V )) : R G f V : [P ] G f F \n: P > Q G f E : P G f F : P > Q G f F (V ): Q G f F (E): Q identity principles cut principles Figure \n5. Focused.-calculus (type system) v EXAMPLE 2.We de.netable1 :2 .2 > 2 . 2, a higher-order function \ntaking a unary boolean operator as input, and returning its truth table as output: table1 = .f.(.b1.(.b2.(b1,b2))(f \nf))(f t) Here f is a function variable, while b1 and b2 are meta-variables quantifying over boolean patterns. \nObserve that the syntax forces us to choose a sequential order for the calls to f (we evaluate f(t) .rst, \nthen f(f)).  3.2 Operational semantics The substitution principle of Section 2.2, translated to the \nlanguage of proof terms, says that for any substitution G, G ' f s :. and arbitrary term G, ., G ' f \nt : J (i.e., a value V :[P ], function F : P >Q, substitution s ' :. ', orexpression E : R), there should \nbe a term [s]t suchthat G, G ' f [s]t : J. Rather thaninternalizing this principle in the syntax, we \nde.ne [s]t as an operation, namely the usual simultaneous, capture-avoiding substitution. The de.nition \nE . E ' f(p) de.ned E . E ' (.f)([s]p) . [s]f(p) id(V ) . VF (E) . F (E ' ) .................................................................... \n [s]t ( F (F/f) . s [s]f = f f/. dom(s) [s]([s ' ]p) = [[s]s ' ]p [s](.f)= .p.[s]f(p)[s]id = id [s]\u00b7 \n= \u00b7 [s](F/f, s ' ) = ([s]F/f, [s]s ' ) [s](F (g(V ))) = [s]F ([s]g([s]V )) [s](F (V )) = [s]F ([s]V \n)[s](F (E)) = [s]F ([s]E) Figure 6. Focused.-calculus (operational semantics) of [s]t (givenin Figure6)is \ncompletely unsurprising,buta couple cases worth mention. Applying [s] to a function .f de.nes a new function \nby composing f with the substitution: [s](.f)= .p.[s]f(p) Moreover, as we observed above, if s maps G/g, \nthen applying s to the irreducibleexpression F (g(V )) converts it into two cuts: the expression G([s]V \n) composed with [s]F . The annotated version of the substitution principle is proven easily by induction, \nas in Section 2.2. Lemma (Substitution). If G, G ' f s :. and G, ., G ' f t : J and then G, G ' f [s]t \n: J. The operational semantics is then given by a transition relation E . E ' on closed expressions, \nwith three rules: f(p) de.ned E . E ' (.f)([s]p) . [s]f(p) id(V ) . VF (E) . F (E ' ) All of the complexity \nof pattern-matching is implemented by the one rule on theleft, so let sunpack it:a function F = .f is \nde.ned (syntactically) as a partial map from patterns to open expressions; a value V =[s]p is a pattern \ntogether with an explicit substitution for its variables; thus to apply F to V , we .nd the expression \nf(p) corresponding to p (assuming one exists), and apply the substitu\u00adtion s. Preservation and progress \nare stated in the usual way. Theorem (Preservation). If G f E : P and E . E ', then G f E ' : P . Proof. \nImmediate by induction on (the derivation of) E . E ' , us\u00adingthe substitution lemmainthe caseofareduction \n(.f)([s]p) . [s]f(p) (like in the proof of reduction from Section 2.2). Theorem (Progress). If f E : \nP , then either E = V or else there exists E ' suchthat E . E ' . Proof. Immediate by induction on f \nE : P , using the fact that well-typed functions are exhaustive. EXAMPLE 3. Recall the functionsand, \nnot, and table1 from Ex\u00adamples1and2. The reader canverify the following calculation: and(table1(not)) \n. and((.b1.(.b2.(b1,b2))(not f))(not t)) . and((.b1.(.b2.(b1,b2))(not f)) f) . and((.b2.(f,b2))(not \nf))  . and((.b2.(f,b2)) t) . and(f, t) . f   3.3 Representation in Coq Yet the reader may still \nhave lurking suspicions about our language de.nition. Aren t we overlooking Reynolds lesson about the \npit\u00adfalls of higher-order de.nitions of higher-order programming lan\u00adguages (Reynolds 1972)? Isn t there \na circularity in our appeal to a meta-level notion of maps while de.ning functions?Here, we will attempt \nto rest these concerns by giving an encoding of fo\u00adcused .-calculus in Coq, a proof assistant based on \nthe Calculus of Inductive Constructions (Coquand and Huet 1988; Coquand and Paulin-Mohring 1989; Coq \nDevelopment Team 2006). But our .rst step will be to try to explain how in a paper with higher-order \nabstract syntax in the title, we will have the chutz\u00adpah to use de Bruijn indexes in this encoding (de \nBruijn 1972). To be clear, we are proposing a new kind of higher-order ab\u00adstract syntax. In its usual \napplication, HOAS refers to represent\u00ading object-language variables by meta-language variables (Pfen\u00adning \nand Elliott 1988). This allows object-language binding con\u00adstructs to be encoded by corresponding meta-language \nconstructs, and thereby eliminates the need for dealing explicitly with tricky notions such as variable-renaming, \nparametric quanti.cation and capture-avoiding substitution. The logical framework Twelf is very well-suited \nfor this kind of representation technique (Twelf 2007).In contrast,thenoveltyof our approachis encoding \nobject\u00adlanguage induction by meta-level induction. The Coq proof assis\u00adtant, it turns out, is well-suited \nfor this kind of representation tech\u00adnique. Ideally, we would be able to combine both forms of HOAS, \naswedidaboveata pre-formallevel.But although therehavebeen some attempts at encoding standard HOAS in \nCoq (Despeyroux et al. 1995), and some work on incorporating induction principles into LF (Sch\u00a8et al. \n2001), these are still at experimental urmann stages.We therefore use Coq to highlight the novel aspects \nof our higher-order encoding, but accept the limitations of a .rst-order representation of variables. \nWith that apology out of the way, let us move on to the formal\u00adization.4 As we did throughout the abovediscussion, \nwe will de.ne the focused .-calculus in Curry-style , that is, with typing rules for type-free terms, \nand a type-free operational semantics. An al\u00adternative Church-style approach would be to directly encode \nthe logical rules of Section 2.2, and then simply extract the language, with typed terms being derivations \nof the logical judgments.5 We begin by de.ningtp : Set as a standard algebraic datatype v with constructors \n0, 1: tp and ., ., .: tp . tp . tp (we will use in.x notation for the latter).For convenience, we also \nadd 2: tp to directly represent booleans. The typeofhypotheses hyp : Set is de.ned by one constructor \nof type tp . tp . hyp,but we will v simply write P .Q : hyp, overloading the tp constructor (it will \nalways be clear from context which constructor we really mean). Now, linear contexts .: linctx are lists \nof hyps, while stable contexts G: ctx are lists of linctxs.We write [] for the empty list, [a] for a \nsingleton, a :: l for the cons operation, and l1 ++ l2 for concatenation. Since contexts are lists of \nlists, de Bruijn indexes are given by pairs of natural numbers, written i.j : index. It is quite reasonable \nto think of these using machine intuitions: if G represents a stack of frames ., then a de Bruijn index \ni.j speci.es a frame pointer i plus an offset j. We write #j(.) for the 4The full Coq source code for \nthe encoding described here is available at: http://www.cs.cmu.edu/~noam/research/focusing.tar 5See http://www.cs.cmu.edu/~noam/research/focus-church.v. \n jth element of ., and #i.j(G) for the jth element of the ith linear These de.nitions make use of Coq \nsbuilt-in pattern-matchingfa\u00adcontext in G. These are both partial operations, returning optionsin cilities \nto in order to pattern-match on pats. Coq,but we will abuse notation and write #i.j(G) = H meaning #i.j(G) \n= Some H, and similarly with #j(.). In general, we EXAMPLE 5. The encoding of table1 : fnc makes careful \nuse of will stray slightly from concrete Coq syntax so as to improve de Bruijn indexes: readability. \n1 0 fvar . Comp (Lam(b1 . We de.ne pat as another algebraic datatype, built using con\u00ad structors (), \nt, f : pat and (-, -): pat . pat . pat, inl, inr : table1 = Lam pat . pat, and fvar : pat. The latter \nstands for a pattern binding a Comp (Lam(b2 .i(b1,b2)l)) 1.0 'f')) 0.0 't' B@ CA . Fail function variable \nsince we are using a de Bruijn representation, patterns do not actually name any variables. The pattern-typing \njudgment . . p : P is encoded by an inductive type family pat tp : linctx . pat . tp . Prop. We omit \nthe names of the constructors for pat tp,butgive their types below (also leaving implicit the .-quanti.cation \nover all free variables): In the .rst call (with value t), the function argument is (the .rst andonly \nentry)onthetopofthe stack,sowe referenceitby0.0.In the second call, a frame (coincidentally empty) has \nbeen pushed in front of the function, so we reference it by 1.0. Q : pat tp [P .Q] fvar P . : pat tp \n[] () 1 : pat tp .1 p1 P . pat tp .2 p2 Q . pat tp (.1 ++ .2)(p1,p2) P . Q : pat tp . pP . pat tp .(inl \np) P . Q : pat tp . pQ . pat tp .(inr p) P . Q : pat tp [] t 2 Now, the syntax of the language is de.ned \nthrough four mutually v vf[]2pattp: Now webuild the four typing-judgments as mutually inductive type-families, \nde.ned as follows (again omitting constructors for the typing rules, and outermost .-quanti.ers): val \ntp : ctx . tp . Prop : pat tp . pP . sub tp G s . . val tp G(Value ps) P fnc tp : ctx . tp . tp . Prop \n:(.p...pat tp . pP . exp tp (. :: G) f(p) Q) . fnc tp G(Lam f) PQ : fnc tp Id PP v : (#i.j(G) = (P . \ntors: sub tp : ctx . linctx . PropValue : pat . sub . val : sub tp G(Subst []) [] : fnc tp G FPQ . sub \ntp G(Subst s). inductive types val, fnc, sub, and exp, with the following construc- Q)) . fnc tp G(IdVar \ni.j) PQ Lam :(pat . exp) . fnc v . sub tp G(Subst (F :: s)) (P .Id : fnc IdVar : index . fnc exp tp : \nctx . tp . Prop Q :: .) : val tp G VP . exp tp G(Return V ) P Subst : list fnc . sub Return : val . exp \nComp : fnc . index . val . exp AppV : fnc . val . exp AppE : fnc . exp . exp Fail : exp As promised, \nfnc contains maps from patterns to expressions, em\u00adbedded through the constructor Lam :(pat . exp) . \nfnc. Note that this is a positive de.nition (and thus acceptable in Coq) be\u00adcause the type pat was already \nde.ned as opposed to, say, the de.nition Lam ' :(val . exp) . fnc (which would be illegal in Coq). On \nthe other hand, Coq requires maps pat . exp to be total, so to simulate partial maps we add an expression \nFail : exp, which can be read as unde.ned or stuck . Following the representation of linear contexts \nas unlabelled listsofhypotheses,a substitutionisjustan unlabelled listof func\u00adtions, while theexpression \nReturn V makesexplicit the implicit in\u00adclusion of values into expressions. Otherwise, the constructors \nare all straightforward transcriptions of terms of focused .-calculus. v : (#i.j(G) = (P . . exp tp \nG(Comp F i.j V ) R : val tp G VP . fnc tp G FPQ . exp tp G(AppV FV ) Q : exp tp G EP . fnc tp G FPQ . \nexp tp G(AppE FE) Q Again, these de.nitions areadirect transcriptionofthe typing rules in Figure 5, including \nthe higher-order rule for function-typing. Finally, to encode the operational semantics, we .rst de.ne \nthe different substitution operations: sub val : nat . sub . val . val sub fnc : nat . sub . fnc . fnc \nsub sub : nat . sub . sub . sub sub exp : nat . sub . exp . exp These are de.ned by (mutual) structural \ninduction on the term being substituted into, essentially asin Figure6,but witha bitof extra reasoning \nabout de Bruijn indices. The extra nat argument is a frame pointer to the linear context . being substituted \nfor, and is used as follows in the IdVar case (and analogously in the Comp case): Q)) . val tp G VP . \nfnc tp G F QR 8 >< >: In the following examples, we abbreviate Value p (Subst []) by #j(s) i = i ' IdVar \ni ' .j i>i ' IdVar (i ' - 1).j i<i ' 'p', and Return ('p') by ipl. sub fnc is (IdVar i ' .j)= EXAMPLE \n4. TheCoq encodingsofand, not : fnc are: 01 (t, t) .it (t, f) .ifl We can then de.ne the transition \nrelation as an inductive family step : exp . exp . Prop, with the following rules: and = Lam BBB@ CCCA \n(f, t) .ifl (f, f) .ifl : step (AppV (Lam f)(Value ps)) (sub exp 0 sf(p)) : step (AppV Id V )(Return \nV ) : step EE ' . step (AppE FE)(AppE FE ' ). Fail not = Lam(t .ifl| f .itl| . Fail): step (AppE F (Return \nV )) (AppV FV ) These mirror the rules in Figure 6, with one additional rule for the (formerly implicit) \ntransition from composition to reduction after the expression argument has been reduced to a value. Finally,we \nde.neapredicate terminal : exp . Prop and assert : terminal (Return V ). Given these de.nitions, we can \nstate the preservation and progress theorems: preservation : exp tp G EP . step EE ' . exp tp G E ' P \nprogress : exp tp [] EP . (terminal E ..E ' .step EE ' ) Both theorems have short proofs in Coq, constructed \nusing the tac\u00adtic language. As in the paper proof, the preservation theorem relies on the substitution \nprinciple, which in turn relies on weakening. Both substitution and weakening require establishing a \nfew trivial We can encode theplus function like so: \u00ab (m, Z) . m plus = .x f.. ' (m, S n) . (.n ' .S \nn )f (m, n) For instance, plus (S(SZ), SZ) .* S(S(SZ)). To verify that plus : Nat . Nat > Nat, we must \ncheck that for any Nat . Nat pattern, there is a corresponding Nat-typed branch of the function. This \nis easily seen to be true, since all Nat . Nat patterns have the form (m, Z) or (m, S n). v v EXAMPLE \n8. Considera domainD = \u00b5X.1 . Nat . (X . facts about arithmetic, lists, and de Bruijn indices. This \n(about 140 lines to prove the trivial lemmas, followed by about 230 lines to prove weakening and substitution, \nmuch of it dealing simply with \u00b7. p : Nat f : D . X): the coding of mutual induction principles in Coq) \nis the main source \u00b7. U : D \u00b7. N p : D D . F f : D ofbureaucracyin the Coq formalization, which otherwise \nfollows our informal presentation very closely.  3.4 Recursion and recursive types We have seen how \nfocusing the .-calculus gives logical explana\u00adtions for notions such as pattern-matching and evaluation \norder, which are typically seen as extra-logical. Once we have this analy\u00adsis,we canextendthe languageinafairly \nopen-endedway without modifying the logical core. In this section, we will consider two particularly \neasyextensions: recursion and recursive types.For re\u00adcursive functions, we add one typing rule and one \nevaluation rule: We de.ne a functionapp : D . D > D, which tries to apply the .rst argument to the second \n(and returns U if the .rst argument is not a function): \u00ab (F f,d) . id(f(d)) app = . ( ,d) . U For instance,app \n(F id,V ) . id(id(V )) . id(V ) . V . G,f : P . G f .x f.F : P >Q (.x f.F )(V ) . ([.x f.F/f]F )(V ) \nsyntax for pattern-matching on recursivetypes, theyalso raise some subtle theoretical questions. A careful \nreader might have noticed These rules can be transcribed directly (modulo de Bruijn indices) that thereis \nanotherwayof de.ningthe plus function in Coq: rather into Coq: than explicitly using the .x operator, \nwe could use Coq sbuilt-in v Q f F : P >Q While these examples illustrate the simplicity of higher-order \n: fnc tp (G, [P . : step (AppV (.x F ) V ) (AppV (sub fnc 0(Subst [.x F ]) F ) V ) To verify the safety \nof this extension, we need only localized checks: one extra case each in the proofs of weakening, substitu\u00adtion, \npreservation, and progress. EXAMPLE 6.We de.ne loop :1 > 1= .x f..().f(). Then v loop() . (.().loop())() \n. loop() . ... . For recursive types, we add a singlepattern-typing rule: . . p :[\u00b5X.P/X]P . . fold(p): \n\u00b5X.P To add general\u00b5-types to our Coq formalization, we would have to introducethe additionalbureaucracyoftype \nsubstitution.Onthe other hand, for particular recursive types (such as those considered below) we can \ndirectly transcribe their pattern-typing rules. And these rules suf.ce: we do not have to extend or modify \nanyother aspectofthe type system or operational semantics. The machinery of focusing and higher-order \nabstract syntax gives us the value\u00adforming rules and pattern-matching on recursive types for free . Q]) \nFPQ . fnc tp G(.x F ) PQ In particular,ourproofoftypesafety(bothonpaperandintheCoq formalization) needs \nabsolutely no modi.cation, since it references the pattern-typing judgment uniformly. EXAMPLE 7. In thisexample, \nwe consider natural numbersNat = \u00b5X.1 . X, de.ned by two pattern-typing rules: \u00b7. p : Nat \u00b7. Z : Nat \n\u00b7. S p : Nat Fixpoint mechanism to de.ne a map plus pat : pat . pat . pat computing the sum of two Nat \npatterns, and then de.ne plus * : fnc = Lam((m, n) .iplus pat mnl| . Fail) Strictly speaking, plus * \nis an exotic term , i.e., does not represent a term of concrete syntax (Despeyroux et al. 1995), since \nit corre\u00adsponds to a function de.ned by in.nitely many pattern-branches. Operationally, it computes the \nsum of two numbers in a single step of evaluation, whereas plus computes it in multiple steps (linear \nin n). Nonetheless, plus and plus * are observationally equivalent.We conjecture that this is always \nthe case, and that anyterm de.nable in the Coq encoding of focused .-calculus with recursive types is \nobservationally equivalent to a term of concrete syntax using ex\u00adplicit recursion.Yet,evenif this conjecture \nholds,itisan interest\u00ading question whether there is a principled way to adapt the HOAS encoding to eliminate \nterms such as plus * altogether. Proof-theoretically speaking, we can put it this way: for some recursive \ntypes P , establishing G; P f Q requires an in.nitely wide derivation in the focusing system.6 Conversely, \nfor other re\u00adcursive types, the identity principle G; P f P requires a deriva\u00adtion that is in.nitely \ndeep. In particular, the subformula relation- D is a subformula of D). v ship may not be well-founded \n(e.g., D .This is not really an issue for our programming language: rather than attempting an in.nite \nderivation, we can simply invoke the in\u00adternalized identity principle. More fundamentally, though, we \ncan give these derivations a coinductive reading this becomes partic\u00adularly signi.cantifwewanttoextendthe \nlanguageand incorporate subtyping through an identity-coercion interpretation, as explored by Brandt \nand Henglein (1998). 6E.g., for Nat we essentially have the .-rule (Buchholz et al. 1981). 4. Related \nwork This paperisbyno meansthe .rstto proposealogicalexplanation for pattern-matching or explicit substitutions. \nRecently, Nanevski et al. (2007) and Pientka (2008) offer a judgmental explanation for explicit substitutions \nin a modal type theory. Methodologically theirworkis closeto ours,but theirdevelopmentis ratherdiffer\u00adent \nsince theyseek to understand the connection between explicit substitutions and meta-variables (as used, \ne.g., in logical frame\u00adworks and staged computation), rather than pattern-matching. Cer\u00adrito andKesner \n(2004)give an interpretation for both nested pat\u00adterns and explicit substitutions in sequent calculus. \nIt seems the dif.culty with taking the unfocused sequent calculus as a start\u00ading point, though, is that \nit suffers from a lack of judgments toexplain pattern-matching Cerrito andKesnermust introduce ad\u00additional \nscaffolding beyond the Curry-Howard isomorphism. For example, to obtain a well-behaved language with \nsubstitution and subject reduction, theymust annotate the single cut rule of sequent calculus as three \ndifferent typing rules, and add anothertyping rule (app)with little proof-theoretic motivation. In contrast, \nevery typ\u00ading rule wegavein Section 3.1was eithera direct annotationof a logical rule in Section 2.2, \nor else internalized one of the cut or identity principles. An additional byproduct of our use of focusing \nas a logical foundation is that the extracted language has explicit evaluation order. As mentioned in \nthe Introduction, this connection has been explored before by various people, at .rst with only a loose \ntie to linear logic (Curien and Herbelin 2000; Selinger 2001;Wadler 2003), but later with an explicit \nappeal to polarity and focusing (Laurent 2005; Dyckhoff and Lengrand 2006). From this line of work, our \nmain technical innovation is the uniform treatment of the positiveconnectives through pattern-matching, \nwhich considerably simpli.esprevious formalisms whileallowingustoconsiderarich set of connectives. Our \napproach is loosely inspired by that of Girard (2001). In a short but prescient paper, Coquand (1992) \nexamines pattern-matching as an alternative to the usual elimination rules in the frameworkof Martin-L\u00a8of \ns type theory, and concludes with an offhand remark, From a proof-theoretic viewpoint, our treatment \ncan be characterized as .xing the meaning of a logical constant by its introduction rules .Wehave seenhow \nthis interpretation arises naturally out of focusing for the positiveconnectives, although how to extend \nour approach to the dependently-typed case remains an important open question. Elsewhere, we explore \nthe dual interpre\u00adtation for the negative connectives (and lazy evaluation), tying the uni.ed analysis \nto Michael Dummett s examination of the justi.\u00adcation of logical laws (Dummett 1991; Zeilberger 2007). \nAcknowledgments Special thanks to Frank Pfenning for his invaluable guidance dur\u00ading the development \nof this work, and for suggestions on improv\u00ading its presentation.I would also like to thank Bob Harper, \nNeel Krishnaswami, Peter Lee,WilliamLovas, Dan Licata, Jason Reed, Rob Simmons, and other members of \nthe CMU PL group for lively discussions, and the POPL reviewers for their helpful comments. Finally,I \nam grateful to the coq-club mailing list for useful ad\u00advice on Coq, and particularly to Xavier Leroyfor \nexplaining a cool trick for coding mutual induction principles. References M. Abadi, L. Cardelli,P.-L. \nCurien, and J.-J.L\u00b4 evy. Explicit substitutions. Journal of Functional Programming, 1(4):375 416, 1991. \nJean-Marc Andreoli.Focussing and proof construction. Annals of Pure and Applied Logic, 107(1):131 163, \n2001. Jean-Marc Andreoli. Logic programming with focusing proofs in linear logic. Journal of Logic and \nComputation, 2(3):297 347, 1992. Michael Brandt and Fritz Henglein. Coinductive axiomatization of recur\u00adsive \ntype equaility and subtyping. Fundamenta Informaticae, 20:1 24, 1998. W. Buchholz,S. Feferman,W. Pohlers,andW.Sieg.Iterated \nInductive Def\u00adinitions and Subsystems of Analysis: Recent Proof-Theoretical Studies. Springer-Verlag, \n1981. Serenella Cerrito and DeliaKesner. Pattern matching as cut elimination. Theoretical Computer Science, \n323(1-3):71 127, 2004. The Coq Development Team. The Coq Proof Assistant Reference Manual Version 8.1. \nINRIA, 2006. http://coq.inria.fr/doc/main.html. Thierry Coquand. Pattern matching with dependent types. \nIn Proceed\u00adings of theWorkshop onTypes for Proofs and Programs, pages 71 83, B\u00b0astad, Sweden, 1992. Thierry \nCoquand andG\u00b4erard Huet. The calculus of constructions. Informa\u00adtion and Computation, 76(2/3):95 120, \n1988. Thierry Coquand and ChristinePaulin-Mohring. Inductively de.ned types. In LNCS 389. Springer-Verlag, \n1989. Pierre-Louis Curien and Hugo Herbelin. The duality of computation. In ICFP 00: Proceedings of the \nSIGPLAN International Conference on Functional Programming, pages 233 243. 2000. Nicolaas G. de Bruijn. \nA lambda calculus notation with nameless dum\u00admies, a tool for automatic formula manipulation, with application \nto the church-rossertheorem. Indagationes Mathematicae, 34:381 392, 1972. Jo\u00a8e Hirschowitz. Higher-order \nab\u00adelle Despeyroux, Amy Felty, and Andr\u00b4stract syntax in Coq. In M. Dezani-Ciancaglini and G. Plotkin, \nedi\u00adtors, Proceedingsofthe International ConferenceonTyped LambdaCal\u00adculi and Applications,volume 902 \nof LNCS, pages 124 138, Edinburgh, Scotland, 1995. Springer-Verlag. Michael Dummett. The Logical Basis \nof Metaphysics. TheWilliam James Lectures, 1976. Harvard University Press, Cambridge, Massachusetts, \n1991. ISBN 0-674-53785-8. RoyDyckhoffand Stephane Lengrand. LJQ:Astrongly focused calculus for intuitionistic \nlogic. In Proceedings of the Second Conference on Computability in Europe, 2006. Cormac Flanagan, Amr \nSabry, Bruce Duba, and Matthias Felleisen. The essence of compiling with continuations. In PLDI 93: Proceedings \nof the SIGPLAN Conference on Programming Language Design and Implementation, 1993. Gerhard Gentzen. Untersuchungen \nuber das logische Schlie\u00dfen. \u00a8Mathe\u00admatische Zeitschrift, 39:176 210, 405 431, 1935. English translation \nin M. E. Szabo, editor, The CollectedPapers of Gerhard Gentzen, pages 68 131, North-Holland, 1969. Jean-Yves \nGirard. Locus solum: From the rules of logic to the logic of rules. Mathematical Structures in Computer \nScience, 11(3):301 506, 2001. Jean-Yves Girard. On the unity of logic. Annals of pure and applied logic, \n59(3):201 217, 1993. Hugo Herbelin. Alambda-calculus structure isomorphic to Gentzen-style sequent calculus \nstructure. In CSL 94: Proceedings of the 8th Interna\u00adtionalWorkshop on Computer ScienceLogic, 1995. Jacob \nM. Howe. Proof search issues in some non-classical logics. PhD thesis, University of St Andrews, December \n1998. URL http:// www.cs.kent.ac.uk/pubs/1998/946. Available as University of St Andrews Research Report \nCS/99/1. Steven C. Kleene. Introduction to Metamathematics.Van Nostrand, Prince\u00adton, NJ, 1952. Olivier \nLaurent. Etude de la polarisation en logique. ese de doctorat, Th`Universit\u00b4e Aix-Marseille II, March \n2002. Olivier Laurent. Classical isomorphisms of types. Mathematical Structures in Computer Science, \n15(5):969 1004, October 2005. Per Martin-L\u00a8of. Hauptsatz for the intuitionistic theory of iterated inductive \nde.nitions. In J. E. Fenstad, editor, Proceedings of the Second Scan\u00addinavian Logic Symposium, pages \n179 216, Amsterdam, 1971. North Holland. Per Martin-L\u00a8of. On the meanings of the logical constants and \nthe justi\u00ad.cations of the logical laws. Nordic Journal of Philosophical Logic, 1(1):11 60, 1996. URL \nhttp://www.hf.uio.no/filosofi/njpl/ vol1no1/meaning/meaning.html. Dale Miller and Chuck Liang. Focusing \nand polarization in intuitionistic logic. In CSL 07: Proceedings of the 21st InternationalWorkshop on \nComputer Science Logic. 2007. Eugenio Moggi. Notions of computation and monads. Information and Compution, \n93(1):55 92, 1991. Aleksandar Nanevski, Frank Pfenning, and Brigitte Pientka. Contextual modal type theory. \nTransactions on Computational Logic, 2007. To appear. Frank Pfenning and Rowan Davies. Ajudgmental reconstruction \nof modal logic. Mathematical Structures in Computer Science, 11(4):511 540, 2001. Frank Pfenning and \nConal Elliott. Higher-order abstract syntax. In PLDI 88: Proceedings of the SIGPLAN Conference on Programming \nLanguage Design and Implementation, pages 199 208, 1988. Brigitte Pientka.Atype-theoretic foundation \nfor programming with higher\u00adorder abstract syntax and .rst-class substitutions. In POPL 08: Pro\u00adceedings \nof the SIGPLAN-SIGACT Symposium on Principles of Pro\u00adgramming Languages, 2008. John C. Reynolds. De.nitional \ninterpreters for higher-order programming languages. In ACM 72: Proceedings of theACM annual conference, \npages 717 740, 1972. Carsten Sch\u00a8urmann, Jo\u00a8elle Despeyroux, and Frank Pfenning. Primitive re\u00adcursion \nfor higher-order abstract syntax. Theoretical Computer Science, 266:1 57, 2001. Peter Selinger. Control \ncategories and duality: on the categorical seman\u00adtics of the lambda-mu calculus. Mathematical Structures \nin Computer Science, 11(2):207 260, 2001. AnneS.Troelstraand Helmut Schwichtenberg. Basic Proof Theory,volume \nCambridge Tracts in Theoretical Computer Science 43. Cambridge University Press, 1996. Twelf 2007. The \nTwelf Project, 2007. http://twelf.plparty.org/. PhilipWadler. Call-by-value is dual to call-by-name. \nIn ICFP 03: Pro\u00adceedings of the SIGPLAN International Conference on Functional Pro\u00adgramming, pages 189 \n201, 2003. PhilipWadler.Atasteoflinear logic. In MFCS: Symposium on Mathemat\u00adicalFoundationsof Computer \nScience, 1993. Noam Zeilberger. On the unity of duality. Annals of Pure and Applied Logic, 2007. To appear \nin a special issue on Classical Logic and Computation .  \n\t\t\t", "proc_id": "1328438", "abstract": "<p>Focusing is a proof-search strategy, originating in linear logic, that elegantly eliminates inessential nondeterminism, with one byproduct being a correspondence between focusing proofs and programs with explicit evaluation order. Higher-order abstract syntax (HOAS) is a technique for representing higher-order programming language constructs (e.g., &#955;'s) by higher-order terms at the\"meta-level\", thereby avoiding some of the bureaucratic headaches of first-order representations (e.g., capture-avoiding substitution).</p> <p>This paper begins with a fresh, judgmental analysis of focusing for intuitionistic logic (with a full suite of propositional connectives), recasting the \"derived rules\" of focusing as <i>iterated inductive definitions</i>. This leads to a uniform presentation, allowing concise, modular proofs of the identity and cut principles. Then we show how this formulation of focusing induces, through the Curry-Howard isomorphism, a new kind of higher-order encoding of abstract syntax: functions are encoded by maps from <i>patterns</i> to expressions. Dually, values are encoded as patterns together with <i>explicit substitutions</i>. This gives us pattern-matching \"for free\", and lets us reason about a rich type system with minimal syntactic overhead. We describe how to translate the language and proof of type safety almost directly into Coq using HOAS, and finally, show how the system's modular design pays off in enabling a very simple extension with recursion and recursive types.</p>", "authors": [{"name": "Noam Zeilberger", "author_profile_id": "81384612996", "affiliation": "Carnegie Mellon University, Pittsburgh, PA", "person_id": "P925382", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1328438.1328482", "year": "2008", "article_id": "1328482", "conference": "POPL", "title": "Focusing and higher-order abstract syntax", "url": "http://dl.acm.org/citation.cfm?id=1328482"}