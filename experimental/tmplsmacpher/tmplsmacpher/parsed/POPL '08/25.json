{"article_publication_date": "01-07-2008", "fulltext": "\n Imperative Self-Adjusting Computation * Umut A. Acar Amal Ahmed Matthias Blume ToyotaTechnological \nInstitute at Chicago {umut,amal,blume}@tti-c.org Abstract Self-adjusting computation enables writing \nprograms that can au\u00adtomatically and ef.ciently respond to changes to their data (e.g., inputs). The \nidea behind the approach is to store all data that can change over time in modi.able references and to \nlet computations construct traces that can drive change propagation. After changes have occurred, change \npropagation updates the result of the com\u00adputationby re-evaluating only thoseexpressions that depend \non the changed data. Previous approaches to self-adjusting computation require that modi.able references \nbe written at most once during execution this makes the model applicable only in a purely func\u00adtional \nsetting. In this paper,we present techniques for imperativeself-adjusting computation where modi.able \nreferences can be written multiple times.We de.nea language SAIL (Self-Adjusting Imperative Lan\u00adguage) \nand prove consistency, i.e., that change propagation and from-scratch execution are observationally equivalent. \nSince SAIL programs are imperative, they can createcyclic data structures.To prove equivalence in the \npresence of cycles in the store, we formu\u00adlate and use an untyped, step-indexed logical relation, where \nstep indices are used to ensure well-foundedness.We show that SAIL accepts an asymptotically ef.cient \nimplementation by presenting algorithms and data structures for its implementation. When the number of \noperations (reads and writes) per modi.able is bounded by a constant, we show that change propagation \nbecomes as ef\u00ad.cient as in the non-imperative case. The general case incurs a slowdown that is logarithmic \nin the maximum number of such op\u00aderations. We describe a prototype implementation of SAIL as a Standard \nML library. Categories and Subject Descriptors D.3.0[Programming Lan\u00adguages]: General; D.3.1[Programming \nLanguages]:Formal Def\u00adinitions and Theory; D.3.3[Programming Languages]: Language Constructs and Features; \nF.2.0[Analysis of Algorithms and Prob\u00adlem Complexity]:General; F.3.2[Semantics of Programming Lan\u00adguages]: \nOperational Semantics GeneralTerms Languages, Design, Semantics, Algorithms Keywords Self-adjusting computation, \nincremental computation, step-indexed logical relations, imperative programming, change propagation, \nmemoization, mutable state * Acaris supportedbyagift from Intel. Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page.To copyotherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. POPL 08, January 7 12, 2008, San Francisco, \nCalifornia, USA. Copyright c &#38;#169; 2008ACM 978-1-59593-689-9/08/0001...$5.00 1. Introduction Self-adjusting \ncomputation concerns the problem of updating the output of a computation while its input undergoes small \nchanges over time. Recent work showedthat a combination of dynamic de\u00adpendence graphs (Acar et al. 2006c) \nand a particular form of mem\u00adoization (Acar et al. 2003) canbe combined to update computation orders \nof magnitudesfaster than re-computing from scratch (Acar et al. 2006b). The approach has been applied \nto a number ofprob\u00adlems including invariant checking (Shankar and Bodik 2007), com\u00adputational geometry \nand motion simulation (Acar et al. 2006d), and statistical inference on graphical models (Acar et al.2007c). \nIn self-adjusting computation, the programmer stores all data that can change over time in modi.able \nreferences or modi.ables for short. Modi.ables are write-once references: programs can read a modi.able \nas many times as desired but must write it exactly once. After a self-adjusting program is executed, \nthe programmer can change the contents of modi.ables and update the computa\u00adtion by performing change \npropagation.For ef.cient change prop\u00adagation, as a program executes, an underlying system records the \noperations on modi.ables in a trace and memoizes the function calls. Change propagation uses the trace, \nwhichis representedbya dynamic dependence graph, to identify the reads of changed mod\u00adi.ables and re-evaluates \nthem. When re-evaluating a read, change\u00adpropagation re-uses previous computations via memoization. Byrequiringthat \nmodi.ablesbe writtenexactly onceatthetime of their creation, the approach ensures that self-adjusting \nprograms are purely functional; this enables1) inspecting thevaluesof mod\u00adi.ables at anytime in the past, \n2) re-using computations via mem\u00adoization. Since change propagation critically relies on these two properties, \nit was not known if self-adjusting computation could be made to work in an imperative setting. Although \npurely functional programming is fully general (i.e., Turing complete), it can be asymptotically slower \nthan imperativemodels of computation (Pip\u00adpenger 1997). Also, for some applications (e.g., graphs), imperative \nprogramming can be more natural. In this paper, we generalize self-adjusting computation to support imperative \nprogramming by allowing modi.ables to be written multiple times. We describe an untyped language, called \nSAIL (Self-Adjusting Imperative Language), that is similar to a higher-order language with mutable references. \nAs in a conven\u00adtional imperative language, a write operation simply updates the speci.ed modi.able.Aread \noperation takes the modi.able being read and the expression (the body of the scoped read) that uses the \ncontents of the modi.able.To guarantee that all dependencies are tracked, SAIL ensures that values that \ndepend on the contents of modi.ables are themselves communicated via modi.ables by re\u00adquiringthatread \noperationsreturna.xed(unit)value.Comparedto a purely functional language for self-adjusting computation, \nSAIL is somewhat simpler because it does not have to enforce the write\u00adonce requirement on modi.ables. \nIn particular, purely functional languages for self-adjusting computation make a modal distinc\u00adtion between \nstable and changeable computations, e.g., Acar et al. (2006c), which is not necessary in SAIL. We describe \na Standard ML library for SAIL that allows the programmer to transform ordinary imperative programs into \nself\u00adadjusting programs (Section 2). The transformation requires re\u00adplacing the references in the input \nwith modi.ables and annotating the code with SAIL primitives. As an example, we consider depth\u00ad.rst-search \nand topological sorting on graphs. The resulting self\u00adadjusting programs are algorithmically identical \nto the standard ap\u00adproach to DFS and topological sort, but via change propagation they can respond to \nchangesfaster thana from-scratch execution when the input is changed. We formalize the operational semantics \nofSAIL and its change propagation semantics (Section 3). In the operational semantics, evaluating an \nexpression returns a value and a trace that records the operations on modi.ables.The semantics models \nmemoization by using non-determinism: a memoized expression can either be evaluated to a value (a memo \nmiss) or its trace and result can be re-used by applying change propagation to the trace of a previous \nevaluation(a memo hit). We prove that the semantics of SAIL is consistent, i.e., that any two evaluations \nof the same expressions are observationally (or contextually) equivalent (Section 4). Since SAIL programs \nare imperative, it is possible to construct cycles in the store. This makes reasoning about equivalence \nchallenging. In fact, in our prior work (Acar et al. 2007b), we proved consistency for purely functional \nself-adjusting computation by taking critical advantage of the absence of cycles. More speci.cally, we \nde.ned the notion of equivalence by using lifting operations that eliminated the store by substituting \nthe contents of locations directly into expressions; lifting cannot even be de.ned in the presence of \ncycles. Reasoning about equivalence of programs in the presence of mutable state has long been recognized \nas a dif.cult problem. The problem has been studied extensively starting with ALGOL-like languages which \nhave local updatable variables but no .rst-class references (O Hearn andTennent 1995; Sieber 1993; Pitts \n1996). The problem gets signi.cantly harder in the presence of .rst class references and dynamic allocation \n(Pitts and Stark 1993; Stark 1994; Benton and Leperchey2005) and harder still in the presence of cyclic \nstores. We know of only two recent results for proving equivalence of programs with .rst-class mutable \nreferences and cyclic stores, a proof method based on bisimulation (Koutavas and Wand 2006) and another \non (denotational) logical relations (Bohr and Birkedal 2006) (neither of which is immediately applicable \nto proving the consistencyof SAIL). We prove consistency for imperative self-adjusting programs using \nsyntactic logical relations, that is, logical relations based on the operational semantics of the language \n(not on denotational models). We use logical relations that are indexed not by types (since SAIL is untyped),butbya \nnatural number that, intuitively, records the number of steps available for future evaluation (Appel \nand McAllester 2001; Ahmed 2006). The strati.cation providedby the step indices is essential for modeling \nthe recursive functions (available via encoding .x)and cyclic stores present in the lan\u00adguage. We show \nthat SAIL accepts an asymptotically ef.cient imple\u00admentation by describing data structures for supporting \nits primi\u00adtivesandbygivingachange propagation algorithm (Section5).For each modi.able wekeep all of its \ndifferent contents, or versions, over time. The write operations create theversions.Forexample, writing \nthe values values 0 and then 5 into the same modi.able m creates two versions of m. This versioning technique, \nwhich is inspired by previous work on persistent data structures (Driscoll et al. 1989), enableskeeping \ntrackof the relationship between read operations and the values that they depend on. We keep the ver\u00ad \nsignature SELF ADJUSTING = sig eqtype a mod val mod: ( a * a -> bool) -> a mod val read: a mod * ( a \n-> unit) -> unit val write: a mod -> a -> unit val hashMod: a mod -> int val memo: unit -> (int list) \n-> (unit -> a) -> a val init: unit -> unit val deref: a mod -> a val change: a mod * a -> unit val propagate: \nunit -> unit end Figure 1. Signature of the library. sions of a modi.able in a version-set and its readers \nin a reader\u00adset. We represent these sets as searchable time-ordered sets that support various operations \nsuch as .nd, insert, and delete, all in time logarithmic in the size of the set. Using these data structures, \nwe describe a change-propagation algorithm that implements the trace-based change propagation of the \nSAIL semantics ef.ciently. In particular, for computations that write to each modi.able a con\u00adstant number \nof times and read each location a constant number of times, we show that change propagation is asymptotically \nas ef.\u00adcient as in the non-imperative case. When the number ofwrites is not bounded by a constant, then \nchange propagation incurs a log\u00adarithmic overhead in the maximum number of writes and reads to anymodi.able. \n2. Programming with Mutable Modi.ables Wegive anoverviewof our frameworkbased on ourML library and a \nself-adjusting version of depth-.rst search on graphs. 2.1 The ML library Figure1shows the signatureof \nour library. The library de.nes the equality type formodi.ables a mod and provides functions to cre\u00adate(mod), \nread(read), write(write), and hash(hashMod)modi\u00ad.ables.For memoization, the library provides the memo \nfunction. We de.ne aself-adjusting program as a Standard ML program that uses these functions. In addition, \nthe library provides meta func\u00adtions for initializing the library(init), inspecting and changing modi.able \nreferences(deref, change)and propagating changes (propagate). Meta functions cannot be used by a self-adjusting \nprogram. Amodi.able referenceis createdby themod function that takes aconservativeequality test on the \ncontents of the modi.able and re\u00adturns an uninitialized modi.able.A conservative equality function returns \nfalse when thevalues are differentbut may return true or false whenthevaluesarethe same.This equality \nfunctionisused to stop unnecessary change propagationbydetecting thata write or change operation does \nnot change the contents of a modi.able. For each modi.able allocated, themod function generates a unique \nintegertag, whichis returnedbythe hashMod function. The hashes are used when memoizing function calls.A \nread takes the modi\u00ad.able to be read and a reader function, applies the contents of the modi.able to \nthe reader, and returns unit. By making read oper\u00adations return unit, the library ensures that no variables \nother than those bound by the readers can depend on the contents of modi\u00ad.ables. This forces readers \nto communicate by writing to modi.\u00adable references and makes it possible to track all dependencies by \nrecording the operations on modi.ables. The memo function creates a memo table and returns a memo\u00adized \nfunction associated with that memo table.Amemoized func\u00adtion takes a list of arguments and the function \nbody, looks up the memo table based on the arguments, and computes and stores the result in the memo \ntable if the result is not already found in it.To 1 datatype node = 1 datatype node = 2 empty 2 empty \n3 | node of int * bool ref * node ref * node ref 3 | node of int * bool mod * node mod * node mod 4 fun \ndepthFirstSearch f root = 4 fun depthFirstSearch eq f root = 5 let 5 let 6 6 val mfun = memo () 7 fun \ndfs rn = 7 fun dfs rn = let val rres = mod eq in read rn (fn n => 8 case !rn of 8 case n of 9 empty => \nf (NONE,NONE) 9 empty => write rres (f(NONE,NONE)) 10 | node (id,rv,rn1,rn2) => 10 | node (id,rv,rn1,rn2) \n=> 11 11 mfun [id, #rv, #rn1, #rn2, #rres] (fn () => 12 if !rv then 12 read rv (fn v => if v then 13 \nlet 13 let 14 val () = rf := true 14 val () = write rf true 15 val res1 = dfs rn1 15 val rres1 = dfs \nrn1 16 val res2 = dfs rn2 16 val rres2 = dfs rn2 17 in 17 in 18 18 read rres1 (fn res1 => read rres2 \n(fn res2 => 19 f (SOME id, SOME(res1, res2)) 19 write rres (f(SOME id, SOME(res1, res2))))) 20 end 20 \nend)) 21 else 21 else 22 NONE 22 write rres NONE) end 23 in 23 in 24 dfs root 24 dfs root 25 end 25 end \n Figure 2. The code for ordinary (left) and self-adjusting(right) depth-.rst search programs. facilitate \nef.cient memo lookups, memoized functions must hash theirargumentstointegers uniquely.Thiscanbeachievedbyusing \nstandard boxing or tagging techniques. Our library does not provide mechanisms for statically or dy\u00adnamically \nensuring the correctness of self-adjusting programs, i.e., for ensuring that change propagation updates \ncomputations cor\u00adrectly. We instead expect the programmer to adhere to certain correct-usage requirements. \nIn particular, correct-usage requires thatallthefreevariablesofa memoized functionbe listedasanar\u00adgument \nto the memoized function and be hashed, and that a mem\u00adoized function be used to memoize only one function. \nIt may be possible to enforce these requirementsbut this may requirea sig\u00adni.cantburden on the programmer \n(Acar et al. 2006a).  2.2 Writing Self-Adjusting Programs As an example of how modi.ables can be used, \nFigure 2 shows the complete code for a depth-.rst-search (DFS) function on a graph with ordinary references \n(left) and with modi.able refer\u00adences (right).Agraphisde.nedtobe eitheremptyoranode con\u00adsistingofaninteger \nidenti.er,aboolean visited .ag, and twopoint\u00aders to its neighbors. In the non-self-adjusting code, the \nneighbor pointers and the visited .ag are placed in ordinary references; in the self-adjusting code, \nthese are placed in modi.able references. The static depthFirstSearch function takes a visitor func\u00adtion \nf and the root of a graph as its arguments and performs a depth-.rst-search(dfs)starting at the root \nby visiting each node that has not been previously visited. The dfs function visitsa node by allocating \na modi.able that will hold the result and reading the node pointedtobyitsargument.Ifthenodeisempty,thenthe \nvisi\u00adtor is applied with arguments that indicate that the node is empty. If the node is not empty,then \nthe visited .ag is read. If the .ag is set to true, then the node was visited before and the function \nwrites NONE to its result.If the.agisfalse (the nodewas not visited before), then the .ag is .rst set \nto true, the neighboring nodes are visited re\u00adcursively, the results of the neighbors along with the \nidentity of the visited node are passed to the visitor(f)to compute the result, and the resultis written.SinceduringaDFS,the \nvisited.ag startswith value false and is later set to true, the DFS requires updateable modi.ables. We \ntransform the static code into self-adjusting code by .rst replacing all the references in the input \n(the graph) with modi.-Figure 3. Agraph before and after insertion of the edge(H,C). ables. We then \nreplace dereference operations with read opera\u00adtions. Since the contents of modi.ables are accessible \nonly locally within the body of the read, when inserting a read operation, we identify the part of the \ncode that becomes the body of the read. In ourexample, we insertaread for accessing the visitor .ag (line \n12). Since read operations can only return unit, we need to allocate a modi.ableforthe resulttobe written(line7).To \nallocatethe result modi.able, we use an equality function on the result type provided as an argument \nto the depthFirstSearch function.We .nish the transformation by memoizing the dfs function. This requires \ncre\u00adating a memo function (line 11) and applying it to the recursive branch of the case statement; since \nthe base case performs con\u00adstantwork,it does not bene.t from memoization.For brevity,in the code, we \nuse # for hashMod. One application of DFS is topological sort, which requires .nd\u00adinganorderingofnodeswhereeveryedgegoesfroma \nsmallertoa largernode.As anexample, consider the graphsin Figure3, where each node is tagged with the \n.rst and the last time theywere visited by the DFS algorithm.For node A these are 1 and 16, respectively. \nThe topological sort of a graph can be determined by sorting the nodes according to their last-visit \ntime, e.g., Cormen et al. (1990). In Figure 3, the left graph is sorted as A,B,C,D,E,F,G,H and the right \ngraph is sorted as A,B,F,G,H,C,D,E.We can compute the structure SAIL: SELF ADJUSTING = ... structure \nGraph = struct fun fromFile s = ... fun node i = ... fun newEdgeFrom (i) = ... fun DFSVisitSort = ... \n end fun test (s,i,j) = let val = SAIL.init () val (root,graph,n) = Graph.fromFile s val r = depthFirstSearch \nGraph.DFSVisitSort (root) val nr = Graph.newEdgeFrom (i) val () = SAIL.change nr (Graph.node j) val () \n= SAIL.propagate (); in r end Figure 4. Exampleof changing input and changepropagation. topological \nsort of a graph by using the depthFirstSearch func\u00adtion (Figure 2). To do this, we .rst de.ne the result \ntype and its equality function, in this case a list consisting of the identi.ers of the nodes in topologically \nsorted order. Since modi.able references accept equality, we can use ML s equals operator for comparing \nmodi.ables as follows. datatype a list = nil | cons a * ( a list) mod fun eqList (a,b) = case (a,b) of \n(nil,nil) => true | (ha::ta,hb::tb) =>ha=hb andalso ta=tb | => false We then writea visitor function \nthat concatenates its argument lists (if any), and then inserts the node being visited at the head of \nthe resulting list. This ordering corresponds to the topological sort ordering because a node is added \nto the beginning of the ordering after all of its out-edges are traversed. We can sort a graph with depthFirstSearch \nby passing the eqList function on lists and the visitor function. 2.3 Propagation In self-adjusting \ncomputation, after the programmerexecutesapro\u00adgram, she can change the input to the program and update \nthe out\u00adputbyperforming change propagation. The programmer can repeat this change-and-propagate process \nwith different input changes as many times as desired. Figure 4 shows an example that uses depthFirstSearch \nto perform a topological sort. The example assumes an implementation of a library, SAIL, that supplies \nprim\u00aditives for self-adjusting computation and a Graph library that sup\u00adplies functions for constructing \ngraphs from a .le, .nding a node, making an edge starting at a particular node, etc. The test function \n.rst constructs a graph from a .le and then computes its topological sort using depthFirstSearch. The \nDFSVisitSort function from the Graph library, whose code we omit, is a typical visitor that can be used \nwith depthFirstSearch as described above. After the initial run is complete, the test func\u00adtion inserts \na new edge from node i to node j as speci.ed by its arguments.To insert the new edge, the function .rst \ngetsa modi.\u00adable for inserting the edge at i and then changes the modi.able to point to node j. The function \nthen performs change-propagation to updatethe result.Forexample, after sortingthegraphin Figure3, we \ncan insert the edge from H to C and update the topological-sort by performing change propagation. 2.4 \nPerformance We show that the self-adjusting version of the standard DFS algo\u00adrithmrespondsto changesef.ciently.Forthe \nproof,we introduce Values v ::= () | n | x | l | .x. e | (v1,v2) | inl v | inr v Prim Ops o ::= + |-| \n= | < | ... Exprs e ::= v | o (v1,...,vn) | v1 v2 | mod v | read v as x in e | write v1 . v2 |memo e \n| let x = e1 in e2 | fst v | snd v |case v of inl x1 . e1 | inr x2 . e2 Figure 5. Syntax some terminology. \nLet G be an ordered graph, i.e., a graph where the out-edges are totallyordered. Consider performinga \nDFS on G such that the out-edges of each node are visited in the order spec\u00adi.ed by their total order. \nLet T be the DFS-tree of the traversal, i.e., the tree that consists of the edges (u, v) whose destinations \nv are not visited during the time that the edge is traversed. Con\u00adsider now a graph G. that is obtained \nfrom G by inserting/deleting an edge. Consider performing DFS on G. and let T . be its DFS\u00adtree.We de.ne \nthe affected nodes as the nodes of T (or G)whose paths to the root are different in T and T '. Figure \n3 shows two example graphs G and G', where G. is obtained from G by insert\u00ading the edge (H,C). TheDFS-trees \nof these graphs consist of the thick edges. The affected nodes are C, D, and E, because these are the \nonly nodes that are accessible through the newly inserted edge (H,C) from the root A. Based on these \nde.nitions, we prove that DFStakes time pro\u00adportional to the number of affected nodes. Since the total \ntime will dependon the visitor(f)that determines the result of the DFS, we .rstshowa bound disregarding \nvisitor computations.Wethen con\u00adsideraparticular instantiationofthe visitorfor performing topolog\u00adical \nsort and show that the same bound holds for this application as well.For the proofs, which willbegivenin \nSection 5.5 after the change-propagation algorithm has been described, we assume that each node has constant \nout-degree. Theorem 2.1 (DFS Response Time). Disregarding the opera\u00adtions performed by the visitor, the \ndepthFirstSearch program responds to changes in time O(m), where m is the number of af\u00adfected nodes after \nan insertion/deletion. Our bound for topological sort is the same as that for DFS, i.e., we only pay \nfor those nodes that are affected. Theorem 2.2 (Topological Sort). Change propagation updates the topological \nsort of a graph in O(m) time where m is the number of affected nodes.  2.5 Implementation We present \ntechniques for implementing the library ef.ciently in Section5.A prototype implementationof the libraryisavailable \non the web page of the .rst author. 3. The Language In this section, we present our Self-Adjusting Imperative \nLanguage SAIL. Since our consistencyproof does not depend on type safety, we leave our language untyped. \nFor simplicity, we assume all expressions to be in A-normal form (Felleisen and Hieb 1992). Unlike in \nour previous work where it was necessary to enforce a write-once policyfor modi.able references, we do \nnot distinguish between stable and changeable computations. This simpli.es the syntax of SAIL considerably. \nModi.able references now behave much like ordinary ML-style references: they are initialized at creation \ntime and can be updated arbitrarily often by the program. 3.1 Values and expressions The syntax of the \nlanguage is shown in Figure 5.Value forms v include unit (), variables x, integers n, locations l, .-abstractions \n.x. e, pairs of values (v1,v2), and injections inl v and inr v into a sum. Expressions e that are not \nthemselves values v can be applica\u00adtions of primitive operations o (v1,...,vn) (where o is something \nlike +, -, or <), function applications v1 v2, allocation and initial\u00adization of modi.able references(mod \nv), scoped read-operations read v as x in e that bind the value stored at the location given by v to \nvariable x and execute e in the scope of x, write-operations write v1 . v2 that store v2 into the location \ngiven by v1, let\u00adbindings let x=e1 in e2,projections from pairs(fst v and snd v), and case analysis on \nsums(case v of inl x1 . e1 | inr x2 . e2). The form memo e marks an expression that is subject to memoiza\u00adtion: \nevaluation of e may take advantage of an earlier evaluation of the same expression, possibly using change \npropagation to account for changes to the store. 3.2 Traces Change propagation requires access to the \nhistory of an evalua\u00adtion.Ahistoryis representedbya trace, andeveryevaluation judg\u00adment speci.es an output \ntrace. The syntax of traces is as follows: T ::= e | let T1 T2 | mod l . v | read l.x=v.e T | write l \n. v Traces can be empty(e), combine two sub-traces obtained by evaluating the two sub-terms of a let-form(let \nT1 T2), or record the allocation of a new modi.able reference l that was initialized to v (mod l . v).A \ntrace of the form read l.x=v.e T indicates that reading l produced a value v that was bound to x for \nthe evaluation of e which produced the sub-trace T . Finally, the trace write l . v records that an existing \nlocation l s contents have been updated to contain the new value v. The only difference between the traces \nwrite l . v and mod l . v is that the former is not counted as an allocation: alloc(write l . v)= \u00d8 while \nalloc(mod l . v)= {l}. In general, alloc(T ) denotes the set of locations that appear in mod l . v within \nthe trace T (formal de.nition elided). 3.3 Stores As mentioned in Section 1, the actual implementation \nof the store maintains multiple time-stamped versions of the contents of each cell. In our formal semantics, \nthe version-tracking store is present implicitlyinthe trace:tolookupthe currentversionofthe contents \nof l atagivenpointoftheexecution,onecan simplywalkthe global trace backwards up to the most recent write \noperation on l. Formalizing this idea, while possible, would require the seman\u00adtics to pass around a \nrepresentation of a global trace representing execution from the very beginning up to the current program \npoint. Moreover, such a trace would need more internal structure to be able to support change propagation. \nThe alternative formalization that we use here takes advantage of the following observation: at anygiven \npoint in time we only need the current view of the globalversion-keeping store.We refer to this view \nas the store because it simply maps locations to values. Thus, instead of representing the version store \nexplicitly, our semanticskeeps trackofthe changestothe current view of the version storeby manipulating \nordinary stores.  3.4 Operational Semantics The operational semantics consists of rules for deriving \nevaluation judgments of the form s, e .k v, s',T , which should be read as: In store s, expression e \nevaluates in k steps to value v, resulting in store s'. The computation is described by trace T . Step \ncounts are irrelevanttotheevaluation itself,butwe will use them laterin the logical relation we formulate \nfor reasoning about consistency. The rules for deriving evaluation judgments are shown in Figure 6. The \nrule for memo has a premise of the form s, T k s',T '. This is a change propagation judgment and should \nbe read as: v = app(o, (v1,...,vn)) (value)(primop) s, v .0 v,s,e s, o (v1,...,vn) .1 v, s, e v1 = .x.e \ns,e[v2/x] .k v3,s',T1 (apply) .k+1 s, v1 v2 v3,s',T1 s, e1 v1,s1,T1 .k1 s1,e2[v1/x] .k2 v2,s2,T2 alloc(T1) \nn alloc(T2)= \u00d8 (let) .k1+k2+1 s, let x = e1 in e2 v2,s2, let T1 T2 (mod) s, mod v .1 l, s[l . v], mod \nl . v s, e[s(l)/x] .k (),s',T (read) s, read l as x in e .k+1 (),s', read l.x=s(l).e T (write) s, write \nl . v .1 (),s[l . v], write l . v s0,e .k0 v, s0',T0 s, T0 .k s',T (memo) .k0+k s, memo e v,s',T (fst)(snd) \ns, fst (v1,v2) .1 v1,s,e s, snd (v1,v2) .1 v2, s, e s, e1[v/x1] .k v',s',T (case/inl) .k+1 s, case inl \nv of inl x1 . e1 | inr x2 . e2 v ',s',T s, e2[v/x2] .k v',s',T (case/inr) .k+1 s, case inr v of inl \nx1 . e1 | inr x2 . e2 v ',s',T Figure 6. Evaluation Rules The computation described by T is adjusted \nin k steps to a new computation described by T ' and a corresponding new store s'. The rules for derivingchange \npropagation judgments are shown in Figure7.Memoizationis modeled (Figure6)by startingat some previous \nevaluation of e (in some other store s0)that is now adjusted to the current store s. Evaluation rules. \nValues and primitive operations, which are considered pure, add nothing to the trace (rules value, primop). \nApplication evaluates the body of the function after substituting the argument for the formal parameter. \nThe resulting trace is the one produced while evaluating the body (rule apply). A let\u00adexpression is evaluated \nby running the two sub-terms in sequence, substituting the result of the .rst for the bound variable \nin the sec\u00adond. The trace is the concatenation (using the let-constructor for traces) of the two sub-traces \n(rule let). Evaluating mod v picks a location l, stores v at l, and returns l. This action (including \nl and v)is recorded in the trace (rule mod). A read-expression substitutes the value stored at location \nto be read for the bound variable in the body. Evaluating the resulting body must return the unitvalue \n(). The read-operation including location, boundvari\u00adable, value read, and body is recorded in the trace \n(rule read).A write-operation modi.es the store by associating the location to be written with the new \nvalue. The result of a write is unit. Both value and location are recorded in the trace (rule write). \nEvaluation of a memo-expression is non-deterministic. When evaluating anexpression memo e inastore s, \nwe can either reuse an evaluation of e in some arbitrary ( previous ) store s0 not neces\u00adsarily the same \nas the current store s provided that the evaluation canbe adjusted to the current store via change propagation, \nor we can evaluate e from scratch in the current store s. The correspond\u00ad (empty) s, e 0 s, e (mod) s, \nmod l . v 0 s[l . v], mod l . v (write) s, write l . v 0 s[l . v], write l . v s, T1 k1 s ' ,T ' 1 k2 \n''' s ' ,T2 s '' ,T alloc(T1) n alloc(T2)= \u00d8 2 (let) k1+k2 s '' '' s, let T1 T2 , let T 1 T2 k ' s(l)= \nv s,T s ' ,T (read/no ch.) k ' s, read l.x=v.e Ts ' , read l.x=v.e T ' s(l) s, e[s(l)/x] .k (),s ' ,T \n= v (read/ch.) k+1 s '' s, read l.x=v.e T, read l.x=s(l).e T Figure 7. Change Propagation Rules ing evaluation \nrules, memo/hit and memo/miss respectively, may be written as follows: s0,e . v, s 0' ,T0 s, T0 s ' ,T \n(memo/hit) s, memo e . v, s ' ,T s, e . v, s ' ,T (memo/miss) s, memo e . v, s ' ,T Our evaluation rule \nfor memo (Figure 6) does not distinguish be\u00adtween memo hits and memo misses. The high degree of freedom \nin the choice of s0 makes a memo miss a special case of a memo hit: in the memo-rule, to simulate a memo \nmiss we pick s0 = s. If evaluation of e in s produces a trace T , then change propagation of trace T \nin store s does nothing (i.e., yields the same s and T ). Hence, picking s0 = s captures the essence \nof the memo miss evaluation proceeds directly in the current store s, not some other previous store s0. \nThe rules fst and snd are the standard projection rules. Pro\u00adjections are pure and, therefore, add nothing \nto the trace. Similarly, rules case/inl and case/inr are the standard elimination rules for sums. In \neach case, the trace records whatever happened in the branch that was taken. The case analysis itself \nis pure and does not need to be recorded. The step counts in each of the evaluation rules are entirely \nstraightforward we simply count each operational step as we would in a small-step operational semantics. \nThe one exception is the memo rule: notice that according to the rule, memo v would evaluate to v in \nzero steps (since evaluation of v in s0 and change propagation of the resulting empty trace would both \ntake zero steps). Intuitively, this re.ects a completely arbitrary decision on our part to treat memo \nas a coercion (i.e., a zero-step operation) rather than as an actual operational step.Treating memo as \na one\u00adstep operation would work just as well, though our proofs would have to be adjusted accordingly. \nChange-propagation rules. Rule empty is the base case and deals with the empty trace. The mod-and write-rules \nre-execute their respectivewrite operationsto the store. There are twopossible scenarios that make this \nnecessary: (1) there may have been a write operation that altered the contents of the location after \nit was originally created or written, or (2) in the rule for memo, the original store s0 was so different \nfrom s that the location in question has a different value (or does not even exist) in s. The def s : \n. . L = Sl.L L = . . FL(s(l)) . dom(s) .L . .L .L. . .L . (.l .L . FL(s(l)) .L )=.L = L Figure 8. Store \nReachability Relation let-rule simply performs change propagation on each of the sub\u00adtraces. The remaining \ntwo rules are those for read one for the case that there is no change, the other for the case that there \nis a change. When the value at the location being read is still the same as the one recorded in the trace, \nthen change propagation simply presses on (rule read/noch.). If the value is not the same, then the old \nsub-trace is thrown away and the body of the read is re\u00adevaluated with the new value substituted for \nthe bound variable (rule read/ch.). Notice that as long as there is no change, the rules for change\u00adpropagation \ndo not increment the step count, because unchanged computationshave alreadybeen accountedforbythe memo \neval\u00aduation rule. Discussion. Our rules are given in a non-deterministic, declar\u00adative style.Forexample, \nthe mod-rule does not place anyspecial requirements on the location being allocated, i.e., thelocation \ncould alreadyexistin the store.For correctness,however, we insist that all locations allocated during \nthe course of the entire program run be pairwise distinct. (This is enforced by side conditions on our \nlet-rules.) Furthermore, allocated locations must not be reachable from the initial expression (see Section \n3.5). As in our previous work (Acar et al. 2007b), the ability for mod to allocate an existing (garbage-) \nlocation during change propa\u00adgation is crucial, since otherwise change propagation would not be able \nto retain any previous allocations. The ability to allocate an existing garbage location during ordinary \nevaluation is not as important,but disallowing the possibility (e.g.,by addinga side\u00adcondition of l . \ndom(s) to the premise of the evaluation rule for mod)would have two undesirable effects: it would weaken \nour re\u00adsult by reducing the number of possible evaluations, and it would make our formal framework for \nreasoning about program equiva\u00adlence more complicated. Evaluating a read-form returns the unit value. \nTherefore, the onlywayfor thebodyofthe read-form to communicate to the rest of the program is by writing \ninto other modi.able references, or even possibly the same reference that it read. This convention guar\u00adantees \nstability of values and justi.es the rule for memo where we return the value computed during an arbitrary \nearlier evaluation in some other store s0. The value so computed cannot actually de\u00adpend on the contents \nof s0. It can, of course, be a location pointing to values that do depend on s0,but those will be adjusted \nduring change propagation.  3.5 ReachabilityandValidEvaluations Consistencyholds only for so-called \nvalid evaluations. Informally, an evaluation is valid if it does not allocate locations reachable from \nthe initial expression e. Our technique for identifying the locations reachable from an expression is \nbased on the technique used by Ahmed et al. (2005) in their work on substructural state. Let FL(e) be \nthe free locations of e, i.e., those locations that are subexpressions of e. The locations FL(e) are \nsaid to be directly accessible from e. The store reachability relation s : . . L (Figure 8) allows us \nto identify the set of locations L reachable in a store s from a set of root locations .. The relation \ns : . . L requires that the reachable set L include the root locations . as well as all locations directly \naccessible from each l .L. It also ensures that all reachable locations are in s. Furthermore, it requires \nthat L be minimal that is, it ensures that the set L does not contain any locations not reachable from \nthe roots. Thus, L is the set of locations reachable from an expression e in a store s iff s : FL(e) \nL. We de.ne valid evaluations s, e .k ,T as follows. ok v, s ' De.nition 3.1 (Valid Evaluation). def \ns, e .k v, s ' ,T = s, e .k v, s ' ,T . ok .L.s : FL(e) L . Ln alloc(T )= \u00d8 4. Consistency via Logical \nRelations In this section, we prove that the semantics of SAIL is consis\u00adtent i.e., that the non-determinism \nin the operational semantics is harmless by showing that any two valid evaluations of the same program \nin the same store yield observationally (contextu\u00adally) equivalent results. 4.1 Contextual Equivalence \nAcontextC is anexpression witha holein it.We write C : (G) to denote that C is a closed context (i.e. \nFV (C)= \u00d8)that provides bindings for variables in the set G. Thus, if FV (e) . G, then C[e] isa closed \nterm.We write s : . as shorthand for: .L.s : . L. We say e1 contextually approximates e2 if, given an \narbitrary C that provides bindings for the free variables of both terms, running C[e1] ina store s (that \ncontains all the appropriate roots) returns n, then (1) there exists an evaluation for C[e2] in s, and \n(2) all such evaluations also return n. De.nition 4.1 (Contextual Equivalence). Let G= FV (e1) . FV (e2). \ndef G e1 ctx e2 = .C : (G). .s, ., n. . = FL(C) . FL(e1) . FL(e2) . s : . . s, C[e1] .ok n, -, - =. ( \n.v. s,C[e2] .ok v, -, - ) . ( .v. s,C[e2] .ok v, -, - =. n = v ) def G e1 ctx G e1 ctx G e2 ctx e2 = \ne2 . e1 4.2 Proving Consistency Having de.ned contextual equivalence, we can be more precise about what \nwe mean by consistency: if e is a closed program, we wish to show that \u00d8f e ctx e, which means that if \nwe run C[e] (where C is an arbitrary context) twice in the same store s,then we get the same result value \nn. It is dif.cult to prove \u00d8f e ctx e directly due to the quan\u00adti.cation over all contexts in the de.nition \nof ctx . Instead we use the standard approach of using a logical relation in order to prove contextual \nequivalence that is, we will show that any term e is logically related to itself (Theorem 4.5), and that \nthe latter implies that e is contextually equivalent to itself (Theorem 4.6). Logical relations specify \nrelations on terms, typically via struc\u00adtural induction on the syntax of types. (Since SAIL is untyped, \nwe will de.ne a logical relation via induction on (available) steps as discussed below.) Thus, for instance, \nlogically related functions take logically related arguments to related results, while logically related \npairs consist of components that are related pairwise.Two expressions are logically relatedif either \ntheybothdiverge, or they both terminate and yield related values. For any logical relation, one must \n.rst prove the so-called Fundamental Property of the log\u00adical relation (also called the Basic Lemma) \nwhich says that any (well-typed) term is related to itself. If the logical relation is in\u00adtended to be \nused for contextual equivalence, the next step is to show that if two terms are logically related, then \nthey are contex\u00adtually equivalent, which typically follows from the Fundamental Property. Since our logical \nrelation is intended to be used to prove consistency,we will show that anyterm e that is logically related \nto itself by the Fundamental Property of our logical relation, this is true of every e is contextually \nequivalent to itself (i.e., e ctx e). The two sources of non-determinism in SAIL are allocation and memoization. \nSince they differ in nature, we deal with them us\u00ading different techniques. The non-determinism causedbyallocation \nonly concernstheidentityof locations.Wehandlethisbymaintain\u00ading a bijection between the locations allocated \nin different runs of the same program. We use the meta-variable S to denote sets of location pairs.We \nde.ne the following abbreviations: S1 ={ l1 | (l1,l2) .S} S2 ={ l2 | (l1,l2) .S} We de.ne the set of \nlocation bijections as follows: def bij (S) = .l . S1 . .!l2 . S2 . (l1, l2) . S . .l . S2 . .!l1 . S1 \n. (l1, l2) . S LocBij = { S . 2Locs\u00d7Locs | bij (S) } When both runs execute a mod v, we extend the bijection \nwith the pair of locations (l1,l2) returned by mod v. Notice that it will always be possible to prove \nthat the result is a bijection because valid evaluations cannot reuse reachable locations. If we start \nwith identical programs (modulo the location bijec\u00adtion), then they will execute in lock-step until they \nencounter a memo, at which point the derivation trees for the evaluation judg\u00adments can differ dramatically. \nThere is no way of relating the two executions directly. Fortunately, this is not necessary, since they \nneed to be related only after change propagation has brought them back into sync.Akey insightis thatat \nsuchsync pointsitisal\u00adways possible to establish a bijection between those locations that are reachable \nfrom each of the two running programs. To show that change propagation does,infact, bring thetwoexecutions \ninto sync, we prove that each memo hit can be replaced by a regular evaluation (Section 4.7). Our logical \nrelation for consistency of SAIL is based on the step-indexed logical relations for purely functional \nlanguages by Appel and McAllester (2001) and Ahmed (2006). In those models, the relational interpretation \nV[t] of a(closed) type t isa setof triples of the form (k, v1,v2) where k is a natural number (called \nthe approximation index or step index)andv1 and v2 are closed values. Intuitively,(k, v1,v2) .V[t ] says \nthat in anycomputation running for no more than k steps, v1 approximates (or looks like ) v2. Informally, \nwe say that v1 and v2 are related for k steps. Anovel aspect of the logical relation that we present \nbelow is that it is untyped that is, it is indexed only by step counts, unlike logical relationsinthe \nliteraturewhicharealwaysindexedbytypes (or in the case of prior step-indexed logical relations e.g., \nAppel and McAllester (2001); Ahmed et al. (2005); Ahmed (2006) by both types and step counts). Another \nnovelty is the way in which our model tracks related\u00adnessofthe storesofthetwo computations.The intuitionisto \nstart at those variables of each program that point into the respective stores (i.e.,the rootsofa tracinggarbage \ncollector),and construct graphs of the reachable memory cells by following pointers. Then the two program \nstores are related for k steps if (1) these graphs are isomorphic, and (2) the contents of related locations \n(i.e., bi\u00adjectively related vertices of the graphs) are related for k - 1 steps. (Since reading a location \nconsumes a step, k - 1 suf.ces here.) 4.3 RelatedValues The value relation V speci.es when two values \nare related. V is a set of tuples of the form (k, ., v1,v2), where k is the step index, v1 and v2 are \nclosed values, and . . LocBij is a local store description. A set of beliefs . is a bijection on the \nlocations directly accessible from v1 and v2 (i.e., FL(v1),FL(v2)).We refer to the locations in .1 and \n.2 as the rootsof v1 and v2,respectively. V = { (k, {}, (), ( )) }. { (k, {}, n, n) }. { (k, {(l1,l2)},l1,l2) \n}. { (k, .c, .x. e1, .x. e2) | .j <k. ..a,v1,v2. (j, .a,v1,v2) .V . (.c 8 .a) de.ned =. (j, .c 8 .a,e1[v1/x],e2[v2/x]) \n.C} . '' { (k, . 8 . ' , (v1,v ), (v2,v )) | 12 '' (k, ., v1,v2) .V . (k, . ' ,v 1,v ) .V} . 2 { (k, \n., inl v1, inl v2) | (k, ., v1,v2) .V} . { (k, ., inr v1, inr v2) | (k, ., v1,v2) .V} C = { (k, .s,e1,e2) \n|.j <k. .s1,s2,.r, S,v1,s ' ,T1. 1 s1,s2 :k (.s 8 .r) S. s1,e1 .j v1,s ' ,T1 . 1 S1 n alloc(T1)= \u00d8 =. \n( .v2,s ' ,T2. 2s2,e2 . v2,s 2' ,T2 . S2 n alloc(T2)= \u00d8 =. ..f , Sf . (k - j, .f ,v1,v2) .V . s ' ,s \n' :k-j (.f 8 .r) Sf . 12 S1 .S1 . alloc(T1) . f S2 .S2 . alloc(T2)) } f G[\u00d8] = { (k, {}, \u00d8, \u00d8) } G[G,x] \n= { (k, .G 8 .x,.1[x . v1],.2[x . v2]) |(k, .G,.1,.2) .G[G] . (k, .x,v1,v2) .V} def G e1 e2 = .k = 0. \n..G,.1,.2. (k, .G,.1,.2) .G[G] =. (k, .G,.1(e1),.2(e2)) .C def G e1 e2 =G e1 e2 . G e2 e1 (where G= \nFV (e1) . FV (e2)) Figure 9. Logical Relation j def .1 . .2 if (.1 . .2) . LocBij .1 8 .2 = unde.ned \notherwise Figure 10. Join Local Store Descriptions The de.nition of the value relation V is given in \nFigure 9. The value () is related to itself for any number of steps. Clearly, no locations appear as \nsubexpressions of ();hence, the de.nition dem\u00adands an empty local store description {}. Similarly,integers \nn1 and n2 are related under the empty store description if they are equal. Two locationsl1 and l2 are \nrelated if the local store description says that they are related. Furthermore, from the values l1 and \nl2, the only locations that are directly accessible are, respectively, the locations l1 and l2 themselves. \nHence, the local store description must be {(l1,l2)}. The pairs (v1,v 1' ) and (v2,v 2' ) are related \nfor k stepsif thereex\u00adist local store descriptions . and . ' such that the components of the pairs are \nrelated (i.e., (k, ., v1,v2) .V and (k, . ' ,v 1' ,v 2' ) .V) and if . and . ' canbe combined intoasingle \nsetof beliefs (written . 8 . ', see Figure 10). Informally, two local store descriptions . and . ' can \nbe combined only if they are compatible; that is, if the beliefs in . do not contradict the beliefs in \n. ', or more precisely, if the union of the two bijections is also a bijection. The left (right) injections \nintoa sum inl v1 and inl v2 (inr v1 and inr v2)with local store description. are related for k steps \nif v1 and v2 are related for k steps with the same local store description (i.e., (k, ., v1,v2) .V). \ns1, s2 :k . S def = S . LocBij . .F. : S . LocBij . S = . 8 J(l1,l2).S F.(l1, l2) . dom(s1) . S1 . dom(s2) \n. S2 . .(l1, l2) . S. .j < k. (j, F.(l1, l2), s1(l1), s2(l2)) . V Figure 11. Related Stores  Since functions \nare suspended computations, their relatedness is de.ned in terms of the relatedness of computations (Section \n4.5). Two functions.x. e1 and .x. e2 with local store description .c where .c describes at least the \nsets of locations directly accessible from the closures of the respective functions are related for k \nsteps if, at some point in the future, when there are j<k steps left to execute, and there are related \narguments v1 and v2 such that (j, .a,v1,v2) .V, and the beliefs .c and .a are compatible, then e1[v1/x] \nand e2[v2/x] are related as computations for j steps. Note that j must be strictly smaller than k. The \nlatter requirement is essential for ensuring that the logical relation is well-founded (despite thefact \nthat it is not indexed by types). Intuitively, j<k suf.ces because beta-reduction consumes a step. Notice \nthat the step-indexed technique of de.ning a logical re\u00adlationyieldsnotonlya speci.cationofthe relation,butalsoguar\u00adantees \ntheexistenceof the relationby making itswell-foundedness explicit. A crucial property of the relation \nV is that it is closed under decreasing step index intuitively, if v1 looks like v2 for upto k steps, \nthen theyshould look alike for fewer steps. Lemma 4.2 (Downward Closed). If (k, ., v1,v2) .V and j = \nk, then (j, ., v1,v2) .V. 4.4 Related Stores The store satisfaction relation s1,s2 :k . S (see Figure \n11) says that the stores s1 and s2 are related (to approximation k)at the local store description . and \nthe global store description S (where S. LocBij ).Wemotivate the de.nitionof s1,s2 :k . S by analogy \nwitha tracinggarbage collector. Here . correspond to (beliefs about) the portions of the stores directly \naccessible from a pair of values (or multiple pairs of values, when . corresponds to 8-ed store descriptions). \nHence, informally . corresponds to the (two sets of) root locations. Meanwhile, S corresponds to the \nset of reachable (root and non-root) locations in the two stores that would be discovered by the garbage \ncollector.1 In the de.nition of s1,s2 :k . S, the function F. maps each location pair (l1,l2) .S to a \nlocal store description. It is our intention that, for each pair of locations (l1,l2), F.(l1,l2) is an \nappropriate local store description for the values s1(l1) and s2(l2). Hence, we can consider (F.(l1,l2))1 \nas the set of child locations traced from the contents of l1 in store s1 (and similarly for (F.(l1,l2))2 \nand the contents of l2 in s2). Having chosen F., we must ensure that the choice is consistent with S, \nwhich should in turn be consistent with the stores s1 and s2. The global store description S combines \nthe local store descriptions of the roots with the local store descriptions of the contents of every \npair of related reachable locations; the implicit requirement that S is de.ned ensures that the local \nbeliefs of the roots and all the (pairs of) store contents are all compatible. The clauses dom(s1) .S1 \nand dom(s2) .S2 require that all of the reachable locations are actually in the two stores. Finally, \n(j, F.(l1,l2),s1(l1),s2(l2)) .V ensures that the contents of 1To be precise, our de.nition requires only \nthatS include the set of reach\u00adable locations. locations l1 and l2 (in stores s1 and s2, respectively) \nwith the local store description assigned by F. are related (for j<k steps). Note that we do not require \nthat S be the minimal set of loca\u00adtions reachable from the roots .. Such a requirement can be added but, \nas we will explain, is not necessary. 4.5 Related Computations The computation relation C (see Figure \n9) speci.es when two closed terms e1 and e2 (with beliefs ., again corresponding to at least the locations \nappearing as subexpressions of e1 and e2) are related for k steps. Informally, C says that if e1 evaluates \nto a value v1 in less than k steps and the evaluation is valid, then given any valid evaluation of e2 \nto some value v2, it must be that v1 and v2 are related (with beliefs .f ). More precisely, we pick two \nstarting stores s1 and s2 and a global store description S such that s1,s2 :k (.s 8 .r) S, where .r is \nthe set of beliefs about the two stores held by the rest of the computation, i.e., the respective continuations. \nIf a valid evaluation of (s1,e1) (where locations allocated duringevaluation are disjoint from those \ninitially reachable in S1)results in(v1,s 1' ,T1) in j<k steps, then given anyvalidevaluations2,e2 . \nv2,s 2' ,T2 (which may take any number of steps), the following conditions should hold: 1. There must \nexist a set of beliefs .f such that the values v1 and v2 are related for the remaining(k - j)number of \nsteps. 2. The following two sets of beliefs must be compatible: .f (what v1 and v2 believe) and .r (what \nthe continuations believe note that these beliefs remain unchanged). 3. There must exist a set of beliefs \nSf about locations reachable from the new roots(.f 8 .r)such that the .nal storess1 ' and s2 ' satisfy \nthe combined set of local beliefs(.f 8 .r)and the global beliefs Sf for the remaining k - j steps. 4. \nThe set of reachable locations in s1 ' (and s2' ),givenby Sf 1 (and Sf 2), must be a subset of the locations \nreachable before evaluat\u00ading e1 (respectively e2) given by S1 (respectively S2) and  the locations allocated \nduring this evaluation. As noted earlier, the global store description S is not required to be the minimal \nset of locations reachable from the roots(.s8.r),it only needs to include that set. This suf.ces because \nSf 1 and Sf 2 only need to be subsets of S1 and S2 and the locations allocated during evaluation(alloc(T1) \nand alloc(T2)). Thus, even though we may pick larger-than-necessary sets at the beginning of the evaluation, \nwe can add to them in a minimal way as the two evaluations progress. 4.6 Related Substitutions and OpenTerms \nLet G= FV (e1) . FV (e2).We write G f e1 . e2 (pronounced e1 approximates e2 ) to mean that for all k \n= 0, if .1 and .2 (mapping variables in G to closed values) are related substitutions with beliefs .G \n(which is the combined local store description for the values in the range of .1 and .2), then .1(e1) \nand .2(e2), with root beliefs .G, are related as computations for k steps.We write G f e1 e2 when e1 \napproximates e2 and vice versa, meaning that e1 and e2 are observationally equivalent.  4.7 Memo Elimination \nWewish to proveG f e e from which consistency theproperty that anytwo valid evaluations of a closed \nterm e in the same store yield observationally equivalent results follows as a corollary. The proof of \nG f e e proceeds by induction on the structure of e (see Theorem 4.5). Unfortunately, in the case of \nmemo e, we cannot directly appeal to the induction hypothesis. To see why, consider the special case \nof the closed term memo e.We must show VM = { (k, ( )) }.{ (k, n) }.{ (k, l) }. { (k, .x.e) |.j <k. .v. \n(j, v) .VM =. (j, e[v/x]) .CM }. { (k, (v, v ' )) | (k, v) .VM . (k, v ' ) .VM }. { (k, inl v) | (k, \nv) .VM }. { (k, inr v) | (k, v) .VM } CM = { (k, e) |.j <k. .s0,s ' , s, s ' , v, T, T ' ,j1,j2. 0 s0,e \n.j1 v, s ' ,T . s, T j2 s ' ,T ' . 0j = j1 + j2 =. ' s, e .=j v, s ' ,T . (k - j, v) .VM } GM= { (k, \n\u00d8) } [\u00d8] GM[G,x] = { (k, .[x . v]) | (k, .) .GM[G] . (k, v) .VM } def G e = .k = 0. ... (k, .) .GM=. \n(k, .(e)) .CM [G] Figure 12. Logical Predicate for Memo Elimination (k, {}, memo e, memo e) .C. Suppose \n(1) s1,s2 :k {} 8 .r S, (2) s1, memo e .j v1,s 1' ,T1, and (3) S1 n alloc(T1)= \u00d8, where j<k. By the induction \nhypothesis we have \u00d8f e e and hence (k, {}, e, e) .C. In order to proceed, we must instantiate the latter \nwith two related stores(s1 and s2 are the only two stores we know of that are related) and provide a \nvalid evaluation of e in the .rst store (i.e., we need s, e .<k -, -,T where T is such that S1 n alloc(T \n)= \u00d8). From (2), by the operational semantics, we j2 s ' have s01,e .j1 v1,s ' 1,T1, where j 01,T01 and \ns1,T01 = j1 + j2. But we know nothing about the store s01 in which e was evaluated. What we need is a \nderivation for s1,e .=j v1,s 1' ,T1. That is, we must show that evaluation in some store s01 followed \nbychangepropagation yields the same results asafrom-scratch run in the store s1. To prove that each memo \nhit can be replaced by a regular evaluation (Lemma 4.4), we de.nea logical predicate (i.e.,a unary logical \nrelation) for memo elimination. Figure 12 de.nes VM and CM as sets of pairs (k, v) and (k, e) respectively, \nwhere k is the step index, v is a closed value, and e is a closed term. Essentially, (k, e) .CM means \nthat e has the memo-elimination property (i.e., j2 .=j1+j2 if s0,e .j1 v, s 0' ,T and s,T s ' ,T ', then \ns, e v, s ' ,T '), and if the combined evaluation plus change propagation consumed j = j1 + j2 steps, \nthen v has the memo-elimination property for the remaining k - j steps. Clearly, all values v have the \nmemo-elimination property: since v is already a value, it evaluates to itself in zero steps, producing \nthe empty trace, which means that change propagation takes zero stepsandleavesboth storeand trace unchanged.Sinceafunctionis \na suspended computation, we must require that its body also have the memo-elimination property for one \nfewer step (see Figure 12). Lemma 4.3 (Fundamental Property of Logical Predicate for Memo Elim). If G= \nFV (e), then G f e. Proof sketch: By induction on the step index k and nested induc\u00adtion on the structure \nof e. All cases are straightforward. The only interesting case is that of read/no ch. where we read a \nvalue v out of the store and then have to use the outer inductionhypothesis to showthat v has the memo-elim \nproperty fora strictly fewer number of stepsbefore we can plug v into the body of the read, appealing \ntothe inner inductionhypothesisto completethe proof. 0 Corollary 4.4 (Memo Elimination). Let e be a closed \nterm, possibly with free locations. If s0,e .j1 v, s 0' ,T and s, T j2 s ' ,T ', then s, e .=j1+j2 v, \ns ' ,T ' .  4.8 Consistency For lack of space we have omitted proof details here. Detailed proofs of \nall lemmas can be found in our extended technical re\u00adport (Acar et al. 2007a). Theorem 4.5 (Fundamental \nProperty of Logical Relation for Consistency). If G=FV (e), then G f e e. Proof sketch: By induction \non the structure of e. As explained above (Section 4.7), in the memo case we useLemma 4.4 before we can \nappeal to the inductionhypothesis. Other interesting cases include mod,wherethevalidevaluation requirement \n(thattheeval\u00aduation not allocate locations reachable from the initial expression) is critical in order \nto extend the bijection on locations; write, where thefact that the locations l1 and l2 being written \nto are reachable from the initial expression guarantees that (l1,l2) is already in the bijection; and \nread, where we need to know that the values be\u00ading read are related, which we can conclude from thefact \nthat the locations being read are reachable and related, together with the fact that related locations \nhave related contents which follows from store relatedness. 0 Theorem 4.6 (Consistency). If G= FV (e), \nthen G f e ctx e. Let us write .k \u00d8 instead of .k forevaluation judgments thathave at least one derivation \nwhere every use of the memo rule picks s0 = s. Such a derivation describes an evaluation without memo \nhits, i.e., that of an ordinary imperative program. Since memo elimination (Lemma 4.4) can be applied \nrepeatedly until no more memo-hits remain, we obtain the following result, which can be seen as a statement \nof correctness since it relates the self-adjusting semantics to an ordinary non-adjusting semantics: \n' v, s ' ,T . \u00d8 5. Implementation Wedescribe data structures and algorithms for implementingSAIL. 5.1 \nData Structures We use order-maintenance, searchable ordered-sets, and standard priority-queue data structures. \nOrder Maintenance(Time Stamps). An order-maintenance data structure maintains a set of time-stamps while \nsupporting all of the following operations in constant time: insert a newly created time\u00adstamp after \nanother, delete a time stamp, and compare two time stamps (Dietz and Sleator 1987). SearchableTime-Ordered \nSets. Atime-ordered set data structure that supports the following operations. new:return an empty set. \n build S:allocate and return a data structure containing all the elements in the set S.  insert (x, \nt):insert the elementx into the set at time t.  delete (x, t):delete the elementx with time t from the \nset.  find (t):return the earliest element (if any) in the set at timet or later.  prev (t):return \nthe element (if any) in the set precedingt.  If a data structure contains no more than one element with \na given time-stamp, then we can support all operations except for build in logarithmic time (in the size \nof the set) by using a balancedbinary searchtreekeyedbythe time-stamps.Ifthesizeof thesetis boundedbya \nconstant,thenwecansupportall operations Figure13. Amodi.able, its writes and reads before (top) andafter \nperforming a write (bottom).  in constant time by using a simple representation that keeps all elements \nin a list.  5.2 The Primitives To support the self-adjusting computation primitives we maintaina global \ntime line and a global priority queue. The time line is an in\u00adstanceofan order-maintenance data structure \nwith current-time pointing to the current time of the computation. During the ini\u00adtial run, the current-time \nis always the last time stamp,but during change propagation it can be anytime in the past. During evalua\u00adtion \nand change propagation, we advance the time by inserting a new time stamp t immediately after current-time \nand setting the current time to t. In addition to the current-time, we also main\u00adtain a time stamp called \nend-of-memo for memoization purposes. For change propagation, we maintain a global priority queue that \ncontains the affected readers prioritized by their start time (we de\u00ad.ne readers more precisely below). \nModi.able References. We represent a modi.able reference as a triple consisting of a version-set,a reader-set, \nand an equality function. The version-set and the reader-set are both instances of searchable, time-ordered \nsets. The version set contains all the different contents of the modi.able over time that is, it contains \npairs (v, t) consisting of a value v and a time stamp t. The reader set of a modi.able l contains all \nthe read operations whose source is l. More precisely, the reader set contains readers, each of which \nis a triple (ts,te,f) consisting of a start time ts, end time te, and a function f corresponding to the \nbody of the read operation. Based on this representation, the operations on modi.able ref\u00aderences can \nbe performed as follows. mod eq: Create an empty version-set and an empty reader-set. Return a pointer \nto the triple consisting of the equality function eq,theversion-set,andthe reader-set. Since pointersinML \nare equality types, so are modi.ables they can be compared by using ML s equal operator. read lf: Identify \ntheversion (v, tv) of the modi.able being read l that comes immediately before current-time byperforming \na combination of find and prev operations on the version set. Advance time to a new time stamp ts. Apply \nthe body of the read f to v. When f returns, advance time again to a new time\u00adstamp te. Insert the reader \nr consisting of the body and the time interval (ts,te) into the reader set of the modi.able being read. \nwrite lv: Advance time to a new tw. Create a new version with thevalue v being written at time tw. Insert \nit into the modi.able l being written. Since creating a new version for l can change the value that further \nreads of l may access, it can affect the readers whose start time comes after tw but before the next \nversion. To identify the affected readers, we check .rst that the value v being written is different \nthan that of the previous versionbyusingthe equality testof l;if not, then no readers are affected. Otherwise, \nwe .nd the readers that come at or after tw byrepeatedly performingfind operations on the reader set \nof l; we stop when we .ndareader that comes after the nextversion. Wethen delete these readers from the \nreader set and insert them into the priority queue of affected readers. Note that during the initial \nrun, all writes take place at the most recent time. Thus, there are no affected readers. deref l: Identify \ntheversion (v, t) of the dereferenced modi.able l at the current-time by using a find operation and return \nv. change lv: Identify the earliest version (v ' ,t) of the changed modi.able l (at the beginning of \ntime) by using a find oper\u00adation, change the value of this version to v. If v is equal to v ' , then \nthe change does not affect the readers of l. Otherwise, in\u00adserts all the readers of the initial version \ninto the priority queue. The readers can be found by .nding the next version (if any) and inserting all \nthe readers between the two versions. Figure13 illustrates aparticular representationof modi.ables assuming \nthat time stamps are real numbers and time-ordered sets are represented as sorted lists. The modi.able \nx points to a version list (versions are drawn as squares) consisting of versions at the speci.ed write; \nthe versions are sorted with respect to their times. Eachversionpointstoareaderlist (readersaredrawnas \ndiamonds) whose start andend times are speci.ed. The readers stored in the reader list of a version are \nthe ones that read that version; they are sorted with respect to their start times. Thus, all the readers \ntake place between the time of the version and the time of the next version.Forexample,inthetop .gure,all \nreadersofversion A take place between times 0.0 and 8.0; the readers of version C take place after 8.0. \nThe bottom .gure illustrates how the reads may be arranged if we create a new version B at time 4.0. \nWhen this happens the reader that starts at time 4.0 will become affected and will be inserted into the \npriority queue. Memoization and Change Propagation. Figure 14 shows the pseudo code for memoization and \nchange propagation. These oper\u00adations are based on an undo function for rolling back the effects of a \ncomputation between two time stamps. The undo function takes a start and an end time-stamp, ts and te \nrespectively, and undoes the computation between ts and te by deleting all the versions, readers, memo \nentries, and time stamps between ts and te. For each time stamp t between ts and te, it checks if there \nis a version, reader, or memo entry at t.To delete a reader starting at t, we drop it from both its reader \nset and the queue(ifitwasinsertedintothe priorityqueue).To deletea memo undo (ts,te)= for each t. ts \n<t<te do if there is a version v =(vt,t) then t ' . time of successor(v) delete version v from its version-set \nR .{r | (t1,t2,f) is a reader . t<t1 <t ' }for each r . R do delete r from its reader set if there is \na reader r =(t, , ) then delete r from its readers-set and from Q if there is a memo entry m =(t, , ) \nthen delete m from its table delete t from time-stamps memo () = let table . new memo table fun mfun \nkey f = case (find (table, key, now)) of NONE => t1 . advance-time () v . f () t2 . advance-time () \ninsert (v, t1,t2) into table return v SOME (v, t1,t2) => undo (current-time,t1) propagate (t2) return \nv in mfun end propagate (t)= while Q = \u00d8 do (ts,te,f) . checkMin (Q) if ts <t then deleteMin (Q) current-time \n. ts tmp . end-of-memo end-of-memo . te f () undo (current-time, te) end-of-memo . tmp else return \nFigure 14. Pseudo code for undo, memo, and propagate. entry that starts at t, we remove it from the memo \ntable. Deleting a version is more complicated because it can affect the reads that come after it by changing \nthe value that they read. To delete a version (v, t) of a modi.able l at time t, we .rst identify the \ntime t ' ofthe earliestversionof l that comesafterit.(If noneexists,then t ' will be t8.)We then .nd \nall readers between t and t ' and insert them into the priority queue; Since theymay now read a different \nvalue than theydid before, these reads are affected by the deletion of the version. To create memoized \nfunctions, the library provides a memo primitive. A memoized function has access to a memo table for \nstoring and re-using results. Each call takes the list of the argu\u00adments of the client function(key)and \nthe client function itself. Before executing the client function, a memo lookup is performed. If no result \nis found, then a start time stamp ts is created, the client is run, an end time stamp te is created, \nand the result along with in\u00adterval (ts,te) is stored in the memo table. If a result is found, then computations \nbetween the current time and the start of the memo\u00adized computation are undone, a change-propagation \nis performed on the computation being re-used, and the result is returned. A memo lookup succeeds if \nand only if there is a result in the memo table whose key is the same key as that of the current call \nand whose time interval is nested within the current time interval de\u00ad.ned by the current-time and end-of-memo. \nThis lookup rule is critical to correctness. Informally, it ensures that side-effects are incorporated \ninto the current computation accurately. (In the for\u00admal semantics, it corresponds to the integration \nof the trace of the re-used computation into the current trace.) Undoing the computation between current-time \nand the start of the memoized computation serves some critical purposes: (1) it ensures that all versions \nread by the memoized computation are updated, and (2), it ensures that all computations that contain \nthis computation are deleted and, thus, cannot be re-used. The change propagation algorithm takes a queue \nof affected readers (set up by change operations) and processes them until the queue becomes empty. The \nqueue is prioritized with respect to start time so that readers are processed in correct chronological \norder. To process a reader, we set current-time to the start time of the reader ts, remember the end-of-memo \nin a temporary variable, and run the body of the reader. After the body returns, we undo the computation \nbetween the current time and te and restore end-of-memo. 5.3 Relationship to the Semantics Adirect implementation \nof the semantics ofSAIL (Section3)is not ef.cient because change propagation relies on a complete traversal \nof the trace 1) to .nd the affected readers, and 2) to .nd the version of a modi.able at a given time \nduring the computation and updateallversions correctly.To .ndtheversionsandtheaffected readers quickly, \nthe implementation maintains the version-set and the readers-set of each modi.able in a searchable time-ordered \nset data structure.Byusingthesedata structuresandthe undo function, the implementation avoids a complete \ntraversal ofthe trace during change propagation. The semantics of SAIL does not specify how to .nd memoized \ncomputations for re-use. In our implementation, we remember the results and the time frames of memoized \ncomputations in a memo table and re-usethem when possible.Fora memoized computation tobe re-usable,we \nrequireits time-frametofall withinthe interval de.ned by current-time and end-of-memo. This ensures that \nwhenamemoized computationis re-used,thewrite operationsper\u00adformedbythe computationareavailableinthe current \nstore.When we re-use a memoized computation, we delete the computations between the current-time and \nthe beginning of the memoized computation. This guarantees that any computation is re-used at most once \n(by deleting all other computations that may contain it) and updates the versions of modi.ables. The \nsemantics of SAIL uses term equality to determine whether a reader is affected or not. Since in ML we \ndo not have access to such equality checks, we rely on user-provided equality tests. Since modi.ables \nare equality types, the user can use ML s equals operator for comparing them. 5.4 Asymptotic Complexity \nWe analyze the asymptotic complexity of self-adjusting computa\u00adtion primitives.For the analysis, we distinguish \nbetween an initial\u00adrun,i.e.,afrom-scratch runofaself-adjusting program,and change propagation. Due to \nspace constraints, we omit the proofs of these theoremsandmakethemavailable separately(Acaretal.2007a). \nTheorem 5.1 (Overhead). All self-adjusting computation primi\u00adtives can be supported in expected constant \ntime during the initial run, assuming that all memo functions have unique sets of keys. The expectation \nis taken over internal randomization usedfor rep\u00adresenting memo tables. For the analysis of change propagation, \nwe de.ne several per\u00adformance measures. Consider running the change-propagation al\u00adgorithm, and let A \ndenote the set of all affected readers, i.e., the readers that are inserted into the priority queue. \nSome of the af\u00adfected readers are re-evaluated and the others are deleted; we refer to the set of re-evaluated \nreaders as Ae and the set of deleted readers as Ad.For a re-evaluated reader r . Ae, let |r| be its re-evaluation \ntime complexity assuming that all self-adjusting primitives take constant time. Note that a re-evaluated \nr may re-use part of a pre\u00advious computation via memoization and, therefore, take less time than a from-scratch \nre-execution. Let nt denote the number of time stamps deleted during change propagation. Let nq be the \nmaximum size of the priority queue at anytime during the algorithm. Let nrw denote the maximum number \nof readers and versions (writes) that each modi.able may have. Theorem 5.2 (Change Propagation). Changepropagation \ntakes ! X O |A| log nq + |A| log nrw + nt log nrw + |r| log nrw r.Ae time. For a special class of computations, \nwhere there is a constant bound on the number of times each modi.able is read and written, i.e., nrw \n= O(1), we have the following corollary. Corollary 5.3 (Change Propagation with Constant Reads &#38; \nWrites). In the presence of a constant bound on the number of reads and writes per modi.able, change \npropagation takes ! X O |A| log nq + |r| . r.Ae amortizedtimewherethe amortizationisoverasequenceofchange \npropagations. 5.5 Complexity of Depth First Search We prove the theorems from Section2 for DFS and topological\u00adsorting. \nBoth theorems use thefact that the DFS algorithm shown in Figure2reads from and writes to each modi.able \nat most once, if the visitor function does the the same. Since initializing a graph requires writing \nto each modi.able at most once, an application that constructs a graph and then performs a DFS with a \nsingle-read and single-write visitor reads from each modi.able once and writes to each modi.able at most \ntwice. Theorem 5.4 (DFS). Disregarding read operations performed by the visitor function and the reads \nof the values returned by the visitor function, the depthFirstSearch program responds to changes in time \nO(m), where m is the number of affected nodes after an insertion/deletion. Proof. Let G be a graph and \nT be its DFS-tree. Let G ' be a graph obtained from G by inserting an edge (u, v) into G. The .rst read \naffected by this change will be the read of the edge (u, v) performed when visiting u. There are a few \ncases to consider. If v has been visited, then v will not be visited again and change propagation will \ncomplete. If v has not been visited, then it will be visited now and the algorithm will start exploring \nout from v by traversing its out-edges. Since all of these out-edge traversals will be writing their \nresults into newly allocated destinations, none of these visits will cause a memo match. Since each visited \nnode now has a different path to the root of the DFS tree that passes through the new edge (u, v), each \nnode visited during this exploration process is affected. Since each visit takes constant time, this \nwill require a total of O(m) time. After the algorithm completes the exploration of the affected nodes, \nit will return to v and then to u. From this point on, there will be no other executed reads and change \npropagation will complete. Since the only read that is ever inserted into the queue is the one that corresponds \nto the edge (u, v), the queue size will not exceed one. By Theorem 5.2, the total time for change propagation \nis O(m). The case for deletions is symmetric. We show that the same bound holds for topological sort, \nwhich is an applicationof DFS.For computing the topological sortof a graph with DFS, we use a visitor \nfunction that takes as arguments the topological sorts of the subgraph originating at each neighbor of \na node u, concatenates them and adds u to the head of the resulting list and returns that list. These \noperations can be performed in constant time by writing to the tails of the lists involved. Since a modi.able \nceases to be at a tail position after a concatenation with a non-empty list, each modi.able in the output \nlist is written at most once by the visitor function. Including the initialization, the total number \nof writes to each modi.able is bounded by two. Theorem 5.5 (Topological Sort). Change propagation updates \nthe topological sort of a graph in O(m) time where m is the number of affected nodes. Proof. Consider \nchange propagation after inserting an edge (u, v). Since the visitor function takes constant time, the \ntraversal of the affected nodes takes O(m) time. After the traversal ofthe affected nodes completes, \ndepthFirstSearch will return a result list that starts with the node u. Since this list is equal to the \nlist that is returned in the previous execution based on the equality tests on modi.able lists (Section \n2), it will cause no more reads to be re\u00adexecuted, and change propagation completes. 6. RelatedWork The \nproblem of enabling computations to respond to changes auto\u00admaticallyhasbeen studiedextensively.Mostoftheearlyworktook \nplace under the title of incremental computation. Here we review the previously proposed techniques that \nare based on dependence graphs and memoization and refer the reader to the bibliography of Ramalingam \nand Reps (1993) for other approaches such as those based on partialevaluation, e.g., Field andTeitelbaum \n(1990); Sun\u00addaresh and Hudak (1991). Dependence-graph techniques record the dependences between data \nin a computation, so that a change-propagation algorithm can update the computation when the input is \nchanged. Demers, Reps, and Teitelbaum (1981) and Reps (1982) introduced the idea of static dependence \ngraphs and presented a change-propagation al\u00adgorithm for them. The main limitation of static dependence \ngraphs is that theydo not permit the change-propagation algorithm to up\u00addate the dependence structure. \nThis signi.cantly restricts the types of computations to which static-dependence graphs can be applied. \nFor example, the INC language (Yellin and Strom 1991), which uses static dependence graphs for incremental \nupdates, does not permit recursion. To address this limitation, Acar, Blelloch, and Harper (2006c) proposed \ndynamic dependence graphs (or DDGs), presented language-techniques for constructing DDGs as programs \nexecute, and showed that change-propagation can update the de\u00adpendence structure as well as the output \nof the computation ef.\u00adciently. The approach makes it possible to transform purely func\u00adtional programs \ninto self-adjusting programs that can respond to changes to its data automatically. Carlsson (2002)gave \nan imple\u00admentation of the approach in the Haskell language. Further research on DDGs showed that, in \nsome cases, they can support incremen\u00adtal updates as ef.ciently as special-purpose algorithms (Acar et \nal. 2006c, 2004). Another approach to incremental computation is based on mem\u00adoization, where we remember \nfunction calls and reuse them when possible (Bellman 1957; McCarthy 1963; Michie 1968). Pugh (1988) and \nPugh and Teitelbaum (1989) were the .rst to apply memoization (also called function caching) to incremental \ncom\u00adputation. One motivation behind their work was the lack of a general-purpose technique for incremental \ncomputation static\u00addependence-graph techniques that existed then applied only to certain computations \n(Pugh 1988). Since Pugh and Teitelbaum s work, other researchers investigated applications of various \nkinds of memoization to incremental computation (Abadi et al. 1996; Liu et al. 1998; Heydon et al. 2000; \nAcar et al. 2003). Until recently dependence-graph based techniques and memo\u00adization were treated as \ntwo independent approaches to incremental computation. Recent work (Acar et al. 2006b) showed that there \nis, infact, an interesting dualitybetween DDGs and memoization in thewaythattheyprovidefor result re-useand \npresented techniques for combiningthem.Otherwork(Acaretal.2007b) presentedase\u00admantics for the combination \nand proved that change propagation is consistent with respect to a standard purely functional semantics. \nThework on this paperbuilds on these .ndings. Initialexperimen\u00adtal results based on the combination of \nDDGs and memoization show the combination to be effective in practice for a reasonably broad range of \napplications (Acar et al. 2006b). Self-adjusting computation based on DDGs and memoization has recently \nbeen applied to other problems. Shankar and Bodik (2007) gave an implementation of the approach in the \nJava lan\u00adguage that targets invariant checking. Theyshow that the approach is effective in speeding up \nrun-time invariant checks signi.cantly compared to non-incremental approaches. Other applications of \nself-adjusting computation include motion simulation (Acar et al. 2006d), hardware-software codesign \n(Santambrogio et al. 2007), and machine learning (Acar et al. 2007c). 7. Conclusions Self-adjusting computation \nhas been shown to be effective for a reasonably broad range of applications where computation data changes \nslowly over time. Previously proposed techniques for self\u00adadjusting computation, however, were applicable \nonly in a purely functional setting. In this paper, we introduce an imperative pro\u00adgramming model for \nself-adjusting computation by allowing mod\u00adi.able references to be written multiple times. We develop \na set of primitives for imperative self-adjusting computation and provide implementation techniques for \nsupport\u00ading theseprimitives.Thekeyideaisto maintaindifferentversions that modi.ables takeover time andkeep \ntrackof dependences be\u00adtweenversions and their readers.We prove that the approach can be implemented \nef.ciently (essentially with the same ef.ciency as in the purely functional case) when the number of \nreads andwrites of the modi.ables is constant. In the general case, the implementa\u00adtion incurs a logarithmic-time \noverhead in the number of reads and writes per modi.able. As an example, we consider the depth-.rst search \n(DFS) problem on graphs and show that it can be expressed naturally.We show that change propagation requires \ntime propor\u00adtional to the number of nodes whose paths to the root of the DFS tree changes after insertion/deletion \nof an edge. Since imperative self-adjusting programs can write to memory without anyrestrictions, theycan \ncreatecyclic data structures mak\u00ading it dif.cult to prove consistency, i.e., that the proposed tech\u00adniques \nrespond to changes correctly.To prove consistency, we for\u00admulate a syntactic logical relation and show \nthat any two evalu\u00adations of an expression e.g., a from-scratch evaluation or change propagation, are \ncontextually equivalent. An interesting property of the logical relation is that it is untyped and is \nindexed only by the numberof stepsavailable for futureevaluation.To handle the unobservable effects of \nnon-deterministic memory allocation, our logical relations carry location bijections that pair corresponding \nlocations in the two evaluations. Remaining challenges includegiving an improved implementa\u00adtion and \na practical evaluation of the proposed approach, reducing the annotation requirementsby simplifyingtheprimitivesordevel\u00adoping \nan automatic transformation from static/ordinary into self\u00adadjusting programs that can track dependences \nselectively. References Martin Abadi, ButlerW. Lampson, and Jean-Jacques Levy. Analysis and caching of \ndependencies. In Proceedings of the International Confer\u00adence on Functional Programming (ICFP), pages \n83 91, 1996. Umut A. Acar,Guy E. Blelloch, and Robert Harper. Selectivememoization. In Proceedings of \nthe 30th Annual ACM Symposium on Principles of Programming Languages (POPL), 2003. Umut A. Acar, Guy \nE. Blelloch, Robert Harper, Jorge L.Vittes, and Mav\u00aderickWoo. Dynamizing static algorithms with applications \nto dynamic trees and history independence. In ACM-SIAM Symposium on Discrete Algorithms (SODA), 2004. \nUmut A. Acar, Guy E. Blelloch, Matthias Blume, Robert Harper,and Kanat Tangwongsan. A library for self-adjusting \ncomputation. Electronic Notes in Theoretical Computer Science, 148(2), 2006a. Also in Pro\u00adceedingsof \ntheACM-SIGPLANWorkshop on ML. 2005. UmutA. Acar,GuyE. Blelloch, Matthias Blume,and KanatTangwongsan. \nAn experimental analysis of self-adjusting computation. In Proceedings of theACM SIGPLAN Conference on \nProgramming Language Design and Implementation (PLDI), 2006b. Umut A. Acar, Guy E. Blelloch, and Robert \nHarper. Adaptive functional programming. ACM Transactions on Programming Languages and Systems (TOPLAS), \n28(6):990 1034, 2006c. Umut A. Acar, Guy E. Blelloch, KanatTangwongsan, and Jorge L.Vittes. Kinetic algorithms \nvia self-adjusting computation. In Proceedings of the 14th Annual European Symposium on Algorithms (ESA), \npages 636 647, September 2006d. Umut A. Acar, Amal Ahmed, and Matthias Blume. Imperative self\u00adadjusting \ncomputation. Technical Report TR-2007-18, Department of Computer Science, University of Chicago, November \n2007a. Umut A. Acar, Matthias Blume, and Jacob Donham. Aconsistent seman\u00adtics of self-adjusting computation. \nIn Proceedings of the 16th Annual European Symposium on Programming (ESOP), 2007b. \u00a8 UmutA. Acar,Alexander \nIhler, Ramgopal Mettu,and Ozg\u00a8umer. Adap\u00adurS\u00a8tive bayesian inference. In Neural Information Systems (NIPS), \n2007c. Amal Ahmed. Step-indexed syntactic logical relations for recursive and quanti.ed types. In Proceedings \nof the 15th Annual European Sympo\u00adsium on Programming (ESOP), pages 69 83, 2006. Amal Ahmed, Matthew \nFluet, and GregMorrisett. Astep-indexed model of substructural state. In Proceedings of the 10th ACM \nSIGPLAN International Conference on Functional programming (ICFP), pages 78 91, 2005. AndrewW. Appel \nand David McAllester. An indexed model of recursive types for foundational proof-carrying code. ACMTransactions \non Pro\u00adgramming Languages and Systems (TOPLAS), 23(5):657 683, Septem\u00adber 2001. Richard Bellman. Dynamic \nProgramming. Princeton University Press, 1957. Nick Benton and Benjamin Leperchey. Relational reasoning \nin a nominal semantics for storage. In Proceedings of the 7th International Confer\u00adence onTyped Lambda \nCalculi and Applications (TLCA),pages 86 101, 2005. Nina Bohr and Lars Birkedal. Relational reasoning \nfor recursive types and references. In Proceedings of the 4th Asian Symposium on Programming Languages \nand Systems (APLAS), 2006. Magnus Carlsson. Monads for incremental computing. In Proceedings of the 7thACM \nSIGPLAN International Conference on Functionalpro\u00adgramming (ICFP), pages 26 35.ACM Press, 2002. ThomasH. \nCormen, CharlesE. Leiserson, and RonaldL.Rivest. Introduc\u00adtion to Algorithms. MIT Press/McGraw-Hill, \n1990. Alan Demers, Thomas Reps, andTimTeitelbaum. Incrementalevaluation of attribute grammars with application \nto syntax directed editors. In Proceedings of the 8th AnnualACM Symposium on Principles of Pro\u00adgramming \nLanguages (POPL), pages 105 116, 1981. P.F. DietzandD.D. Sleator.Two algorithmsfor maintaining orderina \nlist. In Proceedings of the 19thACM Symposium on Theory of Computing (STOC), pages 365 372, 1987. James \nR. Driscoll, Neil Sarnak, Daniel D. Sleator, and Robert E. Tarjan. Making data structures persistent. \nJournal of Computer and System Sciences, 38(1):86 124, February 1989. Matthias Felleisen and Robert Hieb. \nA revised report on the syntactic theories of sequential control and state. Theoretical Computer Science, \n103(2):235 271, 1992. J. Field andT.Teitelbaum. Incremental reduction in the lambda calculus. In Proceedings \nof the ACM 90 Conference on LISP and Functional Programming, pages 307 322, June 1990. AllanHeydon,RoyLevin,andYuanYu. \nCaching function callsusingpre\u00adcise dependencies. In Proceedings of the 2000 ACM SIGPLAN Con\u00adference \non Programming Language Design and Implementation (PLDI), pages 311 320, 2000. Vasileios Koutavas and \nMitchell Wand. Small bisimulations for reason\u00ading about higher-order imperative programs. In Proceedings \nof the 33rdAnnualACM Symposiumon PrinciplesofProgrammingLanguages (POPL), 2006. Yanhong A. Liu, Scott \nStoller, andTimTeitelbaum. Static caching for in\u00adcremental computation. ACMTransactionsonProgramming \nLanguages and Systems, 20(3):546 585, 1998. John McCarthy. A Basis for a Mathematical Theory of Computation. \nInP. Braffort and D. Hirschberg, editors, Computer Programming and Formal Systems, pages 33 70. North-Holland, \nAmsterdam, 1963. D. Michie. Memo functions and machine learning. Nature, 218:19 22, 1968. PeterW. O \nHearnand RobertD.Tennent.Parametricityand localvariables. Journalof theACM, 42(3):658 709, May 1995. \nNicholas Pippenger. Pure versus impure lisp. ACM Transactions on Programming Languages and Systems(TOPLAS),19(2):223 \n238, 1997. Andrew M. Pitts. Reasoning about local variables with operationally-based logical relations. \nIn Proceedings of the IEEE Symposium on Logic in Computer Science (LICS), 1996. Andrew M. Pitts and Ian \nD. B. Stark. Observable properties of higher order functions that dynamically create local names, or: \nWhat s new? In MathematicalFoundations of Computer Science, volume 711 of LNCS, pages 122 141. Springer-Verlag, \n1993. William Pugh. Incremental computation via function caching. PhD thesis, Department of Computer \nScience, Cornell University, August 1988. William Pugh andTimTeitelbaum. Incremental computation via \nfunction caching. In Proceedingsofthe16th AnnualACM Symposiumon Princi\u00adples of Programming Languages \n(POPL), pages 315 328, 1989. G. Ramalingam andT. Reps. A categorized bibliography on incremental computation. \nIn Proceedings of the 20th AnnualACM Symposium on Principles of Programming Languages (POPL), pages 502 \n510, 1993. Thomas Reps. Optimal-time incremental semantic analysis for syntax\u00addirected editors. In Proceedings \nof the 9th Annual Symposium on Prin\u00adciples of Programming Languages (POPL), pages 169 176, 1982. Marco \nD Santambrogio, Vincenzo Rana, Seda Ogrenci Memik, Umut A. Acar, and Donatella Sciuto. A novel SoC design \nmethodology for combined adaptive software descripton and recon.gurable hardware. In IEEE/ACM International \nConference on Computer Aided Design (ICCAD), 2007. Ajeet Shankar and Rastislav Bodik. Ditto: Automatic \nincrementalization of data structure invariant checks (in Java). In Proceedings of the ACM SIGPLAN 2007 \nConference on Programming languageDesign and Implementation (PLDI), 2007. Kurt Sieber. New steps towards \nfull abstraction for local variables. InACM SIGPLANWorkshop on StateinProgramming Languages, 1993. Ian \nD. B. Stark. Names and Higher-Order Functions. Ph. D. dissertation, University of Cambridge, Cambridge, \nEngland, December 1994. R. S. Sundaresh and Paul Hudak. Incremental compilation via partial evaluation. \nIn Conference Recordof the 18th AnnualACM Symposium on Principles of Programming Languages (POPL), pages \n1 13, 1991. D. M.Yellin and R. E. Strom. INC:Alanguage for incremental computa\u00adtions. ACMTransactions \non Programming Languages and Systems, 13 (2):211 236, April 1991.   \n\t\t\t", "proc_id": "1328438", "abstract": "<p>Self-adjusting computation enables writing programs that can automatically and efficiently respond to changes to their data (e.g., inputs). The idea behind the approach is to store all data that can change over time in modifiable references and to let computations construct traces that can drive change propagation. After changes have occurred, change propagation updates the result of the computation by re-evaluating only those expressions that depend on the changed data. Previous approaches to self-adjusting computation require that modifiable references be written at most once during execution---this makes the model applicable only in a purely functional setting.</p> <p>In this paper, we present techniques for imperative self-adjusting computation where modifiable references can be written multiple times. We define a language SAIL (Self-Adjusting Imperative Language) and prove consistency, i.e., that change propagation and from-scratch execution are observationally equivalent. Since SAIL programs are imperative, they can create cyclic data structures. To prove equivalence in the presence of cycles in the store, we formulate and use an untyped, step-indexed logical relation, where step indices are used to ensure well-foundedness. We show that SAIL accepts an asymptotically efficient implementation by presenting algorithms and data structures for its implementation. When the number of operations (reads and writes) per modifiable is bounded by a constant, we show that change propagation becomes as efficient as in the non-imperative case. The general case incurs a slowdown that is logarithmic in the maximum number of such operations. We describe a prototype implementation of SAIL as a Standard ML library.</p>", "authors": [{"name": "Umut A. Acar", "author_profile_id": "81100077236", "affiliation": "Toyota Technological Institute at Chicago, Chicago, IL", "person_id": "P286793", "email_address": "", "orcid_id": ""}, {"name": "Amal Ahmed", "author_profile_id": "81100287263", "affiliation": "Toyota Technological Institute at Chicago, Chicago, IL", "person_id": "P414176", "email_address": "", "orcid_id": ""}, {"name": "Matthias Blume", "author_profile_id": "81100215091", "affiliation": "Toyota Technological Institute at Chicago, Chicago, IL", "person_id": "PP43137357", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1328438.1328476", "year": "2008", "article_id": "1328476", "conference": "POPL", "title": "Imperative self-adjusting computation", "url": "http://dl.acm.org/citation.cfm?id=1328476"}