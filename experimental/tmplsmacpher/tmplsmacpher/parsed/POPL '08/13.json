{"article_publication_date": "01-07-2008", "fulltext": "\n Subcubic Algorithms for Recursive State Machines Swarat Chaudhuri Pennsylvania State University University \nPark, PA 16802, USA swarat@cse.psu.edu Abstract We show that the reachability problem for recursive \nstate machines (or equivalently, pushdown systems), believed for long to have cubic worst-case complexity, \ncan be solved in slightly subcubic time. All that is necessary for the new bound is a simple adaptation \nof a known technique. We also show that a better algorithm exists if the input machine does not have \nin.nite recursive loops. Categories and Subject Descriptors F.1.1 [Computation by ab\u00adstract devices]: \nModels of computation Automata; F.2.2 [Anal\u00adysis of algorithms and problem complexity]: Nonnumerical \nalgo\u00adrithms and problems Computations on discrete structures; F.3.2 [Theory of Computation]: Semantics \nof programming languages Program analysis. General Terms Algorithms, Theory, Veri.cation Keywords Recursive \nstate machines, pushdown systems, CFL\u00adreachability, context-free languages, interprocedural analysis, \ntran\u00adsitive closure, cubic bottleneck. 1. Introduction Pushdown models of programs have numerous uses \nin program analysis (Horwitz et al. 1988; Reps et al. 1995, 2003; Alur et al. 2005). Recursive state \nmachines (Alur et al. 2005), or .nite-state machines that can call other .nite-state machines recursively, \nform a popular class of such models. These machines (called RSMs from now on) are equivalent to pushdown \nsystems, or .nite-state machines equipped with stacks. They are also natural abstractions of recursive \nprograms: each component .nite-state machine models control .ow within a procedure, and procedure calls \nand returns are modeled by calls and returns to/from other machines. Sound analysis of a program then \ninvolves algorithmic analysis of an RSM abstracting it. In this paper, we study the most basic and widely \napplicable form that such analysis takes: determination of reachability be\u00adtween states. Can an RSM, \nin some execution, start at a state v and reach the state v'? Because RSMs are pushdown models, any path \nthat the RSM can take respects the nested structure of calls and re\u00adturns, and reachability analysis \nof an RSM abstraction of a program gives a context-sensitive program analysis. A classic application \nis interprocedural data-.ow analysis can a data-.ow fact reach a Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. POPL 08, January 7 12, 2008, San Francisco, \nCalifornia, USA. Copyright c &#38;#169; 2008 ACM 978-1-59593-689-9/08/0001. . . $5.00 certain program \npoint along a path respecting the nesting of pro\u00adcedure calls? The problem also shows up in many other \nprogram analysis contexts for example .eld-sensitive alias analysis (Reps 1998), type-based .ow analysis \n(Rehof and F\u00a8ahndrich 2001), and shape analysis (Reps 1998). Reachability for RSMs is equivalent to a \nwell-known graph problem called context-free language (CFL) reachability.The question here is: given \nan edge-labeled directed graph and a context-free grammar over the edge labels, is there a path from \nnode s to node t in the graph that is labeled by a word generated by the grammar? This problem, which \nmay be viewed as a gen\u00aderalization of context-free recognition, was originally phrased in the context \nof database theory (Yannakakis 1990), where it was shown that Datalog chain query evaluation on the graph \nrepre\u00adsentation of a database is equivalent to single-source, single-sink CFL-reachability. It has since \nbeen identi.ed as a central problem in program analysis (Reps 1998; Melski and Reps 2000). All known \nalgorithms for RSM and CFL-reachability follow a dynamic-programming scheme known in the literature as \nsum\u00admarization (Sharir and Pnueli 1981; Alur et al. 2005; Bouajjani et al. 1997). The idea here is to \nderive reachability facts of the ' form (v, v), which says that the RSM can start at state v with an \nempty stack and end at state v' with an empty stack. The most well\u00adknown algorithms following this scheme \n(Horwitz et al. 1995; Reps et al. 1995) discover such pairs enumeratively via graph traversal. Unlike \ncontext-free recognition, which has a well-known subcu\u00adbic solution (Valiant 1975), RSM and CFL-reachability \nhave not been known to have subcubic algorithms even in the single-sink, single-source case (for RSM-reachability, \nthe size of an instance is the number of states in it; for CFL-reachability, it is the num\u00adber of nodes \nin the input graph). This raises the question: are these problems intrinsically cubic? The question is \nespecially interest\u00ading in program analysis as problems like interprocedural data-.ow analysis and slicing \nare not only solvable using RSM-reachability, but also provably as hard. Believing that the answer is \nyes , re\u00adsearchers have sometimes attributed the cubic bottleneck of these problems to the hardness of \nRSM or CFL-reachability (Reps 1998; Melski and Reps 2000). In this paper, we observe that summarization \ncan bene.t from a known technique (Rytter 1983, 1985) for speeding up certain kinds of dynamic programming. \nThe idea, developed in the context of language recognition for two-way pushdown automata, is to rep\u00adresent \na computation accessing a table as a computation on row and column sets, which are stored using a fast \nset data struc\u00adture. The latter, a standard data structure in the algorithms litera\u00adture (Arlazarov et \nal. 1970; Chan 2007), splits each operation in\u00advolving a pair of sets into a series of operations on \npairs of sets drawn from a small sub-universe. If the sub-universes are suf.\u00adciently small, all queries \non them may be looked up from a table precomputed exhaustively, allowing us to save work during an ex\u00adpensive \nmain loop. When transferred to the RSM-reachability prob\u00adlem with slight modi.cations, Rytter s method \nleads to an algo\u00adrithm that phrases the computation of reachability as a sequence of operations on sets \nof RSM states, and has an O(n 3/log n) time complexity. The technique may also be applied to the standard \nal\u00adgorithm for CFL-reachability, referenced for example by Melski and Reps (2000), leading to a similar \nspeedup. This implies sub\u00adcubic solutions for Datalog chain query evaluation as well as the many program \nanalysis applications of RSM-reachability. Our other contribution is an observation that the reachability \nproblem for RSMs gets easier, so far as worst-case complexity is concerned, as recursion is restricted. \nWe study the reachabil\u00adity problem for bounded-stack recursive state machines, which are RSMs where the \nstack never grows unboundedly in any execu\u00adtion. Machines of this sort have a clear interpretation in \nprogram analysis: they capture the .ow of control in procedural programs without in.nite recursive loops. \nIn spite of this extra structure, they have not been known to have faster reachability algorithms than \ngeneral RSMs (note that a bounded-stack RSM is in fact a .nite\u00adstate machine however, the latter can \nbe exponentially larger than the RSM, so that it is not an option to analyze it instead of apply\u00ading \nan RSM-reachability algorithm). We show that it is possible to exploit this structure during reachability \nanalysis. The key ob\u00adservation is that empty-stack-to-empty-stack reachability facts in bounded-stack \nRSMs can be derived in a depth-.rst order i.e., if state uhas an edge to state v, it is possible to .rst \ninfer all the states empty-stack-to-empty-stack reachable from v and then use this in\u00adformation to infer \nthe states reachable this way from v (this is not possible for general RSMs). It turns out that, as a \nresult, we can solve the reachability problem using a transitive closure algorithm for directed graphs \nthat allows the following kind of modi.cations to the instance: for an edge (u,v) that goes from one \nstrongly con\u00adnected component to another, compute all descendants v ' of v and add some edges from ubased \non the answer. Unfortunately, none of the existing subcubic algorithms for transitive closure can han\u00addle \nsuch modi.cations. Consequently, we derive a new transitive closure algorithm for directed graphs that \ncan. Our transitive closure algorithm speeds up a procedure based on Tarjan s algorithm to determine \nthe strongly connected compo\u00adnents of a graph. Such algorithms have a sizable literature (Purdom 1970; \nEve and Kurki-Suonio 1977; Schmitz 1983). Their attrac\u00adtion in our setting is that they perform one depth-.rst \ntraversal of the input graph, computing closure using set operations along the way, so that it is possible \nto weave the treatment of added edges into the discovery of edges in the original graph. The idea behind \nthe speedup is, once again, to reuse computations on small patterns common to set computations, except \nthis time, it can be taken fur\u00adther and yields a complexity of O(min{mn/log n,n 3/log2 n}), where nis \nthe number of nodes in the graph and mthe number of edges. This directly leads to an O(n 3/log2 n) solution \nfor all-pairs reachability in bounded-stack RSMs. We .nish our study of the interplay of recursion and \nreachability in RSMs with a note on the reachability problem for hierarchical state machines (Alur and \nYannakakis 1998). These machines can model control .ow in structured programs without recursive calls \nand form a proper subclass of bounded-stack RSMs. The one pub\u00adlished reachability algorithm for such \nmodels is cubic (Alur and Yannakakis 1998); here, we give a simple alternative that has the same complexity \nas boolean matrix multiplication. While this al\u00adgorithm is almost trivial, taken together with our other \nresults, it indicates a gradation in the complexity of RSM-reachability as re\u00adcursion is constrained. \nThe paper is organized as follows. Section 2 de.nes the three classes of RSMs that interest us, CFL-reachability, \nand the fast set data structure. Section 3 discusses reachability in general RSMs and CFL-reachability. \nIn Section 4, we study reachability for bounded-stack RSMs, and Section 5 brie.y examines reachability \nin hierarchical state machines. We conclude with some discussion in Section 6. 2. Basics Recursive state \nmachines (RSMs), introduced by Alur et al. (2005), are .nite-state-machines that can call other .nite-state-machines \nrecursively. RSMs are equivalent to pushdown systems, and any solution for RSM-reachability can be translated \nto a solution the same complexity for pushdown systems. In this section, we de.ne three variants of recursive \nstate machines. We also review their connection with the context-free language reachability problem. \nRecursive state machines A recursive state machine (RSM) M is a tuple (M1,M2,...,Mk), where each Mi = \n(Li,Bi,Yi,Eni,Exi,.i) is a component comprising: a .nite set Li of internal states;  a .nite set Bi \nof boxes;  amap Yi : Bi .{1,2,...,k}that assigns a component to every box;  aset Eni .Li of entry states \nand a set Ex i .Li of exit states;  an edge relation .i.(Li.Retnsi\\Ex i)\u00d7(Li.Callsi\\Eni), where Callsi \n= {(b,en): b.Bi,en .EnYi(b)}is the set of calls and Retnsi = {(b,ex): b .Bi,ex .ExYi(b)}the set of returns \nin Mi.  Note that an edge cannot start from a call or an exit state, and cannot end at a return or an \nentry state. We assume that for every distinct i and j, Li, Bi, Callsi, Retnsi, Lj , Bj , Callsj,and \nRetnsj are pairwise disjoint. Arbitrary calls, returns and internal states in M are referred to as states. \nThe set of all states is given by V =(Li .Callsi .Retnsi), and the set of states in i Mj is denoted by \nVj. We also write B = i Bi to denote the collection of all boxes in M. Finally, the extensions of the \nrelations .i and functions Yi are denoted respectively by ..V \u00d7V and Y : B .{1,2,...,k}. For an example \nof an RSM, see Figure 1-(b). This machine has two components: M1 and M2. The component M1 has an entry \nstate s andanexit state t, boxes b1 and b2 satisfying Y(b1)= Y(b2)=2, and edges (s,(b1,u)) and ((b2,v),t).The \ncomponent M2 has an entry uandanexit v, and an edge (u,v). The semantics of M is given by an in.nite \ncon.guration graph CM .Let a con.guration of M beapair c =(v,w) . V \u00d7B * satisfying the following condition: \nif w = b1 ...bn for some n =1 (i.e., if wis non-empty), then: 1. v .VY (bn),and 2. for all i .{1,...,n-1}, \nbi+1 .BY (bi).  The nodes of CM are con.gurations of M. The graph has an edge ' '' from c =(v,w) to \nc =(v ,w ) if and only if one of the following holds: 1. Local move: v . (Li .Retnsi) \\Exi, (v,v ' ) \n..i,and w ' = w; 2. Call move: v =(b,en) .Callsi, v ' = en,and w ' = w.b; 3. Return move: v .Ex i, \nw = w ' .b,and v ' =(b,v).  Intuitively, the string w in a con.guration (v,w) is a stack,and paths in \nCM de.ne the operational semantics of M.If v is a call (b,en) in the above, then the RSM pushes bon the \nstack and moves to the entry state enof the component Y(b). Likewise, on reaching (a) int g; void bar() \n{ void main () int y = 0; { } int x = 1; bar(); g = 1; bar(); L: x = g; } (b) (c) s  t Figure 1. (a) \nA C example (b) RSM for the uninitialized variable problem (c) CFL-reachability formulation an exit ex, \nit pops a frame b off the stack and moves to the return (b,ex). Unsurprisingly, RSMs have linear translations \nto and from pushdown systems (Alur et al. 2005). Size The size of an RSM is the total number of states \nin it. Reachability Reachability in the con.guration graph is de.ned as usual. We call the state v ' \nreachable from the state v if a con.guration (v ' ,w), for some stack w, is reachable from (v,.) in the \ncon.guration graph. Intuitively, the RSM, in this case, has an execution that starts at v with an empty \nstack and ends at v ' with some stack. The state v ' is same-context reachable from vif (v ' ,.) is reachable \nfrom (v,.). In this case the RSM can start at vwith an empty stack and reach v ' with an empty stack \nnote that this can happen only if v and v ' are in the same component. The all-pairs reachability problem \nfor an RSM is to determine, '' ' for each pair of states v,v ,whether v is reachable from v .The single-source \nand single-sink variants of the problem are de.ned in the natural way. We also de.ne the same-context \nreachability problem,where we ask if v ' is same-context reachable from v. All known algorithms for RSM-reachability \nand pushdown sys\u00adtems, whether all-pairs or single-source/single-sink, same-context or not, rely on a \ndynamic programming scheme called summariza\u00adtion (Sharir and Pnueli 1981; Alur et al. 2005; Bouajjani \net al. 1997; Reps et al. 1995), which we will examine in Section 3. The worst\u00adcase complexity of all \nthese algorithms is cubic. Tighter bounds are possible if we constrain the number of entry and exit states \nand/or edges in the input. For example, if each component of the input RSM has one entry and one exit \nstate, then single-source, single\u00adsink reachability can be determined in O(m+ n) time, where m is the \nnumber of edges in the RSM and n the number of states (the all-pairs problem has the same complexity \nas graph transi\u00adtive closure) (Alur et al. 2005). In this paper, in addition to general RSM-reachability, \nwe study reachability algorithms for RSMs con\u00adstrained in a different way: by restricting or disallowing \nrecursion. To see the use of RSM-reachability in solving a program anal\u00adysis problem, consider the program \nin Figure 1-(a). Suppose we want to determine if the variable g is uninitialized at the line la\u00adbeled \nL. This may be done by constructing the RSM in Figure 1\u00ad(b). The two components correspond to the procedures \nmain and bar; states in these components correspond to the control points of the program e.g., the state \ns models the entry point of main, and (b2,v) models the point immediately before line L. Procedure calls \nto bar are modeled by the boxes b1 and b2. For every state\u00adment that does not assign to g, an edge is \nadded between the states modeling the control points immediately before and after this state\u00adment. Then \ng is uninitialized at L iff (b2,v) is reachable from s. More generally, RSM-reachability algorithms can \nbe used to check if a context-sensitive program abstraction satis.es a safety prop\u00aderty (Alur et al. \n2005). For example, the successful software model checker SLAM (Ball and Rajamani 2001) uses an algorithm \nfor RSM-reachability as a core module. Bounded-stack RSMs and hierarchical state machines Now we de.ne \ntwo special kinds of RSMs with restricted recur\u00adsion: bounded-stack RSMs and hierarchical state machines.We \nwill see later that they have better reachability algorithms than gen\u00aderal RSMs. The class of bounded-stack \nRSMs consists of RSMs M where every call (b,en) is unreachable from the state en. By the seman\u00adtics of \nan RSM, the stack of an RSM grows along an edge from a call to the corresponding entry state. Thus, intuitively, \na bounded\u00adstack RSM forbids in.nite recursive loops, ensuring that in any path in the con.guration graph \nstarting with a con.guration (v,.),the height of the stack stays bounded. To see an application, consider \na procedure that accepts a boolean value as a parameter, .ips the bit, and, if the result is 1, calls \nitself recursively. While this program does employ recursion, it never runs into an in.nite recursive \nloop. As a result, it can be modeled by a bounded-stack RSM. A hierarchical state machine (Alur and Yannakakis \n1998), on the other hand, forbids recursion altogether. Formally, such a ma\u00adchine is an RSM M where there \nis a total order . on the compo\u00adnents M1,...,Mk such that if Mi contains a box b,then MY (b) . Mi. Thus, \ncalls from a component may only lead to a component lower down in this order. For example, the RSM in \nFigure 1-(b) is a hierarchical state machine. Note that every bounded-stack or hierarchical machine can \nbe translated to an equivalent .nite-state machine. However, this causes an exponential increase in size \nin the worst case, and it is unreasonable to analyze a hierarchical/bounded-stack machine by .attening \nit into a .nite-state machine. The question that inter\u00adests us is: can we determine reachability in a \nbounded-stack or hierarchical machine in time polynomial in the input? The only known way to do this \nis to use the summarization technique that also works for general RSMs, leading to an algorithm of cubic \nworst-case complexity. Context-free language reachability RSM-reachability is equivalent to a graph \nproblem called context\u00adfree language (CFL) reachability (Yannakakis 1990; Reps 1998) that has numerous \napplications in program analysis. Let S be a directed graph whose edges are labeled by an alphabet S,and \nlet L be a context-free language over S. We say a node tis L-reachable from a node s if there is a path \nfrom s to t in S that is labeled by a word in L. The all-pairs CFL-reachability problem for S and Lis \nto determine, for all pairs of nodes sand t,if tis L-reachable from s. The single-source or single-sink \nvariants of the problem are de.ned in the obvious way. Customarily, the size of the instance is given \nby the number n of nodes in S, while L is assumed to be given by a .xed-sized grammar G. Let us now see \nhow, given an instance of RSM-reachability, we can obtain an equivalent CFL-reachability instance. We \nbuild a graph whose nodes are states of the input RSM M;for every edge (u, v) in M, S has an edge from \nu to v labeled by a symbol a. For every call (b, en) in the RSM, S has an edge labeled (b from (b, en) \nto en; for every exit ex and return (b, ex) in M,we add a )b-labeled edge in S from ex to (b, ex) (for \nexample, the graph S constructed from the RSM in Figure 1-(b) is shown in Figure 1\u00ad(c)). Now, the state \nv is reachable from the state u in M if and only if v is L-reachable from u in S,where L is given by \nthe grammar S .SS |(bS)b |(bS |a. The translation in the other direction is also easy we refer the reader \nto the original paper on RSMs (Alur et al. 2005). Note that context-free recognition is the special case \nof CFL\u00adreachability where S is a chain. A cubic algorithm for all-pairs CFL-reachability can be obtained \nby generalizing the Cocke\u00adYounger-Kasami algorithm (Hopcroft and Ullman 1979) for CFL\u00adrecognition this \nalgorithm again relies on summarization. The problem is known to be equivalent to the problem of evaluating \nDatalog chain queries on a graph representation of a database (Yan\u00adnakakis 1990). Such queries have the \nform p(X, Y ) .q0(X, Z1). q1(Z1,Z2) .... .qk(Zk,Y ),where the qi s are binary predicates and X, Y and \nthe Zi s are distinct variables, and have wide appli\u00adcations. It has also come up often in program analysis \nfor exam\u00adple, in the context of interprocedural data.ow analysis and slicing, .eld-sensitive alias analysis, \nand type-based .ow analysis (Hor\u00adwitz et al. 1988; Reps et al. 1995; Horwitz et al. 1995; Reps 1995, \n1998; Rehof and F\u00a8ahndrich 2001). The cubic bottleneck of these analysis problems has sometimes been \nattributed to the believed cubic hardness of CFL-reachability. A special case is the problem of Dyck-CFL-reachability.The \nconstraint here is that the CFL L is now a language of bal\u00adanced parentheses. Many program analysis applications \nof CFL\u00adreachability e.g., .eld-sensitive alias analysis of Java programs (Sridharan et al. 2005) turn \nout actually to be applications of Dyck-CFL-reachability, though so far as asymptotic bounds go, it is \nno simpler than the general problem. This problem is equivalent to the problem of same-context reachability \nin RSMs. Fast sets Our algorithms for RSMs use a set data structure that exploits sharing between sets \nto offer certain set operations at low amortized cost. This data structure called fast sets from now \non is standard technology in the algorithms literature (Chan 2007; Arlazarov et al. 1970) and was used, \nin particular, in the papers by Rytter (1983, 1985) on two-way pushdown recognition. Its essence is that \nit splits an operation on a pair of sets into a series of unit-cost operations on small sets. We will \nnow review it. Let U be a universe of n elements of which all our sets will be subsets. The fast set \ndata structure supports the following opera\u00adtions: Set difference:Given sets X and Y ,return a list \nDi. (X, Y ) consisting of the elements of the set (X \\Y ).  Insertion:Inserta value into a set.  Assign-union:Given \nsets X and Y , perform the assignment X .X .Y .  Let us assume an architecture with word size p = .(log \nn).A fast set representation of a set is the bit vector (of length n)for the set, broken into !n/plwords. \nThen: To compute Di. (X, Y ),where X and Y are fast sets, we compute the bit vector for Z = X \\Y via \nbitwise operations on the words comprising X and Y . This takes O(n/p) time assuming constant-time logical \noperations on words. To list the elements of Z, we repeatedly locate the most signi.cant bit in Z, add \nits position in X to the output list, and turn it off. Assuming that it is possible in constant time \nto check if a word equals 0 and .nd the most signi.cant bit in a word, this can be done in O(|Z|+ n/p) \ntime. Note that the bound is given in terms of the size of the output. This is exploited while bounding \nthe amortized cost of a sequence of set differences. Insertion of 0 =x =n-1 involves setting a bit in \nthe Lx/pJ-th word, which can be done in O(1) time.  The assign-union operation can be implemented by \nword-by\u00adword logical operations on the components of X and Y ,and takes O(n/p) time.  In case the unit-cost \noperations we need are not available, they can be implemented using table lookup. Let a fast set now \nbe a collection of words of length p = !log n/2l. In a preprocessing phase, we build tables implementing \neach of the binary or unary word operations we need by simply storing the result for each of the O(2p.2p)= \nO(n) possible inputs. The time required to build each such table is O(p.n) (assuming linear-time operations \non words), and the space requirement is O(n). The costs of our fast set operations are now as before. \n 3. All-pairs reachability in recursive state machines Let us now study the reachability problem for \nrecursive state ma\u00adchines. We remind the reader that all known algorithms for this problem are cubic \nand based on a high-level algorithm called sum\u00admarization. In this section we show that a speedup technique \nde\u00adveloped by Rytter (1985, 1983) can be directly applied to this al\u00adgorithm, leading to an O(n 3/ log \nn)-time solution. The modi.ed algorithm computes reachability via a sequence of operations on sets of \nstates, each represented as a fast set. In this sense it is a symbolic implementation of summarization, \nrather than an iterative one like the popular algorithm due to Reps et al. (1995). We also show that \nthe standard cubic algorithm for CFL-reachability, refer\u00adenced for example by Melski and Reps (2000), \ncan be speeded up similarly using Rytter s technique. 3.1 Reachability in RSMs Let us start by reviewing \nsummarization. We have as input an RSM M = (M1,...,Mk) as in Section 2, with state set V , box set B, \nedge relation .. V \u00d7V ,and a map Y : B .{1,...,k}assigning components to boxes. The algorithm .rst determines \nsame-context reachability by building a relation Hs . V \u00d7V , de.ned as the least relation satisfying: \n1. if u = v or u .v,then (u, v) .Hs; 2. if (u, v ' ) .Hs and (v ' ,v) .Hs,then (u, v) .Hs; 3. if (u, \nv) . Hs and u is an entry and v is an exit in some component, then for all boxes b such that (b, u), \n(b, v) . V , we have ((b, u), (b, v)) .Hs .  For example, the relation Hs for the RSM in Figure 1-(a) \nis drawn in Figure 2 (the transitive edges are omitted). While the de.nition of Hs is recursive, it may \nbe constructed using a least\u00ad.xpoint computation. Once it is built, we construct a relation H . V \u00d7V \nde.ned as: H = ..{((b, en), (b, ex)) .Hs : b .B,and en is an entry and ex an exit of Y (b)}.{((b, en),en): \nen is an entry in Y (b)}, (b1,u)(b2,v) t s  (b1,v)(b2,u)  u v Figure 2. The relation H. Hs is the \ntransitive closure of non\u00addashed edges, and H * is the transitive closure of all edges and compute the \n(re.exive) transitive closure H * of the resultant relation (see Figure 2). It is known that: LEMMA 1 \n((Alur et al. 2005; Bouajjani et al. 1997)). For states v '' ' and v of M, v is reachable from v iff \n(v, v ' ) . H *.Also, v is same-context reachable from v iff (v, v ' ) . Hs . Within the scheme of summarization, \nthere are choices as to how the .xpoint computations for Hs and H * are carried out. For example, the \npopular algorithm due to Reps et al. (1995) employs graph search to construct these relations enumeratively. \nIn contrast, the algorithm we now present, obtained by a slight modi.cation of an algorithm by Rytter \n(1985) for two-way pushdown recogni\u00adtion, phrases the computation as a sequence of operations on sets \nof states. Unlike previous implementations of summarization, our algorithm has a slightly subcubic worst-case \ncomplexity. The algorithm is a modi.cation of the procedure BASELINE-REACHABILITY in Figure 3, which \nuses a worklist W to compute Hs and H * in a fairly straightforward way. Line 1 of the baseline routine \ninserts intra-component edges and trivial reachability facts into Hs and W . The rest of the pairs in \nHs are derived by the while-loop from line 2 10, which removes pairs from W one by one and processes \nthem. While processing a pair (u, v),we derive all the pairs that it implies by rules (2) and (3) in \nthe de.nition of Hs and that have not been derived already,and insert them into Hs and W . At the end \nof any iteration of the loop, W contains the pairs that have been derived but not yet processed. The \nloop continues till W is empty. It is easy to see that on its termination, Hs is correctly computed. \nLines 11-14 now compute H * . Note that a pair is inserted into W only when it is also inserted into \nHs, so that the loop has one iteration per insertion into Hs.At the same time, a pair is never taken \nout of Hs once it is inserted, and no pair is inserted into it twice. Let n be the size of the RSM, and \nlet a = n 2 be an upper bound on the number of pairs (u, v) such that v is reachable from u. Then the \nloop has O(a) iterations. Let us now determine the cost of each iteration. Assuming we can insert an \nelement in Hs and W in constant time, lines 4 6 cost constant time per insertion of an element into Hs. \nThus, the total cost for lines 4 6 during a run of BASELINE-REACHABILITY is O(a). The for-loops at line \n7 and line 9 need to identify all states u ' and v ' satisfying their conditions for insertion. Done \nenumeratively, this costs O(n) time per iteration, causing the total cost of the loop to be O(an). As \nfor the rest of the algorithm, line 14 may be viewed as computing the (re.exive) transitive closure of \na graph with n states and O(a) edges. This may clearly be done in O(an) time. Then: LEMMA 2. BASELINE-REACHABILITY \nterminates on any RSM M in time O(a.n),where a = n 2 is the number of pairs (u, v) . V \u00d7 V such that \nv is reachable from u. On termination, for every pair of states u and v, v is reachable from u iff (u, \nv) . H *, and v is same-context reachable from u iff (u, v) . Hs . BASELINE-REACHABILITY() 1 W . Hs .{(u, \nu): u . V }. . 2 while W= \u00d8 3 do (u, v) . remove from W 4 if u is an entry state and v an exit state \nin a component Mi 5 then for b such that Y (b)= i 6 do insert ((b, u), (b, v)) into Hs , W 7 for (u ' \n,u) . Hs such that (u ' ,v) ./Hs 8 do insert (u ' ,v) into Hs and W 9 for (v, v ' ) . Hs such that (u, \nv ' ) ./Hs 10 do insert (u, v ' ) into Hs and W 11 H * . Hs 12 for calls (b, en) . V 13 do insert ((b, \nen),en) into H * 14 H * . transitive closure of H * Figure 3. Baseline procedure for RSM-reachability \nTo convert the baseline procedure into a set-based algorithm, interpret the relation Hs as an n \u00d7 n table, \nand denote the u\u00adth row and column as sets (respectively denoted by Row(u) and Col(u)). Then we have \nRow(u)= {v :(u, v) . Hs} and Col(u)= {v :(v, u) . Hs}. Now observe that the for-loops at lines 7 and \n9 can be captured by set difference operations.The for-loop in line 7 8 may be rewritten as: ' for u \n' . (Col(u) \\ Col(v)) do insert (u ,v) into Hs and W, and the for-loop in line 9 10 may be rewritten \nas: ' for v ' . (Row(v) \\ Row (u)) do insert (u, v ) into Hs and W. Our set-based algorithm for RSM-reachability \ncalled REACHA-BILITY from now on is obtained by applying these rewrites to BASELINE-REACHABILITY. Clearly, \nREACHABILITY terminates after performing O(a) set difference and insertion operations, and when it does, \nthe tables H * and Hs respectively capture reachabil\u00adity and same-context reachability. We may, of course, \nuse any set data structure offering ef.cient difference and insertion in our algorithm. If the cost of \nset differ\u00adence is linear, then the algorithm is cubic in the worst-case. The complexity, however, becomes \nO(na/ log n)= O(n 3/ log n) if we use the fast set data structure of Section 2. To see why, as\u00adsume that \nthe rows and columns of Hs are represented as fast sets and that set difference and insertion are performed \nusing the op\u00aderations Di. and Ins described earlier. In each iteration of the main loop, the inner loops \n.rst compute the difference of two sets of size n, then, for every element in the answer, inserts a pair \ninto Hs (this involves inserting an element into a row and a col\u00adumn) and W .Ifthe i-th iteration of \nthe main loop inserts si pairs into Hs, the time spent on the operation Di. in this iteration is O(n/ \nlog n+si). Since the result is returned as a list, the cost of it\u00aderatively inserting pairs in it into \nH * and W is also O(si).The cost of these operations summed over the entire run of REACHABILITY sa is \nO(a.n/ log n+ i si)= O(an/ log n+a)= O(an/ log n). The only remaining bottleneck is the transitive closure \nin line 14 of the baseline procedure. This may be computed in O(a.n/ log n) time using the procedure \nwe give in Section 4.1. The total time complexity then becomes O(an/ log n) i.e., O(n 3/ log n). As for \nthe space requirement of the algorithm, T(n 2) space is needed just to store the tables Hs and H *. The \nspace required by tables implementing word operations, if unit-cost word operations are not available, \nis subsumed by this factor. Thus we have: THEOREM 1. The algorithm REACHABILITY solves the all-pairs \nreachability and same-context-reachability problems for an RSM with n states in O(n 3/ log n) time and \nO(n 2) space. Readers familiar with Rytter s O(n 3/ log n)-time algorithm (Rytter 1985) for recognition \nof two-way pushdown languages will note that our subcubic algorithm is very similar to it. Recall that \na two-way pushdown automaton (2-PDA) is a pushdown automaton which, on reading a symbol, can move its \nreading head one step forward and back on the input word, while changing its control state and pushing/popping \na symbol on/off its stack. The language recognition problem for 2-PDAs is: given a word w of length n \nand a 2-PDA A of constant size, is w accepted by A? This prob\u00adlem may be linearly reduced to the reachability \nproblem for RSMs. Notably, there is also a reduction in the other direction. Given an RSM M where we \nare to determine reachability, write out the states and transitions of M as an input word. Now construct \na 2-PDA A that, in every one of an arbitrary number of rounds, moves its head to an arbitrary transition \nof M and tries to simulate the execution. Using nondeterminism, A can guess any run of M, and accept \nthe input if and only if M has an execution from a state u to a state v. This may suggest that a subcubic \nalgorithm for RSM-reachability already exists. The catch, however, is that an RSM of size n may have \nO(n 2) transitions, so that this reduction outputs an instance of quadratic size. Clearly, it cannot \nbe combined with Rytter s al\u00adgorithm to solve reachability in RSMs in cubic (let alone subcubic) time. \nOn the other hand, what Rytter s algorithm actually does is to speed up a slightly restricted form of \nsummarization. Recall the routine BASELINE-REACHABILITY,and let u, v,... be positions in a word rather \nthan states of an RSM. Just like us, Rytter derives pairs (u, v) such that the automaton has an empty-stack \nto empty\u00adstack execution from u to v. One of the rules he uses is: Suppose (u, v) is already derived. \nIf A can go from u ' to u by pushing ., and from v to v ' by popping .,thenderive (u ' ,v ' ). This rule \nis analogous to Rule (3) in our de.nition of summariza\u00adtion: Suppose (u, v) is already derived. If u \nis an entry and v is an exit in some component and b is a box such that (b, u), (b, v) . V ,then derive \n((b, u), (b, v)). The two rules differ in the number of new pairs they derive. Be\u00adcause the size of A \nis .xed, Rytter s rule can generate at most a constant number of new pairs for a .xed pair (u, v). On \nthe con\u00adtrary, our rule can derive a linear number of new pairs for given (u, v). Other than the fact \nthat Rytter deals with pairs of positions and we deal with RSM states, this is the only point of difference \nbe\u00adtween the baseline algorithms used in the two cases. At .rst glance, this difference may seem to make \nthe algorithm cubic, as the above derivation happens inside a loop with a quadratic number of iter\u00adations. \nOur observation is that a tighter analysis is possible: our rule above only does a constant amount of \nwork per insertion of a pair into Hs. Thus, over a complete run of the algorithm, its cost is quadratic \nand subsumed by the cost of the other lines, even af\u00adter the speedup is applied. For the rest of the \nalgorithm, Rytter s complexity arguments carry over. 3.2 CFL-reachability As RSM-reachability and CFL-reachability \nare equivalent prob\u00adlems, the algorithm REACHABILITY can be translated into a set\u00adbased, subcubic algorithm \nfor CFL-reachability. However, Rytter s technique can also be directly applied to the standard algorithm \nfor CFL-reachability, described for example by Melski and Reps (2000). Now we show how. Let us have an \ninstance (S, G) of CFL\u00adreachability, where S is an edge-labeled graph with n nodes and G is a constant-sized \ncontext-free grammar. Without loss of gen\u00aderality, it is assumed that the right-hand side of each rule \nin G has BASELINE-CFL-REACHABILITY() a 1 W . Hs .{(u, A, v): u . v in S,and A . a in G } 2 .{(u, A, u): \nA . . in G } 3 while W = \u00d8 4 do (u, B, v) . remove from W 5 for each production A . B 6 do insert (u, \nB, v) into Hs , W 7 for each production A . CB 8 do for each edge (u ' ,C,u) such that (u ' ,A, v) ./Hs \n9 do insert (u ' ,A, v) into Hs and W 10 for each production A . BC 11 do for each edge (v, C, v ' ) \nsuch that (v, A, v ' ) ./Hs 12 do insert (v, A, v ' ) into Hs and W Figure 4. Baseline algorithm for \nCFL-reachability at most two symbols. The algorithm in Melski and Reps paper called BASELINE-CFL-REACHABILITY \nand shown in Figure 4 computes tuples (u, A, v),where u, v are nodes of S and A is a terminal or non-terminal, \nsuch that there is a path from u to v labeled by a word w that G can derive from A. A worklist W is used \nto process the tuples one by one; derived tuples are stored in a table Hs. It is easily shown, by arguments \nsimilar to those for RSM-reachability, that the algorithm is cubic and requires quadratic space. On termination, \na tuple (u, I, v),where u, v are nodes and I the initial symbol of G,is in Hs iff v is CFL-reachable \nfrom u. As in case of RSM-reachability, now we store the rows and columns of Hs as fast sets of O(n) \nsize. For a node u and a non\u00adterminal A,the row Row (u, A) (similarly the column Col(u, A)), stores the \nset of nodes u ' such that (u, A, u ' ) (similarly (u ' ,A, u)) is in Hs. Now, the bottlenecks of the \nalgorithm are the two nested loops (lines 7 9 and 10 12). We speed them up by implementing them using \nset difference operations for example, the loop from line 7 9 is replaced by: for each production A . \nCB do for u ' . (Col(u, C) \\ Col(v, A)) do insert (u ' ,A, v) into Hs and W. Assuming a fast set implementation, \nthe cost for this loop is in a given iteration of the main loop is O(n/ log n + s),where s is the number \nof new tuples inserted into Hs. Since the number of inser\u00adtions into Hs is O(n 2), its total cost during \na complete run of the algorithm is O(n 3/ log n). The same argument holds for the other loop.Letuscallthemodi.edalgorithm \nCFL-REACHABILITY.By the discussion above: THEOREM 2. The algorithm CFL-REACHABILITY solves the all\u00adpairs \nCFL-reachability problem for a .xed-sized grammar and a graph with n nodes in O(n 3/ log n) time and \nO(n 2) space. Theorem 2 improves the previous cubic bound for all-pairs or, for that matter, single-source, \nsingle-sink CFL-reachability. By our discussion in Section 2, this implies subcubic, set-based algorithms \nfor Datalog chain query evaluation as well as the many program analysis applications of CFL-reachability. \n  4. All-pairs reachability in bounded-stack RSMs Is a better algorithm for RSM-reachability possible \nif the input RSM is bounded-stack? In this section, we show that this is indeed the case. As we mentioned \nearlier, the only previously known way to solve reachability in bounded-stack machines is to use summarization, \nwhich gives a cubic algorithm; speeding it up using the technique we presented earlier leads to a factor-log \nn speedup. Now we show that the bounded-stack property gives us a second logarithmic speedup. Our algorithm \ncombines graph search with a speedup technique used by Rytter (1983, 1985) to recognize languages of \nloop-free 2-way PDAs1. Unlike the algorithm for general RSMs, it is not just an application of existing \ntechniques, and we consider it the main new algorithm of this paper. We start by reviewing search-based \nalgorithms for reachability in (general) RSMs. Let M be an RSM as in Section 2, and recall the relation \nH de.ned in Section 3 henceforth, we view it as a graph and call it the summary graph of M. The edges \nof H are classi.ed as follows: Edges ((b, en),en),where b isabox and en is an entry state in Y (b), \nare known as call edges;  Edges ((b, en), (b, ex)),where b is a box, and en is an entry and ex an exit \nin Y (b), are called summary edges;  Edges that are also edges of M are called local edges.  Note that \na state v is same-context reachable from a state u iff there is a pathin H from u to v made only of local \nand summary edges. Let the set of states same-context reachable from u be denoted by Hs(u). While the \ncall and local edges of H are speci.ed directly by M, we need to determine reachability between entries \nand exits in order to identify the summary edges. The search-based formulation of summarization (Reps \net al. 1995; Horwitz et al. 1995) views reachability computation for M (or, in other words, computation \nof the transitive closure H * of H) as a restricted form of incremental transitive closure. A search \nalgorithm is employed to compute reachability in H; when an exit ex is found to be same-context\u00adreachable \nfrom en, the summary edge ((b, en), (b, ex)) is added to the graph. The algorithm must now explore these \nadded edges along with the edges in the original graph. Let us now assume that M is bounded-stack. Consider \nany call (b, en) in the summary graph H. Because M is bounded-stack, this state is unreachable from the \nstate en. Hence, (b, en) and en are not in the same strongly connected component (SCC) in H,and a call \nedge is always between two SCCs. The situation is sketched in Figure 5. The nodes are states of M (en \nis an entry and ex is an exit in the same component, while b is a box), and the large circles denote \nSCCs. We do not draw edges within the same SCC the dotted line from en to ex indicates that ex is same-context \nreachable from en. We will argue that all summary edges in H may be discovered using a variant of depth-.rst \ngraph search (DFS). To start with, let us assume that the summary graph H is acyclic, and consider a \ncall (b, en) in it. First we handle the case when no path in H from en contains a call. As a summary-edge \nalways starts from a call, this means that no such path contains a summary-edge either, and the part \nof H reachable from en is not modi.ed due to summary edge discovery. Thus, the set Hs(en) of states v \nsame-context reachable (i.e., reachable via summary and local edges) from en can be computed by exploring \nH depth-.rst from en. Further, because the graph is acyclic, the same search can label each such v with \nthe set Hs(v). This is done as follows: if v has no children, then Hs(v)= {v};  if v has children u1,u2,...,um,then \n  HsHs (v)= (ui). i 1 A loop-free 2-PDA is one that has no in.nite execution on any word. The recognition \nproblem for loop-free 2-PDAs reduces to reachability in acyclic RSMs i.e., RSMs whose con.guration graphs \nare cycle-free. Obviously, these are less general than bounded-stack RSMs. local edge call edge summary \nedge Figure 5. All-pairs reachability in bounded-stack RSMs Once we have computed the set Hs(en) of such \nv-s that are same-context reachable from en, we can, consulting the transi\u00adtion relation of M, determine \nall summary edges ((b, en), (b, ex)). Note that these are the only summary edges from (b, en) that can \never be added to H. However, these summary edges may now be explored via the same depth-.rst traversal \nwe may view them simply as edges explored after the call-edge to en due to the DFS order. The same search \ncan compute the set Hs(u) for each new state u found to be reachable from the return (b, ex). Note that \ndescendants of (b, ex) may also be descendants of en for exam\u00adple, a descendant x of en may be reachable \nfrom a different en\u00adtry point en ' of Y (b), which may be called by a call reachable from (b, ex). In \nother words, the search from (b, ex) may encounter some cross-edges, thus needing to use some of the \nHs-sets com\u00adputed during the search from en. Once the Hs-sets for en and all summary-children (b, ex) \nare computed, we can compute the set Hs((b, en)). Since we are only interested in reachability via sum\u00admary \nand local edges and a call has no local out-edges, this set is the union of the Hs-sets for the summary \nchildren. Now suppose there are at most p = 1 call states in a path in H from en. Let the state (b ' \n,en ' ) be the .rst call reached from en in a depth-.rst exploration because of the bounded-stack property, \nno descendant of en ' can reach en in H. Now, there can be at most (p - 1) calls in a path from en ' \n, so that can inductively determine the summary edges from (b ' ,en ' ), explore these edges, and label \nevery state v in the resultant tree by the set Hs(v). It is easy to see that this DFS can be weaved into \nthe DFS from en. The above algorithm, however, will not work when H has cycles. This is because in a \ngraph with cycles, a simple DFS cannot construct the sets Hs(v) for all states v. This dif.culty, however, \nmay be resolved if we use, instead of a plain DFS, a transitive closure algorithm based on Tarjan s algorithm \nto compute the SCCs of a graph (Aho et al. 1974). Many such algorithms are known in the literature (Purdom \n1970; Eve and Kurki-Suonio 1977; Schmitz 1983). Let Reach(v) denote the set of nodes reachable from a \nnode v in a graph. The .rst observation that these algorithms use is that for any two nodes v1 and v2 \nin the same SCC of a graph, we have Reach(v1)= Reach(v2). Thus, it is suf.cient to compute the set Reach \nfor a single representative node per SCC. The second main idea is based on a property of Tarjan s algorithm. \nTo understand it, we will have to de.ne the condensation graph G.of a graph G: the nodes of G.are the \nSCCs of G; the edge set is the least set constructed by: if, for nodes S1 and S2 of G., G has nodes u \n.S1,v .S2 such that there is an edge from u to v,then G.has an edge from S1 to S2. Now, Tarjan s algorithm, \nwhen running on a graph G, piggy\u00adbacks a depth-.rst search of the graph and outputs the nodes of G.in \na bottom-up topological order. This is possible because the con\u00addensation graph of any graph is acyclic. \nFor example, running on the graph in Figure 5 (let us assume that all the edges are known), the algorithm \nwill .rst output the SCC containing en, then the one containing (b, ex), then the one containing (b, \nen), etc.We can,in fact, view the algorithm as performing a DFS on the condensation graph of G. In the \nsame way as when our input graph was acyclic, we can now compute, for every node S in the condensation \ngraph, the set of nodes Reach(S) reachable from that SCC, de.ned as:  Reach(S)= Reach(u). u.S For each \nS, this set is known by the time the algorithm returns from the .rst node in S to have been visited in \nthe depth-.rst search. Assuming that we have a transitive closure algorithm of the above form, let us \nfocus on bounded-stack RSMs again. Let us also suppose that we are only interested in same-context reachabil\u00adity. \nWe apply the transitive closure algorithm to the graph H after modifying it in the two following ways. \nFirst, we ensure that the sets Reach(u), for a state u, only contain descendants of u reach\u00adable via \nlocal and summary edges this requires a trivial modi\u00ad.cation of the algorithm. To understand the second \nmodi.cation, consider once again a call (b, en) in a summary graph H; note that the call edge ((b, en),en) \nis an edge in the condensation graph H.. Thus, the set Reach(Sen),where Sen is the SCC of en, is known \nby the time the transitive closure algorithm is done exploring this edge. Now we can construct all summary \nedges from (b, en) and add them as outgoing edges from (b, en), viewing them, as in the acyclic case, \nas normal edges appearing after the call-edge in the or\u00adder of exploration. The set Reach(S(b,en)) can \nnow be computed. By the time the above algorithm terminates, Reach(Su)= Hs(u) for each state u i.e., \nwe have determined all-pairs same\u00adcontext reachability in the RSM. To determine all-pairs reachabil\u00adity, \nwe simply insert the call edges into the summary graph, and compute its transitive closure. In fact, \nwe can do better: with some extra book-keeping, it is possible to compute reachability in the same depth-.rst \nsearch used to compute same-context reachability (i.e., summary edges). Next we present an algorithm \nfor graph transitive closure that, in addition to being based on Tarjan s algorithm, also uses fast sets \nto achieve a subcubic complexity. Using the technique outlined above, we modify it into an algorithm \nfor bounded-stack RSM\u00adreachability of O(n 3/ log2 n) complexity. 4.1 Speeding up search-based transitive \nclosure The algorithm that we now present combines a Tarjan s-algorithm\u00adbased transitive closure algorithm \n(studied, for example, by Schmitz (1983) or Purdom (1970)) with a fast-set-based speedup technique used \nby Rytter (1983, 1985) to solve the recognition problem for a subclass of 2-PDAs. While subcubic algorithms \nfor graph transi\u00adtive closure have been known for a long time, this is, so far as we know, the .rst algorithm \nthat is based on graph traversal and yet runs in O(n 3/ log2 n) time. Both these features are necessary \nfor an O(n 3/ log2 n)-time algorithm on bounded-stack RSMs. As in our previous algorithms, we start with \na baseline cubic\u00adtime algorithm and speed it up using fast sets. This algorithm, called BASELINE-CLOSURE \nand shown in Figure 6, is simply a DFS-based transitive closure algorithm. Let us .rst see how it detects \nstrongly connected components in a graph G.The main VISIT(u) 1add u to Visited 2 push(u, L) 3 low(u) \n.dfsnum(u) .height(L) 4 Reach(u) .\u00d8; rep(u) .. 5 Out(u) .\u00d8; Next(u)= {children of u }6 for v .Next(u) \n7 do if v/.Visited then VISIT (v) 8 if v .Done 9 then add v to Out(u) 10 else low(u) .min(low(u), low(v)) \n11 if low(u)= dfsnum(u) 12 then repeat 13 v .pop(L) 14 add v to Done 15 add v to Reach(u) 16 Out(u) .Out(u) \n.Out(v) 17 rep(v) .u 18 until v = u 19 Reach(u) .Reach(u) . Reach(rep(v)) v.Out(u) BASELINE-CLOSURE() \n1 Visited .\u00d8; Done .\u00d8 2 for each node u 3 do if u/.Visited then VISIT (u) Figure 6. Transitive closure \nof a directed graph idea is that in any DFS tree of G, the nodes belonging to a particular SCC form a \nsubtree. The node u0 in an SCC S that is discovered .rst in a run of the algorithm is marked as the representative \nof S; for each node v in S, rep(v) denotes the representative of S (in this case u0). A global stack \nL supporting the usual push and pop operations is maintained; height(L) gives the height of the stack \nat any given time. As soon as we discover a node, we push it on this stack note that for any SCC, the \nrepresentative is the .rst node to be on this stack. For every node u, dfsnum(u) is the height of the \nstack when it was discovered, and low(u) equals, once the search from u has returned, the minimum dfsnum-value \nof a node that a descendant of u in the DFS tree has an edge to. Now observe that if low(u)= dfsnum(u) \nat the point when the search is about to return from a node u,then u is the representative of some SCC. \nWe maintain the invariant that all the elements above and inclusive of u in the stack belong to the SCC \nof u. Before returning from u, we pop all these nodes and output them as an SCC. Nodes in SCCs already \ngenerated are stored in a set Done. Now we shall see how to generate the set of nodes reachable from \na node of G.Let S be an SCCof G; we want to compute the set Reach(S) of nodes reachable from S. Consider \nthe condensa\u00ad tion graph G.of G,where S is a node. If S has no children in the graph, then Reach(S)= \nS; if it has children S1,S2,...,Sk,then Reach(S)= i Reach(Si). Once this set is computed, we store it \nin a table Reach indexed by the representatives of the SCCs of G. Of course, we compute this set as well \nas generate the SCCs in one depth-.rst pass of G. Recall that the SCCs of G are generated in a bottom-up \ntopological order (the outputting of SCCs is done by lines 12 19 of VISIT, the recursive depth-.rst traversal \nroutine of our algorithm). By the time S is generated, the SCCs reachable from it in G.have all been \ngenerated, and the entries of Reach corresponding to the representatives of these reachable SCCs have \nbeen precisely computed. Then all we need to .ll out Reach(u0), where u0 is the representative of S, \nis to track the edges out of S and take the union of S and the entries of Reach corresponding to the \nchildren of S in G.. Note that these outgoing edges could either be edges in the DFS tree or DFS cross \nedges. They are tracked using a table Out indexed by nodes of G for any u in S, Out(u) contains the nodes \noutside of S to which an edge from u may lead. At the end of the repeat-loop from line 13 18, Out(u0) \ncontains all nodes outside S with an edge from inside S. Now line 19 computes the set of nodes reachable \nfrom u0. As for the time complexity of this algorithm, note that for each u,VISIT(u) is called at most \nonce. Every line other than 16 and 19 costs time O(m + n) duringa runof BASELINE-CLOSURE,and since line \n16 tries to add a node to Out(u) once for every edge out of the SCC of u in G., its total cost is O(m). \nLine 19 does a union of two sets of nodes for each edge in G., so that its total cost is O(mn). As for \nspace complexity, the sets Reach(u) can be stored using O(n 2) space, a cost that subsumes the space \nrequirements of the other data structures. Then we have: LEMMA 3. BASELINE-CLOSURE terminates on any \ngraph G with n nodes and m edges in time O(mn). On termination, for every node u of G, Reach(rep(u)) \nis the set of nodes reachable from u. The algorithm requires O(n 2) space. We will now show a way to \nspeed up the procedure BASELINE-CLOSURE using a slight modi.cation of Rytter s (1983, 1985) speedup for \nloop-free 2-PDAs. Let V be the set of all nodes of G (we have |V | = n), p = !log n/2l,and r = !n/pl.We \nuse fast set representations of sets of nodes X .V each such set is represented as a sequence r words, \neach of length p. We will need to convert a list representation of X into a fast set representation as \nabove. It is easy to see that this can be done using a sort in O(n log n) time. /* speeds up the operation \n Reach(u) . Reach(rep(v)) */ v.Out(u) let x1,...,xr be the words in the fast set for Out(u) in SPEEDUP() \n1 compute (x1,...,xr) 2 for 1 =i =r 3 do if xi = 0 continue 4 if Cache(i, xi)=. 5 then Cache(i, xi) ..v.Set(i,xi)Reach(rep(v)) \n6 Reach(u) .Reach(u) .Cache(i, xi) Figure 7. The speedup routine Now recall that the bottleneck of the \nbaseline algorithm is line 19 of the routine VISIT, which costs O(mn) over an entire run of the algorithm. \nNow we show how to speed up this line. First, let us implement BASELINE-CLOSURE such that entries of \nthe table Reach are stored as fast sets, and the sets Out(u) are represented as lists. Now consider the \nprocedure SPEEDUP in Fig. 7, which is a way to speed up computation of the recurrence Reach(u) . Reach(rep(v)). \nThe idea is cache the v.Out(u) value (.v.X Reach(rep(v))) exhaustively for all non-empty sets X that \nare suf.ciently small, and use this cache to compute the value for larger sets Out(u). This is done using \na table Cache (of global scope) such that for each 1 = i = r and for each word w = 0 of length p, we \nhave a table entry Cache(i, w) containing either a subset of V , represented as a fast set, or a special \nnull value .(note that the pair (i, w) uniquely identi.es a subset of V of size at most p this set is \ndenoted by Set(i, w)). Initially, every entry of Cache equals .. Let us now use the Assign-Union operation \nfor fast sets (see Section 2) to implement line 6 of SPEEDUP, and replace line 19 of VISIT by a call \nto SPEEDUP. To see that this leads to a speedup, note that Cache has at most r.2p = O(n 3/2/ log n) entries. \nNow, line 5 in SPEEDUP gets executed at most once for each cell in Cache during a complete run of CLOSURE \ni.e., O(r.2p)= O(n 3/2/ log n) times. Each time it is executed, it costs O(n) time (as Set(i, xi) is \nof size O(log n) and as union of two en\u00adtries of Reach costs O(n/ log n) time), so that its total cost \nis O(n 5/2/ log n). Thus, the bottleneck is line 6. Let us compute the total number of times this line \nis executed during a run of closure. Since the total size of all the Out(u) s during a run of BASELINE-CLOSURE \nis bounded by m, the emptiness test in line 3 ensures that line 6 is executed O(m) times in total during \na run of the closure algorithm (this is the tighter bound when the graph is sparse). The other obvious \nbound on the number of exe\u00adcutions of this line is O(r.n) (this captures the dense case). Each time it \nis executed, it costs time O(r). Thus, the total complexity of the modi.ed algorithm (let us call this \nalgorithm CLOSURE)is O(min{m.r, r.n.r}) i.e., O(min{mn/ log n, n 3/ log2 n}). As for the space requirement \nof the algorithm, each fast set stored in a cell of the table Cache costs space O(n).As Cache has O(n \n3/2/ log n) cells, the total cost of maintaining this table is O(n 5/2/ log n). The space costs of the \nother data structures, including the table needed for fast sets operations if unit-cost word operations \nare not available, is subsumed by this cost. Hence we have: THEOREM 3. CLOSURE computes the transitive \nclosure of a di\u00adrected graph with n nodes and m edges in O(min{mn/ log n, n 3/ log2 n}) time and O(n \n5/2/ log n) space. 4.2 Bounded-stack RSMs Using the ideas discussed earlier in this section, the algorithm \nCLOSURE can now be massaged into a reachability algorithm for bounded-stack RSMs. Figure 8 shows pseudocode \nfor a baseline algorithm for same-context reachability in bounded-stack RSMs obtained by modifying BASELINE-CLOSURE.The \nsets Hs(u) in the new algorithm correspond to the sets Reach(u) in the transitive closure algorithm. \nThe main difference lies in lines 14 17, which insert the summary edges into the graph. Also, as it is \nsame-context reachability that we are computing, a child is added to the set Out(u) only if it is reached \nalong a local or summary edge (the else condition in line 17). A correctness argument may be given following \nthe discussion earlier in this section. Adding an extra transitive closure step at the end of this algo\u00adrithm \ngives us an algorithm for reachability. With some extra book\u00adkeeping, it is possible to evade this last \nstep and compute reacha\u00adbility and same-context reachability in the same search we omit the details. \nThe speedups discussed earlier in this section may now be applied. Let us call the resultant algorithm \nSTACK-BOUNDED-REACHABILITY.Itiseasy tosee thatits complexityis the same as that of CLOSURE. The only \nextra overhead is that of inserting the summary edges, and it is subsumed by the costs of the rest of \nthe al\u00adgorithm. Thus, the algorithm STACK-BOUNDED-REACHABILITY has time complexity O(min{mn/ log n, n \n3/ log2 n}),where m and n are the number of edges and nodes in the summary graph of the RSM. The space \ncomplexity is as for CLOSURE. In general, m is O(n 2),sothat: THEOREM 4. The algorithm STACK-BOUNDED-REACHABILITY \ncomputes all-pairs reachability in a bounded-stack RSM of size n in O(n 3/ log2 n) time and O(n 5/2/ \nlog n) space. We note that an algorithm as above cannot be obtained from any of the existing subcubic \nalgorithms for graph transitive closure. All previously known O(n 3/ log2 n)-time algorithms for graph \nVISIT(u) 1add u to Visited 2 push(u, L) 3 low(u) .dfsnum(u) .height(L) 4 Hs(u) .\u00d8; rep(u) .. 5 Out(u) \n.\u00d8 6 if u is an internal state 7 then Next(u) .{v : u .v} 8 else if u is a call (b, en) 9 then Next(u) \n.{en} 10 else Next(u) .\u00d8 11 for v .Next(u) 12 do if v/.Visited then VISIT (v) 13 if v .Done 14 then if \nu =(b, en) is a call and v = en 15 then for exit states ex .Hs(en) 16 do add (b, ex) to Next(u) 17 else \nadd v to Out(u) 18 else low(u) .min(low(u), low(v)) 19 if low(u)= dfsnum(u) 20 then repeat 21 v .pop(L) \n22 add v to Done 23 add v to Hs(u) 24 Out(u) .Out(u) .Out(v) 25 rep(v) .u 26 until v = u 27 Hs(u) .Hs(u) \n. Hs(rep(v)) v.Out(u) BASELINE-SAME-CONTEXT-STACK-BOUNDED-REACHABILITY() 1 Visited .\u00d8; Done .\u00d8 2 for \neach state u 3 do if u/.Visited then VISIT (u) Figure 8. Same-context reachability in bounded-stack RSMs \ntransitive closure use reductions to boolean matrix multiplication and do not permit online edge addition \neven if, as is the case for bounded-stack RSMs, these edges arise in a special way. While Chan (2005) \nhas observed that DFS-based transitive closure may be computed in time O(mn/ log n) using fast sets, \nthis complexity does not suf.ce for our purposes.   5. Reachability in hierarchical state machines \nAs we saw, the reason why reachability in bounded-stack RSMs is easier than general RSM-reachability \nis that summary edges in the former case have a depth-.rst structure. For hierarchical state machines, \nthe structure of summary edges is restricted enough to permit an algorithm with the same complexity as \nboolean matrix multiplication. Let us have as input a hierarchical state machine M with com\u00adponents M1,...,Mk, \nsuch that a call from the component Mi can only lead to a component Mj for j>i. The summary graph H of \nM may be partitioned into k subgraphs H1,...,Hk such that call\u00adedges only run from partitions Hi to partitions \nHj,where j>i. As the component Mk does not call any other component, there are no summary edges in Hk. \nTo compute reachability in M, .rst compute the transitive clo\u00adsure of Hk. Next, for all entries en and \nexits ex of Mk and all boxes b with Y (b)= k, add summary edges ((b, en), (b, ex)). Now remove the call \nedges from Hk-1 and compute its transitive closure and, once this is done, use the newly discovered reachabil\u00adity \nrelations to create new summary edges in subgraphs Hj,where j<k -1. Note that we do not need to process \nthe graph Hk again. We proceed inductively, processing every Hi only once. Once the transitive closure \nof H1 is computed, we add all the call edges from the different H1 s and compute the transitive closure \nof the entire graph. By Lemma 1, there is an edge from v to v ' in the .nal clo\u00adsure iff v ' is reachable \nfrom v. As for complexity, let n be the total number of states in A,and let ni be the number of states \nin the subgraph Hi.Let BM (n)= O(n 2.376) be the time taken to multiply two n \u00d7n boolean matri\u00adces. Since \ntransitive closure of a .nite relation may be reduced to boolean matrix multiplication, the total cost \ndue to transitive clo\u00adsure computation in the successive phases, as well as the .nal tran\u00adsitive closure, \nis SiBM (ni)+ BM (n)= O(BM (n)).The to\u00adtal cost involved in identifying and inserting the summary and \ncall edges is O(n 2). Assuming BM (n)= .(n 2),we have: THEOREM 5. All-pairs reachability in hierarchical \nstate machines 2.376 can be solved in time O(BM (n)),where BM (n)= O(n ) is the time taken to multiply \ntwo n \u00d7n boolean matrices. Of course, the above procedure is far from compelling the cu\u00adbic, summarization-based \nreachability algorithm published in the original reference on the analysis of these machines (Alur and \nYan\u00adnakakis 1998) is going to outperform it in any reasonable applica\u00adtion. However, taken together with \nour other results, it highlights a gradation in the structure of the summary graph and the complexity \nof RSM-reachability as recursion in the input RSM is constrained. 6. Conclusion In this paper, we have \nadapted a simple existing technique into the .rst subcubic algorithms for RSM-reachability and CFL\u00adreachability, \nand identi.ed a way to exploit constraints on re\u00adcursion during reachability analysis of RSMs. In summarization\u00adbased \nanalysis of general RSMs, summary edges can arise in arbitrary orders, and all-pairs reachability can \nbe determined in time O(n 3/ log n). For bounded-stack RSMs, summary edges have a depth-.rst structure, \nand the problem can be solved in O(n 3/ log2 n) time using a modi.cation of a DFS-based transitive closure \nalgorithm. For hierarchical state machines, the problem is essentially that of computing transitive closure \nof the components. Given that RSM-reachability is a central algorithmic problem in program analysis, \nthe natural next step is to evaluate the practical bene.ts of these contributions. Such an effort should \nremember that real implementations of RSM-reachability-based program analyses apply heuristics such as \ncycle elimination and node clustering, and are often .ne-tuned to the speci.c problem at hand. Thus, \ninstead of implementing our algorithms literally, the goal should be to explore combinations of techniques \nknown to work in practice with the high-level ideas used in this paper. As for algorithmic directions, \na natural question is whether this is the best we can do. A hard open question is whether all-pairs CFL-reachability \ncan be reduced to boolean matrix multiplication. This would be especially satisfactory as the former \ncan be trivially seen to be as hard as the latter. Yannakakis (1990) has noted that Valiant s reduction \nof context-free recognition to boolean matrix multiplication (Valiant 1975) can be applied directly to \nreduce CFL-reachability in acyclic graphs to boolean matrix multiplication. However, there seem to be \nbasic dif.culties in extending this method to general graphs. Another set of questions involves stack-bounded \nRSMs and our transitive closure. Given a program without in.nite recursion, can we automatically generate \na stack-bounded abstraction that can be analyzed faster than a general RSM abstraction? Can our transitive \nclosure algorithm have applications in other areas for example, databases? Recall that, being a search-based \nalgorithm, it does not require the input graph to be explicitly represented, and is suitable for computing \npartial closure i.e., computing the sets of nodes reachable from some, rather than all, nodes. Algorithms \nwith such features have been studied with theoretical as well as practical motivations a new engineering \nquestion would be to see how well the techniques of this paper combine with them. Acknowledgements: The \nauthor thanks Rajeev Alur, Byron Cook, Stephen Fink and Mihalis Yannakakis for valuable comments. An \nanonymous referee pointed out that Rytter s speedup could be ap\u00adplied directly to the classical CFL-reachability \nalgorithm; we thank himorher forthis.  References A. V. Aho, J. E. Hopcroft, and J. D. Ullman. The \nDesign and Analysis of Computer Algorithms. Addison-Wesley Series in Computer Science and Information \nProcessing. Addison-Wesley, 1974. R. Alur and M. Yannakakis. Model checking of hierarchical state machines. \nIn 6th ACM Symposium on Foundations of Software Engineering, pages 175 188, 1998. R. Alur, M. Benedikt, \nK. Etessami, P. Godefroid, T. Reps, and M. Yan\u00adnakakis. Analysis of recursive state machines. ACM Transactions \non Programming Languages and Systems, 27(4):786 818, 2005. V. L. Arlazarov, E. A. Dinic, M. A. Kronrod, \nand I. A. Farad.zev. On economical construction of the transitive closure of an oriented graph. Soviet \nMathematics Doklady, 11:1209 1210, 1970. ISSN 0197 6788. T. Ball and S. Rajamani. The SLAM toolkit. In \n13th International Conference on Computer Aided Veri.cation, pages 260 264, 2001. A. Bouajjani, J. Esparza, \nand O. Maler. Reachability analysis of pushdown automata: Applications to model checking. In 8th International \nConfer\u00adence on Concurrency Theory, LNCS 1243, pages 135 150, 1997. T. M. Chan. All-pairs shortest paths \nwith real weights in o(n3/ log n)) time. In 9th Workshop on Algorithms and Data Structures, pages 318 \n324, 2005. T. M. Chan. More algorithms for all-pairs shortest paths in weighted graphs. In 39th ACM Symposium \non Theory of Computing, pages 590 598, 2007. J. Eve and R. Kurki-Suonio. On computing the transitive \nclosure of a relation. Acta Informatica, 8:303 314, 1977. J.E. Hopcroft and J.D. Ullman. Introduction \nto Automata Theory, Lan\u00adguages, and Computation. Addison-Wesley, 1979. S. Horwitz, T. W. Reps, and D. \nBinkley. Interprocedural slicing using de\u00adpendence graphs (with retrospective). In Best of Programming \nLanguage Design and Implementation, pages 229 243, 1988. S. Horwitz, T. Reps, and M. Sagiv. Demand interprocedural \ndata.ow anal\u00adysis. In 3rd ACM Symposium on Foundations of Software Engineering, pages 104 115, 1995. \nD. Melski and T. W. Reps. Interconvertibility of a class of set constraints and context-free-language \nreachability. Theoretical Computer Science, 248(1-2):29 98, 2000. P. W. Purdom. A transitive closure \nalgorithm. BIT, 10:76 94, 1970. J. Rehof and M. F\u00a8ahndrich. Type-base .ow analysis: from polymorphic \nsubtyping to CFL-reachability. In 28th ACM Symposium on Principles of Programming Languages, pages 54 \n66, 2001. T. Reps. Shape analysis as a generalized path problem. In ACM Work\u00adshop on Partial Evaluation \nand Semantics-Based Program Manipula\u00adtion, pages 1 11, 1995. T. Reps. Program analysis via graph reachability. \nInformation and Software Technology, 40(11-12):701 726, 1998. T. Reps, S. Horwitz, and S. Sagiv. Precise \ninterprocedural data.ow analysis via graph reachability. In 22nd ACM Symposium on Principles of Programming \nLanguages, pages 49 61, 1995. T. W. Reps, S. Schwoon, and S. Jha. Weighted pushdown systems and their \napplication to interprocedural data.ow analysis. In 10th Static Analysis Symposium, pages 189 213, 2003. \nW. Rytter. Time complexity of loop-free two-way pushdown automata. Information Processing Letters, 16(3):127 \n129, 1983. W. Rytter. Fast recognition of pushdown automaton and context-free lan\u00adguages. Information \nand Control, 67(1-3):12 22, 1985. L. Schmitz. An improved transitive closure algorithm. Computing, 30: \n359 371, 1983. M. Sharir and A. Pnueli. Two approaches to interprocedural data.ow analysis. Program Flow \nAnalysis: Theory and Applications, pages 189 234, 1981. M. Sridharan, D. Gopan, L. Shan, and R. Bod\u00b4ik. \nDemand-driven points-to analysis for Java. In 20th ACM Conference on Object-Oriented Pro\u00adgramming, Systems, \nLanguages, and Applications, pages 59 76, 2005. L. G. Valiant. General context-free recognition in less \nthan cubic time. Journal of Computer and System Sciences, 10(2):308 315, 1975. M. Yannakakis. Graph-theoretic \nmethods in database theory. In 9th ACM Symposium on Principles of Database Systems, pages 230 242, 1990. \n \n\t\t\t", "proc_id": "1328438", "abstract": "<p>We show that the reachability problem for recursive state machines (or equivalently, pushdown systems), believed for long to have cubic worst-case complexity, can be solved in slightly subcubic time. All that is necessary for the new bound is a simple adaptation of a known technique. We also show that a better algorithm exists if the input machine does not have infinite recursive loops.</p>", "authors": [{"name": "Swarat Chaudhuri", "author_profile_id": "81309496839", "affiliation": "Pennsylvania State University, University Park, PA", "person_id": "P767505", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1328438.1328460", "year": "2008", "article_id": "1328460", "conference": "POPL", "title": "Subcubic algorithms for recursive state machines", "url": "http://dl.acm.org/citation.cfm?id=1328460"}