{"article_publication_date": "01-07-2008", "fulltext": "\n Much Ado about Two (Pearl) A Pearl on Parallel Pre.x Computation Janis Voigtl\u00a8ander Institut f\u00a8ur Theoretische \nInformatik Technische Universit\u00a8at Dresden 01062 Dresden, Germany voigt@tcs.inf.tu-dresden.de Abstract \nThis pearl develops a statement about parallel pre.x computation in the spirit of Knuth s 0-1-Principle \nfor oblivious sorting algorithms. It turns out that 0-1 is not quite enough here. The perfect hammer \nfor the nails we are going to drive in is relational parametricity. Categories and Subject Descriptors \nD.1.1 [Programming Tech\u00adniques]: Applicative (Functional) Programming; D.3.3 [Program\u00adming Techniques]: \nLanguage Constructs and Features Polymor\u00adphism; F.2.2 [Analysis of Algorithms and Problem Complex\u00adity]: \nNonnumerical Algorithms and Problems; F.3.1 [Logics and Meanings of Programs]: Specifying and Verifying \nand Reasoning about Programs General Terms Algorithms, Languages, Veri.cation Keywords 0-1-principle, \nfree theorems, parallel pre.x computa\u00adtion, relational parametricity 1. Introduction Parallel pre.x \ncomputation is the task to compute, given in\u00adputs x1,...,xn and an associative operation ., the outputs \nx1,x1 . x2,...,x1 . x2 . \u00b7\u00b7\u00b7 . xn. It has numerous appli\u00adcations in the hardware and algorithmics .elds \n(Blelloch 1993). There is a wealth of solutions (Sklansky 1960; Brent and Kung 1980; Ladner and Fischer \n1980), employing the associativity of . in different ways to realize different trade-offs between certain \ncharacteristics of the resulting circuits , and more keep coming up (Lin and Hsiao 2004; Sheeran 2007). \nAn obvious concern is that for correctness of such new, and increasingly complex, meth\u00adods. One approach \nto address this concern is the derivation, using well-understood combinators, of new designs from basic \nbuilding blocks (Hinze 2004). Another is explicit proof or at least systematic (possibly exhaustive) \ntesting of new solution candidates. Of course, studies of the latter kind must be suf.ciently generic, \ngiven that in the problem speci.cation neither the type of the inputs x1,...,xn, nor any speci.cs (apart \nfrom associativity) of .are .xed. Here the 0-1-Principle of Knuth (1973) comes to mind. It states that \nif an oblivious sorting algorithm, i.e. one where the sequence of Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. comparisons performed is the same for all \ninput sequences of any given length, is correct on boolean valued input sequences, then it is correct \non input sequences over any totally ordered value set. This greatly eases the analysis of such algorithms. \nIs something similar possible for parallel pre.x computation? That is the question we address in this \npaper. 2. The Problem We cast the problem and our analysis in the purely functional pro\u00adgramming language \nHaskell (Peyton Jones 2003). Since it is uni\u00ad versal, it allows us to precisely capture the notion of \nan algorithm (that may, or may not, be a correct solution to the parallel pre\u00ad.x computation task) in \nthe most general way. It also provides the mathematical expressivity and reasoning techniques that we \nneed. Since Haskell s notation is quite intuitive, we do not pause for a detailed introduction to the \nlanguage, instead introducing concepts as we go. The Haskell Prelude (the language s standard library) \nprovides a function foldl1 :: (a.a .a) .[a] .a such that foldl1 (.)[x1,...,xn]=(\u00b7\u00b7\u00b7(x1 .x2) .\u00b7\u00b7\u00b7) .xn \n for every binary operation and type-conforming input list. While Haskell allows empty, partially de.ned, \nand in.nite lists as well, we from now on consider [a] to be the type of all non-empty, .nite lists (with \nelements of appropriate type).1 The variable aindicates that the function foldl1 can be used at arbitrary \ntype. Now we can specify our computation task as follows: scanl1 :: (a.a .a) .[a] .[a] scanl1 (.) xs \n= map (.k .foldl1 (.)(take kxs)) [1..length xs] by using Haskell s syntactic sugar [a..b] for the list \n[a,a+1,...,b] (with a,b:: Int and a=b) and some further Prelude functions: length :: [a] .Int take :: \nInt .[a] .[a] map :: (a .\u00df) .[a] .[\u00df] Here length xs gives the length of a list xs, take kxs gives the \n.rst k elements of xs,and map f xs gives the result of elementwise applying f to xs. Actually, scanl1 \nitself is also a Prelude function, though usually with a more ef.cient implementation than shown above \nfor speci.cation purposes. POPL 08, January 7 12, 2008, San Francisco, California, USA. Copyright . 2008 \nACM 978-1-59593-689-9/08/0001. . . $5.00 1 See the discussion in Section 7.  c The correctness issue \nwe are interested in is whether a given function candidate :: (a . a . a) . [a] . [a] is semantically \nequivalent to scanl1 for associative operations as .rst argument. The perfect analog to Knuth s 0-1-Principle \nwould be if this were so provided one can establish that equivalence at least on the boolean type. Unfortunately, \nit does not hold. Exhaus\u00adtive testing shows that x1 . (x1 . (x1 . x2)) = x1 . x2 for every x1,x2 :: Bool \nand (.):: Bool . Bool . Bool (associative or not). Thus, the function candidate :: (a . a . a) . [a] \n. [a] candidate (.)[x1,x2]=[x1,x1 . (x1 . (x1 . x2))] candidate (.) xs = scanl1 (.) xs is equivalent \nto scanl1 at type Bool, but clearly not in general (not even for associative .). In the context of systematic \nsearch for new solutions to the par\u00adallel pre.x computation task, Sheeran (2007) observed that verify\u00ad \ning a candidate at type [Int], with a certain .xed operation on that type and a certain form of input \nlists of type [[Int]], is enough to establish general correctness (for arbitrary associative operations). \nEssentially, this holds because the type [Int] can be used to embed freely generated semigroups over \narbitrarily large .nite generator sets. Sheeran s observation already has a feel similar to Knuth s principle, \nexcept that the type [Int] is much bigger than the boolean one; in particular, it is in.nite. Here we \nwant to improve on this. With the boolean type ruled out as above, the best thing we can hope for is \na three-valued type as discriminator between good and bad candidates.  3. The Claim We are going to \nshow that parallel pre.x computation enjoys a 0\u00ad1-2-Principle. To do so, we use the following Haskell \ndatatype: data Three = Zero | One | Two Now, for the remainder of the paper, we .x some Haskell function \ncandidate :: (a . a . a) . [a] . [a] Note that we do not put any restriction on the actual de.nition \nof candidate, just on its type. Nevertheless, we can prove the following theorem. THEOREM 1. If for every \nassociative (.):: Three . Three . Three and xs :: [Three], candidate (.) xs and scanl1 (.) xs give equal \nresults, then the same holds for every type t , associative (.):: t . t . t , and xs :: [t ].  4. The \nProof 4.1 Outline Rather than relating correctness at type Three to correctness at arbitrary type directly, \nwe perform an indirection via the type [Int]. In fact, we go through Sheeran s statement, and prove that \none as well. To formulate the indirection, we need another Haskell Prelude function (++) :: [a] . [a] \n. [a] that concatenates lists of equal type, as well as a function wrap :: a . [a] wrap x =[x] that wraps \nan element into a singleton list, and the following func\u00adtion: ups :: Int . [[Int]] ups n = map (.k . \n[0..k]) [0..n] Then, we prove two propositions. Theorem 1 arises by combining these. PROPOSITION 1. \nIf for every associative (.):: Three . Three . Three and xs :: [Three], candidate (.) xs and scanl1 (.) \nxs give equal results, then for every n :: Int with n = 0, candidate (++) (map wrap [0..n]) = ups n \nPROPOSITION 2. If for every n :: Int with n = 0, candidate (++) (map wrap [0..n]) = ups n then for every \ntype t , associative (.):: t . t . t , and xs :: [t ], candidate (.) xs and scanl1 (.) xs give equal \nresults. 4.2 A Free Theorem The key to connecting the behavior of candidate at different types, as required \nfor both propositions we want to prove, is rela\u00adtional parametricity (Reynolds 1983), in the form of \nfree theorems (Wadler 1989). It allows us to derive the following lemma just from the polymorphic type \nof candidate. LEMMA 1. For every choice of types t1,t2 and functions f :: t1 . t2, (.):: t1 . t1 . t1, \nand (.):: t2 . t2 . t2, if for every x, y :: t1, f (x . y)=(fx) . (fy) then for every z :: [t1], map \nf (candidate (.) z)= candidate (.)(map fz) There is no need to do any proof for this lemma. It can be \nobtained from the general methodology of free theorems as the result of a completely automated process, \nwith just the type of candidate as input (B\u00a8ohme 2007a). The crucial point now is to cleverly instantiate \ntypes and functions quanti.ed over in the above lemma in such a way that the instantiated versions lend \nthemselves to proving our desired propositions. This is unlikely to be achievable by machine, as it requires \nconceptual insight. 4.3 Preparatory Material Figure 1 gives some simple laws about Haskell functions \nthat we are going to use in calculations. The laws are to be read as univer\u00adsally quanti.ed over appropriately \ntyped f, g, k, xs,and ys, with the proviso that 0 = k< length xs in (6) and (7). The Haskell Prelude \nfunction (!!) :: [a] . Int . a extracts an element at a certain position from a list, counting from \nposition 0 (so that [0..n]!!k = k for every 0 = k = n). The notation (xs!!) gives a partial application \nof (!!) to xs as .rst argument, i.e., a function that expects an argument of type Int and will return \nthe element at the corresponding position in xs. Interestingly, all laws in Figure 1 except for the last \none can be obtained automatically by using the machinery of free theorems. The correctness of (7) should \nalso be clear, just as that of the following lemma of similar spirit. LEMMA 2. For every type t and xs, \nys :: [t ],if length xs = length ys and for every k :: Int with 0 = k< length xs, xs!!k = ys!!k,then \nxs = ys. One further statement we need is that for every type t , associa\u00adtive (.):: t . t . t ,and xs, \nys :: [t ], foldl1 (.)(xs ++ ys)=(foldl1 (.) xs) . (foldl1 (.) ys) (8) In fact, for the purposes of \nthis paper the above can be seen as the very de.nition of associativity. map g (map f xs)= map (g . f) \nxs (1) length (map f xs)= length xs (2) take k (map f xs)= map f (take k xs) (3) map f . wrap = wrap \n. f (4) map f (xs ++ ys)=(map f xs)++(map f ys) (5) (map f xs)!!k = f (xs!!k) (6) map (xs!!) [0..k]= \ntake (k +1) xs (7) Figure 1. Some required laws. Now we can usefully instantiate Lemma 1 in a way that \nwill bene.t our proofs of both Proposition 1 and 2. With some further calculation put in, we obtain the \nfollowing lemma. LEMMA 3. For every type t , functions h :: Int . t and associa\u00adtive (.):: t . t . t \n, and n :: Int with n = 0, map (foldl1 (.) . map h)(candidate (++) (map wrap [0..n])) = candidate (.)(map \nh [0..n]) PROOF:For every x, y :: [Int], foldl1 (.)(map h (x ++ y)) = by (5) foldl1 (.)((map hx)++(map \nhy)) = by (8) (foldl1 (.)(map hx)) . (foldl1 (.)(map hy)) Thus, map (foldl1 (.) . map h)(candidate (++) \n(map wrap [0..n])) = by Lemma 1 with t1 =[Int], t2 = t , f = foldl1 (.) . map h, . =++,and z = map wrap \n[0..n] candidate (.)(map (foldl1 (.) . map h)(map wrap [0..n])) = by (1) candidate (.)(map (foldl1 (.) \n. map h. wrap)[0..n]) = by (4) candidate (.)(map (foldl1 (.) . wrap .h)[0..n]) = by the de.nitions of \nfoldl1, wrap, and function composition candidate (.)(map h [0..n])  4.4 Proving Proposition 1 The key \ninsights on why a three-valued type suf.ces to distinguish good from bad candidates for parallel pre.x \ncomputation are en\u00adcapsulated in the following lemma (respectively, its proof and the illustrations given \ntherein; see also Section 5). Quite nicely, it can be formulated in terms of foldl1, i.e., essentially \nin terms of just one element of the output list of scanl1 at a time. The statement s lifting to the overall \ncomputation of scanl1 is then the subject of the lemma immediately following it. LEMMA 4. Let js :: [Int] \nand k :: Int with k = 0. If for every h :: Int . Three and associative (.):: Three . Three . Three, foldl1 \n(.)(map h js)= foldl1 (.)(map h [0..k]) then js =[0..k] map (h1 ki) foldl1 (.1) [ 0, [ Zero, ..., ..., \ni-1, Zero, i, One, i+1, Zero, ..., ..., k ] Zero ] \u00b7\u00b7\u00b7 Zero Zero One One \u00b7\u00b7\u00b7 One Figure 2. .0 = i = \nk. foldl1 (.1)(map (h1 ki)[0..k]) = One PROOF: Consider the following two functions: (.1):: Three . Three \n. Three x .1 Zero = x Zero .1 One = One x .1 y = Two (.2):: Three . Three . Three x .2 Zero = x x .2 \nOne = One x .2 Two = Two It is easy to check that both are associative. Further, consider the following \ntwo functions:2 h1 :: Int . Int . Int . Three h1 kij | i = j = One | 0 = j &#38;&#38; j = k = Zero | \notherwise = Two h2 :: Int . Int . Three h2 ij | i = j = One | i = j - 1= Two | otherwise = Zero Obviously, \nfor every i :: Int with 0 = i = k, foldl1 (.1)(map (h1 ki)[0..k]) = One and for every i :: Int with 0 \n= i< k, foldl1 (.2)(map (h2 i)[0..k]) = Two (see Figures 2 and 3, respectively). Thus, by the precondition, \nfor every i :: Int with 0 = i = k, foldl1 (.1)(map (h1 ki) js)= One (9) and for every i :: Int with \n0 = i<k, foldl1 (.2)(map (h2 i) js)= Two (10) as well. Then, by (9), we know that js contains every \ni :: Int with 0 = i = k exactly once (see Figures 4 and 5), and contains no other elements (see Figure \n6); i.e., js is a permutation of [0..k]. Further, by (10), we know that for every i :: Int with 0 = i< \nk, every occurrence of i in js is subsequently followed by (but not necessarily directly adjacent to) \nan occurrence of i +1 (see Figure 7). The only permutation of [0..k] with this property is [0..k] itself. \n2 Here the equality test is written = (in Haskell ==) to distinguish it from the = used for function \nde.nition. [ 0, ..., i-1, i, i+1,i+2, ..., k ] map (h2 i) [ Zero, ..., Zero, One, Two, Zero, ..., Zero \n] foldl1 (.2) Zero \u00b7\u00b7\u00b7 Zero One Two Two \u00b7\u00b7\u00b7 Two Figure 3. .0 = i< k. foldl1 (.2)(map (h2 i)[0..k]) \n= Two map (h1 ki) foldl1 (.1) [ =i, [ =One, ...,..., =i ] =One ] \u00b7\u00b7\u00b7 =One  =One Figure 4. foldl1 (.1)(map \n(h1 ki) js)= One . js contains i [ ..., i, ..., i, map (h1 ki) [ ..., One, ..., One, foldl1 (.1)  \n=Zero \u00b7\u00b7\u00b7 =Zero Two \u00b7\u00b7\u00b7 ] ... ] ... Two Figure 5. foldl1 (.1)(map (h1 ki) js)= One . js contains i at \nmost once map (h1 k 0) [ ..., /.{0,...,k}, ... ] [ ..., Two, ... ] foldl1 (.1) Two \u00b7\u00b7\u00b7 Two Figure 6. \nfoldl1 (.1)(map (h1 k 0) js)= One . js contains only 0,...,k map (h2 i) foldl1 (.2) [ ..., [ ..., i,One, \n=i+1, =Two, ...,..., =i+1 ] =Two ] One \u00b7\u00b7\u00b7 One One Figure 7. foldl1 (.2)(map (h2 i) js)= Two . every \ni in js is eventually followed by an i +1 LEMMA 5. Let jss :: [[Int]] and n :: Int with n = 0. If for \nevery h :: Int . Three and associative (.):: Three . Three . Three, map (foldl1 (.) . map h) jss then \n= scanl1 (.)(map h [0..n]) jss = ups n PROOF:We have: length jss = by (2) length (map (foldl1 (.) . \nmap h) jss) = by the precondition length (scanl1 (.)(map h [0..n])) = by the de.nition of scanl1 length \n(map (.k . foldl1 (.)(take k (map h [0..n]))) [1..length (map h [0..n])]) = by (2) length [1..length \n(map h [0..n])] = by (2) length [1..length [0..n]] = by the de.nitions of length and [a..b] length [0..n] \n= by (2) length (map (.k . [0..k]) [0..n]) = by the de.nition of ups length (ups n) Moreover, for every \nk :: Int with 0 = k< length jss, h :: Int . Three, and associative (.):: Three . Three . Three,we have:3 \nfoldl1 (.)(map h (jss!!k)) = by (6) (map (foldl1 (.) . map h) jss)!!k = by the precondition (scanl1 (.)(map \nh [0..n]))!!k = by the de.nition of scanl1 (map (.k . foldl1 (.)(take k (map h [0..n]))) [1..length (map \nh [0..n])])!!k = by (2) (map (.k . foldl1 (.)(take k (map h [0..n]))) [1..length [0..n]])!!k = by (6) \n (.k . foldl1 (.)(take k (map h [0..n]))) ([1..length [0..n]]!!k) = by the de.nitions of [a..b] and \n(!!) foldl1 (.)(take (k +1) (map h [0..n])) = by (3) foldl1 (.)(map h (take (k +1) [0..n])) = by the \nde.nitions of take and [a..b] foldl1 (.)(map h [0..k]) By Lemma 4, this implies jss!!k =[0..k] for every \nk :: Int with 0 = k< length jss. Since also (ups n)!!k = by the de.nition of ups (map (.k . [0..k]) [0..n])!!k \n= by (6) (.k . [0..k]) ([0..n]!!k) = by the de.nitions of [a..b] and (!!) [0..k] for every such k,we \nhave jss = ups n by Lemma 2. The .rst (and more complicated) half of our 0-1-2-Principle can now be proved \nusing the previous lemma in combination with (more precisely, by further instantiating) the instantiation \nof the free theorem for candidate s type we prepared in the previous subsection. PROPOSITION 1. If for \nevery associative (.):: Three . Three . Three and xs :: [Three], candidate (.) xs and scanl1 (.) xs give \nequal results, then for every n :: Int with n = 0, candidate (++) (map wrap [0..n]) = ups n PROOF:Let \nn :: Int with n = 0.For every h :: Int . Three and associative (.):: Three . Three . Three, map (foldl1 \n(.) . map h)(candidate (++) (map wrap [0..n])) = by Lemma 3 candidate (.)(map h [0..n]) = by the precondition \nscanl1 (.)(map h [0..n]) By Lemma 5 this implies candidate (++) (map wrap [0..n]) = ups n.  4.5 Proving \nProposition 2 The remaining half of our 0-1-2-Principle (i.e., Sheeran s orig\u00adinal observation) is now \njust a matter of some calculation and another use of the already partially instantiated free theorem \nfor candidate s type we prepared earlier. 3 Note that 0 = k< length [1..length [0..n]] = length [0..n] \nsince we have 0 = k< length jss and just proved that length jss = length [1..length [0..n]] = length \n[0..n]. PROPOSITION 2. If for every n :: Int with n = 0, candidate (++) (map wrap [0..n]) = ups n then \nfor every type t , associative (.):: t . t . t , and xs :: [t ], candidate (.) xs and scanl1 (.) xs give \nequal results. PROOF:Since length xs = by the de.nitions of length and [a..b] length [0..length xs - \n1] = by (2) length (map (xs!!) [0..length xs - 1]) and for every k :: Int with 0 = k< length xs, xs!!k \n= by the de.nitions of [a..b] and !! xs!!([0..length xs - 1]!!k) = by (6) (map (xs!!) [0..length xs - \n1])!!k Lemma 2 implies xs = map (xs!!) [0..length xs - 1] Thus, candidate (.) xs = by the above argument \ncandidate (.)(map (xs!!) [0..length xs - 1]) = by Lemma 3 with h =(xs!!) and n = length xs - 1 map (foldl1 \n(.) . map (xs!!)) (candidate (++) (map wrap [0..length xs - 1])) = by the precondition map (foldl1 (.) \n. map (xs!!)) (ups (length xs - 1)) = by the de.nition of ups map (foldl1 (.) . map (xs!!)) (map (.k \n. [0..k]) [0..length xs - 1]) = by (1) map (.k . foldl1 (.)(map (xs!!) [0..k])) [0..length xs - 1] = \nby (7) and the de.nitions of map and [a..b] map (.k . foldl1 (.)(take (k +1) xs)) [0..length xs - 1] \n= by (1) map (.k . foldl1 (.)(take k xs)) (map (.k . k +1) [0..length xs - 1]) = by the de.nitions of \nmap and [a..b] map (.k . foldl1 (.)(take k xs)) [1..length xs] = by the de.nition of scanl1 scanl1 (.) \nxs For the uses of Lemma 3 and of the precondition, note that n = length xs - 1 = 0 since xs :: [t ] \nis non-empty.  5. Some Re.ection Inside the proof of Lemma 4 we used the lemma s precondition that for \nevery h :: Int . Three and associative (.):: Three . Three . Three, ... only for very particular h and \n..This begs the question whether as a consequence our main result, the 0-1-2-Principle for parallel pre.x \ncomputation, could similarly be based on weaker requirements than having to check correctness of candidate \nfor every associative operation on Three and every input list over that type. And indeed, tracing the \nproofs leading up to Theorem 1 very carefully, one .nds that what we actually proved is the following \ntheorem. THEOREM 2. Let .1, .2, h1, and h2 be as in the proof of Lemma 4. If candidate (.) xs and scanl1 \n(.) xs give equal results for . = .1 and xs = map (h1 ki)[0..n] for every n, k, i :: Int with 0 = i \n= k = n,as well as for  . = .2 and xs = map (h2 i)[0..n] for every n, i :: Int with 0 = i<n,  then \nthe same holds for every type t , associative (.):: t . t . t , and xs :: [t ]. In other words, to establish \ncorrectness of candidate at arbitrary type, it suf.ces to show that it delivers correct results for .1 \non every input list of the form * ** [(Zero, ) One (, Zero)(, Two)] as well as for .2 on every input \nlist of the form [(Zero, ) * One, Two (, Zero) * ] where the () *-notation means arbitrary, including \nempty, repetition of elements. As a further note, the associativity of (.):: t . t . t is essential in \nboth Theorems 1 and 2. Via Proposition 2 it is traced back to Lemma 3. It is easy to see that none of \nthese statements would hold for arbitrary ..  6. A Challenge An interesting extension on our work here \nwould be to consider the situation for operations . that satisfy additional algebraic properties beyond \nassociativity. For example, more circuit opti\u00admization is possible in practice if . is idempotent in \naddition (Stone and Kogge 1973; Lynch and Swartzlander 1991; Knowles 2001), and having a Knuth-like principle \nfor corresponding solu\u00adtion candidates would be interesting as well. For associative, idem\u00adpotent . the \ncounterexample candidate in Section 2 is actually equivalent to scanl1 , but maybe there are other counterexamples \nshowing a 0-1-Principle to be impossible here? What is the small\u00adest type that can serve as discriminator \nbetween good and bad candidates in this situation? Our feeling is that the answers to these questions \nwill be more dif.cult to obtain than for the setting of as\u00adsociativity only as considered in the present \npaper. The reason for this is that the corresponding algebraic structures, so-called bands, appear to \nhave a far more complicated theory than semigroups do.  7. Conclusion Day et al. (1999) demonstrated \nthat Knuth s 0-1-Principle for oblivious sorting algorithms can be proved as a free theorem, ex\u00adpressed \nin terms of Haskell. Except for Dybjer et al. (2004) and Bove and Coquand (2006), this has found surprisingly \nlittle echo in a community that is otherwise so fond of functional pearls. Even though Day et al. s paper \nis not explicitly a pearl, we think that the parts on proving Knuth s principle via relational parametricity, \nand on how this might open up possibilities for proving statements of a similar spirit, but not speci.c \nto sorting, should really be regarded as such. In fact, ever since reading their account, we have been \nlooking for other classes of algorithms that might yield to the same approach. The inspiration for trying \nthat hammer in search of nails on parallel pre.x computation is entirely due to Sheeran. In her work \non hardware design using functional languages, she has em\u00adployed the 0-1-Principle for verifying sorting \nand median networks (Sheeran 2003). She also employs parametric polymorphism in various other ways via \nso-called non-standard interpretations that help to analyze circuits with respect to different criteria, \nand even to draw circuit pictures (Sheeran 2005). It is not surprising, then, that she would come up \nwith Proposition 2. We improved on this by replacing the in.nite type [Int] as discriminator between \ngood and bad candidates with a three-valued type, which is the best one can hope for, given our counterexample \nruling out the boolean type. Throughout, our reasoning was done under certain simplifying assumptions \nabout the semantics of Haskell types and functions. For example, we restricted attention to non-empty, \n.nite lists, while Haskell allows empty, partially de.ned, and in.nite lists as well, for which Lemma \n2 is not necessarily true. More generally, we have completely ignored the presence of unde.ned values, \ni.e., of ., in Haskell. In that sense, our reasoning has been fast and loose, but morally correct (Danielsson \net al. 2006). This was done on purpose to expose our main ideas without too much added noise during calculation. \nGiving a more pedantic account of basically the same material is no doubt possible. It would build on \nthe lessons of Johann and Voigtl\u00a8ander (2004) regarding the possible interactions between . and free \ntheorems in Haskell, and potentially on those of Gibbons and Hutton (2005) regarding reasoning about \nnon-.nite lists. Even independently of intricacies involving partial or in.nite values, the reader might \nbe worried about the overall correctness of our development, in particular in view of our proof by pictures \nfor Lemma 4. Can we really be sure that we have not slipped up somewhere, with potentially dire consequences? \nYes we can, thanks to B\u00a8ohme (2007b), who has reproduced our whole line of reasoning with an interactive \nproof assistant. That is, there is now a complete proof script for Isabelle (Nipkow et al. 2002) leading \nfrom Lemma 1 to Theorem 1.  Acknowledgments I thank Mary Sheeran for sharing her draft paper on systematic \nsearch for new solutions to the parallel pre.x computation task (Sheeran 2007). I also thank the POPL \nreviewers for their feedback. References G.E. Blelloch. Pre.x sums and their applications. In J.H. Reif, \neditor, Syn\u00adthesis of Parallel Algorithms, pages 35 60. Morgan Kaufmann, 1993. S. B\u00a8ohme. Free theorems \nfor sublanguages of Haskell. Master s thesis, Technische Universit\u00a8at Dresden, 2007a. S. B\u00a8ohme. Much \nado about two. Formal proof development. In G. Klein, T. Nipkow, and L. Paulson, editors, The Archive \nof Formal Proofs. http://afp.sf.net/entries/MuchAdoAboutTwo.shtml , 2007b.  A. Bove and T. Coquand. \nFormalising bitonic sort in type theory. In Types for Proofs and Programs, TYPES 2004, Revised Selected \nPapers, volume 3839 of LNCS, pages 82 97. Springer-Verlag, 2006.  R.P. Brent and H.T. Kung. The chip \ncomplexity of binary arithmetic. In ACM Symposium on Theory of Computing, Proceedings, pages 190 200. \nACM Press, 1980. N.A. Danielsson, R.J.M. Hughes, P. Jansson, and J. Gibbons. Fast and loose reasoning \nis morally correct. In Principles of Programming Languages, Proceedings, pages 206 217. ACM Press, 2006. \n N.A. Day, J. Launchbury, and J. Lewis. Logical abstractions in Haskell. In Haskell Workshop, Proceedings. \nTechnical Report UU-CS-1999-28, Utrecht University, 1999. P. Dybjer, Q. Haiyan, and M. Takeyama. Verifying \nHaskell programs by combining testing, model checking and interactive theorem proving. Information &#38; \nSoftware Technology, 46(15):1011 1025, 2004. J. Gibbons and G. Hutton. Proof methods for corecursive \nprograms. Fun\u00addamenta Informaticae, 66(4):353 366, 2005. R. Hinze. An algebra of scans. In Mathematics \nof Program Construction, Proceedings, volume 3125 of LNCS, pages 186 210. Springer-Verlag, 2004.  P. \nJohann and J. Voigtl\u00a8ander. Free theorems in the presence of seq.In Prin\u00adciples of Programming Languages, \nProceedings, pages 99 110. ACM Press, 2004. S. Knowles. A family of adders. In Computer Arithmetic, Proceedings, \npages 277 284. IEEE Press, 2001. D.E. Knuth. The Art of Computer Programming, volume 3: Sorting and Searching. \nAddison-Wesley, 1973. R.E. Ladner and M.J. Fischer. Parallel pre.x computation. Journal of the ACM, 27(4):831 \n838, 1980. Y.-C. Lin and J.-W. Hsiao. A new approach to constructing optimal parallel pre.x circuits \nwith small depth. Journal of Parallel and Distributed Computing, 64(1):97 107, 2004. T. Lynch and E. \nSwartzlander. The redundant cell adder. In Computer Arithmetic, Proceedings, pages 165 170. IEEE Press, \n1991. T. Nipkow, L.C. Paulson, and M. Wenzel. Isabelle/HOL A Proof Assis\u00adtant for Higher-Order Logic, \nvolume 2283 of LNCS. Springer-Verlag, 2002. S.L. Peyton Jones, editor. Haskell 98 Language and Libraries: \nThe Revised Report. Cambridge University Press, 2003. J.C. Reynolds. Types, abstraction and parametric \npolymorphism. In Infor\u00admation Processing, Proceedings, pages 513 523. Elsevier Science Pub\u00adlishers B.V., \n1983. M. Sheeran. Finding regularity: Describing and analysing circuits that are not quite regular. \nIn Correct Hardware Design and Veri.cation Methods, Proceedings, volume 2860 of LNCS, pages 4 18. Springer-Verlag, \n2003. M. Sheeran. Hardware design and functional programming: a perfect match. Journal of Universal \nComputer Science, 11(7):1135 1158, 2005. M. Sheeran. Searching for pre.x networks to .t in a context \nusing a lazy functional programming language. Talk at Hardware Design and Functional Languages, 2007. \n J. Sklansky. Conditional-sum addition logic. IRE Transactions on Elec\u00adtronic Computers, EC-9(6):226 \n231, 1960.  H.S. Stone and P.M. Kogge. A parallel algorithm for the ef.cient solution of a general class \nof recurrence equations. IEEE Transactions on Com\u00adputers, 22(8):786 793, 1973. P. Wadler. Theorems for \nfree! In Functional Programming Languages and Computer Architecture, Proceedings, pages 347 359. ACM \nPress, 1989.  \n\t\t\t", "proc_id": "1328438", "abstract": "<p>This pearl develops a statement about parallel prefix computation in the spirit of Knuth's 0-1-Principle for oblivious sorting algorithms. It turns out that 0-1 is not quite enough here. The perfect hammer for the nails we are going to drive in is relational parametricity.</p>", "authors": [{"name": "Janis Voigtl&#228;nder", "author_profile_id": "81100011863", "affiliation": "Technische Universit&#228;t Dresden, 01062 Dresden, Germany", "person_id": "P394765", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1328438.1328445", "year": "2008", "article_id": "1328445", "conference": "POPL", "title": "Much ado about two (pearl): a pearl on parallel prefix computation", "url": "http://dl.acm.org/citation.cfm?id=1328445"}