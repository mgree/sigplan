{"article_publication_date": "01-07-2008", "fulltext": "\n Cryptographically Sound Implementations for Typed Information-Flow Security C\u00b4edric Fournet Tamara \nRezk Microsoft Research MSR INRIA Joint Centre MSR-INRIA Joint Centre Tamara.Rezk@inria.fr fournet@microsoft.com \nAbstract In language-based security, con.dentiality and integrity policies conveniently specify the permitted \n.ows of information between different parts of a program with diverse levels of trust. These policies \nenable a simple treatment of security, and they can often be veri.ed by typing. However, their enforcement \nin concrete systems involves delicate compilation issues. We consider cryptographic enforcement mechanisms \nfor imper\u00adative programs with untrusted components. Such programs may represent, for instance, distributed \nsystems connected by some untrusted network. In source programs, security depends on an abstract access-control \npolicy for reading and writing the shared memory. In their implementations, shared memory is unprotected \nand security depends instead on encryption and signing. We build a translation from well-typed source \nprograms and policies to cryptographic implementations. To establish its correct\u00adness, we develop a type \nsystem for the target language. Our typing rules enforce a correct usage of cryptographic primitives \nagainst ac\u00adtive adversaries; from an information-.ow viewpoint, they capture controlled forms of robust \ndeclassi.cation and endorsement. We show type soundness for a variant of the non-interference property, \nthen show that our translation preserves typability. We rely on concrete primitives and hypotheses for \ncryptogra\u00adphy, stated in terms of probabilistic polynomial-time algorithms and games. We model these \nprimitives as commands in our tar\u00adget language. Thus, we develop a uniform language-based model of security, \nranging from computational non-interference for prob\u00adabilistic programs down to standard cryptographic \nhypotheses. Categories and Subject Descriptors D.2.0 [Software Engineer\u00ading]: Protection Mechanisms; \nF.3.1 [Specifying and Verifying and Reasoning about Programs]: Speci.cation techniques. General Terms \nSecurity, Veri.cation, Design, Languages. Keywords Secure information .ow, con.dentiality, integrity, \nnon\u00adinterference, type systems, compilers, probabilistic programs, cryp\u00adtography, computational model. \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL \n08, January 7 12, 2008, San Francisco, California, USA. Copyright c &#38;#169; 2008 ACM 978-1-59593-689-9/08/0001. \n. . $5.00 1. Introduction One of the open challenges in security is to reliably protect pro\u00adgram implementations \nby compilation (Abadi 1998). To this end, one needs languages that let the programmer specify security \nre\u00adquirements and reason about them using simple abstractions, as well as tools that can produce code \nto enforce these requirements. In particular, when considering the integrity and con.dential\u00adity of information, \nthe veri.cation of distributed programs entan\u00adgles different aspects of system implementations, ranging \nfrom application-level information-.ow control down to cryptographic algorithms and communication protocols \n(themselves depending on adequate integrity and con.dentiality control for their crypto\u00adgraphic keys). \nOur thesis is that the cryptographic aspects should be left to the compiler. In language-based information-.ow \nsecurity, con.dentiality and integrity policies are speci.ed using security labels, equipped with a partial \norder that describes permitted .ows of information (Denning 1976; Zdancewic and Myers 2001). Security \nlabels as\u00adsociated to program variables specify who can read from (con.\u00addentiality) and who can write \nto (integrity) a given variable. The preservation of con.dentiality and integrity policies is expressed \nas non-interference properties, guaranteeing that the knowledge of an attacker with limited access to \nvariables is not augmented by any program execution. We consider cryptographic enforcement mechanisms \nfor con.\u00addentiality and integrity in imperative programs. Our security model accounts for active adversaries, \nrepresented as untrusted (or un\u00adknown) parts of the program that may change unprotected mem\u00adory during \nexecution. The resulting programs may represent, for instance, distributed systems connected by some \nuntrusted net\u00adwork, or untrusted machines containing protected subsystems. Ac\u00adcording to the program \nsemantics, security depends on an abstract read/write policy for accessing shared memory. In their crypto\u00adgraphic \nimplementation, shared memory is unprotected, and secu\u00adrity depends instead on encryption and signing \nwhen accessing the shared memory. A .rst basic example Consider two parties a and b that wish to perform \nsome computation securely by exchanging a series of messages over some untrusted network. Using shared \nmemory, we may write for instance ;(x := 1)a ; ;(if x then y := 2 else y := z)b ; ;(y := y +1)a ; where \nthe parentheses ()a and ()b indicate code that runs on behalf of a and b, respectively, and where the \nplaceholders stand for any untrusted code that may run in-between. Assuming that untrusted code does \nnot access x, y, and z, we would expect for instance that z remains secret and y =3 at the end of any \nrun. To this end, a needs to securely pass x to b, then b needs to securely pass y to a. In a less abstract \nsetting, such .ows of information between variables may involve communication over untrusted channels, \ne.g. shared memory that may be read and modi.ed by active adver\u00adsaries, with some adequate encryption \nand signing. For example, assuming the variables xe, xs and ye, ys are used to pass the en\u00adcrypted values \nand signatures for x and y, respectively, the second command (if x then y := 2 else y := z)b may be implemented \nas if V(xe,xs,kv) then ( x b := D(xe,kd); bb bb if x then y := 2 else y := z ; ye := E(y b,ke); ys := \nS(ye,ks)) where, in order to read x out of its wire format xe, xs, the code .rst veri.es (V) the signature \nthen performs a decryption (D) to extract a local copy of x into x b; and, conversely, for writing y, \nthe code .rst encrypts (E) its updated copy y b and then signs (S) the encrypted value. This implementation \ncode does not rely on the con.dentiality or integrity of the shared variables used on the wire (variables \nxe, xs, ye, ys). Instead, it relies on the adequate generation and man\u00adagement of the keys used for verifying, \ndecrypting, encrypting, and signing, as well as security assumptions on the cryptographic prim\u00aditives. \nIf we rely on a public-key signature scheme, for instance, the integrity of the veri.cation key kv must \nbe higher than the integrity of x, while the con.dentiality of the signing key ks must be high enough \nto protect the integrity of y. Also, if the keys are used for other purposes (and we can hardly dedicate \n4 keys to every vari\u00adable), we need to carefully control their interaction. For instance, in the code \nabove, we cannot use the same keys for protecting x and y, as an adversary may then achieve y =2 at the \nend of the computation by inserting the code ye := xe; ys := xs between b and a. Besides, we cannot assume \nthat the computation always completes successfully, as indeed an adversary may insert ys := 0 before \nb s code and thus cause the signature veri.cation to fail, so we also need to qualify our notion of integrity \nfor this computation. Symbolic versus computational cryptography In contrast with most language-based \napproaches, we do not rely on symbolic black box cryptography. Despite considerable successes for pro\u00adtocol \nveri.cation, symbolic cryptography may be dangerously ab\u00adstract for protocol design, especially as regards \nindistinguishability properties and information-.ow. For example, cryptographic algo\u00adrithms do not actually \nguarantee the con.dentiality of their keys as values only that not enough information is leaked to effectively \nrecover an encrypted payload or fake a signature (see e.g. Abadi and Rogaway 2002). Hence, under standard \nassumptions, an adver\u00adsary may learn which keys are used by traf.c analysis, potentially opening a side \nchannel if key selection depends on a secret guard. Instead of symbolic cryptography, we use a concrete \nmodel with probabilistic polynomially-bounded algorithms on bitstrings, and standard security hypotheses \n(IND-CCA2, IND-CMA). Thus, we gain a more precise and realistic information-.ow result: we guar\u00adantee \nthat the probability that any given polynomial-time adversary illegally obtains (or in.uences) information \nbecomes negligible as the keys get long enough. Our contributions 1. Starting from an imperative language \nwith information-.ow policies for both con.dentiality and integrity, we adapt a simple type system for \nnon-interference to accommodate a fail-stop semantics for runtime checks in the presence of active attackers. \n 2. We develop a target language for implementations that rely on cryptography, with a probabilistic \nsemantics. We use it to con\u00adveniently express cryptographic algorithms, active adversaries,  and oracles \nas well as our implementation code in a precise set\u00adting. We can thus recast standard cryptographic assumptions \nas properties on probabilistic programs. In order to reconcile this style of properties with classic \ninformation-.ow properties in a uniform framework, we also reformulate non-interference more syntactically, \nas a game coded in our language with ex\u00adplicit commands for programs, active adversaries, and tests. \n3. We equip this probabilistic language with a type system for checking the usage of cryptography. From \nan information view\u00adpoint, we capture controlled forms of declassi.cation (down\u00adgrading of con.dentiality \nlevels of variables) after encryption and endorsement (upgrading of integrity levels of variables) after \nsignature veri.cation. We regard our type system as a tool for structuring cryptographic proofs. Indeed, \nthe game\u00adrewriting arguments in the main proofs are expressed as typed program transformations. To our \nknowledge, this is the .rst computationally sound type system for cryptographic informa\u00adtion .ow that \ncan handle active adversaries. 4. We give a typed translation from the simple language to the tar\u00adget \nlanguage. We show that, if a source program is typable, then its translation is also typable, hence it \nhas the property of com\u00adputational non-interference against probabilistic polynomial\u00adtime active adversaries. \nTo our knowledge, this is the .rst cryp\u00adtographic translation for general information-.ow security. \n Limitations Our results apply to a large class of protocols and program translations, but they still \nhave important limitations from a general programming viewpoint. For example, we do not model concurrency \nto avoid the complications of non-determinism in computational models (see e.g. Ad ao and Fournet 2006). \nAlso, as in prior types for computational cryptography, we separately keep track of every key, which \nexcludes key generations within loops. Our theorems rely on global conditions: that programs are poly\u00adnomial, \nand that some variables are initialized before being read, and assigned only in speci.c parts of the \ncode. We do not enforce these conditions by typing; they can be checked independently, and sometimes \nachieved by preliminary program transformations. Related work Laud (2001) pioneers work on information \n.ow re\u00adlying on concrete cryptographic assumptions. He introduces com\u00adputational non-interference for \nencryption in a model with pas\u00adsive adversaries. Our de.nitions generalize this property to the ac\u00adtive \ncase, with adversaries that may interfere with the normal ex\u00adecution of programs, and also covers integrity \nproperties. Backes (2005) relates negligible information .ows to computational non\u00adinterference and shows \nthat this property is preserved under simu\u00adlatability in reactive systems. Laud and Vene (2005) propose \na type system to verify computa\u00adtional non-interference against passive adversaries in an imperative \nlanguage with symmetric encryption and dynamic key generation. Their types are more precise than ours \nfor tracking key dependen\u00adcies, but their system does not enable to type decrypted values as keys (cf. \nExample 8). In a similar line of work, Smith and Alp\u00b4izar (2006) present a type system with encryption \nand decryption but no explicit keys; they assume a single implicit key-pair for the whole program, which \nleads to a clean type-system presentation. Using symbolic cryptography, Askarov et al. (2006) generalize \nnon-interference to allow .ows that arise from encryption. In their model, a type system enforces secure \ncryptographically masked .ows for a non-deterministic semantics of encryption. Recently, Laud (2008) \ninvestigates conditions such that cryptographically masked .ows imply security in the computational model. \nIntegrity and active adversaries Several works consider the inter\u00adaction between con.dentiality and integrity \npolicies in the presence of active adversaries. (This interaction plays a central role in our handling \nof cryptographic keys.) Zdancewic and Myers (2001) and Myers et al. (2006) propose general de.nitions \nfor non-interference with active adversaries. Their de.nitions are close to ours, but do not consider \ncryptography. Secure implementations for information .ow Jif/Split (Zdancewic et al. 2002; Zheng et al. \n2003) is a compiler that implements secure distributed systems on mutually untrusted hosts from sequential \nprograms annotated with information .ow types. The implemen\u00adtation assumes that all communications are \nprivate. Our work can be seen as an attempt to implement (and verify) a cryptographic back-end for Jif/Split. \nIn principle, many of their techniques could be applied (for instance, as type-preserving transformations) \nbefore applying our translation, to deal with other aspects of distribution such as global control .ow. \nVaughan and Zdancewic (2007) design a typed language with high-level dynamic pack and unpack security \nprimitives, and describe their implementation using authenticated encryption. Their language also uses \nlabels that combine con.dentiality and integrity, expressed in the decentralized label model (Myers and \nLiskov 2000). The correctness of their implementation is proved for a symbolic model of cryptography \nwith passive attackers (who can observe encrypted memory after execution of the program). Cryptographic \nimplementations for programming languages We mention related work only within the computational model \nof cryp\u00adtography. Backes et al. (2003) provide a sound execution frame\u00adwork for protocols that use an \nidealized cryptographic library. Laud (2005) designs a typed cryptographic language and implements it \non top of their framework. His type system also keeps track of key con.dentiality and integrity, in order \nto meet the hypotheses of Backes et al. (2003) and guarantee payload secrecy, which can be interpreted \nas a form of computational non-interference. Abadi et al. (2006) rely on Laud s language to compile security-typed \nvariants of the pi calculus and thereby obtain payload secrecy (but no integrity) for their distributed \nimplementations. Ad ao and Fournet (2006) design a process calculus with ab\u00adstractions for secure communications \n(but no explicit cryptogra\u00adphy) and establish the computational soundness of its implementa\u00adtion for \ntrace and equivalence properties. Contents Section 2 de.nes our source language, policies, security properties, \nand type systems. Section 3 de.nes our target language and security properties computational non-interference \nand ex\u00adplains our cryptographic assumptions. Section 4 presents our cryp\u00adtographic type system. Section \n5 presents our type-safe translation. Section 6 concludes and discusses future work. Additional details \nappear in a companion paper at http:// www.msr-inria.inria.fr/projects/sec/cflow. 2. Security policies \nand non-interference We present a simple imperative while language and equip it with security policies. \nWe recall standard notions of non-interference for con.dentiality and integrity, and a simple type system \nfor checking this property. In preparation for our cryptographic implementation, we then extend these \nnotions to active adversaries. 2.1 A simple imperative while language Our source programs consist of \nexpressions and commands, with the following grammar: e ::= x | v | op(e1,...,en) P ::= x := e | P ; \nP | if e then P else P | while e do P | skip where x ranges over variables, v ranges over literal values \n(for now bitstrings, which may represent integers, strings, booleans), and op ranges over n-ary functions \n(such as arithmetic, string, and boolean operations). For completeness, we assume that these functions \ninclude all standard bitwise operations on values. We write if e then P1 else P2; P3 for (if e then P1 \nelse P2); P3 and write if e then P for if e then P else skip. We let rv(P ) and wv(P ) be the sets of \nvariables that are syntactically read and written by P : x . rv(P ) when x occurs in an expression of \nP ; and x . wv(P ) when a command of the form x := e occurs in P . We let v(P )= rv(P ) . wv(P ). We \nlet \u00b5 range over memories, that is, .nite functions from variables to values plus a special term .. We \ndenote by \u00b5(x)= . that variable x is not initialized in memory \u00b5. We assume that . does not occur in \ncommands. We write \u00b5{x . v} for the memory that maps x to v and any y x to \u00b5(y). For a given memory = \ndomain, we let \u00b5. be the memory that maps every variable to .. We use a standard semantics for programs \n(formally de.ned as a special case in Section 3). Con.gurations range over pairs of a command and a memory, \nwritten (P, \u00b5), plus inert con.gurations, v written (,\u00b5) or just \u00b5, that represent command termination \nwith .nal memory \u00b5. We implicitly assume that all the variables of P are in the domain of \u00b5. We let r \nrepresent single-step execution of commands between con.gurations and let r * be its re.exive ' transitive \nclosure. We denote normal termination as (P, \u00b5). \u00b5, that is, starting with initial memory \u00b5, command \nP completes with .nal memory \u00b5' in any number of steps. 2.2 Non-interference against passive adversaries \n(review) We annotate variables, expressions, and commands with security labels. These labels specify \nthe programmer s security intent, but they do not affect the operational semantics. The labels form a \nlattice (L, =), obtained as the product of two lattices of con.dentiality levels (LC , =C ) and integrity \nlevels (LI , =I ). We write .L and TL for the smallest and largest ele\u00adments of L, and U for the least \nupper bound of two elements of L. For a given label e =(eC ,eI ) of L, the con.dentiality level eC speci.es \na read level for variables, while the integrity level eI spec\u00adi.es a write level; the meaning of e = \ne' is that e' is more con\u00ad.dential (can be read by fewer commands) and less integral (can be written \nby more commands) than e. We let C(e)= eC and I(e)= eI be the projections that yield the con.dentiality \nand in\u00adtegrity parts of a label. Hence, the partial order on L is de.ned as e = e' iff C(e) =C C(e') \nand I(e) =I I(e'). We refer to the de\u00adcentralized label model of Myers and Liskov (2000) and Vaughan \nand Zdancewic (2007) for a concrete syntax for setting such lat\u00adtices. We represent our security policies \nas functions G from variables to security types t of the form t(e), where t (for now) can only be instantiated \nwith data type Data and where e is a security label. Overloading our notations, we lift our con.dentiality \nand integrity projections C and I from labels to security types, and let C(t(e)) = C(e) (resp. I(t(e)) \n= I(e)). We also let T (t(e)) = t. We now proceed to de.ne non-interference for con.dentiality and integrity, \nrelative to a given policy. DEFINITION 1 (Memory indistinguishability). Let V be a set of variables. \nThe memories \u00b50 and \u00b51 are indistinguishable on V , written \u00b50 ~V \u00b51, when x . V implies \u00b50(x)= \u00b51(x). \nDEFINITION 2 (Non-interference on V ). The command P is non\u00adinterferent on V when, for all memories \u00b50 \nand \u00b51, if \u00b50 ~V \u00b51 ''' and (P, \u00b5b). \u00b5b for b =0, 1, then \u00b50 ~V \u00b51. Intuitively, as long as it terminates, \na non-interferent command does not leak any information from the hidden part of its initial memory (outside \nV ) to the visible part of its .nal memory (in\u00adside V ). (In this work, we consider only the termination-insensitive \nvariant of non-interference. Indeed, in our re.ned cryptographic model, we are going to demand that all \ncommands always termi\u00adnate in polynomial time, thereby excluding any termination leak.) Standard de.nitions \nof information-.ow security against pas\u00adsive adversaries are obtained from De.nition 2 by letting the \nset of observed variables V collect either the low-con.dentiality vari\u00adables or the high-integrity variables: \nDEFINITION 3 (Non-interference at a, passive case). Let G be a memory policy and a .L a security label. \nLet VaC = {x | C(G(x)) =C C(a)} VaI = {x | I(G(x)) =I I(a)} The command P preserves con.dentiality at \na when it is non\u00adinterferent on VaC ; it preserves integrity at a when it is non\u00adinterferent on VaI . \n 2.3 Active adversaries and runtime errors In terms of attacker model, memory indistinguishability accounts \nfor adversaries that may partially observe the outcome of the com\u00adputation but do not interfere with \nit. More generally, we are inter\u00adested in non-interference for programs that may include commands representing \nactive adversaries. Our approach largely follows ro\u00adbust declassi.cation (Myers et al. 2006). For a given \na .L, an a-adversary is a command, say A, that reads variables with con.dentiality level less than or \nequal to C(a) and writes variables with integrity level not less than or equal to I(a): rv(A) . VaC wv(A) \nn VaI = \u00d8 In the rest of the paper, we let a denote the security level of the adversary. As usual, once \na is .xed, we may restrict our attention to the product of binary lattices L =C H for con.dentiality \nand H =I L for integrity, with just four labels HL, HH, LL, and LH, and set a = LH. We use this 4-point \nlattice in examples. On the other hand, we intend to develop implementations that do not a priori depend \non a .xed a. We consider programs obtained by composing commands with diverse levels of trust, including \ne.g. arbitrary a-adversaries as well as .xed, trusted commands. To this end, we write P [] for a command \ncontext (with a grammar obtained from that of P by adding a hole ) and P [P ' ] for the command obtained \nby replacing each occurrence of with P '. We also use n-ary command contexts, ' with n distinct holes, \nand write P [PA] for the command obtained ' by instantiating these holes with the vector of commands \nPA. Although our language does not feature procedure calls, we can use command contexts to model arbitrary \ncommands with access to .xed, privileged procedures, sometimes called oracles in cryp\u00adtography, using \n.xed variables for passing their input and output parameters. For example, the command P0;[A[P ' ]] represents \na command that .rst runs initialization code P0 then runs A, which in turn may invoke command P ' any \nnumber of times. Handling runtime errors The integrity of a run may clearly be affected by an active \nadversary that can write into low-integrity variables. Consider for instance the command context P [ \n,Q]= \u00b7 l :=4; ; if l =4 then h := 10 else Q where the hole stands for low-level code. After running command \nP [skip,h := 5] we have h = 10, but this is not the case with command P [l := 0,h := 5], as there is \nan implicit .ow from l to h. Hence, if h has high integrity and l has not, the command P [l := 0,h := \n5], and even P [skip, skip], are typically rejected by type systems for non-interference. This approach \nis too restrictive in our case, as we expect the ad\u00adversary to be able to modify signed or encrypted \nvalues, as long as our cryptographic implementation can catch the attack as a runtime error typically, \nany signature veri.cation is a low-integrity guard. Accordingly, we relax our security de.nitions to \naccept command contexts such as P [ , skip], even if l is less integral than h, on the following ground: \nif h is ever assigned, its value will be 10, so its integrity is preserved. In the following, we interpret \nany read of an uninitialized vari\u00adable as a runtime error. In the example above, a run of com\u00admand P \n[l := 0, skip] (starting with memory l ..,h ..) leaves h uninitialized. Further, we assume that programs \nnever read variables that are uninitialized and not writable by the adversary (I(x) =I I(a)). This property \ncan be enforced independently, for instance by relying on static analyses (as discussed by Laud and Vene \n2005). We relax our notion of indistinguishability (De.nition 1) to disregard the observation of uninitialized \nvariables: DEFINITION 4 (Weak memory indistinguishability). Let V be a set of variables. The memories \n\u00b50 and \u00b51 are weakly indistin\u00adguishable on V , written \u00b50 ~. \u00b51, when x . V implies either V \u00b50(x)= \u00b51(x), \nor \u00b50(x)= ., or \u00b51(x)= .. We also adapt non-interference (De.nition 2) to account for weak indistinguishability. \nTo this end, we further distinguish a set U of variables that must be left uninitialized in initial memories. \nWe intend this set to gather high-integrity variables exclusively written by P . DEFINITION 5 (Weak non-interference \non V, U). Let V and U be two sets of variables. The command P is weakly non-interferent on V, U when, \nfor all memories \u00b50 and \u00b51, if 1. \u00b50(x)= \u00b51(x)= . for every x . U, 2. \u00b50 ~. \u00b51, and 3. (P, \u00b5b). \u00b5b \n' for b =0, 1,  V 0 ~.then \u00b5 ' V \u00b51' . DEFINITION 6 (Weak non-interference at a). Let G be a memory \npolicy and a .L a security label. The command P is weakly non\u00adinterferent at a when P is weakly non-interferent \non both VaC , \u00d8 and VaI ,VaI n wv(P ). Intuitively, the security property for active adversaries re.ects \nthat, even if an adversary may prevent the normal completion of a command (leaving more uninitialized \nvariables in the .nal mem\u00adories), he will neither learn more than by eavesdropping complete runs of the \ncommand, nor be able to affect the .nal value of de\u00ad.ned high-integrity variables. In preparation for \ncryptographic re\u00ad.nements, we express this security property for command contexts P with holes for active \nadversaries in a style close to the one used for cryptographic games (see e.g. Bellare and Rogaway 2004) \nwith subcommands that explicitly code initial memories, adversaries, and observations. DEFINITION 7 (Non-interference \nagainst active adversaries). Let G be a memory policy and a .L a security label. The command context \nP is non-interferent against a-adversaries when, for both V,U = VaC , \u00d8 and V,U = VaI ,VaI n wv(P ), \nand for all commands J writing V \\ U: wv(J) . V \\ U;  Bb for b =0, 1 writing outside V : wv(Bb) n V \n= \u00d8; A a-adversaries; T reading V , writing g: rv(T ) . V ; g/. wv(J, B0,B1,A );  the value of g after \nrunning J; Bb; P [A ]; T does not depend on b: if g '' '' (J; Bb; P [A ],\u00b5.). \u00b5\u00b5b(x)= ., and (T,\u00b5 ' b). \n\u00b5 b, x.rv(T ) b '' '' for b =0, 1 then we have \u00b50 (g)= \u00b51 (g). The command Gb = J; Bb; P [AA]; T represents \na game, pa\u00adrameterized by a meta-variable b that is either 0 or 1; its .rst part J; Bb initializes variables \nand, depending on b, yields two indistin\u00adguishable memories (\u00b50 and \u00b51 in De.nition 2); the second part \nP [AA] runs the command context P in combination with active ad\u00ad versaries AA; this yields two memories \n(\u00b50 ' and \u00b51 ' in De.nition 2); the .nal part of the command, T , represents an observer that at\u00adtempts \nto guess the value of b from memory \u00b5 ' b. Intuitively, an op\u00adponent player that chooses the commands \nJ, B0, B1, AA, and T wins when g =0 after running G0 and g =1 after running G1. EXAMPLE 1 (Non-interference \nfor a simple lattice). Let L be the 4-point lattice with labels HL, HH, LL, and LH. Let a = LH. Let P \nbe a command context that writes wv(P )= {xHH,xHL, xLL,x LH' } and reads rv(P )= wv(P ) .{xLH}, with \na single hole. To show that P is non-interferent against a-adversaries, we check that g does not depend \non the choice between B0 and B1 for any commands that operate on the variables of P as follows: for \nthe con.dentiality game: J sets xLH, xLL, and x '  LH; Bb set xHL and xHH depending on xLH, xLL, and \nxLH' ; A sets xHL and xLL depending on xLL, xLH, and xLH' ; T sets g depending on xLH, xLH' , and xLL; \n  for the integrity game: J sets xLH;  Bb set xHL and xLL depending on xLH; A sets xHL and xLL depending \non xLL, xLH, and x ' LH; T sets g depending on xLH and, only when they are initialized, on xLH ' and \nxHH. EXAMPLE 2. According to De.nition 7, the command context yLH := xHH; yLH := 0 is secure, but yLH \n:= xHH;; yLH := 0 (with an intermediate adversary) is not secure, since for instance J = \u00b7 skip A = \u00b7 \nzLL := yLH Bb = \u00b7 xHH := bT = \u00b7 g := zLL break the con.dentiality game, leaving g = b. The following \nlemma relates non-interference for passive and active adversaries, in case the command P has no holes: \nLEMMA 1. A command P is weakly non-interferent at a if and only if P (as a command context P with no \n) is non-interferent against active adversaries. In the general case, when P has at least one hole, non-interferen\u00adce \nagainst active adversaries implies that P [ A skip] is weakly non\u00adinterferent, but the converse may not \nhold, as can be seen on Ex\u00adamples 2 (due to an explicit con.dentiality .ow) and 3 (due to an implicit \nintegrity .ow): EXAMPLE 3. For the lattice of Example 1, consider the context P = xLL :=0; ; if xLL =4 \nthen yLH := 1 else yLH := 0 P [skip] is weakly non-interferent, but P [] is interferent against active \nadversaries for the integrity game, since we can de.ne J = \u00b7 skip A = \u00b7 if zLL =1 then xLL := 4 Bb = \n\u00b7 zLL := bT = \u00b7 g := yLH  2.4 A simple type system for non-interference Type systems for controlling \ninformation .ows have been widely studied in the literature (see Sabelfeld and Myers 2003 for a survey). \nWe recall a standard type system for establishing non\u00adinterference (De.nition 2) and then adapt it for \nestablishing non\u00adinterference against active adversaries (De.nition 7). The resulting VAR f x : G(x) \nVAL f v : Data (.L) OP f ei : Data (e) for i = 1..n SUBE f e : t t = t ' ' f op(e1,...,en): Data (e) \nf e : t Figure 1. Typing rules for source expressions with policy G. ASSIGN SEQ f e : G(x) f P : e f \nP ' : e f x := e : L(x) f P ; P ' : e COND ' SKIP f e : Data (e) f P : e f P : e f skip : TL f if e then \nP else P ' : e WHILE SUBC f e : Data (e) f P : e f P : ee ' = e f while e do P : e f P : e ' Figure 2. \nTyping rules for source commands with policy G. CHECK '' HOLE f e : Data (e ) C(e ) =C C(e) f P : e f \n: TL f if e then P : e Figure 3. Additional typing rules for source command contexts. type system is \nfurther extended in Section 4 to account for crypto\u00adgraphic primitives. Recall that security types are \nof the form t(e), where t is just Data for now and e .L is a security label. We use L as the projec\u00adtion \nfrom types to security labels, that is L(t(e)) = e. For brevity, in the typing rules we sometimes abbreviate \ntype parameters G(x) to x. We lift the security preorder = from labels to types, and let ''' ' t(e) = \nt (e ) iff t = t and e = e . For a given security policy G, typing judgments for expressions, written \nG f e : t(e), mean that e reads variables of level at most e. Typing judgments for commands, written \nG f P : e, mean that P is secure and writes variables of level at least e. In the following, we often \nomit the .xed policy G in typing judgments. The typing rules appear in Figures 1 and 2, respectively. \nWe also write G f P to denote typability of P with policy G, that is, there exists e such that f P : \ne. The theorem below states that this simple type system is sound with regard to non-interference (De.nition \n2); its proof is a simple induction on the number of reduction steps. THEOREM 2. Let G be a security \npolicy and a .L a security label. If G f P , then P is non-interferent at a. We extend our type system \nto account for active adversaries. We type command contexts with placeholders that stand for a\u00adadversaries \nby supplementing the rules of Figure 2 with those given in Figure 3. (The rules implicitly apply for \nsome .xed policy G.) Rule HOLE types placeholders for a-adversaries with TL. Al\u00adthough the rule does \nnot enforce any restriction, De.nition 7 allows only a-adversaries to be placed in holes. Rule CHECK \nallows some implicit integrity .ows from condi\u00adtional expressions to the then branch P when there is \nno else branch. The rule usefully applies to error handling, as discussed in Section 2.3, when the adversary \ncontrols the conditional execution of P ; it is sound only together with an additional property, given \nbelow, that restricts the high-integrity variables that P may write. This global, syntactic property \ndemands that any variable x . V be written by at most one of a series of subcommands, thereby guar\u00adanteeing \nthat x = . if this subcommand is not executed. DEFINITION 8 (Exclusive assignments). Let V be a set of \nvari\u00adables. Let P1, ..., Pn be subcommands of P , that is, P = Qi[Pi] for some command context Qi for \ni =1..n. P1,. . . , Pn exclusively assign V in P when the hole of Qi is not within any while loop and \nV nwv(Pi)nwv(Qi)= \u00d8 for i =1..n. The next theorem states the soundness of the extended type system for \ncommand contexts with holes representing adversaries. THEOREM 3. Let G be a policy and a .L a security \nlabel. Assume G f P and all commands P ' that occur in commands if e then P ' typed by CHECK exclusively \nassign VaI in P . Then P is non-interferent against active a-adversaries. 3. Target cryptographic language \nand computational non-interference Next, we add probabilistic primitives, de.ne our target security property \nas a re.nement of De.nition 7, and specify cryptographic primitives and hypotheses. 3.1 A probabilistic \nlanguage The target language extends our imperative language with proba\u00adbilistic functions, ranged over \nby f. P ::= ... | x1,...,xm := f(y1,...,yn) For simplicity, these probabilistic functions may occur only \nat top level in commands (so that expressions remain deterministic). We let rv(Ax := f(Ay)) = {yA} and \nwv(Ax := f(Ay)) = {Ax}. Every function f is equipped with an associated parametric probability distribution \n[ f] . (Hence, in the special case f is a deterministic function, the distribution [ f]](Av) gives probability \n1 to f(Av) and 0 to any other output.) We write {0, 1} for the coin\u00adtossing function that returns either \n0 or 1 with probability 12 . Instead of single con.gurations (P, \u00b5), we now consider dis\u00adtributions of \ncon.gurations, ranged over by d. The operational semantics for commands is given in Figure 4, as a Markov \nchain (Hermanns 2002) on the set of all con.gurations, written S, with probabilistic steps induced by \nthe function distributions. (Formally, rule STABLE guarantees that all states have leaving transitions \nwhose probabilities sum to 1.) We omit the usual semantics for expressions [ e]](\u00b5). An initial distribution \nfor P is a distribution of con.gurations that has all its weight on con.gurations of the form (P, \u00b5). \nHence, we can lift initial memories to distributions and de.ne the transition system as a transition \nsystem from input distributions to output distributions (see e.g. Monniaux 2001). DEFINITION 9. Let prob \n: S\u00d7S . [0, 1] such that s rprob(s,s') s ' in Figure 4 and T be the distribution transformer such that \nm ' T (d)(s ' )= s.S prob(s, s )d(s) The semantics of a probabilistic program is given by a sequence \nof distribution transformations, starting from an initial distribu\u00adtion d0. We let di r di+1 when di+1 \n= T (di) and let r * be the transitive clausure of r. The probability that a program P ter\u00adminates after \nn steps starting with initial distribution d0 is pn = m v s=( ,\u00b5) dn(s). We let Pr[P ; .] be the probability \nthat, starting from a given ini\u00adtial distribution d0, command P completes with a .nal memory that m meets \ncondition ., that is, Pr[P ; .] = limn=0 s=(v ,\u00b5)|. dn(s). (The limit exists as the sum increases with \nn and is bounded by 1.) 3.2 Polynomial-time assumptions Our commands capture exactly the algorithms \nthat can be coded on probabilistic Turing machines, using shared memory as input and output tapes. Further, \npolynomial runs of commands correspond to polynomial runs of these machines. Thus, we can recast standard \ncryptographic assumptions and games in the formal setting of this language, quantifying for instance \nover all polynomial-time com\u00admands to represent all polynomial adversaries. In the following de.nitions, \nwe assume that the initial distribu\u00adtion for a given command P is d. 0 , which gives all its weight to \nthe uninitialized state (P, \u00b5.). We assume that all algorithms are probabilistic and computable in time \nbounded by some polynomials in ., the security parameter. Intuitively, . represents the lengths of the \nkeys. In order to avoid passing . explicitly, we assume that x. is a read-only variable initialized with \nthe security parameter. We assume that all primitive operations are polynomial in their parameters and \nthat the distribution for all our probabilistic primi\u00adtive functions are polynomial-time samplable (so \nthat any polyno\u00admial program that calls these primitives could also be written as a polynomial program \nthat includes its own implementation of these primitives as subcommands). In the rest of the paper, rather \nthan distributions, we consider families of distributions parameterized by . (written d(.)), also known \nas ensembles. Thus, for a .xed domain of variables, the initial distribution d. 00 (.) becomes the family \nof distributions d. where all the weight is given to (P, \u00b5.{x. . .}). We overload Pr[P ; .] to denote \nthe probability function parameterized by .. Security properties are often expressed in terms of games, \ncoded as commands that sample a secret boolean b := {0, 1} then interact with adversary commands. The \ngoal of the adversary commands is to write into some variable g its guess as to the value of b: the adversary \nwins when b = g. The trivial adversary g := {0, 1}wins with probability 12 , so we are interested in \nthe advantage of an adversary, de.ned as the probability that b = g minus 12 . For any given ., the adversary \nmay guess any secret vari\u00adables with a non-zero probability, including variables that store cryptographic \nkeys. Thus, in contrast with our de.nitions of non\u00adinterference so far, we cannot expect the advantage \nto be 0 as we start relying on cryptography. Rather, we expect this advantage to be a negligible function \nof .. We recall the de.nition of negligible functions: DEFINITION 10 (Negligible function). A function \nf : N . R is negligible when, for all c> 0 there exists nc such that, for all n = nc, we have f(n) = \nn -c .  3.3 Computational non-interference We re.ne our notions of non-interference to account for probabil\u00adities, \nand in particular for the possibility that some information is leaked (or corrupted) with a negligible \nprobability. Instead of running two commands for b =0, 1, we run a single probabilis\u00adtic command that \n.rst picks b uniformally at random. We begin with a probabilistic, code-based variant of non-interference \nagainst passive adversaries (De.nition 2) similar to the one introduced by Laud (2001). DEFINITION 11 \n(Computational non-interference on V , U). The polynomial command P is computationally non-interferent \non V , U when for all polynomial commands J writing V \\ U: wv(J) . V \\ U;  Bb for b =0, 1 writing outside \nV : wv(Bb) n (V . U)= \u00d8;  T reading V , writing g: rv(T ) . V ; g/. wv(J, B0,B1, AA);  ASSIGNS SEQS \nv [ e]](\u00b5)= v (P, \u00b5) rp (P1,\u00b51) P1 = v (x := e, \u00b5) r1 ( ,\u00b5{x . v}) (P ; P ' ,\u00b5) rp (P1; P ' ,\u00b51) CONDTRUE \nSTABLE vv [ e]](\u00b5)= true ( ,\u00b5) r1 ( ,\u00b5) (if e then P else P ' ,\u00b5) r1 (P, \u00b5) WHILETRUE WHILEFALSE [ \ne]](\u00b5)= true [ e]](\u00b5)= true v (while e do P, \u00b5) r1 (P ; while e do P, \u00b5)(while e do P, \u00b5) r1 ( ,\u00b5) SEQT \nv SKIPS (P, \u00b5) rp ( ,\u00b51) v (skip,\u00b5) r1 ( ,\u00b5) (P ; P ' ,\u00b5) rp (P ' ,\u00b51) CONDFALSE [ e]](\u00b5)= true (if e \nthen P else P ' ,\u00b5) r1 (P ' ,\u00b5) FUN p =[ f]](\u00b5(y1),...,\u00b5(yn))(Av) p> 0 v (Ax := f(y1,...,yn),\u00b5) rp ( \n,\u00b5{Ax . Av}) Figure 4. Probabilistic operational semantics and some variable b/. v(J, B0,B1,T ) in the \ncommand 3.4 Encryption CNI = \u00b7 b := {0, 1}; J; if b =0 then B0 else B1; P the advantage |Pr[CNI; T ; \nb = g] - 1 | is negligible. 2 In the game of the de.nition, J; if b =0 then B0 else B1 prob\u00adabilistically \ninitialize variables. Then P runs. Finally, T attempts to guess the value of b and sets g accordingly. \nHence, the property states that the two memory distributions for b =0 and b =1 af\u00adter running P cannot \nbe separated by an adversary that reads V . Semantically, this property can also be stated as indistinguisha\u00adbility \n(Mao 2003) of the ensembles db for b =0, 1 de.ned by (J; Bb; P, d0(.)). db(.) for the same range of commands \nfor J and Bb. In case the program P is deterministic, this property is equivalent to non-interference \n(De.nition 2): the adversary T guesses b correctly with probability 12 . We .nally generalize the property \nto account for active adver\u00adsaries, as in De.nition 7. DEFINITION 12. (Computational non-interference \nagainst active adversaries). Let P be a polynomial command context, G a policy for its variables, and \na .L. P is computationally non-interferent against a-adversaries when, for both V, U = VaC , \u00d8 and V,U \n= VaI ,VaI n wv(P ), and for all polynomial commands J writing V \\ U: wv(J) . V \\ U;  Bb for b =0, \n1 writing outside V : wv(Bb) n V = \u00d8; A  Aa-adversaries;  T reading V , writing g: rv(T ) . V ; g/. \nwv(J, B0,B1, AA);  and some variable b/. v(J, B0,B1,P, AA, T ) in the command CNI = \u00b7 b := {0, 1}; J; \nif b =0 then B0 else B1; P [AA] g if we have Pr[CNI; x = .]=1, then the advantage x.rv(T ) | Pr[CNI; \nT ; b = g] - 1 | is negligible. 2 The game of this de.nition performs initialization as in De.\u00adnition \n11, then runs P [AA], and .nally tests the resulting memory, g as in De.nition 7. The condition x = . \nprevents that x.rv(T ) T reads possibly-unde.ned memory. As can be expected, De.ni\u00adtion 12 de.nition \ncoincides with De.nition 7 in the deterministic case. Our .rst cryptographic algorithms provide con.dentiality \nby asym\u00admetric (public-key) encryption. We represent them in our target lan\u00adguage as three probabilistic \nfunctions Ge, E, and D that meet the functional and security properties given below. DEFINITION 13 (Encryption \nscheme). Let plaintexts, ciphertexts, publickeys, and secretkeys be sets of polynomially-bounded bit\u00adstrings \nindexed by .. An asymmetric encryption scheme is a triple of algorithms (Ge, E, D) such that Ge, used \nfor key generation, ranges over publickeys\u00d7secretkeys;  E, used for encryption, ranges over ciphertexts; \n D, used for decryption, ranges over plaintexts and is such that, for all ke,kd := Ge() and m . plaintexts, \nwe have D(E(m, ke),kd)= m.  The de.nition abstracts some details, such as input validation, or the possibility \nthat decryption visibly fails on ill-formed inputs for instance. On the other hand, we need to specify \n(or at least bound) the set plaintexts, as we are going to require that encryption hides the length of \nplaintexts. (With our de.nition, the decryption of an encryption of an input outside plaintexts may fail, \nfor instance when the input is too long, but at least the con.dentiality of this input is still preserved.) \nThere are many different notions of security for encryption. The one we use is introduced by Rackoff \nand Simon (1991) and is the strongest usually considered; it can be realized under the Decisional Dif.e-Hellman \nassumption. To code the de.nition in our target language, we rely on auxiliary primitive operations on \nlists: nil for the empty list, + for concatenation, and . for membership test. DEFINITION 14 (IND-CCA2 \nsecurity). Consider the commands E = \u00b7 if b =0 then m := E(x0,ke) else m := E(x1,ke); log := log + m \nD = \u00b7 if m . log then x := 0 else x := D(m, kd) CCA = \u00b7 b := {0, 1}; log := nil; ke,kd := Ge(); A[E, \nD] The encryption scheme (Ge, E, D) provides indistinguishability under adaptive chosen-ciphertext attacks \nwhen the advantage |Pr[CCA; b = g] - 1 | is negligible for any polynomial command 2 context A with b, \nkd ./rv(A) and b, kd,ke, ., log ./wv(A). In this de.nition of security, CCA is a probabilistic command \nthat represents a cryptographic game where the adversary is challenged to guess the secret bit b by interacting \nwith an instance of the encryption scheme. The command A models an adversary that attempts to guess b \nas follows: A can perform arbitrary polynomial-time computation using any variables not excluded in the \nde.nition. For instance, A may include commands that run the algorithms Ge, E, and D on any values that \nA can obtain or compute, including the encryption key ke. A can also invoke encryption and decryption \noracles, modelled as commands E and D, for any values of the parameters x0, x1, m, x, at any point in \nits code. (In the usual presentation of IND-CCA2, the adversary calls the encryption oracle E only once; \nhowever, the two de.nitions are equivalent, see Bellare et al. 2000.)  A can set the variable g and \nterminate to report its guess of the value of the bit b.  In contrast with A, the commands E and D have \naccess to the challenge bit b, the decryption key d, and the log. The encryption oracle E selects which \nof the two values stored in x0 and x1 to encrypt depending on b; it also maintains a log of encrypted \nvalues. The decryption oracle D provides decryption of any value except those produced by E. For any \nrun of the game, the adversary wins when b = g. The adversary A = \u00b7 g := {0, 1} wins with probability \n1 , so the security 2 property states that any adversary that meets our hypothesis cannot do (much) better, \ndespite its control on the usage of the key. The security de.nition we use assumes that the adversary \npro\u00advides all plaintexts to the encryption oracle. Hence, in particular, it does not cover more complex \nusages of encryptions, such as those where encrypted plaintexts may themselves depend on decryption keys. \nSaid otherwise, IND-CCA2 says nothing about con.dential\u00adity in case the plaintexts may depend on the \ndecryption key (see e.g. Abadi and Rogaway 2002). This situation is referred to as a key cycle, and will \nneed to be excluded by typing. For simplicity, we do not introduce primitives for symmetric (shared-key) \nencryption; their de.nition is similar, except that the adversary is not given access to the encryption \nkey. EXAMPLE 4. Assume the adversary reads only xLH, xLH' , and ke. The command ke,kd := Ge(); xLH := \nE(yHH,ke); xLH ' := E(0,ke) is computationally non-interferent (CNI). In particular, we have xLH = xLH \n' with negligible probability, even if yHH =0, so any IND-CCA2 encryption function must be probabilistic. \nConversely, none of the three commands P1 = \u00b7 xLH := E(0,yHH) P2 = \u00b7 ke,kd := Ge(); xLH := E(yHH,ke); \nxLH ' := E(kd,ke) P3 = \u00b7 ke,kd := Ge(); ke' ,k d ' := Ge(); (if yHH then ke ' := ke); xLH := E(zHH,k \ne' ) is CNI for some IND-CCA2-secure encryption schemes: in the .rst case the encryption function is \nnot properly used since yHH isnota key; in the second command, there is a key cycle (ke encrypts kd) \nand IND-CCA2 does not give any assurance for encryptions of the decryption key; in the third command, \nkey selection depends on a secret value, and IND-CCA2 does not prevent extracting ke ' from xLH and comparing \nit with ke. EXAMPLE 5 (Con.dentiality despite active adversaries). Consider two commands mixing local \nsecrets and adversary data, with shared access to keys ke and kd and low con.dentiality variables x0 \nand x1. (Pu)u=0,1 = if eu = u then hu := D(xu,kd); su := hu + lu; x1-u := E(su,ke) The command ke,kd \n:= Ge(); while e ' do (P0;; P1) does not leak any information on xu, even if the adversary controls the \nvalues of e ' , eu, lu, xu for u =0, 1. (The command clearly does not protect the integrity on xu.) \n 3.5 Cryptographic signatures Our second cryptographic scheme provides integrity protection by asymmetric \n(public-key) signatures. DEFINITION 15 (Signature scheme). Let sigkeys, verifykeys, sign\u00adedtexts, and \nplaintexts be sets of polynomially-bound bitstrings indexed by .. A signature scheme is a triple of algorithms \n(Gs, S, V) such that Gs, used for key generation, ranges over sigkeys \u00d7 verifykeys;  S, used for signing, \nranges over signedtexts;  V, used for signature veri.cation, ranges over {0, 1} and is such that, for \nall ks,kv := Gs() and m . plaintexts, we have V(m, S(m, ks),kv)=1.  For convenience, we assume that \nV is deterministic, so that we can use test expressions V(e, e ' ,e '' ) in conditional commands. There \nare also many notions of security for signature schemes. We use a standard notion introduced by Goldwasser \net al. (1988): DEFINITION 16 (CMA security). Consider the commands S = \u00b7 x := S(m, ks); log := log + \nm; CMA = \u00b7 ks,kv := Gs(); log := nil; A[S]; if m . log then b := 0 else b := V(m, x, kv) The signature \nscheme (Gs, S, V) is secure against forgery under adaptive chosen-message attack when Pr[CMA; b = 1] \nis negligi\u00adble for any polynomial command context A that cannot read ks and cannot write ks, kv, log, \n.. In the de.nition, command A represents an adversary that can invoke (as oracle) the command S for \nobtaining the signature x of any message m;  read and write variables m, x; A may also run the veri.cation \nalgorithm, since it can access the veri.cation key kv;  read but not write variables kv, log, and .. \n Conversely, A has no direct access to the signing key ks. This game intuitively says that, after requesting \nas many signatures as he wants from the signing oracle S, the adversary still cannot produce a pair (m, \nx) such that x is the signature for a message m not signed by S, as recorded in log. 4. A type system \nfor cryptography We extend the type system of Section 2 to probabilistic programs, with special rules \nfor typing the usage of cryptography. In the rest of the paper, we assume given two .xed schemes for \nencryption and signing that meet De.nitions 13, 14, 15 and 16, and such that, for each ., the sets plaintexts \ninclude all encrypted and signed values. 4.1 Types We supplement the data type Data of Section 2 with \ntypes for cryptographic values. Data type safety is important for computa\u00adtional soundness inasmuch as \nthe security of its primitives holds only when they are called with properly-generated keys, used only \nas keys. They also help prevent key cycles. We use the following grammar for security types: t ::= t(e) \nSecurity types t ::= Data Data types for payloads | Enc tK | Ke tK | Kd tK Data types for encryption \n| Sig t | Ks F K | Kv F K Data types for signing GENE GENS G(ke)= Ke tK(ee) G(kd)= Kd tK(ed) C(t) =C \nC(ed) G(ks)= Ks F K(es) G(kv)= Kv F K(ev) f ke,kd := Ge() : ee . ed f ks,kv := Gs() : es . ev ENCRYPT \nSIGN f ke : Ke tK(ex) f y : t G(ks)= Ks F K(es) F(t)= t G(x)= Sig t (ex) G(x)= Enc tK(ex) I(t) =I I(x) \nf y : tL(t) = ex I(es) =I I(x) f x := E(y, ke): ex f x := S(t + y, ks): ex VERIFY G(kv)= Kv F K(ev) F(t) \n= G(x) DECRYPT PROBFUN G(x)= t f y : Enc tK(L(x)) f y : Data (e) for y . Ay fV(t + y, m, kv): Data (e \n' ) f P : eP f kd : Kd tK(L(x)) Data (e) = G(x) for x . Ax C(e ' ) =C C(x) . C(eP ) I(ev) =I I(x) f x \n:= D(y, kd): L(x)G f Ax := f(yA): e f if V(t + y, m, kv) then (x := y; P ): L(x) . eP Figure 5. Typing \nrules for probabilistic commands with policy G. where e .L is a security label, K is a key label, and \nF is a map from tags to security types, as explained below. Static key labels The labels K are used to \nkeep track of keys, grouped by their key-generation commands. These labels are at\u00adtached to the types \nof the generated key pairs, and propagated to the types of any derived cryptographic materials. They \nare used to match the usage of key pairs, to prevent key cycles, and to prevent generating multiple signatures \nwith the same key and tag. Tagged signatures Cryptographic signatures are often computed on (hashed) \ntexts pre.xed by a tag or some other descriptor that specializes the usage of the signing key. Accordingly, \nin order to precisely type expressions of the form S(t + m, s) where t is a constant tag, our types for \nsigning embed a partial map, F, from the tags usable with the key to the security type of the correspond\u00ading \nsigned values. Otherwise, we would essentially have to use a distinct key for every signature. EXAMPLE \n6 (Tagged signatures). The command context P [] = \u00b7 ks,kv := Gs(); yLL := S(t0+ xLH,ks); zLL := xLH; \n' ''' yLL := S(t1+ xLH,ks); zLL := xLH; ; if V(t0+ zLL,yLL,kv) then hLH := zLL is CNI (and typable) against \nan adversary that can read and write ' ' yLL, yLL, zLL, zLL. On the other hand, for the same class of \nadver\u00adsaries, the command context obtained by erasing the two tags t0 and t1 is not CNI for integrity, \nas can be seen for an adversary that overwrites the .rst value and signature with the second ones: J \n= \u00b7 xLH := 0; xLH ' := 1 Bb = \u00b7 zLL := b \u00b7 '' A = if zLL then (yLL := yLL; zLL := zLL) T = \u00b7 if xLH \n= xLH ' then g := 0 else g := 1 Subtyping We rely on the two subtyping rules of the source type system, \nso subtyping between data types is just syntactic equality we leave more interesting subtyping for future \nwork. Note that subtyping from Kd tK to Data would not be sound in general, as it may hide some key dependencies \nand encryption cycles.  4.2 Typing rules Our type system extends the source type system (Figures 1, \n2, and 3) with the rules of Figure 5 for commands that call probabilis\u00adtic functions, as explained below. \nIt also has an additional rule for expressions, for typing signature veri.cations; this rule is identical \nto rule OP except for its cryptographic data types: OPVER f y : t(e) f m : Sig t(es)(e) f kv : Kv F K(e) \nF(t)= t(es) fV(t + y, m, kv): Data (e) Rule PROBFUN is the generic rule for typing probabilistic func\u00adtions; \nit requires that all variables have Data types and prevents explicit .ows from the parameters Ay to the \nresults Ax. In particular, PROBFUN applies to the functions S, E, and D in case we do not rely on cryptographic \nassumptions. (The soundness of PROBFUN depends on the fact that f is a probabilistic function; side-effects \nin the evaluation of f would create correlation between successive calls to f.) The rest of the rules \nare for cryptography; they permit some forms of declassi.cation for encryptions (.ows from higher to \nlower con.dentiality levels) and endorsement for signature checks (.ows from lower to higher integrity \nlevels). The soundness of these rules depends both on cryptographic assumptions and on ad\u00additional conditions \non policies and programs, stated in Section 4.3. GENS The two hypotheses bind key types to variables \nks and kv, with the same map F from tags to payload types. The process label es . ev is the meet of the \nlabels of all assigned variables. SIGN Hypotheses 1, 3, and 4 bind types to the variables ks, x, and \ny involved in signing; these types are related by t , which sets the typing guarantees associated with \nsignatures that use any signing key with key-label K and tag t. Intuitively, we care mostly about the \nintegrity of y (so that we only sign correct values) and the con.dentiality of ks (so that the adversary \ncannot sign incorrect values). L(t ) = ex records the .ow from y to x, as in rule ASSIGN. I(es) =I I(x) \nrecords the integrity .ow from the signing key to the signature value. (Conversely, the con.dentiality \n.ow from ks to x is ignored; this is sound only inasmuch as ks is used only for signing.) VERIFY The \nrule has a structure similar to rule CHECK. (Indeed, in the soundness proof, we use game rewritings to \nreplace com\u00admands typable by VERIFY with commands typable by CHECK.) It also permits a limited form of \nendorsement: a lower integrity variable v can be assigned to a higher integrity variable x only if the \nguard performs a speci.c signature veri.cation. Hypotheses 1 and 2 check the veri.cation-key type and \nrelate the type F(t) for the tag used in the veri.cation to the type of x. The typing of the veri.cation \nexpression relies on rule OPVER; it records in e ' the ordinary .ows from y, m, and kv to the condition \nguard; it also enforces that y and F(t) (and thus x) have identical data types. The typing of P records \nin eP the level of the guarded command. The constraint on con.dentiality levels records the implicit \ncon\u00ad.dentiality .ow from the guard to both the assignment and the guarded command. In contrast to normal \nassignment (ASSIGN), there is no constraint relating the integrity of e ' and x, as in\u00adtegrity follows \nfrom dynamic veri.cation. Instead, an integrity constraint records a .ow from the veri.cation key to \nx. GENE The hypotheses bind types to variables ke and kd with the same key-label and payload type t the \ntype of plaintexts that can be encrypted and decrypted. The constraint C(t) =C C(ed) imposes the condition \nthat con.dentiality of the decryption key is greater or equal than the con.dentiality of the plaintext. \nENCRYPT The .rst three hypotheses bind types to the variables ke, y, and x involved in encryption; these \ntypes are related by t and K, which describe the typing assumptions for encryption with key ke. The label \nex in the typing of ke records the .ow from ke to x (by subtyping, we have L(ke) = ex). The hypothesis \nI(t ) =I I(x) records the integrity .ow from y to x. Conversely, there is no constraint on the con.dentiality \n.ow from y to x, as encryption is a form of declassi.cation: the rule is sound only with cryptographic \nassumptions. DECRYPT The hypotheses bind types to the variables x, y, and kd involved in decryption; \nthese types are related by t and K. The label L(x) records .ows from y and kd to x, as in normal assignment. \nBefore stating our soundness result, we illustrate the type sys\u00adtem on a few commands. EXAMPLE 7 (Bad \ndecrypted key). Consider the command ke,kd := Ge(); ke' ,k d ' := Ge(); xLL := E(ke' ,ke); kd := kd' \n; k := D(xLL,kd); y := E(s, k) '' '' For the payload types t = Data (e ) and t = Ke tK (LH), we de.ne \nG as follows: G(ke)= Ke tK(LH) G(kd)= Kd tK(HH) G(ke' )= t G(kd' )= Kd t ' K ' (HH) G(xLL)= Enc tK(LL) \nG(k)= t G(s)= t ' G(y)= Enc t ' K ' (e) In case C(e) = L and C(e ' ) = L, this command is insecure because \nthe key kd used for decryption does not match the key ke used for encryption. Hence, the decrypted value \nk is unspeci.ed it is unlikely to be a valid encryption key and encryption using k is also unspeci.ed \none can easily construct algorithms Ge, D, E that are IND-CCA2 and such that the .nal encryption leaks \nboth its parameters s and k. This command is not typable, since the data types of kd and kd ' are not \ncompatible. This problem is not apparent in the work of Laud and Vene (2005) because, in their system, \ndecryption never yield key types the decrypted value k is just ordinary data that must remain secret \nirrespective of its distribution, so the .nal encryption is not typable. In our setting, we must address \nthe problem in order to guarantee integrity as well as con.dentiality after decryption. Although the \nprogram above is not typable either, we are able to type similar programs that rely on decrypted keys: \nEXAMPLE 8 (Encrypt-then-sign an encryption key). Consider the command context ke,kd := Ge(); ke' ,k d \n' := Ge(); ks,kv := Gs(); xLH := E(ke' ,ke); zLL := S(t + xLH,ks); xLL := xLH; ; if V(t + xLL,zLL,kv) \nthen (xLH ' := xLL; k := D(xLH' ,kd); y := E(s, k)) In this example, thanks to the signature veri.cation, \nk is a valid decrypted key. If the adversary changes the signature stored in x, then signature veri.cation \nfails, leaving m ' , k, and y uninitialized. The program is typable using the same G of Example 7 extended \nwith types Sig (Enc tK(LL))(LL) for zLL, Ks F Ks(LH) for ks, Kv F Ks(HH) for kv, Enc tK(LH) for xLH' \n, and Enc tK(LH) for xLH, with F(t)= Enc tK(LH). 4.3 Computational soundness We give additional conditions \non policies and programs, then state our main soundness theorem. We require that the integrity of en\u00adcryption \nkeys is high enough for protecting con.dentiality of plain\u00adtexts, and that the con.dentiality of decryption \nkeys is high enough for protecting integrity of signed values. These constraints relate integrity and \ncon.dentiality levels, depending on the capabilities of the adversary. DEFINITION 17 (Robust policy). \nLet G be a policy and a .L. The encryption key type Ke tK(e) is robust at a when either C(t ) =C C(a) \nor I(e) =I I(a). The signing key type Ks F K(e) is robust at a when either I(F(t)) =I I(a) for all t \n. dom(F) or C(e) =C C(a). The security-type policy G is robust at a when all its encryption and signing \nkey types are robust at a. In the two statements of robustness for key types, the .rst alter\u00adnatives \nstate that the protection provided by the key is irrelevant against an adversary at a, who could read \nplaintexts before encryp\u00adtion and write signed values before signing; the second alternatives demand \nthat otherwise the key itself be suf.ciently protected. Besides the cryptographic assumptions, we state \nadditional safety conditions for soundness. DEFINITION 18. A command context P is safe when G f P and \n1. All commands guarded either by a signature veri.cation or by a test typed by CHECK exclusively assign \nVaI in P ; and P never reads uninitialized variables in VaI . 2. Each signing-key label/tag pair is \nused for signing at most once. 3. Each key label is used in at most one (dynamic) key generation. 4. \nEach key variable read in P is .rst initialized by P .  These conditions are needed to apply the cryptographic \ngames in the soundness proof of the target type system, for instance to guar\u00adantee the integrity of decrypted \nvalues. They can be enforced by static analysis, for instance by collecting all relevant static occur\u00adrences \nof variables and forbidding signing and encryption-key gen\u00aderation within loops. Condition 1 helps deal \nwith runtime errors, as discussed in Sec\u00adtion 2. Condition 2 prevents signature replay attacks, as illustrated \nin Example 6. (We rely on a static key label so that the unique\u00adsigning constraint is shared between \nall aliases of any given sign\u00ading key.) Condition 3 prevents decryption-key mismatches, as illus\u00adtrated \nin Example 7. Condition 4 recalls our assumption on unini\u00adtialized variables for keys. Relying on these \nconditions, we obtain that well-typed pro\u00adgrams are computationally non-interferent (De.nition 12). THEOREM \n4. Let a .L be a security label. Let G be a policy that is robust at a. Let P be a safe polynomial-time \ncommand context. P satis.es computational non-interference against a-adversaries. The proof relies on \na series of typability-preserving program transformations that match the structure of the games used \nin the cryptographic security assumptions (De.nitions 14 and 16). These transformations eliminate the \ncryptographic primitives, one static key label at a time. Hence, after eliminating a (static) keypair \nfor signing and veri.cation, the values that were signed and veri.ed are now passed on auxiliary shared \nhigh-integrity variables, and we are left with conditionals typable by CHECK instead of VERIFY. 4.4 \nA memory-protection protocol In preparation for our program translation, we de.ne a protocol for sharing \nencrypted-then-signed memory. (Its typing assumptions are detailed in Section 5.) Inits(ks,kv)= \u00b7 ks,kv \n:= Gs() Inite(ke,kd)= \u00b7 ke,kd := Ge() Read(x . xe,xs,xe' ,kd,kv, t)[P ]= \u00b7 if V(t + xe,xs,kv) then (x \n' e := xe; x := D(x ' e,kd); P ) \u00b7 Write(xs,xe . x, xe' ,ke,ks, t)= ' '' xe := E(x, ke); xs := S(t + \nxe,ks); xe := xe Command Inits generates keys for signing and veri.cation. Com\u00admand Inite generates keys \nfor encryption and decryption. Com\u00admand context Read attempts to read x from xe and xs. It veri.es x \ns presumed signature xs, speci.cally for the tag t; if the veri.ca\u00adtion succeeds, the ciphertext is copied \nto a temporary variable x ' e then decrypted to x, and .nally command P runs. Otherwise the command silently \nfails. Conversely, command Write writes x to xe and xs, by .rst encrypting then signing x s value. (The \ntemporary variables xe ' matter only for typing; they enable us to label xe ' with integrity higher than \nxe.) Depending on the relative levels of x and a, similar but simpler protocols may be used instead for \nprotecting only con.dentiality, or only integrity. EXAMPLE 9 (Key Establishment). Relying on the protocol \nabove, we show how two hosts H and H ' may dynamically establish new session keys using their long-term \nkeys. We assume that variables with pre.x H are local to H (readable and writable only in H) and variables \nwith pre.x H ' local to H ' , respectively. We use the command context H ' ' H ' HH Inite(ke ,H .kd ); \nInits(H.ks ,kv ); ; Inite(H.ke, H.kd); Inits(H.ks, H.kv); H.k := H.ke + H.kd + H.ks + H.kv; H ' H Write(xs,xe \n. H.k, H.xe,ke , H.ks ,tk); ; ' '' H ' H ' Read(H .k . xe,xs,H .xe,H .kd ,kv ,tk)[P ] The .rst command \nline models our hypothesis that H and H ' share correctly-generated long-term keys (keH ' and kvH ), \nwith a private signing key for H (H.ksH ) and a private decryption key for H ' ' (H .kdH ' ). The third \nline represents session-key generation by H: four local keys are generated. These keys are then concatenated \n(in H.k) and sent (via shared memory xe and xs) towards H ' using a Write command. The .nal line represents \nthe reception of session keys by H ' . To type this command, we need a slight extension of our typing \nrules for concatenation. Alternatively, we can type a variant of this command where four runs of the \nRead/Write protocol are executed, one for each of the keys H.ke, H.kd, H.ks, and H.kv. On the other hand, \none can optimize the protocol by avoiding encryption for the public keys ke and kv. 5. A cryptographic \ntranslation We are now ready to describe and verify general cryptographic protection mechanisms for shared \nmemory. To illustrate our type\u00adbased approach, we provide a simple translation from programs that rely \non shared-memory security policies to programs that rely on cryptography. Modelling distributed systems \nWe model distributed systems as series of commands from the source language located at different hosts \nthat communicate through shared memory. Untrusted shared memory may conservatively represent a public \nnetwork, or a pro\u00adtocol stack for example. We consider source systems of the form P1; ... ; Pn, such \nthat the control .ow between threads is statically known. We let Ah be the set of all host names. Each \nhost may have several successive threads, which may share private state using the host s local mem\u00adory. \nThus, we de.ne (source) systems as sequences of threads S ::= (P )i | S; S where each thread (P )i is \nannotated with a unique thread iden\u00adti.er i. We let . be a mapping from thread identi.ers to hosts. (Holes \ndo not appear in source systems; the translation insert them between translated threads.) We assume given \na set of variables, X, shared between some of these hosts, that require cryptographic protection. To \nobtain a realistic distributed implementation, this set X should contain all variables shared between \nany two hosts, except possibly for some initialization variables. We also assume that every occurrence \nof every variable x . X in an expression of S is correctly annotated by its last-writer-thread: we write \nx i when i is (always) the last thread to have written x when the expression gets evaluated. If there \nis no such thread (in particular, if a variable of X may be read before being written in S), the program \ncannot be correctly annotated. These annotations can be inferred (or checked) using conservative static \nanalyses. In the translation, they help us meet the requirements on signatures, and thus prevent replay \nattacks, as we always know which veri.cation key and tag should be used when x is read. In summary, the \nsource inputs of the translation consist of a policy G, a subset X of its domain indicating the variables \nto protect, and a well-typed, correctly annotated system S. Although our translation takes as input systems, \ni.e. annotated commands, the annotations do not affect source command typing: after erasing thread-and \nwriter-annotations, we use the simple type system of Section 2.4, Figures 1 and 2. Public-key infrastructure \nWe assume given a public-key infras\u00adtructure for signature veri.cation keys: every host h has a signing \nkey ksh and knows every other host s veri.cation key kvh . We ini\u00adtialize these keys by running the command \nInit Ah =(ksh ,kvh := Gs(); )h.h Auxiliary variables The translation uses the following naming conventions \nand types for variables. We assume that the variables introduced by the translation do not occur in the \nsource system and are pairwise distinct. To every source variable x . X, the translation associates two \nshared variables, xe containing the encrypted value of x, and xs containing the signature for xe. In \naddition, for every thread i that accesses x in the source program, the translation uses a series of \nlocal variables: i.x is a local copy of x; i.xe and i.x ' e are local buffers for the encrypted value \nof x; i.xv is a local buffer for x after veri.cation. Finally, the translation uses two functions from \nx . X to vari\u00adables holding encryption and decryption keys: ke(x) is the encryp\u00adtion key for writing \nx; kd(x) is the decryption key for reading x. We do not assume that these functions are injective: on \nthe con\u00adtrary, subject to typing constraints, the same keys should be used to protect many shared variables. \nWe initialize all variables in the range of these functions by running the command InitX =(kd(x),ke(x) \n:= Ge(); )x.X where each key pair ke(x),kd(x) for x . X is initialized just once. Translating security \npolicies We now translate G, thereby giving types to all target variables: we let [[G]] be the security \npolicy that coincides with G on dom(G)\\X and maps the translation variables to the types given below. \nFor simplicity, we use a single security label for all unprotected shared memory (after encryption and \nsigning): we let e ' be a secu\u00adrity level such that I(e ' )= I(TL) and C(e ' )= C(.L). (Alter\u00adnatively, \nwe could parameterize the translation with target security labels, and write a set of constraints to \nbe satis.ed in order to pre\u00adserve typability.) For every host h . Ah, we let hhh h [[G]](ks )= Ks F Kh(es \n) [[G]](kv )= Kv F Kh(ev ) where Kh is a unique static key label (no other host will have the hh same \nlabel), F is a mapping from tags to security types, and es , ev are security labels, subject to the constraints \ngiven below. For every x . X, let t = G(x). Since source types do not use cryptography, we have t = Data \n(e) for some e. Let ee .L such that I(ee)= I(e) and C(ee)= .C . We let [[G]](xe)= Enc tKx (e ' ) [[G]](xs)= \nSig Enc tKx(ee)(e ' ) [[G]](ke(x)) = Ke tKx(ee) [[G]](kd(x)) = Kd tKx(ed) | C(e)= C(ed) . I(ed) =I I(e) \nwhere Kx is the static, unique key label for the keypair ke(x),kd(x) (no other key will have the same \nlabel). For every thread i that accesses x, we let [[G]](i.x)= t [[G]](i.xe)= Enc tKx(ee) [[G]](i.xv)= \nt [[G]](i.x ' e)= Enc tKx(ee) Moreover, if i writes x at host h = .(i), we have three constraints hh \nev = ee I(es ) =I I(ee) F( x + i)= Enc tKx(ee) Translating systems and threads We now translate commands, \nrelying on the command de.nitions and notations of the memory protocol of Section 4.4. Our translation \nprotects every thread (Pi)i by .rst decoding every shared variable of X (syntactically) read by Pi into \nlocal memory then, if all veri.cations succeed, run\u00adning (a variant of) Pi on local memory and, .nally, \ncommitting ev\u00adery (syntactically) written variable of X back to shared encrypted memory. For every thread \n(Pi)i of S, we let Ri[] be the composition of command contexts Read(i.xv . xe,xs, i.xe,kd(x),kv.(w) , \nx + w)[ ] for all annotated variables x w in rv(Pi) n X with w = i, let Pi ' be the sequence of assignments \ni.x := i.xv for the variables i.xv assigned in Ri, and let W i be the sequential composition of commands \n' .(i) Write(xs,xe . i.x, i.x e,ke(x),ks , x + i) for all x . wv(Pi) n X. We arrive at the top-level \ntranslation: ii' i [[Pi]]= RPi ; Pi{i.x/x for x . X}; W [[(P0)0; ... ;(Pn)n]] = Init Ah; InitX ; ;[[P0]]0;; \n... ; ;[[Pn]]n; For each translated thread, each variable in X is written at most once. This ensures \nthat each tag pair (composed by a unique thread identi.er and a variable name) is used at most once. \nMoreover, the auxiliary assignments Pi ' ensure that the local variables i.x are assigned only if all \nveri.cations succeed; they guarantee exclusive assignments for these variables. EXAMPLE 10. In the 4-point \nlattice, consider the translation of 00 1 (yHH :=0);(if yHH =0 then xHH := 1) with two threads located \nat hosts h0 and h1 that share a single key pair ke,kd for all encryptions. For X = {yHH}, the two translated \nthreads are: P0 =0.yHH := 0;0.yLH,e := E(0.yHH,ke); yLL,s := S( yHH +0+0.yLH,e,ksh0 ); yLL,e := 0.yLH,e \nP1 = if V( yHH +0+0.yLL,e,yLL,s,kvh0 ) then(1.yLH,e := yLL,e;1.yHH,v := D(1.yLH,e,kd); 1.yHH := 1.yHH,v; \nif 1.yHH =0 then xHH := 1) Discussion The translation systematically performs two crypto\u00adgraphic operations \nfor every access to a shared variable in X. Still, we believe that our type system also supports a variety \nof other op\u00adtions, which would be selected by more advanced translations. As illustrated in Example 9, \nthe translation may reduce cryptographic costs by clustering variables with the same level into fewer, \nlarger shared variables (representing messages, or pages in a distributed .le system, for instance). \nBesides, depending on source labels, the translation may select simpler read and write protocol that \nomit en\u00adcryption or signing for low-integrity or low-con.dentiality source variables. Using more expressive \ntypes for cryptographic payloads, the translation may also jointly sign several values. For instance, \na single signature suf.ces after executing every thread, and may be typable by associating a constant \ntag to a series of authenticated types, for a .xed series of variable writes. When reading or writing \nin a high-con.dentiality command, we cannot let runs of the read or write protocols be observable. To \navoid this, the translation pre-fetches all variables potentially read under high-con.dentiality guards, \nand rewrite all variables potentially written under a high-con.dentiality guard. Consider for instance \nthe second source thread of Example 10, in case xHH is added to X. If we directly translated the update \nxHH := 1 by a run of the write protocol under the high-con.dentiality guard, an adversary would be able \nto compare ciphertexts xe before and after running the thread, and thus infer the value of yHH by observing \nwhether the second thread updates the encrypted value or not. In our translation (with xHH . X), the \nlocal value 1.x is always re\u00adencrypted, irrespective of yHH, and thus always looks opaque and different \nfrom its previous value to a polynomial adversary. Correctness We .rst verify that, in the absence of \nan active ad\u00adversary, our translation is functionally correct, that is, the translated system always \nterminates with the same results as the source sys\u00adtem. (The translated memory also includes cryptographic \nmaterials; the domain condition in the statement below excludes their values.) For a given source con.guration \n(P, \u00b5) such that \u00b5 is unini\u00adtialized on X, we let \u00b5d be the memory de.ned as follows: for every host \nh . Ah, and for every x . X and thread i of P such that x occurs in i, ksh , kvh , xe, xs, ke(x), kd(x), \ni.x, i.xv, i.xe, and i.x ' e are de.ned and uninitialized; for every x . dom(\u00b5) \\ X, \u00b5d(x)= \u00b5(x); and \nwe let [[(P, \u00b5)]] be the distribution of con.gu\u00adrations such that [[(P, \u00b5)]](P, \u00b5d)=1. THEOREM 5 (Functional \nAdequacy). Let S be a correctly-anno\u00ad v tated source system and \u00b5 a memory such that (S, \u00b5) r * ( ,\u00b5 \n' ). We have [[(S[skipA],\u00b5)]] r * dT for a distribution dT that gives v probability 1 to con.gurations \n( ,\u00b5 T ' ) such that \u00b5 ' coincides T with \u00b5 ' on their joint domain. In the theorem, the distribution \nof .nal states dT may give positive probabilities to a range of different memories \u00b5T ' , with for instance \ndifferent keys and different encrypted values, but these memories at least coincide on source variables \noutside X. (We can obtain adequacy for the .nal values of source variables in X by copying them to variables \noutside X at the end of P .) Also, since \u00b5 is .xed, we do not need to assume that P is polynomial: for \na given run, we can select sets plaintexts for De.nitions 13 and 15 that include at least the values \nencrypted and signed in this run. We now consider the security of the translation, under the as\u00adsumption \nthat the source system is well-typed. THEOREM 6 (Typability Preservation). Let S be a correctly anno\u00adtated \nsource system. If G f S, then [[G]] f [[S]]. By computational soundness of typing (Theorem 4), we obtain: \nTHEOREM 7 (Computational soundness for the translation). Let a .L a security label, G a source security \npolicy, X . dom(G), and S =(P0)0; ... ;(Pn)n a correctly-annotated source system such that G f S and \nP0,. . . ,Pn exclusively assign VaI \\ X. If [[G]] is a robust policy at a, then [[S]] is computationally \nnon\u00adinterferent against a-adversaries. 6. Conclusions and future work We presented a cryptographic type \nsystem for verifying the cor\u00adrect usage of encryption and signing schemes in programs with information-.ow \nsecurity policies. and used it to develop a secure translation for imperative programs that share protected \nmemory. Security is de.ned by a computational non-interference property against active probabilistic, \npolynomial adversaries. We would like to extend both the type system and the translation to obtain more \nef.cient implementations for a larger class of pro\u00adgrams (for instance by using dependent types rather \nthan tags for signing keys, by extending the subtyping relation, and by support\u00ading symmetric-key cryptography). \nIndependently, it would be inter\u00adesting to extend the class of security properties preserved by cryp\u00adtographic \nimplementations, for instance to account for controlled forms of declassi.cations and endorsements in \nsource programs. Acknowledgments We thank Ricardo Corin, James Leifer, Jean-Jacques L\u00b4evy, Andrei Sabelfeld, \nNobuko Yoshida, and the anonymous reviewers for their helpful comments. References Mart\u00b4in Abadi. Protection \nin programming-language translations. In 25th International Colloquium on Automata, Languages and Programming, \nvolume 1443 of LNCS, pages 868 883. Springer-Verlag, 1998. Mart\u00b4in Abadi and Phillip Rogaway. Reconciling \ntwo views of cryptography (the computational soundness of formal encryption). Journal of Cryp\u00ad tology, \n15(2):103 127, 2002. Mart\u00b4in Abadi, Ricardo Corin, and C\u00b4 edric Fournet. Computational secrecy by typing \nfor the pi calculus. In APLAS 06, volume 4279 of LNCS, pages 253 269. Springer-Verlag, November 2006. \nPedro Ad edric Fournet. Cryptographically sound implementations ao and C\u00b4for communicating processes \n(extended abstract). In 33rd International Colloquium on Automata, Languages and Programming, volume \n4052 of LNCS, pages 83 94. Springer-Verlag, July 2006. Aslan Askarov, Daniel Hedin, and Andrei Sabelfeld. \nCryptographically\u00admasked .ows. In Proceedings of the 13th International Static Analysis Symposium, LNCS, \nSeoul, Korea, 2006. Springer-Verlag. M. Backes, B. P.tzmann, and M. Waidner. A composable cryptographic \nlibrary with nested operations. In 10th ACM Conference on Computer and Communications Security, pages \n220 230, 2003. Michael Backes. Quantifying probabilistic information .ow in computa\u00adtional reactive systems. \nIn ESORICS 05, volume 3679 of LNCS, pages 336 354. Springer-Verlag, September 2005. Mihir Bellare and \nPhillip Rogaway. The game-playing technique, De\u00adcember 2004. At http://www.cs.ucdavis.edu/ rogaway/ papers/games.html. \nMihir Bellare, Alexandra Boldyreva, and Silvio Micali. Public-key encryp\u00adtion in a multi-user setting \n: Security proofs and improvements. In EU-ROCRYPT, pages 259 274, 2000. Dorothy E. Denning. A lattice \nmodel of secure information .ow. Commun. ACM, 19(5):236 243, 1976. Sha. Goldwasser, Silvio Micali, and \nRonald Rivest. A digital signature scheme secure against adaptive chosen-message attack. SIAM Journal \non Computing, 17(2):281 308, 1988. Holger Hermanns. Interactive Markov Chains: The Quest for Quanti.ed \nQuality. Springer Berlin/Heidelberg, 2002. Peeter Laud. Secrecy types for a simulatable cryptographic \nlibrary. In 12th ACM Conference on Computer and Communications Security, pages 26 35, 2005. Peeter Laud. \nSemantics and program analysis of computationally secure information .ow. In 10th European Symposium \non Programming (ESOP 2001), volume 2028 of LNCS. Springer-Verlag, April 2001. Peeter Laud. On the computational \nsoundness of cryptographically-masked .ows. In Proceedings of the 35th Symposium on Principles of Program\u00adming \nLanguages, San Francisco, USA, 2008. ACM Press. Peeter Laud and Varmo Vene. A type system for computationally \nsecure information .ow. In Fundamentals of Computation Theory, LNCS, pages 365 377. Springer-Verlag, \n2005. Wenbo Mao. Modern Cryptography: Theory and Practice. Prentice Hall Professional Technical Reference, \n2003. David Monniaux. Analyse de programmes probabilistes par interpr\u00b4 etation abstraite. PhD thesis, \nUniversit\u00b4e Paris IX Dauphine, 2001. Andrew C. Myers and Barbara Liskov. Protecting privacy using the \ndecen\u00adtralized label model. ACM Trans. Softw. Eng. Methodol., 9(4):410 442, 2000. Andrew C. Myers, Andrei \nSabelfeld, and Steve Zdancewic. Enforcing robust declassi.cation and quali.ed robustness. Journal of \nComputer Security, 14(2):157 196, 2006. Charles Rackoff and Daniel R. Simon. Non-interactive zero-knowledge \nproof of knowledge and chosen ciphertext attack. In CRYPTO 91, volume 576 of LNCS, pages 433 444. Springer-Verlag, \n1991. Andrei Sabelfeld and Andrew Myers. Language-based information-.ow security. IEEE Journal on Selected \nAreas in Communications, 21(1), 2003. Geoffrey Smith and Rafael Alp\u00b4izar. Secure information .ow with \nrandom assignment and encryption. In FMSE 06: Proceedings of the fourth ACM workshop on Formal methods \nin security, pages 33 44, 2006. Jeffrey A. Vaughan and Steve Zdancewic. A cryptographic decentralized \nlabel model. In IEEE Symposium on Security and Privacy, pages 192 206, May 2007. Steve Zdancewic and \nAndrew Myers. Robust declassi.cation. In 14th IEEE Computer Security Foundations Workshop, pages 15 23, \n2001. Steve Zdancewic, Lantian Zheng, Nathaniel Nystrom, and Andrew C. My\u00aders. Secure program partitioning. \nACM Trans. Comput. Syst., 20(3): 283 328, 2002. Lantian Zheng, Steve Chong, Andrew Myers, and Steve Zdancewic. \nUsing replication and partitioning to build secure distributed systems. In 15th IEEE Symposium on Security \nand Privacy, 2003.   \n\t\t\t", "proc_id": "1328438", "abstract": "<p>In language-based security, confidentiality and integrity policies conveniently specify the permitted flows of information between different parts of a program with diverse levels of trust. These policies enable a simple treatment of security, and they can often be verified by typing. However, their enforcement in concrete systems involves delicate compilation issues.</p> <p>We consider cryptographic enforcement mechanisms for imperative programs with untrusted components. Such programs may represent, for instance, distributed systems connected by some untrusted network. In source programs, security depends on an abstract access-control policy for reading and writing the shared memory. In their implementations, shared memory is unprotected and security depends instead on encryption and signing.</p> <p>We build a translation from well-typed source programs and policies to cryptographic implementations. To establish its correctness, we develop a type system for the target language. Our typing rules enforce a correct usage of cryptographic primitives against active adversaries; from an information-flow viewpoint, they capture controlled forms of robust declassification and endorsement. We showtype soundness for a variant of the non-interference property, then show that our translation preserves typability.</p> <p>We rely on concrete primitives and hypotheses for cryptography, stated in terms of probabilistic polynomial-time algorithms and games. We model these primitives as commands in our target language. Thus, we develop a uniform language-based model of security, ranging from computational non-interference for probabilistic programs down to standard cryptographic hypotheses.</p>", "authors": [{"name": "C&#233;dric Fournet", "author_profile_id": "81100547450", "affiliation": "Microsoft Research, Cambridge, United Kingdom", "person_id": "PP14190246", "email_address": "", "orcid_id": ""}, {"name": "Tamara Rezk", "author_profile_id": "81327491335", "affiliation": "INRIA, Sophia-Antipolis, France", "person_id": "PP28002443", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1328438.1328478", "year": "2008", "article_id": "1328478", "conference": "POPL", "title": "Cryptographically sound implementations for typed information-flow security", "url": "http://dl.acm.org/citation.cfm?id=1328478"}