{"article_publication_date": "01-07-2008", "fulltext": "\n Lightweight Semiformal Time Complexity Analysis for Purely Functional Data Structures Nils Anders Danielsson \nChalmers UniversityofTechnology nad@cs.chalmers.se Abstract Okasaki and others have demonstrated howpurely \nfunctional data structures that are ef.cient even in the presence of persistence can be constructed.To \nachieve good time bounds essential useis often made of laziness. The associated complexity analysis is \nfrequently subtle, requiring carefulattention to detail, and hence formalising it is valuable. This paper \ndescribes a simple library which can be used to make the analysis of a class of purely functional data \nstructures and algorithms almost fully formal. The basic idea is to use the type system to annotate every \nfunction with the time required to compute its result. An annotated monad is used to combine time complexity \nannotations. The library has been used to analyse some existing data struc\u00adtures, for instance the deque \noperations of Hinze and Paterson s .nger trees. Categories and Subject Descriptors F.2.m [Analysis of \nAlgo\u00adrithms and Problem Complexity]: Miscellaneous; F.3.1[Logics and Meanings of Programs]: Specifying \nand Verifying and Rea\u00adsoning about Programs; D.1.1[Programming Techniques]: Ap\u00adplicative (Functional) \nProgramming; E.1[Data Structures] General Terms Languages, performance, theory, veri.cation 1. Introduction \nData structures implemented in a purely functional language auto\u00admatically become persistent;evenifa \ndata structureisupdated,the previous version can still be used. This property means that, from a correctness \nperspective, users of the data structure have less to worry about, since there are no problems with aliasing. \nFrom an ef.ciencyperspective the picture is less nice, though: different us\u00adage patternscanleadtodifferenttimecomplexities.For \ninstance,a common implementation of FIFO queues has the property that ev\u00adery operation takes constant \namortised time if the queues are used single-threadedly (i.e. if the output of one operation is always \nthe input to the next), whereas for some usage patterns the complexity of the tail function becomes linear \n(Okasaki1998). Despite this a number of purely functional data structures ex\u00adhibiting good performance \nno matter how they are used have been developed (see for instance Okasaki 1998; Kaplan andTarjan 1999; \nKaplan et al. 2000; Hinze andPaterson 2006).Manyof these data structures make essential use of laziness \n(non-strictness with mem\u00adoisation, alsoknown as call-by-need) in order to ensure good per\u00adformance; see \nSection 8.1 for a detailed example. However, the re\u00adsulting complexity analysis is often subtle, with \nmany details to keep track of. To address this problem the paper describes a simple library, THUNK, for \nsemiformal veri.cation of the time complexity of purely functional data structures. The basic idea is \nto annotate the code (the actual code later to be executed, not a copy used for veri.cation) with ticks, \nrepresenting computation steps: -:Thunk n a . Thunk (1+ n) a Time complexity is then tracked using the \ntype system. Basically,if avalue has typeThunk n a,thenaweak head normal form (WHNF) of type a can be \nobtained in n steps amortised time, no matter how the value is used. Thunk is a monad, and the monadic \ncombinators are used to combine time complexities of subexpressions. Note that the Thunk type constructor \ntakesavalue(n)as argu\u00adment; it is a dependent type. The THUNK library is implemented in the dependently \ntyped functional language Agda (Norell 2007; The AgdaTeam 2007), which is described in Section 2. The \nap\u00adproach described in the paper is not limited to Agda it does not even need to be implemented in the \nform of a library but for con\u00adcreteness Agda is used when presenting the approach. In order to analyse \nessential uses of laziness THUNK makes use of a simpli.ed version of Okasaki s banker s method (1998). \nThis version is arguably easier to explain (see Section 8), but it is less general, so fewer programs \ncan be properly analysed. A generalisation of the method, also implemented, is discussed in Section 11, \nand remaining limitations are discussedin Section 12. Despite anylimitations the methods are still useful \nin practice. The following algorithms and data structureshave been analysed: Linear-time minimum using \ninsertion sort, the standard exam\u00adple for time complexity analysis of call-by-name programs (see Section \n7).  Implicit queues (Okasaki 1998), which make essential use of laziness (see Section 8).  The deque \noperations of Hinze and Paterson s .nger trees (2006).1  Banker s queues (Okasaki 1998), by using the \ngeneralised method described in Section 11.1  Permission to make digital or hard copies of all or part \nof this work for personal or classroom use is granted without fee provided that copies are not made or \ndistributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page.To copyotherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. POPL 08, January 7 12, 2008, San Francisco, California, USA. Copyright \nc &#38;#169; 2008ACM 978-1-59593-689-9/08/0001...$5.00 The time bounds obtained using the library are \nveri.ed with respect to an operational semantics for a small, lazy language; see Section9.To increase \ntrustintheveri.cationithas been checked mechanically (also using Agda, which doubles asaproof assistant). \n1Using an earlier,butvery similar,versionof the library. The source code for the library, the examples \nmentioned above, and the mechanisation of the correctness proofare available from the author sweb page \n(currently http://www.cs.chalmers.se/ ~nad/). A technical report also describes the mechanisation in \nmore detail (Danielsson 2007). To summarise, the contributions of this work are as follows: Asimple, \nlightweight library for semiformal veri.cation ofthe time complexity of a useful class of purely functional \ndata structures.  The library has been applied to real-world examples.  The library has a well-de.ned \nsemantics, and the stated time bounds have been veri.ed with respect to this semantics.  The correctness \nproofs have been checked using a proof assis\u00adtant.  The rest of the paper is structured as follows: \nSection 2 de\u00adscribes Agda and Section 3 describes the basics of THUNK. The implementation of the library \nis discussed in Section 4, some rules for how the library must be used are laid down in Section 5, and \nSections 6 8 contain further examples on the use of THUNK. The correctness proof is outlined in Sections9 \n10, Section 11 motivates and discusses a generalisation of the library, and Section 12 de\u00adscribes some \nlimitations. Finally related work is discussed in Sec\u00adtion 13 and Section 14 concludes. 2. Host language \nThis section discusses some aspects of Agda (Norell 2007; The AgdaTeam2007),the languageusedfortheexamplesinthepaper, \nin order to make it easier to follow the text. Agda is a dependently typed functional language, and for \nthe purposes of this paper it may be useful to think of it as a total variant of Haskell (Peyton Jones \n2003) with dependent types and generalised algebraic data types, but no in.nite values or coinduction. \nTHUNK is not tied to Agda, but can be implemented in any language which supports the type system and \nevaluation orders used, see Sections4and9. Hidden arguments Agda lacks (implicit) polymorphism,but has \nhidden arguments, which in combination with dependent types compensate for this loss. For instance, the \nordinary list function map couldbegiven the following type signature: map :{a, b:*}. (a . b) . List a \n. Listb Here * is the type of (small) types. Arguments within {...} are hidden, and need not be given \nexplicitly, if the type checker can infer their values from the context in some way. If the hidden arguments \ncannot be inferred, then theycan be given explicitly by enclosing them within {...}: map {Int}{Bool} \n:(Int . Bool) . List Int . List Bool The samesyntaxcanbeusedto patternmatchonhiddenarguments: map {a}{b} \nf (x :: xs) = ... Inductive families Agda has inductive families (Dybjer 1994), also known as generalised \nalgebraic data types or GADTs. Data types are introduced by listing the constructors and giving their \ntypes. Natural numbers, for instance, can be de.ned as follows: data N:* where zero :N suc :N . N As \nan example of a family of types consider the type Seq a n of sequences (sometimes called vectors) of \nlength n containing elements of type a: data Seq (a :*) :N . * where nil :Seq a zero (::) :{n :N}. a \n. Seq a n . Seq a (suc n) Note how the index (the natural number introduced after the last : in the \n.rst line) is allowed to vary between the constructors. Seq a isafamilyof types, with one type forevery \nindex n. To illustrate the kind of pattern matching Agda allows for an inductivefamily, let us de.ne \nthe tail function: tail :{a:*}.{n:N}. Seq a (suc n) . Seq a n tail (x :: xs) = xs We can and need only \npattern match on (::), since the type of nil does not match the type Seq a (suc n) given in the type \nsignature for tail. As another example, consider the de.nition of the append function: (+ ) :Seq a n1 \n. Seq a n2 . Seq a (n1 + n2) nil + ys = ys (x :: xs) + ys = x :: (xs + ys) In the nil case the variable \nn1 in the type signature is uni.ed with zero,transforming the result type into Seq a n2, allowingustogive \nys as the right-hand side. (This assumes that zero + n2 evaluates to n2.) The (::) case can be explained \nin a similar way. Note that the hidden arguments of (+ ) were not declared in its type signature. Thisis \nnot allowedby Agda,but often donein the paper to reduce notational noise. Some other minor syntactic \nchanges have also been made in order to aid readability. Run-time and compile-time code Agda evaluates \ncode during type checking; two types match if theyreduce to the same normal form. Hence it is useful \nto distinguish between compile-time code (code which is only evaluated at type-checking time) and run-time \ncode(codewhichisexecutedat run-time).Theprincipal purposeof the THUNK library is to annotate run-time \ncode; the compile-time code will not be executed at run-time anyway, so there is not much point in annotating \nit. Unfortunately Agda has no facilities for identifying compile\u00adtime or run-time code. As a crude .rst \napproximation types are not run-time, though. 3. Library basics An example will introduce the core concepts \nof THUNK. By using the library combinators the append function can be proved to be linear in the length \nof the .rst sequence: (+ ) : Seq a m . Seq a n . Thunk (1+ 2* m)(Seq a (m + n)) nil + ys = -return ys \n(x :: xs) + ys = \u00adxs + ys > = .xsys . \u00adreturn (x :: xsys) The rest of this section explains this example \nand the library in more detail. Ticks As mentioned above the user has to insert ticks manually: -:Thunk \nn a . Thunk (1+ n) a The basic unit of cost is the rewriting of the left-hand side of a de.nition to \nthe right-hand side. Hence, for every function clause, lambda abstraction etc. the user has to insert \na tick. (The -func\u00adtion is a pre.x operator of low precedence, reducing the need for parentheses.) Bydesign \nthe library is lightweight: no special language support for reasoning about time complexity is needed. \nIt would be easy to turn the library from being semiformal into being formal by modi\u00adfying the type-checker \nof an existing language to ensure that ticks were always inserted where necessary (and a few other require\u00adments \nlisted in Section 5). However, the primary intended use of THUNKis the analysis of complicated data structures; \nit should not interfere with ordinary code. Furthermore the freedom to choose where to insert ticks gives \nthe user the ability to experiment with different cost models. Thunk monad The type Thunk is an annotated \nmonad, with the following types for the unit(return)and the bind operator(> =): return :a . Thunk 0a \n(>>=) :Thunk m a . (a . Thunk nb) . Thunk (m+ n) b The monad combinators are used to combine the time \ncomplexities of subexpressions. It makes sense to call this a monad since the monoid laws for0and + make \nsure that the monad laws are still type correct . Time bounds Let us now discuss the time complexityguarantees \nestablished by the library. Assume that thas type a = Thunk n1 (Thunk n2...(Thunk nkb) . . .), where \nb is not itself equal to Thunk something. The library then guarantees that, if t evaluates to WHNF, then \nit does so in at most n = n1 + n2+ ... + nk steps. Denote the number n by time a. The precondition that \ntmustevaluateto WHNFis notaproblem inAgda,sinceAgdaisatotal language.Inpartial languagesonehas to be \nmore careful, though. Consider the following code, written in some partial language: . :N ticks :Thunk \n. a . = 1+ . ticks = -ticks The value ticks does not have a WHNF. Since Agda is total the precondition \nabove will implicitly be assumed to be satis.ed when the examples in the rest of the paper are discussed. \nOne can often extract more information than is at .rst obvious from the given time bounds. For instance, \ntake two sequences xs :Seq a 7andys :Seq a 3(for somea). When evaluating xs + ys a WHNF will be obtained \nin time (Thunk 15 (Seq a 10)) = 15 steps. This WHNF has to be z :: zs for some z : a, zs : Seq a 9. Since \ntime (Seq a 9) = 0this means thatzs evaluates to WHNF in zero steps.Continuing like this we see that \nxs + ys reallyevaluates to spine-normal form in 15 steps; even normal form if a does not contain embedded \nThunks. This example shows that types without embedded Thunks are treatedasiftheywere strict.Section7shows \nhow non-strict types can be handled. Run function There is a need to interface annotated code with ordinary \ncode, which does not run in the Thunk monad. This is done by the force function: force :Thunk n a . a \nThis function must of course not be usedin code which is analysed. Equality proofs The Agda type checker \ndoes not automatically prove arithmetical equalities. As a result, the de.nition of (+ ) above does not \ntype check: Agda cannot see that the tick count of the right-handsideofthelast equation,1+((1+2*m)+(1+0)) \n(for some variable m:N),is the same as1+ 2* (1+ m). This problem can be solved by inserting a proof demonstrating \nthe equality of theseexpressionsintothe code.TheproblemisanartifactofAgda, though; simple arithmetical \nequalities such as the one above could easily be proved automatically,and to aid readability no such \nproofs are written out in the paper, with the exception of a discussion of equality proofs in Section \n10. Summary The basic version of the library consists of just the Thunk monad, -, force, and the function \npay, which is introduced in Section 8; pay isthekeyto takingadvantageoflazyevaluation. The following \nlist summarises the primitives introduced sofar: Thunk :N . * . * -:Thunk n a . Thunk (1+ n) a return \n:a . Thunk 0a (>>=) :Thunk m a . (a . Thunk nb) . Thunk (m+ n) b force :Thunk n a . a 4. Implementation \nIn the implementation of THUNK the type Thunk n a is just a synonym for the type a; n is a phantom type \nvariable (Leijen and Meijer 1999). However, this equality must not be exposed to the library user. Hence \nthe type is made abstract: abstract Thunk :N . * . * Thunk n a = a Makingatype or function abstract means \nthat its de.ning equations are only visible to other abstract de.nitions in the same module. Hence, when \ntype checking, if x :Thunk n a, then this reduces to x :a in the right-hand sides of the library primitives \nbelow,but in other modules the two types a and Thunk n a are different. The primitive operations of the \nlibrary are basically identity functions; return and (>>=) form an annotated identity monad: abstract \n-:Thunk n a . Thunk (1+ n) a -x = x return :a . Thunk 0a return x = x (>>=) :Thunk m a . (a. Thunk nb) \n. Thunk (m + n) b x> = f = fx force :Thunk n a . a force x = x This ensures minimal run-time overhead, \nand also that the imple\u00admentation of THUNK corresponds directly to the erasure function used to prove \nthe library correct (see Section 9.1). Itmaybe possibleto implementavariantofthe libraryinastrict language \nwith explicit support for laziness (with memoisation). The correctness statement and proof, and perhaps \nalso some type signatures, would probably need to be modi.ed, though. 5. Conventions There are some conventions \nabout how the library must be used which are not captured by the type system: Every run-time function \nclause (including those of anonymous lambdas) has to start with -.  The function force may not be used \nin run-time terms.  Library functions may not be used partially applied.  The correctness of the library \nhas only been properly veri.ed for a simple language which enforces all these rules through syntac\u00adtic \nrestrictions (see Section 9.1); Agda does not, hence these con\u00adventions are necessary. Further differences \nbetween Agda and the simple language are discussed in Section 10. The rest of this section discusses \nand motivates the conventions. Run-time vs. compile-time It would be very awkward to have to deal with \nthunks in the types of functions, so the rules for \u00adonly apply to terms that will actually be executed \nat run-time. The function force may obviously not be used in run-time terms, since it can be used to \ndiscard annotations. Ticks everywhere One might think that it is possible to omit \u00adin non-recursive de.nitions, \nand still obtain asymptotically correct results. This is not true in general, though. Consider the following \nfunction, noting that the last anonymous lambda is not ticked: build:(n :N) . Thunk (1+ 2* n)(N . Thunk \n1N) buildzero = -return (.n . -return n) build(suc n) = \u00ad build n > = .f . \u00ad return (.n . f (suc n)) \nThe function build n, when forced, returns a function f : N . Thunk 1N which adds n to its input. However, \nf is not a constant\u00adtime function, so this is clearly wrong. The problem here is the lambda which we \nhave not paid for. Partial applications The guaranteesgivenbyTHUNKareveri.ed by de.ning a function '\u00b7' \nwhich erases all the library primitives, and then showing that, for every term t whose time is n, the \nerased term 't' takes at most n steps amortisedtimetoevaluatetoWHNF (see Section 9). Now,'returnt' = \n't', so if partial applications of library func\u00adtions were allowed we would have 'return' = .x . x. However, \nan application of the identity function takes one step to evaluate, whereas return has zero overhead. \nHence partial applications ofli\u00adbrary functions are not allowed. (It may be useful to see them as annotations, \nas opposed to .rst-class entities.) 6. Some utility functions Before moving on to some larger examples \na couple of utility functions will be introduced. When de.ning functions which have several cases the \ntypes of the different case branches have to match. For this purpose the following functions, which increase \nthe tick counts of their arguments, are often useful: wait :(n :N) . Thunk m a . Thunk (1+ n + m) a wait \nzero x = -x wait (suc n) x = -wait n x returnw :(n :N) . a . Thunk (1+ n) a returnw zero x = -return \nx returnw (suc n) x = -returnw n x Note that returnw cannot be de.ned in terms of wait;the extra tick \nwould give rise to a different type: returnw :(n :N) . a . Thunk (2+ n) a returnw n x = -wait n (return \nx) Note also that, to improve performance (as opposed to time bounds), it is a good idea to add these \nfunctions to the trusted code base: abstract wait :(n :N) . Thunk m a . Thunk (1+ n+ m) a wait x = x \nreturnw :(n :N) . a . Thunk (1+ n) a returnw x = x This does not increase the complexity of the main \ncorrectness proof, since we know that the functions could be implemented in the less ef.cient way above. \nThe function (=<<), bind with the arguments .ipped, is also included in the trusted core: (=<<) :(a . \nThunk mb) . Thunk n a . Thunk (n + m) b f =< c = c > = f This function does not add any overhead to the \ncorrectness proof since it is identical to bind (except for a different argument order). Furthermore \nit is useful; it is used several times in the next section. The following thunki.ed variant of if-then-else \nwill also be used: if then else :Bool . a . a . Thunk 1a if true then x else y= -return x if false then \nx else y= -returny 7. Non-strict data types Data types de.ned in an ordinary way are treated as strict. \nIn order to get non-strict behaviour Thunk has to be used in the de.nition of the data type. To illustrate \nthis a linear-time function which calculatesthe minimum elementina non-emptylistwillbe de.ned by using \ninsertion sort. First lazy sequences are de.ned: data SeqL (a :*) (c :N) :N . * where nilL :SeqL ac 0 \n(::L) :a . Thunk c (SeqL acn) . SeqL ac (1+ n) SeqL acn stands for a lazy sequence of length n, containing \nelements of type a, where every tail takes c steps to force; note the use of Thunk in the de.nition of \n(::L).A variant where different tails take different numbers of steps to force is also possible (see \nSection 11),but not needed here. The function insert inserts an element into a lazy sequence in such \na way thatif the input is sorted the output will also be sorted. To compare elements insert uses the \nfunction (=) : a . a . Thunk 1 Bool;for simplicity it is assumed that comparisons take exactly one step.2 \ninsert : {c :N}. a . SeqL acn . Thunk 4(SeqL a (4+ c)(1+ n)) insert {c} x nilL = \u00ad returnw 2(x::L returnw \n(3+ c) nilL) insert {c} x (y::L ys) = \u00ad x = y> = .b. \u00ad if bthen x ::L wait (2+ c)(waitL 2(y::L ys)) else \ny::L (insert x =< ys) When x = ythe function waitL is used to ensure that the resulting sequence has \nthe right type: ' waitL : (c :N) . SeqLacn . Thunk 1(SeqL a (2+ c + c') n) waitL c nilL = -return nilL \nwaitL c (x ::L xs) = -return (x ::L wait c (waitL c=< xs)) By using waitL all elements in the tail get \nassigned higher tick counts than necessary. It would be possible to give insert a more precise type which \ndid not overestimate any tick counts, but this type would be rather complicated. The type used here is \na compro\u00admise which is simple to use and still precise enough. Note that the library does not give any \nhelp with solving re\u00adcurrence equations; it just checks the solution encoded by the user through type \nsignatures and library primitives. (The arguments to functions like wait can often be inferred automatically \nin Agda, obviating theneed for the user to write them.For clarity theyare included here, though.) 2Agda \nhas parameterised modules, so (=) does not need to be an explicit argument to insert. Insertion sort, \nwhich takes an ordinary sequence as input but gives a lazy sequence as output, can now be de.ned as follows: \nsort :Seq a n . Thunk (1+ 5* n)(SeqL a (4* n) n) sort nil = -return nilL sort (x :: xs) = -insert x =< \nsort xs Note that the time needed to access the .rst element of the result is linear in the length of \nthe input, whereas the time needed to force the entire result is quadratic. Using sort and head the minimum \nfunction can easily be de.ned for non-empty sequences: head :SeqL ac (1+ n) . Thunk 1a head (x ::L xs) \n= -return x minimum :Seq a (1+ n) . Thunk (8+ 5* n) a minimum xs = -head =< sort xs As a comparison \nit can be instructive to see that implementing maximum using insertion sort and last can lead to quadratic \nbe\u00adhaviour: last :SeqL ac (1+ n) . Thunk (1+ (1+ n) * (1+ c)) a last (x ::L xs) = -last ' x =< xs where \nlast ' :a . SeqL acn . Thunk (1+ n * (1+ c)) a last ' x nilL = -return x last ' x (y::L ys) = -last ' \ny=< ys maximum :Seq a (1+ n) . Thunk (13 + 14 * n+ 4* n 2) a maximum xs = -last =< sort xs Fortunately \nthere are better ways to implementthis function. 8. Essential laziness The time bound of the minimum \nfunction only requires non\u00adstrictness,not memoisation.Tomakeuseof lazinessto obtain better time bounds \npay can be used: abstract pay :(m :N) . Thunk n a . Thunk m (Thunk (n - m) a) pay x = x (Here n - m = \n0whenevern< m.) The correctness of pay is obvious, since time (Thunk n a) = time (Thunk m (Thunk (n - \nm) a)). However, more intuition may be provided by the following inter\u00adpretations of pay: 1. When pay \nmt is executed (as part of a sequence of binds) the thunk tis executed for m steps and then suspended \nagain. 2. When pay m t is executed the thunk t is returned immediately, but with a new type. Ift is \nnever forced, then we have paid m steps too much. If t is forced exactly once, then we have paid the \nright amount. And .nally, if tis forced several times, then it is memoisedthe .rsttimeand laterthe memoisedvalueisused, \nso the amount paidis stilla correct upperbound.  The .rst way of thinking about pay may be more intuitive. \nFur\u00adthermore, if it could be implemented, it would lead to worst-case, instead of amortised, time bounds \n(assuming a suitably strict se\u00admantics). However, the extra bookkeeping needed by the .rst ap\u00adproach \nseems to make it hard to implement without non-constant overheads; consider nested occurrences of pay. \n8.1 Implicit queues The interpretations above do not explain whypay is useful.To do thisIwill implementimplicitqueues \n(Okasaki 1998), FIFO queues with constant-time head, snoc and tail.3 In this example using pay corresponds \nto paying off so-called debits in Okasaki s banker s method (1998), hence the name. When using theTHUNKlibrary \ndebits are representedexplicitly using thunked arguments in data type de.nitions. Implicit queues are \nrepresented by the following nested data type: data Q(a:*) :* where empty :Qa single :a . Qa twoZero \n:a . a . Thunk 5(Q(a \u00d7 a)) . Qa twoOne :a . a . Thunk 3(Q(a \u00d7 a)) . a . Qa oneZero :a . Thunk 2(Q(a \u00d7 \na)) . Qa oneOne :a . Q(a \u00d7 a) . a . Qa The recursive constructors take queues of pairs of elements, placed \nafter the .rst one or two elements, andbefore the last zero or one elements. Okasaki s analysis puts \na certain number of debits on the various subqueues. These invariants are re.ected in the thunks above \n(modulo some details in the analysis). The snoc function adds one element to the end of a queue. Okasaki \ns analysis tells us that this function performs O(1) un\u00adsharedworkand dischargesacertain numberof debits.Wedonot \nneedtokeep thesetwo concepts separate(even thoughwe could), hence the following type for snoc: snoc :Qa \n. a . Thunk 5(Qa) snoc empty x1 = -returnw 3(single x1) snoc (single x1) x2 = \u00ad returnw 3(twoZero x1 \nx2 (returnw 4empty)) snoc (twoZero x1 x2 xs3) x4 = \u00ad ' pay 2xs3 > = .xs3 . \u00ad' returnw 0(twoOne x1 x2 \nxs3 x4) snoc (twoOne x1 x2 xs3 x4) x5 = \u00ad ' xs3 > = .xs3 . \u00ad' return (twoZero x1 x2 (snoc xs3 (x4, x5))) \nsnoc (oneZero x1 xs2) x3 = \u00ad ' xs2 > = .xs2 . \u00ad' returnw 0(oneOne x1 xs2 x3) snoc (oneOne x1 xs2 x3) \nx4 = \u00ad pay 3(snoc xs2 (x3, x4)) > = .xs234 . \u00ad return (oneZero x1 xs234) Note howthe invariants encoded \nin the data structure, together with the use of pay, ensure that we can show that the function takes \nconstant amortised time even though it is recursive. Note also that using call-by-value or call-by-name \nto evaluate snoc leads to worse time bounds. Consider a saturated queue q, builtupby repeated applicationofsnoc \nto empty: q= twoOne xx (twoOne (x, x)(x, x) (... empty ...) (x, x)) x Inastrict settingittakes O(d) steps \ntoevaluate snocqx,wheredis the depth of the queue. If snocqxisevaluated ktimes, thiswill take O(kd) steps, \nand by choosing khigh enough it can be ensured that theaverage numberof steps neededby snoc is not constant. \nIf call\u00adby-name is used instead, then the lack of memoisation means that q = snoc (snoc (. . . empty \n. . .) x) x will be evaluated to WHNF each time snocqx is forced, leading to a similar situation. It \nremains to de.ne the view-function (view left), which gives the .rst element and the rest of the queue.Forcing \nthe tail takes longer than just viewing the head, so the following data type is de.ned to wrap up the \nresult of view -: 3The presentationusedhereisduetoRossPaterson (personal commu\u00adnication), with minor \nchanges. data View (a:*) :* where - nil :View a cons :a . Thunk 4(Qa) . View a The function itself is \nde.ned as follows: view :{a :*}. Qa . Thunk 1(View a) view empty = -return nil view (single x1) = \u00ad - \nreturn (cons x1 (returnw 3empty)) - view (twoZero x1 x2 xs3) = -return (cons x1 -' \u00ad (pay 3xs3 > = .xs3 \n. \u00ad ' return (oneZero x2 xs3))) view (twoOne x1 x2 xs3 x4) = -return (cons x1 -' \u00ad (xs3 > = .xs3 . \u00ad \n' return (oneOne x2 xs x4))) 3 view {a} (oneZero x1 xs2) = \u00ad - return (cons x1 (expand =< view =< xs2)) \nwhere expand :View (a \u00d7 a) . Thunk 1(Qa) - expand nil = -return empty - expand (cons (y1, y2) ys3) = \n\u00ad - return (twoZero y1 y2 (wait 0ys3)) view {a} (oneOne x1 xs2 x3) = \u00ad - return (cons x1 (expand =< view \nxs2)) where expand :View (a \u00d7 a) . Thunk 3(Qa) - expand nil = -returnw 1(single x3) - expand (cons (y1, \ny2) ys3) = \u00ad -' pay 1ys3 > = .ys3 . \u00ad' return (twoOne y1 y2 ys x3) 3  8.2 Calculating invariants It \nshould be noted that the library does not help much with the design of ef.cient data structures, except \nperhaps by providing a clear model of certain aspects of lazy evaluation. It may still be instructive \nto see how the invariants used above can be obtained. Assuming that the general structure of the code \nhas been decided, that the code is expected to be constant-time, and that the number of debits on all \nthe subqueues is also expected to be constant, this is how it can be done: 1. Make all the subqueues \nthunked, also the one in the oneOne constructor. 2. Denotethetime boundsandthenumberof debitsonthevarious \nsubqueuesbyvariables.For instance:  oneZero :a . Thunkd10 (Q(a\u00d7 a)) . Qa oneOne :a . Thunkd11 (Q(a\u00d7 \na)) . a . Qa 3. Assume a worst case scenario for how many pay annotations etc. are necessary. Calculate \nthe amounts to pay using the vari\u00adables introducedin the previous step.For instance, the oneOne case \nof snoc takes the following form, if s is the time bound for snoc: snoc (oneOne x1 xs2 x3) x4 = \u00ad ' xs2 \n> = .xs2 . \u00ad' pay (s - d10)(snoc xs2 (x3, x4)) > = .xs234 . \u00adreturn?(oneZero x1 xs234) (The function \nreturn?could be either return or returnw n, for some n, depending on the outcome of the analysis.) 4. \nThe basic structure of the code now gives rise to a number of inequalities which haveto be satis.ed in \norder for the code to be well-typed.For instance, the oneOne case of snoc gives rise to the inequality3+ \nd11 + (s- d10) = s. Solve these inequalities. If no solution can be found, then one of the assumptions \nabove was incorrect. 5. Optionally: If, for instance, a pay annotation was unnecessary, then it may be \npossible to tighten the time bounds a little, since a tick can be removed. 9. Correctness The correctness \nof the library is established as follows: 1. Two small languages are de.ned: the simple one without the \nThunk type, and the thunked one with the library functions as primitives. An erasure function '\u00b7' converts \nthunked terms to simple ones. 2. Alazy operational semantics is de.ned for the simple language, and \nanother operational semantics is de.ned for the thunked language. It is shown that, under erasure, the \nthunked semantics is equivalent to the simple one. 3. TheTHUNK library guarantees (seeSection3) are \nestablished for the thunked semantics. Since the two semantics are equiva\u00adlent this implies that the \nguarantees hold for erased terms eval\u00aduated using the simple semantics.  As shownin Section4the libraryisimplementedby \nignoring all annotations. Hence what is actually run corresponds directly to the erased terms mentioned \nabove, so the correctness guarantees extend also to the actual library (assuming that Agda has an oper\u00adational \nsemantics corresponding to the one de.ned for the simple language; currently Agda does not have an operational \nsemantics). There are two caveats to this statement. One is the time needed to evaluate the library functions. \nHowever, they all evaluate in con\u00adstanttime,andI.ndit reasonabletoignorethesetimes.Theother is the difference \nbetween the small languages de.ned here and a full-scale language like Agda. These differences are discussed \nfur\u00adther in Section 10. All nontrivial results discussed in this section have been proved formally using \nAgda.4 There are some differences between the for\u00admalisation presented here and the mechanised one, most \nnotably that de Bruijn indices are used to represent variables in the mech\u00adanisation. The veri.cation \nof some of the extensions discussed in Sections 10 11 have also been mechanised.For more details, see \nDanielsson (2007). 9.1 Languages Both of the two small languages are simply typed lambda calculi with \nnatural numbers and products. Using dependently typed lan\u00adguages for the correctness proofwouldbe possible,but \nthis aspect of the type systems appears to be largely orthogonal to the correct\u00adness result. Furthermore \nit would have been considerably harder to mechanisetheproofs.HenceIchosetousesimplytyped languages. The \nsyntax of contexts, types and terms for the simple language is de.ned as follows (with x, yvariables): \nl ::= E | l, x:t t ::= Nat | t1 \u00d7 t2 | t1 . t2 t ::= x | .x.t | t1 \u00b7 t2 | (t1, t2) | uncurry (.xy.t) \n| z | s t | natrec t1 (.xy.t2) Here natrec is the primitive recursion combinator for natural num\u00adbers, \nand uncurry is the corresponding combinator forproducts. 4Agda currently does not check that all de.nitionsby \npattern matching are exhaustive. Hence care has been taken to check this manually. Common typing rules \nl(x)= tl I t1:t1 . t2 l I t2:t1 l I x :t l I t1 \u00b7 t2:t2 l, x :t1 I t:t2 l I t1 :t1 l I t2 :t2 l I .x.t:t1 \n. t2 l I (t1, t2) :t1 \u00d7 t2 l, x :t1, y:t2 I t:t l I uncurry (.xy.t) :t1\u00d7 t2 . tl I z :Nat l I t:Nat \nl I t1:t l, x :Nat, y:t I t2:t l I s t:Nat l I natrec t1 (.xy.t2) :Nat . t Extra typing rules for the \nthunked language l I t:Thunk n tl I t:t l I -t:Thunk (1+ n)t l I return t:Thunk 0t l I t1:Thunk n1 t1 \nl I t2:t1 . Thunk n2 t2 l I t1 > = t2:Thunk (n1 + n2)t2 l I t:Thunk n t l I force t:t l I t:Thunk n \nt l I pay mt:Thunk m (Thunk (n - m) t) Figure 1. The type systems. All freshness side conditions have \nbeen omitted. The thunked language extends the syntax with the library prim\u00aditives as follows: t ::= \n... | Thunk n t t ::= ... | -t | return t | t1> = t2 | force t | pay nt Here n stands for a natural number, \nnot a term of type Nat. Thetype systemsforthetwolanguagesaregiveninFigure1.In the remaining text only \nwell-typed terms are considered.No type annotations are present in the syntax above,but this is just \nto sim\u00adplify the presentation. The mechanised versions of the languages are fully annotated. As noted \nabove an erasure operation taking types and terms from the thunked language to the simple one is de.ned: \n'Nat' = Nat 't1 \u00d7 t2 ' = 't1 ' \u00d7 't2 ' 't1 . t2 ' = 't1 ' . 't2 ' 'Thunk n t ' = 't ' 'x' = x '.x.t' \n= .x.'t' 't1\u00b7 t2 ' = 't1 ' \u00b7 't2 ' '(t1, t2)' = ('t1 ', 't2 ') 'uncurry (.xy.t)' = uncurry (.xy.'t') \n'z' = z 's t' = s 't' 'natrec t1 (.xy.t2)' = natrec 't1 ' (.xy.'t2 ') '-t' = 't' 'return t' = 't' 't1> \n= t2 ' = 't2 ' \u00b7 't1 ' 'force t' = 't' 'pay nt' = 't' Erasure extends in a natural way to contexts, and \nterm erasure can easily be veri.ed to preserve types, l I t:t . 'l' I 't' :'t '. Free use of force orfailureto \ninsert tickswouldinvalidateall time complexity guarantees, so a subset of the thunked language is de.ned, \nthe run-time terms: e::= x | .x. -e | e1 \u00b7 e2 | (e1, e2) | uncurry (.xy. -e) | z | s e | natrec ( -e1) \n(.xy. -e2) | -e | return e | e1 > = e2 | pay ne Notethatalltheconventionssetupin Section5are satis.edbythe \nrun-time terms: every right-hand side starts with a tick, force is not used, and library functions cannot \nbe used partially applied. 9.2 Operational semantics Let us now de.ne the operational semantics for \nthe two languages. The semantics, which are inspired by Launchbury s semantics for lazy evaluation (1993), \nde.ne how to evaluate a term to WHNF. Simple semantics We begin with the semantics for the simple language.Terms \nareevaluatedinheaps(orenvironments); listsof bindings of variables to terms: . ::=\u00d8| ., x . t Heaps have \nto be well-typed with respect to a context: l I .l I t:t E I\u00d8 (x fresh) l, x :t I ., x . t Note that \nthese rules ensure that there are no cycles in the heap. This is OK since there is no recursive let allowing \nthe de.nition of cyclic structures,andevenif there werea recursive let theTHUNK library would not be \nable to make use of the extra sharing anyway. Asubset of the terms are identi.ed as being values: v::= \n.x.t | (x1, x2) | uncurry (.xy.t) | z | s x | natrec x1 (.xy.t2) Note that these values are all in WHNF. \nSeveral of the constructors takevariablesasarguments;thisisto increase sharing.Onceagain, THUNK cannot \ntake advantage of this sharing,butI have tried to keep the semantics close to what a real-world lazy \nlanguage would use. (Furthermore the generalised version of the library, described in Section 11, can \ntake advantage of some ofthis sharing.) The big-step operational semantics for the simple language is \ninductively de.ned in Figure 2. The notation .1 | t .n .2 | v means that t, when evaluated in the heap \n.1, reaches the WHNF v in nsteps; the resulting heap is .2. (Here it is assumed that .1and t are well-typed \nwith respect to the same context.) In order to reduce duplication of antecedents an auxiliary relation \nis used to handle application: .1 | v1 x2 .n .2 | v means that the application of the value v1 to the \nvariable x2 evaluates to v in n steps, with initial heap .1 and .nal heap .2. In the description of the \nsemantics all variables are assumed to be globally unique (by renaming, if necessary). The mechanised \nversion of the semantics uses de Bruijn indices, so name clashes are not an issue there. The semantics \nis syntax-directed, and hence deterministic. Fur\u00adthermore types are preserved, l1 I .1 . l1 I t:t . .1 \n| t .n .2 | v . . l2.l2 I .2 . l2 I v :t. In the mechanisation this is true by construction. It is easy \nto make small mistakes when formalising languages, and working Values | .x.t .0 | .x.t | (t1, t2) .0 \n, x1 . t1, x2 . t2 | (x1, x2) | uncurry (.xy.t) .0 | uncurry (.xy.t) | z .0 | z | s t .0 , x . t | s \nx | natrec t1 (.xy.t2) .0 , x1 . t1 | natrec x1 (.xy.t2) Variables 1 | t .n 2 | v '' 1, x . t, | x .n \n2, x . v, | v Application 1 | t1 .n12 | v12, x2 . t2 | v1 x2 .n23 | v 1 | t1\u00b7 t2 .n1+n23 | v 1 | t1[x \n:= x2] .n 2 | v 1 | (.x.t1) x2 .1+n 2 | v 1 | x2 .n12 | (x3, y3) .n2 2 | t1[x := x3, y:= y3] 3 | v \n1 | uncurry (.xy.t1) x2 .1+n1+n23 | v 1 | x3 .n12 | z 2 | x1 .n23 | v 1 | natrec x1 (.xy.t2) x3 .1+n1+n23 \n| v ' 1 | x3 .n12 | s x ' 2, y. natrec x1 (.xy.t2) \u00b7 x | t2[x := x ' ] .n23 | v 1 | natrec x1 (.xy.t2) \n x3 .1+n1+n23 | v Figure 2. Operational semantics for the simple language. Note that only reductions \n(rules with in the left-hand side) contribute to the cost of a computation. with well-typed syntax is \nnice since many mistakes are caught early. The cost model used by the semantics is that of the THUNK \nlibrary: only reductions cost something. Nothing is charged for looking up variables (i.e. following \npointers into the heap), for instance. Thunked semantics Now on to the thunked semantics, which only \napplies to run-time terms. In order to be able to prove cor\u00adrectness the thunked semantics has more structure \nin the heap: ::=\u00d8| , x . e | , x .ne As before, only well-typed heaps are considered: E I \u00d8 l I l, x \n:t I l I e :t , x . e (x fresh) l I l I e :Thunk n t (x fresh) l, x :t I , x .ne The x .ne bindings \nare used tokeep track of terms which have already beenpaidoff,butnotyetevaluated.The credit associated \nwith a heap is the total tick count of such bindings: credit \u00d8= 0 credit (., x . e) = credit credit (., \nx .ne) = credit + n The credit will be used to state the correctness result later. The thunked semantics \nuses the following values, which are all run-time: v::= .x. -e | (x1, x2) | uncurry (.xy. -e) | z | s \nx | natrec ( -x1) (.xy. -e2) | returnnv Here returnnv stands for n applications of -to return v. The \nthunked semantics, denoted by 1 | e in 2 | v, is givenin Figure3(and presented with the same assumptions \nas the previous one). The thunked semantics preserves types, analogously to the simple one. Note that \nonly binds(> =)introduce bindings of the form x .ne, and that when a variable x bound like this is evaluated, \nit is updated with an unannotated binding; this memoisationis the one trackedby the library. The following \nsmall example illustrates what can be derived using the thunked semantics: \u00d8, x .1 (.y. -returny) \u00b7 z \n| x i1 \u00d8, y. z, x . z | z. Note that x :Nat and time Nat = 0,but theevaluation takes one step; the change \nin heap credit pays for this step (compare with the invariant in Section 9.3). Equivalence The thunked \nsemantics is both sound, 1 | e in 2 | v . ' 1 ' | 'e' .n ' 2 ' | 'v', and complete, ' 1 ' | 'e' .n 2 \n| v . ''' '' . 2, v . ' 2 ' = 2 . 'v ' ' = v . 1 | e in 2 | v , with respect to the simple one. (Here \nerasure has been extended in the obvious way to heaps.) These properties are almost trivial, since the \nrules for the two semantics are identical up to erasure, and can be proved by induction over the structure \nof derivations. Some auxiliary lemmas, such as 'e[x := y]' = 'e'[x := y], need to be proved as well. \n 9.3 Time complexity guarantees Now that we know that the two semantics are equivalent the only thing \nremaining is to verify the time complexity guarantees for the thunked semantics. It is straightforward \nto prove by induction over the structure of derivations that the following invariant holds: l I e :t \n1 | e in 2 | v credit 2 + n = credit 1 + time t (The time function was introduced in Section 3.) Note \nthat when a computation is started in an empty heap this invariant implies that n = time t ,i.e.thetime \nboundgivenbythetypeisan upper bound on the actual number of computation steps. In the general case the \ninequality says that time t is an upper bound on the actual number of steps plus the increase in heap \ncredit (which may sometimes be negative), i.e. time t is an upper bound on the amortised time complexity \nwith respect to the heap credit. By using completeness the invariant above can be simpli.ed: l I e:t \n' 1 ' | 'e' .n 2 | v n = credit 1+ time t Values | .x. -e i0 | .x. -e | (e1, e2) i0 , x1 . e1, x2 . \ne2 | (x1, x2) | uncurry (.xy. -e) i0 | uncurry (.xy. -e) | z i0 | z | s e i0 , x . e | s x | natrec ( \n-e1) (.xy. e2) i0 , x1 . e1 | natrec ( -x1) (.xy. e2) Variables m 1 | e in 2 | v 1 | e in 2 | returnv \n'' '' 1, x . e, | x in 2, x . v, | v 1, x .me, | x in 2, x . v, | v Library primitives m m1 v 1+m 0 1 \n| e in 2 | returnv 1 | e in 2 | v 1 | e in 2 | return 1 | -e in 2 | returnv 1 | return e in 2 | returnv \n1 | pay m2 e in 2 | returnm2 (returnm1-m2 v) m2 v 1 | e2 in12 | v22, x1 .m1 e1 | v2 x1 in23 | return \n(if l1 I e1:Thunk m1 t1) 1 | e1 > = e2 in1+n23 | returnm1+m2 v Application m in 1 | e1 in12 | v12, \nx2 . e2 | v1 x2 in23 | v 1 | e1[x := x2] 2 | returnv 1 | e1\u00b7 e2 in1+n23 | v 1 | (.x. -e1) x2 i1+n 2 \n| return1+mv m in2 1 | x2 in12 | (x3, y3) 2 | e1[x := x3, y:= y3] 3 | returnv 1 | uncurry (.xy. -e1) \n x2 i1+n1+n23 | return1+mv m 1 | x3 in12 | z 2 | x1 in23 | returnv 1+m 1 | natrec ( -x1) (.xy. -e2) \n x3 i1+n1+n23 | returnv ' -' m 1 | x3 in12 | s x 2, y. natrec ( -x1) (.xy. e2) \u00b7 x | e2[x := x ' ] in23 \n| returnv 1+m 1 | natrec ( -x1) (.xy. -e2) x3 i1+n1+n23 | returnv Figure 3. Operational semantics for \nthe thunked language. If the right-hand side of an antecedent is returnmv, then the only possible type-correct \nvalues are returnmv (for suitable m and v); this can be seen as a form of pattern matching. This statement \ndoes not refer to the thunked semantics,but is not compositional, since 2 does not carry anycredit. 10. \nExtensions This section discusses some possible extensions of the simple lan\u00adguages used to prove the \nlibrary correct. These extensions are meant to indicate that the correctness proof also applies to a \nfull\u00adscale language such as Agda. Partial applications Partial applications of library primitives were \ndisallowed in Section 5. Other partial applications are al\u00adlowed, though. As an example, two-argument \nlambdas can be in\u00adtroduced (with the obvious typing rules): t::= ... | .xy.tv::= ... | .xy.t e::= ... \n| .xy. -ev ::= ... | .xy. -e The operational semantics are extended as follows: | .xy.t .0 | .xy.t | \n(.xy.t1) x2 .0 | .y.t1[x := x2] | .xy. e i0 | .xy. e - | (.xy. -e1) x2 i0 | .y. e1[x := x2] The proofs \nof equivalence and correctness go through easily with these rules. Note that nothing is charged for the \napplications above; only whenallargumentshavebeen supplied(andhenceevaluationofthe right-hand side can \ncommence) is something charged. If this cost measure is too coarse for a certain application, then two-argument \nlambdas should not be used. (Note that .x. -.y. -e still works.) Partial applications of constructorscan \nbe treatedsimilarly; in the mechanised correctness proof the successor constructor s is a function of \ntype Nat . Nat. Inductive types Theexamples describingtheuseofTHUNKmade use of various data types. Extending \nthe languages with strictly positive inductive data types orfamilies (Dybjer 1994) should be straightforward, \nfollowing the examples of natural numbers and products. Equality When THUNK is implemented using a dependently \ntypedlanguagesuchasAgda,one inductivefamilydeserves further scrutiny: the equality (or identity) type. \nIn practice it is likely that users of the THUNK library need to prove that various equalities hold.As \nanexample,in the appendexamplegivenin Section3the equality 1+ ((1+ 2* m) + (1+ 0)) = 1+ 2* (1+ m) must \nbe established in order for the program to type check. If the host language type checker is smart enough \ncertain such equalities may well be handled automatically using various decision procedures, but in the \ngeneral case the user cannot expect all equalities to be solved automatically. One way to supply equality \nproofs to the type checker is to use the equality type (=) together with the subst function, which expresses \nsubstitutivity of (=): data (=)(x:a) :a . * where re. :x = x subst :(P:a . *) . x = y. Px . Py substPre. \np= p Assuming a proof lemma :(m :N) . 1+ ((1+ 2* m) + (1+ 0)) = 1+ 2* (1+ m) the de.nition of (+ ) \ncan be corrected: (+ ) : {a:*}.{m, n:N} . Seq a m . Seq a n . Thunk (1+ 2* m)(Seq a (m+ n)) (+ ) nil \nys = -return ys (+ ) {a}{suc m}{n} (x:: xs) ys = subst (.x . Thunk x (Seq a (suc m+ n))) (lemma m) ( \n-xs + ys > = .xsys . \u00adreturn (x:: xsys)) However, now subst and lemma interfere with the evaluation of \n(+ ), so the stated time complexity is no longer correct. One way to address this problem would be to \nlet subst cost one tick, and also pay for the equality proofs, just as if (=) was any other inductivefamily.However, \nthisisnot whatwewanttodo. We just want to use the proofs to show that the program is type correct, we \ndo not want to evaluate them. A better solution is to erase all equality proofs and inline the identity \nfunction resulting from subst (and do the same for similar functions derived from the eliminator of (=)). \nThis is type safe as long as the underlying logical theory is consistent and only closed terms are evaluated, \nsince then the only term of type x = yis re. (and only if x = y). Then subst (fully applied) can be used \nfreely by the user of the library, without having to worry about overheads not tracked by the thunk monad. \nImplementing proof erasure just for this library goes against the spirit of the project, though, since \nmodifying a compiler is not lightweight.Fortunately proof erasure is an important, general problem in \nthe compilation of dependently typed languages (see for instance Brady et al. 2004; Paulin-Mohring 1989), \nso it is not unreasonable to expect a good compiler to guarantee that the erasure outlined above willalways \ntakeplace. Functions like subst have been used in the case studies accom\u00adpanying this paper. Fixpoints \nThe simple languages introduced aboveare most likely terminating, sincetheyarevery similartoG\u00a8 odel s \nSystemT.How\u00adever, nothing stops us from adding a .xpoint combinator: t::= ... | .x (.x.t) e ::= ... | \n.x (.x. -e) l, x :t I t:t l I .x t:t . t 1, x . .x (.x.t) | t .n 2 | v 1 | .x (.x.t) .1+n 2 | v - 1, \nx . .x (.x. e) | e in 2 | v - 1 | .x (.x. e) i1+n 2 | v The mechanised correctness proof also includes \n.xpoint operators, and theydo not complicate the developmentat all. There is one problem with unrestricted \n.xpoint operators, though: they make logical systems inconsistent. This invalidates the equality proof \nerasure optimisation discussed above, and hence including an unrestricted .x may not be a good idea, \nat least not in the context of Agda. 11. Paying for deeply embedded thunks THUNK, as described above, \nhas an important limitation: it is im\u00adpossible to pay for thunks embedded deep in a data structure, with\u00adout \na large overhead. This section describes the problem and out\u00adlines a solution. The problem Letus generalisethelazy \nsequencesfrom Section7 by letting the cost needed to force tails vary throughout the se\u00adquence: CostSeq \n:N . * CostSeq n = Seq N n data SeqL (a :*) :(n:N) . CostSeq n . * where nilL : SeqL a 0nil (::L) : a \n. Thunk c (SeqLan cs) . SeqL a (1+ n)(c :: cs) Here SeqLan cs standsforalazy sequenceof length n, containing \nelements of type a, where the costs needed to force the tails of the sequence is given by the elements \nof cs. Now, assume that xs :SeqLan cs, where cs = 0::0::... ::0::2::4:: ... :: nil. Assume further that \nthe analysis of an algorithm requires that the .rst debit in xs is paid off, resulting in xs ' :SeqLan \ncs ' where ' cs = 0::0::... ::0::1::4:: ... :: nil. In order to accomplish this the type of a tail embeddeddeep \ndown in xs needstobe changed. This requiresarecursivefunction,which does not take constant time to execute, \nand this is likely to ruin the analysis of the algorithm. The solution Away around this problem is to \ngeneralise the type of pay: payg : (C:Ctxt) . (m :N) . C[Thunk n a]. Thunk m (C[Thunk (n- m) a]) Here \nC is a context enabling payments deep down in the data structure. These contexts have to be quite restrictive, \nto ensure correctness of the analysis.For instance, theyshould have at most one hole, to ensure that \nonly one thunk is paid off. Similarly one should not be allowed to pay offthe codomain of a function, \nor the element type of a list; the following types are clearly erroneous: payFun : (m :N) . (a . Thunk \nnb) . Thunk m (a . Thunk (n - m) b) payList : (m :N) . List (Thunk n a) . Thunk m (List (Thunk (n - \nm) a)) For instance, ifpayList were allowed one could take a list with n elements, all oftype Thunk 1N, \nand obtain a List (Thunk 0N) by just paying for one computation step, instead ofn. To avoid such problems \nthe following type of contexts is de\u00ad.ned: data Ctxt :*1 where :Ctxt const :* . Ctxt Thunk :N . Ctxt \n. Ctxt ( \u00d7) :Ctxt . * . Ctxt (\u00d7 ) :* . Ctxt . Ctxt (The type *1 is a type of large types.) These contexts \ncan be turned into typesby instantiating the holes: \u00b7 [\u00b7 ]:Ctxt . * . * [b] = b (const a) [b] = a (Thunk \nnC) [b]= Thunk n (C[b]) (C \u00d7 a) [b] = C[b]\u00d7 a (a \u00d7 C) [b] = a \u00d7 C[b] This de.nition of contexts may \nat .rst seem rather restrictive, since no recursive type constructors are included. However, when using \ndependent types one can de.ne new types by explicit recur\u00adsion.Avariant of SeqL can for instance be de.ned \nas follows: SeqL :* . CostSeq n . * SeqL a nil = Unit SeqL a (c :: cs) = a \u00d7 Thunk c (SeqL a cs) (Here \nUnit is the unit type.) By using this type and payg it is now possible to pay offanyof the tails in the \nsequence with only constant overhead: payL : {a :*}. (cs1:CostSeq n1) . (c ' :N) . SeqL a (cs1 + c :: \ncs2) . Thunk (1+ c ' )(SeqL a (cs1+ (c - c ' ) :: cs2)) '  payL {a} cs1 c xs = \u00ad' cast lemma2 (payg \n(Ca cs1) c (cast lemma1 xs)) where C:* . CostSeq n . Ctxt Ca nil = a \u00d7 Ca (c :: cs) = a \u00d7 Thunk c (Ca \ncs) * lemma1:SeqL a (cs1+ c :: cs2) = (Ca cs1) [Thunk c (SeqL a cs2) ] lemma2: * Thunk c ' ((Ca cs1) \n[Thunk (c- c ' )(SeqL a cs2) ]) = Thunk c ' (SeqL a (cs1 + (c - c ' ) :: cs2)) * The equality (=) used \nabove is a variant of the one introduced in Section 10,but this one relates types: ** (=) :* . * . * \ncast :a = b. a . b The generalised pay can be used to analyse banker s queues (Okasaki 1998), which exhibit \nthe problem with deep payments mentioned above. Space considerations preclude further discussion of this \nanalysis, though. Interested readers are referred to the source code of the analysis for details. Finally \nnote that payg has been proved correct, using the same (mechanised) approach as above; for details see \nDanielsson (2007). 12. Limitations This section discusses some limitationsofTHUNK. Dependent bind Onethingwhichmayhavebothered \nusersfamil\u00adiar with dependently typed languages is that the second (function) argument to bind is non-dependent. \nThis could be .xed by replac\u00ading (>>=) with a more general function: bind : (f :a . N) . (b:a . *) . \n(x :Thunk m a) . ((y:a) . Thunk (fy)(by)) . Thunk (m + f (force x)) (b(force x))  However, thiswould \nnotin itselfbevery useful: force is abstract (see Section 4), so force x would not evaluate in the type \nof bind. One way to work around this is by providing the library user with a numberof axioms specifyinghowthe \nlibrary primitivesevaluate. This can be useful anyway, since it makes it possible to prove ordinary functional \nproperties of annotated code inside Agda. This solution is rather complicated, though. A better approach \nis perhaps to avoid the dependently typed bind, and this can often be achieved by using indexed types. \nCon\u00adsider the following two variants of the append function: (+ ) : (xs :List a) . List a . Thunk (1+ \n2* length xs)(List a) (+ ) : Seq a m . Seq a n . Thunk (1+ 2* m)(Seq a (m + n)) The resulttypeofthe \n.rst function dependsonthevalueofthe .rst list. This is not the case for the second function, where the \nresult type instead depends on the indexm. Putting enough information in type indices is often a good \nway to avoid the need for a dependent bind. Aliasing Another limitation is that the library cannot track \nthunk aliases, except in the limited way captured by pay and the .n bindings of the thunked operational \nsemantics. If x, y:Thunk n a are aliases for each other, and x is forced, then the library has no way \nof knowing that yis also forced; the type of ydoes not change just because x is forced. Okasaki (1998) \nuses aliases in this way to eliminate amortisation, through a technique called scheduling. Interface \nstability If thunked types are exposed to external users of a data structure library then another problem \nshows up: the types of functions analysed using THUNK are not robust against smallchangesinthe implementation.Afunctionsuchas \nmaximum, introduced in Section 7, has a rather precise type: maximum :Seq a (1+ n) . Thunk (13 + 14 * \nn+ 4* n 2) a Atype basedonbigOnotationwouldbe more stable: maximum :Seq a (1+ n) . Thunk O(n 2) a However, \nexpressing big O notation in a sound way without a dedicatedtype system seemstobe hard.Itis probablyagoodidea \nto use force to avoid exporting thunked types. 13. Related work Time complexity for lazy evaluation Several \napproaches to analysing lazy (call-by-need) time complexity havebeen developed (Wadler 1988; Bjerner \n1989; Bjerner and Holmstr\u00a8om 1989; Sands 1995; Okasaki 1998; Moran and Sands 1999). Many of them are \ngeneral,but have beendescribed as complicated to use in practice (Okasaki 1998). It seems to be rather \nuncommon to actually use these techniques to analyse non-trivial programs. The main technique in use \nis prob\u00adably Okasaki s banker s method (1998), which is mainly used for analysing purely functional data \nstructures, and which this work is based on. As described in Section 12 the banker s method is more general \nthan the one described here,butit can alsobe seen as more complicated, since it distinguishes between \nseveral kinds of cost (shared and unshared) which are collapsed in this work. Ross Paterson (personal \ncommunication) has independently sketchedan analysis similartotheonedevelopedhere,but without dependent \ntypes or the annotated monad. The ticks used in this work are related to those used by Moran and Sands \n(1999),but their theory does not require ticks to be in\u00adserted to ensure that computation steps are counted; \nticks are in\u00adstead used to represent and prove improvements and cost equiva\u00adlences, with the help of \na tick algebra. Benzinger (2004) describes a system for automated complex\u00adity analysis of higher-order \nfunctional programs. The system is parametrised on an annotated operational semantics, so it may be able \nto handle a call-by-need evaluation strategy. No such experi\u00adments seem to have been performed, though, \nso it is unclear how practical it would be. Tracking resource usage using types Several frameworks for \ntracking resource usage using types have been developed. Usually theseframeworksdonot addresslazyevaluation(for \ninstanceCrary andWeirich 2000; Constable and Crary 2002; Reb\u00b4 on Portillo et al. 2003; Brady and Hammond \n2006; Hofmann and Jost 2006). There can still be similarities; for instance, the system of Hofmann and \nJost uses amortised analysis to bound heap space usage, with po\u00adtential trackedby types. Hughes,Pareto,andSabry(1996)have \nconstructedatypesys\u00adtem whichkeeps trackof bounds on the sizesofvaluesina lazy language with data and \ncodata. This information is used to guaran\u00adtee termination or productivity of well-typed terms; more \nprecise time bounds are not handled. The use ofan annotated monad to combine time complexities of subexpressions \nappears to be novel. However, there is a close connection to Capretta s partiality monad (2005), which \nis a coin\u00adductive type constructor \u00b7. de.ned roughly as follows: codata \u00b7.(a :*) :* where . return :a \n. a . step :a. . a The following de.nition of bind turns it into a monad: (>>=) :a. . (a . b.) . b. return \nx> = f = fx step x > = f = step (x > = f) Compare the de.nitions aboveto the following shallow embedding \nof the thunk monad: data Thunk (a:*) :N . * where return:a . Thunk a 0 -:Thunk a n . Thunk a (1+ n) \n(>>=) :Thunk a m . (a . Thunkbn) . Thunkb(m + n) return x > = f = fx ( -x)> = f = -x > = f The only difference \nis that the thunk monad is inductive, and annotated with the number of ticks. This indicates that it \nmay be interestingtoexplorethe consequencesofmakingthe thunk monad coinductive, annotated with the coinductive \nnatural numbers(N extended with .). 14. Conclusions Asimple, lightweightlibraryforsemiformalveri.cationofthetime \ncomplexity of purely functional data structures has been described. The usefulness of the library has \nbeen demonstrated and its limita\u00adtions discussed. Furthermore the semantics of the library has been precisely \nde.ned, the time complexity guarantees have been ver\u00adi.ed with respect to the semantics, and the correctness \nproof has been checked using a proof assistant. Acknowledgments I would like to thank Ulf Norell, who \nhas taught me a lot about dependently typed programming, discussed many aspects of this work,and.xedmanyofthebugsinAgdaand \nAgdaLightthatI have reported. I am also grateful to Ross Paterson, who showed me his analysis of implicit \nqueues which turned out to yield a nice and compactexample. Other people whodeservethanks are Jeremy \nGibbons, Martin Hofmann, Patrik Jansson, Geraint Jones, David Sands, Anton Setzer, and some anonymous \nreviewers. References Ralph Benzinger. Automated higher-order complexity analysis. Theoretical Computer \nScience, 318:79 103, 2004. Bror Bjerner andS\u00a8oren Holmstr\u00a8om. A compositional approach to time analysis \nof .rst order lazy functional programs. In FPCA 89, pages 157 165, 1989. Bror Bjerner. Time Complexity \nof Programs inType Theory. PhD thesis, Departmentof Computer Science, UniversityofG\u00a8oteborg, 1989. Edwin \nBrady andKevin Hammond. A dependently typed framework for static analysis of program execution costs. \nIn IFL 2005, volume 4015 of LNCS, pages 74 90, 2006. Edwin Brady, Conor McBride, and James McKinna. Inductive \nfamilies need not store their indices. In TYPES 2003: Types for Proofs and Programs, volume 3085 of LNCS, \npages 115 129, 2004. Venanzio Capretta. General recursion via coinductive types. Logical Methods in Computer \nScience, 1(2):1 28, 2005. Robert L. Constable and Karl Crary. Re.ections on theFoundations of Mathematics: \nEssays in Honor of SolomonFeferman, chapter Compu\u00adtational Complexity and Induction forPartial Computable \nFunctions in Type Theory.AKPeters Ltd, 2002. Karl Crary and StephanieWeirich. Resource bound certi.cation. \nIn POPL 00, pages 184 198, 2000. Nils Anders Danielsson. A formalisation of the correctness result from \nLightweight semiformal time complexity analysis for purely functional data structures . Technical Report \n2007:16, Department of Computer Science and Engineering, Chalmers UniversityofTechnology, 2007. Peter \nDybjer. Inductivefamilies. Formal Aspects of Computing, 6(4):440 465, 1994. Ralf Hinze and RossPaterson. \nFinger trees:Asimple general-purpose data structure. Journal of Functional Programming, 16(2):197 217, \n2006. Martin Hofmann and Steffen Jost. Type-based amortised heap-space anal\u00adysis. In ESOP 2006, volume \n3924 of LNCS, pages 22 37, 2006. John Hughes, Lars Pareto, and Amr Sabry. Proving the correctness of \nreactive systems using sized types. In POPL 96, pages 410 423, 1996. Haim Kaplan, Chris Okasaki, and \nRobert E. Tarjan. Simple con.uently persistent catenable lists. SIAMJournal on Computing, 30(3):965 977, \n2000. Haim Kaplan and Robert E. Tarjan. Purely functional, real-time deques with catenation. Journalof \ntheACM, 46(5):577 603, 1999. John Launchbury. Anatural semantics for lazy evaluation. In POPL 93, pages \n144 154, 1993. Daan Leijen and Erik Meijer. Domain speci.c embedded compilers. In 2nd USENIX Conference \non Domain-Speci.c Languages (DSL 99), pages 109 122, 1999. Andrew Moran and David Sands. Improvement \nin a lazy context: an operational theory for call-by-need. In POPL 99, pages 43 56, 1999. Ulf Norell. \nTowards a practical programming language based on depen\u00addent type theory. PhD thesis, Chalmers University \nofTechnology and G\u00a8oteborgUniversity, 2007. Chris Okasaki. Purely Functional Data Structures. Cambridge \nUniversity Press, 1998. Christine Paulin-Mohring. Extracting F. s programs from proofs in the calculus \nof constructions. In POPL 89, pages 89 104, 1989. Simon Peyton Jones, editor. Haskell 98 Language and \nLibraries: The Revised Report. Cambridge University Press, 2003. \u00b4 AlvaroJ. Reb\u00b4on Portillo,Kevin Hammond, \nHans-Wolfgang Loidl, and Pe\u00addroVasconcelos. Cost analysis using automatic size and time inference. In \nIFL 2002, volume 2670 of LNCS, pages 232 247, 2003. David Sands. A na\u00a8ive time analysis and its theory \nof cost equivalence. Journal of Logic and Computation, 5(4):495 541, 1995. The Agda Team. The Agda Wiki. \nAvailable at http://www.cs. chalmers.se/~ulfn/Agda/, 2007. PhilipWadler. Strictness analysis aids time \nanalysis. In POPL 88, pages 119 132, 1988.  \n\t\t\t", "proc_id": "1328438", "abstract": "<p>Okasaki and others have demonstrated how purely functional data structures that are efficient even in the presence of persistence can be constructed. To achieve good time bounds essential use is often made of laziness. The associated complexity analysis is frequently subtle, requiring careful attention to detail, and hence formalising it is valuable. This paper describes a simple library which can be used to make the analysis of a class of purely functional data structures and algorithms almost fully formal. The basic idea is to use the type system to annotate every function with the time required to compute its result. An annotated monad is used to combine time complexity annotations. The library has been used to analyse some existing data structures, for instance the deque operations of Hinze and Paterson's finger trees.</p>", "authors": [{"name": "Nils Anders Danielsson", "author_profile_id": "81309480746", "affiliation": "Chalmers University of Technology, G&#246;teborg, Sweden", "person_id": "P767502", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1328438.1328457", "year": "2008", "article_id": "1328457", "conference": "POPL", "title": "Lightweight semiformal time complexity analysis for purely functional data structures", "url": "http://dl.acm.org/citation.cfm?id=1328457"}