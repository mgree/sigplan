{"article_publication_date": "01-07-2008", "fulltext": "\n High-Level Small-Step Operational SemanticsforTransactions KatherineF. Moore Dan Grossman University \nofWashington {kfm, djg}@cs.washington.edu Abstract Software transactionshavereceived signi.cant attentionasawayto \nsimplify shared-memory concurrent programming,but insuf.cient focus has been given to the precise meaning \nof software transac\u00adtions or their interaction with other language features. This work beginsto rectify \nthat situationby presentingafamilyof formal lan\u00adguages that model a wide variety of behaviors for software \ntrans\u00adactions. These languages abstract away implementation details of transactional memory, providing \nhigh-levelde.nitions suitable for programming languages.We use small-step semantics in order to represent \nexplicitly the interleaved execution ofthreads that is nec\u00adessary to investigate pertinent issues. We \ndemonstrate the value of our core approach to modeling transactionsbyinvestigatingtwoissuesindepth. First,we \nconsider parallel nesting, in which parallelism and transactions can nest ar\u00adbitrarily. Second, we present \nmultiple models for weak isolation, in which nontransactional code can violate the isolation of a transac\u00adtion.For \nboth, type-and-effect systems let us soundly andstatically restrict what computation can occur inside \nor outside a transaction. Weprove somekeylanguage-equivalence theoremsto con.rmthat under suf.cient static \nrestrictions, in particular that each mutable memory location is used outside transactions or inside \ntransactions (but not both), no program can determine whether the language im\u00adplementation uses weak \nisolation or strong isolation. Categories and Subject Descriptors D.1.3[ProgrammingTech\u00adniques]:Concurrent \nProgramming Parallel Programming; D.3.1 [Programming Languages]: Formal De.nitions and Theory Semantics; \nD.3.3 [Programming Languages]: Language Con\u00adstructs and Features Concurrent programming structures; F.3.2 \n[Logics and Meaning of Programs]: Semantics of Programming Languages Operational semantics General Terms \nLanguages 1. Introduction 1.1 The NeedFor Semantics Widespread availability of multicore architectures \nhas incited ur\u00adgent interest in programming-language features that make it easier to write correct and \nef.cient parallel programs. Software transac\u00adtions are particularly appealing for shared-memory programming \nbecause they let programmers declare that an entire computation Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page.To copyotherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. POPL 08, January 7 12, 2008, San Francisco, \nCalifornia, USA. Copyright c &#38;#169; 2008ACM 978-1-59593-689-9/08/0001...$5.00 should happen all at \nonce with respect to other parallel compu\u00adtations. A construct like atomic (e) means e should evaluate \nas a transaction, which provides a mutual-exclusion mechanism that avoids manyofthepitfallsof locksand \nconditionvariables.It dele\u00adgates to the language implementation the task of preserving the all at once \nillusion while striving to preserve parallelism among non\u00adcon.icting computations. Much recent work has \ninvestigated ef.\u00adcient implementation techniques (Harris and Fraser 2003; Harris et al. 2006; Adl-Tabatabai \net al.2006; Carlstrom et al. 2006; Mora\u00advan et al. 2006;Kumar et al. 2006; Damron et al. 2006; Marathe \net al. 2005; Herlihyet al. 2006; Shpeisman et al. 2007). While this informal understanding of software \ntransactions pro\u00advides a simple high-level semantics for programmers, unfortu\u00adnately, typical implementation \napproaches introduce complications that forceprogrammers to abandon this simple view. As a small example, \nmanyimplementations make it a dynamic error to spawn a thread whileexecutinga transaction.If so,a library \nthat encapsu\u00adlates a parallel algorithm cannot be called in the dynamic scope of an atomic block. Even \nmore troubling are questions that arise from trying togive a weak-isolation semantics to transactions. \nUnder weak isolation, nontransactional memory accesses bypass the mechanisms used to implement transactions. \nRecently, we (Shpeisman et al. 2007; Grossman et al. 2006) and others (Blundell et al. 2006; Larus and \nRajwar2006; Hudsonetal.2006;Spearetal.2007)have described many surprising behaviors that can result and \nthat cannot be ex\u00adplained without understanding how transactions are implemented. Asjust oneexample, \nconsider this codeinaJava-like language: Despite races between Threads1and3, intuitively Thread1does \nonly one assignment so m or n or both stay 0. However, weak\u00adisolation implementations using eager-update \n(Harris et al. 2006; Adl-Tabatabai et al. 2006) can violate this intuition: transactions may abort-and-retry \nmultiple times and nontransactional code may see partial results of aborted computations. Such behavior \nis im\u00adpossible in lock-based code. In general, weak isolation as well as the interaction between transactions \nand other language features (such as thread-creation) cause semantic ambiguities that raise two sets \nof questions: Can we design languages that prevent undesirable behaviors while still permitting typical \nimplementations? What restric\u00adtions must we put on source programs? Howcan we provethese restrictions \nsuf.ce? If we make some errors the programmers responsibility, what guarantees must the language still \nprovide? Can an illegal thread-creation or data-race lead to an arbitrary program state (likeCarray-bounds \nviolations do)? Precisely answering these questions requires rigorous semantics and proofs. Such semantics \nmust be high-level enough to provide a simple de.nition to programmersyet detailed enough to incor\u00adporate \nrelevant features. Restrictions on programs must be de.ned in well-understood terms, such as with a type \nsystem. Variations in semantic de.nitions should be compared by showing they are unobservable (via an \nequivalence proof) or observable (via an ex\u00adample program that distinguishes them). Proofs should reveal \nthe keyinvariants that motivate the semantics and type systems. 1.2 OurFamilyof High-Level Small-Step \nSemantics To meet this need, we use operational semantics to de.ne several core languages based on a \ncall-by-value .-calculus with a muta\u00adble heap, threads, and transactions. Collectively, we call these \nlan\u00adguages the AtomsFamily. The languages differ where needed to investigate a language feature or design \ndecision. They are not de\u00adsigned for reasoning about transactional-memory implementation details. Rather, \nthey provide high-level de.nitions of transactions where only one transaction runs at a time. This high \nlevel matches how we want programmers to reason about transactions so it is appropriate for language \nde.nitions. At the same time, we use a small-step semantics in which transactions take multiple steps \nto complete.Apotentially simpler transactions-in-one-step approach would make it too awkward to investigate \nparallel nesting or weak isolation because both features need threads to interleave while a transaction \nexecutes. As Section 7 discusses, a high-level small\u00adstep semantics distinguishes our approach from prior \nwork. We consider four languages in depth: StrongBasic (Section 2) is the simplest language. While a \ntransaction executes, it cannot spawn threads and other threads that alreadyexistmaynotreador write mutableheap \nlocations.  StrongNestedParallel (Section 3) extends StrongBasic with multiple ways to spawn threads. \nThe different ways behave differently inside and outside transactions.  Weak (Section 4) is like StrongBasic \nexcept nontransactional code can access the heap concurrently with a transaction, thus allowingade.nitionof \nweak isolationin which transactions act asif theyare all protectedbya single lock.  WeakUndo (Section \n5) is like Weak except a transaction may abort-and-retry by undoing its heap updates and restarting. \n We also sketch two otherAtomsFamily members: WeakOnCom\u00admit and StrongUndo. WeakOnCommit models transactions \nthat can abort-and-retry,but unlike in WeakUndo, transactions do not update the heap until they commit. \nIn WeakUndo, committing takes only one step; in WeakOnCommit, aborting takes only one step. In-depth \ninvestigation of WeakOnCommit remains future work. StrongUndo has the strong isolation of StrongBasic \nand the abort-and-retry of WeakUndo. This unusual combination is a cru\u00adcial intermediate language for \nproving that, underatype system we de.ne, WeakUndo and StrongBasic are suitably equivalent. Our type \nsystems are all similar type-and-effect systems that classify code based on where it can run safely: \nonly inside trans\u00adactions, only outside transactions, or anywhere.For StrongNest\u00adedParallel, our type \nsystem ensures forms of thread-creation that make sense only inside transactions do not occur outside \ntransac\u00adtions and vice-versa.For Weak and WeakUndo, our type system enforces that the same heap location \nis not accessed inside and out\u00adsidea transaction.A simplevariantcan also prevent one atomic block from \nexecuting in the dynamic scope of another. Our languages and type systems are not exhaustive. To the \ncontrary, we consider it a strength that new variants are easy to de.ne and compare, nontrivial proofs \nnotwithstanding.Weexpect to investigate more language features by adding to the Atoms-Family (see Section \n6) and encourage others to do the same.  1.3 Speci.c Results While generally useful, our approach has \nalso produced several speci.c insights and theorems. The most important results are sum\u00admarized here \nand explained in the remainder of the paper: For nested transactions to interact properly with either \npar\u00adallelism within transactions(StrongNestedParallel)or abort\u00adand-retry(WeakUndo),the stateofatransaction \nshould include whether another transaction is currently executing inside it.  Alanguage with arbitrarily \nnested parallelism and transactions (StrongNestedParallel)can be type-safe even if certain forms for \nspawning threads can be used only in certain contexts.  Weak isolation(Weak)and strong isolation(StrongBasic)are \nindistinguishable (i.e., the languages are equivalent) under a type system that prohibits the same heap \nlocation from being accessed inside and outside transactions. Thekey to the proof is showing that any \ncomputation interleaved with the current transaction would have produced the same result had it pre\u00adceded \nthe transaction.  Weak isolation with abort-and-retry(WeakUndo)and strong isolation(StrongBasic)are \nindistinguishable under a similar type system as the previous result, but with some interest\u00ading caveats: \n(1) WeakUndo has some intermediate states un\u00adreachable from StrongBasic,(2)WeakUndo may allocate more \nmemory,and (3) for simplicity we strengthen the type system to prohibit nested transactions.Thekeytotheproofisto \nseparate the necessary argument that the operational semantics imple\u00adments abort correctly.  Fortunately, \nthe equivalence resultsforWeak and WeakUndo con\u00ad.rm conventional wisdom. Given the until recently unforeseen \nbe\u00adhaviors resulting from races between transactional andnontransac\u00adtional code, it is reassuring to \nprove that such races are necessary for weak isolation to exhibit such behaviors. Moreover, the struc\u00adture \nof our proofs can serve as a guide for extending the results to more sophisticated (and less obviously \ncorrect) static invariants. Full de.nitions and proofs appear in ourtechnical report (Moore and Grossman \n2007). 2. The StrongBasic Language This section presents the syntax and small-step operational seman\u00adtics \nfora .-calculus with threads, shared memory,and transactions. The language is largely a starting point \nfor the additions and type systemsinthe subsequenttwo sections. Threekeydesign decisions characterize \nour approach: The semantics is high-level. It relies on implicit nondetermin\u00adism to .nd a successful \nexecution sequence. There is no notion of transactions con.icting or aborting. Rather, a transaction \nis always isolated from other threads because no thread may ac\u00adcess shared memory if another thread is \nin a transaction. This simple semantics providesa correctness criterion for more real\u00adistic implementations \nanda simple model forprogrammers.  The semantics is small-step. In particular, transactions take many \ncomputational steps to complete. While this decision is an unnecessary complication for StrongBasic, \nit is essential for considering the additional thread interleavings that parallelism within transactions \nand weak isolation introduce.  e ::= c | x | .x.e | e1 e2 | seq(e1,e2) | if e1 e2 e3 | refe | e1 := \ne2 | !e | spawntl e | atomic e | l | inatomic(e) v ::= c | .x.e | l H ::= \u00b7| H, l . v T ::= \u00b7| T i e \na ::= .| Figure 1. StrongBasic Syntax The semantics is strong. Nontransactional code cannot observe or \nin.uence partially completed transactions. We prove later that strong-isolation semantics is equivalent \nto weak-isolation semantics under certainconditions.One cannotdosuchaproof without de.ning both semantics. \nThis language does not have an explicit abort/retry. Adding this construct is easy; as in prior work \n(Harris et al. 2005) one simply has no evaluation rule for it. A transaction that explicitly aborts is \none that can never be chosen by the nondeterministic semantics. However,this typeofabort complicates \nstating type-safety because we would have to accommodate an abort preventing progress. 2.1 Syntax Figure1 \npresents the formal abstract syntax for our .rst transac\u00adtional language. Mostexpression forms are typical \nfora .-calculus with mutable references, including constants(c), variables (x), functions(.x.e), function \napplications(e1 e2), sequential compo\u00adsition(seq(e1,e2)), conditionals(if e1 e2 e3), memory allocation \n(refe), assignment(e1 := e2), and dereference(!e). Manyomitted constructs,suchas records,wouldbe straightforward \nadditions.We also have thread-creation(spawntl e), where thetl indicates it must be used at top-level \n(not within a transaction), and atomic blocks (atomic e)for executinge transactionally. Aprogram state \nhas the forma; H; T where a indicates if any thread is currently executing a transaction(a = for yes \nand a = . for no), H is a mutable heap (a mappingfrom labels l, also known as addresses, to values), \nand T is a collection of threads. Each thread is an expression representing that thread s remaining computation.We \nuse T1 i T2 to combine two thread collections intoa larger one, and we assume i is commutative, associative, \nand has \u00b7 (the empty collection) as an identity.We write e in place of \u00b7i e where convenient. At run-time \nwe need two new expression forms, inatomic(e) and l. The former represents a partially completed transaction \nwith remaining computation e. The latter represents a heap location. The program-state component a deserves \nadditional discussion. Our semantics allows at most one thread to execute a transaction at a time. In \nessence a is like a global lock where indicates the lock is held. We do not suggest our language is \na desirable implementation, but it is the high-level semantics that enforces atomicity and isolation.Wewould \nlike anef.cient implementation to be correct if, by de.nition, it is equivalent to our semantics. 2.2 \nOperational Semantics Our small-step operational semantics (Figure 2) rewrites one pro\u00ad ' gram state \na; H; T to another a; H' ; T ' . Source program e starts with .; \u00b7; e and a terminal con.guration has \nthe form .; H; v1 i ... i vn, i.e., all threads are values (and no transaction is active). Although the \nsource program contains only a single e, the evalua\u00adtion of e can spawn threads, which can spawn more \nthreads, etc. The rule PROGRAM chooses a thread nondeterministically and that thread takes a single step, \nwhich can affect a and H as well as possibly create a new thread. So the judgment form for single\u00ad '' \nthread evaluation is a; H; e . a; H' ; e; T , where T is \u00b7 if the step does not spawn a thread and some \ne'' if it does. For conciseness, we use evaluation contexts(E) to identify where subexpressions are recursively \nevaluated. A single rule (CONTEXT) propagates changes from evaluating the subexpres\u00adsion. As usual, the \ninductive de.nition of E describes expressions with exactly one hole [\u00b7] and E[e] means the expression \nresulting from replacing the hole in E with e.Forexample, CONTEXTlets us '' derive a; H; ref(seq(e1,e2)) \n. a; H' ; ref(seq(e1,e2)); T pro\u00ad '' vided a; H; e1 . a; H' ; e1; T . We do not treat the body of a transaction \nas an evaluation context precisely because we do not use the same a and a' for the subevaluation. Rules \nfor reducing sequences, memory allocations, and func\u00adtion calls are entirely conventional. In APPLY, \ne[v/x] means the capture-avoiding substitution of v for x in e. The rules for reading and writing labels(GET \nandSET)require a = ., meaning no other thread is executing a transaction. This encodes a high-level de.nition \nof strong isolation;it prohibits any memory con.ict with a transaction. If no thread is in a transaction, \nthen anythread may access the heap.Weexplain belowhow rule INATOMIC lets the thread executing a transaction \naccess the heap. The rules de.ning how an expression enters or exits a transac\u00adtion are of particular \ninterest because they affect a.A thread can enter a transaction only if a = . (elseENTERATOMIC does not \nap\u00adply), and it changes a to . Doing so prevents another thread from entering a transaction until EXIT \nATOMIC (applicable only if the computation is .nished, i.e., some value v)changesa back to .. A transaction \nitself needs to access the heap (which, as dis\u00adcussed above, requires a = .)and execute nested transactions \n(which requires . before entry and beforeexit),but a is while a transactionexecutes. Thatiswhythehypothesisin \nrule INATOMIC allows any a and a' for the evaluation of the subexpression e. That way, the in the program \nstate ; H; inatomic(e) constrains only ' the other threads; the evaluation of e can choose any a and \nanec\u00adessary to take a step. If we required a and a' to be ., then e could accesstheheapbutitcouldnotevaluatea \nnested transaction. Note rule INATOMIC ensures a transaction does not spawn a thread (the hypothesis \nmust produce thread-pool \u00b7), which en\u00adcodes that all spawns must occur at top-level. An expression like \ninatomic(spawntl e) is always stuck; there is no a and H with which it can take a step.  2.3 Type System \nWe could present a type system forStrongBasic,but most of the errors it would prevent are standard (e.g., \nusing an integer as a function). The only non-standard stuck states sofar occur whena thread tries to \nperform a spawn inside a transaction. The type-and\u00adeffect system presentedin Section3prevents this error. \n3. The StrongNestedParallel Language While one reasonable semantics for spawn is that it is an error \nfor it to occur in a transaction, there are several reasons to al\u00adlow other possibilities. First, there \nis no conceptual problem with treating isolation and parallelism as orthogonal issues (Moss 1985; Haines \net al. 1994; Jagannathan et al. 2005). Second, if e spawns a thread (perhaps inside a library), then \ne and atomic e behave dif\u00adferently. Third, for some computations it may be sensible to delay anyspawned \nthreads until a transaction commits, and doing so is not dif.cult to implement. Fourth, it is undesirable \nto forfeit the performance bene.ts of parallelism every time we need to isolate a computation from some \nother threads. This last issue becomes more important as the number of pro\u00adcessors increases; otherwise \ntransactions become a sequential bot\u00adtleneck.Forexample, considera concurrent hashtable with insert, \nlookup, and resize operations. Resize operations may be relatively ' '' a; H; e . a ; H ; e ; T CONTEXT \n' '' a; H; e . a ; H ; e ; T E ::= [\u00b7] | Ee | vE | seq(E, e) | if Ee2 e3 | refE | E := e | l := E | !E \n '' ' a; H; E[e] . a ; H ; E[e ]; T IF-NZ APPLY SEQ IF-Z c =0 a; H;(.x.e) v . a; H; e[v/x]; \u00b7 a; H; \nseq(v, e2) . a; H; e2; \u00b7 a; H; if 0 e2 e3 . a; H; e3; \u00b7 a; H; if ce2 e3 . a; H; e2; \u00b7 ALLOC SET GET SPAWN \nTL l . Dom(H) a; H; refv . a; H, l . v; l; \u00b7.; H; l := v ..; H, l . v; v; \u00b7.; H;!l ..; H; H(l); \u00b7 a; \nH; spawntl e . a; H; 0; e INATOMIC ENTER ATOMIC EXIT ATOMIC ' '' a; H; e . a ; H ; e ; \u00b7 .; H; atomic \ne . ; H; inatomic(e); \u00b7 ; H; inatomic(v) ..; H; v; \u00b7 ; H; inatomic(e) . ; H ' ; inatomic(e ' ); \u00b7 ' '' \na; H; T . a ; H ; T PROGRAM ' ''' a; H; e . a ; H ; e ; T ''' ' a; H; T1 i e i T2 . a ; H ; T1 i e i \nT2 i T Figure 2. StrongBasic Operational Semantics e ::= . . . | spawnoc e | spawnip e | inatomic(a, \ne, Toc, Tip) t ::= int | ref t | t e. t ' e ::= emp | ot | wt G ::= \u00b7 | G, x:t Figure 3. StrongNestedParallel \nSyntax(extends Figure1) rare and large yet still need to be isolated from other threads to avoid the \ncomplexities of concurrent operations. By parallelizing the resize operation within a transaction, we \npreserve correctness without letting sequential resize operations dominate performance. Intherestofthis \nsection,weextend StrongBasic byadding sev\u00aderal different.avors of spawn. This new language, StrongNested-Parallel, \ndemonstrates that spawn expressions within transactions canhave reasonable semantics.We also presenta \ntype-and-effect system to ensure the different .avors are used sensibly. 3.1 Syntax and Operational \nSemantics Figure3presents the new syntax and Figure4presents the changes to the operational semantics. \nThe syntax additions are two new .avors of spawn expressions, spawnoc (for on commit ) and spawnip (for \ninternally parallel ). The former is allowed anywhere, but if it occurs inside a trans\u00adaction, the spawned \nthread does not run until after the transaction commits. The latter is allowed only within a transaction \nand the transaction does not commit until the spawned thread completes executing (i.e., becomesavalue).One \ncould certainlydevise ad\u00additional .avors of spawn; we believe these two plus spawntl cover a range of \nbehaviors that are desirable in different situations.It is reasonable to provide them all in one programminglanguage, \nper\u00adhaps with an undecorated spawn beinga synonymfor oneof them. Forexample, the currentFortress speci.cation \n(Allen et al. 2007) treats spawn as spawntl,but it also has constructs for fork-join style parallelism \nthat our model could encode with spawnip. The inatomic expression, which as in StrongBasic does not appear \nin source programs, has also changed. In addition to the e whose eventual result is the transaction s \nresult, it now carries an a and two threadpools, Toc and Tip. The a indicates whether e or any thread \nin Tip is currently executing a transaction. Toc holds the threads that will be produced as on commit \nthreads when the transaction completes. The discussion of the semantics below explains inatomic further. \nAsingle-thread evaluation step produces three possibly-empty threadpools Ttl , Toc, and Tip. The evaluation \nrules for the three .avors of spawn each put the new thread in the appropriate pool with the other pools \nempty. The CONTEXTrule propagates all three threadpools out to evaluation of the larger expression. Other \nrules, likethose for assignment, function application, etc., only changeby producing three empty pools \ninstead of one. The rule for sequences has been included as an example. The PROGRAM rule requires that \nthe thread chosen for evalua\u00adtionproduces an empty Tip, whereas Ttl and Toc are added to the global pool \nof threads, i.e., spawned immediately. Therefore, it is an error to use spawnip outside a transaction. \nAs in StrongBasic, entering a transaction changes . to . The resulting expression inatomic(., e, \u00b7, \u00b7) \nis a transaction with no nested transaction (hence the .), no delayed threads (the .rst \u00b7)and no internally \nparallel threads (the second \u00b7). For a transaction inatomic(a, e, Toc,Tip ), either e or a thread in \nTip can take a step, using INATOMIC DISTINGUISHED or INATOMICPARALLEL, respectively. The only reason \nto distinguish e is so inatomic producesavalue;in languages wherethebodyisa statement that produces no \nresult we could combine these tworules by including e in Tip. In both rules, we evaluate some thread \nusing ' ''' ' a and produce an a , H , e , Toc, and Tip. As in StrongBasic, eval\u00ad uation inside a transaction \nmay not spawn a top-level thread. The '' ' a , Toc, and Tip are added to the resulting expression, i.e., \nthey are part of the transaction s new state. In particular, parallel threads in the transaction may \nproduce other parallel or on-commit threads. Heap changes are propagated outward immediately, which is \nno problem because the outer state is . Atransaction completes when the distinguished expression and \nall parallel threads are values. Rule EXIT ATOMIC then propagates out all the on-commit threads in one \nstep. Notice a transaction never produces any threads visible outside the transaction until it commits. \n' '' a; H; e . a ; H ; e ; Ttl ; Toc; Tip CONTEXT a; H; e . a ' ; H ' ; e ' ; Ttl ; Toc; Tip SEQ a; \nH; E[e] . a ' ; H ' ; E[e ' ]; Ttl ; Toc; Tip a; H; seq(v, e2) . a; H; e2; \u00b7; \u00b7; \u00b7 SPAWN TL SPAWN OC \nSPAWN IP a; H; spawntl e . a; H; 0; e; \u00b7; \u00b7 a; H; spawne . a; H; 0; \u00b7; e; \u00b7 a; H; spawnip e . a; H; 0; \n\u00b7; \u00b7; e oc ENTER ATOMIC EXIT ATOMIC .; H; atomic e . ; H; inatomic(., e, \u00b7, \u00b7); \u00b7; \u00b7; \u00b7 ; H; inatomic(., \nv, Toc, (v1 i ... i vn)) ..; H; v; \u00b7; Toc; \u00b7 INATOMIC DISTINGUISHED ''' '' a; H; e . a ; H ; e ; \u00b7; \nToc; Tip ' ''' ' ; H; inatomic(a, e, Toc,Tip) . ; H ; inatomic(a ,e , (Toc i Toc), (Tip i Tip)); \u00b7; \u00b7; \n\u00b7 INATOMIC PARALLEL ''' '' a; H; e . a ; H ; e ; \u00b7; Toc; Tip ''' ' ''''' ; H; inatomic(a, e0,Toc, (Tip \ni e i Tip)) . ; H ; inatomic(a ,e0, (Toc i Toc), ((Tip i e i Tip) i Tip)); \u00b7; \u00b7; \u00b7 ' '' a; H; T . a ; \nH ; T PROGRAM ' '' a; H; e . a ; H ; e ; Ttl ; Toc; \u00b7 '' ' a; H; T1 i e i T2 . a ; H ; T1 i e i T2 i \nTtl i Toc Figure 4. StrongNestedParallel Operational Semantics (selected rules omitted) Unlike in StrongBasic, \nnested transactions are important; they let one thread in a transaction perform a computation atomically \nwith respect to other threads in the transaction. Each transaction has an explicit a to ensure at most \none of the threads is in a nested transaction. Because we have strong isolation, if a thread is in a \ntransaction, then no parallel threads access the heap. However, in the innermost transaction, parallel \nthreads may access the heap si\u00admultaneously. Note that on-commit threads spawned inside nested transactions \ndo not run until the outermost transaction commits. Other possibilitiesexist,but the soundnessof our \nparticular type\u00adand-effect system relies on this choice.  3.2 Type System StrongNestedParallel has several \nerror states. These include com\u00admon type errors (e.g., treating an integer asa function), performing \na top-level spawn insideatransaction, and performing an internally parallel spawn outsidea transaction.We \nnowpresenta type-and\u00adeffect system that soundly and conservatively prohibits such errors (Figure5).Toprovetypesafety, \nSection3.3extendsthistypesys\u00adtem to run-time states, including labels and inatomic expressions. The judgment \nG; e f e : t means (1) e has type t where G provides types for the free variables of e, and (2) executing \ne only spawns threads of the .avors that the effect e allows. A source program e type-checks if \u00b7; ot \nf e : t for some t . Because (1) is standard, we focus on (2), which makes our judgment an effect system. \nThe empty effect emp describes computations that are safe anywhere (i.e., inside or outside transactions); \nsuch computations spawn neither top-level nor internally-parallel threads. On-commit threads are .ne \nbecause creating them never leads to dynamic errors. Effect ot describes computations safe outside transactions, \npermitting on-commit and top-level threads, and effect wt describes computations safe within transactions, \npermitting on-commit and internally-parallel threads.Wedonothavea top effectthatallows all three .avors \nof spawn. Such an effect is soundbut not useful because code that type-checked only under this most-permissive \neffect could run safely neither inside nor outside a transaction. Most other aspects of our effect system \nare standard. Expres\u00adsions that do not spawn threads can type-check with any effect. Values andvariables \nareexamples, e.g.,T-CONST allows any e.By not requiring effect emp in rules like T-CONST, rules like \nT-SEQ and T-SET can use the same effect for both subexpressions.1 For example, we can derive x:ref int; \not f seq(!x, spawntl 42) : int. As expected, functions have latent effects, meaning function types carry \nan effect that occurs when the function is called.Afunction itself can have any effect, but its body \ns effect is included in the effect of any call to it (see T-LAMBDA and T-APP). In T-APP, the subeffect \nrelation allows using a function with latent effect emp in a computation with effect ot or wt. In practice, \nwe expect most functions to type-check under emp;this subeffecting allows such functions to be called \nanywhere. The most interesting rules are for atomic blocks and spawn expressions. The e in atomic e must \ntype-check under wt, but the atomic block itself is allowed anywhere, which enables nested transactions \nand functions containing atomic blocks that can be called inside and outside transactions. Because all \nspawntl expres\u00adsions mustexecute outside transactions,theeffectofthespawn and ofthe innerexpressionis \not.Bycontrast, allexpressions createdby spawnoc are evaluated at the top level (requiring effect ot),but \nit is acceptable to execute the spawn expression itself at top-level or inside a transaction. Therefore, \nlike for atomic blocks, we do not constrain the effect e for spawnoc. Finally, spawnip needs effect wt \nfor the entire expression and the spawned expression because both execute only within a transaction. \nNote that if our language had expressions other than spawntl that could not occur in transactions (e.g., \nirreversible I/O), our effect system could statically prevent such errors in the same way. 1A.ne alternative \nis to add an effect-subsumption rule. e = e ' REFLEXIVE EMPTY e = e emp = e G; e f e : t T-APP T-LAMBDA \n e T-CONST T-VAR ' . t2 ' G; e f e2 : t1 e = eG,x:t1; e f e : t2 G; e f e1 : t1  G; e f c : int G; e \nf x : G(x) G; e f e1 e2 : t2 e G; e f .x.e : t1 . t2 T-SEQ T-IF T-REF T-SET G; e f e1 : t1 G; e f e2 \n: t2 G; e f e1 :int G; e f e2 : t G; e f e3 : t G; e f e : t G; e f e1 : ref t G; e f e2 : t G; e f seq(e1, \ne2) : t2 G; e f if e1 e2 e3 : t G; e f refe : ref t G; e f e1 := e2 : t T-GET T-ATOMIC T-SPAWN-TL T-SPAWN-OC \nT-SPAWN-IP G; e f e : ref t G; wt f e : t G; ot f e : t G; ot f e : t G; wt f e : t G; e f !e : t G; \ne f atomic e : t G; ot f spawntl e : int G; e f spawnoc e : int G; wt f spawnip e : int Figure 5. StrongNestedParallel \nType System for Source Programs G; e f e : t , additions and changes G ::= . . . | G, l : t T-LABEL \nT-LAMBDA G(l) = t G, x:t1; e ' f e : t2 not-active(e) G; e f l : ref t e G; e f .x.e : t1 . t2 T-INATOMIC \n G; wt f e : t G; ot f Toc G; wt f Tip not-active(Toc) correct(a, e i Tip) G; e f inatomic(a, e, Toc, \nTip) : t G f H : G ' G; e f T f a; H; T G f \u00b7 : \u00b7 G f H : G ' G; e f v : t G f H, l . v : G ' , l:t G; \ne f \u00b7 G; e f T G; e f e : t G; e f T i e G f H : G G; ot f T f a; H; T correct(a, T ) Figure 6. StrongNestedParallel \nType System Extensions for Program State(See also Figure7) 3.3 Type Safety Type safety ensures a program \nnever becomes stuck, which means at least one thread (possibly more) can take a step. Some threads canbe \ntemporarilyblocked.Forexample, when one threadisina transaction, another thread is blocked if its next \nstep is to enter a transaction. Therefore, our type-safety theorem claims that some thread can proceed \nunless we have properly terminated: Theorem 3.1 Type Safety If (1) \u00b7; ot f e : t, (2) after some number \nof steps .; \u00b7; e becomes a; H; T , and (3) not all threads in T are values, then thereexistsa thread \ne ' in T suchthat a; H; e ' can take a single-thread evaluation step. As usual, we prove this theorem \nby showing preservation (any evaluation step fromawell-typed state producesawell-typed state) and progress \n(no well-typed state is stuck). Doing so requires ex\u00adtending the type system to run-time states a; H; \nT and the expres\u00adsion forms that are not present in source programs. The extended system, presentedinFigure6,isonlyfortheproof.Proof \ndetailsare in an available technical report (Moore and Grossman 2007); here we sketch the interesting \ninvariants that the rigorous proof reveals. To set the stage, most of the extensions are straightforward. \nA state is well-typed(f a; H; T ), if (1) the heap is well-typed (G f H :G),2 and (2) each thread type-checks \nunder effect ot and the labels in the heap(G; ot f T ).Note our de.nition of G now includes types for \nlabels. Also note that when type-checking the heap, effects are irrelevant because the heap contains \nonly values and values never spawn threads. The third obligation for a well\u00adtyped state correct(a, T \n) is discussed below. Turning to the typing rules for expressions, the typing rule for labels is as \nexpected. The typing rule for functions has the new hypothesis not-active(e). This technical point ensures \na function body never contains a partially completed transaction. While this is true for any state resulting \nfrom a source program, it is an in\u00advariant that we must establish holds during evaluation. Otherwise, \nafunctioncall couldleadtoa state wheretwo threads wereexecut\u00ading transactions simultaneously. Formal \nsyntax-directed rules for not-active(e) are in the technical report,but as Figure7describes, theysimply \nencode that no inatomic expression occurs in e. The typing rule T-INATOMIC has several subtleties. Because \ne and Tip evaluate within a transaction, they must have effect wt. Similarly, Toc will be evaluated at \nthe top level, so it must have effect ot. As with atomic, the overall effect of inatomic is unconstrained \nto allow nesting. As with function bodies, the not\u00adyet-running threads Toc must not contain inatomic \nexpressions. 2Using G twice is a technical trick to allow cycles in the heap. not-active(e) e (or T )contains \nno not-active(T ) inatomic expression. active(e) e (or T )contains exactly 1non-nested active(T ) inatomic \nexpression, and that occurrence is in a sensible syntactic position. (See discussion for more detail.) \ncorrect(a, T ) (a = . and not-active(T )) or (a = and active(T )) Figure 7. Active, Not-Active, and \nCorrect Activeness The .nalde.nitiontoexplainiscorrect(a,T ), which is used to type-check program states \nand inatomic expressions. This judg\u00adment, de.ned formally in the technical report and summarized in Figure \n7, ensures that each a is correct: a = if and only if exactly one thread is in a transaction, and a \n= . if and only if no thread is in a transaction. Without this invariant, the ma\u00adchine might be stuck. \nFor example, if a = , no thread is in a transaction, and every thread is blocked waiting to enter a transac\u00adtion, \nthen no thread can proceed. The detailed rules for active(e) (and active(T )) require some care. There \nmust be exactly one inatomic expression in e (or T ), not counting possibly nested transactions inside \nit, and that one outermost transaction must occur in a thread s active position. For example, we may \nbe able to show active(seq(inatomic(., 17, \u00b7, \u00b7),e)), but we cannot show active(seq(e, inatomic(., 17, \n\u00b7, \u00b7))). To summarize, proving progress requires tight control over the connection between each a in \nthe program state andthe state of the threads the a describes, and this control is speci.ed with the \ncorrect(a,T )invariant. Prov\u00ading preservation requires establishing this invariant after each step, particularly \nwhen a thread enters or exits a transaction. Withthe abilityto type-checkheaps, thread-pools,and run-time \nexpressions, we can state and prove thekeylemmas: Lemma 3.2 Progress If f a; H; T , then either T is \nall values or ' ' ''' .a ; H ' ,T suchthat a; H; T . a ; H ; T . Lemma 3.3 Preservation If G f H :G, \ncorrect(a, T ), G; ot f T , and a; H; T . a ' ; H ' ; T ', then there exists some G ' extending ''' ' \nG suchthat G ' f H :G ' , correct(a ,T ), and G ' ; ot f T . Because \u00b7; ot f e : t implies the initial \nprogram state type-checks (i.e., f.; \u00b7; e), Theorem 3.1 is a corollary to Lemmas 3.2 and 3.3. 4. The \nWeak Language This section revisits the choice in StrongBasic that if one thread is executing a transaction, \nthen no other thread may access the heap. Allowing such heap accesses lets a race between transactional \nand nontransactional code violate a transaction s isolation. This prop\u00aderty is often referred to as weak \natomicity (Blundell et al. 2006), so we call our language with this behavior Weak. Manysoftware im\u00adplementations \nof transactional memory use weak isolation because it is simpler to implement and usually improves performance. \nIn\u00adtuitively, if transactional and non-transactional code do not access the same memory, then allowing \nheap accesses concurrently with transactions does not lead to anyadditional behavior. The main the\u00adoremwe \npresentvalidatesthis intuition.Giventhe subtletiesof race conditions and isolation, it is wise not to \nrely on intuition alone. 4.1 Operational Semantics Changing StrongBasic to produce Weak is extremely \nsimple; we leave the syntax unchanged and replace the operational rules for reading and writing heap \nlocations: e t ::= int | reft t | t . t ' t ::= ot | wt e ::= emp | t G ::= \u00b7| G,x:t | G,l:(t,t) Figure \n8. Weak Type-System Syntax GET SET a; H;!l . a; H; H(l); \u00b7 a; H; l := v . a; H, l . v; v; \u00b7 That is, \n. is no longer required for heap accesses (but it is still required to enter a transaction). This new \nlanguage clearly allows every sequence of steps StrongBasic does (rules GET and SET apply strictly more \noften), andit allows more.Forexample, from theprogram state: .; l1 . 5,l2 . 6; (atomic (seq(l2 := 7,l1 \n:= !l2))) i (l2 := 4) Weak allows a sequence of steps where the .nal value in l1 is 4. Therefore, the \ntwo languages are not equivalent,but there are still manyprograms for which they are (i.e., anyresult \npossible in one language is possible in the other). In particular, it is intuitive that foraprogramto \ndistinguishthetwosemanticsit musthave thread\u00adshared mutable data that is accessed inside and outside \ntransactions. We now de.ne a type system that allows only programs for which the two languages are equivalent. \n 4.2 Type-And-Effect Systemfor EnsuringSerializability Our type-and-effect system enforces a partition \nin which each memory location can be accessed outside transactions or inside transactionsbut not both. \nMoreexpressive type systems are pos\u00adsible (see Section 6),but this system suf.ces for showing thekey \nideas required to prove equivalence assuming a static discipline. It also corresponds to the partition \nenforced by the monads in STM Haskell (Harris et al. 2005). The syntax for types is in Figure 8. Our \neffects are the same as in StrongNestedParallel;the difference is we now use them to restrict heap accesses. \nAs such, reference types now carry an anno\u00adtation indicatinga sideofa partition.Forexample, refwt (refot \nint) is the type of an expression that produces a label that can be ac\u00adcessed (read or written) inside \ntransactions and that containsalabel that can be accessed outside transactions (and the pointed-to label \ncontains an integer). Notice pointers from one side of the partition to the other are allowed. Continuing \nour example, if x has type refwt (refot int), then (atomic (!x)) := 42 would type-check. Our typing judgment \nhas the same form as before, G; e f e : t, meaning e has type t and effect e where e being wt, ot, or \nemp means e is safe inside transactions, outside transactions, or anywhere, respectively.Infact,except \nfordisallowing spawnoc e and spawnip e (because like in StrongBasic we have only top\u00adlevel spawn), most \nof the typing rules are identical to those in StrongNestedParallel. The differences are in Figure 9. \nRules T-SET and T-GET require the annotation on the reference type to be the same as the overall effect, \nwhich is what enforces the partition on all accesses. Notice rule T-REF has no such requirement; it is \nsafe to allocate an ot reference inside a transaction and vice-versa. (At allocation-time the new memoryis \nthread-local.)Toextend the type system to run-time states, T-LABEL uses G to determine the t for the \naccessed label. This t can differ from the effect of the expression because t controls access to the \nlabel s contents. As before, we extend the type system only for the proofs; the partition and annotations \nare entirely conceptual (i.e., types are erasable). G; e f e : t T-SET T-GET T-REF T-LABEL T-INATOMIC \nG; t f e1 : reft t G; t f e2 : t G; t f e : reft t G; e f e : t G(l) = (t, t) G; wt f e : t correct(a, \ne) G; t f e1 := e2 : t G; t f !e : t G; e f refe : reft t G; e f l : reft t G; e f inatomic(e) : t Figure \n9. Weak Type System (omitted rules and de.nitions are the same asin Figures5,6, and7) Theproofsofpreservationand \nprogressfor Weak are similar to the proofs for StrongNestedParallel,but type safety now ensures evaluationpreserves \nthe heap partition. This invariant is necessary for the equivalence result we discuss next.  4.3 Weak/Strong \nEquivalence UnderPartition Our primary result is that any program that type-checks has the same possible \nbehaviors under StrongBasic and Weak.Formally, letting . * s mean0 or more steps under the strong semantics \nand . * w mean0 or more steps under the weak semantics wehave: Theorem 4.1 Equivalence If \u00b7; ot f e : \nt ,then.; \u00b7; e . * s a; H; T if and only if .; \u00b7; e . * a; H; T . w Infact, the equivalence is stronger; \nthe two semantics can pro\u00adduce the same states in the same number of steps. One direction of the proof \nis trivial: any sequence of transitions under Strong-Basic is also a sequence of transitions under Weak. \nThe other di\u00adrection (given a weak transition sequence, produce a strong transi\u00adtion sequence) is much \nmore interesting. Space constraints compel ahigh-level descriptionbutthefullproofisavailable(Mooreand \nGrossman 2007). Westrengthenthe inductionhypothesisas follows:If Weak can produce a; H; T in n steps, \nthen StrongBasic can produce a; H; T in n steps. Moreover, if a = , then StrongBasic can produce a; H; \nT in n steps using a sequence where a suf.x of the sequence is the active thread entering the transaction \nand then taking some number of steps without steps from any other threads interleaved. In otherwords,the \ncurrent transaction couldhaverun seriallyatthe end of the sequence. In maintaining this stronger invariant, \nthe interesting case is when the next step under Weak is done by a thread not in the transaction. A key \nlemma lets us permute this non-transactional step to the position in the strong-semantics sequence just \nbefore the current transaction began, and the ability to permute like this without affecting the resulting \nprogram state depends precisely on the lack of memory con.icts that our type system enforces. It is clear \nthat this equivalence proof relies on notions similar to classic ideas in concurrent computation such \nas serializability and reducibility. Note, however, that the theorem is purely in terms of two operational \nsemantics. It says that given the type system enforcing a partition, the language de.ned in Section 2 \nmay be correctly implemented by the language de.ned in Section 4. This result is directly useful to language \nimplementors and does not require a notion of serializability. However, the Weak language remains unrealistic \nbecause its transactions never execute partially and then abort. The following section s language incorporates \nthis signi.cant complication. 5. The WeakUndo Language In practice, many implementations of transactions \nemploy abort\u00adand-retry, undoing any changes a transaction has made and start\u00ading again from the original \natomic expression. There are various reasons to do this, such as avoiding a memory con.ict with a con\u00adcurrentlyexecuting \ntransactionornothavinga transactionstaylive e ::= ... | inatomic(a, e, Hlog,e0) | inrollback(Hlog,e0) \nFigure 10. WeakUndo Syntax(extends Figure1) acrossa thread context-switch. Because these and other reasons \nare low-level details not visible in the source language,3 the best way to model abort-and-retry at a \nhigh-level is to let a transaction undo its changes nondeterministically at anypoint. Our WeakUndo lan\u00adguage \ndoes precisely this, revealing interesting interactions with both nested transactions and weak isolation. \n 5.1 Syntax and Operational Semantics Figure 10 presents the new syntax andFigure 11 presents the new \nevaluation rules. Our source language is the same as for StrongBasic and Weak. At run-time, transactions \nneed extra state to enable rollback, so we have inatomic(a, e, Hlog,e0). The a indicates whether the \ncompu\u00adtation e currentlyhasanested transactionlikein StrongNestedPar\u00adallel. This is important because \nwe cannot perform an undo when there is a nested transaction; it must be completed or undone .rst. The \nHlog is a log of labels written to and the values theyhad be\u00adfore the assignment.To undoa transaction \ns memoryeffects, we use the log to undo the assignments in last-in-.rst-out order. Syn\u00adtactically, Hlog \nmaps labelstovalueslikeaheap,butunlikeaheap it is the .rst (or leftmost)entry for a label that holds \nthe relevant value. Finally, e0 is the transaction s initial expression, so a trans\u00adaction starts by \natomic e0 becoming inatomic(.,e0, \u00b7,e0) (i.e., no nested transaction and an empty log). Toundoa transaction,werollbacktheheap \nassignmentsoneat a time using the log. The syntax inrollback(Hlog,e0) maintains the state of an undo. \nIt is like inatomic except we do not need any a or e. The log gets smaller as evaluation rolls back entries. \nFor the operational semantics, an evaluation step for one thread produces Hlog, which contains the entries \nthat must be appended to the log in the nearest enclosing transaction. CONTEXT propagates such entries \nand rules like SEQ (not shown) produce the empty log \u00b7. SET produces the one-entry log l . H(l), i.e., \nl maps to the value before the assignment while updating the heap asusual. GET produces an empty log; \nthere is nothing to undo. Most importantly, INATOMIC appends the log produced by the subexpression eval\u00aduation4 \nand propagates the empty-log. If a transaction eventually aborts,itneverpropagatesanylog entries. Else, \nCOMMIT propa\u00adgates the entire log for the next enclosing transaction in one step. Because logs are unneeded \noutside transactions, PROGRAM does not use thelog itshypothesis produces. Arollback occurs by usingENTER \nROLLBACK, then using one DO ROLLBACK for each log entry, and .nally using COMPLETE ROLLBACK. To begin \nrollback, there must be no nested transac\u00ad 3In the case of memory con.icts,false-sharing issues that \narise from de\u00adtecting con.icts at a coarse granularity (e.g., using hashing or cache-lines) can make \na con.ict unpredictable at the source level. '' 4By HlogH we mean the log where H follows Hlog, so the \nlast-in\u00ad log log ' .rst-out undo will process the H entries before the Hlog entries. log ' '' a; H; e \n. a ; H ; e ; T ; Hlog CONTEXT ' '' SET GET a; H; e . a ; H ; e ; T ; Hlog '' ' a; H; E[e] . a ; H \n; E[e ]; T ; Hlog a; H; l := v . a; H, l . v; v; \u00b7; l . H(l) a; H;!l . a; H; H(l); \u00b7; \u00b7 INATOMIC ENTER \nATOMIC ''' ' a; H; e . a ; H ; e ; \u00b7; Hlog ' ''' .; H; atomic e . ; H; inatomic(., e, \u00b7,e); \u00b7; \u00b7 ; H; \ninatomic(a, e, Hlog,e0) . ; H ; inatomic(a ,e ,HlogHlog,e0); \u00b7; \u00b7 COMMIT ENTER ROLLBACK ; H; inatomic(., \nv, Hlog,e0) ..; H; v; \u00b7; Hlog ; H; inatomic(., e, Hlog,e0) . ; H; inrollback(Hlog,e0); \u00b7; \u00b7 DO ROLLBACK \nCOMPLETE ROLLBACK ; H; inrollback(Hlog,l . vold,e0) . ; H, l . vold; inrollback(Hlog,e0); \u00b7; \u00b7 ; H; inrollback(\u00b7,e0) \n..; H; atomic e0; \u00b7; \u00b7 ' '' a; H; T . a ; H ; T PROGRAM ' '' a; H; e . a ; H ; e ; T ; Hlog '' ' a; \nH; T1 i e i T2 . a ; H ; T1 i e i T2 i T Figure 11. WeakUndo Operational Semantics (selected rules omitted) \n. * . * . * . * . * a lx ly lz lm ln Thread 1 Thread 2 . 0 0 0 0 0 atomic (if (!lx) (ly := 1) (lz := \n1)) lx := 1; ln :=!lz; lm :=!ly 0 0 1 0 0 inatomic(., 1, lz . 0, (if (!lx) (ly := 1) (lz := 1))) lx := \n1; ln :=!lz; lm :=!ly 1 0 1 0 1 inatomic(., 1, lz . 0, (if (!lx) (ly := 1) (lz := 1))) lm :=!ly . 1 0 \n0 0 1 atomic (if (!lx) (ly := 1) (lz := 1)) lm :=!ly . 1 1 0 0 1 1 1 . 1 1 0 1 1 1 1 Figure 12. Selected \nstates froma WeakUndo trace, starting with .; H; T where H maps each label to 0 and T contains Thread1andThread \n 2. In the end, lm and ln map to 1. Under Weak, Thread1could not change both ly and lz. tion else the \nentries in its log would be lost. The . in the inatomic expression in ENTER ROLLBACK enforces this requirement. \nDur\u00ading rollback, the top-level transaction state remains ;we cannot start another transaction until \nthe rollback is complete. Finally, COMPLETE ROLLBACK produces an atomic expression ready to (re)execute,but \nanother transaction may run .rst. After rollback all labels have the values they had before the transactionbegan,but \nthe heapis notexactly the same. Memory allocation (rule ALLOC, not shown in Figure 11) does not produce \na log entry and rollback does not remove the newlabel. However, it willbe unreachable (i.e.,garbage) \nafterthetransaction rolls back. Figure 12 shows an example trace possible in WeakUndo but not in Weak \nor StrongBasic;it is similar to the example in Sec\u00adtion 1. Each row represents a program state (not every \nstate is shown). The transaction in Thread1rolls back between the third and fourth rows and its reexecution \ntakes the other conditional branch. However, Thread 2 uses nontransactional code to see a write from \nThread1before that writeis rolledback. Notice the ini\u00adtial state shown would not type-check under our \neffect system for partitioning the heap because lx, ly, and lz are all accessed inside (Thread1) and \noutside (Thread2) transactions.  5.2 Type-And-Effect System For source programs, we can use the same \ntype system we do for Weak to ensure the same memory is not used inside and outside transactions. Extending \nthe type system to the new inatomic and inrollback forms requires maintaining a number of technical in\u00advariants.Forexample,alllog \nentries mustbefor labelswitheffect wt. Given these details (see the technical report), Preservation and \nProgress hold as in the other AtomsFamily languages. Though we believe WeakUndo is equivalent to StrongBasic, \nfor simplicity our current proof of this result does not consider nested transactions.Fortunately, formalizing \nthis simpli.cation re\u00adquires onlya small change.We can forbid nested transactionsby type-checking atomic \ne only under effect ot rather than e: T-ATOMIC G; wt f e : t G; ot f atomic e : t Type-checking inatomic \nrequires an analogous change. Preserva\u00adtion and Progress also hold for this more restrictive type system. \n 5.3 WeakUndo/Strong Equivalence Using the type system described above and letting . * s mean0 or more \nsteps under StrongBasic and . * mean 0 or more steps wu under WeakUndo, we have the following theorem: \nTheorem 5.1 Equivalence If \u00b7; ot f e : t , then: (1) If .; \u00b7; e . * .; H; T , then .; \u00b7; e . * .; H; \nT and s wu (2) If .; \u00b7; e . * .; H; T , then there exists an H ' such that wu .; \u00b7; e . * s .; H ' ; \nT and for all l and v, if H ' (l) = v, then H(l) = v. This theorem is weaker than the analogous theorem \nfor Weak in two interesting ways. First, WeakUndo may produce a larger heap (the domain of H may exceed \nthe domain of H ',but H restricted to the domain of H ' must be H ')because aborted transactions can \ngenerategarbage. Second, the equivalence holds only when a is ., i.e., no transaction is executing. If \na is , then WeakUndo may be performingarollback and StrongBasic has no corresponding state. Also, as \na technical point, the different syntax for inatomic in the languages precludes having syntactically \nequal programs whenever a transaction is executing. While part (1) of the theorem is trivial, part (2) \nis not. The proof, detailed in the technical report and brie.y summarized here, cannot ignore states \nwhere a is . Instead, we must show that transactions in WeakUndo are serializable (as in the proof for \nWeak) and that rollback is correct (produces a state close enough to the one before the transaction began). \nBecause the latter is much easier to show under strong isolation, we de.ne an intermediate language StrongUndo \nin which we have inrollback and the corresponding evaluation rules as in WeakUndo but when one thread \nis executing a transaction,nootherthreadcanexecute.Wethenhavethesetwo lemmas (with . * for evaluation \nunder StrongUndo): su Lemma 5.2 If \u00b7; ot f e : t and .; \u00b7; e .wu * a; H; T , then .; \u00b7e . * a; H; T . \nsu Lemma 5.3 If \u00b7; ot f e : t and .; \u00b7; e .su * .; H; T , then there exists an H ' such that .; \u00b7; e \n. * s .; H ' ; T and for all l and v, if H ' (l)= v, then H(l)= v. Proving the .rst lemma follows exactly \nthe proof strategy de\u00adscribed in Section 4.3 for Weak and StrongBasic, with addi\u00adtional cases for the \nrollback steps. Proving the second lemma re\u00adquiresa strengthened inductionhypothesis arguing that whenever \nStrongUndo is executing a transaction, all the following hold: If StrongUndo is not rolling the transaction \nback, then Strong-Basic could get to a similar state.  If StrongUndo is not rolling the transaction \nback,but it chose to from this point, then it would produce a state just like before the transaction \nstarted (plus possiblegarbage).  If StrongUndo is rolling the transaction back, then after com\u00adpletingthe \nrollbackitwillhaveastatejustlikebeforethe trans\u00adaction started (plus possiblegarbage).  As a corollary, \nWeak and WeakUndo are equivalent for well\u00adtyped programs because both are equivalent to StrongBasic. \nWe were surprised that we did not prove WeakUndo equivalent to Weak directly,but it is not clear to us \nhow to do so. StrongUndo turned out to be a crucial technical tool. Abadi et al. (2008) inde\u00adpendently \nreached a very similar conclusion, which indicates that this approach is indeed the natural one. 6. FutureWork \nBecause the AtomsFamily approach is amenable to investigating different features, there are many directions \nfor future work. We .rstdescribealanguage thatisinmany ways dualto WeakUndo but for which we have not \nyet proven relevant theorems.We then consider other ways to de.ne transactional semantics, make our type \nsystems more expressive, or add new language features. 6.1 The WeakOnCommit Language Insteadof supporting \nabort-and-retrybykeepingalogof oldval\u00adues, we can maintain a private copy of updated heap values in a \ntransaction and propagate updated values only when a transaction commits.Wehave fully de.ned the syntax \nand operational seman\u00adtics of such a language (see the technical report), which we call WeakOnCommit,but \nhave not yet proven safety andequivalence results for this language. This section sketches WeakOnCommit \nand its relation to WeakUndo. At run-time, transactions have the form inatomic(e, H, e0) where e and \ne0 are like in WeakUndo,but H is an on-commit heap containing labels allocated or updated by the transaction \nso far. Insidea transaction,anyassignmentor allocationis propagated out only to the innermost transaction, \nwhere it is added to the H.To read a reference, the operational semantics looks .rst at the heap for \nthe closest containing transaction and then in the next closest heap if it is not there. To handle this \ngracefully in a formal operational semantics, the judgment for evaluating an expression uses a stack \nof heaps S where S is de.ned inductively as empty or a stack S::H where H is the shallowest stack element. \nWe then have the judgment a; S; e . a ' ; S ' ; e ' ; T . Outside of a transaction, S is just the outermost \nheap, i.e., the H in the program state a; H; T . Inside a transaction, we have a deeper stack: INATOMIC \n' '' a; S::H; e . a ; S::H ; e ; \u00b7 ; S; inatomic(e, H, e0) . ; S; inatomic(e ' ,H ' ,e0); \u00b7 Evaluation \nof e can change H (which is empty when the transaction starts)but not S.For example, the rule for assignment \nis: SET a; S::H; l := v . a; S::(H, l . v); v; \u00b7 However, evaluation needs the entire stack S::H because \nthe rule for !l searches the stack in order from H outward. Aborting a transaction in WeakOnCommit takes \nonly one step and can apply even if there is a nested transaction: ROLLBACK ; S; inatomic(e, H, e0) ..; \nS; atomic e0; \u00b7 On the other hand, to commit a transaction we use the new syn\u00adtax form incommit(H, v). \nA transaction inatomic(v, H, e0) can step to incommit(H, v), after which abort is impossible. Then el\u00adements \nof H are propagated out one label at a time, removing them from H, and .nally incommit(\u00b7,v) becomes v. \nWhereas in WeakUndo the heap in inrollback(H, e0) maps labels to old val\u00adues, in WeakOnCommit the H in \nincommit(H, v) maps labels to new values. For programs that do not type-check under our type-and-effect \nsystem, strange behaviors can arise. As in actual implementations, wehavede.nedthe in-commit rulesto \npropagatethenewvaluesfor the labels in an arbitrary order.5 Hence, nontransactional code rac\u00ading with \natomic (seq(x := 1,y := 1)) could see the assignment to y before the assignment to x. Our prior work \nshows how this .exi\u00adbility leads to strange results (Shpeisman et al. 2007). In the future, we intend \nto prove WeakOnCommit equivalent to StrongBasic for well-typed programsandtoexploretheextentto which \nWeakUndo and WeakOnCommit are equivalent for ill-typed programs. 6.2 MorePermissive Semantics There \nare several ways to relax the type-and-effect system for Weak and WeakUndo without invalidating our equivalence \nresults. Forexample,we couldhaveinvariantsfor thread-localor read-only data because both can be accessed \ninside and outside transactions without interleaving with other threads causing problems. Another extension \nwould be partition polymorphism, which would allow some functions to take arguments that couldpoint into \neither side 5Implementations have strange orders if, for example, they use hashtables. of the partition, \ndepending on the call-site. This extension would require type-level variables that range over effects. \nThe AtomsFamily can also be extended with languages that have more permissive dynamic semantics (i.e., \nallow more behav\u00adiors).Forexample,we could support open-nestingbyhavingacon\u00adstruct open(e) where the \neffects of e are never undone even if an enclosing transaction aborts (Moss and Hosking 2005). Hopefully \nwe can de.ne suf.cient conditions under which open-nesting is safe in the sense that other threads cannot \ndetermine that a trans\u00adaction aborted.Wewould also liketoinvestigate relaxed memory models (Grossman \net al. 2006; Manson et al. 2005), which can be awkward because it is unnatural for a formal operationalsemantics \nnot to be sequentially consistent. 6.3 Other Language Interactions More languages similar to the AtomsFamily \ncould allowadditional constructs and combinations thereof that merit investigation. For example, combining \nthe weak isolation of Weak and the nested parallelism of StrongNestedParallel is straightforward for \nthe se\u00admantics,butthetype system adjustments neededto preserveequiv\u00adalence remain unclear. In addition, \nthe interaction of transactions withexceptions (Harris 2004; RingenburgandGrossman 2005) or .rst-class \ncontinuations (Kimball and Grossman 2007) needs to be de.ned precisely. Programs using transactions also \nneed fairness guarantees from the thread scheduler and con.ict manager; inte\u00adgrating such guarantees \ninto our models would be valuable. 7. RelatedWork 7.1 PriorWork on Operational Semantics The most closely \nrelated prior work uses operational semantics to de.ne various aspects of transactions. All such work \nwe are aware of has signi.cantly different foci and techniques, either focusing on implementation-level \nissues or modeling transactions asasingle computational step. First,Jagannathanetal.(2005)andViteketal. \n(2004)useavari\u00adant of Featherweight Java (Igarashi et al. 2001) to de.ne a frame\u00adworkinwhichdifferent \ntransactional implementations(suchasver\u00adsioning or two-phase locking) can be embedded and proven correct \nby establishing a serializability result. They support parallelism within transactions by requiring each \nthread in the transaction to execute a commit statement for the transaction to complete. This is similarbut \nnot identical to our spawnip;theyhave no analogue of our other spawn .avors nor any effect system. Formally, \nthey assume all code executes within a transaction; there is no notion of weak isolation. Their run-time \nstate and semantics is, in our view, more complicated, with thread identi.ers, nested heaps, andtraces \nof actions. While some of this machinery may be necessary for proving lower-level implementation strategies \ncorrect, it is less de\u00adsirable for a high-level model. Though their system and ours have manytechnical \ndifferences, the fundamental idea of permuting in\u00addependent actions arises (unsurprisingly)inboth settings. \nSecond, Harris et al. (2005) present an operational semantics for STM Haskell. Like our work, it is high-level, \nwith one transaction executing at a time. However, the semantics is layered such that an entire transaction \noccurs as one step at the outer layer, essentially usingalarge-stepmodelfor transactionsthatdoesnotlenditselfto \ninvestigating nested parallelism nor weak isolation. Indeed, theydo not have nested parallelism and the \npartition between mutable data accessed inside and outside transactions (enforced by a monad) lets them \nde.ne strong isolation yet implement weak isolation. It is not signi.cant that we enforced a partition \nwith an effect sys\u00adtem rather than monads as the two technologies have well-known connections(Wadler1999). \nRather,our contributionisprovingthat given a partition, strong and weak isolation are indistinguishable. \nThird,Wojciechowski (2005) proves isolation fora formal lan\u00adguage where transactions with nested parallelism \n(called tasks in the work) explicitly acquire locks before accessing data and the beginning of the task \nmust declare all the locks it might acquire. Explicit locking and version counters leads to a lower-level \nmodel and an effect system that is an extension of lock types (Flanagan and Abadi 1999). The main theorem \nessentially proves a particular low-level rollback-free transaction mechanism correct. Finally, Liblit \n(2006) gives an operational semantics for the hardware-based LogTM (Moravan et al. 2006). This assembly \nlan\u00adguage is at a much lower level. It has neither nested parallelism nor weak isolation. 7.2 ConcurrentWork \non Operational Semantics Concurrent with our work, Abadi et al. (2008) also developed a small-step operational \nmodel for transactions. Among various differences in the basic approach, the most signi.cant is that \nwe have a lexically scoped transaction(atomic (e)) whereas they have primitives for starting and ending \ntransactions. Because they prohibit starting a transaction within another, theydo not have any notion \nof nested transactions. Both projects investigated weak isolation with reassuringly sim\u00adilar results. \nIn our terms, Abadi et al. also proved WeakUndo equiv\u00adalent to StrongBasic and even followed the approach \nof using StrongUndo as an intermediate point. In proving Weak equivalent to StrongBasic, they used a \nsemantic notion of memory con.ict rather than our more restrictive syntactic type-and-effect system. \nBeyond weak isolation, the projects have considered different extensions. Abadi et al. have not considered \nparallelism within transactions. Instead, theyhave considered a model where multiple threads can execute \ntransactions simultaneously but any con.ict aborts all the transactions. This model reveals some additional \nanomalies that WeakUndo does not. 7.3 Unformalized Languages Manyrecent proposals for transactions in \nprogramming languages either do not discuss the effect of spawning inside a transaction or make it a \ndynamic error. In other words, to the extent it is considered, the most common .avor is spawntl. When \ndesigning the AtomCaml system (Ringenburg and Grossman 2005), we felt spawnoc would be most natural, \nbut it was the only option. The Venari system for ML (Haines et al. 1994) had something close to spawnip,butitwasuptothe \nprogrammerto acquire locksexplicitly in the style pioneered by Moss (1985). Weak isolation has primarily \nbeen consideredfor its surprising pitfalls, including its incomparability with strong isolation (Blun\u00addell \net al. 2006) and situations in which it leads to isolation vi\u00adolations that corresponding lock-based \ncode does not (Larus and Rajwar 2006; Hudson et al. 2006; Shpeisman et al. 2007). It is be\u00adlieved that \nall examples of the latter require violating the partition property we de.ned in Section 4, which is \nwhy we proved this re\u00adsult for Weak and WeakUndo. 7.4 Other Semantics Operational semanticsgives meaning \ndirectly to source programs, which lets us study how transactions interact with other language features, \nde.ne type systems, and provide a direct model to pro\u00adgrammers. Other computational models, based on \nnotions of mem\u00adory accesses or computation dependencies, can prove useful for investigating properties \nof transactions. Recent examples include work on specifyingfairness and con.icts (Scott 2006),work on \nus\u00ading the computation-centric model of Frigo and Luchangco (1998) to give semantics to open nesting \n(Agrawal et al. 2006), and work on de.ning open nesting in terms of transactions read and write sets \n(Moss and Hosking 2005). 8. Conclusions The AtomsFamily isacollectionof core languages that uses small\u00adstep \noperational semanticsto model software transactions.Wehave used this approach to investigate the precise \nmeaning of both paral\u00adlelism within transactions and weak isolation. Our approach reveals subtledifferences \namong similar semantics withoutexposing im\u00adplementation details of transactional memory.We have used \ntype\u00adand-effect systems to restrict programs so that evaluation does not get stuck and to establish restrictions \nunder which different Atoms-Family languages are equivalent. In general, our work brings a needed level \nof rigor to the def\u00adinition of programming languages containing software transac\u00adtions.We have provided \nseveral possible de.nitions, an approach that makes de.ning additional possibilities straightforward, \nand metatheory proofs that reveal thekeyinvariants needed for trans\u00adactions to work as expected. References \nMart\u00b4in Abadi, Andrew Birrell,Tim Harris, and Michael Isard. Semantics of transactional memory and automatic \nmutual exclusion. In 35thACM Symposium on Principles of Programming Languages, 2008. Ali-Reza Adl-Tabatabai, \nBrian Lewis, Vijay Menon, Brian R. Murphy, Bratin Saha, and Tatiana Shpeisman. Compiler and runtime support \nfor ef.cient software transactional memory. In ACM Conference on Programming Language Design and Implementation, \n2006. Kunal Agrawal, Charles E. Leiserson, and Jim Sukha. Memory models for open-nested transactions. \nIn ACM SIGPLANWorkshop on Memory SystemsPerformance&#38;Correctness, 2006. Eric Allen, David Chase, Joe \nHallet, Victor Luchangco, Jan-Willem Maessen, Sukyoung Ryu, Guy L. Steele Jr., and Sam Tobin-Hochstadt. \nThe Fortress language speci.cation, version 1.0\u00df, 2007. http://research.sun.com/projects/plrg/Publications/fortress1.0beta.pdf. \nColin Blundell,E Christopher Lewis, and Milo M. K. Martin. Subtleties of transactional memory atomicity \nsemantics. Computer Architecture Letters, 5(2), 2006. Brian D. Carlstrom, JaeWoong Chung, Austen McDonald, \nHassan Cha., Christos Kozyrakis, and Kunle Olukotun. The Atomos transactional programming language. In \nACM Conference on Programming Language Design and Implementation, 2006. Peter Damron, Alexandra Fedorova, \nYossi Lev, Victor Luchangco, Mark Moir, and Daniel Nussbaum. Hybrid transactional memory. In Interna\u00adtional \nConference on Architectural Support for Programming Languages and Operating Systems, 2006. Cormac Flanagan \nand Mart\u00b4in Abadi. Types for safe locking. In European Symposium on Programming, volume 1576 of Lecture \nNotes in Com\u00adputer Science, 1999. Matteo Frigo andVictor Luchangco. Computation-centric memory models. \nIn ACM Symposium onParallel Algorithms andArchitectures, 1998. Dan Grossman, Jeremy Manson, and William \nPugh. What do high-level memory models mean for transactions? In ACM SIGPLANWorkshop on Memory SystemsPerformance&#38;Correctness, \n2006. Nicholas Haines, Darrell Kindred, J. Gregory Morrisett, Scott M. Nettles, and JeannetteM.Wing. \nComposing .rst-class transactions. ACMTrans. on Programming Languages and Systems, 16(6):1719 1736, 1994. \nTim Harris. Exceptions and side-effects in atomic blocks. In PODC Workshopon ConcurrencyandSynchronizationinJavaPrograms,2004. \nTim Harris andKeir Fraser. Language support for lightweight transactions. In ACM Conference on Object-Oriented \nProgramming, Systems, Lan\u00adguages, and Applications, 2003. Tim Harris, Simon Marlow, Simon Peyton Jones, \nand Maurice Herlihy. Composable memory transactions. In ACM Symposium on Principles and Practice ofParallel \nProgramming, 2005. Tim Harris, Mark Plesko,Avraham Shinnar,andDavidTarditi. Optimizing memory transactions. \nIn ACM Conference on Programming Language Design and Implementation, 2006. Maurice Herlihy,Victor Luchangco, \nand Mark Moir.A.exible framework for implementing software transactional memory. In ACM Conference on \nObject-Oriented Programming, Systems, Languages, and Applica\u00adtions, 2006. Richard Hudson, Bratin Saha, \nAli-Reza Adl-Tabatabai, and Benjamin Hertzberg. McRT-Malloc: A scalable transactional memory allocator. \nIn International Symposium on Memory Management, 2006. Atsushi Igarashi, Benjamin C. Pierce, and Philip \nWadler. Featherweight Java:a minimal core calculusforJavaandGJ. ACMTrans. onProgram\u00adming Languages and \nSystems, 23(3), 2001. Suresh Jagannathan, Jan Vitek, Adam Welc, and Antony L. Hosking. A transactional \nobject calculus. Science of Computer Programming, 57(2), 2005. Aaron Kimball and Dan Grossman. Software \ntransactions meet .rst-class continuations. In 8th Annual Workshop on Scheme and Functional Programming, \n2007. SanjeevKumar, Michael Chu, Christopher J. Hughes,ParthaKundu, and AnthonyNguyen. Hybrid transactional \nmemory. In ACM Symposium on Principles andPracticeofParallelProgramming, 2006. James R. Larus and Ravi \nRajwar. Transactional Memory. Morgan &#38; Claypool Publishers, 2006. Ben Liblit. An operational semantics \nfor LogTM. Technical Report 1571, UniversityofWisconsin Madison, 2006. Jeremy Manson, William Pugh, and \nSarita V. Adve. The Java memory model. In 32ndACM Symposium on Principles of Programming Lan\u00adguages, \n2005. Virendra J. Marathe, William N. Scherer, and Michael L. Scott. Adap\u00adtivesoftware transactional \nmemory. In International Symposium on Dis\u00adtributed Computing, 2005. Katherine F. Moore and Dan Grossman. \nHigh-level small-step opera\u00adtional semantics for transactions (technical companion). Technical re\u00adport, \nUniv. ofWash. Dept. of Computer Science&#38;Engineering, 2007. http://www.cs.washington.edu/homes/kfm/atomsfamily \nproofs.pdf. MichelleJ. Moravan, Jayaram Bobba,KevinE. Moore, LukeYen, MarkD. Hill, Ben Liblit, Michael \nM. Swift, and David A. Wood. Supporting nested transactional memory in LogTM. In 12th International Confer\u00adence \non Architectural Support for Programming Languages and Oper\u00adating Systems, 2006. J. Eliot B. Moss. NestedTransactions:AnApproachto \nReliable Distributed Computing. The MIT Press, 1985. J. Eliot B. Moss and Antony L. Hosking. Nested transactional \nmemory: Model and preliminary architecture sketches. In Synchronization and Concurrency in Object-Oriented \nLanguages (SCOOL), 2005. MichaelF. Ringenburgand Dan Grossman. AtomCaml: First-class atomic\u00adity via rollback. \nIn 10thACM International Conference on Functional Programming, 2005. Michael L. Scott. Sequential speci.cation \nof transactional memory seman\u00adtics. In Workshop on Languages, Compilers, and Hardware Support for Transactional \nComputing (TRANSACT), 2006. Tatiana Shpeisman, Vijay Menon, Ali-Reza Adl-Tabatabai, Steve Balen\u00adsiefer, \nDan Grossman, Richard Hudson, KatherineF. Moore, and Bratin Saha. Enforcing isolation and ordering in \nSTM. In ACM Conference on Programming Language Design and Implementation, 2007. MichaelF. Spear,Virendra \nJ. Marathe, Luke Dalessandro, and Michael L. Scott. Privatization techniques for software transactional \nmemory. Technical Report 915, Computer Science Department, University of Rochester, 2007. Jan Vitek, \nSuresh Jagannathan, Adam Welc, and Antony L. Hosking. A semantic framework for designer transactions. \nIn European Symposium on Programming, volume 2986 of Lecture Notes in Computer Science, 2004. PhilipWadler. \nThe marriage of effects and monads. In 3rdACM Interna\u00adtional Conference on Functional Programming, 1999. \nPawel T. Wojciechowski. Isolation-only transactions by typing and ver\u00adsioning. In ACM International Conference \non Principles and Practice of Declarative Programming, 2005.     \n\t\t\t", "proc_id": "1328438", "abstract": "<p>Software transactions have received significant attention as a way to simplify shared-memory concurrent programming, but insufficient focus has been given to the precise meaning of software transactions or their interaction with other language features. This work begins to rectify that situation by presenting a family of formal languages that model a wide variety of behaviors for software transactions. These languages abstract away implementation details of transactional memory, providing high-level definitions suitable for programming languages. We use small-step semantics in order to represent explicitly the interleaved execution of threads that is necessary to investigate pertinent issues.</p> <p>We demonstrate the value of our core approach to modeling transactions by investigating two issues in depth. First, we consider parallel nesting, in which parallelism and transactions can nest arbitrarily. Second, we present multiple models for weak isolation, in which nontransactional code can violate the isolation of a transaction. For both, type-and-effect systems let us soundly and statically restrict what computation can occur inside or outside a transaction. We prove some key language-equivalence theorems to confirm that under sufficient static restrictions, in particular that each mutable memory location is used outside transactions or inside transactions (but not both), no program can determine whether the language implementation uses weak isolation or strong isolation.</p>", "authors": [{"name": "Katherine F. Moore", "author_profile_id": "81331499931", "affiliation": "University of Washington, Seattle, WA", "person_id": "P871679", "email_address": "", "orcid_id": ""}, {"name": "Dan Grossman", "author_profile_id": "81405594870", "affiliation": "University of Washington, Seattle, WA", "person_id": "PP43120333", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1328438.1328448", "year": "2008", "article_id": "1328448", "conference": "POPL", "title": "High-level small-step operational semantics for transactions", "url": "http://dl.acm.org/citation.cfm?id=1328448"}