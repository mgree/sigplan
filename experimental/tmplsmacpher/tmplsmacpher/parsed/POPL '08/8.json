{"article_publication_date": "01-07-2008", "fulltext": "\n Cyclic Proofs of Program Termination in Separation Logic * James Brotherston Richard Bornat Cristiano \nCalcagno Imperial College, London, UK Middlesex University, London, UK Imperial College, London, UK J.Brotherston@imperial.ac.uk \nR.Bornat@mdx.ac.uk ccris@doc.ic.ac.uk Abstract We propose a novel approach to proving the termination \nof heap\u00admanipulating programs, which combines separation logic with cyclic proof within a Hoare-style \nproof system. Judgements in this system express (guaranteed) termination of the program when started \nfrom a given line in the program and in a state satisfying a given precondition, which is expressed as \na formula of separation logic. The proof rules of our system are of two types: logical rules that operate \non preconditions; and symbolic execution rules that capture the effect of executing program commands. \nOur logical preconditions employ inductively de.ned predicates to describe heap properties, and proofs \nin our system are cyclic proofs: cyclic derivations in which some inductive predicate is un\u00adfolded in.nitely \noften along every in.nite path, thus allowing us to discard all in.nite paths in the proof by an in.nite \ndescent argu\u00adment. Moreover, the use of this soundness condition enables us to avoid the explicit construction \nand use of ranking functions for ter\u00admination. We also give a completeness result for our system, which \nis relative in that it relies upon completeness of a proof system for logical implications in separation \nlogic. We give examples illustrat\u00ading our approach, including one example for which the correspond\u00ading \nranking function is non-obvious: termination of the classical algorithm for in-place reversal of a (possibly \ncyclic) linked list. Categories and Subject Descriptors F.3.1 [Logics and Mean\u00adings of Programs]: Specifying \nand Verifying and Reasoning about Programs Logics of programs General Terms Veri.cation, theory, reliability \nKeywords separation logic, termination, cyclic proof, program veri.cation, inductive de.nitions, Hoare \nlogic 1. Introduction Termination is an essential requirement of many computer pro\u00adgrams yet, as every \nundergraduate computing student learns, decid\u00ading whether a particular program terminates on a given \ninput is, in general, impossible. Thus research into proving program termina\u00adtion must focus on the development \nof appropriate frameworks and * Supported by EPSRC. The authors also gratefully acknowledge Josh Berdine, \nPeter O Hearn, Alex Simpson and Hongseok Yang for enlight\u00adening discussions, and the anonymous referees \nfor their helpful comments and suggestions. Permission to make digital or hard copies of all or part \nof this work for personal or classroom use is granted without fee provided that copies are not made or \ndistributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. POPL 08, January 7 12, 2008, San Francisco, California, USA. Copyright \nc &#38;#169; 2008 ACM 978-1-59593-689-9/08/0001. . . $5.00 proof search heuristics for termination proofs \nfor various classes of program. In this paper, we give a proof system, based upon separa\u00adtion logic and \nemploying the method of cyclic proof, that is tailored towards proving termination of simple imperative \nprograms. Many important computer programs in use today are written in low-level imperative languages, \nand the ability to establish termi\u00adnation of such programs is thus clearly desirable. However, imper\u00adative \nprograms have often proven resistant to formal analysis, at least partially due to the dif.culty of reasoning \nabout their use of pointer arithmetic and similar operations that operate on data stored in shared structures, \nsuch as the heap. Recently, separation logic was developed to help overcome this impasse by providing \nlogical tools to reason about shared, mutable resource (Reynolds 2002). As well as the usual additive \nconnectives of .rst-order logic, sepa\u00adration logic employs multiplicative connectives to express proper\u00adties \ninvolving heap resource. The multiplicative conjunction * de\u00adnotes a division of the heap into two parts \nin which each conjunct holds respectively, and the multiplicative implication * expresses a property \nof resource addition: if an arbitrary heap satis.es the antecedent formula and is disjoint with the current \nheap, then the combined heap satis.es the consequent formula. Separation logic has successfully underpinned \nseveral program veri.cation applica\u00adtions to date, including the Smallfoot automated tool (Berdine et \nal. 2006a), local shape analysis (Distefano et al. 2006; Calcagno et al. 2006), inductive recursion synthesis \n(Guo et al. 2007), total cor\u00adrectness proofs of non-trivial algorithms (Torp-Smith et al. 2004; Bornat \net al. 2004) and, pertinently for our purposes, automated termination checking (Berdine et al. 2006b). \nExisting work on program veri.cation in separation logic has relied on the identi.cation and use of suitable \ninductive de.ni\u00adtions to express shape properties of the heap (at some given point in the execution of \nthe program). A recent paper by the .rst au\u00adthor (Brotherston 2007) developed a general framework for \ninduc\u00adtive de.nitions in O Hearn and Pym s logic of bunched implica\u00adtions BI (O Hearn and Pym 99), which \nunderlies the pure-logical part of separation logic, and gave proof systems for the extension, including \nan appropriate notion of cyclic proof. For logics featur\u00ading inductive predicates or similar .xed-point \nconstructions, cyclic proof provides an alternative to traditional inductive proof, mod\u00adelled on Fermat \ns in.nite descent (Brotherston 2006; Brotherston and Simpson 2007; Sprenger and Dam 2003). In the case \nof induc\u00adtively de.ned relations, one way of stating this principle is: if in some case of a proof some \ninductive de.nition is unfolded in.nitely often, then that case may be disregarded. Essentially, this \nprinciple is sound because each inductive de.nition has a least-.xed point interpretation which can be \nconstructed as the union of a chain of approximations, indexed by ordinals; unfolding a de.nition in\u00ad.nitely \noften can thus be seen as inducing an in.nite descending chain of these ordinals, which contradicts their \nwell-foundedness. In cyclic proof systems, the capacity for unfolding a de.nition in\u00ad.nitely often is \nbuilt in to the system by allowing proofs to be non\u00adwell-founded, i.e. to contain in.nite paths. Such \nproof structures are, in general, unsound, so a global well-formedness condition is additionally imposed \non proofs to ensure that every in.nite path can be disregarded by the in.nite descent principle outlined \nabove. The portion of proof that is not disregarded by this principle is thus .nite, and sound for standard \nreasons. Our main contribution in this paper is the formulation of a cyclic proof system tailored to \nproving termination of programs written in a simple, yet relatively expressive imperative programming \nlan\u00adguage. Given some .xed program, the judgements of this system express guaranteed (non-faulting) termination \nof the program when started from a given line in the program and in a state satisfying a given precondition, \nwhich is expressed as a formula of separation logic. The proof rules of the system are of two types: \nlogical rules that operate on the precondition, and symbolic execution rules that simulate program execution \nsteps. Thus, a program execution path corresponds to a path in a derivation, interleaved with logical \ninfer\u00adences. A program terminates just if no in.nite computation is pos\u00adsible, i.e. if all potentially \nin.nite computations can be dismissed as logically contradictory. An instance of this principle is the \nsize change principle for program termination of Lee, Jones and Ben-Amran, which states that a program \nis terminating if every in.\u00adnite computation induces an in.nite descending sequence of values from a \nwell-ordered set (Lee et al. 2001). Here, we employ the ex\u00adisting techniques of cyclic proof (Brotherston \n2007, 2006, 2005; Sprenger and Dam 2003; Sch\u00a8opp and Simpson 2002) to consider and then dismiss potentially \nin.nite computations in the manner described above. A cyclic pre-proof in our system is formed from partial \nderivation trees by identifying every bud (a node to which no proof rule has yet been applied) with a \nsyntactically identical interior node, so that pre-proofs can be immediately understood as cyclic graphs \nin which in.nite program computations correspond to in.nite proof paths. To ensure that all such computations \ncan be disregarded, we demand that, to count as a bona .de cyclic proof, a pre-proof must satisfy a global \ntrace condition which for\u00admalises the fact that some inductively de.ned predicate is unfolded in.nitely \noften along every in.nite path. This ensures the sound\u00adness of our cyclic proofs. Moreover, the use of \nthis condition means that we do not need to construct explicit ranking functions for ter\u00admination. Indeed, \nit appears possible to construct cyclic termina\u00adtion proofs for which it is dif.cult to construct or \ndeploy a ranking function. The remainder of this paper is structured as follows. In Section 2 we give \nthe syntax and small-step semantics of a simple imperative programming language, TOY-C, which is nonetheless \nsuf.cient to express many pointer-based algorithms. In Section 3, we give the syntax and semantics of \nour language for program preconditions, which is an extension of separation logic with a schema for induc\u00adtive \nde.nitions as given in Brotherston (2007). In Section 4, we give the rules of our proof system for termination, \nand then proceed to formalise the notion of a cyclic proof in this system in Section 5, (along very similar \nlines to the notions of cyclic proof employed in Brotherston (2007, 2006)). We provide a soundness theorem \nand also a relative completeness theorem for our system, the latter result demonstrating that the question \nof provability of a valid judgement in our system can always be reduced to the question of provability \nof a valid logical judgement in pure separation logic. In Section 6, we give some sample termination \nproofs in our cyclic system, in\u00adcluding one example the in-place reversal of a cyclic linked list for \nwhich the termination measure is far from obvious. Finally, in Section 7 we summarise and identify the \ndirections for future work. 2. TOY-C: a small imperative programming language In this section we give \nthe syntax and semantics of a toy program\u00adming language, TOY-C, that nevertheless contains the following \nessential features: conditional branching; assignment; dereferenc\u00ading; and memory allocation/deallocation. \nThis allows us to deal with manipulation of the heap, the traditional domain of separa\u00adtion logic, but \nthe cyclic proof method applies to a far wider range of programs than we can consider here. We assume \nthe existence of a denumerably in.nite set Var of variables, and a .rst-order language Sexp, called the \nexpression language, satisfying Sexp . Var and containing a distinguished constant symbol nil. The expressions \nE of TOY-C are just the terms (de.ned as usual) of the expression language. The syntax of branching conditions \nCond and atomic commands C is then given by the following grammar: Cond ::= E = E | E= E C ::= x := E \n| x := [E] | [E] := E | x := new() | free(E) | if Cond goto j | stop where x := [E] and [E] := F access \nand assign to the heap cell with address E.A program in TOY-C is a .nite sequence of indexed commands \n1: C1; \u00b7\u00b7\u00b7 ; n : Cn (we say that Ci is the command at program point i). We write the command goto j as \nan abbreviation of if nil = nil goto j. We give the semantics of TOY-C programs using a basic RAM model. \nWe .x a set Val of (program) values and a set Loc . Val of (program) locations, and assume a distinguished \nnullary value nil . Val-Loc.A stack in this model is a function s : Var . Val, and a heap is a partial, \n.nitely-de.ned function h : Loc -.n Val; we write Stacks and Heaps for the set of all stacks and the \nset of all heaps respectively. We write s[x . v] for the stack de.ned exactly as s except that (s[x . \nv])(x)= v, and adopt a similar notation for heaps. We write . to denote composition of heaps: if h1 and \nh2 are heaps with dom(h1) n dom(h2)= \u00d8, then h1 . h2 is the composite heap de.ned by: . . h1(l) if l \n. dom(h1) (h1 . h2)(l)= h2(l) if l . dom(h2) . unde.ned otherwise The interpretation [ E] s of an expression \nE in a stack s is standard: [ nil] s = nil and [ x] s = s(x) for any x . Var (given some .xed interpretation \nfor any function symbols in the expression language, s can then be extended to all expressions in the \nusual way). Similarly, for branching conditions, we have s . [ E1 = E2] iff [ E1] s =[ E2] s, and s . \n[ E1 E2] iff = [ E1] s [ E2] s. = A (program) state in our model is a triple (i, s, h), where i . N \nrepresents the next line of the program to be executed (i.e. the value of the program counter), s is \na stack and h is a heap. The small-step semantics of programs, presented in Figure 1, is given by a binary \nrelation . on program states, with the intended meaning that ' (i, s, h) . (i',s,h') holds if the execution \nof the next command ' in the state (i, s, h) can result in the new program state (i',s,h'). We write \n(i, s, h). iff there is no in.nite .-sequence beginning with (i, s, h) . ..., i.e. iff the program terminates \nwhen started in the state (i, s, h). When a command tries to access unallocated memory, the execution \ncontinues from the special location fault, and loops. This effectively equates memory errors and divergence. \n(Note also that the absence of a case for stop in the small-step semantics implies that our programs \nterminate immediately on encountering a stop command.) Ci = x := ECi = x := [E][ E] s . dom(h) Ci = [E] \n:= E ' [ E] s . dom(h) (i, s, h)(i +1,s[x . [ E] s],h)(i, s, h)(i +1,s[x . h([[E] s)],h)(i, s, h)(i +1, \ns, h[[[E] s . [ E ' ] s]) Ci = x := new() e . Loc \\ dom(h) v . Val (i, s, h)(i +1,s[x . e],h[e . v]) \nCi = if Cond goto js . [ Cond] (i, s, h)(j, s, h) Ci = free(E)[ E] s . dom(h) (i, s, h)(i +1, s, (h I \n(dom(h) \\{[ E] s})) Ci = if Cond goto js . [ Cond] (i, s, h)(i +1, s, h) Ci = x := [E] | [E] := E ' \n| free(E)[ E] s/. dom(h) (fault, s, h)(fault, s, h)(i, s, h)(fault, s, h) Figure 1. Small-step operational \nsemantics of TOY-C, given by the binary relation over (N\u00d7 Stacks \u00d7 Heaps). Example 2.1 (List traversal \nprogram). Consider the C-like pro\u00adgram for traversing a linked list:1 while (x!=nil) x:=[x]; The equivalent \nTOY-C program can be written as follows: 1: if x = nil goto 4; 2: x := [x]; 3: goto 1; 4: stop Clearly \nthis program terminates if the heap consists of an acyclic, singly-linked list whose .rst node is pointed \nto by x and whose last node contains the null pointer nil (because the length of the list remaining to \nbe traversed clearly decreases at each iteration of the loop, and is initially .nite). We give a formal \nproof in Section 6. Our language has the termination monotonicity property (see Yang and O Hearn (2002)). \nIf a program terminates in a heap h then it terminates in every extension of h. Proposition 2.2 (Termination \nmonotonicity). If (i, s, h). and h.h ' is de.ned then (i, s, h . h ' ).. Proof. (Sketch) By contraposition, \nit suf.ces to show that the exis\u00adtence of an in.nite -sequence starting from (i, s, h . h ' ) implies \nthe existence of an in.nite -sequence starting from (i, s, h). This follows from the following claim: \nClaim. If (i1,s1,h1 . h ' )(i2,s2,h2) then either: = h '' . h ' 1. h2 for some h '', and (i1,s1,h1)(i2,s2,h \n'' ), or; 2. (i1,s1,h1)(fault, s1,h1).  Then given any in.nite -sequence starting from (i, s, h . h \n' ), it is either the case that no step in this sequence accesses h ', in which case every step in this \nsequence can be equally well carried out starting from (i, s, h), or that some step in the sequence accesses \nh ', in which case the equivalent step in the sequence starting from (i, s, h) causes a fault and thus \nthis sequence immediately diverges. In either case we have the required in.nite -sequence starting from \n(i, s, h). It only remains to substantiate the claim, which follows from a straightforward case analysis \non (i1,s1,h1 . h ' )(i2,s2,h2). 3. Separation logic with inductive de.nitions In this section, we reprise \nthe syntax and semantics of separation logic extended with a framework for inductive de.nitions, as given \nin Brotherston (2007). This gives us a framework for expressing heap properties using customised inductive \nde.nitions, which will 1 For simplicity, in this and other examples we treat lists which contain nothing \nbut pointers to successor cells. be useful later in the expression of suitable preconditions for our \nTOY-C programs. We assume a .xed .rst-order language S, and for reasons of expressivity stipulate that \nS . Sexp, i.e., that our logical lan\u00adguage extends the TOY-C expression language. The terms of S are \nde.ned as usual, with variables drawn from the set Var. We write t(x1,...,xk) for a term t all of whose \nvariables occur in {x1,...,xk}, and we often use vector notation to abbreviate se\u00adquences, e.g. x for \n(x1,...,xk). The interpretation [ t] s of a term t of S in a stack s is then de.ned as for expressions \n(provided we have given an interpretation for any constant or function symbol that is not in Sexp 2). \nIn contrast to the usual situation in .rst-order logic, but in keeping with prior developments for .rst-order \nlogic with induc\u00adtive de.nitions (Brotherston 2006), we designate .nitely many of the predicate symbols \nof S as special inductive symbols; a pred\u00adicate symbol that is not inductive is called ordinary. For \neach predicate symbol Q of arity k (say) we assign an intepretation [ Q] . Pow(Heaps \u00d7 Valk). We insist \nthat S contains a 0-ary ordi\u00adnary predicate symbol emp and a binary ordinary predicate symbol ., whose \ninterpretations are given respectively by: [ emp] = {h | dom(h)= \u00d8}[ .] = {(h, v1,v2) | dom(h)= {v1} \nand h(v1)= v2} Thus the predicate emp denotes the empty heap, while v1 . v2 denotes those heaps having \nexactly one location, which is denoted by v1 and whose contents are denoted by v2. Our formulas are the \nusual formulas of predicate BI3, given by the following grammar: F ::= T|.| emp | Q(t1,...,tk) (k = arity \nof Q) | t1 = t2 |F . F | F . F | F . F | F * F | F * F |.xF |.xF where t1,...,tk range over the terms \nof S and Q ranges over the predicate symbols (both ordinary and inductive) of S. We use the standard \nprecedences on the logical connectives, with * and * having the same logical precedence as . and . respectively, \nand use parentheses to disambiguate where necessary. Also, we write \u00acF as an abbreviation of the formula \nF ... We may then de.ne the standard satisfaction relation s, h |= F , by induction on the structure \nof F , as shown in Figure 2. However, we shall further insist that the interpretation [ P ] of each induc\u00adtive \npredicate symbol P coincides with the standard interpretation, which is .xed by a given inductive de.nition \nfor P . Our inductive de.nition schema (essentially the one formulated in Brotherston (2007)) is given \nby the following de.nition: 2 Our extension of stacks to arbitrary logical terms is a technical simpli.ca\u00adtion. \nWe could instead use stack-extending environments to interpret terms. 3 However, note that here we write \nemp for the multiplicative unit I of BI. s, h |= T. true s, h |= .. false s, h |= t1 = t2 . [ t1] s =[ \nt2] s s, h |= Q(t1,...,tk) . (h, [ t1] s, . . . , [ tk] s) . [ Q] (Q ordinary or inductive) s, h |= \nF1 . F2 . s, h |= F1 or s, h |= F2 s, h |= F1 . F2 . s, h |= F1 and s, h |= F2 s, h |= F1 . F2 . s, h \n|= F1 implies s, h |= F2 s, h |= F1 * F2 ..h1,h2.h = h1 . h2 and s, h1 |= F1 and s, h2 |= F2 s, h |= \nF1 * F2 ..h ' .h . h ' de.ned and s, h ' |= F1 implies s, h . h ' |= F2 s, h |= .xF ..v . Val.s[x . v],h \n|= F s, h |= .xF ..v . Val.s[x . v],h |= F Figure 2. The relation s, h |= F for satisfaction of a separation \nlogic formula F by a stack s and heap h. De.nition 3.1 (Inductive de.nition). An inductive de.nition \nof an inductive predicate symbol P is a set of statements: C1(x1) . P t1(x1),...,Ck(xk) . P tk(xk) where \nk . Nand C1(x1),...,Ck(xk) are inductive clauses, given by the following grammar: C(x) ::= P t(x) | F \n(x) | C(x) . C(x) | C(x) * C(x) | F (x) . C(x) | F (x) * C(x) |.xC(x) where P ranges over the inductive \npredicate symbols of S and F (x) ranges over all formulas in which no inductive predicates occur and \nwhose free variables are contained in {x}. Each statement Ci(xi) . Piti(xi) is read as a disjunctive \nclause of the de.nition of the inductive predicate symbol Pi. As in Brotherston (2007), our use of F \n(x) on the left of implications in inductive clauses is designed to ensure monotonicity of our in\u00adductive \nde.nitions. A more liberal de.nition scheme might allow inductively de.ned predicates to occur negatively \nsubject to an ap\u00adpropriate strati.cation of inductive de.nitions, as in iterated induc\u00adtive de.nitions \n(Martin-L\u00a8 of 1971). The standard interpretation of an inductive predicate symbol P is then, as usual, \nthe least pre.xed point of a monotone operator constructed from the inductive de.nitions. This operator, \nand the process for constructing its least .xed point, are essential to under\u00adstanding the soundness \nof our cyclic proof system, so we include the details here: De.nition 3.2 (De.nition set operator). Let \nthe inductive predicate symbols of S be P1,...,Pn with arities a1,...,an respectively, and suppose we \nhave a unique inductive de.nition for each predi\u00adcate symbol Pi. Then, for each i .{1,...,n}, from the \ninductive de.nition for Pi, say: C1(x1) . Pi t1(x1),...,Ck(xk) . Pi tk(xk) we obtain a corresponding \nn-ary function .i :(Pow(Heaps \u00d7 Vala1 ) \u00d7 ... \u00d7 Pow(Heaps \u00d7 Valan )) . Pow(Heaps \u00d7 Valai ) as follows: \n .i(X)= { (h, [ tj]](s[xj . d])) | 1=j=k s[xj . d],h |= [ P] .X Cj (xj)} where s is an arbitrary stack \nand |= [ P] .X is the satisfaction rela\u00adtion de.ned exactly as in Figure 2 except that [ Pi] = pin(X) \nfor each i .{1,...,n}. Note that any variables occurring in the right hand side but not the left hand \nside of the set comprehension in the de.nition of .i above are, implicitly, existentially quanti.ed over \nthe entire right hand side of the comprehension. Then the de.ni\u00adtion set operator for P1,...,Pn is the \noperator .P, with domain and codomain Pow(Heaps \u00d7 Vala1 ) \u00d7 ... \u00d7 Pow(Heaps \u00d7 Valan ), de.ned by: .P(X)=(.1(X),...,.n(X)) \n Example 3.3. Consider the following inductive de.nition for a binary inductive predicate symbol ls: \nemp . ls x x x . x ' * ls x ' y . ls x y Then [ ls] is the least pre.xed point of the following operator, \nwith domain and codomain Pow(Heaps \u00d7 Val2): .ls(X)= {(emp, (v, v)) | v . Val}.{(h1 . h2, (v, v ' )) |.w \n. Val. (h1, (v, w)) . [ .] and (h2, (w, v ' )) . X} The predicate ls denotes singly-linked list segments; \nls x y is true of those heaps that represent a (possibly cyclic) linked list segment whose .rst element \nis pointed to by x and whose last element contains the pointer y. (If the list is acyclic, then y is \na dangling pointer.) The operator generated from a set of inductive de.nitions by De.nition 3.2 is monotone \n(Brotherston 2007), and consequently has a least (pre).xed point, which gives the standard interpretation \nfor the inductively de.ned predicates of the language. As is well\u00adknown (see e.g. Aczel (1977)), this \nleast pre.xed point can be iteratively approached in ordinal-indexed stages or approximants: De.nition \n3.4 (Approximants). Let .P be the de.nition set op\u00aderator for the inductive predicates P1,...,Pn of S \nas in Def\u00adinition 3.2. De.ne a chain of ordinal-indexed sets (.a by P)a=0 trans.nite induction: .a = \n .P(.\u00df ) (note that this im- P \u00df<a P plies .0 =(\u00d8,..., \u00d8)). Then for each i .{1,...,n}, the set P P \na = pin(.a i P) is called the ath approximant of Pi. De.nition 3.5 (Standard interpretation). The function \n[ -] is said to respect the standard interpretation of the inductive predicates P a P1,...,Pn of S \nif for all i .{1,...,n}, we have [ Pi] = . ia From now on, we assume that [ -] always respects the standard \ninterpretation of any inductive predicate symbols. 4. Proof rules for termination judgements In this \nsection we give the rules of a Hoare-style proof system for proving termination of programs written in \nTOY-C. We write termination judgements of the form G hi., where i is a program point and G is a bunch \n(O Hearn and Pym 99): De.nition 4.1 (Bunch). A bunch is a tree whose leaves are formu\u00adlas (as de.ned \nin Section 3) and whose internal nodes are ; or , (which are logically equivalent to . and *, respectively). \nWe write G(.) to mean that G is a bunch of which . is a subtree (also called a sub-bunch ), and write \nG(. ' ) for the bunch obtained by replacing the considered instance of . by . ' in G(.). We observe that \na bunch G can be considered as a formula by replacing every occurrence of ; by . and every occurrence \nof , by * , and thus we extend our notion of satisfaction (cf. Fig. 2) to bunches in the obvious manner. \nDe.nition 4.2 (Coherent equivalence). Coherent equivalence, =, is the smallest binary relation on bunches \nsatisfying the following (commutative monoid) equations: G1; (G2;G3) = (G1;G2); G3 G1, (G2, G3) = (G1, \nG2), G3 G1;G2 = G2;G1 G1, G2 = G2, G1 T;G = G emp, G = G plus the rule of congruence: . = . ' implies \nG(.) = G(. ' ). De.nition 4.3 (Validity). A termination judgement G hi. is valid iff s, h |=G implies \n(i, s, h). for all s . Stacks and h . Heaps. Our Hoare logic rules for termination judgements are given \nin Figure 3. The symbolic execution rules for commands are adapta\u00adtions of standard rules for separation \nlogic (Berdine et al. 2005). The convention is that primed variables x ' and x '' in the precondi\u00adtions \nmust be chosen fresh. The general rules are similar to rules in a proof system for implication. In places, \nour rules differ slightly from the typical presentation of Hoare logic in order to simplify the tracking \nof occurrences of inductive predicates, which is cru\u00adcial for cyclic proofs. For example, our use of \nthe cut rule in place of the usual rule of consequence in Hoare logic allows us to track inductive predicates \nin the context part G of the rule. Our cut rule is presented with just one premise, with the usual second \npremise . h F instead given as a side condition (which can be seen as an analogue of the usual side condition, \ninvolving semantic entail\u00adment, on the rule of consequence). This separates our Hoare rea\u00adsoning, and \ntracking of inductive predicates, from an external rea\u00adsoning process for establishing pure implication \n(which itself might use cyclic proofs (Brotherston 2007) or an existing theorem prover such as Berdine \net al. (2006a)). The multiplicative weakening rule (WkM) does not normally hold in separation logic but \nis valid in the case of termination, justi.ed by Proposition 2.2. The case-split rule for inductive predicates \nwill play a core role in our cyclic proof system, and deserves more explanation. Suppose that the inductive \npredicate P has de.nition: C1(x1) . P t1(x1),...,Ck(xk) . P tk(xk) Our case-split rule: (G(t = tj(x); \nCj (x)) hi.)1=j=k (Case P ) .x .{x}.x . FV (G(P t)) G(P t) hi. unfolds an occurrence P t of an inductive \npredicate according to its de.nition (with the surrounding context G remaining unaffected). Example 4.4 \n(List segment). Let ls be the inductive predicate for (possibly cyclic) list segments de.ned in Example \n3.3. The inductive de.nition of ls determines the following case-split rule: G(t = u; emp) hi. G(t.x \n' * ls x ' u) hi. (Case ls) G(ls t u) hi. Proposition 4.5. The Hoare logic rules given in Figure 3 are \nlocally sound. That is to say, if all of the premises of any rule instance are valid, and any relevant \nside conditions are satis.ed, then the conclusion of the rule instance is also valid. 5. Cyclic proofs \nof program termination We now de.ne a notion of cyclic proof for our termination judge\u00adments. First we \nde.ne cyclic pre-proofs: .nite derivation trees to\u00adgether with a function that, for each leaf node in \nthe tree to which no proof rule has been applied (called a bud), assigns an interior node in the tree \nthat is labelled with an identical judgement; this in\u00adterior node is called the companion of the bud. \nA cyclic pre-proof can thus be seen as a representation of a regular, in.nite derivation tree by a cyclic \ngraph. De.nition 5.1 (Companion). Let B be a bud of a derivation tree D. An internal node C in D is said \nto be a companion for B if they have the same judgement labelling. By assigning a companion to each bud \nnode in a .nite derivation tree, one obtains a .nite representation of an associated (regular) in.nite \ntree: De.nition 5.2 (Cyclic pre-proof). A cyclic pre-proof of a termi\u00adnation judgement F hi. is a pair \nP =(D, R), where D is a derivation tree constructed according to the Hoare logic rules for termination \njudgements given in Section 4 and whose root is la\u00adbelled by F hi., and R is a function assigning a companion \nto every bud of D. If P =(D, R) is a cyclic pre-proof, we write GP for the graph obtained from D by identifying \neach bud node B in D with its companion R(B). De.nition 5.3 (Trace). Let P be a pre-proof and let (Gk \nhik .)k=0 be a path in GP .A trace following (Gk hik .)k=0 is a sequence (tk)k=0 such that, for all k, \ntk is the position of a leaf Ftk of Gk. Furthermore, for each k = 0, one of the following conditions \nmust hold: 1. (Ftk is in the active part of the rule) Gk hik . is the conclusion of one of the following \ninferences, tk is the position of the underlined formula in the conclusion and tk+1 is the position of \none of the underlined formulae in the premise: G(F1; F2) hi. G(F2) hi. (.) . h F1 ( *) G(F1 . F2) hi. \nG(.,F1 * F2) hi. G(F1,F2) hi. G(.; F2) hi. (*) . h F1 (.) G(F1 * F2) hi. G(.; F1 . F2) hi. G(F [t/x]) \nhi. G(t = tj(x); Cj (x)) hi. ... (.) (Case P ) G(.xF ) hi. G(P t) hi. (We remark that, due to the form \nof our inductive de.nitions (cf. Defn 3.1), we never need to trace formulas through the active part of \nthe rules (.) or (.).) In the case where (Case P ) is applied with conclusion Gk hik . as above, and \ntk and tk+1 are the positions of the leaves of Gk and Gk+1 indicated by the underlining, the trace is \nsaid to progress at k. An in.nitely progressing trace is a trace that progresses at in.nitely many points. \n 2. (Ftk is not in the active part of the rule) tk+1 is the position of the leaf in Gk+1 corresponding \nto tk in Gk, modulo any splitting of Gk performed by the rule. (Thus Ftk+1 = Ftk , modulo any substitution \nperformed by the rule.) E.g. if Gk hik . is the conclusion of the inference: G(F2) hi. . h F1 ( *) G(.,F1 \n* F2) hi. then tk+1 and tk are the same position in G(-) (and thus = ). Similarly, if Gk . is the conclusion \nof the Ftk+1 Ftk hik inference: x = t[x ' /x]; (E . t, G)[x ' /x] hi+1. Ci = x := [E] E . t, G hi. then \ntk+1 and tk are the same position in G(-) (and thus Ftk+1 = Ftk [x ' /x]). As a .nal example, if Gk hik \n. is the conclusion of the inference: G ' hi. G = G ' (Equiv) G hi. then the position of tk+1 in the \nrearranged bunch G ' must respect the original position of tk in G in the obvious way (and thus Ftk+1 \n= Ftk ). Symbolic execution rules: x = E[x ' /x]; G[x ' /x] hi+1. Cond;G hj .\u00acCond;G hi+1. Ci = x := \nECi = if Cond goto j G hi. G hi. ' ' ''' x = t[x /x]; (E . t, G)[x /x] hi+1. x . x, G[x /x] hi+1. Ci \n= x := [E] Ci = x := new() E . t, G hi. G hi. E0 . E1, G hi+1. G hi+1. Ci = stop Ci = [E0] := E1 Ci \n= free(E) G hi. E0 . t, G hi. E . t, G hi. General rules: G(.) hi. G(.) hi. G(.; .) hi. G ' hi. (WkA) \n(WkM) (Contr) G = G ' (Equiv) G(.; . ' ) hi. G(., . ' ) hi. G(.) hi. G hi. G(F ) hi. G hi. G(T)[t2/x, \nt1/y] hi. x not a program . h F (Cut) (Subst) (=) variable G(.) hi. G[t/x] hi. G(t1 = t2)[t1/x, t2/y] \nhi. G(F1) hi. G(F2) hi. G(F1; F2) hi. G(.; F2) hi. (.)(.)(.) . h F1 (.) .hi. G(F1 . F2) hi. G(F1 . F2) \nhi. G(.; F1 . F2) hi. G(F [t/x]) hi. G(F [z/x]) hi. G(F1,F2) hi. G(F2) hi. (.) z . FV (G(.xF )) (.)(*) \n. h F1 ( *) G(.xF ) hi. G(.xF ) hi. G(F1 * F2) hi. G(.,F1 * F2) hi. Case-split rule: (G(t = tj(x); Cj \n(x)) hi.)1=j=k C1(x1) . P t1(x1),...,Ck(xk) . P tk(xk) (Case P ) .x .{x}.x . FV (G(P t)) G(P t) hi. \n Figure 3. Hoare logic rules for termination judgements. De.nition 5.4 (Proof). A pre-proof P is a proof \nif for every in.nite path p in GP , there is an in.nitely progressing trace following some tail of p. \nTheorem 5.5. If there is a proof of G hi. then G hi. is valid. An outline proof of Theorem 5.5 is given \nin Appendix A. Proposition 5.6. It is decidable whether a pre-proof is a proof. As well as soundness, \nour proof system enjoys the following relative completeness property: Theorem 5.7 (Relative Completeness). \nUnder the assumption that the underlying proof system for ordinary implications G h F is complete, if \nG hi. is valid then there is a proof of G hi.. Intuitively, it is possible to de.ne inductive predicates \ntermi which ensure termination starting from program point i, and the proof that G ensures termination \nat i can be reduced to an ordinary implication G h termi x. This situation is analogous to what hap\u00adpens \nin ordinary Hoare logic, where relative completeness proofs are typically expressed with respect to an \noracle for implications. We give an outline proof of Theorem 5.7 in Appendix B. 6. Examples of cyclic \ntermination proofs We show three examples, two straightforward and one more intri\u00adcate. In each example \nwe have treated goto as if it had a single\u00adpremise rule of its own. Example 6.1 (Termination of list \ntraversal). Consider the program from Example 2.1 with precondition ls x nil, with predicate ls as in \nExample 3.3. Figure 4 gives a proof of termination. We show the association of a suitable companion to \nthe only bud in the pre-proof with a (red) arrow. Note that there is only one in.nite path in the pre-proof \n which goes around the cycle and the associated trace (underlined) makes progress in.nitely often at \nthe case-split rule. Therefore we have termination. This proof uses multiplicative weakening (WkM), throwing \naway part of the heap, in this case the cells of the list that have already been traversed. This is a \npeculiar thing to do in separation logic, in which proofs are careful to account for all resource. In \nthe next two examples we show that proofs which more carefully account are possible, but for termination \nof list traversal we really only need to know that the list yet to be traversed diminishes, and the proof \nof .gure 4 is all that is required. Example 6.2 (Termination of in-place list reversal). The classical \nin-place reverse algorithm reverses a list in place, using two vari\u00adables and no additional heap space: \ny := nil; while x = nil do z := x; x := [x]; [z] := y; y := z od  Figure 5. Termination of in-place \nlist reversal In TOY-C the program becomes 1: y := nil; 5: [z] := y; 2: if x = nil goto 8; 6: y := z; \n(1) 3: z := x; 7: goto 2; 4: x := [x]; 8: stop With precondition ls x nil the invariant of the loop (lines \n2 8) is ls y nil * ls x nil, and the interesting problem is then ls y nil * ls x nil .2.. The proof is \nshown in .gure 5. (This and the next example are not completely formal: we have used * and . ratherthan \n, and ; inbunches,andwehave omittedstepswhich reorganisebunches.) This proof could have used multiplicative \nweakening in place of cut,but we have chosen not to do so in order to emphasise the connection with the \nnext example. Example 6.3 (Termination of in-place frying-pan list reversal). The previous example algorithm \nwill reverse a cyclic list segment, but the loop measure, and hence the proof of termination, is tricky. \nAcyclic list segment ls x j in which the terminating pointer j points to a node already in the segment \ncan be seen as a separated three-part structure of two acyclic list segments and a join node , represented \nin separation logic as: .k.(ls x j * j .. k * lsk j) (2) Diagrammatically, such segments resemble a \nfrying pan in which ls x j is the handle and lsk j is the pan. The reversal algorithm goes down the handle, \nreversing it until it reaches the join node, which it redirects towards the reversed handle; then it \ngoes round the pan, reversing that; then it re-redirects the join node to the reversed pan; .nally it \ncomes back up the handle, re-reversing it. rev P0 rev P j j j  P rev H0 H1 rev H y rev H0 H1 yx  x \n xy  P1 (a) going in (b) going round (c) coming out Figure 6. Stages of reversing a frying-pan list \nwith handle H and pan P The precondition is (2) and the invariant is: (2006a)) might be expected to do. \nOne can imagine list processing .k1 , k2 , k3 \u00b7 .. (ls x j * ls y nil * j .. k1 * ls k1 j) . (ls k2 \nnil * j .. k2 * ls x j * lsy j) . (ls x nil * lsy j * j .. k3 * ls k3 j) .. in such a tool being so \nstylised that it couldrecognise the need to unroll the ls de.nitionstoformthecyclesB,DandEinthe proof, \n(3) andinventtheempty de.nitionstoformthepathsAandC. in which each of the disjuncts corresponds directly \nto one of the pictures in .gure 6. This invariant is suf.ciently obvious that it can be discovered automatically \n(Distefano et al. 2006), but it s hard to see what formula to use as a measure of the number of loop \nexecutions, because x plays different r oles at different stages of the proof. A termination proof might \nperhaps be hacked up using auxiliary variablesto recordthejoinpointandthestageoftheproof,butthe proof \nstages and the join point are proof artefacts, and it should be unnecessary to reveal them. Figure7showsa \npre-proofof the judgement I .2., where I is the invariant (3) (with existential quanti.cations omitted \nfor sim\u00adplicity, and double-line steps indicating implicit additive weaken\u00ading (WkA)). The cycles in \nthe proof are: B reversing the handle, progressingby unrolling ls x j; D reversing the pan, progressingby \nunrolling ls x j; E re-reversing the handle, progressing (like .gure 5) by unrolling ls x nil; A redirect \nthe join-node to the reversed handle, moveto reverse the pan; C redirect the join-node to the reversed \npan, move to re-reverse the handle. It looks more complicated than it is. The right-hand stack, which \ndeals with the third disjunct of the invariant and the re-reversal of the handle, is a straight-line \nlist reversal, extremely similar to .gure 5, with stop as the left antecedent of the if-goto step, and \ncontradiction (because x = nil.x . = nil)the left alternativeof the unrolling ofls x nil; The left-hand \nstack, which deals with the .rst disjunct of the invariant and the original reversal of the handle, is \nalso similar to .gure5,butin placeof stop it has contradiction(x can tbe nil whilst ls x j * j .. k1 \n)and in place of contradiction it has a sequence of executions (when you exhaust ls x j you reach the \njoin-node j .. k1 , and one execution of the loop deals with that);  The middle stack is very like the \nleft-hand one.  There are no in.nite paths in this proof which don t involve un\u00adrolling one or more \npredicates in.nitely often. Hence we have ter\u00admination. Incidentally we observe that the number of loop \nexecu\u00adtionsisnowclear:it sexactly twicethe lengthofthe handle(cycles BandE)plusthe lengthofthepan(cycleD)plus2(pathsAand \nC). Theproofislongand tedious,butitinvolvesno arithmeticandis verymuchthesortofthinganautomatictool(seee.g. \nBerdineetal. 7. Conclusions and future work In this paper we outline a novel approach to the problem \nof prov\u00ading terminationof imperative programs. Our approachbuilds upon previous theoreticalworkincyclic \nproof (Brotherston 2007; Broth\u00aderston and Simpson 2007; Brotherston 2005; Sprenger and Dam 2003) and \nrelies heavily upon separation logic techniques (Berdine et al. 2005; Reynolds 2002; Bornat et al. 2004). \nThe in.nite descent .avour of our cyclic proofs is highly reminiscent of Lee, Jones and Ben-Amran s size-change \ntermination principle (Lee et al. 2001). However, since the soundness requirement for our proofs is based \non unfolding an inductive de.nition in.nitely often along every in\u00ad.nite path, we avoid the need to explicitly \nconstruct and reason with ranking functions. In the case of our termination proof for reversal of a frying-pan \nlist (Example 6.3) we did not have to in\u00adtroduce auxiliary variables to represent phases of the algorithm \nor deal with the conditionalmeasure function which would be needed to exploit knowledge of the phase \n(and then the phase annotation would require a lexicographical ranking function). The Terminator and \nMutant automated termination proving tools rely on a theoretical result concerning well-founded rela\u00adtions \ndue to Podelski and Rybalchenko (Rybalchenko et al. 2006; Berdine et al. 2006b, 2007). The relationship \nbetween this princi\u00adple and our cyclic proof principle is not yet clear to us. Nor does there seem to \nbe a straightforward comparison with termination tools based on term rewriting (see e.g. Hofbauer and \nSerebrenik (2007)). However, we observe that our approach is amenable to in\u00adteractive as well as automatic \ntheorem proving, and we believe that the Smallfoot assertion checking tool for separation logic (Berdine \net al. 2006a) is a promising candidate platform for implementing our approach. In general, anyproof search \nmechanism for our for\u00admalism would need to extend the usual heuristics with the notion of searching for \ncycles (by identifying suitable companions else\u00adwhereintheproof treeforthe currentbuds, whichcorrespondto \nunproven subgoals). The recent work on shape analysis for separa\u00adtion logic (see e.g. Guo et al. (2007); \nLee et al. (2005); Distefano et al. (2006)) provides one obvious direction, based on abstract interpretation, \nwitha .nite domainbuilt fromseparation logic for\u00admulas. Proof search combined with abstraction immediately \ngives a .nite number of derivation trees, and cyclic pre-proofs for free. This immediately suggests an \nalgorithm, and we can already see that it applies to the in-place list reversal program. Thusfarwehavedealtonlywithsmalliterative \nalgorithms.We already understand how to deal with iterative problems that nor\u00admally require lexicographic \nmeasures; we have not yet considered more complex ranking functions (but we note that Lee and Ben-Amram \n(Ben-Amram and Lee 2007) have shown that measures Figure 7. Atermination proof of in-place reversal \nof a frying-pan list need not be arbitrarily complicated in size-change transition prob\u00adlems). We intend \nto consider more dif.cult problems such as tree algorithms: to deal with recursive algorithms we will \nneed at least to include postconditions in our termination judgements; to deal with iterative tree algorithms \nwe will have to consider subtle termi\u00adnation measures. We do not claim to have invented a panacea. We \nhave uncov\u00adered a novel approach to termination which gives natural-seeming proofs which has already, \nin the frying-pan list example, simpli\u00ad.ed a previously dif.cult problem and which appears to have the \npotential to be extended in other directions. References Peter Aczel. An introduction to inductive de.nitions. \nIn Jon Barwise, editor, Handbook of Mathematical Logic, pages 739 782. N-H, 1977. Amir M. Ben-Amram and \nChin Soon Lee. Ranking functions for size change termination II. Presented at (Hofbauer and Serebrenik \n2007), 2007. J. Berdine, C. Calcagno, and P.W. O Hearn. Symbolic execution with separation logic. In \nAPLAS 2005, volume 3780 of LNCS, pages 52 68, 2005. J. Berdine, C. Calcagno, and P.W. O Hearn. Smallfoot: \nAutomatic modular assertion checking with separation logic. In FMCO, volume 4111 of LNCS, pages 115 137, \n2006a. J. Berdine, B. Cook, D. Distefano, and P. O Hearn. Automatic termination proofs for programs with \nshape-shifting heaps. In CAV, volume 4144 of LNCS, pages 386 400, 2006b. J. Berdine, A. Chawdhary, B. \nCook, D. Distefano, and P. O Hearn. Variance analyses from invariance analyses. In 34th POPL, 2007. Richard \nBornat, Cristiano Calcagno, and Peter O Hearn. Local reasoning, separation and aliasing. In SPACE Workshop, \n2004. James Brotherston. Cyclic proofs for .rst-order logic with inductive de.\u00adnitions. In B. Beckert, \neditor, TABLEAUX 2005, volume 3702 of LNAI, pages 78 92. Springer-Verlag, 2005. James Brotherston. Formalised \ninductive reasoning in the logic of bunched implications. In SAS-14, volume 4634 of LNCS, pages 87 103. \nSpringer-Verlag, August 2007. James Brotherston. Sequent Calculus Proof Systems for Inductive De.ni\u00adtions. \nPhD thesis, University of Edinburgh, November 2006. James Brotherston and Alex Simpson. Complete sequent \ncalculi for in\u00adduction and in.nite descent. In LICS-22, pages 51 60. IEEE Computer Society, July 2007. \nC. Calcagno, D. Distefano, P.W. O Hearn, and H. Yang. Beyond reacha\u00adbility: Shape abstraction in the \npresence of pointer arithmetic. In SAS, volume 4134 of LNCS, pages 182 203, 2006. D. Distefano, P. O \nHearn, and H. Yang. A local shape analysis based on separation logic. In TACAS, volume 3920 of LNCS, \npages 287 302, 2006. Bolei Guo, Neil Vachharajani, and David I. August. Shape analysis with inductive \nrecursion synthesis. In PLDI, June 2007. Dieter Hofbauer and Alexander Serebrenik. The 9th international \nworkshop on termination. Paris, France, 2007. Chin Soon Lee, Neil D. Jones, and Amir M. Ben-Amram. The \nsize-change principle for program termination. In 28th POPL, 2001. O. Lee, H. Yang, and K. Yi. Automatic \nveri.cation of pointer programs using grammar-based shape analysis. In ESOP, volume 3444 of LNCS, pages \n124 140, 2005. Per Martin-L\u00a8 of. Haupstatz for the intuitionistic theory of iterated inductive de.nitions. \nIn J.E. Fenstad, editor, Proceedings of the Second Scandina\u00advian Logic Symposium, pages 179 216. N-H, \n1971. P. W. O Hearn and D. J. Pym. The logic of bunched implications. Bulletin of Symbolic Logic, 5(2):215 \n244, June 99. J. C. Reynolds. Separation logic: A logic for shared mutable data structures. In LICS, \npages 55 74, 2002. A. Rybalchenko, B. Cook, and A. Podelski. Termination proofs for systems code. In \nPLDI, pages 415 426, 2006. Ulrich Sch\u00a8opp and Alex Simpson. Verifying temporal properties using explicit \napproximants: Completeness for context-free processes. In FoSSaCS 2002, volume 2303 of LNCS, pages 372 \n386. Springer-Verlag, 2002. Christoph Sprenger and Mads Dam. On the structure of inductive reasoning: \ncircular and tree-shaped proofs in the \u00b5-calculus. In FOSSACS 2003, volume 2620 of LNCS, pages 425 440, \n2003. N. Torp-Smith, L. Birkedal, and J. Reynolds. Local reasoning about a copying garbage collector. \nIn POPL, pages 220 231, 2004. H. Yang and P. O Hearn. A semantic basis for local reasoning. In 5th FOSSACS, \nLNCS 2303, 2002. A. Outline proof of soundness (Theorem 5.5) In this section we give a sketch of the \nproof of our main soundness result: Theorem 5.5. The proof employs the following auxiliary de.nition: \nDe.nition A.1. Let G be a bunch and let t be the position of a distinguished leaf Ft of G. We observe \nthat G can be inductively de.ned (up to coherent equivalence =) by the following grammar, where F ranges \nover formulas: . ::= F (F = Ft ) | .; . | ., . G ::= Ft | G; . | G, . Now let s be a stack and h be \na heap. We de.ne the relation ~s by induction on the structure of G (as given above) as follows: s, h0 \n|= Ft .h, h0.~s .G,Ft . s, h |=. .h0,h0.~s .Ft ,Ft ..h, h0.~s .(G; .),Ft . .h1,h0.~s .G,Ft . s, h2 |=. \n.h1 . h2,h0.~s .(G, .),Ft . Intuitively, .h, h ' .~s .G,Ft . holds iff s and h satisfy G, Ft is a leaf \nof G and h ' is the sub-heap of h that satis.es Ft . In other words, the splitting of G into Ft and a \nsurrounding bunch context is mirrored by the splitting of h into h ' and a surrounding heap context . \nLemma A.2. Each of the Hoare logic rules for termination judge\u00adments given in Figure 3 enjoy the following \ntwo properties: 1. if the conclusion of the rule, say G hi., is invalid, i.e. there is some stack s and \nheap h such that s, h |=G but (i, s, h). does not hold, then there is some premise G ' hi. . of the rule, \na stack s ' and a heap h ' such that s ' ,h ' |= F ' but (i ' ,s ' ,h ' ). does not hold;  2. if there \nis a trace (t,t ' ) following the edge (G hi., G ' hi. .) then, given a heap h0 satisfying .h, h0.~s \n.G,Ft ., there exists a heap h ' 0 such that .h ' ,h ' 0.~s. .G ' ,Ft. .. Furthermore, the following \nrelation holds (and is well-de.ned):  least a s.t. s, h0 |= [P.Pa] Ft = least a s.t. s ' ,h0 ' |= [P.Pa] \nFt. where |= [P.Pa] is the satisfaction relation de.ned as in Fig\u00adure 2, except that for all i .{1,...,n} \nwe have [ Pi] = , P a i.e. each inductive predicate is interpreted using its ath approx\u00adimant (cf. De.nition \n3.4). Furthermore, if (t, t ' ) is a progress\u00ading trace, then this relation holds with > in place of \n=. Proof. (Sketch) We just need to check that both properties of the lemma hold for each proof rule. \nThe .rst property is just one way of stating that the rules are locally sound, i.e. that falsi.ability \nof the conclusion of a rule implies the falsi.ability of one of its premises (cf. Proposition 4.5). For \nthe second property, we need to show that if there is a trace following the edge from the conclusion \nto this falsi.able premise and .h, h0.~s .G,Ft . holds, i.e. h0 is the sub-heap of h used to satisfy \nFt in the falsifying interpretation of G hi., then we can construct a suitable substate h ' 0 of h ' \nthat can be used to satisfy Ft. in the constructed falsifying interpretation of G ' hi. .. The main interesting \ncase is when the rule applied is a case-split rule (Case P ) and (t,t ' ) is a progressing trace, with \nthe strict inequality relying on the fact that if the formula P t unfolded by the rule is satis.ed by \ns and h0, i.e. (h0, [ t] s) is in some approximant P a of P , then for every case-descendant Qu of P \nt we must have (h ' 0, [ u] s) in some strictly smaller approximant Q\u00df<a of Q. Having proved the above \nlemma, concerning edges in a proof tree, we can straightforwardly extend the two properties of the lemma \nto cover paths in a pre-proof graph: Lemma A.3. Let P be a pre-proof of G0 hi0 . and suppose that G0 \nhi0 . is invalid. Then there exists an in.nite path (Gj hij .)j=0 in GP , a sequence (sj )j=0 of stacks \nand a sequence (hj )j=0 of heaps such that the following two properties hold: 1. for all j = 0, the judgement \nGj hij . is false with respect to the stack sj and heap hj ; 2. if there is a trace (tj )j=m following \na tail (Gj hij .)j=m of the path (Gj hij .)j=0, then there exists a second sequence of heaps (h ' j )j=m \nsuch that, for all j = m:  least a s.t. sj ,h ' j |=a Ft = least a s.t. sj+1,h ' j+1 |=a Ft. Furthermore, \nif j is a progress point of the trace, then this relation holds with > in place of =. Proof. G0 hi0 ., \ns0 and h0 are given by assumption. If we in\u00adductively assume that we have constructed Gk hik ., sk and \nhk, then property 1 of Lemma A.2 tells us that we can construct Gk+1 hik+1 ., sk+1 and hk+1. Now if we \nsuppose that there is a trace following some tail (Gj hij .)m=j=k+1 of the path constructed so far, property \n2 of Lemma A.2 tells us that we can construct the required sequence (h ' j )m=j=k+1. (It is easy to see \nhow to construct the .rst element h ' m of this sequence because we have .hm,hm' .~sm .Gm,Ftm ..) Proof \nof Theorem 5.5. If we suppose that G hi. has a proof P but is invalid, i.e. false in some stack s and \nheap h, then we can use property 1 of Lemma A.3 to construct an in.nite path p in GP to\u00adgether with a \nsequence of stacks and heaps that falsify each sequent along the path. Since P is a proof, there is an \nin.nitely progressing trace following some tail of p. Thus we can invoke property 2 of Lemma A.3 to create \na monotonically decreasing chain of ordinals which, since the trace progresses in.nitely often, must \ndecrease in.nitely often. This contradicts the well-foundedness of the ordi\u00adnals, so G hi. must indeed \nbe valid. B. Outline proof of Theorem 5.7 (via termination weakest preconditions) In this section we \ngive a sketch of the proof of Theorem 5.7. First, we present a construction that transforms a program \ninto a family of mutually de.ned inductive predicates, which capture the weakest precondition for termination \nof the program. Then, we show that every valid termination judgement has a cyclic proof in our system, \nwhich uses the inductive predicates obtained from the program. Consider a program 1: C1; \u00b7\u00b7\u00b7 ; n : Cn, \nand let x be the variables occurring in the program. For each program point i, we de.ne a corresponding \ninductive predicate termi x in Figure 8 (we use the notation E.- as an abbreviation for .x. E.x). The \nresult is a collection of predicates such that cycles in the de.nitions correspond directly to cycles \nin the control .ow of the program. Example B.1 (List deletion program). The list deletion program: 1: \nif x = nil goto 6; 4: free(t); 2: t := x; 5: goto 1; 3: x := [x]; 6: stop; gives the following de.nitions. \n(x = nil . term6 xt) . term1 xt (x = nil . term2 xt) . term1 xt term3 xx . term2 xt x . x ' * ((x.x ' \n) * term4 x ' t) . term3 xt (t .-) * term5 xt . term4 xt term1 xt . term5 xt T. term6 xt By applying \nsimpli.cations we obtain a single inductive predicate for location 1. First notice that inlining term3, \nterm4, term5 we obtain '' ' x . x * (x . x * (x .-* term1 xx)) . term2 xt and the left-hand side can \nbe simpli.ed to x . x ' * term1 x ' x. By further inlining and simpli.cation, and noticing that the parameter \nt is not used actively in the de.nition, we obtain x = nil . term1 x x . x ' * term1 x ' . term1 x which \nis exactly analogous to ls x nil except that we can have garbage in the heap, since garbage does not \naffect termination. Example B.2 (In-place list reversal program). Consider the in\u00adplace list reversal \nprogram presented in Example 6.2. The corre\u00adsponding inductive de.nition for termination at program point \n2 can be simpli.ed to obtain the following de.nition: x = nil . term2 xy x . x ' * (x . y * term2 x ' \nx) . term2 xy In this inductive de.nition we cannot eliminate *. Instead we can give a characterisation \nusing another predicate. If we de.ne the cyclic list predicate cl by: ls x x ' * x ' . x '' * ls x '' \nx ' . cl x then the following equivalence holds: term2 xy . (ls x nil) . (cl x * ls y nil) which means \nthat the program terminates either by traversing the acyclic list starting from x and ending in nil, \nor by traversing the cyclic list and then the acyclic list starting from y and ending in nil. The following \nlemma shows that the termi predicate indeed guarantees termination from program point i. Lemma B.3. (i, \ns, h). implies s, h |= termi x. Proof. The proof is by induction on the length n of the longest computation \n(i.e., -sequence) starting at (i, s, h). We show some cases; other cases are analogous. Case Ci = x := \nE. The computation proceeds with (i +1,s ' ,h), where s ' = s[x . [ E] s]. Then we have (i +1,s ' ,h). \nwith longest computation of length n - 1. By induction hypothesis we have s ' ,h |= termi+1 x, therefore \ns, h |= termi+1 (x[E/x]). By de.nition of termi we have s, h |= termi x, which concludes the case. Command \nInductive de.nition Ci = x := E termi+1(x[E/x]) . termi x Ci = x := [E] E . x ' * (E . x ' * termi+1(x[x \n' /x])) . termi x Ci = [E] := F (E .-) * ((E . F ) * termi+1 x) . termi x ''' ' Ci = x := new() .x,y \n. (x ' .y ) * termi+1(x[x /x]) . termi x Ci = free(E)(E.-) * termi+1 x . termi x Cond . termj x . termi \nx Ci = if Cond goto j \u00acCond . termi+1 x . termi x Ci = stop T. termi x Figure 8. Transformation of commands \nto inductive predicates Case Ci = x := new(). We need to show s, h |= termi x, that ''' ''' is s, h |= \n.x,y . (x ' .y ) * termi+1(x[x /x]) for x ,y fresh in x. Take v1,v2 . Val, and h ' such that s[x ' . \nv1,y ' . v2],h ' |=(x ' .y ' ) and h . h ' is de.ned. Then h ' =[v1 . v2] and v1 . Loc \\ dom(h). It remains \nto show s[x ' . v1,y ' . v2],h[v1 . v2] |= termi+1 (x[x ' /x]). Now the computation can proceed with \n(i +1,s[x . e],h[e . v]) for any e . Loc\\dom(h) and v . Val, in particular with e = v1 and v = v2. Then \nwe have (i +1,s[x . v1],h[v1 . v2]). with longest computation of length n - 1. By induction hypothesis \nwe have s[x . v1],h[v1 . v2] |= termi+1 x, therefore s[x ' . v1,y ' . v2],h[v1 . v2] |= termi+1 (x[x \n' /x]), as required. The following lemma shows how to construct a cyclic proof that termi is a termination \nprecondition for program point i, which will be fundamental for the completeness result. Together with \nsoundness of cyclic proofs (Theorem 5.5) and Lemma B.3, this implies that termi denotes the weakest precondition \nfor termination at program point i. Lemma B.4. There is a cyclic proof of termi x hi.. Proof. We give \na direct construction of the proof. For each com\u00admand Ci in the program, let j1 \u00b7\u00b7\u00b7 jk be the possible \nprogram points where the execution might continue after executing Ci. We show how to construct a derivation \ntree of the form (termm x hm.)m=j1\u00b7\u00b7\u00b7jk \u00b7\u00b7\u00b7 termi x hi. which admits a progressing trace from termi x \nhi. to each termm x hm.. The whole proof is then obtained by stacking the appropriate derivation tree \non top of each bud, unless that bud is already matched with a companion in the derivation tree already \nconstructed. At the end of the process, each bud will be assigned a companion, and every in.nite path \nin the resulting graph will obvi\u00adously progress in.nitely often. We show in detail the construction for \nsome interesting cases: Case Ci = x := E. We have the following inductive de.nition for termi x: termi+1(x[E/x]) \n. termi x We can derive: termi+1 x hi+1. (WkA) x = E[x ' /x]; termi+1(x) hi+1. (=) x = E[x ' /x]; termi+1(x[E[x \n' /x]/x]) hi+1. x := E termi+1(x[E/x]) hi. (Case termi) termi x hi. Case Ci = x := new(). We have the \nfollowing inductive de.nition for termi x: '''' ' .x,y . (x . y ) * termi+1 (x[x /x]) . termi x We can \nderive: termi+1 x hi+1. ( *) '''' '' x . x, ((x.x ) * termi+1 (x[x /x][x/x ])) hi. (.) '' '''' ' x . \nx, (.x,y . (x .y ) * termi+1 (x[x /x])) hi+1. x := new() '''' ' .x,y . (x .y ) * termi+1 (x[x /x]) hi. \n(Case termi) termi x hi. Case Ci = if Cond goto j. We have the following inductive de.nition for termi \nx: Cond . termj x . termi x \u00acCond . termi+1 x . termi x We can derive: termj x hj . termi+1 x hi+1. \n(WkA) (WkA) Cond; termj x hj .\u00acCond; termi+1 x hi+1. if if Cond; termj x hi.\u00acCond; termi+1 x hi. (.)(.) \n Cond . termj x hi.\u00acCond . termi+1 x hi. (Case termi) termi x hi. Proof of Theorem 5.7. Assume that G \nhi. is valid. For any s . Stacks and h . Heaps such that s, h |=G, we have (i, s, h)., hence s, h |= \ntermi x by Lemma B.3. Therefore G h termi x is valid, and it is derivable by the completeness assumption \nof the underlying proof system. The required proof of G hi. can be constructed using an in\u00adstance of \nCut as follows \u00b7\u00b7\u00b7 \u00b7 \u00b7 \u00b7 termi x hi. G h termi x (Cut) G hi. where the dots are a placeholder for the \nproof of termi x hi. from Lemma B.4. \n\t\t\t", "proc_id": "1328438", "abstract": "<p>We propose a novel approach to proving the termination of heap-manipulating programs, which combines separation logic with <i>cyclic proof</i> within a Hoare-style proof system.Judgements in this system express (guaranteed) termination of the program when started from a given line in the program and in a state satisfying a given precondition, which is expressed as a formula of separation logic. The proof rules of our system are of two types: logical rules that operate on preconditions; and symbolic execution rules that capture the effect of executing program commands.</p> <p>Our logical preconditions employ inductively defined predicates to describe heap properties, and proofs in our system are cyclic proofs: cyclic derivations in which some inductive predicate is unfolded infinitely often along every infinite path, thus allowing us to discard all infinite paths in the proof by an infinite descent argument. Moreover, the use of this soundness condition enables us to avoid the explicit construction and use of ranking functions for termination. We also give a completeness result for our system, which is relative in that it relies upon completeness of a proof system for logical implications in separation logic. We give examples illustrating our approach, including one example for which thecorresponding ranking function is non-obvious: termination of the classical algorithm for in-place reversal of a (possibly cyclic) linked list.</p>", "authors": [{"name": "James Brotherston", "author_profile_id": "81100189429", "affiliation": "Imperial College London, London, United Kingdom", "person_id": "PP43124336", "email_address": "", "orcid_id": ""}, {"name": "Richard Bornat", "author_profile_id": "81100414897", "affiliation": "Middlesex University, London, United Kingdom", "person_id": "PP14146914", "email_address": "", "orcid_id": ""}, {"name": "Cristiano Calcagno", "author_profile_id": "81100047402", "affiliation": "Imperial College London, London, United Kingdom", "person_id": "PP43115552", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1328438.1328453", "year": "2008", "article_id": "1328453", "conference": "POPL", "title": "Cyclic proofs of program termination in separation logic", "url": "http://dl.acm.org/citation.cfm?id=1328453"}