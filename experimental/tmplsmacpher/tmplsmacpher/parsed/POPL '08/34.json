{"article_publication_date": "01-07-2008", "fulltext": "\n From Dirt to Shovels Fully Automatic Tool Generation from Ad Hoc Data Kathleen Fisher David Walker Kenny \nQ. Zhu Peter White AT&#38;T Labs Research Princeton University Galois, Inc. kfisher@research.att.com \ndpw,kzhu@CS.Princeton.EDU peter@galois.com Abstract An ad hoc data source is any semistructured data \nsource for which useful data analysis and transformation tools are not readily avail\u00adable. Such data \nmust be queried, transformed and displayed by systems administrators, computational biologists, .nancial \nanalysts and hosts of others on a regular basis. In this paper, we demon\u00adstrate that it is possible to \ngenerate a suite of useful data process\u00ading tools, including a semi-structured query engine, several \nfor\u00admat converters, a statistical analyzer and data visualization rou\u00adtines directly from the ad hoc \ndata itself, without any human in\u00adtervention. The key technical contribution of the work is a multi\u00adphase \nalgorithm that automatically infers the structure of an ad hoc data source and produces a format speci.cation \nin the PADS data description language. Programmers wishing to implement custom data analysis tools can \nuse such descriptions to generate printing and parsing libraries for the data. Alternatively, our software \ninfras\u00adtructure will push these descriptions through the PADS compiler, creating format-dependent modules \nthat, when linked with format\u00adindependent algorithms for analysis and transformation, result in fully \nfunctional tools. We evaluate the performance of our inference algorithm, showing it scales linearly \nin the size of the training data completing in seconds, as opposed to the hours or days it takes to write \na description by hand. We also evaluate the correctness of the algorithm, demonstrating that generating \naccurate descriptions often requires less than 5% of the available data. Categories and Subject Descriptors \nD.3.m [Programming lan\u00adguages]: Miscellaneous General Terms Languages, Algorithms Keywords Data description \nlanguages, grammar induction, tool generation, ad hoc data 1. Introduction An ad hoc data source is any \nsemistructured data source for which useful data analysis and transformation tools are not readily avail-able.XML,HTML \nand CSV are not ad hoc data sources as there are numerous programming libraries, query languages, manuals \nand other resources dedicated to helping analysts manipulate data in these formats. However, despite \nthe prevalence of standard for- Permission to make digital or hard copies of all or part of this work \nfor personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. POPL 08, January 7-12,2008, San Francisco, California, USA. Copyright \nc . 2008 ACM 978-1-59593-689-9/08/0001. . . $5.00 mats, massive quantities of legacy ad hoc data persist \nin .elds rang\u00ading from computational biology to .nance to physics to networking to health care and systems \nadministration. Moreover, engineers and scientists are continuously producing new ad hoc formats despite \nthe presence of existing standards because it is often expedient to do so. Over time, these expedient \nformats become dif.cult to work with because of missing documentation, a lack of tools, and corruption \ncaused by repeated redesign, reuse and extension. The goal of the PADS project (Fisher and Gruber 2005; \nFisher et al. 2006; Mandelbaum et al. 2007; PADS Project) is to improve the productivity of data analysts \nwho need to cope with new and evolving ad hoc data sources on a daily basis. Our central technol\u00adogy \nis a domain-speci.c language in which programmers can spec\u00adify the structure and expected properties \nof ad hoc data sources, whether they be ASCII, binary, Cobol or a mixture of formats. These speci.cations, \nwhich resemble extended type declarations from conventional programming languages, are compiled into \na suite of programming libraries, such as parsers and printers, which are then linked to generic data \nprocessing tools including an XML\u00adtranslator, a query engine (Fern\u00b4andez et al. 2006), a simple statis\u00adtical \nanalysis tool, and others. Hence, an important bene.t of us\u00ading PADS is that a single declarative description \nmay be used to generate many useful end-to-end data processing tools completely automatically. On the \nother hand, a signi.cant impediment to using PADS is the time and expertise needed to write a PADS description \nfor a new ad hoc data source. For data experts possessing clear, unambiguous documentation about a simple \ndata source, writing a PADS descrip\u00adtion may take anywhere from a few minutes to a few hours. How\u00adever, \nit is relatively common to encounter ad hoc data sources that contain valuable information, yet have \nlittle or no documentation. Understanding the structure of the data and creating descriptions for such \nsources can take days or weeks depending upon the com\u00adplexity and volume of the data in question. In \none speci.c example, Fisher spent approximately three weeks (off and on) attempting to understand and \ndescribe an important data source used at AT&#38;T. One of the dif.culties was that the data source suddenly \nswitched formats after approximately 1.5 million entries. Of course, if deal\u00ading with the vagaries of \nad hoc data is time-consuming and error\u00adprone for experts, it is even worse for novice users. To improve \nthe productivity of experts and to make the PADS toolkit accessible to new users with little time to \nlearn the spec\u00adi.cation language, we have developed an automatic format infer\u00adence engine. This format \ninference engine reads arbitrary ASCII data sources and produces an accurate, human-readable description \nof the source. These machine-produced descriptions give experts a running start in any data analysis \ntask as the libraries generated from these descriptions may be incorporated directly into an ordi\u00adnary \nC program. The inference engine is also directly connected to the rest of the PADS infrastructure, making \nit possible for .rst-time users, with no knowledge of the PADS domain-speci.c language, to translate \ndata into a form suitable for loading into a relational database, to load it into an Excel spreadsheet, \nto convert the data into XML, to query it in XQuery, to detect errors in additional data from the same \nsource, and to draw graphs of various data compo\u00adnents, all with just a push of a button. In designing \na format inference engine for PADS, we are in ter\u00adritory explored in the past by the machine learning \ncommunity. For example, there have been many attempts to devise algorithms that learn regular expressions, \ncontext free grammars and more exotic language classes. These algorithms have been used to perform tasks \nranging from natural language understanding to type inference for XML documents to information extraction \nfrom web pages. One key difference in our work is that we target an understudied domain (ad hoc systems \ndata) that allows new techniques for effective in\u00adference. A second key difference is that we solve a \nnew problem by showing how to generate an entire suite of end-to-end data process\u00ading tools with no human \nintervention. Section 6 contains a more in-depth analysis of related work. To summarize, this paper makes \nthree main contributions: We have developed a multi-phase algorithm that infers the for\u00admat of complex, \nad hoc data sources, producing compact and accurate PADS descriptions.  We have incorporated the inference \nalgorithm into a modular software system that uses sample data to generate a toolkit of useful data processing \ntools, without requiring any human intervention.  We have evaluated the correctness and performance \nof our sys\u00adtem on a range of ASCII data sources. For many data sources, training on 5% or less of the \ndata results in accuracy rates greater than 95% (often perfect). In all our benchmarks, the in\u00adference \nalgorithm scales linearly with the quantity of data.  For readers interested in seeing our system operate \nlive, there is an online demo illustrating its many features (http://www. padsproj.org). The remainder \nof this paper describes the sub\u00adset of the PADS description language we attempt to infer (Sec\u00adtion 2), \nthe inference algorithm itself and generated tools (Sec\u00adtion 3), the performance (Section 4), strengths \nand weaknesses of our approach (Section 5), related work (Section 6) and conclusions (Section 7). This \npaper is an extended version of a 2-page sum\u00admary presented at the CAGI 2007 workshop on grammar induc\u00adtion \n(Burke et al. 2007).  2. The Internal Format Description Language Our format inference algorithm comprises \na series of phases that generate and transform an internal format description language we refer to simply \nas the IR. The IR is very similar to the IPADS language we developed and formalized in previous work \n(Fisher et al. 2006). Apart from syntax, the main differences are that the IR omits recursion and function \ndeclarations; the former being beyond the scope of our current inference techniques and the latter being \nunnecessary during the course of the inference algorithm. 2.1 The Language Like all languages in the \nPADS family, the IR is a collection of type de.nitions. These types de.ne both the external syntax of \ndata formatted on disk and the shape of the internal representations that result from parsing. We rely \nupon both of these aspects of type de.nitions to generate stand-alone tools automatically. Figure 1 summarizes \nthe syntax of the IR and of the generated internal representations. The building blocks of any IR data \ndescription are the base types b, which may be parameterized by some number of argu\u00ad c ::= a | i | s \n(constants) x (variables) p :: = c | x (parameters) Base types b::= Pint (generic, unre.ned integer) \n| PintRanged (integer with min/max values) | Pint32 (32-bit integer) | Pint64 (64-bit integer) | PintConst \n(constant integer) | Pfloat (.oating point number) | Palpha (alpha-numeric string) | Pstring (string; \nterminating character) | PstringFW (string; .xed width) | PstringConst (constant string) | Pother (punctuation \ncharacter) | ComplexB (complex base type de.ned by regexp; e.g. date, time, etc.) | Pvoid (parses no \ncharacters; fails immediately) | Pempty (parses no characters; succeeds immediately) Types T ::= b(p1,...,pk) \n(parameterized base type) | x:b(p1,...,pk) (parameterized base type; underlying value named x) | struct \n{T1; ... Tk;} (.xed sequence of items) | array {T;} (array with unbounded repetitions) | arrayFW {T;}[p] \n(array; .xed length) | arrayST {T;}[sep,term] (array; separator and terminator) | union {T1; ... Tk;} \n(alternatives) | enum {c1; ... ck;} (enumeration of constants) | x:enum {c1; ... ck;} (enumeration of \nconstants; underlying value named x) | option {T;} (type T or nothing) | switch xof {c1 => T1; ...ck \n=> Tk;} (dependent choice) Representations of parsed data d::= c (constant) | ini(d) (injection into \nthe ith alternative of a union) | (d1,...,dk) (sequence of data items) Figure 1. Selected elements of \nthe IR. ments p. Arguments may either be constants c, which include char\u00adacters a,integers i and strings \ns, or variables x bound earlier in the description. These base types include a wide range of different \nsorts of integers and strings. In its initial phases, the inference al\u00adgorithm uses general integer Pint, \nalphanumeric string Palpha and punctuation character Pother(a) types. In later phases, these coarse-grained \nbase types are analyzed, merged and re.ned, producing integers with ranges PintRanged(min,max),in\u00adtegers \nwith known size Pint32 or Pint64, constant integers (PintConst(i) for some integer i), or .oating-point \nnumbers Pfloat. Likewise, later stages of our algorithm transform al\u00adphanumeric strings into arbitrary \nstrings with terminating charac\u00adters (Pstring(a) where a terminates the string), .xed width strings (PstringFW(i) \nwhere i is the length of the string) or string constants PstringConst(s). For brevity in our descrip\u00adtions, \nwe normally just write the constant string s inline in a de\u00adscription instead of PstringConst(s). In \naddition to these simple base types, the IR includes a col\u00adlection of higher-level base types commonly \nfound in ad hoc data, speci.ed generally in Figure 1 as ComplexB. For example, we have implemented base \ntypes for IP addresses, email addresses, URLs, XML tags, dates, times and a variety of others. Finally, \nthe types Pvoid and Pempty are two special base types that are in\u00adtroduced at various points in the inference \nprocess. The .rst fails immediately; the second succeeds immediately. Neither consumes any characters \nwhile parsing. Crashreporter.log: Sat Jun 24 06:38:46 2006 crashdump[2164]: Started writing crash report \nto: /Logs/Crash/Exit/ pro.crash.log -crashreporterd[120]: mach_msg() reply failed: (ipc/send) invalid \ndestination port Sirius AT&#38;T Phone Provisioning Data: 8152272|8152272|1|6505551212|6505551212|0|0||no_ii152272|EKRS_6|0|FRED1|DUO|10|1000295291 \n8152261|8152261|1|0|0|0|0||no_ii752261|EKRS_1|0|kfeosf2|DUO|EKRS_6|1001390400|EKRS_OS_10|1001476801 \nFigure 2. Example ad hoc data sources. Complex descriptions are built from simpler ones using a va\u00adriety \nof type constructors. Type constructors include basic struct types struct{T1; ... Tk;}, which indicate \na data source should contain a sequence of items matching T1, ..., Tk, basic array types array T, which \nindicate a data source should contain a sequence of items of arbitrary length, each matching T, and union \ntypes union {T1; ... Tk;}, which indicate a data source should match one of T1, ..., Tk. Initial phases \nof the inference algorithm re\u00adstrict themselves to one of these three sorts of type constructors. Later \nphases of the algorithm re.ne, merge and process these sim\u00adple types in a variety of ways. For example, \nunions may be trans\u00adformed into enumerations of constants enum {c1; ... ck;} or options option {T;}. \nIn addition, later phases bind variables to the re\u00adsults of parsing base types and enums. For example, \nx:b(p1,...,pk) expresses the fact that variable x is bound to the value parsed by base type b(p1,...,pk). \nThese variables express dependencies be\u00adtween different parts of a description.1 For example, the length \nof a string PstringFW(p) or an array ParrayFW(p) may depend upon either a constant or a variable and \nlikewise for any other pa\u00adrameterized base type. In addition, unions may be re.ned into de\u00adpendent switch \nstatements switch x of {c1 => T1; ... ck => Tk;}, where the data is described by T1, ..., or Tk depending \non the value associated with x,be it c1, ..., or ck. In addition to describing a parser, each PADS types \nmay be interpreted as a data structure. We let metavariable d range over such data structures. For the \npurposes of this paper, d may bea constant c, an injection into the ith variant of a union ini(d), or \na sequence of data items (d1,...,dk). The injections are used as the representations of any sort of union \ntype, be it a union, an enumeration, an option or a switch. The sequences are used as the representations \nof any sort of sequence type, whether it be a struct or one of the array variants. Our earlier work (Fisher \net al. 2006) contains a precise treatment of this secondary semantics.  2.2 Running Examples Figure \n2 presents tiny fragments of two different ad hoc data .les on which we have trained our inference algorithm. \nThe .rst, Crashreporter.log, is a Mac system .le that records information concerning process crashes.2 \nThe second, which we call Sirius, is an internal AT&#38;T format used to record phone call provision\u00ading \ninformation. We use the Crashreporter.log data source as our main example throughout the paper; periodically \nwe refer to the Sirius data source to illustrate particular aspects of the inference algorithm. Figure \n3 presents a hand-written description of the Crashre\u00adporter.log .le in the IR syntax. This description \nis most easily read from the bottom, starting with the de.nition of the source type. This de.nition speci.es \nthat the data source is an array of structs 1 We assume every bound variable is distinct from every other \nthat appears in a description. Roughly speaking, the scope of such variables extends as far as possible \nto the right through the description. 2 For expository purposes we have made a minor alteration to the \nCrashre\u00adporter.log format to allow us to explain more concepts with a single exam\u00adple. The evaluation \nsection reports results on both the completely unmodi\u00ad dumpReport = union { struct { \"Started writing \ncrash report to: \"; file:Ppath; }; ... }; reporterReport = struct { function: Ppath; \" reply failed: \n\"); failuremsg: Pstring_( \\n ); }; dateOption = union { \"\u00ad \"; struct { day: PDate; \" \"; time: PTime; \n\" \"; year: Pint32; \" \"; }; }; source = arrayST { struct { date: dateOption; kind: enum {\"crashdump\"; \n\"crashreporterd\";}; \"[\"; dumpid: Pint32; \"]: \"; report: switch kind of { \"crashdump\" => dumpReport \"crashreporterd\" \n=> reporterReport }; }[ \\n ,EOF]; Figure 3. Hand-written IR Crashreporter.log description. separated \nby newline characters and terminated by the end of .le marker. In other words, the data source is a sequence \nof lines, with the struct in question appearing on each line. The struct itself in\u00addicates each line \nis a sequence of dateoption, kind, dumpid and report .elds. The description also speci.es that the delimiter \n\"[\" appears between the kind and dumpid .elds, and the delim\u00aditer \"]: \" appears between the dumpid and \nreport .elds. Most of the variable names associated with .elds (e.g. date, dumpid, etc.) merely serve \nas documentation for the reader. How\u00adever, the kind .eld is different it is used later in the description \nand hence illustrates a dependency. To be speci.c, the form of the report .eld depends upon the contents \nof the kind .eld. If its value is \"crashdump\", then the report is a dumpReport type, while if the kind \n.eld is \"crashreporterd\",the report is a reporterReport type. Figure 3 contains three other de.nitions \naside from source. These de.nitions specify the structure of the dumpReport, .ed Crashreporter.log and \nthe modi.ed version. reporterReport and dateOption types. able to infer data descriptions. Our tool \ncurrently supports chunking on a line-by-line basis as well as on a .le-by-.le basis. We use a lexer \nto break each chunk into a series of simple tokens, which are intuitively atomic pieces of data such \nas numbers, dates, times, alpha-strings, or punctuation symbols. Every simple token has a corresponding \nbase type in the IR, though the converse is not true there are base types that are not used as tokens. \nNevertheless, since simple tokens have a very close correspondence with base types, we often use the \nword token interchangeably with base type. Parenthetical syntax, including quotation marks, curly braces, \nsquare brackets, parentheses and XML tags, often provides very important hints about the structure of \nan ad hoc data .le. There\u00adfore, whenever the lexer encounters such parentheses, it creates a meta-token, \nwhich is a compound token that represents the pair of parentheses and all the tokens within.3 For example, \nin Crashre\u00adporter.log, the syntax [2164] will yield the meta-token [*] in\u00adstead of the sequence of three \nsimple tokens [, Pint,and ].The structure-discovery algorithm eliminates all meta-tokens during its analysis; \nwhenever it encounters a context consisting of matching meta-tokens, it cracks open the meta-tokens so \nit can analyze the underlying structure. Our learning system has a default tokenization scheme skewed \ntoward systems data, but users may specify a different scheme for their own domain through a con.guration \n.le. For example, com\u00adputational biologists may want to add DNA strings CATTGTT... to the default tokenization \nscheme. The con.guration .le is essen\u00adtially a list of name, regular expressions pairs. The system uses \nthe con.guration .le to generate part of the system s lexer, a collec\u00adtion of new IR base types, and \na series of type de.nitions that are incorporated into the .nal PADS speci.cation.  3.2 Structure Discovery \nGiven a collection of tokenized chunks, the goal of the structure\u00addiscovery phase is to quickly .nd a \ncandidate description close to a good .nal solution. The rewriting phase then analyzes, re.nes and transforms \nthis candidate to produce the .nal description. The high-level form of our structure-discovery algorithm \nwas inspired by the work of Arasu and Garcia-Molina (2003) on information extraction from web pages; \nhowever, the context, goals and algo\u00adrithmic details of our work are quite different. Structure Discovery \nBasics. Our algorithm operates by analyz\u00ading the collection of tokenized chunks and guessing what the \ntop\u00adlevel type constructor should be. Based on this guess, it partitions the chunks and recursively analyzes \neach partition to determine the best description for that partition. Figure 5 outlines the overall pro\u00adcedure \nin Pseudo-ML. The oracle function, whose implementa\u00adtion we hide for now, does most of the hard work \nby conjuring one of four different sorts of prophecies. The BaseProphecy simply reports that the top-level \ntype constructor is a particular base type. The StructProphecy speci.es that the top-level description \nis a struct with k .elds. It also speci.es a list, call it css, with k elements. The ith element in css \nis the list of chunks correspond\u00ading to the ith .eld of the struct. The oracle derives these chunk lists \nfrom its original input. More speci.cally, if the oracle guesses there will be k .elds, then each original \nchunk is partitioned into k pieces. The ith piece of each original chunk is used to recursively infer \nthe type of the ith .eld of the struct. The ArrayProphecy speci.es that the top-level structure in\u00advolves \nan array. However, predicting exactly where an array begins and ends is dif.cult, even for the magical \noracle. Consequently, the algorithm actually generates a three-.eld struct, where the .rst .eld 3 If \nparenthetical elements are not well-nested, the meta-tokens are dis\u00adcarded and replaced with ordinary \nsequences of simple tokens. allows for slop prior to the array, the middle .eld is the array itself, \nand the last .eld allows for slop after the array. If the slop turns out to be unnecessary, the rewriting \nrules will clean up the mess in the next phase. Finally, the UnionProphecy speci.es that the top-level \nstruc\u00adture is a union type with k branches. Like a StructProphecy, the UnionProphecy carries a chunks \nlist, with one element for each branch of the union. The algorithm uses each element to re\u00adcursively \ninfer a description for the corresponding branch of the union. Intuitively, the oracle produces the union \nchunks list by hor\u00adizontally partitioning the input chunks, whereas it partitions struct chunks vertically \nalong .eld boundaries. As an example, recall the Crashreporter.log data from Figure 2. Assuming a chunk \nis a line of data, the two chunks in the example consist of the token sequences (recall [*] and (*) are \nmeta\u00adtokens): Pdate  Ptime  Pint  Palpha [*] : ... -  Palpha [*] :  Palpha (*)  ... Given these \ntoken sequences, the oracle will predict that the top\u00adlevel type constructor is a struct with three .elds: \none for the tokens before the token [*], one for the [*] tokens themselves, and one for the tokens after \nthe token [*]. We explain how the oracle makes this prediction in the next section. The oracle then divides \nthe original chunks into three sets as follows. Pdate  Ptime  Pint  Palpha (set 1) -  Palpha [*] \n(set 2) [*] : ... (set 3) :  Palpha(*) ... On recursive analysis of set 1, the oracle again suggests \na struct is the top-level type, generating two more sets of chunks: Pdate Ptime Pint (set4) - Palpha \n(set 5) Palpha Now, since every chunk in set 5 contains exactly one base type token, the recursion bottoms \nout with the oracle claiming it has found the base type Palpha. When analyzing set 4, the ora\u00adcle detects \ninsuf.cient commonality between chunks and decides the top-most type constructor is a union. It partitions \nset 4 into two more sets, with each group containing only 1 chunk (either {Pdate  ...} or { - }). The \nalgorithm analyzes the .rst set to determine the type of the .rst branch of the union and the second \nset to determine the second branch of the union. With no variation in either branch, the algorithm quickly \ndiscovers an accurate type for each. Having completely discovered the type of the data in set 1, we turn \nour attention to set 2. To analyze this set, the algorithm cracks open the [*] meta-tokens to recursively \nanalyze the underlying data, a process which yields struct { [ ; Pint; ] ;}. Analysis of Set 3 proceeds \nin a similar fashion. As a second example, consider the Sirius data from Figure 2. Here the chunks have \nthe following structure: Pint | Pint | ... | Pint | Pint Pint | Pint | ... | Palpha Pint | Pint The \noracle prophecies that the top-level structure involves an array and partitions the data into sets of \nchunks for the array preamble, the array itself, and the array postamble. It does this partitioning type \ndescription (* an IR description *) type chunk (* a tokenized chunk *) type chunks = chunk list (* A \ntop-level description guess *) datatype prophecy = BaseProphecy of description | StructProphecy of \nchunks list | ArrayProphecy of chunks * chunks * chunks | UnionProphecy of chunks list (* Guesses \nthe best top-level description *) fun oracle : chunks -> prophecy (* Implements a generic inference \nalgorithm *) fun discover (cs:chunks) : description = case (oracle cs) of BaseProphecy b => b | StructProphecy \ncss => let Ts = map discover css in struct { Ts } | ArrayProphecy (csfirst,csbody,cslast) => let Tfirst \n= discover csfirst in let Tbody = discover csbody in let Tlast = discover cslast in struct { Tfirst; \narray { Tbody }; Tlast; } | UnionProphecy css => let Ts = map discover css in union{ Ts } Figure 5. \nA generic structure-discovery algorithm in Pseudo-ML. to cope with fence-post problems in which the .rst \nor the last entry in an array may have slightly different structure. In this case, the preamble chunks \nall have the form {Pint | } while the postamble chunks all have the form {Pint}, so the algorithm easily \ndetermines their types. The algorithm discovers the type of the array elements by analyzing the residual \nlist of chunks Pint | ... Pint | Pint | ... Palpha Pint | The oracle constructs this chunk list by removing \nthe preamble and postamble tokens from all input chunks, concatenating the remaining tokens, and then \nsplitting the resulting list into one chunk per array element. It does this splitting by assuming that \nthe chunk for each array element ends with a | token. So far so good, but how does the guessing work? \nWhy does the algorithm decide the Sirius data is basically an array but Crashre\u00adporter.log is a struct? \nAfter all, the Sirius chunks all have a Pint, just as all the Crashreporter.log chunks have a bracket \nmeta-token [*]. Likewise, Crashreporter.log contains many occurrences of the token, which might serve \nas an array separator as the | to\u00adken does in the Sirius data. The Magic. To generate the required prophecy \nfor a given list of chunks, the oracle computes a histogram of the frequencies of all tokens appearing \nin the input. More speci.cally, the histogram for token t plots the number of chunks (on the y-axis) \nhaving a certain number of occurrences of the token (on the x-axis). Figure 6 presents a number of histograms \ncomputed during analysis of the Crashreporter.log and Sirius chunk lists. Intuitively, tokens associated \nwith histograms with high cover\u00adage, meaning the token appears in almost every chunk, and narrow distribution, \nmeaning the variation in the number of times a token appears in different chunks is low, are good candidates \nfor de.ning structs. Similarly, histograms with high coverage and wide distri\u00adbution are good candidates \nfor de.ning arrays. Finally, histograms with low coverage or intermediate width represent tokens that \nform part of a union. Concretely, consider histogram (a) from Figure 6. It is a per\u00adfect struct candidate \nit has a single column that covers 100% of the records. Indeed, this histogram corresponds to the [*] \ntoken in Crashreporter.log. Whenever the oracle detects such a histogram, it will always prophecy a struct \nand partition the input chunks accord\u00ading to the associated token. All of the other top-level histograms \nfor Crashreporter.log contain variation and hence are less certain indi\u00adcators of data source structure. \nAs a second example, consider the top-level histograms (f), (b) and (g) for tokens Palpha, Pint and Pwhite, \nrespectively, and compare them with the corresponding histograms (h), (i) and (j) computed for the same \ntokens from chunk set 1, de.ned in the previous subsection. The histograms for chunk set 1 have far less \nvariation than the corresponding top-level histograms. In particular, notice that histogram (h) for token \nPalpha is a perfect struct his\u00adtogram whereas histogram (f) for token Palpha contains a great deal of \nvariation. This example illustrates the source of the power of our divide-and-conquer algorithm if the \noracle can identify even one token at a given level as de.ning a good partition for the data, the histograms \nfor the next level down become substantially sharper and more amenable to analysis. As a third example, \nconsider histogram (k). This histogram illustrates the classic pattern for tokens involved in arrays \nit has a very long tail. And indeed, the | token in the Sirius data does act like a separator for .elds \nof an array. To make the intuitions discussed above precise, we must de.ne a number of properties of \nhistograms. First, a histogram h for a token t is a list of pairs of natural numbers (x, y)where x denotes \nthe token frequency and y denotes the number of chunks with that frequency. All .rst elements of pairs \nin the list must be unique. The width of a histogram (width(h)) is the number of elements in the list \nexcluding the zero-column (i.e. excluding element (0,y)). A histogram \u00afh is in our normal form when the \n.rst element of the list is the zero column and all subsequent elements are sorted in descending order \nby the y component. For example, if h1 is the histogram [(0, 5), (1, 10), (2, 25), (3, 15)] then width(h1)is3 \nand \u00af its normal form h1 is [(0, 5), (2, 25), (3, 15), (1, 10)]. We often refer to y as the mass of the \nelement (x, y), and given a histogram h, we refer to the mass of the ith element of the list \u00af using \nthe notation h[i]. For instance, h1[3] =15 and h1[3] =10. The residual mass (rm)of a column i in a normalized \nhistogram h is the mass of all the columns to the right of i plus the mass of the Pwidth(h\u00af) \u00af zero-column. \nMathematically, rm(h, i\u00af)= h\u00af[0] + h[j]. j=i+1 \u00af For example, rm(h1, 1) = 5+15 +10 =30. The residual \nmass characterizes the narrowness of a histogram. Those histograms \u00af with low residual mass of the .rst \ncolumn (i.e., rm(h1, 1) is small) are good candidates for structs because the corresponding tokens occur \nexactly the same number of times in almost all records. To distinguish between structs, arrays and unions, \nwe also need to de.ne the coverage of a histogram, which intuitively is the number of chunks containing \nthe corresponding token. Mathemat\u00adically, it is simply the sum of the non-zero histogram elements: Pwidth(h\u00af) \n\u00af coverage(h\u00af)= j=1 h[j]. Finally, our algorithm works better when the oracle considers groups of tokens \nwith similar distributions together because with very high probability such tokens form part of the same \ntype con\u00adstructor. To determine when two histograms are similar,we use a symmetric form of relative entropy \n(Lin 1991). The (plain) rel\u00ad \u00af\u00af ative entropy of two normalized histograms h1 and h2, written \u00af\u00af R(h1 \n|| h2), isde.nedasfollows. width(h\u00af1) X \u00af\u00af\u00af\u00af R(h1 || h2)= h1[j]* log(h1[j]/h\u00af2 [j]) j=1 To createasymmetric \nform, we.rst .nd theaverage of thetwo his\u00adtograms in question (written h1 . h2) by summing corresponding \ncolumns and dividing by two. This technique prevents the denom\u00adinator from being zero in the .nal relative \nentropy computation. Using this de.nition, the symmetric relative entropy is: 11 \u00af\u00af\u00af\u00af\u00af\u00af\u00af\u00af S(h1 || h2)= \nR(h1 || h1 . h2)+ R(h2 || h1 . h2) 22 Now that we have de.ned the relevant properties of histograms, \nwe can explain how the oracle prophecies given a list of chunks. 1. Prophecy a base type when each chunk \ncontains the same sim\u00adple token. If each chunk contains the same meta-token, then prophecy a struct with \nthree .elds: one for the left paren, one for the body, and one for the right paren. 2. Otherwise, compute \nnormalized histograms for the input and group related ones into clusters using agglomerative cluster\u00ading: \nA histogram h1 belongs to group G provided there ex\u00ad  \u00af\u00af ists another histogram h2 in G such that S(h1 \n|| h2) < ClusterTolerance.where ClusterTolerance is a parameter of the algorithm. We do not require all \nhistograms in a cluster to have precisely the same histogram to allow for errors in the data. A histogram \ndissimilar to all others will form its own group. We have found a ClusterTolerance of 0.01 is effective. \n3. Determine if a struct exists by .rst ranking the groups by the minimum residual mass of all the histograms \nin each group. Find the .rst group in this ordering with histograms h satisfying the following criteria: \n rm(h)< MaxMass  coverage(h)> MinCoverage  where constants MaxMass and MinCoverage are parameters of \nthe algorithm. This process favors groups of histograms with high coverage and narrow distribution. If \nhistograms h1,..., hn from group G satisfy the struct criteria, the oracle will prophecy some form of \nstruct. It uses the histograms h1, ..., hn and the associated tokens t1, ..., tn to calculate the number \nof .elds and the corresponding chunk lists. We call t1, ..., tn the identi.ed tokens for the input. Intuitively, \nfor each input chunk, the oracle puts all tokens up to but not including the .rst token t from the set \nof identi.ed tokens into the chunk list for the .rst .eld. It puts t in the chunk list for the second \n.eld. It puts all tokens up to the next identi.ed token into the chunk list for the third .eld and so \non. Of course, the identi.ed tokens need not appear in the same order in all input chunks, nor in fact \nmust they all appear at all. To handle this variation when it occurs, the oracle prophecies a union instead \nof a struct, with one branch per token ordering and one branch for all input chunks that do not have \nthe full set of identi.ed tokens. 4. Identify an array by sorting all groups in descending order by coverage \nof the highest coverage histogram in the group. Find the .rst group in this ordering with any histograms \nthat satisfy the following minimum criteria: width(h)> 3  coverage(h)> MinCoverage  This process favors \nhistograms with wide distribution and high coverage. If histograms h1, ..., hn with corresponding tokens \nFigure 6. Histograms (a), (b), (c), (d), (e), (f) and (g) are generated from top-level analysis of Crashreporter.log \ntokens. The corresponding tokens are (a) [*],(b) Pint,(c) PDate,(d) PTime,(e) -, (f) Palpha and (g) Pwhite. \nHistograms (h) Palpha,(i) Pint,and (j) Pwhite are generated from analysis of Crashreporter.log from set \n1 (the second level of recursion). Histogram (k) is generated from top\u00adlevel analysis of the | token \nfrom the Sirius data. Note that several of these histograms have many bars of very small height, including \n(f) 100 100 80 80 60 60 40 40 20 20 0 0 (a) (b) (c) (d) (e) (f) (g) (h) (i) (j) (k) with 7, (g) with \n8, and (k) with 17. t1, ..., tn satisfy the array criteria, the oracle will prophecy an array. It will \npartition each input chunk into (1) a preamble subsequence that contains the .rst occurrence of each \nidenti\u00ad.ed token, (2) a set of element subsequences, with each subse\u00adquence containing one occurrence \nof the identi.ed tokens, and (3) a postamble subsequence that contains any remaining tokens from the \ninput chunk. 5. If no other prophecy applies, identify a union. Partition the input chunks according \nto the .rst token in each chunk.  3.3 Information-Theoretic Scoring We use an information theoretic \nscoring function to assess the quality of our inferred descriptions and to decide whether to apply rewriting \nrules to re.ne candidate descriptions. Intuitively, a good description is one that is both compact and \nprecise.There are trivial descriptions of any data source that are highly compact (e.g., the description \nthat says the data source is a string terminated by end of .le) or perfectly precise (e.g., the data \nitself abstracts nothing and therefore serves as its own description). A good scoring function balances \nthese opposing goals. As is common in machine learning, we have de.ned a scoring function based on the \nMinimum Description Length Principle (MDL), which states that a good description is one that minimizes \nthe cost (in bits) of transmitting thedata(Gr\u00a8unwald 2007). Mathematically, if T is a description and \nd1,...,dk are representations of the k chunks in our training set, parsed according to T, then the total \ncost in bits is: COST(T,d1,...,dk)=CT(T)+CD(d1,...,dk | T) where CT(T)is the number of bits to transmit \nthe description and CD(d1,...,dk | T)is the number of bits to transmit the data given the description. \nIntuitively, the cost in bits of transmitting a description is the cost of transmitting the sort of description \n(i.e., struct, union, enum, etc.) plus the cost of transmitting all of its sub\u00adcomponents. For example, \nthe cost of transmitting a struct type Pk CT(struct{T1;...;Tk;}) is CARD + i=1 CT(Ti) where CARD is the \nlog of the number of different sorts of type construc\u00adtors (24 of them in the IR presented in this paper). \nWe have de.ned the recursive cost function mathematically in full, but space limita\u00adtions preclude giving \nthat de.nition here. The cost of encoding data relative to selected types is shown in Figure 7. The top \nof the .gure de.nes the cost of encoding all data chunks relative to the type T; it is simply the sum \nof encoding each individual chunk relative to T. In the middle of the .gure, we de.ne the cost of encoding \na chunk relative to one of the integer base types; other base types are handled similarly. Notice that \nthe cost of encoding an inte\u00adger relative to the constant type PintConst is zero because the type itself \ncontains all information necessary to reconstruct the integer no data need be transmitted. The cost of \nencoding data Cost of encoding all training data relative to a type: Pk CD(d1,...,dk | T)= CD (di | T) \ni=1 Cost of encoding a single chunk relative to selected base types: CD (i | PintConst(p)) =0 CD (i | \nPint32) =32 CD (i | Pint64) =64 CD (i | PintRanged(pmin,pmax)) = 8 Cost of encoding a single chunk relative \nto selected types: CD ((d1,...,dk) | struct{T1; ...Tk; }) Pk = CD (di | Ti) i=1 CD (ini(d) | union{T1; \n...Tk; }) = log(k)+ CD (d | Ti) CD (ini(c) | enum{c1; ...ck; }) = log(k) CD (ini(d) | switch xof{c1=>T1; \n...ck =>Tk; }) = CD (d | Ti) Figure 7. Cost of transmitting data relative to a type, selected rules relative \nto Pint32 or Pint64 types is simply32or 64bits, respectively. Finally, we arti.cially set the cost of \nranged types PintRanged(pmin,pmax)to be in.nity because our experi\u00adments reveal that attempting to de.ne \ninteger types with minimum and maximum values usually leads to over.tting of the data.4 The last section \nof Figure 7 presents the cost of encoding data relative to selected type constructors. The cost of encoding \na struct is the sum of the costs of encoding its component parts. The cost of encoding a union is the \ncost of encoding the branch number (log(k)if the union has k branches) plus the cost of en\u00adcoding the \nbranch itself. The cost of encoding an enum is the cost of encoding its tag only given the tag, the \nunderlying data is de\u00adtermined by the type. The cost of encoding a switch is the cost of encoding the \nbranch only the tag need not be encoded because it is determined by the type and earlier data.  3.4 \nStructure Re.nement The goal of the structure-re.nement phase is to improve the struc\u00adture produced by \nthe structure-discovery phase. We formulate the structure-re.nement problem as a generalized search through \nthe description space starting with the candidate produced by structure discovery. The objective of the \nsearch is to .nd the description that minimizes the information-theoretic scoring function. 4 We nevertheless \nretain PintRanged types in our IR to encode the range of values found during the value-space analysis. \nDuring the rewriting phase, we use this range information to rewrite PintRanged into other integer types. \nSince the cost of encoding PintRanged is so high, the appropriate rewriting is guaranteed to be applied. \nIn the future, we may emit this range information as comments in the generated descriptions. Rewriting \nrules. To move around in the description space, we de.ne a number of rewriting rules, the general form \nof which is T . T . , if some constraint p(T )is satis.ed, where T is a type in the candidate description \nand T . is its re\u00adplacement after the rewriting. Some rules are unconditional and thus free of constraints. \nThere are two kinds of rewriting rules: (1) data-independent rules which transform a type based exclusively \non the syntax of the description; and (2) data-dependent rules which transform a type based both on the \nsyntax of the description and on properties of the training data parsed by type T . In general, the data-independent \nrules try to rearrange and merge portions of the description while the data dependent rules seek to identify \ncon\u00adstant .elds and enumerations, and to establish data dependencies between different parts of the description. \nFigure 8 presents a selection of the rewriting rules used in the re.nement phase. We have omitted many \nrules and have simpli.ed others for succinctness. When T [[X]] appears in a pattern on the left-hand \nside of a rewriting rule, X is bound to the set of data representations resulting from using T to parse \nthe appropriate part of each chunk from the training set. Furthermore, let card(X)be the cardinality \nof the set X,and let X(i)be the data representation resulting from parsing the ith chunk in the training \nset. Finally, given a union value inj(v),we de.ne tag(inj (v)) to be j. (* rewriting rules *) type rule \n: description -> description val rules : rule list (* measure the score for a type *) fun score : description \n-> float (* find the type with best score from a list *) fun best: description list -> description \n(* improve the given type by one rewriting rule *) fun oneStep (T:description) : description = let \nall = map (fn rule => rule T) rules in let top = best all in if (score top) < (score T) then oneStep \ntop else T (* main function to refine an IR description *) fun refine (T:description) : description \n= letT =case Tof baseb => b | struct { Ts } => struct { map refine Ts } | union { Ts } => union { \nmap refine Ts } | switch x of { vTs } => switch x of { map (fn (v, t) => (v, refine t)) vTs } | array{ \nT }=> array { refine T } | option { T } => option { refine T } in oneStep T Figure 9. Generic local \noptimization algorithm in Pseudo-ML The Search. The core of the rewriting system is a recursive, depth-.rst, \ngreedy search procedure. By depth-.rst, we mean the algorithm considers the children of each structured \ntype before considering the structure itself. When re.ning a type, the algorithm selects the rule that \nwould minimize the information-theoretic score of the resulting type and applies this rule. This process \nrepeats until no further reduction in the score is possible, at which point we say the resulting type \nT is stable. The rewriting phase applies the algorithm given in Figure 9 three times in succession. The \n.rst time, the algorithm quickly sim\u00adpli.es the initial candidate description using only data-independent \nrules. The second time, it uses the data-dependent rules to re.ne base types to constant values and enumerations, \netc., and to intro\u00adduce dependencies such as switched unions. This stage requires the value-space analysis \ndescribed next. The third time, the algo\u00adrithm re-applies the data-independent rules because some stage \ntwo rewritings (such as converting a base type to a constant) enable fur\u00adther data-independent rewritings. \nValue-space analysis. We perform a value-space analysis prior to applying the data-dependent rules. This \nanalysis .rst generates a set of relational tables from the input data. Each row in a table corresponds \nto an input chunk and each column corresponds to either a particular base type from the inferred description, \nor to a piece of meta-data from the description. Examples of meta-data include the tag number from union \nbranches and the length of arrays. We generate a set of relational tables as opposed to a single table \nas the elements of each array occupy their own separate table (a description with no arrays will have \nonly one associated table). We analyze every column of every table to determine proper\u00adties of the data \nin that column such as constancy and value range. To .nd inter-column properties, we have implemented \na simpli\u00ad.ed variant of the TANE algorithm (Huhtala et al. 1999), which identi.es functional dependencies \nbetween columns in relational data. Because full TANE is too expensive (possibly exponential in the number \nof columns), and produces many false positives when invoked with insuf.cient data, our simpli.ed algorithm \ncomputes only binary dependencies. We use the result of this dependency analysis to identify switched \nunions and .xed-size arrays. Running example. To illustrate the re.nement process, we walk through a \nfew of the steps taken to rewrite the Crashreporter.log description. The .rst part of the candidate description \ngenerated by the structure-discovery algorithm appears below. struct { union { struct { Pdate; Pwhite; \nPtime; Pwhite; Pint; Pwhite; (*) }; struct { \"-\"; Pwhite; (*) }; } Palpha; \"[\"; Pint; \"]\"; union \n{ ... }; }; In the .rst data-independent stage of rewriting, the common trailing white space marked \n(*) is pulled out of the union branches into the surrounding struct using the common post.x in union \nrule. This transformation leaves behind the single-element struct marked (**) in the result below; rewriting \nrules in stage three will trans\u00adform this verbose form into the more compact constant string \"-\". This \n.rst rewriting stage also pulls colon and whitespace characters out of the trailing union (not shown \nin the candidate description). struct { union {  struct { Pdate; Pwhite; Ptime; Pwhite; Pint; }; struct \n{ \"-\" }; (**) } Pwhite; (*) Palpha; \"[\"; Pint; \"]\"; \":\"; Pwhite; union { ... }; }; In the second \nrewriting stage, data-dependent rules 1 and 2 convert appropriate base types into constants and enums. \nMoreover,  Data independent rules 1. Singleton structs and unions struct{T }.T union{T }.T struct{}.Pempty \nunion{}.Pvoid 2. Struct and union clean-up struct{pre types; Pvoid; post types}.Pvoid struct{pre types; \nPempty; post types}. struct{pre types; post types} union{pre types; Pvoid; post types}. union{pre types; \npost types} 3. Uniform struct to .xed-length array struct{T1; ... ; Tn}.arrayFW{T1}[ n] if n =3 and \n.i .[1,n] ,j .[1,n]: Ti = Tj . 4. Common post.x in union branches union{struct{pre types1; T }; struct{pre \ntypes2; T }}. struct{union{struct{pre types1}; struct{pre types2}}; T } union{struct{pre types; T }; \nT }. struct{option{struct{pre types}}; T } 5. Combine adjacent constant strings struct{pre types; PstringConst( \nc1 ); PstringConst( c2 ); post types}.struct{pre types; PstringConst( c1 @ c2 ); post types}  Data dependent \nrules 1. Base type with unique values to constant Pint[[ X]] .PintConst( c) if .x .X : x = c. Palpha[[ \nX]] .PstringConst( c) if .x .X : x = c. Pstring[[ X]] .PstringConst( c) if .x .X : x = c. Pother[[ \nX]] .PstringConst( c) if .x .X : x = c.  2. Re.ne enums and ranges Pstring[[ X]] .enum{s1; ... ; sk} \nif .x .X : x .{s1,...,sk}. Pint[[ X]] .Pint32 if .x .X :0 =x< 2 32 .  3. Union to switch struct{pre \ntypes; enum{c1; ... ; cn}[[ X]]; mid types; union{T1; ... ; Tn}[[ Y ]]; post types} . struct{pre types, \nz : enum{c1; ... ; cn}; mid types; switch( z) {c1 .T.(1); ... ; cn .T.(n)}; post types} where z is a \nfresh variable, and there exists a permutation . ,s.t. .i .[1,card( X)] , .(tag( X( i))) = tag( Y ( i)) \n. Figure 8. Selected and simpli.ed rewriting rules TANE discovers a data dependency between the newly \nintroduced enumeration involving \"crashdump\" and \"mach msg\",and the structure of the following message. \nHence, we introduce a switched union. Notice that the switched union branches on a different enum than \nthe hand-written IR in Figure 3 because the inference algorithm found a different way of structuring \nthe data. Nonetheless, both of these descriptions are accurate. struct { union { struct { Pdate; \" \"; \nPtime; \" \"; 2006; }; struct { \"-\" }; }; \" \"; enum {\"crashreporterd\", \"crashdump\"}; \"[\"; PintRanged [120...29874]; \n\"]\"; \":\"; \" \"; x19:enum {\"crashdump\", \"mach_msg\", \"Finished\", \"Started\", \"Unable\", \"Failed\"}; switch \nx19 of { ... }; }; In the third and .nal stage, data independent rule 5 combines constants and rule \n1 .attens the singleton struct, resulting in the .nal IR description: struct { union { struct { Pdate; \n\" \"; Ptime; \" \"; 2006; }; \"-\"; }; \" \"; enum {\"crashreporterd\", \"crashdump\"}; \"[\"; Pint32; \"]: \"; x19:enum \n{\"crashdump\", \"mach_msg\", \"Finished\", \"Started\", \"Unable\", \"Failed\"}; switch x19 of { ... }; }; }; \n The information-theoretic complexity of the .nal description relative to the data in our training set \nis 304538 bits. The candi\u00addate description produced by the structure-discovery phase had a complexity \nof 416156 bits. The absolute values of these quantities are relatively unimportant, but the fact that \nthe .nal complexity is substantially smaller than the original suggests that our search pro\u00adcedure optimized \nthe description effectively. 3.5 End Products The previous subsections outline the central technical \nelements of our algorithms. The main tasks remaining include converting the internal representation into \na syntactically correct PADS de\u00adscription, feeding the generated description to the PADS compiler and \nproducing a collection of scripts that conveniently package the freshly-generated libraries with the \nPADS run-time system and tools. At the end of this process, users have a number of program\u00adming libraries \nand many powerful tools at their disposal. Perhaps the most powerful tools are the PADX query engine \n(Fern\u00b4andez et al. 2006) and the XML converter, which allow users to write ar\u00adbitrary XQueries over the \ndata source or to convert the data to XML for use by other software. Other useful tools include the accumula\u00adtor \ntool mentioned earlier, converters to translate data into a form suitable for loading into a relational \ndatabase or Excel spreadsheet, and a custom graphing tool that pushes data into gnuplot for data visualization. \nFigure 10 gives snapshots of the output of a couple of these tools.   4. Experimental Evaluation We \nconducted a series of experiments to study the correctness and performance of our format inference algorithm. \nTable 1 lists the data sources we used in the experiments; they range from system logs to application \noutputs to government statistics. Except for sir\u00adius.1000, which is a proprietary format, the .les are \nall available from www.padsproj.org/learning.html. The size of the Tiny fragment of XML output from crashreporter.log: \n<Struct_114> <var_7> <var_6> <var_0><val>Sat Jun 24</val></var_0> <var_2><val>06:38:46</val></var_2> \n<var_4><val>2006</val></var_4> </var_6> </var_7> <var_11><val>crashdump</val></var_11> <var_14><val>2164</val></var_14> \n ... Graph generated from ai.3000 web transaction volume at different times of the day (00:00-8:55 and \n19:00-24:00): BTy_25 Data source KB/Chunks Description 1967Transactions.short 70/999 transaction records \nMER T01 01.cvs 22/491 comma-sep records ai.3000 293/3000 webserver log asl.log 279/1500 log .le of Mac \nASL boot.log 16/262 Mac OS boot log crashreporter.log 50/441 original crash log crashreporter.log.mod \n49/441 modi.ed crash log sirius.1000 142/999 AT&#38;T phone provision data ls-l.txt 2/35 Stdout from \nUnix command ls -l netstat-an 14/202 output from netstat page log 28/354 printer logs quarterlypersonalincome \n10/62 spread sheet railroad.txt 6/67 US rail road info scrollkeeper.log 66/671 application log windowserver \nlast.log 52/680 log from LoginWindow server on Mac yum.txt 18/328 log from pkg install Table 1. Benchmark \npro.le including .lename, size in KB, num\u00adber of chunks and brief description. Data source SD(s) Ref(s) \nTot(s) HW(h) 1967Transactions.short 0.20 2.32 2.56 4.0 MER T01 01.csv 0.11 2.80 2.92 0.5 ai.3000 1.97 \n26.35 28.64 1.0 asl.log 2.90 52.07 55.26 1.0 boot.log 0.11 2.40 2.53 1.0 crashreport.log 0.12 3.58 3.73 \n2.0 crashreport.log.mod 0.15 3.83 4.00 2.0 sirius.1000 2.24 5.69 8.00 1.5 ls-l.txt 0.01 0.10 0.11 1.0 \nnetstat-an 0.07 0.74 0.82 1.0 page log 0.08 0.55 0.65 0.5 quarterlypersonalincome 0.07 5.11 5.18 48 railroad.txt \n0.06 2.69 2.76 2.0 scrollkeeper.log 0.13 3.24 3.40 1.0 windowserver last.log 0.37 9.65 10.07 1.5 yum.txt \n0.11 1.91 2.03 5.0 Table 2. Execution times. SD: time for structure-discovery phase; Ref: time for scoring \nand re.nement; Tot: end-to-end time for complete inference algorithm; HW: time taken in hours to hand\u00adwrite \nthe corresponding description. benchmarks varies from a few thousand lines to just a few dozen. Most \nof the data .les are line based, meaning that every line be\u00adcomes a chunk for the purposes of learning \nthe format. One ex\u00adception is netstat-an, in which chunks comprise multiple lines. We include two versions \nof crashreporter.log: the original crashre\u00adporter.log and the slightly modi.ed crashreporter.log.mod \nthat we used as an example in this paper. We include both to demon\u00adstrate that our minor modi.cations \nwere simply for expository pur\u00adposes. Performance. Our .rst set of experiments measures the time re\u00adquired \nto infer a description from example data. In all our experi\u00adments, we used an Apple PowerBook G4 with \na 1.67 GHz Proces\u00adsor and 512 MB DDR RAM running on Mac OSX 10.4 Tiger. Ta\u00adble 2 presents the execution \ntimes for the structure-discovery phase (SD), the re.nement phase (Ref) and the total (Tot) end-to-end \ntime of the algorithm including printing PADS descriptions and other overhead, all measured in seconds. \nFor accurate timing measure\u00adments, we ran the algorithm 10 times, and found the average after removing \nthe best and the worst times. There are two main lessons to take away from this initial set of benchmarks. \nFirst, the overall time to infer the structure of any our example .les was less than a minute, and was \nless than 10 seconds except on a couple of the larger .les. Hence, although we have spent very little \ntime optimizing our algorithm, it already appears perfectly capable of being used in real time by a programmer \nwishing to understand and process small ad hoc data .les. Second, discovery of an initial format is usually \nvery fast, taking less than 3 seconds in all cases. Most of the algorithm s time is spent in format rewriting, \nwhich often takes a factor of 10 or more time than structure discovery. Moreover, most of the rewriting \ntime is taken in the data analysis phase (numbers not shown). Consequently, if format rewriting (particularly \nthe data analysis phase) is taking too long, the user may abort it to produce a slightly less re.ned \ndescription that may nevertheless be perfectly suf.cient. To give a very rough idea of how using the \ninference system compares with programming descriptions by hand, we also mea\u00adsured the time it took for \na person to write descriptions of all of the data sources (See Table 2 again). Initially, our programmer \n(a Ph.D. in computer science) knew very little about how the PADS system worked in practice, having only \nread a few of our conference pa\u00adpers. Consequently, writing the .rst description took a long time, approximately \n48 hours (two days of working at an ordinary pace) for quarterlypersonalincome. While different people \nwith dif\u00adferent backgrounds will clearly learn at different rates, there is little doubt that the format \ninference algorithm is a tremendous bene.t to novices, particularly to those data analysts without a \nPh.D. in com\u00adputer science, who are uninterested in learning some new data de\u00adscription language. After \nsome practice, our programmer was able to write most descriptions in 1 to 2 hours, so generating descrip\u00adtions \nin a few seconds still has great bene.t, even to experts.  Figure 11. Execution times of training sets \nTo understand the scaling behavior of our algorithm, we ran\u00addomly selected 5%, 10%, 15%, ..., 80% of \nthe chunks in every data source and measured the performance of the algorithm on each sub\u00adset of the \ndata that was selected. Figure 11 plots the execution time against the percentage of each data source \nselected. These exper\u00adiments suggest that once a format is .xed, the cost of inference grows linearly \nwith the amount of data. However, it is also clear that the raw size of the data is not the only factor \ndetermining per\u00adformance. The nature and complexity of the format is also a signif\u00adicant factor. For \ninstance, windowserver last.log is only one third the size of sirius.1000, but takes substantially longer \nfor the infer\u00adence algorithm to process. Correctness. To evaluate the correctness of our algorithm, we \nagain selected random subsets of each data source, trained our algo\u00adrithm on those subsets and measured \nthe error rate of the inferred parser on the remaining data. Figure 12 graphs the percentage of successfully \nparsed records versus the percentage of the data used in training. Note that accuracy does not uniformly \nimprove. This variation is caused by the randomness in our data selection and the fact that in some cases, \nwe have very small absolute quantities of data relative to the underlying complexity of the formats. \nFor in\u00adstance, at 5% training size, ls-l.txt is just one line of data. To understand the correctness \nproperties of our algorithm from a different angle, we record the minimum training sizes in percent\u00adages \nrequired to achieve 90% and 95% accuracy for all the bench\u00admarks in Table 3. This table also reports \nthe normalized cost of a description (NCT), which we compute by dividing the .rst compo\u00adnent of the information-theoretic \nscore in Section 3.3 by the number of bits in the data. NCT gives a rough indication of the complexity \nof the data source. The higher the normalized score, the more com\u00adplicated the data, and the greater \nthe fraction of data is needed to learn an accurate description. The rows of of Table 3 are sorted in \nascending NS score. From the table, one can see that ls-l.txt and railroad.txt have high NS scores. This \nis because they are quite small data sources (2KB and 6KB respectively), yet have relatively complicated \nformats. Consequently, it takes a substantial portion of the data to learn an accurate parser. For most \nof the other data 0 10 20 30 40 50 60 70 80 Training size (%) Figure 12. Success rates of training sets \nData source NCT 90% 95% sirius.1000 0.0001 5 10 1967Transactions.short 0.0003 5 5 ai.3000 0.0004 5 10 \nasl.log 0.0012 5 10 scrollkeeper.log 0.0020 5 5 page log 0.0032 5 5 MER T01 01.csv 0.0037 5 5 crashreporter.log \n0.0052 10 15 crashreporter.log.mod 0.0053 5 15 windowserver last.log 0.0084 5 15 netstat-an 0.0118 25 \n35 yum.txt 0.0124 30 45 quarterlypersonalincome 0.0170 10 10 boot.log 0.0213 45 60 ls-l.txt 0.0461 50 \n65 railroad.txt 0.0485 60 75 Table 3. Correctness measures. NCT: normalized cost of descrip\u00adtion; Min \nTraining size (%) to obtain required accuracy sources, a substantially smaller percentage of the data \nis required to achieve high accuracy. Overall, for 11 of 16 benchmarks, less than 15% of the data is \nneeded to achieve 95% accuracy or more.  5. Discussion Dealing with errors. In 1967, Gold (1967) proved \nthat learning a grammar for any remotely sophisticated class of languages, includ\u00ading regular languages, \nis impossible if one is only given positive example data.5 Given this negative theoretical result, and \nthe prac\u00adtical fact that it is hard to be sure that training data is suf.ciently rich to witness all \npossible variation in the data, errors in inference are inevitable. Fortunately, detecting and recovering \nfrom errors in ad hoc data is one of the primary strengths of the PADS system. To determine exactly how \naccurate an inferred description is on any new data source, a user may run the accumulator tool. This \ntool catalogs exactly how many deviations from the description there 5 A positive example is a data source \nknown to be in the grammar to be learned. A negative example is one known not to be in the target gram\u00admar. \nPerfect learning with both positive and negative examples is possible. Unfortunately, data analysts are \nunlikely to have access to a suf.cient col\u00adlection of relevant ad hoc data that they know does not satisfy \nthe format they are interested in learning, we are forced to tackle the more dif.cult problem of learning \nfrom positive examples only. were overall in the data source as well as the error rate in every individual \n.eld. Hence, using this tool, a programmer can immedi\u00adately and reliably determine the effectiveness \nof inference for their data. If there is a serious problem, the user can easily edit the gen\u00aderated description \nby hand identi.cation of a problem .eld, a minor edit and recompilation of tools might just take 5 minutes. \nHence, even imperfectly-generated descriptions have great value in terms of improving programmer productivity. \nMoreover, all PADS\u00adgenerated parsers and tools have error detection, representation and recovery techniques. \nFor instance, when converting data to XML, errors encountered are represented explicitly in the XML document, \nallowing users to query the data for errors if they choose. Before graphing ad hoc data, an analyst may \nuse the accumulator tool to check if any errors occur in the .elds to be graphed. If not, there is no \nreason to edit the description at all graphing the correct .elds may proceed immediately. Future work. \nDiscovering tokens like IP address and date is highly bene.cial as such tokens act as compact, highly \ndescriptive, human-readable abstractions. Unfortunately, these tokens are also often mutually ambiguous. \nFor instance, an IP address, a .oating point number and a phone number can all be represented as some \nnumber of digits separated by periods. At the moment, we disam\u00adbiguate between them in the same way that \nlex does, by taking the .rst, longest match. In select cases, when we cannot disam\u00adbiguate in the tokenization \nphase, we try to correct problems using domain-speci.c rewriting rules in the structure re.nement phase. \nTo improve tokenization in the future, we plan to look at learning probabilistic models of a broad range \nof token types. We also in\u00adtend to explore .nding new tokens from the data itself, possibly by identifying \nabrupt changes in entropy (Hutchens and Alder 1998). 6. Related Work Researchers have been studying \ngrammar induction, the process of inferring descriptions of text-based data, for decades. Nevertheless, \nthe work we present in this paper represents an important and novel contribution to the .eld for three \nkey reasons: 1. Our system solves a new end-to-end problem not treated in past work the problem of generating \nan extensible suite of fully functional data processing tools directly from ad hoc data. Gen\u00aderating \nthis suite requires the combination of three elements: grammar induction, automatic intermediate representation \ngen\u00aderation and type-directed programming. A key contribution of this work is the conception, development \nand evaluation of this end-to-end system. 2. Past work on grammar induction has focused primarily on \nei\u00adther (1) theoretical problems, (2) natural language processing, (3) web page analysis, or (4) XML \ntyping. Our work tackles an understudied domain, that of complex system logs and other ad hoc data sources. \nSince ad hoc data has different characteristics from the previously studied domains, naive adaptations \nof the existing algorithms are unlikely to be effective. 3. From a technical standpoint, we developed \na new top-down structure-discovery algorithm and showed how to combine that productively with a classic \nbottom-up rewriting system based on the minimum description length principle. We demonstrate that our \nnew algorithm has good practical properties on ad hoc data sources: it usually infers correct descriptions \non a small amount of training data and its performance scales linearly relative to the amount of training \ndata used.  In the rest of this section, we analyze the most closely related work in more depth. Traditional \nGrammar Induction. Classic grammar induction al\u00adgorithms (Vidal 1994) can be divided into two classes: \nthose that require both positive and negative examples to discover a gram\u00admar and those that only require \npositive examples. The problem our system solves is the latter; negative examples of ad hoc data sources \nare not available in practice. Consequently, effective theo\u00adretical algorithms for learning from both \npositive and negative ex\u00adamples such as RPNI (Oncina and Garcia 1992) are not applicable in our context. \nUnfortunately, an early result by Gold (1967) showed that per\u00adfect grammar induction is impossible for \nany super.nite class of languages when the algorithm has no access to negative examples. A super.nite \nclass of languages is any set of languages that in\u00adcludes all .nite languages and at least one in.nite \nlanguage. Hence, all the most familiar classes of languages, including regular expres\u00adsions, context \nfree grammars and PADS are super.nite. There are two main tactics one can use to avoid this negative \nresult: (1) use domain knowledge to explicitly limit the class of languages to a non-super.nite class, \nor (2) give up on perfect language identi.\u00adcation and instead settle for approximate identi.cation (Wharton \n1974) through the use of probabilistic language models. Examples of non-trivial, non-super.nite language \nclasses with known inference algorithms include k-reversible languages (An\u00adgluin 1982), SOREs and CHAREs \n(Bex et al. 2006). None of these languages and the associated algorithms are a good .t for infer\u00adring \nPADS descriptions (even the regular subset of PADS with\u00adout dependencies and constraints). For example, \nad hoc data is un\u00adlikely to be reversible and hence k-reversible languages are not rel\u00adevant. SOREs are \na subset of the k-testable regular languages with a linear-size translation from automata to regular \nexpressions, but they carry the restriction that each symbol in the regular expression appear at most \nonce. A cursory glance at our hand-written PADS descriptions reveals that many such descriptions include \nrepeated use of the same symbol. Finally, it appears that CHAREs restrict the nesting of regular expression \noperators too severely to be of much use to us. For example, when a, b,and care atomic symbols, even \nthe simple expression (ab+ c)*is not a CHARE. Given the dif.culty of .nding useful non-super.nite language \nclasses, it is reasonable to turn to algorithms for approximate infer\u00adence that use probabilistic models. \nClassic examples of such pro\u00adcedures include work by Stolcke and Omohundro (1994) and Hong (2002). These \nand a number of other algorithms operate by repeat\u00adedly rewriting a candidate grammar (or set of candidate \ngrammars) until an objective function is optimized. If the training data for the learning system is the \nstrings s1, s2, ..., sn, these algorithms nor\u00admally start their process using the grammar s1 + s2 + \u00b7\u00b7\u00b7+ \nsn. Consequently, an enormous number of different rewrites may ap\u00adply to the initial candidate grammar. \nOur structure re.nement phase avoids these problems because it is preceded by a highly ef.\u00adcient histogram-based \nstructure-discovery algorithm that identi.es a good candidate grammar from which to start the search. \nAnother category of algorithms are those that learn various kinds of automata as opposed to regular expressions \nor gram\u00admars (Denis et al. 2004; Oncina and Garcia 1992; Raeymaekers et al. 2005). One dif.culty with \nadapting these algorithms to our task is that we would need to convert the inferred automata into a grammatical \nrepresentation so that we can present the result to users and funnel it to our tool-generation infrastructure. \nUnfortu\u00adnately, in theory, conversion from automata into regular expressions can result in an exponential \nblowup in the size of the representation. Moreover, a substantial blowup appears to be relatively common \nin practice (Bex et al. 2006). Consequently, these algorithms are not appropriate for our domain. Information \nExtraction. The basic goal of an information extrac\u00adtion system is to .nd and separate the interesting \nand relevant bits of information (the needles) from a haystack of data. Such systems are fundamentally \ndifferent from ours, in that they choose which bits of information to extract, while we learn a description \nof the entirety of a data source, leaving the choice about which pieces are interesting to down-stream \napplications. Of course, this option is only feasible because we target ad hoc data, which is fairly \nstruc\u00adtured and dense in useful information, rather than web pages or free text, which are the usual \ntargets for information extraction systems. A common approach to information extraction involves an in\u00adductive \nlearning process in which a user manually tags the relevant data in sample documents. An example might \nbe highlighting prod\u00aduct names and prices on a collection of shopping web pages from a particular site. \nThe learning system then uses these labelled docu\u00adments in two ways: .rst, to decide which bits of information \nshould be extracted from the page (i.e., product names and prices), and second, to construct a wrapper \nfunction to extract those bits of in\u00adformation from similar pages. Soderland s WHISK system (1999) is \nan example of such an extraction system. It is particularly gen\u00aderal as it makes few assumptions about \nthe form of the source text, operating over structured data, stylized text such as Craig s List de\u00adscriptions, \nor free-form text. WHISK differs from our system in that it requires user labeling and then only extracts \na collection of tuples from the data source rather than returning the complete structure of the data \nsource. Kushmerick and colleagues (1997; 1997) focus on more struc\u00adtured data to reduce the amount of \nlabeling required during train\u00ading. In particular, this work assumes the labelled pages conform to one \nof six different templates, the most well-developed of which has the form of a header, followed by a \nsequence of K-tuples each of which is .anked by a pair of begin and end tags, followed by a trailer. \nFor such documents, the system generates a wrapper to ex\u00adtract the K-tuples. The use of .xed templates \nand the primary focus on relational data makes this work quite different from ours. Muslea et al. (2003) \ntackle a similar problem, but strive to re\u00adduce the amount of labeling by having the learning system \nchose which documents to have the user label, selecting documents by their probative value. Borkar et \nal. (2001) uses hand-labelled train\u00ading examples and a user-speci.ed set of desired features to train \nHidden Markov Models to select the desired features from simi\u00adlar documents. This work is quite successful \nat learning to select the relevant features of addresses and bibliographic citations from a variety of \ninput formats. In general, systems that depend upon la\u00adbeling are unlikely to be helpful in our context; \nrather than spending time explicitly labeling documents, the user might as well write a PADS description \nby hand. More closely related are various efforts to identify tabular data either from free-form text \n(Ng et al. 1999; Pinto et al. 2003) or from web pages (Lerman et al. 2004). These approaches typically \nuse hand-labelled examples to train machine learning systems to iden\u00adtify the tables. They then use heuristics \nspeci.c to tabular data to extract the tuples contained within those tables. The portion of this work \nrelated to identifying structured data from within more free\u00adform documents is complementary to ours. \nThe portion responsible for deconstructing the identi.ed tables uses more speci.c domain\u00adknowledge related \nto the form of tables than we do. Web pages generated in response to queries tend to be formed by sloting \nthe resulting tuples into a standard template. Another line of work aims to separate such templates from \nthe payload data (Arasu and Garcia-Molina 2003; Crescenzi et al. 2001). Arasu and Garcia-Molina use a \ntop-down grammar induction algorithm somewhat similar to our rough structure-inference phase (though \nit does not use histograms), but has no description-rewriting engine. This algorithm exploits the hierarchical \nnesting structure of XML documents in essential ways and so cannot be applied directly to ad hoc data. \nXML Type Inference. Many researchers have studied the prob\u00adlem of learning a schema such as a DTD or \nXSchema from a col\u00adlection of XML documents (Bex et al. 2006, 2007; Fernau 2001; Garofalakis et al. 2000). \nAt a high level, this task is similar to the format inference component of our system. However, the details \ndiffer because XML has different characteristics from ad hoc data. One difference is that XML documents \ncome in a well-nested tree shape, with obvious delimiters de.ning the structure. A second im\u00adportant \ndifference is that the appropriate tokenization for a given ad hoc data source is often not known in \nadvance. In contrast, tokens in XML documents are clearly demarcated using angle bracket syn\u00adtax. As \na result of these differences, XML inference algorithms cannot be used off-the-shelf for understanding \nthe structure of ad hoc data. They must be modi.ed, tuned and empirically evaluated on this new task. \nOne line of research on schema inference for XML makes use of the observation that 99% of the content \nmodels for XML nodes are de.ned as SOREs or CHAREs (Martens et al. 2006). This ob\u00adservation allows Bex \net al. (2006) to de.ne an ef.cient algorithm for inferring concise DTDs. Later Bex et al. (2007) build \non this work by showing how to infer k-local XML Schema de.nitions also based on SORES. A k-local de.nition \nallows node content to depend on the parent tag, grandparent tag, etc. (up to k levels for some .xed \nk). As mentioned earlier, hand-written PADS descrip\u00adtions do not generally obey the SOREs or CHAREs restriction, \nnor are they generally arranged with a nesting structure that suggests k\u00adlocal inference will be particularly \nuseful. The successful applica\u00adtion of these techniques to XML data reinforces the idea that the ad hoc \ndata we analyze has quite different characteristics from XML, and therefore the ad hoc data inference \nproblem merits study inde\u00adpendent of the XML inference problem. XTRACT (Garofalakis et al. 2000) is another \nsystem for infer\u00adring DTDs for XML documents. It operates in three phases: gen\u00aderalization, factoring \nand MDL optimization. The .rst phase plays a role similar to our structure discovery phase in that it \ngenerates a collection of candidate structures from a series of XML examples. This generalization phase \nsearches for patterns in XML data; it is tuned using the authors knowledge of common DTD structures. \nFactoring decreases the size of generated candidate DTDs; some of the factoring rules resemble our rewriting \nrules. Finally, they tackle the MDL optimization problem by mapping the problem into an in\u00adstance of \nthe NP-complete Facility Location Problem, which they solve using a quadratic approximation algorithm. \nOur MDL-guided rewriting problem considers a more general set of rewriting rules and hence we cannot \nreuse their technique. Other work. Potter s Wheel (Raman and Hellerstein 2001) is a system that attempts \nto help users .nd and purge errors from relational data sources. It does so through the use of a spread\u00adsheet \nstyle interface, but in the background, a grammar inference algorithm infers the structure of the input \ndata, which may be ad hoc, somewhat like ours. This inference algorithm operates by enumerating all possible \nsequences of base types that appear in the training data. Since Potter s Wheel is aimed at processing \nrelational data, they only infer struct types as opposed to enumerations, arrays, switches or unions. \nThe TSIMMIS project (Chawathe et al. 1994) aims to allow users to manage and query collections of heterogeneous, \nad hoc data sources. TSIMMIS sits on top of the Rufus system (Shoens et al. 1993), which supports automatic \nclassi.cation of data sources based on features such as the presence of certain keywords, magic numbers \nappearing at the beginning of .les and .le type. This sort of classi.cation is materially different from \nthe syntactic analysis we have developed. 7. Conclusions Managing ad hoc data is a tedious, error-prone \nand costly enter\u00adprise. By augmenting the PADS data processing language and sys\u00adtem with an ef.cient \nformat inference engine, we have effectively cut the generation time for useful data analysis and transformation \ntools from hours or days to seconds. Now, within moments of re\u00adceiving a new ad hoc data source, programmers \ncan write complex semi-structured queries to extract information, produce informative graphs of key statistics, \nconvert the data into a format amenable to easy loading into Excel or translate to XML for processing \nwith other standard programming libraries and systems. Systems admin\u00adistrators, computational scientists, \n.nancial analysts, industrial data management teams and everyday programmers will all bene.t sub\u00adstantially \nfrom this new capability to translate dirt into useful shov\u00adels for ad hoc data processing.  Acknowledgments \nOur work bene.ted greatly from thoughts and comments from Alex Aiken, David Blei, David Burke, Vikas \nKedia, John Launchbury, Chris Ramming, Rob Schapire and the organizers and attendees of the CAGI 2007 \nWorkshop on Grammar Induction. This material is based upon work supported by DARPA under grant FA8750-07-C-0014 \nand the NSF under grants 0612147 and 0615062. Any opinions, .ndings, and conclusions or recommenda\u00adtions \nexpressed in this material are those of the authors and do not necessarily re.ect the views of DARPA \nor the NSF. References Dana Angluin. Inference of reversible languages. Journal of the ACM,29 (3):741 \n765, 1982. Arvind Arasu and Hector Garcia-Molina. Extracting structured data from web pages. In SIGMOD, \npages 337 348, 2003. Geert Jan Bex, Frank Neven, Thomas Schwentick, and Karl Tuyls. Infer\u00adence of concise \nDTDs from XML data. In VLDB, pages 115 126, 2006. Geert Jan Bex, Frank Neven, and Stijn Vansummeren. \nInferring XML schema de.nitions from XML data. In VLDB, pages 998 1009, 2007. Vinayak Borkar, Kaustubh \nDeshmukh, and Sunita Sarawagi. Automatic segmentation of text into structured records. In SIGMOD, pages \n175 186, New York, NY, USA, 2001. David Burke, Kathleen Fisher, David Walker, Peter White, and Kenny \nQ. Zhu. Towards 1-click tool generation with PADS. In CAGI, Corvallis, OR, June 2007. Sudarshan Chawathe, \nHector Garcia-Molina, Joachim Hammer, Kelly Ireland, Yannis Papakonstantinou, Jeffrey D. Ullman, and \nJennifer Widom. The TSIMMIS project: Integration of heterogeneous informa\u00adtion sources. In 16th Meeting \nof the Information Processing Society of Japan, pages 7 18, Tokyo, Japan, 1994. Valter Crescenzi, Giansalvatore \nMecca, and Paolo Merialdo. Roadrunner: Towards automatic data extraction from large web sites. In VLDB, \npages 109 118, San Francisco, CA, USA, 2001. Franc\u00b8ois Denis, Aur\u00b4elien Lemay, and Alain Terlutte. Learning \nregular languages using RFSAs. Theoretical Computer Science, 313(2):267 294, 2004. Mary F. Fern\u00b4andez, \nKathleen Fisher, Robert Gruber, and Yitzhak Mandel\u00adbaum. PADX: Querying large-scale ad hoc data with \nXQuery. In PLAN-X, January 2006. Henning Fernau. Learning XML grammars. In MLDM, pages 73 87, 2001. Kathleen \nFisher and Robert Gruber. PADS: A domain speci.c language for processing ad hoc data. In PLDI, pages \n295 304, June 2005. Kathleen Fisher, Yitzhak Mandelbaum, and David Walker. The next 700 data description \nlanguages. In POPL, January 2006. Minos N. Garofalakis, Aristides Gionis, Rajeev Rastogi, S. Seshadri, \nand Kyuseok Shim. XTRACT: A system for extracting document type descriptors from XML documents. In SIGMOD, \npages 165 176, 2000. E. M. Gold. Language identi.cation in the limit. Information and Control, 10(5):447 \n474, 1967. Peter D. Gr\u00a8unwald. The Minimum Description Length Principle.MIT Press, May 2007. Theodore \nW. Hong. Grammatical Inference for Information Extraction and Visualisation on the Web. Ph.D. Thesis, \nImperial College London, 2002. Yk\u00a8a Huhtala, Juha K\u00a8arkk\u00a8ainen, Pasi Porkka, and Hannu Toivonen. TANE: \nAn ef.cient algorithm for discovering functional and approximate de\u00adpendencies. The Computer Journal, \n42(2):100 111, 1999. Jason L. Hutchens and Michael D. Alder. Finding structure via compres\u00adsion. In David \nM. W. Powers, editor, Proceedings of the Joint Con\u00adference on New Methods in Language Processing and \nComputational Natural Language Learning, pages 79 82. 1998. N. Kushmerick. Wrapper induction for information \nextraction. PhD thesis, University of Washington, 1997. Department of Computer Science and Engineering. \nNicholas Kushmerick, Daniel S. Weld, and Robert B. Doorenbos. Wrapper induction for information extraction. \nIn IJCAI, pages 729 737, 1997. Kristina Lerman, Lise Getoor, Steven Minton, and Craig Knoblock. Using \nthe structure of web sites for automatic segmentation of tables. In SIGMOD, pages 119 130, New York, \nNY, USA, 2004. J. Lin. Divergence measures based on the Shannon entropy. IEEE Transac\u00adtions on Information \nTheory, 37(1):145 151, 1991. Yitzhak Mandelbaum, Kathleen Fisher, David Walker, Mary Fernandez, and Artem \nGleyzer. PADS/ML: A functional data description language. In POPL, January 2007. Wim Martens, Frank Neven, \nThomas Schwentick, and Geert Jan Bex. Expressiveness and complexity of XML schema. ACM Transactions on \nDatabase Systems, 31(3):770 813, 2006. Ion Muslea, Steve Minton, and Craig Knoblock. Active learning \nwith strong and weak views: a case study on wrapper induction. In IJCAI, pages 415 420, 2003. Hwee Tou \nNg, Chung Yong Lim, and Jessica Li Teng Koo. Learning to recognize tables in free text. In ACL, pages \n443 450, Morristown, NJ, USA, 1999. J. Oncina and P. Garcia. Inferring regular languages in polynomial \nupdated time. Machine Perception and Arti.cial Intelligence, 1:29 61, 1992. PADS Project. PADS project. \nhttp://www.padsproj.org/, 2007. David Pinto, Andrew McCallum, Xing Wei, and W. Bruce Croft. Table extraction \nusing conditional random .elds. In SIGIR, pages 235 242, New York, NY, USA, 2003. Stefan Raeymaekers, \nMaurice Bruynooghe, and Jan Van den Bussche. Learning (k, l)-contextual tree languages for information \nextraction. In ECML, pages 305 316, 2005. Vijayshankar Raman and Joseph M. Hellerstein. Potter s wheel: \nAn inter\u00adactive data cleaning system. In VLDB, pages 381 390, 2001. Kurt A. Shoens, Allen Luniewski, \nPeter M. Schwarz, James W. Stamos, and II Joachim Thomas. The Rufus system: Information organization \nfor semi-structured data. In VLDB, pages 97 107, San Francisco, CA, USA, 1993. Stephen Soderland. Learning \ninformation extraction rules for semi\u00adstructured and free text. Machine Learning, 34(1-3):233 272, 1999. \nAndreas Stolcke and Stephen Omohundro. Inducing probabilistic grammars by bayesian model merging. In \nICGI, pages 106 118, 1994. Enrique Vidal. Grammatical inference: An introduction survey. In ICGI, pages \n1 4, 1994. R. M. Wharton. Approximate language identi.cation. Information and Control, 26(3):236 255, \n1974.  \n\t\t\t", "proc_id": "1328438", "abstract": "<p>An <i>ad hoc data source</i> is any semistructured data source for which useful data analysis and transformation tools are not readily available. Such data must be queried, transformed and displayed by systems administrators, computational biologists, financial analysts and hosts of others on a regular basis. In this paper, we demonstrate that it is possible to generate a suite of useful data processing tools, including a semi-structured query engine, several format converters, a statistical analyzer and data visualization routines directly from the ad hoc data itself, without any human intervention. The key technical contribution of the work is a multi-phase algorithm that automatically infers the structure of an ad hoc data source and produces a format specification in the PADS data description language. Programmers wishing to implement custom data analysis tools can use such descriptions to generate printing and parsing libraries for the data. Alternatively, our software infrastructure will push these descriptions through the PADS compiler, creating format-dependent modules that, when linked with format-independent algorithms for analysis and transformation, result infully functional tools. We evaluate the performance of our inference algorithm, showing it scales linearlyin the size of the training data - completing in seconds, as opposed to the hours or days it takes to write a description by hand. We also evaluate the correctness of the algorithm, demonstrating that generating accurate descriptions often requires less than 5% of theavailable data.</p>", "authors": [{"name": "Kathleen Fisher", "author_profile_id": "81331492634", "affiliation": "AT&T Labs Research, San Jose, CA", "person_id": "PP43124112", "email_address": "", "orcid_id": ""}, {"name": "David Walker", "author_profile_id": "81100426485", "affiliation": "Princeton University, Princeton, NJ", "person_id": "PP43120496", "email_address": "", "orcid_id": ""}, {"name": "Kenny Q. Zhu", "author_profile_id": "81100413639", "affiliation": "Princeton University, Princeton, NJ", "person_id": "PP309015200", "email_address": "", "orcid_id": ""}, {"name": "Peter White", "author_profile_id": "81539386156", "affiliation": "Galois: Inc., Beaverton, OR", "person_id": "PP43124919", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1328438.1328488", "year": "2008", "article_id": "1328488", "conference": "POPL", "title": "From dirt to shovels: fully automatic tool generation from ad hoc data", "url": "http://dl.acm.org/citation.cfm?id=1328488"}