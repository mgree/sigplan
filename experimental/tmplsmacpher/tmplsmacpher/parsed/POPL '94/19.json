{"article_publication_date": "02-01-1994", "fulltext": "\n Analyzing Logic Programs Kim Marriott Maria Dept. of Computer Science Monash University, Clayton Vic \n3168 Australia marriott@cs .monash. edu. au Abstract Traditional logic programming languages, such as \nProlog, use a fixed left-to-right atom schedul\u00ading rule. Recent logic programming languages, however, \nusually provide more flexible schedul\u00ading in which computation generally proceeds left\u00adto-right but in \nwhich some calls are dynamically delayed until their arguments are sufficiently in\u00adstantiated to allow \nthe call to run efficiently. Such dynamic scheduling has a significant cost. We give a framework for \nthe global analysis of logic pro\u00adgramming languages with dynamic scheduling and show that program analysis \nbased on thk frame\u00adwork supports optimizations which remove much of the overhead of dynamic scheduling. \n1 Introduction The first logic programming languages, such as DEC\u00ad10 Prolog, used a fixed scheduling \nrule in which all atoms in the goal were processed left-to-right. Unfor\u00adtunately, this meant that programs \nwritten in a clean, declarative style were often very inefficient, only ter\u00adminated when certain inputs \nwere fully instantiated or ground , and (if negation was used) produced wrong results. For this reason \nthere has been widespread interest in a class of second-generation logic pro\u00adgramming languages, such \nas IC-Prolog, NU-Prolog, Prolog-II, Sicstus-Prolog, Prolog-III, CHIP, Prolog M, and SEPIA, etc., that \nprovide more flexible scheduling in which computation generally proceeds left-to-right but in which some \ncalls are dynamically delayed un\u00ad til their arguments are sufficiently instantiated to al\u00adlow the call \nto run efficiently. Such dynamic schedul\u00ad Pen-rission to copy without fee all or part of this material \nis granted provided that the copies are not made or distributed for direct commercial advantege, the \nACM copyright notice end the title of tha publication and its date appear, and notice is given that copying \nis by permission of the Association for Computing Mechinary. To copy otherwise, or to republish, requiras \na fee and/or epecific permission. POPL 94-1/94, Portland Oragon,USA @ 1994 ACM O-89791 -636-0r94KXY ..$3.50 \n with Dynamic Scheduling JOS6 Garcia de la Banda Manuel Hermenegildo Facultad de InformAtica -UPM 28660-Boadilla \ndel Monte, Madrid Spain {maria, herme}@f i.upm. es ing overcomes the problems associated with traditional \nPrologs and their fixed scheduling. First, it allows the same program to have many different and efficient \noper\u00adational semantics as the operational behaviour depends on which arguments are supplied in the query. \nThus, programs really behave efficiently as relations, rather than as functions. Second, the treatment \nof negation is sound, as negative calls are delayed until all argu\u00adments are ground. Third, it allows \nintelligent search in combinatorial constraint problems. Finally, dynamic scheduling allows a new style \nof programming in which Prolog procedures are viewed as processes which com\u00admunicate asynchronously through \nshared variables. Unfortunately, dynamic scheduling has a signifi\u00adcant cost; goals, if affected by a \ndelay declaration, must be checked to see whether they should delay or not; upon variable binding, possibly \ndelayed calls must be woken or put in a pending list, so that they are woken before the next goal is \nexecuted; also, few register allo\u00adcation optimization can be performed for delayed goals; finally, space \nneeds to be allocated for delayed goals until they are woken [1]. Furthermore, global dataflow analyses \nused in the compilation of traditional Prologs, such as mode analysis, are not correct with dynamic scheduling. \nThis means that compilers for languages with dynamic scheduling are currently unable to per\u00adform optimizations \nwhich improve execution speed of traditional Prologs by an order of magnitude [19, 21, 31, 32, 33]. However, \nit is not simple to extend anal\u00adyses for traditional Prologs to languages with dynamic scheduling, as \nin existing analyses the fixed scheduling is crucial to ensure correctness and termination. Here we develop \na framework for global dataflow analysis of logic languages with dynamic scheduling. This provides the \nbasis for optimizations which remove the overhead of dynamic scheduling and promises to make the performance \nof logic languages with dynamic scheduling competitive with traditional Prolog. First, we give a denotational \nsemantics for lan\u00adguages with dynamic scheduling. This provides the se\u00admantic basis for our generic analysis. \nThe main differ\u00adence with denotational definitions for traditional Prolog is that sequences of delayed \natoms must also be ab\u00adstracted and are included in calls and answers . A key feature of the semantics \nis to approximate sequences of delayed atoms by multisets of atoms which are anno\u00adtated to indicate if \nthey are possibly delayed or if they are definitely delayed. The use of multisets instead of sequences \ngreatly simplifies the semantics with, we be\u00adlieve, little loss of precision. This is because in most \nreal programs delayed atoms which wake at the same time are independent while delayed atoms which are \ndependent will be woken at different times. Second, we give a generic global dataflow analysis algorithm \nwhich is based on the denotational semantics. Correctness is formalized in terms of abstract interpre\u00adtation \n[7]. The analysis gives information about call arguments and the delayed calls, as well as implicit in\u00adformation \nabout possible call scheduling at runtime. The analysis is generic in the sense that it has a para\u00admetric \ndomain and various parametric functions. The parametric domain is the descriptions chosen to approx\u00adimate \nsets of term equations. Different choices of de\u00adscriptions and associated parametric functions provide \ndifferent information and give different accuracy. The parametric functions also allow the analysis to \nbe tai\u00adlored to particular system or language dependent crite\u00adria for delaying and waking calls. Implementation \nof the analysis is by means of a memoization table in which information about the calls and their answers \nen\u00adcountered in the derivations from a particular goal are iteratively computed. Finally, we demonstrate \nthe utility and practical importance of the dataflow analysis algorithm. We sketch an example instantiation \nof the generic analysis which gives information about groundless and freeness of variables in the delayed \nand actual calls. Information from the example analysis can be used to optimize tar\u00adget code in many \ndifferent ways. In particular, it can be used to reduce the overhead of dynamic scheduling by removing \nunnecessary tests for delaying and awak\u00adening and by reordering goals so that atoms are not delayed. \nIt can also be used to perform optimization used in the compilation of traditional Prolog such as: recognizing \ndeterminate code and so allowing unneces\u00adsary backtrack points to be deleted; improving the code generated \nfor unification; recognizing calls which are independent and so allow the program to be run in parallel, \netc. Preliminary test results, given here, show that the analysis and associated optimization used to \nreduce the overhead of dynamic scheduling give signif\u00adicant improvements in the performance of these \nlan\u00adguages. Abstract interpretation of standard Prolog was suggested by Mellish [26] as a way to formalize \nmode anal ysis. Since then, it has been an active research area and many frameworks and applications \nhave been given, for example see [91. The approach to program analv\u00ad . sis taken here is-based on the \ndenotational approach of Marriott et. al. [25]. A common implementation of abstract interpretation based \nanalyses of Prolog is in terms of memorization tables [11, 13] which our analy\u00adsis generalizes. To our \nknowledge this is the first paper to consider the global dataflow analysis of logic pro\u00adgramming languages \nwith delay. Related work includes Marriott et. al. [24] which gives a dataflow analysis for a logic programming \nlanguage in which negated calls are delayed until their arguments are fully ground. However the analysis \ndoes not generalize to the case considered here as correctness relies on calls only being woken when \nall of their arguments are ground. Other related work is the global analysis of concurrent constraint \nprogram\u00adming languages [4, 5, 6, 14]. These languages differ from the languages considered here as they \ndo not have a de\u00adfault left-to-right scheduling but instead the compiler or interpret er is free to choose \nany scheduling. Thus, pro\u00adgram analysis must be correct for all scheduling. In our setting, knowledge \nof the default scheduling allows much more precise analysis. Related work also includes Gudeman et. al. \n[16] and Debray [12] which investigate local analyses and optimization for the compilation of Janus, \na concurrent constraint programming language. The optimization include reordering and removal of redundant \nsuspension conditions. Debray [10] studies global analysis for compile-time fixed scheduling rules other \nthan left-to-right. However this approach does not work for dynamic scheduling, nor for analyses to determine \nfreeness information. Finally, Hanus [17] gives an analysis for improving the residuation mecha\u00adnism \nin functional logic programming languages. This analysis handles the delay and waking of equality con\u00adstraints, \nbut does not easily extend to handle atoms as these may spawn subcomputations which in turn have delayed \natoms. In the next section we give a simple example to il\u00adlustrate the usefulness of dynamic scheduling \nand the type of information our analysis can provide. In Section 3 we give the operational semantics \nof logic languages with dynamic scheduling. In Section 4 we review ab\u00adstract interpretation and introduce \nvarious descriptions used in the analysis. In Section 5 we give the deno\u00adtational semantics. In Section \n6 we give the generic analysis, and in Section 7 we give modifications which ensure termination. In Section \n8 we give an example analysis. Section 9 presents some performance results and in Section 10 we conclude. \n2 Example The following program adapted from Naish [30], illus\u00adtrates the power of allowing calls to \ndelay and the infor\u00admat ion our analysis can provide. The program permute is a simple definition of the \nrelationship that the first argument is a permutation of the second argument. It makes use of the procedure \ndelete (X, Y, Z) which holds if Z is the list obtained by removing X from the list Y. permute(X, Y) i--X \n= nil, Y = nil. permute(X, Y) + X=u: xl, delete(U, Y, Z), permute(Xl, Z)< delete(X, Y, Z) +-Y=x:z. \ndelete(X, Y, Z) * Y= U:Y1, Z=u:zl, delete(X, Yl, Zl) Note that uppercase letters denote variables. Clearly \nthe relation declaratively given by permute is symmetric. Unfortunately, the behavior of the program \nwith traditional Prolog is not: Given the query, Q 1, ? permute(x)a :b :nil) Prolog will correctly backtrack \nthrough the answers X = a:b:nil and X = b:a:nil. However for the query, Q2, ? permute(a : b : nil, X) \nProlog will first return the answer X = a : b : nil and on subsequent backtracking will go into an infinite \nderivation without returning any more answers. For languages with delay the program pemnute does behave \nsymmetrically. For instance, if the above program is given to the NU-Prolog compiler, a pre\u00adprocessor \nwill generate the following when declara\u00adtions: ? permute(X, Y)whenXorY. ? delete(X, Y : Z, U)whenZorU, \n These may be read as saying that the call permute (X, Y) should delay until X or Y is not a vari\u00adable, \nand that the call delete (X, Y : Z, U) should delay until Z or U is not a variable. Of course program\u00admers \ncan also annotate their programs with when dec\u00adlarations. Given these declarations, both of the above \nqueries will behave in a symmetric fashion, backtrack\u00ading through the possible permutations and then \nfailing. What happens is ,that with Q 1 execution proceeds as in standard Prolog because no atoms are \ndelayed. With Q2, however, calls to delete are delayed and only woken after the recursive calls to permute. \nThe dataflow analysis developed in this paper, can be used to analyze this program with these queries. \nIn the case of Q 1 it will determine that the overhead of delaying is not needed as no call ever delays \nif the second argument is ground. Furthermore, it will also determine that in all calls to permute the \nfirst argument will be a variable and the second argument will be ground, and in all calls to delete \nthe first and third arguments will be variables, and the second will be ground. This can be used to optimize \nthe code for unification. In the case of Q2 it will determine that all calls to delete from the second \nclause delay. Furthermore, that in all calls to permute the first argument will be ground and in all \ncalls to delete when unification is performed, the first and third arguments will be ground, and the \nsecond will be a variable. The reader is encouraged to check that this is indeed true! Again this information \ncan be used to optimize the code for unification, parallelism or other purposes. The benefits obtained \nfrom the optimizations made possible with such information are illustrated by the performance results \npresented in Section 9. We note that if a traditional mode analysis is per\u00adformed with the query Q2 it \nwill ignore delaying and incorrectly generate the information that the third ar\u00ad gument of delete is \nfree (which it would be in the non\u00ad terminating execution that the analyzer would be ap\u00ad proximating) \nrather than ground.  3 Operational Semantics In this section we give some preliminary notation and an \noperational semantics for logic programs with dynamic scheduling. A logic program, or program, is a finite \nset of clauses. A clause is of the form H +--B where H, the head, is an atom and B, the body, is a finite \nsequence of literals. A literal is either an atom or an equation be\u00adtween terms. An atom has the form \np(~l, . . . . Zn) where p is a predicate symbol and the z, are distinct variables. An equality constraint \nis essentially a conjunction of equations between terms. For technical convenience equality constraints \nare treated modulo logical equiv\u00adalence, and are assumed to be closed under existen\u00adtial quantification \nand conjunction. Thus equality con\u00adstraints are ordered by logical implication, that is 6 ~ &#38; iff \n6 % 0 . The least, inconsistent equality con\u00adstraint is denoted by false. We let 3 ~.9 denote the equality \nconstraint 3 VI 3 VJ . . . V~@ where variable set w ={ Vi,..., V~ }. We let = WO be constraint @ re\u00adstricted \nto the variables W. That is ~w6 is 3Var$(e)i ~ 8 where function wars takes a syntactic object and returns \nthe set of (free) variables occurring in it. Note that although we concentrate on equality constraints, \nthe analysis generalizes to handle other constraints, such as arithmetic or Boolean, in the more general \ncontext of constraint logic programs [22]. Var is the set of variables, Atom the set of atoms, Eqns the \nset of equality constraints, Lit the set of lit\u00aderals, and Prog the set of programs. A renaming is a \nbijective mapping from Var to Var. We let Ren be the set of renamings, and naturally extend renamings \nto mappings between atoms, clauses, and constraints. Syntactic objects s and s are said to be variants \nif there is a p E Ren such that p s = s . The definition of an atom A in program P with respect to variables \nW, defnp A W, is the set of variants of clauses in P such that each variant has A as a head and has variables \ndisjoint from W \\ (vars A). The operational semantics of a program is in terms of its derivations which \nare sequences of reductions between states where a state (G, 0, D) consists of the current literal sequence \nor goal G, the current equal\u00adity constraints 6, and the current sequence of delayed atoms D. Literals \nin the goals are processed left-to\u00adright. If the literal is an equality constraint, and it is consistent \nwith the current equality constraints, it is added to these. Delayed atoms woken by the addition are \nprocessed. If the literal is an atom, it is tested to see if it is delayed. If so it is placed in the \ndelayed atom se\u00adquence, otherwise it is replaced by the body of a clause in its definition. More formally, \n State = Lit* x Eqns x Atom*. A state (L : G, 19,D) can be reduced as follows: 1. If L ~ Eqns and 8 A \nL is satisfiable, it is reduced to (D :: G,O AL, D \\ D ) where D = (woken D 0A L). 2. If L c Atom there \nare two cases. If (delay L 6) holds, it is reduced to (G, O, L : D). Otherwise it is reduced to (B :: \nG,6, D) for some (L +--B) c (defnp L (vars S)).  Note that :: denotes concatenation of sequences. A \nderivation of state S for program P is a sequence of states SO -+ S1 ~ ... ~ Sn where So is S and there \nis a reduction from each S, to S,+l. The above definition makes use of two paramet\u00adric functions which \nare dependent on the systems or language being modeled. These are, deiay A 6, which holds iff a call \nto atom A delays with the equations 8, and woken D 19, which is the subsequence of atoms in the sequence \nof delayed calls D that are woken by equations @. Note that the order of the calls returned by woken \nis system dependent. We will assume that these functions satisfy the fol\u00adlowing four conditions. The \nfirst ensures that there is a congruence between the conditions for delaying a call and waking it: (1) \nA c (woken D 0) ~ A ED AY (delay A 6). The remaining conditions ensure that delay behaves reasonably. \nIt should not take variable names into ac\u00adcount: (2) Let pcRen. deiay A6+ delay (pA) (p8). It should \nonly be concerned with the effect of O on the variables in A: (3) delay A 6 + delay A S(U=T, ~J@. Finally, \nif an atom is not delayed, adding more con\u00adstraints should never cause it to delay: (4) If 6< # and delay \nA 0, then delay A 6 . Although these conditions can be relaxed, they sim\u00adplify the analysis presentation \nand are met in existing systems and languages. The declarative semantics of a program is in terms of \nits qualified answers . Consider a derivation from state S and program P with last state (G, O, D) where \nG = nil. It is successful if D = nil and it flounders otherwise. We say the tuple (6, D) is a quaiified \nan\u00adswer to S. It is understood as representing the logical implication 3 ~Va,, S)(D A 6) + S. For this \nreason we regard qualified answers (0, D) and (0 , D ) to S as equivalent if In particular qualified \nanswers @ and # are regarded as the same if there is a renaming p such that (p ~) = # and (p S) = S. \nAS there is a non-deterministic choice of the clause in an atom s definition, there may be a number of \nqualified answers generated from the initial state. We denote the set of qualified answers for a state \nS and program P by qansp S. In the case of no calls delaying, this semantics is the same as the usual \noperational semantics of Prolog with left-to-right scheduling. As an example, consider the initial state \n(permute(X, Y), X = a : nil, niz) and the program from Section 2. These have the (single) successful \nderivation shown in Figure 1. 4 Abstract Interpretation In abstract interpretation [7] an analysis is \nformalized as a non-standard interpretation of the data types and functions over those types. Correctness \nof the analy\u00adsis with respect to the standard interpretation is ar\u00adgued by providing an approximation \nrelation which holds whenever an element in a non-standard domain describes an element in the corresponding \nstandard do\u00admain. We define the approximation relation in terms (permute(X, Y), @I, nil) where @I is \nX = a : nil ((X = U : Xl) : delete(U, Y,%) : pemnute(Xl, Z), @I, rrd) (delete(U, Y,Z) : perAute(Xl, Z), \nO,, 7+ where 6zis OlAX=U:Xl (pemnz4te(Xl,Z), $, o!elete(U, Y, Z)) (Xl = nil : Z = nil, 02, delete( U, \nY, Z)) (Z= nil, 6s, {elete(U, Y, Z)) where 03 is 02 A X 1 = nil (delete( U, Y, Z), @4, nil) where 04 \nis &#38; A Z = nil (Y=d, 04, nil) (nil, $, nil) where 05is04AY=U:Z. Figure 1: Example Derivation of \nan abstraction function which maps an element in q(Y) : nil. More formally, the standard domain to its \nbest or most precise de- Ann = {clef, pos}scription. AnnAtom = Atom x AnnA description (22, a, &#38;) \nconsists of a description AnnMSet = p AnnAtom domain D which must be a complete lattice, a data domain \n&#38;, and an abstraction function CY: &#38; -+ D. Let D* E AnnMSet. Define dej D* to be the multiset \nWe say that d a-approximates e, written d ma e, iff of atoms in D* that are annotated with def and all \nD* (a e) < d. The approximation relation is lifted to func\u00adto be the multiset of all atoms in D*, that \nis the atoms tions, Cartesian-products and sets as follows. annotated with either pos or def. The atom \nsequence description is Let (Dl, al, &#38;l) and (92, a2, :2) be descriptions, and F :VI + D2 and F :&#38;l \n-+ 82 be functions. (AnnMSet, a~mm~.e~, Atom.) Then F~F iff Vdg Dl. Ve E&#38;l. dm.le+ (F d) ma, (F \ne). where ffA..&#38;fs~t is defined by Let (2)1, CYl, $1) and (92, aa, &#38;z) be descriptions, ~AnnMS.~ \nD = {(A, clef) \\ A in D} and (all, da) : D1 x 2% and (cl, ea) : El x &#38;a. Then (dl, d2) w (cl, e2) \niff dl ma, el A d2 ma, e2. and AnnMSet is ordered by D; ~ D; iff Let (D, a,&#38;) be a description and \nD C D and&#38; C all D; ~ all D2 A def D; ~ def D;. .5. Then D cx8 iff VeE&#38; .3d ClY . dcxae. It \nfollows that the annotated multiset D* approximates When clear from the context we say that d approxi-the \nsequence D iff all atoms in D are in all D and mates e and write d cx e and let D denote both the every \natom in def D* is in D. description and the description domain. We will also be interested in describing \nequality In the analysis we will need to describe sequences of constraints. The analysis given in the \nnext section is delayed atoms and sequences of woken atoms. Because generic in the choice of description. \nWe require that the of the inherent imprecision in analyzing programs with a description chosen, (AEqns, \naAEqmS, Eqns) say, satisfies dynamic computation rule, we cannot always be definite ~AEqns @ = ~AEqns \n* @ = false that a particular atom is in the sequence, but may only know that it is possibly in the sequence. \nFurther, it is where J_ AEqn$ is the least element in AEqns. difficult to keep track of all possible \nsequence orderings. One example of an equality constraint description Hence, we will describe atom sequences \nby a multiset of is the standard equality constraint description, annotated atoms in which each atom \nis annotated with def if it definitely appears in the sequence and pos if (EqnsT, At?. 9, Eqns) it possibly \nappears in the sequence. For example, the multiset {(p(X), pos), (q( Y), de~)} describes only the in \nwhich constraints describe themselves. More pre\u00adsequences p(X) : q(Y) : nil, q(Y) : p(X) : nil and cisely, \nthe description domain is EqnsT which is the set of constraint equalities with a new top element T. EqnsT \nis a complete lattice ordered by 6<6 @6= fake or0=9 or#=T. The abstraction function is the identity function, \nas the best description of a constraint is just itself.  Denotational Semantics In this section we \ngive a generic denotational semantics for programs with dynamic scheduling. Correctness of the denotational \nsemantics depends on the following re\u00adsults about the operational semantics. The first propo\u00adsition means \nthat we can find the qualified answers of a state in terms of its constituent atoms, the second means \nthat we can consider states modulo variable re\u00adnaming, the third that we can restrict the equality con\u00adstraint \nto the variables in the delayed atoms and the goal. Proposition 5.1 Let P c Prog, A E Atom and (A :G,0,D) \n~ State. Then gansp (A :G,6,D) is D ) / (8 , D ) E qansP(A, 6, D)}. E U{qans~(G, $ , Proposition 5.2 \nLet P E Prog, p c Ren, and S E State. Q c qansp S @(p Q) 6 qansp (p S). Proposition 5.3 Let P c Prog \nand (G, 6, D) c State. qansp (G, 6, D) = u{(e A 0, D ) / (6 , D ) c qansp S}. where S is (G, ~fva,$ G) \nD)et ). U(VaTS Taken together these propositions mean that we can find the qualified answers to a state \nas long as we know the qualified answers to the canonical calls en\u00adcountered when processing the state \nwhere a canonical call is a call that represents all of its variants and in which the constraint is restricted \nto the variables of the call atom and the delayed atoms. This is the basic idea behind the denotational \nsemantics as the denotation of a program is simply a mapping from calls to answers. The last proposition \nmeans that the meaning of a goal is independent of the order that the atoms are scheduled. Thus we can \nignore the sequencing infor\u00admation associated with delayed atoms and treat them as multisets. It is variant \nof Theorem 4 in Yelick and Zachary [34]. Proposition 5.4 Let P be a program and (G, 9, D) be a state. \nIf G is a rearrangement of G then, qansp (G,6, D) = qansp (G ,6, D). E In the denotational semantics \natoms, bodies, clauses and programs are formalized as environment transformers where an environment consists \nof the cur\u00adrent equality constraint description and an annotated multiset of delayed atoms. In a sense \nan environment is the current answer . Thus an environment has type Env = AEqns x AnnMSet and the denotation \nof a program has type Den = Atom -+ Env --+ p Env as it maps a call to its set of answers. The complete \ndenotational definition is shown in Figure 2. The semantics is generic as it is parametric in AEqns the \nequality constraint descriptions and various parametric functions. The semantic functions associ\u00adated \nwith programs P, clause bodies B, and literals L, need little explanation. The only point to note is \nthat the variable set W is passed around so as to ensure that there are no variable (re)naming conflicts. \nThe function A gives the meaning of an atom for the current denotation. Consider the call A [.4] W d \n(n, D*). There are three cases to consider: the first is when A is delayed for all equality constraints \napproximated by r, the second is when A is not delayed for any equality constraints approximated by r, \nand the third is when A is delayed for some equality constraints approximated by T, but not all. A is \ndefined in terms of the parametric functions Awake and A delay. The call Awake A ~ returns a description \nof those equality con\u00adstraints which are described by T and for which A will not delay. Conversely, Adelay \nA m, returns a descrip\u00adtion of those equality constraints which are described by T and for which A will \ndelay. More exactly, Awake and Adelay should satisfy: {(Awake A m)} R {0 Im m @A = (delay A 0)} {( AdeLzy \nA ~)} M {0 IT cx 6 A (delay A 6)}. Note that Awake A m = ~A@n$ implies mm 8a (delay A 8), and Adeiay \nA m = lAEgmS implies rw 6s 1 (deiay A 0). The auxiliary function lookup is used to find the denotation \nof an atom which possibly does not delay. The call, tookup A W d (n, D* ), returns the denotation according \nto d of A with environment (n, D*). However there are complications because d only handles canon\u00adical \ncalls . Hence lookup must (1) restrict T to the vari\u00adables in the call; (2) rename the variables introduced \nin the delayed atoms in the answers so that they do not interfere with the variables in W; and (3), combine \nthe The denotational semantics has semantic functions: : Prog + Den ; : Prog+ Den+ Den : Lit* +(p Var)+Den \n+Env+(p Env) L : Lit + (pVar) + Den + Env + (pEnv) A : Atom + (p Var) + Den -i Env + (p .Env) E : \nEqns + (p Var) + Den -+ Env + (p Env).  It has auxiliary functions: lookup : Atom -+ (go Var) + Den \n+ Env -+ (p Env) wmset : AnnMS et + (p Var) + Den + Env + (p Env) watom : AnnAtom + (p Var) + Den+ Env \n+ (p Env) and parametric functions: Awake : Atom + AEqns + AEqns Adelay : Atom -i AEqns + AEqns Acomb \n: AEqns -+ AEqns -i AEqns Arestvict : p Vars + AEqns + AEqns Aadd : Eqns + AEqns + AEqns Awoken : AnnMSet \n+ Eqns 4 AEqns + AnnMSet Adelayed : AnnilfSet 4 Eqns + AEqns + AnnM$et. The semantic and auxiliary functions \nare defined by: Q[P]d Ae = let V = vars ein  P[P]Ae A [A] (vars e) (ijp (Q [P])) e U{(B [B] VU(uars \nA+ B) de)I(A + B) Edefnp A V} B [nil] W de = {e} B[L:B]Wde = U{(B [B]Wde )Ie 6 (L [L] W d e)} L [L] = \nif L E Atom then (A [L]) else (E [L]) A [A] W d (T, D*) = if (Awake A T) = ~A~~n, then {(T, (A, clef) \nU D*)} else if (Adelay A,T) = ~A~~n$ then (lookup A W d (T, D ) else {( Adelay(A, ~), (A, pos) U D*)}u \n(lookup A W d ((Awake A T), D*)) E[8] W d(x, D*) = let T = Aadd 6Tin if 7r = lAEqn$ then 0 else (wmset \n(Awoken D* 8 m) W d (m , (Adelayed D* O T))) lookup A W d (T, D*) = let V = (wars A) U(.ars D*) in let \nE = d A ((Restrict V T), D*) in {(( Acomb T T ), D ) \\ (~ , D* ) c (rename E V W)} wmset @W de = {e} \nwmset (A* UD*) W de wmset D W de ) Ie E(watom A* W de)} = U{( watom (A, clef) W d (T, D*) = lookup A \nW d (T, D*) watom (A, pos) W d (m, D ) = {(x, D*)] U (2..7-cuP A W d (( AwaLe A T), D*)) Figure 2: Denotational \nSemantics equality constraint description with that of the original call so as to undo the result of \nthe restriction. Lookup is defined in terms of the parametric functions Acomb and A restrict. A comb \ncombines two equality constraint descriptions and should approximate the function add, defined by add \nOO =6A#. Arestrict restricts an equality constraint description to a set of variables and should approximate \nthe function restrict defined by TestTict w e= i ~e, The definition also makes use of the function call \nrename E V W which returns a variant of the envi\u00adronments E which is disjoint from the variables W but \nwhich leaves the variables in V unchanged. More ex\u00adactly it returns p E where p is a renaming such that \nfor allvcv, pv=vandvars(p E)n(W \\V)=O. Equations are handled by the semantic function E. The function \ncall, E [6] W d (T, D* ), first adds the equality constraint 6 to x and tests for satisfia\u00adbility. If \nthis succeeds, it then wakes up the atoms in D* , and processes these. The definition is para\u00admetric \nin the functions Aadd, Awoken and Adelayed. The function Aadd adds an equality constraint to an equality \nconstraint description and must approximate the function add defined previously. Awoken returns the multiset \nof atoms that will be possibly and def\u00adinitely woken by adding an equality constraint to an equality \nconstraint description and Adelayed returns the multiset of atoms that will possibly and definitely remain \ndelayed. Awoken must approximate diflwoken and A delayed must approximate diffdelay where these are defined \nby diffwoken D 69 = woken (D \\ (woken D 6 )) 0 di~delay D 00 = D \\ (woken D (8A 6 )). Note that Adelayed \nmay change the annotation of a delayed atom from def to pos and that Awoken returns a multiset of woken \natoms which are also annotated. The woken atoms are handled by the auxiliary functions wmset and watom \nalmost exactly as if they were a clause body, the only difference is to handle the pos annotated atoms. \nThe standard denotational semantics, Pstd, is ob\u00adtained by from the denotational semantics by instantiat\u00ading \nAEqns to the standard equality constraint descrip\u00adtions and instantiating the parametric functions to \nthe function they are required to approximate, for instance Aadd and Acomb are both instantiated to add. \nUsing the four propositions given at the start of this section: it is possible to show that the denotational \nsemantics is correct: Theorem 5.5 Let D G Atom+, 6 c Eqns, A c Atom, and P c Prog. Then Pstd [~] A (8, \nD*) = qansp (A : nz/, 6, D) where D* = ~AnmMset D. Using results from abstract interpretation theory \nit follows that analyses based on the semantics are correct: Theorem 5.6 Let e E Env, A G Atom, P E Prog. \nIf e cx (6, D), (P [P] A e) IX qansp (A: niz, 0, D). Actually the denotational semantics does not ex\u00adactly \ngive the information a compiler requires for the generation of eflicient code. This is because we are \npri\u00admarily interested in removing unnecessary tests for de\u00adlaying and improving the code for unification. \nThere\u00adfore, we must obtain information about the call pat\u00adterns. That is, for each atom A appearing in \nthe pro\u00adgram we want to know whether the calls to the atom initially delay, and when each call to A is \neventually reduced, perhaps after being delayed, the value of the current equation restricted to the \nvariables in A. It is straightforward to modify the denotational semantics to collect this information \nfor atoms which are not de\u00adlayed. For the case of atoms which are delayed it is more difficult as although \ntreating the delayed atoms as a multiset does not affect the qualified answers, if more than one atom \nis woken it may affect the calls made in the evaluation. Because of space limitations we will ignore \nthis extra complication but note that it has been done in the analyzer used to obtain the results presented \nin Section 9.  6 Implementation The denotational equations given in the previous sec\u00adtion can be considered \nas a definition of a class of pro\u00adgram analyses. Read naively, the equations specify a highly redundant \nway of computing certain mathemat\u00adical objects. On the other hand, the denotational def\u00adinitions can \nbe given a call-by-need reading which guarantees that the same partial result is not repeat\u00adedly recomputed \nand only computed if it is needed for the final result. Wit h such a call-by-need reading the definition \nof P is, modulo syntactic rewriting, a working implementation of a generic dataflow analyzer written \nin a functional programming language. In programming languages which do not support a call-by-need semantics, \nimplementation is somewhat harder. To avoid redundant computations, the result of invoking atom A in \nthe context of environment e should be recorded. Such memoing can be implemented using function graphs. \nThe function graph for a function f is the set of pairs {(e ~ f(e)) I e c dom f} where dom f denotes \nthe domain for f. Computation of a function graph is done in a demand-driven fashion so that we only \ncompute as much of it as is necessary in order to answer a given query. This corresponds to the minimal \nfunction graph semantics used by Jones and Mycroft [23]. However, matters are complicated by the fact \nthat we are performing a fixpoint computation and we must iteratively compute the result by means of \nthe function s Kleene sequence. This idea leads to a generic algorithm for the memorization based analysis \nof programs with dynamic scheduling. The algorithm extends memorization based analysis for traditional \nProlog. The analysis starts from a call and incrementally builds a memorization ta\u00adble. This contains \ntuples of calls and their answers which are encountered in derivations from the initial call. Calls are \ntuples of the form (A, m, D*) where A is an atom, D* is a multiset of annotated atoms describ\u00ading the \nsequence of delayed atoms and T is an equality constraint description restricted to the variables in \nA and D*. An answer to a call (A, ~, D*) is of the form (m , D* ) where D * is a multiset of annotated \natoms describing the sequence of delayed atoms and # is an equality constraint description restricted \nto the vari\u00adables in A and D* . Our actual implementation has two improvements which reduce the size \nof the memo\u00adrization table. The first improvement, is when adding an an\u00adswer to the answers of call, \nto remove redundant answers and merge similar answers together. Answers (ml, D~) and (Tz, D~) are merged \ninto the single answer (ml u ~z, D~) whenever D; < D:. The second improvement is to only consider calls \nmodulo variable renaming. Entries in the memorization table are canonical and really represent equivalence \nclasses of calls and answers. Another possible improvement which has not been implemented yet is based \non the observation that de\u00adlayed atoms which are independent of the calling atom can never be woken when \nthe call is executed. Such atoms need not be considered in the call as they will occur in each answer. \nThe exact definition of inde\u00adpendence is somewhat difficult as it really means inde\u00adpendence from any \ndelayed atom which could be woken in the call.  Termination Correctness of the denotational semantics, \nTheorem 5.6, is not quite enough as it only guarantees partial correct\u00adness of an analysis, and, of course, \nwe would also like the analysis to terminate. Given that all calls to the parametric functions terminate, \nthe analysis will termi\u00adnate iff there are a finite number of calls in the mem\u00adorization table and each \ncall has a finite number of an\u00adswers. This is true if the following two requirements are met. The first \nis that for each finite set of variables W there are only a finite number of descriptions which de\u00adscribe \nsome equality constraints ~ ~6. This is the usual requirement for the termination of memorization based \nanalysis of standard Prolog. The second requirement is that there is a bound on the size of the annotated \nmulti\u00adsets in both the calls and the answers. In this section we sketch two modifications to the analysis \nwhich ensure that only multisets of a bounded size need be consid\u00adered, albeit at some loss of accuracy. \nIn some sense, this is a form of widening [8], however correctness does not depend on the semantics of \nthe description domain but rather on properties of the program semantics. The first modification allows \nus to only consider calls with annotated multisets of a bounded size. Cor\u00ad rectness depends on the following \nproperty of the oper\u00ad ational semantics: Proposition 7.1 Let P c Prog and (G, 6, D) c State. If D = D \n U D , qansp (G,6, D) = qansp (G :: D ,6, D ). 9 This means in the analysis that lookup can be modified \nto (1) remove annotated atoms D* from the multiset of delayed atoms, if it is to large, (2) proceed as \nbefore, and then (3) process D* using a variant of B which handles annotated atoms. The second modification \nallows us to only consider answers with annotated multisets of a bounded size. Now a delayed atom A can, \nif it is woken, only add constraints affecting variables in A and variables which are local to its subcomputation. \nThus in the analysis, when we encounter an answer (T, D*) in which the mul\u00adtiset D* is too large, we \ncan replace it by the answer (#, {(T, pos)}) where {m } approximates and (T, pos) is a special annotated \natom which signi\u00adfies that there are possibly delayed atoms of indetermi\u00adnate name. Note that (T, pos) \ncan never be woken. With these two modifications the analysis will ter\u00adminate whenever the usual termination \nrequirements for memorization based analysis of standard Prolog are met. We can also use the idea behind \nthe second modi\u00adfication to analyse modules. The problem is that when analyzing a module in isolation \nfrom the context in which it will be used we have no idea of the delayed atoms associated with calls \nto the module. However, the delayed atoms can only affect variables in the ini\u00adtial call. Thus by taking \nthe downward closure of the initial call, we are assured to obtain correct informa\u00adtion about the calling \npatterns regardless of the atoms delayed in the actual call. Another approach to ensure termination would \nbe to approximate the delayed multiset of atoms by a star Table after 1st Iteration: (Perm~te(X, Y), \n({ Y}, {X}, O),O) H 0 Table after 2nd Iteration: (permute(X, Y), ({ Y}, {X}, kl), 0) * {(({x, Y}, o,o), \no)} (delete(U, Y, Z), ({ Y], {U, Z}, O),O) k+ O Table after 3rd and Final Iteration: (permute(X, Y), \n({ Y}, {X}, O), O) ~ {(({x, y}, O, O), O)} (delete(U, Y, Z), ({ Y}, {U, Z}, O), O) H {(({IY, Y, Z}, O,O),O)} \nAnalysis of (permute(X, Y), ({ Y}, {X}, 0), rid). Table after 1st Iteration: (permute(X, Y), ({ X}, \n{y}, O),O) ~ 0 Table aft er 2nd Iteration: (permute(X, Y), ({X}, { Y}, O), O) ~(({xj Y}, O,O),O)} : \n(delete(U, Y, Z), ({U, Z}, { Y}, 0),0) Table after 3rd and Final Iteration: (permute(X, Y), ({X}, {Y}, \nO), O) ~ {(({x, y}, O,O),O)} (delete(U, Y, Z), ({ IY, Z}, {Y}, O), O) + {(({U, Y, Z}, O,O),O)} Analysis \nof (pe~mute(X, Y), ({X}, { Y}, 0), nil). Figure 3: Example Analyses abstraction [4] in which variants \nof the same atom are A more complete description of this description domain .. collapsed on to a single \ncanonical atom. and abstrac~ operations over it can be found in [28]. Of course for accuracy more complex \ndomains could be used in the analysis.  Example Analysis The first state description to be analyzed \nis We now present an example of the analysis algorithm s use. In our example analysis we use simple modes \n(permute(X, Y), ({Y}, {X}, 0), d). to describe the equality constraints. We will use this mode descriptions \nto analyze the state corresponding to query Q 1 from Section 2. The domain used is similar to Figure \n3 shows the memorization table at each iteration that of [28] and has been used for analyzing traditional \nin the analysis for this state description. The result of Prolog. A mode description for equality constraint \n6 the analysis is {(({X, Y}, 0, 0), 0)}. That is, if calls to has the form permute have their second \nargument ground and first argument free, the answers will ground the first argu\u00ad ment. As no calls were \ndelayed in this example, the where Vgnd is the set of variables that 8 definitely ( Vgnd , Vjree, Wdep) \nanalysis was virtually the same as given by a traditional grounds, V~,ee is the set of variables that \nt9 leaves defi\u00ad left-to-right mode analysis of the program. If the anal\u00adnitely free , that is not instantiated \nto a non-variable ysis is extended to give information about call patterns term, and Wdep the set of \nsets of variables which 6 it shows, as promised in Section 2, that for calls to makes possibly dependent. \nFor example, the equality permute in which the second argument is ground, and constraint the first free, \nno atom ever delays. Further, it shows X= CZAY=ZAW=f(V) that in all calls to permute the first argument \nwill be free and the second argument will be ground, and in all is (most precisely) described by calls \nto delete the first and third arguments will be free, ({x}, {Y, z}, {{Y, z}, {w, v}}). and the second \nwill be ground. Now consider the state description (permute(X, Y), ({X}, {Y}, O),nil). Figure 3 shows \nthe memorization table at each iteration in the analysis for this state description. The result of the \nanalysis is {(({X, Y}, 0, 0), 0)}. That is, if calls to permute have their first argument ground and \nsec\u00adond argument free, the answers will ground the second argument. Termination is achieved by restricting \ncalls in the memorization table so that they have an empty annotated multiset. Thus when the call (permute(Xl, \nZ), ({Xl, U}, {Y, Z}, 0), {(deMe(U, Y, Z), clef)}) is encountered when processing the second clause \nof permute, first the call (w mute(xlj z), ({xl}, {z}, 0),0), is looked up in the table and then, as \nthis grounds Z, the call (de/ete(U, Y, Z), ({U, Z}, {Y}, O), O), is looked up. If the analysis is extended \nto give infor\u00admation about call patterns it gives the results promised in Section 2.  Performance Results \n We conclude with an empirical evaluation of the accu\u00adracy and usefulness of an implementation in Prolog \nof the analyzer presented. Our first results show that in\u00adformation from the analysis can be used to \neliminate redundant delay declarations, leading to a large perfor\u00admance improvement. The last test illustrates \nhow the analysis can be used to guide optimizations which are performed for traditional Prolog. In this \ncase we show how implicit independent and-parallelism as detected by the analyzer can be used to parallelize \nthe bench\u00admark. The benchmarks used for the evaluation were: perzrut e, the permute program presented \nin Section 2; qsort, the classical quick sort program using append; apps which concatenates three lists \nby performing two consecutive calls to append; nrev which naively reverses a list; and neg, an implementation \nof safe negation us\u00ading suspension on the groundless of the negated goal (a simple test of membership \nin a list). All benchmarks have been implemented in a reversible way, so that they can be used forwards \nand backwards, through the use of suspension declarations. In the first test, the optimizations to eliminate \nun\u00adnecessary delaying were performed in two steps. The first step was to eliminate and/or relax suspension \ndec\u00adlarations as indicated by the analysis. The second step was to reorder the clause bodies provided \nthe analysis indicated that it reduced suspension. It is important to note that although the obtained \norderings are already implicit in the results of the (first) analysis, in order to eliminate suspension \nconditions that are redundant after the reordering, a second pass of the analysis is sometimes needed. \nThe tests where performed with Sic\u00adstus Prolog 2.1, which is an efficient implementation of Prolog with \na rich set of suspension primitives. Due to lack of space we cannot include the code for the benchmarks \nand their resulting specialized ver\u00adsions. However, in order to give an idea of the accuracy of the analyzer \nand to help in understanding the effi\u00adciency results, we point out that in all cases but for permute \nthe information provided by the analyzer was optimal. In the case of permute one condition can be relaxed \nbeyond those inferred by the analyzer. In par\u00adticular, for all the examples in their forward execu\u00adtion \nmode the analyzer accurately infers that no goal suspends and therefore all suspension declarations can \nbe eliminated. With respect to the backwards execu\u00adtion, in all cases but neg the suspension conditions \nare either relaxed or eliminated. This does not occur for neg since the analyzer accurately infers that \nthe exist\u00ading groundless suspension condition is still needed for correctness. Finally, with respect \nto the optimizations where reordering is allowed, all backward executions are reordered in such a way \nthat no suspension conditions are needed. Thus, we can conclude that the accuracy results for the analyzer \nare encouraging. Table 1 lists execution times, expressed in seconds, for the original benchmarks and \nthe optimized versions. Each column has the following meaning: Name pro\u00adgram name, Query number of \nelements in the list given as query, P execution time for the program written in standard Prolog, i.e. \nwith no suspension dec\u00adlarations, S execution time for the program written with suspension declarations, \nSO execution time for program written with suspension declarations and opti\u00admized by removing suspension \ndeclarations as dictated by the analysis information, S/SO -ratio between the last two columns, R execution \ntime for the program optimized by reordering the clause bodies as dictated by the analysis information, \nand R/S ratio between R and S columns. In the P column In stands for non\u00adtermination, and Er stands \nfor a wrong result or an execution error (the fact that these cases appear shows the superiority of the \nversion of the program with sus\u00adpension declarations). Two sets of data (corresponding to two lines in \nthe table) are given for each program, the first one corresponding to forwards execution of the program, \nthe second to the backwards execution. Note that in some cases the number of elements given as queries \nfor forward execution are different from those used for the backward execution of the same pro\u00ad Name \nQuery P s so s/so R S/R - permute 8 In 27.2 24.0 1.1 0.7 38.9 8 2.0 20.6 2.0 10.3 2.0 10.3 app3 20000 \n0.2 4.7 0.2 23.5 0.2 1.5 1000 In 12.2 1.6 7.6 1.4 8.7 qsort 2000 0.8 74.3 0.8 92.9 0.8 92.9 7 Er 20.8 \n4.7 4.4 0.7 29.7 nrev 300 0.2 21.4 0.2 107.0 0.2 107.0 300 In 28.4 3.1 9.2 0.5 56.8 neg 400000 2.4 3.5 \n2.4 1.5 2.4 1.5 400000 Er 3.5 3.5 1.0 2.4 1.5 Table 1: Analysis and optimization with delay Reversible \nquick-sort, 5000 < Standard Prolog ( Suspension declarations, after .Above program, parallelized, r , \nprogram, Above parallelized, Above program, parallelized, Above program, parallelized, 1  gram. The \nreason is the amount of time required by each query due to the different behaviour when run\u00adning forwards \n(one solution) and backwards (multiple solutions). The results are rather appealing as they show that \nthe optimizations based on relaxing and eliminating sus\u00adpension declarations using the information provided \nby the analyzer allows use of the more general version of the program written with suspension declarations \nwith\u00adout a performance penalty when executing the program in the mode that runs in Prolog. Furthermore, \nysis and resultant optimization also improves speed even if some suspensions still need during execution. \nThe optimizations based ing give even more impressive results. This explained by the fact mentioned above \nthat the anal\u00ad execution to be used on reorder\u00ad is mainly for all pro\u00ad grams the reordering has achieved \nthe elimination of all suspension declarations. Finally, in the last test, we show how information from \nthe analysis can be used to perform optimization used in the compilation of traditional Prolog. As an \nex\u00adample we consider automatic parallelization based on the independent and parallelism model. The only \npro\u00adgram in which this kind of parallelism exists for the given queries is qsort. In this case the parallelism \ncan be automatically exploited using the information obtained from the cause the analysis determines \nthat pension in the reordered program techniques described in [20, 27] existing tools given analysis. \nThis is be\u00ad there is no goal sus\u00adand so the tools and are applicable. These elements I Time 1.23 analysis \nand reordering 1.23 1 processor 1.30 2 processors 0.81 4 processors 0.53 6 processors 0.46 techniques \ncan also be extended to deal with cases in which goals are delayed by extending the notion of de\u00adpendence, \nbut that is beyond the scope of this paper. A significant reduction in computation time is obtained from \nparallelism at least for the forward query. This is illustrated in Table 2, which shows results from \nrunning the forward query with the optimized program under &#38;- Prolog [19], a parallel version of \nSicstus on a commercial multiprocessor. Times 10 Conclusion We have given a framework for global ysis \nof logic languages with dynamic framework extends memorization based ditional logic programming languages \nto-right scheduling, Information on the framework can be used which remove the overhead of also to perform \noptimization of traditional Prolog. A potential application the analysis of constraint logic from Prolog, \nrunning are in seconds. dataflow anal\u00ad scheduling. The analyses for tra\u00ad with a fixed left\u00adanalyses \nbased to perform optimization dynamic scheduling and used in the compilation of the framework is for \nprogramming languages which handle difficult constraints by delaying them un\u00adtil they become simpler. \nInformation from an analysis based on our framework could be used constraints for difficulty constraints \nto points simpler, thus avoiding ically for this purpose by Hanus [18]. at run-time, or in the program \nin suspensions. An has also recently to avoid testing to move difficult which they are analysis specif\u00ad \nbeen suggested  Acknowledgements We thank Lee Naish who suggested several of the bench\u00admarks. We also \nthank Peter Breuer for his comments on an earlier version of the paper.  References p] M. Carlsson. \nFreeze, Indexing, and Other Implementa\u00adtion Issues in the Warn. In .Fourth .lnternational Con\u00adference \non Logic Programming, pages 40-58. University of Melbourne, MIT Press, May 1987. [2] M. Carlsson. Sicstu.s \nProlog UseT s Manual. Po Box 1263, S-16313 Spanga, Sweden, February 1988. [3] M. Codish. A Provably Correct \nAlgorithm for Sharing and Freeness Inference. In 199.2 Workshop on Static Analysis WSA 92, September \n1992. [4] M. Codlsh, M. Falaschl, and K. Marriott. Suspension Analysis for Concurrent Logic Programs. \nIn K. Fu\u00adrukawa, editor, Proc. Eighth Int 1 Conf. on Logic Pro\u00adgramming, pages 331 345. The MIT Press, \nCambridge, Mass., 1991. [5] M. Codkh, M. Falaschi, K. Marriott and W. Winsbor\u00adough. Efficient analysis \nof concurrent constraint logic programs. Proc. of Twentieth Int. Coil. Automata, Languages and Programming, \nA. Lingus and R. Karls\u00adson and S. Carlsson (Ed.), LNCS Springer Verlag, pages 633-644. [6] C. Codognet, \nP. Codognet, and M. Corsini. Abstract Interpretation for Concurrent Logic Languages. In S. Debray and \nM. Hermenegildo, editors, Proc. North American Conf. on Logic Programming 90, pages 215\u00ad 232. The MIT \nPress, Cambridge, Mass., 1990. [7] P. Cousot and R. Cousot. Abstract Interpretation: a Unified Lattice \nModel for Static Analysis of Programs by Construction or Approximation of Fixpoints. Proc. of the Fourth \nACM Symposium on Principles of Pro\u00adgramming Languages, 238-252, 1977. [8] P. Cousot and R. Cousot. Comparing \nthe Galois Con\u00adnection and Widening/Narrowing Approaches to Ab\u00adstract Interpretation. Technical report, \nLIX, Ecole Polytechnique, France, 1991. [9] P. Cousot and R. Cousot. Abstract Interpretation and Application \nto Logic Programs. Journal oj Logic Pro\u00adgramming, 13(2 and 3):103-179, July 1992. [10] S. DebraY. Static \nAnalysis of Parallel Logic Programs. In Fifth Int 1 Conference and Symposium on Logic Programming, Seattle, \nWasinghton, August 1988. MIT Press. [111 S. K. Debray. Static Inference of Modes and Data De\u00adpendencies \nin Logic Programs. ACM Transactions on Programming Languages and Systems 11 (3), 418-450, 1989. [12] \nS.K. Debray. QD-Janus: A Sequential Implementation of Janus in Prolog. Technical Report, University of \nAri\u00adzona, 1993. 252 [13] S. K. Debray and D. S. Warren. Functional Computa\u00adtions in Logic Programs. \nACM Transactions on Pro\u00adgramming Languages and Systems 11 (3), 451 481, 1989. [14] M. Falaschi, M. Gabbrielli, \nK, Marriott and C. Palamidessi. Compositional analysis for concurrent constraint programming. IEEE Symposium \non Logic in Computer Science, Montreal, June 1993. [15] M. Garcia de la Banda and M. Hermenegildo. A \nPrac\u00adtical Application of Sharing and Freeness Inference. In 1992 Workshop on Static Analysis WSA 92, \npages 118\u00ad125, Bourdeaux, France, September 1992. [16] D. Gudeman, K. De Bosschere and S.K. Debray. j \nc: An Efficient and Portable Sequential Implementation of Janus. In Proc. of 1992 Joint International \nConfer\u00adence and Symposium on Logic Programming, 399 413. MIT Press, November 1992. [17] M. Hanus. On \nthe Completeness of Residuation. In Proc. off 992 Joint International Conference and Sym\u00adposium on Logic \nProgramming, 192 206. MIT Press, November 1992. [18] M. Hanus. Analysis of Nonlinear Constraints in CLP(R). \nIn Proc. of 1993 International Conference on Logic Programming, 83-99. MIT Press, June 1993. [19] M. \nHermenegildo and K. Greene. &#38;-Prolog and its Per\u00adformance: Exploiting Independent And-Parallelism. \nIn 1990 International Conference on Logic Programming, pages 253-268. MIT Press, June 1990. [20] M. Hermenegildo, \nR. Warren, and S. Debray. Global Flow Analysis as a Practical Compilation Tool. Jour\u00adnal of Logic Programming, \n3(4):349 367, August 1992. [21] T. Hickey and S. Mudambi. Global Compilation of Pro\u00adlog. Journal of Logic \nProgramming, 7, 193-230, 1989. [22] J. Jaffar and J.-L. Lassez. Constraint Logic Program\u00adming. In Proc. \nFourteenth Ann. ACM Symp. Principles of Programming Languages, pages 11 1 1 19, 1987. [23] N. D. Jones \nand A. Mycroft. Dataflow analysis of ap\u00adplicative programs using minimal function graphs. In Proc. Thirteenth \nAnn. ACM Symp. Principles of Pro\u00adgramming Languages, pages 296 306. St. Petersburg, Florida, 1986. [24] \nK. Marriott, H. S@ndergaard, and P. Dart. A charac\u00adterization of non-floundering logic programs. In S. \nK. Debray and M. Hermenegildo, edhors, Logic Program\u00adming: Proc. North American Conf. 1990, pages 661\u00ad \n680. MIT Press, 1990. [25] K. Marriott, H. S@ndergaard and N. D, Jones. Deno\u00ad tational abstract interpretation \nof logic programs. To appear in ACM Trans. Programming Languages and Systems. [26] C. S. Mellish. The \nautomatic generation of mode declarations for Prolog programs. Technical Report 163, Dept. of Artificial \nIntelligence, University of Ed\u00adinburgh, Scotland, 1981. [27] K. Muthukumar and M. Hermenegildo. The CDG, \nUDG, and MEL Methods for Automatic Compile\u00adtime Parallelization of Logic Programs for Independent And-parallelism. \nIn 1990 International Conference on Logic Programming, pages 221-237. MIT Press, June 1990. [28] K. \nMuthukumar and M. Hermenegildo. Combined De\u00adtermination of Sharing and Freeness of Program Vari\u00adables \nThrough Abstract Interpretation. In 1991 in\u00adternational Conference on Logic Programming, pages 49-63 \n.MITPress, June 1991. [29] K. Muthukumar and M. Hermenegildo. Compile-time Derivation of Variable Dependency \nUsing Abstract In\u00adterpretation. Journal of Logic Progmmming, 13(2 and 3):315-347, July 1992. [30] L. \nNaish. Negation and Control in Prolog, LNCS 238, Springer-Verlag, 1985. [31] A. Taylor. LIPS on a MIPS: \nResults from a Prolog Compiler for a RISC. Proc. of the 7th International Conference on Logic Programming, \n174-185, 1990. [32] P. Van Roy and A.M. Despain. The Benefits of Global Dataflow Analysis for an Optimizing \nProlog Compiler. Proc. of the 1990 North American Conference on Logic Programming, 501-515, 1990. [33] \nR. Warren, M. Hermenegildo and S.K. Debray. On the Practicality of Global Flow Analysis of Logic Pro\u00adgrams. \nProc. of the 5th International Conference and Symposium on Logic Programming, 684-699, 1988. [34] K. \nYelick and J. Zachary. Moded type systems for logic programming. In Proc. Sixteenth Annual ACM Symp. \non Principles of Programming Languages, pages 116 124. ACM, 1989.  \n\t\t\t", "proc_id": "174675", "abstract": "<p>Traditional logic programming languages, such as Prolog, use a fixed left-to-right atom scheduling rule. Recent logic programming languages, however, usually provide more flexible scheduling in which computation generally proceed left-to-right but in which some calls are dynamically &#8220;delayed&#8221; until their arguments are sufficiently instantiated to allow the call to run efficiently. Such dynamic scheduling has a significant cost. We give a framework for the global analysis of logic programming languages with dynamic scheduling and show that program analysis based on this framework supports optimizations which remove much of the overhead of dynamic scheduling.</p>", "authors": [{"name": "Kim Marriott", "author_profile_id": "81320492504", "affiliation": "Dept. of Computer Science, Monash University, Clayton Vic 3168, Australia", "person_id": "PP14090039", "email_address": "", "orcid_id": ""}, {"name": "Mar&#237;a Jos&#233; Garc&#237;a de la Banda", "author_profile_id": "81100066534", "affiliation": "Facultad de Inform&#225;tica - UPM, 28660-Boadilla del Monte, Madrid, Spain", "person_id": "P187206", "email_address": "", "orcid_id": ""}, {"name": "Manuel Hermenegildo", "author_profile_id": "81100324241", "affiliation": "Facultad de Inform&#225;tica - UPM, 28660-Boadilla del Monte, Madrid, Spain", "person_id": "P187053", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/174675.177883", "year": "1994", "article_id": "177883", "conference": "POPL", "title": "Analyzing logic programs with dynamic scheduling", "url": "http://dl.acm.org/citation.cfm?id=177883"}