{"article_publication_date": "02-01-1994", "fulltext": "\n Deriving ~lgorithms from type inference systems: Application to strictness analysis Chris Hankin Department \nof Computing, ImperiaJ College, LONDON SW7 2BZ, UK Daniel Le M&#38; ayer INRIA/IRISA Campus de Beaulieu \n35042 RENNES CEDEX, FRANCE Abstract The r?de of non-standard type inference in static program analysis \nhas been much studied recently. Early work em\u00adphasised the efficiency of type inference algorithms and \npaid little attention to the correctness of the inference system. Recently more powerful inference systems \nhave been investi\u00adgated but the connection with efficient inference algorithms has been obscured. The \ncontribution of this paper is twofold: first we show how to transform a program logic into an al\u00ad gorithm \nand, second, we introduce the notion of lazy types and show how to derive an efficient algorithm for \nstrictness analysis. Introduction Two major formal frameworks have been proposed for static analysis \nof functional languages: abstract interpretation and type inference. A lot of work has been done to characterise \nformally the correctness and the power of abstract inter\u00adpretation. However the development of algorithms \nhas not kept pace with the theoretical developments. ThB is now a major barrier that is preventing the \ninclusion of the most ad\u00advanced techniques in compilers. The majority of the effort on improving the \nefficiency of abstract interpretation has concentrated on frontiers-based algorithms [1 3] or widening \ntechniques [6, 9]. The former still haa unacceptable perfor\u00admance for some commonly occurring higher-order \nprograms. The latter is a general approach for accelerating convergence in fixed point computations which, \nin the finite case, leads to some loss in accuracy. In contrast to abstract interpretation, type inference \nsys\u00adtems are routinely implemented as part of production qual\u00adity compilers. This has led some researchers \nto develop pro\u00ad gram analyses based on non-standard type inference. One of the earliest examples is Kuo \nand Mishra s strictness analysis [18]. A natural question arises concerning the relationship between \nthis approaeh and abstract interpretation. Kuo and l%m ssion to copy without fee all or pert of this \nmaterial is grantad provided that tha copies are not made or distributed for direct commercial advantaga, \nthe ACM copyright notice and the titla of the publication and its data appear, and notice is given thet \ncopying is by permission of the Association for Computing Machinery. To copy otharwise, or to republish, \nrequires a fse and/or specifio permission. POPL 94-1/S4, Portland Or@gon,USA @ 1994 ACfvf O-69791 +36-0t94~l \n..$3.50 Mishra s system is strictly weaker than the standard deno\u00adtational approaches but Jensen [14] \nhas shown how it can be extended to regain this equivalence. However, no type infer\u00adence algorithm has \nbeen proposed for this system so far. The logic is not immediately suggestive of an algorithm; this is \nmairdy because of the weakening rule which may be applied at arbitrary points in a derivation. The goal \nof this paper is to bridge the gap between these two contrasting approaches. The contribution of the \npaper is twofold: Methodological: we start from Jensen s logical sys\u00adtem and we derive equivalent systems \ncorresponding to the standard implementation of abstract interpreta\u00adtion and to the frontiers optimisation. \nThe derivation relies on restrictions of the type language. In partic\u00adular we show that frontiers are \nspecial forms of strict types. We believe that describing the various imple\u00admentation techniques in a \ncommon framework and ex\u00adpressing optirniaations as particular type restrictions sheds a new light on \nthe algorithmic aspects of static analysis. e Technical: we propose a refinement of the language of types, \ncalled lazy types and we derive a complete and sound inference system for this language. This algo\u00adrithm \nhas the same power w usual implementations of abstract interpretation but does not exhibit the same inefficiency \nproblems. 1.1 Overview We will use strictness analysis as a case stndy in this paper but the techniques \nare generally applicable [15, 19]. Ab\u00adstract interpretation represents the strictness property of a function \nby an abstract function defined on boolean dm mains [23]. For instance g~bs t f = f means that the result \nof a call to g is undefined if its second argument is unde\u00adfined. In terms of types, this property is \nrepresented by g : t ~ f ~ f. Notice that t and f are now (non-standard) types. Conjunctive types are \nrequired to retain the power of abstract interpretation: a strict function like + must have type (f~ \nt~ f)A (t+ f ~ f). Also an entailment relation is defined on types and the corresponding type inference \nsys\u00adtem includes a weakening rule. This is enough to make type 202 inference a non trivial taak (let \nus notice however that such a system does not suffer the undecidabtity problem of more powerful intersection \ntype systems [1, 2]: this is because we are working with the simply typed ,1-calculus). We first de\u00adfine \na notion of most general type which is equivalent to the conjunction of all the types of an expression. \nThe restriction to most general types allows us to get rid of the weakening rule and to derive an algorithm \nwhich corresponds to the naive implementation of abstract interpretation. The most general type can be \nseen as a representation of the tabula\u00adtion of the function. Then we proceed by showing that a further \nrestriction on most general types naturally leads to the frontiers optimisation. The basic idea behind \nfrontiers is to take advantage of monotonicity during the calculation of least fixed points. The restriction \non types amounts to representing a conjunction of types by its minimal elements. The fact that abstract \ninterpretation computes the most general type of an expression accounts for its accuracy but also for \nits inefficiency. We show that we can avoid some of this inefficiency without losing any of the power \nof abstract interpretation. The point is that abstract interpretation of\u00adten provides much more information \nthan really required. If g is a function of n arguments, the abstract version of g considers all possible \ncombinations of the abstract values of these n arguments: for instance g&#38; t f t f f = f means that \na call to g is undefined if its second, fourth and fifth arguments are undefined. In some cases this \nparticular piece of information will be useful to show that g is strict in one of its arguments but in \nmany caaes it will not be useful at all. The basic idea behind our algorithm is to compute the strictness \ntypes on demand rather than deriving systemati\u00adcally the most precise information as abstract interpretation \ndoes. The corresponding notion of lazy types is defined by allowing source expressions to occur inside \ntypes. Formally wch a lazy type is equivalent to the most general type of the expression, but it is in \nunevaluated form, very much like a closurv in lazy languages. We give a simple example to provide some \nintuition about lazy types. This example is traditionally interpretation used to illustrate the inefficiency \nof [13]. abstract foldrglb = ifl= then else nil b g (head 2) (foldr g (tail 1) b) cat 1 = foldr append \n1 nil The analysis of this function by abstract interpretation, us\u00ading a simple frontiers-bsaed approach, \nis intractable when the abstract domain for lists is the usual four point do\u00admain [25]. We consider only \nsimple strictness in this pa\u00adper but our argument applies to more complex domains as well (and with more \nalgorithmic significance) [15, 20]. As\u00adsume that we want to know if cat is strict. The abstract version \nof cat is defined in terms of the abstrsct version of f oldr. The abstract version of f oidr is a function \nin the domain (Bool -+ Bool + Boo/) -+ Bool * Bool + Bool and its representation is a table of sise 64. \nTwo iteration steps are required to find the least fixed point, so two func\u00adtions of this size are built. \nIn terms of types this means that 128 types are computed to find that cat needs its ar\u00adgument. In our \nalgorithm, the original property to prove is cat : f + f and this requires proving the following property: \nfoldr : append -f + t -+ f where the first component of the type is an unevaluated closure which corresponds \nto the conjunction of all the types of append. This returns True directly because if 1 has type f then \nso does 1 = nil and the body of foldr as well. This example shows that abstract in\u00adterpretation is unnecessarily \nexpensive because it considers all possible abstract values for the arguments of a function when only \nsome of them, are really useful. This problem becomes crucial in the presence of higher-order functions. \nIn contrast, our algorithm finds information about append without computing unnecessary information about \nits argu\u00adments: in this example append is left unevaluated in the type of foldr because it is not necessary \nto answer the orig\u00adinal question. This case is extreme because we do not need any information about append \nat all. A dtierent original question might require proving that append possesses a par\u00adticular type. \n 1.2 Paper Organisation The next section is a brief account of Jensen s logic which is the starting \npoint of the work described here. We define the notion of most general type in Section 3 and we describe \nthe corresponding system. We establish its correctness and com\u00adpleteness with respect to Jensen s logic. \nThree algorithms are derived from this system: a brute force implementa\u00adtion, an implementation of abstract \ninterpretation and the frontiers optimisation. These are presented in Section 4 as different instantiation \nof a single generic abstract machine. Section 5 introduces our notion of lazy types and presents a lazy \ntypes system equivalent to the previous ones. The algorithm for lazy types construction is described \nin Section 6 with its correctness and completeness properties. Section 7 is a review of related work \nand conclusions. Appendix 1 provides more details about the derivation of the abstract machines and Appendix \n2 is an example illustrating the lazy types algorithm. 2 Jensen s Strictness Logic Jensen considers a \nsimply typed J calculus with constants. The terms, A~, are defined by the following syntax: The ordering \non types and the program logic are defined in Fig. 1. r is an environment mapping variables to formulae \n(i.e. strictness types). In the rule Cond-1 u represents the standard type of ez (or ea). The rule states \nthat the con\u00additional is undefined if the predicate is; the type subscript allows the choice of a representation \nfor undefined which has a type structure which is compatible with the outer context of the conditional \nexpression. Cond-2 states that any type which can be inferred for both branches is valid for the con\u00additional \nexpression; this is the counterpart of the least upper bound operation used in traditional abstract interpretation. \nWe define = as the equivalence induced by the ordering on types: u=r~u<rmdr<u 3 Most General Types We \nintroduce a slightly restricted language of strictness for\u00admulae in Fig. 2; this language is closely \nrelated to van Bakel s strict types [I]., Baaically strict types do not al\u00adlow intersections on the right \nhand side of an arrow. This Figure 1: Jensen s Strictness Logic CTCT1 @cTs 1#1 E Ts ...J#~ET~ t,fET~ \n u-+@ET~ #JIA... AI$n ETI Figure 2: The language Tr restriction is convenient because it does not weaken \nthe ex\u00adpressive power of the system and it makes type manipulation easier. We then define the notion \nof complete type. The restric\u00adtion to complete types allows us to avoid the use of weak\u00adening because \na complete type contains (is the conjunction of) all of the elements greater than (or equal to) it. DEFmITION \n3. I CT(T) = /A{ct(a) I a c Stlp(r)} Sup(a) ={a cTSIa >a} Ct(t) = t Ct(f) = f ct(u A r) = et(u) A Ct(T) \nCt(a --+ T) = CT(U) + et(T) Sup(u) can be defined by induction on a. Notice that CT can be extended to \ncontexts in the obvious way. Finally, we can define the notion of most general type of an expression \n(with respect to some context): it is the conjunction of all of the types possessed by the expression \nin the given environment. DEFINITION 3.2 (Most General Types) MGT(r, e) = CT(~{Ct e T,s I r ET e : ai}) \nThe logic for computing most general types is shown in Fig. 3. C. is the set of T,s types compatible \nwith a (with the same arrow structure). In the following, we always con\u00adsider types modulo the congruence \nderived from the follow\u00ading equivalence: aAaza aAb Gb Aa (a Ah) AcSa A(b Ac) Modulo this congruence, \nthere is only a finite number of types u such that u = a . The following theorems account for the correctness \nand the completeness of F,s (with respect to ~T ) : THEOREM 3.3 (Correctness) rt_Se:U+rbe:C7 THEOREM \n3.4 (cOXIpleteIIeSS) CT(I ) ES e : MGT(I , e) The proof of correctness is straightforward. The com\u00adpleteness \ntheorem states that the type derived for an ex\u00adpression e by KS is (equivalent to) the conjunction of \nall the types derived for e by ET. Its proof relies on the correctness theorem and the following property: \nPROPERTY 3.5 r<A,A+Te:a,a~u a~r.CT(r) l-se: ct(a )A~ ThE property is shown by induction on the length \nof the proof of A ETe:cr. 4 Abstract Interpretation and Frontiers In this section we present three abstract \nmachines derived from the above logic. All of the machines are instances of a single scheme. The first \nmachine implements the above logic in a fairly direct manner; the other two are optimisw tions of the \nfirst. The first computes the type of abstractions and fixed points by exhaustive search. There are thus \ntwo areas for optimisation: the first directs the search for fixed points by building an ascending chain \nof approximations, the resulting machine is equivalent to a naive abstract in\u00adterpretation; the second \ndirects the search for abstractions by using a more compact representation of the set of types akin to \nthe notion of frontiers [13]. The machines have three components: an S stack for partial results, an \nenvironment E and a code C contain\u00ading sub-expressions. The schematic machine is shown as a Var ri~+~l~s~:~ \nmut r~sc:t r[cw@l] +se:o; A.. .A*:l . . . r[z+@k] Ese:$y A.. .A+:k Abs r+s Ac.. :((#l+@~)A.. .A (#l+ \n!6&#38;)A. .. A(#k+~$k)) rFsel:(($; A.. .A#;l)+@l)A. .. A((@~A. .. Ac#;m)+f6m) rkse2:o~A,.,A8k App ri-se1e2 \nA,+, i such that +;, ..+k, }a{b l,..., eh} { r +s (Age) : u Fix r fs ~x(~g.e) : A~=l et ?uith(OI A.. \n.A~+6+,)Eu Eu fmdii I?+sel:f Cond-1 r t-s cond(el, .2, e3) : A CT (CO) I?ksel:t I i-Se2:qi~A...A&#38;2 \nrl-Ses:$~A...A+~a Cond-2 r ~s cond(el, e2, .3) : A{+i [ Sk,t.+, = #~ = +?} .n the rule Abs, we generate \nthe premises by taking all +1 = CT(U, ) such that o, c Cc with o the standard type of z. Figure 3: The \nMost General Types system transition system in Fig. 4. We use the following conven\u00adtion: E[z I+ #] is \nthe property x is bound to ~ in E and (z : ~): E represents an environment which is equal to E except \nthat x is bound to #. To specify a particular machine, we must define the fol\u00adlowing operations: ~nitabs, \ninitialabs, update~b~, iterab8, lastabs, comb~b~, initt,=, initiait,z, updatef ,s, iterj,z, iastf 8ZjCombt,=, \n app, cond, jiz (Fig. 4). To motivate the definition of the schematic abstract machine, consider the \nlogic in Fig. 3. The only complicated transition rule corresponds to the im\u00adplementation of Abs. Abs \nrequires an iteration to generate all the ~i 6 CC as indicated in Fig. 3. Since the body of a fixed point \nexpression is always an abstraction (see the abstract syntax), fixed point computations also involve \nit\u00aderations. In our later optimisations we will want to treat these two types of iterations (abstractions \nand fixed points) differently, thus we have separated them in the schematic semantics. With this motivation \nin mind, the init functions pick a start point for the iteration, the iter functions choose the next \nstep in the iteration, the last functions check for ter\u00admination and the comb functions combine the results \nof the iterations. The initiai functions generate a set from which the next iteration is determined; \nthese sets are updated on each iteration. This generic abstract machine can be derived from the type \nsystem of the previous section in several stages very much in the spirit of [11]. More details about \nthis derivation are given in Appendix 1. The result of these refinements is an inference system which \nis an Abstract Evaluation System in the terminology of [11]. Such an inference system can alternatively \nbe presented as an abstract machine as we have done in Fig. 4. The correctness proof of the generic machine \ndepends on conditions on the generic instructions that we do not present here. These conditions ensure \nthat the iteration is completen with respect to the Abs rule of Fig. 3. It can be shown that the three \ninstantiations of this machine described below satisfy these requirements. THEOREM 4.1 (s, r, e:c)b (~:s, \nr, c) + l_ 1-Se:4 4.1 The na7ve machine The following definitions are motivated by consideration of \nthe Abs rule in the logic. The operator initia2 generates the set of complete types which correspond \nto the types in the premises of the rule (except for the type selected by fir-d). On each iteration, \niter chooses another element from this set and update removes it. Termination, decided by the predicate \nlast, occurs when the set is empty. The comb operator builds the result type. init f i= (U) = first(a) \ninitilllfiz (a) CT(C.) first(a) iterf,=(~, u, r) = qtcz updatefim(~, U, T) ~ iterobs(~, C7,T) 2a9tf,=(Z, \na, r) = $X; 0) combf,=(r, s) =  We assume that the choice of @ is deterministic and fiTst selects an \narbitrary complete type compatible with u. The operators app, cond and fix are defined as implied by \nthe type system. Given these definitions, the rule for fixed points can be simplified to a single rule: \n(S, E, fix(~g.e) : C) b (S, E, (Age): Fix: C)  4.2 The abstract interpretation machine In abstract interpretation, \nthe search for a fixed point is no longer random but involves the construction of an ascending chain \nof approximations. We achieve this by modifying the definitions of ic,hitialf;=,updatef;-, kastf;z, initfiterj,m, \ncombf,z and fiz. The result of isthe least com\u00ad initfim(u)  plete type compatible with u. In this algorithm \nthe next (s, E[c * #], .T : c) b (4: s, .E[C* # I, c) (S, E, C:C) b (t:s, ~, c) (S, E!, (Aca.e) : C) \nb (s, (S: iraita~s(u)): E, e : Abs : ~~erabs(~, % ~): c, where ~ = initiaiab~(o) (Ar, :s, (s:u):E, A~s:C) \nb (A(u+TJs ($ U) EJ c, (r : S, (Z: O): E, Ite.rab.($,e,~) : c) b (r,s, (S:ite~ab.(~,a,r)): , .: Ab.: \ncO~bab, :~tera~~(z, e,~ ) :C) where X = Uf)dateob.q(~,a, r) if li~$tab~(~, a, r) (r : S, (Z: U) : E, \nIterab.(r, e,~): c) b (r S, E, C) if lasta~~(s, a, r) (rl : r2 : S, E, C077Jbab. : C) b (corrabab.(rl, \nrz) : S, E, C) (S, E, (e~ez):C) b (S, ~, el :ez :APP:c) (s, E, cond(el, ez, es) : C) b (S, E, el:e2:e3:Cond:C) \n (s, E,; i3x(Aga.e) : c) b (s, (a : initjir(u)) : E, e : Abs : Iterf:s(g, e,~) : ~~z : C) where Z = initiaifi~(a) \n(r : S, (g: u) : E, Iterf,z(g, e,~) : C) b (r : S, (g : iterf,r(~, u, r)) : E, e : Ab~ : CO~bfi= : ~terj,z(g, \n%~ ) : C) where X = upd~te~ir(~, U, r) if 11a8tfi~(z, c, r) (r :S, (9 U) ~, Ite+f-(gt:. %2) c) b (r:s! \nE c, if la~tf,~(u, r) (rl : r2 : S, El, Comb~,= : C) b (combf,z(rl, rz) : S, E, C) (rl:... :rn:S, E, \nOp:c) b (op(r~,..., r~):S, E, C) Op E {App, Fix, Cond} of arity k Figure 4: The schematic abstract machine. \niterate is generated from the result of the Drevious itera\u00adtion; the ~et Z plays no rtde in the computation \nand so initialf,c and updatefi~ are redundant. The iteration ter\u00adminates when the result from one iteration \nis equal to the result from the previous iteration: the ascending chain h~ stabilised. Since the result \nfrom the previous iteration is already accounted for in the type generated by the current iteration, \ncomb f,= just discards it. initf ,= (a) = cqfa)  2 7titiC3ifi.z (d) iter~im(~,~j A(~ + ~s)) ~ ~Ti update~,= \n(Z, u, T) 2astf,$(Z, u, r) : [-)=T) . combf,=(T, .9) T = fax id It is straightforward to show the input/output \nequiva\u00adlence of this machme and the previous one. We observe that the set of types modnlo = is finite. \nMoreover, the type used for each iteration can be shown to be monotonically in\u00adcreasing. As a consequence, \nthe algorithm is guaranteed to terminate with a fixed point. The correctness is shown with respect to \nthe brute force algorithm by a routine inductive argument. 4.3 The frontiers machine Considering first-order \nfunctions, the machine of the last subsection effectively computes a truth-table representation of the \nfunction. Clack and Peyton Jones [5] proposed an al\u00adternative representation of first-order functions: \nrather than representing a function by its truth table, one co~d just record the maximal argument values \nat which the result was O th~ gives a compact representation from which the truth table could be reconstructed. \nThis representation was called the O-frontier of the function. Hunt and Hankin [13] have shown how this \nnotion can be generalised to higher-order functions over non-flat domains. We now show how the frontiers \noptimisation can be in\u00adcorporated into our work. We compute a restricted form of type which records the \nmaximal argument types which give a result of type f. The frontiers machine is considerably more complicated \nthan the other two. For simplicity, we consider unary func\u00adtions whose result type is a basic type. The \nextension to n-ary functions with basic type results is relatively straight\u00adforward. The extension to \nfunctions with more complex re\u00adsult types is possible, but rather complex (see [13] for the encodings \nrequired). In this algorithm, the set Z is used to store the current trial O-frontier [17]. Initially, \nit is empty and %itabs selects the only candidate point. At the end of each iteration, either the last \ntype corresponds to a frontier type (the result is f) and the set is unchanged, or it doesn t and the \nnext lowest elements (preds) are added to the set. The process terminates when the trial frontier is \nempty. We redefine the abstraction operators as follows: CT(tm) @ I+ G nezt(Z, a, r) nezt(~, a, r) ~~eT&#38;s(~, \nu, r) ned{X, a, r) = @ G~(T A .9) the maximal elements of {u e Tsla <u} X, ifres(r, a)=f z u preds(a), \notherfie where Tes selects the result type from a functional type, defined as: res(~(u -+ T,) A T,a) \n= Ar, The fix operators are the same aa for the abstract inter\u00ad pretation machine. It is straightforward \nto show that this machine is correct with respect to the abstract interpreta\u00ad tion machine and thus the \noriginal logic. 5 Lazy Types The frontier representation improves the basic algorithm by avoiding the \ncomputation of certain types which can be de\u00adrived from the canonical form. But frontiers do not change \nthe essence of the implementation and still lead to imprac\u00adtical analyses in the presence of larger domains. \nWe take a more radical approach in this section: rather than returning all possible pieces of information \nabout the strictness of a function we compute only the information required to an\u00adswer a particular question. \nThis new philosophy naturally leads to a notion of lazy evaluation of types. Lazy types are defined in \nFig. 5. The ordering on types and the logic are shown in Fig. 6. The key idea is that an expression from \nthe term lan\u00adguage (with its environment) may appear w part of a type; this plays the r61e of a closure. \nMore formally, a closure (I , e) stands for A4GT(I , e), the conjunction of all of the possible types \nof the term. This correspondence explains the new rules in the definition of ~ G. Not surprisingly, the \nlazy evaluation of types is made explicit in the App rule: rather than deriving all possible types for \nez, we insert ez itself (with the current environment) into the type of el. The fol\u00adlowing definition \nestablishes a correspondence between lazy types and ordinazy types, the extension to environments is \nstraightforward: DEFINITION 5.1 Expand : TG ~ T1 Ezpand(t) = t Ezpand(f) = f ~zpand(al A UZ) = Ezpand(al \n) A Ezpand(a2 ) Ezpancl(ul + u2) = Ezpand(ul ) + l?zpand(az ) Expand((I , e))= MGT(Ezpand(I ), e) We \ncan now state the correctness and completeness of the lazy type system and the subsequent equivalence \nwith the original system. THEOREM 5.2 (Correctness) Ilr330ru3M 5.3 (Completeness) THEOREM 5.4 (Equivalence) \nFirst notice that we do not lose completeness by consid\u00adering TI types: it can be shown quite easily \nthat any type is equivalent to a type in TI. The following theorems are used in the proofs of theorems \n5.2 and 5.3. THEOREM 5.5 THEOREM 5.6 rt_Ge:(fJ$l A... A&#38;a) * (rEGe:h) and . . . and(r bG e : 4.) \nri-Te:(&#38;A... A#n) * (r bTe: #l) and . . . and(r ~T e : 4.) Theorem 5.5 can be proved by induction \non the proof of the left hand side. Theorem 5.6 is shown by deriving a proof of the right hand side from \na proof of the left hand side (it is quite straightforward). Theorem 5.6 allows us to prove theorem 5.2 \nby induction on e. The proof of completeness is carried out in two stages. First we show that the weakening \nrule can be removed from ET without changing the set of derivable types provided we add a form of weakening \nin the Var, Fix and Cond rules. A similar property has been proved for other type systems including a \nform of weakening [1, 21]. Then we use theorems 5.5 and 5.6 and proceed by induction on e to prove completeness. \n6 The lazy types algorithm Applying the same techniques as in Section 4, we can de\u00adrive an algorithm \nfrom the lazy type inference system. Space considerations prevent us from describing the derivation steps \nand we just present the result in the form of an abstract ma\u00adchine in Fig. 7. The implementation of the \nln~ instruction is omitted for the same reason; ~nf(~, ~) computes ~ <G @ aa defined in Fig. 6 (Theorem \n6.2). Notice that a stack ele\u00adment Si is either a boolean value or a disjunction of types. True (resp. \nFalse) is installed at the top of the stack if and only if the original property (of the form (e, 4)) \nin the code is (resp. is not) provable in ~ a. Values which are neither True nor False in the stack are \ndisjunctions of TG types (&#38; V . . . v #n). The occurrence of such a value at the top of the stack \nmeans that the original property is true if (and only if) the recursive function currently being analysed \npossesses one of the ~, types (in order to make the presen\u00ad tation simpler we do not consider embedded \noccurrences of fix here; the extension is straightforward). In order to prove that fix(~g.e) has type \n4 we add the assumption (g :, ~) in the environment and try to prove e : ~. If the result is True or \nFalse then the cwe is settled. Otherwise a list of condi\u00adtions ~i is returned and the algorithm iterates \nto try to show that one of them is satisfied (rule for lter). Instruction Rec is used to remember that \nwe were trying to prove a property on a recursively defined variable (denoted by w, in the en\u00advironment \n); so if it fails we just return th~ property in the stack rather than False. Appendix 2 develops an \nexample illustrating the treatment of recursion in the lasy type al\u00adgorithm. It analyses a function used \nin [18] to demonstrate the limitations of a type system without conjunction. Primitives And and OT are \nextended in the obvious way to apply on types: their result is always supposed to be a disjunction of \nTG types. The following theorem states the correctness of the lazy types algorithm. THEOREM 6.1 1. (S, \nr,(e, #):C) b~ (Tme:S, I , C)*r FGe:# Figure 5: The language TG I I i=l i=l Fix r +G fix(~g.f?) , #~ \n(k ~ [1,~]) r+Gel:f ~kGe2:@ rkGe~:+ Cond-1 Cond-2 r I-G cond(el, e2, e3) : # ri-GcOnd(el, e2, es) : # \nFigure 6: The Lazy Types system (s, Ej(c, t) : c) bG (True : S, E,C) (s, E! (%f) : c) ~G (~aiae , s, \nE,C) (S, E, (e,+, A +z) : c) bG (S, E,(e, #l) : (e, #2) : And: C) (S,E, (k.e, u+ ~) :C) bG (s,(2:u): \n-??, (.,T) :D(z) :c) (s, E,(ele2,4) : c) bG (s, E,(el, (E, ez) + 41) : c) (S, E, (cond(el, e2, e3), +) \n: C) bG (S, E,(el, f):(e2, #):(es, #) :And:Or :C) (S, (z : o) : E,(D(a)) : C) pG (S, E,C) (S, E, (flx(~g.e), \n$) : C) bG (S, (f :r +) : E,(e,4) : Iter(9je) : c) (S,~k +-+, @],(g : V) : C) bG (S, ~[g W, #J],Inj(4, \n@) : (~ec, g,+) : C) (True : S, E,(Rec, g, +) : C) bG (True : S, E,C) (S1 : S, E, (I?.., g, #) : C) bG \n~~ #Sk.i~) (SI : s, (g + #) : ~,~t.+g,e) , c) bG (S1 , s,~,c) S1 = True or S1 = False ((#l V... vh):s, \n(g:r +) :E,~~e@,e) :C) bG (S, E, (.ficAg.e, @A #1) : (jic~g.e, #A 42) : Or . . . . Or C) (s1 :s2 :s, \nE,op:c) bG ((oPs1s2) : s, E>c) Op=And or Op=Or Figure 7: The Lazy Types algorithm 208 2. (S,17, (e,@) \n: C) b~ (Faise : S, I , C) e =(J? FCi e : ~) if I and q$do not contain any I+, assumptions The proof \nof this theorem is made hand in hand with the proof of the following result: THEOFLEM 6.2 if I , ~ and \n+ do not contain any ~, assumption Most of the derivation steps to reach the abstract ma\u00adchine are very \nsimilar to the ones described in Section 4. The most difficult part of the proof concerns the implemen\u00adt \nation of fix. We have two main facts to prove: (1) the iteration terminates and (2) the result is accurate. \nTermi\u00adnation is proved by showing that each type ~ A @, satisfies # A 4, <G d. It is easy to show that \nthe result is accurate when the iteration terminates with the True answer. In or\u00adder to show that the \ninitial property cannot be satisfied if the answer is False, we prove that at least one of the ~i types \nreturned by the iteration step is a necessary condition to prove the original property (in other words, \nwe do not bypass the least fixed point). The algorithm described in this section can be optimised in \nseveral ways: The implementation of the conditional can avoid pro\u00adcessing the second and third term \nwhen the first term has type f.  In the rule for application, when expression e2 is a con\u00adst ant or \na variable then its type (t for a constant, its type in the environment for a variable) can be inserted \ninto the type of el rather than passing the whole envi\u00adronment. Notice that this optimisation is common \nin the implementation of lazy languages.  When an iteration step returns ~1 V.. . V &#38; then each \nof the #l is a sufficient condition to prove the original property ~. So to prove (g :4 A 4,) :r >Ge \n: q3A #, it is enough to prove (g : ~A~i) : r >G e : #i.  These optimisations are easy to justify formally \nand improve the derivation considerably. The reader can easily check that: (nil, nil, (cat, f ~ f) : \nnil) b: (True : nil, nil, nil) where cat is the function defined in the introduction. A more complex \nexample is described in Appendix 2. 7 Related work To summarise: we claim to make two contributions, \none methodological and the other technical. We briefly review related work in these two areas before \ndiscussing further work. The techniques we have used in the first stage of our re\u00adfinements (Sections \n3 and 5) are related to previous work on restricting type systems (see for inst ante [21] for a type \nsys\u00adtem with subtypes and [1] for a type system with conjunction types and subtypes), especially the \ntransformations required to remove weakening. Our approach to the development of abstract machines from \nlogics (Sections 4 and 6) is closely related to Hannan s and Miller s [11]. For example, the first rule \nfor fixed points can be recast as the result of a folding of inference rules [10, 11]. However our presentation \nof the generic abstract machine is akin to notions found in denota\u00adtional semantics: the three abstract \nmachines can be viewed .ss three different interpretations . As far as methodology is concerned, we believe \nthat our main contributions are to describe the various stages of refinement in a systematic way in the \nsame conceptual framework (even if the abstract ma\u00ad chines have been presented using the usual transition \nrule syntax for better readability) and to show that standard implementations of abstract interpretation \ncan be inserted quite naturally in this context. For example it is nice to see that the frontiers optimisation \ncan be described as a partic\u00adular rest riction on types. The main technical contribution of the paper \nis the no\u00adtion of lazy types and the corresponding type system and al\u00adgorithm. This addresses an issue \nthat has taxed the abstract interpret ation community greatly. The papers [4, 5, 7, 8, 9, 13, 16, 18] \nall tackle the same issue. The basic problem is that the choice to abstract functions by functions is \na disas\u00adtrous one for the efficiency of the analysis. We can classify the various proposals to circumvent \nthe problem into two cat egories: (1) some of them [4, 8, 13, 16] strive for a better representation \nof abstract functions to improve their compu\u00adtation without losing completeness (with respect to the \nsys\u00adtem of Section 2) while (2) others [6, 7, 9, 18] trade a cheaper implement ation oft he fixed point \nagainst a loss of accuracy. Our algorithm falls into the first category because it is com\u00adplete with \nrespect to the usual abstract interpretation but it has the syntactic flavour of some of the works in \nthe sec\u00adond category [7, 8, 18, 24]. As noticed earlier, lazy types improve on previous work on ilontiers \n[9, 13]. It is closer in spirit to the minimal function graphs approach in which abstract functions are \nrepresented by relations represent\u00ading the portion of the graph of the function that is needed in a particular \ncomputation (its so-called minimal function graph). However minimal function graphs are only defined \nfor a first-order language and the extension to higher-order does not seem to be easy. The abstract reduction \napproach [7, 24] is also based on a form of lazy evaluation but there is no notion of types and it is \na symbolic form of expressions which is evaluated lazily. As other methods in its category, abstract \nreduction may entail an arbitrary cut in the fixed point iteration to ensure termination. Of course this \nis at the price of accuracy. There are three major directions in which the work pre\u00adsented here can be \nextended. On the methodological side, we would like to follow [10] in extracting some general transfor\u00admations \non inference rules of the type studied here to derive abstract machines. On the technical side, we would \nlike to study the integration within our framework of approximat\u00adion techniques such as widening [6, \n22] and its implication on the efficiency of the analysis. Also we would like to extend our work to logics \ninvolving data types and richer properties [15]. Finally, we are currently implementing the lazy types \nalgorithm; we hope to be able to report results soon. Acknowledgements The first author was partially \nfunded by ESPRIT Work\u00ading Group 6809 (Semantique II). Pascal Fradet and Thomas Jensen gave helpful comments \non an earlier draft. References [15] T. P. Jensen, Abstract Interpretation in Logical Form, [1] [2] \n[3] [4] [5] [6] [7] [8] [9] [10]  [11] [12] [13] [14] S. van Bakel, Complete restrictions of the intersec\u00ad \ntion type discipline, Theoretical Computer Science, 102(1):135-163, 1992. H. Barendregt, M. Coppo, M. \nDezani-Ciancaglini, A filter lambda model and the completeness of type as\u00adsignment, Journal of Symbolic \nLogic, 48(4), 1983. P. N. Benton, Strictness logic and polymorphic invari\u00adance, in Proceedings o,f the \n2nd Int. Symposium on Logical Foundations of Computer Science, LNCS 620, Springer Verlag, 1992. T.-R. \nChuang and B. Goldberg, A syntactic approach to fixed point computation on finite domains, in Pro\u00adceedings \nof the 1992 ACM Conference on Lisp and Functional Programming, ACM Press, 1992. C. Clack and S. L. Peyton \nJones, Strictness Anaiysis -A Practical Approach, in J. P. Jouannaud (cd), Func\u00adtional Programming Languages \nand Computer Archi\u00adtecture, LNCS 201, Springer Verlag, 1985. P. Cousot and R. Cousot, Comparing the Galois \nCon\u00adnection and Widening/Narrowing Approaches to Ab\u00adstract Interpretation, in M. Bruynooghe and M. Wirs\u00ading \n(eds), PLILP 9% , LN CS 631, Springer Verlag, 1992. M. van Eekelen, E. Goubault, C. Hankin and E. Nbker, \nAbstract reduction: a theory via abstract interpreta\u00adtion, in R. Sleep et al (eds), Term graph rewriting: \ntheory and practice, John Wiiey &#38; Sons Ltd, 1992. A. Ferguson and R. J. M. Hughes, Fast abstract \ninter\u00adpretation using sequential algorithms, to appear in the Proceedings WSA 93, Springer Verlag, 1993. \nC. L. Hankin and L. S. Hunt, Approximate ficed points in abstract interpretation, in B. Krieg-Briickner \n(cd), Proceedings of the fth European Symposium on Pro\u00adgramming, LNCS 582, Springer Verlag, 1992. J. \nJ. Hannan, .Investigating a proof-theoretic meta\u00adlanguage, PhD thesis, University of Pennsylvania, DIKU \nTechnical Report Nr 91/1, 1991. J. Hannan and D. Miller, From Operation/ Seman\u00adtics to Abstract Machines, \nMathematical Structures in Computer Science, 2(4), 1992. F. Henglein, Efficient type inference for higher-order \nbinding time analysis, in Proceedings of the 5th ACM Conference on Functional Programming Languages and \nComputer Architecture, LNCS 523, Springer Ver\u00ad lag, 1991. L. S. Hunt and C. L. Hankin, Fixed Points and \nFron\u00adtiers: A New Perspective, Journal of Functional Pro\u00adgramming, l(l), 1991. T. P. Jensen, Strictness \nAnalysis in Logical Form, in J. Hughes(al),Proceedings o~ the 5th ACM Conference on Functional Programming \nLanguages and Computer Architecture, LNCS 523, Springer Verlag, 1991. PhD thesis, University of London, \n1992. Also available as DIKU TechnicaJ Report 93/11. [16] N. D. Jones and A. Mycroft, Data-flow analysis \nof applicative programs using minimal function graphs, in Proceedings of the ACM Conference on Principles \nof Programming Languages, 1986. [17] S. L. Peyton Jones and C. Clack, Finding Fired Points in Abstract \nInterpretation, in S. Abramsky and C. L. Hankin (eds), Abstract Interpretation of Declarative Languages, \nEllis Horwood, [18] T.-M. Kuo and P. Mishra, perspective based on type the fth ACM Conference Languages \nand Computer 1989. [19] J. Launchbury, Strictness 1987. Strictness analysis: a new inference, in Proceedings \nof on Functional Programming Architecture, ACM Press, and binding time: two for the price of one, in \nProceedings of the ACM Confer\u00adence on Programming Languages Design and Imple\u00admentation, 1991. [20] A. \nLeung and P. Mishra, Reasoning about simple and exhaustive demand in higher-order lazy languages, in \nProceedings of the 5th ACM Conference on Functional Programming Languages and Computer Architecture, \nLNCS 523, Springer Verlag, 1991. [21] J. C. Mitchell, Type inference with simple subtypes, Journal of \nFunctional Programming, 1(3), 1991. [22] B. Monsuez, Polymorphic Typing by Abstmct Inter\u00ad pretation, \nin Proceedings of 12th Conference FST tY TCS, Springer Verlag, 1992. [23] A. Mycroft, Abstmct Interpretation \nand Optimizing Transformations for Applicative Programs, PhD the\u00adsis, University of Edinburgh, December \n1981. [24] E. N6cker, Strictness analysis using abstract reduction, in Proceedings of the 6th ACM Conference \non Func\u00adtional Programming Languages and Computer Archi\u00adtecture, ACM Press, 1993. [25] P. Wadler, Strictness \nAnalysis on Non-flat Domains, in S. Abramsky and C. L. Hankin (eds), Abstmct In\u00adterpretation of Declarative \nLanguages, EIJis Horwood, 1987. [26] P. Wadler, Is there a use for linear logic?, in Pro\u00adceedings of \nthe ACM SIGPLA N Symposium on Partiai Evaluation and Semantics-Based Program Manipula\u00adtion, ACM Press, \n1991. Appendix 1: Deriving The Abstract Machines The abstract machines presented in Section 4 can be \nde\u00adrived in a rather systematic way from the Most General Types system presented in Section 3. In this \nappendix we provide some intuition about this derivation by showing how the rules for variables and application \nare successively trans\u00adformed. The original rules are as follows (Fig. 3): rp~~l~s~:~ r+se~:T~ rl-se2:T2 \nr +s e~e~ : App(TI, T2) with TI=((#~A . .. A#&#38;l)~@l) A... A((#~A. .. A@Jflm)~t#m) T2=81A. ..A6h \nand App(Tl$ T2) = ~ @i i such that {#j, #:l }+{o,,..., ok} : The first reason why the inference system \ndescribed in Fig. 3 E not an abstract machme IS that some reference rules have several premises. The \nfirst refinement ensures that sll rules have a single premise. This is achieved by defining a predicate \nMl : /ist(enu) + list(e) + iist(Tr) -Bool such_ that kflrz~e Vii?, +sei : 0:. MI is defined as follows \nfor variables and application: MIECS Ml r[c=~]:~ ~:c ~:s Ml 17:r:E e1:e2:C T1:T2:S Ml I :E (e~ e2):C \nApp(TI, T.3):S Ml nil nil nil In this system a new environment is created for each instruction (subexpression) \nin the code. This is not very sensible and the second transformation replaces the list of environments \nby a single environment. M2 P e1:e2:C T1:T2:S M2 r (.1 ez) :C App(T1, Tz) :S M2 nil nil nil The only \nreason why M2 still does not behave like an abstract machine is that some variables in the premises do \nnot occur in the goal (we can see that this is the case for application by considering the definition \nof App above). In operational terms, this amounts to saying that the system does not exhibit a tail recursive \nbehaviour. This problem is solved by introducing an extra argument R which is not modified in the rules \nand will ultimately be instantiated with the result of the computation. F e1:e2:App:C S RM3 M3 r (e1e2):C \nS R M3 1 C App(T1, T2) :S R M3 r App:C T2:T1:S R M3 nil nil R:nil R We now have an inference system \nwhich is an Abstmct Evaluation System in the terminology of [10, 11]. This means that ewecan alternatively \nresent it as a rewriting system de\u00ad scnbmg a machine with t! ree components. We just have to rewrite \nany rule: M3r c s R M3r CSR (r, c, s) b (r , c , s ) Applying this technique and rearranging the order \nof the arguments, we get the following rules (which are the rules of the schematic abstract machine in \nFig. 4). (S, E[c I-++], z :C) D (~ :S, E[z -#], C) (s, E, (ele2):C) b (S, E, el :e2 :App:C) (T2 :T1 \n:S, E, App :C) b (APP(TI, T2) :S, E, C) Appendix 2: The lazy types algorithm at work The following function \nwas used in [18] to demonstrate the limitations of a type system without conjunction. fix(lg.(k.~y.~z.cond(eg \nz 0)(+ x y)(~ y z ( z l)))) We show how the lazy type algorithm is able to derive that this function \nis strict in its first argument, so haa type T1 = f ~ t ~ t ~ f. The derivation is shown below. This \nexample illustrates the implementation of fix: first the assumption g :, T1 is added to the environment \nand the property to prove is (E, T1 ). The assumption is not strong enough to prove the required property \nbut the first iteration step returns a necessary condition (g : T2 ) which is added to the environment \n(with T2 = t ~ f a t -f). This is because it is necessary to prove that the function is strict in its \nsecond argument to show that it is strict in its first argument. The second iteration step succeeds in \nproving (E, T1 A T2) from the assumption (g : (Tl A T2)) and the find result is True as expected. We \nuse the following notation: G = flx(Ag.(Az.Ay.b. cond(eg z 0)(+ r y)(g y z (-z l)))) E = cond(eg z0)(+ \ncy)(g yc (-z 1)) E = (k. Ay.Az.E) T1 =(f~t~t~f) T2=(t~f~t~f) We show how the property G : TI is proved \nby the Iasy types algorithm: (nil, nil, (G, T1) : nil) b~ (nil, (z :t):(y :t):(o:f):(g:r T1) :niJ, (.E, \nf) : D(z): D(y); D(c); Iter(g, E); nii) b: (nii, (z :t):(y: t):(c:f):(g:r T1) :nil, (((q z O),f) : ((+ \n~v), f); ((gv c( z l)), f); And; @; W~); D(v); D(Z); Iter(g, E); nii) b&#38; (True :False :nil, (z :t) \n:(y :t) :(m:f) :(g :~ Tl) :nii, (((g Yc (-z l)), f) : And; Or; D(z); D(Y); D(c); Iter(g, E)i M) b> (True \n:False :nil, (z :t) :(y :t) :(z :f) :(g :~ TI) :nil, ((g, T2) : And; Or; D(z); ~(y); ~(~); ~~er(g, ~); \n~~~) b~ (False :True False :nil, (z :t) :(y :t) :(c :f) :(g r 2 1) :nil, (Rec, g, Z 2) : And; Or; D(z); \nD(y); D(c); Iter(g, E); nil) b~ ((T2) : nil,(~ :r T l) : nit, Iter(g, E) : rair) p> (nil, nit, (G, T1 \nA Tz) : rail) b: (nil, (j :, (T1AT2)) : nil, (E , T1) : (E , T2) : And: Iter(g, E) : nii) b~ (True : \nnil, (g :r ( T1AT2)) : nil, (13 , T2) : And: Iter(g, E) : nil) b~ (True : False : True: nil, (z t) : \n(y : f) : (z : t) : (g :~ (T1AT2)) : nii, ((g, Tl) : And; Or; D(z); D(y); D(z); And; Iter(g, E); nil) \nb~ (True :True :Faise :True :nil, (z :t) :(y :f) :(c :t) :(g :r (T1AT2)) : nil, (Rec, g, Z l) : And; \nOr; D(z); D(y); D(c); And; Iter(g, E); nil) b: (True : True : nil, (g :r (Tl A T2)) : nit, And : Iter(g, \nE); nii) b: (True : nii, nil, nio \n\t\t\t", "proc_id": "174675", "abstract": "<p>The role of non-standard type inference in static program analysis has been much studied recently. Early work emphasised the efficiency of type inference algorithms and paid little attention to the correctness of the inference system. Recently more powerful inference systems have been investigated but the connection with efficient inference algorithms has been obscured. The contribution of this paper is twofold: first we show how to transform a program logic into an algorithm and, second, we introduce the notion of lazy types and show how to derive an efficient algorithm or strictness analysis.</p>", "authors": [{"name": "Chris Hankin", "author_profile_id": "81100305946", "affiliation": "Department of Computing, Imperial College, London SW7 2BZ, UK", "person_id": "PP39036918", "email_address": "", "orcid_id": ""}, {"name": "Daniel Le M&#233;tayer", "author_profile_id": "81100387737", "affiliation": "INRIA/IRISA, Campus de Beaulieu, 35042 RENNES CEDEX, France", "person_id": "P59017", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/174675.177858", "year": "1994", "article_id": "177858", "conference": "POPL", "title": "Deriving algorithms from type inference systems: application to strictness analysis", "url": "http://dl.acm.org/citation.cfm?id=177858"}