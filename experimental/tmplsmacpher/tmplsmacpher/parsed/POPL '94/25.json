{"article_publication_date": "02-01-1994", "fulltext": "\n Lazy Array Data-Flow Dependence Analysis * Vadim Department of University of Maryland, vadik~cs. umd. \n Abstract Automatic parallelization of real FORTRAN programs does not live up to users expectations yet, \nand de\u00ad pendence analysis algorithms which either produce too many false dependence or are too slow \ncontribute sig\u00adnificantly to this. In this paper we introduce data\u00adflow dependence analysis algorithm \nwhich exactly com\u00adputes value-based dependence relations for program fragments in which all subscripts, \nloop bounds and IF conditions are affine. Our algorithm also computes good affine approximations of dependence \nrelations for non-affine program fragments. Actually, we do not know about any other algorithm which \ncan compute better approximations. And our algorithm is efficient too, because it is lazy. When searching \nfor write statements that supply values used by a given read statement, it starts with state\u00adments which \nare lexicographically close to the read statement in iteration space. Then if some of the read statement \ninstances are not satisfied with these close writes, the algorithm broadens its search scope by look\u00ading \ninto more distant writes. The search scope keeps broadening until all read instances are satisfied or \nno write candidates are left. We timed our algorithm on several benchmark pro\u00adgrams and the timing results \nsuggest that our algo\u00adrithm is fast enough to be used in commercial compilers it usually takes 5 to \n15 percent off 77 -02 compila\u00adtion time to analyze a program. Most programs in the 100-line range take \nless than 1 second to analyze on a SUN SparcStation IPX. *This work is supported by an NSF grant (XW9157384 \nand by the Packard Foundation. Perrrdssion to copy without fee all or part of this matarial is grantad \nprovidad that tha copias ara not mada or distributed for direct commercial advantage, the ACM copyright \nnotica and the titla of the publication and its data appear, and notice ia given that copying 18by permieeion \nof the Association for Computing Machinery. To copy otherwisa, or to rapublish, requiraa a fea andlor \nspecific permission. POPL 94-1/94, PorUand Oragon,USA @ 1994 ACM O-S9791-636-9B41W1 ..$3.50 Maslov Computer \nScience College Park, MD 20742 edu, (301)-405-2726 1 Introduction Currently automatic parallelization \nof real-life FOR-TRAN programs is not as perfect as users desire. As recent studies [EHLP91, B1u92, May92] \nindicate, in many cases false dependence bet ween statements in\u00adtroduced by inexact dependence analysis \nalgorithms prevent loops from being parallelized. In the introduc\u00adtion we analyze the bssic reasons for \nfalse dependence and show how the algorithm introduced in this paper avoids introducing false dependence \nwithout loosing eiliciency. Value-based dependence vs memory-based de\u00adpendence. Traditionally dependence \nanalyzers of parallelizing compilers and environments computed only memoy-based dependence. That is, \nthey r~ ported that there is a dependence between two state\u00adments of a program if these statements access \nthe same memory cell. For example, for the program in Fig\u00adure l(a) traditional dependence analyzer (for \nexample, that of Parascope) reports that there is a flow depen\u00addence from statement SO to statement S1 \ncarried by the loop rs. Since memory-based dependence often can be re\u00admoved by program transformations \nsuch ss array ex\u00adpansion and privatization (for example, array XRSIQ can be privatized in loop rs), recent \nresearch activity has focused on value-based (or data-flow) dependence, which need to be computed to \nperform these trans\u00adformations. Value-based dependence, introduced by Feautrier in [Fea88b], reflect \ntrue flow of values in a program unobscured by details of storing data in mem\u00adory. Intuitively, a valuebssed \ndependence exists between two statement instances if there exists memory-bssed dependence between them \nand value written in the first statement instance is actually used in the second in\u00adst ante, that is, \nthe memory cell written in the first statement instance is not overwritten before the sec\u00adond inst ante \noccurs. INTEGER rs, p, q, i DOrs = 1, nrs DOq=l, np DOi=l, mb so: XRSIQ(i, q) = O END DO END DO DOp=l, \nnp DOq=l,p ... DOi=l, mb Si: XRSICJ(i,q) = XRSIQ(i,q) + . . . S2 : XRSIC)(i,p) = XRSIIJ(i,p) + . . . \nEND DO END DO END DO ... END DO (a) Fra~ment of subroutine OLDA from TRFD .,- Figurel: Examples from \nLet s consider a program in Figure l(a) which is slightly simplified fragment of the subroutine OLDA \nfrom the Perfect Club benchmark suite [B+89]. Init there exists loop-independent value-based dependence \nfromstatement Sotostatement Sl butno loop-carried value-based dependences from So toSl, because state\u00admerit \nSlreads valueofXRSIQ (i,q) written onthesame iteration ofthelooprs and does notread values writ\u00adten on \nprevious iterations of loop rs. Dependence Representation. Traditionally de\u00adpendence were represented \nby direction vectors [Wo182] and dependence distances [Mur71]. Direction vectors represent a relationship \nbetween statement in\u00adstances involved in dependence inexactly, and depen\u00addence distances prelimited to \nrepresenting only fixed differences between write andread variables. Theezact relationship should be \nprovided, if we want to use ad\u00advanced loop transformation and code generation tech\u00adniques such as [Fea92a, \nFea92b, AL93, KP93]. Some array expansion and privatization algorithms also re\u00adquire exact dependence \ninformation [Fea88b]. Recently researchers started to use source &#38;w\u00adtions to represent value-based \ndependence. For a given statement instance Sz[r] the source function produces coordinates of the statement \ninstance Sl[w] such that SI [w] supplies the value used in S2 [r]. The version of source function computed \nin [Fea91] is called Quasi-A@ne Selection Tree (quast). In [MAL93] a different term is used for the same \nob\u00adject Last Write Tree (L WT). For example, quast for the statement Sz in Figure l(a) is Src(Sz ~, \nq, i]) = if q=p then Sib, q,Zl elseif q>2 then 5 2~, q-l, i] . else then Sob, i] [ We found that LWTs/quasts \nhave several drawbacks DO i= 1, NMOL1 DO j =i+l, NNOL  c Inlined subroutine CSHIFT Sl: XL(1) = XMA-XMB \nS2: XL(2) = XMA-XB(l) S3: XL(3) = XMA-XB(3) ... S12: XL(12) = XA(2)-XB(3) S13: XL(13) =XA(l)-XB(2) S14: \nXL(14) = XA(3)-XB(2) DOk= 1,14 S16: XL(k) =XL(k) -. . . END DO ... END DO END DO (b) Fragment of subroutine \nIfiTERFfrom MDG Perfect Club benchmark as a method of dependence representation (see below), and we decided \nto use dependence relations introduced in [Pug91] to represent value-based dependence. If a pair consisting \nof the given instance of write statement S1 [w] and read statement Sz [r] belongs to the depen\u00addence \nrelation then there is a value-based dependence from S1 [w] to Sz[r]. For example, the above LWT can \nbe represented as a union of 3 simple relations: sl~, q,i] +S2~, q,i] / l<p=q<np A l~i<mb S2b,el, i]+ \nf72b,q,~][ 2<q<P<nP A I<iSmb sl)~,i] +S2~, l,i] I 2<p<np A l<i<~b (1) Similarly, the source function \nfor S1 is: S2~, q l, i]+ SI~, g,i] I 2<p=q<np A l<i<mb &#38;~l, q,i]+&#38;b, q,i] I 2<p Snp Aq=p-1 Al<i<rnb \nThese two source functions may seem to be compl~\u00adcated, but if we draw dependence graph that they pro\u00adduce \n(see Figure 2, only axes p and q are shown), we will see that they encode elegant and relatively simple \nvalue flow pattern. We think that dependence relations have the follow\u00ading advantages comparing to LWTs/quasts: \nIf we want to know, under which condition a given LWT leaf is valid, we need to build and simplify a \nconjunction of conditions from nodes on the path from this leaf to the LWT root. Since conditions on \nthe ELSE branches of the tree are negated, we end up having disjunction of conjunctions of con\u00adstraints, \nwhich is much more difficult to handle than conjunction of constraints that we have in each simple relation \nof dependence relation. 4q .6 c S2~,ql at (p+l/2,q+l/2) -5 O Sl[p,ql at (p}q) -4 x So[p]at (p+l/2,0) \n-3 -2 -1 0 Figure 2: Dependence graph for the fragment Integer division is used in quads [Fea88a] to \nrepre sent complicated dependence patterns (see exam\u00adple in Figure 6). It means that quast may contain \nnon-affine functions, On the other hand, all ex\u00adpressions in the dependence relations are atline, because \ninteger division is represented using wild\u00adcard variables. Computing affine approximations of non-affine \nde pendence relations, we encounter a situation when a single read instance is dependent on several write \ninstances (see Section 5) and therefore the relation between read and write instances is not a func\u00adtion \nanymore and can not be represented as LWT. However, it still can be represented as a depen\u00addence relation. \n Computing value-based dependence efficiently. Computing value-based dependence is currently con\u00adsidered \n(by many people) to be too slow and inefficient to be used in production compilers. As we think one of \nthe reasons of existing techniques inefficiency is that they treat all the statement instances that write \nto ar\u00adray in question as having the same chance to be source of a given read. However, since we are looking \nfor a statement inst ante which most recently hit the mem\u00adory location read by a read statement, we can \nexpect that write statement instances which are lexicographi\u00ad cally closer to the read in iteration space \nare more likely to be sources of read data. Having this in mind, we decided to compute the source function \nfor a given read statement starting with A!? of subroutine OLDA projected on p and q write statements \nwhich are lexicographically closer to this read statement, and then proceeding to the more and more distant \nstatements, while keeping track of instances of read statement already covered by writes. When all read \nstatement instances are covered, we can stop and not test for dependence from other writes to the read. \nFor example, let s consider a program in Figure l(a). Using our algorithm, we are able to compute the \nde\u00adpendence relations (1) and (2) not using information about references to XRSIQ other than in statements \nSO, S1, Sz. These other references do exist, and not hav\u00ading to prove that dependence from them to S1 \nand Sz are false dependence improves the performance of dependence analysis. Or, let s consider another \nexample in Figure l(b) 1. All instances of the read XL(k) from the statement SIG are covered with writes \nto XL from statements Sl,..., S14asfollows: &#38;[i, j] ~ s16[ij, 1] I lSi~NMOLl A i-1-l~j~IiMOL ... \n... S14[i, j] ~ SIG[i, j, 14] [ 1 siSNMOLl A i-tl ~ jSNMOL (3) We are able to prove this not examining \nstatement instances which precede S1 [i, j], that is, instances S1[i , j ], ....S14[i , j ] such that \n(i < i) V (i =i A j < j) and statements other than S1, .... S14 referencing the array XL. 1~ it IF statement \nwith non-afline condition is removed from stat ement S1e to make the example more simple, however our \ntechniques work even if this statement is present. DOi=l, n DOj=l, n x= F(i, j) so: IF (X) THEN Sl: \nA(j) =... ELSE S2 : A(j) =... ENDIF S3: ... = A(j) END DD END DO (a) Non-affine IF condition Figure3: \nNon-affine Handling non-fine conditions and subscripts. Let s consider aprogram fragment in Figure 3(a). \nIn it the read A(j) from the statement S3 is covered by the write A(j) from either statement S1 or S2 \nat the innermost loop level. All existing dependence analyzers (that we are aware of) can not recognize \nthis because IF statement So has non-affine condition x = F (i, j ). Not knowing that read A(j) is covered \nwithin the body of loop j, existing systems assume that there exist a flow dependence from SI and S2 \nto S3 carried by the loop i and therefore they can not parallelize loop i. Non-affine subscript functions \nalso confuse many ex\u00adisting systems. For fragment in Figure 3(b) they can not establish that the write \nto A(x) in statement S1 completely covers the read A(x) in S2 at the inner\u00admost level. As before, it \nhappens because x = F (i, j ) is non-afhne function. Assuming that loop-carried de\u00adpendence from S1 to \nS2 exist they can not parallelize loops i and j. We can prove that both dependence are loop\u00adindependent \nby using techniques described in Section 5.  2 Definitions Notation used is summarized in Figure 4. \nDomain of our work. Our data dependence test\u00ading algorithm was originally designed to compute value-based \ndependence for afine program fragments. Affine program fragment is a loop nest such that in ev\u00adery statement \nof it all (1) subscript functions, (2) con\u00additions in IF statements, and (3) loop bounds are affine functions \nof loop variables and symbolic constants. If either of these requirements is not satisfied, this frag\u00adment \nis non-afine. Then we modified the algorithm to handle non-affine fragments (see Section 5), so actually \nit computes de\u00adpendence relations for any structured program frag\u00adment which does not contain GOTO, BREAK \nand WHILE statements. Allowed are assignment state\u00adment, structured IF and FORTRAN-like DO loop. DDi \n=1, n DOj =1, n x = F(i,j) Sl: A(x) =... S2: ... = A(x) END DO END DO (b) Non-affine subscript function \nprogram fragments Vectors and Statement Instances. Vector (also called -tuple) is simply an ordered set \nof integers. Vec\u00adtors are denoted with bold letters, such as w, r,s. They are used to represent points \nin n-dimensional space. The smallest unit of computation we consider in this paper is statement instance. \nThe statement instance W[w, s] is specified by W statement of the pro\u00adgram, w vector of loop variables \nvalues (loops which surround the statement W are included), and by s vector of symbolic constants. We \ncall variable a symbolic constant if it is not a loop variable and it is not assigned in the fragment \nof the program that we analyze. For starters we assume that the fragment being analyzed is the whole \nbody of the procedure being analyzed, but later (in Section 5) we will see that the scope of our dependence \nanalysis al\u00adgorithm is dynamic and so is the definition of symbolic constant. Sequencing predicate. We \nsay that instance of statement W specified by loop variables vector w and symbolic constants vector s \nis executed before instance of statement R specified by loop variables vector r and symbolic constants \nvector s or W [w,s] << .R[r, s] iff w[l..n] << r[l..n] v w[l..n] = r[l..n] A W << R where n is number \nof common loops surrounding both statements W and R. Relations. Relation is a set of ordered pairs of \nvec\u00adtors. (w ~ r) c R. means that pair (w, r) belongs to the relation R. Since statement instance (which \nis elemental unit of computation for us) is specified not just by vector of integers, but by statement \nand vector of integers, we consider relations bet we en statement inst antes. So we write (W[w] -+ R[r]) \n@ R when pair (W[w] --+ R[r]) belongs to the relation R. Operations on sets and relations that we use \nare sum\u00admarized in Figure 5. W, R Specific statement of a program. R.A, W.B specific read fwrite array \nreference A/B in a statement R/W. Arr(A) Array or scalar variable referenced in a reference A. w, r An \niteration space vector that represents a specific set of values of the loop variables. The individual \nvalues of the loop variables are referred to as WI, W2, . . . , T1, rz, . . . r[x..y] Subvector of vector \nr, consisting of components r=, r~+l, ..., rg-l, rw. Iwl Number of components in vector w. For example, \nIr[z..y]l = y z +1. [R, s] The set of iteration vectors for which statement R is executed given symbolic \nconstant vector s. R.A(r, s) The vector of integers produced by subscript function of array reference \nR.A, when the loop variables are specfied by r, and symbolic constants are specified by s. W<R Statement \nW occurs before statement R in a text of a program. w<r Vector w is lexicographictiy less than vector \nr. That is, (w1 < rl ) V (w1 = T1 A wz < rz) V (w1 = rl A wz =rz Aw3<T3) V.... W[w, s] An instance of \nstatement W specified by the loop variables vector w and symbolic constants vector s. W[w, s] < R[r, \ns] Statement instance W[w, s] is executed before statement instance R[r, s]. Figure 4: Notation used \nin this paper Operation Description Definition domain(l ) The domain of the relation F z E domain(l ) \n~ 3V s-t.(x+ V)c F range(F) The range of the relation F F\\S Restrict relation F to domain S F/S Restrict \nrelation F to range S 7rx(P(x, y)) Project problem P on variables x 7f.x(P(x, y)) Project problem P on \nvariables other than x Figure 5: Operations on vectors and relations Value-based dependence definition \nand rep\u00adresentation. The value-based dependence relation DepRel that describes the dependence coming \nto the read reference R.A of statement R is defined by the following: Vr, s : (V[v, s] ~ R.A[r, s]) E \nDepRel(w, r,s) % V[v, s] = m~(W[w, s] I w c [W,s] A r G [R,s] A Arr(VV..B) = Arr(R.A) A W.B(W, s) = R. \nA(r, s) A W[w, s] < R[r, s]) (4) This definition is constructive, that is, we can use it to actually \ncompute the dependence relations. When lexicographical maximum is computed, the result is a dependence \nrelation which is represented as a union of the following m simple dependence relations: Wl[w, s] ~ R. \nA[r, s] I DepRe/l(w, r,s) DepRel = ... Wm[w, s] ~ R. A[r, s] I DepRelm(w, r,s) [ where each DepReli \nis a conjunction of constraints and ~ xr,s(DepRei~(w, r,s)) S [R, s]. ;=1 Since source functions may \ninvolve integer division by constant [Fea88a] and we want to keep conjuncts DepReli affine, we use wild-card \nvariables to represent the integer division. That is, we replace constraint i = [k/cJ with afine constraint \nci + a = k A 0< a < c 1. Let s consider a program in Figure 6 [Fea88a] as an example of representing \nrelatively complex dependence with dependence relations. Source function for the statement S2 is defined \nas: Src(S2) = m<a(Sl[i, j]l O<i<M A O<j<N A 2~+j= k). (5) The PIP algorithm simplifies this to the quast \nin the right column of Figure 6. Our algorithm for computing lexicographical maximum (see Section A. \n1) simplifies (5) to the dependence relation: SIIM, k 2M]~SZ /2 M<k<2M+N A M> O S1[i, k 2i]~S2 lk l<2i<k \nA k N<2i A O~i<LV (6) In the 2nd conjunct of this relation i is not expressed as a function of read variables \nand symbolic constants, even though it is a function of them. There was some discus\u00adsion among researchers \nas to whether dependence relations should contain explicit functional binding between read and write \nvariables. We do not feel that this is necessary, but our algorithm can be modified to produce such binding. \nThe above dependence relation expressed in the functional form but without use of integer division is: \nS1[M, h -2M]4S2\\ 2M5k<2M+N A M~O ISj[i, k 2i] -S.J I 2i+a=k A Osa~l A 0<k~2M+l A JJ>l SI[i,0]+ S2I 2i-i-a=k \nA O< CY<l A 0~k=2c$~2M A N=O I if (-k + 2M+N>O) DOi =0, H then if (k-2M~O) DOj =0, N then SI [M, k \n2J4] Sl: A(2*i+j) = . . . Src(Sz) = else if (( k +N+2(k+2))>O) END DO then Sl[k+2, k 2(k +2)] END DO \nelse 1 S2: ... = A(k) Figure 6: Program and source function represented as a quast 3 Machinery used \nWe use the Disjunctive Normal Form (DNF) to rep\u00ad resent sets of vectors. The DNF is a disjunction of \nconjunctions of constraints which maps integer vectors to boolean values. Each constraint is an affine \nequality or inequality. DNF representing a set of vectors pro\u00adduces hue for the vector that belongs to \nthe set, and False otherwise. We use the following basic operations on DNFs: A, v , 1, r, RelMaxl<, f?elMax2<. \nThey can be broken into 3 classes. Conjunct to conjunct: A , r. We use the Omega test [Pug92] to simplify \nconjunctions of constraints and to prove that they have no solutions. The Omega test always performs \nthe exact simplification. Another useful operation performed by the Omega test that we use is projection. \nProjection of a conjunct P(x, y) on variables x is TX(P(X, y)) = 7ryy(P(x, y)) = {x I dy St. P(x, y)}. \nDNF to DNF: V , n. The Omega test works only with separate conjuncts. To allow the use of V and 7 operations, \nwe implemented the DNF package on top of the Omega test. In it we always maintain the dis\u00adjunctive normal \nform of the formula using distributive properties of operations A and V. To avoid combina\u00adtorial explosion \nwhen computing negation we used gist operation in a way proposed in [PW93a]. Lexicographical maximum: \nRel Max<. The func\u00adtion RelMaxl< (see Appendix Al) computes the lex\u00adicographical maximum of the set of \nvectors w which is described by DNF P(W, r), where r is a vector of parameter variables. The function \nproduces the DNF that binds maximized w with r: P~(v, r) = Rel Maxi< (w I p(w, r)). It is defined as \nVv, r : Pm(v, r) &#38; v = m<u(w Ip(w, r)). The version of this function for a single conjunct is called \nProblem Max<. The function Rel Max2< (see Appendix A.2) computes the lexicographical maximum of two parametrized \nsource functions. Given the source func\u00adtions RI = {Wl[wl, s] a R[r, s] \\ C1(W1, r, s)} and R2 = {W2 \n[w2, S] + R[r, s] I C2(W2, r, s)} it produces the relation &#38; = RelMax2< (Rl, R2) which is defined \nm Vw,r,s :(W[w, s]+ R[r, s])E&#38; w W[w, s] = mq(R;l(r, s), R;l(r, s)) Related work. Feautrier developed \nthe PIP al\u00adgorithm (an integer version of simplex algorithm) [Fea88a] to compute an equivalent of ProblemMax<. \nWe are not aware of any performance figures for the PIP, and Figure 8 suggests that it is slow. His analogue \nof Rel M ax2< does not simplify the resulting quads, so they may become very big. To simplify quasts \none needs to perform negation and it is not mentioned in Feautrier s papers as far as we know. Pugh and \nWonnacott in [PW93a] advocate the use of the Presburger arithmetic subclass for dependence testing. We \nthink that their subclass is equivalent to the CISSSof formulas that can be built using operations listed \nin this section.  4 Lazy dependence analysis In Figure 7 we present the algorithm which com\u00adputes value-based \ndependence for a given read refer\u00adence. Dependence graph for the whole program is built by applying the \nalgorithm to every read reference of ev\u00adery statement. Our algorithm can be viewed as a lazy implementa\u00adtion \nof the definition (4). Let s consider a set of read statements inst antes R[r, s] for which we are comput\u00ading \nsource function. The set of all candidate write in\u00adstances {W[w, s] \\ W[w, s] << R[r, s] } is broken \ninto n convex subsets ui(r, s) such that for any r,s such that R[r, s] is executed These subsets are \ncreated on the fly aa me move from the write instances W[w, s] that are lexicographically close to the \nread instances R[r,s] to the more distant write inst antes (lines 10 12 and 30 44 of the algo\u00adrithm). \n 316 1: INPUT: R.A: read reference surrounded by n loops with variables r = (rl,.., rn). s is a vector \nof symbolic constants. 2: OUTPUT: Dependence relation for the read reference R.A. That is, {W[v, s] ~ \nR[r, s]} E DepRel * W[v, s] = m-( W[w, s] I w c W,s]Ar G[R, s]A Arr(W.B) = Arr(R.A) A W .B(W, s) = R. \nA(r, s) A W[w, s] < R[r, s] ) 4: Relation DepRel := {0}; Relation Wrlfaz 5: Dnf NotCovered(r, s) := IsExecuted(R[r, \ns]) 6: Integer FixLoops := n 7: Statement W:= R 8: Bookmn Single Write := True; Boolean LessFlag := \nFalse 10:While (A oiCovered is feasible) do 11: W := statement preceding statement W 12: Statement W \nis surrounded by m loops with variables w = (WI, .... w~) 13: (* Here unfixed zone consists of loops \nwith depths from FizLoops + 1 to n. *) 15: If ( W is assignment statement and it writes to Arr(R.A)) \nthen 16: (* Find source function for instances of reference R.A[r, s] which are IVotCovered(r, s) *) \n17: Dnf SameCeii(w, r,s) := NotCovered(r, s) A R. A(r, s) = W. B(W, s) A IsExecuted(W[w, s]) 18: Conjunct \nWsub(w, r) := w[l..FixLoops] = r[l. .FixLoops] A (LessFlag * WFi.CLOOpS+I < PFiXLOOpS+I ) 19: Dnf DepProb(w, \nr,s) := SameCell(w, r,s) A Wsub(w, r) 20: Relation Cmaz := RelMaxl<(W[w, s] ~ R. A[r, s] I DepProb(w, \nr, s)) 22: If (Single Write) then 23: DepRel := DepRel u Cmax 24: Not Covered := NotCovered A lrange(Cmax) \n25: Else 26: Wrikfax := RelMax2<( WrMax, Cmax) 27: EndIf 30: ElseIf (statement W is EIIdDO or Do i=) \nthen (* Enter loop body through its end *) 31: If (Single Wriie) then 32: If (statement W is Do i=) then \n 33: FixLoops := FixLoops 1; LessFlag := True 34: W := 13mlDo stint for loop with header W 35: Else \n(* statement W is EndDo *) 36: LessFlag := False 37: EndIf 38: WrhIax := {0}; Single Write := False \n39: SiopLoop := Do i= stint of the loop whose =dDo stint is W 40: ElseIf (m!lingle Write A W = StopLoop) \nthen 41: DepRel := DepRel U WrMax 42: Not Covered := NotCovered A lrange( WrMax) 43: Single Write := \nTrue 44: EndIf 50: ElseIf (statement W is entry to the subroutine) then 51: DepRel := DepRel U {Entry \n~ R.A[r, s] [ NotCovered(r, s)} 52: Break out of While loop 10 55: ElseIf (statement W is EndIf or Else \nor If (. ..) then) then 56: (*Do nothing *) 60: EndIf 61: EndDo 62: Return (DepRe/) Figure 7: Value-based \ndependence analysis algorithm Each of the subsets ~i(r, s) can include instances of one or more write \nstatements., If boolean flag Single Write is True, then current ~i(r, s) includes in\u00adstances of only \none statement W and to get depen\u00addence from W [w,s] to R[r, s] we need simply to com\u00adpute m-of eligible \ninstances of W (lines 20-23). If Single Write is False, then instances of several state\u00adments can be \npresent at ~i and after finding source function for each statement (line 20) we have to com\u00adpute a maximum \nof these source functions (line 26). After examining a statement we move to a preceding statement. If \nwe reach beginning of the loop L which surrounds the read statement R where we have st arted, we move \nto the end of this loop (line 34), unfix the loop L, require to consider the writes only from previous \niterations of L (lines 33 and 18), and enter multiple\u00adwrite-statement zone (line 38). When later we reach \nbeginning of loop L, the multiple-write-statement zone is over and we add lexicographical maximum of \nthe source functions computed in this zone to the resulting dependence relation (line 41). Lines 17 20 \nimplement the value-based dependence definition (4). Line 17 selects writes that are ex\u00adecuted and that \nhit the same memory locations aa reads. Line 18 selects writes which belong to the current ~i. The function \nIs Executed returns conjunct that describes conditions under which the given state\u00adment executes. This \nconjunct consists of conditions imposed on loop variables by loop bounds and IF state\u00adments surrounding \nthe statement. In other words, IsExecuted(R[r, s])% R[r, s] c [R,s]. The subexpression range( Cmaz) in \nlines 24 and 42 describes a set of read instances that haa been covered by writes from the current wi. \nRemaining not covered reads are described by the problem NotCovered which is initialized in line 5 and \nis updated in lines 24 and 42. Termination. Termination of the algorithm is proven trivially. The set \nof vectors specified by the problem Not Covered becomes smaller or remains the same with each iteration \nof the loop 10 61. When Not Covered becomes empty algorithm stops and the resulting relation is returned \n(lines 10 and 62). If this does not happen then we reach the beginning of the program fragment being \nexamined (line 50). We can have some not covered reads left, and we let them to depend on the -En-try \nnode (line 51). Computational complexity. Worst-case computa\u00adtional complexity of the algorithm in the \nnumber of calls to Rel Maxi and Rel Max2 k O(nd), where n is number of statements writing to Arr(.R.A) \nand d is number of loops around statement R. Since usually d <5, we can state that worst-case time complexity \nis O(n). Practical complexity is lower, since usually read inst antes are covered after visiting only \nsmall number of candidate writes. Each call to RelMaxl and RelMax2 k in the worst case NP-complete in \nthe number of integer programming problems to be solved. In practice, however, only small number of problems \nis solved in each call. 4.1 Example of the algorithm work Here we demonstrate how our algorithm computes \nthe source function for statement S1 of subroutine OLDA (see Figure l(a)). First, for each writ~read \npair we summarize all con\u00adstraints on loop variables and symbolic constants ex\u00adcept for ordering constraints: \nCl): sl)+sl[cl:sl+sl IC2:S2-+S1 qw= !7r qw= q. Pw= !lr iW=iv iw=i, iw=i~ l~q, <pr l<qr<pw, pr l~qw<qr<pr \n p. ~np p, ~np Pw, Pr<nP l~i, Smb l~i.~mb l~ir~mb Then we take care about ordering constraints. The \nalgorithm breaks a set of write statement in\u00adstances into a sum of disjoint subsets wz (r), ..., w~~(r) \nsuch that for any r such that R[r] is executed: wn, (r) << ... < ws(r) << R[r]. For the read instances \nSl[rsr,pr, qr, ir]ll~rs, ~nrs A l<q, i, ~ mb these subsets are the following: W2 = S2[TSr, pr, Qr, iW \n]ll~iw<ir Sl[rs,, pr, q,, iW]ll~iW <i. W3 = Sz[rsr, pr, qW,zW ]ll~qW<qr A Sl[rsr, p,, qW,iW]ll<qW<qr \nA W4 = Sz[rsr, pw, qW, iW]11 Sqw Spw<p, A l<i~<rnb SI [r$,, PW, qw, iw ]ll~qw~pw<pr A l<iw<rnb W5 = \nSO[rsr, qw, iw ]ll<qw~np A l<i~<mb WIj = s2[~8w!pw!qWjiW ]\\l~rsw<rsr A l<qv~pw~np A l~iw<mb S1 [rsw, \npw, qw, iw ]ll~rsw<rsr A l~qw~pw~np A l<iw~mb So[~sw, qw, iW]ll<rsW<rs~A l~qW~np A l~i.~mb  Now we \nstart moving from W2 back in space/time keeping track of covered S1 instances. We don t men\u00adtion constraints \non rs for brevity and because this vari\u00adable does not appear in subscript functions. Initially NotCovered(r, \ns) = (l~q, <p. ~np A l~ir <rob). W2: W2 A C I and W2 A C2 have no solutions. So W2 doesn t contribute \nto dependence. W3:Cl AW3is not feasible, but (72 A W3= (1~qw< p~ = q. =pt. S np A 1< L = i, <rob). Computing \nRel Maxl<(S2~W, qW}iW] + S1~r, qr, ir] IC2 AW3) we get S2br, qr-l, ir]+slkr, ~T,irl I 2.<p, =qT<np A \nl<ir<mb Now we cover area 2~p, =qr~np A l~ir~rnb and therefore Not Covered = (%=qr=l AISir S Mb) V (l<qr<pr \nSnp A l<i<~b). W4: (C2 A W4 A No-tCovered) = (1 ~ qW< pW= q, < pr ~ np A 1< iW= ir < rob). Maximum of \nthis is Sz[q,,qr,i,] I l~g, <pr~np A l<ir~rnb. (Cl A U4 A NoM70vered) = (1 ~ qW= q, ~ pW< p,<np A l~iW \n= ir ~ xnb) leading to maximum sl~r ljqr,~r] [ l<qr<pr<np A l<ir<mb. Then we use Rel Max2< to compute \nm-of two source functions (Appendix A.2). The result is S 2~r-ljqrj4] + Slkrjqr,ir] I S<pp~np Aqr=pr-l \nAl<iv~ mb Slpr-l,qr,~r] + sl~r,!lr,~r] I pr~np Al~qr~pr 2Al<&#38; <nib (7) NotCovered = (Pr=qr=l A l<ir<ltlb). \n W5: (CO AU5 ANotCovered) = (q~=q,=p, =1A1 ~ iw = i, < nb). This easily computes to dependence relation \nSo [1, ir] + S1 [1, 1, i,] I 1< ir < mb. Fi\u00adnally Not Covered = False. After W5 step all the read instances \nof S1 are covered and we don t have to compute dependence for WGand any writes which textually precede \nSo. The resulting source function for S1 is given in (2).  5 Non-affine fragments In this section we \npresent our techniques for computing value-based dependence in non-affine program frag\u00adments. What is \na symbolic constant? Variable which is not assigned anywhere within the program fragment that we analyze \nis called symbolic constant. In the pre\u00advious sections we held a traditional point of view that the analyzed \nfragment is the whole body of procedure or function. Now when we want to do a better job of dependence \nanalysis for non-afllne program fragments, we give a dynamic definition of analyzed fragment and symbolic \nconstant. The unfixed zone of depth d around statement S (de\u00adnoted Utd%zed(S, d)) is a loop nest which \nconsists of statements belonging to d innermost loops surrounding S. The fixed zone of depth d around \nstatement S (de\u00adnoted Fixed (S, d)) consists of statements not belonging to UnFixed(S, d). If d = O then \nunfixed zone is empty and everything around S is fixed. A scalar variable v that is last written (defined) \nin the Fized(S, d) is considered to be a symbolic con\u00adstant for the statement S at the depth d (denoted \nv c Sym Const (S, d)). To find the definition for the particular read of scalar variable and to distinguish \nbe\u00adtween different definitions of the same variable we use the Static Single Assignment (SSA) graph of \nthe pro\u00adgram [W0192]. This dynamic definition of symbolic constant is used in our dependence analysis \nalgorithm in the following way. When computing the execution condition for the write statement in line \n17 all the variables v such that v ~ SymConsi (S, Fzz.Loops) are considered to be sym\u00adbolic constants. \nExample: non-fine conditions. Let s consider program fragment in Figure 3(a). Computing the de\u00adpendence \nrelation for the statement S3, we start with both loops i and j fixed: iW = ir A j~ = j,. Therefore unfixed \nzone of S3 is empty and variable z is a sym\u00adbolic constant, After visiting statements S2 and S1 we get \ndependence Sl[i, j]+ Ss[i, j][l<i, j<n A z (8) &#38;[i, j]~&#38;[i, j][l~ijj~n A YZ We discover that \nwe do not have to unfix more loops because these 2 dependence cover all instances of read at S 3: (l<i, \njSn)A(z V-c) = (lsi,jsn). Therefore we have proved that no loop-carried dependence exists from S1 and \nSz to S3. However, dependence relation (8) is affine only if our scope is limited to the body of j loop. \nIf we consider the whole program then dependence relation (8) becomes non-a ffine: This relation can \nnot be represented within our frame\u00adwork which requires all constraints to be affine. More\u00adover, since \nwe do not know which branch of IF state\u00adment SO is taken, we do not know exactly what in\u00adstances of S1 \nand Sz are executed. Computing the upper bound on iteration space. So we expand the actual iteration \nspace to get rid of non-affine constraints, as it was suggested in [Voe92b]. For each non-a ffine expression \nin IF condition we as\u00adsume that it can be both True and False, that is, we replace non-a filne boolean \nterms with True in the posi\u00adtive context (that is, in the conjunction or disjunction), and with False \nin the negative context (that is, in the negation). In the above example the upper bound on iteration \nspace is (Sl[ij j], S2[i, j]) [ 1 < i,j ~ n, and the lower bound is empty. Computing the lower and upper \nbounds on de\u00adpendence. After we expanded the iteration space, we have to expand the set of dependence \nto make them affine too. That is, when dependence relation becomes non-a fiine as more loops are unfixed, \nwe replace newly Our % of f77 -02 Times faster Program Lines f77 -02 [Fea91] [PW93a] algorithm compile \ntime than [Fea91] across 15 200 600 9 7.8 4 62 burg 29 600 5,600 91 82 14 56 relax 13 400 1,700 24 25 \n6 57 gosser 22 700 2,800 62 50 8 43 choles 25 600 2,600 32 32 6 63 lanczos 69 1,700 12,600 119 115 7 \n88 1,119 70 61 1,290 15 446 15 292 5 Figure 8: Timing results (all times are in milliseconds) non-affine \nvariables with either True or False. Follow\u00ading [PW93b], we compute lower and upper bound for each dependence \nrelation: Lower bound on dependence is computed by re\u00adplacing non-affine variables with False in the \npos\u00aditive context (in disjunctions and conjunctions) and with True in the negative context (in nega\u00adtions). \nThat is, we over-constrain dependence to get lower bound. Upper bound is computed by replacing non-a \nffine variables with True in positive context and with False in negative context. That is, we under\u00adconstrain \ndependence to get upper bound. We use lower and upper bound on dependence in the following way: When \nwe have to report non-a ffine dependence, we actually report upper bound on this depen\u00addence. So we add \nminimal number of dependence to make dependence relation affine.  When computing what was covered by \na write statement, we replace non-a fllne dependence with lower bound on it, because we can not be sure \nthat any dependence between lower and upper bound really exist and cover read instances, and we know \nthat dependence in the lower bound definitely ex\u00adist.  We do not compute m-of the relations that contain \nafBne approximations of constraints. It can not be done because we do not know exactly which statement \ninstances described by these ap\u00adproximations are really executed. Instead we as\u00adsume that all statement \ninstances that are de\u00adscribed by the approximated affine constraint are involved in the dependence. Doing \nso, we make dependence relation to bind many write statement inst antes to a read instance (instead of \none). This is inevitable when affine approximations are used and this is the best we can do at the compile \ntime.  For example the upper bound for the dependence Sl[i, j] -+ Sa[i, j] /1< i,j~n relation (9) is: \nTh e Sz[i, j]+ S3[i, j]ll~i, j~n [ lower bound for this relation is empty. Example: non-afflne subscripts. \nThese tech\u00adniques apply equally well to the non-afhe IF condi\u00adtions, loop bounds, and subscript functions. \nComputing dependence for program in Figure 3(b) we start with loops i and j fixed and therefore we have \na problem l<iW=ir, jW=jr~n A Z=Z, where z is a symbolic constant. Simplifying it we get affine depen\u00addence \nrelation: S1 [i, j] ~ S2[i, j] I 1 s i, j ~ n. What s interesting, unfixing loops i and j does not make \nthis dependence non-a ffine because x is not present in the resulting relation when loop j is untixed. \nSo non-affine fragments do not necessarily lead to inexact depen\u00addence relations.  6 How fast is our \nalgorithm We measured time taken by our dependence analysis algorithm to analyze Feautrier s benchmarks \n[Fea91] and some NASA NAS codes. In Figure 8 we compare timing results with time taken ~y: Regular Fortran-77 \ncompiler to compile the pro\u00ad gram [PW93a]. Feautrier s algorithm to compute source functions for the \nprogram [Fea91]. Pugh and Wonnacott techniques to compute memory-based direction vectors and value-based \ndependence relations for the program [PW93a]. Feautrier times were obtained on SUN Spare ELC (SPECint89 \nrating of 18.0). All other measurements were performed on SUN SparcStation IPX (SPECint89 rating of 21.7). \nSince we spend only one month imple\u00admenting our algorithm together with DNF package, we think additional \neffort may improve our algorithm per\u00adformance.  7 Related work We would like to compare our techniques \nto several other approaches to dependence analysis. Memory-based dependence computation. Un\u00adtil recently \nonly techniques for computing memory\u00adbased dependence were considered by most re\u00adsearchers [AK87, WO182, \nMHL91]. The problem SameCeU(w, r,s) defined at line 17 of Figure 7 essen\u00adtially describes a memory-based \ndependence. Since we compute this problem only once for each pair of state ments, we don t take more \ntime to compute memory\u00adbased dependence than existing techniques do. In fact computing value-based dependence \nusing our algorithm can take even less time than computing memory-bssed dependence when full cover is \nfound quickly. Let s consider program in Figure l(b). To compute dependence from S1tj to SIG existing \nsystems have to solve problem with 6 variables, and we know in advance that this dependence does not \nexist (ss value\u00adbased) because statements S1, .... SIA cover S16 com\u00adpletely. So no time is spent disproving \nthis dependence. Feautrier work. Feautrier [Fea91] computes value\u00adbased dependence exactly for what we \ncall afine pro\u00adgram fragments, but his techniques are slow, because while computing dependence using \ndefinition (4), he does not keep track of what read instances were cov\u00adered. So his algorithm always \nhas to call PIP algo\u00adrithm and analogue of RelMax2< nd times, where n is number of candidate writes and \nd is number of loops surrounding read statement, while for us nd is upper bound which practically is \nnever reached. Also Feautrier s algorithm does not handle IF state\u00adments and non-a ffine program fragments. \nVoevodins work. Voevodin &#38; Voevodin [Voe92a, Voe92b] also compute exact value-based dependence for \naffine program fragments and they handle non\u00adaffine program fragments. They use methods that are close \nto that of Feautrier s. So we believe that our al\u00adgorithm should work faster than theirs for the same \nreasons as above. Unfortunately, they do not describe their algorithm in detail and they do not provide \ntim\u00ading results, so it is difficult to compare their algorithm to ours. Maydan, Amarasinghe and Lam work. \nTheir al\u00adgorithm [MAL93, May92] does not apply to the general case of affine program fragment, so they \nuse Feautrier s algorithm for backup. Their algorithm applies only to writes that do not self-interfere \n(that is, there is no out\u00ad put dependence from the write to itself) when unused loop indices are removed. \nOur algorithm is also quick for such writes, because unused loop variables do not add constraints to \nthe problem that we solve, and non-interfering writes usu\u00adally lead to equating write loop variables \nto read loop variables which further simplifies the problem. Also their algorithm does not seem to handle \nnon-ai%e pro\u00adgram fragments. Pugh and Wonnacott work. Pugh and Wonnacott use kill analysis to compute \nexact dependence inform~ tion, that is, they first compute memory-based depen\u00addence and then kill or \nrefine them by techniques orig\u00adinally described in [PW92] and [PW93b]. Since their kill analysis in the \nworst case considers all write-killer\u00adread tm ples, while we in the worst case consider only all write-read \npairs, the kill analysis can be expensive. So they have incorporated our idea of keeping track of the \nread instances that were already covered by an\u00adother dependence under the name of partial covers . They \ncombine the partial cover computation with their traditional kill analysis as described in [PW93a]. However, \ntheir approach is different from ours be\u00adcause they do not use RelMax< functions, instead they use the \nPresburger arithmetic (it can be described as our DNF package minus Rel Max< functions plus V and 3 quantifiers \nthat can appear at any level of the for\u00admula) to perform the kill analysis equivalents of these operations. \nThey also use memory-based dependence to perform some quick kills as it was suggested in the earlier \npaper [PW92], while we do not need them at all, However, if need be, we can compute the memory\u00adbased \ndependence with ease. Both approaches are implemented in the Tiny tool, originally developed by Michael \nWolfe and then consid\u00aderably enhanced in the University of Maryland, College Park, so it is possible \nto compare the timing results (see Figure 8), Handling non-affine constraints. In [Voe92b] and [PW93b] \nthe authors describe their techniques for com\u00ad puting value-based dependence for non-a ffine program \nfragments. Voevodin [Voe92b] suggests that the algorithm graph (that is, iteration space plus dependence) \nfor non\u00adaffine fragment should be extended to become afline, but he does not describe how this is achieved. \nIn [PW93b] the authors propose to compute upper and lower bounds on dependence. However, their tech\u00adniques \ncan not prove that dependence from statement SI to Sz is not carried by loop i in Figure 3(a). A number \nof papers [AK87, HP91, LT88] suggest using symbolically enhanced versions of GCD test and Banerjee s \ninequalities. These techniques work only for memory-based dependence analysis and they are inex\u00ad act \neven in this domain. 8 Source code availability The implementation of our algorithms is integrated into \nthe UMCP version of the Tiny tool for depen\u00addence analysis and program transformations. It is freely \navailable by anonymous FTP from directory publornega on the machine ftp. cs. mnd. edu. 9 Conclusion In \nthis paper we presented the algorithm which com\u00adputes exact value-based (data-flow) dependence for affine \nprogram fragments and good affine approxima\u00adtions of value-based dependence for non-a fiine pro\u00adgram \nfragments. The basic idea of the algorithm to start searching for candidate writes in lexicographically \nclose proxim\u00adity of a read statement for which dependence is be\u00ading computed, and to expand search space \nonly if non\u00adcovered read inst antes remain makes it both efficient and capable of handling non-a ffine \nsubscript functions, loop bounds and conditions without slowing down. It also makes dependence analysis \ninsensitive to the program size. That is, the time spent by our algorithm does not depend on number of \nstatements that write the array in question or on number of loops that sur\u00adround read. The algorithm \ntime depends, however, on how many writes reach the read and on how compli\u00adcated the dependence relation \nis both these charac\u00adteristics are not a function of program size. 10 Acknowledgements My thanks go \nto everyone, who helped me to write this paper. Specials thanks to William Pugh and David Wonnacott from \nthe University of Maryland, in dis\u00adcussions with whom this paper waa born. I also would like to thank \nValentine and Vladimir Voevodin from the Research Computing Center of Moscow State Uni\u00ad versity, whose \nresearch was very inspirational for me. References [AK87] J. R. Allen and K. Kennedy. Automatic transla\u00adtion \nof Fortran programs to vector form. ACM Transactions on Programming Languages and S@ems, 9(4):491-542, \nOctober 1987. [AL93] Saman P. Amarasinghe and Monica S. Lam. Communication optimization and code genera\u00adtion \nfor distributed memory machines. In ACM 9s Coraf. on Programming Language Design and Implementation, \nJune 1993. [B+89] M. Berry et al. The PERFECT Club bench\u00admarks: Effective performance evaluation of su\u00ad \n[B1u92] [EHLP91] [Fea88a] [Fea88b] [Fea91] [Fea92a] [Fea92b] [HP91] [KP93] [LT88] [MAL93] [May92] [MHL91] \npercomputers. International .Jotirnal of Super\u00adcomputing Applications, 3(3):540, March 1989. William \nJoseph Blume. Success And Limita\u00adtions in Automatic Parallelization of the Perfect Benchmarks M Programs. \nPhD thesis, Dept. of Computer Science, U. of fllinois at Urban~ Champaign, 1992. M.S. Thesis. R. Eigenmann, \nJ. Hoeflinger, Z. Li, and D. Padua. Experience in the automatic paral\u00adlelization of 4 Perfect benchmark \nprograms. In Proc. of the #h Workshop on Programming Lm\u00adguages and Compilers for Parallel Computing, \nAugust 1991. P. Feautrier. Parametric integer programming. Operationnelle/Operations Research, 22(3):243\u00ad268, \nSeptember 1988. Paul Feautrier. Array expansion. In ACM Int. C onf. on Supercomputing, St Male, pages \n429 441, 1988. Paul Feautrier. Datafiow analysis of array and scalar references. International Journal \nof Par\u00adallel Programming, 20(1), February 1991. Paul Feautrier. Some efficient solutions to the dine \nscheduling problem, Part I, One\u00addimensional time. Technical Report 92.28, IBP/MASI, April 1992. Paul \nFeautrier. Some efficient solutions to the fine scheduling problem, Part II, Multi\u00addimensional time. \nTechnical Report 92.78, IBP/MASI, Ott 1992. Mohammand Reza Haghighat and Consta\u00adntine D. Polychronopoulos. \nSymbolic dependence analysis for high-performance parallelizing com\u00adpilers. In Advances in Languages \nand Compilers jor Parallel Processing, pages 310-330. The MIT Press, 1991. Wayne Kelly and William Pugh. \nA framework for unifying reordering transformations. Tech\u00adnical Report CS-TR-2995.1, Dept. of Computer \nScience, University of Maryland, College Park, April 1993. A. Lichnewsky and F. Thomasset. Introducing \nsymbolic problem solving techniques in the de\u00adpendence testing phases of a vectorizer. In Pro\u00adceedings \nof the Second International Conference on Supercomputing, St. Malo, France, July 1988. Dror, E. Maydan, \nSaman P. Amarasinghe, and Momca S. Lam. Array data-flow analysis and its use in array privatization. \nIn ACM 9.9 Conf. on Principles of Programming Languages, January 1993. Dror Eliezer Maydan. Accurate \nAnalysis of Ar\u00ad ray References. PhD thesis, Computer Systems Laboratory, Stanford U., September 1992. \nD. E. Maydan, J. L. Hennessy, and M. S. Lam. Efficient and exact data dependence anal\u00adysis. In ACM SIGPLA \nN 91 Conference on Pro\u00ad gramming Language Design and Implementa\u00adtion, pages 1-14, June 1991. [Mur71] \nY. Muraoka. Parallelism Exposure and Exploita\u00adtion in Programs. PhD thesis, Dept. of Com\u00adputer Science, \nUniversity of Illinois at Urbana-Charnpaign, February 1971. [Pug91] William Pugh. Uniform techniques \nfor loop op tirnization. In 1991 International Conference on Supercomputing, pages 341 352, Cologne, \nGer\u00admany, June 1991. [Pug92] William Pugh. The Omega test: a fast and prac\u00adtical integer programming \nalgorithm for depen\u00addence analysis. Communications of the ACM, 8:102 114, August 1992. [PW92] Wdliarn \nPugh and David Wonnacott. Going beyond integer programming with the Omega test to eliminate false data \ndependence. Tech\u00adnical Report CS-TR-2993, Dept. of Computer Science, University of Maryland, College \nPark, December 1992. An earlier version of this paper appeared at the SIGPLAN PLDI 92 conference. [PW93a] \nWilliam Pugh and David Wonnacott. An evrdua\u00adtion of exact methods for analysis of value-baaed array data \ndependence. In Sixth Annual Work\u00adshop on Programming Languages and Compilers for Parallel Computing, \nPortland, OR, August 1993. [PW93b] William Pugh and David Wonnacott. Static analysis of upper and lower \nbounds on depen\u00addence and parallelism. Technical Report CS\u00adTR-2994.2, Dept. of Computer Science, Univer\u00adsity \nof Maryland, College Park, June 1993. [Voe92a] Valentin V. Voevodin. Mathematical founda\u00adtions of parallel \ncomputing. WorM Scientific Se\u00adries in Computer Science, 1992. ~oe92b] Valentin V. Voevodin. Theory and \npractice of sequential programs parallelism investiga\u00adtion. Programmirouaniye (Programming and Computer \nSoftware), March 1992. [W0182] M. J. Wolfe. Optimizing Supercompilers for Supercomputers. PhD thesis, \nDept. of Com\u00adputer Science, University of Illinois at Urban&#38; Champaign, October 1982. [WO192] Michael \nWolfe. Beyond induction variables. In SIGPLAN Conference on Programming Lan\u00adguage Design and Implementation, \nSan Fran\u00adcisco, California, June 1992.  A Appendix: computing lexi\u00adcographical maximum A. 1 max< of \nrelation In Figure 9 we present the algorithm to compute lexico\u00ad graphical maximum of relation which \nis not a function from read to write instances. The core of the algorithm is the function Problem Max< \nwhich finds m-of a single conjunct. The result is a DNF that establishes relation between maximized variables \nw and input pa\u00adrameters r. The number of variables that we have to maximize is m (line 3). The problem \nof maximizing these variables is solved variable by variable. That is, we begin with maximizing lexicographically \nsenior variable WI. When maximum for it is established, it becomes input vari\u00adable and variable W2 is \nmaximized, and so on. Let wl denote the variable that is currently being maximized. To maximize W1, we \nproject out all lexicographically junior variables Wl+l, .,., w~. Then in every resulting convex problem \nwe examine constraints on WI. If W1is fixed by equahty constraint involving only in\u00adput variables and \nWI itself, then for every value of input parameters only one value of wl is defined. Therefore this value \nis the maximal value of WI (lines 11-12). If variable wl is not fixed by equalities, then we con\u00adsider \ninequalities that provide upper bound for WI (line 14). For every upper bound we generate a problem in \nwhich s operator is replaced with =. This problem describes conditions under which this upper bound is \nreached, so we add the original problem to it and send it to the output list (line 18). The upper bound \non WI expressed as awl ~ F(...) is converted in line 18 to the maximum for wl which is [F(. . .)/aJ. \nSince we can not represent the integer division in our framework, we use wild-card variable a ifa#l:aw~+ \na= F(. ..) AO~a~l l. Example of the algorithm work. Here we demon\u00ad strate how our algorithm computes \nthe result of (5): m~((i,j)10~i~it4 A OsjsIV A 2%j=k). Parameters of the algorithm are: w = (i, j), m \n=2, r=(kf, IV, k), n=3, p=(O~i~MAO ~j~~A2~=k). To get upper bounds on i we project away j and find two \nupper bounds on i: 0< i~M k N~ 2i Sk{ Replacing i ~ M with i= M, simplifying and adding the original \nproblem we get the problem that describes when upper bound i ~ M. is reached: pl=(2M~k ~2M+NAO~M. Ai=MA \nj=k-2M) Then to find conditions under which another upper bound on i is a maximum, we replace inequality \n2i < k with 2i-1-cI = k A O~ a <1. Simplifying, we get: p2=(k l<2i~k A k N<2i A O~i<M A j=k 2i) So after \nthe first iteration of the loop 1 (lines 5 26) we have the list OutMaz that consists of two conjuncts \n(P1 and pz) that describe maximal values of i. Relation RelMaxl<(W[w, s] -+ R[r, s] ] p(w, r, s)) Begin \nRelation MazRei = {0} For (cp(w, r,s) in conjuncts of p(w, r, s)) do MaxRel = MaxRel U {W[w, s] + R[r, \ns] I ProblemMax<(w I cp(w, r, s))} EndDo Return (MazRei) Dnf ProblemMax<(w I p(w, r)) Begin Result is \nVv, r : OutMaz(v, r) + v = m~(w I p(w, r)) 3: Integer m := Iwl, n := Irl 4: Dnf lnMaz(w, r), OutMaz(w, \nr) := p 5: Forl:=l tomdo 6: InMax (w, r) := OutMax; OutMaz(w, r) := False 8: For ( C1nMaz(w, r) in conjuncts \nof lnMaz) do 9: Dnf pl(w[l..l], r) := rwIlq,r(ClnMaz) 10: For (cpl(w[l..l], r) in conjuncts of pl) do \n 11: If (cpl contains equality involving Wz and not involving wild-card variables) then 12: OutMax := \nOutMax V (cpl A CInMax) 13: Else 14: Let s represent Cpl as a conjunction of Nub upper bounds on WI \nand everything else: Nub Cpl = Cpother A ~(ai~wl < ci+~aijwj +~bijrj) Wk. ail >0 i=l jz2 jel 17: For \ni := 1to Nub do 18: OutMax := OutMax V (CInMax A CpO~her A mn UilWJ+ ~i=Ci+ .ij Wj + bijrj A O<~i <.il \n1) zx j=2 jcl 21: EndDo 22: If (Nub = O) then OutMax := OutMax V W1 = m 23: EndIf 24: EndDo 25: EndDo \n26: EndDo Return ( OutMaz) Figure 9: Lexicographical maximum of parametrized set of vectors On the 2nd \niteration of loop 1 we maximize variable j considering i as an input variable. Both in pl and p2 the \nvariable j is tlxed by equality, so these conjuncts go directly to the resulting DNF. Finally we obtain \nthe dependence relation (6). A.2 max< of two source functions In Figure 10 we present the algorithm \nRel N!ax2< to compute the lexicographical maximum of two source functions represented as dependence relations. \nWe consider every possible pair of conjuncts Sll c L1 and S12 G L2. If ranges of these conjuncts intersect \nthen we call function Rel MaxVar< to compute the lexi\u00adcographical maximum in the intersection area and \nadd it to the resulting relation MaxRe/. Then range of the intersection is subtracted from both relations \nand the process is repeated. Finally one of the relations becomes empty or the relation ranges do not \nintersect anymore. We add what is left of relations to the result, because the relation that does not \nprovide value for particular read variables value is always lexicographically less than the relations \nthat provides the value. We call the function Rel MaxVar< to compute the maximum of the two simple relations \nover the intersec\u00adtion of their ranges 2. When computing maximum of the two simple rela\u00adtions we start \nwith comparing lexicographically senior write variables WI [1] and W2[1]. We build a conjunct pd that \nlets us know the sign of A = WI [1] W2 [I]. In the line 25 we compute constraints on variables r,s under \nwhich A >0 and therefore L1 >> L2, then in line 26 constraints under which A <0 and L1 << L2. Then if \nfor some values of r,s we have A = O, we zThe d~~tin of so~ce fmction is equal to the range of the relation \nthat represents this function. Relation RelMax2< (Relation LI, Relation L2) Begin Result is Vw, r,s \n: (W[w, s] ~ R[r, s]) G RelMax2<(Ll, Lz) + (l? [w,s] + R[r, s]) E (Ll /=range(L2) u L2 /-range(LI)) V \nW[w, s] = mq(L; (r, s), L~ (r, s)) 1: Relation MaxRel := {0} 2: For (sil = {lVI[w,s] ~ R[r, s] I pl(w, \nr, s)} in simple relations of Ll) do 3: For (siz = {Wz[w, s] ~ R[r, s] IPZ(W, r, s)} in simple relations \nof L2) do 4: Relation Cmax = RelMaxVar< (sll, S/z, 1, (number of loops surrounding both W1 and Wz)) 5: \nIf (Cmaz # {0}) then 6: MaxRel := MaxRel U Cmax 7: L1 := L1 n -range(Cmaz); L2 := L2 n =range(Cmaz) 8: \nStart loop 2 from the beginning 9: EndIf 10: EndDo 11: EndDo 12: Return (MazReZ U LI U L2) Relation RelMaxVar<( \nSimple relation SII = {Wl [wl, s] ~ I?[r,s] I Cl(wl, r, s)}, Simple relation SIZ = {WZ[W2, s] ~ R[r, \ns] I C 2(W2, r, s)}, int level, int mazLeuel) Begin (* Compare the variables wl[levell and w2[ievefl \nassuming wl[l..ievei-l] = wz[l..leve~l] *) 21: Relation MazRel := {0} 22: If (levei > mazLeveJ) Return \n(If WI > W2 then sll Else S12 EndIf) 23: Dnf pd(r, s, Aw) := r7wl,W, (C1(wl, r,s) A Cz(wz, r,s) A Aw \n= wl[leveZl wz[level]) 24: If (pd = False) Return({O}) 25: MaxRel := MaxRel U {WI [WI,s] ~ R[r,s] I \nCI(WI, r,s) A Ta&#38;J (pd A Aw > O)} 26: MaxRel := MaxRel U {Wz[wz, s] ~ R[r, s] I CZ(W2, r,s) A 7aAw(pd \nA Aw < O)} 27: MaxRel := MazRel U RelMaxVar<( {Wl[wl, s] + R[r,s] I Cl(wl,r, s) A n_Aw(pd A Aw = O)}, \n{Wz[wz, s] -R[r, s] I Cz(wz, r,s) A T_Aw(pd A Aw = O)}, /eve/+1, mazLeve~) 30: Return (MazRel) Figure \n10: Lexicographical maximum of two parametrized source functions can not decide at this level which source \nfunction pr~ duces greater value (line 27). So we compare the vari\u00adables W1 [2] and wz [2] by recursively \ncalling the func\u00adtion Rel MaxVar<. The level of the variable that we cur\u00adrently compare is stored in \nthe variable level. Finally, if wl[l.. mazLevelJ = w2[l.. maxLevefl, then the lexi\u00adcal ordering of the \nstatements is used to decide which source function is lexicographically greater (line 22). Example of \nthe algorithm work. When comput\u00ading source function (7) we call the function Rel M ax2< with the following \narguments: L1 = {Sl~~l,~~I,iwJ ~ Slkr, ~r>ir] I 6 1 =(pWl=pr l A q~l=qr A iwJl=ir A l~q, <p. <np A l<i~<rnb)}, \nL2 D {Sz~wz,qwZ,LZ] ~ Slbrj %,ir] I c2=(pw2=qw2=qr A iw2=ir A l<qr<p.<np A l<i<mb)} Since range(Ll) \n= range(Lz), we execute only one call RelMaxVar<(L1, Lz, 1, 3). In RelMaxVar< we start with comparing \nwrite vari\u00ad ables pw 1 and pW2. We form the conjunct pd= (Aw =pwz JIwI A c1 A 6 2), (lo) project away \nall write variables ({p, q, i}~{1,2]), and using the Omega test find that A w s O. Adding to (10) the \ninequality Aw <0 and simplify\u00ading we find that L1 is greater than Lz if l~q,~pr 2Apr~npAl~ir~ mb. (11) \nAfter this we add inequality Aw = O to (10). Sim\u00adplifying, we get that pm 1 = PWZ if qr=Pr lA2~p,~npAl~irsmb. \n(12) Executing the recursive call to the RelMaxVar< we find that qw 1 = qw2 and therefore it s not clear \nyet which source function is greater. Going down one more level we get that iW1 = iw z. Being still undecided, \nwe go one more level down and find that there are no more loop variables to compare. The source function \nL2 is then declared to be a maximum when (12) holds because S2>S1.   \n\t\t\t", "proc_id": "174675", "abstract": "<p>Automatic parallelization of real FORTRAN programs does not live up to users expectations yet, and dependence analysis algorithms which either produce too many false dependences or are too slow to contribute significantly to this. In this paper we introduce dataflow dependence analysis algorithm which exactly computes value-based dependence relations for program fragments in which all subscripts, loop bound and IF conditions are affine. Our algorithm also computes good affine approximations of dependence relations for non-affine program fragments. Actually, we do not know about any other algorithm which can compute better approximations.</p><p>And our algorithm is efficient too, because it is lazy. When searching for write statements that supply values used by a given read statement, it starts with statements which are lexicographically close to the read statement in iteration space. Then if some of the read statement instances are not &#8220;satisfied&#8221; with these close writes, the algorithm broadens its search scope by looking into more distant writes. The search scope keeps broadening until all read instances are satisfied or no write candidates are left.</p><p>We timed our algorithm on several benchmark programs and the timing results suggest that our algorithm is fast enough to be used in commercial compilers&#8212;it usually takes 5 to 15 percent of f77 -02 compilation time to analyze a program. Most programs in the 100-line range take less than 1 second to analyze on a SUN SparcStation IPX.</p>", "authors": [{"name": "Vadim Maslov", "author_profile_id": "81100602451", "affiliation": "Department of Computer Science, University of Maryland, College Park, MD", "person_id": "PP31049468", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/174675.177911", "year": "1994", "article_id": "177911", "conference": "POPL", "title": "Lazy array data-flow dependence analysis", "url": "http://dl.acm.org/citation.cfm?id=177911"}