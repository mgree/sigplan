{"article_publication_date": "02-01-1994", "fulltext": "\n Value Dependence Graphs: Representation Without Taxation Daniel Weise Roger F. Crew Michael Ernst Bjarne \nSteensgaard Microsoft Research* Abstract The value dependence graph (VDG) is a sparse dataflow\u00adlike representation \nthat simplifies program analysis and transformation. It is a functional representation that represents \ncontrol flow as data flow and makes explicit all machine quantities, such ss stores and 1/0 channels. \nWe are developing a compiler that builds a VDG repre\u00adsenting a program, analyzes and transforms the VDG, \nthen produces a control flow graph (CFG) [ASU86] from the optimized VDG. This framework simplifies transformations \nand improves upon several published results. For example, it enables more powerful code motion than [CLZ86, \nFOW87], eliminates as many re\u00addundancies as [AWZ88, RWZ88] (except for redundant loops), and provides \nimportant information to the code scheduler [BR9 1]. We exhibit a fast, one-pass method for elimination \nof partial redundancies that never per\u00adforms redundant code motion [KRS92, DS93] and is sim\u00adpler than \nthe classical [MR79, Dha91] or SSA [RWZ88] methods. These results accrue from eliminating the CFG from \nthe analysis/transformation phases and U5 ing demand dependence in preference to control depen\u00addence. \n Introduction Program analysis is a prerequisite for important pro\u00adgram transformations performed by \ncompilers. The classical program representation for analysis is the con\u00adtrol flow graph (CFG). CFG-based \nanalyses and trans\u00adformations suffer from two burdens: they must main\u00adtain CFG invariants to produce \na semantically equiva\u00ad *Authore address: One Microsoft Way, Redmond, WA 98052. Email: {dueise,rfc,rnernst, \nrusa}Qresearch .microsoft. corn. Permission to copy without fee all or pert of this material is granted \nprovided that the copiee are not made or distributed for direct commercial advantage, the ACM copyright \nnotica and tha titla vf the publketion end its data appeor, end nvticis i= given thet copying ia by perrniesion \nof the Association for Computing Machinery. To copy otherwisa, or to republish, raquiras a faa andlor \nepecific permission. POPL 94-1194, Portland Oragon,USA @ 1994 ACM o-89791+38-91941001 ..$3.5o lent CFG \nafter each code motion or other transforma\u00adtion, and they are sensitive to the names given to val\u00adues. \nCFGS overspecify a computation by totally order\u00ading its operations, requiring the addition of extra nodes \n(such as entry pads and exit pads [RWZ88]) to a CFG before performing code motion. CFGS are statement\u00adbased \nand name all values; the names, usually provided by the programmer, get in the way of analyzing the un\u00adderlying \ncomputation. During the last decade, new program representations have been proposed to alleviate these \nproblems and make analysis simpler, faster, or more thorough. Static single assignment (SSA) form [CFR+89] \ngives each value a distinct name, which improves the efficiency of constant propagation and other analyses \n[WZ85, AWZ88, RWZ88, WZ89]. The dependence flow graph (DFG) improves the efficiency of analyses by ignor\u00ading \nirrelevant sections of code and linking definitions to uses [JP93]. These representations can be viewed \nas augmentations to the CFG that allow more rapid traver\u00adsal of the CFG. The program dependence graph \n(PDG) eliminates some control artifacts by linking together op erations with the same control dependence, \nand builds local data dependence graphs to simplify analysis and transformation [Ott78, FOW87]. The program \ndepen\u00addence web (PDW) [BM090, CKB93] makes value flow in the PDG more explicit by using gated single \nassign\u00adment (GSA) form. The next step in solving the problems of CFGS is to eliminate the CFG as the \nbasis of analysis and trans\u00ad formation. We represent a computation as a value de\u00ad pendence graph (VDG), \nwhich specifies only the value flow through a computation. The VDG does not depend upon information \nabout values names, the locations in which they reside, or when they are computed. The VDG is a parallel \nrepresentation that specifies a par\u00adtial order on the operations in the computation based solely upon \ndata dependence, rather than some arbi\u00adtrary total order specified by a CFG, VDG seman\u00adtics are demand-based; \na value is computed only if it is needed by another computation. Evaluation of the VDG may terminate \neven if the original program would Meke tJPoG OIXimizalion dPOG 4 I ~. I1 L1 Figure 1: The stages in \nthe translation from the initial con\u00adtrol flow graph (CFG) to the final CFG. The parser creates the initial \nCFG, and the back end generates machine code from the final CFG. not, because CFG values that are not \ndemanded do not appear in the VDG. Code generation from CFGS is straightforward, but the CFG s limitations \nthe requirement to maintain a total ordering on program statements and the CFG S statement-based nature \ncomplicate analyses and transformations. We claim that is the wrong trade\u00adoff, since an optimizing compiler \ncontains many analy\u00adses and transformations but only one conversion from its intermediate representation \ninto (say) CFG form in the back end. Analysis and transformation is simpler to implement, understand, \nand express formally, and fre\u00adquently faster, when using a VDG rather than a CFG. While simple-minded \ncode generation from the VDG may result in poor code, the representation provides im\u00adportant information \nto the code scheduler [BR91] which makes code movement and scheduling considerably eas\u00adier. We believe \nthe code generator is the right place for the complexity of generating sequential code, rather than taxing \nevery analysis and transformation with the burden of maintaining structures that are irrelevant to the \noperation being applied. Figure 1 shows the steps in the translation from CFG to VDG form and then, after \nanalysis and optimization, back to CFG form. We use the example program of Figure 2 to illustrate the \nresulting transformations. Fig\u00adures 3 5 show these transformations in the CFG frame\u00adwork, and Figures \n6, 8, and 9 show intermediate steps in the translation from CFG to VDG, which is discussed in detail \nlater. Optimization that can be performed independently of the final order of evaluation are performed \non the VDG. For example, global constant and copy propaga\u00adtion, constant folding, global name-insensitive \ncommon subexpression elimination, and dead code elimination are performed directly on the VDG. Code motion \nop\u00adtimization are decided when the demand dependence graph is constructed from the VDG. A demand de\u00adpendence \ngraph is similar in spirit to a control depen\u00addence graph (CDG) [FOW87], except that the demand dependence \ngraph is constructed using the predicates that lead to a computation contributing to the output of program, \nwhereas the predicates in a CDG are those that lead to a computation being performed. Optimiza\u00adint example \n(int a, int b, int c, int d) { int acopy, bcopy, lp_invl, lp_inv2; int dovn, cse, epr, dead; do { bcopy \n= b; lp_inv 1= C+bcopy; lp_inv2 = d -b; a = a * lp_invl; clown =aY, c; dead =a+d; if(a> d){ acopy = a; \na =down +3; cse = acopy << b; 3 else cse = a << bcopy; epr =a <<b; } while (a > cse); return lp_inv2 \n+ epr; 3 Figure 2: Example procedure containing loop invariant computations, common sub expressions, \npartial redundan\u00adcies, and dead computations. Figures 3 5 show the transfor\u00admations performed on this \nprocedure s CFG by translation into VDG form and back or by VDG manipulations. tions that depend upon \nthe final placement of code are performed on the VDG after the demand dependence graph is constructed. \nFigure 4 demonstrates the results of several transformations performed on the VDG. VDGS are the basis \nof powerful, precise, and efFi\u00adcient program slicing algorithms. Efficient algorithms for interprocedural \nslicing, slicing unstructured pro\u00adgrams, and interactive slicing operate directly upon the VDG [Ern93]. \nThe next section of this paper describes the VDG and the translation from CFGS into VDGS (the first three \nsteps of Figure 1). Analyses and transformations are discussed in Section 3. Section 4 discusses one \nas\u00adpect of producing CFGS from VDGS, the construction of the demand-based program dependence graph (dPDG). \n([Ste93a] describes the translation from the dPDG to the final CFG. ) Section 5 shows that our representa\u00adtion \naffords an algorithm for elimination of partial re\u00addundancies (EPR) that is simpler and faster than the \nclassical or SSA formulations. This algorithm uses the dPDG that was constructed for code generation. \nSec\u00adtion 6 compares our research to related work, and the last sect ion discusses future research directions. \n ,...-..................  ~ ------- : --- -- -: # E bcopy =b lp_invl = c + bcopy lp_inv2 =d -b a = \na * lp invl down =a%c dead =a+d  J===#=? ,-------------------------, ----------------, , , /A\\ ii,, \n ,0acopy = a  # L-:-=:2: , , cse =a<<bcopy ,, = down + 3 ,, = acopy << b ,, ,, ,, , ,, ,, ,, !, ,, \n, ;4>-3 ; ,, to ,, ,, 1, , acopy =a cse =a<< bcopym : . = Wcse = acopy << b tO = lp inv2 +epr [ p% ~~~ \ndeallocate local storage return(tO) Figure3: Original CFGforexample(Figure 2). OurCFGs, unlike those \nof [ASU86], explicitly represent branches and joins. Dotted boxes enclose irreducible SESE regions (see \nSection 2.1). 2 Value Dependence Graphs The VDG is a functional representation that expresses the computation \nof a procedure solely as value flow. The lack of control flow haa three immediate consequences. First, \nall machine quantities that are usually implicit in other representations, such aa store contents, stack \nand heap allocators, input and output channels, and C volatile locations, must be explicitly represented \naa value flow to ensure that state-changing operations oc\u00adcur in the correct order. To simplify the exposition, \nwe will consider the machine state to consist solely of a store. Second, operators that choose among \ncontrol paths (e.g., if and switch statements) are represented by selectors that choose among possible \nvalues; these are essentially the y nodes of [BM090]. Third, loop\u00ading is represented via function calls: \nthe CFG backedge finds its analog in the VDG recursive call. A VDG consists of 1. A directed bipartite \ngraph whose vertices are nodes representing computations and ports representing values. The graph arcs \nconnect from nodes to their \\//. ,,/ Figure 4: Transformations on example done by our anal\u00adysis, shown \nin the CFG framework. These transformations are not performed on the CFG, but occur as a result of trans\u00adlations \ninto VDG form and back, or are done on the VDG. Statements in the light font are moved or removed by \nthe transformations; they do not appear in the final CFG (Fig\u00adure 5). Statements in the heavy underlined \nfont do appear in the iinal CFG. The transformations that occur in this example are: 1) Hoisting a loop \ninvariant above the loop; 2) Lowering a loop invariant below the loop; 3) Name in\u00adsensitive global common \nsubexpression elimination; 4) Code motion into the arm of an if statement; 5) Lowering a non\u00adloop invariant \nexpression below (outside) a loop (the value of this expression is not consumed by the loop); 6) Dead \ncode elimination; and 7) Decoupling values from the store. The analysis also reveals that example carI \nbe run in parallel with other functions that do not depend on its result. 4? \\ \\ A <CSA  =-L-T. Falsa \n+ epr =a <<b lp_inv2 =d-b to = lp_inv2 + epr return (tO) Figure 5: T he final CFG of exanmle. which results \nfrom trz&#38;lation of the VDG of Figure 9 into a CFG. operand values (ports) and from ports to the com\u00adputations \n(nodes) producing them as results. Each port is either produced by exactly one node or is a jree value \nnot produced by any node. Nodes are of the following kinds: primitive operation (prirnop) nodes, includ\u00ading \nbasic arithmetic and data structure oper\u00adations, constants, etc. e boolean (or integral) selector (~) \nnodes rep\u00adresenting conditional expressions, which pro\u00adduce a single result from a predicate (integer) \noperand and two (or more) alternative value operands; e closure (A) nodes that produce function val\u00adues; \ne function application (call) nodes taking a function operand and actual parameter operands corresponding \nto the formal parame\u00adters of that function s VDG (see 3 below) and producing result ports corresponding \nto the function s VDG return nodes; * formal parameter nodes with no operands whose result is a parameter \nvalue; and return nodes whose operand is a return value, and which produce no results every VDG contains \nat least one return node, one for each value it returns. All cycles in the graph include at least one \nA node. 2. Indexingsl for the following sets: parameter nodes, return nodes, operand arcs for each node, \nand re\u00adsult arcs for each node. 3. A bodu for each A node, namely, a VDG and a bijec\u00adtion between the \nfree values of that VDG and the operands of the ~ (in practice, we simply identify the free values with \nthe A operands; note that the ~ operands are not its parameters).  Figures 8 and 9 show VDGS. The free \nvalues of a VDG can be viewed as analogous to the free variables in a lambda term. If a given VDG is \nthe body of some ~ node, its free values denote exter\u00adnal values (e.g., external functions, addresses \nof global variables) that all calls of that A node have in common. The toplevel VDG for a complete program, \nwhich takes the form of a function producing a final machine state (in this paper, a store) from an initial \nmachine state, has no free values. While VDGS are slightly larger than CFGS, we do not anticipate that \nthey will prove too large for practi\u00adcal use. Experiments with thinned gated single assign\u00adment (TGSA) \nform [Hav93] indicate that in practice, TGSA can be constructed in time and space linear in the number \nof variable references for programs satis\u00adfying reasonable assumptions. We would be surprised if the \nsame result did not hold for VDG form, which is similar to TGSA form. Sections 2.1 through 2.3 describe \nthe three steps in constructing a procedure s VDG. The first step, SESE/end analysis, constructs a store \ndependence graph (SDG), which is a translation of the CFG into VDG form with basic blocks left uninterpreted? \nReducible un\u00adstructured control flow is modeled via procedure calls. The second step, inlining of nonrecursive \ncalls, intro\u00adduces extra ~ nodes that allow inlining without code duplication. The third step is symbolic \nexecution of the SDG (and each closure therein), which interprets the basic blocks to produce a VDG. \n2.1 Construction of the Store Depen\u00addence Graph A store dependence graph (SDG) is a VDG having the following \nnode kinds in place of primop nodes: e block nodes which produce a next-store from a previous-store operand, \nand lFor an indexing of a set S, a bijection {1, 2,. ... ISI} * S suffices. This establishes the correspondence \nof call operands with A parameters, call results with A returns, selector branches with predicate values, \netc. z The construction of the SDG is an expository fiction. The implementation finds the information \nnecessary to construct the SDG but uses this information to guide symbolic execution rather than to construct \nthe SDG. 300 The problem thus reduces to constructing the SDG I allocate local storaqe I . . . . . . \n. . . . . . fragment corresponding to a b + e traversal of the CFG. bcopy =b lp_invl = c + bcopy lp_inv2 \n=d -b a =a * 1P invl  down =a%c dead =a+d El ---\u00ad a>d :0 ...-.*. ............ T Call 125zlm ,--,---\u00ad \nf? kern-J ................... 11 tO = lp_inv2 + epr deallocate local storage return (tO) Figure 6: SDG \nfor example, which was produced from the C~G of Figure 3. Our SDG and VDG &#38;iagrams suppress the ports, \nsince the computations here are single-valued. Each heavy box denotes a A node, which produces a func\u00adtion \nas its result. Arcs leaving such a box (e.g., the function operand arc from the imer Call node) denote \nthe A s free value operands. Parameter and return nodes are embedded in its upper and lower boundaries \nrespectively. Dotted lines indicate store values; solid lines are scalar or function val\u00adues. . predicate \nnodes which produce a predicate value from a store operand. The initial CFG is assumed to have explicit \njoin nodes and entry and exit nodes. In addition, statements reside in CFG basic block nodes having just \none predecessor and one successor; branches contain only the predicate (switch) value and joins have \nno content whatsoever. Each block or predicate node in the resulting SDG cor\u00adresponds to a particular \nCFG basic block or branch. Given a CFG node b and one of its postdominators e, consider the function \ngiving the store prior to e as a function of the store prior to b. The SDG is a represen\u00adtation of this \nfunction for the case where b is the entry and e is the exit. In this way, control flow is replaced by \n(store) value flow. Figure 6 shows the SDG for example. A basic block is replaced by a corresponding \nbasic block SDG node applied to the store operand. CFG traversal proceeds from the block s successor. \n To translate a branch node, one of its postdomi\u00adnators is designated as the branch-end (see below). \nA sequence of stores is obtained by traversing the CFG from each branch successor in turn to the branch-end, \nthen the stores are combined via a y node. CFG traversal proceeds from the branch-end.  A join indicative \nof looping or unstructured con\u00adtrol (see below) translates to a A node with a Call node for each join \npredecessor. Such ~ nodes are called internal A nodes, to distinguish them from functions explicitly \ndefined by the programmer; the corresponding Calls are eventually converted into gotos by the code generator. \nAs with branch trans\u00adlation, join translation requires the designation of a postdominator, the join-end, \nboth to designate where the CFG traversal continues after the Call and to produce the A body SDG by traversing \nthe CFG from the join successor to the join-end.  The remaining joins (e.g., the lower join in Fig\u00adure \n3) are merely mergings of distinct paths from a branch; no ~ or Call nodes are necessary.  Whether a \nparticular join needs a corresponding internal A and how to designate branch/join-ends are resolved by \nsingle-entry-single-exit (SESE) analy\u00adsis [JP93]. A pair of distinct CFG nodes a, b encloses a SESE region \niff there exist arcs a ending at a and ~ beginning at b such that a dominates /?, /3 postdomi\u00adnates CY,and \na and ~ are cycle-equivalent (i. e., every CFG graph cycle contains both or neither). The SESE region \nitself consists of a, b, and all nodes that are domi\u00adnated by a and postdominated by b. Merging all consec\u00adutive \njoins in the original CFG prevents artifacts result\u00ading from ordering of joins. Unless otherwise specified, \nSESE region refers to a nontrivial SESE region, one which is neither a single basic block nor the composition \nof other SESE regions. A node d is a SESE-bottom if, for some node c, c and d enclose a SESE region. \nFinally, for each CFG node a, let bottom(a) be the postdomina\u00adtor of the smallest SESE region containing \na; bottom(a) can be found for all nodes in O(E) time [JPP93]. A join is symptomatic of looping or unstructured \ncontrol iff it is not a SESE-bottom. End analysis associates with each branch (join) node a branch-end \n(join-end). The analysis depends on whether the node is a SESE-bottom or not. If a branch is a SESE-bottom \nnode, the branch-end is that branch s unique successor that is outside the SESE re\u00adgion. If a join is \na SESEbottom node, it is its own join-end. When determining the end for a non-SESE bottom branch or join \nn, let z be the end for bottom(n). Nodes in contained SESE regions (that is, nodes a for which bottom(a) \n# bottom(n)) are ignored. The branch\u00adend for a non-SESEbottom branch node n is its nearest postdominator \nthat is either a join node or .z. The join\u00adend of a non-SESEbottom join n is its nearest postdom\u00adinator \nthat is either a branch-end or z. End analysis depends only on postdominator trees and takes linear time \n[Har85]. The important property of this construction is that each CFG basic block and predicate appears \nexactly once within the resulting SDG, as a consequence of each CFG node being visited exactly once in \nthe CFG traver\u00adsal. The rules for determining branch/join-ends were chosen to minimize the number of \nCall nodes generated, though there are numerous variations worth exploring. 2.2 The ~-~ Transform and \nInlining Nonrecursive Calls Initially, the SDG models looping and unstructured control via Call and ,4 \nnodes. To simplify analy\u00adsis and transformation, the ~-~ transform consolidates Calls corresponding to \nunstructured control flow, en\u00adsuring that each recursive A haa only one external Call. This permits inlining \nwithout code duplication, after which Call and A only model looping (and programmer\u00adspecified procedures \nand procedure calls). The result of this process is similar to thinned gated single assignment (TGSA) \n[Hav93] except that the y trees take stores, rather than values, aa inputs. (Later passes produce more \nTGSA-like structures in which the y trees operate on values rather than upon stores. ) Before the ~-~ \ntransform, call loops are discovered, which identifies internal As that are loop entry points and identifies \nrecursive Calls, as well as finding loops with multiple entry points. The A-7 transform for a par\u00adticular \ninternal A proceeds in three steps. First, the con\u00adtrol dependence tree is projected to contain only \npaths leading to nonrecursive Calls of the ~. (Symbolic execu\u00adtion tags all operations with their control \ndependence.) Second, a y tree is generated by converting predicates both of whose branches are contained \nin the projection into 7 nodes and omitting other predicates. The final step depends on whether the Ais \ncalled recursively. If the A is called recursively, a new call is created whose in\u00adput is the y tree, \nwith the operands of the original calls at its leaves. This new call replaces all the old non\u00adrecursive \ncalls, so that there is only one non-recursive call to each internal A. If the A is not called recursively, \nthe third step inlines the A body by making consumers of its store input consume the ~ tree instead and \nmaking consumers of the Calls of the A consume the outputs of the A body. The A-7 transform handles irreducible \nloops in the same fashion, -ollecting and treating together all calls into the loop. For example, consider \nthe simple unstructured pro\u00adgram CFG of Figure 7. Its upper join point is not a SESEbottom. The corresponding \nSDG for this CFG contains a ~ and two Calls to that A, one for each possi\u00adble path from the entry of \nthe CFG to the unstructured join point. The ~-~ transform produces a new y node that selects between \nthe stores on the two paths to the join, predicates the -y on the fork that initiates the two separate \npaths, then inlines the body of the A by replac\u00ading its store parameter node by the just constructed \ny node. The ~-~ transform is illustrated by the two SDGS of Figure 7. The 7 nodes introduced by this \npass represent addi\u00ad tional representational overhead of our form over SSA form. Experiments have shown \nthat this overhead is a linear function of the size of programs in SSA form [Hav93]. 2.3 Symbolic Execution \nSymbolic execution expands the SDG S unevaluated ba\u00adsic blocks into VDG nodes. During the expansion, \nglobal value numbering, copy propagation, and constant propagation that can be performed without recourse \nto fixed point analysis are automatically performed. Fig\u00adure 8 shows the VDG that results from symbolically \nexecuting example s SDG. Symbolic execution occurs independently for the body of each A node, each basic \nblock of which is vis\u00adited in turn after all of its predecessors have been pro\u00adcessed. Symbolic execution \nuses a data structure called a symbolic store that maps locations to VDG ports rep\u00adresenting the values \ncurrently known to be stored there. Essentially, the symbolic store represents what is known about the \nstore operand of the basic block SDG node being executed. The first basic block of the SDG is supplied \nwith a symbolic store that contains information about glob\u00adally allocated locations, but not necessarily \nabout their contents? Symbolic execution of a basic block updates the symbolic store, so that it represents \nwhat is known about the result store that gets passed on to the basic block s successors. Symbolic execution \nproduces VDG nodes according to the following rules: Variable Lookup If the variable s location in the \nsym\u00ad bolic store contains a value that was placed there by a previous update (see below), return that \nvalue. If the location contains no explicit value, build a VDG lookup node representing the corresponding \nruntime load operation and return its result port. If the store is actually the result of a 7 node, then \ncreate a 7 node whose then ( else ) operand is 3 [Ste93b] describes our handling of store operations \nwhen the location being looked up or modified is not yet known due to pointer operations. uS1 uc1S1S1 \n--------d-------\u00ad ....... -&#38;-------- Xmgl q , ... ! Y; ?+ S5 L1I -----.,, ,. .. @-- Figure 7: The \nCFG on the left contains unstructured control flow, which results in non-recursive A nodes in its corresponding \nSD-G (middle figure). These non-recursive As prevent accurate analysis because of the lack of ~ nodes \nto indicate within the A which parameter values come from which paths through the program. That is, there \nis no way to track which Call supplied a particular parameter value. We solve this problem by constructing \ny trees that indicate where the inputs come from, and then inlining the A using the -y as its input (right \nillustration). the result of looking up the variable in the then ( else ) store. Constant folding is \nperformed here: if the then and else values are the same port, simply return it. Expression Evaluation \nIf a given expression can be evaluated statically, then do so and return the value; otherwise build a \nVDG node that repre\u00adsents the runtime execution of the expression and return its result port. Nodes are \ncached, so at\u00adtempts to construct a node whose operation and operands match an existing node will merely \nre\u00adtrieve that existing node. Caching, also known as global value numbering [COC70, CS70], implements \ncommon subexpression elimination (CSE) by repre\u00adsenting multiple (possibly identical) source expres\u00adsions \nby a single node. Variable Update Given an assignment statement, update the location in the symbolic \nstore to contain the value (VDG port) obtained by symbolically ex\u00adecuting the expression part of the \nassignment state\u00adment. The updated store is used to symbolically execute the program after the assignment, \nwhich makes the assigned value available to subsequent lookups. Symbolic execution performs no interprocedural \nanal\u00adysis of a Call s effect on, or use of, the store; later passes perform that work. Therefore, symbolic \nexecution of a Call results in a store that has no information about the contents of locations. Lookups \nin this store during symbolic execution create lookup nodes. Additionally, all values must be homed (placed \nin their single globally\u00adknown locations, not in temporary storage) before each call and return. This \nis done by constructing an Update Store node whose arguments are all of the (changed) val\u00adues and (corresponding) \nlocations in the symbolic store. 3 Analysis and Transformation of VDGS The VDG produced by symbolic \nexecution is too coarse for good program analysis because too much of the com\u00adputation depends upon store \noperations, which inhibit (instruction level) parallelism. The first several anal\u00adyses/transformations \nreduce the reliance on the store, making the VDG sparser and more parallel. Then stan\u00addard transformations, \nsuch as dead code elimination, are performed. Transformations for arit y raising, both on parameter and \nreturn values, are also performed. The VDG representation makes many transformations simpler to state \nand to prove correct, and their imple\u00adment ations more efficient. Our formulation does not admit a standard \ndataflow ..................-. -4------ E %+ 1111 m G) rTL-J#.. If vu I 1: y-l; &#38; .....-..... Figure \n8: The VDG of example (Figure 2) after symbolic execution. The dashed circles taking a store operand \nare lookup (load) operations. For clarity, the location argu\u00adments to lookup and Update Store nodes are \nsuppressed, but their values (which are produced by the Allocate node) are indicated by labels on the \nlookup nodes and on the value operand arcs for Update Store nodes. model for proving properties of program \npoints because our model has no program points. Instead, analysis proves invariant about runtime values. \nEach node is given an abstract interpretation, and a fixpoint engine finds a fixpoint of the equations \nspecified by the VDG and the abstract functions. This framework accommo\u00addates both forward flow problems \nand backward flow problems; the latter are all variants of the question, What information does the rest \nof the program de\u00admand? For example, dead update elimination, which eliminates operations on the store, \nis a backwards flow analysis that determines which elements of the store are demanded by the rest of \nthe program. The VDG is called a sparse representation for analysis because in\u00adformation flows directly \nto where it is consumed. Loop Dependence This analysis pass annotates each internal A node with the locations \nthe node either demands or mutates. When the A represents a loop, locations that are both demanded and \nmutated by the loop are called loop de\u00adpendent locations. In the do-while loop of example (Figure 2), \nvariable a is both demanded and changed, b is demanded but not changed, and epr is changed but not demanded. \nThis pass employs a fixed-point finding algorithm to determine this information. In a particular strongly \nconnected component (SCC) of the VDG each A de\u00admands and changes the same locations. Our algorithm performs \nan inside-out traversal of SCCS by visiting in\u00adner SCCS and internal As first, so that inner loops get \nmore detailed information than outer ones. A traversal of a A reveals which locations are de\u00admanded and \nchanged by it. This traversal only needs to follow store arguments, because only operations that perform \nlookups or modifications are of interest, not the values that are retrieved or stored. In Figure 8, only \nthe dotted arcs need be followed, so the traversal is quicker than a full traversal of the VDG. After \nall components of an SCC have been traversed, each A in the SCC is annotated with the demand and modification \ninformation, which later transformation passes depend on. Accounting for the Effects of Calls on the \nStore This analysis/transformation is a constant folding pass whose major goal is to make the program \nas indepen\u00addent of the store as possible by symbolically executing lookup nodes to pull values out of \nthe store. The con\u00adstruction of VDGS left all lookups of the store produced by a Call as nodes in the \ngraph, Likewise, nothing was known about the store parameters of As, which pro\u00adduced many lookup nodes \non an internal A s store pa\u00adrameter. This pass uses the information found by loop down pass through the \nVDG. A node is processed only after all its inputs have been processed. Processing propagates a symbolic \nstore, much like the one used dur\u00ading symbolic execution, through the graph. Processing a Call blanks \nout those locations mutated by the A being called and leaves the others unchanged. Process\u00ading a lookup \nnode attempts a lookup on its incoming symbolic store. If the lookup is successful, then the lookup node \nis replaced by what was found; otherwise, it remains. dependence analysis to approximate the stores on \nentry and exit of each Call. The effects of Calls are accounted for in a single top\u00ad An internal A node \nis processed after its (single) ex\u00adternal Call is processed. Its incoming store is approxi\u00admated by the \nsymbolic store at the Call, less the loop dependent locations of the A. Many lookups are symbol\u00adically \nexecuted, and thereby eliminated. For example, for the VDG for example (Figure 8), this pass replaces \nthe lookups on b, c, and d within the internal A with direct pointers to the inputs of the procedure. \nInput Arity Raising Lookups can be eliminated in A bodies by passing the desired values directly, instead \nof (or in addition to) passing a store parameter. This is profitable if it releases the caller from the \nobligation to update the store before the call. Every A node takes a store as an argument and pro\u00adduces \na store aa a result; internal A nodes have no other arguments or results. The store argument is too coarse; \nonly the locations whose contents are both de\u00admanded (i. e,, used before being changed) and modified \nwithin the ~ (i. e., its loop dependent locations) need be its arguments. All A invocations are sequentialized, \nwhich inhibits transformations, especially code motion. (Alissed variables, the requirement not to reorder \nout\u00adput, and similar constraints may introduce dependence that force such sequentialization.) Arity raising \nof the store argument changes the con\u00adtract between As and their Calls. Those lookup nodes within a A \nthat operate on its incoming store can be replaced by arranging for the Calls of the A to provide those \nvalues ss arguments by executing the lookup prior to calling the A. For example, arity raising is how \nthe lookup of a within the internal A of example is replaced by a new formal parameter. (The store parameter \nnode is then eliminated by dead code removal because after this transformation, it is not accessed within \nthe inter\u00adnal A.) Output Arity Raising Just as an internal A can have its input arity increased, so can \nits output arity be increased. Originally, each A returns a store. External lookup operations then re\u00adtrieve \nvalues from the store. We can move these lookup operations into the A by having the A return the value \nof the lookup ss as additional return value that is used instead of the original lookup. For example, \nthis trans\u00adformation is applied to the epr lookup of the Call of the internal A in (Figure 8), causing \nthat value to be returned by the internal A. (The store return node is then eliminated by dead code removal \nsince, after this transformation, it is dead, i.e., none of the A s Callshave any consumers for their \nstore result ports). Dead Update Elimination Due to arity raising and symbolically evaluated lookups, \nthere will be many dead update operations on stores. Dead update elimination removes these now extraneous \noperations. First, a bottom-up traversal determines at each store mutating site (e.g., update nodes and \nCalls) those locations whose contents are demanded (i. e., may affect the computation s final value) \nafter the store mu\u00adtator. (This is also called live variable analysis.) If the locations that are changed \nby the operation are not de\u00admanded by the rest of the computation, then the opera\u00adtion is deleted, and \nits consumers are changed to accept the store that was the input to the store mutator. It would seem \nthat more direct methods could be used for transforming the VDG, in particular, for de\u00adtermining the \nvalues demanded and modified by loops. Other sparse representations, such ss dependence flow graphs [PBJ+ \n90, JP93] and SSA form, are constructed directly from the CFG. They can collect in one pass the variables \nused or modified within a loop. We do not do so because we are actively extending the system to handle \ndata structures and pointers, which requires a uniform and incremental method for determining the locations \nused or modified within a loop. Incremental Transformations Because the VDG models (only) data dependence \nexplicitly, transformations occur without the classical problem of data flow and control flow information \nget\u00adting out of sync and without the need for a global anal\u00adysis to put them back in sync. For example, \nconsider se\u00adlector deletion, which occurs when analysis determines the value of the predicate of a selector \n(-j) node. We snip out all ~ nodes with that predicate by directly con\u00adnecting their outputs to the relevant \ninputs and tran\u00adsitively rebuilding successor nodes as necessary (which performs common subexpression \nelimination, constant folding, etc.). We safely ignore the issue of whether some variable was defined \non the deleted selector, or some definition was allowed to pass through on the deleted selector, as all \nthe dat aflow ramifications are directly accounted for in the VDG. Nodes that were Ip. IV2 a Y Call . \nI A Figure 9: The VDG for example (Figure 2) after the VDG transformations of Section 3. Dotted lines \nseparate regions of equivalent demand dependence; there are no store values. demand dependent solely \non the predicate (or its nega\u00adtion, whichever is false) are garbage collected without the need to employ \na separate dead code elimination pass, After local changes propagate, a scan determines if any enclosing \nA formals may now be removed, or if return values are no longer computed within the A. If so, the relevant \nCalls are updated, with local transfor\u00admations proceeding outward. The same effect can be achieved with \nPDG methods [FOW87, Section 5.1], but at higher cost in conceptual complexity, overhead, and anal ysis \npasses. Code Generation Generating code from the VDG is done in two stages. The first stage transforms \nthe VDG into a demand\u00adbased progmm dependence graph (dPDG). A standard PDG [FOW87] consists of a data \ndependence graph and a control dependence graph (CDG). In the dPDG the VDG operand arcs provide the data \ndependence and \\ TnJe False T-we Region 70 +3 8 Figure 10: The demand dependence graph constructed from \nthe VDG of example (Figure 9). the CDG is replaced by a demand dependence graph. The second stage transforms \nthe dPDG into a CFG from which code may be generated directly. This stage is described in [Ste93a], which \nextends earlier work on sequentializing PDGs [SF93]. The demand dependence of a VDG node is character\u00adized \nby the ~ nodes encountered on paths from a return node, much as the control dependence of a CFG node \nis characterized by branch nodes encountered on paths from an entry node. That is, any path from a return \nnode to a VDG node yields a sequence of-y selector ports and a corresponding sequence of selector/predicate \nval\u00adues required in order that the given node actually be demanded along that path. Very loosely, node \nn is de\u00admand dependent upon y node g if n dominates, within the VDG, one arm (either True or False) of \ng, but not the other. Space prevents us from giving the formal def\u00adinitions of demand dependence and \nVDG dominance. In example s final VDG (Figure 9), the % and +3 nodes have demand dependence a > d , the \nrecursive Call node has demand dependence a < cse , the a << b node has demand dependence not a < cse \n, and the remaining nodes are always demanded? Figure 10 shows the demand dependence graph for example. \nViewed as predicates, the demand dependence for a computation implies the control dependence: in the \noriginal CFG, a computation may be computed in a program before it is used and the use may occur at a \nprogram point that doesn t post-dominate the compu\u00adtation. For example, the control and demand depen\u00addence \nof the expression dom = a % c in example dif\u00adfer. The control dependence is vacuous (z. e., always 4Since \nthe VDG contains no names, we should really refer to the result port of the > node rather than to some \nparticular expression like a > d . b c abc through the -Y node, without changing the VDG S se- P Y \na OP F Figure 11: Distributing an operation through a Y node. execute ), but the demand dependence is \na > d . The corresponding code motion into an arm of a conditional is called the revival transformation \nin [FKCX94]. The use of the dPDG enables more code motion and other transformations than representations \nbased on control dependence. Motion out of loops occurs auto\u00admatically, since the dPDG reflects only \nwhen a compu\u00adtation is used, not its location in the original program. When the data dependence do not \nenforce a total or\u00addering, the code generator is free to reorder computa\u00adtions and conditional constructs. \nFor example, our sys\u00adtem performs more strict (nonspeculative) code motion than [CLZ86]. In particular, \nfor Figure 2 of that paper, our system would move the a. 1 and a. 5 assignments into the else clause \nof the if movable 1 statement on the right side of the figure; resulting in a clear improve\u00adment in the \ncode. We extend the results of [CLZ86] in other ways: because of our redundancy detection, we find at \nleast as many (structural) common subexpres\u00adsions as their COMMON algorithm finds, and state\u00adments moved \nby our method don t have to have the same values on all iterations (our method handles the problem case \nof [CLZ86, Figure 11]). 5 Elimination of Partial Redun\u00addancies A computation is partially redundant \nat a program point if there is some path to the point that performed the same computation (the redundant \npath) and some path to the program point where the computation is not performed. For example, the second \n<< operation of example (Figure 2) is partially redundant. We will show how to remove partially redundant \ncomputations from a program via transformation. Consider an operation node at least one of whose operands \nis a y node. The operation can be distributed 5 We assume that unmovable expressions are unmovable due \nto loop carried dependence, and not because of semantic issues such as volatile variables in C. mantics, \nby constructing two new operation nodes (Fig\u00adure 11). This transformation is equivalent to converting \naop(p?b: c)intop?aopb:aopc,which is transformation rule 3.2 .a of [SHKN76]. Our contribu\u00adtion is a program \nrepresentation that allows this simple transformation to be the basis of efficient EPR. Distribution \nthrough -ys duplicates code and increases the size of the program (though not the number of op\u00aderations \nperformed at runtime). It removes redundant computations (i. e., is profitable) if either of the new \noperation nodes matches (has the same operator and operands as) a node that it subsumes, or if further \nap placations of the transform match such a node. Node n subsumes node m iff n s demand dependence implies \nm s demand dependence (i. e., if n is demanded, then so is m.) In other words, if n subsumes m, then \nthe com\u00adputation of m will be available (or be costlessly made available) at the computation of n. Our \nEPR algorithm performs only profitable transformations, searching for a subsumed match on a candidate \nnode s ~ tree input (s). When the inputs are y nodes with the same predicate, minor bookkeeping ensures \nthat only possible program paths are considered during the search and transforma\u00adtion. Assume that hashing \n(i. e., matching two nodes) is a constant time operation, that bit-vector operations are also constant \ntime (the bit vector length is the num\u00adber of predicate nodes in a procedure s CFG), and that the subsumption \nrelation can be verified in time linear in the depth of the demand dependence graph, IDDGI. Let Gtree(n) \ndenote the size of the ~ tree rooted at node n. A candidate node with -y tree input g is pro\u00adcessed in \nworst case time O(IDDGI . Gtree(g)), which is equivalent to being quadratic in the size of the proce\u00addure \nbeing optimized. If both of the node s inputs are y trees, the time is no worse than cubic. However, \nempir\u00adical results [CFR+89] indicate that in practice Gtree(g) cent ains at most a few elements, which \nimplies that in practice the depth of the demand dependence graph has a small bound. Because our EPR \nmethod employs only a partial or\u00addering of the computation as expressed by the dPDG, it is much simpler \nthan methods that employ total order\u00adings expressed as a CFG. For example, as compared to the pure CFG \nmethods [MR79, Dha91, KRS92, DS93, DRZ92], it requires no availability or global (partial) an\u00adticipatibility \nanalysis for every expression nor any ba\u00adsic block by basic block code motion. Like more re\u00adcent CFG \nbased EPR methods, it does not perform re\u00addundant code motion. It is also simpler than the SSA method \n[RWZ88] with its CFG graph conditioning (e.g., adding landing pads), assignment of ranks or orders, LCT \nand MCT tables, etc. Related Research Our graph structure is a direct descendant of the graph structure \nused in the Fuse partial evaluator [Wei90, WCRS91]. The graph structure was designed for rea\u00ad soning \nabout and transforming strict functional pro\u00ad grams. The only accommodation made for imperative programs \nis the explicit presence of a store datatype (and other usually implicit machine quantities), and op\u00aderations \non stores. Our transformational engine is an extension of Fuse s, which, because Fuse was a special\u00adize, \nonly did constant folding and function specializa\u00adtion. Because we now use graphs to reason about im\u00adperative \nprograms with unstructured control, the trans\u00adlation from the source to graph form is more complex than \nit was in Fuse. PDG-based compilers simplify transformations by eliminating the CFG, at the cost of needing \nto recon\u00adstruct the CFG to produce serial code [FM85, FMS88, SAF90, SF93, Ste93a]. The underlying representation \nis still statement-oriented, however (control dependence are attached to statements) so the PDG approach \ndoes not enable as many analysis and transformation simpli\u00adfications as the VDG and the dPDG, which also \nelim\u00adinates the CFG but attaches demand dependence to expressions. The program dependence graph (PDG) \n[FOW87] con\u00adsists of a control dependence graph (CDG) and a data dependence graph. The CDG is the novel \ncontribution; it ties together elements of the program that execute under the same control conditions. \nCDGS provide in\u00adformation that enables and simplifies many transforma\u00adtions, such as code motion [CLZ86]. \nThe theta graph [Cli93a, Cli93b] is a PDG for a pro\u00adgram in SSA form [CFR+89]. Click suggests removing \nthe control information from his program representa\u00adtion; if this occurs, the theta graph will probably \nend up very similar to the VDG. The dependence flow graph (DFG) [PBJ+90, JP93] is an extension of the \nstandard SSA form in which, in addition to the 4 nodes inserted at merge points, switch nodes are inserted \nabove branch points. Switches are merge points for backwards program execution, so the DFG makes it as \neasy to perform backwards as forwards anal yses. MIT dataflow graphs [AN90] differ from VDGS by including \ntokens for control and by not needing to rep\u00adresent machine stat e, such as stores. [BJP91] suggests \na dataflow-like intermediate representation that shares many of our goals and concerns. However, it uses \nto\u00adkens to represent control, which keeps explicit control in the program representation. The authors \nof [BJP91] have since abandoned the dataflow model and are now investigating DFGs [J P93]. The gated \nsingle assignment (GSA) component of the program dependence web [BM090, CKB93] is similar to the VDG, \nbut transformation is hindered by the need to keep three different representations in sync, and it fails \nto handle irreducible programs. Thinned gated single assignment (TGSA) form [Hav93] is also similar to \nour represent ation. The major difference between the VDG and TGSA form is that VDGS represent looping \nvia procedure call and return, whereas TGSA, like PDW form, represents looping with special nodes (v \nand p). [Fie92] presents a framework for reasoning about and partially evaluating programs in graphical \nform, based on the idea of a guarded expression. Its rewrite rules are similar to VDG transformations, \nand similar techniques are used to extract values from stores, converting lookup operations into the \nvalues they would return. This work is similar to [CF89], which discusses the semantics of the program \ndependence graph (PDG) when considered as an executable dat aflow program. Sparse evaluation graphs [CCF91] \nare a program rep\u00adresent ation for efficient solution of dat aflow problems. For a given dataflow problem, \na program is converted to a sparse evaluation graph which is used as the data structure for analysis. \nVDGS can be translated into a similar data structure by removing nodes whose ab\u00adstract function is the \nidentity function, removing y nodes whose true and false inputs are identical, and eliminating (internal) \nA and Call parameters that no longer represent loop dependence. One advantage of constructing sparse \nevaluation graphs from VDGS is the presence of ~ nodes, which allow for conditional analyses (e.g., conditional \nconstant propagation). 7 Future Work We expect the VDG representation to simplify pointer and alias \nanalysis. We approximate the locations to which a pointer may refer by explicitly modeling lo\u00adcations \nas the operation that allocates them. We use mini-stores, which represent portions of the global store \nthat are not aliased to other locations but within which aliasing may occur, in order to minimize false \nde\u00adpendence and maximize parallelism. We eliminate not only the names that values may be bound to, but \nthe locations that those names really denote. Our system will give two expressions the same global value \nnumber even when those expressions have differ\u00ading demand dependence. When this occurs, there may be \nno program point where the value can be computed without unnecessarily y increasing register pressure. \nThe system will be forced to duplicate the computation, which will be no worse than the original program, \nin which the computation appeared multiply. To help the system decide whether a computation with multiple \nuses appeared duplicated in the original program, we plan to keep track of the control dependence of \neach expres\u00adsion, and combine them when global value numbering merges computations. We anticipate that \nwe can con\u00adstruct control dependence on the fly during symbolic execution and store propagation. ~ nodes \nwith the same predicate are either all pro\u00adduced from the same if statement in the program, or from different \nif statements that used the same boolean value aa their predicate. Consider the case where they come \nfrom the same if statement. When they all share the same demand dependence, the code generator pro\u00adduces \nonly one branch operation. However, when they have different demand dependence, then the code gen\u00aderator \nmay decide to place them in different parts of the program, which will require saving the test value, \nand replicating the if statement throughout the program. Doing so may or may not improve the program, \nWe need to address this issue in the back end. Our representation raises many other issues for a code \ngenerator. For example, a computation that is demanded at two sites in the program with different demand \ndependence may be duplicated to reduce reg\u00adister pressure [BCT92]. Similarly, ~ nodes with similar predicates \nbut dissimilar demand dependence force a recompute vs. register pressure vs. code size engi\u00adneering tradeoff. \nWhile these issues are not trivial, they are where they belong, in the back end. Program slicing [Wei84, \nVen91] is determines which elements of a program affect, or can be affected by, a given computation. \nThis analysis can be useful in de\u00adbugging and in distributing computations across pro\u00adcessors. The VDG \nis a particularly convenient repre\u00adsentation for slicing because it is sparse (operands are connected \ndirectly to inputs) and all computation is ex\u00adpressed as value flow; a VDG is effectively a representa\u00adtion \nof the slice for every computation in the program, simultaneously. Other authors have noticed the efficacy \nof the PDG for slicing [HRB90] and addressed the prob\u00adlems of irreducible control flow [BH92]. Our algorithms \nare equally as precise as the best of these, simpler to state, and often more efficient [Ern93]. Because \nthe VDG represents only value flow, we are able to slice only on values, not on particular program points, \nwhich no\u00adtion does not make sense in the VDG framework. This does not appear to be a serious limitation. \n Acknowledgments We thank Ellen Spertus, Erik Ruf, Todd Knoblock, and the referees for their helpful \ncomments. References [AN90] Arvind and R. S. Nikhil. Executing a program on the MIT tagged-token dataflow \narchitecture. IEEE Transactions on Computers, 39(3):300-318, March 1990. [ASU86] Alfred V. Aho, Ravi \nSethi, and Jeffrey D. U1lman. Comp\u00adilers: Principles, Techniques, and Tools. Computer Science Series. \nAddison-Wesley, Reading, Massachusetts, 1986. [AWZ88] Bowen Alpern, Mark N. Wegman, and F. Kenneth Zadeck. \nDetecting equality of variables in programs. In Pro\u00adceedings oj the Fijteenth Annual ACM Sympo~ium on \nPrinci\u00adples oj Programming Languages, pages 1 11. ACM Press, Jan\u00aduary 1988. [BCT92] Preston Briggs, Keith \nD. Cooper, and Linda Torczon. Rematerialization. In Proceedings oj the SIGPLAN 92 Confer\u00adence on Programming \nLanguage Design and Implementation, pages 31 1 321. ACM Press, June 1992. [BH92] Thomas Ball and Susan \nHorwitz. Slicing programs with arbitrary control flow. Technical Report 1128, University of Wisconsin \n Madison, December 21, 1992. [BJP91] Micah Beck, Richard Johnson, and Keshav Pingali. From control flow \nto dataflow. Journal oj Parallel and Dis\u00adtributed Computing, 12:118 129, 1991. [BM090] Robert A. Ballance, \nArthur B. Maccabe, and Karl J. Ottenstein. The program dependence web: a representation supporting control-, \ndata-, and demand-driven interpretation of imperative languages. In Proceedings of the SIGPLA N 9 O Conference \non Programming Language Design and Implemen\u00adtation, pages 257-271. ACM Press, June 1990. [BR91] David \nBernstein and Michael Rodeh. Global instruction scheduling for superscaiar machines. In Proceedings of \nthe SIG-PLAN 91 Conference on Programming Language Design and Implementation, pages 241-255, June 1991. \n[CCF91] Jong-Deok Choi, Ron Cytron, and Jeanne Ferrante. Au\u00adtomatic construction of sparse data flow \nevaluation graphs. In Proceedings oj the Eighteenth Annual ACM Symposium on Principles oj Programming \nLanguages, pages 55-66. ACM Press, January 1991. [CF89] Robert Cartwright and Matthias Felleisen. The \nsemantics of program dependence. In Proceedings oj the SIGPLAN 89 Conference on Programming Language \nDesign and Implemen\u00adtation, pages 13 27, Portland, OR, June 1989. [CFR+89] Ron Cytron, Jeanne Ferrante, \nBarry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. An efficient method of computing static single \nassignment form. In Proceedings oj the Sixteenth Annual ACM Symposium on Principles oj Pro\u00adgramming Languages, \npages 25 35. ACM Press, January 1989. [CKB93] Philip L. Campbell, Ksheerabdhi Krishna, and Robert A. \nBallance. Refining and defining the program de\u00adpendence web. Technical Report CS93-6, University of New \nMexico, Albuquerque, March 1993. [Cli93a] Cliff Click. Combining analyses, combining optimiza\u00adtion. PhD \nthesis proposal, April 1993. [Cli93b] Cliff Click. From quads to graphs: An intermediate rep\u00adresentation \ns journey. Submit ted for publication, October 18, 1993. [CLZ86] Ron Cytron, Andy Lowry, and Kenneth \nZadeck. Code motion of control structures in high-level languages. In Proceed\u00adings oj the Thirteenth \nAnnual ACM Symposium on Principles oj Programming Languages, pages 70 85, January 1986. [COC70] John \nCocke. Global common subexpression elimination. SIGPLAN Notices, 5(7):20 24, July 1970. [CS70] John Cocke \nand Jacob T. Schwartz. Programming lan\u00adguages and their compilers. Technical report, Courant Insti\u00adtute, \nNYU, April 1970. Preliminary notes. [Dha91] D. M. Dhamdhere. Practical adaptation of the global optimization \nalgorithm of Morel and Renvoise. ACM Transact\u00adions on Programming Languages and Systems, 13(2):291 294, \nApril 1991. 309 [DRZ92] Dhananjay M. Dhamdere, Barry K. Rosen, and F. Ken\u00adneth Zadeck. How to analyze \nlarge programs efficiently and informatively. In Proceedings oj the SIGPLA N 92 Conference on Programming \nLanguage Design and Implementation, pages 212 223, San Francisco, California, June 17-19, 1992. ACM Press. \n[DS93] Karl-Heinz Drechsler and Manfred P. Stadel. A varia\u00adtion of Knoop, Riithing, and Steffen s lazy \ncode motion. ACM SIGPLAN Notices, 28(5):29 38, May 1993. [Ern93] Michael Ernst. Program slicing using \nthe value depen\u00addence graph. In preparation, 1993. [F1e92] John Field. A simple rewriting semantics for \nrealistic imperative programs and its application to program analysis (preliminary report). In Proc. \nACM SZGPLAN Workshop on Partial Evaluation and Semantics-Based Program Manipula\u00adtion, pages 98 107, San \nFrancisco, June 1992. Published as Yale University Technical Report YALEU/DCS/RR-909. [FKCX94] Lawrence \nFeigen, David Klappholz, Robert Casazza, and Xing Xue. The revival transformation. In Proceedings oj \nthe Twenty-jirst Annual ACM SIGPLAN-SIGA CT Sympo\u00adsium on Principles oj Programming Languages, Portland, \nOR, January 1994. [FM85] Jeanne Ferrante and Mary Mace. On linearizing parallel code. In Proceedings \noj the Tweljth Annual ACM Symposium on Principles of Programming Languages, pages 179-190, Jan\u00aduary 1985. \n[FMS88] Jeanne Ferrante, Mary Mace, and Barbara Simon.. Generating sequential code from parallel code. \nIn Proceed\u00adings of the 1988 International Conference on Supercomputing, pages 582 592, June 1988. [FOW87] \nJeanne Ferrante, Karl J. Ottenstein, and Joe D. War\u00adren. The program dependence graph and its use in \noptimiza\u00adtion. ACM Transactions on Programming Languages and Sys\u00adtems, 9(3):319-349, July 1987. [Har85] \nDov Harel. A linear time algorithm for finding domina\u00adtors in flow graphs and related problems. In Proceedings \nof the Seventeenth ACM Symposium on Theory of Computing, pages 185 194, May 1985. [Hav93] Paul Havlak. \nConstruction of thinned gated single\u00adassignment form. Draft Private distribution, February 20, 1993. \n[HRB90] Susan Horwitz, Thomas Reps, and David Binkley. In\u00adterprocedural slicing using dependence graphs. \nACM Trans\u00adactions on Programming Languages and Systems, 12(1):26 60, January 1990. [JP93] Richard Johnson \nand Keshav Pingali. Dependenc~based program analysis. In Proceedings oj the SIGPLAN 93 Confer\u00adence on \nProgramming Language Design and Implementation, pages 78 89, Albuquerque, NM, June 23-25, 1993. ACM Press. \n[JPP93] Richard Johnson, David Pearson, and Keshav Pingali. Finding regions fast: Single entry single \nexit and control re\u00adgions in linear time. Technical Report CTC93TR141, Cornell University, July 1993. \n[KRS92] Jens Knoop, Oliver Ruthing, and Bernhard Steffen. Lazy code motion. In Proceedings oj the SIGPLAN \n92 Confer\u00adence on Programming Language Design and Implementation, pages 224 234, San Francisco, California, \nJune 17-19, 1992. ACM Press. [MR79] Etienne Morel and Claude Renvoise. Global optimization by suppression \nof partial redundancies. Communications of the A CM, 22(2):96-103, February 1979. [0tt78] Karl Joseph \nOttenstein. Data-flow graphs as an inter\u00admediate program jorm. PhD thesis, Purdue University, August \n1978. [PBJ+90] Keshav Pingali, Micah Beck, Richard Johnson, Mayan Moudgill, and Paul Stodghill. Dependence \nflow graphs: An algebraic approach to program dependencies. Technical Report 90-1152, Department of Computer \nScience, Cornell University, Ithaca, NY 14853, September 1990. [RWZ88] Barry K. Rosen, Mark N. Wegman, \nand F. Kenneth Zadeck. Global value numbers and redundant computations. In Proceedings of the Fijteenth \nAnnual ACM Symposium on Prin\u00adciples of Programming Languages, pages 12 27. ACM Press, January 1988. [SAF90] \nBarbara Simon., David Alpern, and Jeanne Ferrante. A foundation for sequentializing parallel code extended \nab\u00adstract. In Proceedings oj the 2nd ACM Symposium on Parallel Algorithms and Architectures, pages 350-359, \n1990. [SF93] Barbara Simon. and Jeanne Ferrante. An efficient algo\u00adrithm for constructing a control flow \ngraph for parallel code. Technical Report TR 03.465, IBM, Santa Teresa Laboratory, San Jose, California, \nFebruary 1993. [SHKN76] T. A. Standish, D. C. Harriman, D. F. Kilber, and J. M. Neighbors. The Irvine \nprogram transformation catalogue. Technical Report 161, University of California at Irvine, De\u00adpartment \nof Information and Computer Science, January 1976. [Ste93a] Bjarne Steensgaard. Sequentializing program \ndepen\u00addence graphs for irreducible programs. Technical Report MSR\u00adTR-93-14, Microsoft Research, Redmond, \nWA, August 1993. [Ste93b] Bjarne Steensgaard. A store algebra for graphical pro\u00adgram representations. \nIn preparation, November 1993. [Ven91] G. A. Venkatesh. The semantic approach to program slic\u00ading. In \nProceedings oj the SIGPLAN 91 Conference on Pro\u00adgramming Language Design and Implementation, pages 107 \n119, Toronto, Ontario, Canada, June 26-28, 1991. [WCRS91] Daniel Weise, Roland Conybeare, Erik Ruf, and \nScott Seligman. Automatic online partial evaluation. In J. Hughes, editor, Functional Programming Languages \nand Computer Ar\u00adchitecture, number 523 in Lecture Notes in Computer Science, pages 165-191, Cambridge, \nMA, August 26-30, 1991. ACM, Springer-Verlag. [Wei84] Mark Weiser. Program slicing. IEEE Transactions \non Sojtware Engineering, SE-10(4):352 357, July 1984. [Wei90] Daniel Weise. Graphs as an intermediate \nrepresentation for partial evaluation. Technical Report CSL-TR-90-421, Stan\u00adford Computer Systems Laboratory, \nStanford, CA, March 1990. [WZ85] Mark N. Wegman and Frank Kenneth Zadeck. Constant propagation with condition \nbranches. In Proceedings oj the Tweljth Annual ACM Symposium on Principles oj Program\u00adming Languages, \npages 291 299, January 1985. [WZ89] Mark N. Wegman and F. Kenneth Zadeck. Constant propagation with conditional \nbranches. Technical Report CS\u00ad89-36, IBM T.J. Watson Research Center, Yorktown Heights, NY, May 1989. \n310   \n\t\t\t", "proc_id": "174675", "abstract": "<p>The <italic>value dependence graph</italic> (VDG) is a sparse dataflow-like representation that simplifies program analysis and transformation. It is a functional representation that represents control flow as data flow and makes explicit all machine quantities, such as stores and I/O channels. We are developing a compiler that builds a VDG representing a program, analyzes and transforms the VDG, then produces a control flow graph (CFG) [ASU86] from the optimized VDG. This framework simplifies transformations and improves upon several published results. For example, it enables more powerful code motion than [CLZ86, FOW87], eliminates as many redundancies as [AWZ88, RWZ88] (except for redundant loops), and provides important information to the code scheduler [BR91]. We exhibit a fast, one-pass method for elimination of partial redundancies that never performs redundant code motion [KFS92, DS93] and is simpler than the classical [MR79, Dha91] or SSA [RWZ88] methods. These results accrue from eliminating the CFG from the analysis/transformation phases and using <italic>demand dependences</italic> in preference to control dependences.</p>", "authors": [{"name": "Daniel Weise", "author_profile_id": "81100605135", "affiliation": "Microsoft Research, One Microsoft Way, Redmond, WA", "person_id": "PP31049559", "email_address": "", "orcid_id": ""}, {"name": "Roger F. Crew", "author_profile_id": "81100204295", "affiliation": "Microsoft Research, One Microsoft Way, Redmond, WA", "person_id": "P247807", "email_address": "", "orcid_id": ""}, {"name": "Michael Ernst", "author_profile_id": "81100204056", "affiliation": "Microsoft Research, One Microsoft Way, Redmond, WA", "person_id": "PP39071495", "email_address": "", "orcid_id": ""}, {"name": "Bjarne Steensgaard", "author_profile_id": "81100440791", "affiliation": "Microsoft Research, One Microsoft Way, Redmond, WA", "person_id": "P30938", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/174675.177907", "year": "1994", "article_id": "177907", "conference": "POPL", "title": "Value dependence graphs: representation without taxation", "url": "http://dl.acm.org/citation.cfm?id=177907"}