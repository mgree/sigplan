{"article_publication_date": "02-01-1994", "fulltext": "\n CHOCOLATE: Calculi of Higher Order Communication and LAmbda TErms (Preliminary Report) Bard Bloom* Cornell \nUniversity, Ithaca, NY bard@cs. cornell. edu October 29, 1993 Abstract We propose a general definition \nof higher-order pro\u00adcess calculi, generalizing CHOCS [Tho89] and related calculi, and investigate its \nbasic properties. We give sufficient conditions under which a calculus is finitely\u00adbranching and effective. \nWe show that a suitable no\u00ad tion of higher-order bisimulation is a congruence for a subclass of higher-order \ncalculi. We illustrate our def\u00ad initions with a sample calculus strictly stronger than CHOCS. Introduction \n Higher-order process calculi provide have captured con\u00adsiderable interest recently. These are calculi \nfor con\u00adcurrency in which the basic elements of the system, processes or channels, are presented as first-class \nob\u00adjects. As in sequential programming, higher-order fea\u00adtures grant programmers a wide variety of programming \nmethods. For example, in first-order calculi (CCS, CSP, ACP, and so forth), servers must use fixed communication \nchannels. Requests come to the server on channel a, replies return on channel b, and woe betide the system \nif anyone else uses these channels. Clients of the server will have to have some kind of mutual exclusion \nor other access control protocol, requiring some careful program\u00adming and introducing more ways to make \nerrors: a bad thing in a system intended for specifications. *Supported by NSF grants CCR-9003441 and \nCCR-9223183. Permission to copv without fee all or part of this material is granted provided that the \ncopies are not made or distributed for direct comm.rci.l edvantege, the ACM copyright notke end the title \nof the publication and ittr data appaar, and notica in given that copying is by parmiaaion of tha Association \nfor Computing Machinery. To copy otherwise, or to rapublish, raquires a faa and/or apacific permission. \nPOPL 94-1/94, Portland Oregon, USA @ 1994 ACM 0-69791 -636-9194~1 ..$3.50 Higher-order calculi such as \nCHOCS [Tho89] and the related calculus of [Hen93], and the 7r-calculus [MPW92] address these issues. \nIn these calculi, pro\u00adcesses and channels may be communicated along chan\u00adnels. Thus, in CHOCS, a client \nof a server could send the server a small process capable of sending messages back to the client, thereby \nreducing and encapsulating the possible misuse of the server. In the n-calculus, the interface is even \ncleaner: the client sends the server a channel to use for the reply. There is a distinction between two \nkinds of higher\u00adorder calculi. Some calculi, CHOCS and its relatives, allow transmission of processes \nalong channels. This is extremely powerful, and the implications of this pro\u00adgramming paradigm remain \nto be fully explored. The other school, the n-calculus and its relatives, simply al\u00adlow transmission \nof channels (or channel names) along channels. Programming in this setting is somewhat harder (e.g., \nthe implementation of the A-calculus in the ~-calculus is rather more subtle than in CHOCS), and the \nclassical operational semantics is considerably trickier. Implementation of ~-calculus-like systems is \nmuch easier; e.g., the communications fragment of Con\u00adcurrent ML [Rep91, Rep89] resembles the r-calculus \nin flavor. In this study, we make preliminary investigations of the general theory of CHOCS-like languages. \n(The 7r\u00adcalculus is rather more delicate, and will be the subject of later studies. ) The operations \nprovided with CHOCS and its relatives are the basic concurrent operations: e.g., a CCS-like parallel \ncomposition plq which allows p and q to either execute independently or communicate. One useful operation \nnot definable in CHOCS is broadcast, ~. This associative, commutative binary op\u00aderation has the property \nthat, if one combines several processes: P$q~r@s if one process, say p, sends a message m on channel \na, then all of q, T, and s that are currently capable of reading from channel a receive m. Unlike anything \npro\u00adgrammable with simple parallel composition, this guar\u00ad antees that all possible interested receivers \nget the mes\u00adsage, simultaneously. Broadcast in various forms is an essential programming operation in \nmany distributed systems [BCG91, BC90]. Another operation which cannot even be simulated easily in CHOCS \nis the LOTOS operation of disabling, [>. The process p [> q behaves like p, until such time as q takes \nits first step; thereafter, p is killed and q executes. This is vital in the higher-order setting: we \nmay receive a process, start to execute it, discover that it is misbehaving, and wish to kill it before \nit can do any further damage. The process a?x. ((z\\{ b}) [> (b?y.0)) receives a process p along a, and \nruns it but provides an off switch, as a signal along channel b will cancel it. Any number of other \noperations have been proposed and used in first-order process calculi. There are syn\u00adchronous products \nwhich run processes in lock-step par\u00ad allel. There are polling operations, which allow one pro\u00adcess to \nask if another is ready to communicate without actually performing the communication. Some opera\u00ad tions \nare simply for programming convenience: sequen\u00adtial composition p; q and while loops are useful pro\u00adgramming \nconstructs which can be simulated to some extent in most process calculi, but might reasonably be made \npart of the language. 1.1 Developing a Metat heory Rather than rebuilding the basic theory whenever a \nnew operation is needed, we investigate the metatheory of CHOCS-like process calculi. In particular, \nwe define a type system and form of rules, called the chocolate rules in honor of their derivation from \nCHOCS. CHOCS and Hennessy s calculus are chocolate, mutatis mutandis, as are their extensions by all \nof the operations mentioned above and much more. Chocolate languages have two kinds of computation. The \nprimary form is process-algebraic: processes pro\u00adduce signals (carrying other processes as data) and \nevolve into other processes, much as in CCS and CHOCS. The other form of computation is reduction in \na typed A-calculus.l For example, a term that is prepared to receive a value is written as a function \ntak\u00ad ing values as arguments. Giving it a value corresponds to function application. This application \nis evaluated, eventually yielding a process capable of engaging in fur\u00ad ther communication. The evaluation \npart of computa\u00ad ti$m ii+ hiddsa insids ths tw~ Qr tkmw rsleyant ruleti in CHOCS; it is made explicit \n(though instantaneous) in 1Typing is not essential to the definition of computation. It w essential to \nmany finiteness and computability theorems, chocolate. Some basic properties hold for all chocolate lan\u00adguages: \nin particular, the evaluation part of computa\u00adtion is strongly normalizing (which is no great surprise, \nas the evaluation part of the calculus is little more than simply typed A-calculus. ) We use this fact \nto give suf\u00adficent conditions on chocolate languages guaranteeing some important properties; e.g., explaining \nwhen the transition relation is computable and finitely branching. We illustrate with NESTLE, an extension \nof CHOCS in\u00adcluding broadcast and channel creation and transmis\u00adsion. We then investigate notions of \nbisimulation. The most natural and useful definition of bisimulation at all types is dangerous; technically, \nit is not a monotone functional and hence does not necessarily have a great\u00adest fixed point. The theory \nmay work out appropriately, but this remains to be shown. Instead, we provide a generalization of Thomsen \ns notion of higher-order bisimulation. Thomsen s notion is not an instance of the natural notion, as \nwe explain in Section 4. Nonetheless, in a restricted second-order case, Thomsen s notion can be generalized \nto the con\u00adcept of white chocolate bisimulation.2 We show that white chocolate bisimulation is a congruence \nfor a re\u00adspectable class of operations, including those of CHOCS and Hennessy s calculus. This suffices \nfor most of the operations we used to motivate this study. Unfortu\u00adnately, it fails for the most interesting \nexample, the channel transmission part of the calculus NESTL15.  1.2 Types and Channels A fully-fledged \nhigher-order process calculus should al\u00adlow the kinds of programming paradigms that make higher-order \nsequential computation powerful: encap\u00adsulating and transmitting programming constructs, such as transmission \nand reception operations. That is, we should be able to transmit channels, rather in the style of the \nx-calculus. Keeping track of types will prove informative. Let P be the type of processes. A receiver \nis a func\u00adtion that takes an input (of type P) and then acts like a process. It is thus an abstraction, \nof type Ab=P~P A sender produces a datum (of type P) as output, and then continues running its main thread \nof communica\u00adtion, That is, it simply is a pair of processes (d, p): d to be used as data, p for computation. \nIt is thus a wmcwtiwi [Mi191]J vf typs CO=PXP It s good, but it s not real chocolate. Most higher-order \nprocess algebras have send and receive operations, written something like a!v.p and a?z.p. We follow \n[Mi191] in considering a! and a? to be the operations. a! takes a value and a process, and returns a \nprocess. a? takes a function from values to processes, returning a process. Thus, we have ~! : Snd = \nCO+P ~? : RCV = Ab+P Given ~ : Rev, we read a value from r and call it v by a construct of the form \nr(Av. U). Given s : Snd, we transmit n along it by s((n, Dl)). In both cases, the l is the continuation: \nthe process to be executed after the communication. A communication channel consists of a way to send, \nand a way to receive: a pair (a!, a?), of matching send\u00ading and receiving capabilities. They thus have \ntype: Chan = Snd x Rcv If, in our process algebra, we are able to communicate values of type Chan, then \nwe can do much ~-calculus\u00adlike programming straightforwardly. For example, con\u00adsider a server: a process \nwhich repeatedly accepts a Chan, viz. a pair (s, r) : Snd x Rev, reads a value v from the channel, performs \nsome computation F, and sends its result F(v) back along the channel. The server should be reentrant: \nonce it has gotten a channel, it can fork off a process r (Av.s(F(v), O)), abbreviated as TV . s(F(v)) \n.0, to handle the request along that chan\u00adnel; the server itself would wait for more requests. Each request \ncan use a different channel. S = a?(s, ~) . (S/(~v . s(F(v)) . O)) (1) Or, without abbreviations: /S= \na?(Ah.sl((nlh) (Av.(nOh)(F(u), O)))) A first draft of a client (sending first the channel b, as the \nvalue (b!, b?); then sending the value 3 coded in some unspecified way, and sending the answer along \nthe channel print) would look like: Co = a!(b!, b?) . b!3 . b?z . print!z .0 To make full use of this, \nwe should have a process which generates new channels on request. Suppose that there are actions co, \ncl, . . . . We d like a process with roughly the following behavior, repeatedly offering new channels \nas values on some fixed channel c: NC(n) = C!(cn!, cm?) . NC (n + 1) (A somewhat more powerful version \nof the new-channel process will be provided as a primitive.) A better client would first get a channel \nfrom NC , and use that channel to communicate with the server. Cl = c?(s, ~) . a!(s, ~) . s3. rz .print!x \n.0 (2) The whole system will simply consist of the server, the client, and the channel server in parallel: \nSystem = (SICllNC(l))\\{a, b,c, cl, cz,. . .}. Note that the c! and c? used in NC and Cl are higher\u00adorder \ncommunication operations, as they are being used to communicate values of type Chan. c? : (Chan + P)+ \nP = ((:: ;:P,)+P)+P This development of channel transmission was quite natural and straightforward. \nIt is somewhat surpris\u00ading that it requires constants of such high types. We are now beyond the scope \nof existing process algebras, even higher-order ones, and must check to see that the foundations are \nsolid. 1.2.1 Typed Transition Relations Hennessy s calculus has two actions a? and a! for each channel, \nand rules a?Az.p Q-Ax.p a!(9, p) ~ (97P) We have transitions p ~ q, where p : P, a is an action, ~ is \na type, and q has type (. (The definitions are sketched in Section 2.4. ) The type information renders \nthe ? and ! redundant. In our system, those transitions are ., Ab aAbAz.p Ax.p aco * (q,p) (97P) Type \ninformation distinguish between sending and re\u00adcieving, so we use the same operation and action sym\u00adbols. \nThis notion of transition is sensible in practice. It has the somewhat disturbing property that it doesn \nt respect types in any sense: a process that is, a closed term of type P can take a-transitions to \nterms of two different types. This presents no difficulties, though it may seem odd. 2 Calculi We generalize \nthe notion of a GSOS language [BIM88, B1089]. GSOS languages are a first-order generalization of the \nrules used to define CCS.3 We add a type system and higher-typed terms. sThe acronym Gsos currently stands \nfor Grand Structmeri Operational Semantics .  2.1 Types and Terms The type system is straight from the \nsimply typed A\u00adcalculus with products and one base type, which we call ~+ x. Types ~ have the form: f::== \nPl(+(ltxt Definition 2.1 A Higher Order GSOS Signature is a set Act of actions, a set of operation symbols, \nand a vector of kf types (&#38;fl, . . . , <fkf ) for each operation symbol f, signifying that f takes \narguments of types tfl,..., tf k, and returns a value of type P. We assume infinitely many variables \nx( of each type. We define terms and their types simultaneously: ~e:[ t:(ox(l ti : &#38;ili 6 {0,1} 7ri(t) \n: &#38; (to, tl) :(0 x &#38; t:(cl-+&#38;l, t :&#38;l t:(l tt : fl Ax~ .t:&#38;J+ &#38; We use a number \nof fairly obvious abbreviations to make processes easier to read. As we are combining process algebra \nand ~+ x, we have in effect two com\u00adputation systems, plus a rule connecting them. 2.2 The languages \nNESTLE and NESTLE/2 Based on the considerations above, we define two higher-order process calculi, NESTLE \nand its second\u00ad order subset NESTLE/2 . NESTLE includes all the oper\u00adations of CHOCS, plus full higher-order \ncommunication, broadcast, disabling, and channel creation; it illustrates all the features of the rules \ngiven in our study. Our ac\u00adtion alphabet Act is infinite; a, b, c range over it. It includes an infinite, \ncoinfinite subset {co, cl, . . .}, and a distinguished symbol ~. The operation symbols are: 0 () Null \nprocess a< : (~) Prefixing at all types, for each action a + : (P, P) Nondeterministic choice I : (P, \nP) Parallel Composition [> : (P, P) Disabling /s: (P) Restriction: forbid actions in S Q Act [R] : (P) \nRenaming a s to R(a) s, where R : Act -+ Act tic : (P, P) Broadcasting on channels in C ~ Act [.1s : \n(P) Delimit broadcasting NCn : () New channel creator: absent from NESTLE/2 We write operations in infix \nin fairly obvious ways. We use the abbreviations a!p.q = a(p, q) and a?x.p = a(Az.p). We have one rule \nfor each choice of a, ~, C , S, and R. The rules for prefixing and choice are quite standard: For parallel \ncomposition, we define ylfx (where y : ( and x : P) to be an abbreviation for the following term, which \nhas the same communication effect as y does, but has z still running in parallel with it. That is, if \ny wants to receive a value (i. e., has type (O + (I), then ylfO+f, Z will receive a value and route it \nto y. Similarly, if y wants to transmit a value (viz, has type to x ,$1), ylf, tIx will transmit the \nsame value Toy, and continue running the rest of y in parallel with x . This is the natural generalization \nof [Hen93]. = y[x Ylpx ((7roy), (my)l(,x ) YlfO fl~ = M .(y.+ cl x ) Ylfo+l$ = Note that ylfx : ~. We \nomit the symmetric rules, and use informal pattern matching notation for clarity. a,t ~ a,f+P , U,( x \nY Y! ~ = (Y(,Y!) a, t Zlx Ylt$ X1X ~ p (YYA)IY; For example, ., P+P (a?x.p)lq A.z. (pzlq) Po = = (O,!s.t)[u \n= (s, tlu) PI then s is sent top as desired in POIpl: = (pslq)l(tlti) Po IP1 Disabling p [> q runs p \nuntil q takes a step, then kills p and runs q. The type extension [>c is defined in the same way as If. \na>( z,( x Y x v X[>x fi y [>~ x Z[>x tiy Restriction and renaming must be extended to higher types in \nmuch the same way that parallel composition is. Let t : f; we define ft(t) as follows: fp(t) = t ffo+ \nt,xzf ffl(tz) (t)= . ftoxf, (~) = (~ot, fcl (ret)) The restriction and renaming rules are fairly stan\u00ad \ndard: x ~yfora$S % ( x 9 x\\s s lJ\\ts x [R] w y[R]&#38; Broadcasting ~ *C x is similar to parallel composi\u00ad \n tion. It has the parallel composition rules for a @ C, plus the following rules and their symmetric \nversions. If z broadcasts y., then z must receive it if it can; either way, z *C z will broadcast y.. \na,gx P x (Y07Y1)> -Y , a,~x XQCX = (Ye>(YI tc Y YO)) a,fx P x (yo,y,), z s x@cx = (YO,Y1 *C ~ ) Note \nt~: a broadcast always emits a value y.. If -r for some a c C , then ~ is always aPQcq pair (TO,T1) \nwhere To was the value broadcast and rl the system after the broadcast occurred. Recall that a, t ift \n u, then t must be a process. We thus need a broadcast-delimiting operation [z1 C which strips off the \nvalue being broadcast and lets the remaining pro\u00adcess continue. a,fx P att x a~C x y, a$C Y! [Zlc -[Zlc, \n([Zlc37r,y a, P x Y [Xlc = [ylc So, the idiom for broadcast is [Ptcq$c tc tcslc much as the idiom \nfor parallel composition with point\u00adto-point communication is (Plql l~ls)\\s. The new channel creator \nNCn simply offers new com\u00admunication channels (on channel co). Let chan(~) = ((+ P) X((XP). c,, Chan(fjXP \nNC. Az.cnf+p(x), AZcn@p (Z)), NCn+l) (( Recursive definition of processes is straightforward as a consequence \nof our general theory: we allow constants defined by an arbitrary set of axioms. For example, the server \nof(1) would be defined as a new constant S, with a single rule s === A(S, T). (SIT(AV.S(F(V), O))). \nNESTLfi is a remarkably powerful calculus, allowing CHOCS-like and x-calculus-like programming. With \na bit of care (having a single NC operator in parallel, and avoiding use of Cn for n > O), programming \nwith dynamically-allocated channels is st~aightforward. The only ~-calculus operation missing from NESTL6 \nis matching: [a = b]q behaves like q if the channel names a and b are the same, and is stopped otherwise. \nThis is programmable (with some extra ~-moves) in NESTLE. Suppose that we have two channel values of \nthe same type <, xl = (sl, T1) and X2 = (sz, ~2), produced by NC. The two are equal if a signal sent \non S1 is received on T2. We use broadcasting to force the communication. Let Of be any closed term of \ntype ~. Choose channels a, by, and b~. The following code will send a signal on bY if Xl = X2, and on \nbn otherwise. Let G = {a, co, cl, . . .}; = (~z?z . bY!O . O) + (a?x . bn!O . O) P1 [(sl!o~ . a!O . O) \n~~pllG\\G P= If xl = X2, this will evolve as follows: p [a!().o Q7Gb,!o.O]G \\G = ((), D) P where D is \nO inside some communication delimiters. (There might be another ~-move at one end or the other. ) The \nuse of $~ forces the communication be\u00adtween S1 and TZ to occur if it is possible. On the other hand, \nif XI # X2, then the left-hand process broadcasts into the void; we have only a single computation: 7, \nP [a!O.O $~ pllG\\GP ., P [0$. bn!O . OIG\\G ~ (0,0) Thus, p can tell if two channels are the same or \nnot. NESTLE/2 is NESTLIi restricted to second-order types: no channel creation, and only tuples of processes \ncan be transmitted. It is strictly between CHOCS and NESTLE in expressive power, and our stronger theory \napplies to it. 2.3 Operational Semantics A chocolate language consists of a signature and some structured \noperational rules defining the behavior of the operators of the signature. The A-calculus fragment is \nstandard. Definition 2.2 Higher Order GSOS rules have the form all, <II bll, <; Z1 21-----4, .... Ihl! \n. ..> (3) f(z)= u There may be infiniteig many antecedents, both positive and negative; u may be any \nterm. The variables xi and yaj must ail be distinct, and the only variables appearing in the term u are \nxi S and yij s. Definition 2.3 A chocolate language L over a signa\u00adture is a set of higher-order GSOS \nrules over that sig\u00adnature. The evaluation rules for ~-~ x are standard, and not left up to the language \ndesigner.4 (Ax.t)u + t[z:==u] T~((to,ti tl))+ t=x tWt tu * t u 7ri(t) + 7r~(t ) The transition relation \na is deterministic. By stan\u00addard theories of typed A-calculus, it is strongly normal\u00adizing; that is, \nthere are no infinite reductions sequences pi)-pl+... Chocolate calculi all use the Instanteous Evaluation \nRule: local computation is instantaneous, and only communication takes time. This is actually moderately \nrealistic, in software systems at least: local processing time are generally insignificant compared to \ncommuni\u00adcation delays. In the full paper we explain why having evaluation steps take ~-moves is undesirable. \nFor example, let p be a process which receives a chan\u00adnel along a, sends some d along that channel, and \nstops; and q be a process which sends a channel b along a, reads a value from b, and thereafter behaves \nlike that value. p = a?(Ac.xo (c)(~, 0)) q = a!((b!, b?), b?(Az.z)) 41t would probably make good senseto \nuse a more powerful calculus, though the finiteness results are unlikely to hold if the base calculus \nis not strongly normalizing. We have transitions: P -Ac.7r(l (c) (d, o) q ~ (@!, b?), b?(Az.x)) and \nhence = ((At.~o(c)(d,O))(b!, b?)) lb?(Ax.x) = p lq Plq Only the evaluation rule applies to p . We have \n((k~o(c)(d, O))(b!, b?)) =+ ~o((b!, b?))(d, O) + b!(d, O) ~ (d, O) , b,Ab which matches q s transition \nq Ax.x to give us a transition P P I!? ~ ol(Xz.x)d That is, p has sent d to q, which is what we intended. \n 2.4 Definition of the Transition Rela\u00adtion A transition p % q occurs in .C iff there is a proof of \nthis formula from the rules of L, plus the evaluation rules and the connection rule. The formal definition \nin the full paper is nontrivial, especially as we must define a>( proofs for as well. following lemma \nshows p  ++The that the definitions fit together right: Lemma 2.4 Let L be any chocolate language, \nand p : P any closed term. Then there is a L-proof of p ~ q for some q ifl the? e is no ~-proof of p \nS . 3 Computability and Finite Branching There are some fundamental concerns about this cal\u00adculus. Clearly, \nas we have allowed arbitrary sets of op\u00aderation symbols, actions, and rules, we cannot expect anything \nto be computable, or even countable. It is of\u00adten desirable to keep the transition relation computable \nand finitely branching. 3.1 Finite Branching It is frequently important to have some kind of finite\u00adness \nand computability properties. We discuss two im\u00adportant finite-branching properties. Suppose that H is \na proof about p. The immediate subproofs of II are either about the arguments of p (if p = .f(IY)), or \nabout a term p with p + p . This suggests the following definition: 344 Definition 3.1 p ~ q i~ either \np ~ q, or p = f(Pl,... ,pn) and q = pi for some i such that pi : P. x is a lousy notion of reduction: \nit is nondetermin\u00adistic, non-confluent (though finitely branching), and not respecting of meaning in \nany sense. The main excuse for = is the following: Lemma 3.2 Let II be a proof about p. If II contains \na subproof about a term p , then p #-o p!. The usual proofs for strong normalization of the sim\u00adply typed \nA-calculus can be adapted to show that ~ is strongly normalizing and finitely branching. Thus, the following \nfunction is well-defined: dur(p) = max {n/dp .p &#38;o p } The heart of the finiteness and computability \nproofs are the following lemma, where depth(lI) is the depth of a proof in the obvious sense. Lemma 3.3 \nLet II be a proof about p. Then depth(II) < dur(p). Definition 3.4 L is image finite if, for all processes \np, actions a, and tppes .$, the set p lp * p } is finite. { L is finitely branching if, for all processes \np, the set {(%4,2$)IPx P } is finite. Image finiteness is the weakest finite-branching prop\u00aderty in \ngeneral use. In many calculi, it is the strongest property that we want. The NESTLE operation N C~ is \nimage-finite, but (as it offers channels of all types) is not finitely branching. Theorem 3.5 Suppose \nthat there are onlg a finite number of rules for f(?) * for each f, a, and ~, and that all rules have \na jinite number of positive an\u00adtecedents. Then .C is image finite. The essence of the following method \nto get finite branching is taken from [Ace92, Vaa93]. Definition 3.6 The trigger of a rule p in the form \n(3) is the vector (S1, . . . , S~), where i= {@@~@+J =ante(~)} Definition 3.7 A chocolate language is \nfinitely trig\u00ad gered if, for each operation sgmbol f and potential trig\u00adger T, there are ordg jiniteiy \nmany rules for f with trig\u00adger T; and each rule has only finitely many positive antecedents. NESTLE \nis not finitely triggered, as there are infinitely many rules for NCn with the empty tuple for a trigger. \nWithout N Cn or even with NC. restricted to a finite set of types it would be finitely triggered. Theorem \n3.8 If L is jinitelp triggered, then it is finitely branching. 3.2 Computability There are several possible \ndefinitions of computability. For this study, we choose one of the strongest: that the computation tree \nof a process is computable. a, P Definition 3.9 Let D(p) = (a, q)lp q . Z is {} strongly computable \niff D(p) is computable in canonical index as a function of p. By insisting that appropriate parts of \nLemma 3.8 be computable, we can give sufficient conditions for strong computability. Lemma 3.10 Let \nL be a chocolate language, such that: 1. The sets of actions, operations, and rules have re\u00ad cursive \ncodings, such that equality is recursive. 2. All rules have finite numbers of positive an\u00adtecedents, \nand recursive sets of negative an\u00ad tecedents; and that the set of antecedents in canon\u00adical index and \na program deciding the negative an\u00adtecedents are computable from the code of rule. 3. For any operation \nsymbol and finite trigger, the set of rules for that symbol and trigger is computable in canonical index. \n  Then L is strongly computable. These conditions are not sufficient, for stupid reasons at least: e,g., \na thoroughly nonrecursive language without constants would have no closed terms, and thus no pro\u00adcesses \nand an easily compatible (viz., empty) transition relation. 4 Bisimulation as a Congru\u00ad ence: White Chocolate \n There are technical obstacles to defining bisimulation for chocolate languages in general. The definition \nthat we would like is De flnMon 4.1 A relation between closed terms of the same type, -, is a V bisimulation \nrelation if it is sgm\u00admetm c and whenever p w p : 345 e If P,P : P, then whenever p-@+ q, there is a \nq vq such thatp ~ q . e Ifp, p : CO ~ &#38;, then forailq w q of type (o, Pq w P (l . (This isakind oflatebisimulation.) \nWe would then define s to be the largest V bisimulation relation. It works for first-order processes; \nthe first-order parts of this definition are monotone, and thus there is indeed a well-defined notion \nof e. However, for higher-order processes, the universal quantifier in the CO+ (I case of the definition \nmakes the definition non-monotone; the more processes of type <O which are related, the harder it is \nto be related at type CO -+ (I. It k an open question whether or not there is a maximum V bisimulation \nrelation. There are several variant definitions, replacing the universal quantifier at functional type \nwith an exis\u00adtential: if p = p : (O + CI, for every q there is a 9 w q such that pq w p q . This definition \nis mono\u00adtone at least, so there is a well-defined &#38; relation, but the weaker definition complicates \nthe analysis. For this study, we restrict attention to a much smaller class of languages, scarcely more \npowerful than CHOCS. Defi\u00adnitions specific to this weaker class are termed white chocolate. We take a \nrestricted type system, the White Choco\u00adlate Type System: (:= PIP IP + P where n ~ O. We do not bother \nto identify P, PI, and PO -+ P. We define the natural generalization of Thomsen s notion of higher-order \nbisimulation, which does not try to relate functions directly; instead, it ap\u00adplies them arguments in \nall ways and relates the result\u00ading processes. Let nf(q) be the =+-normal form of q. Definition 4.2 A \nrelation w between closed terms of type P is a white bisimulation relation iff it is symmet\u00adric, and \nwhenever p -p , we have: p: Ifp* q, then there exists q u q such that a, P P 9 . ., Pk Pk: If p + q, \nthen there exists q such that pk ~ p: ~fp P + q, then for every closed r : Pk, there is a r componentwise \n-to r and a q such P +P that p -q and q? w q r . Processes p and p a? e white bisimilar, p ~Wh?te p , \nif there exists a white bisimulation relation relating them. The definition of white bisimulation is \nmonotone, and thus the usual basic bisimulation theory applies; e.g., ~w~zte is itself a white bisimulation. \nNote that ~~~~tc is a rather peculiar bisimulation. Suppose that we have the following operation, where \nb is a fixed action, but a and ~ ranges over types. That is, f(p) behaves like p if p has a b-transition \nat type P on its first move, and O otherwise. Consider the processes = (a?z.0) + (a?x.x) = a(Ax.0) + \na(Ax.x) P = p+ (a?z.f(x)) = p -t a(Az.f(x)) P Clearly, p can evolve via an a-transition into the state \nAz. f(x), which is clearly different from both O and Ax.x. That is, p and p are intuitively not bisimilar. \nHowever, we have p ti~h,t. p . Clearly both of p s moves match moves of p , and two of p s moves match \nmoves of p. For any ~ : P, we have q? =pn f(T), which is either bisimilar to r or to O depending on whether \nor not T can take a b action at P. In particular, the usual intuitions of bisimulation, of staying in \nrelated states on all actions, have been lost in white bisimulation.5 Nonetheless, white bisimulation \nis a worthwhile no\u00adtion. It enjoys many of the properties of bisimulation in ordinary process algebra; \ne.g., the method of bisimu\u00adlation up to =Whtte works. More importantly, =White is a good notion of process \nequivalence on a suitable class of languages; that is, all suitably defined opera\u00adtions respect &#38;Whl~.. \nDefinition 4.3 A White Chocolate Language is a set of operations over a white chocolate type system, \nand a set of rules, subject to the following condition. For each rule p, let u be either the target of \np (if the target has type P or Pk), or the normal form of target(p) applied to a fresh variable .z (if \nthe target has type Pk + P). Then, we require that 1. If a target variable y of type Pk -i P appears \nin u, then it appears onlg once, and that occurrence is in a subterm of the form y(p) for some term p. \n., P q , such that nf(q) is componentwise vP 5This may also be construed as a flaw in the higher-order \nto nf(q ). transition system. 2. There are no abst? actions in u. [B1089] For example, if we restrict \nNESTLfi to white chocolate types, we get a white chocolate language. Consider the restriction rule. Ignoring \nsome tupling and projecting that the actual system requires, we have the rule [Hen93] a, P +P x P pl+ \nx\\s ~ p Aw.(yw)\\s Then u = (Azo. (yw)\\S)z =Pm (yz)\\S, which satisfies the definition. [Mi191] Theorem \n4.4 Let L be a white chocolate language. White bisimulation is a congruence for L. This subsumes, e.g., \nThomsen s result that white bisim\u00ad[MPW92] ulation (his higher-order bisimulation) is a congruence for \nCHOCS; it also implies that white bisimulation is a congruence for Hennessy s somewhat more elaborate \nlanguage, and for NESTLE/2. [Rep89] Acknowledgements lVe would like to asknowledge Paul Taylor for his \ndia\u00adgram package, which was helpful for drawing the many [Rep91] sorts of amazing arrows used in this \nstudy. References [Tho89] [Ace92] Luca Aceto. Eliminating junk rules from GSOS languages. (Unpublished \nnote; to ap\u00adpear, probably as part of something else.), July 1992. [Vaa93][BC90] Kenneth Birman and \nRobert Cooper. The Isis project: Real experience with a fault tol\u00aderant programming system. Technical \nRe\u00adport 90-1138, Department of Computer Sci\u00adence, Cornell University, July 1990. [BCG91] Kenneth P. Birman, \nRobert Cooper, and Barry Gleeson. Programming with pro\u00adcess groups: Group and multicast semantics. Technical \nReport 91-1185, Department of Computer Science, Cornell University, Jan\u00aduary 1991. [BIM88] Bard Bloom, \nSorin Istrail, and Albert R. Meyer. Bisimulation can t be traced (pre\u00adliminary report ). In Conje rence \nRecord of the Fifteenth Annual ACM Symposium on Principles of Programming Languages, pages 229-239, 1988. \nAlso appears as MIT Tech\u00adnical Memo MIT/LCS/TM-345. Bard Bloom. Ready Simulation, Bisimula\u00ad tion, and \nthe Semantics of CCS-Like Lan\u00adguages. PhD thesis, Massachusetts Institute of Technology, August 1989. \nMatthew Hennessy, A fully abstract de\u00adnotational model for higher-order processes (extended abstract). \nIn Proceedings of the Eighth Annual IEEE Symposium on Logic in Computer Science, pages 397 408. IEEE \nComputer Society Press, 1993. (Full version in Sussex TR 6/92). Robin Milner. The polyadic r-calculus: \nA tutorial. In PTOC. International Summer School on Logic an dAlgebr a of Specification. Ma~ktoberdorf, \n1991. Robin Milner, Joachim Parrow, and David Walker. A calculus of mobile processes I and II. Information \nand Computation, 100:1-40 and 41 77, 1992. John H. Reppy. First-class synchronous op\u00aderations in Standard \nML. Technical Report 89-1068, Department of Computer Science, Cornell University, December 1989. John \nH. Reppy, CML: A higher-order concurrent language. SIGPLAN Notices, 26(6):293-305, June 1991. Bent Thomsen. \nA calculus of higher or\u00adder communicating systems. In Confe? cnce Record of the Sixteenth Annual ACM \nSym\u00adposium on Principles of Programming Lan\u00adguages, pages 142-154, 1989. Frits Vaandrager. Expressiveness \nresults for process algebras. In de Bakker, de Roever, and Rozenberg, editors, Semantics: Foun\u00addations \nand Applications, pages 609-620. Springer-Verlag, 1993. LNCS 666. Also ap\u00adpears as CWI Tech Report CS-R9301. \n  \n\t\t\t", "proc_id": "174675", "abstract": "<p>We propose a general definition of higher-order process calculi, generalizing CHOCS [Tho89] and related calculi, and investigate its basic properties. We give sufficient conditions under which a calculus is finitely-branching and effective. We show that a suitable notion of higher-order bisimulation is a congruence for a subclass of higher-order calculi. We illustrate our definitions with a simple calculus strictly stronger than CHOCS.</p>", "authors": [{"name": "Bard Bloom", "author_profile_id": "81100272990", "affiliation": "Cornell University, Ithaca, NY", "person_id": "PP14102884", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/174675.177948", "year": "1994", "article_id": "177948", "conference": "POPL", "title": "CHOCOLATE: Calculi of Higher Order COmmunication and LAmbda TErms (preliminary report)", "url": "http://dl.acm.org/citation.cfm?id=177948"}