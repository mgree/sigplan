{"article_publication_date": "02-01-1994", "fulltext": "\n Multi-Pass Execution of Functional Logic Programs Jukka Paakki Department of Computer Science University \nof Jyvaskyla P.O.Box 35 FIN -40351 Jyvaskyla, Finland (paakki@cs.jyu.fi) Abstract An operational semantics \nfor functional logic programs is presented. In such programs functional terms provide for reduction of \nexpressions, provided that they ground. The semantics is based on multi-pass evaluation techniques orig\u00adinally \ndeveloped for attribute grammars. Program execu\u00adtion is divided into two phases: (1) construction of \nan in\u00adcomplete proof tree, and (2) its decoration into a complete proof tree. The construction phase \napplies a modified SLD\u00adresolution scheme, and the decoration phase a partial (multi\u00adpass) traversal over \nthe tree. The phase partition is gen\u00aderated by static analysis where data dependencies are ex\u00adtracted \nfor the functional elements of the program. The method guarantees that all the functional terms of a \npro\u00adgram can be evaluated, and no dynamic groundless checks are needed, Introduction One of the most \nactive directions in multi-paradigm pro\u00adgramming has been the integration of functional and logic programming, \nThk approach has a pragmatic motivation because logic programs typically contain a significant amount \nof functional computations [Dra87]. The development has resulted in a number of integrated functional-logic \nlanguages and their various imrJementation stratetties. The differ\u00adent approaches have one characteristic \nin common: They combine logical deduction (SLD-resolution) with reduction of functional expressions. \nReductions can be either non\u00addeterministic (such as narrowing in e.g. SLOG [Fri85], BA-BEL [MoR92]) or \ndeterministic (in e.g. LeFun [ALN87], NUE-Prolog [Nai91]). The non-deterministic approach is general \nbut involves computational and conceptual problems. Deterministic re\u00adductions are more efficient but \nthey impose conditions on the degree of instantiation of the expressions. For instance, when the functional \nexpressions are reduced by calling func\u00adtions written in some foreign language, the natural require\u00adment \nis that an expression is grormd when calling the asso\u00adciated external function (e.g., residing in a library). \nPermission to copy without fee oft or part of this meteriel is granted provided that the copies ore not \nmode or distributed for direot commercial edventege. the ACM copyright notice and the title of the pubfketion \nand ite dete eppeer, end notice is @en that copying ie by permission of the Ae.societion for Computing \nMechinery. To copy otherwise, or to repubfish, requiree a fee end/or epecific perrnksion. POPL 94-1/94, \nPortland Oregon, USA @ 1994 ACM 0-89791 -$36-0/94/001 ..$3.50 Usually groundless of functional expressions \nis tested dy\u00adnamically, when running the program. If some argument turns out to be non-ground, two solutions \nare possible. The first solution, applied e.g. by Prolog s arithmetic facilities, is to abort the execution \nand report a runtime error. Be\u00adsides destroying completeness of the declarative semantics, this is even \noperationally too harsh a decision because the arguments might well become ground if the program were \nexecuted further. A better solution, used e.g. in LeFun and NUE-Prolog, is to delay the call until the \narguments become fully instantiated. However, also thk strategy has declara\u00adtive drawbacks (due to possibfity \nof functional expressions that never become ground) as well as operational overhead (due to dynamic groundless \nchecks with associated freezing and unfreezing operations). To summarize, two issues are of particular \nimportance when considering the operational semantics of functional logic languages based on reduction: \n1. Operational covnpletenesx A program is operationally complete iff no delayed call remains non-ground \non success of the execution. 2. Delay-freeness: How to avoid the dynamic groundless checks over the \nprogram?  Thk paper addresses these questions in a uniform manner by presenting a static analysis method \nthat guarantees opera\u00adtional completeness and an execution scheme that is delay\u00adfree for all the programs \naccepted by the static analysis, The functional elements of a program are expressed as junc\u00ad tional terms \nthat are assumed to represent calls to external functions.1 Example 1.1 The syntax of our language can \nbe illustrated by the following factorial program, where X*Y and X-1 are functional terms. Note that \nthe same program would have to be written in Prolog using the rather cumbersome arithmetic predicate \nis/2. fac(O, 1) + fac(X, X*Y) + fac(X-1, Y) o I Note, however, that the technique can be applied on \nall fun\u00adctional logic programming languages based on reduction. In fact, even narrowing-based languages \ncan enhance their efficiency by our analy\u00adsis that could discover such expressions that can be deterministically \nreduced. Our approach is based on techniques developed for attribute grammars, a formalism originally \nproposed for defining the semantics of context-free languages [Knu68]. Attribute gram\u00ad mars are a most \nsuitable model for establishing the o~era\u00ad tional semantics of the class of languages we are de&#38;g \nwith, because the attribute grammar research has produced effective methods for analyzing and implementing \ncalls of external functions embedded in a grammar. The technique introduces the concept of multi-pass \neval\u00ad uation into (functional) logic programming. Such a strategy is needed if the functional primitives \nof a program depend on each other in such a way that they cannot be executed in synchron with a standard \nlinear resolution scheme of logic programming. As in attribute grammar implementations, a multi-pass \nscheme divides program execution into two sepa\u00ad rate phases: First (1) an incomplete proof tree is constructed \nwith some arguments instantiated, and then (2) the tree is decorated into a proof tree by evaluating \nthe rest of the ar\u00ad guments. The tree construction phase instantiates as many arguments as possible because \nthey assist in making the in\u00ad complete proof tree close to the ultimate proof tree. The first phase covers \nat least all those arguments whose value is not needed in functional computations. The tree traver\u00ad sal \nis made according to the functional dependencies in the program such that whenever a function is called, \nall its ar\u00ad guments are ground. The multi-pass evaluation technique is useful when solv\u00ading problems \nwhere the flow of data may be from right to left with respect to the program, whiie the flow of control \nis from left to right. A general application area of thk kind is op\u00adtimization where the optimal decision \nto be taken depends on the global information, not necessarily available when control reaches the decision \npoint in the program. Concrete examples include layout derivation in electronic publishing systems, transaction \nscheduling in databases, and code op\u00adtimization in compilers. We proceed as follows. In Section 2 some \nbasic concepts are introduced to relate functional logic programs and at\u00adtribute grammars. We also give \na non-trivial example to be used throughout the paper as a subject of our approach. The central notations \nare explained in Section 3. The static analysis method is presented in Section 4, followed by the established \noperational semantics in Section 5. Implemen\u00adtation aspects are discussed in Section 6. Finally, relation \nto other work is discussed in Section 7, and conclusions are drawn in Section 8. 2 Functional logic programs \nand attribute grammars In this section we present our two target formalisms, func\u00adtional logic programs \nand attribute grammars. Exhaustive definitions are not given, but we assume that the reader is familiar \nwith the basic concepts and properties of attribute grammars and logic programs, presented e.g. in [Knu68, \nDJL88] and in [StS86, L1087], respectively. A functional logic program is like an ordinary logic pro\u00adgram, \nexcept that the functors are divided into two dis\u00adjoint sets: ordinary constructors and defined symbols. \nAc\u00adcordingly, a program may contain both ordinary constrwc\u00adtov terms, and functional terms whose principal \nfunctor is a defined symbol. Each defined symbol j with arity n is as\u00adsumed to be associated with an \n(external) function, taking n ground constructor terms as input and yieldlng one ground constructor term \nas output. Thus, a ground functional term $(tl, . . . . tn) represents a ground constructor term that \nwe de\u00ad note with f(tl ,..., tn) J. Thk reduced value is obtained by calling the external function for \n.f with tl, . . . . tn as input. The program in Example 1.1 is a functional logic program with defined \nsymbols */2 and -/2. The presence of functional terms calls for an extended unification algorithm (see \ne.g. [MBB+93]). Suppose a func\u00ad tional term f(tl, .... t~) is to be unified with a constructor term s. \nIf tl,....tnare ground, the associated function is called and f(tl, . . . . t~) J is syntactically unified \nwith s. If some ti is non-ground, the unification algorithm returns the constraint f(tl, . . . . tn) \n= s. Thanks to verification of ground\u00ad less, our method makes the handling of constraints unnec\u00ad essary. \nAn attribute grammar has three elements: a contezt-free grammar G, a finite set of attributes A, and \na finite set of semantic rules R. The component G defines the syntax of the target language, whale its \nsemantics is specified by the components A and R. The attributes are divided into two dkjoint subsets, \nthe inherited and the .q@hesized at\u00adtributes. The semantic rules are expressed with semantic functions \nthat specify the value of an attribute occurrence in terms of other attribute occurrences. Intuitively, \ninher\u00adited attributes involve a top-down computation since they define the value of an attribute occurrence \nin terms of its ancestors (and brothers) in an underlying tree structure, whereas synthesized attributes \nexpress bottom-up com\u00adputation by defining an attribute occurrence in terms of the subtree whose root \nthe occurrence is associated with. The fact that makes it feasible to relate (functional) logic programs \nand attribute grammars is that the semantics of both of them can be defined in terms of labelled trees. \nFor (functional) logic programs these are the proo} trees that are generated for a given goal. For attribute \ngrammars the corresponding concept is an attributed (or decorated) parse tree with respect to an input \nstring. The labels express argument values for a (functional) logic program and at\u00adtribute instances \nfor an attribute grammar. Thk provides for a unified view on (functional) logic programs and at\u00adtribute \ngrammars. The seminal work by Deransart and Maluszyriski [DeM85, Ma191, DeM93] discusses the relation \nbetween these two formalisms and presents transformation methods between them. Whale attribute grammars \nand logic programs have much in common, there are also some fundamental differences. First, in attribute \ngrammars the tree decoration phase (at\u00adtribute evaluation) is conceptually separated from the con\u00adstruction \nphase (parsing), whereas the traditional opera\u00adtional semantics of logic programming (SLD-resolution) \nin\u00adtegrates decoration wit h proof tree construction. Second, in contrast to the disciplined use of inherited \nand synthe\u00adsized attributes in attribute grammars, arguments of a logic program may be used rather freely \nwithout consistency re\u00adstrictions. Finally, the dependency concept that is central in analyzing and implementing \nattribute grammars does not appear in unidirectional logic programs. WhJe the dependency concept is uncommon \nin ordinary logic programming, it is a most natural one in fictional logic programming: In order to compute \nthe value of a functional term . . . . tin),arguments t;must be first .f(tl, the evaluated into ground \nform. In other words, the functional term f(tl,....tn)depends on its arguments. Thk observation forms \nthe basis for establishing an operational semantics for functional logic programs. (G) typeset(lnd, Line, \n[ table(Tl), table(T2) \\ TL ], [ Instl, lnst2 I lnstTL ]) t max(Ll, L2) < pagelength -Line, WI + W2 < \npagewidth -Ind, typesettab(lnd+ (pagewidth-(lnd+ Wl+W2))/3, Line+l, Tl, Ll, WI, typesettab(lnd+ Wl+2*(pagewidth-( \nlnd+Wl+W2))/3, Line+l, T2, typeset(lnd, Line+ max(Ll,L2), TL, lnstTL) instl), L2, W2, lnst2), (G) typesettab(lnd, \nLine, typesetrow(lnd, T, Length, Width, Line, MaxColW, Inst) +-T, Length, Width, MaxColW, Inst) (G) typesetrow(-, \n-, -, [], O, 0,0, []) 4\u00ad (C4) typesetrow(lnd, Line, COIW, [ Row I RL ], l+ LengthRL, max(Wl,W2), max(WidthCol,WidthRL), \n[ lnstRow I lnstRL ]) typesetcol(lnd, Line, COIW, Row, WI, WidthCol, lnstRow), typesetrow(lnd, Line+l, \nCOIW, RL, LengthRL, W2, WidthRL, lnstRL) - (c,) typesetcol(-, -, . . [], O, 0, []) + (c,) typesetcol(lnd, \nLine, COIW, [ Text / CL ], ColW+WidthCL, max(length(Text), MaxCL), [ put(Line,lnd,Text) typesetcol(lnd+ \nColW, Line, COIW, CL, WidthCL, MaxCL, I lnstCL lnstCL) ]) .+ Table 1: Functional logic program. Example \n2.1 As an extensive example, let us consider the problem of deriving an optimal layout for typesetting \ntables in a report. This example will be used throughout the paper.  As input a document preparation \nsystem gets a descrip\u00adtion of tables, and transforms it into instructions for a type\u00adsetting device. \nThe document preparation system optimizes the layout such that two consecutive tables are printed sym\u00admet \nrically side by side, if possible. Each table is printed such that the columns are of equal width, according \nto the widest element in a column. For inst ante, the input might describe two tables: [table([ [This, \nis, a], [table] ]), table([ [This, is], [another], [nice, table] ])] When printed, these tables should \nbe organized as follows: This is a This is table another nice table The typesetting instructions put( \nLine, Indentation., Tew$) specify the placement of the table elements as follows, as\u00adsuming initial line \n1, no global indentation, and a page width of 80 units: [ put(l,17,This), put(l,22,is), put(l,27,a), \nput(2,17,tabie), put(l,49,This), put(l,56,is), put(2,49,another), put(3,49,nice), put(3,56,table) ] The \noptimal layout problem can be expressed with the clauses in Table 1. For simplicity, the other clauses \nof the system are omitted. From left to right, the arguments of typeset represent the current (global) \nindentation, the number of lines produced so far on the current page, the table descriptions, and the \nlist of typesetting instructions. A table description is pro\u00adcessed by predicate typesettab whose arguments \ngive (from left to right) the indentation, the line to start typesetting, the table description, the \nlength of the table (in lines), the width of the table (in characters), and the typesetting in\u00adstructions \nfor the table. The rows of a table are processed by typesetrow whose arguments give the indentation, \nthe line number, the fixed column width, the row descriptions, the length of the (sub) table, the maximum \nrow width, the maximum column width, and the typeset ting instructions, The columns are processed by \ntypesetcol whose arguments stand for the indentation, the line number, the fixed width, the column descriptions, \nthe cumulative width of the cur\u00adrent row, the maximum column width, and the typesetting instructions. \nDue to the nature of the problem, the clauses contain a number of functional terms expressing arithmetic \ncompu\u00adtations. In addition to ordinary arithmetic functions (such as +), we have used the external (constant) \nfunctions page-Iength and pagewidth giving the length and the width of a page, respectively, max that \nreturns the greater of its two integer arguments, and length that gives the length of its argument string \n(in characters). This program is rather involved and it is not immedl\u00adately obvious how it should be \nexecuted. Most notably, the fixed column width (which is input data for typesetting) is not available \nuntil all the columns of a table have been ana\u00adlyzed. This is a common situation in optimization problems \nwhere the proper action to be taken depends on the global substance of input data. In the following sections \nwe will return to this example and show how the problem can be tackled by using attribute grammars as \nthe model for both static analysis and dynamic execution. n Preliminaries predicate p in P, Sel(p) ~ \nArgpos(p). The set Sel(p) is In this section we introduce some central notations. For exact definitions, \nrefer to [BPM93a, BPM93b, Paa93]. Let P be a functional logic program. We let the clauses of P be numbered \nCl, Cz, . . .. and the atoms of every clause be numbered 0,1,. ... so that the O:th atom is the head, \nthe 1st atom is the leftmost atom in the body, and so forth. Let g be a predicate in P. We let the argument \npositions of q be numbered 1,2, . . .. and denote the k:th argument position in q with qk. Let Argpos(q) \nbe the set of argument positions in q, and let Argpo~(P) = Ug in ~ Argpos(g). Let C be a clause in P \nof the form czo +-al, . . ..an. We can now unambiguously refer to the k:th argument position in a~ with \nthe tuple (C, j, q, k), where g is the predicate symbol of aj. We call such a tuple a (program) position. \nLet Pos(F ) denote the set of positions in P, and 7 os(C) the set of positions in C. We will not make \na distinction between positions and terms contained in them; e.g. if -y is a position, we may write @ \ninstead of to,where tis the term in ~. We extend a functional logic program with the desig\u00adnated startsymbol \nS/0 which cannot be used as an ordinary symbol. Accordingly, S t-gl, . . . , gn is the start clause for \nthe query + gl, . . ..g2.2 Since we want to divide execution into two phases, we must be able to partially \nprocess elements of a program, That is why we need the notion of a partial application of a .. substitution \nOto an atom a or a clause C = a. + al, ...,an. These are denoted by a@/S and CO/( SO, . . . . S~), respec\u00ad \ntively. Here S, SO, . . . . S~ are sets that specify on which po\u00ad sitions O is applied; the other positions \nremain unaffected. For instance, if o = {X = 1, Y = 2}, then fac(X-l,Y)O/{facl} = fac(l-l,Y). Two atoms \na and b partially unijg with respect to a set S, if there exists a (most general) substitution u such \nthat the positions indicated by S are identical in so/S and bufS. We also need to apply functional reduction \npartially to an atom a or a clause C. These are denoted al/S and Cl /(s0, . . . . Sri), respectively. \nFor instance, fac(l-l,Y)l/{facl } = fac(O,Y). Partial substitutions and reductions make it possible to \ncharacterize when a clause D = bo + bl, . . . . bn is a partially reduced instance of another clause \nC = a. G al, . . . . an with respect to a sequence of position sets So, S1, . . . . S~. This is the case, \nif there exists a substitution $ such that for all j= o,... ,n, each argument in (D, j,q, k), where q \nis the predicate of bj, is identical to the corresponding argument in ((a28/Sj )j/SJ ), if q~ c Sj. In \nother words, in that case the indicated arguments of C can be made identical to those in D by partial \nsubstitution and functional reduction, The concept of partially reduced instance is essential for defining \nwhat we mean by an incomplete proof tree for a functional logic program. Intuitively, an incomplete proof \ntree is a plan for a final proof tree such that only parts of the tree have been processed. An incomplete \nproof tree provides an interface between the two main phases of our execution scheme. Definition 3,1 \n[Incomplete proof tree] Let P be a func\u00adtional logic program, and SeZ(p) a set of positions for each \nThe notion of a start symbol corresponds to that in context-free (and attribute) grammars. Likewise, \na start clause corresponds to the notion of a start production. These are needed for defining the tree \nstructures that correspond to (attributed) parse trees. called the selector of p. An incomplete proof \nt~ee for P is a labelled ordered tree T such that 1.The root node of T is labelled by the start symbol \nS of P. 2. Every non-root node of T is labelled by an atom. 3. Let n be a node with label ao and with \nm children la\u00adbelled by al, . . . . am (m > O). Then there exists a clause C=a~ +-a~, . . ..a~ in P such \nthat ao i-al, . . ..a~ is a partially reduced instance of C with respect to the sequence (Sel(po ), Sel(pl \n), . . . . Sel(p~)) where p, is the predicate symbol of atom a;, i = O, 1, ..., m. o  Notice the analogy \nto attribute grammars: An incomplete proof tree with each selector being the empty set corre\u00adsponds to \na parse tree, while an incomplete proof tree with each selector Sel(p) = Argpos(p) corresponds to an \nattributed tree. An incomplete proof tree between these two extremes can be seen as an intermediate form \nfrom a parse tree into an attributed tree, with some functional expressions still un\u00adevaluated. Definition \n3.2 [Proof tree] Let P be a functional logic program. An incomplete proof tree T for P is a proof tree \niff (1) Set(p) = Argpos(p) for each predicate p in P; and (2) No label of a node n in T contains a functional \nterm as argument.3 o 4 Static analysis In thk section we present a static analysis method for verify\u00ading \noperational completeness of a functional logic program, and for dividing its execution into two phases \nsuch that no dynamic delays are needed for the evaluation of the func\u00adtional terms. The method is founded \non the notion of de\u00adpendency graphs that are the standard data flow concept in attribute grammars. A \nmore extensive discussion, with proofs of the propositions, can be found in [BPM93a] and [Paa93]. For \napplying techniques of attribute grammars on func\u00adtional logic programs, we must be able to model the \ndata flow within a program. The idea is that some of the predl\u00adcate arguments in P are annotated, either \nas inherited (J) or synthesized (~), that is: there is a function (an annotation) p : Argpos(P) ~ {J, \nt, o}, where o is read unannotated . An annotation is partial if some positions are unannotated. Intuitively, \ninherited arguments represent input information for a predicate, whereas synthesized arguments represent \nits output . Another important concept is the notion of input and output arguments of a clause C: If \n#(gk) =1 and a~ is the head atom of C, or if p(g&#38;) =T and a, is a body atom in C, we cd (C, j,q, \nk) an input position. If p(qk) =t and a, is the head atom in C, or if P(qk) =1 and al is a body atom \nin C, we call (C, j, q, k) an output position. The intuitive explanation for these names is that data \nis brought in to a clause through the input positions, and sent out through the output positions. Let \nZ(C) and O(C) denote the input 3 This concept is analogous to the conventional concept of proof trees \nin logic programming, with the additional requirement of op\u00aderational completeness of functional expressions. \nA proof is given in [Paa93]. 364 and output positions of C, respectively. We denote X(P) = UC6FZ(C) \nand O(P)= (_JCeP O(C). We write Cl + CZ if the clause Cj can be called from Cl, i.e. there is an atom \na in the body of Cl such that unification of a and the head of CZ (with variables renamed) does not fail. \n(This means that unification either succeeds or suspends with functional constraints. ) The relation \nt+ is called the static call gTaph. In a (functional) logic program, information can be passed in two \nways: either withina clause (between two arguments sharing a variable), or between two clauses (through \nunifica\u00adtion at an SLD-resolution step). Let us assume that there exists an annotation p : Argpos(P) \n+ {J, I, o}. As for at\u00adtribute grammars, this information can be used to extract the direction of information \nflow: from input positions to out put positions wit hin a clause, and from output positions to input \npositions at unification. To reflect this data flow, we int reduce the notions of local dependency graph \nu c for each clause C and transition graph + C,D for each pair of clauses C and D, respectively. Definition \n4.1 [Local dependency graph] For each clause C, a local dependency graph uc ~ Z(C) x O(C) is defined \nas follows: ~ xc -y if ~ and -y have at least one common variable. o Definition 4.2 [Transition graph] \nLet C and D be two clauses, bO the head atom in D, and a; a body atom in C such that unification of a; \nand bO (with variables renamed) does not fail. The transition graph +C,D on O(C U D) ; Z(C U D) is defined \nas follows: ~ = (D, O,q, k) 7 = (C, j,% ~) 7*C,D ~ if P = (D, o,9, ~) or @= (C, j,g, k) { AL(m) =1 { \n/J(9k ) t As for attribute grammars, the transition graph specifies how clauses are pasted together in \na proof tree. If the connected positions are inherited, data flows from the (caU\u00ading) body atom of clause \nC into the (called) head atom of clause D. The flow is reverse for synthesized positions, that is, from \nthe (called) head atom of D into the (calling) body atom of C. The total data flow for a functional logic \nprogram can be modelled by combining the local dependency graphs with the transition graphs. This program \ndependency graph is a safe approximation of the data flow in any proof tree for the program. Definition \n4.3 [Program dependent y graph] The pro\u00adgram dependency graph -p is defined as follows: QS denotes the \ntransitive and reflexive closure of -p. o Operational completeness can be ensured using the program dependency \ngraph. The essential condition is that the initial minimal elements of -p are ground. This implies that \nthe arguments of all functional positions will be ground in any proof tree for P and they can thus be \nreduced at some point of execution. The groundless condition applies to the start clause as well: its \ninherited positions must be ground. Another necessary condition is non-circularity. Intu\u00aditively, if \nthe annotation is simple and non-circular, recur\u00adsive equations of the form X = ~(X) will never have \nto be solved during unification. We will address the problem of non-circularity in more detail in Section \n5. The groundless condition can be stated by the following definition and proposition over a program. \nRecall that op\u00aderational completeness is relevant for functional expressions only. Definition 4.4 [Simple \nposition] A position -y ~ O(P) is simple if for all output positions ~ in every clause C holds: @ -> \nv = every variable in ~ occurs at some input position in C. Further, the annotation is simple if every \nposition in O(P) is simple. Note that if v is simple, and if @ E O(P) and 9 -v; ~, then ,6 is also simple. \nu Proposition 4.1 Let P be a program and p its non-circular annotation. If each functional term occurs \nin a simple out\u00adput position of P and all the inherited positions of the initial goal are ground, then \nP is operationally complete. c1 We next address the question of allocating the program posi\u00adtions into \nphases. We assume the standard computation rule to be applied in (functional) SLD-resolution. Then a \nfunc\u00adtional term at an output position y can be evaluated, if all its arguments are ground when invoking \nthe associated atom; otherwise the evaluation has to be delayed. Groundless of ~ is assured, if(1) All \nthe minimal elements oft he program dependency graph that have a path to ~ are ground, and (2) All the \ndependency edges leading to y flow in the same manner as the execution, depth-first, left-to-right. Property \n (1) is covered by the notion of simple annotation. Property (2) is analogous to the definition of L-attributed \ngrammars [LRS74, Boc76] that apply a preorder attribute evaluation strategy. Hence, we can conclude that \nthe functional term in position ~ can be immediately evaluated if v is L-annotated, as defined below. \nA proof is given in [BPM93a].  First an auxiliary definition is given that extracts all such local dependencies \nthat may affect the value of the (func\u00adtional) term in position y: Definition 4.5 Let D and C be two \nclauses such that D * C, and let y G O(C). -7 is defined by: PI % @2 if (PI D ~2 and /32+; ? ) o Definition \n4.6 [L-annotated, R-annotated position] A position -y E O(P) is L-annotated ifi (C,i,p, k)+7 (C,j,q,l) \n* (i<j or j= O). -y is R-annotated if it is not L-annotated. o As for L-attributed grammars, all the \nlocal dependencies that affect the value of an L-annotated position must be 365 from left to right withhr \nthe clause, or directed to the head (in which case the position is synthesized). Now we are ready to \nallocate the program elements into two disjoint sets that are going to be processed in two dif\u00adferent \nphases during execution. For effective proof tree con\u00adstruction, as many arguments as possible should \nbe pro\u00adcessed already during the first phase, and only those argu\u00adments should be allocated to the second \nphase that can\u00adnot be evaluated during the first one. This principle is due to founding the first phase \non SLD-resolution which is largely driven by the unification algorithm over predicate arguments. Wlthhr \nour framework, the arguments evalu\u00adated eagerly in the first phase are contained in the unanno\u00adtated \nand L-annotated positions, and the arguments subject to delayed evaluation are those that reside in R-annotated \npositions. These observations lead to the following phase partition where positions allocated to the \nfirst and second phase of execution are defined as spanning and completing positions, respectively. Definition \n4.7 [Phase partition] Let p be a predicate symbol of a functional logic program P. The phase partition \nfor p is the pair (Span(p), Comp(p)), where Cornp(p) = {IJk I there exists an R-annotated position Y \n= (c,~,P, ~)} Span(p) = Argpos(p) \\ CormJ(p) Span(p) is called the spanning position set of p, and Comp(p) \nis called the completing position set of p. o Our technique relies on a simple and non-circular (partial) \nannotation for the program. It is possible to synthesize such annotations automatically, as presented \nin [BPM93a, BPM93b]. The inference algorithm starts from an initial annotation, allocating all the functional \nterms into output positions. The initial annotation is incrementally and con\u00adsistently extended, until \na simple and non-circular one is obtained. For each variable in an output position, the algo\u00adrithm selects \nan input position with the same variable and connects the two positions with a local dependency edge. \nThe selected input positions are connected with transition edges to other clauses, where the same pattern \nof inference is repeated. Hence, the algorithm works backwards with respect to the resulting program \ndependency graph. The method is minimal in the sense that it annotates only such positions that may affect \nthe evaluation of a functional term. Therefore, the direction of data flow is fixed for a functional \nsubset of the positions only, and the unannotated part of the program can still be used in a multi-directional \nway. The dependency graphs are constructed by the algorithm simultaneously with annotating the program. \nSince we aim at maximal spanning position sets, L-annotated dependen\u00adcies are preferred in case there \nexist choices within a clause. The (local) dependency graphs are optimized by selecting for each variable \nin an output position only one input posi\u00adtion with the same variable (the leftmost one).4 As a third \nresult of the inference algorithm, we obt tin those positions that must be ground in a query for the \nprogram. Example 4.1 Let us analyze the typesetting program intro\u00adduced in Example 2.1. The atoms with \nbuilt-in comparison 4 Notice that this is sufficient, since the formal properties of func\u00adtional logic \nprograms guarantee that the final value of all occurrences of a variable will be the same. predicate \n< (having a fixed annotation) are omitted. We will use the abbreviated names t for typeset, tt for typesettab, \ntr for typesetrow,, and tc for typesetcol. The initial annotation makes all the functional positions \ninto output ones: p(t.2) =1 (from Line+ max(Ll,L2) in clause Cl), /J(ttl) =1, p(ttz) =1, p(trz) =1, p(trs) \n=t, P(trc) =T, p(tr7) =T, p(tcl) =J, ,u(tc, ) =~, p(tc5) =~. Consider e.g. the functional term ColW+WidthCL \nin clause CLi. The variable WidthCL appears in the annotated (syn\u00adthesized) input position (Cc, 1, tc, \n5) and does not generate additional annotations. The variable COIW, however, is not contained in any \ninput position, and the annotation must be extended. Now we have a choice because COIW appears in two \ncandidate positions, (Cc, O, tc, 3) and (Cc, 1, tc, 3). We prefer L-annotated dependencies and select \nthe leftmost candidate, (Cc, O, tc, 3), resulting in p(tc3 ) =1. By analyz\u00ading the other functional terms \nin Cs, we infer further that p(tc4) =1. In terms of the dependency relation, the functional term ColW+WidthCL \nhas introduced the following local depen\u00addencies for clause cc, one for each variable in the term; (CS, \nO,k 3) -G. (C6,0, tc,s) (Colw) (C6,1,@5)+Ce (C6,0,@5) (WidthCL) Since unification between the body atom \nfor typesetcol in C.I and the head atom of C6 does not fail, the following transitions are also induced: \n(C4, l,tc, i) ~C4,C8 (C6j0, tc, i) for each (inherited) i = 1,3,4; (C.s, o,tc, s)+c.,c. (c4,1, tc, s) \nfor each (synthesized) s = 5,6. Continuing in the similar fashion, the following simple anno\u00adtation \nfor the program is generated (denoted by a sequence of annotations): typeset: (lJIC!), typesettab: (~Jl~Tu), \ntypesetrow: (Jj.JJTTtO), typesetcol: (jCIJ1l ~0). A call to typeset in a query must have the first three \n(in\u00adherited) arguments ground, since they may be (indirectly) needed in functional computations. Notice \nthat no data flow is assumed for the unannotated (CI) positions. The program dependency graph is illustrated \nin Figure 1. For simplicity, only the clauses C z and C4, and the head of cc are shown. Predicate positions \nare indicated with their ordinal numbers, inherited to the left and synthesized to the right of a symbol. \nTransition edges are denoted by a dashed arrow. Note especially the right-to-left dependency from tr7 \nto tr3 in the local graph for clause Cz. This implies that the position (C2, 1, tr, 3) is R-annotated \nand cannot get a ground value at invocation under the standard computa\u00adtion rule of SLD-resolution. Yet, \nthe value of (C2, 1, tr, 3) is needed in evaluating the functional term ColW+WidthCL in position (C5, \nO, tc, 5), if the clause CZ (indirectly) calls the clause Cs (notice the data flow from (C2, 1, tr, 3) \nto (C6, O, tc, 5) in the graph). Therefore, trs must be allocated to Comp(t r). This also applies to \nall those positions that depend on (C2, 1, t r, 3); see the graph. When analyzing the complete graph, \nwe derive that Comp(t) = 0, Comp(tt) = {ttl , tt,}, Comp(tr) = {trl, trs, trs}, Comp(tc) = {tcl, tc,, \ntc~}. 366 3 tt4 51 (c,) {+ 2 34tr56 pIt I Ii III Figure 1: Program These positions will not be evaluated \nuntil during the sec\u00adond phase of our execution scheme. The rest oft he posit ions are included in the \nspanning sets and will be processed dur\u00ading the first phase of execution. We will continue with this \nexample in Section 5. Q 5 Operational semantics In thki section we present an execution scheme for functional \nlogic programs in terms of an interpreter. Recall from Sec\u00adtion 4 that our analysis divides the arguments \nof each pred\u00adicate into two disjoint sets: (1) Those that can be processed during (functional) SLD-resolution, \nand (2) Those whose evaluation has to be postponed due to right-to-left depen\u00addencies. Accordingly, ourinterpreter \nexecutes in two phases: , 1. SLD-resolution with the standard computation rule over the spanning program \npositions. Due to the spe\u00adcial characteristics of execution, ordinary (syntactic and total) unification \nis enriched with reduction of ground functional terms and partiality (see Section 2). An incomplete proof \ntree is constructed to build an interface to the second execution phase. 2. Tree traversal. Thetree \nproduced by the first phase is traversed by visiting and reducing all the unevaluated completing positions \nin the tree, resulting in a com\u00adplete proof tree for the program. The visiting order is determined bythe \nprogram dependency graph.  Thetwo-phase interpreter fora functional logic program P is given in the \nAppendm. When executed, the interpreter constructs a proof tree for P with respect to a query (if the \nquery succeeds). The set of all proof trees can be obtained by repeatedly backtracking over the interpreter. \nThe interpreter runsin two consecutive phases, induced by the phwe prwtition for the program. In the \nfirst phase partially reduced clause instances preselected from the pro\u00adgram and pushed into an explicit \nproof tree pattern. As dependency graph. in the standard implementation of L-attributed grammars [LRS74,BOC76], \nthe synthesized output arguments are not reduced until after processing the body of the associated clause \n(since they typically depend on the body). As usual, the computed substitutions are spread throughout \nthe in\u00adcomplete proof tree. We assume that the variables have been uniquely renamed. The second phase \ncompletes the pattern into a proof tree. This is done by traversing the tree, and by instantiating and \nreducing those argument positions that have been passive in the tree construction process. Because our \nnotion of a proof tree forbids unevaluated functional terms, the traver\u00adsal phase is ezhaustivewith respect \nto the incomplete argu\u00adments. That is, each completing position is visited exactly once. The static analysis \nguarantees that (1) When anode is pushed into the tree in phase 1, all its inherited functional spanning \narguments are reduced into a ground form; (2) When a synthesized spanning position ofahead is reduced \nin phase 1, it will obtain a ground value; and (3) When a node gets a new label in phase 2, its selected \nposition is reduced into a ground form, Hence, in addition to the two level phase partition, the method \ncan be seen as gen\u00aderating a three-level classification of arguments as well: (i) Those that assist in \nconstructing the incomplete proof tree and that can be afterwards refined by unification; (ii) Those \nthat obtain their final ground value by functional reduction during tree construction; and(iii) Those \nthat get their final ground value during tree traversal. The following proposition provides a rigorous \nsemantics for the two-phase interpreter (a proof is given in [Paa93]). Proposition 5,1 Let l(P) bethetwo-phase \ninterpreter for a functional logic program P. If a tree T is produced as output of I(P) then T is a proof \ntree for P. o Ina manner similar to attribute grammars, we can identify special classes of functional \nlogic programs. For instance, 367 [12 3 tt 4 51 L  (c,) H [1 2 Figure 2: Clause we can define a counterpart \nto L-attributed grammars as an effective program class which can be executed in a single phase. Other \nclasses are characterizedin [Paa93]. Definition 5.1 [L-annotated program] Let P be a func\u00ad tional logic \nprogram. P is L-annotated if Comp(p) = O for each predicate p of P. For instance, the program in Example \n1.1 is L-annotated. On the other hand, the program in Example 2.1 is not L\u00adannotated (recall the analysis \nin Example 4.1) and must therefore be executed in two phases. Recall that the two main requirements \nfor operational completeness are simplicity and non-circularity (cf. Propo\u00adsition 4.1). The circularity \ntest can be made in terms of the program dependency graph, using the techniques devel\u00adoped for attribute \ngrammars. Whale thegeneraJ circularity problem is exponential [JOR75], it has polynomial complex\u00adity \nfor restricted grammar classes that cover most practical cases. The following definition corresponds \nto a well-known class in this category, the absolutely non-circular attribute grammars [KeW76]. Definition \n5.2 [Absolutely non-circular program] Let -P be a program dependency graph for the functional logic program \nP and C a clause in P. The clause dependency graph -C. is defined as follows: *@-c.y if ,Bti G-y. for \neach -y c (9(C) and ,6 ~ Z(C), where 7 and ~ are in the same body atom of C, y -c. ,6 if -y -+> ~. If \nthe clause dependency graph -+G. is acyclic for each clause C, the program P is absolutely non-circular \n(and the annotation is non-circular). o Intuitively, a clause dependency graph -c. condenses all the \npossible data flow paths in any subtree whose root is labelled with the head of C, by inducing transitive \ninherited\u00adto-synthesized dependencies for the predicate symbols in the body of C. Example 5.1 Let u~ \nanalyze the program of Example 2.1 in more detail. We construct the clause dependency graph for clause \nCz by introducing an edge from an inherited position tri to a synthesized position tr, whenever there \nis a transitive path from tr, to tr. in the program dependency graph (see Example 4.1). The graph is \nillustrated in Figure 2. The clause dependency graph -c; is acyclic. By analyz\u00ading the other clauses \nin the same manner, we can verify that the program is absolutely non-circular. Thus, the program is operationally \ncomplete under the annotation inferred in dependency graph. Example 4.1 and can be executed such that \nno unevaluated function calls remain at the end of computation. The evaluation order of the positions \nin clause Cz is illus\u00adtrated in the clause dependency graph. Taking into account the phase partition \nfor the program (see Example 4.1), we can see e.g. that the evaluation sequence tts x tr.r -trr is interleaved \nwith the first execution phase, while the se\u00adquence trt * tre _+ tts is evaluated during the second completing \nphase. Q Example 5.2 For demonstrating the two-phase execution scheme, let us interpret the typesetting \nprogram against the following query, assuming the page width of 80 characters: +-typeset(C),O, [ table([ \n[simple] ]), table([ [table] ])],X) Recall from Example 4.1 the phase partition for the pro\u00ad gram: Span(t) \n= {tl, tz, tt, t4}, Span(tt) = {ttz} ttt, tt4, tte}, Span(tr) = {trZ, trA, tr5, tr7, trS}, f@n(tc) = \n{tcz , tc4 , tc6 , tc7}, Corrzp(t) = 0, Comp(tt) = {tt 1, tt5 }, Comp(tr) = {trl, tr3, trIj}, Comp(tc) \n= {tcl, tct, tcs}. When executing the first phase of interpretation, the in\u00adcomplete proof tree depicted \nin Figure 3 is obtained. The unevaluated vslues are indicated by a question mark. The recursive call \nto typeset is omitted. The second phase completes thk tree into the proof tree shown in Figure 4 by evaluating \nthe undefined completing positions in the order determined by the program depen\u00addency graph (see Example \n4.1), without dynamic delays. Notice that the second phase may also implicitly (via sharing of logical \nvariables) refine the values of spanning positions, e.g. the second argument of the put instructions. \nc1 6 Implementation aspects Our framework of operational semantics for functional logic programs involves \nstatic analysis and multi-pass execution. Static analysis is founded on annotating the program and on \nconstructing u program dependency graph for thor.c pro\u00adgram positions that may be needed in functional \ncomputa\u00adtions. An algorithm for the static analysis phase is given in [BPM93a, BPM93b]. Execution of \na program consists of the construction of an expIicit incomplete proof tree, and its decoration into \na proof tree by a tree traversal. An implementation of these is given as a two-phase interpreter in the \nAppendm. The con\u00adstruction phase can also be embedded into a conventional SLD-resolution scheme, applied \ne.g. in Prolog. The main 368 s I t(O,O,[ table([ [simple] ]), table([ [table] ])], [ [put(l,?,simple)], \n[put(l,?,table)] ]) \\ tt(?,l,[[simple]],l, ?,[put(l,?,simple)]) tt(?,l,[[table]],l, ?,[put(l,?,table)]) \n/ tr(?,l,?,[[simple]],l, ?,6,[put(l,?,simple)]) tr(?,l,?,[[table]],l, ?,5,[put(l,?,table)]) /\\ /\\ tc(?,l,?,[simple], \n?,6, tr(?,2, ?,[ ], tc(?,l, ?,[table], ?,5, tr(?,2,?,[ ], [put(l,?,simple)]) [put(l,?,table)]) 0,?,0,[ \n]) 0,?,0,[ ]) tc(?,l,?,[ ],?,0,[]) tc(?,l,?,[]>?,0,[]) Figure 3: Incomplete proof tree. special features \nneeded in our framework are partial unifica-to the subtree immediately below n (where n is the root). \ntion, functional reduction, and delayed processing of synthe-This check is due to eventual failure of \none of the clauses. sized output arguments. Functional reduction can be imple-Failure during tree traversal \nmay be rather costly be\u00admented via a foreign function interface that is provided in cause control is \nreturned to the skeleton construction pro\u00admany Prolog systems. A facility for synthesized evaluation \nis cess. This implies reconstruction of an explicit tree struc\u00adprovided in some experimental systems \nmodelling attribute ture. Clearly, the reconstruction should produce a tree that grammars. For example \nPROFIT [Paa91] replaces each syn-is as close as possible to the rejected one so as to minimize thesized \nfunctional term by a new variable V and moves the the amount of recomputation.5 We are currently investigat\u00adfunction \ncall from the head of the clause into the end of ing such an optimal reconstruction technique, based \non Ear\u00adits body where the result is unified with V. This strategy ley s parsing algorithm for ambiguous \ncontex-free grammars can be used for implementing L-annotated programs whose [Ear70].s After reconstruction, \nthe techniques developed for execution consists of one phase only. incremental attribute evaluation can \nbe applied to minimize Partial unification can be approximated by conventional the recomputation in the \nsecond phase of interpretation (see primitives as well. Recall that a central assumption is that e.g. \n[ReT89]). the completing positions do not take part in the first phase of interpretation. This principle \ncan be mimicked in a sim\u00ad 7 Related work ple fashion by transforming the program into a form where each \nterm in a completing position is replaced by a unique This work belongs to the general research of relating \nat\u00advariable. When executing the modltied program under a tribute grammars and logic programming. Conceptual \nre\u00adstandard (Prolog) implementation, unification over the com\u00ad sults in this area are presented especially \nin [DeM85, Ma191, pleting positions will always succeed, that is, the positions DeM93]. Some of the methods \napplied in this paper orig\u00adremain neutral. This method, integrated with the delayed inate in those publications, \nmost notably the use of anno\u00adsynthesized evaluation technique described above, is cur-tations for analyzing \na program. The concept of dependen\u00adrently being implemented for the first phase of our execution cies \nover an annotated program has been proposed e.g. for scheme as a tool that produces an incomplete proof \ntree with occur-check elimination [DFT91, ApP92], failure exclusion respect to indicated sets of selector \npositions [Gro93]. [Bou92], termination proofs [P1u90], static slicing [PGH93], The second interpretation \nphase is a dependency driven groundless analysis [DeM85], and operational completeness traversal over \nan explicit tree structure. As for attribute [Boy91]. The last two topics are covered in this paper as \ngrammars, visiting a (completing) position of a node n in\u00ad well (in an improved form). Operational completeness \nhas volves reducing its label by calling an associated function. also been analyzed wit h abstract interpretation \n[Han92]. (Recall that our method guarantees a ground value for the The TYPOL project has developed methods \nto trans\u00adlabel at reduction time). We can use any attribute gram-form special classes of natural semantics \ndefinitions into at\u00admar class (such as absolutely non-circular grammars) as the tribute grammars [AtF88, \nAtC90]. By this, it is possible to model for the tree traversal; refer to [DJL88, Alb91]. In indirectly \napply a multi-pass execution scheme on logic pro\u00adgeneral, the tree traversal may comprise several passes \nover grams. The TYPOL approach, however, is more restricted the incomplete proof tree. Unlike in attribute \ngrammars, the obtained value has to be checked for consistency against 5Note that backtracking as implemented \ne.g. in Prolog does not meet this requirement. two cent exts of n: (1) With respect to the clause that \ncor\u00ads Notice that a (functional) logic program, without arguments, can responds to the subtree immediately \nabove n (where n is a be considered as a context-free grammar producing the empty string. leaf ); and \n(2) With respect to the clause that corresponds 369 s I t(O,O,[ table([ [simple] ]), table([ [table] \n])], / tt(23,1,[[simple] ],l,6,[put(l,23, simple)]) tr(23,1,6,[[simple] ],l,6,6,[put(1,23, simple)]) \n/\\ tc(23,1,6,[simple],6,6, tr(23,2,6,[ ], [pr.it(l,23,simple)]) 0,0,0,[]) tc(29,1,6,[ ],0,0,[ ]) Figure \n4: than ours because it only deals with programs that corre\u00ad spond to standard attribute grammars. For \ninstance, the annotation must be total and predefine (in contrast to the automatically generated partial \none in our method), the pro\u00ad grams may have deterministic rules only (implying exactly one proof tree \nfor each query, whereas we accept backtrack\u00ad ing and any number of proof trees), and only defined ground \nattribute values are accepted (whale our method handles un\u00ad defined and non-ground (spanning) values \nas well). Another major difference is that TYPOL programs do not contain functional expressions. A method \nfor transforming a (pure) logic program into a corresponding functional program by analyzing data depen\u00addencies \nis presented in [Red84]. In contrast to our approach, that method does not accept mutual dependencies \nbetween atoms but they must have a strict total order. (Hence, the approach covers only the restricted \nclass of one-pass pro\u00ad grams.) A more general transformation method for functional logic programs is \nproposed in [Boy93]. Using the same anal\u00adysis technique as we for obtaining operational completeness \nand delay-freeness, the method transforms a program into an equivalent one by moving function calls into \nsuch points that the program can be executed (in a single phase) with\u00adout dynamic delays under the standard \ncomputation rule of SLD-resolution. For complex programs (cf., Example 2.1), the transformation is non-trivial \nand introduces new argu\u00adments and (eventually) new clauses for all the R-annotated terms to transfer \nthem into a proper evaluation point. Con\u00adsequently, the original program may be quite different from \nthe one that is actually executed, and thus hard to maint tin. Also, for intricate dependencies the method \nmay require a long and expensive chain of dynamically transferred func\u00adtional values. Conclusions We \nhave presented an operational semantics for functional logic programs. The approach facilitates declarative \npro\u00adgramming since the user does not have to design a suit\u00ad [ [put(l,23,simple)], [put(l,52,table)] ]) \n\\ tt(52,1,[[table]],l,5, [put(l,52,table)])  tr(52,1,5,[[table]],l,5,5,[put(l,52,table)]) /\\ tc(52,1,5, \n[table],5,5, tr(52,2,5,[ ], [put(l,52,table)]) 0,0,0,[]) tc(57,1,5,[ ],0,0,[]) Proof tree. able execution \nstrategy for a program, but it is instead pro\u00advided automatically. In contrast to conventional execution \nschemes, the method guarantees operational completeness. The approach has also potentiality in speeding \nup execu\u00adtion because it avoids the dynamic delays, whose propaga\u00adtion may involve a run-time overhead \nin the order of magni\u00adtude [Boy93], A dk.advantage is that in general an explicit proof tree has to be \nconstructed. The space consumption is, however, decreased by reduction of functional expressions. Moreover, \nthe basic implementation algorithm can be im\u00adproved by several optimizations. On the conceptual side \nour work integrates functional computations into a logic programming framework. Such focusing is more \nadvantegous than the previous approaches of relating attribute grammars with pure logic programs (without \nfunctions), because it makes possible to more nat\u00adurally apply the central notion of attribute dependencies. \nWe have presented how techniques developed for attribute grammars can be adjusted to statically analyze \ndata flow as\u00adpects of functional logic programs, providing also a formal basis for programming. From \na more general perspective the technique promotes the approach of taking a computa\u00adtional paradigm (attribute \ngrammars) to effectively solve a problem in another paradigm (reduction in functional logic programming). \nThe main original contribution of our work is the applica\u00adtion of multi-pass execution techniques on \n(functional) logic programs with complex data dependencies. Thk improves the previous results in relating \nlogic programs and attribute grammars by showing that it is feasible to integrate them not only conceptually \nbut even operationally. The approach is quite general because it assigns a data flow direction on those \npositions only that are needed in functional compu. tations. In a typical situation the functional entities \ncover only a small fragment of the program. As a consequence, only the core functional subset of the \nprogram arguments will be subject to multi-pass evahation and the rest of the arguments can still be \nprocessed with the standard unifica\u00adtion scheme of SLD-resolution (and may be used in a multi\u00addirectional \nway). Thus, we believe that the method accepts 370 all the disciplined programs. Implementation of the \ntechnique is under way. The cur\u00adrent act ivities concentrate on static analysis, and on the first phase \nof interpretation with construction of an incomplete proof tree. The final step will be to integrate \na functional tree treversal algorithm into the execution scheme. Acknowledgements This work has been \ncarried out dur\u00ading a sabbatical year at the Department of Computer and Information Science, Linkoping \nUniversity, Sweden. Many of the presented ideas and results have been inspired by joint research of the \nauthor with Jan Ma}uszynski and Johan Boye. The comments by the referees have been helpful in clarifying \nthe main points. Tapio Elomaa has kindly helped in solving some technical problems. The work has been \nfi\u00ad nancially supported by the Nordic Academy for Advanced Study (NorFA) and by the Academy of Finland. \nReferences [Alb91] [ALN87] [ApP92] [AtC90] [AtF88] [Boc76] [BcIu92] [Boy91] Alblas H.: Attribute Evaluation \nMethods. In: Proc. Int. Summer School on Attribute Grwm,\u00admars, Applications and Systems, Prague, 1991. \nLNCS 545, Springer-Verlag, 1991, 48-113. Ait-Kaci H., Lincoln P., Nasr R.: Le Fun: Logic, Equations and \nFunctions. In: Proc. 1987 Sym\u00adposium on Logic Programming, San Francisco. IEEE Computer Society Press, \n1987, 17-23. Apt K. R., Pellegrini A.: Why the Occur-check is Not a Problem. In: Proc. lthInt. Symposium \non Programming Language Implementation and Logic Programming, Leuven, 1992, LNCS 631, Springer-Verlag, \n1992, 69-86. At t ali I., Chazarain J.: Functional Evaluation of Strongly Non Circular TYPOL Specifications. \nIn: Proc. Id. Conjevence on Attribute Gram\u00ad mars and Their Applications, Paris, 1990. LNCS 461, Springer-Verlag, \n1990, 157-176. At t ali I., Franchi-Zannettacci P.: Unification-Free Execution of TYPOL Programs by Seman\u00adtic \nAttribute Evaluation. In: Proc. 5th Int. Con\u00adference and Symposium on Logic Programming, Washington, \nSeattle (Kowalski R.A., Bowen K.A., eds.). The MIT Press, 1988, 160-177. Bochmann G. V.: Semantic Evaluation \nfrom Left to Right. CACM 19, 2, 1976, 55-62. Bouquard J.-L.: Etude des Rapports En\u00adtre Grammaires Attributes \net Programmation Logique: Application au Test d Occurence et 6 l Analyse Statique. PhD Thezis, University \nof Tours, 1992. Boye J.: S-SLD-resolution -An Operational Se\u00admantics for Logic Programs with External \nProce\u00addures. In: Proc. 3rd Int. Symposium on Program\u00adming Language Implementation and Logic Pro\u00adgramming, \nPassau, 1991. LNCS 528, Springer-Verlag, 1991, 383-393. [Boy93] [BPM93a] [BPM93b] [DeM85] [DeM93] [DFT91] \n[DJL88] [Dra87] [Ear70] [Fri85] [Gro93] [Har192] [JOR75] Boye J.: Avoiding Dynamic Delays in Functional \nLogic Programs. In: Proc. 5th Int. Symposium on Programming Language Implementation and Logic Programming, \nTallinn, 1993. LNCS 714, Springer-Verlag, 1993, 12-27. Boye J., Paakki J., Maluszyriskl J.: Dependency- \nBased Groundless Analysis of Functional Logic Programs. Research Report LiTH-IDA-R-93-20, Department \nof Computer and Information Sci\u00ad ence, Linkoping University, 1993. Boye J., Paakkl J., Mrduszyfiski J.: \nSynthesis of Directionality Information for Functional Logic Programs. In: Proc. 3rd lnt. Workshop on \nStatic Analysis, Padova, 1993. LNCS 724, Springer-Verlag, 1993, 165-177. Deransart P., Maluszyriski J.: \nRelating Logic Programs and Attribute Grammars. Journal of Logic Programming 2, 1985, 119-156. Deransart \nP., Maluszy&#38;dri J.: A Grammatical View on .Logic P~ogramming. The MIT Press, 1993 (To appear). Deransart \nP., Ferrand G., T6guia M.: NSTO Programs (Not Subject To Occur-check). In: Proc. Int. Symposium on Logic \nProgramming (Saraswat V., Ueda K., eds.), San Diego, 1991. The MIT Press, 1991, 533-547. Deransart P., \nJourdan M., Lorho B.: Attribute Grammars -Definitions, Systems and Bibliogra\u00ad phy. LNCS 323, Springer-Verlag, \n1988. Drabent W.: Do Logic Programs Resemble Pro\u00adgrams in Conventional Languages? In: Proc. 1987 Symposium \non Logic Programming, San Francisco. IEEE Computer Society Press, 1987, 389-396. Earley J.: An Efficient \nContext-Free Parsing Al\u00adgorithm. CACM 13, 2, 1970, 94-102. Fribourg L.: SLOG: A Logic Programming In\u00adterpreter \nBased on Clausal Superposition and Rewriting. In. Proc. 1985 Symposium on Logic Programming, Boston. \nIEEE Computer Society Press, 1985, 172 184. Gronholm C.: Metainterpretation in Logic Pro\u00adgramming. MSC \nThesis (in Finnish), Department of Computer Science, University of Helsinki, 1993 (To appear). Hanus \nM.: On the Completeness of Residuation. In: Proc. Joint Int. Conference and Symposium on Logic Programming \n(Apt K., cd.), Washing\u00adton, 1992. The MIT Press, 1992, 192 206. Jazayeri M., Odgen W. F., Rounds W. C.: \nThe In\u00adtrinsically Exponential Complexity of the Circu\u00adlarity y Problem for Attribute Grammars. CA CM \n18, 1975, 679-706. [KeW76] Kennedy K., Warren S. K.: Automatic Genera\u00adtion of Efficient Evaluators for \nAttribute Gram\u00admars. In: Cont. Record of the 3rd ACM Sympo\u00adsium on Principles of P~ogramming Languages, \nAtlanta, 1976, 32-49. [StS86] Sterling L., MIT Press, Shapiro 1986. E.: The Art of Prolog. The [Km168] \nKnuth guages. 127-145. D.: Semantics Mathematical of Context-Free Systems Theory 2, Lan\u00ad1968, [L1087] \nLloyd J.W.: Springer-Verlag, Logic 1987. Programming, 2nd ed. [LRS74] Lewis P. M., Rosenkrantz D. J., \nStearns R. E.: tributed Translations. Journal of Computer System Sciences 9, 1974, 279-3(Y7. At\u00adand [Ma}91] \nMaluszyfiski J.: Attribute Grammars and Logic Programs: A Comparison of Concepts. In: Proc. Int. Summer \nSchool on Attribute Grammars, Ap\u00adplications and Systems, Prague, 1991. LNCS 545, Springer-Verlag, 1991, \n330-357. (MBB+931. Ma}uszvfiski J.. Bonnier S.. Bove J.. Kluiniak F., Ki%g;dal A., Nilsson U.: Logic-Programs \nwith External Procedures. In: Logic Programming Languages -Constraints, Functions, and Objects (Apt K.R., \nde Bakker J.W., Rutten J.J.M.M., eds.). The MIT Press, 1993, 21 48. [MoR92] Moreno-Navarro J., Rodriquez-Artalejo \nM.: Logic Programming with Functions and PredL cates: The Language BABEL. Journal of Logic Programming \n12, 1992, 191-223. [Nai91] Naish L.: Addkg Equations to NU-Prolog. In: Proc. 3rd Int. Symposium on Programming \nLan\u00adguage Implementation and Logic Programming, Passau, 1991. LNCS 528, Springer-Verlag, 1991, 15-26. \n[Paa91] Paakki J: PROFIT -A System Integrating At\u00adtribute Grammars and Logic Programming. In: Proc. 3rd \nInt. Symposium on Programming Lan\u00adguage Implementation and Logic Programming, Passau, 1991. LNCS 528, \nSpringer-Verlag, 1991, 243-254. [Paa93] Paakki J.: Multi-Pass Evaluation of Functional Logic Programs. \nResearch Report LiTH-IDA-R\u00ad93-02, Department of Computer and Information Science, Linkoping University, \n1993. [PGH93] Paakki J., Gyim6thy T., HorvAth T.: An Integrated Method for Algorithmic Debugging of Logic \nPrograms. Submitted for publication, 1993. [P1U90] Pliimer L.: Termination Proofs for grams. Lecture \nNotes in Artificial 446, Springer-Verlag, 1990. Logic Pro-Intelligence [Red84] Reddy U. S.: Transformation \nof Logic into Functional Programs. In: Proc. posium on Logic Programming. IEEE Society Press, 1984, 187-197. \nPrograms 198~ Sym-Computer [ReT89] Reps T., Teitelbaum erator. Springer-Verlag, T.: The 1989. Synthesizer \nGen\u00ad Appendix Two-phase interpreter I(P). In.puti A phase partition (Span. (p), Comp(p)) for each predlcatepof \nP; A query tgl, ....g~ (giving rise to the start clause S +-gl, ....g~). output A proof tree T for P, \nor failure. Auxiliaries: (1) Let C be a clause in P. We denote by Func(C) the set of synthesized spanning \npositions containing a functional term in the head of C. (2) Let traverse(T) be an algorithm that traverses \nthe incomplete proof tree T and visits each completing position of a node in T exactly once. We denote \nby Nezt(T) a function that returns the next position in T to be visited by traverse(T). For each call, \nNeat(T) returns a pair (node, pos) such that pos &#38; Conap(p) where p is the predicate of node (with \n(nil, O) as the last pair). Algorithm: procedure Resolve (atom g); begin Let p be the predicate symbol \nof g; Choose a clause C = h + al, .... am from P such that g and h partially unify wrt Spun(p) \\ FUTZC(C) \nwith mgu u; if no such C exists then backtrack end if; for each node n in T, replace its label 1 with \nla end for; e := eu; fori=l torndo Push a node with label aiOJ into T as the i-th chdd of the node for \ng; Resolve(aiOJ) end for; if Func(C) # 0 then  if gd and ((h6/.Func(C))J/.unc(C) ))) partially unify \nwrt Fwnc(C) with mgu 6 then for each node n in T, replace its label 1 with M end for; 0 :=66 else backtrack \nend if end if end Resolve. phase 1: begin T := empty tree; 6 ;= 0; Push a node with label S into T; \nfori=l tondo  Push a node with label giOJ into T as the i-th chdd of S; Resolve(g;$l) end for end; \nphase 2: begin if T = empty tree then output failure else while Nezt(Z ) # (nil, O) do Let (n, j) := \nNezt(Z ); (note that j ~ Comp(n), Comp(n) # 0) Let b be the label of node n; Let n be the i-th child \nof a node with label a and with k children labelled bl ,..., bk(k > 1) (that is, b = bi for some i < \n{1, ....k}). Let Cl be the clause of P whose partially reduced instance (wrt the sequence (Sa, S~l,..., \nS~, S*h))*h)) is a + bl,..., b, = b,...,b~; Let n have 1 children labelled c1 ,..., C1(1 > o); Let Cz \nbe the clause of P whose partially reduced instance (wrt the sequence (S*, SC,, .... S..)) is b +-cl, \n....Q. Replace the label b of n with atom b = (bu/{j})l/{j} for a most genemd substitution u such that \n (1) a t bl,...,, ,..., bk is a partially reduced instance of c1 wrt the sequence (S~, Sbl , .... Sb \nU {~},..., sbk), and (2) b +-C,,..., cz is a partially reduced instance of C2 wrt the sequence (Sb U \n{j}, 5 .1, ....s.l).  if no such c exists then backtrack to phase 1 else for each node n in T, replace \nits label 1 with la end for; 9;= eu end if; end while; end i~ output T.  end. \n\t\t\t", "proc_id": "174675", "abstract": "<p>An operational semantics for functional logic programs is presented. In such programs functional terms provide for reduction of expressions, provided that they ground. The semantics is based on multi-pass evaluation techniques originally developed for attribute grammars. Program execution is divided into two phases: (1) construction of an incomplete proof tree, and (2) its decoration into a complete proof tree. The construction phase applies a modified SLD-resolution scheme, and the decoration phase a partial (multi-pass) traversal over the tree. The phase partition is generated by static analysis where data dependencies are extracted for the functional elements of the program. The method guarantees that all the functional terms of a program can be evaluated, and no dynamic groundness checks are needed.</p>", "authors": [{"name": "Jukka Paakki", "author_profile_id": "81100400307", "affiliation": "Department of Computer Science, University of Jyv&#228;skyl&#228;, P. O. Box 35, FIN -40351 Jyv&#228;skyl&#228;, Finland", "person_id": "P150683", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/174675.177965", "year": "1994", "article_id": "177965", "conference": "POPL", "title": "Multi-pass execution of functional logic programs", "url": "http://dl.acm.org/citation.cfm?id=177965"}