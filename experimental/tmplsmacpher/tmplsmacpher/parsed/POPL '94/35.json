{"article_publication_date": "02-01-1994", "fulltext": "\n Selective and Lightweight Closure conversion Mitchell Wand and Paul Steckler College of Computer Science \nNortheastern University 360 Huntington Avenue, 161CN Boston, MA 02115, USA {wand,steck}@ccs. neu.edu \nAbstract We consider the problem of selective and lightweight clo\u00adsure conversion, in which multiple \nprocedure-calling prot o-COISmay coexist in the same code. Flow analysis is used to match the protocol \nexpected by each procedure and the pro\u00adtocol used at each of its possible call sites. We formulate the \nflow analysis as the solution of a set of constraints, and show that any solution to the constraints \njustifies the resulting transformation. Some of the techniques used are suggested by those of abstract \ninterpretation, but others arise out of alternative approaches. Introduction Modern compilers perform \na variety of program analyses in order to produce good code. The goal of the analyses is to annotate \na program with certain propositions about the behavior of that program. One can then apply optimizations \nto the program that are justified by those propositions. For imperative languages, the justification \nof such opti\u00admiz ations or transformations has been investigate ed for many years and is reasonably well-understood. \nFor higher-order languages su ch as Scheme, ML, or Haskell, however, it has proven remarkably difficult \nto specify the semantics of these propositions in a way that justifies the resulting transforma\u00adtions. \nIn this paper, we study one such transformation, called ciosrwe conversion. In a higher-order language, \na procedure is typically represented by a record consisting of a piece of pure code (a A-expression with \nno free variables) and the val\u00adues of the free variables of the original procedure. This data structure \nis called a closure. In closure conversion, these data structures are built at the source-language level. \nProce\u00addure creation is replaced by closure creation, and procedure application is replaced by invocation \nof the first component of the closure on the actual parameter and the closure itself. This is sometimes \ncalled closure-passing style [.5]. For example, we might convert Work supported by the National Science \nFoundation and DARPA under grants CCR-9002253 and CCR-9014603 Permission to copy without fee all or part \nof this material is granted provided that the copies are not mede or distributed for direct commercial \nadvantage. the ACM copyright notice and the title of the publication end its date eppeer, end notice \nie given that copying is by permission of the Association for Computing Machinery. To copy otherwise, \nor to republish, requires e fee end/or epecific permission. POPL 94-1/94, Portland Oregon, USA ~ 1994 \nACM O-89791-636-9194A201 ..$3.50 let~ = Azy.Az.(z g z) g = Ah. h3 in g(~uv) to let $ = Axy.[(Aez. destr \ne(ky.(z Y z))), [z, gr]] g= Ah. apph3 in g(~uv) where app is defined by app = Jr. (fst r)(snd r) Here \n~ takes two arguments and emits a procedure of one argument; in the closure-converted version, the procedure \nis represented by a record. g takes a procedure represented as a closure record and applies it by taking \nthe piece of code and applying it to the record of free variables and the actual parameter. destr restructures \nthe record of free variables and binds the pieces to z and y, thus recreating the desired environment \nfor the procedure body (z y z). Closure conversion is a crucial step in the compilation process for \nhigher-order languages. The code parts of clo\u00adsures are closed terms, so they can be moved to the top \nlevel of the program if desired. Furthermore, this conversion is a source-to-source transformation, so \nthe represent ation of closures can be optimized using all the tools available to the compiler. In general, \na closure-converted program is not equivalent to the original program under ~-conversion or under any \nreasonable A-theory, so it is a difficult step to justify. In this paper, we prove the correctness of \na closure conversion algorithm that is generalized in two ways: 1. Closure conversion is not performed \neverywhere, so call sites obeying the closure record application proto\u00adcol coexist with sit es which \nuse ordinary A-application. We call this feature seiective closure conversion. For selective conversion, \nthe analysis must distinguish data flows consisting of procedures converted to clo\u00adsures from those which \nthe translation will leave un\u00adchanged. Our example already showed the utility of this feature: the value \nreturned by ~, which was a procedure that escaped the scope of its free variables, was represented as \na closure, but the other procedures were not. In general, one might have many different procedure\u00adcalling \nprotocols coexisting in the same program. For example, procedures coming from some library might have \na different protocol from those defined in the client program, and the compiler must keep these sepa\u00adrate. \nAnother example is an interpreter based on deno\u00adtational semantics, in which one might have radically \ndifferent representations for environments, procedures, and cent inuations; one needs to replace each \napplica\u00adtion by apply -env, apply -procor apply-cent. 2. In many cases, a closure need not include all \nthe free variables of a procedure, because some of the free vari\u00adables are available at all the places \nwhere the closure might be invoked. We call such variables dynamic, be\u00adcause they behave the same as \nif the language used dynamic scope, and we call the incomplete closures lightweight closures. Intheextreme, \nthe closure might be able to find all its free variables at the call site, so no closure need be created. \nAvoiding closure cre\u00adation plays unimportant part increasing good code for higher-order languages [4]. \nOur results give a method for proving the correctness of conversion algorithms which in appropriate cases \navoid building closures. These results are significant in several ways: First, we believe the analysis \nmaybe turn out to bea useful one for real languages. Second, it provides a example of the formal justifica\u00adtion \nof a fairly complex analysis-based program trans\u00adformation. We believe the techniques and approach we \nhave used will be useful for other analyses and transformations. Last, the techniques are suite different \nfrom those of conventional abstract interpretation. We discuss this further in Section 8. The outline \nof the paper is as follows: 1. Section 2presents the transformation and some exam\u00adples in more detail. \nSection 3 presents the input and output languages to be used. Our input language is an untyped A-calculus \nwith constants, boolean values, and conditionals. Our output language adds records to the input language. \nBoth input and output languages use call-by-value evaluation [12]. 2. Inorder to distinguish distinct \noccurrences of identical terms and to eliminate explicit substitution, we use a representation of terms \nusing occurrences (i.e., parse tree nodes) and environments. We prove that the oc\u00adcurence evaluator simulates \nthe original (Section 4). 3. In Section 5wegive thesyntax andsemantics ofalan\u00adguage of propositions \nfor occurrences. The semantics allows us to judge whether an proposition is true at a particular value \nor environment. 4. We annotate a program by assigning every phrase in the program a set of input propositions \nand an output proposition. We give a set of conditions defining when a program s annotations are locally \nconsistent. Every  program has a minimal locally-consistent annot ation, computable by a conventional \niterative algorithm. We show the soundness of our annotation scheme by prov\u00ading that any consistently-annotated \nprogram makes all its annotations true: if the inputs to a node satisfy the node s input proposition, \nthen its outputs satisfy its output proposition. This is analogous to the sound\u00adness of ordinary Hoare \nlogic. (Section 6). 5. IrI Section 7 we finally define the closure-conversion transformation from annotated \nsource programs into the target language, and we show the correctness of the transformation. The correctness \ntheorem says that if we have a consistently-annotated source program that evaluates to some value, then \nthe transformed program evaluates to the transform of that value. In particu\u00adlar, if the source program \nevaluates to some constant, then the transformed program evaluates to the same constant. We close with \na comparison with related work and some directions for further work. 2 Some Examples In closure conversion, \na X-calculus term (Xx .J4) is replaced by a closure record r = [( Jez. destr e (~ul . . .un, Jf)), [til, \n. . . ,Un]] where ul, ..., u~ are the free variables of (Jz .lf), and destr is a record-restructuring \noperation (see Section 3). The record consists of a closed A-term and a record of the free variables. \nIf r is bound to such a closure, then ~ can be applied to x by writing app ~ z, where app is defined \nby app = Ar. (fst r)(snd r) An easy calculation shows that app r is convertible to kc .J1. Another version \nof closure conversion defines app to be Ar.(fst r)r. This is similar to the protocol used in [5]. With \nthis protocol, the code body would become (AT-UI . . . v~x. destr (snd T)(AuI . . . a~.iM )) This protocol \nallows more freedom in converting Yexpres\u00adsions; for example, a closure with no saved variables could \nbe represented by [(h, . . . Wmz.iw )] This protocol would have the added benefit that recursive procedures \ncould be represented easily. All the results in this paper work for this protocol as well; the only thing \nthat would need to be reproved is Theorem 3. Having this pro\u00adtocol coexist with the one in Figure 4 is, \nof course, another application of the analysis we have presented. 2.1 Selective Conversion Our goal is \nto use flow analysis to distinguish procedures and call sites for translation to a language in which \nthere are mul\u00adtiple procedure-calling protocols. Initially, let us consider the case where there are \nexactly two protocols: ordinary A-calculus application and closure applications, as in our in\u00adtroductory \nexample. We assume that unknown procedures are A-calculus functions. Thus each application must use one \nof the two protocols, depending on whether closure records or ordinary functions are involved. Therefore, \nthe transformation must assure that all the procedures that might be applied at a given call site agree \non their application protocol. To do this, we must perform a flow analysis. Thus, in let~=kz .... g= \nAx..<. inlet h = Ay. if zero? y then $ else g in Xzy. (hy)x  the J-expressions for ~, g, and h can be \nconverted to clo\u00adsures, and both the applications in the last line of the pro\u00adgram can be converted to \napplications of closures. On the other hand, in Ag. letj=Az..,. in let h= Ay.if zero? uthen j else g \nin Xzy. (hy)z the value of (hrj) might be either $ or the unknown function g. Therefore the application \nof (by) to z must be an ordinary application, and ~ cannot be closure-converted. 2.2 Lightweight closures \nIf some of the free variables in a function are available at the call site, we may be able to omit those \nvariables from the closure. To do so, we must be sure that the bindings for the omitted variables are \nthe same at the call site as at the point of function definition. Consider: Ay.letj = lz.~z.(z y z) \ng = N1.h3 in g($v)  One might expect to create a closure record for the (Az. . ..) procedure, as we \ndid before, cent aining values for the vari\u00adables x and II. But the only call site for this procedure \n(at (h3)) is within the scope of y, so we can omit y from the closure and make it an additional argument \nat the call site: /ly.let f = Xz.[(Aeyz.(z -y z)), [z]] g=A~.appk~3 in g(f~)  We call a variable like \ng, which the procedure expects to pick up at the call site, a dynamic variable. We call a closure like \nthis one, in which the dynamic variables are not included, a lightweight closure. Now things are more \ncomplicated: all procedures at a call site must agree not only on whether to use ordinary ap\u00adplication \nor closure application, but in the case of closure application they must also agree on which variables \nare dy\u00adnamic, and on the order in which they should be supplied at the call site. So now we have exponentially \nmany possible procedure-calling protocols to be managed. The flow analysis must assure not only that \nthe dynamic variables of each procedure are present at every csll site for that procedure, but also that \nthose variables have the same values they had at the procedure-creation site. A free variable of a procedure \nmay be lexically available at a call site, but wit h a different binding. This can happen when a procedure \nescapes from its original bindings but is then passed in to a new invocation of the original lexical \nscope. For example, consider letg = (kg. let~ = (~z.zz) in yf) in g c1 (g C2(,lv.v)) Variable z is in \nscope at the call site (y f). But there are two calls to g, with two invocations of the scope for x. \nFor the left-hand call, z is bound to CI; for the right-hand call, x is bound to cz. During the left-hand \ncall to g, y is bound to (Jz. zz), which has escaped from the other invocation of z s scope. Therefore, \nthe flow analysis must assure that z is not made a dynamic variable. 3 The Language Our input language \nA,n is an untyped A-calculus with con\u00ad stants, boolean values, and conditionals. Our output lan\u00adguage \nAOut adds records to the input language. We use [M:,..., M:] to build a record with n components (n > \nO). If the value of Jf is a record with n components, then the term destr MN applies N to the n components. \nFor exam\u00adple, we define fst = AT. destr r (Xzy.z) snd = AT. destr r (ky.y) Note that the rule for destr \nincludes the possibility that n = O. It would be straightforward to extend the results of this paper \nto include records in the input language. Both input and output languages use call-by-value eval\u00aduation. \nFollowing [12], we define a value to be a term that is a constant, variable, abstraction, or a record \nof values. We define a relation ~ between terms and values. The rules defining ~ are given in Figure \n1. In addition to these rules, there are the usual rules for application of constants such as succ, pred, \nzero?, etc. 4 The Occurrence Evaluator Our flow analysis needs to keep track of what procedures in the \nsource program are applied where. Unfortunately, the call-by-value evaluator sketched above uses substitution, \nwhich throws away information about the source of a pro\u00adcedure: it builds terms that no longer contain \ninformation about how the term was obtained from the source program. To keep track of the source of each \nterm, write an occurrence evaluator that represents a term as a substitution instance of a subterm of \nthe original program. To do this, we will need to refer to particular occurrences of terms, allowing \nidentical subterms with differing annota\u00adtions to be distinguished. To do this, we imagine that we are \ntraversing the parse tree of a single program. An occurrence is then an address in the tree, given as \na finite string over the alphabet {rater-, ram-l, bv, body, test, then, else} We can use these terms \nto navigate in the parse tree in the usual way: the empty string represents the root of the tree, Vyv \nCyc Ax.M ~ ~z.M M ~ ~x.Q N~ N Q[N /x] ~ V MN~V M ~ true if Mthen Nelse P~ N M ~ false if Mthen Nelse \nP=p P Vi, l<i<n, M,~~ [M,,..., Mn] ~ [VI,..., VJ M~ [VI,..., V.] N ~ /iX1 . ..xn.Q QIVI/Xl,..., Vxn]n] \ny W destrMN~ W Figure 1: Rules for the term evaluator i. body. rcmd represents the operand of the body \nof node i, The correctness theorem for the occurrence evaluator if z is a node that denotes an abstraction \nwhose body is says that the occurrence evaluator simulates the operation an application, etc. We write \n[i] for the term represented of the term evaluator on the unwound occurrence closures: by node Z, and \nApp (i), Abs (z) for the predicates that test whet her node i is an application, abstraction, etc. We \nwill Theorem 1 (Simulation) I_j similarly provide addresses for constants, and for terms that appear \nin the initial environment. We will write M{p} for the term obt ained by performing a substitution p \non term M. then U[(i, t))] ~ U[(j, TJ )] We next define the data structures manipulated in the evaluator. \nProofi By induction on the definition of ~ . 9 Definition 1 The following definitions are mutually refer\u00adential. \n5 The Language of Annotations 1. An occurrence closure (i,+) is a pair consisting of an  occurrence \nindex and an occurrence environment. We want to annotate each occurrence with a proposition that describes \nits possible values. For our analysis, this de\u00ad 2. An occurrence environment @ is a finite map from \n  scription includes the following information: variables to occurrence closures, where for each z G \nDorn (~), ~(z) = (i, ~ ) implies [i] is a value in A,n.  1. A flow, which describes the set of procedures \nto which the given occurrence might evaluate, The meaning of occurrence closures is given by the map \n.?J[-] defined recursively on this definition by: 2. A protocol tag, which describes the protocol used \nby all the procedures to which the occurrence might evaluate, U[(i, ~)] = [z]{ ZJ[-] o ~} and 3. An invariance \nset, which describes the set of variables finite depth by Definition 1; termination occurs when @ is \nThis definition is well-founded since occurrence closures have whose values are left invariant by evaluation \nof the empty. We call /-/[-] the unwinding function. occurrence. The occurrence evaluator uses occurrence \nclosures to sim-An invariance set is a set of variables; flows will be de\u00adulate substitutions. We define \na relation ~ between oc\u00adfined below. A protocol tag is an element of {id} U {cIC I a c currence closures \nthat represent terms and occurrence clo-VaT*}. If a is the sequence (vi, . . . . v~), we write [al for \nsures that represent values. The rules for the occurrence the unordered set {VI, . . . . v~}. evaluator \nare given in Figure z. Var (i) (2, @) ~ @([i]) Const (2) (i,@) ~ (2,0) Abs (i) (2, +) * (i,@) Cond \n(z) (i.test, ~) ~ (j, ~ ) [j] = true (i.then, TJ) ~ (k, + ) (2,?)) ~ (k, ?) ) C ond (z) (i.test, ~) \n~ (j, #) [j] = false (i.else, ~) ~ (k, ~ ) (2, ?/)) ~ (k, + ) App (i) (i.rater, ~) ~ (j,@ ), Abs (j) \n(i.rand, +) ~ (k, @ ) (j.bodw #)[[j.bvJJ w (k 4 )] ~ (m, 4 ) (i, +) ~ (m, ~ ) Figure 2: Rules for the \noccurrence evaluator Definition 2 The following definitions are mutually refer\u00adential. 1. A proposition \nT is a triple consisting of a flow, a pro\u00adtocol tag, and a set of variables. 2. A flow @ is a finite \nset of abstract closures. 3. An abstract closure (i, A) is a pair consisting of the occurrence index \nof an abstraction and a proposition environment. 4. An proposition environment A is a finite map from \nvariables to propositions.  Our goal is to annotate each occurrence i with an anno\u00adtation environment \nA, and a proposition P, such that if this occurrence i is evaluated in some occurrence environment ~, \nand the value $(z) of each variable z free in [i] satis\u00adfies the assumption A,(z) that A, makes about \nit, then the output satisfies the proposition ?,. Thus the pair (A,, P,) is the functional analog to \na Hoare-style partial correctness assertion. To do this, of course, we have to explain what it means \nfor an occurrence closure to satisfy a proposition. Define a protocol assignment to be a map from occurrences \nto proto\u00adcol tags. Definition 3 The following definitions are mutually recur\u00adsive. 1. An occurrence environment \n@ satisfies a set of anno\u00adtation assumptions A under protocol assignment D, as follows: @A iff Vz c Dcm(A) \n(a) z c Dorn(tj) and @(z) ~ A(z), and (b) If A(z) = (~, x, O), +(z) = (i, $ ), and y c 0, then v G Dom(ti) \nn Donz(@ ), and @(y) = + (y)  2. An occurrence closure (i, $) satisfies a proposition (4, x, 0) under \na protocol assignment II, as follows: (i, @) ~ (@,ir, O) ill (a) [i] is a constant, variable, true, or \nfalse, or (b) II(i) = ~ and there exists A such that (i, A) c #  and @~ A. For @ ~ A, the first condition \nsays that for any vari\u00adable ~, the occurrence closure @(z) satisfies the associated proposition A(z). \nThe second condition says that if we look at the occurrence closure (i, ~ ) that we get by looklng up \na variable x in ~, then @ and ~ agree on the variables in the invariance set /3. For (i, ~) # (4, r, \n0), the first condition tells us that the propositions only deal with the J-terms that may arise as values. \nThe second condition says that i must be a appear in an abstract closure listed in the flow +, and furthermore \nthat its accompanying environment must satisfy the proposition environment associated with z in the flow. \nThis is roughly equivalent to saying that U[(i, @)] is a substitution instance of [i] where the substitution \nsatisfies A. We can order the set of propositions by setting (4, m, 8) < (~ ,m , t? ) if ~ ~ + , z = \nm , and 0 ~ 0. Then we have Lemma 1 1. If (i, ~) # (~, T,8) and (q$, rrj O) < (~ , m , 8 ), then (2,4) \n# (# , T , #). 2. If for all z, A(z) < A (z), then @ # A implies @ # A .  Proofi Immediate. n The analysis \ncould be generalized to track scalar values or other flow properties; see [1 I]. In this case, the set \nof tags would be replaced by a partial order, and the condition II(i) = ~ in Definition 3.2(b) would \nbe replaced by II(z) L n. Consistent Annotations An armotat~on map is a map r that associates with each \noc\u00adcurrence t an proposition environment A, and a proposition ~~ = (d~,~~)o~). An annotation map is the \nfunctional analog of a Floyd\u00adHoare-style annotation of a flowchart or imperative pro\u00adgram. As in the \nimperative case, the plan is to find some local conditions such that if the local conditions are satisfied \nat every node, then the annotations will all be true. Our local conditions are shown in Figure 3. If \nan an\u00adnot at ion map r satisfies these conditions at every node, we say r is locally consistent. Observe \nthat those portions of the conditions not dealing with protocol tags and invariance sets are the same \nas those for ordinary closure analysis in the style of [13], transposed to our notation. The conditions \non protocol tags and invariance sets are a bit more involved; the motivation behind all the conditions \nwill be discussed further in the proof of Theorem 2. Furthermore, we say that an annotation map r is \nmono\u00advariant if for every flow @ considered in the annotations of r, if (j, B) c d, then B = A, (recall \nthat A, is the proposi\u00ad tion environment associated with occurrence J by 17). More formally, we can say \nDefinition 4 (Monovariance) 1, An annotation map 17 M monovar-iant tflfor all occur\u00adrences i, the annotations \n(A,, P,) are J7-rnonovar-iant. .2. A f70w 4 is r-monovarwant zff for all (j, B) c q5, B = Aj and B is \nr-mono variant. 3. A proposition (q$, x, $) is r-mono. ariant iff ~ is 17\u00admonovariant. 4. A proposition \nenvironment A is r-monovariant iff for all x c Donr (A)j A(x) is I -monovariant.  As before, the empty \nproposition environment provides the base case for the definition. In a monovariant annotation, flows \ncan be represented as sets of occurrences, rather than sets of occurrence closures. Hence there are only \nfinitely many monovariant annotations for any given program. Most monovariant closure analyses put only \noccurrences, but not environments, in their flows. By including propo\u00adsition environments in abstract \nclosures, our version allows the semantics of propositions to be defined independently of a global annotation \nmap. We can use the conditions of Figure 3 as an iteration operator on monovariant annotations: Given \na monovariant annotation, we can use the rules of Figure 3 to increase the P, by adding new elements \nto the @, and shrinking the 9,, (See [II] for additional discussion.) Notice that this iteration operator \nis the identity on the protocol tags. We can obtain a monovariant, locally consistent annota\u00adtion by \ninitializing the algorithm with a <-minimal annota\u00adtion (initializing all the ~, to empty, all the x, \nto id, and all the 0, to the set of all variables in the program), and then applying the iteration operator \nuntil a fixed point is reached. Since all the protocol tags are id, the condition ~t.rater = CZL7 * [al \nS 6 z, ratOr cannot fail. (One could equally well initialize all the protocol tags to Cl.; the iterative \nalgorithm would compute the same flows and invariance sets). Since there are only finitely many monovariant \nannotations, and the algorithm monotonically improves each annotation according to the < order, the al\u00adgorit \nhm always terminates, and yields a locally-consistent monovariant annotation. One can then search for \nother protocol assignments that are consistent; we leave the algorithmic of this search as an open problem. \nWe can now state the first main theorem: Theorem 2 (Soundness) Let I be a locally consistent, monovariant \nannotat~on map. Let II be the protocol assignment defined by II(z) = r,. Let i be any occurrence zndexj \nand let @ be an occurrence envi\u00ad ronment such that ~ ~ A,. If (i, +) ~ (j, O ), then 1. (j,+ ) # ((#A,7rt, \not) .2. Vz c 0,, z E Dorrz(@), z c Dom(@ ), and @(z) = ?/(z). Proofi (Sketch). The proof is by induction \non the struc\u00adture of the proof that (i, ~) ~ (j, + ). Since the definition of ~ is by cases on the form \nof [z], the proof follows this form as well. Here we sketch the proof to give some intuition behind the \nconsistency rules; pieces of a more detailed proof is given in the appendix. The various conditions on \nA, embody the usual lexical scoping rules. For an abstraction, we have (i,@) ~ (i, 4). We have {(z, A,)} \ns +, by the local consistency of r, @ ~ A, by assumption, and T, = II(i) by construction of II, so (i, \n~) # (~,, XL, o~). The second condition holds trivially. A, test = Ai. then = Ai. eise = A:} Cond (z) \n+ P,,then < P,, and P,,e&#38; < P,, { Figure 3: Local Consistency Conditions for Annotations Suppose \n(z, +) is an application, with the following proof for its evaluation: (2. WdO~, ?j) ~ (j, $ ), Abs (j) \n(i.rcmd, TJ) =J. (k,+ ) (j. body, TJ [~.bzJ] + (k, 4 )]) + (m, 4 ) The operator evaluates to (j, ~ \n), so by the induction hypothesis (j, AJ ) must be in ~,.rat~r. So any value of the operand i. rand is \na possible value of j. bv, and any value of the procedure body j. body is a possible value of the applica\u00adtion \ni. These observations give rise to the requirements that P ,.rand < Pj,bv and p3, body < pt. What values \nin @ are left invariant by this calculation? + is obtained from @ in two steps: First the operator is \nevaluated, yielding a + agreeing with @ on the vari\u00adables in 0, ,rator. We then evaluate j. body in an \nextension of @ , yielding 4 , which agrees with @ on 8J, body ~. bv]. Hence we must have 8, ~ @L,ra~OrU \n6 jbOdY [j. bv]. This .. explains the conditions 0~ ~ O!.ratOr, et ~ @J,body , and [i~~] @ Oi. The \nCondp [j.b~l $? Oj,bw is necessary so that @ [[j. M I+ (~, d )] K Aj body to support the induction. A \nmore detailed exposition is given in the appendix. m 7 The Closure-Conversion Transformation Once we \nhave annotated the program, we are ready to per\u00ad form closure conversion. In closure conversion, each \nabstrac\u00adtion term (kc.ikf) that is marked by the tag cIC is replaced by a closure record [(~ezq . . .v~z. \ndestr e(~ul . ..un.lf )). [ul, . ... an]] where (V1...Vm) = u, {ul ...~~} = ~V(~z.Jf) [al, and Jf is \nthe closure-conversion of M. Each application MN whose operator is marked by the tag Cia is replaced \nby app J4 VI . . . Vm N , where (VI . . . V~) = a, app = Ar. (fst r)(snd r) and M and N are the converted \nversions of M and N. It is easy to see that if r is the record above, then app r V1 . . . Vm converts \nto Jz.,kf . In this way the dynamic variables are passed at call-time. For a = (), this is the same as \nordinary closure conversion. Note that the exact details of T are unimportant, so long asapp Tvl . . \n. v~ converts to Az. Jf ; for example, we have slightly different code for the case where n = O. Of course, \nthis description is not quite accurate, since the transformation needs to know the annotations, not just \nthe terms. Therefore we formulate the transformation as a map from the set of occurrences to the language \nof output terms. The rules are given in Figure 4. We extend the transformation to occurrence closures \nby recursively applying it in each occurrence environment: @(i, o) = 0(2){6 o ~} Since occurrence environments \nare of finite depth, ~ is well\u00adfounded. Now we are finally in a position to formulate the correct\u00ad ness \nof the transformation. The correctness theorem says that evaluating a transformed program gives the same \nre\u00adsult as transforming the value of the original program. In particular, if the program evaluates to \na constant, then the transformed program evaluates to the same constant, We can illustrate this theorem \nby a commutative dia\u00adgram: Var (2) =+ @(z) = [i] Const (t) =+ O(t) = [2] AtIs (i) An, = id =+ @(z) = \nkC.@(2. bOd~) Abs(i) A ?r, = Cl. =+ @(z) = [($~&#38;str,; (AiLiP(2.body))), [?-i]] andii=u and [ill \n= FV([Z]) [al App (z) A Con$t (i. rotor) ==$ 0(2) = qi.rater) @(i. rarsd) App (z) A ~ Const (z. rcstor) \nA m, ,a~o, = id =+-@(z) = @(i. rcstor) @(i. rand) App(t) A = Corzst (i. rotor) A ~, rater = c~~ =+ @(z) \n= app @(z.rater) v] . . . v~ ~(i.rcs~~) where; = u Cond (i) ==+ O(i) = if l?(i.test) then @(i. then) \nelse @(i. ebe) Figure 4: Definition of the transform flows. He then uses this to prove the correctness \nof a trans\u00adformation that replaces parameter-passing by assignment to (h+) ==j (Jo ) global variables. \n6 Shivers [15] presents a number of analyses, including one 11@ he calls OCFA. OCFA is equivalent to \nSestoft s closure anal\u00ad  6(2,?)) ~ @(j,@ ) ysis. He proves a soundness theorem for this analysis for \na rather complex language, including side-effects, but he does Theorem 3 (Correctness) Let 17be a monovar-iant \nlocally not prove the correctness of any transformations based on consistent annotation map, and II the \nprotocol assignment the analysis. Shivers work is done in the context of pro\u00addefined by Vi, II(i) = r,. \nLet@ be an occurrence environment grams in continuation-passing style. Our original develop\u00ad ment of \nthe flow semantics in Sections 5 6 was motivated by an attempt to generalize and simplify Shivers proof. \nsuch that @ ~ A,. I.. (2, O) ~ (j, ?/) The idea of approximating an occurrence closure by an abstract \nclosure comes from [9]; this is the straightforward then extension of the idea of record types from [10] \nto the occur\u00ad6(2, ~) ~ qj, 4 ) rence-closure evaluator. In [16] we gave an alternative proof of a closure-conver-Proofi \n(Sketch) The proof is by induction on the size sion algorithm by semantic methods. However, that was \na of the derivation that (z, d) ~ (j, 4 ). The soundness the-proof of a different property: it showed \nthat if the converted term produced a constant under certain restrictions, then orem is used to guarantee \nthat for each invocation of the the original term produced the same constant. Here we show induction \nhypothesis for an occurrence closure (k, ~ ), the the converse: that if the original term succeeds, so \ndoes the needed condition + ~ Ak is satisfied. m convert ed term. The paradigm of formulating a flow \nanalysis as the so-Corollary 1 Let r be a monovariant locally consistent an-lution of a set of equations \nis, of course, traditional in the notation map, and If the protocol assignment defined by Vi, realm of \nimperative languages. The idea of showing that II(i) = m,. Let @ be an occurrence environment such that \nany solution to the constraints yields a justified transforma\u00adtion was introduced in [17] in the context \nof binding-time @ # A,. If (i, 4) ~ (~, V ), where [j] is a constant c. Then analysis and off-line partial \nevaluation. 6(2,v) y c. Appel [4] gives a useful catalog of transformations; many of these will provide \ngood challenges for transformation\u00ad correctness proofs. Our transformation is superficially simi\u00ad 8 \nComparison with Other Work lar to Appel s callee-saves transformation, in that both pass The immediate \nsources for our work are Sestoft [13] and extra variables to the procedure. However, in callee-saves, \nShivers [15]. Sestoft [13] presents an analysis called closure the procedure is responsible to passing \nthose values un\u00adamdys is which he uses to detect definition-use chains in a changed to its continuation; \nwe do not use continuation\u00adhigher-order language. His closure analysis is equivalent to passing style, \nand the variables are passed to the procedure our use of flows. His soundness theorem [13, Lemma 5.1-1] \nfor its own use. appears to be equivalent to our Theorem 2 specialized to Heintze [8] and Aitken et al. \n[2, 3] have modeled pro\u00adgram analyses as solving constraint equations. Their work, however, has centered \non the algorithmic of solving con\u00adst raints, in forms more general than those considered here, rather \nthan on the use of the solutions to justify program transformations. Abadi et al. [I] use a similar representation \nof J-terms as substitution inst antes, and prove a result (Lemma 3.5) similar to our Simulation Theorem \n(Theorem 1). However, there is nothing like our flow analysis or lightweight closures in their system. \nIt is enlightening to compare our approach to a more conventional abstract -interpret ation approach, \nIn abstract interpret ation, one normally starts with a denot at ional se\u00admantics. One then defines a \nnon-standard interpretation by adding a cache or similar device to keep track of the differ\u00adent values \nto which each program variable can be bound. Then one takes an abstraction of the cache interpretation. \nBecause this analysis is more indirect, it is more difficult to relate the results of the abstract interpretation \nto the orig\u00adinal semantics, and therefore to use the results to justify a transformation. Furthermore, \nbecause we deal directly with operational semantics, we avoid any domain-theoretic diff\u00adiculties. The \ngreat advantage of the abstract-interpretation ap\u00adproach is that the algorithm for the analysis comes \nfor free, as a fixed point of the abstract-interpretation equations. Our approach factors the problem \ndifferently: we formulate the analysis as a constraint problem. We then show that any solution to the \nconstraints yields a correct t ransformat ion. Our technique does not provide an automatic method for \nsolving the constraints, though in many cases, such as the one here, the form of the equations leads \nto easy solution methods. Future Research We believe the techniques and approach we have used will be \nuseful for other analyses and transformations. It would be useful to compare this analysis with the lifetime \nanalyses [6, 7] for replacing heap-based closures by stack represen\u00adtations of procedures. As pointed \nout in [11, 14], closure analysis works for call-by-name evaluation also; extending our analysis to call-by-name \nlanguages should be straight\u00adforward. Another area for investigation is the integration of these techniques \nwith the verified-compiler technology sketched in [18]. Last, it would be desirable to have a better \nunderstanding of the relationship between our approach and that of conventional abstract interpretation. \nAcknowledgements We would like to thank Bob Muller, Flemming Nielson, and John Hughes for useful discussions \nabout closure analysis. Richard Kelsey provided useful insights into escape analysis and possible applications. \nA Appendix: Proof of Theorem 2 Proofi Induction on structure of proof that (I,@) ~ (j, 4 ) We show only \nthe cases where [z] is in the subset of A,n corresponding to terms of the pure J-calculus. The remaining \ncases are straightforward. case Var (z): Then (i,@) ~ (J, + ), where (j, #) = @([z]). Since r is $cally \nconsistent, we have A, ([z]) = (4;, 7,, O,).nsince @ K A,, we know that=t([i]) E (4,, T,, O,), so (j, \n#) ~ (~,, T,, 0,). Also since @ + A,, z c 6, implies z E Dorn(@) n Don(@ ), and @(z) = @ (x). case Abs(z): \nFor abstractions, we have (z, ~) ~ (z, +). First we show that (i, ~) ~ (~,, m,, 0,). By the definition \nof ~, this holds if fI(i) = rr, (true by the construction of If)j and if there is some (j, A) c d, such \nthat (i, 4) # (j, A). ~ is l:ally consistent, so {(z, A,)} ~ ~,, and by assumption, @ + A,. By the local \nconsistency condition, we have z c @t implies n x E Dom(At). Since 4 + At, z c Dom(At) implies z c Dorn(i), \nso Vz c o,, z 6 Dorn(Tj). case App (i): Suppose (z, ~) ~ (m, ~ ), so we have: (i.r-ator-, ~) ~ (j, 4 \n), Abs (i) (t.rand, ~) ~ (k, ~ ) (j. bodv, ti [[.i.bv] * (k, @ )]) ~ (m, + ) Then we deduce the desired \nresult by calculating as fol\u00adlows: (1) d FAt.rater, At r-and [A, = A, rat., = A, rami] (2) (2.rater, \n+) * (j, ?/)  [proof that (~, o) ~ (m, @ )]  (3) h-am) (i 4 ) F(baton ~,?-ator, [IH re soundness at \ni.r-ator] (4) YGOt,rater=+ Y e Dom(TJ) n Dom(qY), w)= 4 (?J) [IH re invariant at i.rater] (5) (i4) c \nA,nztor [clefs of ~, monovariance, (3)] (6) (i, ti ) p (i, 4) [by (5), def of ~] (7) # #A, [by (6), \ndef of $] (8) (i.rard, ~) ~ (k, T/ ) [proof that (z, ~) ~ (m, ~ )]  [by (7), (23), (24)] (9) (~, 0 \n) ~ (4, rand, ~t.mnd,~t.rand) [IH re soundness at z.rcmd] (26) AJ bodv = AJ[[j. bv] M ? J,bo] (lo) [clef \nof consistent annotation]  (27) V [[j.bvl H (~, 0 )1 ~ Aj, body [(25), (26)] (11) (28) (m, ~ ) ~ (@J, \nbodg, ~J b~d , j body) [IH re soundness at J. bocly ! (12) (29) Y e @j.body ~ y c ~om(~ [[j. bu] t-t \n(k, + )]) n Dom(+ ), O [[j.bv] w (k, @ )](y)= @ (y) (13) ~z.rand = ~3.bu [IH re invariant at j. bocly] \n[~, r-and < P, bv] (14) ~1 body = xc (30) (m, qb ) ~ (#t, 7r,,0,) [P,.body < Tz] [(12), (14), and (28)] \n(15) et g 8t, rater (31) Y E (@j,body lD. bvl) = [invariance set constraints] y E Dom(@ ) n Dom(# ), \n4 (Y) = 4 (Y) (16) [by (29)] (32) Y ~ (~, rat.. n (~j.body [j. bv])) =+ (17) y c Dom(@) n l)om(~ ), \n 4(Y) = 4 (Y) [by (4), (31)] (18) f] bu ~ Oi.rater [invariance set constraints] (33) y~eza y E Dom(@) \nn Dom(# ), WY) = W (Y) (19) [~~~1 @ ~j.bu [by (17), (15), (20), (32)][invariance set constraints] (20) \n[~.bv] @ 0, References [invariance set constraints] [1] Martin Abadi, Luca Cardelli, Pierre-Louis Curien, \nand (21) Y ~ y (@,,rater n c Dom(#) ~t,rand) * n Dom(# ), Jean-Jacques Levy. Explicit Substitutions. \nJournal of Functional Programming, 1(4):375-417, October 1991. W(Y) = + (Y) [2] Alexander Aiken and Brian \nR. Murphy. Static Type [by (4), (10)] Inference in a Dynamically Typed Language. In Cont. Rec. 18th ACM \nSymposium on Principles of Program\u00ad (22) Y E Oj.bu ~ ming Languages, pages 279 290, 1990. y c Dom(@ ) \nn Dom(@ ), [3] Alexander Aiken and Edward L. Wimmers. Solving + (Y) [by (18), = @ (Y) (16)] Systems of \nSet Proc. 7th IEEE Constraints Symposium (Extended on Logzc an Abstract). Computer In Sci\u00ad ence, pages \n329 340, 1992. (23) Y ~ 6 9.bu * y,6 ~om(i [[j. bv] t-+ @ [lb.bvll E+ (k, + )](Y) (k, = @ )]) n @ (Y) \nDom(q5 ), [4] Andrew W. Appel. Cambridge University Cornpalzng wtth Press, Cambridge, Contznuatzons. \n1992. [by (19)] [5] Andrew W. Appel and Trevor Jim. Continuation- Passing, Closure-Passing Style. In \nCont. Rec. 16th (24) ACM Symposium on Prtnctples guages, pages 293-302, 1989. of Programm~ng Lan\u00ad [6] \nAlain Deutsch. On Determining Lifetime and Aliasing (25) of Dynamically Allocated tional Specifications. \nIn Data in Conf. Rec. Higher-order 17th ACM Func-Sympo\u00ad sium on Principles of Programming Languages, \npages 157 168, 1990. [7] Benjamin Goldberg and Young Gil Park. Higher Order Escape Analysis: Optimizing \nStack Allocation in Func\u00adtional Program Implementations. In Proc. 3rd Euro\u00adpean Symposium on Programming, \nvolume 432 of Lec\u00adture Notes in Computer Science, pages 152 160, Berlin, Heidelberg, and New York, 1990. \nSpringer-Verlag. [8] Nevin Heintze. Set Based Program Analysis. PhD the\u00adsis, Carnegie-Mellon University, \nOctober 1992. [9] Neil D. Jones. Flow Analysis of Lambda Expressions. In International Colloquium on \nAutomataf Languages, and Programming, 1981. [10] Neil D. Jones and Steven S. Muchnick. A Flexible Ap\u00adproach \nto Interprocedural Data Flow Analysis and Pro\u00adgrms with Recursive Data Structures. In Conf. Rec. 9th \nACM Symposium on Principles of Programming Lan\u00adguages, pages 66-74, 1982. [II] Jens Palsberg and Michael \nI. Schwartzbach. Safety Analysis vs. Type Inference. Information and Com\u00adputation, to appear. [12] Gordon \nD. Plotkin. Call-by-Name, Call-by-Value and the XCaJculus. Theoretical Computer Science, 1:125\u00ad159, 1975. \n[13] Peter Sestoft. Replacing Function Parameters by Global Variables. Master s thesis, DIKU, University \nof Copenhagen, October 1988. [14] Peter Sestoft. Anolysis and Eficient Implementation of Functional Programs. \nPhD thesis, DIKU, University of Copenhagen, October 1991. [15] Olin Shivers. Control-Flow Analysis of \nHigher-Order .Languages. PhD thesis, Carnegie-Mellon University, May 1991. [16] Mitchell Wand. Correctness \nof Procedure Repre\u00adsentations in Higher-Order Assembly Language. In S. Brookes, editor, Proceedings Mathematical \nFounda\u00adtions of Programming Semantics 91, volume 598 of Lecture Notes in Computer Science, pages 294-311. \nSpringer-Verlag, Berlin, Heidelberg, and New York, 1992. [17] Mitchell Wand. Specifying the Correctness \nof Binding-Time Analysis. In Conf. Rec. .%.Xh ACM Symposium on Principles of Programming Languages, pages \n137-143, 1993. To appear in Journal of Functional Programming. [18] Mitchell Wand and Dino P. Oliva. \nProving the Cor\u00adrectness of Storage Representations. In Proc. 199.2 ACM Symposium on Lisp and Functional \nProgram\u00adming, pages 151 160, 1992. \n\t\t\t", "proc_id": "174675", "abstract": "<p>We consider the problem of selective and lightweight closure conversion, in which multiple procedure-calling protocols may coexist in the same code. Flow analysis is used to match the protocol expected by each procedure and the protocol used at each of its possible call sites. We formulate the flow analysis as the solution of a set of constraints, and show that any solution to the constraints justifies the resulting transformation. Some of the techniques used are suggested by those of abstract interpretation, but others arise out of alternative approaches.</p>", "authors": [{"name": "Mitchell Wand", "author_profile_id": "81100072594", "affiliation": "College of Computer Science, Northeastern University, 360 Huntington Avenue, 161CN, Boston, MA", "person_id": "PP39025873", "email_address": "", "orcid_id": ""}, {"name": "Paul Steckler", "author_profile_id": "81100131044", "affiliation": "College of Computer Science, Northeastern University, 360 Huntington Avenue, 161CN, Boston, MA", "person_id": "PP14056611", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/174675.178044", "year": "1994", "article_id": "178044", "conference": "POPL", "title": "Selective and lightweight closure conversion", "url": "http://dl.acm.org/citation.cfm?id=178044"}