{"article_publication_date": "02-01-1994", "fulltext": "\n Memory Subsystem Performance of Programs Using Copying Garbage Collection Amer Diwan David Tarditi Eliot \nMoss* School of Computer Science Carnegie Mellon University 5000 Forbes Avenue Pittsburgh, Abstract \nHeap allocation with copying garbage collection is believed to have poor memory subsystem performance, \nWe conducted a study of the memory subsystem performance of heap alloca\u00adtion for memory subsystems found \non many machines. We found that many machines support heap allocation poorly. However, with the appropriate \nmemory subsystem organi\u00adzation, heap allocation can have good memory subsystem performance. Introduction \nHeap allocation with copying garbage collection is widely believed to have poor memory subsystem performance \n[31, 38, 39, 24, 40]. To investigate this, we conducted an extensive study of memory subsystem performance \nof heap allocation intensive programs on memory subsystem organi\u00adzations typical of many workstations. \nThe programs, com\u00adpiled with the SML/NJ compiler [3], do tremendous amounts of heap allocation, allocating \none word every to 4 to 10 instructions. The programs used a generational copying garbage collector to \nmanage their heaps. To our surprise, we found that for some configurations corresponding to ac\u00adtual machines, \nsuch as the DECStation 5000/200, the mem\u00adory subsystem performance was comparable to that of C and *The \nauthors can be reached electronically via Internet addresses diwan@cs.umass. edu, dtarditi@cs.cmu. edu, \nmoss@ cs.umass.edu, This work was done while Amer Diwan and Eliot Moss were on leave from University \nof Massachusetts. This research is sponsored by the Defense Advanced Research Projects Agency, DoD, through \nARPA Order 8313, and monitored by ESD/AVS un\u00adder contract F19628-9 1-C-0168. Views and conclusions contained \nin this document are those of the authors and should not be interpreted as represent\u00ading the official \npolicies, either expressed or implied. of the Defense Advanced Research Projects Agency or the United \nStates Government. David Tarditi is also supported by an AT&#38;T PhD Scholarship. Permksion to copy \nwithout fee all or part of this material is granted provided that the copies ere not mede or distributed \nfor direct commercial edventage, the ACM copyright notice and the title of the publication and its date \neppeer, end notice is given that copying is by permission of the Association for Computing Mechinery. \nTo copy otherwise, or to republish, requires a fee and/or epecific permission, POPL 94-1~4, Portland \nOregon, USA @ 1994 ACM 0-69791 -636-0/94/001..$3.50 PA 15213 Fortran programs [10]: programs ran only \n16% slower than they would have with an infinitely fast memory. This perfor\u00admance is similar to that \nfor C and Fortran programs For other configurations, the slowdown was often higher than 100%, The memory \nsubsystem features important for achieving good performance with heap allocation are subblock place\u00adment \nwith a subblock size of one word combined with write\u00adallocate on write-miss, a write buffer and page-mode \nwrites, and cache sizes of 32K or larger. Heap allocation performs poorly on machines which do not have \none or more of these features; this includes most current workstations. Our work differs from previous \nreported work [31, 38,39, 24, 40] on memory subsystem performance of heap alloca\u00adtion in two important \nways. First, previous work used overall miss ratios as the performance metric and neglected the po\u00adtentially \ndifferent costs of read and write misses. Overall miss ratios are misleading indicators of performance: \na high overall miss ratio does not always translate to bad perfor\u00admance. We separate read misses from \nwrite misses. Second, previous work did not model the entire memory subsystem: it concentrated solely \non caches. Memory subsystem features such as write buffers and page-mode writes interact with the costs \nof hits and misses in the cache and should be simulated to give a correct picture of memory subsystem \nbehavior. We simulate the entire memory subsystem, We did the study by instrumenting programs to produce \ntraces of all memory references. We fed the references into a memory subsystem simulator which calculated \na perfor\u00admance penalty due to the memory subsystem. We fixed the architecture to be the MIPS R3000 [23] \nand varied cache con\u00adfigurations to cover the design space typical of workstations such as DECStations, \nSPARCStations, and HP 9000 series 700. All the memory subsystem configurations we studied had a write \nbuffer and page-mode writes. We studied eight substantial programs. We varied the following cache parameters: \nsize (8K to 128 K), block size (16 or 32 bytes), write miss policy (write allocate or write no allocate), \nsubblock placement (with and without), and associativit y (one and two way). We simulated only split \ninstruction and data caches, i. e,, no unified caches. We report data only for write-through caches but \nthe results extend easily to write-back caches (see Section 5.2). Section 2 gives background information. \nSection 3 de\u00adscribes related work. Section 4 describes the simulation methods used, the benchmarks used, \nand the metrics used to measure memory subsystem performance. Section 5 presents the results of the simulation \nstudies, and an analysis of those results. Section 6 concludes. 2 Background The following sections describe \nmemory subsystems, copy\u00ading garbage collection, SML, and the SML/NJ compiler. 2.1 Memory subsystems This \nsection reviews the organization of memory subsystems. Since terminology for memory subsystems is not \nstandard\u00adized we use Przybylski s terminology [32]. It is well known that CPUS are getting faster relative \nto DRAM memory chips; main memory cannot supply the CPU with instructions and data fast enough. A solution \nto this problem is to use a cache, a small fast memory placed be\u00adtween the CPU and main memory that holds \na subset of memory. If the CPU reads a memory location which is in the cache, the value is returned quickly. \nOtherwise the CPU must wait for the value to be fetched from main memory. Caches work by reducing the \naverage memory access time. This is possible since memory accesses exhibit temporal and spatial locality. \nTemporal locality means that a memory loca\u00adtion that was referenced recently will probably be referenced \nagain soon and is thus worth storing in the cache. Spatial locality means that a memory location near \none which was referenced recently will probably be referenced soon. Thus, it is worth moving the neighboring \nlocations to the cache. 2.1.1 Cache organization This section describes cache organization for a single \nlevel of caching. A cache is divided into blocks, each of which has an associated tag. A cache block \nrepresents a block of memory. Cache blocks are grouped into sets. A memory block may reside in the cache \nin exactly one set, but may reside in any block within the set, The tag for a cache block indicates what \nmemory block it holds. A cache with sets of size n is said to be n-way associative. If n= 1, the cache \nis called direct-mapped. Some caches have valid bits, to indicate what sections of a block hold valid \ndata. A subblock is the smallest part of a cache with which a valid bit is associated, In this paper, \nsubblockplacement implies a subblock size of one word, i. e,, valid bits are associated with each word, \nMoreover, on a read miss, the whole block is brought into the cache not just the subblock that missed. \nPrzybylski [32] notes that this is a good choice. A memory access for which a block is resident in the \ncache is called a hit. Otherwise, the memory access is a miss. A read request for memory location m causes \nm to be mapped to a set. All the tags and valid bits (if any) in the set are checked to see if any block \ncontains the memory block for m. If a cache block contains the memory block form, the word corresponding \ntom is selected from the cache block. A read miss is handled by copying the missing block from the main \nmemory to the cache. A write hit is always written to the cache. There are several policies for handling \na write miss, differing in their performance penalties. For each of the policies, the actions taken on \na write miss are: 1. write no allocate: DO not allocate a block in the cache s Send the write to main \nmemory, without putting the write in the cache. 2. write allocate, no subblock placement: Allocate a \nblock in the cache.  Fetch the corresponding memory block from main memory.  o Write the word to the \ncache and to memory. 3. write allocate, subblock placement: c Allocate a block in the cache. Write the \nword to the cache and to memory.  Invalidate the remaining words in the block.  Write allocate/subblock \nplacement will have a lower write miss penalty than write allocateho subblockplacement since it avoids \nfetching a memory block from main memory. In addition, it will have a lower penalty than write no allocate \nif the written word is read before being evicted from the cache. See Jouppi [22] for more information \non write miss policies. A miss is a compulsory miss if it is due to a memory block being accessed for \nthe first time. A miss is a capacity miss if it results from the cache (size C) not being big enough \nto hold all the memory blocks used by a program, This corresponds to the misses in a fully associative \ncache of size C with LRU replacement policy (minus the compulsory misses). It is a conflict miss if it \nresults from two memory blocks mapping to the same set. [20] A write buffer may be used to reduce the \ncost of writes to main memory. A write buffer is a queue containing writes that are to be sent to main \nmemory. When the CPU does a write, the write is placed in the write buffer and the CPU continues without \nwaiting for the write to finish. The write buffer retires entries to main memory using free memory cycles. \nA write buffer stall occurs if the write buffer is full when the CPU tries to do a write or tries to \nread a location queued up in the write buffer. 1ReC~l subblock size is assumed to be 1 word. Main memory \nis divided into DRAM pages. Page-mode writes reduce the latency of writes to the same DRAM page when \nthere are no intervening memory accesses to another DRAM page. 2.1.2 Memory subsystem performance This \nsection describes two metrics for measuring the perfor\u00admance of memory subsystems. One popular metric \nis the cache miss ratio. The cache miss ratio is the number of memory accesses that miss divided by the \ntotal number of memory accesses. Since different kinds of memory accesses usually have different miss \ncosts, it is useful to have miss ratios for each kind of access. Cache miss ratios alone do not measure \nthe impact of the memory subsystem on overall system performance. A metric which better measures this \nis the contribution of the memory subsystem to CPI (cycles per useful instruction). CPI is calculated \nfor a program as number of CPU cycles to complete a program/total number of useful instructions executed. \nIt measures how efficiently the CPU is being uti\u00adlized. The contribution of the memory subsystem to CPI \nis calculated as number of CPU cycles spent waiting for the memory subsystem /total number of useful \ninstructions exe\u00adcuted. As an example, on a DECStation 5000/200, the lowest CPI possible is 1, completing \none instruction per cycle. If the CPI for a program is 1,50, and the memory contribution to CPI is 0.3, \n2070 of the CPU cycles are spent waiting for the memory subsystem (the rest may be due to other causes \nsuch as nops, multi-cycle instructions like integer division, etc.). CPI is machine dependent since it \nis calculated using actual penalties.  2.2 Copying garbage collection A copying garbage collector [18, \n11] reclaims an area of memory by copying all the live (non-garbage) data to another area of memory. \nThis means that all data in the garbage\u00adcollected area is now garbage, and the area can be re-used. Since \nmemory is always reclaimed in large contiguous areas, objects can be sequentially allocated from such \nareas at the cost of only a few instructions. Figure 1 gives an example of pseudo-assembly code for allocating \na cons cell. ra contains the car cell contents, rd contains the cdr cell contents, al 1 oc is the address \nof the next free word in the allocation area, and top contains the end of the allocation area. The SML/NJ \ncompiler uses a simple generational copying garbage collector [28]. Memory is divided into an old gen\u00aderation \nand an allocation area. New objects are created in the allocation area; garbage collection copies the \nlive objects in the allocation area to the old generation, freeing up the allocation area. Generational \ngarbage collection relies on the ZAI] in$~ction~ besides nops are considered to be useful. A noP (null \noperation) instruction is a software-controlled pipeline statl % check for heap overflow cmp alloc+12, \ntop branch-if-gt call-gc % write the object store tag, (allot) store ra,4(alloc) store rd, 8 (allot) \n% save pointer to object move alloc+4, result % add 12 to allot pointer add allot, 12 Figure 1: Pseudo-assembly \ncode for allocating an object fact that most allocated objects die young; thus most objects (about 99% \n[3, p. 206]) are not copied from the allocation area. This makes the garbage collector efficient, since \nit works mostly on an area of memory where it is very effective at reclaiming space. The most important \nproperty of a copying collector with respect to memory subsystem behavior is that allocation ini\u00adtializes \nmemory which has not been touched in a long time and is thus unlikely to be in the cache. This is especially \ntrue if the allocation area is large relative to the size of the cache since allocation will knock everything \nout of the cache. This means that for small caches there will be a large number of (write) misses. For \nexample consider the code in Figure 1. Assume that a cache write miss costs 16 CPU cycles and that the \nblock size is 4 words. On average, every fourth word allocated causes a write miss. Thus, the average \nmemory subsystem cost of allocating a word on the heap is 4 cycles. The average cost for allocating a \ncons cell is seven cycles (at one cycle per instruction) plus 12 cycles for the memory subsystem over\u00adhead. \nThus, while allocation is cheap in terms of instruction counts, it is expensive in terms of machine cycle \ncounts. 2.3 Standard ML Standard ML (SML) [30] is a call-by-value, Iexically scoped language with higher-order \nfunctions, garbage collection, static typing, a polymorphic type system, provable safety properties, \na sophisticated module system, and a dynami\u00adcally scoped exception mechanism. SML encourages a non-imperative \nprogramming style. Variables cannot be altered once they are bound, and by default data structures cannot \nbe altered once they are cre\u00adated. Lisp s rplaca and rplacd do not exist for the de\u00adfault definition \nof lists in SML. The only kinds of assignable data structures are ref cells and arrays3, which must be \nex\u00adplicitly declared. To emphasis the point, assignments are sA1thoUghthe Imguage definition omitted \narrays, all implementations have arrays. permitted but discouraged as a general programming style. The \nimplications of this non-imperative programming style for compilation are clear: SML programs tend to \ndo more allocation and copying than programs written in imperative languages. SML is most closely related \nto Lisp and Scheme[34]. Implementation techniques for one of these languages are mostly applicable to \nthe other languages, with the following caveats: SML programs tend to be less imperative than Lisp or \nScheme programs and Scheme and SML programs use function calls more frequently than Lisp, since recursion \nis the usual way to achieve iteration in Scheme and SML. 2.4 SML/NJ compiler The SML~J compiler [3] is \na publicly available compiler for SML. We used version 0.91. The compiler concentrates on making allocation \ncheap and function calls fast. Allocation is done in-line, except for the allocation of arrays. Ag\u00adgressive \n~-reduction (inlining) is used to eliminate functions calls and their associated overhead. Function arguments \nare passed in registers when possible, and register targeting is used to minimize register shuffling \nat function calls. A split caller/callee-save register convention is used to avoid exces\u00adsive spilling \nof registers. The compiler also does constant\u00adfolding, elimination of functions which trivially call \nother functions, limited code hoisting, uncurrying, and instruction scheduling. The most controversial \ndesign decision in the compiler was to allocate procedure activation records on the heap instead of the \nstack [1, 5]. In principle, the presence of higher-order functions means that procedure activation records \nmust be allocated on the heap. With a suitable analysis, a stack can be used to store most activation \nrecords [25]. However, using only a heap simplifies the compiler, the run-time system [2], and the implementation \nof first-class continuations [19]. The decision to use only a heap was controversial because it greatly \nincreases the amount of heap allocation, which is believed to cause poor memory subsystem performance. \n  3 Related Work There have been many studies of the cache behavior of sys\u00adtems using heap allocation \nand some form of copying garbage collection. Peng and Sohi [31] examined the data cache behavior of some \nsmall Lisp programs. They used trace\u00addriven simulation, and proposed an ALLOCATE instruction for improving \ncache behavior, which allocates a block in the cache without fetching it from memory. Wilson et. al. \n[38, 39] argued that cache performance of programs with generational garbage collection will improve \nsubstantially when the youngest generation fits in the cache. Koopman et. al. [24] studied the effect \nof cache organization on com\u00adbinator graph reduction, an implementation technique for lazy functional \nprogramming languages. Combinator graph reduction does more heap allocation and assignments than SML/NJ \nprograms. They observed the importance of a write\u00adallocate policy with subblock placement for improving \nheap allocation. Zorn [40] studied the impact of cache behavior on the performance of a Common Lisp system, \nwhen stop\u00adand-copy and mark-and-sweep garbage collection algorithms were used. He concluded that programs \nrun with mark-and\u00adsweep have substantially better cache locality than when run with stop-and-copy. These \nworks all used data cache miss ratios to evaluate cache performance. They did not separate read and write \nmisses, despite the different costs of these misses. Also, they did not simulate the entire memory subsystem. \nOur work separates read misses from write misses and completely models the memory subsystem, including \nwrite buffers and page-mode writes. Appel [3] estimated CPI for the SML/NJ system on a single machine \nusing elapsed time and instruction counts. His CPI differs substantially from ours. Apparently instructions \nwere undercounted in his measurements [4]. Jouppi [22] studied the effect of cache write policies on \nthe performance of C and Fortran programs. Our class of programs is different from his, but his conclusions \nsupport ours: that a write-allocate policy with subblock placement is a desirable architecture feature. \nHe found that the write miss ratio for the programs he studied was comparable to the read miss ratio, \nand that write-allocate with subblock placement eliminated the cost of write misses. For programs compiled \nwith the SML/NJ compiler, this is even more important due to the high number of write misses caused by \nallocation. 4 Methodology We used trace-driven simulations to evaluate the memory subsystem performance \nof programs. For trace-driven sim\u00adulations to be useful, there must be an accurate simulation model and \na good selection of benchmarks. Simulations that make simplifying assumptions about important aspects \nof the system being modeled can yield misleading results, Toy benchmarks, or unrepresentative benchmarks, \ncan be equally misleading. We have devoted much effort to addressing these issues. 4.1 Tools We have \nextended QPT [7, 26, 27] to produce memory traces for SML/NJ programs. QPT rewrites an executable program \nto produce a full instruction and data trace. Because QPT operates on the executable program, it can \ntrace both the SML code and the garbage collector (written in C). We used Tycho [21] for the memory subsystem \nsimula\u00adtions. Tycho uses a special case of all-associativity simula\u00adtion [29] to simulate multiple caches \nconcurrently. We have added a write-buffer simulator to Tycho, which concurrently simulates a write buffer \nfor each instruction and data cache pair being simulated. The write-buffer simulator also takes page-mode \nwrites and memory refreshes into consideration. 4.2 Simplifications and Assumptions We wanted to simulate \nthe memory subsystems as completely as we could. Thus, we have tried to minimize simplifications which \nmay reduce the validity of our data. The most impor\u00adtant simplifications are: 1. We ignore the effects \nof context switches and system calls. 2. Our simulations are driven by virtutd addresses even though \nmany current machines have physically\u00adaddressed caches. 3. We use default compilation flags which enable \nexten\u00adsive optimization. We set the soft limit of the garbage collector to 20000K4. 4. When comparing \ndifferent cache organizations we as\u00adsume that the CPU cycle time is the same.  4.3 Benchmarks Table \n1 describes the benchmark programs5. Knuth-Bendix, Lexgen, L$e, Simple, VLIW, and YACC are identical \nto the benchmarks measured by Appel [3]6. Table 2 gives the sizes of the benchmarks in terms of lines \nof SML code (excluding comments and blank lines), maximum heap size in kilobytes, size of the compiled \ncode in kilobytes (does not include the garbage collector and other run-time support code which is about \n60K)7, and run time, in seconds, on a DECStation 5000/200. The run times are the minimum of five runs. \nTable 3 characterizes the memory references of the bench\u00admark programs. The Writes column lists the number \nof full word writes done by the program and the garbage collec\u00adtor; the Assigmnents column lists the \nnon-initializing writes done by the program only, The Partial Writes column lists the number of partial \nword (bytes, half-word, etc.) writes done by the program and the garbage collector*. All the benchmarks \nhave long traces; most other work on memory system performance uses traces that are an order of magni\u00adtude \nsmaller. The benchmark programs do few assignments; the majority of the writes are initializing writes. \nTable 4 gives the allocation statistics for each benchmark program. All allocation and sizes are reported \nin words. The Allocation column lists the total allocation done by the bench\u00admark. The remaining columns \nbreak down the allocation by 4~,~ is Imge enough to allow the garbage collector to resize the heap ~ \nneeded. 5Av~lable from the authors. c~e ttescriptionof these benchmarks have been copied from [3]. 7~e \ncode size includes 207K for the standard libraries. sp~i~.~~rd Wn&#38;S are distinguished from full-word \nwnteS since theY are often more expensive than full-word writes. We charge 11 cycles for each partial-word \nwrite. kind: closures for escaping functions, closures for known functions, closures for callee-save \ncontinuations, records, and others (includes spill records, arrays, strings, vectors, ref cells, and \nstore list records). For each allocation kind, the 70 column is the percentage of total allocation allocated \nfor that kind of object and Size is the average size (including the 1 word tag) for that kind of object. \n 4.4 Metrics We state cache performance numbers in cycles per useful in\u00adstruction (CPZ). All instructions \nbesides nops are considered useful. Table 5 lists the penalties used in the simulations. These numbers \nare derived from the penalties for the DECStation 5000/200, but are similar to those in other machines \nof the same class. Note that write misses have no penalty (besides write buffer costs) for caches with \nsubblock placement 10.  5 Results and Analysis Section 5.1 qualitatively analyzes the memory behavior \nof programs. Section 5.2 lists the cache configurations sim\u00adulated and explains why they were selected. \nSections 5.3 presents and analyzes data for memory subsystem perfor\u00admance. 5.1 Qualitative Analysis Recall \nfrom Section 2 that SML/NJ uses a copying collector which leads to a large number of write misses. The \nslowdown this translates into depends on the cache organization being used. Recall from Section 4.3 that \nSML/INJ programs have the following properties. First, they do few assignments; the majority of the writes \nare initializing writes. Second, pro\u00adgrams do heap allocation at a furious rate: 0.1 to 0.22 words per \ninstruction. Third, writes come in bunches because they correspond to initialization of a newly allocated \narea. The burstiness of writes combined with the property of copying collectors mentioned above suggests \nthat an aggres\u00adsive write policy is necessary. In particular, writes should not stall the CPU. Memory \nsubsystem organizations where the CPU has to wait for a write to be written to memory will perform poorly. \nEven memory subsystems where the CPU does not need to wait for writes if they are issued far apart (e.g., \n2 cycles apart in the HP 9000 series 700) may perform poorly due to the bunching of writes. This leads \nto two requirements on the memory subsystem. First, a write $ C]osures for c~lee-save continuations can \nbe trivially allocated on a strickin the absence of first class continuations. 101n ~ actu~ implementation, \nthe penahy of a miss -Y be one cYcle since unlike hits, the tag and valid bits needs to be written to \nthe cache after the miss is detected. This will not change our results since it adds at most 0.02 0.05 \nto the CPI of caches with subblock placement. Program Description Cw The Concurrency Workbench [12] is \na tool for analyzing networks of finite state processes expressed in Milner s Calculus of Communicating \nSystems. Leroy An implementation of the Knuth-Bendix completion algorithm. Lexgen A lexical-analyzer \ngenerator [6], processing the lexical description of Standard ML. Life The game of Life implemented using \nlists [33]. PIA The Perspective Inversion Algorithm [37] decides the location of an object in a perspective \nvideo image. Simple A spherical fluid-dynamics program [13]. VLIW A Very-Long-Instruction-Word instruction \nscheduler. YACC An implementation of an LALR(l) parser generator [36] processing the grammar of Stan- \n Table 1: Benchmwk Programs w---. . . .. ...... Program Lines Heap size (K) Code size (K) Non-gc (see) \nGc (see) Cw 5728 1107 894 22.74 3.09 Knuth-Bendix 491 2768 251 13.47 1,48 -----., -Lexgen 1L.z4 Llbl \n305 15.07 1.06 Life 111 1026 221 16.97 0.19 PIA 1454 1025 291 6.07 0.34 Simrrle 999 11571 314 25.58 4.23 \nVLti 3207 1088 ] 486 23.70 I 1.91 YACC I 5751 I 1632 I 580 I 4.60 I 1.98 Table 2: Sizes of Benchmark \nprograms Program Inst Fetches Reads (%) Writes (%) Partial Writes (%) Assignments (%) Nops (%) Cw 523,245,987 \n17.61 11.61 0.01 0.41 13.24 Knrrth-Bendix 312,086,438 19.66 22.31 0.00 0.00 5.92 Lexgen 328,422,283 16.08 \n10.44 0.20 0.21 12.33 Life 413,536,662 12.18 9.26 0.00 0.00 15.45 PIA 122,215,151 25.27 16.50 0.00 0.00 \n8.39 Simple 604,611,016 23.86 14.06 0.00 0.05 7.58 VLIW 399,812,033 17.89 1599 0.10 0.77 9.04 YACC 133,043,324 \n18.49 14.66 0.32 0.38 11.14 Table 3: Characteristics of benchmark programs Escaping Known Crdlee Saved \nRecords Other Program Allocation % Size % Size % Size % Size % Size Cw 56,467,440 4.0 4.12 3.3 15.39 \n672 6.20 19.5 3.01 6.o 4.00 Krmth-Bendix 67,733,930 37.6 6.60 0.1 15.22 49.5 4.90 12.7 3.00 0.1 15.05 \nLexgen 33,046,34! 9 3.4 6.20 5.4 12.96 72.7 6.40 15.1 3.00 3.7 6.97 Life 37,,849,681 0.2 3.45 0.0 15.00 \n77.8 5.52 22.2 3.00 0.0 10.29 PIA 13J,047,041 0.6 5,56 40.4 11.99 36.5 4.73 18,4 3.41 4.1 16.01 F====Simple \nVLIW YACC 67,261,664 59,496,919 17,015,250 4.8 9.9 2.3 5.70 5.22 4.83 1.3 6.0 15.3 15.33 26.62 15.35 \n81.8 61.8 54.8 6.43 7.67 -?.44 9.9 20.3 23.7 3.00 3.01 3.04 2.2 2.1 4.0 5.01 2.60 10.22 Table 4: Allocation \ncharacteristics of benchmark programs 6 I Task Penalty (in cycles) Non Daxe mode write 1 51 Page mode \nwrite 1 Read 16 bytes from memory 15 Read 32 bvtes from memosw 19 Write hit (16 bytes, no subblocks) \nWrite hit (32 bytes no subblocks) Write miss (16 bytes, no subblocks) Write miss (32 bytes, no subblocks) \n= Table 5: Penalties of memory operations buffer or fast page mode writes are essential to avoid waiting \nfor writes to memory, Second, on a write miss, the memory subsystem must avoid reading a cache block \nfrom memory if it will be written before being read. Of course, this re\u00adquirement only holds for caches \nwith a write-allocate policy. Subblock placement [24], a block size of 1 word, and the ALLOCATE directive \n[31] can all achieve this 11. For large caches, when the allocation area fits in the cache and thus there \nare few write misses, the benefit of subblock placement will be reduced. 5.2 Cache configurations simulated \nSince the design space for memory subsystems is enormous we had to prune the design space that we could \nstudy. In this study, we restrict ourselves to features found in currently pop\u00adular RISC workstations. \nExploration of more exotic memory subsystem features is left to future work. Table 6 summarizes the cache \norganizations simulated. Table 7 lists the memory subsystem organization for some popular machines. We \nsimulated only separate instruction and data caches (i.e., no unified caches). While many current machines \nhave separate caches (e.g., DECStations, HP 9000 series), there are some exceptions (notably SPARCS). \nWe simulated cache sizes from 8K to 128K. This range includes the primary caches of most current machines \n(see Table 7). We consider only direct mapped and two-way set associative caches (with LRU replacement). \nWe simulated block sizes of 16 bytes and 32 bytes. Przy\u00adbylski [32] notes that block sizes of 16 or 32 \nbytes optimize the read access time for the memory parameters used in the CPI calculations (see Section \n4.4). We report data only for write-through caches but the CPI for write-back caches can be inferred \nfrom our graphs. Write\u00adthrough and write-back caches give identical misses, but the penalties for write \nhits and write misses differ. A write hit or miss in a write-back cache may take one cycle more than \nin a write-through cache [22]. This tells us at most how much the write-through graphs need to be shifted \nto obtain the CPI graphs for write-back caches, For instance, if the program has w writes and n useful \ninstructions, then we must add 11Since the effects on cache performance of these featUreS are so .SiIS3i1W \nwe tatk just about subblock placement. w/n to the CPI. For CWthis adds 0.13. Write-through and write-back \ncaches may have different write buffer penalties. We expect the write buffer penalties for write-back \ncaches to be smaller than that for write-through caches since writes to main memory are less frequent \nfor write-back caches than for write-through caches. In any case, write buffer penalties are negligible \neven for write-through caches (Section 5.3). Two of the most important cache parameters are write allocate \nversus write no allocate and subblock placement versus no subblock placement. Of these, the combination \nwrite no allocate/subblockplacement offers no improvement over write no allocate/no subblock placement \nfor cache per\u00adformance. Thus, we did not collect data for the write no allocatefsubblo ckplacement configuration. \nWe restrict ourselves only to the first two levels of the memory hierarchy, which on most current machines \ncorre\u00adsponds to the primary cache and main memory. The results, however, are mostly applicable when the \nsecond level is a sec\u00adondary cache and the cost of accessing the secondary cache is similar to the cost \nof accessing main memory on the DEC-Station 5000/20012. In such machines, there is a memory subsystem \ncontribution to the CPI that we did not measure: a miss on the second level cache. Therefore the CPI \nobtained on these machines can be higher than that reported here. We do not simulate the exotic features \nappearing on some newer machines, such as stream buffers, prefetching, and victim caches. These features \ncan reduce the cache miss rates and miss costs. Further work is needed to understand the impact of these \nfeatures on performance of heap allocation. 5.3 Memory Subsystem Performance Memory subsystem performance \nis presented in summary graphs and breakdown graphs. Each summary graph summa\u00adrizes the memory subsystem \nperformance of one benchmark program for a range of write-miss policies (write allocate or no write allocate), \nsubblock placement (with or with\u00adout), cache sizes (8K to 128 K), and associativity (1 or 2). Each curve \nin a summary graph corresponds to a different memory subsystem organization. There are two summary graphs \nfor each program, one for a block size of 16 bytes and another for a block size of 32 bytes. Each breakdown \ngraph breaks down the memory subsystem overhead into read misses, instruction-fetch misses, write-buffer \noverhead, and partial-word write overhead for one configuration in a sum\u00admary graph. The write-buffer \ndepth in these graphs is fixed at 6 entries. In this paper we present only the summary graphs for CW \n(Figure 2). The summary graphs for other programs are similar to those for CW and are thus omitted for \nspace considerations13. Any significant differences between CW S graphs and the omitted graphs are noted \nin the text. Figures 12For instance, Borg et at. [8] use 12 cycles as the latency for going to Write \nPolicy Write Miss Policy Write Buffer Subblocks Assoc Block Size jCache Sizes through allocate 6 deep \nyes 1,2 16,32 bytes 8K-128K through allocate 6 deep no 1,2 16,32 bytes 8K-128K through no allocate 6 \ndeep no 1,2 16,32 bytes 8K-128K the second level cache and 200 250 cycles for going to memory. ISThe \nfu]] set of graphs are available via anonymous ftp from ibis. cs.umass.edu in pub/memory-subsystem. \nTable 6: Cache organizations studied Archhecture WritePolicy WriteMissPolicy WriteBufferSubblocksAssocBlockSizeCacheSize \nDS31OO[16] through allocate 4 deep  1 4 bytes 64K DS5000/200 [15] through allocate 6 deep yes 1 16 bytes \n64K HP 9000 [35] back allocate none no 1 32 bytes 64K-2M SPARCStation II [14] through no allocate 4 deep \nno 1 32 bytes 64K Note: * SPARCStations have umfied caches. * Most HP 9000 series 700 caches are much \nsmaller &#38;an 2M, 128K instruction cache and 256K data cache for models 720 and 730. and 256K instmcbon \ncache and 256K data cache for model 750  The DS5000/200 actually has a block size of four bytes with \na fetch size of sixteen bytes. This is actually stronger than subblock placement since it has a full \ntag on every subblock .  The higher end HP 9000 machines (model 735 and above) prowde a cache-control \nhint in theu store insmactions [9]. The Mint can specify that a block will be overwritten before being \nread, W avoids the read if the write nusses.  Table 7: Memory subsystem organization of some popular \nmachines 3 and 4 are the breakdown graphs for CW for the 16 byte block size configurations; the remaining \nbreakdown graphs for CWare omitted for space considerations. The breakdown graphs for the other benchmarks \nare similar and are thus also omitted for space considerations 14. In the summary graphs, the rtops curve \nis the base CPI: the number of useful (not nop) instructions executed divided by the total number of \ninstruction executed; this corresponds to the CPI for a perfect memory subsystem15. For the break\u00addown \ngraphs, the nop area is the CPI contribution of nops; read miss is the CPI contribution of read misses; \nif miss is the CPI contribution of instruction fetch misses; write bu~er is the CPI contribution of the \nwrite buffer; partial word is the CPI contribution of partial-word writes*6. The 64K point on the write \nallot, subblock, assoc=l curves corresponds closely to the DECStation 5000/200 memory subsystem. In the \nfollowing sections we describe the impact of write\u00admiss policy and subblock placement, associativity, \nblock size, cache size, write buffer, and partial-word writes on the mem\u00adory subsystem performance of \nthe benchmark programs. 5.3.1 Write Miss Policy and Subblock Placement From the summary graphs, it is \nclear that the best cache organization we studied is write allocate/subblock place\u00ad lJ Lexge~>s graphsarea \nlittle different in that there is a steep drop in the instruction cache contribution to the CPI in going \nfrom an SK to 16K cache. 15nqrs constitute between 5 .9q0 and 15.4% of afl instructions executed for \nthe benchmarks (see Section 4.3). 16TKIS overhead is so small that it is not visible in most of the breakdown \ngraphs. ment; in every case, write-allocatelsubblock placement sub\u00adstantially outperforms all other configurations. \nSurprisingly, for sufficiently large caches with the write allocate/subblock placement organization, \nthe memory subsystem performance of SML/NJ programs is acceptable (around 1770 or less overhead) *7. \nFor caches with write allocate/subblock place\u00adment, the average memory subsystem contribution to the \nCPI over all benchmarks is 16?I0for 64K direct mapped caches and 17% for 32K two-way associative caches. \nThe DS5000/200 organization does well for most programs. It is worth empha\u00adsizing that the memory subsystem \nperformance of SML/NJ programs is good on some current machines despite the very high miss rates; for \na 64K write allocate/no subblock place\u00adment organization with a block size of 16 bytes, the write miss \nand read miss ratios for cw are 0.18 and 0.04 respectively. Recall that in Section 5.1 we argued that \nsubblock place\u00adment would be a big win, but its benefit would decrease for larger caches. Our data indicates \nthat the reduction in ben\u00adefit is not substantial even for 128K cache sizes although a slight tapering \noff is seen in CW.This indicates that 128K is not large enough to hold the allocation area of most of \nthe benchmark programs. The performance of write allucate/no subblock is almost identical to that of \nwrite no allocate/no subblock (Leroy is an exception) 18 This . suggests that an address is being read \nsoon after being written; even in an 8K cache, an address is 17For the pen~ties used, a 17% overhead \ntranslates roughly into one fetch from memory-instruction or data-every 100 useful instructions, lg~e \ndifference between write allocate/no subblock and write no all~~\u00ad cate/no subblock is so smafl in most \ngraphs that the two curves overlap. read after being written before it is evicted from the cache (if \nit was evicted from the cache before being read, then write allocate\\no subblock would have inferior \nperformance). The only difference between these two schemes is when a cache block is read from memory. \nIn one case, it is brought in on a write miss; in the other, it is brought in on a read miss. Bmause \nSML/NJ programs allocate sequentially and do few assignments, a newly allocated object remains in the \ncache until the program has allocated another C bytes, where c is the size of the cache. Since our programs \nallocate 0.4-0.9 bytes per instruction, our results suggest that a read of a block occurs within 9K 20K \ninstructions of it being written. 5.3.2 Changing Associativity From Figure 2 we see that increasing \nassociativity improves all organizations. However the improvement in going from one-way to two-way set \nassociativity is much smaller than the improvement obtained from subblock placement: inmost cases, it \nimproves the CPI by less than 0.1. The maximum benefit from higher associativity is obtained for small \ncache sizes (less than 16K). However, increasing associativity may increase CPU cycle time and thus the \nimprovements may not be realized in practice [20]. From Figures 3 and 4 we see that higher associativity \nimproves the instruction cache performance but has little or no impact on data cache performance. Surprisingly, \nfor direct mapped caches (Figures 3 (a) and 4 (a)) the instruction cache penalty is substantial for caches \nsmaller than 128K. For caches with subblock placement, the instruction cache penalty dominates the penalty \nfor the memory subsystem. The improvement observed in going to a two-way associative cache suggests that \na lot of the penalty from the instruction cache is due to conflict misses and that from the data cache \nis due to capacity misses: the data cache is simply not big enough to hold the working set. When the \ncode produced by SML/NJ is examined, the performance of the instruction cache is not surprising: the \ncode consists of small functions with frequent calls, which lower the spatial locality. Thus, the chances \nof conflicts are greater than if the instructions had strong spatial locality. 5.3.3 Changing Block \nSize From Figure 2 we see that increasing block size from 16 to 32 bytes also improves performance. For \nthe write allocate organizations, an increased block size decreases the number of write misses caused \nby allocation. When the allocation area does not fit in the cache, doubling the block size can halve \nthe write-miss rate. Thus, larger block sizes improve performance when there is a penalty for a write \nmiss [24]. In particular, larger block sizes have little to offer to caches with write allocate/subblock \nplacement. From Figure 2 we see that the write no allocate organizations benefit just as much from larger \nblock size as write allocate/no subblock placement; this suggests that the spatial locality in the reads \nis comparable to that in the writes. Note that subblock placement improves performance more than even \ntwo-way associativity and 32 byte blocks com\u00adbined. 5.3.4 Changing Cache Size Increasing the cache size \nimproves performance for all con\u00adfigurations. In most cases, the performance improvement from doubling \nthe cache size is small. We expect to see a sharp improvement in performance for some larger cache size \nonce the allocation area fits in the cache (this will not be nearly as significant for caches with subblock \nplacement) 19. From the breakdown graphs we see that the cache size has little effect on the data cache \nmiss contribution to CPI. Most of the improvement in CPI that comes from increasing the cache size is \ndue to improved performance of the instruction cache. As with associativity, cache sizes have interactions \nwith the cycle time of the CPU: larger caches can take longer to access. Thus, improvement due to increasing \nthe cache size may not be achieved in practice. 5.3.5 Write Buffer and Partial-Word Write Overheads \nFrom the breakdown graphs we see that the write buffer and partial word write contribution to the CPI \nis negligible. A six deep write buffer coupled with page-mode writes is sufficient to absorb the bursty \nwrites. As expected, memory subsystem features which reduce the number of misses (such as higher associativity \nand larger cache sizes) also reduce the write buffer overhead.  6 Conclusions We described an in-depth \nstudy of the memory subsystem per\u00ad formance of programs compiled with SMLINJ. The important characteristics \nof these programs, with respect to memory subsystem performance, were intensive heap allocation and the \nuse of copying garbage collection. In agreement with[31, 38, 39,40], programs with intensive heap allocation \nperformed poorly on most memory subsys\u00adtem organizations. However, on some current machines (in particular \nthe DECStation 5000/200), the performance was good. The memory organization parameter crucial for good \nper\u00adformance was subblockplacement. For caches with subblock placement, the memory subsystem overhead \nwas under 17% for 64K or bigger caches; for caches without subblock place\u00adment, the overhead was often \nas high as 100Yo. Associativity and cache sizes (up to 128K) had little im\u00adpact on data cache performance. \nbut were more important for instruction cache performance. While higher associativity, lgsub~q~ent work \nhas shown that 5 12K is large enough to hold the allocation area of most of the benchmark programs [17]. \n~ write-no-allcc, no-subblk,assw= 1 3 2.8 ~ ~rite.dl~, n~.~ bblk,a~~m.l 2.6i F ..m 1.4 --0--........ \n ------.~ ... ................. 1.2 tk e 1; I 8K 16K 32K 64K 128K I and D cache sizes (a) block size=16 \nbytes 3 ~ write-no-alloc, no-subblk,asscc= ~ wnte-allcc,subblk, asscw.l 2.8 - ~rite.~l~,n~.S bb~,aSS~=l \n2.6 -\u00a4----~nte. ~.allm,n~. Subb~,aSS~=: L L......-----\u00ad -----. -w-.... ------. . [ ...... - -.-w...... \n------... -... .0--.... 1.4 - -y=\u00ad ------.~. ................... 1 ?.K 16K 32K 64K 128K I and D cache \nsize (b) block size=32 bytes Figure 2: CW summary, write buffer depth=6 3 2.8 partial word 2.6 [ o write \nbuffer z 2.4 .s o if miss ~ 2.2 ~ o read miss 22 ~ 1.8 K u 1.6 1,4 ,, 1,2  -4 1 128K 64K 32K UK 16K \n... I and D cache size (a) assoc=l 3 2,8 2.6 @id wo][d f r\u00adz 2.4 o write buffer ,2 t ~ 2.2 &#38; 32 \n$1.8 $1,6 1.4 1 128K 64K 32K 16K SK I and D cache size (b) assoc=2 Figure 3: CW breakdown, write no \nallot, no subblk, block size= 16, wb depth=6 11 3 2.8 2.6 [ Ptid word z 2.4 .2 o write buffer g 2.2 \n~ 1 n o if miss 1.2 1 8K 16K 32K 64K 128K I and D cache sizs (a) assoc=l T 2,8 + o write buffer o if \nmiss I o Ieadmss j 1.8 nop K I 1 v 1.6 1.4 ,, 1.2 6 I 1 8K I 16K 32K , 64K 128K I and D cache size (b) \nassoc=2 Figure 4: CW breakdown, write allot, subblk, block size= 16, wb depth=6 12 larger cache sizes, \nand larger block sizes improved perfor\u00admance, their contribution to performance was usually small. To \nsummarize, most current machines support heap allo\u00adcation poorly. For these machines, compilers should \navoid heap allocation as much as possible. However, with the ap\u00adpropriate memory subsystem organization, \nheap allocation can achieve good memory subsystem performance.   Acknowledgements We would like to \nthank Edoardo Biagioni, Eric Brown, Brad Chen, Olivier Danvy, Alessandro Forin, Urs Hoel\u00adzle, Kathryn \nMcKinley, Erich Nahum, and Darko Stefanovi6 for comments on drafts of this paper. We thank Peter Lee \nfor his encouragement and advice during this work. We thank Brian Milnes and the facilities at CMU for \nsetting up the hardware according our every whim. We thank Tom Dewey for explaining the partial-word \nwrite mechanism in the DS5000/200 to us. We thank Andrew Appel, Dave Mac-Queen and many others for creating \nSML/NJ. We thank James Larus for creating qpt and for answering the ques\u00adtions which arose while we were \nextending his tool. We thank Mark Hill for creating his cache simulators, Tycho and DineroIII. Last but \nnot least, we thank all the members of the Fox project for their interest in this work and for accommo\u00addating \nour demand for compute cycles. References [1] Andrew W. Appel. Garbage collection can be faster than \nstack allocation. Information Processing Letters, 25(4):275 279, 1987. [2] Andrew W. Appel. A Runtime \nSystem. Lisp and Symbolic Computation, 3(4):343 380, November 1990. [3] Andrew W. Appel. Compiling with \nContinuations. Cambridge University Press, first edition, 1992. [4] Andrew W. Appel. Personal Communication. \nMarch 221993. [5] Andrew W. Appel and Trevor Y. Jim. Continuation-Passing, Closure-Passing Style. In \nProceedings of the 16th Annual ACM Symposium on Principles of Programming Lunguages, pages 293-302, Austin, \nTexas, January 1989. ACM. [6] Andrew W. Appel, James S. Mattson, and David Tarditi. A lexical analyzer \ngenerator for Standard ML. Distributed with Standard ML of New Jersey, 1989. [71 Thomas Ball and James \nR. Larus. Optimally Profiling and tracing programs. In 19th Symposium on Principles of Programming Lunguages. \nACM, January 1992. [8] Anita Borg, R. E, Kessler, Georgia Lazana, and David W. Wall. Long address traces \nfrom RISC machines: Generation and analysis. Technical Report 89/14, DEC Western Research Laboratory, \nSeptember 1989. [91 Brian Case. PA-RISC provides rich instruction set within RISC framework. Microprocessor \nReport, 5(6), April 1991. [10] J. Bradley Chen and Brian N. Bershad. The impact of operating system structure \non memory system perf ormance. In Fourteenth Symposium on Operating System Principles. ACM, Dec 1993. \n[11] C.J. Cheney. A nonrecursive list compacting algorithm. Communications of the ACM, 13(1 1):677-678, \n1970. [12] Rance Cleveland, Joachim Parrow, and Bernhard Steffen. The Concurrency Workbench: A semantics-based \ntool for the verification of concurrent systems. Transactions on Programming Languages and Systems, 15(1):36-72, \nJanuary 1993. [13] W. P. Crowley, C. P. Hendrickson, and T. E. Rudy. The SIMPLE code. Technical Report \nUCID 17715, Lawrence Livermore Laboratory, Livermore, CA, February 1978. [14] Cypress Semiconductor, \nRoss Technology Subsidiary. SPARC RISC User s Guide, second edition, February 1990. [15] Digital Equipment \nCorporation, DS5000\\200 KN02 System Module Functional Speci$cation. [16] Digital Equipment Corporation, \nPalo Alto, CA. DECStation 3100 Desktop Workstation Function Specification, 1.3 edition, August 1990. \n[17] Amer Diwan, David Tarditi, and Eliot Moss. Memory subsystem performance of programs with intensive \nheap allocation. Work in progress, oct 1993. [18] R. R. Fenichel and J. C. Yochelson. A LISP garbage-collector \nfor virtual-memory computer systems. Communications of the ACM, 12(11):611-612, 1969. [19] Robert Hieb, \nR. Kent Dybvig, and Carl Bruggeman. Representing control in the presence of first-class continuations. \nIn Proceedings of the SIGPLAN 90 Conference on Programming Lunguage Design and Implementation, pages \n6G77, ACM, June 1990. [20] Mark D. Hill. A case for direct mapped caches. Computer, 21(12):2540, December \n1988. [21] M.D. Hill and A.J. Smith. Evaluating associativity in CPU caches. IEEE Transactions on Computers, \n38(12):1612 1630, December 1989. [22] Norman P. Jouppi. Cache write policies and performance. In Proceedings \nof the 20th Annual International Symposium on Computer Architecture, pages 191 201, San Diego, California, \nMay 1993. [23] Gerry Kane and Joe Heinrich. MIPS RISC Architecture. Prentice-Hall, 1992. [24] Philip \nJ. Koopman, Jr., Peter Lee, and Daniel P. Siewiorek. Cache behavior of combinator graph reduction. Transactions \non Programming Languages and Systems, 14(2):265 277, April 1992. [25] David Kranz, Richard Kelsey, Jonathan \nRees, Paul Hudak, James Philbin, and Norman Adams. ORBIT: An optimizing compiler for Scheme. In Proceedings \nof the SIGPLAN 86 Conference Symposium on Compiler Construction, pages 219 233, Palo Alto, California, \nJune 1986. ACM. [26] James R. Larus. Abstract Execution: A technique for efficiently tracing programs. \nSoflwa~ Practice and Experience, 20(12): 1241 1258, December 1990. [27] James R. Larus and Thomas Ball. \nRewriting executable files to measure program behavior. Technical Report Wk 1083, Computer Sciences Department, \nUniversity of Wisconsin-Madison, March 1992. [28] H. Lieberman and C. Hewitt. A real-time garbage collector \nbased on the lifetimes of objects. Communications of the ACM, 26(6):419-429, 1983. [29] R. L. Mattson, \nJ. Gecsei, D. R. Slutz, and I. L. Traiger. Evaluation techniques for storage hierarchies. IBM Systems \nJournal, 9(2), 1970. [30] Robin Milner, Mads Tofte, and Robert Harper. The Definition of Standard ML. \nMIT Press, 1990. [31] Chih-Jui Peng and Gurindar S. $ohi. Cache memory design considerations to support \nlanguages with dynamic heap allocation. Technical Report 860, Computer Sciences Department, University \nof Wisconsin-Madison, July 1989. [32] Steven A. Przybylski, Cache and Memory Hierarchy Design: A Performance-Directed \nApproach. Morgan Kaufmann Publishers, first edition, 1990. [33] Chris Reade. Elements of Functional Programming. \nAddison-Wesley, Reading, MA, 1989. [34] Jonathan Rees and William Clinger. Revised report on the algorithmic \nlanguage Scheme. SZGPLAN Notices, 21(12):37 79, December 1986. [35] Michael Slater. PA workstations set \nprice/performance records. Microprocessor Report, 5(6), April 1991. [36] David Tarditi and Andrew W. \nAppel. ML~YACC, version 2.0. Distributed with Standard ML of New Jersey, April 1990. [37] Kevin G. Waugh, \nPatrick McAndrew, and Greg Michelson. Parallel implementations from function prototypes: a case study. \nTechnical Report Computer Science 90/4, Heriot-Watt University, Edinburgh, August 1990. [38] Paul R. \nWilson, Michael S. Lam, and Thomas G, Moher. Caching considerations for generational garbage collection: \na case for large and set-associative caches. Technical Report EECS-90-5, University of Illinios at Chicago, \nDecember 1990. [39] Paul R. Wilson, Michael S. Lam, and Thomas G. Moher. Caching considerations for generational \ngarbage collection. In 1992 ACM Conference on Lisp and Functional Programming, pages 32-42, San Francisco, \nCalifornia, June 1992. [40] Benjamin Zorn. The effect of garbage collection on cache performance. Technical \nReport CU-CS-528-91, University of Colorado at Boulder, May 1991. 14 \n\t\t\t", "proc_id": "174675", "abstract": "<p>Heap allocation with copying garbage collection is believed to have poor memory subsystem performance. We conducted a study of the memory subsystem performance of heap allocation for memory subsystems found on many machines. We found that many machines support heap allocation poorly. However, with the appropriate memory subsystem organization, heap allocation can have good memory subsystem performance.</p>", "authors": [{"name": "Amer Diwan", "author_profile_id": "81100202872", "affiliation": "Carnegie Mellon Univ., Pittsburgh, PA", "person_id": "PP15025608", "email_address": "", "orcid_id": ""}, {"name": "David Tarditi", "author_profile_id": "81100531922", "affiliation": "Carnegie Mellon Univ., Pittsburgh, PA", "person_id": "PP14184693", "email_address": "", "orcid_id": ""}, {"name": "Eliot Moss", "author_profile_id": "81406593781", "affiliation": "Carnegie Mellon Univ., Pittsburgh, PA", "person_id": "PP31087595", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/174675.174710", "year": "1994", "article_id": "174710", "conference": "POPL", "title": "Memory subsystem performance of programs using copying garbage collection", "url": "http://dl.acm.org/citation.cfm?id=174710"}