{"article_publication_date": "02-01-1994", "fulltext": "\n Reducing Indirect Function Call Overhead In C++ Programs Brad Calder and Dirk Grunwald (Email:{calder, \ngrunwald}(?cs. colorado. edu) Department of Computer Science, Campus Box 430, University of Colorado, \nBoulder, CO 80309-0430 Abstract Modern computer architectures increasingly depend on mechanisms that \nestimate fhture control flow decisions to increase performance. Mechanisms such as speculative execution \nand prefetching are becoming standard architec\u00ad tural mechanisms that rely on control flow prediction \nto prefetch and speculatively execute future instructions. At the same time, computer programmers are \nincreasingly turning to object-oriented languages to increase their pro\u00ad ductivity. These languages commonly \nuse run time dis\u00ad patching to implement object polymorphism. Dispatch\u00ad ing is usually implemented using \nan indirect finction call, which presents challenges to existing control flow predic\u00ad tion techniques. \nWe have measured the occurrence of indirect function calls in a collection of C++ programs. We show that, \nal\u00adthough it is more important to predict branches accurately, indirect call prediction is also an important \nfactor in some programs and will grow in importance with the growth of object-oriented programming. We \nexamine the improve\u00adment offered by compile-time optimization and static and dynamic prediction techniques, \nand demonstrate how com\u00adpilers can use existing branch prediction mechanisms to improve performance in \nC++ programs. Using these meth\u00adods with the programs we examined, the number of in\u00adstructions between \nmispredicted breaks in control can be doubled on existing computers. Keywords: Object oriented programming, \noptimization, profile-based optimization, customization Introduction The design of computer architectures \nand languages are tightly entwined. For example, the advent of register dis\u00adplacement addressing enabled \nefficient implementation of Perrnksion to copy without fee ell or part of this material is granted provided \nthat the copies sro not made or distributed for direct commercial advsntage, the ACM copyright notice \nand the title of the publication end its date appaar, and notice is given thet copying is by permission \nof the Association for Computing Machinery. To copy otherwise, or to republish, requiras a fea and/or \nspecific permission. POPL 94-1/94, Portland Oregon, USA ~ 1994 ACM o-89791 -636-9/04~1 ..$3.s0 Algol \nand the increased use of COBOL emphasized the use of BCD arithmetic. Likewise, the C and FORTRAN lan\u00adguages \nhave become ubiquitous, strongly influenced RISC processor design. Object-oriented programming has re\u00adcently \ngained popularity, illustrated by the wide-spread popularity of C++. Object-oriented languages exercise \ndif\u00adferent aspects of computer architectures to support the object-oriented programming style. In this \npaper, we ex\u00adamine how indirect fi.mction calls, used to support object polymorphism, influence the pefiormance \nof an efficient object oriented language. Modern architectures using deep instruction pipelines and speculative \nexecution rely on pre\u00addictable control flow changes, and indirect function calls cause unpredictable \nchanges in program control flow. For example, the DEC Alpha AXP 21064 processor, one of the first widely-available \ndeeply pipelined superscalar micro\u00adprocessors, stalls for 10 instructions if the processor mis\u00adpredicts \nthe flow of control. This increases if the mispre\u00addicted target is not in the instruction cache and must \nbe fetched. As systems increasingly rely on speculative exe\u00adcution [19, 16], the importance of control \nflow prediction will increase. In most programs, conditional branches introduce the main uncertainty \nin program flow, and architectures use a variety of branch prediction techniques to reduce instruc\u00adtion \ncache misses and to insure instructions are available for the processor pipeline. Most fimction calls \nspecifi ex\u00adplicit call targets, and thus most fimction calls can be triv\u00adially predicted. Control flow \nprediction is just as important in object-oriented programs, but these languages tend to use indirect \nfinction calls, where the address of the call target is loaded horn memory. Fisher et al [12] said indi\u00adrection \nfimction calls ... are unavoidable breaks in control and there are few compiler or hardware tricks that \ncould allow instruction-level parallelism to advance past them . By accurately predicting the calling \naddress, the processor can reduce instruction stalls and prefetch instructions. Our results show that \naccurately predicting the be\u00adhavior of indirect function calls can largely eliminate the control-flow \nrnisprediction penalty for using static-typed object-oriented languages such as C++. Figure 1 shows the \nnormalized execution time for a variety of C++ pro\u00adgrams. We measured the number of instructions executed \n397 by each program, and collected information concerning the conditional branches and indirect fimction \ncalls executed by each program. Although we measured the programs on a DECstation-5000, we simulated \nthe branch mispre\u00addiction characteristics of a deeply pipelined superscalar architecture, similar to \nthe DEC Alpha AXP 21064. For each program, each bar indicates the number of machine cycles spent executing \ninstructions and suffering from the delay imposed by mispredicting control flow un\u00adder different assumptions. \nA value of 1 indicates the pro\u00adgram spends no additional time due to delays, while a value of 2 indicates \nthe program would execute twice as slowly. The left-most bar indicates the delay that would be incurred \nif every control flow change was incorrectly predicted or there was no prediction. The next bar indi\u00adcates \nthe increase in execution if only conditional branches are predicted, using static profile based prediction. \nThe next three bars indicate the decrease in execution time if branches and indirect function calls are \ncorrectly pre\u00addicted, using three different techniques described in this paper. For the programs we measured, \nwe found we could improve performance 270-24% using these simple tech\u00adniques; the final improvement depends \non the number of indirect function calls, how program libraries are used and the underlying architecture. \nArchitectures that have deeper pipelines or that issue more instructions per cycle would evince greater \nimprovement. We are interested in reducing the cost of indirect func\u00adtion calls (I-calls) for modern \narchitectures. If a compiler can determine a unique or likely call target, indirect func\u00adtion calls can \nbe converted to direct calls for unique call targets. Likewise, compilers may choose to inline likely \nor unique call targets. Inlining I-calls not only reduces the number of function calls, it exposes opportunities \nfor other optimizations such as better register allocation, constant folding, code scheduling and the \nlike. However, type in\u00adferencing for C++ programs is an N_P-hard problem [221, and will be difficult \nto integrate into existing compilers, be\u00adcause accurate type inference requires information about the \nentire type hierarchy. This information is typically not available until programs are linked, implying \nthat type\u00addetermination algorithms will require link-time optimiza\u00adtion. We are interested in more subtle \noptimizations that have a modest, albeit respectable, performance improve\u00adment. Many of the optimization \nand code transforms rely on modifjing an existing program executable and the pos\u00adsibility y of assisting \nthe compiler with profile-based opti\u00admization. When we began this study, we asked the fol\u00adlo%ing questions: \nCould we predict how frequently compiler-based methods could eliminate indirect function calls? How accurate \nis profile-based prediction? Are dy\u00adnamic prediction methods more accurate? Can we use existing branch \nprediction hardware to predict indirect function calls? How effective are combinations of prediction \ntech\u00adniques?  What type of other compiler optimizations can be ap\u00adplied towards indirect function calls, \nin order to im\u00adprove their performance?  To date, our experimentation has demonstrated that al\u00adthough \nobject-oriented libraries support object polymor\u00adphism, the target of most indirect function calls can \nbe ac\u00adcurately predicted, either fi-om prior runs of the program or during a program s execution. Furthermore, \nmany ex\u00adisting C++ programs can be optimized naively by a linker, using information about the C++ type \nsystem. The opti\u00admization we examine involves converting an indirect func\u00adtion call that only calls a \nunique function name into a direct fimction call. For more sophisticated compilers with link-time code \ngeneration, significant optimization oppor\u00adtunities exist. Lastly, a compiler optimization that we term \nI-call if conversion can be used to increase the performance of indirect function calls. This information \ncan be used by a simple profile-based binary modification to improve ex\u00adecution on existing C++ programs \nby 2-24% on modern architectures. We measured the behavior of a variety of publicly avail\u00adable C++ programs, \ncollecting information on instruction counts and function calls. This information is also part of a larger \nstudy to quantiti the differences between C++ and conventional C programs [71. In this study, we show \nthat call prediction is important for many C++ programs and show to what extent static, dynamic and compile-directed \nmethods can reduce indirect function call overhead. We also demonstrate the opportunity for profile-based \nopti\u00admization in the C++ programs we measured. These op\u00adtimization can profitably be applied to existing \narchi\u00adtectures that incur significant control-flow misprediction penalties. The results we present are \ndivided into two portions; the first considers applying hardware branch prediction mechanisms to existing \nC++ programs, while the second considers additional profile based optimizations that can be applied to \nC++ programs. In !2, we discuss some rel\u00adevant prior work. In $3, we describe the experimental methodology \nwe used and describe the programs we in\u00adstrumented and measured. In $4 we compare the various I-call \nprediction mechanisms we studied and summarize how we can improve the performance of ezisting C++ pro\u00adgrams. \n2 Prior Work A considerable amount of research has been conducted in reducing the overhead of method \ncalls in dynamically typed object-oriented languages. Many of the solutions relevant in that domain apply \nto the optimization of com\u00adpiled languages using I-calls, as in C++. We will discuss these shortly. Furthermore, \nnumerous researchers have examined the similar issue of reducing branch overhead. 398 No Prediction \nBranches Only Branches &#38; Icalls Branches, l-calls &#38; Unique Branches, l-calls, Unique &#38; IF \nConv )___( ) (2 I (2.32) (2.35) (1. )_ ~T .. regress Doc Idl Idraw 3roff Morpher Rtsh Figure 1: Normalized \nExecution Time of Various C++ Programs, Including Expected Instructions and Stalls Due to Mispredicted \nControl Flow, For A Computer Similar to DEC Alpha AXP 21064. 399 Since both function calls and branches \nalter the control flow, there is considerable overlap in our work, yet sub\u00adstantial differences as well. \nOne such difference is that a conditional branch has only two possible targets, while an indirect function \ncall can have a large number of potential targets. We have measured programs with 191 different subroutines \ncalled from a single indirect fimction call. This makes branches easier to predict. Some dynamic branch \nprediction mechanisms achieve w 95% 97% prediction accuracy [21, 27, 29]. This level of accuracy is \nneeded for super-scalar processors issuing several instructions per cy\u00adcle [281. The most relevant prior \nwork was on predicting the destination of indirect function calls conducted by David Wall [26] while \nexamining limits to instruction level paral\u00adlelism. He states iLittle work has been done on predicting \nthe destinations of indirect jumps, but it might pay off in instruction-level parallelism. He simul \na tes static profile based prediction and infinite and finite dynamic last call prediction, and finds \nthey accurately predict I-call destina\u00adtions; however, the benefit of I-call prediction was minimal because \nhe only examined C and Fortran programs. We go beyond his research by (1) showing that I-call prediction \nis important for C++ programs, (2) that compile-time opti\u00admization can be combined with static profile \nbased predic\u00adtion to increase a programs performance, (3) that simple techniques yield very accurate \nI-call prediction rates and, (4) we do a more in-depth comparison between different static and dynamic \nmechanisms for doing I-call prediction. 2.1 Compiler Optimization Two classes of compiler-oriented optimization \nare most relevant to our research. A large body of research exists in dataflow analysis to determine \nthe set of possible call targets for a given call site. Ryder[24] presented a method for computing the \ncall-graph, or set of functions, that may be called by each call site using dataflow equations. More \nrecent work by Burke [4] and Hall[131 has refined this tech\u00adnique. All of this work characterized programs \nwhere pro\u00adcedure values may be passed as function parameters. Oth\u00aders, including Hall [13], also examine \nfunctional programs. More recently Pande and Ryder[221 have shown that in\u00adferring the set of call targets \nfor call sites in languages with the C++ type system is a NP-hard problem. In this paper, we seek to \nminimize pipeline stalls using information concerning the frequency of calling specific call sites. Unless \nthe previous algorithms can determine a unique call target, more information is needed for I-call prediction. \nHowever, the results of this study indicates that single call targets occur frequently, and the techniques \nin e.g., Ryder [22] may be very successful in practice. To our knowledge, there has been little work \nin speci&#38;ng the probability of specific call targets being called using dataflow techniques. By comparison, \nthere has been considerable work in adaptive runtime systems to reduce the cost of polymor\u00adphism in dynamically \ntyped languages. Most recent, and foremost, in these efforts is the work by the SELF project [8, 9, 14] \non customization of method dispatches and exten\u00adsive optimization of method lookup. SELF is a dynamicly\u00adtyped \nlanguage, providing a rich set of capabilities not present in statically-typed languages such as C++. \nHow\u00adever, statically-typed languages such as C++ are very effi\u00adcient, using a constant-time method dispatch \nmechanism. Statically-type object-oriented languages are popular be\u00adcause less compiler effort is needed \nto achieve reasonable performance and the object-oriented programming style encourages software resources \nand structured software li\u00adbraries. Customization and other optimization have produced considerable performance \nimprovement in the SELF im\u00adplementation. In many ways, we are extending some of the optimization explored \nby the SELF project to the C++ language. However, we must rely on hardware for cus\u00adtomization (e.g., \nprediction hardware) rather than soft\u00adware, because most C++ implementations are already very efficient. \nFor example, an indirect function call in C++ takes seven instructions on a DECstation -reducing this \ncost to greatly improve the performance of an application is difficult. We also feel that the results \nof our research will benefit prototyping languages such as SELF and SmallTalk as those languages are \nfurther optimized for modern ar\u00adchitectures. There are also secondary effects to these opti\u00admization. \nIn certain cases, our code transformations will allow fimction inlining, facilitating further optimization. \n 2.2 Branch Prediction There are a number of mechanisms to ameliorate the ef\u00adfect of uncertain control \nflow changes, including static and dynamic branch prediction, branch target buffers, de\u00adlayed branches, \nprefetching both targets, early branch res\u00adolution, branch bypassing and prepare-to-branch mecha\u00adnisms \n[181. Conventional branch prediction studies typ\u00adically assume there are two possible branch targets \nfor a given branch point, because multi-target branches occur infrequently in most programs. Rather than \npresent a comprehensive overview of the field, we focus on methods related to the techniques we consider \nin this paper. Some architectures employ static prediction hints us\u00ading either profile-derived information, \ninformation de\u00adrived flom compile time analysis or information about the branch direction or branch opcode \n[25, 20, 21. Wall[261 found profile-driven static prediction to be reasonably ac\u00adcurate for indirect \nfunction calls. Fisher and Freuden\u00adberger [12] found profile-derived static prediction to be effective \nfor conditional branches, but they hypothesize that inter-run variations occurred because prior input \ndid not cover sufficient execution paths, and our results pro\u00advide support for their hypothesis. In general, \nprofile based prediction techniques outperform compile-time prediction techniques or techniques that \nuse hueristics based on branch prediction or instruction opcodes. 400 Some architectures use dynamic \nprediction, either us\u00ading tables or explicit branch registers. A branch target buffer (BTB) [17, 23] \nis a small cache holding the address of branch sites and the address of branch targets. There are myriad \nvariations on the general idea. Typically the cache contains from 32 to 512 entries and may be 2 or 4\u00adway \nassociative. The address of a branch site is used as a tag in the BTB, and matching data is used to predict \nthe branch. Some designs include decoded instructions as well as the branch target. Other designs eliminate \nthe branch target address, obeerving that most branches only go one of two ways (taken/not taken). Still \nother designs eliminate the tag, or branch site address horn the table. These designs use the branch \nsite address as an index into a table of prediction bits. This information can actually be the prediction \ninformation for another branch. However, there s at least a 50q0 chance the prediction information is \ncorrect, and this can be improved somewhat [5]. The most common variants of table-based designs are l-bit \ntechniques that indicate the direction of the most recent branch mapping to a given prediction bit, and \n2-bit techniques that yield much better performance for programs with loops [25, 17, 20]. The advantage \nof these bit-table techniques is that they keep track of very little information per branch site and \nare very effective in practice. Lastly some computers use explicit branch target ad\u00address registers (BTARs) \nto specifi the branch target [11, 1]. This has numerous advantages, because branch instruc\u00adtions are \nvery compact and easily decoded. BTARs can be applied to both conditional branches and function calls, \nand the instructions loading branch targets can be moved out of loops or optimized in other ways. Furthermore, \nad\u00addresses specified by explicit branch targetregisters provide additional hints to instruction caches, \nand those instruc\u00adtions can be prefetched and decoded early. However, most proposed implementations provide \nonly 4-8 BTARs[l]. The contents of these registers will probably not be saved across fimction calls. \nThus, instructions manipulating the BTARs must occur early in the instruction stream to effec= tively \nuse BTARs. At first glance, some of these techniques, such as bit\u00adtable techniques, do not appear applicable \nto I-call predic\u00adtion, because indirect function calls can jump to a number of call targets. Later, we \nshow how profile-based I-call if conversion can use these mechanisms. Experimental Design Our comparison \nused trace-based simulation. We in\u00adstrumented a number of C++ programs, listed in Table 1, using a modified \nversion of the QPT[31 program tracing tool. We emphasized programs using existing C++ class li\u00adbraries \nor that structured the application in a modular, ex\u00adtensible fashion normally associated with object-oriented \ndesign. A more extensive comparison between the charac\u00adteristics of C and C++ programs can be found in \n[7]. Em\u00adpirical computer science is a labour-intensive undertaking. The programs were compiled and processed \non DECstation 5000 s. Three C++ compilers (Gnu G++, DEC C++, AT&#38;T C++ V3.O.2) were required to successfully \ncompile all pro\u00ad grams; much of this occurred because the C++ language is not standardized. This collection \nof programs, when in\u00ad strumented, consumed % lGb of disk space. Despite the good performance of the QPT \ntracing tool for conventional programs, it offers little trace compression in programs using indirect \ncalls. We constructed a simulator to analyze the program traces. l?vpically the simulator was run once \nto collect .. information on call and branch targets, and a second time to use prediction information \nfi-om the prior run. For one program (GROFF), we compared predictions using input from differing runs \nto better assess the robustness of our results. We modified QPT to indicate what caused a basic-block \ntransition (a direct branch. indirect branch or fall-through) and record whether a fimction call was \ncaused by a direct or indirect call. For most programs, we were also able to indicate what fimctions \nwere C++ methods. 1 We classified unpredictable breaks in control into three classes: a 2Brs is a conditional \nbranch, a >2Brs is a branch with multiple destinations, usually arising from switch or case state\u00adments. \nand an I-call is an indirect function call. Table 2 lists the number of occurrences for each type of \nunpre\u00addictable break in control for the different programs. We show three entries for GROFF because we \nuse these three executions for coverage analysis later. Entries in the col\u00adumn Sites From Trace lists \nthe number of branch or call sites of that type encountered during the program execu\u00adtion. For example, \nDOC actually has 2,367 indirect func\u00adtion calls, but we only encountered 1,544 of those calls dur\u00ading \nthe program execution. The heading Occurvwnces During Execution lists the number of times breaks of that \ntype appear during program execution. Thus, there were 5,310,059 indirect fimction calls when we traced \nDOC. Approximately 99% of the indirect calls were to C++ methods, except in GROFF, where 95% were to \nmethods. 4 Performance comparison There are many metrics that can be used to compare I-call prediction \nand combined I-call and branch prediction techniques. Table 3 shows the number of instructions be\u00adtween \nbreaks (NIBBs) without branch or I-call prediction. We tracked only breaks in control flow that will \ncause a long pipeline delay. These breaka are conditional branches and indirect calls. Returns also cause \na long pipeline stall, but returns can be accurately predicted by using a return stack [151, and we did \nnot track these. We assume that un\u00adconditional branches, procedure calls and assigned gotos are accurately \npredicted. These control-transfer instruc\u00adtions, conditional branches and I-calls can also cause an Wecouldnotextractthisinformationfor \nIDL,whichwascompiledby DEC C++ 401 Name Description CONGRESS Interpreter for a PROLOG-like language. \nInput was one of the examples distributed with CONGRESSfor configuration management. Doc Interactive \ntext formatter, based on the InterViews 3.1 library Input was briefly editing a 10 page document. GROFF \nGroff Version 1.7 A version of the ditroff text formatter. One input was a collection of manual pages, \nanother input was a 10-page paper. IDL Sample backend for the Interface Definition Language system distributed \nby the Object Management Group. Input was a sample IDL\u00ad specification for the Fresco graphics I library. \nIDRAW Interactive structured graphics editor, based on the InterViews 2.6 library Example was drawing \nand editing a figure. MORPHER Structured graphics morphing demonstration, based on the InterViews 2.6 \nlibrary. Example was a morphed running man example distributed with the program. RTSH Ray Tracing Shell \n An interactive ray tracing environment, with a TC~K user inter\u00ad face and a C++ graphics library Example \nwas a small ray traced image distributed with the program. Table 1: C++ Programs Instrumented instruction \nmisfetch penalty because they mustbe decoded before the instruction stream knows the instruction type. \nThus, the instruction fetch unit may incorrectly fetch the next instruction, rather than the target destination. \nIn another paper [6], we show how these misfetch penalties can be avoided using extra instruction type \nbits andfor a simple instruction type prediction table, coupled with the techniques discussed in this \npaper. If we are only considering conditional branches and in\u00addirect calls. We would expect the parameters \nin Table 3 to be similar for C and C++ programs. To the contrary, we found that our sample C++ programs \nhad a higher number of instructions between breaka, indicating that C++ pro\u00adgrams tend to either be more \npredictable than C programs or, they use a different linguistic construct for conditional logic. For \nexample, consider a balanced-tree implementa\u00adtion in C and C++. A C programmer might implement a single \nprocedure to balance a tree, passing in several flags to control the actual balancing. A C++ programmer, \non the other hand, would tend to use inheritance and the object model to provide similar functionality. \nThus, it is not sur\u00adprising that C++ programs tend to have more procedure calls and fewer conditional \noperations [71. In the remaining tables, we only show the mean NIBB for each program and use the harmonic \nmean of the NIBB as a summary. We also use the percent of breaks pre\u00addicted (YoBP) to understand how \nwell various techniques predict breaks. This metric, or the more common mis\u00adprediction rate, is commonly \nused to compare branch pre\u00addiction mechanisms. However, note that the %BP metric does not account for \nthe density of breaks in a program. For example, there may be a single conditional branch in a 100,000,000 \ninstruction program. While the the branch may always be mispredicted, the number of instructions between \nbreaks remains high. Therefore, it is useful to look at both the NIBB and %BP when comparing predic\u00adtion \ntechniques across different programs, 4.1 Bounds on Compile-time i-Call Prediction We were interested \nin determining how well interprocedu\u00adral dataflow analysis could predict indirect method calls [22] and \nwe compared these results to profile-based static prediction methods. Method names in C++ are encoded \nwith a unique type signature. If a linker knew the in\u00adtended type signature at a call site,2 and there \nwas a sin\u00adgle function with that signature, then no other function could be appropriate for that call \nsite and the indirect call could be replaced by a direct call. We call this the Unique Name measure, \nand feel it represents a lower bound on what could be accomplished by a dataflow optimization al\u00adgorithm \n in practice, a dataflow method should be more accurate, because a symbol table in UNIX system typically \nincludes methods horn classes that are never invoked. At the other extreme, we recorded the number of \nSingle Tar\u00adget I-call sites, or I-call sites that record a single call target during a trace. Compiler \ndirected I-call prediction is most useful if a single target can be selected. In the number of traces \nwe recorded, the Single Target measure represents an upper bound on the target prediction we could expect \nfrom dataflow-based prediction algorithms. Both Unique Name and Single Target values measure the number \nof dynamic occurrences. In general, our results indicate sig\u00ad nificant promise from static analysis of \nC++ programs. In particular, because it is so effective and simple to imple\u00adment, we feel the Unique \nName measure should be inte\u00adgrated into existing compilers and linkers. 2Currently,compilersdon tprovidethis \ninformation at call sites,but thisinformation is easy to capture. 402 From ace OccurrencesD( mgExec \nion ICalls > 2Brs 2Brs ICalls > 2Brs Instructions  IzE??J&#38; congress I 1,817 309 4 18,352,179 j \n342,266 43,593 152,658,312 doc 5,398 1,544 6 48,536,165 5,310,059 13,585 406,673,898 groff-1 1,974 671 \n9 4,525,946 214,219 63,116 37,655,949 groff-2 2,110 722 9 6,047,864 205,487 70,759 46,416,495 groff-3 \n2,370 759 10 7,977,220 304,948 94,298 63,641,709 idl 1,011 525 6 11,809,125 2,991,355 38,157 151,419,127 \nidraw 6,473 2,279 12 18,699,046 1,490,460 51,664 184,930,700 morpher 4,050 1,200 6 6,613,548 425,072 \n7,807 52,131,648 rtsh 1,390 59 3 52,357,435 5,547,705 489 822,054,191 L Table 2: Detailed statistics \non number of instructions and breaks in control for each program measured. Metric congress doc groff-3 \nidl idraw morpher rtsh Mean 8,15 7.55 7.60 10.20 9.14 7.40 14.20 Median 6.00 6.00 5.00 9.00 5.00 4.00 \n7.00 StdDev 7.57 5.99 7.19 6.06 15.94 8.76 30.94 Table 3: Number of instructions between breaks in the \nabsence of any control flow prediction. Program Unique Names Single Target Static Inf l-Bit BTB Inf 2-Bit \nBTB CONGRRSS 18.5 29.5 73.7 76.7 88.8 Doc 56.2 76.7 93.2 92.2 96.5 GROFF-3 6.6 31.2 86.4 79.0 95.3 IDL \nt 99.9 99.9 99.9 99.9 IDRAW 34.9 83.5 94.6 95.6 98.0 MORFHER 70.3 92.0 96.7 96.6 97.7 RTSH 2.9 51.1 93.6 \n96.0 98.0 Mean 31.6 66.3 91.2 90.9 96.3 Table 4: Percentage of indirect function calls predicted using \ncompile-time, profile-based and dynamic prediction. (t could not be computed) Using using Using Using \nAll Program GROFF-1 GROFF-2 GROFF-3 Combined GROFF-1 87.4 84.4 86.6 86.6 GROFF-2 82.5 86.4 85.6 86.4 \nGROFF-3 79.6 85.6 86.4 86.3 Table 5: Percentage of I-calls predicted for GROFF when using static prediction \nwith different input files. Metric CONGRESS Doc Break 9.02 8.36 Call 14.45 14.62 Table6: Meannumber ofinstructions \nabove the I-call GROFF 8.42 14.57 IDL 9.04 11.27 IDRAW 9.86 18.92 MORPHER 11.17 15.16 RTSH 9.12 14.30 \nintheinstruction stream before hitting abreakor another finction  call. Both of these include thestati \nofaprocedure as break pint. EiiiIiE congress 67.7 doc 56.0 groff-3 74.0 idl 45,7 idraw 71.0 morpher \n51.9 rtsh 71.6 ~ HM 1Avg 60.7 Brs I Uniuue Name II Single Target \\ Static ICalls %BP II NIBB \\ %BP I \nNIBB %BP EmtEIEE 88.0 II 69.7 I 88.3 II 70.9 88.5 \\\\ 76.2 89.3 86.5 95.2 92.1 127.6 94.1 176.4 95,7 \n89.7 75.8 90.0 83.2 90.9 106.7 92.9 77.7 45.7 77.7 467.3 97.8 468.0 97.8 87.1 88.7 89.7 136.0 93.3 154.8 \n94.1 85.8 74.0 90.0 85.1 91.3 87.9 91.6 80.2 72.6 80.4 95.0 85.1 130.5 89.1 I1 85.0 71.1 86.9 106.5 92.9 \n~ Table 7: Measurements of breaka that can be predicted using compile-time and static I-call prediction \nwith 2-Bit branch prediction. 4.2 Static vs. Dynamic Prediction Although compiler techniques appear promising, \nwe found that profile-based and dynamic prediction techniques were clearly better. We implemented a simple \nmajority profile\u00adbased technique. We ran the programs, recorded the most likely target for each call \nsite and used that to predict call targets in future runs. The results are shown in the col\u00adumn labeled \nStatic in Table 4. This simple technique ac\u00adcurately predicted a surprisingly large number of I-Calls. \nIn each of these runs, we used the same program input to generate the prediction trace and the measurements \nshown. To determine how accurate these prediction rates are for different inputs, we ran GROFF with two \nother in\u00adputs. Table 5 shows the percentage of I-Calls predicted us\u00ading all combinations of the different \ninput files. We found a small number of prediction sets appear sufficient to provide accurate predictions. \nIn some cases, profile-based methods have poor performance. In our experience, this usually occurs because \nthe inputs, that were used to es\u00adtablish the profile used to predict the branches and I-calls, did not \nprovide adequate coverage of all the branches and indirect function calls. This problem has been mentioned \nby others [26, 12], but has not be studied in detail. Table 4 also shows the effectiveness of idealized \ndy\u00adnamic prediction techniques. We simulated two infinitely large Branch Target Buffers. The first BTB \nVi-bit ) simply used the previous I-Call target as the prediction for fhture I-calls, much like the method \ncaching used in Self, where the most recent method is saved. The second ( 2-bit ) used a 2-bit strategy \nthat avoids changing the prediction infor\u00admation until the previous prediction is incorrect twice in \na row. I-Calls were considered unpredicted when first en\u00ad countered. Surprisingly, the l-bit mechanism \nhas worse performance than static prediction; however, it does not require profiling runs. The improvement \nshown by the 2\u00adbit technique illustrates that the l-bit technique changes prediction too rapidly. For \nexample, if a call site calls the sequence of methods A.:X(), B::X(), A:X(), the l-bit method would miss \nthree times, while the 2-bit method would miss only once. This information is important for designers \nof wide-issue processors. For example, some recent design proposals consider using up to 16KB of memory \nfor such BTB s. In another paper, we show how to eliminate the need for most of those resources [6]. \nIt is likely that our prediction architecture would benefit from a small 2-bit prediction mechanism for \nindirect fi.mction calls. The last prediction mechanism we considered was branch target address registers \n(BTAR s). We assumed ar\u00adchitectures would implement a small number of BTAR s, and they would likely not \nbe saved across procedure calls. Thus, there are two limits on using BTAR s to indicate in\u00adtended branch \ntargets. We assumed that the BTAR could be loaded anywhere in the previous basic block, providing a lower \nbound on the interval when the BTAR is loaded and the branch taken. Likewise, we assume a clever com\u00adpiler \nmight be able to load the BTAR immediately follow\u00ad ing the previous procedure call or return (because \nwe as\u00adsumed BTARs are not saved across function calls). Table 6 shows these two values (instructions \nsince beginning of basic block and instructions since last call/return). In gen\u00aderal, there are very \nfew instructions in which to schedule the prepare to jump information before the first target instruction \nis needed. For simple prediction of targets for indirect function calls, we found:  Dynamic methods \nusing the 2-bit branch target buffer were the most effective technique we consid\u00adered. However, this \nstyle of prediction maybe expen\u00adsive to implement.  By combining a simpler branch prediction technique \nwith BTB s for indirect function calls, the resource demands become more realistic.  Static profile-driven \nprediction was very accurate, and is used in the remainder of this paper.  4.3 Using Profiles to Eliminate \nIndirect Function Calls Our prior measurements have shown the percentage of I-calls predicted using these \ndifferent techniques. By com\u00adparison, Table 7 shows the percent of total breaks pre\u00addicted. Using static \nprediction with prior profiles for I-calls accurately predicts half of the remaining breaks in control, \ndoubling the number of instructions between breaks (as\u00adsuming that the breaks are evenly distributed). \nRecall Figure 1. In this figure conditional branches and indirect fimction calls are predicted using \nthe static profile-based technique just described. The second bar for each pro\u00adgram indicates the additional \ndelays incurred by breaks from indirect function calls and mispredicted conditional branches. While the \nthird bar, eliminates delays horn statically predicted indirect function calls. For architec\u00adtures providing \nBTB s, the delay would be slightly smaller. Clearly, predicting branches is the foremost priority, but \npredicting indirect calls removes a substantial number of breaks. The success of profile-based static \nprediction also indi\u00adcates that many methods could be successfidly compiled inline, even without compile-time \ntype analysis. We can convert an indirect function call, e.g.,ob j ec t -> f oo ( ) ; to a conditional \nprocedure call with a run-time type check if (typeof(ob-ject) == A ) object -z A:: fooo; else object \n> fooo; This transformation is useful for three reasons. First, once this code transformation has been \nperformed, fimc\u00adtion call A: : f oo ( ) can be inlined. Secondly, If there is a high likelyhood of calling \nA: : f oo ( ), this code sequence is less expensive on most RISC architectures, using on 4\u00ad5 instructions \nrather than 5-8. Lastly if an architecture provides branch prediction but does not support prediction \nfor indirect fimction calls, the transformed code can avoid many misprediction penalties. Existing branch \nprediction hardware may be able to improve on strictly profile-based prediction because it can accommodate \nbursts of calls to a secondary call target. Although inliningflmctions is useful, it does not always \nreduce program execution time [10]. However, many indi\u00adrect fimction calls in C++ tend to be very short, \nbecause programmers are more likely to employ proper data encap\u00adsulation techniques. We believe automatic \ninlining will be more useful for C++ than C. Further, on most architec\u00adtures, the converted indirect-fimction \ncall is more efficient if there is a high Iikelyhood of calling the most common function (A: : f oo ( \n) above). We constructed the following cost models for handling I-calls and used this to optimize I-calls \nin more detail. Assume the cost of a direct method call, Cdmc, is 2 in\u00adstructions. This comes from an \nextra instruction needed to compute the object pointer which is passed to the call in\u00adstruction. The \ncost of an indirect method call, Cimc, is 7 in\u00adstructions, and is because of the extra instructions needed \nto compute the pointer addresses and fbture branch tar\u00adget. The cost of an i~ Cif, as shown in the previous \nexample, is 3 instructions, including an indirect load for the object pointer, a load of a constant and \nthe comparison. The penalty for mispredicting a conditional branch or an indirect finction call, Cmiss, \nis 10 instruction times. This is because we assume mispredicted breaks can cause a 10 cycle pipeline \ndelay. From this we get the cost for indirect method calls with no prediction to be Since the indirect \ncall is not predicted, it is considered to be mispredicted. By comparison, with the static profile\u00adbased \nprediction mechanism as discussed in the previous section, the cost becomes cp..d~ct(p) = Came + (1 P) \nCma$$ . if there is a P?ZOprobability of accurately predicting the call target for a call site. The \ncost for converting an indirect method call to an i~ as done above, would be however, we can use the \nexisting profile information to compute Q, the percentage of the second most likely call target being \nselected. Note that Q has to be less than or equal to min(P,( l-P)), else it would be the most likely \ntarget selected. It is interesting to note that Q might be a high percentage of the remaining I-calls. \nFor example, we may have P = 40%, with the next most likely branch occurring Q = 35% of the time. This \nmeans that 35/(100 40), or 58% of the remaining 60% of I-calls are correctly predicted. Thus, the cost \nfor converting an indirect ftmction call to an if construct is actually Cti..-ij(P, ~) = Cit + Pcdmc \n+ (1-P)(Cm+.$ + Cp.edict(~/(1 -p))) . Figure 2 shows the costs for C&#38;p-edict (the horizon\u00adtal line), \nc=~,dici (the lower line) and the boundaries for 405 Ca.e ~f (P, Q) for Q = rnin(P, (1 P)) the best \ncase, and Q = 0.0 the worst case. In the worst case, our if con\u00adversion is the same as CU8e it (P). This \nis a hypotheti\u00adcal case when there are so many second most likely tar\u00adgets that Q is approximately equal \nto zero percent. In the graph the best case if conversion, Cs.. -,t (P, Q) for Q = m~n(p, (1 P)), is \nachieved when an I-call site has only 2 targets. This can only happen when P >= 0.5. So when P < 0.5, \nat best Q can only equal P, and the remaining 1 (P + Q) I-call targets cannot be predicted. This is \nthe reason the line in the graph for the best case if conversion changes slope at P = 0.5. From the graph, \none can see that it is always a benefit to do static I-call predic\u00adtion. Given the values for C~f, Cdmc, \nCmi$,and CimCthat we used, when P = 0.6, depending on how accurate one can predict the second most likely \ntarget Q, it can be bet\u00adter to do the if conversion on the I-call rather than only predicting the most \nlikely target. From the graph, one can see that, Cti, c--if(P) eventually hIteI%eCtS Cpredict (P), where \nit is always beneficial to do the if conversion on an architecture that provides static prediction. With \nour architecture assumptions, this occurs at P = 0.86. The lines c~~redict and Cu,. i j (P) (WOrst Case) \nalso eventu\u00adally intersect, where it is always beneficial to do the if conversion on an architecture \nthat does not movide static prediction. With our architecture assumptions, this occurs at P = 0.52. Thus, \ndoing the if conversion on an architec\u00ad ture that does not provide static prediction gives the user, \nin a sense, the benefit of static prediction. As mentioned, the architecture we ve considered is similar \nto the Digital AXP 21064. Other architectures, including the Intel Pen\u00adtium, also issue two instructions \nper clock, and some newly announced architectures, such as the IBM RIOS-11 issue up to eight instructions \nper cycle. On these architectures, the advantage of if-conversion occurs with much lower probabilities \nfor P. In general, prediction information can greatly reduce the penalty for indirect function calls. \nAs noticed from the graph it is always beneficial to predict the destination for an I-call. Accurate \nprofile-based measurements expose other optimizations when the accuracy of predicting the most frequently \ncalled function exceeds 80-90%. Table 4 shows this occurs for many of the programs we measured. This \ntransformation also provides opportunity for inlining the body of the function, allowing the compiler \nto customize the parameters to the function, avoid register spills and the like. The right-most bar for \neach program shown in Figure 1 shows the effect of applying this transformation where ap\u00adpropriate, \nbased on our model, for each call site in the programs. By comparison, the second bar horn the right \nshows the benefit of using prediction and unique name elimination, without using the if conversion. Although \nthe advantage is small, similar costtbenefit analysis can be used to determine the advantage of additional \nfunction inlining. Note, there will be a greater advantage in using the if conversion when the architecture \ndoes not support static prediction. 5 Conclusions and Future Work As object-oriented programs become \nmore common, there will be an increasing need to optimize indirect fimction calls. This will become even \nmore important as processor pipeline depths increase and superscalar instruction issue and speculative \nexecution become more common. Exist\u00ading branch prediction mechanisms accurately prediction 95% 97% of \nconditional branches. Because branch predic\u00adtion is so successful, accurate prediction of the remaining \nbreaks in control-flow becomes increasingly important as processors begin to issue more instructions \nconcurrently. Eliminating the misprediction penalty for indirect function calls in C++ programs can remove \n10% of the remaining breaks in control in a C++ program. We found that static profile-based prediction \nmecha\u00adnisms worked well for the collection of existing C++ pro\u00adgrams we examined. We saw additional improvements \nby combining compiler optimization techniques (unique name elimination and if conversion ) with static \nindirect call pre\u00addiction. The information from profile-based prediction is aIso useful for other code \ntransformations, such as inlin\u00ading and better register scheduling. Our results show that we get an average \nof 10VOimprovement in the number of instructions executed for a program by using our I-call predictionloptimization \ntechniques. We recommend that compilers for highly pipelined, speculative execution architectures : use \nprofile-based static prediction methods to optimize C++ programs, use link-time information to remove \nindirect function calls, and customize call-sites using if conversion based on pro\u00adfile information. \nFurthermore, we hope the architecture and benchmarking community expands benchmark suites to include \nmodern programming languages such as C++, Modula-3 and the like, because these languages exercise different \narchitectural features than C or Fortran pro\u00adgrams. Acknowledgements We would like to thank Ben Zorn, \nJohn Feehrer and the reviewers for comments on the paper. We d also like to thank James Larus for developing \nQPT and helping us solve the various problems we encountered applying it to such large programs. This \nwork was funded in part by NSF grant No. ASC-9217394, and is part of a continued effort to make languages \nsuch as C++ suitable for scientific computing. References [1] Robert Alverson, David Callahan, Daniel \nCummings, Brian Koblenz, Allan Porterfield, and Burton Smith. The tera computer system. In Proc. 17th \nAnnual Sym\u00adposium on Computer Architecture, Computer Archi\u00ad 406 Optimizing an I-Call Site from the \nStatic Profile 30%*.:. 11t 11f 11[ g:;;..a No Prediction 4-\u00ad x:;,- .0 ... Static Prediction -+-. ..... \n25 x. m.. Using If (Worst Case) -U-\u00ad ....., - a.. Using If (Best Case) x-. x..., D.. ...... x.., m.. \n,...., x 20 - m .. .,..., m.. x. ..,, m. <b. 4> --+---+ m -. --+--- x .. w ...., o 15 - t---- Q,. \ni. ---+ x, --- Q. + -.. .,, .. -1\u00ad ----l. -.- x .... Q.. -=%...% 5.. -..*-,-6 10 - n. - -x .:::+--\u00ad \n%, -+:>: ~ -x. ... -..x.: -&#38;--+--\u00ad x...;;.g,.,>-\u00ad 5 ~J 1I 11111110 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 \n0.8 0.9 1 Percentage of an I-Calls Site Most Probable Target (P) Figure 2: Cost, in instructions and \nadditional delay for different indirect function call methods tecture News, pages 1-6, Amsterdam, Netherlands, \nJune 1990.ACM,ACM. [2] T. Ball and J. R. Larus. Branch prediction for flee. 1n1993SIGPLAN Confernce \non Programming Lan\u00adguageDesignand Implementation. ACM, June 1993. [3] T. Ball andJ.R. Larus. Optimally \nprofilingandtrac\u00ading programs. In Conference Record of the 19thAn\u00adnualACMSympo~um on Principles ofProgramming \nLanguages,pages 59 70,Albuquerque, NM,January 1992. [4] Michael Burke. An interval-based approachto ex\u00adhaustive \nand incremental interprocedural analysis. ACM Transactions on Programming Languages and Systems, 12(3):341 \n395, July 1990. [5] Brad Calderand Dirk Grunwald. Branch alignment. Technical Report (In Preperation), \nUniv. of Colorado-Boulder, 1993, [61 Brad Calder and Dirk Grunwald. Fast &#38; accurate instruction fetch \nand branch prediction. CU-CS xxx, Univ. of Colorado, October 1993. (In preperation). [71 Brad Calder, \nDirk Grunwald, and Benjamin Zorn. Ex\u00adploiting behavioral differences between C and C++ programs. Technical \nReport (In Preperation), Univ. of Colorado-Boulder, 1993. [81 Craig Chambers and David Ungar. Customiza\u00adtion: \noptimizing compiler technology for SELF, a dynamically-typed object-oriented programming lan\u00adguage. In \nProceedings of the ACM SIGPLAN 89 Con\u00adference on Programming Language Design and Imple\u00admentation, pages \n146-160, Portland, OR, June 1989. [91 Craig Chambers and David Ungar. Iterative type analysis and extended \nmessage splitting: optimiz\u00ading dynamically-typed object-oriented programs. In Proceedings of the ACM \nSIGPLAN 90 Conference on Programming Language Design and Implementation, pages 150-164, White Plains, \nNY, June 1990. [10] Jack W. Davidson and Anne M. Holler. Subprogram inlining: A study of its effects \non program execution time. IEEE Transactions on Software Engineering, 18(2):89 102, February 1992. [11] \nJack W. Davidson and D.B. Whalley. Reducing the cost of branches by using registers. In Proceedings of \nthe 17th Annual International Symposium on Computer Architecture, pages 182 191, Los Alamitos, CA, May \n1990. [121 J. A. Fisher and S. M. Freudenberger. Predicting conditional branch directions from previous \nruns of a program. In Fifth International Conference on Ar\u00ad chitectural Support for Programming Languages \nand Operating Systems, pages 85 95, Boston, Mass., Oc\u00ad tober 1992. ACM. 407 [131 [14] [15] [16] [171 \n[18] [191 [20] [21] [22] [23] [24] [25] Mary W. Hall and Ken Kennedy. Efficient call graph analysis. \nLetters on Programming Languages and Systems, 1(3):227-242, September 1992. Urs Holzle, Craig Chambers, \nand David Unger. Op\u00adtimizating dynamically-typed object-orientred lan\u00adguages with polymorphic inlines \ncaches. In ECCOP 91 Proc. Springer-Verlag, July 1991. David R. Kaeli and Philip G. Emma. Branch history \ntable prdiction of moving target branches due to sub\u00adroutine returns. In 18th Annual International Sym\u00adposium \nof Computer Architecture, pages 34-42. ACM, May 1991. M. S. Lam and R. P. Wilson. Limits of control flow \non parallelism. In 19th Annual International Sym\u00adposium of Computer Architecture, pages 46-57, Gold Coast, \nAustralia, May 1992. ACM. Johnny K. F. Lee and Alan Jay Smith. Branch predic\u00ad tion strategies and branch \ntarget buffer design. IEEE Computer, pages 6-22, January 1984. David J. Lilja. Reducing the branch penalty \nin pipelined processors. IEEE Computer, pages 47 55, July 1988. M. S. Lam M. D. Smith, M. Horowitz. Efficient \nsuper\u00adscalar performance through boosting. In Fifth Inter\u00adnational Conference on Architectural Support \nfor Pro\u00adgramming Languages and Operating Systems, pages 248 259, Boston, Mass., October 1992. ACM. Scott \nMcFarling and John Hennessy. Reducing the cost of branches. In 13th Annual Internationa~ Sym\u00adposium of \nComputer Architecture, pages 396-403. ACM, 1986. S.-T. Pan, K. So, and J. T. Rahmeh. Improving the accuracy \nof dynamic branch prediction using branch correlation. In Fifth International Conference on Ar\u00adchitectural \nSupport for Programming Languages and Operating Systems, pages 76-64, Boston, Mass., Oc\u00adtober 1992. ACM. \nHemant D. Pande and Barbera G. Ryder. Static type determination for C++. Technical Report LCSR-TR\u00ad197, \nRutgers Univ., February 1993. Chris Perleberg and Alan Jay Smith. Branch target buffer design and optimization. \nIEEE Transactions on Computers, 42(4):396-412, April 1993. Barbera G. Ryder. Constructing the call graph \nof a program. IEEE Transactions on Software Engineer\u00ading, SE-5(3):216-226, May 1979. J. E. Smith. A study \nof branch prediction strategies. In 8th Annual International Symposium of Computer Architecture. ACM, \n1981. [26] D. W. Wall. Limits of instruction-level parallelism. In Fourth International Conference on \nArchitectural Support for Programming Languages and Operating Systems, pages 176-188, Boston, Mass., \n1991. ACM. [27] Tse-Yu Yeh and Yale N. Patt. Alternative implemen\u00adtations of two-level adaptive branch \npredictions. In 19th Annual International Symposium of Computer Architecture, pages 124-134, Gold Coast, \nAustralia, May 1992. ACM. [28] Tse-Yu Yeh and Yale N. Patt. A comprehensive in\u00adstruction fetch mechanism \nfor a processor support\u00ading speculative execution. In 19th Annual Interna\u00adtional Symposium on Microarchitecture, \npages 129 139, Portland, Or, December 1992. ACM. [29] Tse-Yu Yeh and Yale N. Patt. A comparison of dy\u00adnamic \nbranch predictors that use two levels of branch history In 20th Annual International Symposium of Computer \nArchitecture, pages 257 266: San Diego, CA, May 1993. ACM. 408   \n\t\t\t", "proc_id": "174675", "abstract": "<p>Modern computer architectures increasingly depend on mechanisms that estimate future control flow decisions to increase performance. Mechanisms such as <italic>speculative execution</italic> and <italic>prefetching</italic> are becoming standard architectural mechanisms that rely on <italic>control flow prediction</italic> to prefetch and speculatively execute future instructions. At the same time, computer programmers are increasingly turning to object-oriented languages to increase their productivity. These languages commonly use <italic>run time dispatching</italic> to implement object polymorphism. Dispatching is usually implemented using an indirect function call, which presents challenges to existing control flow prediction techniques.</p><p>We have measured the occurrence  of indirect function calls in a collection of C++ programs. We show that, although it is more important to predict branches accurately, indirect call prediction is also an important factor in some programs and will grow in importance with the growth of object-oriented programming. We examine the improvement offered by compile-time optimizations and static and dynamic prediction techniques, and demonstrate how compilers can use existing branch prediction mechanisms to improve performance in C++ programs. Using these methods with the programs we examined, the number of instructions between mispredicted breaks in control can be doubled on existing computers.</p>", "authors": [{"name": "Brad Calder", "author_profile_id": "81100088945", "affiliation": "Department of Computer Science, Campus Box 430, University of Colorado, Boulder, CO", "person_id": "PP17000250", "email_address": "", "orcid_id": ""}, {"name": "Dirk Grunwald", "author_profile_id": "81100218381", "affiliation": "Department of Computer Science, Campus Box 430, University of Colorado, Boulder, CO", "person_id": "PP15026204", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/174675.177973", "year": "1994", "article_id": "177973", "conference": "POPL", "title": "Reducing indirect function call overhead in C++ programs", "url": "http://dl.acm.org/citation.cfm?id=177973"}