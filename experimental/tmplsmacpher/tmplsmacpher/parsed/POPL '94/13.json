{"article_publication_date": "02-01-1994", "fulltext": "\n Soft Typing with Conditional Types Alexander Aiken* and Edward L. Wimmers T. K. Lakshmant IBM Almaden \nResearch Center Department of Computer Science 650 Harry Rd., San Jose, CA 95120 University of Illinois \nat Urbana-Champaign {aiken,wimmers}@almaden. ibm.com 1304 W. Springfield Ave, Urbana, IL 61801 lakshman@)cs.uiuc. \nedu We present a simple and powerful type inference method for dynamically typed languages where no \ntype information is supplied by the user. Type inference is reduced to the problem of solvability of \na system of type inclusion con\u00adstraints over a type language that includes function types, constructor \ntypes, union, intersection, and recursive types, and conditional types. Conditional types enable us to \nana\u00adlyze control flow using type inference, thus facilitating com\u00adputation of accurate types. We demonstrate \nthe power and practicrdity of the method with examples and performance results from an implementation. \nIntroduction Most modern programming languages employ type checking to guarantee that functions are \napplied only to appropriate arguments. Languages differ in the degree to which the type checking is stai?ic \n(performed at compile-time) or dynamic (performed at run-time). Statically typed languages, such as ML, \nrequire that function applications be proven type-safe at compile-time, Thk is enforced by a type inference \nalgo\u00adrithm that assigns types to program phrases. If the type in\u00adference algorithm verifies that a program \ncannot go wrong (i.e., is free of run-time type errors) the program is accepted; otherwise, the program \nis rejected. Static type checking eliminates the need to perform run-time type checldng and detects many \nprogramming errors at compile-time. The cost of this efficiency and security is loss of programming flexibil\u00adity, \nbecause no decidable type inference system can be both sound and complete---some programs that cannot \ngo wrong must be rejected in any statically typed language. Dynamically tgped languages, such as Lisp \nand Scheme, impose no type constraints on programs and, in the worst c~e, perform all type-checking at \nrun-time. This permits maximum programming flexibility at the potential cost of efficiency and security. \nHowever, in an implementation of Author s current address: Computer Science Division, Uni\u00adversity of \nCalifornia at Berkeley, Berkeley, CA 94720, email: aiken@cs.berkeley.edu tThi~ work was done while visiting \nIBM Almaden Research Center. Permission to copy without fee all or pmt of this meterial is granted provided \nthat tha copies are not made or distributed for direct commercial advantage, the ACM copyright notica \nand the titJe of the publication and its data appear, and notice is given that copying is by permission \nof the Association for Computing Machinery. To copy otherwise, or to rapubiish, requires a fea andor \nspecific permission. POPL 94-1/94, Portland Oregon, USA @ 1994 ACM O-S9791-636JX141001 ..$3.50 a dynamically \ntyped language it is beneficial to perform at least some static type checking. This regains some of the \nbenefits of statically typed languages; how much is re\u00ad gained depends on the power of the type inference \nalgo\u00ad rithm. Type inference systems for dynamically typed lan\u00ad guages have been dubbed sojl typing systems \nby Cartwright and Fagan [6]. Interest in inferring types for dynamically typed pro\u00adgrams began with Reynolds \n[19]. Since then, numerous al\u00adgorithms have been proposed, baaed variously on tree gram\u00admars [13, 23], \nad hoc extensions of static type systems [6, 10, 21], abstract interpretation [2, 20], and constraint \nsolving [11, 12]. Many of these techniques are complicated, limited in power, or both. In this paper \nwe present a soft typing system that is both simple and powerful. Our method is simple because we ad\u00address \na type inference problem directly using type inference techniques; there are no special encodings or \nawkward cases to consider. Our method is also powerful: our algorithm au\u00adtomatically infers accurate \ntypes for recursive, higher-order, dynamically typed programs with no type information sup\u00adplied by the \nuser. The essence of our approach is to perform standard ML\u00adstyle type inference, but over a much richer \ndomain of types. Our type language (Section 3) includes function types, a leaat type O, a greatest type \n1, intersection, union, recur\u00adsive types, and wnditional types (see below). A program is well-typed if \nit is provable that the program is free of run\u00adtime type errors. Our type inference system reduces the \nproblem of determining whether a program is well-typed to the satisfiability of a system of type inclusion \nconstraints (Section 4). The inclusion constraints are solvable, which provides an effective type inference \nprocedure (Section 5). The most novel feature of our type language is coridi\u00adtional types (Section 3). \nWith conditional types, the type of an expression e can be constrained using information about the results \nof run-time tests in the context surrounding e. For example, in an expression case el of true : ea, false \n: es conditional types can express that ez is evaluated only in environments where el is true, and es \nis evaluated only in environments where el is jalse. Thk+ kind of analysis usually is called control-jlow \nanalysis. The ability to take advantage of control-flow information in type inference is crucial to computing \naccurate type information in dynamically typed programs [2, 20]. Using conditional types, we eliminate \nthe ad hoc steps in [2, 20] used to perform control-flow analysis. The type inference system presented \nin Section 4 is quite powerful and interesting in its own right. However, since no decidable type inference \nsystem can be both sound and com\u00adplete, our system rejects some programs that make no run\u00adtime type errors. \nIn a dynamically typed language programs cannot be rejected by a type system after all, the language \nitself imposes no type constraints. Following [6, 12], we take the view that explicit run-time type checks \nshould be added to the program to make it well-typed. Section 7 describes how our system automatically \nadds such type checks to a program. This paper makes three contributions. First, we believe our system \ninfers the most accurate types of any proposed type inference system for dynamically (or statically) \ntyped languages. Second, we show how control-flow analysis can be performed using type inference with \nconditional types. Third, we show that several proposals for performing pro\u00adgram analysis of dynamically \ntyped languages are in fact specitd cases of solving systems of type inclusion constraints. Thus, our \napproach unifies a diverse body of work on pro\u00adgram analysis (Section 9). The type inference system described \nhere has been imple\u00admented and extensively tested for the functional programm\u00ading language FL. Section \n6 presents examples, while Sec\u00adtion 8 discusses the implementation and presents the results of performance \nmeasurements. Proofs and some technical discussion are deferred to appendices. 2 A Programming Language \nWe illustrate our system using a simple dynamically typed, higher-order functional language. Our system \nshould be extensible to imperative language features using standard techniques [22]. The programming \nlanguage .Cis the lambda calculus with constructors, let, case, and patterns: e ::= xl Az.ellel e21c(el, \n. . ..e~) llletz=eline21 caaeeofpl :el, . . ..pn :en ::= ZIZ aspllc(pl, . . ..pn) P Giving a formal semantics \nto C is routine and we omit it. We briefly summarize the important features of the seman\u00adtics. The semantic \ndomain D of C satisfies the equations D+ = (D++D)U U C(D+ ,..., D+) Cec D = D+ U {1, wrong} The set C \nis a set of data constructors (e.g., constants, pairs, etc.), the value 1 denotes non-termination, and \nthe value wrong denotes an erroneous computation. The meaning function p : Expr x Env + D for L expres\u00adsions \nmaps an expression e and an environment assigning meaning to the free variables of e to an element of \nD. Ex\u00adpressions are generally strict in J_ and wron~ for example, ~ l= J-, c(1, c) =-l-, etc. The exceptions \nare lambda ab\u00adstraction (k. -1 is a function that returns 1) and case (see below). The choice of a strict \nsemantics over a lazy seman\u00adtics is not important; our techniques work for lazy languages as well, In \nan expression case e of pl : en,. . ., pn : en, if e eval\u00aduates to v and v matches the shape of pattern \npi, then the result is ei with the variables in pi bound to correspond\u00ading components of v. The other \nbranches are not evaluated. The pattern x asp binds z to a value that matches p. If no pattern matches \na case analysis, the expression evaluates to wrong. For example, p(case false of true : nil, 0) = wrong. \nWe impose two standard restrictions on patterns. Patterns must be linear (each variable in a pattern \noccurs exactly once). WMin a single case, patterns must be pairwise dk\u00adjoint (so that no two patterns \nmatch the same value). We prefer to use case instead of a conditional if because case is more general. \nThe usual conditional if el ea es is defined as case el of true : ez, false : ea. 3 Types This section \npresents the syntax and semantics of our type language. The syntax of type expressions is given by the \nfollowing grammar: .. .. u rival, . . . ,an.7whem {.. .,rl~T2, . ..} Unquantified types are written T, \nr,, T2, . . . . Types that may be quantified are written a, aI, U2, . . . . For semantics we adopt the \nidenl model, in which types are certain subsets (called ideals) of the semantic domain D [15]. In the \nideal model, every type r satisfies four condi\u00adtions: r is non-empty, wrong @ T, r is directed-closed \n(closed under limits), and r is downward-closed, which means that if y E T and if x < yl then x C T. \nSince types are sets, types are ordered by set inclusion. In the grammar above, a is a type variable. \nGiven an assignment p of types to type variables, Figure 1 extends p to give semantics to all type expressions. \nWe briefly ex\u00adplain each case. The first three type expressions are familiar from typed functional languages. \nThey are function types ~1 + ~2 (the set of functions mapping elements of rl to elements of m), constructor \ntypes c(71, ..., r~ ) (the set of c data structures with components drawn from n, ..., ~~ ), and type \nvariables. Note that we use the same name c for both a value constructor in expressions and a type construc\u00adtor \nin types. The type expressions rl U r2 and rl rl T2 denote set-theoretic union and intersection of types \nrespectively. The type O contains only 1, the value denoting non\u00adtermination. Since types are non-empty \nand downward\u00adclosed, O is the least type: O ~ T for any type r. The type 1 is the entire semantic domain \nexcept wrong. Note that 1 is the greatest type: ~ ~ 1 for any type r. The type 1 is handy for defining \nthe set of all values of a particular kind. For example, the set of all functions (that don t go wrong) \nis 1 + 1, the set of all cons pairs is cons(l, 1), and so on. The type cons(l, 1) is an example of a \nmonotype a type with no variables. For monotypes and types with no free (unquantified) variables, the \ntype denotes the same set regardless of the choice of substitution in Figure 1. In this case we drop \nthe substitution and treat the the type expression itself as a set. The type rl ?rz, read rl if rz , \nis a conditional type. Conditional types can express a restricted form of overload\u00ad ing, which is useful \nin giving accurate types for case expres\u00ad sions. For example, consider the function ~y.case ~ of true \n: zero, false : succ(zero) Figure 1: Semantics of type expressions. p(Tl + r~) {m(P(n) -{4 wm~9}) c \nP(T2)} u {L} p(c(Tl, . . . . %)) {C(tl, . . . ,tn)lt, e p(n) {1, wrong}} u {1} p(rl u 72) p(n) u p(T2) \nP(T1 n 72) P(71) n P(T2) P(Tl?m) { p(r~) {1} if p(r~) otherwise # {J-} p(o) {1] p(1) D -{wrong} p(val,..., \na..7 where S) n ~ (r) p cx where X = Sol(s) fl {p lp (@) = p(~) if@ @ {al, ..., a.}} A fragment of the \ntype inferred for this function by our algorithm is a + (zero?(a n true)) u (succ(zero)?(a n false)) \n Substituting true (resp. false) for a and simplifying,z this type has instance true + zero (resp. false \n+ succ(zero)). Thus, this type accurately captures the input-output depen\u00addencies of the case expression. \nThe ability to constrain the types of expressions using information about run-time tests is usually crdled \ncontrol-flow analysia and is very important in inferring accurate types for dynamically typed programs \n[20]. Conditional types were first introduced by Reynolds in his algorithm for analyzing Lisp programs \n119]. Type schemes have the form VcrI,..., a..~ where S where a~ is a type variable, r is an unquantified \ntype expression, and S is a set of t~pe constmints of the form T1 ~ rz. The un\u00adusual aspect of our type \nschemes is the use of subsidiary con\u00adstraints. Intuitively, the constraints are a form of bounded quantification \nrestricting T to instances satisfying the con\u00adstraints. More formally, the solutions Sol(S) of a system \nS of constraints is the set of assignments p of types to type variables such that p(~l ) ~ p(Tz) holds \nfor all constraints rl ~ TZ in S. In the case where Val, ....~m.r whew S is fully quantified (i.e., there \nare no free type variables in r), the meaning is the intersection nPe~Ol(~) p(r). For ex~ple, the type \nVa.a + a where @is just the type of the identity function. However, Vcu.a + a where {a ~ int} is the \ntype of the identity on the integers. For the function using case defined above, the full type inferred \nby our algorithm is Va. a + (zero?(a n true)) IJ (rjucc(~ero)?(~ n false)) where {a ~ true U false} \n The constraint on a says the function is guaranteed to be well-defined only if applied to the constructors \ntrueor false. Recursive types are not included in the grammar for type expressions because recursive \ntypes are definable using con\u00adstraints. Let ~1 = m stand for the pair of constraints rl ~ T2 and 7-2 \n~ ~1. Then a constraint such as a = cons(~, a) U nil atrue + (zero? (true n true)) U (succ(zero)?(true \nn false)) = true + (zero?true) U (succ(zero) ?0) n true + zero U Os true + zero has a unique solution \nwhich defines (given the usuaJ inter\u00ad pretation of cons aud nil) a to be all lists with elements of type \n~. We assume the set of type constructors includes con\u00adstants such as zero, true, false, and nil, a \nunary construc\u00adtor SUCC, and a binary constructor cons. We separate true and false (resp. nil and cons) \nfrom the more conventional type boo] (resp. list) in order to assign more precise types to expressions. \n4 Type Inference A type inference system for L is given in F@re 2. Asso\u00adciated with every conclusion \nis a set of assumptions A and a set of type constraints S. The rules prove sentences of the form A, S \n+ e : r , which should be read if the free variables of e have the types given by assumptions A, then \ne has type p(~) for any solution p of the constraints S2 That is, the conclusion holds only for solutions \nof S. We briefly exphain the inference rules in Figure 2. The rule [VAR] is standard: given the assumption \nx : u one can prove z : u. The rule [STRUCT] says that the type of a construction is the construction \nof the component types. For nullary constructors such as true, the scheme in [STRUCT] simplifies to an \naxiom A, S 1-true : true (recall that a constructor name is used both in values and types). In the [APP] \nruie, if r~ + 7A is a superset of the type for el and T3 is a superset of the type for ez, then rd is \na type for el ez. To simplify presentation of the [CASE] rule, we introduce two auxiliary functions for \npatterns. The set of all variables in pattern p is V(p). The type ~ is the set of W values that can match \nthe pattern p ~=1 zasp=$ C(pl , . . ..p*)=c(p Y....,%) For exampIe, the pattern cons(z, y) matches any \ncons pair. This fact is captured by cons(z, ~) = cons(l, 1). For an expression case e of pl : el,. ... \npn : en, the rule [CASE] takes as hypotheses a type T for e, a type ri for each e~, and a type T,f for \neach pi. The type Ti appears in the conclusion of the rule in the disjunct ri?(~ fl ~). Thus, T$ is included \nin the result type if there is something in -r mattilng pi. Otherwise, if T fl ~ = O then thk branch \nAu{x:u}, Stx:a A,Skei:Ti l<i<n A, Skc(el,..., e~):c(~l, ~~), ~~) Au{z:rl}, St-e:r2 A,SFAz.e:71+~z A, \nSkx:Tl, p:rz A,sl-xasp:Tln T2 A, Ske:r Au{x:~, [zc V(pl)}, Skel :n, PI :Ti A, S1-el:a AU{z:a}, Skez:r \nA,sl-letc=e1ine2:~ A. St-e: rand Sol(S) &#38;i?andal . . . ..a. not free inA A, Ot-e:Vcq ,.. ., ffrwhe?vsvs \nA, S1-e:Val,..., ~rwhemSm S A, S U S [r,/a,] t-e : r[r,/et,] Figure 2: Type inference rules. [VAR] [STRUCT] \n[ABS ) [APP] [AS 1 [LET] [GEN] [INST] {Z:a, y:l}, ot-z:a z:a}, Ol-Ag.c:l-+a Ax.Ay.x : a + 1 + a {y: \ntrue}, 0 + y : true {y: true}, 0 F zero : zero {y: true}, 0 k succ(zero) : succ(zero) {v : true}, 0 E \ncase y of true : zero, false : succ(zero) 1-Ay.case ~ of true : zero, false : succ(zero) : true 1-true \n: true 1-(Ay.case y of true : zero, false : succ(zero)) true : zero + zero : zero (a) The K combinator. \n(b) An example using case. Figure 3: Examples of well-typed expressions. of the case cannot be taken \nand r~?(r n fi) = T*?O = O. The constraint ~ c (J ~< ~<n T: serves two purposes. First, it ensures that \nthe set of-case branches is an exhaustive analysis of the type r. Second, thk constraint propagates type \ninformation about e to the types of the variables in the e;. Finally, the rules [LET], [GEN], and [INST] \nare stan\u00addard except for the use of subsidiary constraints in [GEN] and [INST]. The following lemma shows \nthat the inference system is sound. Lemma 4.1 (Soundness) Let A, S 1-e : C, let p be any solution of \nthe constraints S, and let E be an environment for the free variables of e such that if E(z) = O. and \nz : UC E A, then v= c p(u=). Then p(e, 1?) E p(u). Definition 4.2 A pmgmm is an L expression with no \nfree variables. Corollary 4.9 Let e be program and let 0,01-e : a. Then p(e, 0) Eu. The sentence 0,0 \ni-e : u is abbreviated 1-e : u. Recall that wrxmg # r for any type r (Section 3). The following corollary \nis immediate. Corollary 4.4 If 1-e : a then p(e, 0) # ummg. A program e is well-typed if t-e : u. Figure \n3a gives a proof that the K comblnator is well-typed. Figure 3b is a more complex example involving a \ncase expression. In Figure 3b, some types have been simplified. For in\u00adstance, by rule [CASE] the first \nconclusion is zero? (true fl true) U succ(zero)?(false n true), but thk is zero?true U succ(zero)?O = \nzero. In addition, we have elided the con\u00adstraints, which are all vacuously true in this proof. For in\u00adstance, \nby rule [CASE] there is a constraint true ~ true U false in the first conclusion, but thk always holds, \nand so has the same solutions as the empty set of constraints. The inference system in Figure 2 is a \nvery powerful static type system and may be of interest in its own right. In our application, the reaaon \nfor having such a powerful system is to make the set of well-typed programs aa large aa possible, because \nany program that is well-typed requires no run-time type checking. To make a convincing case for using \nthis type inference system with a dynamically typed language, there are still three thhgs that need to \nbe explained. In the rest of this section, we show that the set of well-typed programs is in fact very \nlarge. This shows that many programs pass the type checker without modification. In Section 5, we show \nthat the type constraints can be solved thk shows that it is decidable whether or not a program is well-typed. \nFinally, in Section 7 we explain how dynamic type checks can be inserted into ill-typed programs to make \nthem well-typed; thk guarantees that every program can be modified to paas the type checker. It is important \nto know the size of the set of well-typed programs. If the set is small, than the type inference system \nis not of much practical interest. We do not have an exact characterization of the set of well-typed \nprograms, but the following two lemmas show that the set is very large. Lemma 4.5 A lambda term is a \nprogram without construc\u00adtors or case. Every lambda term is well-typed. The Hindley/Milner type inference \nsystem [16] is used in most functional languages. We write I-HM e : cr if e haa type a in the Hindley/Mllner \nsystem. Lemma 4.6 shows that all programs well-typed in the Hhdley/Mllner system are also well-typed \nin our system. Lemma 4.6 If I-HM e : u then 1-e :0. 5 Computing Types In this section we discuss an algorithm \nthat derives types in the logic given in Figure 2. We do this in two steps: first, we prove that expressions \nhave minimal types in this logic. We then show that the minimal type is computable. Definition 5.1 A \nprogram e has minimal type u iff 1-e : u and for any u , if 1-e :u then u ~ CT . MlnimaJ types are related \nto, but not the same aa, princi\u00ad pal types [7]. Principal types are defined syntactically, with the \nusual definition being that u is a principal type for e if 1-e : u and if for any other type u such that \n1-e : u , the type u is a substitution instance of u. In the Hind\u00adley/Milner system the two notions coincide, \nso that a type is principal if and only if it is minimal in the ideal model. Thus, in this instance minimal \ntypes are simply the seman\u00ad tic counterpart of principrd types. In our system, programs have minimtd \ntypes but may not have principal types (e.g., because many type expressions denote the same type). A \nminimal type is the smallest derivable type in the se\u00admantic model. To prove that programs have minimal \ntypes, we int reduce the notion of a most general type derivation. let Y = ~u.(kr.(~to.(u (z z)) w) (Jw.(u \n(z z)) w) in let last = Y Af.Az.case x of cons(y, nil) : y cons(u, v as cons(a, ~)) : f v in let a = \nlast cons(zero, cons(zero, cons(zero, nil))) in last cons(true, cons(false, cons(a, nil))) Figure 4: \nTaking the last element of a list. Definition 5.2 A most geneml type derivation satisfies: 1. Every type \nassumption has the form a : a where a is a distinct type variable. 2. In every use of the [APP] rule \nthe types 73 and rd are fresh type variables. 3. In every let z = e in e , the [GEN] rule is applied \nonce to quantify as many type variables as possible in the type of e. 4. The [GEN] rule is applied in \nthe last step of the entire type derivation to quantify over all type variables. 5. The [INST] rule \nis applied immediately after uses of the rule [VAR] to eliminate any quantified variables. Fresh type \nvariables are substituted for the quantified variables. 6. The [GEN] and [INST] rules are applied nowhere \nelse  .. in the derivation.  It is easy to check that the restrictions in Definition 5.2 specify a \nunique derivation up to renaming of type variables. Lemma 5.3 shows that the minimal type of a program \nis the one generated by a most general derivation. Lemma 5.3 If t-e : a by a most general derivation, \nthen u is the minimal type for e. To obtain an effective type inference procedure, the only missing link \nis an algorithm to solve the type inclusion con\u00adstraints generated in a most general derivation. We make \nuse of the following theorem. Theorem S.4 It is decidable whether systems of proper constraints have \nsolutions. Furthermore, all solutions can be exhhited [3]. The constraints generated by a most general \nderivation are proper, with an extension to handle conditional types, which are not treated in [3]. A \ndefinition of proper con\u00adstraints and the extension for conditional types are given in Appendix B. Examples \n In this section we present two examples illustrating the power of our type inference system. The first \nexample is an imple\u00admentation of the Lisp car function, which returns the first component of a cons when \napplied to a cons, nil when applied to nil, and goes wrong for any other argument. let cm = Az.case z \nof cons(V, z) : y, nil : nil in cm cons(zero, nil) Our system reports that car cons(zero, nil) : zero. \nThe interesting aspect of this example is the type of car, which illustrates the role of conditional \ntypes in our system. The type inferred for car is v~,~,~. a -+ (~?(fy n cons(l, 1))) LJ (nil?(a n nil)) \nwhere {cY ~ cons(~, -f) U nil} If we let a = cons(zero, 1), /3 = zero, and ~ = 1, then thk type haa \ninstance cons(zero, 1) -+ zero. Thus, the application has type zero; thk is also the minimal type. In \naddition to handling the distinction between cons and nil accurately, the inferred type for car is completely \naccurate even for heterogeneous lists, since only the type of the head of the list is relevant. For example, \nour system also reports that car cons(zero, cons(true, nil)) : zero. A more substantial example is given \nin Figure 4, which defines a recursive function last that selects the last element of a list. Note that \na Y combinator is used for recursion be\u00adcause L has no special syntax for declaring recursive func\u00adtions. \nThe particular version of Y used here is one appro\u00adpriate for a strict language such as Z . The function \nlast is used polymorphically in two places: once with a list of zero and once with a heterogeneous list \nconsisting of some booleans followed by zero. This expression illustrates some of the difficulties of \nper\u00adforming automatic type inference in a dynamically typed language such as Lisp or Scheme. The program \ncontains no type declarations-everything must be inferred. The pro\u00adgram uses higher-order, recursive, \npolymorphic functions. The function last works for any heterogeneous list of at least length one; for \nany other input value, it goes wrong. Control-flow analysis is required to make distinctions be\u00adtween \nwhat last does with the last element of the list and all other elements of the list. The expression in \nFigure 4 means zero. Our algorithm infers that this expression has type zero. Therefore, it is well-typed \nand no run-time type checking is required. The key to proving this expression is well-typed is the type \nfor last. The minimal type of last computed by our system is: Va.X 4 a when {X = cons(l, X) U cons(a, \nnil)} In this example we have rewritten the type of last produced by our system to an equivalent, but \nmore readable, form. The type X is the set of all lists of length at least one ending in a, Because of \nthe polymorphic let, the two uses of last are typed separately using distinct instances of the type above. \nIn the first instance last haa type X + zero whew {X = cons(l, X ) U cons(zero, nil)} The result type \nzero is the type of the variable a. Since the second application of last is to a list with last element \na, thk instance is also assigned the type X + zero. The type of the application (and of the whole expression) \nis then zero. 7 Inserting Dynamic Type Checks Recall that an expression e is well-typed if 1-e : cr. \nIf e is ill-t yped then it cannot be said whether or not e makes a run-time type error. In a dynamically \ntyped language, it is not reasonable to reject ill-typed programs, because no type constraints are imposed \non programs. Instead, it is neces\u00adsary to add dynamic type checks to the program to make it well-typed. \nTo make an ill-typed program well-typed we insert functions celled narrowers [6] in certain places. A \nnarrower performs a run-time check and returns a value in\u00addicating whether or not the check succeeded. \nA narrower Checkx is defined for every monotype X. The meaning of Checkx is v ifv CX Checkx V = 1-otherwise \n{ Checkx is the identity on X and 1 elsewhere. Thus, Checkx ~~n~rows~l the set of possible values of \nthe input to X. The type of Checkx is Va.a + a n X. From Figure 2 and Definition 5.2 there are three \nkinds of constraints in a most general derivation: T~CY from [APP] (1) r c a + ~ from [APP] (2)uid from \n[CASE] (3) ~G In a most-general derivation, constraints of form (1) are sat\u00adisfied by the assignment \na = 1. Thus, only constraints (2) and (3) may be inconsistent and result in an ill-typing. Con\u00adstraint \n(2) guarantees that el is a function in an application el ea. Constraint (3) guarantees that all possible \nvalues of e are covered by the case analysis in case e of . . .. The following lemma shows that by inserting \nnarrowers in all applications and case expressions, any program can be made well-typed. In other words, \nthere is at least one way to add enough dynamic type checking to guarantee that a program makes no run-time \ntype errors. Lemma 7.1 Let e be a program and let e be e modified as follows: 1. Replace every application \nel ea by ( Checkl+l el) en. 2. Replace every case e of pl : el,. . . ,p~ : en by  case (Checkx e) ofpl \n:el, . . . , Pn : en where x = (JI<l<n m. Then e is well-typed. Lemma 7.1 gives one very conservative \nway of inserting dynamic checks. To minimize the run-time overhead of dy\u00adnamic type checking, our system \nattempts to minimize the number of checks inserted. This is done performing some ex\u00adtra bookkeeping in \nconstraint solving, so that it is possible to tell which constraints of forms (2) and (3) are violated \nin an ill-typed program. The following trivial example illustrates the idea. The type derivation for \nthe ill-typed application (true false) generates the constraints: The constraints are inconsistent since \ntrue ~ a + @ There\u00ad fore, a dynamic type check is required on the application. The expression is modified \nto (( Checkl+l true) false). It is easy to verify that the modified program is well-typed and that its \nminimal type is O, thereby proving that its meaning is 1. The rest of thk section describes how inconsistent \ncon\u00ad straints are detected by the implementation; this is the (ex\u00ad tra bookkeeping mentioned above. \nIn fact, checldng which constraints of forms (2) and (3) are violated is done by solv\u00ad ing an extended \nsystem of constraints. The extended con\u00ad straints always have at least one solution, so the constraint \nsolver is guaranteed to terminate successfully. By exam\u00ad ining the solutions of this extended system \nof constraints, it is possible to determine conservatively whkh constraints of forms (2) and (3) are \nviolated in the original constraint system. The definition of the extended constraints requires the \nintroduction of the type nX, the complement oft ype X. Definition 7.2 -IX is the largest type such that \n.xnx=o. The type -IX is unique, so YX is well-defined. Also, con\u00adstraints involving negated types can \nbe solved; for details see [3]. As an example, the type -I(1 + 1) is the type of all non-functions, which \nis UCCC c(1,..., 1) where C is the set of all data constructors. Consider the application constraint \ntrue C a + ~ from the example above. This constraint is inconsistent (has no solutions) because the right-hand \nside can contain at most the set of all functions. To modify thk constraint so that it always has a solution, \nan error term is added to the right\u00adhand side to include those values that result in run-time errors. \nThe modified constraint is true ~ (a+ ~) u (-fn -I(1 + 1)) This constraint is satisfiable, since if \na = /3 = ~ = 1 then the right-hand is 1. In addition, by setting the error variable 7 = O the error term \ndrops out and the ori@d constraint is recovered. The intuition is that the error term allows for all \n(and only) the values that may cause a run-time error. The error variable ~ makes it possible to distinguish \nbe\u00adtween solutions where there are no run-time errors (~ = O) and solutions that may admit run-time errors \n(~ # O). For example, the constraint above implies that true C ~. Thus, there is no solution where ~ \n= O, which implies that the original constraint is inconsistent. In general, the extended constraints \ninclude constraints of form (1) and modifications of forms (2) and (3): In each case the error term covers \nexactly those values that may cause a run-time error. These constraints are proper and can be solved \nusing the algorithm in [3]. In addition, these constraints always have a solution (e.g., let all vari\u00adables \nbe 1; then the right-hand side of every constraint is 1). Let S be an extended system of constraints \ngenerated from a program e. After solving the constraints, each error variable 7 is considered in turn. \nIf the system S U {7 = O} has a solution, then the constraint ~ = O is added to S. In this case, the \noriginal constraint (without the error term) Program Size vs Elapsed Time 30 0 25 0 0 20 0 0 0 00 0 0 \nSeconds15 0 0 % o 0 10 ~ 00 0 00 0 00 0 0 0 5  O. 083 0 0 0 o !9.0 o I I I I I 0 500 1000 1500 2000 \n2500 3000 Program Size Figure 5: Performance results. must be satisfiable, since setting ~ = O in a \nconstraint with Figure 5 are total elapsed times. an error term recovers the original constraint. It \nis easy to The implemented algorithm for solving systems of type show that in thk case, no run-time check \nis required for the constraints requires exponential time in the worst case. From program phraae of e \nassociated with the constraint. If there Figure 5, however, it is clear that in practice the complexity \nisnosolution ofth.esystem SU{7 = O}, then the appropriate does not grow rapidly for realistic-sized modules, \nand the run-time check is added to e and S is not changed, Note absolute speed is fast enough to be useful. \nMemory require\u00ad that the caae where all error variables are replaced by O ments are moderate, with a \nmaximum of 5MB allocated corresponds to a well-typed program. for any example in the test suite. We have \nnot measured the maximum amount of live data, but it is certainly much smaller than 5MB. 8 Implementation \nThk level of performance is not trivial to achieve-a naive implement ation is unusable even for small \nprograms. There are two implementations of the type inference algo\u00adrithm presented here: one for C and \none for FL [5], a dy- We have relied heavily on a number of simple optimization namically typed, higher-order \nfunctional language based on that improve the performance of the constraint solver by or-Backus FP [4]. \nThe implementation of type inference for ders of magnitude [1]. In the current implementation, only about \n107o of the time is spent solving constraints. TheL is small (about 100 lines of Lisp code). The implementa\u00adtion \nfor FL is considerably larger (about 1000 lines of Lisp majority of the time is spent simplifying the \nrepresentation code), reflecting the increased complexity of FL, in which it of type expressions and \ndetermining where dynamic type is necessary to deal with exceptions, side-effects from 1/0, checks should \nbe inserted. Further performance improve\u00adand a rich set of primitive functions. Both implementations \nments should be possible, as this portion of the system has are functional programs built on top of an \nimplement at ion not been tuned extensively. of a constraint solver for type inclusion constraints. The \nimplementation for FL haa been extensively tested 9 Related Work on a diverse suite of about 80 FL programs, \nwhich range from small utilities (e.g., sort) to modules of several hundred This section compares our \nwork with a wide variety of re\u00ad lines of code (e.g., a general 1/0 package). Figure 5 gives lated work. \nThe coverage of each proposal is necessarily performance results for the test suite. Program size in \nbrief. this figure is the number of nodes in the program s parse tree. Thk is a more relevant measure \nthan lines of code, 9.1 Type Inference Systems since the number of parse tree nodes corresponds to the \nnumber of steps in a type derivation. For the test suite A number of type inference systems for dynamically-typed \nprograms, the number of lines of code is anywhere between languages have been proposed, Gomard s system \nadds to a factor of 2 and 10 less than the number of parse tree nodes. the Hlndley/Mllner type system \nan undejined type [10] and For example, escher implements a simple graphics language; then uses a minor \nvariation of the HindIey/Milner type as\u00adescher has 278 lines of FL code, which translates into 2770 signment \nalgorithm. The type undefined represents a value parse tree nodes. All experiments were done on an unloaded \nfor which nothing is statically known, which is the same role IBM RS/6000 running Lucid Common Lisp. \nThe times in played by the type 1 in our system. Without other type op\u00aderators, the types aasignedby \nGomard s system are not very precise, and aa a result many dynamic type checks have to be added to make \nprograms well-typed. Another generalization of the Hlndley/Milner system is partial types. In partial \ntypes, the type 1 is added to the usual Hindley/Milner types, but there is a different type as\u00adsignment \nalgorithm [14]. Partial types havethe samelimited expressive power aa Gomard s system, and again the \ntypes assigned are not very precise. Closest in spirit to our own work is soj? typing [6]. In [6], Cartwright \nand Fagan set out criteria that they feel any typing algorithm for dynamically typed languages should \nmeet and provide an algorithm meeting those criteria. The typing algorithm generates type constraints \nthat must be solved. The constraints are not solved directly; they are first encoded in a special representation \nin which circular unifi\u00adcation is used to obtain representations of solutions, which are then decoded \nback to types. We feel our approach also meets the criteria in [6] for a soft typing system, while providing \na simpler formalkm and a more accurate type assignment algorithm. Our approach is simpler becausewe deal \ndirectly with the type constraints, without any intermediate encoding. With respect to accu\u00adracy, the \nalgorithm in [6] does include union types, function types, andparametric polymorphkm. It doesnot haveinter\u00adsection \ntypes or conditional types, and unions types are re\u00adstricted to be discm minative (disjuncts in unions \nmust have distinct outermost constructors). At the present time we do not know how our method compares \nto that of [6] in practice. In future work we hope to make some empiric~ measurements of the strengths \nand weaknessesof both systems. Outside the realm of dynamically typed languages, Fuh and Mishra [9] and \nMitchell [18] have given algorithms for type inference with subtypes. Neither approach incorpo\u00adrates \nunion types, parametric polymorphism, or conditional types. The subtyping algorithm of [17] haa limited \nunion and intersection types, but no function types. Also related to our work are the re$nement types \nof Freeman and Pfen\u00adning [8]. Refinement types include union and intersection types, but not conditional \ntypes. For each of the systems listed above, it is immediately apparent that our type language is more \nexpressive, and with the exceptions of [6, 8] it is relatively easy to prove formally that our rdgorithm \ninfers types that are at least as accurate. We believe such a proof is also possible with re\u00adspect to \nthe algorithm in [6], although a formal proof would be tedious to write down. Currently we do not know \nthe exact relationship of our system to that of [8]. The greatest qualitative jump in accuracy in our \nsystem comes from the ability to encode control-flow analysis of case expressions; using conditional \ntypes, we are able to constrain the type of the branch of a case to reflect the possible values that \ncould match the pattern. This ability is crucial to giving accurate types to many programs and it is \nnot found in any of the systems above. 9.2 Constraints over Regular Trees Several analysis techniques \nfor dynamically typed languages have been based on solving equations over sets of regular trees. Reynolds \nproposes a method for analyzing Lisp pro\u00ad grams [19]. This is the first and only other use of condl\u00adtional \ntypes of which we are aware. Jones and Mu&#38;nick propose a different analysis system baaed on solving \nequa\u00adtions over sets of regular trees [13], which is used not only to eliminate dynamic type checks but \nalso to reduce refer\u00adence counting. Recently Wang and Hilfinger have proposed an analysis method baaed \non tree grammars [23]. Since a grammar X::=x,l...lxn is, by a smrdl twist of perspective, a constraint \nX=xlu... uxn this algorithm also falls into the general class of constraint systems over regular trees. \nThe common weakness of these approaches is that they are inherently first-order (no function types) rmd \nmonomor\u00adphic. Furthermore, none of these techniques take advantage of control-flow information. One feature \nfound in [13, 19] is the use of projections c-1 (c(X, Y)) = X to model selector functions. We do not \nhave projections in our type language, but a selector can be assigned a function type c(X, Y) + X instead. \n 9.3 Abstract Interpretation We are aware of two systems based on abstract interpreta\u00adtion that have \nbeen implemented. Shivers proposes a type recovery system for Scheme [20]. The algorithm is a chia\u00adsical \nabstract interpretation with a number of features de\u00adsigned specifically for type inference in dynamically \ntyped programs. Most notably the system includes a mechanism for constraining the types of the branches \nof a conditional using information about the predicate. The system of Aiken and Murphy is a predecessor \nto the one presented here. In [2], an analysis algorithm based on abstract interpretation combined with \nconstraint solv\u00ading over sets of regular trees is described. Essentially, the abstract interpretation \nis used to generate constraints that must be solved. The constraint language has no function types, intersection \ntypes are restricted, and there are no conditional types. Like Shiver s algorithm, thk algorithm performs \nan ad hoc analysis to constrain the types of the branches of a conditional using the type of the predicate. \nBaaed on several years experience with the implement&#38; tion of [2], we have come to the conclusion \nthat there are two serious problems with techniques such aa these. First, in these works (as in most \nwork on abstract interpretation) there is an implicit assumption that the entire program is available \nfor compilation at once, which is unrealistic. Our type inference algorithm, on the other hand, is a \ncompo\u00adsitional, bottom-up algorithm. This makes it amenable to use in an environment that supports separately \ncompiled modules. Second, while it is easy to prove that these techniques are correct, it is dHEcult \nto prove anything useful about the quality of the information the algorithms compute. Thus, it is very \ndifficult for a programmer to predict for which programs the analysis performs well. Removing dynamic \ntype checks makes enough difference in performance that programmersneedachancetounderstand thetype inference \nsystem. Our algorithm is sufficiently declarative that we can give at leaat some guidance (Lemmas 4.5 \nand 4.6) to programmers who need to write efficient code. 9.4 Dynamic Typing Henglein proposes an algorithm \nfor dynamic typing that re\u00admoves both dynamic type checks and dynamic tags [12]. Since our algorithm \ndoes not deal with type tags (in fact, we assumeall values are tagged) our techniques do not subsume \nthose in [12]. On the other hand, our algorithm removes more dynamic type checks than the dynamic typing \nalgo\u00adrithm, which is monomorphic, has no intersection or union types, and performs no control-flow analysis. \nThus, the two algorithms are incomparable and to some extent aimed at different problems. References \n[1] AIKEN, A., AND MURPHY, B. Implementing regular tree expressions. In Proceedings of the 1991 Conference \non Functional Programming Languages and Computer Architecture (Aug. 1991), pp. 427-447. [2] AIKEN, A., \nAND MURPHY, B. Static type inference in a dynamically typed language. In Eighteenth Annual ACM Symposium \non Principles of Progmmming Lan\u00adguages (Jan. 1991), pp. 279-290. [3] AIKEN, A., AND WIMMERS, E. Type \ninclusion con\u00adstraints and type inference. In Proceedings of the 1993 Conference on Functional Progmmming \nLanguages and Computer Architecture (Copenhagen, Denmark, June 1993), pp. 31-41. [4] BACKUS, J. Can programming \nbe liberated from the von Neumann style? A functional style and its algebra of programs. Commun. ACM \n21, 8 (Aug. 1978), 613\u00ad 641. [5] BACKUS, J., WILLIAMS, J. H., WIMMERS, E. L., Lu-CAS, P., AND AIKEN, \nA. The FL language manual parts 1 and 2. Tech. Rep. RJ 7100 (67163), IBM, 1989. [6] CARTWRIGHT, R., A~~ \nFAGAN, M. Soft typing. In Pro\u00adceedings of the ACM SIGPLAN 91 Conference on Pro\u00adgmmming Language De8ign \nand Implementation (June 1991), pp. 278-292. [7] DAMAS, L., AND MILNER, R. Principle type-schemes for \nfunctional programs. In Ninth Annual ACM SVmpo\u00adsium on Principles of Progmmming Languages (1982), pp. \n207-212. [8] FREEMAN, T., AND PFENNING, F. Refinement types for ML. In Proceedings of the ACM SIGPLAN \n91 Confer\u00adence on Progmmming Language Design and Implemen\u00adtation (June 1991), ACM Press, pp. 268-277. \n[9] FUH, Y., AND MISHRA, P. Type inference with sub\u00adtypes. In Proceedings of the 1988 Eurvpean Symposium \non Pmgmmming (1988), pp. 94 114. [10] GOMARD, C. Partial type inference for untyped func\u00adtional programs \n(extended abstract), In Proceedings of the 1990 ACM Confenmce on Lisp and Functional Pro\u00adgmmming (1990), \npp. 282-287. [11] HEINTZE, N. Set Based Progmm Anal~sis. PhD thesis, Carnegie Mellon University, 1992. \n[12] HENGLEIN, F. Dynamic typing. In Proceedings of the 1992 ACM Conference on Lisp and Functional Pm\u00adgmmming \n(July 1992), pp. 205 215. [13] JONES, N. D., AND MUCHNICK, S. S. Flow analysis and optimization of LISP-like \nstructures. In Sixth An\u00adnual ACM Symposium on Principles of Pmgmmming Languages (Jan. 1979), pp. 244-256. \n[14] KOZEN, D., PALSBERG,J., AND SCHWARTZBACH,M. I. Efficient inference of partial types. In Foundations \nof Computer Science (Oct. 1992), pp. 363-371. [15] MACQUEEN, D., PLOTKIN, G., AND SETHI, R. An ideal \nmodel for recursive polymorphic types. In Eleventh Annual ACM Symposium on Principles of Progmm\u00adming \nLanguages (Jan. 1984), pp. 165-174. [16] MILNER, R. A theory of type polymorphism in pro\u00adgramming. J. \nComput. Syst. Sci. 17 (1978), 348-375. [17] MISHRA, P., AND REDDY, U. Declaration-free type checking. \nIn Proceedings of the Twelfth Annual ACM Symposium on the Principles of Pmgmmming Lan\u00adguages (1985), \npp. 7-21. [18] MITCHELL, J. Coercion and type inference (summary). In Eleventh Annual ACM Symposium on \nPrinciples of Progmmming Languages (Jan. 1984), pp. 175-185. [19] REYNOLDS, J. C. Automatic Computation \nof Data Set Definitions. Information Processing 68. North-Holland, 1969, pp. 456-461. [20] SHIVERS, O. \nControl flow analysis in scheme. In Pro\u00adceedingsof the ACM SIGPLAN 388 Conference on Pro\u00adgmmming Language \nDesign and Implementation (June 1988), pp. 164-174. [21] THATTE, S. Type inference with partial types. \nIn Automata, Languages and Progmmming: 15th Interna\u00adtional Colloquium (July 1988), Springer-Verlag Lecture \nNotes in Computer Science,,vol. 317, pp. 615-629. [22] TOFTE, M. Opemtional Semantics and Polymorphic \nType Inference. PhD thesis, University of Edinburgh, 1987. [23] WANG, E., ANDHILFINGER,P. N. Analysis \nof recursive types in Lisp-like languages. In Proceedings of the 1992 ACM Conference on Lisp and Functional \nPmgmmming (June 1992), pp. 216-225. A Proofs Proof: [of Lemma 4.1] This is an eaay induction on the structure \nof the proof that A, S 1-e : u. 0 Proofi [of Lemma 4.5] For lambda terms, type constraint? are introduced \nonly by the [APP] rule. From Figure 2, the constraints are T2 g 7-3 T1 g T3+T4 Let T3 = T4 = x where \nx is the solution of the equation z = z -i Z. Let e be a lambda term. By induction on the structure \nof the type derivation for e it is easy to check that this assignment satisfies the constraints for e. \nTherefore e is well-typed. R Proofi [of Lemma 4.6] The usual Hindley/Milner rules can be expressed in \nour notation. Except for the [APP] and [CASE] rules, the rules in Figure 2 either are Hindley/Milner \nrules ([VAR], [STRUCT], [ABS], and [LET]) or differ only superficially ([GEN] and [INST] ). The Hlndley/Milner \n[APP] rule is: A,S~el: rl,ez:Tz The Hindley/Milner [CASE] rule is: Since equality is a special caseof \ncontainment, it is clear that the Hlndley/Milner [APP] rule is a speciaJcaseof the rule in Figure 2. \nSimilarly, the constraints in the Hlndley/Milner [CASE] rule are a restriction of the constraints in \nthe [CASE] rule in Figure 2. Therefore, if EHM e : T then 1-e : T. 0 Proof: [of Lemma 5.3] The type a \ngenerated by a most general derivation has the form Vcrl,, ... crn.7 where S. It is easy to show that \nfor any other typing 1-e : a , a is equal to Val,... , an.~ where S such that Sol (S ) ~ Sol(S). Since \nthe mewing of the quantified types is the intersection of the meaning of T in all solutions of the constraints \n(Section 3), it follows that a ~ a . 0 Proof: [of Lemma 7.1] Let p(a) = 1 for all variables a. To show \nthat e is well-typed, it suffices to show that p is a solution for the type constraints in a most general \nderivation for e . Consider uses of rule [APP]. Every application in e has the form ( Checkl+l el) ez. \nFor constraints of form (l), we have p(~) ~ 1 = p(a). Now consider constraints of form (2). If el : r, \nthen the type of Checkl+l guarantees that the type of Checkl+l el haa the form -r (l (1 -+ 1), It follows \nthat p(~n(l+l))gl+l=p(~+~) so the type constraint is satisfied. Consider usesof rule [CASE]. In e , \nevery case expression has the form case Checkx (e) of pl : el,..., p~ : en where x = Fi.If e : ~, then \nthe type of Chwkx (e) is  Ul<ian ~.It isalsoeasytocheckthat inamostgeneral ~flul<a<n P$ deriva~o~, if \np, : ~~ then p(r~) = ~. Now we have that p(rn (J z)g (J pi=p( u T;) l<i<tl I<a<n I<i<n and so the type \nconstraint is satisfied, l B Proper Constraints We briefly explain the main result of [3] and then present \nthe extension to condltionrd types. A proper t~pe ezpredon is either an R (for right) or an L (for left) \ntype as defined by the following grammar: L ::= OIIICIIC(L1,..., Ln)lR-+LILlnLzlLIULz R ::= Otlla[c(Rl,... \n,Rn)lL+RIRlnRzlRIURz The following restrictions are placed on R and L types. In an L type L1 nL~, Lz \nmust be both a monotype and upwanf\u00adclosed. A type X is upward-closed if X = {yl~ > x c X {1 }}. In an \nR type RIuRz, it must be the casethat R~nRz = O in all assignments of the variables in R1 and Rz. The \ndefinitions of L and R types rule out certain forms of constraints for which it is not known how to perform \ncon\u00adstraint resolution. The problematic casesare an intersection on the left (i.e., L1 fl Lz Q R1 ) and \na union on the right (i.e., L1 ~ R1 U R2). Thus, L types restrict intersections (on the lefi-hand sides \nof constraints) and R types restrict unions (on the right-hand sides of constraints). A proper system \nof constraints haa the form {Li ~ &#38;}. In [3], an algorithm is given for solving proper systems of \nconstraints. To extend the result of [3] to include condltionrd types, we extend the definition of L \ntypes: L::= OIIIQIC(L1,..., L.) IR-+ LlLlflL2, [LIuLz [L1?Lz To show that proper systems with conditional \ntypes can be solved, it is sufficient to show how to decompose a constraint of the form L1?Lz ~ R into \nsmaller constraints while pre\u00adserving the set of solutions. From the semantics of ?-types it follows \nthat where -means that the two sides have the same set of solutions and V means the union of the solutions \nof the two systems. The new constraints L1 C R and La C O both have the correct form L ~ R. Finally, \nthe constraints generated in a most general derivation (Definition 5.2) are proper under the extended \ndefinition. \n\t\t\t", "proc_id": "174675", "abstract": "<p>We present a simple and powerful type inference method for dynamically typed languages where no type information is supplied by the user. Type inference is reduced to the problem of solvability of a system of type inclusion constraints over a type language that includes function types, constructor types, union, intersection, and recursive types, and conditional types. Conditional types enable us to analyze control flow using type inference, thus facilitating computation of accurate types. We demonstrate the power and practicality of the method with examples and performance results from an implementation.</p>", "authors": [{"name": "Alexander Aiken", "author_profile_id": "81100399954", "affiliation": "Computer Science Division, University of California at Berkeley, Berkeley, CA and IBM Almaden Research Center 650 Harry Rd., San Jose, CA", "person_id": "P13911", "email_address": "", "orcid_id": ""}, {"name": "Edward L. Wimmers", "author_profile_id": "81341498495", "affiliation": "IBM Almaden Research Center, 650 Harry Rd., San Jose, CA", "person_id": "PP42049420", "email_address": "", "orcid_id": ""}, {"name": "T. K. Lakshman", "author_profile_id": "81100430360", "affiliation": "Department of Computer Science, University of Illinois at Urbana-Champaign, 1304 W. Springfield Ave, Urbana, IL", "person_id": "P275207", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/174675.177847", "year": "1994", "article_id": "177847", "conference": "POPL", "title": "Soft typing with conditional types", "url": "http://dl.acm.org/citation.cfm?id=177847"}