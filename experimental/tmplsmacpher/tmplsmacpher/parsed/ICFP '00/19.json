{"article_publication_date": "09-01-2000", "fulltext": "\n Syntactic Accidents in Program Analysis: On the Impact of the CPS Transformation * Daniel Damian and \nOlivier Danvy  BRICS Department of Computer Science University of Aarhus Ny Munkegade, Building 540, \nDK-8000 Aarhus C, Denmark {damian,danvy}@brics.dk ABSTRACT We show that a non-duplicating CPS transformation \nhas no e.ect on control-.ow analysis and that it has a positive e.ect on binding-time analysis: a monovariant \ncontrol-.ow analy\u00adsis yields equivalent results on a direct-style program and on its CPS counterpart, \nand a monovariant binding-time anal\u00adysis yields more precise results on a CPS program than on its direct-style \ncounterpart. Our proof technique amounts to constructing the continuation-passing style (CPS) coun\u00adterpart \nof .ow information and of binding times. Our results con.rm a folklore theorem about binding-time analysis, \nnamely that CPS has a positive e.ect on binding times. What may be more surprising is that this bene.t \nholds even if contexts or continuations are not duplicated. The present study is symptomatic of an unsettling \nprop\u00aderty of program analyses: their quality is unpredictably vul\u00adnerable to syntactic accidents in source \nprograms, i.e., to the way these programs are written. More reliable program analyses require a better \nunderstanding of the e.ect of syn\u00adtactic change.  Categories and Subject Descriptors D.1.1 [Software]: \nProgramming Techniques applicative (func\u00adtional) programming; D.3.2 [Programming Languages]: Language \nClassi.cations applicative (functional) languages; D.3.3 [Programming Languages]: Language Constructs \nand Features procedures, functions, and subroutines; D.3.4 [Programming Languages]: Processors Optimization; \nF.3.1 [Logics and Meanings of Programs]: Specifying and Verifying and Reasoning about Programs Speci.cation \n* Extended version to appear as the technical report BRICS-RS-00-15 in the BRICS Research Series. Basic \nResearch in Computer Science (www.brics.dk), Centre of the Danish National Research Foundation. Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. ICFP 00, Montr\u00b4 \neal, Canada. Copyright 2000 ACM 1-58113-202-6/00/0009 ..$5.00 techniques; F.3.2 [Logics and Meanings \nof Programs]: Semantics of Programming Languages partial evaluation, program analysis; F.4.1 [Mathematical \nLogic and For\u00admal Languages]: Mathematical Logic Lambda calculus and related systems.  General Terms \nBinding-time analysis (BTA), constraints, continuation-pass\u00ading style (CPS), CPS transformation, control-.ow \nanalysis (CFA), direct style, monovariance, o.ine partial evaluation, security analysis.  Keywords CPS \ntransformation of control-.ow information, CPS trans\u00adformation of binding-time information, binding-time \nimprove\u00adments, continuation-based partial evaluation. 1. INTRODUCTION 1.1 Motivation Program analyses \nare vulnerable to syntactic accidents in source programs in that innocent-looking, meaning-preserv\u00ading \ntransformations may substantially alter the precision of an analysis. For a simple example, binding-time \nanalysis (BTA) is vul\u00adnerable to re-association: given two static expressions s1 and s2 and one dynamic \nexpression d, it makes a di.erence whether the source program is expressed as (s1 + s2)+ d or as s1 +(s2 \n+ d). In the former case, the inner addition is classi.ed as static and the outer one is classi.ed as \ndynamic. In the latter case, both additions are classi.ed as dynamic. With the exception of BTA, little \nis known about the e.ect of programming style on program analyses. BTA is an ex\u00adception because its output \ncritically determines the amount of specialization carried out by an o.ine partial evaluator [5, 16]. \nTherefore, the output of binding-time analyses has been intensively studied, especially in connection \nwith syntactic changes in their input. As a result, binding-time improve\u00adments have been developed to \nmilk out extra precision from binding-time analyses [16, Chapter 12], to the point that partial-evaluation \nusers are encouraged to write pro\u00adgrams in a very speci.c style [15]. That said, binding-time\u00adimprovements \nare not speci.c to o.ine partial evaluation they are also routine in staging transformations [17] and \nin the formal speci.cation of programming languages for se\u00admantics-directed compiling [22, Section 8.2]. \nSince one of the most e.ective binding-time improvements is the transformation of source programs into \ncontinuation\u00adpassing style (CPS) [3, 32], people have wondered whether CPS may help program analysis \nin general. Nielson s early work on data-.ow analysis [21] suggests so, since it shows that for a non-distributive \nanalysis, a continuation seman\u00adtics yields more precise results than a direct semantics. The CPS transformation \nis therefore a Good Thing, since for a direct semantics, it gives the e.ect of a continuation seman\u00adtics. \nIn the early 90s, Muylaert-Filho and Burn s work [20] was providing further indication of the value of \nthe CPS transformation for abstract interpretation when Sabry and Felleisen entered the scene. In their \nstunning article Is continuation-passing useful for data-.ow analysis? [31], Sabry and Felleisen showed \nthat for constant propagation, analyzing a direct-style pro\u00adgram and analyzing its CPS counterpart yields \nincompara\u00adble results. They showed that CPS might increase precision by duplicating continuations, and \nalso that CPS might de\u00adcrease precision by confusing return points. These results are essentially con.rmed \nby Palsberg and Wand s recent CPS transformation of .ow information [29]. At any rate, except for continuation-based \npartial evaluation [10], there seems to have been no further work about the e.ect of CPS on the precision \nof program analysis in general. The situation is therefore that the CPS transformation is known to have \nan unpredictable e.ect on data-.ow analysis and is also believed to have a positive e.ect on binding-time \nanalysis. However, we do not know for sure whether this positive e.ect is truly positive, or whether \nit makes binding times worse elsewhere in the source program. One may also wonder whether there exist \nprogram analyses on which CPS has no e.ect. In this article, we answer these two questions by studying \nthe e.ect of a non-duplicating CPS transformation on two o.-the-shelf constraint-based program analyses \ncontrol-.ow analysis (0-CFA) and BTA. Using a uniform proof technique, we formally show that: (1) CPS \nhas no e.ect on 0-CFA, i.e., analyzing a direct\u00adstyle program and analyzing its CPS counterpart yields \nequivalent results. (2) CPS does not make BTA yield less precise results, and for the class of examples \nfor which continuation-based partial evaluation was developed, it makes BTA yield results that are strictly \nmore precise. (3) CPS has no e.ect on an enhanced BTA which takes into account continuation-based partial \nevaluation.  This increased precision entailed by CPS also concerns analyses that have been noticed \nto be structurally similar to BTA, such as security analysis, program slicing, and call tracking [1]. \nThese analyses display a similar symptom: for example, we are told that in practice, users tend to .nd \nsecurity analyses too conservative, without quite knowing what to do to obtain more precise results. \n(Here, more precise results means that more parts of the source program should be classi.ed as low security.) \nIn the next section, we point out how the dependency induced by let-expressions leads to a loss of precision. \n 1.2 A loophole: the let rule A binding-time analysis classi.es a let expression to be dy\u00adnamic if its \nheader is dynamic, regardless of the binding time of its body. (Similarly, if a let header is classi.ed \nto be of high security, the whole let expression is also classi.ed to be of high security, regardless \nof the security level of its body.) The body of the following .-abstraction is thus classi.ed as dynamic \nif e is dynamic: .x.let v = e in b The CPS counterpart of this .-abstraction reads as follows: .x..k.e. \n(.v.b. k) where e . and b. are the CPS counterparts of e and b,respec\u00adtively. Now assume that b naturally \nyields a static result but is coerced to be dynamic because of the let rule. In the CPS term, e . also \nyields a dynamic result, i.e., intuitively, v is classi.ed to be dynamic.1 Intuitively, b. also yields \na static result and passes it to its continuation k. Therefore, in direct style, b yields a dynamic result \nwhereas in CPS, it yields a static result. Two observations need to be made at this point: (1) The paragraph \nabove is the standard motivation for im\u00adproving binding times by CPS transformation [3] (see Section \n5.2 for further detail). However, what this para\u00adgraph leaves unsaid, and what actually has always been \nleft unsaid, is whether this local binding-time improve\u00adment corresponds to a global improvement as well, \nor whether it may make things worse elsewhere in the source program. (In Section 4, we prove that this \nlocal im\u00adprovement actually is a global improvement as well.) (2) In their core calculus of dependency \n[1], Abadi et al. make a point that any function classi.ed as d . s (resp. h . l, etc.) is necessarily \na constant function. However, as argued above, given a direct-style function classi.ed to be d . d because \nof the let rule, its CPS counterpart may very well be classi.ed as d . (s . o) . o and not be a constant \nfunction in continuation-passing style (i.e., a function applying its continuation to a constant).  \nTogether, these two observations tell us that the let rule is overly conservative in BTA, security analysis, \netc. CPS makes it possible to exploit the untapped precision of this rule non-trivially by providing \na local improvement which is also a global improvement. Before moving on to the rest of this article, \nlet us brie.y get back to Sabry and Felleisen s observation that any im\u00adprovement in precision provided \nby CPS is solely due to continuation duplication [31]. True as this observation may be for data-.ow analysis, \nwe have just shown that it does not necessarily hold for other analyses such as BTA. Let us also point \nout that the CPS transformation leads to binding-time improvements for conditional expressions. Indeed, \nthe case rule makes conditional branches dynamic if the test is dynamic. This approximation can be circum\u00advented \nwith a CPS transformation. The improvement, how\u00adever, is not produced by the duplication of the analysis, \nbut merely by the context relocation induced by the CPS trans\u00adformation. This point is developed further \nin Section 4.4. 1 This intuition is formalized in the rest of this article.  (CGcf ,.Gcf ) Fp n \u00a3 .. \ntrue cf (CGcf ,.Gcf ) Fp cf x \u00a3 .. .Gcf (x) . CGcf (f) (CGcf ,.Gcf ) Fp (.p x.e \u00a31 )\u00a3 .. { p}. CGcf \n(f) . (CGcf ,.Gcf ) Fp e \u00a31 cf cf (CGcf ,.Gcf ) Fp (let x = t\u00a3 in e \u00a31 )\u00a32 .. (CGcf ,.Gcf ) Fp t\u00a3 . (CGcf \n,.Gcf ) Fp e \u00a31 . CGcf (f) . .Gcf (x) . CGcf (f1) . CGcf (f2) cf cfcf \u00a30 \u00a31 \u00a30 \u00a31 (CGcf ,.Gcf ) Fcf p \n(let x = t0 t1 in e \u00a32 )\u00a33 .. (CGcf ,.Gcf ) Fp cf t0 . (CGcf ,.Gcf ) Fp cf t1 . (CGcf ,.Gcf ) Fcf p e \n\u00a32 . CGcf (f2) . CGcf (f3) . . (.p y.e \u00a3 1) . CGcf (f0).(CGcf (f1) . .Gcf (y) . CGcf (f) . .Gcf (x)) \n(CGcf ,.Gcf ) Fp (let x = op(t\u00a3 ) in e \u00a31 )\u00a32 .. (CGcf ,.Gcf ) Fp t\u00a3 . (CGcf ,.Gcf ) Fp e \u00a31 . CGcf (f1) \n. CGcf (f2) cf cfcf \u00a3\u00a30 \u00a31 \u00a30 \u00a31 (CGcf ,.Gcf ) Fcf p (let x = if0 te0 e .. (CGcf ,.Gcf ) Fp cf t\u00a3 . (CGcf \n,.Gcf ) Fp e0 . (CGcf ,.Gcf ) Fp e1 . (CGcf ,.Gcf ) Fp e \u00a32 . 1 cf cf cf \u00a32 )\u00a33 GGG in eCcf (f0) . .Gcf \n(x) . CGcf (f1) . .Gcf (x) . Ccf (f2) . Ccf (f3) Figure 5: Syntax-directed 0-CFA Lamp The set of .-abstraction \nlabels in p Var p The set of identi.ers in p Labp The set of term labels in p Val p = P (Lamp) Abstract \nvalues cf G Ccf . Cachep = Labp . Val p Abstract cache cf cf .Gcf . Env p = Var p . Val p Abstract environment \ncf cf Fp . (Cachep \u00d7 Envp ) \u00d7 Labp cf cfcf Figure 4: 0-CFA relation for a program p analysis is speci.ed \nas a relation Fcf on caches, environ\u00adments and terms. Given a term e,(CGcf ,.Gcf ) Fcf e means that (CGcf \n,.Gcf ) is a result of the control-.ow analysis of e. 4 In this work we use the syntax-directed variant \nof the analysis [24, Chapter 3], and we restrict its analysis relation toarelation Fp associated to each \nprogram being analyzed. cf Given a closed direct-style program p, the functionality of the associated \nrelation Fp is de.ned in Figure 4. The anal\u00ad cf ysis relation is de.ned in Figure 5 by induction over \nthe syntax of the program. Any solution ( G.cf ) accepted by the relation Fp (i.e., Ccf , Gcf such that \n(CGcf ,.Gcf ) Fp p holds) is a conservative approxi\u00ad cf mation of the exact .ow information [24, Chapter \n3]. Fur\u00adthermore, the analysis relation Fp has a model-intersection cf property, i.e., the set of solutions \naccepted by Fp is closed cf under intersection. The model-intersection property ensures the existence \nof a least solution of the analysis, i.e., a most precise one. (Here the order relation is given by the \npoint\u00adwise ordering of functions induced by set inclusion.) In prac\u00adtice, a work-list based algorithm \ncomputes the least solution. The rest of this section is organized as follows. First, we show how to \nCPS-transform control-.ow information (Sec\u00adtion 3.1). Given a direct-style program p and an arbitrary \nsolution of its associated analysis (CGcf ,.Gcf ), we construct asolution ( Gcf , Gcf ) of the analysis \nassociated to the CPS C. .. counterpart of the program, p . We then ensure that the construction, C p \n, provides a valid solution (Section 3.2). We cf present a converse transformation, D p (Section 3.3), \nwhich cf we also prove to be correct (Section 3.4). Graphically: 4In the notation of Nielson, Nielson, \nand Hankin [24], Fcf is simply F. C p cf . ( G(CG. .. Ccf ,.Gcf ) cf , Gcf ) D p cf 0-CFA0-CFA . p \np ]]Pgm [[\u00b7 The speci.cation of the analysis puts us in an ideal position to compare absolute precisions \n(Section 3.5). We show that the least solution of the analysis of an arbitrary program is transformed \ninto the least solution of the analysis of the CPS counterpart of this program and vice versa. This leads \nus to conclude that the CPS transformation has no in.uence on the .ow information obtained by 0-CFA. \n 3.1 CPS transformation of control .ow Given a solution (CGcf ,.Gcf ) of the analysis of a program p \n(i.e., a cache-environment pair such that (CGcf ,.Gcf ) Fp p holds), cf we now construct in linear time \na solution ( Gcf , Gcf )of the C. .. analysis of p . =[[p]]Pgm , the CPS counterpart of p (i.e., such \nC. ... that ( Gcf , Gcf ) Fp;p holds). By analogy, we refer to the cf construction of (CG. .cf . )out \nof (CGcf ,.Gcf ) as the CPS trans\u00ad cf , Gformation of (CGcf ,.Gcf )into ( Gcf , Gcf ). C. .. As mentioned \nin Section 2, we have designed the CPS transformation on labeled terms so that it preserves the labels \nof each trivial term. In addition, each direct-style .-abstraction is annotated with the same label as \nits CPS counterpart. As a consequence, the abstract values in di\u00adrect style are included into the abstract \nvalues in CPS, i.e., ;; Lamp . Lampand Val p . ValpThe CPS transformation cf cf . preserves all the variables \nde.ned in the original direct-style program. Therefore Var p . Var p; . In essence, we construct a solution \nfor the CPS program such that the .ow informa\u00adtion assigned to the variables and to the trivial terms \npre\u00adserved by the transformation is identical to the information found in the direct-style solution. \nWe also assign .ow information to the newly introduced terms and variables, in particular to continuation \nabstrac\u00adtions and continuation identi.ers. To this end, we use two auxiliary functions . and .. \u00a3 ]]Pgm \n\u00a3 ]]Expk)\u00a30 [[e =(.pk.[[e CG. .G. cf (f0)= {p} cf (k)= \u00d8 \u00a3 ]]Triv \u00a3 CG. [[n = n cf (f)= CGcf (f) \u00a3 \n]]Triv \u00a3 CG. [[x = x cf (f)= CGcf (f) ]]Triv \u00a30 ]]Exp [[(.p x.e \u00a30 )\u00a3 =(.p x.(.p1 k.[[ek)\u00a32 )\u00a3 CG. CG. \ncf (f)= CGcf (f) cf (f2)= {p1}Gcf (x)= .Gcf (x) Gcf (k)= .(k) .. .. \u00a3 ]]Exp \u00a3 ]]Triv [[tk =(let x = \nk\u00a30 [[tin x \u00a31 )\u00a32 CG. .. cf (f0)= Gcf (k) CG. C. cf (f2)= Gcf (f1)= .Gcf (x)= \u00d8 \u00a30 \u00a3 )\u00a31 ]]Exp \u00a30 ]]Triv \n\u00a3 ]]Exp k)\u00a32 CG. .. [[(let x = t0 in ek =(let x =[[t0 in [[e cf (f2)= \u00d8 Gcf (x)= .Gcf (x) \u00a30 ]]Triv \u00a31 \n]]Triv CG. .. (let x0 =[[t0 [[t1 in cf (f3)= Gcf (x0)= .(CGcf (f0)) \u00a30 \u00a31 \u00a3 )\u00a32 ]]Exp\u00a33 \u00a3 ]]Exp [[(let \nx = tt in ek =(let x1 = x (.p x.[[ek)\u00a34 CG. .G. 01 0cf (f4)= {p} cf (x)= .Gcf (x) in x1 cf (f7)= Gcf \n(f6)= Gcf (f5)= Gcf (x1)= \u00d8 \u00a35 )\u00a36 )\u00a37 CG. C. C. .. \u00a3\u00a30 )\u00a31 ]]Exp \u00a3 ]]Triv \u00a30 ]]Exp [[(let x = op(t) \nin ek =(let x = op([[t) in [[ek)\u00a32 .G. CG. cf (x)= .Gcf (x) cf (f2)= \u00d8 \u00a32 ]]Exp (let k1 =(.p x.[[ek)\u00a34 \nin \u00a3\u00a30 \u00a31 \u00a3 ]]Triv \u00a30 ]]Expk1) (let x = if0 te e Exp (let x1 = if0 [[t([[e.G. C. .G. 01 k = \u00a30 1 ]]Expk1) \ncf (k1)= Gcf (f4)= {p} cf (x)= .Gcf (x) \u00a32 )\u00a33 CG. C. C. .. in e ([[e1 cf (f7)= Gcf (f6)= Gcf (f5)= Gcf \n(x1)= \u00d8 \u00a35 )\u00a36 )\u00a37 in x 1 Figure 6: Flow transformation from direct style to CPS . extracts the labels \nof partially applied CPS .-abstrac\u00ad tions. Formally, considering A to be a set of CPS .-ab\u00adstractions \n{.pi xi..p1 i ki.ei|1 = i = n},for some n,then .(A)= {p1i |1 = i = n}.  . assigns .ow information to \neach continuation iden\u00adti.er k introduced by the CPS transformation of a .\u00adabstraction from p. This information \ncan be obtained from the direct-style .ow information, since we can syn\u00adtactically identify the continuation \nof the CPS counter\u00adpart of any direct-style application.  G Given p, Ccf , .Gcf , and a continuation \nidenti.er k intro\u00ad duced by the transformation of a .-abstraction from p: x.e]]Triv [[.p1 = .x..k.[[e]]Exp \nk we gather in .(k) all the continuations that are passed at the program points where .p1 x.e can be \napplied. For\u00admally, .(k) is de.ned as the set of all labels p such that in the CPS transformation of \np into p there exists a transformation step \u00a30 ]]Triv [[t1]]Triv Exp let x0 =[[t let x = t0 \u00a30 t1 0 .p \nx.[[e]]Exp k1 = in let x1 = x0 k1in e in x1 such that p1 . CGcf (f0). Using . and ., we de.ne ( CG. .cf \n. ) inductively, following cf , G Figure 6. In the right part, for each CPS-transformation C. .. step, \nwe assign .ow values into Gcf and Gcf using previously de.ned values. In fact, the construction of .ow \ninformation de.nes a func\u00adtion Cp cf :(Cachep \u00d7 Envp ) . (Cachep; \u00d7 Envp; ). cfcf cfcf It is easy to \nshow that Cp is monotone. cf  3.2 Correctness of the transformation Let us show that the cache-environment \npair constructed by Cp is indeed a valid solution of the analysis of the CPS cf counterpart of p. Theorem \n1. Given a direct-style program p and its CPS counterpart p . =[[p]]Pgm ,let (CGcf ,.Gcf ) be a solution \nof the 0-CFA of p (i.e., such that (CGcf ,.Gcf ) Fp cf p holds) and let (CG. .. ( G.cf ).Then (CG. .. \n; p . holds. cf , Gcf )= Cp Ccf , Gcf ) Fp cf cf , Gcf Under the assumptions of the theorem, we start \nby observ\u00ading three immediate properties of the .ow transformation: .. Lemma 1. For all variables x in \np, Gcf (x)= .Gcf (x); for all CG. CG. trivial terms t\u00a3 in p, cf (f)= CGcf (f); and for all expressions \ne \u00a3 in p . , cf (f)= \u00d8. For an arbitrary expression, we de.ne the notion of return label to capture the \nreturn point from which 0-CFA collects .ow information, as shown just below in Lemma 2. De.nition 1. \nGiven a labeled expression e \u00a3 . Exp,we de.ne the return label R[[e \u00a3 ]] of e \u00a3 by structural induction \nas follows: R[[t\u00a3 ]] = f \u00a31 )\u00a3 R[[(let x = s in e ]] = R[[e \u00a31 ]] Lemma 2. Let e \u00a3 be an arbitrary subexpression \nof p.Then CGcf (R[[e \u00a3 ]]) . CGcf (f). A return label identi.es the point where a continuation is called \nin the CPS-transformed program. Return labels thus provide a syntactic connection between the points \nwhere \u00a3 ]]Pgm \u00a3 ]]Exp [[e =(.pk.[[ek)\u00a30 \u00a3 ]]Triv \u00a3 GC. [[n = nCcf (f)= Gcf (f) nLamp \u00a3 ]]Triv \u00a3 GC. \n[[x = xCcf (f)= Gcf (f) nLamp \u00a30 )\u00a3 ]]Triv \u00a30 ]]Expk)\u00a32 )\u00a3 GC. .. [[(.p x.e =(.p x.(.p1 k.[[e Ccf (f)= \nGcf (f) nLamp .Gcf (x)= Gcf (x) nLamp \u00a3 ]]Exp \u00a3 ]]Triv \u00a31 )\u00a32 [[tk =(let x = k\u00a30 [[tin x \u00a30 \u00a3 )\u00a31 ]]Exp\u00a30 \n]]Triv \u00a3 ]]Exp [[(let x = t0 in ek =(let x =[[t0 in [[ek)\u00a32 CGcf (f1)= CGcf (f) .Gcf (x)= .Gcf . (x) \nnLamp \u00a30 ]]Triv \u00a31 ]]Triv (let x0 =[[t0 [[t1 \u00a30 \u00a31 \u00a3 )\u00a32 ]]Exp\u00a33 \u00a3 ]]Expk)\u00a34 [[(let x = t0 t1 in ek \n= in (let x1 = x0 (.p x.[[eCGcf (f2)= CGcf (f) .Gcf (x)= .Gcf . (x) nLamp \u00a35 )\u00a36 )\u00a37 in x 1 \u00a3\u00a30 )\u00a31 \n]]Exp \u00a3 ]]Triv \u00a30 ]]Exp [[(let x = op(t) in ek =(let x = op([[t) in [[ek)\u00a32 CGcf (f1)= CGcf (f0) .Gcf \n(x)= .Gcf . (x) nLamp \u00a32 ]]Exp (let k1 =(.p x.[[ek)\u00a34 in \u00a3\u00a30 \u00a31 \u00a3 ]]Triv \u00a30 ]]Exp k1) Exp (let x = \nif0 te e (let x1 = if0 [[t([[e 01 0 G.. k = \u00a31 ]]Exp k1) Ccf (f3)= CGcf (f2) .Gcf (x)= Gcf (x) nLamp \n\u00a32 )\u00a33 in e ([[e1 \u00a35 )\u00a36 )\u00a37 in x 1 Figure 7: Flow transformation from CPS to direct style .ow information \nis collected in direct style and the points where .ow information is sent to continuations in CPS. Lemma \n3. Let k be a continuation identi.er introduced by the CPS transformation of a .-abstraction from p: \np1 Triv p1 Exp [[.x1.e \u00a30 ]]= .x1..k.[[e \u00a30 ]]k Then, for each .p x.e \u00a31 ..Gcf . (k), CGcf (R[[e \u00a30 ]]) \n..Gcf . (x). By the de.nition of ., it is immediate to show that CGcf (f0) ..G. cf (x). From Lemma 2, \nCGcf (R[[e \u00a30 ]]) .CGcf (f0). Proof of Theorem 1. The proof proceeds by induction on the transformation \nof p into p . We sketch the induction steps. \u00a3 ]]Triv We show that ( CGcf . ,.G. cf ) Fpcf ; (let x = \nk\u00a30 [[tin x \u00a31 )\u00a32 holds. For an arbitrary continuation .p y.e \u00a33 in the set CG. .. cf (f1)= Gcf (k), \nwe show that two .ow constraints are sat\u00adis.ed. CG. .. The .rst constraint is cf (f) . Gcf (y). By Lemma \n1, CG. G cf (f)= Ccf (f). We make a case analysis on the introduc\u00adtion of k by the CPS transformation. \nIf k is the top-level continuation, then the constraints are vacuously satis.ed. If k is introduced by \nthe transforma\u00adtion of a named conditional, then f is the return point of G one of the two branches of \nthe test. Obviously Ccf (f) . Gcf (y). Otherwise, k comes from the transformation of a .. .-abstraction \n.p1 x1.e \u00a34 from p, such that f = R[[e \u00a34 ]]. We apply Lemma 3. The second constraint is CG. .cf . (x). \nFollowing Lem\u00ad cf (f3) .Gma 1, it amounts to \u00d8.\u00d8. For the rest of the induction steps, the induction \nhypothe\u00adses and the de.nition of . su.ce to show that the constraints are satis.ed.  3.3 Reversing the \ntransformation In the previous section we have shown that direct-style .ow information can be transformed \ninto CPS .ow information. We can also show that any result of the analysis of a CPS\u00adtransformed program \ncan be matched by a result of the anal\u00adysis of its direct-style counterpart. Using again the structure \ngiven by the CPS transformation, we exhibit a direct-style .ow transformation. Given a direct-style program \np and its CPS counterpart p .,and given( CG. .cf . ) a valid solution of cf , Gthe analysis on p ., we \nrecover in linear time a valid solution (CGcf ,.Gcf ) of the analysis of p. Recovering a direct-style \nsolution is straightforward. For variables and trivial terms in p, we are only .ltering out the labels \nof continuations from the results of the analysis of p . . We de.ne the direct-style solution by induction \non the CPS transformation, following Figure 7. In the right part, for each CPS-transformation step, we \nassign .ow values into G Ccf and .Gcf . The left parts of Figures 6 and 7 are identical. We can show \nthat Figure 7 de.nes another function Dp :(Cachep; \u00d7Envp; ) .(Cachep \u00d7Envp ). cf cfcf cfcf p It is also \nrelatively easy to show that, like Ccf in Section 3.2, Dp is monotone. cf  3.4 Correctness of the reverse \ntransformation Let us show that the reverse transformation indeed yields a valid solution of the analysis \nof the original program. Theorem 2. Given a direct-style program p and its CPS . =[[p]]Pgm C. .. counterpart \np ,let ( Gcf , Gcf ) be a solution of the 0-CFA of p . (i.e., such that (CG. .. cf ; p . holds) and let \ncf , Gcf ) Fp p ( G(CG. .. cf p holds. Ccf ,.Gcf )= Dcf cf , Gcf ).Then (CGcf ,.Gcf ) Fp As in Section \n3.2, we use intermediate results to prove Theorem 2. Working under the assumptions of the theo\u00adrem, we \nobserve two immediate properties of the reverse transformation: .. \u00a3 GC. Lemma 4. For all x . Varp, \n.Gcf (x)= Gcf (x) nLamp;and for all trivial terms tin p, Ccf (f)= Gcf (f) n Lamp . Valbt = {S, D} Abstract \nvalues CGbt . Cachep = Labp . Valbt Abstract cache bt For an arbitrary expression, the new solution collects \nall .Gbt . Envbt p = Var p . Valbt Abstract environment the .ow information from the return point of \nthe expression. \u00a3 GFp . (Cachep \u00d7 Envp ) \u00d7 Labp Lemma 5. Let e be an expression in p.Then Ccf (f)= bt \nbtbtCGcf (R[[e \u00a3 ]]). Figure 8: BTA relation for a program p As a parallel of Lemma 3, the following \nlemma connects the .ow at the return points of functions with the .ow col\u00adlected for the variables declared \nby continuations.  4. BINDING-TIME ANALYSIS Lemma 6. Let k be a continuation identi.er introduced We \nconsider a constraint-based binding-time analysis (BTA) by the transformation of a .-abstraction from \np: [10, 25, 26, 28]. The analysis determines binding times of p1 Triv p1 Expprogram points and program \nvariables. This information [[.x1.e \u00a30 ]]= .x1..k.[[e \u00a30 ]]k is used to annotate the source program for \no.ine partial Then, for each .p x.e \u00a31 . .G. cf (k), CGcf (R[[e \u00a30 ]]) . .G. cf (x). evaluation [5, 16, \n25]. The results of the analysis therefore determine the static computation performed at specializa\u00adtion \ntime. Proof of Theorem 2. The proof is by induction on the The constraint-based BTA uses .ow information \nto de\u00adtransformation of p into p . . We sketch the induction steps. \u00a3 ]]Triv termine the binding times \nof the operators and operands For the transformation step [ t, the constraints follow of applications. \nAlternatively, we could have considered an from the induction hypothesis. The same applies for the analysis \ncomputing both .ow and binding-time information\u00a3 ]]Exp k. transformation step [ tat the same time, but \nthis approach is known to be equiva- For the transformation of a named application: lent [26]. We have \nchosen to separate the .ow analysis from \u00a33 ]]Triv [[t1]]Triv Exp let x0 =[[tthe binding-time analysis \nin order to reuse the results from \u00a33 0 let x = t 0 t1 k == x0 .p\u00a32 Section 3. in let x1 x.e in e2 The \nformal de.nition of the analysis is similar to the def\u00ad inition of the 0-CFA of Section 3. The analysis \nis a rela\u00adlet .p1 y.e \u00a31 4 be an arbitrary .-abstraction from p such that tion de.ned on essentially \nthe same domains (Figure 8); the p1 . CGcf (f3). Let the CPS transformation of the .-abstraction di.erence \nis that the domain of abstract values is now the be .p1 y..k1.e2.Then p . .G. cf (k1). From Lemma 5 and \nstandard lattice {S . D} of static and dynamic annotations. Lemma 6 we obtain that Ccf (f4) . .Gcf (x). \nin x1 GThe analysis relation is de.ned inductively over the syntax (Figure 9). At applications, the \nde.nition of the BTA refers 3.5 Equivalence of .ow to the .ow information (CGcf ,.Gcf ), which is considered \nto be Let p be an arbitrary direct-style program and p . =[[p]]Pgm the least solution of the control-.ow \nanalysis of Section 3. its CPS counterpart. It is a matter of tedious calculations In contrast to the \n0-CFA of Section 3, the BTA accepts to prove the following lemma: non-closed terms. Following the tradition, \nwe consider the program to be dynamic and its free variables to be dynamic Lemma 7. Given (CGcf ,.Gcf \n) a solution of the 0-CFA of p as well. The .ow information for the free variables is consid\u00ad (i.e., \nsuch that (CGcf ,.Gcf ) Fp p holds), Dp (Cp (CGcf ,.Gcf )) . ered to be empty, which is the result of \napplying the 0-CFA cf cfcf ( GGiven (CG. .. a solution of the 0-CFA of p . , to the program closed by \nabstraction over the free variables. Another di.erence from the 0-CFA of Section 3 is that the constraints \ngenerated by the BTA are equality constraints. Moreover, additional constraints are generated for .-abstrac\u00adtions, \nconditionals and let-expressions. The signi.cance of these additional constraints is discussed in Section \n4.4. A proof of correctness of a specializer using the annotations obtained by the BTA can be found in \nHatcli. and Danvy s work [10]. The rest of the section is organized as follows. First, we de.ne a CPS \ntransformation of binding times (Section 4.1), which we show to be correct and to preserve the quality \nof the binding times (Section 4.2). Unlike for 0-CFA, how\u00adever, we show examples where BTA on CPS terms \ngives more precise results than on the corresponding direct-style terms, thus showing that the CPS transformation \nmay lead to more specialization opportunities (Section 4.3). Finally (Section 4.4) we show that if we \nrelax the constraints of the BTA to take into account continuation-based partial evalu\u00adation, then, just \nlike 0-CFA, no loss and no gain of informa\u00adtion can be observed after the CPS transformation. Ccf ,.Gcf \n). cf , Gcf ) (i.e., such that (CGcf . ,.Gcf . ) Fp; p holds), Cp (Dp (CGcf . ,.Gcf . )) . cf cfcf (CG. \n.. cf , Gcf ). From these two properties the following main theorem fol\u00adlows directly. Theorem 3 (Equivalence \nof flow). Given a direct\u00ad . =[[p]]Pgm style program p and its CPS counterpart p ,let (CGcf ,.Gcf ) be \nthe least solution of the 0-CFA of p and let (CG. .cf . ) be the least solution of the 0-CFA of p ..Then \ncf , GCp C. .. C. .. (CGcf ,.Gcf )=( Gcf ) and Dp ( Gcf )=(CGcf ,.Gcf ). cf cf , Gcf cf , G Theorem 3 \nshows that the least .ow information obtained by a constraint-based analysis on a direct-style program \ncan be transformed into the least .ow information obtainable by the same analysis on the CPS counterpart \nof this program and vice versa. Lemma 1 and Lemma 4 show that the two solutions are in fact equal on \nthe variables and program points common to the two programs. We conclude that, for 0-CFA as de.ned in \nFigure 5, no information is lost or gained by the CPS transformation. 4.1 CPS transformation of binding \ntimes We show that the binding times obtained by analyzing the CPS counterpart of a program are at least \nas good as the ones obtained by analyzing the original program. With the same technique as in Section \n3, we construct in linear time a solution of the BTA over the CPS-transformed program from a solution \nof the BTA over the original program, such that the quality of the binding times is preserved. Given \nthe program p and (CGbt,.Gbt) a solution of the BTA C. ... over p, we de.ne ( Gbt, Gbt) as a solution \nof the BTA over p , the CPS counterpart of p. The de.nition is by induction on the CPS transformation \nand is given in Figure 10, where the left parts are identical to the left parts of Figures 6 and 7. In \nthe right part, we assign binding times into Gbt and Gbt. C. .. As in Section 3, we use Cp to denote \nthe function induced bt by the transformation. Cp :(Cachep \u00d7 Envp ) . (Cachep; \u00d7 Envp; ). bt btbtbtbt \n 4.2 Correctness of the transformation Let us show that the solution de.ned in Figure 10 is indeed a \nvalid solution of the BTA. We follow the same technique as in Section 3.2. The correctness of the transformation \nis established by the following theorem. Theorem 4. Given a direct-style program p and its CPS . =[[p]]Pgm \ncounterpart p ,let (CGbt,.Gbt) be an arbitrary solu\u00adtion of the BTA of p (i.e., such that (CGbt,.Gbt) \nFbt p p holds). If (CGbt. ,.Gbt. )= Cp (CGbt,.Gbt) then (CGbt. ,.Gbt. ) Fp; p holds. btbt Under the assumption \nof the theorem, we .rst observe immediate properties of the CPS transformation of binding times, similar \nto the ones stated in Lemma 1. .. Lemma 8. For all variables x in p, Gbt(x)= .Gbt(x);for all trivial \nterms t\u00a3 in p, CG. Cbt(f); and for all expres\u00ad bt(f)= G . CG. sions e in p , bt(e)= D. The binding time \nof an expression in p is equal to the binding time of its return point. Lemma 9. Let e \u00a3 be an arbitrary \nsubexpression of p.Then GG Cbt(R[[e \u00a3 ]]) = Cbt(f). The .ow of the continuation abstractions connects \nthe binding times of the return point of expressions and contin\u00aduation variables. The binding time of \nthe value abstracted by a continuation is equal to the binding time of any expres\u00adsion that the continuation \ncan be passed to. Lemma 10. Let k be a continuation identi.er introduced by the transformation of a .-abstraction \nfrom p: p1 Triv p1 Exp [[.x1.e \u00a30 ]]= .x1..k.[[e \u00a30 ]]k Then, for each .p x.e \u00a31 . .G. G\u00a30 ]]) = .G. \ncf (k), Cbt(R[[e bt(x). Proof of Theorem 4. The proof is an adaptation of the proof of Theorem 3 to equality \nconstraints. In addi\u00adtion, we need to prove the satisfaction of the additional con\u00adstraints introduced \nby BTA. We sketch the induction steps. C. .. \u00a3 ]]Triv \u00a31 )\u00a32 We show that ( Gcf , Gcf ) Fp; (let x = \nk\u00a30 [[tin x cf \u00a33 . CG. holds. For this purpose, given an arbitrary .p x.e cf (f0) = Gwe must show that \ntwo equality constraints are .. cf (k) satis.ed. Similarly to the proof of Theorem 3, we make a case \nanalysis on the introduction of k, using Lemma 9 and Lemma 10 to prove the satisfaction of the constraints. \nCG. CG. We also need to show that bt(f0)= D . bt(f)= D. Again, we make a case analysis on the introduction \nof k. The top-level case is trivial. The case where k is introduced by the transformation of a function \n(.y.e\u00a31 5 )\u00a34 implies that GC. Cbt(f4)= D.Thus CGbt(f5)= D and then Gbt(f)= D,since f = R[[e1 \u00a35 ] . \nThe same reasoning follows for the case where k comes from the transformation of a named conditional. \nThe remaining cases follow directly from the induction CG. .. G hypotheses and the de.nition of bt, Gbt, \nCcf and .. Theorem 4 and Lemma 8 show that we can transform any binding-time solution of a direct-style \nprogram into a solu\u00adtion of its CPS counterpart in such a way that the binding times of variables and \ntrivial terms are preserved. In par\u00adticular, this implies that no values are forced to be dynamic by \nthe CPS transformation. It also implies that the static computations (applications, tests or base-type \noperations) in a direct-style program remain static as well in its CPS counterpart. We thus conclude \nthat the same amount of specialization of the input program can be achieved after CPS transformation. \n 4.3 Reversing the transformation Let us now show that it is not always possible to reverse the CPS \ntransformation of binding times. There are cases when the least analysis of a CPS-transformed program \nproduces strictly more static annotations than the least analysis of its direct-style counterpart. Here \nis a canonical example [10], where inc is the successor function and the free variable z is considered \nto be dynamic: let r =(.p y.let v = inc(z) in 2) 1 in let r1 = inc(r) in r1 In the least solution of \nthe BTA on this term, even if the application of .p to 1 is classi.ed as static, its result is clas\u00adsi.ed \nas dynamic because of the dynamic header of the let\u00adexpression. Thus r is dynamic. Since the second increment \noperation depends on r, it is dynamic as well. In a realistic setting, simply discarding the dynamic \ncomputation inc(z) might not be meaning-preserving since it can, for instance, yield an integer over.ow \nat run-time. The CPS counterpart of the canonical example above reads as follows (without embedding it \ninto direct style, for read\u00adability): .k..py..k1.let v = inc(z)1.r.let r1 = inc(r) in k1 2in kr1 The \ncontinuation denoted by k1 is static, and thus the ap\u00adplication k1 2 is performed statically (even if \nits result is dynamic). Thus, r is static as well, and further computa\u00adtion based on r can be performed \nat specialization time. Other binding-time improvements can be obtained when a dynamic test disables \nfurther computations based on its result. The canonical example is as follows: let v = if0 z 01 in let \nv1 = inc(v) in v1 It is true that one bene.ts from such an improvement only by allowing code duplication. \nHowever, the code duplication takes place at specialization time, not at BTA time. Thus in contrast to \nSabry and Felleisen s observation [31], the im\u00adprovement in this case is not due to duplicating the analysis \non the two branches. (CGbt,.Gbt)Fp n \u00a3 .. true bt (CGbt,.Gbt)Fp x \u00a3 .. .Gbt(x)=CGbt(f) bt (CGbt,.Gbt)Fp \n(.p x.e \u00a31 )\u00a3 .. (CGbt,.Gbt)Fp e \u00a31 . (.Gbt(f)=D . CGbt(f1)=.Gbt(x)=D) bt bt (CGbt,.Gbt)Fp (let x =t\u00a3 \nin e \u00a31 )\u00a32 .. (CGbt,.Gbt)Fp t\u00a3 . (CGbt,.Gbt)Fp e \u00a31 . CGbt(f)=.Gbt(x). CGbt(f1)=CGbt(f2). bt btbt G \n.Gbt(x)=D . Cbt(f1)=D \u00a30 \u00a31 \u00a30 \u00a31 (CGbt,.Gbt)Fp (let x =ttin e \u00a32 )\u00a33 .. (CGbt,.Gbt)Fp t. (CGbt,.Gbt)Fp \nt. (CGbt,.Gbt)Fp e \u00a32 . CGbt(f2)=CGbt(f3). bt 01 bt0 bt1 bt (CGbt(f0)=D . CGbt(f1)=.Gbt(x)=D). (.Gbt(x)=D \n. CGbt(f2)=D). . (.p y.e \u00a3 1). CGcf (f0).(CGbt(f1)=.Gbt(y). CGbt(f)=.Gbt(x)) (G\u00a3\u00a31 )\u00a32 (G\u00a3 . (G\u00a31 . GG \nCbt,.Gbt)Fp (let x =op(t)in e .. Cbt,.Gbt)Fp tCbt,.Gbt)Fp eCbt(f). .Gbt(x). Cbt(f1)=CGbt(f2). bt btbt \nG .Gbt(x)=D . Cbt(f1)=D \u00a3\u00a30 \u00a31 \u00a30 \u00a31 (CGbt,.Gbt)Fp (let x =if0 te e .. (CGbt,.Gbt)Fp t\u00a3 . (CGbt,.Gbt)Fp \ne . (CGbt,.Gbt)Fp e . (CGbt,.Gbt)Fp e \u00a32 . bt 01 bt bt0bt1bt \u00a32 )\u00a33 in eCGbt(f0)=CGbt(f1)=.Gbt(x). (CGbt(f)=D \n. CGbt(f0)=CGbt(f1)=D). (.Gbt(x)=D . CGbt(f2)=D). CGbt(f2)=CGbt(f3) (CGbt,.Gbt)Fp p .. (. x.x freeinp \n. .Gbt(x)=D). (p =e \u00a3 . CGbt(f)=D) bt Figure 9: Syntax-directed BTA \u00a3 ]]Pgm \u00a3 ]]Exp [[e =(.pk.[[ek)\u00a30 \nCG. .. bt(f0)=Gbt(k)=D \u00a3 Triv \u00a3 CG. [[n ]]= n bt(f)=CGbt(f) \u00a3 Triv \u00a3 CG. [[x ]]= x bt(f)=CGbt(f) \u00a30 \n)\u00a3 Triv \u00a30 ]]Exp \u00a32 )\u00a3 CG. CG. [[(.p x.e ]]=(.p x.(.p1 k.[[ek)bt(f2)=CGbt(f) bt(f)=CGbt(f) Gbt(x)=.Gbt(x) \nGbt(k)=CGbt(f) .. .. Triv \u00a32 [[t\u00a3 ]]Exp k =(let x =k\u00a30 [[t\u00a3 ]]in x \u00a31 )CG. .. bt(f0)=Gbt(k) CG. C. bt(f2)= \nGbt(f1)=.Gbt(x)=D \u00a30 \u00a3\u00a31 ]]Exp \u00a30 Triv \u00a3 ]]Exp \u00a32 CG. .. [[(let x =t in e )k =(let x =[[t ]]in [[ek)bt(f2)=D \nGx)=.Gbt(x) 00 bt( \u00a30 Triv \u00a31 Triv CG. C. .. (let x0 =[[t0 ]][[t1 ]]in bt(f4)= Gbt(f3)=Gbt(x0)=CGbt(f0) \n\u00a30 \u00a31 \u00a3\u00a32 ]]Exp \u00a33 \u00a3 ]]Exp \u00a34 .. .. [[(let x =t0 t1 in e )k =(let x1 =x0 (.p x.[[ek)Gbt(x)=.Gbt(x) Gbt(x1)=D \nin x \u00a31 5 )\u00a36 )\u00a37 CG. C. C. bt(f7)= Gbt(f6)= Gbt(f5)=D \u00a30 )\u00a31 ]]Exp k =(let x =op([Triv )in [[\u00a30 ]]Exp \nk)\u00a32 .. CG. [[(let x =op(t\u00a3 )in e [t\u00a3 ]]e Gx)=Gx) bt(.bt(bt(f2)=D \u00a32 ]]Exp Exp (let k1 =(.p x.[[ek)\u00a34 \nin .. C. .. \u00a3\u00a30 \u00a31 Gbt(k1)= Gbt(f4)=Gbt(x)=.Gbt(x) (let x =if0 te e \u00a3 Triv \u00a30 ]]Exp \u00a31 ]]Exp 01 k =(let \nx1 =if0 [[t]]([[ek1)([ ek1) CG. C. C. \u00a32 )\u00a33 01 bt(f7)= Gbt(f6)= Gbt(f5)=D \u00a35 \u00a37 in e in x )\u00a36 ).G. \nx1)=D 1 bt( Figure 10: Transformation of binding times from direct style to CPS (CGbt,.Gbt)Fp n \u00a3 .. \ntrue bt. (G\u00a3 Cbt,.Gbt)Fp x .. .Gbt(x)=CGbt(f) bt. (CGbt,.Gbt)Fp (.p x.e \u00a31 )\u00a3 .. (CGbt,.Gbt)Fp e \u00a31 \n. (.Gbt(f)=D . CGbt(f1)=.Gbt(x)=D) bt. bt. (CGbt,.Gbt)Fp (let x =t\u00a3 in e \u00a31 )\u00a32 .. (CGbt,.Gbt)Fp t\u00a3 . \n(CGbt,.Gbt)Fp e \u00a31 . CGbt(f)=.Gbt(x). CGbt(f1)=CGbt(f2) bt. bt. bt. \u00a30 \u00a31 \u00a30 \u00a31 (CGbt,.Gbt)Fp (let x \n=ttin e \u00a32 )\u00a33 .. (CGbt,.Gbt)Fp t. (CGbt,.Gbt)Fp t. (CGbt,.Gbt)Fp e \u00a32 . CGbt(f2)=CGbt(f3) bt. 01 bt. \n0 bt. 1 bt. . (CGbt(f0)=D . CGbt(f1)=.Gbt(x)=D) \u00a3 G.(G .. (.p y.e1). Ccf (f0)Cbt(f1)=.Gbt(y). CGbt(f)=.Gbt(x)) \n(CGbt,.Gbt)Fp (let x =op(t\u00a3 )in e \u00a31 )\u00a32 .. (CGbt,.Gbt)Fp t\u00a3 . (CGbt,.Gbt)Fp e \u00a31 . CGbt(f). .Gbt(x). \nCGbt(f1)=CGbt(f2) bt. bt. bt. \u00a3\u00a30 \u00a31 \u00a30 \u00a31 (CGbt,.Gbt)Fp (let x =if0 te e .. (CGbt,.Gbt)Fp t\u00a3 . (CGbt,.Gbt)Fp \ne . (CGbt,.Gbt)Fp e . (CGbt,.Gbt)Fp e \u00a32 bt. 01 bt. bt. 0 bt. 1 bt. in e \u00a32 )\u00a33 . CGbt(f0)=CGbt(f1)=.Gbt(x). \nCGbt(f2)=CGbt(f3) (CGbt,.Gbt)Fp p .. (. x.x freeinp . .Gbt(x)=D). (p =e \u00a3 . CGbt(f)=D) bt. Figure 11: \nSyntax-directed BTA for continuation-based partial evaluation 4.4 Continuation-based partial evaluation \nIn the two examples above the binding-time improvements come from two constraints in the speci.cation \nof the BTA (Figure 9): the body of a let-expression has to be dynamic if the header is dynamic, and both \nbranches of a conditional have to be dynamic if the test is dynamic. The binding-time constraint in the \nbody of a let-expression re.ects the concern about which reductions can be per\u00adformed safely by the specializer. \nIn the context of the com\u00adputational metalanguage [10], a named dynamic computa\u00adtion cannot be discarded \ndue to possible computational ef\u00adfects. Similarly, the constraint over the branches of a con\u00additional \nis introduced because one cannot decide statically which conditional branch should be selected. The above-mentioned \nconstraint on the body of a let\u00adexpression can be relaxed if one uses a continuation-based program specializer \n[2, 10, 18]. The constraint connecting the branches of a test with the test itself can be relaxed as \nwell if one allows the same continuation-based specializer to lift the test above the context, either \nby duplicating the con\u00adtext or by using a let-expression. Given such a specializer, we can show that \nenhancing the BTA by relaxing the two special constraints voids the impact of CPS on the global result. \nMore formally, we consider the BTA of Figure 9, without the constraints mentioned just above. Naming \nFp the new bt. relation, we replace the let rules as speci.ed in Figure 11. The result is BTA. . Using \nthe same proof technique as in Section 3, we can formally show that the CPS transformation has no e.ect \non BTA. , i.e., it entails no local increase and also no loss of precision elsewhere in the program: \nthe best binding times in direct style are the best binding times in CPS as well. More precisely, we \ncan de.ne Cp bt. , the CPS transforma\u00adtion of the binding times obtained by BTA. . The de.nition is only \na slight modi.cation of the de.nition of Cp in Sec\u00ad bt tion 4.1. Given the program p and a solution (CGbt. \n,.Gbt. )of BTA. (i.e., such that (CGbt. ,.Gbt. ) Fp p holds), we can show bt. that Cp ; p . holds. We \ncan also de.ne the re\u00ad bt. (CGbt. ,.Gbt. ) Fbtp. verse binding-time transformation Dp bt. , which is \nessentially the same as the reverse .ow transformation of Section 3.3 and also operates in linear time: \nfor each term we just ex\u00adtract the binding time of its CPS counterpart. We can show that given a solution \n(CG. .bt. . )of BTA. for p . (i.e., such bt. , G that (CG. .. ; p holds), Dp C. .. p holds bt. , Gbt. \n) Fpbt. ( Gbt. ) Fp bt. bt. , Gbt. too. Graphically: Cp bt. . ( G(CG. .. Cbt. ,.Gbt. ) bt. , Gbt. ) \nDp bt. BTA. BTA. p . . [[\u00b7]]Pgm p We are now in position to connect the binding times in direct style \nand in CPS as obtained by BTA.: Theorem 5. Given a direct-style program p and its CPS . =[[p]]Pgm counterpart \np ,let (CGbt. ,.Gbt. ) be the least solution of BTA. for p and let ( Gbt. , Gbt. ) be the least solution \nof C. .. BTA. for p . . Then for all variables x in p, .Gbt. (x)= .Gbt. . (x) and for all trivial terms \nt\u00a3 in p, GC. Cbt. (f)= Gbt. (f). We thus conclude that the CPS transformation has no e.ect on the amount \nof specialization that can be performed when using continuation-based partial evaluation.  5. RELATED \nWORK 5.1 Program analysis in general Theissueof syntactic accidentsseemstobefolklore in the program-analysis \ncommunity (Hanne Riis Nielson, personal communication, March 2000). We are, however, only aware of three \nstudies: Nielson s early work on data-.ow anal\u00adysis [21],5 Henglein s invariance properties of polymorphic \ntyping judgments with respect to let unfolding and fold\u00ading and .-reduction [12], and Sabry and Felleisen \ns work on constant propagation which shows that performing a CPS transformation leads to incomparable \nresults of the analy\u00adsis [31]. Sabry and Felleisen conclude that CPS can (1) improve results by duplicating \nthe analysis over conditionals and (2) worsen results by confusing the return points of function calls. \n(1) None of the 0-CFA and BTAs we consider here dupli\u00adcates the analysis over conditionals. (2) Sabry \nand Felleisen s treatment of function calls distin\u00adguishes the order in which these calls are encountered. \nConfusing return points exerts an impact on their analy\u00adsis (namely a loss in the precision of the results), \nbecause of this distinction. In contrast, the 0-CFA and the BTAs are not speci.ed operationally but with \nconstraints, and as such, they do not have such chronological dependen\u00adcies. Already in direct style, \nthe constraint-based ap\u00adproach (in the monovariant case) propagates the result of a function at once \nto all the application sites of this function. (This property enabled us to show that the CPS transformation \npreserves the results of both anal\u00adyses.)  Recently, Palsberg and Wand have conducted a similar study \nfor 0-CFA [29], supporting Sabry and Felleisen s con\u00adclusion that the extra precision enabled by the \nCPS trans\u00adformation is due to the duplication of the analysis. In their study, they developed a CPS transformation \nof .ow informa\u00adtion comparable to the one of Figure 6, but independently and prior to us. Palsberg and \nWand also mention that least solutions may or may not be preserved by administrative reductions of CPS-transformed \nprograms. In that, they im\u00adplicitly share our concern about syntactic accidents, even though their primary \ngoal was to transfer Wand s pioneer results on the CPS transformation of types [19, 34] to the CPS transformation \nof .ow types. 5Actually, Nielson s work is only indirectly connected since it addresses a continuation \nsemantics instead of a direct se\u00admantics of a CPS-transformed program. 5.2 Binding-time analysis and \nthe CPS trans\u00adformation Binding-time improvements have always been customary for users of binding-time \nanalysis [16, 22]. One of them amounts to considering source programs in CPS [4, 6], which suggests that \nsource programs should be systematically CPS-trans\u00adformed [3]. (Muylaert-Filho and Burn take the same \nstand for strictness analysis and the call-by-name CPS transfor\u00admation [20].) Essentially, the CPS transformation \nrelocates potentially static contexts inside de.nitely dynamic contexts (let ex\u00adpressions and conditionals), \nthereby providing a binding\u00adtime improvement. To this end, the CPS transformation itself is continuation-based \n[7], which paved the way to cont\u00adinuation-based partial evaluation [2, 18]. Hatcli. and Danvy have characterized \nthe full e.ect of continuation-based partial evaluation as online let .attening in Moggi s computational \nmeta-language [10]. This charac\u00adterization justi.es why o.ine let .attening is also, partially, a binding-time \nimprovement [13]. In any case, o.ine let .at\u00adtening is known to be part of the CPS transformation [9]. \nWhat had not been shown before, however, and what we do address here, is whether such improvements worsen \nbinding times elsewhere in a source program.  6. CONCLUSION AND ISSUES Observing that program analyses \nare vulnerable to syntactic accidents, we have considered a radical syntactic change: a transformation \ninto CPS. We have studied the interaction between a non-duplicating CPS transformation and two pro\u00adgram \nanalyses: control-.ow analysis (0-CFA) and binding\u00adtime analysis. Through a systematic construction of \nthe CPS counterpart of .ow information, we have found that 0-CFA is insensitive to continuation-passing, \nand that the CPS transformation does improve binding times. Using the same technique, we have also found \nthat with modi.ed let and case rules, BTA is insensitive to the CPS transforma\u00adtion. These results suggest \ntwo further avenues of study: In BTA, the bene.cial e.ect of the CPS transformation can be accounted \nfor by tuning the let rule (as well as the case rule, if one is willing to duplicate static contexts \nat specialization time). The price of this change, how\u00adever, is that the corresponding program specializer \nhas to be made continuation-based [10]. We conjecture that the situation is similar, e.g., for security \nanalysis, which has similar let and case rules. Just like BTA, a security analysis thus ought to yield \nmore precise results over CPS-transformed programs. We therefore also conjec\u00adture that the bene.cial \ne.ect of the CPS transformation can be accounted for by tuning the let and case rules, if one is willing \nto develop a corresponding continuation\u00adbased processor of security information.  More generally, as \na step towards more robust program analyses that are less vulnerable to syntactic accidents, we need \nto understand better the program-analysis per\u00adspective over syntactic landscapes. Two key questions arise \nwhich may be general to program analysis or spe\u00adci.c to individual program analyses: which program transformations \na.ect precision? And among those that  do, which ones a.ect precision monotonically?Answer\u00ading these \nquestions would enable one to develop more reliable program analyses, possibly with some kind of subject \nreduction property or with some kind of inter\u00admediate language for program analysis. 7. ACKNOWLEDGMENTS \nWe are grateful to Andrzej Filinski and Julia L. Lawall for substantial discussions and comments. Thanks \nare also due to Torben Amtoft, Anindya Banerjee, Bernd Grobauer, Dan Hernest, Niels O. Jensen, Lasse \nR. Nielsen, Morten Rhiger, Amr Sabry, Zhe Yang, and the anonymous referees for fur\u00adther comments. 8. \nREFERENCES [1] Mart\u00b4in Abadi, Anindya Banerjee, Nevin Heintze, and Jon G. Riecke. A core calculus of \ndependency. In Alex Aiken, editor, Proceedings of the Twenty-Sixth Annual ACM Symposium on Principles \nof Programming Languages, pages 147 160, San Antonio, Texas, January 1999. ACM Press. [2] Anders Bondorf. \nImproving binding times without explicit cps-conversion. In William Clinger, editor, Proceedings of \nthe 1992 ACM Conference on Lisp and Functional Programming,LISP Pointers, Vol. V, No. 1, pages 1 10, \nSan Francisco, California, June 1992. ACM Press. [3] Charles Consel and Olivier Danvy. For a better \nsupport of static data .ow. In John Hughes, editor, Proceedings of the Fifth ACM Conference on Functional \nProgramming and Computer Architecture, number 523 in Lecture Notes in Computer Science, pages 496 519, \nCambridge, Massachusetts, August 1991. Springer-Verlag. [4] Charles Consel and Olivier Danvy. Static \nand dynamic semantics processing. In Robert (Corky) Cartwright, editor, Proceedings of the Eighteenth \nAnnual ACM Symposium on Principles of Programming Languages, pages 14 24, Orlando, Florida, January 1991. \nACM Press. [5] Charles Consel and Olivier Danvy. Tutorial notes on partial evaluation. In Susan L. Graham, \neditor, Proceedings of the Twentieth Annual ACM Symposium on Principles of Programming Languages, pages \n493 501, Charleston, South Carolina, January 1993. ACM Press. [6] Olivier Danvy. Semantics-directed compilation \nof non-linear patterns. Information Processing Letters, 37:315 322, March 1991. [7] Olivier Danvy and \nAndrzej Filinski. Abstracting control. In Mitchell Wand, editor, Proceedings of the 1990 ACM Conference \non Lisp and Functional Programming, pages 151 160, Nice, France, June 1990. ACM Press. [8] Kirsten L. \nSolberg Gasser, Flemming Nielson, and Hanne Riis Nielson. Systematic realisation of control .ow analyses \nfor CML. In Mads Tofte, editor, Proceedings of the 1997 ACM SIGPLAN International Conference on Functional \nProgramming, pages 38 51, Amsterdam, The Netherlands, June 1997. ACM Press. [9] John Hatcli. and Olivier \nDanvy. A generic account of continuation-passing styles. In Hans-J. Boehm, editor, Proceedings of the \nTwenty-First Annual ACM Symposium on Principles of Programming Languages, pages 458 471, Portland, Oregon, \nJanuary 1994. ACM Press. [10] John Hatcli. and Olivier Danvy. A computational formalization for partial \nevaluation. Mathematical Structures in Computer Science, pages 507 541, 1997. Extended version available \nas the technical report BRICS RS-96-34. [11] Nevin Heintze. Set-based program analysis of ML programs. \nIn Talcott [33], pages 306 317. [12] Fritz Henglein. Syntactic properties of polymorphic subtyping. Technical \nReport Semantics Report D-293, DIKU, Computer Science Department, University of Copenhagen, May 1996. \n[13] Carsten K. Holst and Carsten K. Gomard. Partial evaluation is fuller laziness. In Paul Hudak and \nNeil D. Jones, editors, Proceedings of the ACM SIGPLAN Symposium on Partial Evaluation and Semantics-Based \nProgram Manipulation, SIGPLAN Notices, Vol. 26, No 9, pages 223 233, New Haven, Connecticut, June 1991. \nACM Press. [14] Suresh Jagannathan and Stephen Weeks. A uni.ed treatment of .ow analysis in higher-order \nlanguages. In Peter Lee, editor, Proceedings of the Twenty-Second Annual ACM Symposium on Principles \nof Programming Languages, pages 393 407, San Francisco, California, January 1995. ACM Press. [15] Neil \nD. Jones. What not to do when writing an interpreter for specialisation. In Olivier Danvy, Robert Gl\u00a8uck, \nand Peter Thiemann, editors, Partial Evaluation, number 1110 in Lecture Notes in Computer Science, pages \n216 237, Dagstuhl, Germany, February 1996. Springer-Verlag. [16] Neil D. Jones, Carsten K. Gomard, and \nPeter Sestoft. Partial Evaluation and Automatic Program Generation. Prentice-Hall International, 1993. \nAvailable online at http://www.dina.kvl.dk/~sestoft/pebook/pebook.html [17] Ulrik J\u00f8rring and William \nL. Scherlis. Compilers and staging transformations. In Mark Scott Johnson and Ravi Sethi, editors, Proceedings \nof the Thirteenth Annual ACM Symposium on Principles of Programming Languages, pages 86 96, St. Petersburg, \nFlorida, January 1986. ACM Press. [18] Julia L. Lawall and Olivier Danvy. Continuation-based partial \nevaluation. In Talcott [33]. [19] Albert R. Meyer and Mitchell Wand. Continuation semantics in typed \nlambda-calculi (summary). In Rohit Parikh, editor, Logics of Programs Proceedings, number 193 in Lecture \nNotes in Computer Science, pages 219 224, Brooklyn, June 1985. Springer-Verlag. [20] Juarez A. Muylaert-Filho \nand Geo.rey L. Burn. Continuation passing transformation and abstract interpretation. In G. Burn, S. \nGay, and M. Ryan, editors, Theory and Formal Methods 1993: Proceedings of the First Imperial College \nDepartment of Computing Workshop on Theory and Formal Methods, pages 247 259. Springer-Verlag, 1993. \n[21] Flemming Nielson. A denotational framework for data .ow analysis. Acta Informatica, 18:265 287, \n1982. [22] Flemming Nielson and Hanne Riis Nielson. Two-Level Functional Languages,volume 34 of Cambridge \nTracts in Theoretical Computer Science. Cambridge University Press, 1992. [23] Flemming Nielson and Hanne \nRiis Nielson. In.nitary control .ow analysis: a collecting semantics for closure analysis. In Neil D. \nJones, editor, Proceedings of the Twenty-Fourth Annual ACM Symposium on Principles of Programming Languages, \npages 332 345, Paris, France, January 1997. ACM Press. [24] Flemming Nielson, Hanne Riis Nielson, and \nChris Hankin. Principles of Program Analysis. Springer Verlag, 1999. [25] Jens Palsberg. Correctness \nof binding-time analysis. Journal of Functional Programming, 3(3):347 363, 1993. [26] Jens Palsberg. \nComparing .ow-based binding-time analyses. In Peter Mosses, Mogens Nielsen, and Michael Schwartzbach, \neditors, Proceedings of TAPSOFT 95, number 915 in Lecture Notes in Computer Science, pages 561 574, Aarhus, \nDenmark, May 1995. Springer-Verlag. [27] Jens Palsberg and Patrick O Keefe. A type system equivalent \nto .ow analysis. ACM Transactions on Programming Languages and Systems, 17(4):576 599, July 1995. [28] \nJens Palsberg and Michael I. Schwartzbach. Binding-time analysis: Abstract interpretation versus type \ninference. In Proceedings of the Fifth IEEE International Conference on Computer Languages, pages 289 \n298. IEEE Computer Society Press, 1994. [29] Jens Palsberg and Mitchell Wand. CPS transformation of .ow \ninformation. Unpublished manuscript, available at http://www.cs.purdue.edu/~palsberg/publications.html, \nMarch 2000. [30] Gordon D. Plotkin. Call-by-name, call-by-value and the .-calculus. Theoretical Computer \nScience, 1:125 159, 1975. [31] Amr Sabry and Matthias Felleisen. Is continuation-passing useful for data \n.ow analysis? In Vivek Sarkar, editor, Proceedings of the ACM SIGPLAN 94 Conference on Programming Languages \nDesign and Implementation, SIGPLAN Notices, Vol. 29, No 6, pages 1 12, Orlando, Florida, June 1994. ACM \nPress. [32] Guy L. Steele Jr. Rabbit: A compiler for Scheme. Technical Report AI-TR-474, Arti.cial Intelligence \nLaboratory, Massachusetts Institute of Technology, Cambridge, Massachusetts, May 1978. [33] Carolyn L. \nTalcott, editor. Proceedings of the 1994 ACM Conference on Lisp and Functional Programming, LISP Pointers, \nVol. VII, No. 3, Orlando, Florida, June 1994. ACM Press. [34] Mitchell Wand. Embedding type structure \nin semantics. In Mary S. Van Deusen and Zvi Galil, editors, Proceedings of the Twelfth Annual ACM Symposium \non Principles of Programming Languages, pages 1 6, New Orleans, Louisiana, January 1985. ACM Press. \n \n\t\t\t", "proc_id": "351240", "abstract": "We show that a non-duplicating CPS transformation has no effect on control-flow analysis and that it has a positive effect on binding-time analysis: a monovariant control-flow analysis yields equivalent results on a direct-style program and on its CPS counterpart, and a monovariant binding-time analysis yields more precise results on a CPS program than on its direct-style counterpart. Our proof technique amounts to constructing the continuation-passing style (CPS) counterpart of flow information and of binding times.Our results confirm a folklore theorem about binding-time analysis, namely that CPS has a positive effect on binding times. What may be more surprising is that this benefit holds even if contexts or continuations are not duplicated.The present study is symptomatic of an unsettling property of program analyses: their quality is unpredictably vulnerable to syntactic accidents in source programs, i.e., to the way these programs are written. More reliable program analyses require a better understanding of the effect of syntactic change.", "authors": [{"name": "Daniel Damian", "author_profile_id": "81100466167", "affiliation": "BRICS, Department of Computer Science, University of Aarhus, Ny Munkegade, Building 540, DK-8000 Aarhus C, Denmark", "person_id": "P58665", "email_address": "", "orcid_id": ""}, {"name": "Olivier Danvy", "author_profile_id": "81100394275", "affiliation": "BRICS, Department of Computer Science, University of Aarhus, Ny Munkegade, Building 540, DK-8000 Aarhus C, Denmark", "person_id": "PP15031217", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/351240.351260", "year": "2000", "article_id": "351260", "conference": "ICFP", "title": "Syntactic accidents in program analysis: on the impact of the CPS transformation", "url": "http://dl.acm.org/citation.cfm?id=351260"}