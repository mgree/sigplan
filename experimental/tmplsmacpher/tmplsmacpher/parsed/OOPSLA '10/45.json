{"article_publication_date": "10-17-2010", "fulltext": "\n Concurrent Programming with Revisions and Isolation Types Sebastian Burckhardt Alexandro Baldassin \nDaan Leijen Microsoft Research State University of Campinas, Brazil Microsoft Research sburckha@microsoft.com \nalebal@ic.unicamp.br daan@microsoft.com Abstract Building applications that are responsive and can exploit \npar\u00adallel hardware while remaining simple to write, understand, test, and maintain, poses an important \nchallenge for develop\u00aders. In particular, it is often desirable to enable various tasks to read or modify \nshared data concurrently without requiring complicated locking schemes that may throttle concurrency \nand introduce bugs. We introduce a mechanism that simpli.es the parallel ex\u00adecution of different application \ntasks. Programmers declare what data they wish to share between tasks by using isolation types, and execute \ntasks concurrently by forking and joining revisions. These revisions are isolated: they read and mod\u00adify \ntheir own private copy of the shared data only. A runtime creates and merges copies automatically, and \nresolves con\u00ad.icts deterministically, in a manner declared by the chosen isolation type. To demonstrate \nthe practical viability of our approach, we developed an ef.cient algorithm and an implementation in \nthe form of a C# library, and used it to parallelize an interac\u00adtive game application. Our results show \nthat the parallelized game, while simple and very similar to the original sequen\u00adtial game, achieves \nsatisfactory speedups on a multicore pro\u00adcessor. Categories and Subject Descriptors 1.3 [Concurrent Pro\u00adgramming]: \nParallel Programming; 3.3 [Language Con\u00adstructs and Features]: Concurrent Programming Structures General \nTerms Languages Keywords Concurrency, Parallelism, Transactions, Isola\u00adtion, Revisions 1. Introduction \nDespite much research on parallel programming, how to ef\u00adfectively build applications that enable concurrent \nexecu- Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA/SPLASH \n10, October 17 21, 2010, Reno/Tahoe, Nevada, USA. Copyright c &#38;#169; 2010 ACM 978-1-4503-0203-6/10/10. \n. . $10.00 tion of tasks that perform various functions and may execute asynchronously is not generally \nwell understood. This prob\u00adlem is important in practice as a wide range of applications need to be responsive \nand would bene.t from exploiting par\u00adallel hardware. Consider an application where many tasks are executing \nin parallel. For example, an of.ce application may concur\u00adrently run tasks that (1) save a snapshot of \nthe document to disk, (2) react to keyboard input by the user who is editing the document, (3) perform \na spellcheck in the background, and (4) exchange document updates with collaborating re\u00admote users. Some \nof these tasks are CPU-bound, others are IO-bound; some only read the shared data, others may mod\u00adify \nit. However, all of them need to potentially access the same data at the same time; thus, they must avoid, \nnegotiate, or resolve con.icts. Ensuring consistency of shared data while allowing tasks to execute concurrently \nis often challenging, as it may re\u00adquire not only complex locking protocols, but also some form of data \nreplication. We present a programming model that simpli.es the sharing of data between such tasks. Its \nkey design choices are: Declarative Data Sharing. The programmer uses special isolation types to declare \nwhat data can be shared be\u00adtween concurrent tasks. Automatic Isolation. Whenever the programmer forks \nan asynchronous task (we call these tasks revisions), it oper\u00adates in isolation. Conceptually, each revision \noperates on a private copy of the entire shared state, which is guaran\u00adteed to be consistent and stable. \nDeterministic con.ict resolution. When the programmer joins a revision, all write-write con.icts (data \nthat was modi.ed both by the joinee and the joiner) are resolved deterministically as speci.ed by the \nisolation type. For example, if there is a con.ict on a variable of type versioned(T ) (the most common \nisolation type), the value of the joinee always overwrites the value of the joiner. Deterministic con.ict \nresolution never fails, thus revisions never roll back . These choices ensure deterministic concurrent \nprogram execution. Unlike conventional approaches, however, we do not require executions to be equivalent \nto some sequential int x = 0; versioned(int) x = 0; task t = fork { revision r = rfork {x=1; x=1; }} \nassert(x = 0 . x = 1); assert(x = 0); join t; rjoin r; assert(x = 1); assert(x = 1); Figure 1. Comparison \nof a classic asynchronous task oper\u00adating on a standard integer variable (left) and of a revision operating \non a versioned variable (right). The assert state\u00adments show the possible values of x at each point. \nexecution, which would unnecessarily restrict the available concurrency. Instead, we posit that programmers, \nif given the right abstractions, are capable of reasoning about concurrent executions directly. For example, \nsee Fig. 1. Under standard asynchronous task semantics (left), accesses to the integer variable by the \nmain thread and the task are interleaved nondeterministically. But when working with revisions and a \nversioned type (right), the revision is guaranteed to work on an isolated copy. Effects of a revision \nare only seen once that revision is joined. In the meantime, other revisions can freely access that same \ndata. Our mechanism eliminates the need to perform any syn\u00adchronization (such as critical sections) inside \ntasks. Each task is guaranteed to see a stable snapshot of the whole shared state, on which it can perform \nreads and writes at any time without risking blocking, interference, aborts, or retries, no matter how \nlong it runs. Our approach is data-centric in the sense that it removes complexity from the tasks (which \nneed no longer worry about synchronization) and adds it to the data declarations (which now need to specify \nthe isola\u00adtion type). Our main contributions are: (Section 2) We present a precise description of the \nse\u00admantics of revisions and of the various isolation types. We introduce revision diagrams as a novel \nway to reason about visibility of effects.  (Section 3) We review common implementation tech\u00adniques \nfor parallelizing application tasks and how they re\u00adlate to revisions. In particular, we elaborate the \nsemantic differences between revisions and transactions, and dis\u00adcuss related work.  (Section 4) We \ndescribe a case study on how we par\u00adallelized an example application, a full-featured multi\u00adplayer game \ncalled SpaceWars3D [17]. We describe what changes were involved in parallelizing the game, and how we \nsolved some of the more interesting issues. Moreover, we perform a quantitative evaluation in the form \nof mea\u00adsurements that compare the frame rates, the task execu\u00adtion times, and the overhead of accessing \nshared state be\u00adtween the sequential and the parallel version.  (Section 5) We lift the covers and explain \nour runtime implementation (a C# library) and our optimized algo\u00adrithm, which uses lazy copy-on-write, \ndisposal of re\u00addundant replicas, and has a low overhead for accessing shared data. Our results show \nthat revisions and isolation types pro\u00advide an elegant yet ef.cient mechanism for enabling paral\u00adlelization \nof tasks in reactive or interactive applications. 2. Revisions and Isolation Types In this section, we \npresent a thorough abstract description of the semantics of the two ingredients of our method: re\u00advisions \nand isolation types. Our descriptions are informal, but we include a formal revision calculus in the \nappendix for reference. 2.1 Revisions Revisions represent the basic unit of concurrency. They function \nmuch like asynchronous tasks that are forked and joined, and may themselves fork and join other tasks. \nWe chose the term revision to emphasize the semantic simi\u00adlarity to branches in source control systems, \nwhere program\u00admers work with a local snapshot of shared source code. There are two important differences \nbetween revisions and asynchronous tasks. First, we consider the main thread that is executing the program \nto be a revision as well, called the main revision. Thus, all code executes inside some well\u00adde.ned revision. \nSecond, the programmer must explicitly join all revisions that she forked. This contrasts with asyn\u00adchronous \ntasks for which the join is usually optional. 2.1.1 Revision Diagrams One of the most important aspects \nof revisions is that they provide a precise way to reason about how tasks may see or not see each others \neffects. To this end, we .nd it very helpful to visualize the concurrent control .ow using revi\u00adsion \ndiagrams (Fig. 2). Revisions correspond to vertical lines in the diagram, and are connected by horizontal \narrows that represent the forks and joins. We sometimes label the revi\u00adsions with the actions they perform. \nSuch diagrams visualize clearly how information may .ow (it follows the lines) and how effects become \nvisible upon the join. Note that our use of revision diagrams to reason about program executions is a \nmarked departure from traditional concurrency models such as sequentially consistent mem\u00adory or serializable \ntransactions, which reason about concur\u00adrent executions by considering a set of corresponding to\u00adtally \nordered sequential histories. These traditional models make the fundamental assumption that programmers \nmust think sequentially, and that all concurrency must thus be lin\u00adearized by some arbitration mechanism. \nHowever, such ar\u00adbitration invariably introduces nondeterminism, which may easily present a much larger \nproblem for programmers than direct reasoning about concurrent executions. versioned(int) x; versioned(int) \ny; x=0 y = 0; revision r = rfork { x = 1; } y = x; rjoin r; print x, y; Figure 2. An example of a revision \ndiagram (on the right) representing the execution of a program (on the left). Re\u00advisions correspond to \nvertical lines in the diagram, and are connected by horizontal arrows that represent the forks and joins. \nWe label the revisions with the actions they perform. In contrast, we reason directly about the concurrent \nexe\u00adcution by visualizing concurrent effects and isolation guar\u00adantees using revision diagrams, while \nhaving deterministic join results.  2.1.2 Nested Revisions To simplify modular reasoning about program \nexecutions, it is important to allow revisions to be nested. See Fig. 3 for examples on how revisions \nmay or may not be nested. On the left, we see a revision that forks its own inner revision, then joins \nit. This corresponds to classical nesting of tasks. In the middle, we show how an inner revision survives \nthe join of the revision that forked it, and gets subsequently joined by the main revision. On the right, \nwe show that not all diagrams we can draw are actually possible, because a join can only join revisions \nfor which it has a handle (the handle returned by the second fork becomes accessible to the main revision \nonly after the outer revision has been joined). Just like asynchronous tasks, revisions are a basic build\u00ading \nblock that can be used to express many different forms of concurrency or parallelism. Often, we may wish \nto .rst fork a number of revisions, and then immediately join all of them. This pattern is sometimes \ncalled the fork-join pat\u00adtern and is common for divide-and-conquer algorithms. Re\u00advisions are more general \nthough and their lifetime is not re\u00adstricted by the lexical scope, and can for example be used to model \nlong-running background tasks. Particularly, there is no implicit join at the end of each function as \nin the Cilk framework [11, 15, 33].  2.2 Isolation Types When joining revisions, we wish to merge the \ncopies of the shared data back together. Exactly how that should be done depends on what the data is \nrepresenting, which can not be easily inferred automatically. We thus ask the programmer to explicitly \nsupply this information by choosing an appro\u00adpriate type for the data. Figure 3. Revisions can be nested. \nInner revisions may be joined before or after the outer revision is joined (left, mid\u00addle). However, \nsome diagrams are impossible (right) because the main revision can not join a revision before it has \nac\u00adcess to its handle (the handle returned by the second fork becomes accessible to the main revision \nonly after the outer revision has been joined). Choosing the right isolation type for every single shared \nobject, .eld or variable may seem daunting at .rst. However, in our experience with parallelizing the \ngame application we found that just a few isolation types cover almost all situations. Our isolation \ntypes fall into the following two major categories: 1. Versioned types. When joining versioned types, \nwe .rst check whether the value has been modi.ed in the revision since it was forked. If not, we do nothing. \nOtherwise, we change the current value of the revision that is performing the join to the current value \nof the revision that is being joined (Fig.4). For a basic type T , we write versioned(T )for the corresponding \nversioned type. In our game application, versioned types were the most common case. They are a good \nchoice both for data on which concurrent modi.cations do not happen (many variables were concurrently \nwritten and read, but only a few were concurrently written to), or for situations in which there is clear \nrelative priority between tasks (in the sense that some tasks should override the effects of other tasks). \n 2. Cumulative types. When joining cumulative types, the combined effect of modi.cations is determined \nby a gen\u00aderal merge function. Such a function takes three values and returns a result value. The three \narguments are the original value (value at the time when the revision was forked), the master value (current \nvalue in the revision that performs the join), and the revised value (current value in the revision that \nis being joined). For a basic type T , we write cumulative(T,f) for the corresponding cu\u00admulative type \nwith merge function f.   int merge(int original, int master, int revised) { return master + revised \n- original; } which produces the desired result (Fig. 5). In our game application, we used cumulative \ntypes for collections (lists or sets) where tasks were adding ele\u00adments concurrently. An interesting \naspect of using isolation types is the ques\u00adtion of data granularity. Sometimes, the values of variables \nare correlated in the sense that they may be subject to some invariant. For example, valid values for \nthe coordinate vari\u00adables x and y may be restricted to the unit circle. Then, as\u00adsigning only one of \nthem may appear to locally preserve that invariant, while it is not globally preserved (Fig. 6). The \nso\u00adlution is either to always assign both variables, or to group them together using a composite type. \nOur de.nitions have been mostly informal and by exam\u00adple . For reference and to remove potential ambiguities, \nwe level (i.e. most con.icts do not express true data depen\u00addencies). The tasks have varying characteristics \n(I/O vs. CPU bound) and may exhibit unbounded latencies.  The tasks may need to react to external, nondeterministic \nevents such as user input or network communication.  In this section, we review common implementation \ntech\u00adniques and how they relate to revisions, provide a compari\u00adson between revisions and transactions, \nand discuss related work on isolation types. 3.1 Traditional Locking and Replication Sometimes, standard \nlocking schemes are appropriate for safely sharing data between tasks. However, locking com\u00adplicates \nthe code because it requires programmers to think about the placement of critical sections, which involves \nnon\u00adtrivial tradeoffs and complicates code maintenance. Moreover, locking alone does not always suf.ce. \nFor ex\u00adample, consider a game application which executes concur\u00adrently (1) a physics task which updates \nthe position of all game objects based on their speed and the time elapsed, and (2) a render task which \ndraws all objects onto the screen. Then, any solution based solely on locks would either ham\u00adper concurrency \n(too coarse) or provide insuf.cient isolation (too .ne), as some of the objects may be rendered at the \nfu\u00adture position, while others are rendered at the current posi\u00adtion. For this reason, replication is \noften a necessary ingredi\u00adent to achieve parallelization of application tasks. Games, for example, may \nmaintain two copies of the shared state (using so-called double-buffering) to guarantee isolation of \ntasks while enabling any number of read-only tasks to ex\u00adecute concurrently with a single writer task. \nHowever, this pattern is somewhat speci.c to the synchronization struc\u00adture of games, and maintaining \njust two buffers is not always enough (for example, there may be multiple concurrent mod\u00adi.cations, or \nsnapshots may need to persist for more than a single frame). Moreover, performing a full replication \nof the shared state is not the most space-ef.cient solution. Another common replication-based solution \nis to use im\u00admutable objects to encode shared state. Any tasks that wish to modify an immutable object \nmust instead create a copy. This pattern can ef.ciently guarantee isolation and enables concurrency. \nHowever, it can introduce new challenges, such as how to resolve con.icting updates, or how to bound \nspace requirements in situations where frequent modi.ca\u00adtions to the data may cause excessive copying. \nRevisions solve both of these problems by implicitly linking the copy\u00ading and merging to the concurrent \ncontrol .ow, and by using programmer-declared isolation types to resolve con.icts de\u00adterministically. \n 3.2 Related Work on Transactions Like revisions, transactions or transactional memory [16, 22] address \nthe problem of handling concurrent access to shared data. The key difference between transactions and \nrevisions is that transactions (whether optimistic or pessimistic) han\u00addle con.icts nondeterministically, \nwhile revisions resolve con.icts deterministically. Moreover, revisions do not guar\u00adantee serializability, \none of the hallmarks of transactions, but provide a different sort of isolation guarantee (as discussed \nabove in Section 2). See Fig. 7 for an example that highlights the semantic difference between revisions \nand transactions. Just as we do with revisions, proponents of transactions have long recognized that \nproviding strong guarantees such as serializability [31] or linearizability [19] can be overly conservative \nfor some applications, and have proposed alter\u00adnate guarantees such as multi-version concurrency control \n[30] or snapshot isolation (SI) [4]. SI transactions (for ex\u00adample, see SI-STM [34]) are similar to revisions \ninsofar as they operate on stable snapshots and do not guarantee serial\u00adizability. However, they are \nmore restricted as they do not perform deterministic con.ict resolution (but rather abort transactions \nin schedule-dependent and thus nondetermin\u00ad void foo() void bar() {{ if (y = 0) if (x = 0) x=1; y=1; \n }} Revisions and Isolation Types: versioned(int) x,y; x = 0; y = 0; revision r = rfork { foo() ; } \nbar(); rjoin r; assert(x = 1 . y = 1);  Transactions: int x,y; x = 0; y = 0; task t = fork { atomic \n{ foo(); }} atomic { bar(); } join t; assert((x = 1 . y = 0) . (x = 0 . y = 1));  Figure 7. Example \nillustrating the semantic difference be\u00adtween transactions and revisions. The assert statements in\u00addicate \nthe possible .nal values, which are different in each case. The transactional program has two possible \nexecutions, both of which are different from the single (deterministic) execution of the program that \nuses revisions and isolation types. istic way) and do not support nesting of transactions in a comparably \ngeneral manner. Optimistic transactions do not fare well in the presence of con.icts that cause excessive \nrollback and retry. Moreover, combining optimistic transactions with I/O can be done only under some \nrestrictions [39] because the latter cannot always be rolled back. None of these issues arises with revisions \nas they are not optimistic and never require rollback. 3.3 Related Work on Deterministic Concurrency \nRecently, researchers have proposed programming models for deterministic concurrency [6, 9, 32, 38]. \nThese models differ semantically from revisions, and are quite a bit more restrictive: as they guarantee \nthat the execution is equivalent to some sequential execution, they cannot easily resolve all con.icts \non commit (like revisions do) and must thus restrict tasks from producing such con.icts either statically \n(by type system) or dynamically (pessimistic with blocking, or opti\u00admistic with abort and retry). Also, \nunlike our revisions, some of these models[6, 9] allow only a restricted fork-join form of concurrency. \nHardware architects have also proposed supporting deter\u00administic execution[5, 12]. However, these mechanisms \nguar\u00adantee determinism only, not isolation.  3.4 Related Work on Isolation Types Isolation types are \nsimilar to Cilk++ hyperobjects [14]: both use type declarations by the programmer to change the se\u00admantics \nof shared variables. Cilk++ hyperobjects may split, hold, and reduce values. Although these primitives \ncan (if properly used) achieve an effect similar to revisions, they do not provide a similarly seamless \nsemantics. In particular, the determinacy guarantees are fragile, i.e. do not hold for all programs. \nFor instance, the following program may .nish with either x == 2 or x == 1: reducer opadd(int) x = 0; \ncilk spawn { x++ } if (x= 0) x++; cilk sync Isolation types are also similar to the idea of transactional \nboosting, coarse-grained transactions, and semantic commu\u00adtativity [18, 20, 21], which eliminate false \ncon.icts by rais\u00ading the abstraction level. Isolation types go farther though: for example, the type \nversioned(T ) does not just avoid false con.icts, but resolves true con.icts deterministically. Note \nthat isolation types do not suffer from the weak\u00advs. strong-atomicity problem [8] because all code executes \ninside some revision. The insight that automatic object replication can im\u00adprove performance also appears \nin work on parallelizing compilers[35].  3.5 Related Work on Fork-Join Models Once a revision is forked, \nits handle can be stored in arbi\u00adtrary data structures and be joined at an arbitrary later point of time. \nThe join is always explicitly requested by the pro\u00adgrammer: this is important as it has side effects. \nSome languages statically restrict the use of joins, to make stronger scheduling guarantees (as done \nin Cilk++ [15, 33]) or to simplify the most common usage patterns and to eliminate common user mistakes \n(as done in X10 [25]). In fact, many models use a restricted fork-join parallelism [6, 9]. In our experience, \nsuch restrictions (while reasonable for data-parallel problems) can make it dif.cult to write ap\u00adplications \nthat adapt to external nondeterminism or to unpre\u00addictable latencies. In our game, for example, we wish \nto run the autosave task in the background as it has unpredictable latency, rather than forcing a join \nat the end of the frame. 4. Case Study We now describe how we parallelized an example applica\u00adtion using \nrevisions and isolation types. We .rst describe the sequential game application and why parallelization \nis a challenge. Next, we describe how we used revisions to parallelize the game loop and how we wrapped \nthe shared data using isolation types. We also discuss how we address nondeterminism. Finally, we present \nexperimen\u00adtal results that evaluate the performance characteristics of using revisions and isolation \ntypes and measure the gains from parallelization. 4.1 The Game The game application is a multiplayer \ngame called Space-Wars3D; it was originally designed to teach DirectX pro\u00adgramming with C# [17]. We have \nused this same game to investigate parallelization in a preliminary case study, as re\u00adported in earlier \nresults [3]. Its conceptual architecture is de\u00adpicted in Fig. 8 (top left). The square boxes represent \ntasks of varying sizes and characteristics (CPU-bound or IO-bound). The code amounts to about 12,000 \nlines. There is ample op\u00adportunity for executing different tasks in parallel and for par\u00adallelizing individual \ntasks. The key challenge is to ensure that the data is concurrently available, yet remains consis\u00adtent. \nThe starting point is a completely sequential game loop design shown in Fig. 8 (bottom left). It suffers \nfrom some major performance issues:1 1. (not parallel enough) There is room to parallelize tasks. For \ninstance, the CollisionCheck(i) could be executed in parallel but are performed sequentially. Also, although \nthe render task RenderFrameToScreen cannot itself be par\u00adallelized (due to restrictions in the framework), \nit can ex\u00adecute in parallel with other tasks. 2. (not responsive enough) The periodic automatic SaveGame \ncall that occurs every 100 frames has unpredictable la\u00adtency, and causes annoying freezes in the game \nexperi\u00adence.  To improve the frame rate and make the gameplay smoother, we would like to .x the issues \nabove. However, there are nu\u00admerous con.icts between these tasks that we need to pay attention to. For \nexample, consider the coordinates of the game objects (like ships, bullets, asteroids, etc.). All of \nthe following tasks may potentially access these coordinates at the same time: RenderFrameToScreen reads \nthe position of all objects.  UpdateWorld modi.es the positions of all objects based on the elapsed \ntime.  CollisionCheck(i) reads the positions of all objects and may also modify some positions. These \nmodi.cations are supposed to override the updates done by UpdateWorld.  SendNetworkUpdates reads positions \nof local objects and sends them to the remote player.  HandleQueuedPackets receives updates from the \nremote player and modi.es positions of local objects. These  1 Note that we modi.ed the original game \n(by creating the asteroids and the autosave feature) for the speci.c purpose of causing performance issues \nfor illustration purposes. Our naive collision check, while not representative for well-designed games \nthat may employ the GPU and more sophisticated geometric partitioning, does illustrate the problem correctly. \n Revision UpWrl, SendNtw, HdlPckts, AutoSave; Revision[] ColDet = new Revision[physics.numsplits]; while \n(!done) { input.GetInput(); UpWrl = rfork { input.ProcessInput(); physics.UpdateWorld(); } for (int i \n= 0; i (physics.numsplits; i++) ColDet[i] = rfork { physics.CollisionCheck(i); } SendNtw = rfork { network.SendNetworkUpdates(); \n} HdlPckts = rfork { network.HandleQueuedPackets(); ) if (frame % 100 = 0 . AutoSave = null) while (!done) \nAutoSave = rfork { SaveGame(); }; { input.GetInput(); ProcessGuiEvents(); input.ProcessInput(); screen.RenderFrameToScreen(); \nphysics.UpdateWorld(); for (int i = 0; i (physics.numsplits; i++) join(UpWrl); physics.CollisionCheck(i); \nfor (int i = 0; i (physics.numsplits; i++) network.SendNetworkUpdates(); join ColDet[i]; network.HandleQueuedPackets(); \njoin(SendNtw); if (frame % 100 = 0) join(HdlPckts); SaveGame(); if (AutoSave = null . AutoSave.HasFinished()) \n{ ProcessGuiEvents(); join(AutoSave); screen.RenderFrameToScreen(); AutoSave = null; audio.PlaySounds(); \n} frame++; } audio.PlaySounds(); frame++; } Figure 8. (top left) Illustration of the conceptual game \narchitecture. (bottom left) Pseudocode for the sequential game loop. (right) Pseudocode for the parallel \nmain loop. updates are supposed to override the updates done by UpdateWorld. and by CollisionCheck(i). \nSaveGame reads the positions of all objects and saves them to disk (expecting a consistent snapshot). \nAll of the tasks are expected to work with a consistent view of the data. For instance, it is not acceptable \nto render half of the objects at the old position and the other half at the new position. Such consistency \nguarantees can be challenging to achieve without some form of support from the framework. Although tasks \nare sensitive to instability of the shared data, it is often acceptable to work with slightly stale data. \nFor example, we could move SendNetworkUpdates to the top of the loop without harm, because it would simply \nsend the positions of the last frame which is perfectly acceptable. This illustrates that the precise \nsemantics of the sequential game loop are not set in stone: parallelization may make slight changes as \nlong as the overall behavior of the game remains the same.  4.2 Parallelization We now describe the \nprocess we performed to parallelize the game. It involved two main steps: Parallelizing the game loop \nusing revisions, and declaring shared data using isola\u00adtion types. This process involved making choices \nthat require understanding of the semantics of the game: to achieve bet\u00adter parallelism, the parallel \nloop is not fully equivalent to the sequential loop, but only close enough . See Fig. 8 (right) for pseudocode2 \nrepresenting our par\u00adallel version of the game loop. All tasks are now inside con\u00adcurrent revisions, \nexcept for four tasks that have to remain on the main thread because of restrictions of the GUI and graphics \nframeworks. In each iteration, we fork revisions and store their han\u00addles. Each CollisionCheck(i) is \nin a separate revision. AutoSave only forks a revision every 100 frames, and only if there is 2 We show \npseudocode rather than the actual C# code for the sake of using a more concise syntax and omitting details \nunrelated to the point. not an autosave still in progress. After forking all revisions, the main thread \nperforms the render task and processes GUI events. Then it joins all the revisions; however, it joins \nthe autosave revision only if it has completed. Note that the con\u00adcurrent revisions are joined in an \norder such that con.icting updates are correctly prioritized (collision check overrides update, network \npackets override both). 4.2.1 Declaring Isolation Types We replaced a total of 22 types with isolation \ntypes. Iden\u00adtifying all the shared .elds was a matter of identifying the model state (the game follows \nvaguely a model-view\u00adcontroller architecture). Note that the majority of .elds and variables do not need \nto be versioned (for example, they may be readonly, or may never be accessed concurrently). Over\u00adall, \nwe used the following isolation types (we describe these types in more detail later, in Section 5.2), \nlisted in the order of frequency: VersionedValue(T) (13 instances). This was the most fre\u00adquently used \nisolation type, and the type T ranged over all kinds of basic types including integers, .oats, booleans, \nand enumerations.  VersionedObject(T) (5 instances). These were used for game objects such as photons, \nasteroids, particle effects, as well as for positions.  CumulativeValue(T) (3 instances). 2 instances \nwere used for sound .ags (which are essentially a bitmask imple\u00admentation of a set), and one was used \nfor a message buffer that displays messages on the screen.  CumulativeList(T) (1 instance). This was \nused for the list of asteroids; new asteroids are added when old ones burst, which happens on collisions. \n  4.2.2 Deterministic Record and Replay At an abstract level, concurrent revisions do guarantee deter\u00administic \nexecution for correctly synchronized programs (that is, programs that join each revision they fork exactly \nonce, and that do properly declare all shared data to have an isola\u00adtion type). In our parallel loop \n(Fig. 8 (right)) this guarantee does not hold completely, however, because we query whether the revision \nAutoSave has completed before joining it. Because timing varies between runs, this test does not always \nreturn the same result in each execution and thus introduces nonde\u00adterminism. This example showcases \nan important dilemma: if we want to enforce complete determinism, we cannot dy\u00adnamically adapt to unpredictable \nlatency variations. Thus, there is a fundamental tension between determinism and re\u00adsponsiveness. We \nobserve that there are in fact many sources of nonde\u00adterminism that quickly nullify deterministic execution \neven in the completely sequential game. Examples include user input, network packet timing, and random \nnumber genera\u00adtors. Thus, we decided to adjust our goal from deterministic execution to deterministic \nrecord and replay . By recording and replaying all sources of nondeterminism we can recover some of the \nbene.ts of determinism, such as a better de\u00adbugging experience. Note that record/replay of revisions \nis much easier than record/replay of standard shared-memory programs[24] because there are only a few \nordering facts that need to be recorded.  4.3 Quantitative Evaluation In this section we analyze the \nperformance characteristics of revisions in the parallelized SpaceWars3D game (Fig. 8, right). We are \nparticularly interested in the following ques\u00adtions: What is the overhead of executing tasks inside \nrevisions? This question is signi.cant because in a revision, every read of a shared location must look \nup the correct version, and every .rst write to a shared location must create a copy.  What is the overhead \nof forking and joining revisions? In particular, is it expensive to resolve con.icts during join?  What \nis the memory overhead of maintaining replicas of the shared state?  Did the gameplay experience improve? \nSpeci.cally, (a) how much did the frame rate increase, and (b) did we suc\u00adcessfully eliminate the freezes \ncaused by the automatic periodic save?  4.3.1 Methodology The evaluation methodology devised for the \nexperiments takes advantage of the ability to record and replay a game as pointed out in Section 4.2.2. \nAs the .rst step, we record 2500 frames of a network game session with two players chas\u00ading each other \nbut without any asteroids being destroyed. We call such game session a typical execution. We exclude \nasteroid explosions from our initial experiments since they agressively decrease the number of frames \nexecuted per sec\u00adond (see Section 4.3.4 for an analysis with varying number of asteroids). We also disabled \nauto-save when recording. After the game is saved, we replay it and examine only the last 2000 frames. \nThe .rst 500 frames are skipped so that the runtime system can be appropriately warmed up. Notice that \nwhen replaying a game to collect the results only one machine is actually used. Although no network packages \nare sent during replay, we do record the packages received and replay them appropriately. Unless stated \notherwise, all results are reported as the mean of .ve replays. Time measurements are performed with \nthe high-resolution performance counter provided by Windows, which is ac\u00adcurate within 383 nanoseconds \nin our workstation. While collecting timing information we only kept the game appli\u00adcation opened in \norder to reduce external interferences. All experiments were conducted on a 4-core machine, an HP Z400 \nworkstation running a 64-bit version of Windows 7 Enterprise, with 6GB of DDR3 RAM, a NVIDIA Quadro FX580 \n512MB graphics card, and a quad-core 2.66GHz Intel Xeon W3520 processor 3.  4.3.2 Revisions Overhead \nTo quantify the overhead of revisions in SpaceWars3D we recorded a typical execution of the game using \n800 asteroids. This number was chosen because it provided a reasonable frame rate on our workstation \n(around 54 FPS in the single\u00adthreaded case). We then replay the game and compare the results for two \ndistinct execution modes: (i) sequential, and (ii) sequential with revisions. Scenario (i) serves as \nthe sequential baseline to which revisions are compared. In scenario (ii), we enforce a sequential execution \nof the revisions by running them synchronously and performing the join operations at the end of the frame. \nThis mode allows us to accurately quantify the single-thread overhead incurred by revisions for each \ngame task (see Fig. 9 and 10). As can be seen from Fig. 9, the revisioning framework only caused a slowdown \nof 5% if compared to the sequential execution. Two observations are worth mentioning. Firstly, the collision \ndetection task (ColDet) accounts for approx\u00adimately 82% of the total execution time per frame. This task, \ntherefore, is the main target for parallelization as dis\u00adcussed in more detail in Section 4.3.4. Secondly, \nrevisions incur an extra overhead since it is necessary to perform a join operation for each revision \nforked. Note, however, that this overhead contributed only 1.2% of the total execution time, showing \nthat for the game application the join costs are mostly negligible. To gain further insight into the \nperformance of revisions we present the normalized execution time for each Space-Wars3D task in Fig. \n10, along with the number of versioned reads and writes issued by each one. Notice that for the tasks \ndominated by reads (Renderer and ColDet) the overhead is mininum. This clearly shows that our current \nimplementa\u00adtion is quite effective for read-based workloads. The remain\u00ading tasks suffer from versioned \nwrites in different degrees, most notably UpWrl (almost 2x). However, since these tasks do not have a \nhuge impact in the overall execution time, their cost is not a major concern in SpaceWars3D. For workloads \nwhere writing to versioned types dominates, we would ex\u00adpect higher overhead.  4.3.3 Memory Consumption \nEvery time a revision writes to a versioned object, the revi\u00adsioning subsystem needs to create a clone \nin order to enforce isolation. It is therefore important to quantify the overhead of maintaining replicas \nof the shared state. 3 Although hyperthreading is supported (totalizing 8 logical cores), this feature \nwas disabled during our measurements. Figure 9. Average frame time breakdown. The overall slow- Figure \n10. Normalized execution time for each task in SpaceWars3D. At the bottom, the number of versioned reads \nand writes performed by each task. Baseline for normaliza\u00adtion is the sequential execution. Table 1 presents \nthe number of bytes of managed mem\u00adory allocated by SpaceWars3D after a replay. We show the best available \napproximation as retrieved from a call to the .NET garbage collector method GetTotalMemory 4, for both \nthe sequential version and revisions. As expected, the total amount of memory allocated increases as \nmore as\u00adteroids are added to the game. The overhead of revisions slightly increases when compared to \nthe sequential version due to more versioned objects being read/written. 4 We allow the system to collect \ngarbage and .nalize objects before return\u00ading the number of bytes allocated. Figure 12. The frame rate \ndrops as more asteroids are added to the game. With 1300 asteroids, the sequential version is mostly \nunplayable (22 FPS), whereas concurrent revisions still provide smooth gameplay (61 FPS) on a quad-core \nmachine. # asteroids Sequential Revisions Overhead 800 1,162,588 1,578,660 1.36 900 1,199,388 1,654,100 \n1.38 1000 1,236,196 1,734,200 1.40 1100 1,277,092 1,814,296 1.42 1200 1,324,932 1,914,504 1.44 1300 1,361,732 \n1,991,304 1.46 1400 1,398,532 2,068,104 1.48 1500 1,435,332 2,144,904 1.49 Table 1. Comparison between \nallocated managed memory with and without revisions (approximate value in bytes), for a varying number \nof asteroids.  4.3.4 Parallel Performance We now proceed to analyze the performance of the game when \nexecuting the revisions concurrently on our quadcore machine. This performance depends on exactly how \ntasks are scheduled in each frame, which can vary quite a bit (our runtime does not perform the scheduling \nitself, but relies on an off-the-shelf dynamic task scheduler). We show a typical frame schedule in Fig. \n11. Time proceeds from left to right, and the bars indicate when a task begins and ends. We also show \nbars for all the joins performed. Note that the bars do not show which tasks are currently scheduled \nvs. waiting to be scheduled, thus there are sometimes more than 4 tasks active at a time even though \nthere are only 4 cores. The schedule shown corresponded to parameters under which we achieved an average \nspeedup of 2.6x relative to the sequential baseline. How good is a speedup of 2.6x on 4 cores? Not bad \nfor the circumstances. As we can see, the frame rate is limited mostly by the render task, which takes \nabout 95.5% of the total frame time and can not be parallelized. Thus, even if everything else took zero \ntime our speedup could not be better than 2.6 * 100/95.5=2.72. By increasing the number of asteroids, \nwe change the pro\u00adportion of the program that can be parallelized and achieve better speedups (Fig. 12), \nup to 3.03 for 1500 asteroids. We believe that even better speedups are possible with our li\u00adbrary, for \nthe following reason. Assuming a workload that is fully parallelizable except for the joins which amount \nto about 1.5%, Amdahl predicts a maximal speedup on 4 cores of (1/(1 - 0.985 + 0.985/4) = 3.83, so once \nwe account for the average execution overhead of 5% the best possible speedup is still 3.83 * 0.95 = \n3.64. In this example, we do not achieve such good speedups even if we increase the num\u00adber of asteroids \nto increase the proportion of parallelizable work. We are not sure why, but observe an unexpected slow\u00adness \nof the render task for which we believe some form of contention (outside of our runtime library) to be \nresponsible. class Versioned(T) : Versioned { Map(int,T) versions; ... } class Segment { int version; \nclass Revision { int refcount; Segment root; Segment parent; Segment current; List(Versioned) written; \n... ... } } Figure 13. A quick overview of the classes our algorithm is built on, and how they relate. \n Versioned(int) x = 0; x=0 Versioned(int) y = 0; y=0 1 Revision r = rfork { y = 1;b rfork { ... }x \n= y;c } x = 2 a b y = 1  - ... x = 2;a rjoin r; 1 . 1 c x = y Figure 14. Example revision diagram. \n Besides speeding up the average frame rate, our paral\u00adlelization also improved the responsiveness: \nunlike in the se\u00adquential version of the game, the periodic automatic save had no perceptible effect \non the gameplay. 5. Implementation We now describe how we actually implemented revisions and isolation \ntypes. The key design principles of our algo\u00adrithm are: The (amortized) cost of a Get or Set operation \non a versioned object must be very ef.cient, as it is called every time a revision accesses shared data. \n To save time and space, we must not copy data eagerly (such as on every fork), but lazily and only \nwhen nec\u00adessary (that is, when the write may invalidate somebody else s snapshot).  We must release \ncopies that are no longer needed as soon as possible.  In this section we .rst describe our algorithm \nthat satis.es these requirements, starting with a stripped-down version in pseudo object-oriented code. \nThen we describe various extensions and optimizations that we used to implement our C# library.  Segment \nversion= 0 written= {x, y} parent1 b parent  root root parent a Segment version= 2 written= {y} 1 \n parent - ... 1 Revision main current t Segment version= 1 written= {x}1 . 1 Segment version= 3 written= \n{x} ccurrent   1Revision r Figure 15. The state of our implementation for the example in Figure 14, \nright before the rjoin r statement. For illustra\u00adtion purposes, We show the Segment objects superimposed \non the respective segments of the revision diagram. 5.1 The Essential Algorithm See Fig. 13 for a quick \noverview of the three classes we use and how they relate. We give detailed listings of these classes \nin Figures 16, 17 and 18, but .rst discuss the high-level idea. Revision objects represent revisions \nas de.ned earlier. Re\u00adcall that in revision diagrams, revisions are the vertical lines, which consist \nof one or more line segments sepa\u00adrated by forks or joins. Revision objects are created when a revision \nis forked, and released after a revision is joined. Each revision object has a current segment (the currently \nlast segment of this revision) and a root segment (the line segment right above the fork that created \nthis revision). Segment objects correspond to vertical line segments in the revision diagrams, and are \nuniquely identi.ed by their version number (the .eld version). Segment objects form a tree (by parent \npointer). Segment objects are created when line segments are added (each fork creates two new segments, \neach join creates one new segment), and are released when refcount reaches zero. Segment objects also \nmaintain a list of all Versioned objects5 that were written to in this segment. 5 The Versioned(T) class \nderives from the Versioned class so we can create a homogenous list of non-generic versioned objects. \n Versioned objects contain a versions map that stores sev\u00aderal versions of the data, indexed by version \nnumbers. It stores for each line segment the last value written to this object in that segment, or the \nspecial value . to signify that there was no write to this object in that segment. In the next few subsections, \nwe are going to discuss several aspects of our implementation in more detail. First though, we show a \nsmall example program and its revision diagram in Fig. 14, which will serve as a running illustration \nexample. We labeled some of the segments with a, b, and c, and assume they will have version numbers \n1, 2, and 3 respectively. To explain the design of the algorithm, we now discuss the state of our implementation \nright before the rjoin r statement. At that point, we have two Revision objects and .ve Segment objects, \nand they are related as shown in Fig. 15. At the same point of time, the versions map for the variable \nx is {0 . 0, 1 . 2, 3 . 1}, and the map for y is {0 . 0, 2 . 1}. As we can see, only the last writes \nto a value are in the versions map; i.e. even though y is read in the c edge, there is no entry for version \n3 in the versions map of y. 5.1.1 Accessing Versioned Data To access versioned data, we use the public \nGet and Set methods in Fig. 16. These methods .rst consult the thread\u00adlocal static .eld Revision.currentRevision \n(see Fig. 17) to au\u00adtomatically .nd the correct revision for the current thread. The Get method then \nreturns the current value associated with a particular revision. It cannot just return the content of \nversions[r.current.version] since only the last write is stored in this map. If the revision has not \nwritten to this particular object, we need to follow the parent chain to .nd the last write. The Set \nmethod sets the current value for a particular revision. It .rst looks to see if the entry for the current \nsegment is uninitialized. If so, it adds this versioned object to the written list of the segment, before \nwriting the new value to the map.  5.1.2 Fork The Fork operation (Fig. 17) starts with creating a fresh \nrevision r for the forked off branch. We .rst create a new revision using our current segment as its \nroot, and then create a new current segment. For example, in Fig. 15 we create segments with version \nnumbers 1 and 2. After creating a new revision r, we create a new concurrent task that assigns the new \nrevision to the thread local currentRevision. Here we assume that Task.StartNew starts a new concurrent \ntask with the provided action delegate (anonymous function). Light\u00adweight concurrent tasks based on work \nstealing are provided by .NET 4.0; on other frameworks we can use any similar kind of way to start concurrent \nthreads [23, 26, 28]. Finally, the new revision r is returned such that it can be joined upon later. \n class Versioned {void Release(); void Collapse(Revision main, Segment parent); void Merge(Revision main, \nRevision joinRev, Segment join); } public class Versioned(T) : Versioned {Map(int,T) versions; // map \nfrom version to value public T Get() { return Get(Revision.currentRevision); }public void Set(T v) { \nSet(Revision.currentRevision, v); } T Get(Revision r) { Segment s = r.current; while (versions[s.version] \n= .) { s = s.parent; } return versions[s.version]; } void Set(Revision r, T value) { if (versions[r.current.version] \n= .) { r.current.written.Add(this); } versions[r.current.version] = value; } void Release( Segment release \n) { versions[release.version] = .; } void Collapse( Revision main, Segment parent ) { if (versions[main.current.version] \n= .) { Set(main, versions[parent.version]); } versions[parent.version] = .; } void Merge(Revision main, \nRevision joinRev, Segment join) {Segment s = joinRev.current; while (versions[s.version] = .) { s = s.parent; \n} if (s = join) { // only merge if this was the last write Set(main, versions[join.version]);  }}} Figure \n16. The Versioned class.  5.1.3 Join The Join operation (Fig. 17) .rst waits till the associated con\u00adcurrent \ntask of the revision is done. Note that if an exception is raised in the concurrent task, it is re-raised \nin the call to Wait and in that case we will not merge any changes. When Wait succeeds, the actual written \nobjects in the join revision are merged. public class Revision {Segment root; Segment current; Task \ntask; threadlocal static Revision currentRevision; Revision( Segment root, Segment current ) { this.root \n= root; this.current = current; } public Revision Fork( Action action ) {Revision r; r= new Revision(current, \nnew Segment(current)); current.Release(); // cannot bring refcount to zero current = new Segment(current); \ntask = Task.StartNew( delegate (){ Revision previous = currentRevision; currentRevision = r; try { action(); \n} .nally { currentRevision = previous; } }); return r; } public void Join(Revision join) { try { join.task.Wait(); \nSegment s = join.current; while (s = join.root) { foreach (Versioned v in s.written) {v.Merge(this,join,s); \n} s = s.parent; } } .nally { join.current.Release(); current.Collapse(this); }}} Figure 17. The Revisioned \nclass. In a while loop, we visit each segment from join.current upto its root. If we look at our example \nin Figure 15, joining on r would visit the segments with versions 3 and 2. Indeed, together the written \nlists of those segments contain all objects that need to be merged back. For each segment, we iterate \nover all written objects and call their Versioned(T).Merge method with three arguments: the main revision, \nthe joined revision, and the current segment. When we look at the implementation of that method in Figure \n16 we see that it .rst .nds the .rst segment that wrote to this object. Only if the merged segment join \nis the same will we do a merge. If the merged segment is not equal, it means that that segment did not \ndo the last write to that object and we should not class Segment {Segment parent; int version; int refcount; \nList(Versioned) written; static int versionCount = 0; Segment( Segment parent ) { this.parent = parent; \nif (parent = null) parent.refcount++; written = new List(Versioned)(); version = versionCount++; refcount \n= 1; } void Release() { if (--refcount = 0) { foreach (Versioned v in written) { v.Release(this); } \nif (parent = null) parent.Release(); } } void Collapse( Revision main ) { // assert: main.current = \nthis while (parent = main.root . parent.refcount = 1) {foreach (Versioned v in parent.written) {v.Collapse(main,parent); \n} parent = parent.parent; // remove parent }}} Figure 18. The Segment class. For the purpose of reference \ncounting, we assume that ++ and --are atomic operations. merge older versions. If this happens to be \nthe last write, we merge by simply overwriting the value in the main revision (if it exists). Finally, \nthe Join function releases the reference count on the joined revision, and calls Collapse on our current \nsegment. We describe these situations in more detail in the following two sections. 5.1.4 Releasing \nSegments Each Segment object (Fig. 18) maintains a refcount to keep track of how many parent and current \n.elds are pointing at that segment (it does not count the root .elds). The Release method is called by \nrevisions to decrease the reference count, and whenever the reference count drops to zero, we can release \nany objects referenced by this version. Since only written objects are stored in the versions map of \nVersioned(T), the objects referenced by the version of the segment are exactly those that are in its \nwritten list. The Release method calls the Versioned(T).Release method on each of the objects in its \nwritten list and then releases its parent segment. When we look at the Versioned(T).Release method in \nFigure 16 we see that it simply clears the entry for that object in the versions map. In our example \nin Figure 15 the segment with version 3 will be released and the versions map of x will become {0 . 0, \n1 . 1} after the join. Note that the map for y becomes {0 . 0, 1 . 1, 2 . 1} since the segment for version \n2 is not released as the inner forked revision could potentially still refer to that version of y.  \n5.1.5 Collapsing Segments The Collapse method (Fig. 18) is only ever called on some current segment and \nit is the case that main.current = this when this method is called from Revision.Join. The Collapse method \ntries to merge the parent segment into the current segment. In particular, when we join on some revision, \nwe might .nd that our parent segment has a reference count of 1 and that we are the only one holding \non to it. By collapsing with that segment we both reduce the chain of segments (which improves reads), \nbut more importantly, we might release older versions of objects that are never referenced again. The \nCollapse operations ensures that we do not leak memory over time. Collapse visits the parent recursively \nwhile the reference count is 1. For each written object in the parent we call Versioned(T).Collapse on \nthat object with the current revi\u00adsion and the parent segment. After visiting each written ob\u00adject, we \noverwrite the parent .eld with the parent of the par\u00adent, effectively removing the parent segment (which \nis now collapsed into the current revision). The implementation of Versioned(T).Collapse is shown in \nFigure 16. If the current revision has not written this object yet, we set it to the value of the parent \nrevision. Finally, the parent version is cleared releasing its reference.  5.2 Additional Isolation \nTypes Our presentation of the algorithm includes the single iso\u00adlation type Versioned(T) only. This type \nis actually called VersionedValue(T) in our library, which contains a variety of isolation types. For \nexample, the type CumulativeValue(T)enables users to specify a speci.c merge function. This merge function \nneeds to know the original snapshot value, which our implementation can access by following the Revision.root \npointer of the revision being joined. For reference values, we implement VersionedObject(T)and CumulativeObject(T), \nwhich version all .elds of an ob\u00adject as a whole (cf. Fig. 6). To access such objects, Get and Set are \nnot appropriate, but we use similar operations T GetForRead( Revision r ); T GetForWrite( Revision r \n); where GetForRead is used to get a readonly reference to an object, while GetForWrite is used to get \na mutable version of the object. Ideally, if natively supported by a language, the use of these operations \ncould be hidden and inserted automatically by the compiler. Beyond those isolation types, our library \nalso supports the cumulative collection classes CumulativeList(T) and CumulativeSet(T) with their natural \nmerge functions, and a VersionedRandom class that serves as a deterministic pseu\u00addorandom generator. \n 5.3 Optimization We employ a number of optimizations: We use a specialized mostly lock-free implementation \nfor the versions map. It uses arrays that may be resized if necessary.  To further speed up the Get \noperation, we maintain a cache that contains the version and corresponding index of the last read or \nwrite to this object. It is implemented as a 32 bit word that contains a version number in the lower \n16 bits, and an index in the upper 16 bits. By keeping it the size of a word, we can atomically read \nand write this cache without using locks.  When forking a new revision, we .rst check if the current \nsegment contains any writes. If not, it can stay the current segment, and we can use its parent as the \nparent of the new segment.  When merging objects, we can distinguish many special cases that can be \nhandled a bit faster. In our optimized implementation, the Versioned(T).Merge function is the most complicated \npart, consisting of eight separate cases. Partly the complexity is due to the application of merge functions \nfor cumulative objects, and partly because we release slots directly during the merge and try to reuse \nand update slots in-place whenever possible.  6. Conclusion and Future Work We have presented a novel \nprogramming model based on re\u00advisions and isolation types. First, we explained what guaran\u00adtees it provides, \nusing revision diagrams as the basic means of reasoning. Then we demonstrated how an example game application \ncan take advantage of this model. Finally, we elaborated on how to build an ef.cient runtime library. \nOur results show that revisions and isolation types pro\u00advide an elegant yet ef.cient mechanism for executing \ndiffer\u00adent tasks within a reactive or interactive application. As future work, we would like to apply \nrevisions and iso\u00adlation types to more general settings, such as applications that execute some tasks \non the GPU, applications that run on many-core processors without full shared-memory guaran\u00adtees, or \napplications that run in the cloud. Acknowledgments We thank Tom Ball, Ben Zorn, and Tim Harris for helpful \ncomments and discussions. References [1] S. Aditya, Arvind, L. Augustsson, J.-W. Maessen, and R. Nikhil. \nSemantics of pH: A Parallel Dialect of Haskell. In Paul Hudak, editor, Proc. Haskell Workshop, La Jolla, \nCA USA, pages 35 49, June 1995. [2] E. Allen, D. Chase, C. Flood, V. Luchangco, J.-W. Maessen, S. Ryu, \nand G. Steele Jr. Project fortress: A multicore lan\u00adguage for multicore processors. In Linux Magazine, \nSeptem\u00adber 2007. [3] A. Baldassin and S. Burckhardt. Lightweight software trans\u00adactions for games. In \nWorkshop on Hot Topics in Parallelism (HotPar), 2009. [4] H. Berenson, P. Bernstein, J. Gray, J. Melton, \nE. O Neil, and P. O Neil. A critique of ANSI SQL isolation levels. In Proceedings of SIGMOD, pages 1 \n10, 1995. [5] T. Bergan, O. Anderson, J. Devietti, L. Ceze, and D. Gross\u00adman. Coredet: A compiler and \nruntime system for determin\u00adistic multithreaded execution. In Architectural Support for Programming Languages \nand Operating Systems (ASPLOS), 2010. [6] E. Berger, T. Yang, T. Liu, and G. Novark. Grace: Safe mul\u00adtithreaded \nprogramming for c/c++. In Object-Oriented Pro\u00adgramming, Systems, Languages, and Applications (OOPSLA), \n2009. [7] G. Blelloch, S. Chatterjee, J. Hardwick, J. Sipelstein, and M. Zagha. Implementation of a portable \nnested data-parallel language. Journal of Parallel and Distributed Computing, 21(1):4 14, April 1994. \n[8] C. Blundell, E. Lewis, and M. Martin. Deconstructing trans\u00adactions: The subtleties of atomicity. \nIn Workshop on Dupli\u00adcating, Deconstructing, and Debunking (WDDD), 2005. [9] R. Bocchino, V. Adve, D. \nDig., S. Adve, S. Heumann, R. Ko\u00admuravelli, J. Overbey, P. Simmons, H. Sung, and M. Vakilian. A type \nand effect system for deterministic parallel java. In Object-Oriented Programming, Systems, Languages, \nand Ap\u00adplications (OOPSLA), 2009. [10] S. Burckhardt and D. Leijen. Semantics of concurrent revi\u00adsions \n(full version). Technical Report MSR-TR-2010-94, Mi\u00adcrosoft, 2010. [11] J. Danaher, I. Lee, and C. Leiserson. \nThe jcilk language for multithreaded computing. In Synchronization and Concur\u00adrency in Object-Oriented \nLanguages (SCOOL), San Diego, California, October 2005. [12] J. Devietti, B. Lucia, L. Ceze, and M. Oskin. \nDMP: Determin\u00adistic shared-memory multiprocessing. Micro, IEEE, 30(1):40 49, jan.-feb. 2010. [13] C. \nFlanagan and M. Felleisen. The semantics of future and its use in program optimization. In Rice University, \npages 209 220, 1995. [14] M. Frigo, P. Halpern, C. E. Leiserson, and S. Lewin-Berlin. Reducers and other \ncilk++ hyperobjects. In Symposium on Parallel Algorithms and Architectures (SPAA), pages 79 90, 2009. \n[15] M. Frigo, C. Leiserson, and K. Randall. The implementation of the Cilk-5 multithreaded language. \nIn Programming Lan\u00ad guage Design and Implementation (PLDI), pages 212 223. ACM, 1998. [16] T. Harris, \nA. Cristal, O. Unsal, E. Ayguad\u00b4e, F. Gagliardi, B. Smith, and M. Valero. Transactional memory: An overview. \nIEEE Micro, 27(3):8 29, 2007. [17] E. Hatton, A. S. Lobao, and D. Weller. Beginning .NET Game Programming \nin C#. Apress, 2004. [18] M. Herlihy and E. Koskinen. Transactional boosting: a methodology for highly-concurrent \ntransactional objects. In Principles and Practice of Parallel Programming (PPoPP), pages 207 216, 2008. \n[19] M. Herlihy and J. Wing. Linearizability: a correctness condi\u00adtion for concurrent objects. ACM Trans. \nProgram. Lang. Syst., 12(3):463 492, 1990. [20] E. Koskinen, M. Parkinson, and M. Herlihy. Coarse-grained \ntransactions. In Principles of Programming Languages (POPL), pages 19 30, 2010. [21] M. Kulkarni, K. \nPingali, B. Walter, G. Ramanarayanan, K. Bala, and L. Chew. Optimistic parallelism requires abstrac\u00adtions. \nIn Programming Language Design and Impl. (PLDI), 2007. [22] J. Larus and R. Rajwar. Transactional Memory. \nMorgan &#38; Claypool Publishers, 2007. [23] D. Lea. A java fork/join framework. In Java Grande, pages \n36 43, 2000. [24] D. Lee, B. Wester, K. Veeraraghavan, S. Narayanasamy, P. Chen, and J. Flinn. Respec: \nEf.cient online multiproces\u00adsor replay via speculation and external determinism. In Archi\u00adtectural Support \nfor Programming Languages and Operating Systems (ASPLOS), 2010. [25] J. Lee and J. Palsberg. Featherweight \nx10: a core calculus for async-.nish parallelism. In Principles and Practice of Parallel Programming \n(PPoPP), 2010. [26] D. Leijen, W. Schulte, and S. Burckhardt. The design of a task parallel library. \nIn Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA), 2009. [27] A. Martin, \nA. Birrell, T. Harris, and M. Isard. Semantics of transactional memory and automatic mutual exclusion. \nIn Principles of Programming Languages (POPL), pages 63 74, 2008. [28] Microsoft. Parallel extensions \nto .NET. http://msdn. microsoft.com/en-us/concurrency, June 2009. [29] L. Moreau. The semantics of scheme \nwith future. In In In ACM SIGPLAN International Conference on Functional Programming (ICFP 96, pages \n146 156, 1996. [30] P.A.Bernstein and N.Goodman. Multiversion concurrency control theory and algorithms. \nACM Trans. Database Syst., 8(4):465 483, 1983. [31] P.A.Bernstein, V.Hadzilacos, and N.Goodman. Concurrency \nControl and Recovery in Database Systems. Addison-Wesley, 1987. [32] P. Pratikakis, J. Spacco, and M. \nHicks. Transparent proxies for java futures. SIGPLAN Not., 39(10):206 223, 2004. [33] K. Randall. Cilk: \nEf.cient Multithreaded Computing. PhD thesis, Department of Electrical Engineering and Computer Science, \nMassachusetts Institute of Technology, May 1998. [34] T. Riegel, C. Fetzer, and P. Felber. Snapshot isolation \nfor software transactional memory. In Workshop on Transactional Computing (TRANSACT), 2006. [35] M. Rinard \nand P. Diniz. Eliminating synchronization bottle\u00adnecks in object-based programs using adaptive replication. \nIn International Conference on Supercomputing, 1999. [36] V. Saraswat, V. Sarkar, and C. von Praun. X10: \nconcurrent programming for modern architectures. In Principles and Practice of Parallel Programming (PPoPP), \n2007. [37] G. Steele. Parallel programming and parallel abstractions in fortress. In Invited talk at \nthe Eighth International Symposium on Functional and Logic Programming (FLOPS), April 2006. [38] A. Welc, \nS. Jagannathan, and A. Hosking. Safe futures for java. In Object-Oriented Programming, Systems, Languages, \nand Applications (OOPSLA), pages 439 453, 2005. [39] A. Welc, B. Saha, and A.-R. Adl-Tabatabai. Irrevocable \ntrans\u00adactions and their applications. In Symposium on Parallel Al\u00adgorithms and Architectures (SPAA), \npages 285 296, 2008. A. Revision Calculus For reference and to remove potential ambiguities, we now present \na formal calculus for revisions and isolation types. It is based on a similar calculus introduced by \nprior work on AME (automatic mutual exclusion) [27]. Before looking at the calculus, let us introduce \na few notations we use to work with partial functions. For sets A, B, we write A-B for the set of partial \nfunctions from A to B. For f, g . A-B, a . A, b . B, and A' . A, we adopt the following notations: f(a)= \n. means a/. domf, f[a . b] is the partial function that is equivalent to f except that f(a)= b, and f \n::g is the partial function that is equivalent to g on dom g and equivalent to f on A \\ dom g. In our \ntransition rules, we use patterns of the form f(a1 : b1) ... (an : bn) (where n = 1)) to match partial \nfunctions f that satisfy f(ai)= bi for all 1 = i = n. We show the syntax and semantics of our calculus \ncon\u00adcisely in Fig. 19. The syntax (top left) represents a standard functional calculus, augmented with \nreferences. References can be created (ref e), read ( !e) and assigned (e := e). The result of a fork \nexpression rfork e is a revision identi.er from the set Rid, and can be used in a rjoin e expression \n(note that e is an expression, not a constant, thus the revision being joined can vary dynamically). \nTo de.ne evaluation order within an expression, we syn\u00adtactically de.ne execution contexts (Fig. 19 right \ncolumn, in the middle). An execution context C is an expression with a hole , and as usual we let C[e'] \nbe the expression obtained ' from C by replacing the hole with e. The operational semantics (Fig. 19, \nbottom) describes transitions of the form s .r s' which represent a step by revision r from global state \ns to global state s'. Consider .rst the de.nition of global states (Fig. 19, top right). A global state \nis de.ned as a partial function from revision identi.ers to local states: there is no shared global state. \nThe local state has three parts (s, t, e): the snapshot s is a partial function that represents the initial \nstate that this revision started in, the local store t is a partial function that represents all the \nlocations this revision has written to, and e is the current expression. The rules for the operational \nsemantics (Fig. 19, bottom) ' all follow the same general structure: a transition s .r smatches the \nlocal state for r on the left, and describes how the next step of revision r changes the state. The .rst \nthree rules (apply, if-true, if-false) re.ect stan\u00addard semantics of application and conditional. They \naffect only the local expression. The next three rules (new, get, set) re.ect operations on the store. \nThus, they affect both the lo\u00adcal store and the local expression. The (new) rule chooses a fresh location \n(we simply write l ./s to express that l does not appear in any snapshot or local store of s). The last \ntwo rules re.ect synchronization operations. The rule (fork) starts a new revision, whose local state \nconsists of (1) a snap\u00adshot that is initialized to the current state s :: t, (2) a local store that is \nthe empty partial function, and (3) an expression that is the expression supplied with the fork. Note \nthat (fork) chooses a fresh revision identi.er (we simply write r/. s to express that r is not mapped \nby s, and does not appear in any snapshot or local store of s). The rule (join) updates the lo\u00adcal store \nof the revision that performs the join by merging the snapshot, master, and revision states (in accordance \nwith the declared isolation types), and removes the joined revision. It can only proceed if the revision \nbeing joined has executed all the way to a value (which is ignored). As usual, we de.ne the step relation \n. to be the union of the local step relations .r. We call a global state s an initial state if it is \nof the form s = {(r, (E, E,e)}. We call a sequence of steps s0 . s1 . \u00b7\u00b7\u00b7 . sn an execution if s0 is \nan initial state, and maximal if there exists no s' such that ' sn . s. We refer readers that are interested \nin additional details to our Tech Report [10] where we discuss various properties of variations of this \ncalculus, such as determinacy, and that the revision diagrams are semilattices. State Syntactic Symbols \ns . GlobalState = Rid -LocalState v . Value = c | l | r | .x.e LocalState = Snapshot \u00d7 LocalStore \u00d7 Expr \nc . Const = unit | false | true s . Snapshot = Loc -Value l . Loc t . LocalStore = Loc -Value r . Rid \nx . Var Execution Contexts e . Expr = v | x C = [] | ee | (e ? e : e) |C e | v C| (C ? e : e) | ref \ne | !e | e := e | ref C| !C|C := e | l := C | rfork e | rjoin e | rjoin C Operational Semantics (apply) \ns(r : (s, t, C[.x.e v])) .r s[r .(s, t, C[[v/x]e])] (if-true) s(r : (s, t, C[(true ? e : e ')])) .r s[r \n.(s, t, C[e])] (if-false) s(r : (s, t, C[(false ? e : e ')])) .r s[r .(s, t, C[e '])] (new) s(r : (s, \nt, C[ref v])) .r s[r .(s, t [l . v], C[l])] if l/. s (get) s(r : (s, t, C[!l])) .r s[r .(s, t, C[(s::t \n)(l)])] if l . dom s::t (set) s(r : (s, t, C[l := v])) .r s[r .(s, t [l . v], C[unit])] '' (fork) s(r \n: (s, t, C[rfork e])) .r s[r .(s, t, C[r '])][r .(s::t, E, e)] if r ./s '' ' (join) s(r : (s, t, C[rjoin \nr ']))(r : (s ' ,t ,v)) .r s[r .(s, merge(s ' , t, t '), C[unit])][r ..] . . t (l) if t '(l)= . where \nmerge(s ' , t, t ')(l)= t '(l) if t '(l) = . and l is of type versioned(T ) . f(s '(l),t(l),t '(l)) \nif t '(l)= . and l is of type cumulative(T,f) Figure 19. Syntax and Semantics of the revision calculus. \n   \n\t\t\t", "proc_id": "1869459", "abstract": "<p>Building applications that are responsive and can exploit parallel hardware while remaining simple to write, understand, test, and maintain, poses an important challenge for developers. In particular, it is often desirable to enable various tasks to read or modify shared data concurrently without requiring complicated locking schemes that may throttle concurrency and introduce bugs.</p> <p>We introduce a mechanism that simplifies the parallel execution of different application tasks. Programmers declare what data they wish to share between tasks by using <i>isolation</i> types, and execute tasks concurrently by forking and joining <i>revisions</i>. These revisions are isolated: they read and modify their own private copy of the shared data only. A runtime creates and merges copies automatically, and resolves conflicts deterministically, in a manner declared by the chosen isolation type.</p> <p>To demonstrate the practical viability of our approach, we developed an efficient algorithm and an implementation in the form of a C# library, and used it to parallelize an interactive game application. Our results show that the parallelized game, while simple and very similar to the original sequential game, achieves satisfactory speedups on a multicore processor.</p>", "authors": [{"name": "Sebastian Burckhardt", "author_profile_id": "81350574118", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P2354181", "email_address": "", "orcid_id": ""}, {"name": "Alexandro Baldassin", "author_profile_id": "81309506860", "affiliation": "State University of Campinas, Brazil, Campinas, Brazil", "person_id": "P2354182", "email_address": "", "orcid_id": ""}, {"name": "Daan Leijen", "author_profile_id": "81100572466", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P2354183", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1869459.1869515", "year": "2010", "article_id": "1869515", "conference": "OOPSLA", "title": "Concurrent programming with revisions and isolation types", "url": "http://dl.acm.org/citation.cfm?id=1869515"}