{"article_publication_date": "10-17-2010", "fulltext": "\n Hera-JVM: A Runtime System for Heterogeneous Multi-Core Architectures Ross McIlroy * Joe Sventek Microsoft \nResearch Cambridge University of Glasgow rmcilroy@microsoft.com joe@dcs.gla.ac.uk Abstract Heterogeneous \nmulti-core processors, such as the IBM Cell processor, can deliver high performance. However, these processors \nare notoriously dif.cult to program: different cores support different instruction set architectures, \nand the processor as a whole does not provide coherence between the different cores local memories. We \npresent Hera-JVM, an implementation of the Java Virtual Machine which operates over the Cell processor, \nthereby making this platforms more readily accessible to mainstream developers. Hera-JVM supports the \nfull Java language; threads from an unmodi.ed Java application can be simultaneously executed on both \nthe main PowerPC\u00adbased core and on the additional SPE accelerator cores. Mi\u00adgration of threads between \nthese cores is transparent from the point of view of the application, requiring no modi.ca\u00adtion to Java \nsource code or bytecode. Hera-JVM supports the existing Java Memory Model, even though the underly\u00ading \nhardware does not provide cache coherence between the different core types. We examine Hera-JVM s performance \nunder a series of real-world Java benchmarks from the SpecJVM, Java Grande and Dacapo benchmark suites. \nThese benchmarks show a wide variation in relative performance on the dif\u00adferent core types of the Cell \nprocessor, depending upon the nature of their workload. Execution of these benchmarks on Hera-JVM can \nachieve speedups of up to 2.25x by using one of the Cell processor s SPE accelerator cores, compared \nto execution on the main PowerPC-based core. When all six SPE cores are exploited, parallel workloads \ncan achieve speedups of up to 13x compared to execution on the single PowerPC core. Categories and Subject \nDescriptors C.1.3 [Processor Ar\u00adchitectures]: Other Architecture Styles Heterogeneous (hy\u00adbrid) systems; \nD.3.4 [Programming Languages]: Proc\u00adessors Run-time environments. General Terms Design, Languages, Performances. \n* Work performed while at the University of Glasgow. Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. 1. Introduction Commodity microprocessors are providing increasing \nnum\u00adbers of cores to improve their performance, as issues such as memory access latency, energy dissipation \nand instruction level parallelism limit the performance improvements that can be gained by a single core. \nCurrent commodity multi\u00adcore processors are symmetric, with each processing core being identical. This \nkind of architecture provides a simple platform on which to build applications, however, a Hetero\u00adgeneous \nMulti-core Architecture (HMA), consisting of dif\u00adferent types of processing cores, has the potential \nto provide greater performance and ef.ciency [1, 6]. There are two primary ways in which an HMA can im\u00adprove \nperformance. First, heterogeneous cores allow special\u00adisation of some cores to improve the performance \nof particu\u00adlar application types, while other cores can remain more gen\u00aderal purpose, such that the performance \nof other applications does not suffer. Second, an HMA can also enable programs to scale better in the \npresence of serial sections of a paral\u00adlel workload. Amdahl s law [4] shows that even a relatively small \nfraction of sequential code can severely limit the over\u00adall scalability of an algorithm. A HMA can devote \nsilicon area towards a complex core, on which sequential code can be executed quickly, and use the rest \nof its silicon area for a large number of simple cores, across which parallel work\u00adloads can be scaled. \nThis enables an HMA to provide better potential speedups compared with an equivalent symmetric architecture \nwhen Amdahl s law is taken into account [8]. However, this potential for higher performance comes at \nthe cost of program complexity. In order to exploit an HMA, programmers must take into account: the different \nstrengths and weaknesses of each of the available processing cores; the lack of functionality on certain \ncores (e.g., .oating point hardware or operating system support); potentially different instruction sets \nand programming environments on each of the core types; and (often) a non-coherent shared memory system \nbetween cores of different types. If mainstream application developers are to exploit HMAs, they must \nbe made simpler to program. High level virtual machine based languages, such as Java, present an opportu\u00adnity \nto hide the details of a heterogeneous architecture from the developer, behind a homogeneous virtual \nmachine inter\u00adface. OOPSLA/SPLASH 10, October 17 21, 2010, Reno/Tahoe, Nevada, USA. Copyright &#38;#169; \n2010 ACM 978-1-4503-0203-6/10/10. . . $10.00 c  This paper introduces Hera-JVM, a Java Virtual Machine \n(JVM) which hides the heterogeneous nature of the Cell multi-core processor behind a homogeneous virtual \nmachine interface. The Cell multi-core processor is a particularly challenging environment on which to \ndevelop applications, due to cores with different instruction set architectures and a non-coherent memory \nsubsystem. Hera-JVM supports the full Java language1; unmodi.ed Java applications can be executed across \nboth the Cell pro\u00adcessor s main PowerPC-based core and the additional SPE accelerator cores. Migration \nof threads between core types is handled transparently from the point of view of the ap\u00adplication and \ndoes not require application source code to be modi.ed. Hera-JVM uses a Just-In-Time (JIT) compiler to \ngenerate machine code for the disparate instruction sets of these two core types on-demand. Threads running \non ei\u00adther core type can invoke native methods, dynamically allo\u00adcate memory, have it recovered by GC, \nand synchronize us\u00ading shared-memory data structures (consistent with the Java Memory Model [10]), even \nthough the hardware does not provide hardware cache coherency. This paper builds upon the work presented \nin [13], but describes a much more complete runtime system that can support real-world Java applications \nas well as providing a much more thorough evaluation of this runtime system. The main contributions of \nthis work are: The creation of the .rst JVM implementation to support execution of real-world Java applications \nacross hetero\u00adgeneous processing core types with different instruction sets architectures (ISAs) and \nprovide transparent migra\u00adtion of threads between these core types.  A software caching mechanism that \nprovides ef.cient access to the non-coherent memory subsystem of the Cell processor by employing high-level \ntype information embedded in Java bytecode.  Demonstration of real-world Java workloads that exhibit \nup to a 2.25x speedup when executed on one of the Cell processor s SPE accelerator cores, compared to \nexecution on its main PPE core, and up to a 13x speedup if scaled across all 6 SPE cores.  Section 2 \nintroduces the Cell processor in more detail, outlining the main features of its architecture that make \nap\u00adplication development dif.cult. Section 3 presents the de\u00adsign principles around which Hera-JVM is \nbased and dis\u00adcusses the problems that the Cell processor s unusual ar\u00adchitecture presents in achieving \nthese principles. Section 4 describes the implementation of these design principles in Hera-JVM for the \nCell processor. Section 5 expands upon this implementation overview to provide more in-depth de\u00adtails \nof the features which are required for Hera-JVM to 1 The only deviation from the Java Runtime Speci.cation \nis that it uses a different .oating point rounding mode (rounding towards zero instead of rounding to \nnearest). This is due to lack of hardware support on one of the Cell processor s cores types. It only \naffects the least signi.cant bit of single precision .otation point calculations; the more commonly used \ndouble precision format is unaffected.  Control Flow  (b) An SPE core s memory subsystem. Figure 1. \nThe Cell Processor. support real-world Java applications on the Cell processor. Hera-JVM s performance \nunder both synthetic and real\u00adworld Java benchmarks is presented in Section 6. Section 7 contrasts Hera-JVM \nwith relevant related work. Finally, Sec\u00adtion 8 concludes and discusses possible future directions for \nthis work. 2. Background: The Cell Processor The Cell processor [6, 9, 17] was developed primarily for \nmultimedia applications, speci.cally the game market, where it is the main processor used by the Sony \nPlaysta\u00adtion 3. It is also being actively employed in a variety of other areas, such as scienti.c and \nhigh performance computing. The Cell processor contains two different processing core types: a single \nPower Processing Element (PPE) core; and eight Synergistic Processing Engine (SPE) cores (Fig\u00adure 1(a)). \nBoth core types are dual issue, in-order architec\u00adtures, running at 3.2 GHz, however, they have substantially \ndifferent architectures. The PPE is a conventional 64-bit PowerPC-based core, supporting the Linux operating \nsystem and any applications compiled for the PowerPC architecture. The SPEs are designed to perform the \nbulk of the computa\u00adtion on the Cell processor. They have a unique instruction\u00adset, highly tuned for \n.oating point, data-parallel workloads. The SPEs do not run any operating system code, relying on the \nPPE to perform operations such as page table updates or .le I/O. The processing cores share access to \nexternal DRAM memory through a circular ring-based Element Interconnect Bus [2]. The PPE core has a two-level \ncache to reduce data access latencies, with a 64KB L1 cache (split evenly between data and instruction \ncaches) and a 512KB L2 cache. Unlike the PPE, the SPE cores do not have transparent hardware caches for \naccessing main memory; instead, each SPE contains 256KB of non-coherent, private, local mem\u00adory. The \nprocessing elements of the SPEs can access only this local memory directly. To access data in main memory, \nan SPE must initiate a Direct Memory Access (DMA) trans\u00adfer to copy the data from main memory to its \nlocal memory. It can then modify this data in local memory, but must initi\u00adate another DMA transfer to \nwrite the results back into main memory. Each SPE has an associated DMA engine, called a Memory Flow \nController (MFC), that performs these data transfers (Figure 1(b)). These DMA engines have virtual memory \nsupport, therefore different threads of a single pro\u00adcess share a consistent view of the process s virtual \nmemory address space whether running on the SPE or PPE cores. However, data which has been copied to \nan SPE s local memory is not automatically kept consistent with the orig\u00adinal copy in main memory or \nany copies made by other SPE cores, meaning cores do not automatically share a coherent view of data. \n These features, of heterogeneous core types and an un\u00adusual memory architecture, make developing ef.cient, \nor even correct, applications for the Cell processor a challenge. 3. Two Architectures, One JVM The aim \nof Hera-JVM is to abstract the Cell processor s challenging architectural features behind a more typical \nsymmetric multi-core virtual machine abstraction, whilst still preserving the performance bene.ts provided \nby the Cell processor s heterogeneous cores. Hera-JVM provides a conventional JVM interface to applications: \nthe runtime sys\u00adtem can then schedule and migrate Java threads across the heterogeneous core types of \nthe Cell processor in a manner that is completely transparent from the point of view of the application. \nThe two core types provided by the Cell processor have different instruction set architectures, and therefore \nrequire different compiled machine code to execute the same Java code. Hera-JVM exploits the fact that \nJava code is dis\u00adtributed in architecturally-neutral Java bytecode. This byte\u00adcode is just-in-time compiled \nto a particular core-type s ma\u00adchine code only if it is to be executed on that core type. Hera-JVM allows \ntransparent migration of Java threads between these different core types. Whenever a method is invoked, \nHera-JVM can migrate the current thread s execu\u00adtion to another core type for the duration of this method \nand any methods which it calls. By only allowing migrations at method invocations, there is a well de.ned \npoint at which the thread s execution transfers from one core type to another. This enables Hera-JVM \nto tailor a thread s execution for the core type on which it is executing, by for example, struc\u00adturing \nstack-frames differently for each of the core types or performing inter-bytecode optimizations. Consequently, \nHera-JVM can exploit heterogeneous cores without having to execute in a sub-optimal manner on one or \nmore of the core types; however, this does mean that a thread must mi\u00adgrate back to its original core \ntype once a migrated method returns, since the method to which it will return is part-way through its \nexecution on the original core type. To enable this form of seamless migration of threads be\u00adtween the \ndifferent core types of the Cell processor, Hera-JVM supports the full Java speci.cation on both core \ntypes. If an operation cannot be supported by a particular core type, this limitation is hidden from \nthe application by automati\u00adcally migrating the thread to a more capable core type for the duration of \nthe operation. Since migration is a relatively expensive operation, Hera-JVM supports all of the core \nJava operations (e.g., arithmetic, method invocation, object al\u00adlocation, thread synchronization and \nre.ection) natively on both core types. Only heavy-weight operations, such as .le opening and process \ncreation, are limited to a particular core type. Hera-JVM must also hide the fact that the SPE cores \ncan only directly access their 256KB of private local mem\u00adory, and must perform DMA transfers to access \nany data in main memory. An SPE core can only execute machine code that is resident in its local memory. \nSince most appli\u00adcations are likely to require more than 256KB of machine code, Hera-JVM provides an \nef.cient mechanism to auto\u00admatically cache code in an SPE core s local memory as it is required. Similarly, \ndata must reside in local memory before it can be accessed by a thread executing on an SPE core. Hera-JVM \nprovides a software cache of recently accessed data in each SPE s local memory to limit the overhead \nof DMA transfers of data between main memory and local memory. However, since this local memory is private \nto the SPE core, any changes made to this cached data will not be visible to threads running on other \ncores. For Hera-JVM to sup\u00adport multi-threaded Java applications correctly, this software cache must \nconform to the Java Memory Model [10], by performing coherency operations at thread synchronization points. \nSection 4 describes how these design decisions were im\u00adplemented in Hera-JVM for the Cell processor. \nSection 5 expands upon this overview to provide more in-depth imple\u00admentation details of features which \nare required for Hera-JVM to support real-world Java applications on the Cell pro\u00adcessor. 4. Hera-JVM \nHera-JVM is based upon Version 3.0 of the JikesRVM [3] JVM. JikesRVM is a full implementation of the \nJVM with performance comparable to production JVMs (however, Hera-JVM only supports the slower, non-optimizing \nbase\u00adline compiler backend). It supports execution on the Pow\u00aderPC processor architecture, and can therefore \nexecute Java code on the PowerPC-based PPE core of the Cell processor without any modi.cation. Hera-JVM \nextends JikesRVM in three main ways: (i) runtime system and compiler support for the SPE cores; (ii) \nchanges to the overall runtime sys\u00adtem to support simultaneous execution of a Java application across \ntwo different architectures; (iii) support for migration between the different core types of the Cell \nprocessor. JikesRVM (and thus Hera-JVM) is a Java in Java virtual machine, with the majority of the runtime \nsystem written in Java. This confers a number of advantages in the design of Hera-JVM. Given Java s write \nonce, run anywhere philoso\u00ad  Key Java Code Assembly Code Processing Core Figure 2. The structure of \nHera-JVM. Much of Hera-JVM s runtime can be shared by both cores, given its Java in Java design. phy, \nthis code is largely portable. Thus, other than a small number of architecture-speci.c routines, the \nsame runtime system code is shared by both core types (Figure 2). This approach extends the philosophy \nof hiding the architecture s heterogeneity right through application code, the Java Lib\u00adrary code and \nthe majority of the runtime system s code, simplifying the runtime system s design. This also improves \nthe runtime system s maintainability; the fact that the same code is shared by both core types reduces \nthe likelihood of introducing integration bugs and inconsistencies in shared data structures. Hera-JVM \nis a non-interpreting JVM; all application, lib\u00adrary and runtime Java methods are compiled to machine \ncode before being executed. Other than the subset of the run\u00adtime system methods which are pre-compiled \ninto the boot\u00adimage, all Java methods are compiled just in time. Thus Java code is distributed in architecturally-neutral \nJava Bytecode, which will only be compiled for a particular core architec\u00adture if it is to be executed \nby a thread running on that core type. Since it is expected that most applications will exhibit a partitioning \nbetween code which is best run on the PPE or the SPEs, most methods will only ever be compiled for one \nof the two core s architectures. Thus, the compilation over\u00adhead (both in time and memory requirements) \nof running an application on an HMA, such as the Cell, need be little more than running on a single architecture \nprocessor. 4.1 Compiling Java Bytecode for the SPE Cores To execute Java code on the SPE cores of the \nCell processor, Hera-JVM requires a Java bytecode to SPE machine code compiler and some low-level runtime \nsystem support code. The low-level runtime system support code is the only part of the Java runtime system \nwhich is kept permanently resident in the SPE s local memory (taking up less than 4KB of each SPE s 256KB \nof local memory). This low-level support code deals with caching of data and code, and the lowest levels \nof inter-thread synchronization and interrupt handling. The rest of the Hera-JVM runtime system is written \nin Java and can be cached into the SPE s local memory as required like any other Java method. The remainder \nof this section describes the process by which this compiler and runtime system support code en\u00adables \nthe SPE cores to execute Java code. A running example of a simple Java method sum() , that calculates \nthe total of all the elements in a linked list, will be used to illustrate int sum( ListNode n) {int \ntotal = 0; 0: iconst 0 1: istore 1 while (n != null ) { 2: aload 0 3: ifnull <21> total += n.val; 6: \niload 1 7: aload 0 8: getfield <val > 11: iadd 12: istore 1 n=n.next(); 13: aload 0 14: invokevirt <next \n> 17: astore 0 }18: goto <2> return total ; 21: iload 1  } 22: ireturn (a) Java Code (b) Resulting Bytecode \nFigure 3. Example Java method -summing a linked list. different aspects of this process. Figure 3 shows \nthe source code for this method (left), and the resulting Java bytecode (right). Hera-JVM does not require \nany changes to the Java source-to-bytecode compiler or to the bytecode format. A Java method, such as \nsum() in Figure 3, is compiled into a block of machine code that can be executed natively by an SPE core. \nFundamental bytecodes, such as arithmetic and branch operations, can be translated directly into one \nor more SPE machine instructions by the compiler. More complex bytecodes, such as the new bytecode used \nfor ob\u00adject allocation, are translated into calls to special runtime system entry points. These runtime \nsystem entry points are special Java methods that perform the required operation, then return execution \nto the original method. Since this run\u00adtime system code is shared by both the PPE and SPE cores, these \ncomplex bytecode operations can essentially be lever\u00adaged from the existing JikesRVM implementation. \nSimilarly, complex runtime system components, such as .le handling, class loading or thread scheduling, \ncan be supported on ei\u00adther core type with little modi.cation. As a stack-oriented language, Java bytecodes \nimplicitly operate on variables located on an operand stack. For exam\u00adple, the iadd bytecode in Figure \n3 pops two integer values off the operand stack, adds them, and pushes the result back onto the operand \nstack. Since almost every bytecode pushes or pops values from the stack, it is important that these op\u00aderations \nare ef.cient. A thread s stack resides in main memory (so that it can be accessed by any core upon which \nit is scheduled), how\u00adever, having SPE cores operate directly on this stack in main memory would be incredibly \ninef.cient, due to their DMA\u00adbased access to main memory. Therefore, the top portion of the currently \nexecuting thread s stack is held in the SPE s lo\u00adcal memory to provide ef.cient stack access. Upon a \nthread switch, a 16KB block at the top of the thread s stack is copied into a reserved portion of the \nSPE s local memory. Stack updates are performed on this local copy, which is then written back to main \nmemory when the thread is context switched from this core. Stack over.ow checks are required when a \nmethod is invoked to ensure that this method s stack frame will not cross the 16KB limit of the block \nheld in SPE local memory. If the stack does grow beyond this limit, the stack over.ow routine pages this \nblock out to main memory, and pages in the next 16KB of the thread s stack. Whilst accessing local memory \non the SPE cores is much more ef.cient than accessing main memory, it is compli\u00adcated by the SPE s unusual \ninstruction set. SPE registers are 128 bits wide, with instructions treating these 128 bits as a vector \nof sixteen 8-bit, eight 16-bit, four 32-bit or two 64\u00adbit values, depending upon the operation. Hera-JVM \nonly ever uses the .rst vector slot, since Java has no in-built vec\u00adtorization support. However, the \nSPE s instruction set re\u00adquires that loads and stores from local memory are 128bit aligned. Therefore, \neither each stack variable must be 128bit aligned, wasting a considerable proportion of the valuable \nlocal memory, or stack push and pop operations must shuf.e variables between the .rst vector slot and \nthe variable s orig\u00adinal alignment, making stack access inef.cient. Hera-JVM uses the second approach, \nof shuf.ing variables, however, an optimization is employed to reduce its overhead. One of the SPE s \nregisters is reserved to hold the 128-bits currently at the top of the stack. Variables are shuf.ed in \nand out of this register as required, but the values are only written to the local memory stack when \nthe stack pointer passes a 128\u00adbit boundary. This optimization reduces the overhead of pop operations \nfrom two machine instructions to one instruction, and the overhead of push operations from four instructions \nto two. A Java method can also store method arguments and intermediate values in an array of variables \nknown as locals. For example, in Figure 3, the total variable is stored in local number 1, and is accessed \nusing the iload 1 and istore 1 bytecodes. Hera-JVM exploits the large register .le provided by the SPEs \n(each SPE core has 128 registers) to hold each local variable in its own register. If a method has too \nmany local variables to .t in the available SPE registers, the additional local variables are spilled \nto the method s stack frame. The registers holding local variables must be non-volatile across method \ninvocations, therefore code in each method s pro\u00adlogue saves the previous value of any local variable \nregisters the method might overwrite and then restores these values when it returns.  4.2 Software Caching \nof Heap Objects In addition to stack and local variables, which are private to a method, Java bytecode \ncan also access data in a shared heap of object instances and arrays. In Figure 3 the sum method accesses \nthe val .eld of a ListNode object in the heap, using the getfield bytecode. Since the heap is shared \nbetween cores, it is located in main memory. Therefore, before accessing data from the heap (e.g., the \ngetfield bytecode in Figure 3), an SPE core must perform a DMA transfer of the data it wishes to access \nfrom main memory into its private local memory. To reduce heap data access latencies and limit the number \nof DMA transfers required, Hera-JVM provides a software data cache for SPE heap access. Setting up a \nDMA operation to transfer data to and from local memory is an expensive operation (about 30-50 cycles, \nnot including the data transfer itself). Therefore, an early design decision of the software data cache \nwas to transfer large blocks of memory wherever possible. However, to limit cache pollution, only data \nwhich is likely to be used in the future should be cached. The high-level type information preserved \nin Java bytecode provides the opportunity to meet these two con.icting demands. The getfield and setfield \nbytecodes access a single .eld of an object. However, rather than caching just the .eld that is being \naccessed, or a .xed size block around that .eld, Hera-JVM exploits the fact that high level type information \nis encoded in these bytecodes to cache the full object in\u00adstance. Subsequent accesses to any of the .elds \nof this ob\u00adject instance will result in a cache hit, and can be read di\u00adrectly from the cached local \nmemory copy without requiring further DMA transfers. This approach exploits object-based locality of \nreference -i.e., the thread is likely to access other .elds in the same object instance. Arrays are accessed \nusing a different set of bytecodes (iaload, iastore, etc). Array accesses can therefore be cached in \na different manner to object accesses. Array in\u00adstances are generally much larger than object instances, \nand may be too large to .t in their entirety into the local memory cache. Therefore, rather than attempting \nto cache entire ar\u00adray instances, Hera-JVM caches arrays in 1KB blocks. Spa\u00adcial locality of reference \nis exploited by this scheme, with neighbouring elements cached alongside the element being accessed, \non the assumption that they are likely to be ac\u00adcessed shortly. When the SPE compiler encounters a bytecode \nthat re\u00adquires accesses to data in the heap (e.g., getfield, iaload, etc.) it generates inline machine \ncode that checks if this ac\u00adcess hits the local memory cache and then performs the oper\u00adation on this \ncached copy. In the case of a cache miss, execu\u00adtion traps to a cache miss handler, which is part of \nthe SPE s permanently resident runtime system support code. Thus, the fast path cache hit code is performed \ninline to reduce perfor\u00admance overheads, whilst the more complex, but less used cache miss code is kept \nout-of-line to reduce code bloat. The data cache is structured as a small 1024-entry hash\u00adtable resident \nin the core s private memory (see Figure 4). Each entry is either blank, or holds the main memory address \nof an object instance or array block as a key, and the local memory address of its cached copy as a value. \nA cache lookup involves hashing the main memory address of the object or array block which is being accessed \nusing a simple XOR folding hash (chosen due to its simplicity), which provides an index into this hash \ntable. If the entry at this index is the same as the main memory address requested, this access has hit \nthe cache and the bytecode operation is performed directly on the cached copy pointed to by this entry. \nA cache lookup that results in a cache hit requires Size Location Hash Index Main Memory Local Memory \nAddress Address / Size Cache Lookup Hash Table Cache Data Store Figure 4. Outline of the SPE data cache. \nonly 12 fast SPE machine instructions. Otherwise, a cache miss has occurred and the data must .rst be \npulled into local memory. On a cache miss, space is reserved for this object instance in local memory, \na DMA operation is set up to copy its data into local memory, the hash table is updated with this cached \nentry, and the thread blocks until the DMA copy completes. No collision resolution is performed by this \nsoftware cache hash-table. A cache miss simply overwrites the hash-table entry to re.ect this newly cached \nelement, thereby evicting the previous element from the cache. Operations which write to the heap (setfield, \niastore, etc.) must update both the cached data in local memory and the original copy in main memory. \nWrite operations update the data in the local memory cache directly, then immedi\u00adately initiate a DMA \ntransfer to copy the object .eld or ar\u00adray element which was modi.ed to its original main memory location \n(i.e., this software cache uses a write-through pol\u00adicy). Unlike the cache read operations, the DMA transfers \ninitiated by write operations are non-blocking; the thread can continue executing while the DMA engine \nperforms this write to main memory concurrently. Threads do, however, block on these write operations \nbefore reading data from main memory to service a cache miss and when perform\u00ading thread synchronization \noperations, to maintain memory consistency. The process of maintaining a coherent heap across multiple \nSPE s software caches, as required by the Java Memory Model [10], is discussed in Section 5.3.  4.3 \nInvocation and Caching of Methods Code must also reside on the SPE s local memory before it can be executed. \nSince a Java thread is likely to execute more code than can .t in an SPE s local memory, a software-based \ncode caching scheme is used so that code can be transfered into the SPE s local memory, as required, \non-demand. In keeping with Hera-JVM s approach of using DMA to transfer large blocks of data wherever \npossible, Java methods are cached in their entirety. When the SPE com\u00adpiler encounters a method invocation \nbytecode, such as the invokevirtual bytecode which calls the next() method in Figure 3, this will be \ncompiled into SPE machine code which checks if this method is cached in local memory and, if so, jump \nto the address of this cached copy. As with the data cache, a cache miss will result in the thread s \nexecu\u00adtion trapping to a cache miss handler, located in the SPE s low-level runtime system support code. \n Unlike the data cache, the code cache does not use a hash\u00adtable to perform look-ups. A hash-table is \nnot suitable as a means of looking up a method because of the need to support virtual method invocation \nfor Java instance methods. When an instance (as opposed to a static) method is called, the actual method \nwhich is invoked depends upon the type of the instance object upon which this method was called. For \nexample, if the object n in Figure 3 is a sub\u00adclass of ListNode, then n.next() must invoke the sub\u00adclasses \nimplemention of the next() method, not ListNode s implementation. Therefore, the actual code that should \nbe invoked by the invokevirtual bytecode is unknown at compile time; it must be inferred at runtime, \nbased upon the object instance s type. The standard method of supporting virtual method invo\u00adcation in \na JVM is to include a pointer to a type informa\u00adtion block (TIB) in each object instance s header. A \nsingle TIB exists for every class loaded into the runtime system. Each TIB contains an entry for each \nmethod declared by the class, with each entry pointing to the code that implements the method. The TIB \nis laid out such that inherited methods are located at the same index in the sub-class s TIB as in the \nsuper-class s TIB. By looking up the index of the virtual method being invoked in the TIB of the object \nupon which it is being invoked, the runtime system can .nd the actual instance method which it should \nrun for this virtual method invocation. Since TIB entries point to the machine code implement\u00ading a method, \nHera-JVM requires two TIBs for every class one which points to the PPE machine code and one which points \nto the SPE machine code. To limit memory overheads, Hera-JVM uses a two stage class-loading system. A \nclass is initially resolved for the PPE core, with only the PPE TIB being created. If the class is referenced \nby code running on the SPE core, it will then be resolved for the SPE core, which will create the SPE \nTIB. Hera-JVM exploits the fact that only a limited number of classes will be resolved for the SPE core \nto simplify TIB and method caching. A small (4KB) class table of contents (TOC) resides in SPE local \nmemory, with an entry for each class that has been resolved for the SPE. Rather than a direct pointer \nto the SPE TIB s main memory address, each object instance has an index to its class s entry of this \nTOC in its header (see Figure 5). Each TOC entry initially points to the location of that class s SPE \nTIB in main memory. When a method is invoked on an SPE core, the appropriate class s TOC entry is read \nto locate the class s TIB, which is cached if necessary. When a TIB is cached, its TOC entry is updated \nto point to this local memory copy, thus, subsequent look-ups immediately know the location of the cached \ncopy. The required method is then looked up in the TIB, and if necessary, cached in local memory, with \nthe TIB entry being updated to point to the cached method s address. An added bene.t of this approach \nis that, while a direct pointer to a class s SPE TIB would require a full word to Heap ListNode objects \nEmployee object PPE TIB Addr PPE TIB Addr 0x31562350 1 21 0x31562350 1 414 0x24546500 3 Tom ListNode \nTIB Method Cache Meta-Data Cached Methods Figure 5. The code cache data structures. specify, a class \ns TOC index only requires 10 bits in Hera-JVM. Therefore, it can be accommodated in spare bits of the \nobject instance s header, rather than having to reserve an additional word in every object instance s \nheader. The object instance headers do still contain a pointer to their PPE TIB, so that the PPE code \ncan perform virtual method invocation in the usual manner; however, since the TOC index is hidden in \nspare bits of the header, object instances are the same size in Hera-JVM as they are in JikesRVM. Static \nmethod invocations always invoke the same class s method (they are statically associated with the class, \nnot a particular object instance). These method invocations are cached in the same manner as instance \nmethods, the only difference being that the class TOC index is supplied stat\u00adically by the compiler, \nrather than being read dynamically from an object instance s header at runtime. 5. Implementation Details \nA number of complexities arise when real-world applica\u00adtions are executed on the Cell Processor under \nHera-JVM. This section details some of the implementation choices made in the creation of Hera-JVM to \ndeal with these chal\u00adlenges. 5.1 Variable Sized Cache Elements Both the data and code software caching \nschemes used by the SPE cores are unusual in that the elements being cached are not of a .xed size. In \norder to service a cache miss, Hera-JVM s runtime system must be made aware of the size of the cache \nelement which is to be cached, so that it can transfer it in its entirety the SPE s local memory. For \nobject instance accesses, type information embedded in the getField and setField bytecodes enables the \nruntime system to infer, at compile time, the type, and thus the size of the object instance being accessed. \nHera-JVM embeds this information into the machine code that it compiled for the SPE cores, such that \nthe object s size can be passed automatically to the cache miss handling routing. Array accesses cache \neither a full block, of 1KB in size, or, if accessing the last block in the array, a block sized to .t \nthe remainder of the array. The length of an array is held in its header. The cache miss handling code \nchecks this length when caching a block to discover if this is the last block of the array and, if so, \nwhat size this last block should be. Finally, the size of a method s machine code is known once it has \nbeen compiled. This size is included in the class s TIB, next to the pointer to the method s machine \ncode (see Figure 5), where it can be easily accessed by the code cache miss handler. One issue with \nthis approach relates to Java s subtyp\u00ading inheritance abilities. An object access bytecode (e.g. getfield \nor setfield) has a particular type associated with the operation. However, the actual instance object \nac\u00adcessed at runtime may be a subtype of the type associated with the bytecode. This subtype may have \nmore .elds than the supertype referred to by the bytecode, resulting in its in\u00adstance objects being larger. \nSince the caching system uses the type associated with the bytecode to infer the size of the object being \ncached, it will not cache the full subtype ob\u00adject instance, only those .elds associated with its supertype. \nThis is not a problem for the execution of this speci.c byte\u00adcode, since it is accessing one of the .elds \nassociated with the supertype. However, subsequent accesses to this object instance will hit this cached \ncopy directly. If they are trying to access one of the subtype .elds, invalid data will result from reading \nfrom this cached copy, since it does not con\u00adtain any of the subclass data. To avoid this, Hera-JVM stores \nthe size of the data it has cached alongside the local memory address of the cached object in the data \ncache s hash table. Since the local memory has a small address space (18 bit address width), the object \ns size and cached local memory address can .t in a single 32 bit word entry of the hash ta\u00adble. When \na cache hit occurs, the size of the cached data is compared to the expected size of the object type being \nac\u00adcessed. If the cached data is not large enough, the object is re-cached. 5.2 Returning from a Method \nWhen a method returns, execution should return to the point in the caller method immediately after the \ncallee method was called. Typically, this is supported by placing a return address on the stack; a return \nstatement will branch to this return address to resume execution of the caller method. However, code \nexecuted by an SPE core is dynamically cached in local memory. Therefore, a simple return address is \nnot suf.cient; the caller method may no longer be at the same location in local memory when execution \nreturns to it (it could have been evicted from the cache or re-cached at a different location). Instead, \nHera-JVM places a return offset on the stack when code running on an SPE core invokes another method. \nThe value of this return offset is the distance of the invoking instruction from the start of the caller \nmethod. Alongside this return offset, the caller s stack-frame also contains a method ID, which speci.es \nboth the TOC and TIB indexes necessary to look up this method in the code cache. When the callee method \nreturns, it ensures the caller method is cached by performing the same look up process as if it were \ninvoking the method, using the indices speci.ed in the method ID in its caller s stack-frame. Adding \nthe return offset on the caller s stack-frame to the start address of this cached method provides the \ncallee method with an absolute return address, to which it can jump in order to resume execution of the \ncaller method.  5.3 Synchronization and Coherence In a multi-threaded Java application, the same object \ncould be accessed by multiple threads simultaneously. Conse\u00adquently, as well as residing in the main \nmemory heap, mul\u00adtiple copies of this object could reside in different SPE local memory caches. In order \nto correctly execute multi-threaded Java applications, some form of synchronization is required to keep \nthe data in these cached copies consistent with main memory. This is usually performed by a hardware \ncache coherence system, however, the Cell processor does not pro\u00advide hardware cache coherency for SPE \nlocal memory, and providing a software coherence protocol which broadcasts every object update to all \ncopies of the object s data would cripple the SPEs performance. Fortunately, it is not neces\u00adsary to \nkeep all of the replicas of a given object consistent all the time. This is because the Java Memory Model \n[10] allows data to be cached in a non-coherent manner, as long as these inconsistencies are resolved \nbefore thread synchro\u00adnization points. Java s memory model is based upon a notion of happens\u00adbefore relationships. \nSynchronization operations, such as lock acquire/release operations and volatile .eld accesses, impose \na happens-before order on program execution. A read operation is not allowed to observe a write which \nhap\u00adpens after this read in the happens-before partial order (i.e., it should not see any writes which \nhappen after a subsequent synchronization point). Similarly, the read can observe a write w, as long \nas there was no intervening write w', where w happened before w' in the happens-before partial order \n(i.e., the thread does not see a value which was overwritten before the previous synchronization point). \nIn virtual machine implementation terms, the effect of this model is to allow heap data to be cached \nby a thread (i.e., be inconsistent with the globally accessible original copy in main memory), as long \nas these cached copies are re\u00adsynchronized with main memory at thread synchronization points. After performing \na lock acquire operation, a thread must see all heap updates which happened-before this lock. Before \nreleasing this lock, the thread must ensure that any updates it has made to heap variables are visible \nto any thread which later synchronizes on this same lock object. In a traditional JVM implementation, \nthis model requires that (for instance) writes to a .eld do not remain in a pro\u00adcessor s registers when \nreleasing a lock: the writes must be made back to memory and, on some architectures, a low\u00adlevel memory \nfence operation must be performed before re\u00adleasing the lock. In the case of the SPE core this means \nfully propagating all that thread s writes to main memory before releasing the lock, due to the lack \nof cache coherence on the SPE cores. This state is also .ushed to main memory when\u00adever a thread is context \nswitched or migrated off of the SPE core, to ensure the changes it has made are visible to the core on \nwhich it will next be executed. Hera-JVM ensures this by completely purging an SPE s data cache whenever \nthe thread it is executing locks an object or reads a volatile .eld. Before unlocking an object or writing \nto a volatile .eld, the thread is blocked until all of its DMA write transfers have been completed. The \nJava memory model also requires synchronization order consistency, where the order of synchronization \nop\u00aderations and volatile variable accesses is sequentially con\u00adsistent across threads. These operations \nmust, therefore, be performed atomically. Volatile .eld accesses are restricted to reading or writ\u00ading \nfrom a single .eld, which can have a maximum size of 8-bytes. DMA transfers, performed by an SPE core \ns mem\u00adory .ow controller (MFC), that are less than 16-bytes op\u00aderate atomically on the Cell processor. \nTherefore, volatile .eld accesses can be treated like normal .eld accesses by Hera-JVM, with the additional \nconstraint that: the SPE local memory cache is .ushed before a volatile read is performed (which is also \nrequired for happens-before consistency); and the thread blocks on volatile writes to ensure they have \nbeen written to memory before continuing (unlike non-volatile .eld write operations, which are non-blocking). \nTo perform lock and unlock operations atomically, a compare-and-swap type of operation must be used. \nThe SPE MFCs provide two blocking DMA operations, called GETLLAR and PUTLLC, which can be used to build \nan atomic compare-and-swap operation. The GETLLAR operation per\u00adforms a blocking read from a memory address, \nwhilst simul\u00adtaneously setting a reservation on this address. The PUTLLC operation conditionally writes \nto a memory address, if the processor still holds a reservation on this address, and re\u00adturns a success \nor failure noti.cation. If another core writes to the memory address between the GETLLAR and PUTLLC operations, \nthe reservation will be lost and the PUTLLC op\u00aderation will fail. Thus, an SPE core can use these operations \nto perform an atomic (from the point of view of other cores) compare-and-swap operation to lock or unlock \nobjects. By conforming to the Java Memory Model, any cor\u00adrectly synchronized multi-threaded application \nwill exhibit sequentially consistent behaviour and run correctly under Hera-JVM. Hera-JVM sizes DMA write \noperations such that only the single .eld or data element being operated upon is overwritten. Therefore, \neven if neighbouring elements are protected by different locks, Hera-JVM will provide sequen\u00adtially consistent \nbehaviour. Finally, since all writes are per\u00adformed in-order by code on the SPE cores, Hera-JVM con\u00adforms \nto the no out of thin air values property in the presence of data races, as required by the Java Memory \nModel. 5.4 Scheduling and Thread Switching To support multi-threaded Java applications, Hera-JVM must \nschedule the execution of multiple Java threads onto each of the available processing cores. The version \nof JikesRVM upon which Hera-JVM is based (version 3.0) uses a green thread model to schedule Java Threads. \nThis model maps multiple Java threads to a single OS thread, with the runtime system performing user \nlevel scheduling of the Java threads, rather than the underlying operating system. JikesRVM uses an \nm-to-n threading model, with the run\u00adtime system starting an OS thread for each processing core and pinning \nits execution to this core. Thus only a single OS thread (known as a virtual processor) executes Java \ncode on a particular processing core, on behalf of different Java threads. When a Java thread performs \na blocking operation or a timer tick event occurs, the virtual processor s execu\u00adtion traps to runtime \nsystem scheduling code. The runtime system scheduler selects another Java thread from the virtual processor \ns run-queue and morphs its identity from the pre\u00advious Java thread to this new Java thread. Thus, the \noperat\u00ading system is not involved in the scheduling of Java threads at all; it is only involved in sharing \nthe processing core s execution between the JikesRVM virtual processor and any other processes running \non this processing core. Since the SPE cores do not run an operating system, code executes bare-metal, \nwith no OS level support for multi\u00adthreading. Thus, this green thread model is a natural .t for the creation \nof Java threading on SPE cores in Hera-JVM. A single virtual processor thread of execution is run bare\u00admetal \non the SPE core. This SPE virtual processor employs the same runtime system scheduling code as the PPE \ncore to schedule Java threads. No OS level scheduling support need be created to support threading on \nthe SPEs. 5.4.1 SPE Virtual Processor Initialisation Hera-JVM initialises each SPE core by having the \nSPE ex\u00adecute a specially written boot-loader program, using the lib\u00adspe2 library provided by IBM. This \nboot-loader is written in C so that it can be supported by libspe2. It initialises some re\u00adserved registers \nused by the SPE runtime system, then copies the low-level, out-of-line, runtime system code (used to \npro\u00advide code and object caching, as well as other services such as interrupt handling) into its local \nmemory. The boot-loader then traps to this out-of-line code to cache and invoke the Java entry function \nof the SPE runtime system (overwriting the C boot-loader code in the process). The Java entry function \nperforms some additional ini\u00adtialisation to set-up the SPE core s virtual processor data\u00adstructures. \nIt then invokes the scheduling code to .nd a Java thread which it can execute. Initially, only a pre-built \nidle thread will be runnable on this SPE virtual processor. The idle thread does nothing other than yield, \nto enable the scheduling of a useful thread. After a given number of yields, it puts the core to sleep, \nby performing a blocking read on an inter-core signalling channel. To wake this core, another thread \nwill send a signal through this channel to wake the core, after placing a thread in its run-queue or \n(in the case of the PPE core) migrating a thread to the SPE core.  5.4.2 Scheduling Mechanism Each virtual \nprocessor (whether PPE or SPE) has its own run-queue of Java threads. It schedules these threads for \nexecution in a round-robin manner, with each thread running for a full scheduling quantum or until it \nblocks (e.g., on an I/O operation). When a virtual processor is making a scheduling de\u00adcision, it checks \nwhether it has more threads in its run queue than the other virtual processors. If so, it will per\u00adform \nload balancing by transferring some threads to another virtual processor s run-queue. This load balancing \nis only performed by virtual processors running on the same core type (i.e., SPE to SPE or PPE to PPE, \nbut not SPE to PPE or PPE to SPE). The thread migration mechanism, described in Section 5.4.5, must be \nused to transfer a thread to a different core type. 5.4.3 Context Switching Mechanism The process of \ncontext switching a virtual processor s exe\u00adcution to a different Java thread is highly architecture \ndepen\u00addent. The scheduling code calls a magic method to perform a context switch. This magic method is \ncompiled directly into inline context switching machine code, speci.c to the core type to which it is \nbeing compiled. To perform a context switch on an SPE core, the currently executing thread s state must \nbe saved and the new thread s state restored onto the core. This involves the context switch code saving \nall non-volatile registers to an array associated with the executing thread. Reserved registers, such \nas the frame pointer and the top of stack register are also saved in this array. The thread s current \nmethod ID and offset is saved onto the stack as if a method was being invoked. The stack block, currently \ncached in the SPE s local memory, is then written back to the thread s stack in main memory. To restore \nthe new thread s context onto this core, the process is reversed. The reserved and non-volatile registers \nare set to those values which were saved in this new thread s register array when it was last swapped \nout. A block at the top of this new thread s stack is loaded onto the SPE s local memory stack area. \nThe context switch code then performs a process similar to a method return, using the method ID and offset \nsaved on this new thread s stack when it was swapped out. This ensures that the method which was being \nexecuted by the thread when it was last executing is cached in local memory. Execution of the thread \nthen resumes at the correct point of this method. 5.4.4 Timer Interrupts To implement pre-emptive scheduling, \nthe scheduler must be able to interrupt the execution of a Java thread. SPE cores have a simple hardware \ninterrupt mechanism which can be employed to provide timer interrupts and enable pre-emptive scheduling. \nAn SPE core can be set up to asynchronously transi\u00adtion to interrupt handling code whenever a particular \nset of hardware events occurs. One of the hardware events which can cause an SPE interrupt is an incoming \nsignal on the SPE s inter-core signalling channel. Therefore, to provide SPE timer interrupts, a thread, \nrunning on the PPE core, sig\u00adnals each SPE core every 10ms. The SPE interrupt handler saves the core \ns context and processes the hardware signal which caused the interrupt. As well as handling timer interrupts, \nthe interrupt handler main\u00adtains hardware controlled data-structures, such as a hard\u00adware decrementer \nused for low-level timing information. If a scheduling operation should be performed during this inter\u00adrupt, \nthe interrupt handler then invokes the scheduler s entry\u00adpoint method. A number of runtime system operations \nmust be com\u00adpleted in their entirety, without being pre-empted by an\u00adother thread. Many low-level operations, \nsuch as updating a thread s stack frame pointer or transferring data from main memory, cannot be completed \natomically under the SPE s unusual instruction set. Disabling and then re-enabling in\u00adterrupts around \nall these low-level operations would be a considerable overhead, as well as being dif.cult to maintain. \nInstead, Hera-JVM explicitly checks for an interrupt event at speci.c points in a method s execution. \nThe SPE com\u00adpiler inserts a branch on external condition instruction into method prologues and loop branches. \nIf an interrupt event is pending, this instruction triggers the interrupt handling code, otherwise it \ndoes nothing. Checking for interrupt events on loop branches, as well as method prologues, ensures that \nonly a small .nite amount of time will pass between a timer interrupt being .red, and the interrupt handler \nrunning. Some higher level runtime system operations must also be non-preemptible. For example, runtime \nsystem methods that deal with thread scheduling or heap allocation should not be pre-empted. Such methods \nhave been annotated with an @Uninterruptable annotation by JikesRVM to enable them to be treated specially. \nTo ensure that these methods are not pre-empted, the SPE compiler simply does not in\u00adclude explicit interrupt \ncheck instructions when compiling methods which are tagged with the @Uninterruptable an\u00adnotation.  5.4.5 \nMigration between Core Types Hera-JVM supports migration of Java threads between the PPE and SPE cores \nto enable an application to exploit both the core types available on the Cell processor. This migration \nprocess is transparent from the point of view of the applica\u00adtion; no changes in application code are \nrequired to enable the application to be migrated between core types. The in\u00advocation of any Java method \n(other than those marked as @Uninterruptable) can act as a migration point. A mi\u00adgration can be triggered \neither dynamically by Hera-JVM s scheduler or explicitly by invocation of a method that is an\u00adnotated \nwith a @RunOnSPECore or a @RunOnPPECore anno\u00adtation. The experiments presented in this work use explicit \nannotations to trigger migration; future work will examine automatic migration triggered by the scheduler \nguided by runtime monitoring of a program s behaviour. If a method is to trigger a migration, it is invoked \nin the normal manner, however, code in its prologue causes the thread to trap to migration support code. \nThe migration sup\u00adport code (executing on the original core type), will package the arguments of the \nmigrating method and, if necessary, JIT compile this method for the core type to which the thread is \nbeing migrated. It then places the migrating thread on a per\u00adcore type migration queue when performing \na context swap, rather than inserting it back into its own virtual processor s run queue. During scheduling \noperations, each virtual pro\u00adcessor will periodically check the migration queue associ\u00adated with its \ncore type. Any threads it .nds will be removed from the migration queue to be added to its own run queue, \nthus migrating the thread to a virtual processor running on the appropriate core type. When a virtual \nprocessor pulls a thread off the migration queue, the migrating thread s current stack-frame is laid \nout for the other core type, from which it migrated. Therefore, before this thread is added to the virtual \nprocessor s run queue, a stack-frame for this core type is added to the end of the thread s stack. This \nsynthetic stack-frame causes the thread to start executing at a migration entry-point method when it \nis scheduled. The migration entry-point method will unpack the parameters which were passed to the migrating \nmethod, then invoke the appropriate method using Java s re.ection mechanism. The thread continues to \nexecute on this new core for the duration of this migrated method and the whole tree of methods which \nit calls (i.e., to migrate a thread for the entire duration of its execution, the thread s run() method \ncan be migrated). Of course, a subsequent method invoked by this thread could cause it to migrate back \nto the previous core type using the same mechanism. Once a thread returns from a migrated method it must \nre\u00adturn to its original core type. This is required by Hera-JVM because the frames below this point on \nthe stack are for\u00admatted for the core type on which it was originally execut\u00ading. To return to the original \ncore type, the migration entry\u00adpoint method performs a return migration once the migrating method it \ninvoked has returned.  5.5 Stack Scanning A number of runtime system processes must scan a thread s \nstack for information. For example, the garbage collector must scan every thread s stack for references \nto act as roots in its tracing algorithm. Similarly, exception handling code must also scan the stack \nto .nd the location of an appropriate catch block to handle a thrown exception. If a thread has been \nmigrated to another core type, its stack will consist of a mix of PPE and SPE stack-frames, which could \nconfuse such stack scanning code. For example, while the vast majority of the garbage collector stack \nscanning code is architecture neutral, the actual code which retrieves an object reference from a stack-frame \nis necessarily architecture dependent, since stack-frame layout varies between the core types. The synthetic \nstack-frame, placed on a thread s stack as part of the migration process, acts as a marker to signal \nthe transition from stack-frames of one core type to those of another. This enables these stack scanning \nalgorithms to transition between PPE and SPE stack-frame scanning code as required. The garbage collector \nstack scanning code uses these markers to switch between PPE and SPE stack-frame walkers. The exception \nhandling code, on the other hand, is scanning the stack to .nd a suitable catch block in which to resume \nthe thread s execution. Therefore, if it encounters a migration marker it immediately migrates the thread \nto the other core type, such that it is already on the correct core type on which to resume the thread \ns execution when it .nds a suitable catch block.  5.6 System Calls and Native Methods The .nal implementation \nconsideration is to provide support for Java threads to call non-Java native code. Occasionally, a method \nin the runtime system, the Java Library or a Java application requires access to native code (e.g. to \nwrite to a .le or start an external process). Application and library code can execute non-Java native \ncode using the the JNI (Java Native Interface). JikesRVM / Hera-JVM also provides a fast system call \nmechanism for trusted code within the runtime system to perform native system calls. However, if a thread \nis running on an SPE core, there is no underlying OS to support native methods. SPE cores must rely on \nthe PPE core to execute native code. In the case of a JNI method, the thread is migrated to the PPE core \nfor the duration of the native method, using the process described in Section 5.4.5. For fast system \ncall methods, the SPE core uses an inter-core mailbox channel to signal a dedicated thread on the PPE \ncore with an appropriate message. This dedicated thread performs the required system call on the SPE \nthread s behalf, then signals the SPE with the result. There is one set of native methods which is treated \nspe\u00adcially by Hera-JVM. The Classpath Java library, used by HeraJVM, implements the Math class natively. \nThis is done purely for performance reasons; these methods do not re\u00adquire OS support. Thus, they do \nnot need to be of.oaded to the PPE when invoked by a thread on an SPE core. Indeed, since these methods \nperform complex .oating point opera\u00adtions, they are likely to perform much better on the SPE core, than \non the PPE core. The SPE compiler treats these meth\u00adods like intrinsic functions -directly generating \nthe machine code required to perform the required operation -rather than of.oading them. 6. Experimental \nAnalysis This section presents an experimental evaluation of Hera-JVM under both synthetic micro-benchmarks \nand real-world Java benchmarks. The aims of this evaluation are: to in\u00advestigate the effectiveness of \nthe mechanisms used to hide the Cell processor s heterogeneous architecture; to ensure that unmodi.ed \nreal-world Java applications can be executed correctly under Hera-JVM under the non-coherent memory subsystem \nof the Cell Processor; and to characterise the per\u00adformance of each of the core types under different \napplica\u00adtion behaviours. 6.1 Experimental Setup All the experiments in this section are performed on \na Playstation 3 (PS3), with 256MB of RAM, running Fedora Linux 9. A 256MB swap space is located on the \nPS3 s rel\u00adatively fast video RAM, to minimise the paging overhead incurred due to the small amount of \nRAM available on the PS3. The Cell processor contains 8 SPE cores, however, only 6 of these SPE cores \nare available on the PS3 used in this evaluation. All experiments compare single threaded perfor\u00admance \nof code executed on a single SPE core to that on a single PPE core, unless otherwise stated. Hera-JVM \ncurrently supports only non-optimizing com\u00adpilation for the SPE cores, therefore, the baseline, non\u00adoptimizing \ncompiler was used to compile both PPE and SPE machine code for these experiments2. Hera-JVM was built \nwith a stop-the-world, mark and sweep garbage collector. This collector only runs on the PPE core and \nthus becomes a scalability limitation if it runs for a considerable proportion of a benchmark. There \nis no fundamental reason the garbage collector cannot also execute on the SPE cores (it is writ\u00adten in \nJava like the rest of the runtime system), however, this support was not implemented in Hera-JVM for \ntime reasons. The execution times of these benchmarks were calculated using the System.currentTimeMillis() \nmethod in the Java library. Each experiment was repeated ten times, with the average being reported and \nthe standard deviation, be\u00adtween these runs, shown using error bars. 6.2 Micro-Benchmarks The micro-benchmarks \nprovided by the Java Grande bench\u00admark suite [11] were employed to characterise the perfor\u00admance of the \nvarious fundamental Java operations on both core types under Hera-JVM3. Figure 6 shows the difference \nin performance between the core types for each of the micro-benchmarks included in section one of the \nJava Grande Suite. There is clearly a wide variation in capability between the PPE and SPE cores depending \nupon the type of Java operation being performed. Basic operations, such as arithmetic, primitive casting \nand looping code, perform much better on the SPE core than on the PPE core. Some of these operations, \nsuch as .oating point arithmetic and casting operations, are more than .ve times faster on the SPE core. \nThis was expected, given that the SPE is highly tuned for .oating point performance, how\u00adever, even integer \noperations are signi.cantly faster on the SPE core. The fact that looping code performs better on the \nSPE core, compared to the PPE core, was surprising. The PPE core has branch prediction hardware that \nis not found in the SPE cores. This should reduce pipeline stalls on the PPE, thus increasing the performance \nof looping code. The 2 While the use of an optimizing compiler would signi.cantly change the absolute \nperformance of the benchmarks presented in this section (using the optimizing PowerPC backend of JikesRVM \ncan yield up to an 8x improvement in performance for some of the benchmarks on the PPE core), we do not \nbelieve that it would signi.cantly change the relative difference in performance between the SPE and \nPPE cores. An initial investigation, where three of the methods in a mandelbrot benchmark were manually \ninlined, showed that this optimization could improved the performance of code running on both the PPE \nand SPE cores by a similar margin (2.5x and 3.1x respectivly). 3 These experiments use version 2.0 of \nthe sequential Java Grande suite, available at http://www2.epcc.ed.ac.uk/computing/research_ activities/java_grande/sequential.html \n  Percentage of Writes over Reads Figure 7. Performance as ratio of reads to writes is varied. Figure \n6. Relative performance of SPE and PPE cores un\u00adder the different Java Grande micro-benchmarks. fact \nthat the loop benchmark performs worse on the PPE core may be explained by the shorter pipeline in the \nSPE core, which reduces the impact on performance incurred by pipeline stalls. More complex operations, \nsuch as object creation, excep\u00adtion handling, mathematical calculations and method invo\u00adcation have roughly \nequivalent performance on both core types. The benchmarks that perform worst on the SPE core type are \nthose which exercise the SPE s software data and code caches. Access to local variables (e.g. method \nparam\u00adeters or variables on the thread s stack) is very fast on the SPE cores; however, accessing scalar \nobjects or arrays on the heap is considerably slower due to the costs involved in setting up DMA transfers \nfrom main memory. Invocation of synchronized methods also has a large overhead on the SPE core, due to \nthe SPE core s software data cache having to be purged before entering the synchronized method. This \nhas a high cost on the SPE core for two reasons: it will cause cache misses for future heap accesses, \nwhich are much more ex\u00adpensive on the SPE core than the PPE core; and the software cache on the SPE must \nmanually purge the cache by over\u00adwriting entries in the cache s hash-table, whereas the PPE does this \nin hardware. 6.2.1 Writing to the Heap The tests in the assign benchmark of the Java Grande Suite read \nfrom, and write to, memory in equal measure. To inves\u00adtigate whether reading from objects and arrays \non the SPE is equally as costly as writing to them, the benchmark was modi.ed so that the ratio of reads \nto writes could be var\u00adied. Figure 7 shows the difference in performance between the SPE and PPE cores, \nas this ratio is varied. This bench\u00admark has a small enough working set that reads always hit the cache, \nthus it only exercises the fast-path of the software cache. At small write ratios, the SPE core actually \noutper\u00adforms the PPE. The SPE is almost 55% faster than the PPE when reading from scalar objects and \n40% faster when read\u00ading from array elements. Thus, even though the SPE must perform a software cache \nlook-up operation for each ob\u00adject or array access, the SPE core s simple design and thesoftware cache \ns lightweight implementation make these ac\u00adcesses faster than the hardware cache on the PPE core. However, \nthe SPE s performance falls signi.cantly as the write ratio increases. Above a write ratio of between \n10% and 15%, the PPE core s performance outstrips that of the SPE. Writes are expensive on the SPE core \nbecause: a write\u00adthrough policy is used, meaning every write must be prop\u00adagated to main memory; and \neach write to main memory requires a DMA transfer, which is relatively expensive to set up. Informal \nexperiments show that the use of a write-back caching policy to enable batching of DMA transfers signif\u00adicantly \nimprove write performance on the SPE core, how\u00adever, implementing this write-back policy signi.cantly \ncom\u00adplicates the design of the cache. A complete implementation of write-back caching is left as future \nwork. 6.2.2 Data Caching Overheads Each of the micro-benchmarks presented above has a small enough working \nset that the data it accesses always .ts in the cache. To investigate the overhead of data caching, a \nmicro\u00adbenchmark was devised in which the size of the program s data working set could be varied. This \nbenchmark reads from, and writes to, randomly selected elements of an array. The size of the array can \nbe varied to alter the program s working set size and affect its cache hit rate. This benchmark represents \nthe worst case in performance for a particular working set size, since access is entirely random and \nno real work is done between heap accesses. Figure 8 shows the performance of the SPE and PPE cores for \nthis benchmark. The SPE s performance initially surpasses that of the PPE. However, as expected, cache \nmisses on the SPE core are more expensive than on the PPE core, due to the caching being performed in \nsoftware, rather than being under hard\u00adware control. Once the size of the working set grows larger than \nthe amount of local memory reserved for the SPE s data cache (96KB), its performance degrades severely. \nThe PPE core has a larger data cache (256KB in its L2 cache). Its performance does suffer after the working \nset size increases above this cache size, however, not so severely as on the SPE core. For the maximum \nworking set size of 16MB, the overhead due to cache misses reduces the SPE core s perfor\u00admance to about \nan eighth of its original value, while the PPE core s performance drops by a half. 8 4 7 3.5 6 3 5 2.5 \n4 2  Percentage Slowdown Runtime (s) Percentage Slowdown Runtime (s) 3 1.5 2 1 1 0.5 0 0Working Set \nSize (bytes) Number of Methods (a) Absolute performance.(a) Absolute performance. 1 1  0.8 0.6 0.4 0.2 \n0.8 0.6 0.4 0.2 0 8k 16k 32k 64k 128k 256k 512k 1M 2M 4M 8M 16M Working Set Size (bytes) (b) Slowdown \nrelative to 8k working set. Figure 8. The effect of a thread s data working set on per\u00adformance.  6.2.3 \nCode Caching Overheads Method invocation also involves caching of code from main memory. To investigate \nthe performance of the software caching scheme used by Hera-JVM, a micro-benchmark was developed in which \nthe amount of code executed can be var\u00adied, while the same amount of real work is done. This bench\u00admark \nperforms three million method invocations, randomly selecting which method to invoke from a set of available \nmethods. Every method in this set performs the same opera\u00adtion (incrementing a local variable). However, \neach is com\u00adpiled to separate machine code, therefore, its code is cached separately when run. By varying \nthe number of methods in the set, the amount of code which the benchmark executes can be varied, without \naltering the amount of real work that it performs. Figure 9 shows the performance of this benchmark on \nboth the SPE and PPE cores. Once the working set of meth\u00adods that this benchmark invokes grows beyond \n1024, it can no longer .t in the SPE s local memory cache. Performance on the SPE core drops to about \none .fth of its original value, since almost every method invocation will need to re-cache the method \ns code, as it is likely to have been evicted since the method was last called. The benchmark s performance \ndoes not suffer as severely on the PPE core, again due to its dedicated caching hardware.  6.3 Real \nWorld Benchmarks In this section, a selection of benchmarks from three real world benchmark suites are \nused to evaluate Hera-JVM in a realistic setting. To provide a range of applications with different types \nof behaviour, benchmarks were selected from: SpecJVM 2008 [21], a suite which mimics a variety of general \npurpose applications; the Java Grande Parallel 0 32 64 128 256 512 1024 2048 4096 8192 Number of Methods \n(b) Slowdown relative to 32 methods. Figure 9. The effect of a thread s code working set on performance. \nbenchmark suite [22], which aims to replicate high perfor\u00admance computing workloads, such as scienti.c, \nengineering or .nancial applications; and the Dacapo 2006 benchmark suite [5], which focuses on memory \nhungry benchmarks. The following benchmarks were run under Hera-JVM: mandelbrot generates an 800x600 \npixel image of the man\u00addelbrot set (this benchmark is not part of any of the benchmark suites). JavaGrande: \nmol dyn performs a molecular dynamics par\u00adticle simulation, using the Lennard-Jones potential. JavaGrande: \nmonte carlo performs a .nancial simulation, using Monte Carlo techniques to price products derived from \nthe price of an underlying asset. JavaGrande: ray trace renders a scene containing 64 spheres, using \na 3D ray tracer with a resolution of 150x150 pixels. Spec: fft performs Fast Fourier Transformation, \nusing a one-dimensional, in-place algorithm with bit-reversal and Nlog(N) complexity. Spec: lu computes \nthe LU factorization of a dense, in-place matrix using partial pivoting. It uses a linear algebra kernel \nand dense matrix operations on a 100x100 matrix. Spec: monte carlo approximates the value of Pi by com\u00ad \nv puting the integral of the quarter circle y = 1 - x2. Spec: sor simulates the Jacobi successive over-relaxation \nalgorithm for a 250x250 grid data set. Spec: sparse performs matrix multiplication on an unstruc\u00adtured \nsparse matrix in compressed-row format with a pre\u00adscribed sparsity structure. Spec: compress compresses \nand decompresses 3.36MB of data, using a modi.ed Lempel-Ziv method. Spec: mpegaudio decodes six MP3 .les \nwhich range in size from 20KB to 3MB. Speedup on SPE vs. PPE 2.5 2 1.5 1 0.5 0 Figure 10. Performance \ncomparison between benchmarks running on a single SPE core, and running on the single PPE core. Dacapo: \nantlr parses multiple grammar .les, and generates a parser and lexical analyzer for each. Dacapo: hsqldb \nuses JDBC to invoke an in-memory, SQL relational database, modelling a banking application. In future \nwork, Hera-JVM will be augmented so that it can automatically choose the most appropriate core type on \nwhich to execute blocks of code, based upon program be\u00adhaviour; however, before doing so, it is necessary \nto un\u00adcover the relative performance of each core type under dif\u00adferent workloads. Therefore, in the \nfollowing experiments, the timed portion of each of these benchmarks was executed on either the PPE core \nor the SPE core in its entirety (other than forced migrations due to invocation of native code), to compare \ntheir relative performance. The only modi.cation required to execute these bench\u00admarks on the SPE cores \nwas to split a small number of ex\u00adceptionally long methods into multiple smaller methods, so that they \ncould .t in the SPE s code cache in their entirety. Developing a code caching scheme which splits a method \ninto multiple cacheable blocks would remove the need for these modi.cations. 6.3.1 Single Threaded Performance \nFigure 10 shows the difference in the performance of these benchmarks when they are run on a single SPE \ncore, versus the PPE core. The error bars represent the standard deviation between ten runs on each core \ntype. There is a wide variation in the performance of the benchmarks between core types, from a 2.25x \nincrease in SPEC: sor on the SPE core, to a 3x slowdown for DACAPO: hsqldb. The mandelbrot, Java Grande \nSuite and SpecJVM 2008 suite (other than SPEC: compress) all perform well on the SPE core. These benchmarks \nare of a similar workload to that which the SPE was designed to support: computation\u00adally intensive scienti.c \nor multimedia centric workloads. The SPEC: compress and Dacapo benchmarks do not per\u00adform as well on \nthe SPE core. The common trait linking these benchmarks is that they access large amounts of data, thus \nexercising the software cache on the SPE. SPEC: sor SPEC: lu JG: mol dyn Floatng SPEC: sparse Integer \nmandelbrot Branch SPEC: ft Stack SPEC: monte carlo Local Memory SPEC: mpegaudio JG: ray trace Main \nMemory JG: monte carlo SPEC: compress DACAPO: antlr DACAPO: hsqldb  Figure 11. Percentage of cycles \nspent executing different classes of machine instructions on SPE. Benchmarks are ordered upwards by increasing \nSPE performance. To further investigate how a program s behaviour af\u00adfects its performance on the different \ncore types, a simulator was used to calculate the percentage of time the SPE core spends executing different \nclasses of machine instructions. Figure 11 shows this breakout by instruction type for each benchmark. \nThe benchmarks are ordered by their perfor\u00admance on the SPE core, relative to the PPE core, with the \nbest performing benchmark at the top. Benchmarks which perform well on the SPE core also generally spend \nmore of their time executing .oating point or integer-based calculations. Benchmarks which spend a large \nproportion of time accessing data elements in the heap (the local memory and main memory categories) \nperform more poorly on the SPE core than the PPE core. This is especially prominent when those accesses \nresult in cache misses or write operations which require DMA operations to main memory. 6.3.2 The Effect \nof Cache Size By default, the size of software data and code caches on the SPE core are .xed at 92KB \nand 88KB respectively. These sizes were chosen to give roughly equal weighting to caching of code and \ndata by default. However, applications do not necessarily access the same amounts of heap data as code. \nTherefore, these applications may bene.t from having a different proportion of local memory reserved \nfor each cache. Figure 12 show how the performance is affected as this ratio is altered for an interesting \nsubset of the benchmarks (results for all benchmarks are presented in [12]). The effect of cache size \non hit rate for both data and code accesses is also shown in these .gures. The cross formed by the dotted \nlines in these .gures shows the performance at the default cache sizes. The relationship between a benchmark \ns performance and this ratio falls into four main categories: Peaked: The ray trace and mpegaudio benchmarks \nshow a peak in performance, when roughly an equal percentage of memory is reserved for each cache. These \nbenchmarks are equally affected by small code or data caches. How\u00ad 1 1 1 0.95 0.99 0.9 0.95 0.98 0.85 \n0.970.8 0.9 0.75 0.96 Speedup vs. default SPECache Hit Rate Speedup vs. default SPECache Hit Rate Speedup \nvs. default SPECache Hit Rate Speedup vs. default SPECache Hit Rate Speedup vs. default SPECache Hit \nRate Speedup vs. default SPECache Hit Rate 0.7 0.65 0.850.95 0.94 0.6 0.8 Code Cache Code Cache Code \nCache 0.93 0.55 Data Cache Data Cache Data Cache 1.1 1.1 1.25 0.5 0.92 0.75 1.5 1.1 1.05 1.2 1 1 0.9 \n0.9 0.8 0.8 0.7 0.7 0.6 0.6 1.4 1 0.95 1.3 0.9 1.2 0.85 1.1 0.8 0.75 1 Speedup vs. PPE Speedup vs. PPE \nSpeedup vs. PPE 1.15 1.1 1.05 1 0.95 0.5 0.5 0.9 0.7 0.9 0.4 0.65 0.85 0 32 64 96 128 160 180 - Code0 \n32 64 96 128 160 180 - Code0 32 64 96 128 160 180 - Code180 148 116 84 52 20 0 - Data 180 148 116 84 \n52 20 0 - Data 180 148 116 84 52 20 0 - Data Code / Data Cache Size (KB) Code / Data Cache Size (KB) \nCode / Data Cache Size (KB) (a) JavaGrande: ray trace (b) SpecJVM: mpegaudio (c) Dacapo: antlr 1 1 1 \n0.98 0.9 0.96 0.94 0.92 0.9 0.99 0.98 0.5 Code Cache Code Cache Code Cache 0.88 Data Cache Data Cache \nData Cache 1.4 1.1 1.02 0.4 0.86 0.97 0.45 1.71.05 1 Speedup vs. PPE Speedup vs. PPE Speedup vs. PPE \n1.6 1 0.98 0.96 0.4 0.35 0.3 0.95 1.5 0.9 0.85 0.94 1.4 0.92 0.9 0.88 1.3 0.8 1.2 0.75 0.860.70.25 1.1 \n 0.7 0.65 0.84 0 32 64 96 128 160 180 - Code 0 32 64 96 128 160 180 - Code 0 32 64 96 128 160 180 - Code \n180 148 116 84 52 20 0 - Data 180 148 116 84 52 20 0 - Data 180 148 116 84 52 20 0 - Data Code / Data \nCache Size (KB) Code / Data Cache Size (KB) Code / Data Cache Size (KB) (d) Dacapo: hsqldb (e) SpecJVM: \nfft (f) SpecJVM: sparse Figure 12. The effect of varying the proportion of local memory reserved for \nuse by the data and code caches. ever, the plateau shape of these two graphs suggests that the working \nset of both code and data for both bench\u00admarks .ts comfortably in the local memory provided by the SPE \ncore. Rising: The performance of the JavaGrande: monte carlo and Dacapo benchmarks rises as the proportion \nof mem\u00adory provided to cached code increases. The working set of code required by these benchmarks is \nclearly too large to completely .t into the SPE s local memory, leading to this behaviour. Falling: The \nperformance of the mol dyn, fft, lu and com\u00adpress benchmarks are more heavily affected by the size of \nthe data cache. These benchmarks would seem to bene.t from an even larger data cache size than can be \nprovided by the SPE s local memory. Flat: Finally, the SPEC: monte carlo, sor and sparse bench\u00admarks \nexhibit little variation in performance as the cache sizes change, except at extremely small cache sizes. \nThese benchmarks therefore have very small working sets. Given the different behaviours of these benchmarks, \nit is clear that no .xed segregation of the code and data caches will provide the best performance for \nall applications. One possible solution to this would be to mix code and data in a single larger cache. \nThe problem with this approach is that the data cache must be purged on thread synchro\u00adnization operations, \nwhereas the code cache need not. With a single shared cache, either the code must be needlessly purged \nalongside data, or a more complex cache allocation and purging scheme must be employed. Another approach \nwould be to provide Hera-JVM with the capability to dy\u00adnamically alter the code / data cache ratio, based \nupon run\u00adtime monitoring of a program s cache hit rates. The provi\u00adsion of such a system for Hera-JVM \nis left for future work; the default code / data cache ratio of 88KB / 92KB is used in all subsequent \nexperiments. 6.3.3 Scalability The preceding experiments evaluated the performance of the benchmarks \nwhen run on a single SPE core compared with single PPE core. However, the Cell processor contains eight \nSPE cores and can provide signi.cantly more com\u00adputing power if an application can be parallelised. Other \nSpeedup 6 5 4 3 2 1 0  Number of Cores Number of Cores Number of Cores (a) Java Grande Parallel benchmarks(b) \nSpecJVM scimark benchmarks(c) Remaining benchmarks Figure 13. Scalability of benchmarks running on SPE \ncores. 14 than the two Dacapo benchmarks, these benchmarks have all been developed with scalability in \nmind. The SpecJVM benchmarks scale by running multiple instances of the same benchmark simultaneously, \nand therefore only evaluate the scalability of the runtime system itself. However, the Man\u00ad dlebrot and \nJavaGrande benchmarks scale by having mul\u00ad tiple threads coordinate on a single instance of the bench- \nSpeedup on 6 SPEs vs. PPE 12 10 8 6 4  mark, therefore these benchmarks also evaluate interaction 2 \n0 and synchronization between application threads running under Hera-JVM. The Cell processor in the Playstation \n3 used for these ex\u00adperiments only provides six SPE cores for user applications Figure 14. Performance \nof each benchmark running on all (one core is disabled, due to manufacturing defects, and the other runs \na secure hypervisor). Figure 13 shows the speedup obtained by each of the benchmarks as they are scaled \nfrom one to six SPE cores. 6 SPE cores compared to a single PPE core. migrated between the CPU and GPU \ncores as workload or Most of the benchmarks scale well as the number of SPE cores increase. However, \nthe lu, fft and both monte carlo benchmarks stop scaling after four cores. The reason these benchmarks \nstop scaling is due to garbage collection. These benchmarks allocate a large amount of data during their \nexecution. Since the garbage collector currently runs on only the PPE core, this leads to a scaling bottleneck. \nFigure 14 provides an overview of the performance of running each benchmark on all six SPE cores, compared \nwith running on the single PPE core. For those benchmarks which scale, running on all six SPE cores provides \nfrom a 3x to a 13x speedup, compared to running on the single PPE core. 7. Related Work The abstraction \nof heterogeneous processing resources has been examined by a large body of related work over the years. \nOne of the most widely available types of heteroge\u00adneous processing resource is the Graphics Processing \nUnit (GPU) present in most commodity computer systems. As GPUs have become more capable, a number of \nsystems, such as Cuda [18], Sieve C++ [7] and OpenCL [14], have been developed to enable general purpose \napplications to ex\u00adploit a GPUs potential processing performance. While these systems abstract many of \nthe dif.culties involved in writ\u00ading general purpose code for GPUs, they are relatively in\u00ad.exible; a \nprogram s threads of execution cannot easily beresource requirements change. Data transfers must also, \ntyp\u00adically, be controlled explicitly by the program. A number of projects have investigated techniques \nto aid programing of the Cell processor. The Cell Superscalar (CellSs) framework [16] enables the developer \nto identify blocks of code that would bene.t from execution on the SPE cores and then automatically schedules \ntask blocks on the appropriate core type, by employing a task dependency graph. However, the developer \nmust decide ahead of time which core type to use for a given piece of code, reduc\u00ading .exibility in moving \na thread s execution to a differ\u00adent core type dynamically at runtime as workload require\u00adments change. \nThe developer is also expected to have in\u00addepth knowledge of the Cell processor s unusual architecture \nto exploit it effectively, reducing the appeal of this model for mainstream developers. The CellVM [15] \ntakes a similar approach to Hera-JVM, by supporting the execution of applications written in Java on \nboth of the Cell processor s core types. However, Hera-JVM provides a number of features which are not \nfound in CellVM: in CellVM each Java thread is bound to a sin\u00adgle SPE core and cannot be transparently \nmigrated between core types as in Hera-JVM; CellVM is structured as two dif\u00adferent runtime systems (one \nfor each core type), complicat\u00ading maintenance and integration; CellVM s SPE core type runtime system \nsupports only a limited subset of the Java language speci.cation, relying on the PPE core to perform \noperations such as thread synchronization and object allo\u00adcation, which limits scalability; and CellVM \ndoes not pro\u00advide the coherence guarantees that are required by the Java memory model. These limitations \nmean that CellVM does not support signi.cant real-world Java applications and can only run parallel applications \nwhere the threads do not share access to data objects. Some proposed multi-core architectures provide \ncores that have symmetric instruction set architectures but are asymmetric in their performance. Saez \net al. [19] describe the CAMP scheduler, which uses a utility function to opti\u00admize system-wide performance, \nby placing CPU intensive workloads on fast cores and memory intensive workloads on the slower cores. \nOther work has investigated similar approaches to those used in Hera-JVM, but for different reasons. \nCashmere [23] provides software-based, pseudo-coherent shared memory similar to Hera-JVM, but on a different \nscale (multi-node clusters instead of multi-core processors). Intel s many-core runtime McRT [20] currently \ntargets symmetric multi-core architectures, however, their sequestered mode, where ap\u00adplication threads \nrun bare metal on processing cores, is similar to our execution of Java threads on the SPE cores. 8. \nDiscussion and Future Work The Cell processor is a challenging architecture upon which to develop general \npurpose software. However, by abstract\u00ading the details of this architecture behind a JVM, Hera-JVM hides \nthe heterogeneous nature of the Cell processor and the unusual features of the SPE core. This enables \ndevelopers to write application code for this challenging architecture in the same manner as they would \nfor more conventional archi\u00adtectures, while still gaining a performance bene.t from the architectures \nheterogeneity. Of course, this abstraction does not come for free. The wide impedance mismatch between \na JVM abstraction and the SPE core s architecture means that applications running under Hera-JVM cannot \ntake take full advantage of the SPE core s potential performance. However, the techniques de\u00adscribed \nin this work, such as ef.cient stack management and software caching using high level type information, \nenable the SPE core to provide better performance than the PPE core for the majority of the benchmarks \nwith which Hera-JVM was evaluated. Given that Hera-JVM is targeting main\u00adstream application developers, \nwho would otherwise be un\u00adlikely to exploit the Cell processor s SPE cores at all, the cost of providing \nthis abstraction would appear to be justi.ed. The SPE core s simple design means that it requires much \nless area on a silicon die to implement. On the Cell processor die, the PPE core requires roughly the \nsame area of silicon as 4 SPE cores. Thus, if an application can be parallelised it can be executed on \nmany more SPE cores than PPE cores, for the same sized processor. The scalability results show that, \nfor those benchmarks which scale, even the worst per\u00adforming benchmark provides a 3x speedup when running \non four SPE cores, compared to running on a single PPE core, thus providing signi.cantly better performance \nfor the same silicon area. Finally, the ability of Hera-JVM to transparently migrate a thread between \nthe PPE and SPE core types provides the runtime system with the .exibility to fully utilise the het\u00aderogeneous \ncore types of the Cell processor under varying conditions and workloads. Future work will investigate \ntech\u00adniques which enable the runtime system to automatically se\u00adlect the most appropriate core type on \nwhich to schedule dif\u00adferent threads and phases of a given application s execution, based upon runtime \nmonitoring of the application s execu\u00adtion behaviour. Acknowledgments This work was funded by the Carnegie \nTrust for the Uni\u00adversities of Scotland. We would also like to thank Microsoft Research for providing \nfunding for some of the equipment used in this work. Many thanks to Tim Harris, Simon Pey\u00adton Jones and \nRachel Lo for their invaluable advice and feed\u00adback on early drafts of this paper, which greatly improved \nthe presentation of this work. References [1] M. Adiletta, M. Rosenbluth, D. Bernstein, G. Wolrich, and \nH. Wilkinson. The Next Generation of Intel IXP Network Processors. Intel Tech. Journal, 6(3), 2002. [2] \nT. Ainsworth and T. Pinkston. Characterizing the Cell EIB On-Chip Network. IEEE Micro, 27(5):6 14, 2007. \n[3] B. Alpern, S. Augart, S. Blackburn, M. Butrico, A. Cocchi, P. Cheng, J. Dolby, S. Fink, D. Grove, \nM. Hind, et al. The Jikes Research Virtual Machine project: building an open-source research community. \nIBM Systems Journal, 44(2):399 417, 2005. [4] G. Amdahl. Validity of the single processor approach to \nachieving large scale computing capabilities. In Proceedings of the Spring Joint Computer Conference, \npages 483 485, 1967. [5] S. Blackburn, R. Garner, C. Hoffmann, A. Khang, K. McKin\u00adley, R. Bentzur, A. \nDiwan, D. Feinberg, D. Frampton, S. Guyer, et al. The DaCapo benchmarks: Java benchmarking development \nand analysis. In Proceedings of the 21st Confer\u00adence on Object-Oriented Programming Systems, Languages, \nand Applications (OOPSLA 06), pages 169 190, 2006. [6] T. Chen, R. Raghavan, J. N. Dale, and E. Iwata. \nCell Broad\u00adband Engine Architecture and its First Implementation: A Per\u00adformance View. IBM Journal of \nResearch and Development, 51(5):559 572, 2007. [7] A. Donaldson, C. Riley, A. Lokhmotov, and A. Cook. \nAuto\u00adparallelisation of Sieve C++ programs. Lecture Notes in Computer Science, 4854:18, 2008. [8] M. \nHill and M. Marty. Amdahl s Law in the Multicore Era. Computer, 41(7):33 38, 2008. [9] H. Hofstee. Power \nef.cient processor architecture and the cell processor. 11th International Symposium on High-Performance \nComputer Architecture (HPCA-11), pages 258 262, 2005.  [10] J. Manson, W. Pugh, and S. V. Adve. The \nJava Memory Model. In Proceedings of the 32nd Symposium on Princi\u00adples of Programming Languages (POPL \n05), pages 378 391, 2005. [11] J. A. Mathew, P. D. Coddington, and K. A. Hawick. Analysis and development \nof Java Grande benchmarks. In JAVA 99: Proceedings of the ACM 1999 conference on Java Grande, pages 72 \n80. ACM, 1999. [12] R. McIlroy. Using Program Behaviour to Exploit Hetero\u00adgeneous Multi-Core Processors. \nPhD thesis, Department of Computing Science, The University of Glasgow, 2010. [13] R. McIlroy and J. \nSventek. Hera-JVM: Abstracting Processor Heterogeneity Behind a Virtual Machine. In Workshop on Hot Topics \nin Operating Systems (HotOS), 2009. [14] A. Munshi. The OpenCL Speci.cation. Khronos OpenCL Working Group, \n2009. [15] A. Noll, A. Gal, and M. Franz. CellVM: A Homogeneous Virtual Machine Runtime System for a \nHeterogeneous Single-Chip Multiprocessor. In Workshop on Cell Systems and Ap\u00adplications, June 2008. [16] \nJ. Perez, P. Bellens, R. Badia, and J. Labarta. CellSs: Making it easier to program the Cell Broadband \nEngine processor. IBM Journal of Research and Development, 51(5):593 604, 2007. [17] D. Pham, S. Asano, \nM. Bolliger, M. Day, H. Hofstee, C. Johns, J. Kahle, A. Kameyama, J. Keaty, Y. Masubuchi, et al. The \ndesign and implementation of a .rst-generation CELL pro\u00adcessor. IEEE Solid-State Circuits Conference, \n2005. [18] S. Ryoo, C. Rodrigues, S. Baghsorkhi, S. Stone, D. Kirk, and W. Hwu. Optimization principles \nand application per\u00adformance evaluation of a multithreaded GPU using CUDA. In Proceedings of the 13th \nSymposium on Principles and Practice of Parallel Programming (PPoPP 08), pages 73 82, 2008. [19] J. Saez, \nM. Prieto, A. Fedorova, and S. Blagodurov. A Com\u00adprehensive Scheduler for Asymmetric Multicore Processors. \nIn Proceedings of EuroSys 10, 2010. [20] B. Saha, A. Adl-Tabatabai, A. Ghuloum, M. Rajagopalan, R. Hudson, \nL. Petersen, V. Menon, B. Murphy, T. Shpeisman, E. Sprangle, et al. Enabling scalability and performance \nin a large scale CMP environment. In Proceedings of EuroSys 07, pages 73 86, 2007. [21] K. Shiv, K. \nChow, Y. Wang, and D. Petrochenko. SPECjvm2008 Performance Characterization. In Proceedings of the 2009 \nSPEC Benchmark Workshop on Computer Perfor\u00admance Evaluation and Benchmarking, pages 17 35. Springer, \n2009. [22] L. Smith, J. Bull, and J. Obdrizalek. A parallel Java Grande benchmark suite. In Proceedings \nof the Conference on Super\u00adcomputing (SC 01), 2001. [23] R. Stets, S. Dwarkadas, N. Hardavellas, G. Hunt, \nL. Kon\u00adtothanassis, S. Parthasarathy, and M. Scott. Cashmere-2L: software coherent shared memory on a \nclustered remote-write network. In Proceedings of the 16th Symposium on Operating Systems Principles \n(SOSP 97), pages 170 183, 1997.   \n\t\t\t", "proc_id": "1869459", "abstract": "<p>Heterogeneous multi-core processors, such as the IBM Cell processor, can deliver high performance. However, these processors are notoriously difficult to program: different cores support different instruction set architectures, and the processor as a whole does not provide coherence between the different cores' local memories.</p> <p>We present Hera-JVM, an implementation of the Java Virtual Machine which operates over the Cell processor, thereby making this platforms more readily accessible to mainstream developers. Hera-JVM supports the full Java language; threads from an unmodified Java application can be simultaneously executed on both the main PowerPC-based core and on the additional <i>SPE</i> accelerator cores. Migration of threads between these cores is transparent from the point of view of the application, requiring no modification to Java source code or bytecode. Hera-JVM supports the existing Java Memory Model, even though the underlying hardware does not provide cache coherence between the different core types.</p> <p>We examine Hera-JVM's performance under a series of real-world Java benchmarks from the SpecJVM, Java Grande and Dacapo benchmark suites. These benchmarks show a wide variation in relative performance on the different core types of the Cell processor, depending upon the nature of their workload. Execution of these benchmarks on Hera-JVM can achieve speedups of up to 2.25x by using one of the Cell processor's SPE accelerator cores, compared to execution on the main PowerPC-based core. When all six SPE cores are exploited, parallel workloads can achieve speedups of up to 13x compared to execution on the single PowerPC core.</p>", "authors": [{"name": "Ross McIlroy", "author_profile_id": "81351606978", "affiliation": "Microsoft Research, Cambridge, United Kingdom", "person_id": "P2354047", "email_address": "", "orcid_id": ""}, {"name": "Joe Sventek", "author_profile_id": "81342513026", "affiliation": "University of Glasgow, Glasgow, United Kingdom", "person_id": "P2354048", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1869459.1869478", "year": "2010", "article_id": "1869478", "conference": "OOPSLA", "title": "Hera-JVM: a runtime system for heterogeneous multi-core architectures", "url": "http://dl.acm.org/citation.cfm?id=1869478"}