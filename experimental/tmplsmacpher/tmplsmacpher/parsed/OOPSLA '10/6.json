{"article_publication_date": "10-17-2010", "fulltext": "\n A Simple Inductive Synthesis Methodology and its Applications Shachar Itzhaky * Sumit Gulwani Neil \nImmerman Tel-Aviv University Microsoft Research University of Massachusetts shachar@tau.ac.il sumitg@microsoft.com \nimmerman@cs.umass.edu Mooly Sagiv Tel-Aviv and Stanford Universities msagiv@acm.org Abstract Given a \nhigh-level speci.cation and a low-level program\u00adming language, our goal is to automatically synthesize \nan ef\u00ad.cient program that meets the speci.cation. In this paper, we present a new algorithmic methodology \nfor inductive synthe\u00adsis that allows us to do this. We use Second Order logic as our generic high level \nspeci.cation logic. For our low-level languages we choose small application-speci.c logics that can be \nimmediately translated into code that runs in expected linear time in the worst case. We explain our \nmethodology and provide examples of the synthesis of several graph classi.ers, e.g, linear-time tests \nof whether the input graph is connected, acyclic, etc. In an\u00adother set of applications we automatically \nderive many .nite differencing expressions equivalent to ones that Paige built by hand in his thesis \n[Pai81]. Finally we describe directions for automatically combining such automatically generated building \nblocks to synthesize ef.cient code implementing more complicated speci.cations. The methods in this paper \nhave been implemented in Python using the SMT solver Z3 [dMB]. Categories and Subject Descriptors I.2.2 \n[Arti.cial In\u00adtelligence]: Automatic Programming Program Synthesis; * This work was supported in part \nby Israeli Academy of Science This work was supported in part by NSF grants CCF-0541018, CCF\u00ad0830174, \nand IIS-0915071 This work was supported in part by grants NSF CNS-050955 and NSF CCF-0430378 with additional \nsupport from DARPA. Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. OOPSLA/SPLASH 10, October 17 21, 2010, Reno/Tahoe, Nevada, USA. Copyright c &#38;#169; \n2010 ACM 978-1-4503-0203-6/10/10. . . $10.00 D.1.2 [Programming Techniques]: Automatic Programming; \nD.3.1 [Programming Languages]: Formal De.nition and Theory Semantics General Terms Languages, Performance \nKeywords Finite Differencing, High Level Program, In\u00adductive Synthesis, Transformational Programming \n1. Introduction We describe an algorithmic methodology that takes a high level speci.cation and a low-level \ntarget language, and searches for an ef.cient implementation of the speci.cation. If there is no such \nimplementation then our methodology reports failure. The input is a speci.cation in an expressive logic. \nA naive syntax-directed translation of this speci.cation into machine code usually produces an inef.cient \nand sometimes exponential-time implementation. Instead, we automatically convert the input speci.cation \ninto an equivalent low-level speci.cation in the target language. Using a simple syntax\u00addirected translation, \nthe resulting low-level speci.cation is then converted into imperative code that is guaranteed run in \nexpected linear time in the worst case. The high level speci.cations are written in subsets of second-order \nlogic (SO), second-order existential logic (SO.) which by Fagin s Theorem expresses exactly those proper\u00adties \ncheckable in NP, and .rst-order logic plus transitive closure (FO(TC)) which expresses exactly the properties \ncheckable in NSPACE[log n] [Imm99]. We describe two simple target languages, both of which can express \nonly linear-time properties, one for expressing graph properties, and another for expressing properties \nof sets. Of course, our algorithm cannot succeed if there is no program in the target language that implements \nthe speci.\u00adcation in question. However, when there is such an imple\u00admentation, we do succeed in a large \nnumber of cases. In the .rst problem domain we automatically generate linear-time graph classi.ers for \nmany simple properties such as connectivity, acyclicity, etc. In the second domain, we automatically \ngenerate constant-time .nite differencing ex\u00adpressions for many of the examples that Paige worked out \nby hand in his Ph.D. thesis [Pai81]. Our methods are fairly simple and general. We use the SMT solver \nZ3 to generate structures satisfying the speci.\u00adcations. Using these models we learn candidate implementa\u00adtions \nin the low-level languages that are correct on the gen\u00aderated structures. Then we use the solver repeatedly \nto see if there exist any other examples that satisfy the original spec\u00adi.cations but for which the candidate \nimplementation fails. When there are no further counterexamples, we are done. We feel that it is surprising \nhow well this simple method works to .nd asymptotically optimal algorithms which are implied by the high-level \nspeci.cations, but de.nitely not obvious from the speci.cations. Indeed a naive implementa\u00adtion of the \nhigh-level speci.cations would lead to very inef\u00ad.cient algorithms. While many people have tried to do \nautomatic synthe\u00adsis, much of our inspiration comes from the work of Bob Paige and Jack Schwartz on automatically \ngenerating ef.\u00adcient data structures and algorithms for speci.cations written in the very high-level \nprogramming language SETL [Pai81]. An important analogy of our work is the automatic optimiza\u00adtion of \nSQL, in which the speci.cation in SQL (essentially .rst-order logic plus counting, FO(COUNT) [Imm99, \nThm 14.9]) is transformed into a logically equivalent speci.ca\u00adtion, also in SQL but with better run \ntime. In our setting, since the speci.cation language is SO a language exponen\u00adtially more powerful \nthan FO the possible transformations include a much wider range of possibilities and dif.culties. This \npaper is organized as follows: \u00a72 provides back\u00ad ground and de.nitions. \u00a73 explains our synthesis method\u00ad \nology at a high level. In \u00a74 we explain the details of an implementation of the methodology. \u00a75 shows \nour .rst ap\u00adplications: learning ef.cient code to check whether input graphs have certain properties. \nIn \u00a76 we automatically gen\u00ad erate numerous examples of .nite differencing code equiva\u00adlent to code that \nPaige worked out by hand in his Ph.D. thesis [Pai81]. In \u00a77 we brie.y discuss how we generate program \ncode from the synthesized low-level speci.cations. In \u00a78 we discuss some related work. In \u00a79 we conclude \nand suggest some future directions of this work. 2. Background We use standard notation from mathematical \nlogic. For background in descriptive complexity we recommend [Imm99]. All our logical structures are \n.nite and ordered. For ex\u00adample, an ordered graph, G, is a .nite logical structure: G = ([n],E), whose \nuniverse of vertices is the set |G| = [n]= {0, 1,...n - 1}, and whose edge relation is a subset of the \nset of ordered pairs of vertices: E .|G|2. In this case the vocabulary of G consists of a single binary \nrelation symbol, s = {E2}, which we sometime describe as an input relation symbol. We use STRUC[s] to \ndenote the set of all .nite, ordered structures of vocabulary s, and STRUC=k[s] denotes the subset of \nthese structures with universe size at most k. If sI . s is a larger vocabulary, and if A. STRUC[s] and \nAI . STRUC[sI] is identical to A except that it also interprets the symbols of sI - s then we say that \nAI is an expansion of A to sI and we write A < AI. If A. STRUC[s] and a, . are formulas of vocabulary \ns, then we write A|= . to mean that A satis.es ., and a f . to mean that there is a proof of . from assumption \na. We write a = . to mean that a is equivalent to ., i.e., a f . and . f a. The logical languages we \nconsider include .rst-order logic, FO; .rst-order logic plus transitive closure, FO(TC) (the closure \nof .rst-order logic under the transitive closure operation, i.e, if we can express the binary relation \nR, then we can also express its transitive closure, R+, and its re\u00ad.exive, transitive closure, R*); .rst-order \nlogic plus a least\u00ad.xed-point operator, FO(LFP) (The least-.xed-point opera\u00adtor formalizes the process \nof making inductive de.nitions, so FO(LFP) is the closure of .rst-order logic under the ability to de.ne \nnew relations by induction.); second-order existen\u00adtial logic, SO.; and full second-order logic, SO. \nWe assume that these languages have access to the numeric ordering re\u00adlation (=) and the numeric constant \nsymbols, 0, 1,..., max. It is well known that natural logical languages capture natural complexity classes, \nfor example, FO = CRAM[1], FO(TC)= NSPACE[log n], FO(LFP)= P, SO. = NP, and SO = PH. Here CRAM[1] is \nthe set of problems checkable in constant time by a parallel random access machine with polynomially \nmuch hardware, PH is the polynomial-time hierarchy. Thus, SO is an extremely rich, very expressive algorith\u00admic \nlanguage, which we use as the main input to our tool. In the future we plan to develop an equally expressive, \nbut more user-friendly speci.cation language. 3. A Methodology for Inductive Synthesis We start with \na speci.cation . . SO and a target language L. We are hoping to derive an a . L such that a = .. Thus \na will be an ef.cient implementation of the speci.cation, .. We have an input vocabulary, s, which contains \nall of the numeric and input predicate, constant, and function symbols. For example, the numeric symbols \nmight be 0, 1, max, =2 . The input symbols might be E2,r, for a rooted graph with constant symbol, r, \ndenoting a speci.ed root node. The output vocabulary sI consists of s together with the output symbols. \nFor example, for topological sort, sI would include the output function symbol, g1, that denotes a topological \nordering, and for the minimum spanning tree algorithm, sI, would include the relation symbol, T 2, denoting \nthe tree edges. In the .nite differencing examples, we usually have input relations symbols such as E2,S1, \nand their new values one step later as the output, i.e., EI2,SI1 . Note that the superscripts denoting \narity are shown in the vocabularies, but are omitted when these symbols are used in formulas. Our synthesis \nalgorithm works in two settings both of which come up often in practice. Sometimes we are asked to compute \na well-de.ned single valued function, e.g., con\u00adnected components, strongly connected components, etc., \nin this deterministic case, the desired answer is unique. How\u00adever, sometimes the problem corresponds \nto a relation and any answer or the answer that there is no solution is what is desired, e.g., minimum \nspanning tree, topological sort, depth-.rst search, max network .ow, and, of course, SAT. In this nondeterministic \ncase, there may be more than one correct answer. The two cases are distinguished as fol\u00adlows: . is a \ndeterministic speci.cation. In this setting, for all A. STRUC[s] there exists a unique expansion A < \nAI . STRUC[sI] such that AI |= .. In this case we wish to synthesize a formula a . L such that a = .. \n . is a nondeterministic speci.cation. In this setting, for all A. STRUC[s] there exists at least one \nexpansion A < AI . STRUC[sI] such that AI |= .. In this case we wish to synthesize a formula a . L such \nthat a f . and for all A. STRUC[s] there exists at least one expansion A < AI . STRUC[sI] such that AI \n|= a.  While our methodology can handle both cases, all the ex\u00adamples in this paper are deterministic \nspeci.cations. Thus we make the exposition simpler by assuming we are always given a deterministic speci.cation \n. . SO. We .rst gen\u00aderate a set of instance structures M = {A1,... Ak}. STRUC=n[sI] such that M|= .. \nHere k, n, the initial num\u00adber of structures, and the upper bound on the number of ele\u00adments in the universe \nof each structure, are parameters. In this paper we make the simplifying assumption that our target language \nL consists of conjunctions from the set of base formulas, B. We .rst compute a set of good formulas, \nG . B, having the property that M|= G, i.e., every instance structure satis.es every good formula. Next \nwe use a greedy algorithm to compute a minimal cover w.r.t. M, C . G. C is a cover w.r.t. M iff M|= C \nand for all A.M, C determines all the output bits of A. In symbols,  M|= C and .A . Mc. .s(A) f .sI \n(A) (1) c.C Here, .t (A), is the diagram of A, i.e., the conjunction of all ground literals (from the \nvocabulary t together with constants for the elements of the universe of A) that are satis.ed by A. The \ngreedy algorithm incrementally chooses C by suc\u00adcessively choosing an element of G that determines the \nmax-Once we have such a cover, C, it gives us a candidate  n a = c.C c. To determine whether a is a \ncorrect candidate, we ask two questions of the SMT solver. 1. (.A . STRUC=n[sI])(A|= . .\u00aca) 2. (.A . \nSTRUC=n[sI])(A|= ..(a..s(A) f .sI (A)) That is, (1) Is there a small instance for which a is not good? \nand (2) Is there a small instance for which a does not determine the answer? If the answer to either \nof these questions is, yes , then we add the instance structure to M, and repeat the above construction. \n4. An Implementation of the Methodology We now explain our more detailed algorithm that imple\u00adments the \nabove methodology. Fig. 1 shows a .owchart of the algorithm, and pseudocode is shown in Fig. 2. We now \n.ll in the details of each step. The main phases of the algo\u00adrithm are: 1. Generate M. STRUC=n[sI], a \nset of instance struc\u00adtures that all satisfy the speci.cation ..  Inductive-SynthesisL(.) M := GENERATE-INSTANCES(.); \ndo { t := Synthesize-From-InstancesL(M); } while (Re.ne-Query(., t, M)) Figure 2. The main counterexample-guided \nre.nement loop for synthesizing ef.cient programs. 2. Find a cover C w.r.t. M (Eqn. 1). n 3. Let a = \nc.C c. Test whether there is a new instance structure, i.e., A. STRUC=n[sI] and A|= ., such that a is \nnot good for A, or a does not determine the answer for A. If so, add A to M and repeat, otherwise output \na. 4.1 Generate the Instances In both the .rst and the last step we are required to search for models \nof the speci.cation .. For all examples in this paper, the speci.cations are fully characterized by .nite \nstructures, and in fact fairly small such structures. We will .x a parameter, n, throughout this paper \nto bound the size of these structures. For all the examples we have tested so far, n = 10 has been suf.cient. \nOnce we have a formula a that passes the test in phase (3), we have found no further counterexamples \nof size = 20. We use the subset SO. of SO to express our speci.ca\u00adtions. By Fagin s Theorem (SO. = NP) \nit follows that we can phrase the searches for structures in (1) and (3) as sin\u00adgle calls to a SAT solver \n[Imm99, Thms 7.8, 7.16]. The tar\u00ad get languages, L, that we use are always a small subset of SO.n SO. \nand thus queries that involve . and a, or \u00aca, can still be translated to a single instance of SAT, see \n\u00a7A for details.  4.2 Synthesize the Cover Given the set of instances, M, we must build a cover, C, satisfying \nEqn. 1. In the two problem domains explored in this paper, the target languages, L1 in \u00a75 and L2 in \u00a76, \nare sets of conjunctions of base formulas, B1 . L1, B2 . L2. Recall that a formula, \u00df, is good w.r.t. \nM iff M|= \u00df, i.e., it satis.es all the current instances. Let GM denote the set of good base formulas \nand assume for the sake of discussion that GM is .nite. n Now, if there is any cover, then . = \u00df.GM \u00df \nis a cover because it is the strongest good formula. However, note that . would not be an appropriate \ncandidate for a because typically GM and thus . will be huge. Furthermore, . would probably include too \nmuch information about the particular chosen instances. Instead it makes sense to use the principle of \nOccam s razor, and search for the smallest cover, i.e., the smallest Synthesize-From-InstancesL(M) Good \n:= {\u00df | \u00df a base formula of L, .A . MA |= \u00df}Find a minimal subset n C . Good such that c.C c determines \nall output bits; return C Figure 3. Compute Minimal Cover. good formula that determines the output for \nall the given instances. Recall from \u00a72 that the universe of each structure of size n is [n]= {0,...,n \n- 1} and that we have the numeric constants, 0, 1,..., max, available. Assume for simplicity in the following \ndiscussion that the output vocabulary sI - s consists of a single unary function symbol, f. (Additional \nfunction symbols would be treated similarly, and relations would be treated as boolean functions.) Let \nA. STRUC[sI], i . |A| and let a . L. We say that a determines f(i) for A if there is a value j . |A| \nsuch that a . .s(A) f f(i)= j The following proposition is just a restatement of the cover property \n(Eqn. 1): PROPOSITION 4.1. If for all A.M, A|= a and for all i . |A|, a determines f(i) for A, then a \nis a cover. It is relatively straightforward to keep track of which output bits are determined by the \ncurrent set, C, of base formulas. The process SYNTHESIZE-FROM-INSTANCES is de.ned in Fig. 3. It uses \na greedy algorithm to .nd a minimal cover, i.e., it chooses a good base formula that adds the largest \nnumber of points to the determined set and adds that formula to C until C is a cover. Note that it could \nbe the case that there is no cover. This can only happen if there is no formula a . L that is equivalent \nto .. In this case our procedure will report failure. 4.3 Re.ne-Query An a found by the previous phase \nmay still be incorrect since it is only guaranteed to produce correct results for the in\u00adstances represented \nby M. To test for the existence of in\u00adstances outside of M for which a violates the speci.cation ., we \nuse the subroutine GENERATE-INSTANCE again, but instead of just trying to .nd instances satisfying the \nspeci.\u00adcation, we attempt to .nd models of . that do not satisfy a, or for which a does not determine \nthe answer. This is imple\u00admented in the procedure Refine-Query shown in Fig. 4. 5. Deriving Graph Classi.ers \nIn this section, we apply the methodology of \u00a73 to derive ex\u00ad pected linear-time algorithms for checking \nproperties of di\u00adrected graphs. Such algorithms have many applications. For example, they can be used \nto dynamically check runtime as\u00adTable 1. Graph-classi.ers with their input speci.cations, synthesized \nformulas, number of instances needed and time to do synthesis. A global integrity constraint is that \nno relation has self-loops. Name Example Input Speci.cation (+ Integrity Constaints) Synthesized Formula \nInst. Time SLL Singly Linked List 1:1 N . .u(\u00acN+(u, u)) root r via N functional N #pN (r) = 0 . .v(#pN \n(v) = 1) 4 14 sec CYCLE Cyclic Linked List .u, v(N*(u, v)) root r via N functional N #pN (r) = 1 4 11 \nsec DLL Doubly Linked List 1:1 F . 1:1 B . .u, v (F (u, v) . B(v, u)) . \u00acF +(u, u) root r via F functional \nF, B #pF (r) = 0 . .v(sF (v) = pB(v)) 18 149 sec TREE Directed Tree 1:1 C . .u(\u00acC(u, r)) root r via C \n#pC (r) = 0 . .v(#pC (v) = 1) 8 21 sec TREEPP Tree with Parent Ptr. 1:1 C . .u, v (C(u, v) . P (v, u)) \n. \u00acC(u, r) root r via C functional P #sP (r) = 0 . .v(sP (v) = pC (v)) 26 181 sec TREERP Tree with Root \nPtr. 1:1 C . .u, v \u00acC(u, s) . \u00acR(r, u) . (u = s . R(u, r)) root r via C functional R #pC (r) = 0 . pR(r) \n= sC+ (r) . .v(#pC (v) = 1) 48 359 sec SC Strongly Connected .u, v E*(u, v) root r via E pE (r) = sE \n(r) 4 9 sec  Re.ne-QueryL(., a, M) example := GENERATE-INSTANCE(a not good or does not determine answer) \nif (example = null) add example to M return (example = null) Figure 4. Procedure to Generate New Instances. \nAbbrev. Meaning self-loop-free N .u(\u00acN(u, u)) root r via N .u(N*(r, u)) functional N .u, v, x N(x, u) \n. N(x, v) . u = v 1:1 N .u, v, x N(u, x) . N(v, x) . u = v Table 2. Common Abbreviations. The .rst three \nare used vertex, v, the cardinality of that set is at most one. Note that the formula is checkable in \nlinear time by doing a depth .rst search and recording the number of edges labeled N that leave each \nvertex. 5.2 Target Language L1 Let s be a vocabulary that includes the constant symbol, r, which is assumed \nto be present and a root throughout this section. The target language L1(s) or just L1 when s is understood \n is based on single traversals of the graph, clas\u00adsifying the paths along the way using regular expressions. \nL1 is a two-sorted .rst-order logic, with sorts nodes and sets of nodes. Refer to the formal de.nition \nof L1 in Table 3. As function symbols, L1 has set constructors s. and p., which are maps from nodes to \nsets of nodes, based on regular expressions (.) for classifying paths. In order to make sure that all \nL1 statements can be imple\u00adas integrity constraints. The last one is used in some of the mented with \nlinear time complexity, and in order to limit the speci.cations. search space, several restrictions \napply: L1 has only one variable, v, ranging over nodes. sertions for programs that manipulate dynamicaly \nallocated data structures. When the starting point is a constant, . is restricted to the We ask the \nreader to familiarize herself with the seven ex-following set of regular expressions: .v (v)=1 ample \ninput speci.cations to our algorithm shown in Table 1. A * B * ,A * B * C, A * B, A * BC * R(s)= 222 \nA,B,C.s All these speci.cations are in .rst-order logic plus transitive closure (FO(TC)), a subset of \nSO. (Later on, we may abbreviate A*A as A+.)  5.1 Integrity Constraints and Abbreviations When the \nstarting point is a variable, . is restricted to: In the speci.cations shown in Table 1, certain integrity \ncon- A2 . sS(s)=A straints are assumed in addition to the written formulas. When we specify root r via \nN we assume that every ver\u00adtex is reachable from the root by following edges labeled N . When a relation, \nN, is speci.ed as functional, we as\u00adsume that it has outdegree at most one. Furthermore, unless otherwise \nstated, we make the default assumption that no re\u00adlation has self-loops. In addition to the integrity \nconstraints, The second restriction is somewhat arbitrary and was chosen to reduce the size of the search \nspace, while still considering the most common cases. In order to understand why the last restriction \nis impor\u00adtant, consider the following formula: pA we also use the abbreviaton 1:1 N to meant that N \nis one\u00adto-one. Evaluating pA (v) for all v essentially means construct\u00ad ing the entire relation A* . \nThe cost of this procedure is O(n2 lg n). In order to de.ne the semantics of L1 we need some These abbreviations \nand their meanings are given in Ta\u00ad ble 2. Note that they may all be checked in linear time. We assume \nthat the integrity constraints are part of the input speci.cation and the synthesized speci.cation. Since \nthe speci.cations in Tables 1 and 2 make use of notation involving regular expressions. DEFINITION 5.1. \nFor a graph G . STRUC[s] and a regu\u00ad transitive closure and doubly nested quanti.ers, some of e lar \nexpression e . R(s), we write x . y to mean that there their naive implementations would run in cubic \ntime. (The situation becomes even more interesting when in later sec\u00adtions we use second-order quanti.cation \nbecause then the naive implementation would have exponential runtime.) Table 1 also shows the linear-time \nspeci.cations in L1 that our tool automatically synthesized. We next de.ne the language L1. To help the \nreader get a feeling for L1 we give an example .rst. The base formula .v(#sN (v) = 1) express the fact \nthe the relation N is functional. The function symbol sN (v) denotes the set of vertices that are successors \nof v via an edge labeled N. Thus the formula says that for every is a path from vertex x to vertex y \nsuch that the resulting sequence of edge labels is an element of L(e). EXAMPLE 5.2. Let G = ([4],AG,BG,rG) \nbe a struc\u00adture of vocabulary s =(A2,B2,r) such that AG = G {(0, 1), (1, 2), (3, 0)}, BG = {(0, 3), \n(2, 3)}, and s=0. Then AA G |=0 A. 0 . r . 2 .\u00acr . 3 . The functions of L1(s) mapping nodes to sets \nof nodes are de.ned as follows, (stmt) ::= (clause) . \u00b7 \u00b7 \u00b7 . (clause) . d (clause) ::= (atom) | .v \n(atom) | .v (v = r . (atom)) (atom) ::= (int) = (const) | (int) = (const) | (set) = (set) (int) ::= (const) \n| #(set) (const) ::= 0 | 1 (set) ::= {r} | se(r) | pe(r) | e . R(s) g . S(s) s\u00a3(v) | p\u00a3(v) Table 3. \nGrammar for L1(s). DEFINITION 5.3. For each regular expression e . R(s) the function symbols se,pe are \navailable in L1(s) and have meaning, ee se(i)= ji . jpe(i)= jj . i In words, se(i) is the set of e-successors \nof i, i.e., those points reachable from i via a path of label L(e); and pe(i) is the set of e-predecessors \nof i. EXAMPLE 5.4. Using the graph G from Example 5.2, sA (r)= {0, 1, 2}  sA+ (r)= {1, 2}  sAB(r)= \n{3}  pB(3) = {0, 2}  The symbol d is a boolean-valued constant that is true iff the graph satis.es \nthe speci.ed property. Thus instances satisfying the speci.cation will have d true while instances not \nsatisfying the speci.cation will have d false. The term, #A denotes the cardinality of set A. Although \n(stmt) has non-bounded length, it is possible to search through all statements in L1 using the following \nprocedure: 1. From the set of instances, M, disinguish the positive ones, P = A A|= . , from the negative \nones, N = A A|= \u00ac. 2. Enumerate all (clause)s, picking up the formulas . such that .A . P A|= .. 3. \nFrom the set of .1 \u00b7\u00b7\u00b7 .k found in the previous step, .nd a minimal conjunction . = .i1 . \u00b7 \u00b7\u00b7 . .it \nsuch that .A . N A|= \u00ac..  This makes sure that for every instance A. M, the for\u00admula . identi.es with \nthe speci.cation: A|= . .. A|= .. The generated low-level speci.cation, a, will therefore be d .. .. \nTHEOREM 5.5. Every element of the language L1 runs in expected linear time in the worst case. Proof: \nThis follows from the fact that each set se(r) or pe(r) can be computed in linear time via a depth .rst \nsearch starting at r. Furthermore, each set of sets s\u00a3(v) or p\u00a3(v), v . [n] has a total number of elements \nbounded by the number of edges in the input graph, and can be computed in linear time by examining each \nedge of label g once. Finally, by maintaining the sets via hash tables, we can test equality of sets \nin expected linear time. D 5.3 Result Summary The current implementation is Python-based and does not \nmeet high standards for ef.ciency, but still, the running time is in the order of minutes rather than \nhours. The number of instances shows how many examples were needed before the correct formula was synthesized. \nWe suspect that the reason so many examples were needed in the case of TREERP and TREEPP is that on small \nexamples there were many alternate characterizations of the extra pointers, P, R. We are con.dent that \nthis algorithm will be similarly successful in quickly and automatically deriving linear-time tests for \nmany other simple properties of graphs and data structures. 6. Finite Differencing In his Ph.D. thesis, \nBob Paige considered the common pro\u00adgramming situation in which an expression, C = f(x1,...,xk) (2) \nis repeatedly evaluated in a block of code after some of the variables xi may have been slightly modi.ed \n[Pai81]. It may be the case that a slight change to the variable xi, xi = e, results in a slight change \nfrom C to CI so that it is much easier to compute the change incrementally than to I recompute CI = \nf(x1,...,xi,...,xk) from scratch. If so, Paige says informally that C is continuous with respect to this \nchange, and he calls the code that incrementally computes CI the formal derivative of C with respect \nto the change. Paige calls the part of the code that goes before the change xi = e the pre-derivative \n(.-(C, xi = e)) and the part of the code that goes after the change he calls the post-derivative (.+(C, \nxi = e)). However, for simplicity we will assume that we have access to the before and after values, \nxi and I x. Thus we will refer to the code to compute CI and thus i reestablish the invariant Eqn 2 \nas the derivative of C w.r.t. the change xi = e,(.(C, xi = e)). See Table 4 for fourteen examples of \nformal derivatives, the code we automatically synthesized to evaluate them, and the required time. In \neach case the code which is constant time and thus asymptotically optimal is equivalent to the derivatives \nthat Paige computed by hand and listed in tables in his thesis. As in \u00a75, our algorithm follows the methodology \nde\u00ad scribed in \u00a73 and \u00a74. The program was written in Python making use of the built in implementation \nfor sets. 6.1 Target Language, L2 For each input vocabulary, s, the target language, L2(s) is similar \nto L1(s) but somewhat simpler. We again have a sin\u00adgle domain variable, v. For each relation symbol R \n(or func\u00adtion symbol f) from s that may change, the vocabulary, sI of L2(s) contains both R and RI (or \nf and fI) denoting the value before and after the change. When the target formula is the relation C, \nwe model this in L2 as the boolean function c. When the target formula is an integer, or boolean variable, \nc, we model it as a domain valued, or boolean valued function also called c. DEFINITION 6.1. The base \nformulas B2(s) consist of all universally quanti.ed literals or implications of literals from sI , B2(s)= \n.v(g1), .v(g1 . g2) g1,g2 literals from sI . From the formulas in B2(s) we derive a subset BB2(s) of \ndesirable formulas, using a simple .lter that ensures that all chosen formulas will yield a O(1)-run-time \nprocedure. (The formulas in BB2(s) are those such that any literal asserting a changed value, e.g., c \n(t), is restricted so that t is a constant, or if it is variable, v, then it is restricted by an assumption \nclause, g1(v), that can hold for at most one value of v.) L2 consists of arbitrary conjunctions of base \nformulas from BB2(s). 6.2 Results Besides being able to regenerate, for some base cases, re\u00adsults that \nwere previously done manually, the same method proves useful in some more advanced settings when using \na complex expression as the speci.cation, as seen in Table 5. Moreover, such expressions cannot be synthesized \nsimply by composing base rules in a deductive manner. To illustrate this, the .rst statement in the table, \n.uf(u)= vE(v, u) is a composition of the base constructs: 1. c = |S| 2. S = v . V.(v) 3. .u.(u)  Without \nany insight on the correlations among the par\u00adticles, a composition of the above 3 programs will have \nto deduce a loop iterating over V , and, for each u . V , runs the corresponding differencing program \nfor Su = v E(v, u) . This results in linear run-time for the entire pro\u00adgram. However, in this special \ncase, when a single edge is added to E, only a single node is in fact affected with re\u00adspect to f, and \nso this update can be computed in constant time We are pleased that for many examples we can automat\u00adically \ngenerate formal derivatives that run in constant time and are thus asymptotically optimal. This is a \nvalidation of our methodology and will be useful in future work on syn\u00adthesis. The automatic derivation \nof asymptotically optimal formal derivatives can be a useful building block for the au\u00adtomatic synthesis \nof ef.cient data structures and algorithms for a richer class of algorithmic speci.cations. 7. Code \nGeneration Actual, procedural program code can be generated directly from the formulas synthesized by \nthe framework. We do not go into the detail of how exactly this is done, but rather just mention that \nit is done by a series of syntactic transforma\u00adtions, and refer the reader to Table 6 for some entry-level \nexamples. 8. Related Work Counterexample Guided Inductive Synthesis Inductive synthesis refers to the \nprocess of generating a system from input-output examples. This process involves using each new input-output \nexample to re.ne the hypothesis about what the correct system should be until convergence is reached. \nInductive synthesis had its origin in the pioneer\u00ading work by Gold on language learning [Gol67] and by \nShapiro on algorithmic debugging and its application to au\u00adtomated program construction [Sha83]. The \ninductive ap\u00ad proach [Mug92, FP94] for synthesizing a program involves debugging the program with respect \nto positive and negative examples until the correct program is synthesized. The nega\u00adtive examples can \nbe counterexamples discovered while try\u00ading to prove a program s correctness. Counterexamples have been \nused in incremental synthesis of programs [SLTB+06] and discrete event systems [BMM04]. The technique \npre\u00ad sented in this paper is also a form of inductive synthesis, but applied in a novel program synthesis \nsetting. SAT/SMT Based Program Synthesis Recent advances in SAT/SMT technologies have revived interest \nin pro\u00adgram synthesis techniques. Recent work shows how to user SMT constraint based program veri.cation \ntechniques for synthesizing programs from .rst order logic speci.\u00adcations [SGF10]. In contrast, we allow \nfor more expres\u00ad sive second order logic constraints and use a very different methodology based on inductive \nsynthesis. Sketching[SLTB+06] uses SAT solvers to implement the inductive program syn\u00adthesis technique, \nbut requires the programmer to write a par\u00adtial program whose holes are limited to taking values over \n.nite domains. Though our inductive synthesis based tech\u00adnique also uses SAT solvers, it searches for \nfull programs (as opposed to values for holes) over the space of their rep\u00adresentations as logical formulas \n(as opposed to small .nite set of values for holes). Finite Differencing This paper was inspired in part \nby Bob Paige s work on transformational program. He used .nite differencing to try to automatically derive \nef.cient data structures and algorithms for high level speci.cations Expression Change Synthesized Derivative \nHandwritten Code Time C = T + S T + = {a} v = a . cI(v) = 1 v = a . cI(v) = c(v)) C += {a} 121.88 sec \nC = T + S S += {a} v = a . cI(v) = 1 v = a . cI(v) = c(v) C += {a} 121.94 sec C = T + S T -= {a} v = \na . cI(v) = c(v) \u00acT (a) . cI(a) = 0 T (a) . cI(a) = c(a) if a . S : C -= {a} 87.95 sec C = T + S S -= \n{a} v = a . cI(v) = c(v) \u00acS(a) . cI(a) = 0 S(a) . cI(a) = c(a) if a . T : C -= {a} 96.5 sec C = T - S \nT + = {a} v = a . cI(v) = c(v) \u00acS(a) . cI(a) = 1 S(a) . cI(a) = 0 if a . S : C += {a} 69.85 sec C = T \n- S T -= {a} c(v) = 0 . cI(v) = 0 v = a . cI(v) = c(v) v = a . cI(a) = 0 C -= {a} 115.57 sec C = T - \nS S += {a} c(v) = 0 . cI(v) = 0 v = a . cI(v) = c(v) v = a . cI(a) = 0 C -= {a} 117.0 sec C = T - S S \n-= {a} v = a . cI(v) = c(v) \u00acT (a) . cI(a) = 0 T (a) . cI(a) = 1 if a . T : C += {a} 115.57 sec C = f(S) \nS += {a} v = f(a) . cI(v) = c(v) v = a . cI(f(a)) = 1 C += {f(a)} 100.91 sec C = f-1(S) f(a) = b v = \na . cI(v) = c(v) S(b) . cI(a) = 1 \u00acS(b) . cI(a) = 0 if b . S : C -= {a}else : C += {a} 71.35 sec cS = \n#S S += {a} S(a) . cI S = cS \u00acS(a) . cS + 1 = cI S if a . S : cS += 1 70.28 sec cS = #S S -= {a} \u00acS(a) \n. cI S = cS S(a) . cI S + 1 = cS if a . S : cS -= 1 61.29 sec c = (#S == 0) S += {a} v = a . cI = 0 c \n= false 4.46 sec c = (#S == 0) S -= {a} cS = 1 . cI = c cI S = cS . c = cI cI S = 0 . cI = 1 if a . S \n: cS -= 1 c = (cS == 0) 7.59 sec Table 4. Finite differencing problems with their automatically synthesized, \nasymptotically optimal formal derivatives in L2, assumed to be universally quanti.ed conjuncts; When \nthe expression is a set C, c and cI are the characteristic functions of C before and after the change, \nrespectively. in SETL [Pai81, CP87]. Annie Liu is doing some notable work pushing this methodology forward \n[LT95, LS09]. 9. Conclusion We have shown that a simple, general inductive synthesis algorithm can automatically \ntranslate high-level algorithmic speci.cations into asymptotically optimal code. There is much further \nwork to do. In particular, This methodology as presented is widely applicable and should be used and \ntested in many settings to see how far it can go in this simple form. Once these limits are better understood, \nthere is room to test richer learning algorithms on richer target languages.  Most exciting to us is \nthe idea that building blocks such as automatic .nite differencing can let us take second order existential \nspeci.cations and derive ef.cient algorithms to maintain their implied invariants as we start with the \nempty graph and add edges incrementally until we have the entire input graph. This might also be an approach \nfor synthesizing incremental algorithms.   Expression Change Synthesized Derivative Time .u f(u) = \n{v | E(v, u)} E+ = {(u, v)} x = v . fI(x) = x \u00acE(u, v) . fI(v) = f(v) + 1 E(u, v) . fI(v) = f(v) 55.60 \nsec b = x.T g(x) T + = {a} \u00acT (a) . bI = b + g(a) T (a) . bI = b 33.35 sec b = max K K + = {c} \u00ac(c < \nb) . bI = c c < b . bI = b 16.35 sec ar: N . V, 1:1 .i f ar(i) = i exchange ar(b) . ar(c) c = b . fI(ar(b)) \n= b c = b . fI(ar(b)) = c c = b . fI(ar(c)) = b \u00ac(x = ar(c) . x = ar(b)) . fI(x) = f(x) 1075 sec Table \n5. Examples of more advanced .nite differencing programs that were synthesized using the same procedure. \nSynthesized Derivative Functional Representation _ Automatically Generated Code Handwritten Code v = \na . cI(v) = 1 v = a . cI(v) = c(v)) cI(v) = 1 v = a c(v) v = a c(a) := 1 C += {a} v = a . cI(v) = c(v) \n\u00acT (a) . cI(a) = 0 T (a) . cI(a) = c(a) cI(v) = . . . 0 v = a . a . T c(a) v = a . a . T c(v) v = a if \na . T : c(a) := 0 if a . T : C -= {a} v = a . cI(v) = c(v) \u00acS(a) . cI(a) = 1 S(a) . cI(a) = 0 cI(v) = \n. . . 1 v = a . a . S 0 v = a . a . S c(v) v = a if a . S : c(a) := 1 else : c(a) := 0 if a . S : C += \n{a} c(v) = 0 . cI(v) = 0 v = a . cI(v) = c(v) v = a . cI(a) = 0 cI(v) = . . . 0 v = a 0 c(v) = 0 c(v) \nv = a c(a) := 0 C -= {a} v = a . cI(v) = c(v) \u00acT (a) . cI(a) = 0 T (a) . cI(a) = 1 cI(v) = . . . 0 v \n= a . a . T 1 v = a . a . T c(v) v = a_ if a . T : c(a) := 1 else : c(a) := 0 if a . T : C += {a} v = \nf(a) . cI(v) = c(v) v = a . cI(f(a)) = 1 cI(v) = 1 v = f(a) c(v) v = f(a) c f(a) := 1 C += {f(a)} v = \na . cI(v) = c(v) S(b) . cI(a) = 1 \u00acS(b) . cI(a) = 0 cI(v) = . . . 1 v = a . b . S 0 v = a . b . S c(v) \nv = a _ if b . S : c(a) := 1 else : c(a) := 0 if b . S : C -= {a}else : C += {a} S(a) . cI S = cS \u00acS(a) \n. cS + 1 = cI S cI S(v) = cS + 1 a . S cS a . S_ if a . S : cI S := cS + 1 if a . S : cS += 1 \u00acS(a) . \ncI S = cS S(a) . cI S + 1 = cS cI S(v) = cS - 1 a . S cS a . S if a . S : cI S := cS - 1 if a . S : cS \n-= 1 v = a . cI = 0 cI = 0 c := 0 c = false cS = 1 . cI = c cI S = cS . c = cI cI S = 0 . cI = 1 cI = \n. . . 1 cI S = 0 c cS = 1 c cS = cI S if cI S = 0 : c := 1 if a . S : cS -= 1 c = (cS == 0) Table 6. \nCode generated from synthesized derivative A. Propositional Encoding of First and Second-Order Formulas \nIn this appendix, we explain how to use a SAT solver to test whether there exists a model of a given \nsize for a second\u00adorder exsitential formula, .. Our construction is immediate from the proof of Cook \ns Theorem from Fagin s Theorem in [Imm99, Thms 7.8, 7.16]. PROPOSITION A.1. Given a second-order existenital \nfor\u00admula . and a positive integer n, we can construct a boolean formula . such that . . SAT iff F has \na model of size n. Proof: Let s take as an example F =.R1. with .rst\u00adorder part . =.x.y(E(x, y) . R(y)). \nWe want to know if there exists a structure A = ([n],EA) that satis.es .. That is the same question as \nwhether there exists a structure B = ([n],EB,RB) that satis.es .. To guess such a structure, B, we must \nguess the an\u00adswers to n2 + n binary questions, i.e, whether E(0, 0) holds, whether E(1, 0) holds, . . \n. , whether E(n - 1,n - 1) holds, whether R(0), . . . , whether R(n - 1) holds. Thus in the boolean formula \n. that we construct there will be n2 + n boolean variables: E(0, 0),E(1, 0),...,E(n - 1,n - 1); R(0),...,R(n \n- 1). The boolean formula . will assert that the chosen structure B satis.es .: n-1 nn-1 . = (E(i, j) \n. R(j)) . i=0 j=0 We believe that the reader will be able to synthesize the general algorithm from this \none example1. Note that the formula . can be constructed in time linear in its size. The size of . is \nO(nmax(a,r)|.|) where a is the maximum arity of the relations existentialy quanti.ed, and r is the depth \nof nesting of .rst-order quanti.ers in .. D 1 One of us has been accused of being too terse in his writing. \nFor him, the elegance and simplicity of Proposition A.1 is embedded in the simple ex\u00ad ample given. Other \ncoauthors said, What about the cases when a function or a numeric relation occurs in .? Well, each value \nof a function is de\u00adtermined by log n bits, so we need n log n boolean variables to encode a unary function. \nFor numeric relations, if x<y occurs in ., then in . it would be replaced by i<j for each .xed value \nof i and j. For example, 3 < 7 would then be replaced by true and 5 < 2 would be replaced by false. \nReferences [BMM04] B.A. Brandin, R. Malik, and P. Malik. Incremen\u00adtal veri.cation and synthesis of discrete-event \nsystems guided by counter examples. Control Systems Tech\u00adnology, 12(3), May 2004. [CP87] Jiazhen Cai \nand Robert Paige. Binding performance at language design time. In POPL, pages 85 97, 1987. [dMB] Leonardo \nde Moura and Nikolaj Bj\u00f8rner. Z3 an ef.cient smt solver. http://research. microsoft.com/en-us/um/redmond/ \nprojects/z3/. [FP94] Pierre Flener and Lubos Popelmnsky. On the use of inductive reasoning in program \nsynthesis: Prejudice and prospects. In LOBSTR 94. 1994. [Gol67] E. Mark Gold. Language identi.cation \nin the limit. Information and Control, 10(5):447 474, 1967. [Imm99] Neil Immerman. Descriptive Complexity. \nSpringer, New York, 1999. [LS09] Yanhong A. Liu and Scott D. Stoller. From datalog rules to ef.cient \nprograms with time and space guar\u00adantees. ACM Trans. Program. Lang. Syst., 31(6), 2009. [LT95] Yanhong \nA. Liu and Tim Teitelbaum. Systematic derivation of incremental programs. Sci. Comput. Pro\u00adgram., 24(1):1 \n39, 1995. [Mug92] Stephen Muggleton, editor. Inductive Logic Program\u00adming, volume 38 of The APIC Series. \nAcademic Press, 1992. [Pai81] Robert Paige. Formal Differentiation -A Program Synthesis Technique. UMI \nPress, 1981. [SGF10] Saurabh Srivastava, Sumit Gulwani, and Jeff Foster. From program veri.cation to \nprogram synthesis. In POPL, pages 313 326, 2010. [Sha83] Ehud Y. Shapiro. Algorithmic Program DeBugging. \nMIT Press, Cambridge, MA, USA, 1983. [SLTB+06] Armando Solar-Lezama, Liviu Tancau, Rastislav Bod\u00b4ik, \nSanjit Seshia, and Vijay Saraswat. Combinato\u00adrial sketching for .nite programs. In ASPLOS, 2006.   \n\t\t\t", "proc_id": "1869459", "abstract": "<p>Given a high-level specification and a low-level programming language, our goal is to automatically synthesize an efficient program that meets the specification. In this paper, we present a new algorithmic methodology for inductive synthesis that allows us to do this.</p> <p>We use Second Order logic as our generic high level specification logic. For our low-level languages we choose small application-specific logics that can be immediately translated into code that runs in expected linear time in the worst case.</p> <p>We explain our methodology and provide examples of the synthesis of several graph classifiers, e.g, linear-time tests of whether the input graph is connected, acyclic, etc. In another set of applications we automatically derive many finite differencing expressions equivalent to ones that Paige built by hand in his thesis [Pai81]. Finally we describe directions for automatically combining such automatically generated building blocks to synthesize efficient code implementing more complicated specifications.</p> <p>The methods in this paper have been implemented in Python using the SMT solver Z3 [dMB].</p>", "authors": [{"name": "Shachar Itzhaky", "author_profile_id": "81470654446", "affiliation": "Tel-Aviv University, Tel-Aviv, Israel", "person_id": "P2354001", "email_address": "", "orcid_id": ""}, {"name": "Sumit Gulwani", "author_profile_id": "81100315615", "affiliation": "Microsoft Research, Seattle, WA, USA", "person_id": "P2354002", "email_address": "", "orcid_id": ""}, {"name": "Neil Immerman", "author_profile_id": "81408601113", "affiliation": "University of Massachusetts, Amherst, MA, USA", "person_id": "P2354003", "email_address": "", "orcid_id": ""}, {"name": "Mooly Sagiv", "author_profile_id": "81100150928", "affiliation": "Tel-Aviv and Stanford Universities, Tel-Aviv, Israel", "person_id": "P2354004", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1869459.1869463", "year": "2010", "article_id": "1869463", "conference": "OOPSLA", "title": "A simple inductive synthesis methodology and its applications", "url": "http://dl.acm.org/citation.cfm?id=1869463"}