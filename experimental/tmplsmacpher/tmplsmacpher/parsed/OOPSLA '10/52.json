{"article_publication_date": "10-17-2010", "fulltext": "\n Patterns and Statistical Analysis for Understanding Reduced Resource Computing Martin Rinard Henry \nHoffmann Sasa Misailovic Massachusetts Institute of Technology rinard@mit.edu Massachusetts Institute \nof Technology hank@csail.mit.edu Massachusetts Institute of Technology misailo@csail.mit.edu Stelios \nSidiroglou Massachusetts Institute of Technology stelios@csail.mit.edu Abstract We present several general, \nbroadly applicable mechanisms that enable computations to execute with reduced resources, typically at \nthe cost of some loss in the accuracy of the result they produce. We identify several general computational \npat\u00adterns that interact well with these resource reduction mech\u00adanisms, present a concrete manifestation \nof these patterns in the form of simple model programs, perform simulation\u00adbased explorations of the \nquantitative consequences of ap\u00adplying these mechanisms to our model programs, and relate the model computations \n(and their interaction with the re\u00adsource reduction mechanisms) to more complex benchmark applications \ndrawn from a variety of .elds. Categories and Subject Descriptors D.2.11 [Software Ar\u00adchitectures]: Patterns; \nD.2.4 [Software/Program Veri.ca\u00adtion]: Statistical Methods; D.3.1 [Formal De.nitions and Theory]: Semantics \nGeneral Terms Design, Measurement, Performance, Reli\u00adability, Veri.cation Keywords Reduced Resource Computing, \nDiscarding Tasks, Loop Perforation, Cyclic Memory Allocation, Statistical Analysis Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for pro.t or commercial advantage and that copies bear \nthis notice and the full citation on the .rst page. To copy otherwise, to republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. Onward! 2010, October 17 \n21, 2010, Reno/Tahoe, Nevada, USA. Copyright c &#38;#169; 2010 ACM 978-1-4503-0236-4/10/10. . . $10.00 \n1. Introduction The amount of available resources is a central factor in the existence of virtually all \nliving organisms. Mechanisms that adapt the operation of the organism to variations in resource availability \noccur widely throughout nature. For example, during prolonged starvation, the human body preserves mus\u00adcle \nmass by shifting its fuel source from proteins to ke\u00adtone bodies [3]. Peripheral vasoconstriction, which \nmini\u00ad mizes heat loss by limiting the .ow of blood to the ex\u00adtremeties, is a standard response to hypothermia. \nThe nasal turbinates in dehydrated camels extract moisture from ex\u00adhaled respiratory air, thereby limiting \nwater loss and enhanc\u00ading the ability of the camel to survive in dessicated environ\u00adments [31]. All of \nthese mechanisms take the organism away from its preferred operating mode but enable the organism to \ndegrade its operation gracefully to enhance its survival prospects in resource-poor environments. The \nvast majority of computer programs, in contrast, exe\u00adcute with essentially no .exibility in the resources \nthey con\u00adsume. Standard programming language semantics entails the execution of every computation the \nprogram attempts to per\u00adform. If the memory allocator fails to return a valid reference to an allocated \nblock of memory, the program typically fails immediately with a thrown exception, failed error check, \nor memory addressing error. This inability to adapt to changes in the underlying operating environment \nimpairs the .exi\u00adbility, robustness, and resilience of almost all currently de\u00adployed software systems. \nReduced resource computing encompasses a set of mech\u00adanisms that execute programs with only a subset \nof the resources (time and/or space) that the standard program\u00adming language semantics and execution \nenvironment pro\u00advides. Speci.c reduced resource computing mechanisms in\u00adclude:  Discarding Tasks: Parallel \ncomputations are often struc\u00adtured as a collection of tasks. Discarding tasks produces new computations \nthat execute only a subset of the tasks in the original computation [23, 24].  Loop Perforation: Loop \nperforation transforms loops to execute only a subset of the iterations in the original com\u00adputation \n[16, 20]. Different loop perforation strategies in\u00ad clude modulo perforation (which discards or executes \nev\u00adery nth iteration for some .xed n), truncation perforation (which discards either an initial or .nal \nblock of itera\u00adtions), and random perforation (which discards randomly selected iterations).  Cyclic \nMemory Allocation: Cyclic memory allocation allocates a .xed-size buffer for a given dynamic alloca\u00adtion \nsite [21]. At each allocation, it returns the next ele\u00ad ment in the buffer, wrapping back around to the \n.rst el\u00adement when it reaches the end of the buffer. If the num\u00adber of live objects allocated at the \nsite is larger than the number of elements in the buffer, cyclic memory alloca\u00adtion produces new computations \nthat execute with only a subset of the memory required to execute the original computation.  1.1 Resource \nReduction in Practice Unsurprisingly, these mechanisms almost always change the output that the program \nproduces. So they are appropriate only for computations that have some .exibility in the out\u00adput they \nproduce. Examples of such computations include many numerical and scienti.c computations, sensory appli\u00adcations \n(typically video and/or audio) that involve lossy en\u00adcoding and decoding, many machine learning, statistical \nin\u00adference, and .nance computations, and information retrieval computations. The relevant question is \nwhether these kinds of computations are still able to deliver acceptable output after resource reduction. \nInterestingly enough, our empirical results show that many of these computations contain components that \ncan successfully tolerate the above resource reduction mecha\u00adnisms the computation still produces acceptably \naccurate outputs after the application of these mechanisms to these components. And these resource reduction \nmechanisms can often endow computations with a range of capabilities that are typically otherwise available \nonly through the manual development of new algorithms. Speci.cally, discarding tasks has been shown to \nenable computations to tolerate task failures without retry [23], to produce accuracy and per\u00ad formance \nmodels that make it possible to purposefully and productively navigate induced accuracy versus performance \ntrade-off spaces (for example, maximizing accuracy subject to performance constraints or maximizing peformance \nsub\u00adject to accuracy constraints) [23], and to eliminate barrier idling at the end of parallel loops \n[24]. Cyclic memory al\u00ad location has been shown to eliminate otherwise potentially fatal memory leaks \n[21]. Loop perforation has been shown to reduce the overall execution time of the computation and to \nenable techniques that dynamically control the compu\u00adtation to meet real-time deadlines in the face of \nclock rate changes and processor failures [16]. A key to the successful application of these mechanisms \nin practice is identifying the components that can success\u00adfully tolerate resource reduction, then applying \nresource re\u00adduction only to those components. This empirical fact leads to usage scenarios in which the \nresource reduction mecha\u00adnisms generate a search space of programs close to the orig\u00adinal programs. An \nautomated (or semiautomated) search of this space .nds the components that can tolerate resource reduction, \nwith resource reduction con.ned to those compo\u00adnents when the computation executes. The remaining com\u00adponents \nexecute with the full set of resources with which they were originally designed to operate. The resulting \neffect is conceptually similar to the mechanisms that biological or\u00adganisms use to deal with reduced \nresources, which direct the delivery of scarce resources to those critical functions most necessary for \nsurvival.  1.2 Inherent Redundancy The success of reduced resource computing shows that many computations, \nlike biological organisms, have inherent sources of redundancy that enable them to operate success\u00adfully \nin the face of reduced resources. Note, however, that these sources of redundancy were not explicitly \nengineered into the computation they emerge as an unintended con\u00adsequence of the way the computation \nwas formulated. In this paper we analyze various sources of redundancy that enable these computations \nto tolerate resource reduction. The result of this analysis is several general computa\u00adtional patterns \nthat interact in very reasonable ways with the different resource reduction mechanisms. Viewing our com\u00adputations \nthrough the prism of these patterns helped us un\u00adderstand the behavior we were observing; we anticipate \nthat recognizing these patterns in other computations will facili\u00adtate the prediction of how these other \ncomputations will react to resource reduction. In the future, trends such as the increasing importance \nof energy consumption, the need to dynamically adapt to com\u00adputing platforms with .uctuating performance, \nload, and power characteristics, and the move to more distributed, less reliable computing platforms \nwill increase the need for com\u00adputations that can execute successfully across platforms with a range \nof (potentially .uctuating) available resources. Ini\u00adtially, we expect developers to let automated techniques \n.nd and exploit patterns in existing applications that interact well with resource reduction. They may \nthen move on to deploy\u00ading such patterns into existing applications to enhance their ability to function \neffectively in a range of environments. Ul\u00adtimately, we expect developers to engineer software systems \nfrom the start around patterns that interact well with resource reduction in much the same way that developers \nnow work with more traditional design patterns [12] in all phases of the engineering process.  1.3 \nContributions This paper makes the following contributions: Computational Patterns: It identi.es computational \npatterns that interact well with resource reduction mech\u00adanisms such as discarding tasks, perforating \nloops, and dynamically allocating memory out of .xed-size buffers. Understanding these patterns can help \ndevelopers de\u00advelop a conceptual framework that they can use to reason about the interaction of their \napplications with various resource reduction mechanisms.  Model Computations: It presents concrete manifesta\u00adtions \nof the general patterns in the form of simple model computations. These model computations are designed \nto capture the essential properties of more complex real\u00adworld applications that enable these applications \nto op\u00aderate successfully in the presence of resource reduction mechanisms.  In this way, the model computations \ncan give developers simple, concrete examples that can help them think pro\u00adductively about the structure \nof their applications (both existing and envisioned) and how that structure can affect the way their \napplications will respond to the application of different resource reduction mechanisms. The model computations \ncan also serve as the founda\u00adtion for static analyses that recognize computations that interact well \nwith resource reduction mechanisms. Such analyses could produce statistical models that precisely characterize \nthe effect of resource reduction mechanisms on the application at hand, thereby making it possible to \nautomatically apply resource reduction mechanisms to obtain applications with known statistical accuracy \nprop\u00aderties in the presence of resource reduction. Simulations: It presents the results of simulations \nthat use the model computations to quantitatively explore the impact of resource reduction on the accuracy \nof the re\u00adsults that the computations produce. The simulation re\u00adsults can help developers to better \nestimate and/or ana\u00adlyze the likely quantitative accuracy consequences of ap\u00adplying resource reduction \nmechanisms to their own ap\u00adplications.  Relationship to Applications: It relates the structure of the \nmodel computations and the simulation accuracy re\u00adsults back to characteristics of speci.c benchmark \nappli\u00adcations. Understanding these relationships can help de\u00advelopers better understand the relationships \nbetween the model computations and their own applications.  A New Model of Computation: Standard models \nof computation are based on formal logic [11, 14]. In these models, the computation is rigidly .xed by \nthe applica\u00adtion source code, with formulas in discrete formal logic characterizing the relationship \nbetween the input and out\u00adput. This paper, in contrast, promotes a new and fun\u00ad  damentally different \nmodel in which the computation is .exible and dynamic, able to adapt to varying amounts of resources, \nand characterized by (conceptually) contin\u00aduous statistical relationships between the input, output, \nand amount of resources that the computation consumes. Of course, almost every program has some hard \nlogical correctness requirements even a video encoder, for example, must produce a correctly formatted \nvideo .le (even though it has wide latitude in the accuracy of the encoded video in the .le). We therefore \nanticipate the de\u00advelopment of new hybrid analysis approaches which ver\u00adify appropriate hard logical \ncorrectness properties using standard program analysis techniques but use new statis\u00adtical techniques \nto analyze those parts of the computation whose results can (potentially nondeterministically) vary as \nlong as they stay within acceptable statistical accuracy bounds. 2. The Mean Pattern Consider the following \ncomputations: Search: The Search computation [7] from the Jade benchmark suite [28] simulates the interaction \nof electron beams with solids. It uses a Monte-Carlo simulation to track the paths of electrons, with \nsome electrons emerg\u00ading back out of the solid and some remaining trapped in\u00adside. The program simulates \nthe interaction for a variety of solids. It produces as output the proportion of elec\u00adtrons that emerge \nout of each solid. Each parallel task simulates some of the electron/solid interaction pairs.  String: \nThe String computation [13] from the Jade benchmark suite [25] uses seismic travel-time inversion to \ncompute a discrete velocity model of the geology be\u00adtween two oil wells. It computes the travel time \nof rays traced through the geology model, then backprojects the difference between the ray tracing times \nand the exper\u00adimentally observed propagation times back through the model to update the individual elements \nin the velocity model through which the ray passed. Each parallel task traces some of the rays.  The \ncore computations in both Search and String generate sets of numbers, then compute the mean of each set. \nIn Search, each number is either one (if the electron emerges from the solid) or zero (if the electron \nis trapped within the solid). There is a single set of ones and zeros for each solid; the output is the \nmean of the set. In String there is one set of numbers for each element of the discrete velocity model. \nEach number is a backprojected difference from one ray that traversed the element during its path through \nthe geology model. String combines the numbers in each set by computing their mean. It then uses these \nnumbers to update the corresponding elements of the velocity model. The resource reduction mechanism \nfor both computa\u00adtions, discarding tasks, has the effect of eliminating some of the numbers from the \nsets. It is possible to derive empir\u00adical linear regression models that characterize the effect of this \nresource reduction mechanism on the output that these two computations produce [23]. These models show \nthat dis\u00ad carding tasks has a very small impact on the output that the computation produces. Speci.cally, \nthe models indicate that discarding one quarter of the tasks changes the Search out\u00adput by less than \n3% and the String output by less than 1%; discarding half of the tasks (which essentially halves the \nrun\u00adning time) changes the Search output by less than 6% and the String output by less than 2%.  2.1 \nThe Model Computation Our model computation for these two computations simply computes the mean of a \nset of numbers: for (i = 0; i < n; i++) { sum += numbers[i]; num++; } mean = sum/num; The resource reduction \nmechanism for this model compu\u00adtation executes only a subset of the loop iterations to com\u00adpute the mean \nof a subset of the original set of numbers. The speci.c mechanism we evaluate (loop perforation) simply \ndiscards every other number when it computes the sum by executing only every other iteration in the model \ncomputa\u00adtion: for (i = 0; i < n; i += 2) { sum += numbers[i]; num++; } mean = sum/num; We evaluate the \neffect of this resource reduction mecha\u00adnism via simulation. Our .rst simulation works with .oating point \nnumbers selected from a continuous probability distri\u00adbution. Each trial in the simulation starts by \n.lling the ar\u00adray numbers with the values of n independent pseudoran\u00addom variables selected from the \nuniform distribution over the interval [0, 1]. It then computes the difference between the computed mean \nvalues with and without resource reduc\u00adtion i.e., the difference between the mean of all n values in \nthe numbers array and the mean of every other value in the numbers array. For each even value of n between \n10 and 100, we perform 1,000,000 such trials. Our second simulation works with integers selected from \na discrete probability distribution. Each trial .lls the ar\u00adray numbers with the values of n psuedorandom \nvariables selected from the uniform discrete distribution on the set {0, 1}. We then perform the same \nsimulation as detailed above for the continuous distribution. Figure 1 presents the results of these \ntrials. This .gure presents four graphs. The x axes for all graphs range over the sizes (values of n) \nof the sets in the trials. The upper left graph plots the mean of the differences between the re\u00adsults \nthat the standard and resource reduced computations produce (each point corresponds to the mean of the \nob\u00adserved differences for the 1,000,000 trials for sets of the cor\u00adresponding size) for the continuous \ndistribution. The upper right graph plots the variances of the differences for the con\u00adtinuous distribution. \nThe lower left graph plots the mean of the differences for the discrete distribution; the lower right \ngraph plots the corresponding variances. These numbers show that our model computation exhibits good \naccuracy properties in the presence of resource reduc\u00adtion. Speci.cally, for all but the smallest sets \nof numbers, resource reductions of a factor of two cause (in most cases substantially) less than a 10% \nchange in the result that the computation produces. We attribute this robustness in the face of resource \nreduction to a diffuse form of partial re\u00addundancy that arises from the interaction of the computation \nwith the data on which it operates. Because the numbers in the reduced resource computation are a subset \nof the com\u00adplete set of numbers and because the numbers are all drawn from the same probability distribution, \nthe two mean values are highly correlated (with the correlation increasing as the size of the sets increases). \nWe note that the graphs show that the accuracy of the resource reduced computation depends on the size \nof the set of numbers, with larger sets producing smaller differences and variances than larger sets \nof numbers. This phenomenon is consistent with our redundancy-based perspective, since smaller sets of \nnumbers provide our model computation with less redundancy than larger sets of numbers. The discrete \ndistribution has higher mean differences and variances than the continuous distribution, which we at\u00adtribute \nto the concentration of the weight of the discrete probability distribution at the extremes. We also \nnote that all of our simulation numbers are for resource reductions of a factor of two, which is much \nlarger than necessary for many anticipated scenarios (for example, scenarios directed towards tolerating \nfailures).  2.2 Relationship to Search and String The model computation provides insight into why the \nappli\u00adcations tolerate the task discarding resource reduction mech\u00adanism with little loss in accuracy. \nWhile discarding tasks may, in principle, cause arbitrary deviations from the stan\u00addard behavior of the \napplication, the underlying computa\u00adtional patterns in these applications (although obscured by the speci.c \ndetails of the realization of the patterns in each application) interact well with the resource reduction \nmech\u00adanism (discarding tasks). The .nal effect is that the resource reduction mechanism introduces some \nnoise into the com\u00adputed values, but has no other systematic effect on the com\u00adputation. And in fact, \nthe results from our model compu\u00adtation show that it is possible to discard half the tasks in the computation \nwith (depending on the size of the set of numbers) single digit percentage accuracy losses. This result \n  Figure 1. Means and Variances of Differences Between Standard and Resource Reduced Mean Computations \nfrom our model computation is consistent with the results from both the Search and String applications. \nThe larger mean differences and variances for the discrete distribution show that the Search application \n(in which each number is either 0 or 1) needs to work with larger sets than the String application (in \nwhich each number is a .oating point num\u00adber) to obtain similar accuracies under similar resource re\u00adduction \nfactors. 3. The Sum Pattern Consider the following computations: Water: Water evaluates forces and potentials \nin a sys\u00adtem of molecules in the liquid state [25]. Although the structure is somewhat obscured by the \napplication mod\u00adels the interactions between the water molecules, the core computations in Water boil \ndown to computing, then tak\u00ading the sum of, sets of numbers. For example, a key in\u00adtermolecular force \ncalculation computes, for each wa\u00adter molecule, the sum of the forces acting on that water molecule from \nall of the other water molecules. Water is coded as a Jade program [25], with each task computing, then \ntaking the sum of, a subset of the corresponding set of numbers.  Swaptions: Swaptions uses a Monte-Carlo \nsimulation to solve a partial differential equation to price a portfolio of swaptions [5]. The core computation \ntakes the sum of the results from the individual simulations. The application computes the .nal result \nby dividing the sum by the number of simulations.  The resource reduction mechanism for Water is discard\u00ading \ntasks [23, 24]; the resource reduction mechanism for Swaptions is loop perforation [16, 20]. In both \ncases the effect is a reduction in the result proportional to the num\u00adber of discarded tasks or loop \niterations. Unlike the Search and String computations discussed in Section 2, for Water and Swaptions \ndiscarding many tasks or loop iterations can therefore induce a large change in the overall result that \nthe computation produces. 3.1 The Model Computation The model computation for Water and Swaptions computes \nthe sum of a set of psuedorandom numbers. for (i = 0; i < n; i++) sum += numbers[i]; As for the mean \nmodel computation, the resource reduction mechanism is to discard every other iteration of the loop: \nfor (i = 0; i < n; i += 2) sum += numbers[i]; The effect is to divide the result (the value of the sum \nvari\u00adable) by approximately a factor of two. It is possible, how\u00adever, to use extrapolation to restore \nthe accuracy of the com\u00adputation [16, 23] -simply multiply the .nal result by two, or, more generally, \nthe number of tasks or loop iterations in the original computation divided by the number of tasks or \nloop iterations in the resource reduced computation. Note that the former number (the number of tasks \nor loop iterations in the original computation) is typically available in the resource reduced computation \nas a loop bound (for our model com\u00adputation, n) or some other number used to control the gener\u00adation \nof the computation. After extrapolation, the accuracy picture is essentially similar to the accuracy \npicture for the computations with the mean pattern (see Section 2) the extrapolated sum is simply the \nmean multiplied by the ap\u00adpropriate ratio.  3.2 Relationship to Water and Swaptions Water and Swaptions \ntolerate resource reduction for essen\u00adtially the same reason that Search and String tolerate re\u00adsource \nreduction all of these computations combine a set or sets of partially redundant numbers together into \na .\u00adnal result or results, with addition as the basic combination mechanism. In all of the applications \nthe resource reduction mechanism has the effect of eliminating some of the num\u00adbers from the combination, \nwith the redundancy present in the numbers enabling the computations to tolerate the elim\u00adination with \nsmall accuracy losses. 4. The Minimum Sum Pattern x264 is an H.264 video encoder from the Parsec benchmark \nsuite [5]. It encodes a sequence of frames, with each frame encoded as a collection of blocks (rectangular \nregions of the frame). One way to encode a block is to reference a block in a previously encoded frame, \nthen specify the contents of the block as a set of differences relative to the referenced block. The \nmore similar the blocks are, the more ef.cient the encoding. The search for a similar block in a previously \nencoded frame is called motion estimation [18]. x264 spends the majority of its time in motion estimation. \nThe following code fragment implements a comparison between an encoded block and a reference block. It \ncom\u00adputes the sum of a set of numbers designed to character\u00adize the visual difference between the two \nblocks (the vari\u00adable value holds the sum). This code block executes for a variety of potential reference \nblocks, with the computation choosing to use the block with the smallest sum as the .nal reference block \nfor the relative encoding (if that encoding is more compact than the standard discrete cosine transforma\u00adtion \nencoding). for (i = 0; i < h; i += 4) { for (j = 0; j < w; j += 4) { element_wise_subtract(temp, cur, \nref, cs, rs); hadamard_transform(temp, 4); value += sum_abs_matrix(temp, 4); } cur += 4*cs; ref += 4*rs; \n} return value; For this application, we consider the loop perforation resource reduction mechanism [16, \n20]. In this example loop perforation coarsens the comparison by changing the loop increments from 4 \nto 8. Note that because of the loop nesting there is a multiplicative effect on the running time the \nloop nest runs four, not just two, times faster. for (i = 0; i < h; i += 8) { for (j = 0; j < w; j += \n8) { element_wise_subtract(temp, cur, ref, cs, rs); hadamard_transform(temp, 4); value += sum_abs_matrix(temp, \n4); } cur += 4*cs; ref += 4*rs; } return value; We measure the quality of the encoder using two mea\u00adsures: \nthe peak signal to noise ratio and the bitrate of the en\u00adcoded video. Perforating the outer loop alone \nproduces an encoder that runs 1.46 times faster than the original. The peak signal to noise ratio decreases \nby 0.64% and the bi\u00adtrate increases by 6.68%. Perforating the inner loop alone makes the encoder run \n1.45 times faster, the peak signal to noise ratio decreases by 0.46%, and the bitrate increases by 8.85%. \nFinally, perforating both loops makes the encoder run 1.67 times faster, the peak signal to noise ratio \ndecreases by 0.87%, and the bitrate increases by 18.47%. All versions of the encoded videos are visually \nindistinguishable. 4.1 The Model Computation The model computation starts with a collection of m sets \nof n numbers. It .nds the set with the smallest sum. min = DBL_MAX; index = 0; for (i = 0; i < m; i++) \n{ sum = 0; for (j = 0; j < n; j++) sum += numbers[i][j]; if (min < sum) { min = sum; index = i; } } \nWe consider two resource reduction mechanisms for this computation. The .rst transforms the inner loop \n(over j) so that it performs only a subset of the iterations. The spe\u00adci.c transformation is loop perforation, \nwhich executes ev\u00adery other iteration instead of every iteration. The resource reduced computation increments \ni by 2 after each loop iter\u00adation instead of 1. The resulting computation takes the sum of only a subset \nof the numbers in each set (in this case the sum of every other number) rather than the sum of all of \nthe numbers. The computation may therefore compute the index of a set in which the sum of the remaining \nunconsidered numbers in the set may add up to a value large enough to make the sum of all of the numbers \nof the set larger than the sum of the set with the minimum sum. Conceptually, this transformation corresponds \nto the application of loop perforation to the x264 application as described above.  Figure 2. Mean \nDifferences With Resource Reduc-Figure 3. Variance in Differences With Resource Re\u00ad Figure 4. Mean Differences \nWith Resource Reduc-Figure 5. Variance in Differences With Resource Re\u00adtion on Number of Sets duction \non Number of Sets Figure 7. Mean Scaled Differences With Resource Figure 6. Mean Maximum Sums Reduction \non Each Set Figure 9. Mean Scaled Differences With Resource Figure 8. Mean Minimum Sums Reduction On \nNumber of Sets The second resource reduction mechanism transforms the outer loop (over i) so that it \nperforms only a subset of the iterations, once again using loop perforation so that the loop executes \nevery other iteration. The resulting computation does not consider all of the sets of numbers; it instead \ncon\u00adsiders only every other set. In this case the algorithm may not consider the set with the minimum \nsum and may com\u00adpute the index of a set with a larger sum. Each trial in the simulations .lls each of \nthe m sets with n numbers psuedorandomly selected from the uniform dis\u00adtribution on [0, 1/n]. Note that \nfor any n, the maximum pos\u00adsible value for the sum of the numbers in the set is 1; the minimum possible \nsum is 0. Each simulation runs 10,000 trials for each combination of m and n such that m and n are both \na multiple of 4 between 4 and 44 (inclusive). 4.2 Absolute Differences Our .rst set of simulations applies \nresource reduction to each set by perforating the inner loop in the model computation. Figure 2 presents, \nas a function of the number of sets m and the size of sets n the mean difference (over all trials) be\u00adtween \nthe sum of the set that the resource reduced computa\u00adtion .nds and the sum of the set that the original \ncomputation .nds. The mean differences grow as the size of the sets de\u00adcreases and (more slowly) as the \nnumber of sets increases. The variance of the differences (in Figure 3) exhibits a sim\u00ad ilar pattern. \nWe attribute this pattern to the fact that as the size of the sets decreases, the difference depends \non fewer numbers. The variance in the sum of the numbers in the set is therefore larger, as is the difference \nbetween the re\u00adsource reduced set and the original set. For larger set sizes the mean differences and \nvariances are quite small. This phe\u00adnomenon re.ects that fact that there is a signi.cant correla\u00adtion \nbetween the sum of half of the numbers in the set and the sum of all of the numbers in the set. So while \nchoosing a set based only on a subset of the numbers in the set may cause the algorithm to make a suboptimal \nchoice, the sum of the selected suboptimal set will still be reasonably close to the sum of optimal set. \nOur next set of simulations applies resource reduction to the number of sets by perforating the outer \nloop in the model computation. In this case both the mean differences (in Figure 4) and variances (in \nFigure 5) both decrease as the number of sets and size of the sets increases. We attribute this behavior \nto the fact that increasing the size of the set decreases the variance of the sum of the numbers in the \nset and that increasing the number of sets also decreases the variance in the sums of the sets. So if \nthe resource reduced computation discards the minimum set, increasing the size and/or number of the sets \nincreases the chances that the resource reduced computation will .nd another set with a minimum close \nto the minimum found by the original computation. Note that the resource reduced computation for a given \nnumber of sets considers the same number of sets as the original computation for half of the number of \nsets. For larger set sizes and numbers of sets the mean differences are very small. 4.3 Scaled Differences \nWe next consider not the absolute difference between the minimum values that the resource reduced and \noriginal com\u00adputations .nd, but the scaled differences that is, the abso\u00adlute difference divided by \nthe observed range in the sums of the sets (here the range is the difference between the maxi\u00admum and \nminimum sums). Figure 6 presents the mean max\u00adimum sum in the collection of sets; Figure 8 presents the \nmean minimum sum. Note that the two graphs are symmet\u00adrical. Note that the distance of the minimum and \nmaximum sums from the expected value of the sum (0.5) increases as the number of sets increases and the \nsize of the sets de\u00adcreases. Figure 7 presents the scaled difference for the resource reduced computation \non each set (in effect, Figure 2 di\u00advided by the difference between Figure 6 and Figure 8). Note that \nthis scaled difference is essentially constant at approx\u00adimately 15% across the entire range. The scaled \ndifferences for the resource reduced computation on the number of sets, on the other hand, decreases \nas the number of sets increases, with a range between approximately 25% and 7%.  4.4 Application to \nx264 The model computation provides insight into why x264 tol\u00aderates the loop perforation resource reduction \nmechanism. This mechanism produces a new, more ef.cient, but less precise metric for evaluating potential \nmatches between ref\u00aderence blocks and the current block to encode. This cor\u00adresponds to the .rst resource \nreduction mechanism in our model computation (which causes the computation to con\u00adsider only a subset \nof the numbers in each set). The result\u00ading reduction in the precision of the metric can cause the algorithm \nto choose a suboptimal reference block for the en\u00adcoding. But because of redundancy present in the reference \nblocks (which manifests itself in redundancy in the sums of the numbers used to measure the match at \nindividual points), this suboptimal choice causes only a small reduction in the overall effectiveness \nof the encoding. x264 has also been manually coded to work with re\u00adsource reduction mechanisms that correspond \nto the sec\u00adond resource reduction mechanism in our model computa\u00adtion (which causes the computation to \nconsider only some of the sets of numbers). Speci.cally, considering every pos\u00adsible reference block \nis computationally intractable, so x264 has been manually coded to consider only a subset of the reference \nblocks. The robustness of the video encoder to considering only a subset of the possible reference blocks \ncorresponds to the robustness of the model computation (see Figures 4 and 9) to considering only some \nof the sets of num\u00ad bers.  5. Linked Data Structures Linked data structures are pervasive in modern \nsoftware sys\u00adtems. If the nodes in the data structure are allocated dynam\u00adically, it is typically impossible, \nwith standard language se\u00admantics, to bound the amount of memory the data structure may consume. In extreme \ncases, the data structure may have memory leaks, typically because it contains references that make nodes \nreachable even though the data structure will never access the nodes in the future. 5.1 SNMP in Squid \nThe SNMP module in Squid implements a mapping from names to objects. It stores the mapping in a search \ntree, with the nodes dynamically allocated as the SNMP module stores new mappings. It is possible to \nuse cyclic memory allocation to reduce the amount of memory required to store the tree (and elimi\u00adnate \nany memory leaks). This allocation strategy sets aside a .xed-size buffer of nodes, then cyclically allocates \nthe data structure nodes out of that buffer [21]. If the data structure requires more nodes than are \navailable in the buffer, this ap\u00adproach will allocate two logically distinct nodes in the same physical \nnode, with the values from the two nodes overwrit\u00ading each other in the same memory. In general, the \neffect of this memory allocation strategy depends on the speci.c characteristics of the computation. \nFor the SNMP module in Squid, the empirical results show that the computation degrades gracefully. Speci.cally, \nforc\u00ading the data structure to .t in a .xed-size buffer with node overlaying makes the SNMP module unable \nto respond suc\u00adcessfully to some search queries. But the module contin\u00adues to execute and successfully \nrespond to a subset of the queries. Moreover, the remainder of the computation re\u00admains unaffected and \nable to continue to provide service to its users. 5.2 The Model Computation The SNMP search tree is \na linked, null-terminated, acyclic data structure. We use a simple example of such a data structure, \na singly-linked list, to explore the potential con\u00adsequences of subset memory allocation strategies that \nreal\u00adlocate live memory. The linked list implements a set of val\u00adues, with the allocation taking place \nwhen the list inserts a new value into the set. We consider two implementations of this operation: one \nthat inserts the value at the front of the list and one that inserts the value at the end of the list. \nThe usage scenario has the client repeatedly inserting values into the set, interspersing the insertions \nwith lookups. typedef struct node { int value; struct node *next; }; struct node *first, *newNode(); \nvoid prepend(int v) { struct node *n = newNode(); n->value = v; n->next = first; first = n; } void append(int \nv) { struct node *n = newNode(); if (first == NULL) first = n; else { l = first; while (l->next != NULL) \nl = l->next; l->next = n; } n->value = v; n->next = NULL; } int lookup(int v) { struct node *l = first; \nwhile (l != NULL) if (l->value == v) return 1; return 0; } We consider various reduced resource allocation \nmech\u00adanisms, all of which allocate new list nodes out of a .xed buffer of nodes. We note that all of \nthe mechanisms we con\u00adsider may cause newNode to return a node that is already in the list. Because a \nprepend operation applied to a node al\u00adready in the list creates a cyclic list and causes subsequent \nlookups of values not already in the list to in.nite loop, our further analysis only considers append \noperations. 5.2.1 Least Recently Allocated Allocation The .rst allocation mechanism is cyclic allocation, \nwhich returns the least recently allocated node in the buffer in re\u00adsponse to each node allocation request \n(in other words, the allocator always returns the least recently allocated node). When presented with \na sequence of append operations, this mechanism builds up a standard list. When the allocation mechanism \nwraps around, it returns the .rst element in the list and the assignment n->next=NULL truncates the list. \nThe result is a list containing only the most recently inserted element. Successive append operations \nbuild up a list con\u00adtaining the most recently inserted elements. Over time, given an unbounded number \nof insertions, the list contains, on av\u00aderage, the most recently inserted b/2 values (where b is the \nnumber of nodes in the buffer).  5.2.2 Most Recently Allocated Allocation The next allocation mechanism \nalways returns the most re\u00adcently allocated node (i.e., the last node in the buffer) once all of the \nnodes in the buffer have been allocated once. With a sequence of append operations, the effect is simply \nto re\u00adplace the last value in the list (the one most recently inserted) with the new value. After b insertions, \nthe list contains the .rst b - 1 inserted nodes and the last inserted node (where b is the number of \nnodes in the buffer).  5.2.3 Random Allocation The next allocation mechanism returns a random node \nin the buffer. Over time, given an unbounded number of in\u00adsertions, the list contains, on average, approximately \n0.125b nodes (where b is the number of nodes in the buffer). We obtained this number through a simulation \nwhich repeatedly inserts nodes into the list, psuedorandomly selecting one of the nodes in the buffer \nas the node to insert, then measur\u00ading the length of the list after each insertion to compute the mean \nlength of the list during the simulation (which is ap\u00adproximately 0.125b). Note the relatively low utilization \nof the available nodes in the buffer ideally, the list would contain all b of the nodes in the buffer. \nThe reason for the low utilization is that when the allocation algorithm selects a node that is already \nin the list, the insertion truncates the list at that node. So as the list grows in length, insertions \nthat shorten the list become increasingly likely. 5.2.4 Discussion Data structure consistency is a critical \nacceptability prop\u00aderty [10, 26]. Reduced resource allocation strategies and null-terminated linked data \nstructures work best with algo\u00adrithms that append null-terminated nodes to the leaves of the data structure \n other strategies may cause cycles that leave the data structure inconsistent. The linked list data structure \nin our model computation is one example of a data structure that appends at the leaves (in this case, \nsimply at the end of the list). More complex examples include binary search trees and separately-chained \nhash tables that store all ele\u00adments that hash to the same location in the table together in a linked \nlist. Like linked lists, these data structures can be used to implement a variety of abstractions such \nas sets and maps. We note that for linked lists and data structures (like separately-chained hash tables) \nthat incorporate linked lists, inserting only at the leaves can complicate strategies that at\u00adtempt to \nreduce lookup times by keeping the lists sorted standard insertion algorithms for sorted lists involve \ninser\u00adtions in the middle of the list.  5.3 Usage Scenarios To date, the most compelling usage scenarios \nfor reduced resource allocation involve the elimination of memory leaks, which can threaten the survival \nof the entire system [21]. The primary drawback of the resource reduction mechanism is the inadvertent \nelimination of elements that would, with standard allocation mechanisms, remain present in the data structure. \nWe next outline several scenarios in which the bene.ts of reduced resource allocation can outweigh the \ndrawbacks. One scenario occurs when the application does not need the data structure to necessarily contain \nall of the inserted elements for example, the application may use the data structure to cache previously \nfetched elements [21]. In this case reduced resource allocation may degrade the effective\u00adness of the \ncache, but will leave the application capable of delivering its full functionality to its users. Another \nscenario occurs when the computation can tolerate the loss of ele\u00adments that typically occurs with reduced \nresource alloca\u00adtion. The use of reduced resource allocation in the SNMP module, for example, degrades \nthe delivered functionality of the SNMP module (the SNMP module does not correctly process some queries), \nbut leaves the remaining Squid func\u00adtionality unimpaired. We see this general pattern, applying reduced \nresource allocation to eliminate a problem within one module that would otherwise threaten the survival \nof the entire system, as a particularly compelling usage scenario. 5.4 Reduced-Resource Aware Implementations \nWe have so far assumed that the developer of the data struc\u00adture manipulation algorithms is oblivious \nto the potential ap\u00adplication of resource reduction mechanisms and that the im\u00adplementation is therefore \ncoded only to consider cases that arise with standard semantics. But it is of course possible to develop \nalgorithms that are purposefully designed to work with resource reduction mechanisms. Potential implementa\u00adtion \nstrategies could include: Relaxed Invariants: Implementing the data structure manipulation algorithms \nto work with data structures that have a relaxed set of invariants. For example, a data struc\u00adture implementation \ncoded to operate in the presence of cycles would enable the use of resource reduction algo\u00adrithms that \ninsert items at internal points in lists and trees (as opposed to at the leaves).  Data Structure Repair: \nInstead of coding every data structure manipulation operation to work with relaxed data structure invariants, \nit is possible to instead simply use data structure repair [8 10] to restore the desired in\u00advariants \nat appropriate points (typically the beginning or end of data structure operations). This approach would \nmake it possible for the vast majority of data structure operations to be coded to operate on data structures \nthat satisfy all of the stated data structure consistency proper\u00adties.  Explicit Removal: Instead of \nsimply reusing allocated data structure nodes (which introduces the possibility of conceptually dangling \nreferences to and/or from the pre\u00advious location of the node in the data structure), the data structure \nimplementation could be coded to explicitly re\u00admove all reused nodes from their previous location be\u00adfore \ninserting them into the new location. This approach would preserve all of the standard invariants and \nelimi\u00adnate the inadvertent deletion of nodes originally reach\u00adable via the reallocated node when it was \nin its original position. This strategy would also maximize the achiev\u00ad   able utilization of the available \nspace for most linked data structures at capacity, this strategy would leave all of the nodes linked \ninto the data structure and accessible via standard data structure operations. Speci.cally for the linked \nlist in our model computation, the list would (af\u00adter the insertion of at least b values) always contain \nthe maximum b values.  5.5 Fixed-Size Tables In this section we have focused on linked data structures, \nwhich are often deployed because they enable applications to adjust their memory usage to the characteristics \nof dif\u00adferent inputs as different inputs elicit different memory us\u00adage behavior from the application \n(the application typically uses less memory to process small inputs and more mem\u00adory to process large \ninputs). But it is also possible to apply resource reduction mechanisms to .xed-sized tables. These data \nstructures are typically implemented as a single block of memory allocated at the start of the computation. \nThey are designed to store at most a .xed number of items determined by the size of the memory block. \nIf the computation attempts to insert an item into a table that is already full, it typically halts with \nan error indicating that a larger-sized table is re\u00adquired to process the current input. One example \nof such a data structure is an open-addressed hash table (which stores all of the values or key/value \npairs directly in the hash table buckets, using probing techniques to resolve collisions). One resource \nreduction mechanism overwrites old values with new values or discards new values when it is impossible \nor inconvenient to store them both. For example, one strategy would simply overwrite the least recently \nused or inserted table entry with the new entry when the client attempts to insert a new entry into a \ntable that is already full. In some circumstances it may be convenient to apply such mecha\u00adnisms even \nwhen the table is not full. For example, if two values or key/value pairs hash to the same bucket, the \nimple\u00admentation may simply discard one of the values or key/value pairs rather than apply techniques \nsuch as linear probing, quadratic probing, double hashing, or converting the bucket in question into \na separately-chained bucket. The potential advantages of avoiding these more complex techniques in\u00adclude \nsimplicity, convenience, ef.ciency, and even correct\u00adness (a simpler implementation provides the developer \nwith fewer opportunities to introduce errors). For example, an out of bounds addressing error introduced \nduring an attempt to implement a more sophisticated response to events such as hash table collisions \ncan (depending on how the application uses the table) easily have more serious consequences than overwriting \nexisting table entries or discarding new table en\u00adtries. Note that such mechanisms eliminate the requirement \nthat the .xed-size table be able to store all of the items that client attempts to insert into the table. \nThey may therefore make it possible to make the table smaller because it does not need to be sized for \nthe largest anticipated usage scenario it can instead be sized for more common-case scenarios while \nproviding successful execution with reduced function\u00adality for less common scenarios that exceed the \ntable size. We note that current approaches often involve the use of .xed-size tables that (if they abort \nwhen the client attempts to take them beyond their capacity) threaten the survival of the entire computation. \nResource reduction mechanisms can therefore enhance the robustness and resilience of the system by enabling \nthe table to execute successfully through other\u00adwise fatal events such as attempting to insert a new \nitem into a table that is already full. 6. Redundancy and Correlation All of our resource reduction mechanisms \nexploit redun\u00addancy in the values that they manipulate. One way to see this redundancy is to examine \ncorrelations between the val\u00adues. Resource reduction works well with the mean pattern because (for most \nprobability distributions) the mean of a subset of numbers is highly correlated with the mean of the \nset of numbers itself. Similarly, the sum of a subset of num\u00adbers is correlated (after extraplotation) \nwith the sum of the set. And, the minimum of a subset of numbers is correlated with the minimum of the \nset. The correlations in all of these examples are relatively easy to see because all of the exam\u00adples \nperform an aggregation or reduction operation. The re\u00addundancy that appears in the original set of values \nis the root cause of the correlation between the results of computations on subsets of the original set \nof values. Of course, there are other forms of redundancy and corre\u00adlation that make computations amenable \nto resource reduc\u00adtion. Consider, for example, standard iterative computations (such as differential \nequation solvers), that converge to an acceptably accurate answer. As the computation converges, successive \niterations compute highly correlated values. This correlation is the reason that the standard resource \nreduction mechanism for converging iterative computations (terminat\u00ading the iteration before it converges) \nworks well in practice. Internet search engines also compute correlated values the results for a given \nquery are highly correlated, which en\u00adables resource reduction mechanisms (for example, search\u00ading only \na subset of the documents [6]) that cause the search engine to return only a subset of the results that \nthe original computation would have returned. This example generalizes to virtually all information retrieval \ncomputations. As another example, the sets of moves that automated game-playing algorithms consider are \noften of roughly equivalent quality in other words, the effects that the dif\u00adferent moves have on the \noutcome of the game are highly correlated. This correlation enables the successful applica\u00adtion of resource \nreduction mechanisms that force the algo\u00adrithm to execute with reduced memory by allocating con\u00adceptually \ndistinct data in the same memory. While the re\u00adduced resource version does not generate the same moves \nas the original version, it exhibits roughly equivalent compe\u00adtence [21].  Finally, the values that \napplications communicate over a network are often correlated. One simple way to exploit this fact is \nto use short-circuit communication instead of per\u00adforming a standard request/response interaction, memoize \nthe results of previous interactions and return the response whose request most closely matches the current \nrequest. It is also possible to similarly memoize direct communica\u00adtions that do not occur as part of \na request/response inter\u00adaction [26]. 7. Finding New Patterns We anticipate that, as the .eld progresses, \nresearchers and practitioners will .nd other computational patterns that work well with reduced resource \ncomputing mechanisms. We pro\u00adpose several strategies for .nding such patterns: Empirical Testing: We \nobtained our current set of patterns by applying resource reduction mechanisms broadly across selected \nbenchmark applications, empiri\u00adcally .nding computations that responded well to these mechanisms, then \nanalyzing the computations to under\u00adstand the reasons why they responded so well. We antic\u00adipate that \napplying existing and new resource reduction mechanisms across a broader set of benchmark applica\u00adtions \nwould reveal additional patterns that work well with such mechanisms.  Analytical Analysis: Another \napproach reverses the pro\u00adcess it starts with simple computations (such as the mean, sum, and minimum \ncomputations) that compute correlated values when applied to correlated data sets such as subsets of \nvalues drawn from the same set or probability distribution. As the results in this paper show, such computations \noften respond well to resource reduc\u00adtion mechanisms. The next step would be to .nd in\u00adstances of such \ncomputations in existing applications, then evaluate (either empirically or analytically) the suit\u00adability \nof resource reduction as applied to these compu\u00adtations in the context of the application.  Correlated \nValues: To date, every successful applica\u00adtion of reduced resource computing mechanisms has ex\u00adploited \na form of redundancy that shows up as correla\u00adtions in the values that various computations compute. \nLooking for correlations between computed values may therefore be a productive way to .nd other patterns \nthat interact well with resource reduction. This could be done either empirically, by examining computed \nvalues to look for correlations, or analytically.  Reduction Operations: Three of our four patterns \nap\u00adply reduction operations (sum, mean, min) that reduce a set of values to a single value. We anticipate \nthat other reduction operations would be good candidates for the application of resource reduction mechanisms. \n We also anticipate that, as the bene.ts of resource re\u00adduced computing become more widely known, developers \nwill begin to consciously engineer systems to use computa\u00adtion patterns that interact well with reduced \nresource com\u00adputing mechanisms. Over time, systems will become in\u00adcreasingly engineered to work productively \nwith these mech\u00adanisms and other modern program transformations that may change the semantics of the \nprogram in return for bene.ts such as increased robustness, resilience, security, memory ef.ciency, and \nperformance [4, 19, 21, 22, 26, 27, 32, 33]. 8. Criticality Testing In our experience, it is possible \nto apply resource reduc\u00adtion successfully to only a subset of the subcomputations in a given computation. \nIndeed, the inappropriate applica\u00adtion of resource reduction can easily cause applications to crash, \ntake longer to execute, or produce wildly inaccurate results [16, 20, 23]. Examples of unsuitable computations \nthat we have encountered in practice include tasks or loops that allocate subsequently accessed objects, \nef.cient pre\u00ad.lters that eliminate objects unsuitable for subsequent more involved processing (applying \nresource reduction to the pre\u00ad.lter can decrease the overall performance of the applica\u00adtion), and tasks \nor loops that produce the output of the ap\u00adplication or drive the processing of input units (for exam\u00adple, \napplying resource reduction to the loop that iterates over the video frames in X264 unacceptably causes \nX264 to drop frames). We therefore advocate the use of criticality testing, which automatically applies \nresource reduction across the applica\u00adtion, using a test suite of representative inputs and an ap\u00adpropriate \naccuracy metric to automatically .nd subcomputa\u00adtions that work well with resource reduction [16, 20, \n23]. One can view the resource reduction mechanisms as au\u00adtomatically generating a search space of applications \nsur\u00adrounding the original application, with the accuracy metric enabling the search algorithm to explore \nthe space to .nd variants with more desirable performance/accuracy charac\u00adteristics for the current usage \ncontext than the original appli\u00adcation. An alternative approach requires the developer to provide multiple \nimplementations of different subcomputations, po\u00adtentially with different performance and accuracy character\u00adistics \n[1, 2]. A search algorithm can then explore various implementation combinations to .nd desirable points \nin the performance/accuracy trade-off space. A claimed advantage is that it is less likely to deliver \napplications that can pro\u00adduce unanticipated results. A disadvantage is the developer effort required \nto .nd appropriate subcomputations and de\u00advelop alternate versions of these subcomputations. We anticipate \nthat, in practice, developers may often en\u00adcounter some dif.culty in .nding and developing alternate \nversions of subcomputations with good accuracy/performance characteristics in large applications, especially \nif the op\u00adtimization effort is separate from the initial development effort and the developer starts \nout unfamiliar with the appli\u00adcation. In typical scenarios we believe the most productive approach will \ninvolve the automatic application of resource reduction mechanisms in one (or more) of three ways:  \n Pro.ling: Automatically applying resource reduction to subcomputations, then observing the resulting \naccuracy and performance effects, to help developers identify suit\u00adable subcomputations (subcomputations \nwith desirable accuracy and performance characteristics after resource reduction) that they then manually \noptimize [20].  Developer Approval: Instead of manually optimizing the identi.ed suitable subcomputations, \nthe developer can simply examine the resource reduced subcomputa\u00adtions and approve or reject the resource \nreduced versions for production use. This scenario preserves the devel\u00adoper involvement perceived to \npromote the safe use of resource reduction, but eliminates the need for the devel\u00adoper to modify the \napplication.  Automatic Application: The direct application of re\u00adsource reduction without any developer \ninvolvement with the source code of the application. In these scenarios per\u00adformance and accuracy results \nfrom running the applica\u00adtion on representative inputs provide the primary mech\u00adanism for evaluating \nthe acceptability of the resource re\u00adduction. This approach may be particularly appropriate when developers \nthat can work with the application are unavailable, when developers are a scarce resource that can be \nmore productively applied to other activities, or when the source code of the application is not available. \nWe note that it is relatively straightforward to apply the speci.c resource reduction mechanisms presented \nin this paper directly to compiled binaries, eliminating the need for source code access.  Static Analyses: \nWe anticipate the development of static program analyses that will automatically recognize pat\u00adterns \n(such as those identi.ed in this paper), or, more generally, computations, that interact well with resource \nreduction mechanisms. In combination with either simu\u00adlations (such as those presented in this paper) \nor the de\u00advelopment of symbolic analytical models (or a combina\u00adtion of the two), these static analyses \nwould make it pos\u00adsible to precisely characterize the impact on the accuracy (and potentially the performance) \nof applying different resource reduction mechanisms to the speci.c analyzed application at hand. The \nsimulations and models could be tailored to the recognized pattern and its usage con\u00adtext within the \napplication. It would also be possible to draw on an existing set of simulation results and/or ana\u00adlytical \nmodels. And of course combinations of these two approaches are also possible.  9. Related Work The resource \nreduction mechanisms discussed in this paper were all developed in previous research [16, 20, 21, 23, \n24]. This paper identi.es general computational patterns that interact well with these mechanisms, presents \nconcrete manifestation of these patterns in the model computations, and uses simulation to quantitatively \nexplore the impact of resource reduction. There is a long history of developing application-speci.c algorithms \nthat can execute at a variety of points in the per\u00adformance versus accuracy trade-off space. This paper, \nin contrast, focuses on general, broadly applicable resource re\u00adduction mechanisms that can automatically \nenhance appli\u00adcations to productively trade off accuracy in return for per\u00adformance, even though the \napplications may not have been originally designed to support these kinds of trade offs. 9.1 Memory Reduction \nMechanisms In this paper we have considered two kinds of resource re\u00adduction mechanisms: mechanisms that \nreduce the amount of memory and mechanisms that reduce the amount of com\u00adputation. To the best of our \nknowledge, the .rst (and, to date, only) proposal for a general memory reduction mecha\u00adnism was cyclic \nmemory allocation [21]. It is also possible to use out of bounds access redirection (cyclically or otherwise \nredirecting out of bounds accesses back within the accessed memory block) [27, 29, 30] to reduce the \nsize of tables or arrays (or, for that matter, any data structure allocated in a contiguous block of \nmemory). The out of bounds redirection eliminates any cross-block memory corruption or memory error exceptions \nthat might otherwise threaten the survival of the application after reducing the size of the memory block \nholding the table or array.  9.2 Computation Reduction Mechanisms To the best of our knowledge, the \n.rst proposal for a general computation reduction mechanism was discarding tasks [23, 24]. The proposed \nuses included surviving errors in tasks, eliminating barrier idling, and using the automatically de\u00adrived \ntiming and distortion models to navigate the induced performance versus accuracy space. The models make \nit pos\u00adsible to either maximize accuracy subject to speci.ed per\u00adformance constraints, to maximize performance \nsubject to speci.ed accuracy constraints, or to satisfy more complex combinations of performance and \naccuracy optimization cri\u00adteria. Many of the discarded tasks in the benchmark appli\u00adcations simply execute \nblocks of loop iterations. This fact makes it obvious that discarding iterations of the correspond\u00ading \nloops (i.e., loop perforation) would have the same ef\u00adfect on the performance and accuracy as discarding \nthe cor\u00adresponding tasks. One advantage of discarding loop itera\u00adtions instead of tasks is that it is \npossible to apply the cor\u00adresponding loop perforation transformations directly to a broad range of programs \nwritten in standard programming languages [16, 20] (as opposed to programs written in lan\u00ad guages with \na task construct).  9.3 Manual Versus Automatic Techniques The resource reduction mechanisms in this \npaper were all initially developed for largely automatic application, with criticality testing typically \nused to .nd subcomputations that can pro.tably tolerate resource reduction. The PetaBricks and Green \nsystems, in contrast, require the developer to create the trade-off space by designing and implementing \nmultiple implementations of the same subcomputation [1, 2]. As described in Section 8, the automatic \napplication of resource reduction can signi.cantly reduce the developer effort required to generate and \n.nd desirable points within this trade-off space. Moreover, automatic mechanisms such as loop perforation \nand discarding tasks can discover latent desirable trade-offs that are simply unavailable to human developers: \n the developer may not suspect that a pro.table trade-off exists in a given subcomputation,  the developer \nmay not understand the subcomputation well enough to realize that a pro.table trade-off may be available, \n the developer may not have the time, expertise, or knowl\u00adedge required to provide another implementation \nof the subcomputation, or  the developer may simply be unaware that the subcom\u00adputation exists at all. \n  Finally, automatic techniques can even enable users with no software development ability whatsoever \nto obtain new versions of their applications that can execute at a variety of different points within \nthe underlying performance versus accuracy trade-off space that resource reduction mechanisms automatically \ninduce. 9.4 Dynamic Control It is possible to apply resource reduction mechanisms such as loop perforation \n[16] and discarding tasks [23] dynam\u00adically to move a running application to different points in its \nunderlying performance versus accuracy trade-off space. It is also possible to convert static con.guration \nparameters into dynamic knobs dynamic control variables stored in the address space of a running application \n[17]. Like re\u00ad source reduction mechanisms, dynamic knobs make it pos\u00adsible to move a running application \nto different points in the induced performance versus accuracy trade-off space with\u00adout otherwise perturbing \nthe execution. These mechanisms make it possible for an appropriate control system to monitor and dynamically \nadapt the exe\u00adcution of the application to ensure that the application meets performance or accuracy \ntargets in the face of .uctuations in the amount of resources that the underlying computa\u00adtional platform \ndelivers to the application [16, 17]. Such con\u00ad trol systems typically work with a model that characterizes \nhow the application responds to different resource reduction mechanisms or dynamic knob settings. Ideally, \nthe control system would guarantee good convergence and predictabil\u00adity properties such as stability, \nlack of oscillation, bounded settling time, and known overshoot [17]. We have developed controllers with \nsome or all of these properties [16, 17] in particular the PowerDial control system [17] has all of \nthese properties. These controllers are designed for applica\u00adtions (such as video encoders) that are \nintended to produce a sequence of outputs at a regular rate with a target time be\u00adtween outputs. These \ncontrollers use the Application Heart\u00adbeats framework [15] to monitor the time between outputs and use \neither loop perforation [16] or dynamic knobs [17] to control the application to maximize accuracy while \nensur\u00ading that they produce outputs at the target rate. It would also be possible to combine manual techniques \n(such as those supported by PetaBricks or Green) with dy\u00adnamic controllers. PetaBricks does not have \na dynamic con\u00adtrol component [1]. Green uses heuristic control to man\u00ad age accuracy but does not control \nor even monitor perfor\u00admance [2]. Green s control system is also completely heuris\u00ad tic, with no guaranteed \nconvergence or predictability prop\u00aderties whatsoever. 9.5 Unsound Program Transformations Reduced resource \ncomputing mechanisms are yet another example of unsound program transformations. In contrast to traditional \nsound transformations (which operate under the restrictive constraint of preserving the semantics of \nthe orig\u00adinal program), unsound transformations have the freedom to change the behavior of the program \nin principled ways. Un\u00adsound transformations have been shown to enable applica\u00adtions to productively \nsurvive memory errors [4, 27, 32], code injection attacks [22, 27, 32, 33], data structure corruption \nerrors [8 10], memory leaks [21], and in.nite loops [21]. Like some of the reduced resource computing \nmechanisms identi.ed in this paper, unsound loop parallelization strate\u00adgies have been shown to increase \nperformance at the cost of (in some cases) potentially decreasing the accuracy of the computation [19]. \nFinally, reduced resource computing mechanisms can help applications satisfy acceptability prop\u00aderties \ninvolving the amount of resources required to execute the computation [26]. 10. Conclusion Dealing with \nlimited resources is a critical issue in virtually every aspect of life, including computing. The standard \nap\u00adproach to dealing with this fact is to rely on developers to produce algorithms that have been tailored \nby hand for the speci.c situation at hand. Over the last several years researchers have developed a collection \nof simple, broadly applicable mechanisms that en\u00adable computations to execute successfully in the presence \nof reduced resources. This development promises to dramati\u00adcally simplify the process of obtaining computations \nthat can operate successfully over the wide range of operating condi\u00adtions that will characterize future \ncomputing environments.  10.1 Value Systems All of these resource reduction mechanisms go against the \nbasic value system of the .eld in that they may change the result that the computation produces in the \ntradi\u00adtional (and, in our view, outmoded) terminology, they are unsound. Through its identi.cation of \ngeneral patterns and model computations that interact well with these mecha\u00adnisms and the statistical \nproperties of these mechanisms, this paper attempts to enhance our understanding of the under\u00adlying computational \nphenomena behind the observed em\u00adpirical success of these mechanisms. In this way, this pa\u00adper transcends \nthe traditional use of discrete formal logic to characterize application behavior in terms of rigid, \n.xed input/output relationships. It instead promotes a new, more .exible model that incorporates continuous \nstatistical rela\u00adtionships between not just the input and output, but also the amount of resources that \nthe computation consumes to pro\u00adduce the output. 10.2 Bene.ts We anticipate several bene.ts that can \nstem from this en\u00adhanced perspective. The .rst is a greater comfort with and acceptance of resource reduction \nmechanisms, which, in turn, can make the bene.ts they provide more broadly avail\u00adable. The second is \nthat it may enable developers to more easily recognize and use computational patterns that interact well \nwith these mechansims. The third is that it may inspire the development of additional resource reduction \nmecha\u00adnisms and the identi.cation of both existing and new com\u00adputational patterns that work well with \nthese mechanisms. A .nal bene.t is that it can broaden the scope of the .eld itself and promote its movement \naway from an outmoded, regressive value system that is increasingly hindering its de\u00advelopment. Speci.cally, \nthis research promotes the develop\u00adment and evolution of software systems via the automated exploration \nof automatically generated search spaces rather than full manual development. The evaluation mechanism \nis empirical, based on observing the behavior of the system as it executes on representative inputs, \nrather than theoretical and based on an analysis of the text of the program before it runs. The goal \nis to deliver .exible programs that usually produce acceptably accurate output across a broad range of \noperating conditions rather than attempting to develop rigid, in.exible programs that satisfy hard logical \ncorrectness con\u00additions when run on perfect execution platforms. In this way the research points the \ndirection to software that can operate successfully in the increasingly dynamic and unpredictable computing \nenvironments of the future. Acknowledgements We would like to thank the anonymous reviewers for their \nuseful criticism, comments, and suggestions. This research was supported by DARPA Cooperative Agreement \nFA8750\u00ad06-2-0189, the Samsung Corporation, and NSF Awards 0811397, 0835652, and 0905244. References [1] \nJ. Ansel, C. Chan, Y. Wong, M. Olszewski, Q. Zhao, A. Edel\u00adman, and S. Amarasinghe. PetaBricks: a language \nand com\u00adpiler for algorithmic choice. In PLDI 09. [2] W. Baek and T. Chilimbi. Green: A system for supporting \nenergy-conscious programming using principled approxima\u00adtion. Technical Report TR-2009-089, Microsoft \nResearch, Aug. 2009. [3] J. Berg, J. Tymoczko, and L. Stryer. Biochemistry. W.H. Freeman, 2006. [4] E. \nBerger and B. Zorn. DieHard: probabilistic memory safety for unsafe languages. In PLDI, June 2006. [5] \nC. Bienia, S. Kumar, J. P. Singh, and K. Li. The PARSEC benchmark suite: Characterization and architectural \nimplica\u00adtions. In PACT 08. [6] E. Brewer. Towards robust distributed systems. Keynote, ACM Symposium \non Principles of Distributed Systems (PODC), 2000. [7] R. Browning, T. Li, B. Chui, J. Ye, R. Pease, \nZ. Czyzewski, and D. Joy. Low-energy electron/atom elastic scattering cross sections from 0.1-30 keV. \nScanning, 17(4), 1995. [8] B. Demsky, M. Ernst, P. Guo, S. McCamant, J. Perkins, and M. Rinard. Inference \nand enforcement of data structure con\u00adsistency speci.cations. In ISSTA 06. [9] B. Demsky and M. Rinard. \nAutomatic detection and repair of errors in data structures. In OOPSLA 03, 2003. [10] B. Demsky and M. \nRinard. Data structure repair using goal\u00addirected reasoning. In ICSE 05, 2005. [11] R. Floyd. Assigning \nmeanings to programs. In Proceedings of the Symposium in Applied Mathematics, number 19, 1967. [12] E. \nGamma, R. Helm, R. Johnson, and J. Vlissides. De\u00adsign Patterns: Elements of Reusable Object-Oriented \nSoft\u00adware. Addison-Wesley, 1995. [13] J. Harris, S. Lazaratos, and R. Michelena. Tomographic string inversion. \nIn Proceedings of the 60th Annual International Meeting, Society of Exploration and Geophysics, Extended \nAbstracts, 1990. [14] C. A. R. Hoare. An axiomatic basis for computer program\u00adming. Commun. ACM, 12(10), \n1969. [15] H. Hoffmann, J. Eastep, M. D. Santambrogio, J. E. Miller, and A. Agarwal. Application Heartbeats: \nA Generic Interface for Specifying Program Performance and Goals in Autonomous Computing Environments. \nIn ICAC 10: 7th International Conference on Autonomic Computing, 2010. [16] H. Hoffmann, S. Misailovic, \nS. Sidiroglou, A. Agarwal, and M. Rinard. Using Code Perforation to Improve Performance, Reduce Energy \nConsumption, and Respond to Failures . Tech\u00adnical Report MIT-CSAIL-TR-2009-042, MIT, Sept. 2009.  [17] \nH. Hoffmann, S. Sidiroglou, M. Carbin, S. Misailovic, A. Agarwal, and M. Rinard. Power-Award Computing \nwith Dynamic Knobs. Technical Report TR-2010-027, Computer Science and Arti.cial Intelligence Laboratory, \nMIT, Aug. 2010. [18] D. Le Gall. MPEG: A video compression standard for multi\u00admedia applications. CACM, \nApr. 1991. [19] S. Misailovic, D. Kim, and M. Rinard. Parallelizing sequential programs with statistical \naccuracy tests. Technical Report TR-2010-038, Computer Science and Arti.cial Intelligence Laboratory, \nMIT, Aug. 2010. [20] S. Misailovic, S. Sidiroglou, H. Hoffman, and M. Rinard. Quality of service pro.ling. \nIn ISCE 10. [21] H. Nguyen and M. Rinard. Detecting and eliminating memory leaks using cyclic memory \nallocation. In ISMM 07. [22] J. Perkins, S. Kim, S. Larsen, S. Amarasinghe, J. Bachrach, M. Carbin, C. \nPacheco, F. Sherwood, S. Sidiroglou, G. Sulli\u00advan, W. Wong, Y. Zibin, M. Ernst, and M. Rinard. Automati\u00adcally \npatching errors in deployed software. In SOSP 09. [23] M. Rinard. Probabilistic accuracy bounds for \nfault-tolerant computations that discard tasks. In ICS 06. [24] M. Rinard. Using early phase termination \nto eliminate load imbalancess at barrier synchronization points. In OOP-SLA 07. [25] M. Rinard. The Design, \nImplementation and Evaluation of Jade, a Portable, Implicitly Parallel Programming Language. PhD thesis, \nStanford University, Department of Computer Sci\u00adence, 1994. [26] M. Rinard. Acceptability-oriented computing. \nIn OOPSLA 03 Companion, Oct. 2003. [27] M. Rinard, C. Cadar, D. Dumitran, D. M. Roy, T. Leu, and J. William \nS. Beebee. Enhancing Server Availability and Security Through Failure-Oblivious Computing. In OSDI 04. \n[28] M. Rinard, D. Scales, and M. Lam. Jade: A high-level, machine-independent language for parallel \nprogramming. parallel computing, 29, 1993. [29] M. C. Rinard, C. Cadar, D. Dumitran, D. M. Roy, and T. \nLeu. A dynamic technique for eliminating buffer over.ow vulner\u00adabilities (and other memory errors). In \nACSAC 04. [30] M. C. Rinard, C. Cadar, and H. H. Nguyen. Exploring the acceptability envelope. In OOPSLA \n05 Companion. [31] K. Schmidt-Nielsen, R. Schroter, and A. Shkolnik. Desatura\u00adtion of exhaled air in \ncamels. Proceedings of the Royal Society of London, Series B, Biological Sciences, 211(1184), 1981. [32] \nS. Sidiroglou, O. Laadan, C. Perez, N. Viennot, J. Nieh, and A. D. Keromytis. Assure: automatic software \nself-healing using rescue points. In ASPLOS 09. [33] S. Sidiroglou, M. E. Locasto, S. W. Boyd, and A. \nD. Keromytis. Building a reactive immune system for software services. In USENIX 05.     \n\t\t\t", "proc_id": "1869459", "abstract": "<p>We present several general, broadly applicable mechanisms that enable computations to execute with reduced resources, typically at the cost of some loss in the accuracy of the result they produce.We identify several general computational patterns that interact well with these resource reduction mechanisms, present a concrete manifestation of these patterns in the form of simple model programs, perform simulationbased explorations of the quantitative consequences of applying these mechanisms to our model programs, and relate the model computations (and their interaction with the resource reduction mechanisms) to more complex benchmark applications drawn from a variety of fields.</p>", "authors": [{"name": "Martin Rinard", "author_profile_id": "81100087275", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA, USA", "person_id": "P2354152", "email_address": "", "orcid_id": ""}, {"name": "Henry Hoffmann", "author_profile_id": "81416603389", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA, USA", "person_id": "P2354153", "email_address": "", "orcid_id": ""}, {"name": "Sasa Misailovic", "author_profile_id": "81330495396", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA, USA", "person_id": "P2354154", "email_address": "", "orcid_id": ""}, {"name": "Stelios Sidiroglou", "author_profile_id": "81100257720", "affiliation": "Massachusetts Institute of Technology, Cambridge, MA, USA", "person_id": "P2354155", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1869459.1869525", "year": "2010", "article_id": "1869525", "conference": "OOPSLA", "title": "Patterns and statistical analysis for understanding reduced resource computing", "url": "http://dl.acm.org/citation.cfm?id=1869525"}