{"article_publication_date": "10-17-2010", "fulltext": "\n An Input-CentricParadigmfor Program Dynamic Optimizations KaiTian Yunlian Jiang Eddy Z. Zhang Xipeng \nShen Department of Computer Science The College ofWilliam and Mary,Williamsburg,VA, USA {ktian,jiang,eddy,xshen}@cs.wm.edu \nAbstract Accurately predicting program behaviors (e.g., locality, de\u00adpendency, method calling frequency) \nis fundamental for program optimizations and runtime adaptations. Despite decades of remarkable progress, \nprior studies have not sys\u00adtematically exploited program inputs, a deciding factor for program behaviors. \nTriggered by the strong and predictive correlations be\u00adtween program inputs and behaviors that recent \nstudies have uncovered, this work proposes to include program inputs into the focus of program behavior \nanalysis, cultivating a new paradigm named input-centric program behavior analy\u00adsis. This new approach \nconsists of three components, form\u00adinga three-layerpyramid.Atthebaseis program input char\u00adacterization, \na component for resolving the complexity in program raw inputs and the extraction of important features. \nIn the middle is input-behavior modeling, a component for recognizing and modeling the correlations between \nchar\u00adacterized input features and program behaviors. These two components constitute input-centric program \nbehavior anal\u00adysis, which (ideally) is able to predict the large-scope be\u00adhaviors of a program s execution \nas soon as the execution starts. The top layer of the pyramid is input-centric adap\u00adtation, which capitalizes \non the novel opportunities that the .rst two components create tofacilitate proactive adaptation for \nprogram optimizations. By centering on program inputs, the new approach re\u00adsolves a proactivity-adaptivity \ndilemma inherent in previous techniques. Its bene.ts are demonstrated through proactive dynamic optimizations \nand version selection, yielding sig\u00adni.cant performance improvement on a set of Java and C programs. \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page.To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA/SPLASH \n10, October 17 21, 2010, Reno/Tahoe, Nevada, USA. Copyright c &#38;#169; 2010ACM 978-1-4503-0203-6/10/10... \n$5.00. Categories and Subject Descriptors D.3.4[Programming Languages]: Processors optimization, compilers \nGeneral Terms Languages, Performance Keywords Program inputs, Dynamic optimizations, Java Virtual Machine, \nProactivity, Seminal behaviors, Dynamic version selection, Just-In-Time compilation 1. Introduction The \ngoal of program behavior analysis is to uncover the pat\u00adterns in a program s dynamic behaviors (e.g., \ncache require\u00adment, function calling frequency) so that the future behav\u00adiors of the program can be accurately \npredicted. As program optimizations rely on accurate predictions of program be\u00adhaviors, program behavior \nanalysis is essential for the max\u00adimization of computing ef.ciency. The inputs to a program refer to \nall the data that are not generatedbut accessedby the program, including command line arguments, interactively \ninput data, .les to read, and so on. Many studies have reported strong in.uence program inputs impose \non the program s behaviors [6, 21, 25, 27, 28, 40, 42]. Such in.uence has been commonly regarded a hurdle \nfor program optimizations: Static compilers have to optimize conservatively through transformations that \n.t all possible inputs [1, 2]; pro.ling-based optimizers often encounter cases that an optimization they \napply based on some training runs work inferiorly on an execution of the program ona new input [5,6,27,28]. \nThe work described in this paper comes from a differ\u00adent perspective: The strong in.uence from program \ninputs, although causing challenges, may meanwhile provide valu\u00adable hints and opportunities for program \nbehavior predic\u00adtion and program optimizations. The rationale is that because of the decisive role of \nprogram inputs, the knowledge about them may offer important clues on how the program would behave, and \nbecause in many cases (although not always) program inputs become known when an execution starts, the \nclues they offer may help produce a large-scope prediction of the execution at its early stage, offering \nimportant guid\u00ading information for dynamic optimizers. In the experiment to be reported in Section 4.1, \nfor example, the clues indirectly derived from program inputs lead to accurate prediction of Figure 1. \nThe components of input-centric program behav\u00adior analysis and optimizations. They form a pyramid, with \nthe top level exerting the power of the analysis conducted by the lower levels.  the appropriate optimization \nlevel for each Java method at the early stage of an execution. The prediction helps the JIT (Just-In-Time) \ncompiler appropriately optimize the Java method earlier than it does by default, yielding 10 29% per\u00adformance \nimprovement. Afew previous studies [25, 40, 43] have used certain fea\u00adtures of program inputs for optimizations. \nBut theyhave con\u00adcentrated on several scienti.c kernels (e.g., sorting, FFT), and the exploitation of \nprogram inputs mainly relies on domain-speci.c knowledge (e.g., the data distribution is a feature important \nfor sorting). A recent work [27] attempts to generalize the idea to a broader class of applications,but \nstill with manual characterization of inputs required. It re\u00admains an open question how to exploit program \ninputs for optimizing general applications in a systematic and auto\u00admatic manner. As a result, program \ninput, a deciding factor for program behaviors, remains outside the focuses of most optimizers. In this \nwork, we aim to bring program inputs into the center of program optimizations by answering the following \nquestions. First, is it worthwhile? In another word, are there any distinctive opportunities that an \nexploitation of program inputs can bring to program optimizations?  If so, how to expose such opportunities, \nespecially given the remarkable complexities of program inputs and pro\u00adgram behaviors?  Finally, if \nthose opportunities can be exposed, how to capitalize on them for performance improvement? What changes \nneed to be done to optimizers for the capitaliza\u00adtion? How much performance improvement can be pro\u00adduced? \n Our efforts for answering these questions yield a set of techniques forming a three-layer pyramid as \nshown in Fig\u00adure 1. The bottom layer, program input characterization (Section 3.1), is fundamental. It \nextracts important features from raw, often complex, program inputs by taking advan\u00adtage of statistical \ncorrelations among program behaviors. The second layer, input-behavior modeling (Section 3.2), recognizes \nand models the statistical relations between the features produced by the .rst layer and various program \nbe\u00adhaviors. The process is based on machine learning theory and techniques, with a systematic treatment \nto some special features of program behavior analysis (e.g., identi.cation of categorical features, the \ntension between many input fea\u00adtures and limited training runs). These two layers constitute input-centric \nprogram behavior analysis, through which, the runtime system is able to predict from the program inputs \nthe behaviors of a large scope of the current execution, (ide\u00adally) as soon as the execution starts. \nThe prediction opens many novel opportunities for the enhancement of dynamic optimizations. The third \nlayer, input-centric adaptation (Section 3.3), helps overcome some inherent limitations speci.cally, \na proactivity-adaptivity dilemma in current dynamic optimizers and converts the opportunities created \nby the .rst two components into performance improvement. Thesethree layers together forma new paradigm, \nnamely the input-centric program behavior analysis and optimiza\u00adtions. Its central theme is the exploration \nand exploitation of program inputs. The techniques developed in this work address thekeydif.culties in \neach of its components, elimi\u00adnate the needs for manual efforts, and for the .rst time, make automatic \ninput-centric program optimizations feasible and pro.table. To examine the potential of the new paradigm, \nwe apply it to two optimizers (Section 4). One is the JIT optimizer in Jikes RVM for Java programs, the \nother is the optimizer in a product compiler (IBM XL compiler) for C programs. Both experiments show \nthat input-centric optimizations con\u00adsistently outperform existing techniques. In the Jikes RVM, the \nspeedups are up to 81% with average ranging from 10% to 29%; in the IBM XL compiler, the speedups are \nup to 58% with averages between 5% and 13%. In summary, this work makes the following major contri\u00adbutions. \n It develops the .rst input-centric paradigm for program behavior analysis and optimizations. Some recent \nstud\u00adies [21,27] have tackled certain challenges in some layers of the input-centric paradigm, offering \nthe basis for this study. But none of the previous studies has proposed such a paradigm, or offered a \ncompletely automatic solution to realize the paradigm.  This work systematically explores the special \nchallenges existing in the construction of the statistical models be\u00adtween input features and program \nbehaviors. Some re\u00adlated studies have employed machine learning tools for program optimizations but without \nconsidering the spe\u00adcial properties of program behavior analysis. This work demonstrates that a systematic \ntreatment to these prop\u00aderties may signi.cantly improve the learning results and yield substantial enhancement \nto the optimization results.   This work demonstrates input-centric adaptation by de\u00adveloping two example \nschemes. Meanwhile, it assesses the potential of input-centric optimizations by compar\u00ading them with \nboth manual endeavors and the state-of-art optimization techniques. 2. QualitativeView on the Importance \nof Program Inputsfor Program Optimizers Before describing the techniques in detail and reporting their \nquantitative results, we .rst give a qualitative discussion to convey some intuition for the importance \nof program inputs and the distinctive opportunities they may bring to program optimizations. 2.1 ADecidingFactorfor \nProgram Behaviors The importance of program inputs for program optimiza\u00adtions stems from their important \nrole in determining program behaviors.Formally, program behaviors in this paper refer to the operations \nof a program and the ensuing activities of the computing system in relation to the program input and \nrun\u00adning environment. Examples include dynamic call graphs, data access patterns, memory requirement, \ncache usage, and so forth.Thevariousfactors decidingthe behaviorsofa pro\u00adgram may be qualitatively expressed \nby the following pro\u00adgram behavior equation: P rog. Behaviors = Inputs + Code + Environments. (1) The \nprogram code determines the set of instructions that may be executed in a run; the running environments \nconsist of all the elements in the execution platform, including the OS, virtual machine, architecture, \nsystem workload, and so on; program inputs determine the exact set of instructions to be executed, their \nexecution order and frequencies, as well as the data to be accessed. As shown by the behavior equation, \nfor a given program in a given environment, program inputs are the single impor\u00adtant factor that decides \nthe behaviors of the program in the execution. Manyquantitative measurements have con.rmed this strong \nconnection on various kinds of program behav\u00adiors, including data locality [47], sorting algorithm selec\u00adtion \n[25,40], computationof.oading [42], and memory man\u00adagement [28]. This connection is the intuition for \nprogram inputs to serve as clues for program behaviors prediction the essence of input-centric program \nbehavior analysis.  2.2 Implications to Program Optimizations Conceptually, the bene.ts of exploiting \nprogram inputs for optimizations may be summarized as its potential to ad\u00address a proactivity-adaptivity \ndilemma that limits existing program optimizers. Existing approaches to program behavior analysis fall \ninto three categories asillustrated in Figure 2. Static compi\u00adlation [1,2] focuses on code analysis, \nconsiders certain run- Adaptivit y Proactivity Figure 2. Insuf.cient treatments to program inputs causes \na dilemma between the proactivity and adaptivity of program optimizations. time environments (e.g., the \nnumber of registers),but mostly ignores inputs. They conservatively limit themselves to the properties \nholding for all inputs. Of.ine pro.ling-based techniques typically choose sev\u00aderal inputs as representatives \nfor pro.ling and optimize the program accordingly. Their optimizations are limited to the behaviors exposed \nin the pro.ling runs, hence impairing their adaptivity to new inputs. Finally, runtime behavior analysis[4,8,24,31,46]indy\u00adnamic \noptimizers (e.g., the runtime systems of Java and C#), overcomes the limitations of static and pro.ling \ntechniques by sampling and analyzing program executions on the .y. It has good adaptivity being able \nto adapt to the changes in running environments and program inputs. Butit does not model or exploit program \ninputs: It simply uses the observed behaviors in a recent interval as the prediction for the fu\u00adture. \nAs a result, runtime behavior analysis lacks the proac\u00adtivity that the static and of.ine pro.ling techniques \nhave referring to that theyanalyze and predict the behaviors of the entire program before the start of \nany production run of the program. The importance of proactivity may be less straightfor\u00adward than that \nof adaptivity, but no less important. Its ab\u00adsence in existing dynamic optimization systems has resulted \nin three limitations. First, without a large-scope prediction of the behaviors of the current execution, \nan optimizer has to go through a behavior-monitoring phase periodically to learn about the execution \nbefore optimizing it. The delay impairs the bene.ts the optimizations may bring. Second, what the reactive \nway to learn about program behaviors re\u00adgards is the execution in only some recent intervals. Conse\u00adquently, \nthe corresponding optimizations may be suitable to those intervalsbut inferiortothe entireexecution.Forexam\u00adple, \nsome Java methods that are heavily used in initialization stagesmaybe rarelyinvokedinthe mainexecution.Without \na large-scope view, the JIT compilerinexistingJavaVirtual Machines may be misled to optimize those methods \nsophis\u00adticatedly, bringing virtually no bene.ts to the main execu\u00adtion,but considerable slowdown to the \nstart-up [17]. Finally, the lack of proactivity limits the applicability of dynamic optimizations. For \ninstance, a reactive way to dynamically overcome the dif.culty in making decisions for optimiza\u00adtions \n(e.g. unrolling levels for a loop, registers to spill) is to generate a version for each possible option \nand then try them one by one to select the best. The approach is hard to apply when the option space \nis large as the whole process happens during the current run. Even for small option space, the ap\u00adproach \nmay have limited effectiveness especially when the segment of code (e.g. a subroutine) to be optimized \nhas few invocations (as con.rmed in Section 4.2).  Prior studies have revealed some evidences to those \ndraw\u00adbacks of reactivity. In a study by Arnold and others [5], on a commercial Java Virtual Machine (IBM \nJ9 [16]), programs may run 17-49% faster if the delay in optimizations is just partially removed. A later \nstudy [27] shows that enhanced proactivity increases performance even further. Other studies have seen \nsimilar bene.ts in memory management [28], lo\u00adcality phase prediction [35] and library development [25,40]. \nAs a side note, the limitations of reactive approaches are not only for dynamic program optimizations, \nbut also for dynamic adaptations in other levels, including operat\u00ading systems and architectures. For \ninstance, reactive co\u00adscheduling [38] on chip multiprocessors requires the trials of possible co-runs \n(multiple jobs running on a single chip) to .nd the schedule that minimizes the effects of shared-cache \ncontention. It is hard to scale as the number of possible co\u00adruns is exponential in the numbers of jobs \nand computing units [20]. By providing a proactive way for large-scope program behavior prediction, the \ntechniques presented next are potentially bene.cial to those levels as well. Detailed discussions are \nout of the scope of this paper. 3. Input-Centric Behavior Analysis and Optimizations Driven by the potential \nbene.ts of exploiting program inputs for optimizations, we have developed a set of techniques en\u00adcapsulated \nin thepyramid in Figure 1. They are designed to tackle some critical obstacles to input-centric optimizations, \nincluding the characterization of complex program inputs, the construction of predictive models mapping \nfrom input features to program behaviors, and the capitalization of these models for program optimizations. \nThis section presents the three layersof thepyramidina bottom-up order. 3.1 Input Characterization One \nof the major hurdles to exploiting program inputs is their complexity. An application may allow hundreds \nof options; those options may overshadow each other; input .les may contain millions of data elements, \norganized in complex structures and representing various semantics (e.g., trees, graphs, videos). The \ngoal of input characterization is to address these complexities in a systematic and fully automatic manner. \nSpeci.cally, it tries to reduce the raw, complex program in\u00adputs to a set of features. These features \ncritically determine the behaviors of the program that are essential to its perfor\u00admance. Consider the \nGNU compression tool, Gzip. Its core includes a loop that applies Lempel-Ziv coding to a 32 KB segment \nof the input .le in each iteration. Although the cod\u00ading results and some .ne-grained behaviors may differ \non different input .les, the major behaviors (loop trip-counts, function calling frequencies, etc.) of \nthe program do not as long as the input .les are of a similar size and some critical compression options \nremain the same [34]. Similarly, for a quick-sort program, the distribution rather than the values of \ninput data determines the sorting process [25, 40]. As to be presented in Section 4, our examination \nof 10 Java and 14 C applications shows that for most of them, it is enough to predict their main behaviors \nfrom a small number of input features. However, automatically extracting the critical features directly \nfrom program inputs is a remarkably challenging task. An input data .le may have arbitrary structures \nand semantics, ranging from a graph to an audio to a database or even a program. It may have a large \nnumber of attributes, from as simple as the values of some special numbers in a .le to as concealing \nas the density of a graph, the frequency range of an audio signal, the distribution of a bag of data, \nand the numbers of various constructs in a program. In this work, we employ a technique, named seminal\u00adbehavior \nanalysis, to circumvent the dif.culties. Seminal\u00adbehavior analysis is a technique proposed recently by \nJiang and others [21]. It is enlightened by the strong correlations among program behaviors. Such correlations \nare statistical properties. For instance, in .ve runs of the program mcf on .ve different inputs, one \nof its loops (denoted as loop-1) has iterations (15, 41, 52, 89, 101), and another (denoted as loop-2) \nhas iterations (69, 173, 217, 365, 413). Statistical analysis can easily determine that the trip-counts \n(i.e., the numbers of iterations) of these two loops have a linear rela\u00adtion as C2 =4 * C1 +9 (C1 ,C2 \nfor the trip-counts of the two loops). Jiang and others show that such statistical cor\u00adrelations widely \nexist both among loop trip-counts and from loop trip-counts to other types of behaviors, including func\u00adtion \ninvocations, data values, and so on. Based on those observations, they developed a three-step automatic \napproach to recognizing a small set of behaviors in a program, named a seminal behavior set. These behaviors \nsatisfy two properties. First, they have strong correlations with many other behaviors in the program \nso that knowing their values would lead to accurate prediction of the values of other behaviors. Second, \nthe values of those behaviors be\u00adcome known in an early stage in a typical execution of the program. \nThe seminal behavior set of the program mcf, for example, is composed of 10 behaviors: the trip-counts \nof .ve of its loops, the values of four of its variables whose values come directly from command line \narguments or input .les, and its input .le size. In all measured runs, the values of most seminal behaviors \nbecome explicit during the .rst 10% portion of an execution. Their values show strong statistical correlations \nwith the trip-counts of most loops and the call\u00ading frequencies of most functions in the programs.  \nThe entire process for .nding seminal behaviors is through a fully automatic tool; no manual efforts \nare necessary. The tool is composed of a compiler (speci.cally, a modi\u00ad.ed GCC [21]), a pro.ling component, \nand a data analysis component. Given a program and a set of inputs, during the compilation stage, the \ncompiler inserts some instructions into the program to prepare for program behavior collection. Then \nthe pro.ling component runs the program on each of the inputs; the behaviors reported by the inserted \ninstruc\u00adtions form a database. After the pro.ling step .nishes, the data analysis component analyzes \nthe content of the database (using regression techniques) to examine the statistical rela\u00adtions among \nobserved behaviors, and then determines which of those behaviors may serve as seminal behaviors based \non their earliness (i.e., how early their values become known in an execution) and the strength of their \ncorrelations with other behaviors. The previous paper [21] contains the formal de.nition of seminal behaviors \nand the detailed design and implementation of the tool. In this work, we adopt seminal behaviors for \nprogram in\u00adput characterization. The basic rationale is that because sem\u00adinal behaviors have strong correlations \nwith many other be\u00adhaviors, their values essentially capture the important fea\u00adtures of the current program \ninputs and offers clues for pro\u00adgram behavior prediction.For instance,in the 2-loopexam\u00adple mentioned \nin the earlier paragraph, suppose loop-1 is identi.ed as a seminal behavior. In a new execution, as soon \nas its loop trip-counts(C1 )become known, we may imme\u00addiately predict the trip-counts of loop-2 by plugging \nC1 into the linear equation C2 =4 * C1 +9. By using seminal behaviors, we avoid the needs for direct \nattacks to the complexities in program raw inputs. It offers an indirect way to characterize program \ninputs in a fully automatic manner.  3.2 Input-Behavior Modeling Input-behavior modeling is the second \ncomponent of input\u00adcentric behavior analysis. Its goal is to construct mod\u00adels, namely input-behavior \nmodels, that capture the con\u00adnections between input features represented by seminal behaviors and program \nbehaviors. With such models, the prediction of program behaviors from an arbitrary input be\u00adcomes possible. \nThe modeling is through cross-run incremental learning. For a given application, during each of its executions, \nthe runtime system records the values of seminal behaviors and (sampled) program behaviors in a database. \nAfter a certain number of runs, a learning agent applies statistical learning to the database to construct \ninput-behavior models. For a run on a new input, the runtime system uses the constructed models to predict \nhow the program will behave, preparing for proactive dynamic optimizations. The learning occurs repeatedly \nfor continuous enhancement of the models. In this section, we .rst describe the formulation of the problem \nof input-behavior modeling as a statistical learning problem and outline the basic solutions.We then \nconcentrate on several challenges in the learning process that are special to input-behavior modeling \nand describe our answers. Sec\u00adtion4will show that treating these special challenges is crit\u00adical for \nthe quality of the produced input-behavior models. 3.2.1 ProblemFormulation The input-behavior modeling \nis a statistical learning process. Its objective is to determine a function that maps inputs, characterized \nby seminal behaviors (denoted by V ), to target behaviors (denoted by B). The mapping function is repre\u00adsented \nas B = f (V ), where V is a vector with each element corresponding to one seminal behavior. The learning \ntarget, B, can be one behavior or a vector of multiple target behav\u00adiors.Inthe latter case,the learning \nprocessbuildsa mapping function between V and each of the target behaviors. During the learning process, \non every run of the program on an input data set, we obtain the values of both V and B. After a number \nof runs, we accumulate a database {< Vi ,Bi >}(i =1, 2, \u00b7\u00b7\u00b7 ,N ). Determining the function f from such \na database is a typical statistical learning problem. Speci.cally, when the target behavior has categorical \nvalues (i.e., its value set has a limited number of members), the problem is a classi.cation problem;when \nthe target behavior has continuous values, it is a regression problem [18]. The function f can be in \na form of mathematical formulas or in a less structured form, such as DecisionTrees, SupportVector Machines, \nNeural Networks. To give a concrete explanation, we take the compilation of Java methods in Jikes RVM \n[3] as an example. In Jikes RVM, the JIT compiler may optimize a Java method at 4 levels (-1, 0, 1, 2). \nDue to the tradeoff between compila\u00adtion time and execution time, the appropriate optimization level \ndiffers for different methods. Moreover, for a speci.c method, the best level may vary across program \ninputs. To apply input-centric analysis and optimizations to this exam\u00adple, we may build a model between \nprogram input features and the appropriate optimization level for each Java method. So, in this example, \nB is not a single behavior, but a set of target behaviors, each member of which is the appropriate optimization \nlevel of a Java method. Correspondingly, f is a set of functions, with each member mapping from input \nfea\u00adtures to one member of B. Because optimization levels are categorical, all functions in f are classi.ers, \nthe determina\u00adtions of which may proceed independently from one another.  3.2.2 Classi.cation and Regression \nMany classi.cation and regression methods are applicable to input-behavior modeling. In this work, we \nselect Deci\u00adsionTrees as the primary approach for both classi.cation and regression, because of its simplicity \nand other appealing  X2 7 6 5 4 3 2 1 0  02468 X1 Figure 3. A training dataset (o: class 0; x: class \n1) and the classi.cation tree. Each leaf node is labeled with a class number. properties to be listed \nat the end of this section. The spe\u00adci.c forms of Decision Trees for classi.cation and regres\u00adsion are \nnamed Classi.cation Trees and Regression Trees respectively. A classi.cation tree is a hierarchical data \nstructure im\u00adplementing the divide-and-conquer strategy [18]. It divides the input space into local regions, \nand assigns a class label to eachof those regions. Figure3shows such anexample. Each non-leaf node asks \na question on the input features. Each leaf node has a class label. For a new input, going through the \ntree with its features would lead us to a leaf node. The label of that leaf node is the prediction on \nthe new input. Thekey in constructinga classi.cation treeisin selecting the appro\u00adpriate questions to \nask in each non-leaf node. The goodness ofa questionis quanti.edby purity:A questionis desirable if after \nthe split based on the question, the data in each sub\u00adspace has the same class label. Many techniques \nhave been developed to automatically select the appropriate questions based on the entropytheory [18]. \nA regression tree shares with a classi.cation tree in the form of the data structure and the construction \nprocess. The primary differences exist in the calculation of purity and the .nal label of a leaf node. \nBoth differences are due to the nu\u00admerical rather than categorical values of the learning targets. By \ndefault, a leaf node in a regression tree is labeled as the average value of all the instances contained \nin the node. In our implementation, we add an extension to enhance the pre\u00addictive capability by applying \nLeast Means Square (LMS) to the instances in the leaf node to produce a linear function of the input \nfeatures. We adopt Decision Trees as the main learning technique for its .ve-fold appealing properties. \nFirst, the construction and use of DecisionTrees are simple, ef.cient, and fully au\u00adtomatic. Second, \nDecision Trees has excellent interpretabil\u00adity. Unlike Neural Networks or other learning models work\u00ading \nas a black box to users, a decision tree is essentially a set of if...else... rules. This interpretability \ncan not only help users analyze and verify the results easily, but also increase the understanding to \nthe problem. Third, DecisionTrees han\u00addle both categorical and numerical input features smoothly. Both \nkinds of features are common in program behavior analysis. Fourth, its tree structure is a natural match \nwith non-linear relations. The combination with linear regression techniques (e.g., LMS) makes it capable \nto handle various relations between input features and the learning targets. Fi\u00adnally, it automatically \nselects important input features. It is possible that some input features are either constant across \nall inputs or irrelevant to the learning targets. During the con\u00adstruction of decision trees, because \nquestions on those fea\u00adtures cause no impurity reduction,theywill not appear in the trees. The reduction \nof features helps reduce the dimension\u00adality of the problem, preventing certain noises from hurting the \nquality of the constructed decision trees. Our strategy for the statistical leaning is a two-level strat\u00adegy. \nFor a given target behavior, before applying Decision Trees, we .rst try LMS to .t the training data \nset with poly\u00adnomial (linear or quadratic) functions. Using cross valida\u00adtion [18] (i.e., part of data \nfor training and others for testing), we estimate the quality of the .tting. We resort to Decision Trees \nonly when the linear models cannot .t the data well; such cases typically suggest the existence of non-linear \nrela\u00adtions. The entire learning process involves no manual inter\u00advention.  3.2.3 Challenges Special \nto Input-Behavior Modeling Input-behavior modeling imposes several special challenges to the classi.cation \nand regression techniques.A systematic treatment to these challenges in the modeling process turns out \nto be critical for the quality of the modeling results. Categorical Features. The .rst challenge is on \nthe pres\u00adence of categorical features, referring to the features whose values can only be one of a limited \nnumber of values. Ex\u00adamples include the optimization levels in Jikes RVM, the options that control which \ncompression algorithm to select in a compression tool, the type of an input .le, and so forth. In many \nstatistical learning problems, categorical features are marked beforehand along with their value ranges. \nBut in input-behavior modeling, such knowledge typically does not exist. Consequently, special operations \nare necessary for identifying and utilizing such features during the statistical learning process. For \nidenti.cation of categorical features, we exploit two heuristics. First, if the data type of a feature \nis not a number (integer, .oat, etc.), the feature is considered as categorical. Otherwise, if the number \nof unique values of the feature contained in the trainingdata set is smaller than a threshold, the feature \nis considered as categorical as well. The threshold is de.ned as 10% of the total number of the occurrences \nof the feature in the training data set. The intuition is that the large number of repetitive values \nsuggest that the value of the feature in a new run is likely to be one of its values that have appeared \nin the training runs. So even if it is not categorical, treating it as a categorical feature would typically \nwork well in terms of the predictive capability of the produced model. Note, if the new value happens \nto be something not covered by the training set, the risk control (to be presented) would prevent the \nmistaken predictions from causing inferior consequences.  During the model construction process, we \ndeal with cat\u00adegorical features via the use of indicator matrices [18]. A feature with k categories, \nis converted to a vector of k - 1 binary features. If an observed value of this feature is the ith (i \n=1, 2, \u00b7\u00b7\u00b7 ,k - 1)category, the ith binary feature is set to 1, and all the others are set to 0. When \nthe feature value equals the kth category, all k -1 features are set to 0. Feature Selection. The second \nspecial challenge resides in the tension between the manyseminal behaviors and the lim\u00adited number of \ntraining runs.A large number of seminal be\u00adhaviors form a high-dimensional input space with each sem\u00adinal \nbehavior as one dimension. Learning in such a space demands a large number of training data. However, \ncollect\u00ading manyrepresentative inputs and then conducting pro.ling runs are time consuming and not always \nfeasible. We alleviate that tension through feature selection tech\u00adniques. As mentioned earlier in this \nsection, DecisionTrees can automatically .lter out unimportant features. Moreover, during the LMS regression, \nwe apply a standard stepwise method to further .lter out important features. The method works as follows. \nIt .rstbuilds an initial model based on the observations in the training runs with a minimum number of \nfeatures included in the model. It then examines each fea\u00adture that does not show up in the current model. \nIt adds a feature into the model only if it .nds out that the addition improves the predictive capability \nof the model substantially (evaluated through F-statistic analysis [18]). This process continues until \nno more features can be added. In addition to the feature reduction, a risk control scheme (presented \nnext) helps reduce the tension between the number of fea\u00adtures and the number of training runs as well. \nThe scheme distinguishes the subspaces in the input space that are pre\u00addictable from those that are not, \nhence pruning the learning complexity. An alternative to the stepwise method for feature selec\u00adtion is \nthe Principle Component Analysis (PCA). It casts fea\u00adtures to the orthogonal axes of a principle component \nspace and selects only those directions along which the values of the data show large variations. The \nuse of PCA allows no presence of categorical features. We apply PCA only when all features are numerical, \nand use the stepwise method oth\u00aderwise. Risk Control and Model Evolvement. It is important to prevent \nwrong predictions from hurting program optimiza\u00adtions. Discriminative prediction is an approach proposed \nin a recentwork [27] for risk control. The learnerkeeps assess\u00ading the con.dence level of the input-behavior \nmodels and predicts only if the con.dence level is higher than a pre\u00adset threshold. (The optimizer falls \nback to the default reac\u00adtive strategy when the con.dent is low.) The con.dence is measured through cross-validation \non the collected behav\u00adiors of history runs stored in the database. This risk control is coarse-grained \nin that the whole program has only one con.\u00addence value, regardless of the variations in the input feature \nspace and the behaviors to predict. In this work, we extend the approach to allow .ne-grained control. \nThe motivation is that the predictive capability of a model often varies in different regions in the \ninput feature space. It also depends on the behaviors to predict. The .ne\u00adgrained risk control maintains \na con.dence value for each input sub-space to capture such differences. When Decision Trees are constructed \nin the model training process (Sec\u00adtion 3.2), the tree leaf nodes, along with the value ranges of training \nseminal behaviors, form the input sub-spaces. Otherwise, there is only one subspace, outlined by the \nvalue ranges of all seminal behaviors in the training data set. All con.dence values are compared against \na single prede.ned con.dence threshold (0.7 in our experiments, the same as previouswork uses [27]).Ifa \nnew inputfalls outsideof the con.dent sub-spaces, the prediction is shut down automat\u00adically and the \nsystem falls back to the default optimization scheme. The behavior models and the con.dence values may \nevolve on newly conducted training runs. The observed sem\u00adinal behaviors and target behaviors in a new \nrun may be added into the training data set so that the safe region can be continuously expanded. The \nmodels may be retrained using the expanded data set, with the con.dence levels updated accordingly. \n 3.3 Input-Centric Adaptation The large-scope prediction of program behaviors, enabled by input-centric \nbehavior analysis, opens many new oppor\u00adtunitiesfor program optimizations.We namethenewwayof optimization \ninput-centric adaptation. Two features, proac\u00adtivity and holism, distinguish input-centric adaptation \nfrom existing dynamic optimizations. 3.3.1 Proactivity and Holism Being proactive means that the optimizations \nhappen at the early stage of a program s execution, no need to wait for the .nish of periodical monitoring \nphases. The direct advan\u00adtage is that it can determine and apply suitable optimization decisions early, \navoiding the optimization delays in reactive schemes. In addition, it expands the applicability of dynamic \noptimizations to the scenarios where reactive schemes can\u00adnot work well, such as the job co-scheduling \nproblem men\u00adtioned in Section 2.2. The holism has two-fold meanings. On the program level, it means that \ninput-behavior models predict the behaviors of the entire program (or a large portion of it) rather than \na small windowof theexecution.With that view, optimizers may make more accurate decisions so that manyissues, \nsuch as the JIT compilation problem mentioned in Section 2.2, can be resolved. The second meaning of \nthe holism is that the large-scope proactive behavior prediction makes cross\u00adlayer optimization more \nfeasible than before. The predicted behaviors may be passed across software execution layers, facilitating \nthe coordination among compilers, OS, and vir\u00adtual machines. Detail in this cross-layer aspect is yet \nto be explored in our future work.  3.3.2 Example Uses In this section, we describe two uses of input-centric \nadap\u00adtation to explain how it may be integrated into existing dy\u00adnamic optimizers. The next section reports \nthe resulting per\u00adformance improvement, showing the quantitative bene.ts brought by the proactivity and \nholism of the input-centric adaptation. Use 1: JIT Optimizations in Jikes RVM. The .rst use is on the \nenhancement of Java program performance through JIT. We implement a prototype of seminal-behavior-based \nproac\u00adtive dynamic optimizationsin JikesRVM [3]. Like mostJava Virtual Machines, Jikes RVM is reactive: \nDuring an exe\u00adcution, the RVM observes the behaviors of the application through sampling, whereby, it \ndetermines the importance of each Java method in the application, and invokes the JIT compiler to (re)optimize \nthe method accordingly. As com\u00adpilation incurs runtimeoverhead,theJITinRVMoffers four compilation levels. \nThe high-level optimizations (more so\u00adphisticated and hence taking more time) are supposed to be used \nonly for important Java methods, and low-level opti\u00admizations for others. Complexities reside in the \ndetermination of the impor\u00adtance of a method. At each time point, Jikes RVM assumes that the time a method \nwill take in the rest of the execution is the same as the time it has already taken; the longer that \ntime is, the more important the method is. In reality, this as\u00adsumption often does not hold, causing \ninaccurate prediction of the importance. The JIT compiler may be hence misled to optimize an unimportant \nmethod sophisticatedly. On the other hand,RVM may not recognize the importanceofa re\u00adally critical method \nuntil the late stage of the execution due to the reactivity of the scheme. As a result, the compiler \nmay compile the method multiple times in increasing levels grad\u00adually, rather than in the highest level \nat the early encounter of the method. These issues cause extra compilation overhead and impair the effectiveness \nof the dynamic optimizations. We integrate input-centric behavior analysis into Jikes RVM 2.9.1 to enable \nproactive dynamic optimizations. First, we identify seminal behaviors through of.ine training and build \nthe models between seminal behaviors and the appro\u00adpriate optimization levels for every Java method in \na pro\u00adgram. The appropriate optimization levels used in the train\u00ading process come from the default cost-bene.t \nmodel in the JikesRVM. The cost-bene.t model determines the appropri\u00adate optimizationlevel from the hotness \n(i.e.,invocation times and length) of a Java method. By feeding the cost-bene.t model with the hotness \nof all Java methods obtained at the end of a training run, we get the appropriate optimization levels \nto be used in the training process. After that, as soon as the values of seminal behaviors be\u00adcome explicit \nin an execution, Jikes RVM can plug those values into the constructed predictive models to predict the \nappropriate optimization levels of the Java methods (if the input falls into the safe regions and the \ncon.dence level is high enough). Speci.cally, when a Java method is encoun\u00adtered for the .rst time, it \nis compiled using the basic op\u00adtimizer (at the lowest optimization level), but meanwhile, a recompilation \nevent is pushed into the Jikes RVM event queue so that the method will quickly be recompiled at the level \npredicted from the seminal behaviors. (Not using the predicted level for the .rst-time compilation is \nto avoid im\u00admature optimizations because many references are possibly not resolved yet. This scheme is \nconsistent with those used in previous studies [5, 27].) Compared to the default dynamic optimization \nschemes in JikesRVM, the new approachavoids unnecessary recom\u00adpilations and tends to generate high-quality \ncode at the early stage of the program execution. Section 4.1 reports the re\u00adsulting performance improvement. \nUse 2: Proactive Dynamic Version Selection. The second useis onC program optimizations through dynamicversion \nselection. Dynamic code version selection is a technique for enabling the adaptation of program optimizations \non input data sets. Our study is particularly based on the work by Chuang and others [10]. In that work, \nfor each function, the compiler generates several versions using different optimiza\u00adtion parameters. \nDuring runtime, those versions are used and timed in the .rst certain number of invocations of a function; \nthe version taking the shortest time to run is selected for the rest of the execution. The technique \nshows promising results. But as the au\u00adthors point out, the way a version is selected is subject to some \nlimitations. First, the timed execution of the different versions may operate on different parts of a \ndata set, caus\u00ading unfairness in the comparison, and leading to an inferior selection result. Second, \nthe technique is unlikely to bene\u00ad.t the functions that have only a few invocations, because of the requirement \nof runtime trials. This limitation is espe\u00adcially serious when such functions contain large loops and \ndominate the entire execution time. Seminal behaviors offer a solution to these issues. The key insight \nis that because seminal behaviors capture the dominant in.uence of program inputs on the program exe\u00adcution,ifwe \ncanbuilda mapping fromthevaluesof seminal behaviors to the suitable versions during training time, we \ncan immediately predict the best version to use for a new run as soon as the values of seminal behaviors \nbecome ex\u00adplicit in that run. In this way, we do not need the trials of the different versions during \nruntime, hence circumventing both limitations of the previous work. Details of the imple\u00admentation will \nbe presented in Section 4.2 along with the performance results.  Discussions. As a short summary, to \napply input-centric analysis and optimizations to a program, the user needs to do the following two-step \noperations: prepare a set of pro\u00adgram inputs and conduct a pro.ling run on each of the input, and then \napply the input-centric analysis tool to the data col\u00adlected during the pro.ling runs. During the second \nstep, the tool automatically recognizes seminal behaviors and builds up predictive models that map from \nseminal behaviors to some behaviors of interest (e.g., calling frequencies). Af\u00adter that, when the program \nruns with a modi.ed optimizer (e.g., the modi.ed Jikes RVM in our implementation), the program will be \nautomatically optimized in a proactive, dy\u00adnamicfashion based on the predictive models. Proactive optimizations \ndo not con.ict with the presence of program phase shifts. Pior studies [35,36]have shown the predictability \nof phase shifts. The proactive optimizations in input-centric adaptation can thus be applied before each \nphase. Despite that proactive dynamic optimizations overcome some limitations of existing (reactive) \noptimization schemes, we view the proactive scheme as a complement rather than a replacement to existing \ndynamic optimizations. The runtime sampling in existing reactive schemes is important for the assessment \nof the quality of the behavior prediction, and the reactive optimizations serve as a fall-back option \nwhen the proactive schemes cannot work well. Moreover, in the sce\u00adnarios where cross-run learning is \nnot desirable for overhead or other reasons, reactive schemes are especially valuable. 4. Evaluations \nIn this section, we report the results of the two input-centric adaptation techniques described in the \nprevious section. The comparisons of these results with those from the default dynamic optimizers in \nan open-source JavaVirtual Machine (Jikes RVM), a product compiler (IBM XL compiler), and previous manual \nendeavors, demonstrate the potential of the input-centric paradigm for program optimizations. In all \nthe experimental results, the input sets for train\u00ading and testing have no overlaps. Speci.cally, we \nemploy the standard cross-validation scheme [18] in all experiments. The scheme evaluates a predictive \nmodel iteratively. In each iteration, it takes out some inputs (e.g., 90% in our experi\u00adment) from the \ninput sets for the training of the model and then uses the rest inputs for testing. The overall average \nof the prediction accuracies of all the iterations are taken as the .nal result. The seminal behaviors \nused in all experiments are ob\u00adtained in the way described in a previous study [21]. The earliness threshold \nis 0.9, meaning that the trip-count of a loop cannot serve as a seminal behavior if its value cannot \nbe determined (directly or indirectly) in the .rst 10% of an execution. It is worth noting that because \nthe trip-counts of many loops can be determined from their iteration upper\u00adbounds and lower-bounds, they \noften become known much earlier than the executions of the loops .nish. 4.1 Optimizations by JIT In \nthis section, we .rst report the effectiveness of seminal behaviors in predicting the behaviors of 10 \nJava benchmarks, and then present the performance improvement coming from the JIT compilations guided \nby the predicted behaviors. Methodology. The machine we use is equipped with In\u00adtel Xeon E5310 processors, \nrunning Linux 2.6.22. All ex\u00adperiments use Jikes RVM as the virtual machine. Table 1 reports the used \nbenchmarks, which come from three bench\u00admark suites. Theyhave been used in a previous study [27], in \nwhich, manually characterized input features are employed for proactive optimizations1. So using this \nset of benchmarks makes it convenient to compare with the previous results. In the previous work [27], \nthe authors collected a number of extra inputs for each of the programs for their experiments. Those \ninputs are used in this current work as well. Behavior Prediction Accuracy. The fourth to sixth columns \nin Table 1 report the accuracies of three types of behav\u00adiors predicted from seminal behaviors.We select \nthese three types of behaviors for prediction because of their relevance to program optimizations and \nmemory management. The .rst type is method calling frequency, a type of behaviors critical for inter-procedure \noptimizations (e.g., function in\u00adlining). The second type is minimum heap size, referring to the minimum \nsize of the heap on which a Java program can execute successfully. This property is important for deter\u00admining \nthe heap pressure and has been used for the selection ofgarbage collectors[28,37].The thirdtypeisthe \nappropri\u00adate optimization level of each Java method, a key decision affecting the optimization results \nby the JIT compiler in JikesRVM as mentioned in Section 3.3. The results show that the seminal behaviors \ncan predict most of those behaviors with over 94% accuracy. The right\u00admosttwo columnsinTable1 listthe \nresults fromaprevious work [27]. It uses manually characterized input features to help proactive dynamic \noptimizations. The numbers of fea\u00adtures are not as large as the numbers of recognized semi\u00adnal behaviors \nfor half of the programs. The average accu\u00adracy of the predictions from seminal behaviors is 9% higher \nthan the previous results. The higher accuracy comes from two sources. The .rst is that the recognized \nseminal be\u00adhaviors characterize the input features better than those fea\u00adtures manually produced. This \nis not surprising: Given the high complexity in some of those programs (especially those from JVM 98 \nand Dacapo suites) and their inputs, it is hard for manual characterization to determine all the input \nfea\u00adtures that are critical to the programs behaviors. On the other hand, even when the manual characterization \n.nds all im\u00ad 1We exclude one program named fop because of some implementation limitation of the current \nseminal behavior analysis tool.  Table 1. The Numbers of Seminal Behaviors and Prediction Accuracies \nProgram # Through seminal behaviors Manual approach [27] of #of Pred accuracy # of Pred accuracy inputs \nsem. call min opt. features opt. beh. freq heap level level Compressj 20 2 0.93 0.99 0.99 1 0.94 Dbj \n54 4 0.98 0.96 0.84 2 0.86 Mtrtj 100 2 0.89 0.84 0.97 2 0.82 Antlrd 175 39 0.95 0.96 0.95 3 0.83 \nBloatd 100 7 0.76 0.99 0.96 2 0.85 Eulerg 14 1 0.99 0.98 0.99 1 0.91 MolDyng 15  2 0.83 0.98 0.98 \n1 0.81 MonteCarlog 14 1 0.98 0.99 0.99 1 0.83 Searchg 9 2 0.97 0.99 0.99 2 0.96 RayTracerg 12  1 0.9 \n0.98 0.98 1 0.85 Average 51.3 6.1 0.92 0.97 0.96 2 0.87 j: jvm98; d: dacapo; g: grande portant features \n(e.g., for most of the Grande benchmarks), the enhanced statistical learning algorithms, as described \nin Section 3.2, improves the prediction accuracy substantially by systematically handling categorical \nfeatures and reducing the dimensionality of the input space through PCA and step\u00adwise selection. These \nresults demonstrate the importance of a systematic treatment to the special properties of the input\u00adbehavior \nmodeling during the learning process. Performance Comparison. We modify the Jikes RVM to enable proactive \noptimizations guided by the seminal\u00adbehaviors based prediction, as described in Section 3.3. Compared \nto the default dynamic optimization scheme in JikesRVM, the new optimization strategy saves compilation \ntime by avoiding unnecessary recompilations of a method, and saves execution time by generating ef.cient \ncode early. We compare the resulting performance with the perfor\u00admance of the default executions and \nwith the performance (denoted as Manual ) of the programs optimized [27] us\u00ading the previously proposed \nproactive optimizations based on manually characterized input features. Figure 4 shows the results. The \nbaseline is the perfor\u00admance of the default executions. As the speedup differs on different inputs, each \nbar shows the average and maxi\u00admum speedup of all the runs of a program. On average, the seminal-behavior \nbased proactive optimizations yield 10.2 29.4% speedup over the default executions. Because of the improved \nprediction accuracies, theyoutperform the Manual results by 1.9 5.3%. These results, for the .rst time, \ndemon\u00adstrate that proactive optimizations based on automatic in\u00adput characterization may produce even \nhigher speedup than manual endeavors do.  4.2 Proactive DynamicVersion Selection Our experiments of \nproactive dynamic version selection are based on the PDF (pro.le-driven-feedback) compilation of-feredbytheIBMXLCcompiler.ThedefaultPDF \ncompila\u00adtionworksintwo steps.Foragiven application,the compiler .rst instruments it (through the option \n-qpdf1 ) and lets users run it on a training input. That run generates a .le, con\u00adtaining three sections \nthat correspond to the node, edge, and data pro.les, re.ecting the dynamic behaviors of the pro\u00adgram \non basic block frequencies and function return values. The compiler then recompiles the application using \nthe pro\u00ad.ling results as feedback (through the option -qpdf2 ) and generatesa specializedexecutable for \nthe input.We conduct this experiment on an IBM server equipped with Power5 processors and AIX v5.3. In \nthis experiment, as a preparation, for each program, we .rst select .ve representative inputs to do .ve \nindependent PDF compilations, generating .ve versions of the program. During that process, we also record \nthe values of the seminal behaviors in each of the .ve runs. Next, we run the programs with different \ninputs.We col\u00adlect following running times of the programs to compare the performance improvements of \ndifferent version selection techniques. default: the default static compilation at the highest opti\u00admization \nlevel.  def-pdf: the default PDF compilation. As default PDF compilation does not adapt to inputs, we \nobtain the per\u00adformance of a program on an input by running each of the .ve versions on this input and \ngetting the average running time of the .ve runs.  dyn-trial: corresponding to the previous version \nselection technique [10]. The .ve versions of each function are tried in its .rst .ve invocations and \nthe one with the shortest running time is used for the rest of the run. This scheme is a typical reactive \nscheme for runtime version selection.  dyn-sem: seminal-behavior-based version selection. In each testing \nrun, its seminal behavior values are com\u00adpared against the seminal behaviors of the .ve training runs. \nThe highest similarity determines which of the .ve versions will be used for the rest of the execution. \nThe similarity comparison is in terms of Euclidean distance (normalized to remove the differences of \nvalue ranges among dimensions).  ideal: the ideal case, in which, the real pro.le of a run is used for \nthe PDF optimization of that run.  The performance in the .rst case, default, is taken as the baseline. \nMethodology. Table2lists the programs we ve used in this experiment. Theyinclude 14Cprograms in SPEC \nCPU2000 and SPEC CPU2006. We include no C++ or Fortran pro\u00adgrams because the instrumenter we implement \ncurrently works forCprograms only.Weexclude those programs that are either similar to the ones included \n(e.g., bzip2 versus gzip) or have special requirements on their inputs and make the creation of extra \ninputs (which are critical for this study) dif.cult.  is mcount. Although it takes 41% time of an execution, \nit is invoked only once. This limitation explains the distance of the resulting performance from the \nfull speed. 4) By cir\u00adcumventing both issuesfacing dyn-trial, dyn-sem boosts the speedup to 5 13%, about \n3% from the ideal. The essential reason for the promising results is that seminal behaviors make it possible \nto get both proactivity and cross-input adap\u00adtivity,gaining the strengths of both the of.ine pro.ling(def\u00adpdf)and \nthe runtime adaptation(dyn-trial). Given that the baseline is a highly tuned commercial compiler and \nthe speedups come from a fully automatic process, the results demonstrate the promise of input-centric \ndynamic optimizations. As a side note, the results illustrate the imperfectness of the compiler implementation. \nOn some programs (e.g., h264ref), sometimes the ideal case is worse than the default or other cases. \nThis imperfectness is no surprise given the complexity and the use of heuristics in compiler implemen\u00adtation. \nThe majority of the overhead of our technique is on the training process, including the identi.cation \nof seminal be\u00adhaviors and the construction of predictive models. But since the training happens of.ine, \nthe overhead is not critical. The prediction of a behavior using the predictive models takes little time, \nas it only requires the computation of a linear ex\u00adpression and possibly several conditional checks (for \nregres\u00adsion trees). In our experiments, the prediction of the 7,615 loops of gcc takes the longest time, \nbut still .nishes within 11 milliseconds. 5. RelatedWork Prior research in program optimizations falls \ninto three cat\u00adegories in light of the treatment to program inputs. First, static compilation either \nlimits itself to the properties hold\u00ading for anyinput, or uses ad-hoc estimation for dynamic be\u00adhaviors \n[1, 2]. For example, a loop is considered ten times more frequently accessed than others [11]. Second, \nof.ine pro.ling-based methods typically choose several inputs as the representatives to conduct pro.ling \nruns and optimize the program accordingly. Such empirical opti\u00admization methods have been adopted in \nthe construction of some numerical libraries or kernels, such as ATLAS [43], PHiPAC [7], SPARSITY [19], \nSPIRAL [33], FFTW [14], STAPL [40]. However, the lack of cross-input adaptivity im\u00adpairs the effectiveness \nof of.ine pro.ling-based techniques on optimizing programs with input-sensitive behaviors. The third \ncategory includes run-time optimizations that transform a program during its execution. Some of them \nexploit runtime invariants through programmers annota\u00adtions or other efforts; examples include C from \nKaashoek s group [32], Tempo from Consel s group [29], and DyC from Chambers and Eggers group [15]. Others \nmonitor ex\u00adecution through runtime pro.ling to optimize a program; examples include the dynamic feedback \nwork by Diniz and Rinard [12], the continuous program optimizations by Kistler and Franz [22], the ADAPT \nproject by Voss and Eigenmann [41], the CoCo project by Childers, Davidson and Soffa [9], the continuous \nprogram optimization (CPO) projectbyWisniewski and his colleagues [44], the dynamic optimizations on \nLLVM [23], the runtime support for man\u00adaged languageslikeJavaandC#[4,8,24,31,39,46]. These techniques \nobserve runtime behaviors directly, and typically employ reactive optimization scheme. They do not treat \ninputs explicitly; their effectiveness is limited by the con\u00adstraints discussed in Section 2.2. There \nhas been some work that uses input features, such as the computation of.oading byWang and Li [42], the \nadaptive algorithm selection from Li and his colleagues [25] and from Rauchwerger and oth\u00aders [40]. Their \nexplorations are mainly on a speci.c class of applications and use manually de.ned input features. Ama\u00adral \nand others have investigated the in.uence of inputs on benchmark design [6]. A recent work [27] also \nuses input features as the clue to predict the best optimization levels for a Java method to en\u00adable \nproactive optimizations. Our current study differs from the previous work substantially. To the best \nof our knowl\u00adedge, this current work is the .rst that proposes and system\u00adatically develops the three-layer \nparadigm of input-centric program behavior analysis and adaptations. The previous work [27] is a case \nstudy showing the potential of cross-run information for Java method optimizations. It is preliminary \nin all the three aspects of the input-centric paradigm. First, for input characterization, it relies \non programmers to pro\u00advide a speci.cation of input features, which adds extra bur\u00adden to programmers \nand introduces possible errors. Second, it uses classi.cation trees for input-behavior modeling but without \nsystematically considering categorical features, fea\u00adture selection (the stepwise method), and .ne-grained \nrisk control. Finally, for runtime adaptation, this current work ex\u00adplores optimizations for not only \nmanaged environmentsbut also traditional imperative languages. This current study draws on the observations \non behav\u00adior correlations and the concept of seminal behaviors con\u00adtributed by Jiang and others [21]. \nThat previous work shows the existence of the correlations and seminal behaviors, but contains no use \nof them either for systematic input charac\u00adterization, or for any kind of proactive dynamic optimiza\u00adtions. \nThere are some other studies [30] that have exploited the connections between loops and method hotness. \nThey mainly use loop upperbounds as heuristics, with no system\u00adatic exploration conducted into the statistical \ncorrelations among a broad range of program behaviors. In the realm of software testing, there has been \na body of work on input speci.cation and generation. Those tech\u00adniques focus on the interface to program \nmodules such as procedures or classes. They do not characterize the hidden attributes for the prediction \nof runtime behaviors.  [6] P. Berube and J. N. Amaral. Benchmark design for robust pro.le-directed \noptimization. In Standard Performance Evaluation Corporation (SPEC) Workshop, 2007. [7] J. Bilmes, K. \nAsanovic, C.-W. Chin, and J. Demmel. Optimizing matrix multiply using PHiPAC:Aportable, high\u00adperformance, \nANSI C coding methodology. In Proceedings of the ACM International Conference on Supercomputing, pages \n340 347, 1997. [8] W. Chen, S. Bhansali, T. M. Chilimbi, X. Gao, and W. Chuang. Pro.le-guided proactive \ngarbage collection for locality optimization. In Proceedings of PLDI, pages 332 340, 2006. [9] B. Childers, \nJ. Davidson, and M. L. Soffa. Continuous compilation: A new approach to aggressive and adaptive code \ntransformation. In Proceedings of NSF Next Generation Software Workshop, 2003. [10] P. Chuang, H. Chen, \nG. Ho.ehner, D. Lavery, and W. Hsu. Dynamic pro.le driven code version selection. In Proceed\u00adings of \nthe 11th Annual Workshop on the Interaction between Compilers and Computer Architecture, 2007. [11] J. \nDean and C. Chambers. Towards better inlining decisions using inlining trials. In Proceedings of ACM \nConference on Lisp and Functional Programming, pages 273 282, 1994. [12] P. Diniz and M. Rinard. Dynamic \nfeedback: an effective technique for adaptive computing. In Proceedings of ACM SIGPLAN Conference on \nProgramming Language Design and Implementation, pages 71 84, LasVegas, May 1997. [13] M. Ernst, J. Perkins,P. \nGuo, S. McCamant, M.T. C.Pacheco, and C. Xiao. The daikon system for dynamic detection of likely invariants. \nScience of Computer Programming, 69:35 45, 2007. [14] M. Frigo and S. G. Johnson. The design and implementation \nof FFTW3. Proceedings of the IEEE, 93(2):216 231, 2005. [15] B. Grant, M. Philipose, M. Mock, C. Chambers, \nand S. J. Eggers. An evaluation of staged run-time optimizations in DyC. In Proceedings of ACM SIGPLAN \nConference on Programming Language Design and Implementation, pages 293 304, Atlanta, Georgia, May 1999. \n[16] N. Grcevski,A. Kilstra,K. Stoodley,M. Stoodley, andV. Sun\u00addaresan. Java just-in-time compiler and \nvirtual machine improvements for server and middleware applications. In Proceedings of the 3rd Virtual \nMachine Research and Tech\u00adnology Symposium (VM), May 2004. [17] D. Gu and C.Verbrugge. Phase-based adaptive \nrecompilation ina JVM. In Proceedings of the International Symposium on Code Generation and Optimization, \npages 24 34, 2008. [18] T. Hastie, R. Tibshirani, and J. Friedman. The elements of statistical learning. \nSpringer, 2001. [19] E.-J. Im, K. Yelick, and R. Vuduc. Sparsity: Optimization framework for sparse matrix \nkernels. Int. J. High Perform. Comput. Appl., 18(1):135 158, 2004. [20] Y. Jiang, X. Shen, J. Chen, and \nR. Tripathi. Analysis and approximation of optimal co-scheduling on chip multipro\u00adcessors. In Proceedings \nof the International Conference on Parallel Architecture and Compilation Techniques (PACT), pages 220 \n229, October 2008. [21] Y. Jiang, E. Zhang, K. Tian, F. Mao, M. Geathers, X. Shen, and Y. Gao. Exploiting \nstatistical correlations for proactive prediction of program behaviors. In Proceedings of the Inter\u00adnational \nSymposium on Code Generation and Optimization (CGO), pages 248 256, 2010. [22]T.P. KistlerandM. Franz. \nContinuous program optimization: a case study. ACM Transactions on Programming Languages and Systems, \n25(4):500 548, 2003. [23] C. Lattner. Macroscopic Data Structure Analysis and Optimization. PhD thesis, \nComputer Science Dept., Univ. of Illinois at Urbana-Champaign, May 2005. [24] J. Lau, M. Arnold, M. Hind, \nand B. Calder. Online performance auditing: Using hot optimizations without getting burned. In Proceedings \nof PLDI, pages 239 251, 2006. [25] X. Li, M. J. Garzaran, and D. Padua. A dynamically tuned sorting library. \nIn Proceedings of the International Symposium on Code Generation and Optimization, pages 111 124, 2004. \n[26] M. Lipasti and J. Shen. Exceeding the data.ow limit via value prediction. In Proceedings of the \nInternational Symposium on Microarchitecture (MICRO-29), pages 226 237, 1996. [27] F. Mao and X. Shen. \nCross-input learning and discriminative prediction in evolvable virtual machine. In Proceedings of the \nInternational Symposium on Code Generation and Optimization (CGO), pages 92 101, 2009. [28] F. Mao, E. \nZhang, and X. Shen. In.uence of program inputs on the selection ofgarbage collectors. In Proceedings \nof the International Conference on Virtual Execution Environments (VEE), pages 91 100, 2009. [29] R. \nMarlet, C. Consel, and P. Boinot. Ef.cient incremental run-time specialization for free. In Proceedings \nof ACM SIGPLAN Conference on Programming Language Design and Implementation, pages 281 292, Atlanta, \nGA, May 1999. [30] M. A. Namjoshi and P. A. Kulkarni. Novel online pro.ling for virtual machines. In \nProceedings of the International Conference on Virtual Execution Environments (VEE), pages 133 144, 2010. \n[31] M. Paleczny, C. Vic, and C. Click. The Java Hotspot(TM) server compiler. In USENIX Java Virtual \nMachine Research and Technology Symposium, pages 1 12, 2001. [32] M. Polettto, W. Hsieh, D. R. Engler, \nand M. F. Kaashoek. C and tcc: A language and compiler for dynamic code generation. ACM Transactions \non Programming Languages and Systems, 21(2):324 369, March 1999. [33] M. Puschel, J. Moura, J. Johnson, \nD. Padua, M. Veloso, B. Singer, J. Xiong, F. Franchetti, A. Gacic, Y. Voronenko, K. Chen, R. Johnson, \nand N. Rizzolo. SPIRAL: code generation for DSP transforms. Proceedings of the IEEE, 93(2):232 275, 2005. \n[34] X. Shen and F. Mao. Modeling relations between inputs and dynamic behavior for general programs. \nIn Proceedings of the Workshop on Languages and Compilers for Parallel Computing, 2007.  [35] X. Shen,Y. \nZhong, and C. Ding. Locality phase prediction. In Proceedings of the International Conference on Architectural \nSupport for Programming Languages and Operating Systems, pages 165 176, 2004. [36] T. Sherwood, S. Sair, \nand B. Calder. Phase tracking and prediction. In Proceedings of International Symposium on Computer Architecture, \npages 336 349, San Diego, CA, June 2003. [37] J. Singer, G. Brown, I. Watson, and J. Cavazos. Intelligent \nselection of application-speci.c garbage collectors. In Proceedings of the International Symposium on \nMemory Management, pages 91 102, 2007. [38] A. Snavely and D. Tullsen. Symbiotic jobscheduling for a \nsimultaneous multithreading processor. In Proceedings of the International Conference on Architectural \nSupport for Programming Languages and Operating Systems, pages 66 76, 2000. [39] S. Soman, C. Krintz, \nand D. F. Bacon. Dynamic selection of application-speci.cgarbage collectors. In Proceedings of the International \nSymposium on Memory Management, pages 49 60, 2004. [40] N. Thomas, G. Tanase, O. Tkachyshyn, J. Perdue, \nN. M. Amato, and L. Rauchwerger. A framework for adaptive algorithm selection in STAPL. In Proceedings \nof the Tenth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, pages 277 288, \n2005. [41] M.Voss and R. Eigenmann. High-level adaptive program op\u00adtimization with ADAPT. In Proceedings \nof ACM Symposium on Principles and Practice of Parallel Programming, pages 93 102, Snowbird, Utah, June \n2001. [42] C.Wang andZ. Li.Parametric analysis for adaptive compu\u00adtation of.oading. In Proceedings of \nACM SIGPLAN Confer\u00adence on Programming Languages Design and Implementa\u00adtion, pages 119 130, 2004. [43] \nR. C. Whaley, A. Petitet, and J. Dongarra. Automated empirical optimizations of software and theATLAS \nproject. Parallel Computing, 27(1-2):3 35, 2001. [44] R.W.Wisniewski,P.F. Sweeney, K. Sudeep, M. Hauswirth, \nE. Duesterwald, C. Cascaval, and R. Azimi. Perfor\u00admance and environment monitoring for whole-system char\u00adacterization \nand optimization. In PAC2 Conference on Power/Performance Interaction with Architecture, Circuits, and \nCompilers, 2004. [45]T.Yeh andY.N.Patt. A comparisonof dynamic branch pre\u00addictors that use two levels \nof branch history. In Proceedings of the 20th Annual International Symposium on Computer Architecture, \npages 257 266, May 1993. [46] C. Zhang and M. Hirzel. Online phase-adaptive data layout selection. In \nProceedings of the European Conference on Object-Oriented Programming, pages 309 334, 2008. [47] Y. Zhong, \nX. Shen, and C. Ding. Program locality analysis using reuse distance. ACM Transactions on Programming \nLanguages and Systems, 31(6), 2009.    \n\t\t\t", "proc_id": "1869459", "abstract": "<p>Accurately predicting program behaviors (e.g., locality, dependency, method calling frequency) is fundamental for program optimizations and runtime adaptations. Despite decades of remarkable progress, prior studies have not systematically exploited program inputs, a deciding factor for program behaviors.</p> <p>Triggered by the <i>strong</i> and <i>predictive</i> correlations between program inputs and behaviors that recent studies have uncovered, this work proposes to include program inputs into the focus of program behavior analysis, cultivating a new paradigm named input-centric program behavior analysis. This new approach consists of three components, forming a three-layer pyramid. At the base is <i>program input characterization</i>, a component for resolving the complexity in program raw inputs and the extraction of important features. In the middle is <i>input-behavior modeling</i>, a component for recognizing and modeling the correlations between characterized input features and program behaviors. These two components constitute input-centric program behavior analysis, which (ideally) is able to predict the large-scope behaviors of a program's execution as soon as the execution starts. The top layer of the pyramid is <i>input-centric adaptation</i>, which capitalizes on the novel opportunities that the first two components create to facilitate proactive adaptation for program optimizations.</p> <p>By centering on program inputs, the new approach resolves a proactivity-adaptivity dilemma inherent in previous techniques. Its benefits are demonstrated through proactive dynamic optimizations and version selection, yielding significant performance improvement on a set of Java and C programs.</p>", "authors": [{"name": "Kai Tian", "author_profile_id": "81430601479", "affiliation": "The College of William and Mary, Williamsburg, VA, USA", "person_id": "P2354022", "email_address": "", "orcid_id": ""}, {"name": "Yunlian Jiang", "author_profile_id": "81384618613", "affiliation": "The College of William and Mary, Williamsburg, VA, USA", "person_id": "P2354023", "email_address": "", "orcid_id": ""}, {"name": "Eddy Z. Zhang", "author_profile_id": "81375606973", "affiliation": "The College of William and Mary, Williamsburg, VA, USA", "person_id": "P2354024", "email_address": "", "orcid_id": ""}, {"name": "Xipeng Shen", "author_profile_id": "81452603368", "affiliation": "The College of William and Mary, Williamsburg, VA, USA", "person_id": "P2354025", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1869459.1869471", "year": "2010", "article_id": "1869471", "conference": "OOPSLA", "title": "An input-centric paradigm for program dynamic optimizations", "url": "http://dl.acm.org/citation.cfm?id=1869471"}