{"article_publication_date": "10-17-2010", "fulltext": "\n Pure and Declarative Syntax De.nition: Paradise Lost and Regained Lennart C. L. Kats Eelco Visser Guido \nWachsmuth Delft University of Technology Delft University of Technology Delft University of Technology \nl.c.l.kats@tudelft.nl visser@acm.org g.h.wachsmuth@tudelft.nl Abstract Syntax de.nitions are pervasive \nin modern software sys\u00adtems, and serve as the basis for language processing tools like parsers and compilers. \nMainstream parser generators pose restrictions on syntax de.nitions that follow from their implementation \nalgorithm. They hamper evolution, main\u00adtainability, and compositionality of syntax de.nitions. The pureness \nand declarativity of syntax de.nitions is lost. We analyze how these problems arise for different aspects \nof syntax de.nitions, discuss their consequences for language engineers, and show how the pure and declarative \nnature of syntax de.nitions can be regained. Categories and Subject Descriptors D.3.1 [Programming Languages]: \nFormal De.nitions and Theory Syntax; D.3.4 [Programming Languages]: Processors Parsing; D.2.3 [Software \nEngineering]: Coding Tools and Techniques General Terms Design, Languages Prologue In the beginning were \nthe words, and the words were trees, and the trees were words. All words were made through grammars, \nand without grammars was not any word made that was made. Those were the days of the garden of Eden. \nAnd there where language engineers strolling through the garden. They made languages which were sets \nof words by making grammars full of beauty. And with these grammars, they turned words into trees and \ntrees into words. And the trees were natural, and pure, and beautiful, as were the gram\u00admars. Among them \nwere software engineers who made soft\u00adware as the language engineers made languages. And they dwelt with \nthem and they were one people. The language en- Permission to make digital or hard copies of all or part \nof this work for personal or classroom use is granted without fee provided that copies are not made or \ndistributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. Onward! 2010, October 17 21, 2010, Reno/Tahoe, Nevada, USA. Copyright \nc &#38;#169; 2010 ACM 978-1-4503-0236-4/10/10. . . $10.00 gineers were software engineers and the software \nengineers were language engineers. And the language engineers made language software. They made recognizers \nto know words, and generators to make words, and parsers to turn words into trees, and formatters to \nturn trees into words. But the software they made was not as natural, and pure, and beautiful as the \ngrammars they made. So they made soft\u00adware to make language software and began to make language software \nby making syntax de.nitions. And the syntax def\u00adinitions were grammars and grammars were syntax de.ni\u00adtions. \nWith their software, they turned syntax de.nitions into language software. And the syntax de.nitions \nwere language software and language software were syntax de.nitions. And the syntax de.nitions were natural, \nand pure, and beautiful, as were the grammars. The Fall Now the serpent was more crafty than any other \nbeast of the .eld. He said to the language engineers, Did you actually decide not to build any parsers? \nAnd the language engineers said to the serpent, We build parsers, but we decided not to build others \nthan general parsers, nor shall we try it, lest we loose our syntax de.nitions to be natural, and pure, \nand beautiful. But the serpent said to the language engineers, You will not surely loose your syntax \nde.nitions to be natural, and pure, and beautiful. For you know that when you build particular parsers \nyour benchmarks will be improved, and your parsers will be the best, running fast and ef.cient. So when \nthe language engineers saw that restricted parsers were good for ef.ciency, and that they were a delight \nto the benchmarks, they made software to make ef.cient parsers and began to make ef.cient parsers by \nmaking parser de.ni\u00adtions. Those days, the language engineers went out from the garden of Eden. In pain \nthey made parser de.nitions all the days of their life. But the parser de.nitions were not gram\u00admars \nand grammars were not parser de.nitions. And by the sweat of their faces they turned parser de.nitions \ninto ef.\u00adcient parsers. But the parser de.nitions were not natural, nor pure, nor beautiful, as the grammars \nhad been before.  The Plagues Their software was full of plagues. The .rst plague were grammar classes. \nOnly few grammars could be turned directly into parser de.nitions. And language engi\u00adneers massaged their \ngrammars all the days of their life to make them .t into a grammar class. And the parser de.ni\u00adtions \nbecame unnatural, and impure, and ugly. And there was weeping and mourning. The second plague was disambiguation. \nTheir new parsers were deterministic. So the language engineers encoded precedence in parser de.nitions. \nAnd the parser de.nitions became unnatural, and impure, and ugly. The third plague was lexical syntax. \nThe new software could not handle lexical syntax de.nitions. So the language engineers made another software \nto turn lexical syntax def\u00adinitions into scanners. But lexical syntax de.nitions were less expressive \nthan the grammars they used before. And they were separated from parser de.nitions, as were scan\u00adners \nfrom parsers. And there was weeping and wailing. The fourth plague was tree construction. The language \nengineers wanted the ef.cient parsers to turn words into trees, as their old parsers did. So they added \ncode to their parser de.nitions. And the parser de.nitions became unnat\u00adural, and impure, and ugly. And \nthose who were oblivious to the working of the ef.cient parsers made parsers that turn the right words \ninto the wrong trees. The .fth and sixth plague were evolution and composi\u00adtion. Once the language engineers \nadded a new rule to their parser de.nitions, those tended to break. And they massaged them by the sweat \nof their faces to make them .t again into the grammar class. And they were not able to compose two parser \nde.nitions to a single parser de.nition because of grammar classes and separate scanners. And there was \nweeping and groaning. The seventh plague was the restriction to parsers. The language engineers turned \nparser de.nitions into recognizers and into parsers. But they could not turn them into generators or \nformatters. That was because parser de.nitions were not grammars. Dedication Many have undertaken to \ncompile a narrative of the things that have been accomplished among us. It seemed good to us also, having \nfollowed all things closely for some time past, to write an orderly account for you that you may have \ncertainty concerning the things you have been taught. So this is the story about the loss of the garden \nof Eden and about the pain and the sweat and the plagues of parser de.nitions. But it is also the story \nabout the promised land and about the naturalness and the pureness and the beauty of syntax de.nitions. \nAnd it is the story about the stiff-necked people of language engineers which ignores the promised land \nand sticks to the pain and the sweat and the plagues. N . DN N . D D . \"0\" D . \"1\" Figure 1. A generative \ngrammar for binary numbers. So in this paper, we show the consequences of giving up the declarativity \nof natural syntax de.nitions. We show how practical issues and trade-offs have lead to grammars plagued \nwith restrictions. We show how upholding and pro\u00adtecting pure and declarative syntax de.nition really \ncan make grammars full of beauty. We show how no compromise must be made: stray even slightly from the \nstraight and nar\u00adrow path and fall afoul of the maintainability and usability of declarative syntax de.nition. \nWe base our story on lessons learned from 20 years of experience with SDF [28, 52], a syntax de.nition \nformalism that has withstood the tempta\u00adtions of impurity and stayed true to the way of declarative syntax \nde.nition. 1. The Beauty of Grammars and Trees Grammars are a simple yet powerful formalism. Most of \ntheir beauty comes from this simplicity of power. Let us discover this beauty from different perspectives. \nWords were made through grammars. Chomsky empha\u00adsizes that a linguistic theory needs to provide .nite \nmod\u00adels for the in.nite productivity of language. The oldest of such models handed down to us is the \nAst.adhy\u00afay\u00afi [40, 41], a \u00af .model of the morphology of Sanskrit. Written in the 4th cen\u00adtury BC by P\u00afanini, \nan Ancient Indian Sanskrit grammar\u00ad . ian, it includes 3,959 rules for the generation of well-formed \nSanskrit words. In the 1950 s, Chomsky formalized generative gram\u00admars [13] as a .nite set of terminal \nsymbols, non-terminal symbols, and production rules. Terminal symbols are the elementary building blocks \nwords in the language are con\u00adstructed from. Non-terminal symbols are syntactic variables used to generate \nwords. Production rules specify which sym\u00adbols can be used in place of a non-terminal. At their left-hand \nside they specify a non-terminal and at the right-hand side they specify the symbols it generates. We \ncan read these rules as rewrite rules: the left-hand side of a rule can be rewritten to its right-hand \nside. Consider the grammar given in Figure 1. It uses the termi\u00ad nal symbols 0 and 1 and the non-terminal \nsymbols D and N to generate strings of binary numbers. For example, to generate the word 10, we can start \nwith symbol N and apply the rule N . DN, giving DN then N . D can be applied, giving DD then D . \"1\" \ncan be applied, giving  E E E . E \"+\" . E \"*\" . NUM E E (a) In productive form. E \"+\" E \"*\" NUM E E \n. E . E . E (b) In reductive form. E E E E (c) + E E * As tree construction rules. E NUM Figure 2. A \ngrammar for arithmetic expressions 1D and .nally D . \"0\" gives 10 Note that the same word can often be \ngenerated in multiple different ways. For example, a sequence of production rule applications of the \nform N; DN; 1N; 1D; 10 also gener\u00adates the word 10. It is common practice in linguistics to discuss the \nstruc\u00adture of words (morphology) and the structure of sentences (syntax) separately. This practice found \nits way to computer science as well. Generative grammars are useful in both cases: in the same way that \nwe can generate words from a vocabulary of letters (i.e., an alphabet), we can generate sen\u00adtences from \na vocabulary of words. Computer scientists are interested in both cases, but the stronger emphasis is \non sen\u00adtences made out of words. As such, we focus on sentences from now on and return to words made \nout of letters later. As an example of a generative grammar that focuses on sen\u00adtences rather than words, \nconsider Figure 2(a). It uses non\u00ad terminal symbol E and terminals + and *. The NUM symbol represents \ndecimal numbers. At the sentence level, NUM is considered a terminal symbol, and the individual letters \nit is constructed from are ignored. They made languages by making grammars. Grammars as proposed by Chomsky \nprovide an intuitive, natural means to describe languages. A grammar is a perfect language de\u00adscription \nif it generates all the sentences of the language and only these. Conversely, grammars can also be employed \nto prescribe new languages. In this case, a sentence is part of the language if it can be generated with \nthe grammar. Other\u00adwise, it is not. The grammar is the only truth. So how can we determine if a sentence \ncomplies to a grammar, i.e. if it can be generated by a grammar? A naive approach would be to generate \nsentences until we .nd the one in question. A better approach is to consider gram\u00admars from a different \nperspective, no longer viewing them as a generative device, but as an oracle that determines if a sentence \ncomplies to it. To illustrate this view, Figure 2(b) shows our expression grammar again, now with the \nleft-hand and right-hand sides rules switched. This emphasizes an al\u00adternative reading of the grammar \nrules as reduction rules: the symbols from the left-hand reduce to the non-terminal symbol of the right-hand \nside. Grammars in practical appli\u00adcations are written in one of the two forms depending on conventions \nand the grammar engineering software used. In the remainder of this paper we show grammars in the reduc\u00adtive \nform. Relying on the reductive reading of a grammar, we can try to rewrite a sentence to a single non-terminal \nsymbol. If this works, the sentence is part of the language, otherwise it is not. We start with a sentence \nand repeatedly interpret the reduction rules as rewrite rules: 3*4+5 given this sentence, we can apply \nNUM . E successively: E*4+5 E*E+5 E*E+E given the last sentence, we can apply E \"*\" E . E: E+E and .nally \nreducing that to E Again note that multiple different ways can be used to re\u00adduce this sentence to a \nnon-terminal symbol. The result is always E. Generation and reduction are symmetric processes. If we \ncan generate a sentence from a non-terminal symbol by applying a sequence of production rules, we can \nreduce this sentence to the non-terminal symbol by applying the same rules in reversed order and vice \nversa. They turned words into trees and trees into words. Often, we are not only interested in recognizing \nwhether sentences are part of the language, but also in the grammatical structure of sentences. This \nstructure can naturally be represented us\u00ading trees. Even in primary school where we learned to recog\u00adnize \nword classes like nouns and adjectives as well as higher\u00adlevel concepts like clauses and phrases, ultimately \ntrees were the basis for analyzing sentences in natural language. Each node of a tree corresponds to \na syntactic construct, made up by the constructs represented by its child nodes. The leaves of the tree \ncorrespond to the terminals of the grammar, and the words in a sentence. Like grammars, trees are simple \nyet powerful. We can combine these two beautiful formalisms by reading grammar production rules as tree \nconstruction rules. Figure 2(c) shows the grammar we know already from Figures 2(a) and 2(b). In this \nnotation, production rules are represented as trees of depth 1. The leaves of each tree indicate the \nsymbols that match the tree construction rule. Before, these symbols were rewritten simply to the non-terminal \nsymbol at the root of the tree. Now, they are rewritten to the tree representing the production rule. \n Let us see how this works for the sentence 3*4+5 again, we apply the last production rule successively: \nE EE EEE 3*4+5; 3*4+5; 3*4+5 given the last tree, we can apply the multiplication rule: E  E EE 3 * \n4+5 and .nally the addition rule: E  E  E E E 3*4+5 Turning trees back into words is conceptually very \nsimple. We just need to collect the terminal symbols at the leaves of a tree from left to right. This \ngives us the sentence represented by the tree structure. Things will become more complicated when we \ntake layout into account, something we will discuss later. They made software to make language software. \nGram\u00admars can be employed in a variety of different ways. So far we discussed how grammars can be used \nto generate, recognize, and build trees from sentences. These and more tasks are automated by language \nsoftware: generators pro\u00adduce sentences, recognizers decide wether a sentence is part of a language or \nnot, parsers turn words into trees, and for\u00admatters turn trees into words. These components are essen\u00adtial \nfor building compilers, interpreters, documentation gen\u00aderators, and other tools. Since the formalisation \nof grammars by Chomsky, com\u00adputer scientists came up with generic tools to turn gram\u00admars into language \nsoftware. They made syntax de.nitions that encode a grammar for use by these tools. Syntax def\u00adinitions \ncan only be applied for the complete spectrum of language software if they maintain the virtue of declarativ\u00adity \nthat grammars provide. Grammars are declarative as they only describe what language software should do, \nnot nec\u00adessarily how to do it. This description does not have to be speci.cally written or altered with \nany one of the applica\u00adtions in mind. Tools to construct language software have to address many practical \nissues, and, as we discuss in the following sections1, there are many potential pitfalls where declara\u00ad \ntivity is lost due to imprudent design decisions. Many tool vendors are tempted to leave the righteous \npath of declarativ\u00adity and incorporate various facets of the implementation of language software into \nsyntax de.nitions. Unfortunately, by deviating from the path, the declarativity of grammars is lost. \nThis loss always comes at a price, incurring limitations in the way syntax de.nitions can be used, or \nrequiring additional effort in the speci.cation of syntax de.nitions. An early ex\u00adample of the loss of \ndeclarativity is the Astadhy\u00afay\u00afi: Though \u00af .. it is known as one of the most consolidated descriptions \nof human knowledge, the description is highly algorithmic and technical. Due to the focus on brevity, \nits structure is quite unintuitive, reminiscent of contemporary machine code. 2. Parser Generation Parsers \ncan be implemented by hand or generated using a parser generator. Handwritten parsers tend to use left-to\u00adright \nscanning of the input, constructing a leftmost derivation of a parse tree. In contrast, generated parsers \ncan use a variety of ef.cient algorithms that may not be easy to write by hand. Grammar subclasses Most \nparsing algorithms work only for a subclass of the set of all context-free grammars, such as LL(1), LL(k), \nLR(1), LR(k), LALR(1), LALR(k), etc. We recall that the .rst L in these abbreviations stands for left\u00adto-right \nscanning. Instead of keeping the whole sentence in memory, ef.cient parsers process them word by word. \nThe constant k indicates the number of words of lookahead that is available. The second L in LL and the \nR in LR stand for a leftmost respectively rightmost derivation. To prospective users, grammar class restrictions \ncan seem quite arbitrary. For instance, LL grammars do not sup\u00adport recursion in the left-most symbol \nof production pat\u00adterns [37]. This means that the expression grammar of Fig\u00ad ure 2 is not supported. \nFrom an implementation point of view, the restrictions make sense for the algorithms used for these parsers. \nLeft recursion in an LL-style recursive de\u00adscent parser would lead to non-termination. However, from \na usability point of view, they reveal a leaky abstraction: the implementation directs and restricts \nthe way in which grammars can be written. LL parsers In 1968, Lewis and Stearns [37] described a class \nof grammars that could be ef.ciently parsed us\u00ading a simple top-down algorithm that constructs a left-most \nderivation. Variations of the algorithm are still in popular use today, notably in the ANTLR parser generator \n[42]. The dis\u00ad tinguishing feature of the class of grammars supported by these LL parsers is that they \ndo not support recursion in the left-most symbol of productions. While they do not support 1 Examples \ninclude handling ambiguous grammars, supporting layout and comments that may interleave syntactic constructs, \nconstructing trees with a particular API, and ensuring acceptable performance.  Term (\"+\" Term)* . E \nFact (\"*\" Fact)* . Term NUM . Fact Figure 3. A left-factored expression grammar. the natural grammar \nfrom Figure 2, language engineers can manually left factor the grammar, eliminating all left recur\u00adsion, \nto allow it to be parsed using an LL parser. The result\u00ading grammar2 in Figure 3 no longer corresponds \nto the nat\u00ad ural grammar and becomes harder to maintain. Consider for example what happens when new operators \nsuch as minus or modulo need to be introduced. Other forms of left recursion may be indirect, which makes \nit harder to pinpoint and solve the problem. As the grammar loses its naturalness, so do the trees con\u00adstructed \nby the parser. Consider the sentence 3+4+5. Using the natural grammar, we can construct the following \ntree3: E  E E E E 3+4+5 This tree perfectly captures the left-associativity of the + op\u00aderator. In \ncontrast, the only possible tree according to the left-factored grammar from Figure 3 looks like this: \nE 3+4+5 Here, the pattern (\"+\" Term)* introduces right-associative structure for each repetition. By \neliminating left recursion from the grammar, the tree structure does not capture the associativity of \nthe operators. The tree no longer incorpo\u00adrates logical subtrees for the subexpressions (3 + 4) and (3 \n+ 4) + 5. This complicates the work of compilers and other language processing tools that operate on \nthe tree. Ad\u00additional work is required to construct trees that have a more natural form, a topic we revisit \nin Section 5. 2 Note that we employ the Kleene star (*) operator in this grammar to de.ne repetitive \nsyntactic structures more concisely. 3 Note that our natural grammar can also be used to construct a \nright\u00adassociative tree for this input sentence. We discuss how to select the desired tree in Section \n3.  LR parsers LR(k) parsers, introduced by Knuth in 1965 [36], can handle a strictly larger set of \ngrammars than LL(k) parsers and are able to cope with left recursion. Un\u00adlike LL parsers, they can provisionally \nmatch multiple pro\u00adductions with the same left-hand side at the same time. Like LL parsers, they rely \non a strictly deterministic algorithm. For every symbol that is consumed, the parser must decide to either \nconsume more input (shift) or to apply a partic\u00adular production rule (reduce). A state in which the parser \ncannot decide whether to shift or reduce is said to have a shift/reduce con.ict; if it cannot decide \nwhich production to apply it has a reduce/reduce con.ict. In order to employ LR parser generators, users \nhave to eliminate such con.icts by factorization. LALR parsing is a common variation of LR parsing that \nslightly restricts the set of supported grammars in order to allow for more ef.cient implementation [18]. \nWell-known LALR(1) parser generators are YACC and the very similar Bison4. A recent case study by Malloy \net al. [38] gives an interesting insight into the typical development process of a Bison-based LALR parser \nfor the C# language. The case study started with a grammar from the C# language de.\u00adnition, and described \nhow over the course of 19 revisions it was painstakingly factored to eliminate all 40 shift/reduce and \n617 reduce/reduce con.icts that were reported for it. Ac\u00adcording to Malloy et al., this is not a surprising \nnumber of con.icts for a grammar not speci.cally designed for the tar\u00adgeted grammar class. In their efforts \nto resolve all con.icts, they encountered various limitations of the parser: for ex\u00adample, using only \n1 symbol for lookahead, their parser had problems distinguishing array types of the form int[][] and \nthe form [,]. Another notable problem they ran into is the use of context-sensitive keywords: for example, \nthe add keyword used for properties is not reserved in C#. In general, they had to make a large number \nof small changes for aspects of the original, natural grammar that were not supported by Bison. Lookahead \nUsing lookahead increases the recognition power of a parser. LL parsers are particularly dependent on \nlookahead [43]. Programmers can manually left-factor common pre.xes of competing productions in order \nto de\u00adcrease the required lookahead. However, at best this process leads to unnatural grammars since \ngrammars using LL(k) or LR(k) with k> 1 are often more natural than with k =1 [43]. At worst, it may \nbe insuf.cient as there are languages that are LL(k) but not LL(k - 1) [22]. While traditionally the \nimportance of the lower memory consump\u00adtion and performance overhead of LL(1) and LR(1) was emphasized, \nmodern parser generators such as ANTLR can automatically select an appropriate lookahead k for a given \ngrammar. 4 In fact, on many systems yacc is an alias of bison.  Consequences of restricted grammar classes \nBased on parsing algorithms such as LL and LR, which only support a subset of all context-free grammars, \nlanguage engineers are forced to focus on the accidental complexity of the in\u00adner workings of parser \nimplementation. Instead of focusing on language design, they have to get absorbed into the id\u00adiosyncrasies \nof parsing algorithms. Factorization and mas\u00adsaging of syntax de.nitions leads to speci.cations that \nhave little correspondence to the high-level declarative descrip\u00adtion of a natural grammar for the language. \nIt leads to a loss of obliviousness: a property used in the aspect-oriented pro\u00adgramming community [21] \nthat says that software engineers should be able to reason about something while being un\u00adconcerned or \neven unaware about the implementation de\u00adtails. Without obliviousness, language engineers are forced \nto write parser de.nitions instead of declarative syntax def\u00adinitions, describing the parsing process \ninstead of the lan\u00adguage while destroying declarativity of natural grammars. Paradise lost indeed. Parser \nde.nitions are a step back from natural syntax def\u00adinitions. To maintain the declarativity of natural \ngrammars and to ensure obliviousness in language design, syntax de.\u00adnitions should be freed from the \nshackles of restricted gram\u00admar classes. Only parser generators that support the full class of context-free \ngrammars should be used. Generalized parsers In order to keep to the straight and narrow path of declarative \nsyntax de.nition, generalized parsing algorithms that extend or substitute LL and LR must be used. Parsing \nalgorithms that can handle the full class of context-free grammars instead of being restricted to a partic\u00adular \nsubclass. Unfortunately, a naive implementation of such a parsing algorithm using backtracking risks \nexponential ex\u00adecution time. The CYK algorithm, independently proposed by Cocke, Younger, and Kasami \n[14, 31, 54] in the 1960s, was histor\u00ad ically the .rst generalized parsing algorithm that operated in \npolynomial (cubic) time. The algorithm required gram\u00admars to be written in or converted to Chomsky normal \nform. Another algorithm introduced in the late 1960s was Ear\u00adley s algorithm [19], which did not require \nthis normal form. In 1985, Tomita designed a generalized form of LR pars\u00ading [49], of which many variations \nand improvements have been developed since [7, 30, 44]. Of these, Tomita s generalized LR (GLR) has the \nattrac\u00adtive property that it runs in linear time for unambiguous grammars and gracefully copes with ambiguities. \nIn prac\u00adtice, most programming languages have few or no ambigui\u00adties, ensuring good performance with \na GLR parser. A recent variation of GLR is generalized LL [47], which acts much like a recursive descent \nparser but uses GLR machinery to handle ambiguities. It achieves linear execution time for LL grammars, \nand cubic execution time in the worst case. Parser generators based on generalized parsing algo\u00adrithms \nhave a number of major advantages. They can con\u00adstruct a working parser for any context-free grammar, \nand do not require massaging, ensuring that the syntax de.nition re.ects the natural, intended structure \nof the language, ensur\u00ading maintainability and preservation of obliviousness. Fur\u00adthermore, the full \nclass of context-free grammars is closed under composition, allowing for modular grammars and reuse, \na topic we revisit in Section 6. 3. Ambiguity Handling Some grammars allow us to turn the same words \ninto differ\u00adent trees. Such grammars are called ambiguous. The expres\u00adsion grammar from Figure 2 is ambiguous. \nFor instance, for the sentence 3+4+5, there are two possible trees: 3+4+5 3+4+5 In this section we discuss \ndifferent ways of handling ambi\u00adguities and their effects on the declarativity of syntax de.ni\u00adtions. \nWe .rst revisit deterministic parsing algorithms such as LL and LR, where ambiguities result in con.icts. \nNext, we discuss parsing expression grammars, a class of gram\u00admars that by de.nition does not allow ambiguous \ngrammars. Finally, we discuss how ambiguity is embraced by gener\u00adalized parsing algorithms. We then show \nhow to amend an ambiguous grammar with disambiguation rules in a way that preserves the naturalness and \nthe declarativity of syntax def\u00adinitions. Ambiguity as con.icts LL and LR parsers are determinis\u00adtic \nparsers: they can only return one parse tree for a given string. This means that they cannot handle ambiguous \ngram\u00admars. Detecting whether a context-free grammar is ambigu\u00adous is a classical undecidable problem \nin formal language theory [11, 25]. However, based on the restrictions of the LL and LR grammar classes, \ncon.icts can give an indication of ambiguity. Tools can statically tell if there is a con.ict. Ab\u00adsence \nof such con.icts indicates an unambiguous grammar. The restrictions posed by LL and LR parsers are far \nfrom a panacea for ambiguity handling. For one thing, they are restricted to only a subset of the unambiguous \ngrammars. Grammars in these classes also do not enjoy good closure properties: often, many new, non-local \ncon.icts are intro\u00adduced when modifying the grammar. This can make modi.\u00adcations tedious as a grammar \nevolves. Few reported con.icts actually indicate ambiguities. Recall the over 600 con.icts encountered \nin the case study by Malloy et al. [38]. Each had to be resolved by hand. In general, to address all \ncon\u00ad.icts can require signi.cant changes to a parser de.nition to the point that the original structure \nis lost and the de.nition becomes harder to maintain.  E \"+\" Term . E Term . E Term \"*\" Fact . Term \nFact . Term NUM . Fact Figure 4. Encoding of operator precedence in grammar pro\u00adductions. One way to \nresolve ambiguities in grammars is by encod\u00ading precedence and associativity directly in the productions \n(Figure 4). Precedences can be encoded by introducing an extra level of indirection, and associativity \ncan be enforced by restricting the production patterns. Unfortunately, by en\u00adcoding precedences and operator \nassociativity directly in the grammar, we lose the declarativity and conciseness of the natural grammar \nin Figure 2. Ambiguity ignored A recent addition to the spectrum of parser generators is that of packrat \nparsers [23]. Like LL parsers, these belong to the family of recursive descent parsers, but they implement \na form of backtracking by means of memoization, allowing them to consider multiple candi\u00addate productions \nrather than just one with LL. Packrat parsers are based on parsing expression gram\u00admars (PEGs) [24], \nand are an adaptation of the TS formal\u00ad ism originally conceived in the 1970s [3]. Parsing expres\u00ad sion \ngrammars are a distinct class of grammars: they cannot be used to express all context-free grammars (CFGs), \nbut are also not a strict subset of this class. PEGs are based on greedy matching of repetitions and \nthe idea of a strict order\u00ading between all productions: the .rst alternative that matches wins. As such, \nthey use an ordered choice operator (/) in\u00adstead of the unordered choice operator (|) of BNF. By virtue \nof disallowing any kind of non-deterministic choice, ambi\u00adguity is ignored and effectively de.ned away \nin PEGs. Grammars with disjoint production patterns describe the same language in both PEGs and CFGs. \nFor non-disjoint pat\u00adterns, this is not the case: in CFGs, ab|a .A and a|ab .A, as well as a . A ab . \nA, describe the language {ab, a}. However, for PEGs, ab/a . A describes {ab, a}, whereas a/ab . A describes \n{a}. Detection of disjointness of pro\u00adductions is undecidable [24]. As an example, consider the if statement \nin a context\u00adfree grammar: \"if\" E \"then\" E . E \"if\" E \"then\" E \"else\" E . E which has the notorious \ndangling else ambiguity: for a nested if statement, it is not clear whether the else clause belongs to \nthe inner or the outer if. In PEGs, production ordering forces one alternative to be selected: \"if\" E \n\"then\" E \"else\" E / \"if\" E \"then\" E . E causing the outer if to win. Programmers may not al\u00adways be \naware of the effects of such orderings, especially for larger, modular grammars with injection productions. \nIncor\u00adrect orderings can cause subtle errors, where a tree different from the intended tree is selected. \nNo con.icts are reported for such grammars, and the alternative trees are never shown. Errors in orderings \ncan also cause parse errors at runtime, as we show next. Consider the following grammar: \"if\" E \"then\" \nE / \"if\" E \"then\" E \"else\" E . E Intuitively, this variation would now cause the inner if to win. However, \nbecause the pre.x of a statement of the form if e1 then e2 else e3 matches the .rst produc\u00adtion, the \nsecond is never considered. A parse error is then reported at the else keyword, which can be confusing \nto programmers that are oblivious to the subtle semantics of PEGs that are essential for ef.cient packrat \nparsing. Ambiguity embraced Generalized parsers take a funda\u00admentally different approach to ambiguity \nhandling. They can handle the full class of context-free grammars, includ\u00ading those that are ambiguous. \nAs such, they can return all possible derivations for an input: a parse forest rather than a parse tree. \nThis parse forest can be used to visu\u00adalize ambiguities that may not be obvious from mere in\u00adspection \nof a grammar. As an example, for a statement if e1 then if e2 then e3 else e4, there are two pos\u00adsible \ninterpretations: if e1 then if e2 then e3 else e4 and E E E E E E if e1 then if e2 then e3 else e4 \nFor each ambiguity in an input sentence, the parse forest branches at that point, combining all possible \ntrees into a single, larger forest, as shown in Figure 5. To some, the no\u00ad tion of parse forests can \nseem intimidating, as something that adds to the complexity of language engineering. We argue that parse \nforests reduce the complexity, by showing am\u00adbiguities in terms of the grammar, rather than in terms \nof the implementing algorithm. Language engineers can remain oblivious about the implementation of a \nparser, and only deal with grammars and trees. Using a textual or visual represen\u00adtation of the parse \nforest, language engineers can simply in\u00adspect any ambiguity and decide which is the intended tree. Disambiguation \nrules As early as 1975, Aho and John\u00adson recognized [1] that the most natural grammar of a lan\u00ad guage \nis often not accepted by the parser generators that are  ... Amb E E     E  E E E E E E E  E \n E if e1 then if e2 then e3 else e4 if e1 then if e2 then e3 else e4 Figure 5. A parse forest for the \ndangling else ambiguity. Note that parse forests can be ef.ciently represented in memory by sharing identical \nsubtrees. used in practice, since the grammar does not fall in the sub\u00adclass of context-free grammars \nfor which the generator can ef.ciently produce a parser. They proposed to de.ne lan\u00adguages using an ambiguous \ngrammar instead indeed, em\u00adbracing ambiguity supplemented with a set of disambigua\u00adtion rules that \nare consulted to resolve parsing con.icts. This approach was .rst implemented in the YACC [29] parser \ngenerator. Unfortunately, most of the work on disambiguation rules has been guided by parser implementation \nalgorithms [5]. Disambiguation rules were simply a way to control (shift/re\u00adduce) actions of the parser. \nThis resulted in an ef.cient im\u00adplementation, but also resulted in very subtle, sometimes un\u00adexpected \nsemantics. To use them, language engineers still needed to understand the underlying implementation. \nOnce again, obliviousness was lost. Therefore, in practice, many language speci.cations tend to encode \nprecedence rules di\u00adrectly in grammars instead, to ensure the semantics are clear and well-understood \n[5]. Disambiguation rules are only effective if they are de\u00adsigned from the ground up to be declarative \nand natural of form. Any premature optimization should be avoided and obliviousness must be maintained. \nTo avoid tie-in to parsing algorithms, they can be implemented as a generic .ltering mechanism that simply \npicks the intended tree from a parse forest. Then, and only then, should optimizations be consid\u00adered \nfor a particular algorithm. For the disambiguation rules we describe here, it turns out most can be encoded \nin a gener\u00adated GLR parse table, eliminating the overhead of post-parse .ltering [50]. Declarative disambiguation \nrules can take different forms [35, 45, 46], such as follow restrictions (a form of longest match), exclusion/reject \nrules, priority and associativity rules, and preference attributes for selecting a default among several \nalternatives. As an example, the dangling else am\u00adbiguity can be resolved in SDF by placing a preference \nat\u00adtribute on either production [35]: \"if\" E \"then\" E . E {prefer} \"if\" E \"then\" E \"else\" E . E Similarly, \nwe can disambiguate the grammar of Figure 2 us\u00ad ing the following priority rule and associativity annotations: \nE \"*\" E . E {left} > E \"+\" E . E {left} which speci.es that the multiplication rule has priority over \nthe addition rule, and that the operators should be treated as left-associative. By embracing ambiguity, \nany undesired ambiguities that are not captured by ambiguity rules can be addressed through other means, \nsuch as non-context-free disambigua\u00adtion strategies. For example, a fall-back disambiguation strategy \ncan be to simply select the .rst alternative (much like the strategy of packrat parsers). Another strategy \ncan be to employ semantic information for disambiguation, as seen in [4] for the C language. Testing \nStandard software engineering practices are essen\u00adtial for creating robust parsers [34]. Version management \nand automated testing are particularly important. Grammar testing has an important role in all three \napproaches to am\u00adbiguity handling: testing against regressions as a grammar is factored to .t a certain \nclass (as described for C# in [38]), testing against inconsistencies because of possible ordering errors \nin PEGs, and direct testing against ambiguities with generalized parsers. Only using generalized parsers \ncan ambiguities in failing tests be visualized and explicitly resolved. They are also the only class \nof parsers that maintain the declarativity of natu\u00adral grammars when handling ambiguities, by separating \nthe concern of disambiguation into separate rules and annota\u00adtions. 4. From Terminals to Lexical Syntax \nSo far we have focused on parsing sentences, in the form of sequences of terminals. For practical applications, \nwe also need to consider the individual characters associated with each terminal. A lexical syntax de.nition \ndescribes the structure of terminals.  [\\ \\t\\n] . LAYOUT \"//\" ~[\\n]* [\\n] . LAYOUT Figure 6. Lexical \nsyntax for whitespace and comments. Lexical syntax is often speci.ed using regular grammars. As such, \nregular expressions form the basis of lexical syntax speci.cations. For example [0-9]+ -> NUM speci.es \nthe lexical production for integer numbers. Lexical syntax is also used to recognize whitespace and comments. \nThese are not considered by the productions of the context-free syntax. As an example, consider an expres\u00adsion \nwith a C++-style comment: 3* //4 + 5 When parsed using the context-free syntax of Figure 2, only the \nterminals 3*5 are considered. Whitespace and comments are only relevant for the lexical syntax of the \nlanguage. Figure 6 de.nes the lexical syntax of these constructs. Scanners and parsers Character-level \ngrammars often re\u00adquire arbitrary length lookahead to parse, as non-terminals can be separated by arbitrary \nlength layout and comments. This means that conventional LL(1), LR(1) or even LR(k) parsers cannot cope \nwith grammars that incorporate a lexi\u00adcal syntax de.nition [7]. Instead, conventional parser imple\u00ad mentations \nseparate the processing of lexical and context\u00adfree syntax into a separate scanner and parser. The scanner \n(or lexer ) then uses regular grammars to tokenize the in\u00adput, and the parser operates on these tokens. \nSeparating lexical and context-free analysis poses a num\u00adber of restrictions on syntax de.nitions. First, \nit requires two different syntax de.nition languages, with different ex\u00adpressive power (e.g., Lex and \nYACC). Using only regular grammars, constructs such as nested comments cannot be expressed. Another restriction \nis that a separate scanner can\u00adnot consider the context in which a token occurs. For exam\u00adple, consider \na sentence \"array [1..10] of integer\" in Pascal, a language that to some degree was designed to be easy \nto parse. The range 1..10 can be tokenized either as the real 1. followed by the real .10, or as an integer \n1 followed by the range operator .. and another integer. Similarly, the modern C# language has several \nnon-reserved keywords that can be used as an identi.er in some contexts. Only based on the syntactic \ncontext could a scanner decide which to select. One solution is to introduce lexical state, letting the \nscanner and parser interact (as done with the infamous lexer hack for parsing C), but this complicates \nthe implementation and is usually detrimental to the declarativity of syntax de.ni\u00adtions. Scannerless \nparsers A scannerless parser [45, 46] elimi\u00ad nates the need for a separate scanner to tokenize the input. \nInstead, it parses directly at the level of characters. Scanner\u00adless parsers use a single syntax de.nition \nfor both lexical and context-free syntax, and can be used to parse languages with a sophisticated lexical \nsyntax such as C [4] and AspectJ [10], without resorting to hacks. Scannerless parsers can only be implemented \nusing generalized parsing algorithms. Scanner\u00adless GLR (SGLR) [7, 51] combines scannerless parsing with \nGLR parsing, and is used for parsing SDF. Other scannerless parsers include the parser of TXL [15] (based \non recursive descent backtrack parsing), and various implementations of packrat parsing [23]. 5. From \nParse Trees to Abstract Syntax Trees So far, we have considered parse trees to represent tree struc\u00adtures. \nAbstract syntax trees abstract over these trees, hid\u00ading details such as whitespace, grouping parentheses, \nand keywords. Only elements with semantic value are main\u00adtained. For example, for the if construct of \nSection 3, a tree node can be constructed with only the condition and the two branches as its children, \nignoring any layout, comments, and literals such as if which are part of the parse tree. Ab\u00adstract syntax \ntrees are useful for subsequent processing of the input, such as semantic analysis and transformations. \nSemantic actions One way of constructing an abstract syn\u00adtax tree is by use of semantic actions. Semantic \nactions are functions that are called once a production successfully matches (or sometimes before that \npoint). They can be used to construct an abstract syntax tree by calling tree construc\u00adtion functions. \nThey can encode properties of the abstract syntax tree that are not encoded by the grammar. For exam\u00adple, \nthey can use functions to construct left-associative data structures for the + operator from the left-factored \ngrammar of Figure 3. Liberal use of semantic actions can constrain the imple\u00admentation of parsing algorithms. \nSemantic actions are bound to a particular implementation language (e.g., C in YACC), reducing the portability \nof grammars. Semantic actions can include side effects, which means that they can become de\u00adpendent on \na particular evaluation order. Users oblivious to the way such an algorithm is implemented can be caught \nby surprise when an implementation does not evaluate the functions in the order they would expect. Any \nchanges to the parsing algorithm such as optimization, generalization, or adding error recovery has \nthe potential to break existing grammars that depend on semantic actions. Semantic actions also tie a \ngrammar to syntax recognition, whereas a declar\u00adative grammar can also be used for other purposes such \nas pretty printing. While syntax tree construction is a common reason for parser generators to support \nsemantic actions, another use case is disambiguation. Predicate functions can be used to resolve con.ict \nstates in a parser algorithm. However, compared to declarative disambiguation rules, they have the same \ndisadvantages as other semantic actions. Rather than  module Expressions exports context-free syntax \nE \"+\" E . E {cons(\"Plus\")}E \"*\" E . E {cons(\"Mul\")}NUM . E {cons(\"Num\")}\"(\" E \")\" . E {bracket}\"if\" E \n\"then\" E . E {cons(\"If\"),prefer}\"if\" E \"then\" E \"else\" E . E {cons(\"IfElse\")} context-free priorities \nE \"*\" E . E {left} > E \"+\" E . E {left} lexical syntax [0-9]+ . NUM [\\ \\t\\n] . LAYOUT \"//\" ~[\\n]* [\\n] \n. LAYOUT Figure 7. The full SDF grammar for the expression lan\u00adguage. polluting declarative grammars \nwith semantic actions, we argue that declarative disambiguation rules should be used to address this \nconcern for context-free languages. For other, non-context-free languages such as C++, semantic postpro\u00adcessing \ncan be used, separating the syntax de.nition from the semantics. Declarative tree construction An alternative \nto full-.edged semantic actions is to add small, declarative annotations with information for constructing \nabstract syntax trees. In SDF, productions can be annotated with a {cons(n)} annotation to map a production \nto a node constructor n. Productions for grouping parentheses can use the {bracket} annota\u00adtion, and \nlexical productions and injections do not need to be annotated. Figure 7 shows the full SDF grammar for \nour expression language as given so far, including priorities, lex\u00adical syntax, and constructor annotations. \nIt also introduces a new production for parenthesized expressions. Based on the constructors in the language, \nan abstract syntax tree of the following form can be created for SDF grammars: t ::= \"...\" // terminals \n| c(t1,...,tn) // constructor applications |[t1,...,tn] // lists of terms By including only terminals, \nconstructor applications, and lists, only those elements that have semantic value are in\u00adcluded in the \ntree. As an example, an expression 3*4+5 in abstract syntax has the form Plus(Mul(\"3\",\"4\"),\"5\") and an \nexpression 3*(4+5) becomes Mul(\"3\",Plus(\"4\",\"5\")) Declarative tree construction is most effective for \ngram\u00admars with a natural structure. The usefulness of these semi\u00adautomatically constructed trees decreases \nas grammars are massaged to .t a certain grammar class (Figure 3) or are changed to encode precedence \nand associativity (Figure 4). Declarative tree construction does not dictate a particular implementation \ntechnology. For SGLR, a simple tree walker is used that creates abstract syntax tree nodes that are ef.\u00adciently \nstored using the ATerm library [6]. Other implemen\u00ad tations can use their own tree walker or factory \nclass (when using the Java version of SGLR [48]) that transforms the parse tree to an abstract syntax \ntree using the annotations on the productions. 6. Language Evolution and Composition Languages evolve \nover time [20], based on new domain insights, new applications, and new technological develop\u00adments. \nSyntax de.nitions should be .exible and allow for change and reuse. A typical example of language evolution \nis language ex\u00adtension. Languages can be extended to meet new require\u00adments. For instance, we may want \nto add comparison ex\u00adpressions to our expression language: E \"==\" E . E E \"<\" E . E E \">\" E . E When \nusing a conventional parser generator, actually inte\u00adgrating these constructs to the parser de.nition \nrequires lan\u00adguage engineers to factorize the productions to .t the gram\u00admar class. The new expressions \nalso have to be integrated into the existing sequence of encoded priorities, and have to be restricted \nto encode associativity. Unfortunately, when extending a grammar G1, even a fully factorized and massaged \ngrammar extension G2 can cause con.icts when added together: G1 . G2 may have additional overlapping \nleft-hand sides and other con.icts. These problems arise as subclasses of the context-free gram\u00admars \nsuch as LL(k) or LR(k) are not closed under compo\u00adsition. More often than not, compositions need to be \nmas\u00adsaged to conform to the class again. In some cases, it may not even be possible to express the combined \nlanguage using the grammar subclass. These problems make parser de.nitions rather fragile artifacts. \nSeparate scanners Using a separate scanner and parser in\u00adcreases problems with extensibility and compositionality \nof parsers. Tokens introduced by language extensions are added to the global set of tokens for the grammar. \nThis can lead to con.icts. These con.icts are particularly prevalent for larger extensions or combinations \nof languages. For example, ex\u00adtending the Java language with (AspectJ) aspects is simply not possible \nwith a stateless scanner [10]. Separate scanners do not always lead to full-blown break\u00ading problems. \nIntroducing new tokens can also simply mean that they are reserved from being used as an identi.er. Con\u00adsider \nfor example the enum keyword introduced in Java 5. Before it was introduced, it could be used as an identi.er. \nIn fact, as it expresses a useful programming concept, its oc\u00ad  public boolean authenticate(String user, \nString pw) { SQL stm = <| SELECT id FROM Users WHERE name = ${user} AND password = ${pw} |>; return executeQuery(stm).size() \n!= 0; } Figure 8. An extension of Java with SQL queries, adapted from [9]. module Java-SQL imports Java \nSQL exports context-free syntax \"<|\" Query \"|>\" . Expr {cons(\"ToSQL\")} \"${\" Expr \"}\" . SqlExpr {cons(\"FromSQL\")} \nFigure 9. Syntax of Java with embedded SQL queries, adapted from [9]. currence was not rare in Java programs. \nStill, because Java is parsed using a conventional, stateless scanner, the language designers had to \nconcede and reserve keywords, breaking backward compatibility. Using a scannerless parser instead, they \ncould simply use these keywords in the declarative syn\u00adtax de.nition without it implicitly causing such \nharmful side effects. Rather, any uses of enum as identi.er could be dep\u00adrecated and phased out as desired. \nUsing disambiguation rules, keywords can be explicitly marked reserved when needed: \"if\" | \"then\" | \"else\" \n-> ID {reject} Composite and embedded languages Declarative syntax de.nitions can be reused by grouping \nreusable productions into modules. Based on modules, we can even build com\u00adplete, reusable language components. \nUsing declarative syn\u00adtax de.nitions, we can combine independent languages, such as Java and SQL. Consider \nFigure 8, which shows such an embedding. In this Java method, SQL is embedded using <| ... |> brackets. \nIn turn, Java expressions are embedded in the SQL query using ${ ... } brackets. Figure 9 de.nes the \nsyntax for this composite language by importing the def\u00adinitions of Java and SQL and adding additional \nproductions for the embedding constructs. Unfortunately, when combining entire modules of lan\u00adguages, \nthe composition problems of conventional parser generators are greatly ampli.ed. The entire, combined \nlan\u00adguage must then be factorized to .t into a grammar subclass. Likewise, a stateless scanner for the \ngrammar of Figure 9 would run into problems with the added <| ... |> brackets, which in standard Java \nwould be recognized as comparison signs and pipes. A stateless scanner would also reserve all keywords \nin both languages, including keywords such as SELECT in Java or this in SQL. Another challenge to the \nde.nition of a composite scanner is that identi.ers in both languages have subtly different de.nitions, \nbut only one can be supported by a combined scanner. 7. Error Recovery To use a language in an interactive \ndevelopment environment (IDE), a parser with error recovery is essential. Using error recovery, parsers \ncan create a (partial) abstract syntax tree for incomplete or erroneous inputs. Such inputs are common \nwhen programmers are actively editing a .le. Using the (partial) tree, IDEs can still provide editor \nservices such as the error markers and content completion, even when programs are not in a syntactically \nvalid state. LL, LR, and GLR parsers have the valid pre.x property, which means that the pre.x string \nparsed at any point can always form a syntactically valid program. When a syntax error is encountered, \nthe state of the parser at the point of failure can be used to recover from the error. A common approach \nto error recovery is to insert or delete symbols at or near the point of failure, in order to return \nthe parser in a syntactically correct state and continue parsing. Sometimes, the parse failure is caused \nby a mistake in the pre.x maybe an extra closing bracket that closed a method prematurely which means \nthat symbols have to be inserted or deleted in the pre.x. Using a declarative syntax de.nition is essential \nfor .ex\u00adibility in the implementation of error recovery. For example, a backtracking algorithm may be \nadded to a parser, which inserts and deletes symbols at a point earlier in the input. If syntax de.nitions \nuse semantic actions that control the be\u00adhavior of the parser or manipulate the global state, such an \napproach is not viable. Automated error recovery can often be improved with help of the language engineer. \nSome parser generators use semantic actions as an imperative exception handling mechanism in parser de.nitions. \nWhile very .exible, this ties syntax de.nitions that use them to a particular platform and implementation \nalgorithm. Worse yet, is also destroys obliviousness. A declarative approach to describe error recovery \nstrate\u00adgies is by using error recovery rules [2, 27]. Using auto\u00ad mated analysis of a grammar it is even \npossible to derive such rules [12, 17, 32]. Derivation of rules is based on recog\u00ad nizing typical programming \nlanguage patterns, such as scope structures or string literals, for which recovery rules can be formulated. \nFor such an analysis to work, it is essential that a syntax de.nition uses a declarative formalism, free \nfrom semantic actions that in.uence its meaning. The grammar should also have a natural structure that \nmakes it easier to use heuristics to derive recovery rules. 8. Beyond Parsers So far we focused on grammars \nas a way to declaratively construct parsers. Indeed, as a software artifact, grammars are primarily used \nto construct parsers. However, as a declar\u00adative formalism, grammars can also be used for other appli\u00adcations. \n Complementary to parsing, grammars can also be used for unparsing or pretty printing of trees. Using \na formatting language such as the Box language [8], parse trees and ab\u00ad stract syntax trees can be formatted \nand printed according to a set of pretty printing rules. Using a declarative syntax def\u00adinition, such \nrules can be automatically derived [16]. Auto\u00ad matically derived rules can be adapted by hand or composed \nwith handwritten rules for improved results. Similar to unparsing, sentence generation is a technique \nfor constructing sentences using just the grammar of a lan\u00adguage, following all paths or a selection \nof paths from a non\u00adterminal symbol. One use case of generated sentences is au\u00adtomated testing, as done \nwith the DGL tool [39]. Generated tests can ensure coverage of language features in processing tools \nsuch as code generators. Sentence generation can also be used for detecting ambiguities in grammars [25, \n26]. Languages, tools, and frameworks for language process\u00ading tools generally operate on the tree structure \nde.ned by a syntax de.nition. Using a declarative syntax de.nition, it is possible to automatically generate \nstatically typed classes, data structures, and type signatures for use in different tools. This may not \nbe feasible if the grammar is encoded in a parser de.nition for a particular parser generator. Some meta-programming \nlanguages and syntax-aware template engines can also embed the concrete syntax of a language, to allow \nfor concise speci.cations of transformation and code generation [53]. In IDEs we can distinguish syntactic \nand semantic editor services. Syntactic editor services are based purely on the syntax of a language, \nand are closely tied to its de.nition, whereas semantic services typically use a description of the semantics \nof a language de.ned using a meta-programming language or framework. Examples of syntactic editor ser\u00advices \nare syntax highlighting, the outline view, code folding, and syntactic code completion. These services \ncan be declar\u00adatively described using annotations in the grammar or using separate descriptor languages. \nBased on heuristic analysis of the structure of a grammar, default speci.cations can be au\u00adtomatically \nderived [33]. Similar to derived error recovery and pretty printing rules, this works best for a natural \ngram\u00admar. Derived speci.cations can be adapted and composed with handwritten rules. 9. Discussion In \nthe preceding sections, we have shown how pure and declarative syntax de.nitions have signi.cant advantages \nover parser de.nitions. Generalized parsing algorithms can be used to parse such syntax de.nitions, supporting \nthe full class of context-free grammars and preserving obliviousness about the parser implementation. \nThey can smoothly handle ambiguity by building parse forests instead of trees, while declarative disambiguation \nrules can be speci.ed separately rather than requiring the syntax to be changed. Scannerless parsing \nalgorithms allow for the seamless integration of lex\u00adical and context-free syntax. Declarative annotations \non pro\u00adductions can be used to construct abstract syntax trees with\u00adout polluting syntax de.nitions with \nimperative semantic ac\u00adtions that rely on speci.c parser implementations. Modular syntax de.nitions ease \nlanguage evolution and composition. Pure and declarative syntax de.nitions also allow error re\u00adcovery \nrules to be derived or to be speci.ed as separate rules. Finally, unlike parser de.nitions, declarative \nsyntax de.ni\u00adtions are not restricted to parser generation but can be used to generate a variety of software \nartifacts, such as pretty\u00adprinters, sentence generators, and IDE services. Despite their advantages, \ntechniques that support declara\u00adtive syntax de.nition without restriction have not yet become mainstream. \nMost current parser generators still use re\u00adstricted grammar classes, separate scanners, encoded prece\u00addence \nand associativity, and semantic actions. Only a few parser generators allow truly declarative syntax \nde.nitions. SGLR for the modular syntax de.nition SDF is one of these, and may be the most used scannerless \nGLR implementation. Adoption The rather unfortunate state of the practice is that general parsing technology \nand declarative syntax de.\u00adnitions have seen a lack of adoption by both users and devel\u00adopers of parser \ngenerators. In part, practical issues can be blamed for lack of adop\u00adtion by prospective users. Bravenboer \net al. [10] recently analyzed some of these issues. First, the implementation of SGLR was in C, targeting \nthe Unix/Linux platform. With the development of JSGLR [48], this issue has recently been addressed. \nA second issue they listed was the lack of good syntax error handling. This issue has also been recently \nad\u00addressed, supporting error recovery and integrating SGLR into the Eclipse platform [17, 32, 48]. A \nthird issue they listed was the lack of tool support for analyzing ambiguities, which has unfortunately \nnot yet been addressed for scanner\u00adless GLR. Finally, they mentioned that the syntax of produc\u00adtions \nin SDF may be awkward and unappealing to develop\u00aders used to BNF-style rules. This is certainly something \nto consider for a potential successor to SDF. Tool developers have focused for the most part on LL and \nLR parser generators. These algorithms are reasonably sim\u00adple to implement, and do no require the investment \nin time and effort that general algorithms such as (S)GLR or Earley do. This allows tool developers to \nfocus on peripheral is\u00adsues, such as supporting different languages, integrating into meta-programming \nframeworks, and providing tool support. While these are all important areas indeed, SGLR adop\u00adtion may \nhave suffered from a lack of attention in this area only adopting general parsing algorithms can truly \nchange the way these tools are used. Education Even LL, LR, and LALR are not simple algo\u00adrithms that \ncan be easily explained to the lay programmer. Implementations of these algorithms, generated by parser \ngenerators, can be quite large to the point of being intimi\u00addating. Still, to eliminate left recursion, \nreduce lookahead, and resolve shift/reduce and reduce/reduce con.icts, a cer\u00adtain level of understanding \nof these algorithms is compul\u00adsory. Not all language engineers fully understand or even aspire to understand \n what the real meaning of these issues is. More often than not parser implementation is a process of \ntrial-and-error: users simply torture the parser de.nition until it confesses.  The requirement of understanding \nthe parsing process for working with parser generators has lead to the misunder\u00adstanding that simpler \nalgorithms are easier to use. Writing a parser in recursive descent style is still doable, and even straightforward. \nThen why not use an LL parser generator that automates this? Why would one use a LALR parser generator \nthat parses programs in bottomup order based on some unfathomable compressed parse table? Let alone a \nmore complicated algorithm that can handle larger grammar classes? Current computer science curricula \ntend to focus on the implementation of parsing algorithms. Students implement their own LL(1) or LR(1) \nparser and are familiarized with tools such as ANTLR and YACC that give an order of mag\u00adnitude in productivity \ngain over manual implementations. At the same time, they reinforce the line of thinking that pro\u00adgrammers \nneed to understand the inner workings of parsers to work with grammars. Instead, we feel courses should \nfo\u00adcus on language engineering topics as described in [34] and grammar design involving modularity, composition, \nand am\u00adbiguity. Epilogue Since the days language engineers had left the garden of Eden, they carried \nheavy burdens. When they built parsers they suffered from the plagues of parser de.nitions. They were \nslaves to parser generators and their lives were bitter with turning grammars into parser de.nitions \nand parser def\u00adinitions into parsers. And the people of language engineers groaned because of their slavery \nand cried out for help. The promised land The advent of scannerless, generalized parsing delivered language \nengineers out of the slavery to parser generators. It brought them up out of that to a good and broad \nland, a land .owing with milk and honey. In this land, there are no grammar classes. Grammars are syntax \nde.nitions and syntax de.nitions are grammars. And syntax de.nitions are parser de.nitions and parser \nde.ni\u00adtions are syntax de.nitions. And the syntax de.nitions are natural, and pure, and beautiful. In \nthis land, there is declarative disambiguation. And the syntax de.nitions stay natural, and pure, and \nbeautiful. In this land, there are no separate scanners. Lexical and context-free syntax de.nitions are \none de.nition. And scan\u00adners and parsers one tool. In this land, there is declarative tree construction. \nThe parsers turn the right words into the right trees. And the trees are natural, and pure, and beautiful, \nas are the syntax de.nitions. In this land, there is language evolution and composition without pain. \nLanguage engineers can add new rules to their syntax de.nitions. And the syntax de.nitions stay natural, \nand pure, and beautiful. And they can compose two syntax de.nitions to a single syntax de.nition because \nthere are neither grammar classes nor separate scanners. In this land, there is no restriction to parsers. \nSyntax de.nitions are parser de.nitions and parser de.nitions are syntax de.nitions. And syntax de.nitions \nare grammars and grammars are syntax de.nitions. And language engineers turn syntax de.nitions into recognizers, \nand into generators, and into parsers, and into formatters. Exodus But the language engineers did not \nfollow, be\u00adcause of their broken spirit and harsh slavery. Only few went out of the house of slavery, \ninto the promised land. Those few made new software to make parsers and began to make parsers by making \nsyntax de.nitions again. And the syntax de.nitions were grammars again and grammars were syntax de.nitions \nagain. And the syntax de.nitions were natural, and pure, and beautiful again, as were the grammars. The \nother language engineers feared greatly. And the people of language engineers cried out, Leave us alone \nthat we may make parser de.nitions. For it would have been better for us to serve the parser generators. \nBut here is our answer, Fear not! Stand .rm! See the naturalness, and the pureness, and the beauty of \ndeclarative syntax de.\u00adnitions, which will work for you. For the parser de.\u00adnitions that you see today, \nyou shall never see again. Go out to the promised land! Make new software to make parsers and begin to \nmake parsers by making syntax de.nitions again. Let the syntax de.nitions be grammars again and grammars \nbe syntax de.nitions again. And the syntax de.nitions will be natural, and pure, and beautiful again, \nas will the grammars. Acknowledgements We thank Glenn Vanderburg and the anonymous referees for providing \nvaluable feedback to improve the presentation of this paper. This research was supported by NWO/JACQUARD \nprojects 612.063.512, TFA: Transformations for Abstractions, and 638.001.610, MoDSE: Model-Driven Software \nEvolution. References [1] A. V. Aho, S. C. Johnson, and J. D. Ullman. Deterministic parsing of ambiguous \ngrammars. Commun. ACM, 18(8):441 452, 1975. [2] A. V. Aho and T. G. Peterson. A minimum distance error\u00adcorrecting \nparser for context-free languages. SIAM J. Com\u00adput., 1(4):305 312, 1972.  [3] A. Birman and J. D. Ullman. \nParsing algorithms with back\u00adtrack. In Conference Record of 1970 Eleventh Annual Sympo\u00adsium on Switching \nand Automata Theory, 28-30 October 1970, Santa Monica, California, USA, pages 153 174. IEEE, 1970. [4] \nA. Borghi, V. David, and A. Demaille. C-Transformers: a framework to write C program transformations. \nACM Cross\u00adroads, 12(3):3, 2005. [5] E. Bouwers, M. Bravenboer, and E. Visser. Grammar engi\u00adneering support \nfor precedence rule recovery and compatibil\u00adity checking. In A. Sloane and A. Johnstone, editors, Seventh \nWorkshop on Language Descriptions, Tools, and Applications (LDTA 2007), volume 203 of Electronic Notes \nin Theoreti\u00adcal Computer Science, pages 85 101, Braga, Portugal, March 2008. Elsevier. [6] M. G. J. van \nden Brand, H. de Jong, P. Klint, and P. Olivier. Ef.cient annotated terms. Software, Practice &#38; Experience, \n30(3):259 291, 2000. [7] M. G. J. van den Brand, J. Scheerder, J. Vinju, and E. Visser. Disambiguation \n.lters for scannerless generalized LR parsers. In N. Horspool, editor, Compiler Construction (CC 2002), \nvolume 2304 of Lecture Notes in Computer Science, pages 143 158, Grenoble, France, April 2002. Springer-Verlag. \n[8] M. G. J. van den Brand and E. Visser. Generation of formatters for context-free languages. TOSEM, \n5(1):1 41, January 1996. [9] M. Bravenboer, E. Dolstra, and E. Visser. Preventing in\u00adjection attacks \nwith syntax embeddings. In C. Consel and J. L. Lawall, editors, Generative Programming and Compo\u00adnent \nEngineering, 6th International Conference, GPCE 2007, pages 3 12, Salzburg, Austria, 2007. ACM. \u00b4 mal, \nand extensible syntax de.nition for AspectJ. In P. L. Tarr and W. R. Cook, editors, Proceedings of the \n21th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Sys\u00ad tems, Languages, and Applications, \nOOPSLA 2006, October 22-26, 2006, Portland, Oregon, USA, pages 209 228. ACM, 2006. [10] M. Bravenboer, \nEric Tanter, and E. Visser. Declarative, for\u00ad [11] D. G. Cantor. On the ambiguity problem of backus systems. \nJournal of the ACM, 9(4):477 479, 1962. [12] P. Charles. A practical method for constructing ef.cient \nLALR(K) parsers with automatic error recovery. PhD thesis, New York University, 1991. [13] N. Chomsky. \nSyntactic Structures. Mouton de Gruyter, 1957. [14] J. Cocke. Programming languages and their compilers: \nPre\u00adliminary notes. Courant Institute of Mathematical Sciences, New York University, 1969. [15] J. R. \nCordy, C. D. Halpern-Hamu, and E. Promislow. TXL: a rapid prototyping system for programming language \ndialects. In Conf. on Comp. Languages, pages 280 285. IEEE, 1988. [16] M. de Jonge. A pretty-printer \nfor every occasion. In The In\u00adternational Symposium on Constructing Software Engineer\u00ading Tools (CoSET2000), \npages 68 77. University of Wollon\u00adgong, Australia, 2000. [17] M. de Jonge, E. Nilsson-Nyman, L. C. L. \nKats, and E. Visser. Natural and .exible error recovery for generated parsers. In M. van den Brand, D. \nGasevic, and J. Gray, editors, Second International Conference, SLE 2009, Denver, CO, USA, Oc\u00ad tober \n5-6, 2009, Revised Selected Papers, Lecture Notes in Computer Science. Springer, 2010. [18] F. L. DeRemer. \nPractical translation for LR(k) languages. PhD thesis, MIT, Cambridge, MA, USA, September 1969. [19] \nJ. Earley. An ef.cient context-free parsing algorithm. Com\u00admun. ACM, 13(2):94 102, 1970. [20] J.-M. Favre. \nLanguages evolve too! Changing the software time scale. In 8th International Workshop on Principles of \nSoftware Evolution (IWPSE 2005), 5-7 September 2005, Lis\u00adbon, Portugal, pages 33 44. IEEE Computer Society, \n2005. [21] R. Filman and D. Friedman. Aspect-oriented programming is quanti.cation and obliviousness. \nIn Workshop on Advanced separation of Concerns, 2000. [22] C. Fischer and R. LeBlanc. Crafting a compiler. \nBenjam\u00adin/Cummings Menlo Park, California, USA, 1988. [23] B. Ford. Packrat parsing: simple, powerful, \nlazy, linear time, functional pearl. In Proceedings of the seventh ACM SIG-PLAN international conference \non Functional Programming (ICFP 2002), pages 36 47, 2002. [24] B. Ford. Parsing expression grammars: \na recognition-based syntactic foundation. In N. D. Jones and X. Leroy, edi\u00adtors, Proceedings of the 31st \nACM SIGPLAN-SIGACT Sympo\u00adsium on Principles of Programming Languages, POPL 2004, Venice, Italy, January \n14-16, 2004, pages 111 122. ACM, 2004. [25] S. Ginsburg and J. Ullian. Ambiguity in context free lan\u00adguages. \nJ. ACM, 13(1):62 89, 1966. [26] S. Gorn. Detection of generative ambiguities in context-free mechanical \nlanguages. J. ACM, 10(2):196 208, 1963. [27] S. L. Graham, C. B. Haley, and W. N. Joy. Practical LR error \nrecovery. In Proceedings of the 1979 SIGPLAN Symposium on Compiler Construction, Denver, Colorado, USA, \nAugust 6-10, 1979, pages 168 175. ACM, 1979. [28] J. Heering, P. R. H. Hendriks, P. Klint, and J. Rekers. \nThe syn\u00adtax de.nition formalism SDF -reference manual. SIGPLAN Notices, 24(11):43 75, 1989. [29] S. C. \nJohnson. YACC yet another compiler-compiler. Tech\u00adnical Report CS-32, AT &#38; T Bell Laboratories, Murray \nHill, N.J., 1975. [30] A. Johnstone, E. Scott, and G. Economopoulos. Evaluating GLR parsing algorithms. \nScience of Computer Programming, 61(3):228 244, 2006. [31] T. Kasami. An ef.cient recognition and syntax \nanalysis al\u00adgorithm for context free languages. Science Report AF CRL\u00ad65-758, Air Force Cambridge Research \nLaboratory, Bedford, Mass., USA, 1965. [32] L. C. L. Kats, M. de Jonge, E. Nilsson-Nyman, and E. Visser. \nProviding rapid feedback in generated modular language envi\u00adronments. Adding error recovery to scannerless \ngeneralized-LR parsing. In S. Arora and G. T. Leavens, editors, Pro\u00adceedings of the 24th Annual ACM SIGPLAN \nConference on Object-Oriented Programming, Systems, Languages, and Ap\u00adplications, OOPSLA 2009, October \n25-29, 2009, Orlando, Florida, USA., pages 445 464, 2009.  [33] L. C. L. Kats and E. Visser. The Spoofax \nlanguage workbench. Rules for declarative speci.cation of languages and IDEs. In M. Rinard, editor, Proceedings \nof the 25th Annual ACM SIGPLAN Conference on Object-Oriented Programming, Sys\u00adtems, Languages, and Applications, \nOOPSLA 2010, October 17-21, 2010, Reno, NV, USA, 2010. [34] P. Klint, R. L\u00a8ammel, and C. Verhoef. Toward \nan engineering discipline for grammarware. ACM Transactions on Software Engineering Methodology, 14(3):331 \n380, 2005. [35] P. Klint and E. Visser. Using .lters for the disambiguation of context-free grammars. \nIn Proc. ASMICS Workshop on Pars\u00ading Theory, pages 1 20, Milano, Italy, October 1994. Tech. Rep. 126 \n1994, Dipartimento di Scienze dell Informazione, Universit`a di Milano. [36] D. E. Knuth. On the translation \nof languages from left to right. Information and control, 8(6):607 639, 1965. [37] P. M. Lewis II and \nR. E. Stearns. Syntax-directed transduction. Journal of the ACM, 15(3):465 488, 1968. [38] B. A. Malloy, \nJ. F. Power, and J. T. Waldron. Applying soft\u00adware engineering techniques to parser design: the develop\u00adment \nof a C# parser. In SAICSIT 02: Proceedings of the 2002 annual research conference of the South African \ninsti\u00adtute of computer scientists and information technologists on Enablement through technology, pages \n75 82, Port Elizabeth, Republic of South Africa, 2002. South African Institute for Computer Scientists \nand Information Technologists. [39] P. M. Maurer. Generating test data with enhanced context-free grammars. \nIEEE Software, 7(4):50 55, 1990. [40] Pan\u00afini. As. tadhy\u00afay\u00afi. ohtlingk, editor, K\u00a8\u00afOtto B\u00a8onig, 1839\u00ad1840. \n. . \u00b4 [41] P\u00afanini. Ast\u00afay\u00afBenares, 1896. Translated by Sr\u00afi\u00b4saadhy\u00afi. Chandra Vasu. . .. [42] T. J. \nParr. ANTLR Parser Generator. http://www.antlr. org/. [43] T. J. Parr and R. W. Quong. LL and LR translators \nneed k>1 lookahead. SIGPLAN Not., 31(2):27 34, 1996. [44] J. Rekers. Parser Generation for Interactive \nEnviron\u00adments. PhD thesis, University of Amsterdam, Amsterdam, The Netherlands, January 1992. [45] D. \nSalomon and G. Cormack. The disambiguation and scan\u00adnerless parsing of complete character-level grammars \nfor pro\u00adgramming languages. Technical Report 95/06, Department of Computer Science, University of Manitoba, \nWinnipeg, Canada, 1995. [46] D. J. Salomon and G. V. Cormack. Scannerless NSLR(1) pars\u00ading of programming \nlanguages. SIGPLAN Not., 24(7):170 178, 1989. [47] E. Scott and A. Johnstone. GLL parsing. In Workshop \non Language Descriptions, Tools and Applications (LDTA 09), 2009. [48] The Spoofax project. http://www.spoofax.org/. \n[49] M. Tomita. Ef.cient Parsing for Natural Language: A Fast Al\u00adgorithm for Practical Systems, volume \n14. Kluwer Academic Publishers, 1988. [50] E. Visser. A case study in optimizing parsing schemata by \ndisambiguation .lters. In IWPT, pages 210 224, 1997. [51] E. Visser. Scannerless generalized-LR parsing. \nTechnical Report P9707, Programming Research Group, University of Amsterdam, July 1997. [52] E. Visser. \nSyntax De.nition for Language Prototyping. PhD thesis, University of Amsterdam, September 1997. [53] \nE. Visser. Meta-programming with concrete object syntax. In D. S. Batory, C. Consel, and W. Taha, editors, \nGenerative Pro\u00adgramming and Component Engineering, ACM SIGPLAN/SIG-SOFT Conference, GPCE 2002, Pittsburgh, \nPA, USA, Octo\u00adber 6-8, 2002, Proceedings, volume 2487 of Lecture Notes in Computer Science, pages 299 \n315. Springer, 2002. [54] D. Younger. Recognition and parsing of context-free lan\u00adguages in time n 3 \n. Information and control, 10(2):189 208, 1967.  \n\t\t\t", "proc_id": "1869459", "abstract": "<p>Syntax definitions are pervasive in modern software systems, and serve as the basis for language processing tools like parsers and compilers. Mainstream parser generators pose restrictions on syntax definitions that follow from their implementation algorithm. They hamper evolution, maintainability, and compositionality of syntax definitions. The pureness and declarativity of syntax definitions is lost. We analyze how these problems arise for different aspects of syntax definitions, discuss their consequences for language engineers, and show how the pure and declarative nature of syntax definitions can be regained.</p>", "authors": [{"name": "Lennart C.L. Kats", "author_profile_id": "81381609357", "affiliation": "Delft University of Technology, Delft, Netherlands", "person_id": "P2354188", "email_address": "", "orcid_id": ""}, {"name": "Eelco Visser", "author_profile_id": "81100561215", "affiliation": "Delft University of Technology, Delft, Netherlands", "person_id": "P2354189", "email_address": "", "orcid_id": ""}, {"name": "Guido Wachsmuth", "author_profile_id": "81384609090", "affiliation": "Delft University of Technology, Delft, Netherlands", "person_id": "P2354190", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1869459.1869535", "year": "2010", "article_id": "1869535", "conference": "OOPSLA", "title": "Pure and declarative syntax definition: paradise lost and regained", "url": "http://dl.acm.org/citation.cfm?id=1869535"}