{"article_publication_date": "10-17-2010", "fulltext": "\n Faith, Hope, and Love An essay on software science s neglect of human factors Stefan Hanenberg University \nDuisburg-Essen, Institute for Computer Science and Business Information Systems stefan.hanenberg@icb.uni-due.de \nAbstract Research in the area of programming languages has differ\u00adent facets from formal reasoning about \nnew programming language constructs (such as type soundness proofs for new type systems) over inventions \nof new abstractions, up to per\u00adformance measurements of virtual machines. A closer look into the underlying \nresearch methods reveals a distressing characteristic of programming language research: develop\u00aders, \nwhich are the main audience for new language con\u00adstructs, are hardly considered in the research process. \nAs a consequence, it is simply not possible to state whether a new construct that requires some kind \nof interaction with the de\u00adveloper has any positive impact on the construction of soft\u00adware. This paper \nargues for appropriate research methods in programming language research that rely on studies of de\u00advelopers \n and argues that the introduction of corresponding empirical methods not only requires a new understanding \nof research but also a different view on how to teach software science to students. Categories and Subject \nDescriptors D.3.3 [Programming Languages]: Language Constructs and Features General Terms Human Factors, \nLanguages Keywords Research methods, programming language re\u00adsearch, software engineering, empirical \nresearch 1. Introduction The term software crises [8] is often applied in the area of software engineering \nand programming language research in order to argue that these crises still exist, and in order to ar\u00adgue \nthat new techniques are required in order to overcome the crises. And indeed, programming language and \nsoftware Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. Onward \n10 October 17-21, 2010, Reno-Tahoe, Nevada, USA. Copyright c &#38;#169; 2010 ACM 978-1-4503-0236-4/10/10. \n. . $10.00 engineering research seem to be inexhaustible fountains that produce over and over again new \ntechniques that reduce the software crises: new development processes, modeling no\u00adtations, programming \nlanguage constructs, frameworks, etc. are invented and such techniques are claimed to overcome existing \nproblems. However, a closer look reveals that research in the area of programming languages and software \nengineering has a fundamental problem with how they reason on their newly invented artifacts. It turns \nout that many artifacts have an inadequate foundation of how they are justi.ed because they do not consider \nthat developers are part of the software construction process. This means that many new techniques are \nclaimed as solutions for existing problems without any suf.cient investigation. Hence, it is adequate \nto ask whether software engineer\u00ading and programming language research can be considered as serious scienti.c \ndisciplines that construct new artifacts and examine them in an objective way. Spoken in a more practical \nway, it is valid to ask whether these artifacts con\u00adtribute to a solution to today s problems or whether \nthey are the main cause for today s problems and consequently the main cause for the crises that they \nclaim to reduce. As a consequence, developers have to decide on their own whether a new artifact should \nbe considered good or reason\u00adable: for practitioners, it s hard to know what to read, what to believe, \nand how to put the pieces together [23, p. 67]. Hence, the assessment of new artifacts is the product \nof sub\u00adjective experiences and sensations, which is an unacceptable situation -faith, hope, and love \nare a developer s dominant virtues to estimate the bene.t of new artifacts. This essay argues for the \nurgent need to consider human factors when reasoning about programming language and software engineering \nartifacts and emphasizes the need for appropriate empirical methods in order to provide valid and adequate \nrationales for such artifacts. This paper analyzes current research approaches in soft\u00adware sciences \nand discusses their validity and adequacy. It shows that there is already a practice to use human fac\u00adtors \nto argue for certain artifacts -but a practice which is rather based on speculations instead of scienti.c \nmethods.  After comparing the consideration of human factors in other research disciplines, it is critically \ndiscussed why humans factors hardly play any role in programming language and software engineering research. \nFinally, the paper argues that a number of fundamental changes are necessary in research as well as in \nteaching in order to overcome the current inad\u00adequacies. Notes: This paper uses a number of prominent \nscienti.c works to illustrate the inappropriateness of current ratio\u00adnales used in software research. \nThe aim is de.nitively not to discredit any author. Because of that, research papers are chosen in a \nway that they represent fundamental statements about known and popular topics in software research and \nrather no up-to-date papers. It is also necessary to note that this paper does not claim that the statements \nof these papers are wrong it only argues about the missing evidence or the inappropriateness of the \nchosen research methods. This essay will use the term software science as a com\u00admon term for software \nengineering research as well as pro\u00adgramming language research in order to ease the reading1 . 2. Research \nMethods in Software Science This section gives an overview of research methods and ra\u00adtionales which \nare currently applied in software science. The overview should not be considered as a complete descrip\u00adtion \nof all practiced and possible research approaches. The main intention is to show that there is already \na variety of different approaches which differ with respect to the sub\u00adject of research, the kinds of \nstatements being promoted by them, and the techniques used to back up the corresponding statements. Then, \nthe validity and adequateness of such ap\u00adproaches is discussed with a special focus on how the results \ncan be used by a developer in order to determine whether the application of a certain artifact improves \nthe development of software -with the result that human factors, which are es\u00adsential to determine whether \nan artifact improves software development, are hardly (or inadequately) considered. 2.1 Classi.cation \nof research approaches The origin of software science is mathematics. Classical dis\u00adciplines such as \nalgorithms and data structures are based on the approach to examine a program according to some for\u00admal \ncharacteristics such as run-time behavior. Typical ap\u00adproaches in these disciplines are correctness proofs \nor run time estimations using the O-notation. The programs, which are understood as formal descriptions \nof a number of actions that take place in a certain ordering, are the focus of these approaches. Here, \na program itself is the subject which is be\u00ading studied. Mathematics is the underlying discipline which \n1 The term software science was already used in [16] for a different purpose -to describe a system of \nmetrics. The author of this essay uses this term for a different purpose because he thinks that it describes \nbest the here addressed topic and also meets best the common understanding of the topic. The author considers \nthe risk of misinterpreting the term to be rather low. provides the research method. The aim is to construct \ntheo\u00adrems and to prove them. This approach considers programs as deterministic methods that transform \ninput data into out\u00adput data. Programs are the subjects of research. In the fol\u00adlowing, this approach \nwill be describes as the classical ap\u00adproach. Over time, new approaches were developed that differ from \nthe classical approach. First, the assumption of deter\u00adminism was softened: disciplines such as parallel \ncomputing do no longer assume that the ordering of statements during the execution of a program is known. \nFurther disciplines concentrate on randomized algorithms where the result is permitted to depend on random \ndistributions. Nevertheless, they still have in common with the classical approach that the subject being \nstudied is the program itself. However, the research methods applied here differ from the classical ap\u00adproach. \nFirst, there is the stochastic-mathematical approach where stochastic statements are achieved by mathematical \nand analytical reasoning. Second, there is the stochastic\u00adexperimental approach where stochastic statements \nare achieved using statistical methods applied on measurements resulting from corresponding experiments. \nThe main differ\u00adence to the classical approach is that statements are no longer of kind true and false. \nInstead, the statements are stochastic statements based on probabilities. The stochastic-mathematical \napproach as well as the stochastic-experimental approach depend on random dis\u00adtributions of certain variables \ncontained in the programs to be analyzed. One characteristic of these randomized vari\u00adables is that they \nare under the control of the researcher: researchers can control the input parameters, the underlying \ndistributions, etc.2. There are further approaches that differ from the previous ones. For examples, \napproaches for improving the perfor\u00admance of software often have a characteristic that does not match \nthe previous descriptions. There, different strategies or algorithms are examined in order to improve \nthe perfor\u00admance of applications. The characteristic of the approach is, that software plays two different \nroles: .rst, there is a piece of software that is examined (such as a new run-time sys\u00adtem), second, \nthere are further pieces of software that are used as input parameters (at least, the approach permits \nto use software also as input parameters). A noteworthy difference to the previous approaches is the \nsecond role of software: software plays the role as in\u00adput parameters (instead of raw data such as integers, \netc.). Hence, this approach considers software as existing (real\u00ad 2 It is important to note that the \nstochastic-experimental approach does not describe all kinds of approaches that perform experiments. \nThe approach describes those approaches where the subject of research and all other in.u\u00adencing variables \ncan be formally described (and controlled). Hence, works that perform experiments on concrete machines \n(such as the measurement of time of a concrete algorithm on concrete CPUs) typically do not fall into \nthis category, because they (typically) cannot control all in.uencing vari\u00adables.  world) phenomena \nwhich are used to study the original sub\u00adject (which is in this case the new run-time system). The soft\u00adware \nbeing used as input parameter is (typically) intended to give a representative sample from the reality. \nIn order to gain such a sample (and to compare different research results), a typical approach is to \nuse benchmarks, i.e. sets of upfront known pieces of software. We call this approach, where a prede.ned \nset of data is being used as input parameters for experiments, the benchmark-based approach. All previous \napproaches have a purely technical nature software is being examined either in an analytical way (classical \napproach, stochastic-mathematical approach) or in an experimental way (stochastic-experimental approach, \nbenchmark-based approach). However, the software devel\u00adoper or the user of a piece of software does not \nplay any role in these approaches. Consequently, we call of these ap\u00adproaches technical approaches in \nthe following. Figure 1. Categorization of research approaches Apart from the technical approaches, \nfurther research di\u00adrections have been followed that fundamentally differ from the technical ones -works \nthat provide or invent new pro\u00adgramming language constructs or new tools, such as the invention of object-oriented \nprogramming. The main argu\u00admentation in such directions is that a new construct or ab\u00adstraction permits \ndevelopers to write better software. Bet\u00adter typically means in this context that the piece of soft\u00adware \nto be written using the new abstraction or tool has fewer errors, is better maintainable or more reusable \n(cor\u00adresponding qualitative criteria can be found in many text books such as [34]). The fundamental change \nto the previ\u00adous technical approaches is that the subject being examined has changed. While in all previously \ndescribed approaches a concrete piece of software (algorithm, run-time machine, etc.) was analyzed, this \nnew approach studies the way devel\u00adopers construct pieces of software using a new artifact. Con\u00adsequently, \nthe developer becomes part of the argumentation for or against new techniques a developer, a human being, \nis in addition to a new artifact in the focus of research. In the following we call this approach the \nsocio-technical ap\u00adproach (see Figure 1). Before considering the socio-technical approach we will consider \nthe technical approaches from two perspectives. First, we consider to what extent the approaches are \nable to provide valid rationales. Next, we discuss to what extent the technical approaches are adequate \nto provide arguments in software science. Here, the main perspective is to ask to what extend the technical \napproaches provide adequate arguments for the decision whether the application of a new artifact is bene.cial. \n 2.2 Validity of technical approaches It is obviously not necessary to discuss the classical ap\u00adproach \nwith respect to its validity: the subject of research can be formally described and the theorems can \nbe proven. The same is true for the stochastic-mathematical approach, although only stochastic statements \ncan be proven. The stochastic-experimental approach already widely dif\u00adfers from the previous ones: the \nresearch method is no longer based on formal reasoning. Instead, the results of experi\u00adments are used \nas rationales for or against a certain technique (or piece of software). Although there are obvious parallels \nto empirical methods from other disciplines apart from com\u00adputer science (such as medicine, experimental \nphysics, etc.), it must be emphasized that there are also huge differences to them: the subject (the \nalgorithm, etc.) as well as the result that is being examined (run-time bene.t, exactness of result) \ncan be formally described. As a consequence, all in.uencing variables that play a role in experiments \ncan be (typically) formally described and are completely under the control of the researcher: the researcher \ncan de.ne upfront the distri\u00adbution of input parameters, the random number generators being used, etc.. \nConsequently, there are no unknown factors that potentially in.uence the results of the experimentation. \nAs a consequence, a repetition of an experiment using the stochastic-experimental approach leads to the \nsame results. Nevertheless, it seems clear that this way of reasoning on software leads to valid results, \nespecially in situations where the subject cannot be analyzed using analytical methods. Nevertheless, \nthe approach also has the characteristic that the experimenter decides the chosen distributions for input \nvariables and it is at least speculative how the results potentially differ if different distributions \nwould have been chosen. For the same reason, it is potentially problematic to compare different pieces \nof research based on the stochastic\u00adexperimental approach, since the input parameters can be individually \nchosen by researchers. The benchmark-based approach is quite similar to the stochastic-experimental approach \nwith respect to its ex\u00adperimental character. Both perform experiments and apply statistical methods. \nHowever, in contrast to the stochastic\u00adexperimental approach, the experimenter cannot in.uence the data \nused within the experiment the benchmark is typ\u00adically an external factor. Consequently, the in.uence \nof an experimenter on the results is much more reduced in com\u00adparison to the stochastic-experimental \napproach, which im\u00adproves the ability to compare different research works (since the experiments are \nbased on the same input data)3. How\u00adever, in order to gain this bene.t it is necessary that that there \nis a commonly accepted de.nition for such a bench\u00admark. This situation is typically only given if the \ntechniques under examination are already applied since a number of years. Furthermore, it requires some \nconsensus in the (sci\u00adenti.c or industrial) community about such benchmarks. The benchmark-based approach \nstill has some subjective el\u00adement: a benchmark is intended to be some representative sample over the \nset of all possible data which is constructed by human. For example, in [4] a benchmark suite is pro\u00adposed \nwhich is a set of general purpose, realistic, freely available Java applications [4] which can be used \nto mea\u00adsure for example the performance of Java Virtual Machines. Although the authors of the benchmark \nsuite argue for the quality of the suite it is at least questionable whether these applications are representative. \nNevertheless, this is not the subjectivity of the researcher applying the benchmark -this is the subjectivity \nwhich is part of the benchmark itself.  The bene.t of comparing different research works based on the \nbenchmark-based approach lies in the application of new techniques to the same benchmark. Consequently, \na benchmark is hardly able to evolve, because otherwise this bene.t would no longer exist. But if the \nbenchmark does not evolve, it cannot consider the continuous change in soft\u00adware development: new programming \ntechniques, develop\u00adment environments, architectures, etc. frequently appear and have a direct impact \non the resulting software (with respect to size, complexity, etc.) but these changes are not part of \nthe benchmark. Because of the above described poten\u00adtial problems, benchmark composition is always hotly \nde\u00adbated [36, p. 36]. Although these problems are known, it still seems obvi\u00adous to consider research \nstatements or theories whose ra\u00adtionales are based on the benchmark-based approach to be valid, because \nit seems obvious that it is not possible to de\u00ad.ne benchmarks that evolve over time and that still permit \nto compare different pieces of work based on common data. The problem with the subjectivity of the benchmark \nremains, but since we have to accept that it is not possible to gather all current and possible future \npieces of software in one single benchmark, we have to live with the remaining subjectivity.  2.3 Adequacy \nof technical approaches It is important to consider the validity of research results based on the underlying \nresearch method. It is maybe even more important to consider whether the research methods are adequate \nto reason on statements about the techniques. A general view on software science is that it provides \n(new) tools and techniques to build, maintain and exe\u00adcute software. The terms tools and techniques should \nbe 3 Of course, researchers still have the freedom to decide which benchmark they use -in case there \nare different alternatives available. But once a benchmark is chosen, the in.uence is reduced. considered \nto be rather abstract. Examples for such tools are concrete software tools (such as development environ\u00adments, \nsoftware libraries, programming languages), as well as methods (such as development processes, or test \ntech\u00adniques) up to models (such as formal languages, modeling notations, etc.). It is important to note \nthat the construction of a new artifact itself does not represent a scienti.c activity. The scienti.c \nactivity is the evaluation of statements about the technique, where the bene.t of a certain technique \nis shown (or disproved). Other scienti.c activities are the con\u00adstruction and evaluation of theories \nthat permit to predict certain phenomena that appear while a piece of software is constructed, maintained \nor run. In the technical approaches the bene.t of an artifact can be argued based on rationales on an \narti.cial artifact. For example, the bene.t of a JIT compiler in comparison to an interpreter can be \nargued by comparing the run-time using benchmarks (benchmark-based approach). The bene.t of a certain \ntype system that requires some additional type annotations can be argued by proving its type soundness \n(classical approach). Although the technical arguments seem to be quite strong, they still have weaknesses. \nThe argumentation for the JIT compiler is problematic, because it is unclear whether the underlying benchmark \nrepresents a representative sample but we already argued above that this argument is rather weak. However, \nthere are more serious objections against the argumentation for the second example (type system). Although \ntype soundness has been proven, it is not shown whether a developer is able to use the type system. It \nmight be possible that the type system is too complicated that de\u00advelopers are overstrained when applying \nit. Consequently, the positive statement based on the technical approach would turn out to be rather \nmisleading if used by developers in or\u00adder to determine whether the type system should be applied: a \ntype soundness proof does not say a word about whether the type system improves the construction of software. \nIn this situation, the classical approach turns out to be inad\u00adequate for developers to decide whether \nthe application of the new artifact is bene.cial. For the JIT compiler the situa\u00adtion is different. The \napplication of the JIT does not require additional actions by the developer. Consequently, a pure technical \nstatement is adequate here. The examples show that the technical approach is ade\u00adquate in some situations \n and in some situations it is in\u00adadequate. Although technical statements can be potentially proven, they \nprove a formal characteristic within a formal system. This does not permit one to reason about a possible \nbene.t of the artifact that requires special user interactions because the possible behavior of users \nis outside the formal system. If we assume that most techniques provided by software science require \nadditional user interaction, it can be con\u00adcluded that most of the time the application of a technical \napproach is rather inappropriate.  2.4 Further approaches It should be noted that even other kinds \nof approaches are applied a close look into international journals and confer\u00adences reveals that technical \napproaches do not represent the majority of research approaches4. Frequently, a common approach is to \nidentify a problem by means of an example, to provide a new artifact and to show that the problem is \nsolved by applying the artifact. An example for such an approach can be found in [38]5. There, it is \nargued that class-based object-oriented languages tend to be too complex, since they provide constructs \nsuch as classes, etc.. Then, a new language (the programming language Self ) is introduced. Then, the \nbene.t of Self is being argued by the absence of certain language constructs such as classes, etc.. From \nthe scienti.c point of view the argumentation is problematic. First, it is unclear whether the addressed \nprob\u00adlem is really a problem. However, this (weak) argument is directed to the relevance of the work \n-which can be argued against any kind of research work. However, a really prob\u00adlematic question is, what \nexactly the research question is in the paper. If it was whether a programming language can be provided \nwithout the language construct class, then the answer already could have been given upfront (with a ref\u00aderence \nto procedural or functional programming languages). If the intention was to provide a language that is \neasier to use than a class-based language, then the paper failed to pro\u00advide any rationales showing that \nthe resulting language is easier. Hence, from the scienti.c point of view, it must be concluded that \nthe paper does not give a scienti.c argument for the new language. A different kind of approach that \nalso can be frequently found is the transfer of artifacts from one discipline to an\u00adother one. An example \ncan be found in [18]. Here, the au\u00adthors address the topic of how to document software frame\u00adworks. They \nprovide a hint, that pattern languages have been used in architecture (not software architecture). Then, \nthey transfer this idea to framework documentation. Then, the au\u00adthors mention that a group of developers \nwas satis.ed with the pattern-based documentation after some iterations. Fi\u00adnally, they conclude that \npattern languages are a good way to document software frameworks. Again, the arguments are problematic. \nThe transfer of the artifact (pattern language) from one discipline (architecture) 4 See e.g. [37, 42] \nfor overviews of research methods and [3, 31] for overviews of experiments found in past conferences \nand journals. 5 Once again, the author would like to emphasize that the intention is not to discredit \nany authors or any techniques (in fact, the author is an enthusiastic Self-programmer and an admirer \nof the works by David Ungar and Randy Smith). This essay also does not make any statement about the possible \nbene.t of Self. The intention is only to argue that the approach in the Self\u00adpaper does not permit to \ndraw the conclusions drawn in the paper. to another one (software construction) is achieved more or less \narbitrarily. Moreover, the paper just states that a group of developers was satis.ed after some iterations. \nIt does not state whether the developers were unsatis.ed with the existing solution or whether they were \nmore satis.ed with the new solution. Finally, no scienti.c argument (based on a valid research approach) \nis given in order to conclude that patterns language are suited for documenting frameworks6. A characteristic \nof the examples is, that their argumenta\u00adtion does not follow any scienti.c approach. Consequently, the \nargumentation is not valid and the argued bene.t of the proposed artifacts is purely speculative. For \nreaders of such works it is quite complicated how to handle such situations. Either they ignore the works \nbecause of the missing scienti.c approach, or they decide for them\u00adselves whether or not they consider \nthe proposed artifacts to be bene.cial. In the latter case the bene.t of the artifacts lies only in the \neye of the beholder. Expressed in a more provocative way this means that it is up to the developer s \nfaith to decide whether or not he be\u00adlieves in the proposed artifact; in case he decides to use the artifact \nin an industrial setting, he has to hope that the artifact will not have a bad impact on the software \nconstruction pro\u00adcess. Finally, the developer has to decide on his own whether he loves the new artifact \n since no objective rationales are given, the choice of a new artifact is a purely subjective and rather \nemotional process. Faith, hope, and love turn out to be the dominant factors for selecting and applying \ntechnical artifacts provided by software science.  2.5 Speculative considerations of human factors in \nsoftware science While the previously discussed approaches are often used in the scienti.c literature, \nthe socio-technical approach is (still) controversially discussed. There is no answer to the question \nwhether or not human factors should play any role in the sci\u00adenti.c argumentation in software construction \nthat is com\u00admonly accepted among all researchers. For example, Tichy reports in [36] about the fear that \ncomputer science will fall into the trap of soft science -where human subjects are typically considered \nto be the characteristic of soft science. Hence, it is even unclear whether a socio-technical approach \nreally exists or whether this is rather one branch of popular and unscienti.c work. However, a closer \nlook into a number of commonly ac\u00adcepted research works reveals that human factors already play an essential \nrole in software science although they are typically not considered within a valid research approach. \nIn order to exemplify this, two examples should be mentioned here. 6 Again, the author (who likes software \npatterns and design patterns) would like to mention that he refers here only to the paper and discusses \nwhether the argumentation within the paper follows a valid scienti.c approach. In fact, the area of software \ndocumentation using design patterns is already studied in other works (see for example [26]).  Dijkstra \nwrote in [9, p. 210] that it is a characteristic of intelligent thinking to study in depth an aspect \nof one s subject matter in isolation , a principle that he calls separation of concerns . A large bunch \nof literature refers to the phrase separation of concerns in order to argue for the bene.t of a certain \ntechnique (typically, the aspect-oriented literature, see [13], refers to this phrase in order to argue \nfor aspect-oriented software composition).  Another example can be found in [21, p. 8]: there, in or\u00adder \nto argue for the bene.t of object-oriented program\u00adming, it is stated that object-orientation is close \nto the natural perception of the real world: viewed as consist\u00ading of objects with properties and actions \n.  For both (very prominent) arguments the human factors are essential. In both cases the reference \nto the human factors is the key argument the .rst argument is the foundation for the aspect-oriented \nliterature7, the second argument intends to highlight the need for the new approach object-oriented programming. \nBoth arguments refer to human characteris\u00adtics human thinking and human perception . But from the scienti.c \npoint of view the above examples must be con\u00adsidered to be problematic: Both works apply a characteristic \nfrom one discipline (psychology) more or less arbitrarily to a different one (software science).  In \nboth cases, the statements come out of nowhere . Neither of the statements has any references to any \nneu\u00adronal science or psychology journal or something similar. Consequently, in the best case both sentences \ncan only be considered as a possible hypothesis that needs to be tested . In the worst case, one could \nsay that both state\u00adments are just the result of the author s fantasy. In any case, it is not valid to \nconclude anything from them8.  Even under the assumption that the statements are right, it is unclear \nhow the causal relationship between each statement and the argued technique (aspect-orientation, object-orientation) \nis. Even assuming the validity of a statement people perceive the world as objects it needs to be checked \nwhether this has anything to do with soft\u00adware construction and whether this has anything to do with \nour understanding of object-oriented programming.  So far, we can conclude that the human factors are \nalready considered even in programming language research. How\u00adever, the way how they are used does not \nfollow any valid scienti.c approach. Consequently, it is necessary to under\u00ad 7 It should be noted that \nthe strong emphasis on the phrase separation of concerns was probably not intended by Dijkstra: the phrase \nhardly plays any role in [9]. 8 Both statements are different from those ones that are traditionally \nana\u00adlyzed by computer scientists. Consequently, it is highly probable that com\u00adputer scientists are not \nable to determine whether these statements have any scienti.c background -or whether those statements \nrepresent some com\u00admon sense in these disciplines. stand how the human factors can be integrated into \na valid socio-technical approach for software science. Therefore, it is reasonable to have a look into \nsciences that do not rely on formal methods, i.e. sciences that do not purely rely on the technical approaches. \n3. Beyond Formal Systems: Consideration of Human Factors in other Disciplines Disciplines that do not \nrely on formal systems typically des\u00adignate the critical rationalism as the foundation of their re\u00adsearch \nmethod. The core of the critical rationalism according to Karl Popper [24] is that all scienti.c statements \nmust by falsi.able, i.e. it must be possible to test the statement in order to determine whether it is \nfalse. Furthermore, a sci\u00adenti.c statement must be universally quanti.ed: an existen\u00adtially quanti.ed \nstatement is not considered to be scienti.c. The tasks of a scientist are twofold. First, his task is \nto construct sets of (hopefully consistent) scienti.c statements which are called theories. Second, he \nneeds to try to falsify them using empirical methods. It is important to note that the critical rationalism \ndoes not assume that the correctness of a theory can be proven. Instead, it can only be shown whether \na theory is false. The more often falsi.cation trials of a theory fail, the more stable is a theory considered. \nThe most extreme interpretation of the critical rationalism demands only one falsi.cation of a theory \nin order to reject it. In order to explain the critical rationalism, Popper uses a number of examples, \nmainly from physics. However, if human behavior is observed using empirical methods the idea of falsi.cation \nslightly differs. For physical objects a number of objective metrics such as mass exist and the physical \nobject is not able to change this metric be\u00adcause of the absences of a free will . Humans on the other \nhand can re.ect on themselves and change in that way their behavior. Hence, humans are even able to behave \nin a way they usually would do not . As a consequence, a singular observation in disciplines that depend \non human factors can\u00adnot be used to falsify a theory (or a single statement, see for example [6], p. \n11). Instead, it is necessary to make observa\u00adtions on multiple subjects. This implies that a corresponding \nstatistical analysis is necessary on those subjects. And a con\u00adsequence of this is that the result of \nobservations can only be expressed using probabilistic methods. This is probably the main reason why \ncomputer scientists, whose educational background is mainly in.uenced by mathematics, are rather reluctant \nto consider this as a hard science: they use rather the devalued term soft science. Disciplines that \nuse humans as subjects are for example medicine, psychology, pedagogic, or social sciences. How\u00adever, \neach discipline typically has a different view on hu\u00admans with a different focus. All of these disciplines \nhave in common that they require statistical methods in order to in\u00adterpret their measurements.  In \nthe German computer science literature, medicine (es\u00adpecially drug research) is sometimes named as a \ndiscipline whose research method should be applied in software sci\u00adences (see for example [32]). The \nparallel to software sci\u00adence seems obvious neither a human nor a certain medicine or therapy is in \nthe main focus of research. Instead, it is the in.uence of a certain artifact (medicine or therapy) on \na set of individuals. This seems to be very similar to software sci\u00adence where a certain artifact has \nsome effect on a developer (or a development team) and the resulting software. How\u00adever, a closer look \nreveals that this parallel does not match. In drug research, some objective (at least well studied) met\u00adrics \nsuch as blood pressure are being used to evaluate the ef\u00adfect of a medicine; metric whose causal effect \non subjects is known (such as the effect of high blood pressure on heart dis\u00adeases, or the probability \nof suffering from an apoplexy)9. For such metrics, differences between subjects caused by differ\u00adent \neducational or cultural background typically do not play any role because it is assumed that the subjects \ncannot at\u00adtentively in.uence the results of the experiments. An excep\u00adtion to this are experiments where \nthe effects of placebos are also measured or where no objective metrics are available. Here, the experimental \ndesign explicitly assumes a potential in.uence of the subjects on the results and which also in.u\u00adences \nthe analysis of the measurements. Applying this approach to software science would imply that the effect \nof an artifact on a developer would hardly be in.uenced by the developer s background. For obvious reasons, \nthis cannot be considered to be serious. It needs to be emphasized that software artifacts require an \nintellectual action of the developer: instead of considering the developer as some kind of physical machine \nhe is considered as an intellectual individual who performs some creative actions while developing software. \nAlthough the developer (who corresponds to the patient in medical research) used a new technique (which \ncorresponds to a medicine), the effect of this technique cannot be predominantly measured on the subject \nitself (which would correspond to the measurement of blood pressure): the effect can predominantly measured \non the arti.cial product (the software) resulting from the creative actions. Here, it seems more likely \nto consider other empirical dis\u00adciplines that also have a focus on human factors. Psychology seems closely \nrelated to software science. In psychology, hu\u00adman beings are in the focus of or at least part of the \nsubject being examined. Different facets such as cognition, percep\u00adtion, learning, memory, thinking, \nproblem solving, knowl\u00adedge, etc. are being studied facets which are for soft\u00adware science of obvious \nimportance, too. Furthermore, psy\u00adchology has a long experience in empirical methods which means that \nthere is already a knowledge-base for experimen\u00ad 9 It should be clear that such objective metrics from \nmedicine have hardly something in common with metrics such as lines of code where causal ef\u00adfects of \nthis metric (other than the length of a program) are rather unknown. tal design, experiment execution \nand analysis techniques. Furthermore, there are standards (cf. e.g. [40]) that need to be considered \nin order to gain valid scienti.c results and in order to get such works published. From that perspective, \npsychology seems to be an adequate discipline that could be used as an example for software science. \n4. The Socio-Technical Approach: Empirical Software Engineering Today The demand for applying empirical \nmethods with special fo\u00adcus on human factors is far from being new. Especially, in the 70 s and 80 s \nthere were a number of works that em\u00adphasized the need for empirical methods (taken from psy\u00adchology) \nin software science (see for example [27, 28, 39]). Probably the most drastic criticism of current practice \nin software science can be found in [27], which describes the study of programming as an unholy mixture \nof mathematics, literature criticism and folklore . Also, there are a number of German authors that argue \nfor the need of corresponding em\u00adpirical methods from (see for example [25, 32, 33, 35, 36])10. It should \nbe mentioned that the terms empirical meth\u00adods or empirical software engineering in the area of software \nscience typically describes the socio-technical ap\u00adproach, i.e. the explicit consideration of the human \nfactors within an empirical approach11. Meanwhile, there are a number of teaching books about empirical \nmethods available for software science (see for example [19, 25, 41]). An interesting characteristic \nis, that the content of these books is quite heterogeneous: for exam\u00adple sometimes the necessary statistical \nmethods are the main content of the books (see for example [19]) or fundamental considerations for the \nexperimental design and experiment execution are the main content (see for example [25, 41]), sometimes \nin combination with descriptions of experiments done so far (see for example [25]). What is slightly \nirritating about the literature on empiri\u00adcal software science is the fact that the teaching books de\u00adscribe \nconcrete results of performed experiments, but they hardly provide any special knowledge for software \nscience. This means, concrete, domain-speci.c knowledge required to perform the socio-technical approach \nin software science is hardly provided. The following example should explain this in more detail. For \nexample Prechelt gives in [25] the hint, that subjects par\u00adticipating in an experiment, should have homogeneous \ncapa\u00adbilities (see [25], p. 112). In fact, many empirical studies ex\u00ad 10 It also needs to be emphasized \nthat there is a number of authors that ar\u00adgue against the use of empirical methods and the consideration \nof human factors in computer science. For example [15] states that when comparing approaches that have \nan empirical character and those ones that are primar\u00adily speculative, none should be considered to me \nmore worthy. 11 Of course, this use of the term empirical software engineering is quite misleading, because \nit ignores that even technical approaches such as the stochastic-experimental as well as the benchmark-based \napproach are em\u00adpirical approaches.  hibit the need to distinguish between professional software developers \nand beginners and to analyze both kinds of devel\u00adopers separate from each other. However, if someone \ntries to perform an experiment, one question remains: How to do that? It turns out that a number of different \nstudies try to ad\u00address this question in many different ways. Diverse kinds of questionnaires were applied \nin the past, different kinds of pretests for developers, etc.. This means that every experi\u00admenter has \nhis own view on how to classify subjects . But it is important to note that the experience on developers \nis domain-speci.c knowledge for software science. While information about different kinds of experimental \ndesigns, statistical analysis, etc. can be directly gathered from funda\u00admental teaching books about psychology \nor social sciences, information about how to classify subjects cannot. As a consequence, teaching books \nabout empirical soft\u00adware science are typically summaries of knowledge gathered from other disciplines \nabout experimental design and analy\u00adsis but they hardly provide special, domain-speci.c knowl\u00adedge required \nin the area of software science in order to per\u00adform experiments. Literature that provides domain-speci.c \nknowledge (such as concrete laws for software science which can be found in [11] or concrete lessons \nlearned from a long lists of experiments which can be found for example in [3]) is rather the exception. \nIf we take a look into concrete experiments that were performed in the past, we see that a typical approach \nof these experiments is to compare two techniques. For example, it is measured how long a group of subjects \nrequires to solve a given programming problem using technique A or B, or it is measured how many failures \nwere performed by the subjects using technique A or B (an example for such an approach can be found in \n[1]). A characteristic of these studies is, that .nally some insights are collected because it was possible \nto measure a difference between technique A and B but it is hard to get any more insights from these \nstudies that could be used in different experimental settings. For example, it is unclear whether the \nsame or similar results would have been measured under slightly different experimental settings. The \nreason for this problem is, that most of the studies compare technique A and B in a concrete setting, \nbut this comparison is not meant to back up any underlying theory such as technique A requires 10% more \ntime to solve a problem or for problems that require only 200 lines of code technique A requires less \ntime, if more than 200 lines of code are required to solve the problem technique B requires less time \n12. In fact, there is no underlying theory (set of scienti.c statements) but typically only one single \nstatement be tested. Hence, these theories have hardly any means to predict any possible situation. 12 \nOf course, a theory that states that a technique A requires less time for all problems than a technique \nB is some kind of prediction. But for obvious reasons, such theories cannot be often found. Coming back \nto the original idea of the critical rational\u00adism, this means that the current situation in empirical \nsoft\u00adware science (using the socio-technical approach) is rather frustrating: the original idea of constructing \nand testing the\u00adories is hardly followed (such theories are not even con\u00adstructed). Instead, only singular \nstatements are tested. It should be mentioned that there are prediction models in software science, i.e. \nmodels that represent some kind of theory in order to predict future situations, like for example the \none proposed in [5]) for estimating the effort for soft\u00adware projects13. From the perspective of programming \nlan\u00adguage research, these models are far away from the topic of programming languages, because the in.uencing \nfactor pro\u00adgramming language is hardly considered in these models. Hence, from the programming language \nresearch perspec\u00adtive these prediction models rather do not represent domain knowledge that could be \nused to design and perform experi\u00adments. However, beyond the area of programming languages, modeling \nnotations, etc. there are disciplines that could be considered as part of software science and which \nintensively make use of the socio-technical approach and which al\u00adready use theories with corresponding \nempirical evidence: the area of Human-Computer-Interaction (HCI, see for ex\u00adample [29]). The overlap \nwith the area of software science is (typically) in the area of design of graphical user inter\u00adfaces. \nAn interesting characteristic of HCI is, that psychol\u00adogy does not only provide the dominant research \nmethod (the socio-technical approach), but that psychology also pro\u00advides a number of theories that are \napplied there. Examples for such theories are models for the cognitive capabilities (see [17]) or the \nreaction time of users (see [14]). Both kinds of models represent domain-speci.c knowledge and theo\u00adries; \nfundamental knowledge that researchers in these areas need to know. Such corresponding theories and common \nknowledge (with empirical evidence) does not exist in the area of programming language. A .rst conclusion \nis that the socio-technical approach is hardly applied in software science, but that there are already \nstudies that consider the socio-technical approach. We have already discussed potential problems of the \napproach (which was the missing domain knowledge). But there are more reasons why the socio-technical \napproach is not frequently applied which need to be discussed. 5. Why is the Socio-Technical Approach \nHardly Applied? If the socio-technical approach is a valid approach to vali\u00addate statements about software \nartifacts, why isn t it simply applied and why should it be necessary to write this essay? First, we \nneed to accept that empirical works are hardly applied (see for example [36, 42]) and that the socio\u00ad \n13 However, some authors rather doubt about the empirical evidence of current prediction models (see \nfor example [12], p. 445).  technical approach is only a subset of all empirical studies14. Hence, only \na very small part of all empirical works (which are already just a small part of current scienti.c publications) \nfollows the socio-technical approach. One could argue that this small number is just a matter of time \n considering that software science is still a very young discipline, it is no wonder that no adequate \nresearch method has been established and applied so far. Although it is ob\u00adviously true that software \nscience is relatively young, this argument is still hard to follow, since the area of software science \nwas established in the 20th century and it should be assumed that researchers in the 20th century are \nalready fa\u00admiliar with different kinds or research methods and are able to distinguish between valid \nand adequate research methods, unscienti.c reasoning and pure speculations. However, there is also a \nnumber of different arguments why the socio-technical approach plays a minor role in soft\u00adware science. \n 5.1 Problems in education Foundations for empirical studies, which are also the foun\u00addation for the \nsocio-technical approach, are typically not taught in the area of software science. Furthermore, empir\u00adical \neducation requires knowledge from a number of differ\u00adent disciplines. First, knowledge from the area \nof stochastics is required which provides knowledge about different dis\u00adtributions and their characteristics. \nFurthermore, knowledge in the area of statistics is necessary which provides knowl\u00adedge about descriptive \nand inductive statistics, the differ\u00adences between both, and knowledge about signi.cance tests which \nare required in order to interpret measurements. Fur\u00adthermore, knowledge from the area of experimental \ndesign is required in order to understand how an experiment can be designed, what the pros and cons \nof a certain experimen\u00adtal design are, what implications a certain design has on the resulting analysis \ntechniques, etc.15. However, looking into the current curricula at universi\u00adties that teach different \nfacets of software science reveals that hardly any stochastics, statistics, and experimental de\u00adsign \nis being taught. As a consequence, students are simply not aware of what this empirical thing is. Even \nif students should (by luck) see some empirical works during their stud\u00adies, their knowledge is simply \nnot suf.cient to test, whether these works potentially suffer from errors in the experimen\u00adtal design \nor analysis. Consequently, students are not able to verify, whether the results of these empirical studies \nare to be trusted and whether the conclusions drawn from the studies by the corresponding authors are \nvalid and adequate. Hence, 14 In [7], Denning reports about a panel in 2004 that discussed the accom\u00adplishments \nof computer science [22] that hardly said a word about experi\u00admentation. 15 An interesting (or maybe \neven alarming) observation is, that such a knowledge is not only required in order to understand and \napply the socio\u00adtechnical approach: it is required in order to understand and apply empirical approaches \n(see .gure 1) in general. the original motivation for empirical studies becomes ab\u00adsurd. The motivation \nis to gather objective, empirical knowl\u00adedge by backing up or falsifying theories. If no one is able \nto understand these empirical works, no one is able to come to an objective conclusion about the subject \nof the study. Con\u00adsequently, students are doomed to perceive empirical works as strange collections of \nhuge, arbitrary data sets, without any obvious relationship to the topic software science. The problem \nis not only, that the research method is not explicitly taught. The method is typically also not implic\u00aditly \ntaught, too. In mathematics for example, the underly\u00ading research method (formal reasoning) is also typically \nnot explicitly taught. Nevertheless, the research method is prac\u00adtices in all courses. A course in mathematics \ndoes not only teach theorems, it also contains the proofs for such theorems. Students always come in \ntouch with the research method. They are implicitly educated in the research method with\u00adout explicit \ncourses about it. In psychology, the research method is typically explicitly taught. Furthermore, the \nre\u00adsearch method is part of most courses. There, students do not only learn theories. They learn the \nexperiments that back up these theories. They learn how the experiments were built up, what data has \nbeen measured, how the data has been an\u00adalyzed, and what conclusions were drawn from it. In software \nscience, education practice is completely dif\u00adferent. While in theoretical computer science the research \nmethod is also typically implicitly taught, in the area of soft\u00adware science research methods hardly \nplay any role. New (or old) techniques such as programming languages, software development processes, \nmodeling notations, etc. are taught without any scienti.c argumentation for or against them16. In the \nbest case, students learn examples where a certain technique does not seem to be adequate or possible \nsce\u00adnarios where a certain technique possibly dominates another one . However, how such examples or possible \nscenarios could be evaluated, i.e. how the objective reasoning about these examples works is typically \nnot part of the education. Due to the missing educational background, students are not able to distinguish \nbetween singular observations, specula\u00adtive reasoning on measurements and valid applications of empirical \nresearch. This has tragic consequences. Students typically do not learn how to investigate a certain \ntechnique they do no learn how to doubt about the bene.t of a certain technique. One of the main academic \nproperties the objective reason\u00ading on certain topics is hidden to the students. This also typically \nmeans that hardly any bachelor or master thesis is done using the socio-technical approach, because the \nnec\u00ad 16 A small test for the reader: Most of us believe in object-oriented program\u00adming. What experiment \nare you aware of that measured a positive impact of object-oriented programming over procedural programming? \nIn case you rather believe in e.g. function programming: what experiment are you aware of that measured \na positive impact of functional programming over proce\u00addural programming? Try to answer this question \nwithout using additional literature.  essary background is unknown to students and the time for such \ntheses is not suf.cient to train students in theses topics. In case a student is still willing to apply \nthe socio-technical approach, the problems of designing and performing an ex\u00adperiment (which will be \ndiscussed in sections 5.2 and 5.3) become relevant -problems that make it even more improb\u00adable that \nstudents get a chance to apply the socio-technical approach. If any kind of socio-technical approach \nwas hidden to the students, what should be their motivation to search for them (and to apply them) when \nthey are scientists?  5.2 Problems in designing experiments As already mentioned in section 4, a fundamental \nproblem of the socio-technical approach is, that domain-speci.c knowl\u00adedge from the area of software \nscience is missing. As a con\u00adsequence, an experimenter is forced to make a large num\u00adber of assumptions \non his own. Following the argumentation from section 4, an experimenter decides on his own how to distinguish \ngood developers from bad ones . Consequently, the results highly depend on the experimenter s personal \nde\u00adcision about the distinction of good and bad developers \u00adwhich is a subjective decision. One could \nargue here in a ma\u00adlicious way: from the current perspective, the socio-technical approach provides, \nbecause of the missing domain knowl\u00adedge, means to misuse the original idea of gathering objec\u00adtive knowledge. \nInstead of objective knowledge this ap\u00adproach provides subjective number generators which are highly \nin.uenced by the researcher. Although this is not satisfying, there are from the current point of view \nno means to get rid of this problem: if no empirical domain-speci.c knowledge is available, there is \nno way to get it somehow by magic. The probability that current socio-technical studies rely on wrong \nassumptions is very high. The only way to reduce this problem (and to get still meaningful data that \ncan be potentially used and interpreted in the future) is to document experiments as detailed as possible. \nAdditionally, there are already some (preliminary) guidelines available that state how to perform empirical \nresearch in the area of software science (see for example [20]). For the researcher applying the socio-technical \napproach the missing domain-knowledge has the tragic consequence that he is aware that possibly a number \nof experiments he performs might turn out to be in vain (maybe already in the near future), because they \nstart from wrong assumptions -a problem that researchers applying for example the classical approach \ntypically do not suffer from. Although this problem is real, it should be emphasized that this fear is \nrather the result of the researchers math\u00adematical background: students and researchers which are mainly \nby the classical approach tend to believe that a re\u00adsearch statement, once it is proven, is eternally \nvalid. How\u00adever, it is rather na\u00efve to assume that research results (out\u00adside formal systems) are stable \nover decades or centuries. And the reason for today s problems (the absence of missing domain-knowledge \namong many others) is that the socio\u00adtechnical approach (and empirical methods in general) has hardly \nbeen applied in the last decades -and still plays only a minor role in the software science today.  \n5.3 Problems in performing experiments Even if someone ignores the problems described above, another \nproblem is: What chances do I have to apply the socio-technical approach? . Assuming that the socio\u00adtechnical \napproach typically relies on experiments with hu\u00admans, the question can be re.ned: What chances do I \nhave to perform an experiment with humans ? The main problem here is that universities and other insti\u00adtutions \nhardly provide a corresponding required infrastruc\u00adture. A researcher has to .nd subjects, who typically \nvol\u00aduntary participate in an experiment. Paying these subjects is typically not possible because such \na payment assumes some kind of funding and a grant for getting such a funding typ\u00adically requires a bunch \nof publications in conferences and journals with high reputation, which is typically quite prob\u00adlematic \n(which will be discussed in section 5.4). However, even if the number of publications would not play \na major role for getting a grant, the problem of the socio-technical approach is, that it competes with \nresearch proposals that rely on technical approaches and which do not require any funding for paying \nsubjects. For the organization providing a grant the question is, whether a grant should be given for \na proposal where the chances for publications are rather low (see section 5.4) and which costs a lot, \nor whether the grant should be given to a proposal that does not suffer from these problems. Even ignoring \nthe payment issue, the researcher has still a problem to .nd subjects. Since students hardly come in \ntouch with the socio-technical approach during their studies, a socio-technical experiment rather looks \nstrange from their point of view what should be their motivation to participate in experiments? In empirical \nsciences such as psychology this problem of .nding subjects was already identi.ed and addressed. It is \na typical scenario that the participation in experiments is a necessary prerequisite for psychology students \nto pass their studies the execution of experiments becomes in that way part of the daily business at \nuniversities17. In these domains, it is also quite normal that a bachelor thesis or a master thesis performs \nan experiment where other students are used as subjects. Such a situation in the area of software science \nis from the current point of view far from being realistic. 17 Although it should be mentioned that there \nare different practical prob\u00adlems that need to be addressed. While in psychology an experiment about \nfor example eye movement control in reading requires maybe just one hour time from each subject, experiments \nin software science, which require sub\u00adject to build software, typically require much more time.  5.4 \nProblems for the academical career The socio-technical approach plays a minor role in the lit\u00aderature \nabout software science. In conferences and journals with a good reputation it is hard to .nd works that \nap\u00adply the socio-technical approach. Although there are some means to publish socio-technical studies \nin conferences and journals that explicitly call for socio-technical studies (such as the International \nSymposium on Empirical Software En\u00adgineering and Measurement or the Journal on Empirical Software Engineering), \nthese conferences and journals play rather a minor role. However, scientists need to publish their works \n and they need to publish at conferences and journals with a good reputation. Consequently, doing research \nusing the socio-technical approach drastically reduces a young re\u00adsearcher s career opportunities. 6. \nDemands to Research and Teaching In order to overcome the current frustrating situation with software \nscience s carelessness with regard to human fac\u00adtors, there is a need for a number of fundamental changes \nin software science not only with respect to research but also with respect to teaching. Hence, this \nsection describes a number of demands directed to different audiences: to peo\u00adple that teach students, \nto students, to researchers, and to people participating in the publication process. 6.1 Demands to teaching \nIn teaching, research methods need to be taught. This implies an explicit as well as an implicit teaching \nof those meth\u00adods. Here, the socio-technical approach needs to be com\u00admunicated as a different approach \namong the possible re\u00adsearch approaches. Valid and adequate rationales must be made explicit in teaching, \ni.e. teaching must not only con\u00adsist of the introduction of new artifacts. Instead, for every artifact \ncorresponding valid and adequate rationales must be given. Consequently, this means that curricula in \nsoftware science require an orientation toward curricula of those sci\u00adences that have already identi.ed \na strong impact of hu\u00adman factors. Of course, this orientation should not mean that socio-technical approaches \nshould become the dominant ap\u00adproaches. Instead, they should at least be considered as of equals status \nor level. For those artifacts for which such rationales do not exist, at least possible studies must \nbe proposed and additional effort should be spent on performing such studies as soon as possible. Furthermore, \nin these situations it must be com\u00admunicated to students that currently objective knowledge is missing \nfor these artifacts in order to preserve them from the (potentially misleading) faith about artifacts \ntaught at uni\u00adversities. Finally, universities should provide some kind of infras\u00adtructure to students \nthat permits them to perform socio\u00adtechnical experiments using subjects. This could be done by pushing \na duty to students to participate for a number of hours (or days) in experiments. 6.2 Demands to students \nWhile the previous demands were mainly directed to teach\u00aders, there are also demands to students. Students \nneeds to be encouraged to doubt about artifacts taught at universities. Whenever new artifacts (or statements) \nare being taught, stu\u00addents should actively ask for valid and adequate rationales for such artifacts. \nMost importantly, students should ask for such rationales directly at the beginning of a course in or\u00adder \nto determine whether the course provides insights into a topic with suf.cient evidence: students should \nconsider ev\u00adidence as a desirable aim for their studies and should make explicit that missing evidence \nis a reason for disapproving certain courses and topics. This implies that students should try to establish \na new attitude with regard to their studies: in\u00adstead of trying to come in touch with new technologies \nover and over again, students should consider stable knowledge about fewer topics to be worth more than \nspeculations about many topics. Students should become aware that many artifacts that they are being \ntaught are not as well studied as they are probably being told. Instead of considering this situation \nto be frustrating, it should be considered as a great challenge that needs to be addressed -and that \na thesis could be a good mean to address such a challenge. This, of course, requires also a cooperative \nattitude of students, because if a fellow student requires subjects for experiments, they should try \nto provide help.  6.3 Demands to researchers The demands for researchers are manifold. As a .rst step, \nit is necessary to guarantee that all research works and publi\u00adcations at least contain a falsi.able \nstatement (which permits at least other researchers to test the proposed statement). For all scienti.c \npapers it must be made explicit what the cho\u00adsen research method is with a short discussion, why the \nre\u00adsearch method is valid and adequate in order to follow the corresponding research question. Hence, \nresearchers should actively work on discussions about chosen research methods and possible alternative \nresearch methods. It would probably also help if researchers would discuss why certain research methods \nare considered to be inadequate for a certain re\u00adsearch question. When referring to related work in scienti.c \npublications, researchers should make explicit which of these works pro\u00advide evident knowledge and which \nones do not. This does not mean that literature without suf.cient evidence should be ignored but such \nliterature should be used more carefully. At least it is very problematic if literature whose statements \nare not provided with suf.cient evidence becomes part of the argumentation for or against a certain artifact. \nFinally, the current trend of researchers to become mainly experts for a certain topic (object-oriented \nlanguages design, etc.) should be abandoned. Instead, the main focus of re\u00adsearchers should be to become \nan expert in a certain research method -the more mature the applied research methods are, the better \nare the scienti.c results.  6.4 Demands to editors, pc chairs, and reviewers One demand to editors, \npc chairs and reviewers of journals and conferences is that a consciousness must be established that \nresearch methods are the foundation for every kind of research, that a research method must be valid \nand adequate to evaluate a certain artifact and that empirical research (es\u00adpecially the socio-technical \napproach) is one possible way to evaluate statements about artifacts. The persons responsible for the \npublication process must become aware that techni\u00adcal approaches are rather inadequate to back up arguments \nthat focus on human factors. The persons responsible for the publication process should also explicitly \nask authors to make the research method ex\u00adplicit and to argue about the appropriateness of the chosen \nresearch method. Editors and pc chairs should make sure that reviewers are familiar with the research \nmethods of those papers they are responsible for. Reviewers should also make sure on their own that they \nare familiar with the research method of paper they are asked to review. In case they are not familiar \nwith the method, they should reject to review the paper. This also requires an awareness that there is \na dif\u00adference between a familiarity of the subject of research and a familiarity of a research method: \nwhile familiarity with the subject helps to identify problems in a related work section of a paper, this \ndoes not necessarily mean that it helps to understand (and evaluate) the value of a paper. Reviewers \nof socio-technical papers must be quite disci\u00adplined to distinguish between a plausible, probable, subjec\u00adtive, \nvalid, or invalid argumentation for and against a paper. Using the example from the previous sections, \nit is plausible that reviewers of a paper about a socio-technical program\u00adming experiment ask for a classi.cation \nof subjects within an experiment. However, reviewers must also be aware that currently no objective (or \nat least commonly accepted) dis\u00adtinction between good and bad programmers exists. Conse\u00adquently, reviewers \nmust be aware that in such a situation the missing classi.cation is not a scienti.c objection against \na paper -it is a speculative objection where the author (cur\u00adrently) has no objective chance to ful.ll \nthis requirement. Finally, editors and pc chairs should be aware that a lot of knowledge about fundamental \nquestions in software science is missing. Instead of only asking for papers whose focus is mainly in \nup-to-date topics in software science, they should ask for works that address more fundamental questions \nin order to reduce the problem of missing domain knowledge that we currently have -and that we will still \nhave in 20 years if this kind of research will not be promoted. 7. Summary and Conclusion Software science \nis a domain which frequently provides new artifacts which claim to solve current problems. This essay \naddresses the question what research methods are valid and adequate to argue for or against statements \nabout such artifacts and emphasizes the need to consider human factors by using appropriate scienti.c \nmethods. This essay started with a short introduction of research methods and identi.es, that although \na number of research works address human factors, typically no valid and ade\u00adquate research methods are \napplied a socio-technical ap\u00adproach, which considers technical artifacts as well as hu\u00admans which apply \nthem, is more or less missing in software science. Furthermore, the paper shortly discussed possible \nreasons why the socio-technical approach is hardly applied. In order to establish the application of \nsuch a research approach, this paper argued that software science should take sciences that address the \nhuman factors as an example psychology might be the right choice here. The essay argued that this does \nnot only imply how research should be performed, this also implies how teaching should be done in software \nscience -in order to establish an environment where the socio-technical approach can be practices and \ntaught. In general, this paper advocates the need for empirical methods in software science considering \nthe human factors. In fact, this idea is far from being new. This paper is just another one in a long \nhistory of papers that try to raise this issue (see for example [27, 28, 35, 36, 39]). It should be noted \nthat there is literature available where authors already argue about the future of empirical software \nscience (see for example [2, 30]). The author of this essay shares the idea that there is a need to discuss \nthe future of empirical software engineering in general but the author considers fundamental changes \nin teaching and research to be necessary as a .rst step. While this paper speaks about the socio-technical \nap\u00adproach, it should be clear that there is no single socio\u00adtechnical approach which considers human \nfactors. Instead there are a number of different research methods which can be considered as socio-technical \napproaches (see for exam\u00adple [10] for an introduction into different kinds of empirical methods which \nfocus on the human factor). It would be de\u00adsirable and necessary to have more literature that discusses \nthe validity and adequacy of these approaches to back up different kinds of research statements. In general \nsoftware science rather suffers from the problem that the topic of research methods rather plays a minor \nrole in literature, re\u00adsearch, and teaching. A situation which the author of this essay considers to \nbe tragic. The author would like to emphasize that this essay does not not try to claim that the socio-technical \napproach (as well as empirical approaches in general) should be the only re\u00adsearch approaches to be taught \nand practiced. Especially, the author does not try to argue that non-empirical approaches (such as the \nclassical approach with its focus on mathe\u00admatical reasoning) should be abandoned by the research community \nor ignored in teaching. This essay argues that researchers and students need to become aware that there \nare different research methods for different purposes. And researchers and students should at least become \nskeptical whenever a software artifact appears which claims to have a positive impact on software development \nand which pro\u00advides rationales where the underlying research method does take the existence of software \ndevelopers into account.  An implicit demand of this essay is to distinguish clearly between scienti.c \nand non-scienti.c works in the scienti.c literature. This does not imply that unscienti.c work is con\u00adsidered \nto be useless -but it is very dangerous if unscienti.c literature becomes part of an argumentation for \nor against certain artifacts. It is important to note that this paper mainly addresses the need to apply \nthe socio-technical approach in order to provide evidence for scienti.c statements that consider not \nonly a piece of software but also the developer or a user of a piece of software. This essay does not \nsay a word about how scienti.c statements appear -the question of how sci\u00adenti.c statements appear needs \nto be discussed completely separate from the question how scienti.c statements should be evaluated. This \nessay can be considered as a catalyst to start the dis\u00adcussion about a topic which is rarely addressed \nin software science although this topic plays a major role in other sci\u00adences: research methods. It emphasizes \nthe need to address one obvious fundamental problem in software science: the carelessness with regard \nto human factors. And it argues that without considering human factors in software science, the construction \nof software and the application of new software artifacts will still be based on the developer s subjectivity \nwhich relies mainly on three principles faith, hope and love. Acknowledgments The author would like \nto thank David Ungar, Randy Smith, Robert Hirschfeld, and Michael Haupt for discussions about the paper, \nand Walter Tichy, whose keynote at the German Software Engineering Conference largely in.uenced this \nes\u00adsay (and who made the author aware of the con.ict of the term software science). Mark Mahony shepherded \nthis pa\u00adper and helped to signi.cantly improve the paper. The au\u00adthor would like to thank Holger Wiese \nfrom the department of general psychology from the University of Jena for his patience and guidance into \nthe research methods of psychol\u00adogy. Finally, the author would like to thank Erik Ernst for giving very \ndetailed, critical feedback which substantially improved this essay. References [1] BARTSCH, M., AND \nHARRISON, R. An exploratory study of the effect of aspect-oriented programming on maintainability. Software \nQuality Control 16, 1 (2008), 23 44. [2] BASILI, V. R. The role of experimentation in software engi\u00adneering: \npast, current, and future. In ICSE 96: Proceedings of the 18th international conference on Software engineer\u00ading \n(Washington, DC, USA, 1996), IEEE Computer Society, pp. 442 449. [3] BASILI, V. R., SELBY, R. W., AND \nHUTCHENS, D. H. Ex\u00adperimentation in software engineering. IEEE Trans. Software Eng. 12, 7 (1986), 733 \n743. [4] BLACKBURN, S. M., GARNER, R., HOFFMANN, C., KHANG, A. M., MCKINLEY, K. S., BENTZUR, R., DI-WAN, \nA., FEINBERG, D., FRAMPTON, D., GUYER, S. Z., HIRZEL, M., HOSKING, A., JUMP, M., LEE, H., MOSS, J. E. \nB., MOSS, B., PHANSALKAR, A., STEFANOVI, D., VAN-DRUNEN, T., VON DINCKLAGE, D., AND WIEDERMANN, B. The \nDaCapo benchmarks: Java benchmarking develop\u00adment and analysis. In OOPSLA 06: Proceedings of the 21st \nannual ACM SIGPLAN conference on Object-oriented pro\u00adgramming systems, languages, and applications (New \nYork, NY, USA, 2006), ACM, pp. 169 190. [5] BOEHM, B., BOEHM, B., CLARK, B., HOROWITZ, E., WESTL, C., \nMADACHY, R., AND SELBY, R. Cost models for future software life cycle processes:. In Annals of Soft\u00adware \nEngineering (1995), pp. 57 94. [6] BORTZ, J., AND D\u00d6RING, N. Forschungsmethoden und Evaluation: f\u00fcr Human-und \nSozialwissenschaftler, 4. ed. Springer, Heidelberg, 2006. [7] DENNING, P. J. Is computer science science? \nCommun. ACM 48, 4 (2005), 27 31. [8] DIJKSTRA, E. W. The humble programmer. Commun. ACM 15, 10 (1972), \n859 866. [9] DIJKSTRA, E. W. A Discipline of Programming. Prentice Hall, Inc., October 1976. [10] EASTERBROOK, \nS. M., SINGER, J., STOREY, M., AND DAMIAN, D. Selecting empirical methods for software en\u00adgineering research. \nIn Guide to Advanced Empirical Soft\u00adware Engineering, F. Shull, J. Singer, and D. Sj\u00f8berg, Eds. Springer, \n2007. [11] ENDRES, A., AND ROMBACH, D. A Handbook of Software and Systems Engineering. Pearson Addison-Wesley, \n2003. [12] FENTON, N. E., AND PFLEEGER, S. L. Software Metrics: A Rigorous and Practical Approach. PWS \nPublishing Co., Boston, MA, USA, 1998. [13] FILMAN, R. E., ELRAD, T., CLARKE, S., AND AK\u00b8SIT, M., Eds. \nAspect-Oriented Software Development. Addison-Wesley, Boston, 2005. [14] FITTS, P. M. The information \ncapacity of the human motor system in controlling the amplitude of movement. Journal of Experimental \nPsychology 47, 6 (June 1954), 262 269. [15] G\u00c9NOVA, G. Is computer science truly scienti.c? Commun. ACM \n53, 7 (2010), 37 39.  [16] HALSTEAD, M. H. Elements of Software Science (Operating and programming systems \nseries). Elsevier Science Inc., New York, NY, USA, 1977. [17] HICK, W. E. On the rate of gain of information. \nThe Quarterly Journal of Experimental Psychology 4, 1 (1952), 11 26. [18] JOHNSON, R. E. Documenting \nframeworks using patterns. In OOPSLA 92: Conference proceedings on Object-oriented programming systems, \nlanguages, and applications (New York, NY, USA, 1992), ACM, pp. 63 76. [19] JURISTO, N., AND MORENO, \nA. M. Basics of Software Engineering Experimentation. Springer, 2001. [20] KITCHENHAM, B., AL-KHILIDAR, \nH., BABAR, M. A., BERRY, M., COX, K., KEUNG, J., KURNIAWATI, F., STA-PLES, M., ZHANG, H., AND ZHU, L. \nEvaluating guidelines for empirical software engineering studies. In ISESE 06: Pro\u00adceedings of the 2006 \nACM/IEEE international symposium on Empirical software engineering (New York, NY, USA, 2006), ACM, pp. \n38 47. [21] MADSEN, O. L., AND M\u00d8LLER-PEDERSEN, B. What object-oriented programming may be -and what \nit does not have to be. In Proceedings of European Conference on Object-Oriented Programming (ECOOP 88) \n(1988), Springer, pp. 1 20. [22] NATIONAL RESEARCH COUNCIL. Computer Science: Re\u00ad.ections on the Field, \nRe.ections from the Field. National Academy Press, 2004. [23] PFLEEGER, S. L. Soup or art? the role of \nevidential force in empirical software engineering. IEEE Software 22 (2005), 66 73. [24] POPPER, K. In \nLogik der Forschung (2007), H. Keuth, Ed., Akademie Verlag GmbH. [25] PRECHELT, L. Kontrollierte Experimente \nin der Soft\u00adwaretechnik. Springer, Berlin, March 2001. [26] PRECHELT, L., UNGER, B., AND TICHY, W. Two \ncontrolled experiments assessing the usefulness of design pattern doc\u00adumentation in program maintenance. \nIEEE Transactions on Software Engineering 28 (2002), 595 606. [27] SHEIL, B. A. The psychological study \nof programming. ACM Comput. Surv. 13, 1 (1981), 101 120. [28] SHNEIDERMAN, B. Software Psychology: Human \nFactors in Computer and Information Systems. Winthrop Publishers, August 1980. [29] SHNEIDERMAN, B., \nAND PLAISANT, C. Designing the User Interface: Strategies for Effective Human-Computer Interac\u00adtion, \n5. ed. Pearson Addison-Wesley, Upper Saddle River, NJ, 2009. [30] SJ\u00d8BERG, D. I. K., DYBA, T., AND J\u00d8RGENSEN, \nM. The future of empirical methods in software engineering research. In FOSE 07: 2007 Future of Software \nEngineering (Washing\u00adton, DC, USA, 2007), IEEE Computer Society, pp. 358 378. [31] SJ\u00d8BERG, D. I. K., \nHANNAY, J. E., HANSEN, O., BY KAM-PENES, V., KARAHASANOVI C\u00b4, A., LIBORG, N.-K., AND C. REKDAL, A. A \nsurvey of controlled experiments in soft\u00adware engineering. IEEE Trans. Softw. Eng. 31, 9 (2005), 733 \n753. [32] SNELTING, G. Paul Feyerabend und die Softwaretechnologie. Informatik-Spektrum 21, 5 (October \n1998), 273 276. [33] SNELTING, G. Feyerabend -zwei Jahre sp\u00e4ter. Softwaretechnik-Trends 21, 1 (February \n2001), 40 43. [34] SOMMERVILLE, I. Software Engineering, 9. ed. Addison-Wesley, Harlow, England, 2010. \n[35] TICHY, W. F. Die Bedeutung der Empirie f\u00fcr die Soft\u00adwaretechnik. Keynote at German Conference on \nSoftware En\u00adgineering, Essen, March, 8-11, 2005. [36] TICHY, W. F. Should computer scientists experiment \nmore? IEEE Computer 31 (1998), 32 40. [37] TICHY, W. F., LUKOWICZ, P., PRECHELT, L., AND HEINZ, E. A. \nExperimental evaluation in computer science: A quan\u00adtitative study. Journal of Systems and Software 28, \n1 (1995), 9 18. [38] UNGAR, D., AND SMITH, R. B. Self: The power of sim\u00adplicity. In OOPSLA 87: Conference \nproceedings on Object\u00adoriented Programming Systems, Languages, and Applications (December 1987), ACM, \npp. 227 242. [39] WEINBERG, G. M. The Psychology of Computer Program\u00adming. John Wiley &#38; Sons, Inc., \nNew York, NY, USA, 1985. [40] WILKINSON, L., AND THE TASK FORCE ON STATISTI-CAL INFERENCE. Statistical \nmethods in psychology jour\u00adnals: Guidelines and explanations. American Psychologist 54 (1999), 594 604. \n[41] WOHLIN, C., RUNESON, P., H\u00d6ST, M., OHLSSON, M. C., REGNELL, B., AND WESSL\u00c9N, A. Experimentation \nin soft\u00adware engineering: an introduction. Kluwer Academic Pub\u00adlishers, Norwell, MA, USA, 2000. [42] \nZELKOWITZ, M. V., AND WALLACE, D. R. Experimental models for validating technology. Computer 31 (1998), \n23 31.    \n\t\t\t", "proc_id": "1869459", "abstract": "<p>Research in the area of programming languages has different facets -- from formal reasoning about new programming language constructs (such as type soundness proofs for new type systems) over inventions of new abstractions, up to performance measurements of virtual machines. A closer look into the underlying research methods reveals a distressing characteristic of programming language research: developers, which are the main audience for new language constructs, are hardly considered in the research process. As a consequence, it is simply not possible to state whether a new construct that requires some kind of interaction with the developer has any positive impact on the construction of software. This paper argues for appropriate research methods in programming language research that rely on studies of developers -- and argues that the introduction of corresponding empirical methods not only requires a new understanding of research but also a different view on how to teach software science to students.</p>", "authors": [{"name": "Stefan Hanenberg", "author_profile_id": "81100540390", "affiliation": "University of Duisburg-Essen, Essen, Germany", "person_id": "P2354191", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1869459.1869536", "year": "2010", "article_id": "1869536", "conference": "OOPSLA", "title": "Faith, hope, and love: an essay on software science's neglect of human factors", "url": "http://dl.acm.org/citation.cfm?id=1869536"}