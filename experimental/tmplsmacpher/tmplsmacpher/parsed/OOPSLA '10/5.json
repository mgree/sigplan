{"article_publication_date": "10-17-2010", "fulltext": "\n An Experiment About Static and Dynamic Type Systems Doubts About the Positive Impact of Static Type \nSystems on Development Time Stefan Hanenberg Institute for Computer Science and Business Information \nSystems University of Duisburg-Essen Sch\u00fctzenbahn 70, D-45117 Essen, Germany stefan.hanenberg@icb.uni-due.de \nAbstract Although static type systems are an essential part in teach\u00ading and research in software engineering \nand computer science, there is hardly any knowledge about what the im\u00adpact of static type systems on \nthe development time or the resulting quality for a piece of software is. On the one hand there are authors \nthat state that static type systems decrease an application s complexity and hence its developmenttime \n(which means that the quality must be improved since developers have more time left in their projects). \nOn the other hand there are authors that argue that static type sys\u00adtems increase development time (and \nhence decrease the code quality) since they restrict developers to express themselves in a desired way. \nThis paper presents an em\u00adpirical study with 49 subjects that studies the impact of a static type system \nfor the development of a parser over 27 hours working time. In the experiments the existence of the static \ntype system has neither a positive nor a negative im\u00adpact on an application s development time (under \nthe con\u00additions of the experiment). Categories and Subject Descriptors D.3.3 [Program\u00adming Languages]: \nLanguage Constructs and Features General Terms Experimentation, Human Factors, Lan\u00adguages. Keywords Type \nSystems, Programming Languages, Em\u00adpirical Study, Dynamically Typed Languages 1. Introduction Static \ntype systems (see for example [1, 19]) are one of the major topics in research, teaching as well as in \nindustry. In research, new type systems appear frequently either for existing programming languages (such \nas for example the introduction of Generics in Java) or new programming languages are constructed that \nprovide a new static type Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for profit or \ncommercial advantage and that copies bear this notice and the full citation on the first page. To copy \notherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission \nand/or a fee. OOPSLA/SPLASH 10 October 17 21, 2010, Reno/Tahoe, Nevada, USA. Copyright &#38;#169; 2010 \nACM 978-1-4503-0203-6/10/10 $10.00. system. In teaching, students are educated in the formal notation \nof static type systems as well as in proofs on static type systems (see for example [1, 19]). In industry, \ntype systems become important for different reasons. Possibly, a programming language in use evolves \nby introducing a new static type system. If this new static type systemshould be applied, developers \nneed to be educated, which causes additional costs. Maybe existing libraries or products should be adapted \nto match the new type systemwhich also causes additional costs. Finally, additional tools might be required \ndue to the new type system (such as tools that measure the current state of the software product) which \npotentially also cause additional costs. An example for such a static type system that evolves ina programming \nlanguage with industrial relevance is Java:since the introduction of Generics in Java there is still \na huge number of applications and APIs available that do notuse Generics. Hence, for industry a main \nquestion is whether it is rewarding to educate developers to use the new static type system and whether \nthere will be a returnon investment for migrating existing non-generic APIs tothe new static type system. \nIn general, for industry it is important to determine whether such an investment is reasonable, i.e. \nwhether the expected benefit of static type systems represents somefuture revenues. This means, it is \nnecessary to understand what the advantages and maybe additional costs of using a static type system \nare. In industry, it is also observable that dynamically typed programming languages such as PHP or Ruby \nbecomemore and more important for specific domains such as website engineering. Also, dynamically typed \nprogramming languages such as Tcl or Perl are still used in software development. For future developments \nof such languages it seems valid to ask, whether new releases of such languages should provide a static \ntype system assuming that a static type system has a positive impact onsoftware development. However, \nwhile static type systems are well-studiedfrom the perspective of theoretical computer science, there \nis hardly any knowledge about whether a static type systemplays a relevant role in the practical application \nof aprogramming language. There is even a number of researchers that advocate the use of dynamically \ntyped programming languages instead of statically typed ones (see for example [26]) and who emphasize \nthe tradition of dynamically typed programming languages such as Smalltalk, Lisp, etc..  In literature, \ntypical arguments for static type systems (see e.g. [2]) are for example: Static type systems... capture \na large fraction of recurring programming errors  have methodological advantages for code development \n reduce the complexity of programming languages  improve the development and maintenance in security \nareas  On the other hand common arguments against static type systems can be found for example in [6, \n18, 26]: Static type systems unnecessarily restrict the developers  No-such-method exceptions which \nare caused at run\u00adtime because of missing type-checks do not occur that often  No-such-method exceptions \nmainly occur because of null-pointer exceptions (which also occur in typed programming languages)  The \narguments for and against static type systems seem to be valid but contradict each other. For industry, \na common problem is that it is unclear which arguments should be trusted. This paper contributes to this \nquestion by an experimentwhose focus is on static type systems and software development, i.e. we address \nthe question whether a type system has a positive impact on the development of a piece of software. Thereto, \nwe introduce an experiment which measures at two different points whether either the development time \nof a rather easy piece of software was improved by a static type system, or whether the resulting quality \nof a large piece of software was decreased. This paper shows that within the experiment the use of the \nstatically typed programming language did not have a significant positive impact on the software project \n while there was a significant difference in the development time for the smaller piece of software (for \na subproject of the whole one, the dynamically typed developers significantlyperformed better), there \nwas no significant difference between the statically and dynamically typed solutions withrespect to the \nresulting quality of the whole project. Section 2 briefly discusses related work in the area of empirical \nstudies on type systems. Section 3 discusses shortly the background of the experiment by explaining first \nthe logic of empirical experimentation and second the initial considerations for the experiment. Section \n4 introduces the experiment. Sections 5 and 7 analyze the data. Finally, section 7 summarizes and concludes \nthis paper.  2. Related Work We are aware only of three works which are directly in the area of empirical \nevaluations of usability of static type systems apart from our works. The first one by Prechelt and Tichy \n[23] concentrates on the impact of static type checking in procedure arguments. Prechelt and Tichy checked \nthe impact of static typing byletting subjects perform a programming task in a language with static type \nchecking as well as in a language withoutstatic type checking with the result that there is a significant \npositive impact on the developer s productivity for those developers that used the static type checker. \nThe main difference between the here introduced experiment and the one by Prechelt and Tichy is, that \nthe here introduced experiment considers the development of an application from scratch, while the authors \nof [23]mainly address the use of a certain API. The amount of time required by the task studied in [23] \n(which is approximately 5 hours) is comparable to the first point of measurement analyzed here. It is \nnoteworthy, that the results of the experiment in [23] contradict the results of the here introduced \nexperiment. Concerning the languages the difference between the here introduced experiment and the one \nin [23] is, that in [23] two C dialects were used, where both of them required type declarations in the \ncode (where only one language performed the static type check) the here introduced experiment uses a \nlanguage with and without type declarations. The second experiment we are aware of is the one performed \nby Gannon [7]. There, also a controlled experiment was performed that compares the use of a statically \ntyped vs. dynamically typed programming language. Again, the result of the study in [7] reveals a positive \nimpact of static typing. The third experiment is a qualitative pilot-study on typesystems [5] where programmers \nwere observed that used a new type system for an existing language. Although the study is just a qualitative \none, the authors seem to suggest that at least in the specific setting of the experiment the benefit \nof the type system could not directly be shown. In [13] we performed a small study (where the required \ndevelopment time for all tasks per subject was less than 2hours) on the impact of static typing using \nthe programming languages Java and Groovy, where Groovywas applied only as a dynamically typed Java without \nthe additional language features provided by Groovy. In fact, the study in [13] was performed after the \nstudy described inthis paper and the outcome of the work described in thispaper directly influenced the \nexperimental design of [13]. The tasks in [13] were designed in a way, that the tasks differed with respect \nto the number of expected type caststhat needed to be performed in order to fulfill the programming tasks. \nThe outcome of the experiment in [13]was, that subjects required less development time in order to fulfill \ntasks which have a small number of type casts. However, for bigger tasks (with even more expected type \ncasts) no difference in the development time was measured. One further experiment by Prechelt [22] could \nalso be considered as an experiment that compares statically typed and dynamically typed programming \nlanguages. However, the main focus of the experiment is not the typing issue butthe question whether \nor not the language is a script or compiler language. In that experiment, the programming effort for \ndevelopers using the (untyped) languages Tcl, REXX, Python and Perl was rather lower than the effort \nfor C++, Java and C. However, while this experiment seemed to be an argument for dynamically typed languages, \nit isunclear whether the static type system was an influencing factor for this difference. Based on the \npreviously described experiment byPrechelt, Gat performed an experiment [8] in order to determine differences \nin development times, execution times, etc. between Java, C++ and Lisp. With respect to programming effort, \nit turned out that the effort for Lisp programmers was less than for C++ and Java programmers. Again \n(corresponding to the previous paragraph) it is unclear whether this difference was caused by the different \ntype systems or whether it was caused by differentlanguage semantics, IDEs, etc..  In [14] Hudak and \nJones describe an experiment thatalso measures the impact of different programming languages. While in \ncorrespondance to [8] Lisp seems tohave a positive impact on development time, it also seems that programming \nin (the statically typed programming language) Haskell requires less effort than for example Ada. Finally, \nin [12] a first impression of the here described experiment is given (without any detailed experiment \ndescription or any detailed analysis).  3. Background and Motivation While the use of empirical methods \nhas some relevance and background in the area of software engineering (cf. for example [15, 21, 27]) \nits application in the area of pro\u00adgramming language design is not often practiced. Due tothis, this \nsection gives first a short overview of the logic of experimentation. Afterwards, initial considerations \nfor de\u00adsigning an experiment are described. 3.1 Logic of Empirical Experimentation In order to understand \nthe possible insights of the here pre\u00adsented experiment, it is necessary to understand the logic of the \nunderlying empirical experimentation. There is a number of different approaches to achieve empirical \ninsights via experimentation (see for example the introduction in [15] and the discussion in [10]) such \nas benchmarks for measuring the performance of languages or simulation for generating possible scenarios \nfor user input, etc.. A number of these approaches follow the idea of being capable to generalize from \nan experiment's results. The author would like to emphasize that this is not the case for the here described \nexperiment. The experiment in this paper is built to test a hypothesis: starting from the existing experiments \nwith a quantitative analysis that were already described in the related worksection (see [23, 7] with \nthe corresponding description insection 2) the experiment's hypothesis is, that staticallytyped programming \nlanguages have a positive impact on the development time for a programming task, i.e. using a statically \ntyped language instead of a dynamically typed one saves development time for a programming task. Using \nthis hypothesis an experiment was designed and performed in order to check whether this hypothesis (still) \nholds. Incase the hypothesis turns out to be confirmed by the ex\u00adperiment, the experiment can be considered \nas one more indicator that the initial hypothesis is true. In case the ex\u00adperiment rejects the hypothesis, \na situation is found where the hypothesis does not hold consequently, the generalstatement of the original \nhypothesis needs to be reconsid\u00adered and additional research and experimentation is re\u00adquired in order \nto determine more precisely under which circumstances static type systems save development time. It is \nimportant to note that it is not possible to draw a general statement from an experiment. If for example \nthe experiment confirms the hypothesis, it is still no general proof of the hypothesis. In the same way, \nif the outcome of the experiment is completely different than the original hypothesis (i.e. if the experiment \nwould show that dynami\u00adcally typed languages reduce development time) it is still no general proof that \ndynamically typed languages save development time. It is also not the case that a rejected hy\u00adpothesis \nproves the falsity of previous experiments such as the ones in [23, 7]: a single experiment is just another \ntestof the original hypothesis1. Although it would be desirable to have a large set of ex\u00adperiments to \ncheck a single hypothesis, there is a reason why typically no larger sets of experiments are available \n(see also [25, 10]): performing a single experiment as the one described in this paper requires a large \nset of resources. The experiment of this paper required approximately 2000 hours working time of subjects \nin a controlled setting (time for teaching subjects and time required by subjects to solve a programming \ntask). Furthermore, the preparation of the experiment (experimental design, implementing a pro\u00adgramming \nlanguage for the experiment, etc.) required someadditional months. Due to a typical limit of resources, \nit is rather not the case that a larger number of experiments with a size comparable to the one described \nin this paper is available (especially not by a single working group) al\u00adthough is would be desirable \nin order to have a broader spectrum of empirical knowledge (see [10]).  3.2 Initial Considerations for \nan Experiment The starting point of the experiment is the hypothesis un\u00adderlying the work in [23, 7]: \nthe application of static type systems reduces the development time of programming tasks. Here, especially \nthe study in [23] has quite a mature ex\u00adperimental design where the subjects solved programming tasks \nin a programming language with static type checks as well as in a programming language without such staticchecks. \nHowever, two main points were unclear for the author of the here introduced paper. First, is is unclear \nhow far the fact that existing languages were used in the ex\u00adperiment influenced the results. It is possible \nthat differentqualities of development environments (IDEs, debuggers) as well as different qualities \nin the documentation of the programming languages influenced the results. Seconds, itis unclear whether \nit is possible that subjects which are already familiar with a statically type-checked version of a language \ncan just switch to the language without such statictype checks. Here, it is questionable whether having \nal\u00adready static types in mind already influences the devel\u00adopers. This objection can be supported by \nthe fact that both languages in the experiment in [23] required type declara\u00adtions in the code hence, \nit seems as if the programmers mental models with respect to type systems were the same. Hence, a first \nconsideration for a new experiment is toprovide a new programming language with the same toolsupport \nand the same documentation to subjects. Next, in\u00adstead of trying to teach both versions to the same set \nof subjects, the idea is to teach subjects only one of both lan\u00ad 1 A more detailed motivation for, and \ndiscussion of empirical studies and their implications can be found in [10].  guages, i.e. subjects \nwhich should solve the programming tasks using the dynamically typed language should noteven know the \nstatically typed language (and the other wayaround). A resulting problem of this approach is, that the \nexperimental design is quite limited (because the same sub\u00adjects are not tested twice) and as a consequence, \nthe results possibly depend on the different qualities of the subjects inthe two groups (statically and \ndynamically typed groups). In order to reduce this problem the size of both groups should be high. One \nfurther problem is that a programming task descrip\u00adtion that is not precise enough has the problem that \ndiffer\u00adent subjects understand the tasks in different ways. In order to reduce the problem, it seems \ndesirable to provide the task description in a formal way which does not permit toleave any space for \nmisinterpretation. Consequently, it cannot be argued that differences in the solutions are the result \nof possible misunderstanding of the programming tasks. The next consideration is related to the way how \nit should be determined how development time differs. Here, different forces seem to have a direct impact. \nFirst, time and quality seem to be two facets of the same problem: the more time developers have, the \nmore time they can spent on design decisions, code improvements, testing, etc. Hence, it seems intuitive \nthat more time also increases the quality of the resulting software. Second, the term quality consists \nof different facets such as maintainability, read\u00adability, extensibility etc. as well as the number of \nimple\u00admented functional requirements. Concerning the problem of dependent time and quality, it is desirable \nto design an experiment in a way that one of both variables is fixed. While it is clear how to fix time, \nit is not obvious how to fix the quality of the solutions. The problem with maintainability, readability \nand exten\u00adsibility is that no stable metrics are available for them which makes it hard to come to an \nobjective decision abouthow large the difference of the quality between one pro\u00adgram and another one \nis. A possible solution could be toperform code reviews. However, code reviews have the problem of subjective \ncode reviewers - a problem that only can be addressed by having a larger number of inde\u00adpendent code \nreviewers. A larger set of code reviews for each program requires a statistical analysis for each single \nprogram in order to determine the average review of the program before having the possibility to compare \ndiffer\u00adent programs. This also has some potential risks in cases where different reviewers have completely \ndifferent opin\u00adions about the quality of a program. From the practical perspective, having a large number \nof code reviews (withthe previously explained potential risk) was considered tobe not adequate for the \nexperiment. Due to this, a decision for the experiment was to do no code reviews, but to testonly the \nnumber of implemented functional requirements via test cases. Hence, since the way how time and quality \nshould be measured is determined, it needs to be determined which of the variables should be fixed. The \nidea of the experimentwas to have both measurements in the same experiment. For the whole experiment, \na fixed amount of time should be given to the subjects in order to determine how manytest cases the subjects \nwere able to fulfill within the time. Additionally, for a subtask of the experiment, a set of test cases \nshould be used in order to determine how long sub\u00adjects needed to fulfill these test cases. It is important \nto note that if we are interested in meas\u00aduring differences in the number of passed test cases (byfixing \ndevelopment time) then we have to make sure thatthe programming task given to the subjects cannot be \nsolved by the subjects completely otherwise no difference in the number of passed test cases can be \nobserved. Hence, the task must be hard enough or (from the other perspec\u00adtive) the time to solve such \na task must be small enough.  4. Experiment The introduced study is a relatively large study with 49subjects \nand required for each subject approximately 27 hours pure development time. Including the teaching time \nfor each subject, between 43 and 45 hours working time was required per subject. While the recording \nof the experiment permits a detailedanalysis after the experiment took place, the focus of the experiment \nis on the development for two tasks: a parser and a scanner. Section 4.1 gives an overview of the whole \nexperiment. Section 4.2 briefly describes the programming language and its IDE used in the experiment. \nSection 4.3 describes the experiment setting, section 4.4 describes the measure\u00adment and 4.5 briefly \ndiscusses the validity of the experi\u00adment. 4.1 Experiment Overview and Design In the experiment, 49 \nsubjects (undergraduate students, selected using convenience sampling [27]) were asked towrite a simplified \nJava parser (for Mini Java). The subjects had already passed fundamental Java programming courses as \nwell as fundamental courses on formal languages. The specification of the simplified Java parser to be \nwritten was delivered via a context-free grammar. Furthermore, the subjects were asked to start their \ntask by developing a (simplified) scanner that removes special characters from a word to be tested in \nthe parser and that groups characters as tokens. For both, the scanner and parser, we expected a simple \ninterface: the scanner (class JScanner) required a method scan with one parameter that receives the string \nto be scanned and which returns a list of tokens. The Parser (class JParser) required a method parse \nwhich receives a string as a parameter and which returns a boolean value that determines whether or not \nthe passed string representsa word of the language defined by the grammar. The subjects were divided \ninto two groups. Each group was trained in a programming language written for the experiment. The only \ndifference between both groups was, that one had a language with a static type system while the other \none used a dynamic type system. After training, each subject had a fixed amount of time of 27 hours to \nimplement the parser. The scanner implementation was part of these 27 hours. The 27 hours working time \nwere controlled: the subjects could only work on their implementation within supervised rooms of our \ninstitute. According to this description, the underlying experimentfollows a 2x2 design where the programming \nlanguage isone variable and the task is another one. Both variables (programming language and task) have \ntwo treatments. For the programming language, the two treatments are the language with and without the \nstatic type system, for the tasks we have the treatment scanner and parser.  Furthermore, the experiment \nwas designed as a between-subject design (see for example [20]), since a subject did not perform the \nsame tasks twice (nor changed between the language). I.e. the comparison is done between different subjects \n(and not between the subjects themselves). The reason for this design choice was mainly motivated by \nthe fact that once a type system is being taught, we assume that people already think in term of this \ntype system , i.e. even removing the static type system from the language does not make these people \n dynamically typed programmers (see discussion in section 3).  4.2 Programming Language While other \nresearchers (such as for example [23]) use ex\u00adisting programming languages and IDEs within their ex\u00adperiments, \nwe decided to implement a new language with a corresponding IDE just for the purpose of the experiment(and \nfollowing experiments). Our motivation for this is, that we wanted to exclude any influence on the experiment \ncaused either by subjects who already know the language or which already have practical experience with \nthe IDE. In that way we also exclude the effect of different qualities of documentationsbetween different \nlanguages: different languages have different sources of information such as handbooks and web forums, \nso that differences in the measurement of an experiment might not be (only) the result of the language \nfeatures being studied, but also the result of different qualities of information about the language. \nConsequently, since a new language was used in the experiment, the language was being taught within the \nexperiment. We wrote a new object-oriented programming language (with some similarities to the programming \nlanguages Smalltalk, Ruby and Java) called Purity in two versions: a statically typed as well as a dynamically \ntyped version. Purity is a simple class-based, object-oriented language with single implementation inheritance \nand late binding. Corresponding to the language design of Smalltalk, Purity does not distinguish between \nprimitives (such as Integer or Boolean with corresponding operations) and objects. Instead, all elements \nof the language are objects which have a class definition with corresponding methods. The dominating \nlanguage construct of Purity is blocks (which is the Smalltalk version of closures, see [9]) which are \nused for different purposes such as conditional statements or loops. As a consequence, Purity contains \nfew language constructs. Similar to the language design of Smalltalk, if is not a language construct \nitself, but isprovided by corresponding methods in class Boolean. The only loop provided is the while-loop \nwhich is represented by a method in block objects. Field access is only provided via methods, i.e. only \nan object itself is allowed to access its fields. More mature features such as multi-threading orGUI \nsupport are not implemented in the language. The programming language includes a simple API with14 basic \nclasses such as Integer, Boolean, Block, String or LinkedList and five additional helper classes (different \nkinds of Exceptions). Additional to the programming language, a small IDE was provided, which includes \na class browser, a test browser (for running the application and tests of applications) and a console \nwindow. Figure 1 illustrates Purity's IDE (for the dynamically typed Purity). On the left hand side, \nthere is a Class Browser which shows all classes, methods, fields, and superclass. The class browser \nalso contains an editor for editing methods. On the right hand side, there is the testbrowser which shows \nall test classes and their test methods. The test browser permits to run a test (the only way toexecuted \ncode in Purity) and shows failures and errors caused by a test case. The output window represents the \nconsole, which permits developers to print out strings. While developers are allowed to open as many \nclass browsers as they like, only one output window and one testbrowser was allowed.  //simple method \nfor dynamically typed Purity // myMethod { in // method definition with parameter in // |locVar| // \ndeclaration of local variable // locVar := in; // sets local variable to in // [ || 10.largerThan(locVar);].whileFalse([ \n// loop from 1 to 10 // |blockVar| // declaration of local block variable // locVar:=locVar.plus(1); \n// increases local variable //  blockVar:=locVar; // assignment to block variable //  Transcript.printString(blockVar.asString()); \n// prints value // ]); ^42; // returns 42 // } //simple method for statically typed Purity // Integer \nmyMethod { Integer in // method with int parameter // |Integer locVar| // declaration of local variable \n// locVar := in; // sets local variable to in // [ || 10.largerThan(locVar);].whileFalse([// loop from \n1 to 10 // |Integer blockVar| // declaration of local block variable // locVar:=locVar.plus(1); // increases \nlocal variable //  blockVar:=locVar; // assignment to block variable //  Transcript.printString(blockVar.asString()); \n// prints value // ]); ^42; // returns 42 // } Figure 2. Example code for dynamically typed and statically \ntyped Purity The static type system of the statically typed Purity is non-generic and nominal and can \nbe compared to the type system of Java up to version 1.4 (without primitive types): all classes implicitly \nrepresent a type, whereby a type consists of its name and its associated methods. In contrast to e.g. \nJava, Purity does not provide any access modifiers (such as public, private, etc). Hence, it is not possible \ntoexclude any methods from its static type. Since the type system is non-generic, elements of LinkedList \n(as the onlyavailable collection in Purity) are accessed by the static type Object. The static typing \nof blocks was implemented similar to the proposal that can be found in [9]. The static type checks are \n(for the statically typed version of Purity) performed just before the developer tries to execute a piece \nof code. Static type errors are shown in an additional window which lists all static type errors in the \ncode in addition to where these typed errors occur (of course, such a window is not available in the \ndynamicallytyped version of Purity). The static type check is performed on the whole code base. Hence, \na type error in one single method (even though it is not the method currently edited by the developer) \nprevents the developer from executing the code. It should be emphasized that the whole API of the statically \ntyped Purity is the same as the API of the dynamically typed Purity (except the type annotations inthe \ncode). The only semantic difference between the statically typed and dynamically typed Purity is the \nexistence of a type cast operator in the statically typedPurity. Figure 2 illustrates a basic method \ndefinition for the dynamically typed as well as the statically typed Purity. Inboth cases the method \nreceives an Integer object as input parameter and stores it to the local variable locVar. Then, a block \n(without any parameter and without any local variable) represents a loop invariant that checks whether \nlocVar is smaller than 10. The loop s body is represented by a corresponding block passed to the method \nwhileFalse. The block has no parameter and one local block variable(blockVar). The loop body increases \nthe variable and printsit out. The teaching material used for Purity is the same for the statically typed \nand the dynamically typed version. The only difference is, that the teaching material for statically \ntyped Purity contains a section about the type system with a special focus on blocks.  4.3 Experiment \nExecution 49 subjects (undergraduate students at the University of Duisburg-Essen, selected using convenience \nsampling2)participated in the experiment as subjects. They were not aware of what was being studied within \nthe experiment(they were told that it was an exploratory study on how students program). None of the \nsubjects ever wrote a parser or a scanner. First, an interview with the students was performed that checked \ntheir background. The motivation for this was, thatwe wanted to have balanced groups, i.e. the statically \nas well as the dynamically typed group should have a balanced number of experienced and unexperienced \ndevelopers. Thereto, we asked for their current programming skills, their known programming languages \nand how long and in what environment they worked already in industry. The whole study was not performed \nin a single session. Instead, the process of selecting students and dividing theminto 2 groups was distributed \nover one year (with six sessions). The programming language was taught to the subjects for the dynamically \ntyped programming language in 16 hours including pure presentations, as well as hand-on sessions to the \nsubjects. The statically typed programming language was taught in 18 hours the difference in the time \ncan be explained by the additional effort for teaching the static type system, where a special focus \nof the training was on blocks (since the subjects all had the background of Java). After that, each subject \nhad to implement the scanner and parser within 27 working hours divided into 4 working days. The corresponding \ncontext-free grammar was described by a Backus-Naur notation. These 27 hours were controlled and took \nplace in the experimental environment. Coffee-breaks etc. were not included in these 27 hours. The subjects \nwere permitted to arrange their time freely, i.e. they were permitted to arrive at different times. The \nsubjects were not permitted to take any material from the experimental environment. I.e. the language \nitself and its IDE, handbooks, etc. always stayed in the experimental environment. Furthermore, the subjects \nsigned a paper which obliged them not to share anyinformation with other subjects. 2 Convenience sampling \ndescribes the construction of a sample from a population which is close at hand. See 27 for a more detailled \ndiscussion.  4.4 Recording and Measurement While the experiment was running, all changes in the code \nbase were logged so that it was later on possible to recon\u00adstruct all user inputs at a certain point \nin time: whenever the developer added or removed a class or whenever a method was added to (or removed \nfrom) the system, a cor\u00adresponding log entry was generated in the background. A change in the code base \nwas the moment, when developers edited a method and stored it in their environment. Based on that, we \nmeasured the time the subjects re\u00adquired to implement a scanner and the quality of the result\u00ading parser. \nI.e. the measurement of the experiment consists of two different metrics: time and quality. For the scanner \nimplementation we defined 23 test cases that represented from our point of view the minimal func\u00adtionality \nrequired in the system. These test cases were notdelivered to the subjects but only used for evaluation \npur\u00adposes later on. Hence, subjects were not able to use the definition of test cases in order to increase \ntheir develop\u00adment speed. Based on the log entries and the test cases we were able to determine the point \nin time when subjects fulfilled these test cases. We did that by reconstructing all developer ac\u00adtivities \nand after any change in the code we rerun the tests. We considered the point in time when all test cases \nwere fulfilled as the reference point where the developer finisheda minimal scanner. Once, we have the \ndevelopment times for the statically and the dynamically typed group, we can compare them via a corresponding \nstatistical analysis. In order to measure the quality of the resulting parser we defined a set of 200 \ntest cases that represented valid words of the language defined by the grammar. Additionally, we specified \n200 invalid word. Hence, fulfilling 50% of the test cases has the same quality as a parser which returns \nthe constant true or false. The test cases were not delivered to the subjects. We ran the test cases \non the delivered project of each subject. Hence, we get for all subjects a number of successful test \ncases and can compare the number of suc\u00adcessful test cases for the statically as well as for the dy\u00adnamically \ntyped group.  4.5 Threats to Validity According to the known guidelines for empirical research in software \nengineering (see for example [16, 17]), it is necessary to make explicit all elements that potentially \nthreaten the validity of an experiment. First, it is necessary to emphasize the argumentation ofthe measurement: \nthe paper compares development times between two groups using a different programming language. While \nwe think that this is valid for comparing both groups, we doubt that the measured developmenttimes for \nthe scanner or the number of successful test cases for the parser are representative for any other scenario: \nsince the programming language being used is a new one, itis unclear how much of the development time \nis spent on handling a new technique instead of solving the problem. Consequently, neither the development \ntimes for the scanner nor the number of test cases can be considered representative for the same tasks \nusing a known language. Nevertheless, the study has not the focus on the development time its focus \nis on the difference of development times. The study uses students as subject and it is often argued \nthat students do not represent a valid sample for developers (see [24] for further discussion). It could \nbe argued here, that one characteristic of the experiment is, that a new language must be learned. Here, \nit is unclear whether students or professional developers (or any of them) have a benefit (see further \n[3]). Concerning the students, it isnecessary to state that none of them already implemented a scanner \nor a parser. Consequently, the measured development time does not only contain the translation of the \nprogramming task into code but also the intellectual effort to design a solution . Another threat to \nvalidity is, that the subjects were grouped into two groups based on interviews. It is possible that \nthe estimation of the person, who did the experiment, was wrong concerning the developers' experiences. \nNext, it can be argued that 16 hours training is notenough for learning a new language (and a new IDE). \nThisis definitively the case. However, it should be emphasized that the language, its API as well as \nits IDE was kept verysimple, so that we considered the training to be sufficient. The only problematic \nelement we identified was that the subjects had problems to understand the semantics of blocks due to \nthe Java background of the subjects. However, since this problem was equal for all subjects, we consider \nit to be less problematic for the internal validity ofthe experiment. Concerning the additional two hours \nfor the type system, it can be argued that two hours are not sufficient to learn the type system of a \nlanguage. Here, it needs to be emphasized that all subjects already had a Java background. Consequently, \nthey were already familiar with nominal type systems. The new element for the subjects were blocks, hence \nthe training of the static type systemmainly focused on the static typing of blocks. One serious argument \nagainst the validity is, that the programming task is a relative large one. Although this can be seen \nas a good approach to reduce the often mentioned argument, that empirical studies are typically too small \ninorder to get any meaningful results (see [25]), it has the disadvantage that possible effects such \nas design choices of programmers become an influencing factor of the experiment. We do not think that \nit is possible to reduce this problem: either an experiment has a relative trivial taskwhere developers \ndo not have enough freedom to do relevant design choices, or it has not. In the first case, the programming \ntask can be reduced to the typing speed of the programmer. The latter one inherently contains such design \nchoices. Concerning the tasks, it is problematic that it was notexplicitly requested from the subjects \nthat they have tofinish the scanner completely before continuing withwriting the parser. As a consequence, \nwe cannot be sure atwhat point in time the subjects switched to the parser taskwithout finishing the \nscanner task. The experimentaddresses this threat by using only a minimal set of testcases for the scanner \n assuming that developers do noteven want to finish before working on the parser task.  Dyn. Typed Subject \nsec hours 1 8141 2.26 2 38374 10.66 3 31275 8.69 4 24666 6.85 5 7260 2.02 6 11224 3.12 7 23218 6.45 8 \n18317 5.09 9 7335 2.04 10 16994 4.72 11 12766 3.55 12 13975 3.88 13 39123 10.87 14 23041 6.40 15 5430 \n1.51 16 2851 0.79 17 8379 2.33 18 7510 2.09 19 18334 5.09 20 42501 11.81 21 30393 8.44 Stat. Typed Subject \nsec hours 22 40970 11.38 23 32208 8.95 24 23413 6.50 25 28463 7.91 26 51572 14.33 27 32112 8.92 28 28752 \n7.99 29 29509 8.20 30 56985 15.83 31 8972 2.49 32 3653 1.01 33 40864 11.35 34 37022 10.28 35 14701 4.08 \n36 46032 12.79 37 20949 5.82 38 26612 7.39 39 25872 7.19 40 11172 3.10 41 16464 4.57 42 6305 1.75 Figure \n3. Measured experiment results for scanner One further concern we see is our decision to use black\u00adbox \ntesting via test cases as the criterion for the quality of the resulting applications. As a consequence, \nwe consider a piece of software that has a high number of passed test cases to be better than a piece \nof software with a lower number of passed test cases. This implies, that a very good implementation of \na parser, where the developer has misunderstood the start production rule of the grammar, isconsidered \nto be bad. A different alternative to address this problem could be to do code reviews by experts that \njudge on the quality of the resulting pieces of code. We discussed this issue intensively in section \n3 where our main argument is that for this experimental setting the use of code reviews is rather not \npractical (see discussion of initial considerations in section 3.2).  5. Analysis of Scanner Task We \nstart the analysis by describing the experiment s resultsfor the scanner implementation and then performing \nsig\u00adnificance tests in order to check whether there are signifi\u00adcant differences in the development times \nusing the statically typed and dynamically typed programming lan\u00adguage. The data used here comes from \n49 subjects, 25 in the dynamically typed group and 24 in the statically typed group. 4 subjects from \nthe dynamically typed group and 3 subjects from the statically typed group failed to fulfill the scanner \ntask (while some of them still were able to finishthe parser task). Hence, we removed those seven subjects \nfor the analysis of the scanner. Hence, the analyzed data within this experiment comes from two groups \nof equalsize: the group with the dynamically typed language as wellas the group with the statically typed \nlanguage consisted of 21 subjects. 5.1 Results and Descriptive Statistics Figure 3 shows the measured \nresults of the experiment, i.e. the number of seconds (and hours) required by each subject to fulfill \nthe test cases. For example, on the left hand side, the third line says that subject number 3 required \n31275 seconds (respectively 8.69 hours) in order to fulfill the minimum requirements. Subject number \n24 (which is a subject that used the statically typed language) required 23413 seconds (respectively \n6.50 hours). Based on the experiment results, we computed the de\u00adscriptive statistics (see Figure 4). \nHere, we see that the sumof development times (and hence the arithmetic mean), the maximum, the minimum \nand the median of the group withthe dynamically typed programming language is less than the corresponding \nvalues of the group with the staticallytyped programming language. Furthermore, we see that the differences \nare quite high (for example, the sum of devel\u00adopment times of the dynamically typed solution is 391,107 \nseconds, while the sum of times for the statically typed solution is 582,602 seconds). sum (sec.) max \n(sec.) min (sec.) arith. mean (sec.) median (sec.) std. dev. Dyn. Typed 391107 42501 2851 18624 16994 \n11726 Stat. Typed 582602 56985 3653 27743 28463 14256 Figure 4. Descriptive statistics for scanner task \nIn order to have a more intuitive representation of these differences, Figure 5 shows a box plot for \nthe dynamicallytyped and the statically typed group, which visualized the lower quantile, the median \nand the upper quantile of the underlying data set. Because of these results it seems reasonable to assume \nthat there is a significant difference between both times which will be checked in the next section. \n 5.2 Checking Significance in Means In order to check whether there is a significant difference between \nboth values it is necessary to formulate a hypothe\u00adsis which needs to be checked by a corresponding signifi\u00adcance \ntest. Since the subjects differ from each other with respect to the applied programming language (no \nsubject did the im\u00adplementation with the statically and the dynamically typedlanguage), a significant \ntest for independent samples is required. At the current point, we cannot assume any underlying distribution \nfor the measured data. Hence, it is necessary toapply a so-called non-parametric significance test (cf. \n[4]). A test that can be applied here is the (non-parametric) Mann-Whitney U-test which compares for \ntwo samples whether they come from the same population. The Mann-Whitney U-Test is a standard significance \ntest in statistics and empirical methods (see for example [25] and [13]) for the here described purpose. \n  Figure 5. Box plot for scanner task The underlying hypothesis and alternative hypothesis tobe tested \nare H0: The data for statically and dynamically typed de\u00advelopment times are drawn from the same population \nH1: The samples come from different populations  5.3 Checking Significance for Quantiles A possible \nexplanation of the previous result could be thatin the statically typed group the number of underperform\u00ading \nsubjects is larger than in the dynamically typed group. According to [19] it is quite common to divide \nthe subjects into different quantiles and to perform the statistical analy\u00adsis in separate in order to \nreduce such effect. We constructed the 2-quantiles from the experiment re\u00adsults, i.e. all subjects whose \ndevelopment time is smaller than or equal to the corresponding median are in the firstquantile, all subjects \nwhose development time is smaller inthe second quantile. For the dynamically typed language, subjects \n16, 15, 5, 9, 18, 1, 17, 6, 11, 12 and 10 are in the first quantile, for the statically typed language, \nsubjects 32, 42, 31, 40, 35, 41, 37, 24, 39, 38 and 25 are in the first quantile. Figure 7 illustrates \na box plot for the resulting quantiles (we neglect here to give the corresponding descriptive sta\u00adtistics). \nIt shows that the median for the first dynamically typed quantile is lower which is also true for the \nsecond quantile. Performing the Mann-Whitney U-Test for each quantile, i.e. comparing the first dynamically \ntyped with the first statically typed quantile as well as comparing the second dynamically typed with \nthe second statically typed quantile reveals in both cases a significant difference (with p=0.03, respectively \np=0.04). Language N Mean Rank Sum Ranks Dyn. Typed 21 17.62 370 Stat. Typed 21 25.38 533 Figure 6. Ranks \n(Mann-Whitney U-Test) for scanner test (with p=0.04) Since the underlying distribution is not known, \nthe Mann-Whitney U-Test works on ranks, i.e. the number of the development times for the statically typed \nas well as dynamically typed scanner become ordered and the number of positive and negative ranks for \nthe statically typed anddynamically typed development times are computed. Based on the ranks, the U-Test \ncomputes a p-value which indi\u00adcates whether the result is significant according to the un\u00adderlying. The \nsmaller the p-value the more significant is the rejection of the hypothesis: typically, p-values of 0.01, \n0.05 or 0.1 are used to decide whether the null hypothesis can be (significantly) rejected, i.e. a result \nof p=0.02 means a significant rejection of the null hypothesis under the sig\u00adnificance level of 0.05. \nIn case the null hypothesis is re\u00adjected, the rank sum for each approach indicates whether the statically \ntyped or dynamically typed programs domi\u00adnate the other approach. The p-value for the Mann-Whitney U-test \nis p=0.04. Since p < 0.05, the null hypothesis can be rejected, i.e. both samples come from different \npopulations. By comparing the rank sums it turns out that the development times usingthe dynamically \ntyped language are significant lower than the development times using the statically typed program\u00adming \nlanguage: the rank sum for the dynamic typed lan\u00adguages is smaller than the rank sum of the statically \ntypedlanguage (370 vs. 533, see Figure 6). Figure 7. Boxplot for 2-quantiles (scanner task) Since there \nare significant results for both quantiles, this strengthens the argument that the significance determined \nin section 4.1 cannot be explained by a different number of outperformers or underperformers in each \ngroup.  5.4 Comparing Difference in Means While the previous section determined that there is a sig\u00adnificant \ndifference in the development times of statically and dynamically typed solutions, it did not state how \nlarge this difference is (note that the use of the arithmetic meanor the median is statistically not \nvalid here). A typical ap\u00adproach here is to determine first the data s underlying dis\u00adtribution and to \nuse (based on the result) a corresponding significance test.  Figure 8. Histogram (statically typed \nlanguage) for scanner task Figure 8 illustrates a histogram for the typed language group. Furthermore, \nthe diagram illustrates a plot of the normal distribution. According to the histogram it seems reasonable \nto test, whether the underlying data is normallydistributed. A check for normal distribution is done \nusing the Shapiro-Wilk test (cf. [4]) which computes a value p. Incase p is smaller than 0.05, the test \nrejects the assumption that the underlying data is normally distributed (under a significance level of \n0.5). t df sig (2-tailed) mean diff. 95% confidence interval lower Upper Language -2.209 40 0.03 -13140.3 \n-17461 -777 Figure 9. t-test results for scanner task Computing the p-values reveals for the dynamicallytyped \nprogramming language as well as for the staticallytyped programming language a value larger than 0.05 \n(al\u00adthough the p-value for the dynamically typed language is0.08). Hence, in both cases the test does \nnot reject the hy\u00adpothesis of a normal distribution of the underlying sample. Therefore, we can perform \na t-test for independent samples which will also reveal the size of the difference between both samples \n(although it must be critically mentioned that a value p=0.08 is only not significant under a significance \nlevel of p=0.05). Figure 9 shows the results of the t-test. Again, we can see that there is a significant \ndifference between both sam\u00adples (with p=0,03), but we have this information alreadyfrom the previous \nsection. However, the important infor\u00admation is the 95% confidence interval which states that with the \nprobability of 95% the development time using the dynamically typed language is between 777 and 17461 \nseconds less than the development time using the staticallytyped language. According to this, the advantage \nof using the dynami\u00adcally typed programming language in the experiment isapproximately between 19 and \n291 minutes per subject but (again) it needs to be emphasized that the value for the Shapiro-Wilk-Test \nfor the untyped language was p=0.08 (which is formally correct that the hypothesis of the normaldistribution \ncannot be rejected, but which obviously tells, that the distribution is very close to be non normal). \n 5.5 Impact of Testing / Type Error Fix Time The previous results are slightly surprising, since it \nseems reasonable to assume (by relying on the argumentation pro typing as given in section 1) that typing \nshould have in the experiment a positive impact on the development time: for all subjects the programming \nlanguage and itsunderlying IDE was new. It could be assumed here that the additional documentation character \nof type declarations eases the application of a new language  being new in a language could mean that \ndevelopers do rather trivial mistakes (such as sending wrong messages to objects). In such a case the \ntype systemsaves developers from wasting run-time on testing applications that (predictably) cannot be \nexecuted due to type errors.  However, since the previous results contradict the im\u00adplication of this \nargumentation it is reasonable to analyze how the data for the scanner implementation times is related \nto this argumentation. While the first argument (documentation characteristics) cannot be measured fromthe \nexperiment s data, it is possible to define a corresponding measurement for the second one. The second \nargument (reduction of test time) implies that the correction of a type error takes less time than running \na testcase, receiving a NoSuchMethod exception and correcting the corresponding piece of code. In order \nto check this argument, we reconstructed from the logged data the testruns performed by the subjects. \nFor the dynamically typed subjects we measured the time from the start of a test run that stopped with \nexcep\u00adtions that could be handled by a type systems (non-declared variables, NoSuchMethod exceptions, \nbut not NullPointer exceptions) until the next test run successfully ran. Then, we built for each subject \nthe sum of these test times. For the typed subjects we measured the time from the point in time where \nsubjects tried to run a test case whose execution was prevented by the static type checker until the \nnext test ran that was accepted by the type checker. Under the assumption that developers directly try \ntosolve errors in the code that cause exception, the first case identifies the times people require to \nfix no such method exceptions in the code (which could have been handled bythe type checker). The latter \none measures the time for peo\u00adple to fix their type errors identified by the type checker. Again, we \nbuilt the sum of these times for each subject. It should be noted that this measurement represents onlya \nrough approximation of the debugging time for each sub\u00adject, because it was not explicitly required that \nsubjects directly fix a bug that was found during testing (or by the type checker). Hence, the measurement \nis build upon the assumption that once an error occurred, developers directlytry to fix it the measurement \nis only an index for the time required to fix a bug.  subject subject Dyn. Typed Stat. Typed sec minutes \nsec minutes 1 942 15.70 22 6482 108.03 2 2007 33.45 23 5310 88.50 3 6857 114.28 24 1199 19.98 4 2588 \n43.13 25 3425 57.08 5 896 14.93 26 5376 89.60 6 2780 46.33 27 3689 61.48 7 7379 122.98 28 5902 98.37 \n8 1873 31.22 29 3138 52.30 9 405 6.75 30 6714 111.90 10 1465 24.42 31 339 5.65 11 2658 44.30 32 33 0.55 \n12 2034 33.90 33 4720 78.67 13 1600 26.67 34 4096 68.27 14 2681 44.68 35 1589 26.48 15 587 9.78 36 3847 \n64.12 16 393 6.55 37 585 9.75 17 1832 30.53 38 614 10.23 18 3759 62.65 39 1324 22.07 19 2538 42.30 40 \n645 10.75 20 4662 77.70 41 2810 46.83 21 350 5.83 42 98 1.63 more detailed studies are necessary to \nstudy this phenome\u00adnon in more detail.  6. Analysis of Parser Task While section 5 analyzed the data \nfor the scanner imple\u00admentation, we now analyze the number of successful testcases for the statically \nand dynamically typed solutions. The performed techniques are identical to the techniques used in the \nprevious section. Hence, we do not explain indetail each step. 6.1 Checking Differences in Means Figure \n10. Measured (approximated) debugging time. Figure 10 represents the results of the measurement. A first \nglimpse already shows that the measured data does notreveal similar large differences between the error \nfix time and the measurement of the development times from the previous section. Figure 11 shows the \ndescriptive statistics for the measurement. It shows that the arithmetic mean of statically and dynamically \ntyped programming language are rather closely related. Figure 12 shows the percentages of passed test \ncases for each subject. Here, in contrast to section 5, we include all subjects (the reason they were \nremoved in the last section was, that no time could be measured for them). Dyn. Typed Stat. Typed Subject \n% succ TestCases Subject % succ TestCases 1 50% 26 50% 2 90% 27 50% 3 50% 28 75% 4 50% 29 87% 5 50% 30 \n80% 6 56% 31 50% 7 80% 32 50% 8 60% 33 68% 9 50% 34 50% 10 52% 35 100% 11 50% 36 50% 12 50% 37 85% 13 \n50% 38 50% 14 50% 39 50% 15 50% 40 55% 16 56% 41 50% 17 69% 42 89% 18 92% 43 68% 19 85% 44 86% 20 50% \n45 57% 21 50% 46 50% 22 85% 47 98% 23 50% 48 50% 24 79% 49 51% 25 50% sum max min arith. median std. \n(sec.) (sec.) (sec.) mean (sec.) deviation (sec.) Dyn. Typed 50286 7379 350 2395 2007 1997 Stat. Typed \n61935 6714 33 2949 3138 2113 Figure 11. Descriptive statistics for approximatederror fix time Performing \nagain a Mann-Whitney U-test on the resultssupports that impression: the resulting value p=0,49 does not \npermit to reject the null-hypothesis stating that both samples come from the same distribution. Hence, \nno sig\u00adnificant difference in both samples could be measured. This result is slightly surprising. Having \nno significantdifference in the debug time between dynamically and statically typed group means that \nthe effort for fixing type\u00adrelated bugs (NoSuchMethod exceptions, non-declared variable) can be assumed \nto be the same. Although thisfalsifies the statement that debugging typing errors takes less time having \na static type system (and a static type check), this does not explain why the dynamically typed group \nrequires less time for the same task. A possible explanation for this phenomenon could be, that the dynamically \ntyped group gets more information about the correctness of the application from their testing:instead \nof knowing only that no NoSuchMethod exceptions occur within the code, they already get some earlier \npre\u00adliminary results from the testing which helps to determine what parts of the application are already \ncorrect. However, Figure 12. Measured experiment results and descriptive statistics. We see that a large \nnumber of subjects fulfil only 50% of the test cases these are those subjects who did notachieve a meaningful \nparser, since they were able to reject strings that are not words of the grammar, but they were not able \nto detect any of the other ones (or the other wayaround). In both cases, we see a slight difference in \nnumber of subjects without a meaningful result: for the dynamically typed language 14 subjects and the \nstatically typed lan\u00adguage 11 subjects. Hence, it seems that more people withthe dynamically typed language \nfailed to provide a mean\u00adingful parser. However,. we think that 14 and 11 is com\u00adparable . Again, we \nperform the Mann-Whitney U-Test in order to compute the p-value. However, the p-value for the table above \nis p=0.40. Hence, there is no significant difference between the statically typed and the dynamically \ntyped version.  Removing those subjects that have only 50% accepted test cases (i.e. we repeat the test \nwith 12 subjects using the dynamically typed language and 11 with the staticallytyped language) and repeating \nthe Mann-Witney U-Testgives a p-value p=0.60. Hence, also no difference in both groups was determined. \nPerforming a test on grouped subject, i.e. we consider only one the outperforming and underperforming \nsubjects gives the p-values p=0.60 (outperforming subjects) and p=0,31 (underperforming subjects). Dyn. \nTyped subject sec minutes 1 1287 21.45 2 3075 51.25 3 6533 108.88 4 1991 33.18 5 6857 114.28 6 2588 43.13 \n7 896 14.93 8 4622 77.03 9 7379 122.98 10 1873 31.22 11 405 6.75 12 1470 24.50 13 3417 56.95 14 4250 \n70.83 15 1600 26.67 16 2861 47.68 17 1604 26.73 18 709 11.82 19 2950 49.17 20 3430 57.17 21 5012 83.53 \n22 2950 49.17 23 2626 43.77 24 4662 77.70 25 350 5.83 Figure 13. Measured (approximated) debugging time. \n Stat. Typed subject sec minutes 26 7091 118.18 27 3901 65.02 28 4588 76.47 29 5313 88.55 30 3912 65.20 \n31 2313 38.55 32 5271 87.85 33 8500 141.67 34 4741 79.02 35 3416 56.93 36 4763 79.38 37 2715 45.25 38 \n7344 122.40 39 5266 87.77 40 3754 62.57 41 777 12.95 42 3900 65.00 43 3637 60.62 44 4006 66.77 45 2804 \n46.73 46 5881 98.02 47 3045 50.75 48 4029 67.15 49 2180 36.33 However, it seems also reasonable to analyze \nthe outper\u00adforming and underperforming subjects that have more than 50% successful test cases. However, \nit should be noted that this test is problematic, since the group of outperforming, successful subjects \nconsists only of 6 subjects. However, even taking into account that a possible significant differ\u00adence \nwould be problematic to interpret, we do not find a significant different for the outperforming subjects \n(p=0,33). For the underperforming subjects we find also no statistical significant difference for a significance \nlevel of 5%. The p-value is p=0,91. 6.2 Debug Times Again, we repeated the measurement of the debug times \ninthe same way we did as in section 5 under the assumption that developers try to solve their exceptions \ndirectly: the measured times for the dynamically typed group describes the debug time of those exceptions \nthat could have been handled by a type checker, the measured time for the typed developers describes \nthe type required to handle typing problems identified by the type checker. Figure 13 de\u00adscribes the \nresults of that measurement. Performing again the Mann-Whitney U-Test on the results reveals something \nremarkable: with p=0.01 the time for the dynamically typedgroup is less than the time for the statically \ntyped group.  7. Conclusion In this paper we presented results of an experiment thatexplores the impact \nof static and dynamic type systems on the development of a piece of software (a parser) with 49 subjects. \nWe measured two different points in the devel\u00adopment: first, the development time until a minimal scanner \nhas been implemented, and second the quality of the result\u00ading software measured by the number of successful \ntestcases fulfilled by the parser. In none of these measured points the use of the static type system \nturned out to have a significant positive impact. In the first case, the use of the statically typed \nprogramming language had a significantnegative impact, in the latter one, no significant difference could \nbe measured. One interesting point in the study is that we rather ex\u00adpected that developers benefit from \nthe statically typed language, especially, because the language in use was new to them. Hence, we rather \nexpected a positive impact oftyping because it is commonly assumed that static typing improves the ability \nto learn new APIs since static typingrestricts a possible faulty use of a new API. We analyzed this \nby defining as (approximated) debugging time on the experiment data and performing a corresponding signifi\u00adcance \ntest. However, the result has been rather surprising: while in the small task no significant different \ncould be measured, in the latter one the statically typed group re\u00adquired significant more time for fixing \nthe type errors. Al\u00adthough it seems obvious that this should have been led to a worse result in the parser \ndevelopment, in fact it was not. This implies that there must be a positive impact of static typing in \nsomething else which compensates this negative effect. Possibly, it is easier for developers to extend \ntheir own code, if static type information is added and this effectis larger than a negative effect of \nfixing type errors. Butthis is rather a speculation and requires much more ex\u00adploratory studies in the \narea of usability of static type sys\u00adtems. It must not be forgotten that there are some threats to va\u00adlidity \nthat threaten the experiment s results. There are two problematic issues: the first one is related to \nthe grouping of subjects into the statically typed / dynamically typedgroup, the latter one is related \nto the testing of the results. A problematic point here is that it is unclear whether both groups contained \nan equal number of good develop\u00aders. However, the classification of developers is currently a problem \nsince no objective metrics available that permit toclassify subjects: the current state of the art in \nsoftware engineering does not provide until now a better classifica\u00adtion. Hence, we built quantiles for \nthe analysis which is a way to handle this situation. Concerning the testing of the results, we are \nbased on test cases. Here, it is important to note that 50% of the (black-box) test cases (for the parser) \nwere positive tests. It is possible that if the number of test cases would been changed, that results \nwould have changed, too. Further\u00admore, white box testing could be applied which potentiallyrepresent \ndifferent result. However, until now we are notaware of any better way to determine the quality of the \nresulting software which is also practical in such a large scale study.  Probably the most interesting \npoint of the experimentresults is that they contradict studies such as [21] where a positive impact of \ntyping could be measured. We are cur\u00adrently not aware of how to explain this contradiction. Pos\u00adsibly, \nthe effect of typed programs is better in some situations while it is not in others. However, since the \nknowledge about different kinds of programs is currentlyquite restricted, it is currently unknown whether \ndifferent kinds of programs have a different impact on the use (and the impact) of type systems. Here, \nit is desirable torepeat the experiment under similar conditions but with different tasks. It should \nalso be noted that the experimentdesign differs widely from the one in [21]: while the ex\u00adperiment in \n[21] uses existing languages and has possiblythe effect of better / worse documentation, tutorials, etc. \nofthese languages, the experiment in this paper has imple\u00admented a language only for the purpose of the \nexperiment:the language and their documentation are identical (exceptthe difference in the static type \nsystem). A subjective opin\u00adion of this paper s author is that blocks, and especially the static typing \nof blocks turned out to be problematic for the subjects of the experiment it would be desirable to studythis \nmatter in isolation. It seems that the experiment at least does not contradictwith a common believe that \nis often articulated in the area of type systems: that dynamically typed systems have a benefit in small \nprojects while this benefit is reduced (or even negated) in larger projects. In the experiment, the small \ntask was quicker implemented by the dynamically typed developers while no difference could be measured \nfor the whole task (scanner and parser). Here, it could be argued that the possible effect of dynamically \ntyped lan\u00adguages for the smaller task was compensated by the benefit of the static type system later \non and that (possibly) the positive effect of statically typed languages would have been even larger, \nif the project would have been larger. Hence, it could be argued that the positive impact of static typing \ncould not be measured because the project was too small. Even though the experiment seems to suggest \nthat statictyping has no positive impact on development time it must not be forgotten that the experiment \nhas some special con\u00additions: the experiment was a one-developer experiment. Possibly, static typing \nhas a positive impact in larger pro\u00adjects where interfaces need to be shared between develop\u00aders. Furthermore, \nit must not be forgotten that previous experiments showed a positive impact of static type sys\u00adtems on \ndevelopment time. Possibly, such a positive impact occurs in special situations, which might depend on \nthe kind of programing task - however, up to now no classi\u00adfication of programming tasks is known that \npotentiallyinfluences the topic of static type systems. Here, additionalresearch is required, too. A \nfurther point that needs to be emphasized is, that the experiment here addresses only pure programming \ntime. Possible influences of static type systems on design time, readability or maintenance of software \ncannot be concluded from the experiment s results. A general conclusion is that an experiment is available \nthat doubts whether static type systems really have a posi\u00adtive impact on development time. However, \nin order to geta more detailed understanding of the impact of static type systems more studies are needed. \nSince static type systems play such an important role in software development it is rather tragic that \nthe discussion about the need for staticallyor dynamically typed programming languages is mainlydriven \nby personal experiences and personal references of researchers instead of empirical knowledge received \nfroman adequate research method. From that point of view, the here introduced experiment represents a \nfirst starting pointand a valuable contribution.  Acknowledgments The author would like to thank the \nUniversity of Duisburg-Essen for providing a grant that made this empirical studypossible. Furthermore, \nthe author would like to thank the companies GFOS, Swarc, Senacor, clavis, and ISKV for supporting the \nproject. Additionally, the author would like to thank Robert Hirschfeld, Erik Ernst and Michael Hauptfor \ndiscussing with them the ideas of empirical studies in programming language research. Finally, the author \nwould like to thank David Ungar for his encouragements and his time he spent on long discus\u00adsions with \nthe author.  References [1] Bruce, K.: Foundations of Object-Oriented Languages: Types and Semantics, \nMIT PRESS, 2002 [2] Cardelli, Luca: Type Systems, In: CRC Handbook of Computer Science and Engineering, \n2nd Edition, CRC Press, 1997. [3] Carver, J; Jaccheri, L.; Morasca, S., Schull, F. Using empirical stud\u00adies \nduring software courses, Empirical Methods and Studies in Soft\u00adware Engineering, Experiences from ESERNET, \nLNCS 2765, Springer, 2003, pp. 81-103. [4] Conover, W. J.: Practical nonparametric statistics, 3rd edition, \nJohn Wiley &#38; Sons, 1998. [5] Daly, M.; Sazawal, V., Foster, J.: Work in Progress: an Empirical Study \nof Static Typing in Ruby, Workshop on Evaluation and Usabil\u00adity of Programming Languages and Tools (PLATEAU) \nat ONWARD 2009. [6] Eckel, B.: Strong Typing vs. Strong Testing, mindview, 2003, http://www.mindview.net/WebLog/log-0025, \nlast access: August 2009 [7] Gannon, J. D.: An Experimental Evaluation of Data Type Conven\u00adtions, Comm. \nACM 20(8), 1977, S. 584-595. [8] Gat, E.: Lisp as an alternative to Java. Intelligence 11(4): 21-24, \n2000. [9] Graver, J. O.; Johnson, R. E.: A Type System for Smalltalk, Seven\u00adteenth Symposium on Principles \nof Programming Languages, 1990, pp. 136-150 [10] Hanenberg, S.: Faith, Hope, and Love - A criticism of \nsoftware sci\u00adence's carelessness with regard to the human factor, To appear in Proceedings of Onward \n2010. [11] Hanenberg, S.: What is the Impact of Type Systems on Programming Time? - Preliminary Empirical \nResults, Workshop on Evaluation and Usability of Programming Languages and Tools (PLATEAU) at ONWARD \n2009. [12] Hanenberg, S. Doubts about the Positive Impact of Static Type Sys\u00adtems on Programming Tasks \nin Single Developer Projects -An Em\u00adpirical Study, Proceedings of ECOOP 2010, Springer, pp. 300-303. \n[13] Hanenberg, S.; Stuchlik, A.: What is the impact of type casts on development time? An empirical \nstudy with Java and Groovy, Sub\u00admitted, 2010. [14] Hudak, P.; Jones, M.P.: Haskell vs. Ada vs. C++ vs. \nAwk vs. .. An Experiment in Software Prototyping Productivity, technical report, Dept. of Computer Science, \nYale Univ., New Haven, Conn., July 1994.  [15] Juristo, N.; Moreno, A.: Basics of Software Engineering \nExperimen\u00adtation, Springer, 2001. [16] Kitchenham, B. et al.: Preliminary guidelines for empirical research \nin software engineering, IEEE Transactions on Software Engineer\u00ading, Volume 28 , Issue 8, 2002, pp. 721 \n- 734. [17] Kitchenham, B. et al.: Evaluating guidelines for empirical software engineering studies, \nInternational Symposium on Empirical Software Engineering, Rio de Janeiro, Brazil, 2006, pp. 38 47. \n[18] Lamport, L.; Paulson, L. C.: Should your specification language be typed, vol. 21, ACM, New York, \nNY, USA. 1999. [19] Pierce, B.: Types and Programming Languages, MIT Press, 2002. [20] Pfleeger, S. L., \nExperimental Design and Analysis in Software Engi\u00adneering Part 4: Choosing an Experimental Design, Software \nEngi\u00adneering Notes, vol 20, no 3, ACM, 1995 [21] Prechelt, L.: Kontrollierte Experimente in der Softwaretechnik: \nPotenzial und Methodik, Springer-Verlag, 2001. [22] Prechelt, Lutz: An empirical comparison of C, C++, \nJava, Perl, Py\u00adthon, Rexx, and Tcl for a search/string-processing program. Techni\u00adcal Report 2000-5, \nMarch 2000. [23] Prechelt, L., Tichy W.: A Controlled Experiment to Assess the Bene\u00adfits of Procedure \nArgument Type Checking, IEEE Transactions on Software Engineering 24(4), 1998, S. 302-312. [24] Svahnberg, \nM.; Aurum, A.; Wohlin, C.: Using Students as Subject, Proceedings of the 2nd Symposium on Empirical software \nengineer\u00ading and measurement, Kaiserslautern, Germany, pp. 288-290 . [25] Tichy, W.: Should Computer \nScientists Experiment More?, IEEE Computer 31(5): S. 32-40 [26] Tratt, L., Wuyts, R.: Dynamically Typed \nLanguages. IEEE Software 24(5), 2007, S. 28-30. [27] Wohlin, C., Runeson, P., H\u00f6st, M.: Experimentation \nin Software Engineering: An Introduction, Springer, 1999   \n\t\t\t", "proc_id": "1869459", "abstract": "<p>Although static type systems are an essential part in teach-ing and research in software engineering and computer science, there is hardly any knowledge about what the impact of static type systems on the development time or the resulting quality for a piece of software is. On the one hand there are authors that state that static type systems decrease an application's complexity and hence its development time (which means that the quality must be improved since developers have more time left in their projects). On the other hand there are authors that argue that static type systems increase development time (and hence decrease the code quality) since they restrict developers to express themselves in a desired way. This paper presents an empirical study with 49 subjects that studies the impact of a static type system for the development of a parser over 27 hours working time. In the experiments the existence of the static type system has neither a positive nor a negative impact on an application's development time (under the conditions of the experiment).</p>", "authors": [{"name": "Stefan Hanenberg", "author_profile_id": "81100540390", "affiliation": "University of Duisburg-Essen, Essen, Germany", "person_id": "P2354000", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1869459.1869462", "year": "2010", "article_id": "1869462", "conference": "OOPSLA", "title": "An experiment about static and dynamic type systems: doubts about the positive impact of static type systems on development time", "url": "http://dl.acm.org/citation.cfm?id=1869462"}