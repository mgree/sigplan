{"article_publication_date": "10-17-2010", "fulltext": "\n Instrumentation and Sampling Strategies for Cooperative Concurrency Bug Isolation * Guoliang Jin Aditya \nThakur Ben Liblit Shan Lu Computer Sciences Department University of Wisconsin Madison, Madison, WI, \nUSA {aliang,adi,liblit,shanlu}@cs.wisc.edu Abstract General Terms Experimentation, Reliability Fixing \nconcurrency bugs (or crugs) is critical in modern Keywords concurrency, statistical debugging, random \nsam\u00adsoftware systems. Static analyses to .nd crugs such as data pling, bug isolation races and atomicity \nviolations scale poorly, while dynamic approaches incur high run-time overheads. Crugs manifest 1. Introduction \n only under speci.c execution interleavings that may not arise during in-house testing, thereby demanding \na lightweight pro-1.1 Motivation gram monitoring technique that can be used post-deployment. Concurrency \nbugs (or crugs), such as data races [9, 12,We present Cooperative Crug Isolation (CCI), a low\u00ad 36] and \natomicity violations [13, 26], are among the most overhead instrumentation framework to diagnose production\u00ad \ntroublesome software bugs. The unique non-determinism run failures caused by crugs. CCI tracks speci.c \nthread inter\u00ad of crugs makes them dif.cult to expose during in-house leavings at run-time, and uses statistical \nmodels to identify testing. As a result, many crugs slip into production runs and strong failure predictors \namong these. We offer a varied suite manifest at user sites. Even worse, many crugs can cause of predicates \nthat represent different trade-offs between com\u00ad severe software failures, varying from data corruption \nto plexity and fault isolation capability. We also develop variant program crashes [16]. Crugs have caused \nreal-world disasters random sampling strategies that suit different types of predi\u00ad in the past, such \nas the Northeastern Blackout of 2003 [37].cates and help keep the run-time overhead low. Experiments \nGrowing use of concurrent programs on multi-core hardware with 9 real-world bugs in 6 non-trivial C applications \nshow means that software reliability is increasingly threatened that these schemes span a wide spectrum \nof performance by crugs. Tools for diagnosing production-run failures in and diagnosis capabilities, \neach suitable for different usage concurrent software are sorely needed. scenarios. To date, it has been \nextremely dif.cult to diagnose production-run software failures caused by crugs. Perfor- Categories \nand Subject Descriptors D.1.3 [Programming mance is the biggest challenge. Most prior crug detection \nTechniques]: Concurrent Programming parallel program\u00adtools either have huge overhead (10\u00d7 100\u00d7 slowdown \n[36]) ming; D.2.4 [Software Engineering]: Software/Program or require specialized hardware that does \nnot yet exist [26]. Veri.cation statistical methods; D.2.5 [Software Engineer-Accuracy is another challenge, \nas many previous detec\u00ad ing]: Testing and Debugging debugging aids tors [13, 36] have high false positive \nrates. For example, * Supported in part by AFOSR grants FA9550-07-1-0210 and FA9550-Narayanasamy et al. \n[31] show that only about 10% of real 09-1-0279; DoE contract DE-SC0002153; LLNL contract B580360; NSF \ndata races are harmful and could cause software failures. grants CCF-0621487, CCF-0701957, CCF-0953478, \nand CNS-0720565; Recently Marino et al. [29] and Bond et al. [5] smartly use and a Claire Boothe Luce \nfaculty fellowship. Any opinions, .ndings, and conclusions or recommendations expressed in this material \nare those of the sampling to improve the performance of race detection. Al\u00adauthors and do not necessarily \nre.ect the views of NSF or other institutions. though inspiring as a race detector, it unavoidably suffers \naccuracy and coverage problems in failure diagnosis; we discuss this further in Section 5.4 and Section \n6. Furthermore, it can be very hard for developers to reproduce .eld-detected Permission to make digital \nor hard copies of all or part of this work for personal or concurrent software failures which manifest \nonly under spe\u00ad classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation cial interleavings. \nEven if the bug-triggering input is known, on the .rst page. To copy otherwise, to republish, to post \non servers or to redistribute it may still take developers several days to reproduce a crug to lists, \nrequires prior speci.c permission and/or a fee. [32]. As a result, failure diagnosis is a nightmare for \nthe OOPSLA/SPLASH 10, October 17 21, 2010, Reno/Tahoe, Nevada, USA. Copyright cdevelopers of concurrent \nprograms. &#38;#169; 2010 ACM 978-1-4503-0203-6/10/10. . . $10.00  Thread 1 Thread 2 len = strlen ( \nstr ); memcpy(&#38;buf[cnt], str, len ); len = strlen ( str ); memcpy(&#38;buf[cnt], str, len ); cnt \n+= len; cnt += len; Figure 1: Atomicity violation bug from the Apache HTTP Server. The variables buf \nand cnt are both shared. Both threads are inside function ap buffered log writer . The Cooperative Bug \nIsolation project (CBI) aims to au\u00adtomatically diagnose production-run software failures with small run-time \noverhead [8, 23, 25]. It achieves this goal through three steps. First, it statically instruments a program \nat particular program points so as to monitor various predi\u00adcates on program state and behavior, such \nas variable value predicates (e.g., x > y) or the paths followed at conditional branches. Next, at run-time, \nit gathers feedback about pro\u00adgram execution by collecting these predicate samples, as well as corresponding \nlabels of execution results (success or fail\u00adure). Lastly, CBI performs statistical debugging: statistical \nanalysis of aggregated feedback data to identify program (mis)behaviors that are highly correlated with \nfailure. The CBI framework achieves low monitoring overhead through sparse random sampling of the instrumentation \nand by col\u00adlecting information from many user sites. Unfortunately, prior CBI work is poorly suited to \ndiag\u00adnosing software failures caused by crugs. The root causes of concurrency bugs fundamentally involve \nsequences of actions from multiple threads. They cannot be captured by predicates used in prior CBI work, \nwhich focus on one thread at a time. Figure 1 shows an example simpli.ed from a real-world crug in the \nApache HTTP Server [1]. Shared variable cnt is the tail index of shared buffer buf. Every thread appends \nlog messages to this buffer based on the index. Unfortunately, without proper synchronization, buffer \nupdates and index accesses from different threads can race with each other and lead to garbage data in \nthe log, as shown in Figure 1. Previous CBI tools fail to diagnose this problem. In our experiment, none \nof the standard predicates behaves differently in failing versus successful runs with at least 95% con.dence \n(CBI s standard acceptance threshold to counteract the effects of sampling noise). This is because the \nsoftware s misbehavior (garbage log data) can happen with normal variable values (e.g., cnt remains in \nbound) and normal execution paths, if considered only within individual threads. Thus we require new \ninstrumentation schemes in order to diagnose software failures due to crugs. 1.2 Contributions This \npaper presents Cooperative Crug Isolation (CCI), a low\u00adoverhead dynamic strategy for diagnosing production-run \nfailures in concurrent programs. Following the Cooperative Bug Isolation philosophy, CCI monitors interleaving-related \npredicates at run time; leverages sampling to keep the run\u00adtime overhead low; and uses statistical models \nto process run\u00adtime information aggregated through many runs and many users and identify the root causes \nof production-run failures. Applying the Cooperative Bug Isolation idea to crugs raises two major questions: \nWhat types of predicates are suitable for crug diagno\u00adsis? Instrumentation must balance failure-predictive \npower and computational simplicity. Poorly-designed predicates may be unable to explain any crug failures. \nYet costly predi\u00adcates must use low sampling rates in order to provide perfor\u00admance guarantees, thereby \ndelaying diagnosis. If the evalua\u00adtion of a predicate relies on long and continuous monitoring, it may \nbe unsuitable for sampling. How can we sample predicates that are related to concurrency bugs? Previous \nCBI work made predicate sampling decisions independently in each thread at each execution point. CCI \nsampling is much more complex. It may require cross-thread coordination, because crugs involve multiple \nthreads. It must also keep each sampling period active for some time, because crugs always involve multiple \nmemory accesses. Proper sampling design affects both the number of predicates that can be collected as \nwell as the correctness of the collected data. There may be no single solution to all of the above challenges. \nWe consider three different types of predicates together with new sampling strategies that support each: \n1. CCI-Havoc tracks whether the value of a memory loca\u00adtion is changed between two consecutive accesses \nfrom one thread. This captures the change of program states in the view of one thread at two nearby points. \nCCI-Havoc monitoring is supported by thread-independent and bursty\u00adstyle sampling in CCI. 2. CCI-FunRe \ntracks function re-entrance: simultaneous execution by multiple threads. This captures the interac\u00adtion \namong multiple threads at a coarse granularity. It is supported by thread-coordinated and unconditional \nsam\u00adpling. 3. CCI-Prev tracks whether two consecutive accesses to one memory location come from the \nsame thread or distinct threads. This captures interactions among multiple threads at a .ne granularity. \nIt is supported by thread-coordinated and bursty sampling.  These three schemes offer different types \nof information that may help diagnose crug failures. They provide differ\u00adent trade-offs between performance \nand failure-predicting  capability, and demonstrate different ways of sampling in concurrent programs. \nSpeci.cally, this paper makes the following contributions: A new suite of predicates that effectively \ndiagnose production-run failures in concurrent programs. Each re.ects a common type of root cause for \nsynchronization failure. Together, they span a wide spectrum of trade-offs between diagnostic capability \nand complexity.  A new suite of sampling schemes that support differ\u00adent types of crug instrumentation. \nInterleaving-related predicates are more complex than those used in previ\u00adous CBI work. CCI offers suitable \nsampling strategies to support proposed and future interleaving-related pred\u00adicates: thread-independent \nand thread-coordinated sam\u00adpling; bursty and non-bursty sampling; and conditional and un-conditional \nsampling.  A tool, CCI, that diagnoses production-run failures in concurrent programs. CCI provides: \n Low run-time overhead bene.ting from the sampling techniques developed herein. Low false positive rate \nbene.ting from statistical de\u00adbugging. Many previous detection tools [12, 13, 36] have high false positive \nrates, because races and atom\u00adicity violations can be benign. As a failure diagnosis tool, CCI is immune \nto these false positives, because its statistical analysis leverages information about whether a particular \nrun succeeded or failed and identi\u00ad.es those predicates whose values are truly correlated with observed \nfailure. Good diagnosis coverage bene.ting from the crug\u00adrelated predicates used in CCI. CCI-Prev predicates \ncapture data races and atomicity violations; CCI-FunRe predicates capture misuse of thread-unsafe functions; \nand CCI-Havoc predicates capture atom\u00adicity violations. These are among the most common causes of crugs \n[27]. We validate CCI by using it to identify root causes of real-world failures in several concurrent \nC applications, in\u00adcluding Apache [1], Cherokee, Mozilla, PBZIP2 [15], and the SPLASH-2 [44] benchmarks. \nExperimental results show that CCI is very effective, dramatically outstripping CBI, for crug diagnosis. \nTraditional CBI tools fail completely, providing no predictors for any of our buggy concurrent test subjects. \nThe predictors reported by CCI, however, accurately point to the root causes of a wide variety of failures. \nFurthermore, CCI achieves this excellent failure diagnosis with small run-time overhead (mostly within \n10%), thanks to its sampling mechanisms. The remainder of this paper is organized as follows. Sec\u00ad tion \n2 provides an overview of the CBI framework. Sections 3 and 4 respectively describe our instrumentation \npredicates and sampling strategies in detail. We present experimental results in Section 5 and discuss \nrelated work in Section 6. Section 7 concludes and suggests directions for future work. 2. Background \nCCI builds upon Cooperative Bug Isolation (CBI), a frame\u00adwork for lightweight instrumentation and statistically-guided \ndebugging [23, 25]. CBI collects information about program execution from both successful and failing \nruns and applies statistical techniques to identify the likely causes of software failures. 2.1 Data \nCollection Using Sampled Instrumentation CBI s instrumenting compiler uses source-to-source transfor\u00admation \nto add instrumentation code that monitors the values of predicates at particular program points, called \ninstrumen\u00adtation sites. In this paper, we denote recording the value of predicate p at instrumentation \nsite s as record(s, p). The tra\u00additional CBI framework tracks following types of predicates: Branches: \nEach branch is an instrumentation site. Two pred\u00adicates are associated with each site: one is true when \nthe true branch is taken, and the other is true when the false branch is taken. Returns: Each function \nreturn point is an instrumentation site. A set of three predicates at each site track whether the returned \nvalue is negative, zero, or positive. Scalar-pairs: At each assignment to a scalar variable x, one instrumentation \nsite is created for each other same-type in-scope variable y. Each such site has three predicates, recording \nwhether x is smaller than, larger than, or equal to y. Value comparisons between x and program constants \nare also added, one instrumentation site per constant. During execution, the instrumentation code collects \npred\u00adicate pro.les recording whether each monitored predicate was ever observed, and if observed, whether \nit was ever true. The feedback report from each run is a bit vector with two bits for each predicate \n(observed and true), plus one .nal bit indicating overall execution success or failure.  2.2 Data Analysis \nUsing Statistical Debugging CBI s statistical debugging models operate on large collec\u00adtions of feedback \nreports. The models assign a score to every available predicate and identify the best failure predictor \namong them. Intuitively, a good predictor should be both sen\u00adsitive (accounts for many failed runs) and \nspeci.c (does not mis-predict failure in successful runs). Thus, a good predictor is an instrumented \npredicate that is true in many failed runs but very few successful runs. CBI s scoring model considers \nboth sensitivity and speci.city to select top predictors. Prior work shows that the best predictor often \npoints to a bug that is responsible for many observed failures. An iterative ranking and elimination \nprocess continues to pick up the best remaining predicate to explain the remaining failures until all \nfailures are explained or all available predicates are discarded. We omit a detailed discussion of these \nstatistical models here, as they are identical to those used in prior CBI work. The main focus of the \npresent research is how to collect informative raw data ef.ciently in the .rst place.  Monitoring overheads \nmust be very low for this approach to be feasible in post-deployment environments. The CBI framework \nachieves this goal through sparse random sam\u00adpling. At run time, each time an instrumentation site is \nreached, a Poisson (memory-less) random choice decides whether or not the predicate information associated \nwith that site will be collected. In this paper, [[ instr ; ]]? rep\u00adresents the random sampling of \nthe instrumentation instr . Sparse sampling means that most instrumentation code is not run, and therefore \nmost run-time events are not actually observed. However, sampling is statistically fair, so the small \namount of data that is collected is an unbiased representation of the complete-but-unseen data. Therefore, \ngiven a large number of user runs and appropriate statistical models, the root causes of failure emerge \nas consistent signals through the sparsely-sampled noise.  2.3 Practical Suitability for Concurrency \nBugs Prior CBI work has not focused directly on concurrency\u00adrelated issues. There are good reasons to \nbelieve, however, that the two can be a sensible, natural .t in practice. Analysis of feedback reports \ndoes not depend on the exact failure location (e.g., stack trace). The statistical models can diagnose \nboth fail-stop errors as well as nonfatal bugs which allow execution to continue. They only require outcome \nlabels marking each run as successful or failed. If one can only recognize crashes as failures, then \nwe can diagnose crugs that lead to crashes. If one can automatically recognize corrupted output, then \nwe can diagnose crugs that corrupt output. If users must manually .ag runs as failed, then only crugs \nthat suf.ciently disturb users will be diagnosed. Thus, statistical debugging is pay as you go : developers \nreceive diagnoses of whatever bugs they can recognize when they arise. This was true in prior CBI work \nand it remains true in CCI. Conversely, the statistical models automatically ignore benign race conditions \nthat never cause noticeable failures. This is a key bene.t of our approach. The sampled nature of CBI \nand CCI instrumentation might be seen as a limitation: if crugs are rare, then sampling could miss key \nclues. However, we see this as a strength. Even if crugs manifest less frequently than sequential bugs, \nCCI can help diagnose them in the .eld as long as the bugs have caused a suf.cient number of failures \njust like CBI diagnoses sequential bug failures. For deployed software, diagnosis preference is usually \ngiven to those bugs (whether sequential or concurrent) that have manifested and bothered a suf.cient \nnumber of users. Statistical debugging excels in exactly this scenario. Indeed, it would be quite reasonable \nto deploy a program with both CBI and CCI instrumentation, then let bugs compete for developer attention \nvia the single statistical model that CBI and CCI both share. Thread 1 Thread 2 mut = NULL; S: unlock(mut); \n(a) Failing run: localS = false, remoteS = true lock(mut); S: unlock(mut); mut = NULL; (b) Correct run: \nlocalS = true, remoteS = false Figure 2: State of CCI predicates in two different thread interleavings. \nAbove code is simpli.ed from a data race bug from PBZIP2 in which thread 1 nulli.es the shared mutex \nvariable, mut, when thread 2 is still using mut. 3. CCI Instrumentation Schemes CCI does not gather anything \napproaching a complete trace at run time. Rather, it collects just a small amount of potentially\u00adinformative \ndata that is readily available with minimal over\u00adhead. The format of such information (i.e., predicates) \nmust be designed carefully, taking the following factors into ac\u00adcount: Simplicity. Simpler predicates \nallow more intensive sam\u00adpling without causing excessive slowdown during produc\u00adtion runs. Failure-predictive \ncapability. Predicates must re.ect com\u00admon root causes of crugs in order to explain crug failures during \nproduction runs. Suitability for sampling. Sampled feedback data is incom\u00adplete. This limitation can \nmake some types of predicates hard to evaluate. We discuss this further in Section 4.3. It is unlikely \nthat a single instrumentation strategy can satisfy all of these requirements. Therefore, in CCI, we design \nand explore three types of predicates that represent different design trade-offs: CCI-Prev, CCI-FunRe, \nand CCI-Havoc. The remainder of this section discusses the design of these three instrumentation schemes \nand how to monitor these predicates at run time without concern for sampling. The next section develops \nsampling strategies suitable for use with these three instrumentation schemes. 3.1 CCI-Prev Scheme CCI-Prev \ntracks whether two successive accesses to a given location were by two distinct threads or were by the \nsame thread both times. 3.1.1 CCI-Prev Instrumentation Sites CCI-Prev monitors each instruction I that \nmight access a shared location g. Each such instruction can exhibit two pos\u00adsible behaviors at run time: \neither the thread now accessing g at I was the same thread that accessed g previously, or  1 lock(glock); \n2 changed = test and insert(&#38;g, curTid); 3 record(s, changed); 4 access(g); 5 unlock(glock); Figure \n3: CCI-Prev instrumentation. The original program code, represented here as access(g) on line 4, was \neither a read or a write of possibly-shared memory location g. the previous access was by a different \nthread. In CBI terms, we say that I constitutes a single instrumentation site with two predicates: localI \nis true if the previous access was from the same thread, while remoteI is true if the previous access \nwas from a different thread. Each time instruction I is exe\u00adcuted, exactly one of these two predicates \nmust be true, and the other false. For example, when execution follows Fig\u00ad ure 2a, CCI records that \nremoteS is true. Conversely, when the interleaving follows Figure 2b, localS is true.  3.1.2 Diagnostic \nPotential of CCI-Prev CCI-Prev predicates are closely tied to the root causes of crugs, such as atomicity \nviolations and data races. Most atom\u00adicity violation bugs happen when one thread s consecutive accesses \nto a shared variable are non-serializably interleaved with accesses from a different thread [26, 42]. \nThe Apache bug shown in Figure 1 is one such example: two consecutive read accesses to cnt in thread \n2 are interleaved. Our remoteI predicates capture these bad interleavings. Data races occur when con.icting \naccesses from different threads touch the same shared variable without proper synchronization. The PBZIP2 \nbug shown in Figure 2 is a typical example of a data race bug, and likewise can be recognized using CCI-Prev \npredicates. CCI-Prev predicates are de.nitely not true atomicity\u00adviolation or race detectors. Extra information, \nincluding memory access type (i.e., read vs. write) and synchronization (i.e., when and which locks are \nacquired and released), would be needed to precisely record atomicity violations and data races. We intentionally \nignore this information to keep instrumentation simple, following the CCI design principles mentioned \nabove. 3.1.3 CCI-Prev Instrumentation Strategy CCI-Prev instrumentation requires recording which thread \nissued the latest access to each shared memory location at run-time. Figure 3 illustrates the instrumentation \ninserted by CCI at compile time. This instrumentation interacts with a global last-access hash table \nthat stores mappings from memory ad\u00addresses to the ID of the thread that last accessed each memory location. \nThis hash table is checked and updated at every in\u00adstrumentation point by function test and insert on \nline 2. As a result of test and insert , the local variable changed is set to true if the entry in the \nhash table corresponding to &#38;g does not equal curTid (i.e., the ID of the current thread), and to \nfalse otherwise. In addition, test and insert inserts the entry &#38;g . curTid into the last-access \nhash table. Subsequently, the value of changed is used to record CCI-Prev predicates as shown on line \n3. Also note that in this scheme the variable glock is a global lock that is used to synchronize accesses \nto the entire hash table and ensure that the instrumented code appears to execute atomically. Notice \nthat this global lock will never cause previously impossible program interleavings or deadlocks. Future \nwork can further optimize CCI-Prev using lock-free hash tables [18] or .ne-grained locks. One practical \nchallenge in our static instrumentation is that a single code statement might access multiple possibly-shared \nlocations, and thereby require multiple hash table inserts and look-ups. We solve this by automatically \nrewriting complex source statements as multiple, equivalent, simpler statements which each accesses at \nmost one possibly-shared location. Our current implementation uses an extremely lightweight analysis \nto decide which accesses touch possibly-shared memory locations. We do not instrument accesses to const\u00adquali.ed \ndata. We instrument every direct access to a named global variable that is not declared as thread-local \nor a named local variable whose address is taken anywhere in the containing function. Finally, we instrument \nevery indirect access through a pointer. Future work can use escape analysis to prune out unnecessary \ninstrumentation sites and further improve CCI s performance.  3.2 CCI-FunRe Scheme CCI-FunRe tracks \nwhether the execution of a function F overlaps with the execution of F from a different thread. 3.2.1 \nCCI-FunRe Instrumentation Sites CCI-FunRe monitors each function F. Each execution of F can exhibit two \npossible behaviors at run time: either no other thread executes F during the current thread s execution \nof F, or there exists another thread that executes F simultaneously. We say that F yields a single instrumentation \nsite with two predicates: ReentF is true if the execution of F overlaps with the execution of F from \nanother thread, while NonReentF is true if there is no such overlap. Each time function F is executed, \nexactly one of these two predicates must be true, and the other false. For example, Reentap buffered \nlog writer is true for the interleaving shown in Figure 1.  3.2.2 Diagnostic Potential of CCI-FunRe \nCCI-FunRe captures thread-interaction at function-level gran\u00adularity. Since this granularity is much \ncoarser than the access\u00adlevel granularity adopted by CCI-Prev, CCI-FunRe has lower overhead than CCI-Prev \nunder the same sampling rate. CCI-FunRe covers a common pattern of crugs: a function is not thread-safe \nbut is invoked by multiple threads concur\u00adrently. The Apache bug shown in Figure 1 is one such ex\u00ad ample. \nFunction ap buffered log writer updates the global log of Apache server. It should never be executed \nby mul\u00ad  1 F (...) { 2 if (( local FCount++)==0) 3 oldFCount = atomic inc(FCount); 4 record(s, oldFCount); \n5 ... 6 if ((--local FCount)==0) 7 atomic dec(FCount); 8 } Figure 4: CCI-FunRe instrumentation. Line \n5 represents the original function body, transformed so that all exits from the function pass through \nline 7. tiple threads at the same time. The buggy implementation leads to logging failure when this function \nis re-entered. The Reentap buffered log writer predicate captures this mistake. Of course, CCI-FunRe \ns course granularity may limit accuracy. If function F performs multiple tasks, some parts of F may be \nsafely reentrant while others are not. In this case, CCI will observe that ReentF is true whenever the \nprogram fails, but ReentF being true does not ensure failure. CCI can leverage such information to zoom \ninto function F and get more accurate failure predictors. Section 5.2.2 discusses an example of this \nsort of diagnosis, currently performed manually, but potentially amenable to automation.  3.2.3 CCI-FunRe \nInstrumentation Strategy In order to track re-entrance predicates, CCI maintains a global counter for \neach function F to indicate how many threads are executing F right now. This counter is updated at the \nentrance and the exit of F, as shown on lines 3 and 7 of Figure 4. A per-thread counter local FCount \nis also maintained to prevent a thread from updating the global counter multiple times during recursive \ncalls. Based on the global counter s old value (oldFCount) at function entry, CCI records a single observation \nof the ReentF and NonReentF predicates on line 4. The counter update is implemented using atomic instructions. \n 3.3 CCI-Havoc Scheme CCI-Havoc tracks whether the value of a given shared loca\u00adtion changes between \ntwo consecutive accesses by one thread. Its name refers to the value-scrambling havoc x statement of \nElmas et al. [11]. 3.3.1 CCI-Havoc Instrumentation Sites CCI-Havoc monitors each instruction I that might \naccess a shared location g. Each such instruction can exhibit two possible behaviors at run time: either \nthe value in g has not changed since this thread s last access to g, or the value has changed. We say \nthat I constitutes a single instrumentation site with two predicates: ChangedI is true if the current \nvalue of g is different from its old value right after this thread s last access to g, while UnchangedI \nis true if the prior and current values are the same. Each time I is executed, CCI records a single true \nobservation of either ChangedI or UnchangedI . In the example from Figure 1, if len is nonzero in thread \n1, then Changedcnt += len is true in thread 2 because cnt has changed its value since the previous access \nby thread 2. In the example from Figure 2b, UnchangedS is true, since mut has the same value seen in \nthe previous access by thread 2. 3.3.2 Diagnostic Potential of CCI-Havoc Each CCI-Havoc predicate is \nonly concerned with the state of the program as observed by a single thread. No cross-thread coordination \nis required. This makes CCI-Havoc instrumen\u00adtation much simpler than that required for CCI-FunRe and \nCCI-Prev, and is similar to traditional CBI instrumentation. Unlike traditional CBI predicates, CCI-Havoc \npredicates are closely related to atomicity violations, the most common root causes of crugs [27]. Previous \nstudies [26, 32, 42] have shown that all atomicity violations that involve single variable can be categorized \ninto four cases, shown in Figure 5. Three of these four cases are accurately captured by some CCI-Havoc \npredicate.  3.3.3 CCI-Havoc Instrumentation Strategy CCI-Havoc instrumentation requires tracking the \nvalue held in each memory location right after each thread s last access of that location. Given this \ninformation, CCI can evaluate CCI-Havoc predicates when needed as the instrumented program runs. Figure \n6 illustrates the instrumentation inserted by CCI at compile time. The instrumentation performs two tasks: \n(1) evaluating predicates on the present state, and (2) updating the history information for future use. \nLines 1 and 2 of Figure 6 perform the .rst task. Function test uses a per-thread hash table to .nd the \nold value stored at g right after the last access to g from the current thread. The comparison between \nthe old and the current values stored at g sets the local variable changed to true if and only if g has \nbeen changed. Subsequently, the value of changed is used to record CCI-Havoc predicates as shown on line \n2. The insert call on line 4 updates the same per-thread hash table to map &#38;g . g, i.e., to record \nthe current value of g as the last value seen by this thread at address &#38;g. Note that, unlike CCI-Prev \nand CCI-FunRe, all informa\u00adtion used by CCI-Havoc is thread-local. Therefore, evaluating CCI-Havoc predicates \nrequires no locking or synchronization of any kind, which helps the performance and scalability of CCI-Havoc \nas shown in Section 5. Actually, there is a (be\u00ad nign) race in our instrumented code: theoretically, \nthe value of g could be changed by other threads between lines 1 and 3. Fortunately, this type of low-probability \nnoise is easily han\u00addled by CCI s supporting statistical models [25] and therefore does not perceptibly \naffect CCI s failure diagnosis. The current implementation of CCI-Havoc uses the similar lightweight \nanalysis as that in CCI-Prev to decide which are possibly-shared memory locations. This can be enhanced \nby escape analysis in the future.  Thread 1 Thread 2 Thread 1 Thread 2 Thread 1 Thread 2 Thread 1 Thread \n2 A: read g A: read g A: write g A: write g C: write g C: write g C: write g C: read g B: read g B: write \ng B: read g B: write g Case 1: read followed by read, in-Case 2: read followed by write, in-Case 3: write \nfollowed by read, in-Case 4: write followed by write, terleaved by a write terleaved by a write terleaved \nby a write interleaved by a read Figure 5: Possible cases of single-variable atomicity violation. Cases \n(1) (3) are captured by ChangedB predicates. 1 changed = test(&#38;g, g); 2 record(s, changed); 3 access(g); \n4 insert(&#38;g, g); Figure 6: CCI-Havoc instrumentation. The original program code, represented here \nas access(g) on line 3, was either a read or a write of possibly-shared memory location g. 4. CCI Sampling \nSampling can be highly effective in achieving the low over\u00adheads that production usage demands. The challenge \nis to .nd suitable sampling strategies for each type of predicate. A care\u00adless or inappropriate approach \nto sampling cannot guarantee the correctness of predicate evaluation and may sample 90% of the execution \nwith only 10% of the predicates collected. Like CBI, CCI randomly decides which code regions to sample \nat run time. The sampling rate can be adjusted to control the imposed overhead. Unlike CBI, CCI sampling \nmust make several extra decisions to maintain correctness and good coverage: Thread-coordinated versus \nindependent sampling. When using predicates (e.g., CBI, CCI-Havoc) that concern them\u00adselves with data \nand control activity within single threads, each thread can make local, independent decisions about when \nto start/stop the sampling. However, CCI predicates that monitor interactions among multiple threads \nrequire inter\u00adthread coordination. Length of each sampling period. In CBI, each sampling period only \nlasts for one statement. This works well for CBI predicates, as each observes the program state at one \nexecu\u00adtion point. However, this is not suitable for CCI predicates that require considering multiple \nexecution points together. Correctness of predicate evaluation. Monitoring interleav\u00ading patterns may \nrequire history information (e.g., regarding the previous access to this variable). Unfortunately, sampling \nnever presents a complete history of program execution. Cor\u00adrect evaluation of certain predicates demands \na hybrid ap\u00adproach with both unconditional and sampled instrumentation. The remainder of this section \ndiscusses the design and implementation of appropriate sampling strategies for the three types of CCI \npredicates presented earlier. 1 2 if (gsample) {lock(glock); 3 changed = test and insert(&#38;g, curTid, \n&#38;stale); 4 record(stale ? s1 : s2, changed); 5 access(g); 6 gLength++; 7 unlock(glock); 8 lLength++; \n9 if (( iset == curTid &#38;&#38; lLength > lMAX) 10 11 || gLength > gMAX) {clear (); 12 iset = unusedTid; \n13 gsample = false; 14 15 16 }} else {access(g); 17 [[ gsample = true; iset = curTid; lLength=gLength=0;]]? \n18 } Figure 7: Sampled CCI-Prev instrumentation. [[. . . ]]? marks a block of code that is randomly \nsampled using tradi\u00adtional CBI sampling methods. 4.1 CCI-Prev Sampling CCI-Prev requires thread-coordinated, \nbursty sampling. Recall that CCI-Prev detects intervening accesses by any other active thread. Thus, \nwe must activate sampling at roughly the same time in all threads in order to collect accurate CCI-Prev \npredicates. If sampling were not thread\u00adcoordinated, then any non-sampling thread could sneak in and \naccess shared data without notifying other sampling threads that it had done so. To implement thread-coordinated \nsampling, CCI uses one shared global variable gsample to control whether to run instrumentation in all \nthreads. Once gsample is set/unset in one thread, all threads begin/end their sampling. Figure 7 shows \nhow the basic instrumentation from Figure 3 is aug\u00ad mented with sampling. CCI uses the basic random sampling \nframework of CBI to set gsample at line 17 to turn on sam\u00ad pling. Once sampling is turned on the instrumentation \ncode at lines 2 14 is enabled in all threads. When sampling is turned off, lines 16 17 are executed. \nThe length of each sampling period is critical for CCI-Prev sampling. A sampling period must last long \nenough time to cover at least one pair of consecutive accesses to a shared memory location in order to \naccurately record one CCI-Prev predicate. However, the sampling period cannot be too long, or else performance \nwill suffer. Park et al. [32] have shown that temporal locality exists in crugs, especially atomicity \nviolation bugs. One previous study has observed that the atomic regions involved in typical atomicity \nviolation bugs range from 500 to 750 instructions [28]. Based on this, our current CCI prototype uses \n.xed thresholds to end sampling periods: 100 global accesses executed by the thread that starts the sampling \nor 10,000 global accesses executed by all threads, whichever occurs .rst. Of course, there could be other \nschemes to decide when to stop a sample period, such as variants of Hirzel and Chilimbi s bursty tracing \n[19]. We plan to explore these alternatives in the future.  Lines 9 14 of Figure 7 demonstrate how to \nend a sampling period. Our implementation keeps the identity of the thread that turns on sampling by \na global variable iset , which is set at line 17 along with gsample. That thread can turn off the sampling \nby clearing the gsample .ag when the per\u00adthread sampling length threshold is reached, as shown on line \n9. Other threads can also turn off the sampling if the global sampling length threshold is reached .rst, \nas shown on line 10. Finally, we need to revise predicate evaluation to maintain correctness in the presence \nof sampling. Speci.cally, we need to differentiate information collected during earlier sampling periods \n(referred to as stale information) and that during the current period (referred to as fresh information). \nWhen an instruction I conducts the .rst access to a memory location g during a sampling period, CCI does \nnot have fresh information about preceding accesses to g and cannot guarantee the correctness of remoteI \nand localI . Our implementation uses a generation counter to differen\u00adtiate fresh from stale entries \nin CCI-Prev s hash table. The generation count of each new entry is set to be the current generation \ncounter at the time of insertion. At the end of every sampling period, function clear is called (line \n11 in Figure 7) to increment the generation counter. Thus, a hash table entry is stale if its generation \ncount is smaller than the current generation counter. Only fresh entries are used to update remote and \nlocal predicates. To fully leverage the sampled information, each in\u00adstrumented instruction maintains \na secondary instrumentation site which considers stale hash table entries. Our rationale is that wrong \npredicates will be pruned out with high probabil\u00adity through CBI-style statistical analysis anyway. Therefore, \nkeeping these secondary predicates can exploit more run-time information without increasing false positives. \nThis design de\u00adcision affects lines 3 4 in Figure 7. Function test and insert sets stale to true if the \nentry corresponding to &#38;g is stale. Instrumentation site s1 uses the stale entry and records the \npredicate, while site s2 always uses information gathered in the current sampling period. 1 F (...) { \n2 if (( local FCount++)==0) 3 oldFCount = atomic inc(FCount, curTid); 4 [[ record(s, oldFCount); ]]? \n5 ... 6 if ((--local FCount)==0) 7 atomic dec(FCount, curTid); 8 } Figure 8: Sampled CCI-FunRe instrumentation. \n[[. . . ]]? marks a block of code that is randomly sampled using traditional CBI sampling methods.  \n4.2 CCI-Havoc Sampling CCI-Havoc sampling requires thread-independent, bursty sampling. CCI-Havoc predicates \ncompare the values of one memory location at two nearby execution points in one thread. Since each predicate \nonly cares about one thread, the sampling decision can be made independently by each thread, as in CBI. \nSince each predicate involves more than one execution point, each sampling period needs to reach certain \nlength, as in CCI-Prev. In our current prototype, guided by previous study of typical atomic-regions \nlength [28], each sampling period ends when the sampling thread conducts more than 100 accesses to shared \nmemory locations. The implementation of CCI-Havoc sampling uses tradi\u00adtional CBI mechanism to randomly \nturn on a thread-local variable that indicates sampling is active. Subsequently, a thread-local counter \nis incremented whenever this thread ac\u00adcesses a shared memory location. Sampling is turned off when the \ncounter exceeds the prede.ned 100-access threshold. The thread-local hash table that hold mappings from \nmemory lo\u00adcations to their old values is periodically .ushed in a similar way to that done for the global \nhash table in CCI-Prev.  4.3 CCI-FunRe Sampling CCI-FunRe requires a mixture of unconditional and CBI-style \n(thread-independent, non-bursty) sampled instrumentation. The special challenge of CCI-FunRe is that \nwe must instrument all invocations of F in order to guarantee the correctness of ReentF . This challenge \nis different from CCI-Prev and CCI-Havoc, where correctness is guaranteed as long as fresh information \nis used. For CCI-FunRe, when fresh information indicates that ReentF is false, this may not re.ect reality, \nbecause an non-sampled invocation to F may have occurred before the current sampling period began. This \nproblem has two potential solutions. The .rst is to statically divide functions to groups and monitor \nall invoca\u00adtions of functions from one group at each user s site. The disadvantage of this approach is \nthe burden of deploying differently-instrumented executables for different users. The second solution \nis to perform unconditional (always-active) instrumentation for all function invocations (lines 3 and \n7 in Table 1: General characteristics of buggy test subjects. KLOC is total program size in thousands \nof lines of code.  Runs Program KLOC Symptoms Total Failed Apache-1 Apache-2 Cherokee FFT LU Mozilla-JS-1 \nMozilla-JS-2 Mozilla-JS-3 PBZIP2 333 333 83 1.3 1.2 107 107 107 2.1 corrupted log crash corrupted log \nwrong output wrong output crash wrong output crash crash 3,000 3,000 3,000 3,000 3,000 3,000 3,000 3,000 \n3,000 1,372 1,566 1,705 1,766 1,179 1,660 1,493 1,507 1,350 Figure 8) and only apply sampling to predicate \nevaluation and recording (line 4). The advantage is that only one instru\u00ad mented executable is needed \nand the overhead of production\u00adrun monitoring can be adjusted via the sampling rate as ef\u00adfectively as \nbefore. The disadvantage is that unconditional overhead due to lines 3 and 7 cannot be controlled through \nthe sampling rate. We use the second solution in our current prototype, and leave the .rst for future \nwork. 5. Experimental Evaluation 5.1 Methodology We have implemented CCI and evaluated it to answer two \nkey questions: (1) how accurate is CCI in reporting root causes of concurrent program failures, and (2) \nwhat is the performance overhead of CCI monitoring? To get a better understanding of CCI s failure diagnosis \ncapability, we tried two conven\u00adtional CBI instrumentation schemes for comparison: one that records the \ndirections of conditional branches, and one that monitors the relative values of same-typed pairs of \nscalar variables. To assess the effectiveness of CCI sampling, we compared the performance of CCI with \nand without sampling. To demonstrate the difference between race detection and CCI failure diagnosis, \nwe also did experiments with Helgrind [2], a state-of-practice race detector. Our experiments were carried \nout on quad-core Intel machines using several widely used C applications with real crugs. In all experiments, \nwe added randomly-executed thread yield calls in the source code in order to make the program fail more \nfrequently. These random yields will not affect the quality of CCI evaluation; they merely change the \nratio of successful to failing runs. In practice, failures would be much less common in the .eld. However, \nas enough failures have occurred, one could analyze these failures along with a randomly-selected subset \nof successes to achieve a similar mix. Table 1 shows some characteristics of the buggy test subjects \nand the experimental runs. The Mozilla-JS test subject is a standalone JavaScript engine from the Mozilla \nweb browser. Program CBI CCI-Prev CCI-Havoc CCI-FunRe Apache-1 -/ 1 / 1 / 1 Apache-2 -/ 1 / 1-Cherokee \n--/ 2-FFT -/ 1--LU -/ 1-\u00adMozilla-JS-1 --/ 2 / 1 Mozilla-JS-2 -/ 1 / 1 / 1 Mozilla-JS-3 -/ 2 / 1 / 1 PBZIP2 \n-/ 1 / 1- Table 2: Overall failure diagnosis results. / n indicates that the nth highest ranked predictor \ncaptures the root cause, while - indicates that neither of the top two predictors is useful. We target \nan effective sampling rate close to 1/100 as recommended by Liblit et al. [24]. Speci.cally, for CCI-Prev \nand CCI-Havoc that use 100-access bursty sampling periods, we use start sampling periods with probability \n1/10,000. For CCI-FunRe, which uses single-sample bursts , the sampling rate is exactly 1/100. We use \nthe iterative ranking and elimination model of Liblit et al. [25] to mine collected data for failure \npredictors. Failure predictors discovered by the statistical model must be correlated with a positive \nincrease in failure likelihood with at least 95% con.dence. Tables 3 5 visualize analysis results using \nbug thermome\u00adters, one per predictor selected by the statistical model [25]. The width of a thermometer \nis logarithmic in the number of runs in which the predicate was observed. The black band on the left \ndenotes the context of the predicate: the probability of failure given that the predicate was observed \nat all, regardless of whether it was true or false. The dark gray or red band denotes the 95%-certain \nincrease in the probability of fail\u00adure given that the predicate was true. The light gray or pink band \nshows the additional increase that is estimated but not at least 95% con.dent. A large dark gray/red \narea indicates that the predicate being true is highly predictive of failure, and a small light gray/pink \nband indicates that this prediction carries high con.dence. Any white space at the right edge of the \nband indicates the number of successful runs in which the predicate was observed to be true: a measure \nof the bug predictor s non-determinism. Complete analysis results allow some interactivity, and include \nmore details such as the precise source .le name and line number on which the predictor was observed. \nThese features are helpful in a programmer s hands, but we omit them here to simplify presentation. \n 5.2 Failure Diagnosis Results 5.2.1 Overall Results Table 2 shows the overall failure diagnosis results \nof three CCI schemes together with a baseline CBI scheme, all under the roughly 1/100 effective sampling \nrate described in Section 5.1. We consider diagnosis successful if either of the topmost two predictors \nclearly describes the conditions for failure and would lead a developer directly to the bug.  As we \ncan see, CCI-Prev, CCI-Havoc, and CCI-FunRe can all help diagnose real-world crugs. Their top-ranking \npredictors can help explain 7, 7, and 4 out of the tested 9 crug failures, respectively. By contrast, \nthe baseline CBI cannot diagnose any crug failures. In some cases (Apache-1, Mozilla\u00adJS-1, and LU), all \nconventional CBI predicates are eliminated due to low(< 95%) con.dence that they behave differently in \nfailing versus successful runs. In other cases, some CBI predicates have statistical correlation with \nfailure, but none of them, no matter how high-or low-ranked, are relevant to the failures. This af.rms \nour earlier claim that conventional CBI is ill-equipped to diagnose crugs. Among the three CCI schemes, \nthe failure diagnosis ca\u00adpabilities of CCI-Prev and CCI-Havoc are especially good. CCI-Prev can diagnose \nnot only atomicity violation bugs (Apache-1, Apache-2, Mozilla-JS-2, and Mozilla-JS-3) but also order \nviolation bugs and races (FFT, LU, and PBZIP2). CCI-Havoc cannot detect order-violation bugs, but it \nsuccess\u00adfully diagnoses all atomicity violation bugs in our experi\u00adments with high con.dence. CCI-FunRe \nshows the weakest diagnosis capability due to its coarse granularity. In order to understand whether \nsampling has any impact on above diagnosis results, we repeat the experiments without sampling. We .nd \nexactly the same results except for two cases: the Mozilla-JS-1 failure can be successfully diagnosed \nby CCI-Prev without sampling; the Mozilla-JS-3 failure can be successfully diagnosed by CBI without sampling. \nFor CCI-Prev, the reason is that the Mozilla-JS-1 failure involves a shared memory access and a remote \npreceding access that are so far away from each other that they do not .t into one bursty sampling period. \nThis shows that the .xed size bursty windows could lead to false negatives in some cases. As mentioned \nin Section 4.1, future work can randomize the bursty-sampling window size within some suitable distribution. \nIn all other cases, sampling never hurts CCI s diagnoses, thanks to the statistical models previously \ndeveloped by Liblit et al. [25]. For the CBI case, the reason is that some program sites and the corresponding \npredicates may not reach statistical signi.cance under sparse sampling. Thus, all but two - marks in \nTable 2 are caused not by the information loss due to sampling, but rather by the inherent limitations \nof each instrumentation scheme. We explore this further below.  5.2.2 Case Studies We now use case studies \nto explain the differing diagnosis capabilities of CCI-Prev, CCI-Havoc, and CCI-FunRe. We examine just \na few of the bugs from our experiments in detail here; Sections 5.2.1 and 5.3 respectively report diagnosis \nand performance results across the entire suite of buggy programs. Apache HTTP Server Case 1 is a case \nwhere all three CCI schemes successfully diagnose the failure. In this experiment we use CCI to diagnose \na non\u00addeterministic log corruption problem in Apache. This bug (illustrated in Figure 1) was originally \nreported by Apache users Sussman and Trawick [40] on the Apache Bugzilla bug tracker. Our experiment \nconsists of 3,000 runs based on this bug report. Each run starts the Apache HTTP Server, downloads two \n.les in parallel ten times, then stops the server. Each run is labeled as a failure if the server s log \n.le is corrupted and a success otherwise. Table 3 lists the top-ranked bug predictor from each CCI scheme. \nAs we can see, all CCI schemes successfully identify the root cause of this Apache failure. CCI-Prev \npoints out that the failure is related to an unexpected remote access to cnt preceding the read of cnt \nat cnt += len in function ap buffered log writer . CCI-Havoc points out that the failure is related to \nan unexpected change to cnt between its two uses in this same function; CCI-FunRe identi.es reentrant \nexecution of this same function as the reason for the failure. This Apache failure and the three Mozilla \nfailures are the four failures in our experiments that CCI-FunRe can successfully diagnose. In all cases, \nthe involved functions are relatively short. Each performs a simple task (e.g., write to the global log) \nthat should not be carried out without synchronization. Cherokee is a case where only CCI-Havoc successfully \ndiagnoses the failure. Table 4 shows the top two predictors of CCI-Havoc. Actually, the .rst predictor \npoints to a crug that we were previously unaware of; the second predictor successfully explains the random \nlog corruption problem that we tried to diagnose. Figure 9 illustrates this bug, originally reported \nby Chero\u00ad kee users. Its symptom is non-deterministic log corruption, similar to that of the Apache bug \ndiscussed above. However, only CCI-Havoc successfully diagnoses Cherokee s variant of this problem. As \nFigure 9 shows, the Cherokee log (g buf) is corrupted whenever the buffer index g buf->len is modi\u00ad.ed \nbetween S1 and S2. CCI-Havoc correctly identi.es the failure predictor as shown in Table 4. CCI-Prev \nwas unable to identify remoteS2 as a failure pre\u00addictor because S2 could be preceded by a read to g buf->len \nfrom a different thread, which will not cause the failure. Ignor\u00ading the access type of preceding accesses \ncauses this false neg\u00adative. Future work can extend CCI-Prev to collect four types of predicates remote \n-write, remote -read, local -write, and local -read, in order to diagnose failures similar to this Cherokee \none. CCI-FunRe did not identify function update guts as a failure predictor, simply because update guts \nis a large, com\u00adplex function. The majority of its actions do not use g buf and can be safely executed \nby multiple threads simultaneously. CCI-FunRe could diagnose this failure if its monitoring gran\u00adularity \nwere smaller than one function. We tried splitting update guts to several monitoring blocks using preexisting \n Scheme Thermometer Predicate Function CCI-Prev CCI-Havoc CCI-FunRe cnt remote -read cnt changed -read \nreentrant call ap buffered log writer ap buffered log writer ap buffered log writer Table 3: Top predictor \nfrom each scheme for Apache HTTP Thermometer Predicate Function buf2->len changed -read g buf->len changed \n-read buffer add buf update guts Table 4: Top CCI-Havoc predictors for Cherokee Thread 1 Thread 2 int \nupdate guts(void) int update guts(void) {{ ... ... S1: g buf->len = 0; S1: g buf->len = 0; S2: memcpy( \ng buf->buf+g buf->len, str , count); S3: g buf->len += count;  S2: memcpy( g buf->buf+g buf->len, str \n, count); S3: g buf->len += count; ... ... }} Figure 9: Simpli.ed illustration of the Cherokee bug return \nstatements as block-boundaries This lets CCI-FunRe uncover an accurate failure predictor. Our splitting \nwas per\u00adformed manually, but could certainly be automated as part of a process of iterative instrumentation \nre.nement. FFT and LU from SPLASH-2 are two cases where only CCI-Prev successfully diagnoses the failures. \nFailures in these two applications are non-deterministic wrong outputs of timing statistics. They are \ncaused by order\u00adviolation bugs, introduced by wrong implementation of the WAIT FOR END macro in the c.m4.ia32 \n.le used. The missing macro allows a race between two sets of operations: (1) assignments at the end \nof the worker thread to global variables that maintain the timing information, and (2) the printing of \nthese global variables at the end of the parent thread. Our experiment consists of running the given \nprogram 3,000 times and marking each run as a failure if the printed execution .nish-time is invalid \n(i.e., negative or zero). CCI-FunRe and CCI-Havoc cannot effectively diagnose these two failures because \nthe root causes have nothing to do with function reentrance or unexpected value changes be\u00adtween two \nuses. However, CCI-Prev does correctly diagnose these failures bene.ting from the secondary instrumentation \nsites that leverage stale information per Section 4.1. The top CCI-Prev predictor, shown in Table 5, \ncorresponds to the assignment at the end of the worker thread which stores the correct timing information \ninto initdonetime. Only during failure runs, this assignment is preceded, instead of followed, by a read \naccess to initdonetime from the parent thread. The second-ranked predictor of FFT can also explain the \nfail\u00adure. It shows that failure occurs when the read access to initdonetime in the parent thread is not \npreceded by a remote assignment to initdonetime.  5.3 Performance Results Table 6 shows the overheads \nof each instrumentation scheme with and without sampling for the evaluated applications (without inserting \nany random yields). The two bugs in Apache require different source code con.gurations and have different \nperformance features. The three Mozilla bugs can be triggered with the same input and source code. Therefore, \nwe only present one number in the table.  Thermometer Predicate Function Global->initdonetime remote \n-write SlaveStart Global->initdonetime local -read main Table 5: Top CCI-Prev predictors for FFT No \nSampling Sampling Prev FunRe Havoc Prev FunRe Havoc Apache-1 62.6% 1.1% 27.4% 1.9% 1.8% 2.8% Apache-2 \n8.4% 0.2% 4.2% 0.5% 0.2% 0.4% Cherokee 19.1% 0.3% 2.1% 0.3% 0.4% 0.0% FFT 169 % 72.8% 33.5% 24.0% 30.0% \n5.5% LU 57827 % 1682 % 1693 % 949 % 926 % 8.9% Mozilla-JS 11311 % 123 % 7587 % 606 % 97.0% 356 % PBZIP2 \n0.2% 0.3% 0.2% 0.2% 0.2% 0.2% Table 6: Run-time overheads. Overheads below 10% are boldfaced. Overall, \nsampling signi.cantly decreases CCI s monitor\u00ading overhead. CCI offers the potential of low-overhead \ncrug diagnosis in production runs for many of the applications in our experiments. Speci.cally, if we \nuse 10% run-time overhead as a thresh\u00adold for deployment, then without sampling overheads are already \nlow enough to deploy CCI-Prev in two out of seven applications, CCI-Havoc in three applications, CCI-FunRe \nin four applications. CCI-FunRe is so simple a scheme that it has negligible overhead (around 1%) for \nall I/O intensive applica\u00adtions (Apache-1, Apache-2, Cherokee, and PBZIP2) in our experiments. Unfortunately, \nmemory-access intensive appli\u00adcations, such as the Mozilla JavaScript Engine and SPLASH2 applications, \nstill incur huge monitoring overhead for all three schemes. Sampling signi.cantly shrinks overheads for \nCCI-Prev and CCI-Havoc. With sampling, CCI-Prev achieves small enough overhead to deploy in two more \napplications (Apache\u00ad1 and Cherokee). CCI-Havoc is even better: overhead drops below 10% for three more \napplications, pushing the number of deployable applications to six out of seven. Sampling al\u00adlows all \nCCI schemes to achieve negligible (< 3%) overhead for all I/O intensive applications in our experiments. \nIt also helps CCI-Havoc achieve small (< 10%) overhead for all evaluated applications except for the \nMozilla JavaScript En\u00adgine. The large overhead in the later is caused by its extremely intensive heap \naccesses and loops. CCI pessimistically as\u00adsumes that any pointer-crossing operation might touch shared \nmemory, but this is clearly an over-approximation. We ex\u00adpect that static thread-escape analysis and \nCCI-aware loop unrolling will signi.cantly decrease overhead in the future. Comparing the different CCI \ninstrumentation schemes, CCI-Prev has the worst performance because it uses locks and global .ags (i.e., \ngsample) to coordinate sampling and predicate evaluation across all threads. CCI-FunRe has the best performance \nwithout sampling, but bene.ts the least from sampling, per Section 4.3. CCI-Havoc has a simpler design \nthan CCI-Prev and is also easier to sample than CCI-FunRe. Thus, CCI-Havoc achieves the best balance \nbetween performance and failure diagnosis among all three schemes. What is the minimum overhead CCI can \ndeliver? Theoret\u00adically, lower sampling rates yield lower overheads. However, the overhead can never \nreach zero. In both CCI-Prev and CCI-Havoc, the run-time environment maintains a random number generator, \nchecks the gsample .ag if necessary, and conducts a countdown in order to support random sam\u00adpling, which \ndetermines the lower-bound of CCI monitor\u00ading overhead. CCI-Prev s minimum overhead for FFT, LU, and \nMozilla-JS is 6.6%, 2.8%, and 562%, respectively; CCI\u00adHavoc s minimum overhead for these three applications \nare 4.4%, 0.8%, and 298%, respectively. For CCI-Havoc s thread\u00adindependent sampling, we can skip the \ngsample .ag check when we know that sampling is currently off and will not be turned on during several \nforthcoming sites. Thus we see lower overheads for this scheme. In general, this minimum over\u00adhead increases \nwith the density of instrumentation sites and loops in the program. Mozilla-JS has high density of both \nand therefore has high minimum overhead. As discussed above, we expect that improved static analysis \nwill help applications like Mozilla-JS in the future. What type of sampling can help CCI-FunRe? As dis\u00adcussed \nin Section 4.3, one sampling strategy that can further decrease the overhead of CCI-FunRe is to statically \nsplit func\u00adtions into multiple groups and release multiple versions of software with each version monitoring \none group. We have implemented this scheme, and .nd that it does help CCI-FunRe to signi.cantly decrease \nits monitoring overhead on Mozilla-JS to less than 15%. However, it still has drawbacks. For example, \na single small yet frequently-invoked function  # of False Find Failure- Program Positives Relevant \nRace? Overhead Apache-1 Apache-2 Cherokee FFT 44 81 10 27 / / / - 57% 29% 212% 20,233% LU 22 - 41,172% \nMozilla-JS-1 Mozilla-JS-2 Mozilla-JS-3 PBZIP2 9 24 19 11 / / / / 45,957% 45,957% 45,957% 4,803% Table \n7: Bug detection results of Helgrind race detector could become a performance bottleneck with CCI-FunRe, \na possibility not addressed by this alternative sampling scheme. How scalable are CCI sampling schemes? \nOur experi\u00adments show that CCI-Havoc has excellent scalability. We .nd that with a given workload, adding \nmore threads reduces CCI-Havoc overhead in our SPLASH2 experiments. This is because evaluation of CCI-Havoc \npredicates is completely independent by thread and therefore can be parallelized per\u00adfectly. The scalability \nof CCI-FunRe and CCI-Prev is not as good as CCI-Havoc, making CCI-Havoc the instrumentation scheme of \nchoice for very-highly-concurrent applications.  5.4 Comparison With Helgrind Race Detector To demonstrate \nthe difference between race detectors and CCI, we applied Helgrind, a state-of-practice happens-before \nrace detector from Valgrind [2], to all 9 bugs with exactly the same inputs and experiment settings as \nthose used by CCI. The results are shown in Table 7. Helgrind reports many false positives, ranging from \n9 in Mozilla-JS-1 to 81 in Apache-2, which could cost signi.cant developer effort during failure diagnosis. \nThese false positives can be categorized into two types. The .rst type are true races, but are considered \nharmless by programmers and are intentionally left there. For example, all 9 false positives in Mozilla-JS-1 \nbelong to this type. Prior work reports similar results [7, 31], and in general no race detector can \navoid these false positives as they genuinely are true races, just not failure-inducing ones. The second \ntype of false positives are not truly races at all. Helgrind mistakenly reports them because it only \nrecognizes synchronizations implemented by the pthread library. For example, 16 out of the 19 false positives \nin Mozilla-JS-3 are actually well-synchronized by ad-hoc condition variables implemented by the programmers. \nA more accurate race detector could avoid reporting these. Helgrind is able to report races that are \nrelated to the software failure for 7 out of the 9 bugs in our study. However, due to the different design \npurposes, the information provided by Helgrind is usually less useful to failure diagnosis than that \nprovided by CCI. For example, for the Cherokee bug shown in Figure 9, Helgrind reports a data race between \nS1 and S3. Following this clue, programmers might put S1 into a critical region and S3 into another critical \nregion protected by the same lock. This would eliminate the data race, but would not .x the problem. \nBy contrast, CCI-Havoc reports that Cherokee tends to fail when the global variable g buf->len is modi.ed \nbetween S1 and S2. This is exactly the root cause of the failure and can help developers to .x the bug. \nFinally, Helgrind has a huge overhead that is obviously not suitable for production run. This is part \nof the motivation of CCI and other sampling-based race detectors [5, 29]. 6. Related Work Pre-deployment \ntools for detecting races and atomicity vi\u00adolations fall into two categories: static and dynamic. Static \napproaches [12, 17, 21] are conservative and must consider all potential races. A problem with using \nstatic analysis is that it is dif.cult to distinguish benign races from those that can genuinely cause \nfailures. Benign races occur, for exam\u00adple, in test-and-set-lock operations and performance counter updates. \nScalability of analyses to target large programs is also problematic. Many dynamic analysis tools have \nbeen proposed to de\u00adtect data races and atomicity violation bugs [13, 14, 26, 36]. All have high run-time \noverheads (around 25\u00d7 for C appli\u00adcations) which make them impractical for post-deployment use. Furthermore, \neach of these tools targets only a speci.c class of crugs viz. either atomicity violations [13, 26] or \ndata races [36], and often assume a particular synchronization mechanism. For example, the lockset analysis \nused in Eraser [36] applies only to lock-based multi-threaded programs. CCI targets the root causes of \na wide variety of software failures caused by not just data races but also atomicity violations and other \ntypes of crugs and is agnostic to which particular synchronization mechanism is used. Furthermore, by \nfocus\u00ading on predictors for genuine failures, CCI avoids the false positives caused by benign races which \nplague other static and dynamic approaches. Some of these issues also apply to recent approaches that \nuse sampling to improve the performance of race detection, such as LiteRace by Marino et al. [29] and \nPACER by Bond et al. [5]. CCI shares the same sampling philosophy with LiteRace and PACER. However, they \nare designed for differ\u00adent purposes: CCI targets failure diagnosis while LiteRace and PACER focus on \nrace detection. Because of this differ\u00adence, CCI covers a wider variety of synchronization problems than \nLiteRace and PACER, and is explicitly agnostic with respect to synchronization mechanisms. CCI also leverages \nits statistical model and failure information to achieve high accuracy, while LiteRace and PACER would \ncause many false positives due to benign races if used in failure diagnosis, similar to Helgrind race \ndetector discussed in Section 5.4. In addition, the sampling techniques used are also different. LiteRace \nuses adaptive, thread-independent, bursty sampling; PACER uses thread-coordinated, bursty sampling. Our \nthree CCI schemes cover different sampling design choices, in\u00adcluding both thread-independent, thread-coordinated, \nand different bursty designs.  Our earlier and shorter version of this work [41] designed one predicate \n(here identi.ed as CCI-Prev) and one basic thread-coordinated sampling strategy. The present work en\u00adhances \nthe previous CCI-Prev sampling scheme with tunable, bursty sample lengths. More importantly, the present \nwork designs and thoroughly evaluates a wider range of predicates and sampling strategies; the prior \nwork by Thakur et al. can now be seen as one point in the much richer design space explored herein. Previous \nwork also improves the performance of race detection by replacing heavyweight vector clocks with an adaptive \nlightweight representation in FastTrack [14] and using adaptive coarse granularity memory monitoring \nin RaceTrack [46]. These performance enhancing techniques are orthogonal to the sampling techniques used \nin CCI. It is conceivable to further improve their performance with sampling. CCI can also use adaptive \nmonitoring granularity to further improve its performance. Of course, since CCI is looking at failure \ndiagnosis instead of race detection, the design tradeoffs will be different than those in RaceTrack. \nMuch prior research has focused on testing concurrent pro\u00adgrams, such as testing based on synchronization \ncoverage [6], context-bounded testing [30], race-directed random testing [38], and unit testing[33]. \nRecord-and-replay for multi-core machines could also aid in debugging concurrent programs. Unfortunately, \nexisting proposals are not practical for pro\u00adduction usage due to high overhead (around 10\u00d7 slowdown \n[10, 22]) or reliance on non-existing hardware [20]. Recent approaches aim to not detect, but to automatically \navoid or tolerate certain kinds of crugs [4, 28, 34, 35, 43, 45]. This is orthogonal to our goal of diagnosing \nthe root causes of concurrent software failures. DefUse[39] proposes a family of de.ne-use related invari\u00ad \nants to capture dynamic data-.ow and detect software bugs. As DefUse is used for in-house testing while \nCCI is used in the .eld, they have focused on different challenges and proposed different techniques. \nCCI predicates are different from DefUse invariants. To maintain simplicity, CCI-Prev and CCI-Havoc are \nintentionally ignorant of the access type (read or write) and hence also ignorant of de.ne-use data dependencies. \nCCI also uses sampling techniques to lower overheads, and a statistical model that handles the resulting \nnoise/inaccuracy. CCI is based on the Cooperative Bug Isolation (CBI) project [23, 25], which uses a \nsampling-based monitoring framework to ensure that run-time overheads are low, and uses statistical techniques \non the collected data to infer likely root causes from this sparsely-sampled data. Subsequent work has \nfurther re.ned the CBI paradigm to .nd root causes of more complex bugs [3, 8], but our experiments have \ndemonstrated that crugs demand a new approach. 7. Conclusion We have described CCI, a low-overhead, scalable \nstrategy for root-cause analysis of crugs. We have implemented the sys\u00adtem and shown our technique to \nbe effective at .nding bugs in several large, real-world applications. Our approach inten\u00adtionally tracks \nfar less information than exhaustive dynamic detectors. Combined with novel approaches to cross-thread \nsampling, this allows CCI to achieve very low overheads, making it practical for use in production environments. \nAt the same time, the data collected is suf.cient to isolate root causes of failures that are invisible \nto prior statistical debug\u00adging work. In the future we plan to extend this work by explor\u00ading other instrumentation \nschemes that track different con\u00adcurrency events, by experimenting with other thread-aware sampling mechanisms, \nand by augmenting CCI s dynamic approach with complementary static analyses. References [1] The Apache \nSoftware Foundation. Apache HTTP Server Project. http://httpd.apache.org/, 2009. [2] C. Armour-Brown, \nJ. Fitzhardinge, T. Hughes, N. Nether\u00adcote, P. Mackerras, D. Mueller, J. Seward, B. V. Assche, R. Walsh, \nand J. Weidendorfer. Valgrind User Manual. Val\u00adgrind project, 3.5.0 edition, Aug. 2009. http://valgrind. \norg/docs/manual/manual.html. [3] P. Arumuga Nainar, T. Chen, J. Rosin, and B. Liblit. Statistical debugging \nusing compound Boolean predicates. In ISSTA, 2007. [4] E. D. Berger, T. Yang, T. Liu, and G. Novark. \nGrace: safe multithreaded programming for c/c++. In OOPSLA, 2009. [5] M. D. Bond, K. E. Coons, and K. \nS. McKinley. Pacer: Proportional detection of data races. In PLDI, 2010. [6] A. Bron, E. Farchi, Y. Magid, \nY. Nir, and S. Ur. Applications of synchronization coverage. In PPOPP, 2005. [7] J. Burnim and K. Sen. \nAsserting and checking determinism for multithreaded programs. In FSE, 2009. [8] T. Chilimbi, B. Liblit, \nK. Mehra, A. V. Nori, and K. Vaswani. HOLMES: Effective statistical debugging via ef.cient path pro.ling. \nIn ICSE, May 2009. [9] J.-D. Choi, K. Lee, A. Loginov, R. O Callahan, V. Sarkar, and M. Sridharan. Ef.cient \nand precise datarace detection for multithreaded object-oriented programs. In PLDI, 2002. [10] G. Dunlap, \nD. Lucchetti, M. Fetterman, and P. Chen. Execution replay of multiprocessor virtual machines. In VEE, \n2008. [11] T. Elmas, S. Qadeer, and S. Tasiran. A calculus of atomic actions. In POPL, 2009. [12] D. \nEngler and K. Ashcraft. RacerX: Effective, static detection of race conditions and deadlocks. SIGOPS \nOper. Syst. Rev., 37 (5), 2003. [13] C. Flanagan and S. N. Freund. Atomizer: a dynamic atomicity checker \nfor multithreaded programs. In POPL, 2004. [14] C. Flanagan and S. N. Freund. Fasttrack: ef.cient and \nprecise dynamic race detection. In PLDI, 2009.  [15] J. Gilchrist. PBZIP2: Parallel BZIP2 Data Compression \nSoftware. http://compression.ca/pbzip2/, 2009. [16] P. Godefroid and N. Nagappan. Concurrency at Microsoft \n an exploratory survey. In Workshop on Exploiting Concurrency Ef.ciently and Correctly, 2008. [17] T. \nA. Henzinger, R. Jhala, and R. Majumdar. Race checking by context inference. In PLDI, 2004. [18] M. Herlihy. \nWait-free synchronization. ACM Trans. Program. Lang. Syst., 13(1), 1991. [19] M. Hirzel and T. M. Chilimbi. \nBursty tracing: A framework for low-overhead temporal pro.ling. In 4th ACM Workshop on Feedback-Directed \nand Dynamic Optimization, 2001. [20] D. R. Hower, P. Montesinos, L. Ceze, M. D. Hill, and J. Torrel\u00adlas. \nTwo hardware-based approaches for deterministic multi\u00adprocessor replay. CACM, 2009. [21] V. Kahlon, Y. \nYang, S. Sankaranarayanan, and A. Gupta. Fast and accurate static data-race detection for concurrent \nprograms. In CAV, 2007. [22] T. J. LeBlanc and J. M. Mellor-Crummey. Debugging parallel programs with \ninstant replay. IEEE Trans. Comput., 36(4), 1987. [23] B. Liblit, A. Aiken, A. X. Zheng, and M. I. Jordan. \nBug isolation via remote program sampling. In PLDI, 2003. [24] B. Liblit, M. Naik, A. X. Zheng, A. Aiken, \nand M. I. Jordan. Public deployment of Cooperative Bug Isolation. In RAMSS, 2004. [25] B. Liblit, M. \nNaik, A. X. Zheng, A. Aiken, and M. I. Jordan. Scalable statistical bug isolation. In PLDI, 2005. [26] \nS. Lu, J. Tucek, F. Qin, and Y. Zhou. AVIO: Detecting atomicity violations via access-interleaving invariants. \nIn ASPLOS, 2006. [27] S. Lu, S. Park, E. Seo, and Y. Zhou. Learning from mistakes a comprehensive study \nof real world concurrency bug charac\u00adteristics. In ASPLOS, March 2008. [28] B. Lucia, J. Devietti, L. \nCeze, and K. Strauss. Atom-aid: Detecting and surviving atomicity violations. IEEE Micro, 29 (1), 2009. \n[29] D. Marino, M. Musuvathi, and S. Narayanasamy. Effective sampling for lightweight data-race detection. \nIn PLDI, 2009. [30] M. Musuvathi and S. Qadeer. Iterative context bounding for systematic testing of \nmultithreaded programs. In PLDI, 2007. [31] S. Narayanasamy, Z. Wang, J. Tigani, A. Edwards, and B. Calder. \nAutomatically classifying benign and harmful data races using replay analysis. In PLDI, 2007. [32] S. \nPark, S. Lu, and Y. Zhou. CTrigger: exposing atomicity violation bugs from their hiding places. In ASPLOS, \n2009. [33] W. Pugh and N. Ayewah. Unit testing concurrent software. In ASE, 2007. [34] S. Rajamani, G. \nRamalingam, V.-P. Ranganath, and K. Vaswani. Isolator: dynamically ensuring isolation in comcurrent pro\u00adgrams. \nIn ASPLOS, 2009. [35] P. Ratanaworabhan, M. Burtscher, D. Kirovski, B. Zorn, R. Nagpal, and K. Pattabiraman. \nDetecting and tolerating asymmetric races. In PPoPP, 2009. [36] S. Savage, M. Burrows, G. Nelson, P. \nSobalvarro, and T. An\u00adderson. Eraser: A dynamic data race detector for multithreaded programs. ACM Transactions \non Computer Systems, 15, 1997. [37] SecurityFocus. Software bug contributed to blackout. http: //www.securityfocus.com/news/8016, \nFeb. 2004. [38] K. Sen. Race directed random testing of concurrent programs. In PLDI, 2008. [39] Y. Shi, \nS. Park, Z. Yin, S. Lu, Y. Zhou, W. Chen, and W. Zheng. Do I Use the Wrong De.nition? DefUse: De.nition-Use \nInvariants for Detecting Concurrency and Sequential Bugs. In OOPSLA, 2010. [40] A. Sussman and J. Trawick. \nCorrupt log lines at high vol\u00adumes. https://issues.apache.org/bugzilla/ show_bug.cgi?id=25520, 2003. \n[41] A. Thakur, R. Sen, B. Liblit, and S. Lu. Cooperative Crug Isolation. In WODA, 2009. [42] M. Vaziri, \nF. Tip, and J. Dolby. Associating synchronization constraints with data in an object-oriented language. \nIn POPL, 2006. [43] M. T. Vechev, E. Yahav, and G. Yorsh. Abstraction-guided synthesis of synchronization. \nIn POPL, 2010. [44] S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta. The SPLASH-2 programs: \nCharacterization and methodological considerations. In ISCA, 1995. [45] J. Yu and S. Narayanasamy. A \ncase for an interleaving constrained shared-memory multi-processor. In ISCA, 2009. [46] Y. Yu, T. Rodeheffer, \nand W. Chen. Racetrack: ef.cient detection of data race conditions via adaptive tracking. In SOSP, 2005. \n    \n\t\t\t", "proc_id": "1869459", "abstract": "<p>Fixing concurrency bugs (or \"crugs\") is critical in modern software systems. Static analyses to find crugs such as data races and atomicity violations scale poorly, while dynamic approaches incur high run-time overheads. Crugs manifest only under specific execution interleavings that may not arise during in-house testing, thereby demanding a lightweight program monitoring technique that can be used post-deployment.</p> <p>We present Cooperative Crug Isolation (CCI), a low-overhead instrumentation framework to diagnose production-run failures caused by crugs. CCI tracks specific thread interleavings at run-time, and uses statistical models to identify strong failure predictors among these. We offer a varied suite of predicates that represent different trade-offs between complexity and fault isolation capability. We also develop variant random sampling strategies that suit different types of predicates and help keep the run-time overhead low. Experiments with 9 real-world bugs in 6 non-trivial C applications show that these schemes span a wide spectrum of performance and diagnosis capabilities, each suitable for different usage scenarios.</p>", "authors": [{"name": "Guoliang Jin", "author_profile_id": "81470644031", "affiliation": "University of Wisconsin-Madison, Madison, WI, USA", "person_id": "P2354051", "email_address": "", "orcid_id": ""}, {"name": "Aditya Thakur", "author_profile_id": "81350570347", "affiliation": "University of Wisconsin-Madison, Madison, WI, USA", "person_id": "P2354052", "email_address": "", "orcid_id": ""}, {"name": "Ben Liblit", "author_profile_id": "81100555854", "affiliation": "University of Wisconsin-Madison, Madison, WI, USA", "person_id": "P2354053", "email_address": "", "orcid_id": ""}, {"name": "Shan Lu", "author_profile_id": "81100052818", "affiliation": "University of Wisconsin-Madison, Madison, WI, USA", "person_id": "P2354054", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1869459.1869481", "year": "2010", "article_id": "1869481", "conference": "OOPSLA", "title": "Instrumentation and sampling strategies for cooperative concurrency bug isolation", "url": "http://dl.acm.org/citation.cfm?id=1869481"}