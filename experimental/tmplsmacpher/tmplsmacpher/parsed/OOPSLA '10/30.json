{"article_publication_date": "10-17-2010", "fulltext": "\n Parallel Inclusion-based Points-to Analysis Mario Mendez-Lojo\u00b41 Augustine Mathew2 Keshav Pingali1,2 \n1Institute for Computational Engineering and Sciences, University of Texas, Austin, TX 2 Dept. of Computer \nScience. University of Texas, Austin, TX marioml@ices.utexas.edu, amathew@cs.utexas.edu, pingali@cs.utexas.edu \nAbstract Inclusion-based points-to analysis provides a good trade-off between precision of results and \nspeed of analysis, and it has been incorporated into several production compilers in\u00adcluding gcc. There \nis an extensive literature on how to speed up this algorithm using heuristics such as detecting and col\u00adlapsing \ncycles of pointer-equivalent variables. This paper describes a complementary approach based on exploiting \nparallelism. Our implementation exploits two key insights. First, we show that inclusion-based points-to \nanalysis can be formulated entirely in terms of graphs and graph rewrite rules. This exposes the amorphous \ndata-parallelism in this algorithm and makes it easier to develop a parallel imple\u00admentation. Second, \nwe show that this graph-theoretic formu\u00adlation reveals certain key properties of the algorithm that can \nbe exploited to obtain an ef.cient parallel implementation. Our parallel implementation achieves a scaling \nof up to 3x on a 8-core machine for a suite of ten large C programs. For all but the smallest benchmarks, \nthe parallel analysis outper\u00adforms a state-of-the-art, highly optimized, serial implemen\u00adtation of the \nsame algorithm. To the best of our knowledge, this is the .rst parallel implementation of a points-to \nanaly\u00adsis. Categories and Subject Descriptors D.1.3 [Programming Techniques]: Concurrent Programming \nParallel Program\u00adming; D.3.3 [Programming Languages]: Language Con\u00adstructs and Features Frameworks General \nTerms Algorithms, Languages, Performance Keywords Inclusion-based Points-to Analysis, Irregular Programs, \nAmorphous Data-parallelism, Galois System, Synchronization Overheads, Extensive Transformers, Iter\u00adation \nCoalescing, Binary Decision Diagrams. Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. OOPSLA/SPLASH 10, October 17 21, 2010, Reno/Tahoe, Nevada, USA. Copyright \nc &#38;#169; 2010 ACM 978-1-4503-0203-6/10/10. . . $10.00 1. Introduction Points-to analysis is a static \nanalysis technique that deter\u00admines what a pointer variable may point to during the execu\u00adtion of a program. \nThe results of this analysis are useful for program optimization, program veri.cation, debugging and \nwhole program comprehension [13]. In the literature, there are many variations of points-to analysis: \nsome analyses are context-sensitive while others are context-insensitive, some are .ow-sensitive while \nothers are .ow-insensitive, etc. [3, 4, 7, 8, 10, 32, 35]. These variations make different trade-offs \nbetween precision and running time, but production compil\u00aders like gcc and LLVM [21] seem to have settled \non context\u00adinsensitive, .ow-insensitive points-to analysis because more precise alternatives are currently \nintractable for large pro\u00adgrams. The most popular algorithm for context-insensitive, .ow\u00adinsensitive \npoints-to analysis is known as inclusion-based or Andersen-style analysis [3]. As explained in Section \n2, it requires the solution of a system of set constraints derived from the program. The asymptotic worst-case \ncomplexity of the solution procedure is O(n3), where n is the number of variables in the program. Although \nthis worst-case behavior is not usually observed in practice, the solution procedure is suf.ciently expensive \nthat much research has gone into heuristics for speeding it up. A key observation was made by Fahndrich \net al [7], who showed that detecting and elim\u00adinating cyclic constraints can make a big difference in \nthe running time because it reduces the size of the constraint system. Since cyclic constraints can arise \ndynamically dur\u00ading the solution of the constraint system, it is not suf.cient to preprocess the system \nof constraints of.ine before the con\u00adstraint system is solved [29]. A recent advance was made by Hardekopf \nand Lin who invented ingenious heuristics for determining when to trigger cycle detection online during \nconstraint solving [8]. The advent of multicore processors that support a shared\u00admemory programming model \nopens up a new avenue for speeding up these kinds of expensive static analysis algo\u00adrithms. Unfortunately, \nmost work on parallel programming has focused on high-performance applications in which the key computations \nare matrix and vector operations such as linear system solvers and FFTs; as a consequence, little is \nknown about how to parallelize irregular algorithms like inclusion-based points-to analysis in which \nthe key data structures are graphs, sets, binary decision diagrams [5] (BDD s), etc. There is an enormous \nliterature on static anal\u00adysis techniques for .nding parallelism in programs [15], but these techniques \nfail to .nd much parallelism in most ir\u00adregular algorithms. This is because dependences between computations \nin irregular algorithms are functions of run\u00adtime values, so static parallelization techniques end up \nbeing overly conservative. In the literature, several papers mention the possibility of parallelizing \npoints-to analysis, but there are no imple\u00admentations or evaluations1. These papers use partitioning \nof the analysis problem to reduce memory consumption [30] or to perform different kinds of points-to \nanalysis on dif\u00adferent partitions [14, 36], and suggest in passing that parti\u00adtioning might also be useful \nfor parallel implementations of points-to analysis. Pereira et al [25] discuss the paralleliza\u00adtion of \ntheir wavescalar approach, but they do not have an implementation. It is not clear to us that partitioning \nis a useful approach for inclusion-based points-to analysis; as we mentioned before, a key step in speeding \nup inclusion\u00adbased points-to analysis is the detection and collapsing of certain cyclic constraints that \narise dynamically during the constraint solving process, and these cycles may cross parti\u00adtions. In any \ncase, partitioning is complementary to the ap\u00adproach presented here. This paper describes the .rst parallel \nimplementation of inclusion-based points-to analysis and it makes the follow\u00ading contributions. 1. In \nSection 3, we show that inclusion-based points-to analysis can be formulated entirely in terms of graphs \nand graph rewrite rules. The system of set constraints is expressed as a graph, and the solution to the \nset con\u00adstraints is expressed in terms of rewrite rules for this graph. This formulation exposes the \namorphous data\u00adparallelism [26] in this algorithm, permitting the deriva\u00adtion of a parallel implementation \nof inclusion-based points-to analysis. As in most irregular algorithms, de\u00adpendences in this algorithm \nare functions of runtime data, so it is necessary to use optimistic or speculative paral\u00adlelization. \nOur implementation is based on the Galois system [1], described in Section 4. 2. Speculative parallel \nexecution of programs can incur sub\u00adstantial runtime overheads. Recent work has shown algo\u00adrithmic structure \nmust be exploited to eliminate this over\u00adhead [23]. In Section 5, we show that the graph-theoretic formulation \nof inclusion-based points-to analysis reveals important and novel algorithmic structure that can be ex\u00adploited \nto dramatically reduce the overheads of specula\u00adtive parallel execution of this algorithm.  1 We are \ngrateful to Ben Hardekopf for this information, and for the citations in this paragraph. 3. Although \nonline cycle detection is fundamental for achiev\u00ading good performance, its addition to the basic algorithm \nrepresents a challenge for parallelization. In Section 6, we show how to reduce the problem of online \ncycle de\u00adtection to the simpler problem of merging two nodes in the graph, and then show how to safely \ninterleave merges with graph rewriting rules for constraint solving. Using these ideas, we implemented \na parallel Java version of a state-of-the-art points-to analysis algorithm by Hard\u00adekopf and Lin [8] \n(their implementation has been incor\u00adporated into gcc and LLVM). The experimental results in Section \n7 show that our parallel implementation scales rea\u00adsonably well (up to 3x in a 8-core machine) for a \nbench\u00admark suite of C programs ranging in size from 53,000 to 558,000 variables. Furthermore, this parallel \nversion out\u00adperforms the serial reference implementation for all but the smallest benchmarks. 2. Inclusion-based \npoints-to analysis Inclusion-based points-to analysis is context-insensitive and .ow-insensitive. A context-sensitive \nanalysis analyzes a pro\u00adcedure separately for each context in which it is invoked; in contrast, a context-insensitive \nalgorithm would merge information from all call sites. Context-insensitive analy\u00adses produce less accurate \ninformation, but they run faster than context-sensitive alternatives for most problems. Flow\u00adsensitive \nanalyses produce results that are speci.c to particu\u00adlar points in the program, while .ow-insensitive \nalternatives do not consider control-.ow and produce a single conser\u00advative approximation valid for all \nprogram points. Hind has written an excellent survey of the enormous literature on different varieties \nof points-to analysis [13]. The precision of .ow and context-insensitive alternatives is suf.cient for \nmany application clients, so they have been incorporated into production compilers such as gcc or LLVM. \nContext-insensitive, .ow-insensitive points-to analyses are either uni.cation-based or inclusion-based. \nIf a and b are pointer variables, and a = b is an assignment statement in the program, a uni.cation-based \nalgorithm will assert con\u00adservatively that the points-to sets of a and b are equal; in contrast, an inclusion-based \nalgorithm makes the minimal deduction that the points-to set of b must be a subset of the points-to set \nof a. The points-to analyzers present in gcc and LLVM are inclusion-based. 2.1 Set constraint formulation \nInclusion-based pointer analysis is usually formulated as a set-constraint problem. A single pass through \nthe program code generates a system of set constraints that implicitly de\u00ad.nes the points-to set pts(v) \nfor each variable v in the pro\u00adgram. Nested pointer dereferences are eliminated by intro\u00adducing auxiliary \nvariables, leaving only one pointer deref\u00aderence per statement. Figure 1 shows the different types of \nassignment statements and the set constraint generated for Statement Name Constraint a = &#38;b pointer \nloc(b) . pts(a) a = b copy pts(a) . pts(b) a = *b load .v . pts(b) : pts(a) . pts(v) *a = b store .v \n. pts(a) : pts(v) . pts(b) Figure 1. Formulating set constraints Program Constraints a =&#38;v; loc(v) \n. pts(a) *a = b; .v . pts(a): pts(v) . pts(b) b = x; pts(b) . pts(x) x =&#38;w loc(w) . pts(x) Figure \n2. Running example each type. In these constraints, loc(v) represents the memory location denoted by \nv. The most complex constraints result from variable dereferencing in load and store statements. For \na load statement, the constraint asserts that for every variable v in the points-to set of b, the points-to \nset of v is a subset of the points-to set of a. The constraint for a store statement asserts that for \nevery variable v in the points-to set of a, the points-to set of b is a subset of the points-to set of \nv. Load and store constraints are called complex constraints. Figure 2 shows a simple program and its \nassociated system of set con\u00adstraints. Systems of set constraints such as the one in Figure 2 are higher \norder constraint systems, so one cannot appeal to the usual monotonicity theorems [2] to assert that \nthese systems have solutions. For a given program P in which the set of variables is V , let a points-to \nassignment be a function V . 2V (intuitively, it is a function that maps each variable to the subset \nof variables it points to). Points-to assignments for a given program can be partially ordered in a natural \nway: if A1 and A2 are points-to assignments, then A1 = A2 if for all variables v, A1(v) . A2(v) (in words, \nfor every variable v, the points-to set of v in A1 is a subset of the corresponding set in A2). Let A \nbe the set of points-to assignments for a given program P with variables V . De.ne A1 . A2 (v)= A1(v) \nn A2(v)  A1 . A2 (v)= A1(v) . A2(v)  Theorem 1. Let C be a system of set constraints generated from \na program P using the rules in Figure 1. C has a least solution. Proof. The proof involves the following \nsteps. If A is the set of pointer assignments for P , (A, =, ., .) is a .nite lattice.  Let T denote \nthe largest element of A (so T(v)= V for all v . V ). T satis.es C.  If S1 and S2 are two solutions \nof C, then S1 . S2 is also a solution.   2.2 Solving systems of set constraints Andersen described \na simple iterative algorithm for .nding the least solution of these set constraint systems [3]. The points-to \nsets of all variables are initialized to the empty set, and the constraints are processed iteratively \nuntil the points-to sets converge. When a constraint is considered, it is satis.ed locally by growing \nsome of the points-to sets as needed. The constraint for a pointer statement can be satis.ed by adding \nloc(b) to pts(a) if it is not in that set. For all other kinds of statements, a constraint of the form \nP1 . P2 is satis.ed by setting P1 to P1 . P2. Making these kinds of updates to satisfy one constraint \nmay violate others considered previously, so it is necessary to iterate over the system of constraints \nrepeatedly until convergence. To avoid redundant evaluations, implementations of this iterative algorithm \nconstruct a propagation graph in which there is a node for each variable, labeled with its points\u00adto \nset; these labels are initialized to the empty set. Pointer constraints are processed by adding loc(b) \nto the label of a (we will use the notation b . pts(a)). Each copy constraint pts(a) . pts(b) is processed \nby adding an edge b . a; intuitively, any element added to the points-to set (label) of b will .ow down \ninto the points-to set of a. Complex constraints are not explicitly represented in the graph; they are \nmaintained in a separate list. A node b is processed in two steps: 1. For each outgoing edge b . a, check \nwhether pts(a) . pts(b) holds. If not, propagate pts(b) to node a, i.e., pts(a) := pts(a) . pts(b). \n2. For each v . pts(b):  for each load constraint a = *b, add an edge v . a to ensure that pts(a) . \npts(v).  for each store constraint *b = a, add an edge a . v to ensure that pts(v) . pts(a).   This \niterative process is repeated until the points-to sets and graph edges do not change. Figure 3 shows \nthe iterative constraint solving process for the running example in Figure 2. The points-to set of a \nvariable is shown in braces above that node. In each stage, violated constraints are shaded. 3. Constraint \nsolving by graph rewriting In this section, we introduce one of the contributions of this paper: we show \nthat inclusion-based points-to analysis can be formulated entirely in terms of graph rewriting rules \nfor certain constraint graphs that can be constructed from the program. As we will see later in this \nsection, there are just three rewrite rules, and although they can be applied in any arbitrary order, \nthe .nal graph is unique (for a given initial graph). The solution to the inclusion-based points-to analysis \nproblem can be read off from the .nal constraint graph. Propagation graph Constraint system  loc(v) \n. pts(a) .v . pts(a): pts(v) . pts(b) pts(b) . pts(x) loc(w) . pts(x) loc(v) . pts(a) .v . pts(a): pts(v) \n. pts(b) pts(b) . pts(x) loc(w) . pts(x)  loc(v) . pts(a) .v . pts(a): pts(v) . pts(b) pts(b) . pts(x) \nloc(w) . pts(x) Figure 3. Solving the constraint system for the running example The major advantage \nof this graph rewriting formulation is that it simpli.es the reasoning about these kinds of analy\u00adsis \nalgorithms -in particular, about their parallelization. The other bene.t is that it enables us to take \nadvantage of exist\u00ading optimization technology and tools that were developed by the Galois project for \nother irregular problems, as we will show in Section 4. 3.1 Constraint graphs Given a program with the \nfour kinds of statements shown in Figure 1, the constraint graph for that program contains (i) one node \nfor each variable in the program, and (ii) one edge a . b for each type of statement involving variables \na and b, labeled with the type of that statement, as shown in Figure 4. Note that there might be many \nedges from a node a to a node b, but they must have different labels2. Figure 5 shows the constraint \ngraph for the running example. More formally, a constraint graph for a program P is a graph (V, E) in \nwhich there is one node in V for each variable in the program P, and E = Ep . Ec . El . Es , where these \nsets of edges are de.ned as follows. p Ep: These are points edges. There is an edge a - . b in the \nconstraint graph if a =&#38;b is a statement in program P.   Ec: These are copy edges. There is an \nedge b - . a in the constraint graph if a = b is a statement in program P, .  El: These are load edges. \nThere is an edge b -  c l . a in the constraint graph if a = *b is a statement in program P,h. s Es: \nThese are store edges. There is an edge a - . b in the constraint graph if *a = b is a statement in program \nP, . 2 To be consistent with the standard de.nition of a graph, we should replace these edges with a \nsingle edge with multiple labels, but we have not done this, to keep the description more intuitive. \nEdge Code Name a=&#38;b points a=b copy a=*b load *a=b store  Figure 4. Basic edge types Constraint \nGraph Program a =&#38;v; *a = b; b = x; x =&#38;w Figure 5. Constraint graph for running example As \nwe will see next, points and copy edges are added dynamically to the graph during the graph rewriting \nprocess. 3.2 Graph rewriting There are three graph rewrite rules as shown in Figure 6. Each rewrite \nrule enforces a certain invariant in the con\u00adstraint graph by adding edges to the graph as needed. As \nin general term rewriting systems, these graph rewrite rules can be considered as purely syntactic rules, \nbut it is useful to understand the intuition behind them. c Copy rule: Edge b - . a represents the constraint \npts(a) . p pts(b), so if v . pts(b), v must be in pts(a). Edge a - . v is added if it is not already \nin the graph, to ensure this. l Load rule: Edge b -. . a represents the constraint .v pts(b): pts(a) \n. pts(v). As a step towards enforcing c this, edge v - . a is added if it is not already in the graph. \nThe addition of this copy edge may trigger one or more applications of the copy rule. Store rule: The \nintuition behind this rule is similar to the intuition behind the load rule. Notice that each rewrite \nrule is triggered if there is a node with two outgoing edges at which the relevant invariant is not satis.ed \nbecause of a missing edge between the destination nodes of the outgoing edges. Such a node is called \nan active node. In Figure 6, the active node for each rule is shaded. When an active node is processed \nand an edge is added to the graph, it may cause other nodes to become active. There may be many active \nnodes in a given constraint graph, a fact that we exploit in the parallel algorithm described in Section \n4. A high level description of the algorithm is the following. The constraint graph is built from the \nprogram. At each step, Ensures Name Invariant Rewrite rule - . a c  - . v ..b . .a -  . v - . a \nlp p .b copy - . v ..b . .v -  . a c p .b load - . b Figure 6. Constraint graph rewriting rules an \narbitrary active node is selected and the relevant rewrite We give a sketch of the proof. rule is applied \nto the graph. This process can be described formally in terms of pushouts and graph morphisms as is Proof. \n(a) Follows from the fact that each of the rewrite done in graph grammars [6], but we will not do so \nhere. The rules in Figure 6 adds an edge to the graph and does algorithm terminates when there are no \nmore active nodes not remove any nodes or edges. In the worst case, the in the graph. The solution to \nthe inclusion-based points-to rewriting must terminate when there are points and copy analysis problem \ncan be read off from the .nal graph: if edges between every pair of nodes. s . v ..a . .b-  . v c p- \n.a store v - . w is an edge in the graph, w . pts(v). Figure 7 shows this rewriting process applied to \nthe running example. This high level algorithm can be implemented in the obvious way by keeping a work-list \nof active nodes. To identify the rewrite rule to be applied, we can store the two outgoing edges of the \nactive node together with that active node in the work-list. When an active node is removed from the \nwork-list, we .rst check to see if the necessary edge has already been added by a previous step. If so, \nthere is pnothingtobedone.Otherwise,therelevantedge isadded e as required by the rewrite rule, and the \nsource node s of this edge is checked to see if the newly added edge makes s active. If so, it is added \nto the work-list together with e and its partner edge. Theorem 2. Given a program P , let C be the system \nof set constraints, and let G be the constraint graph. The following facts hold. (a) The graph rewriting \nprocess always terminates. (b) The .nal graph produced at the end of the rewriting process is independent \nof the order in which the rewrite rules are applied. (c) The solution to the points-to analysis problem \nread off from the .nal graph is identical to the least solution of C.  (b) It is easy to see that the \nrewrite rules of Figure 6 are lo\u00ad cally con.uent. From part (a), we know that there are no in.nite chains \nof rewriting steps. From Newman s lemma [16], it follows that the rewriting system is glob\u00adally con.uent \nand the .nal constraint graph is unique.  (c) Using an induction on any sequence of rewriting steps, \nit is easy to show that if A1 is the points-to assignment read off from the .nal graph and A2 is the \npoints-to assignment that is the least solution of C, then A1 = A2. The fact that A1 <A2 cannot hold \ncan be proved from the fact that when the rewriting stops, there cannot be any active nodes left in the \ngraph.  3.3 Parallelism in constraint graph rewriting In general, there will be many active nodes in \na constraint graph at any point during the rewriting process. If the rewrit\u00ading steps at two active nodes \ndo not interfere with each other, they can obviously be performed in parallel. Therefore, the key problem \nin parallelizing the graph rewriting process is .nding non-interfering active nodes. As mentioned earlier, \nthis is a far more dif.cult problem than the well-studied problem of parallelizing regular ap\u00adplications \nlike dense matrix multiplication, FFTs and stencil  Figure 7. Constraint graph rewrite rule example. \ncodes [15]. In a regular application, dependences between computations are independent of runtime values, \nso it is possible in principle to produce a parallel schedule for the application before the program \nis executed. Unfortunately, inclusion-based points-to analysis is an example of an irreg\u00adular computation \nin which the computations that need to be performed, as well as the dependences between these com\u00adputations, \nare functions of runtime data, so it is not possible to produce a parallel schedule statically. 4. Parallel \ngraph rewriting using the Galois system One solution to parallelizing constraint graph rewriting is to \nuse the Galois system [1]. To make this paper self-contained, we give a brief description of the Galois \nsystem in this section, and show how it can be used to implement parallel graph rewriting. 4.1 Background: \nthe Galois system The Galois system is intended to support the parallel exe\u00adcution of irregular applications \nsuch as those that operate on large graphs and trees. Examples of such applications in\u00adclude n-body simulations, \nmesh generators, social network applications, SAT solvers, etc. The abstractions supported by the Galois \nprogramming model are derived from an operator formulation of irregu\u00adlar algorithms, which we explain \nusing the graph shown in Figure 8. At each point during the execution of such an al\u00adgorithm, there are \ncertain nodes or edges in the graph where computation might be performed. Performing a computa\u00adtion may \nrequire reading or writing other nodes and edges in the graph. The node or edge on which a computation \nis centered is called an active element, and the computa\u00adtion itself is called an activity. It is convenient \nto think of an activity as resulting from the application of an opera\u00adtor (graph transformer) to the \nactive node. The set of nodes and edges that are read or written in performing the activ\u00adity is called \nthe neighborhood of that activity. In Figure 8, the .lled nodes represent active nodes, and shaded regions \nrepresent the neighborhoods of those active nodes. In some Figure 8. Data-centric view of algorithms. \n algorithms, activities may modify the graph structure of the neighborhood by adding or removing graph \nelements. In general, there are many active nodes in a graph, so a sequential implementation must pick \none of them and per\u00adform the appropriate computation. In this discussion, we fo\u00adcus on algorithms such \nas the pre.ow-push max.ow algo\u00adrithm and Delaunay mesh re.nement in which the imple\u00admentation is allowed \nto pick any active node for execution. These are called unordered algorithms. Programmers write algorithms \nin a sequential, object-oriented language like se\u00adquential Java, using a Galois set iterator to assert \nthat the algorithm is unordered. The Galois set iterator is similar to a set iterator in Java except \nthat it permits new elements to be added to the set while the iterator executes. In addition, the Galois \nsystem has a library of concurrent data structures like graphs, priority queues, sets, etc. All concurrency \ncontrol is implemented within this library. 4.1.1 Baseline parallel execution model Figure 8 shows how \nopportunities for exploiting parallelism arise in graph algorithms: if there are many active elements \nat some point in the computation, each one is a site where a processor can perform computation, subject \nto neighborhood constraints. In the baseline parallel execution model, the graph is stored in shared-memory, \nand active nodes are pro\u00adcessed by some number of threads. Like thread-level specu\u00adlation [17] and transactional \nmemory [12], the Galois system uses speculative parallel execution to handle the problem of dependences \nthat can only be elucidated at runtime. A free thread picks an arbitrary active node and speculatively \nap\u00adplies the operator to that node, making calls to the graph class API to perform operations on the \ngraph as needed. The neighborhood of an activity can be visualized as a blue ink\u00adblot that begins at \nthe active node and spreads incremen\u00adtally whenever a graph API call is made that touches new nodes or \nedges in the graph. To ensure that neighborhood constraints are respected, each graph element has an \nassoci\u00adated exclusive abstract lock. Locks are held until the activity terminates. If a lock cannot be \nacquired because it is already owned by another thread, a con.ict is reported to the run\u00adtime system, \nwhich rolls back one of the con.icting activi\u00adties. To enable rollback, each graph API method that modi\u00ad.es \nthe graph makes a copy of the data before modi.cation. Like abstract lock manipulation, rollbacks are \na service im\u00adplemented by the library and runtime system. The activity terminates when the application \nof the operator is complete and all acquired locks are released. Intuitively, the use of abstract locks \nensures that graph API operations from concurrently executing iterations com\u00admute with each other, ensuring \nthat the iterations appear to execute in some serial order as required by the semantics of the Galois \nset iterator. There are more sophisticated tech\u00adniques for checking commutativity, but these are more \ncom\u00adplex to implement [18]. Commuting graph API operations that touch the same locations in the concrete \nrepresentation must be synchronized. The Galois system has been used to parallelize many complex applications \nincluding Delaunay mesh generation and re.nement, agglomerative clustering, survey propaga\u00adtion, and \nthe pre.ow-push max.ow algorithm [19].  4.2 Baseline parallelization of constraint graph rewriting \nThe constraint graph rewriting algorithm described in Sec\u00adtion 3 is an unordered algorithm: the active \nnodes in the graph are the nodes where an invariant of Figure 6 are vi\u00adolated, the operator is the appropriate \nrewrite rule, and the neighborhood of an activity consists of the three nodes and their incident edges. \nSince the baseline implementation uses exclusive abstract locks, a con.ict occurs when two or more activities \ntouch the same node. Figure 9 shows two examples of con.icts. p In Figure 9(a), one rule wants to add \nan edge a - . v, while the other wants to add x -p . v. Because both activities try to lock b and v, \nthey cannot proceed in parallel. A con.ict arises in Figure 9(b) because the two activities are trying \nto add points edges to node a (the destination nodes of these edges are different).  For this algorithm, \nthe baseline parallelization approach adds substantial overheads to a small computation (two edge reads, \none edge addition). These overheads come from four sources: Enforcing neighborhood constraints: Acquiring \nand re\u00adleasing abstract locks on neighborhood elements can be a major source of overhead.  Copying data \nfor rollbacks: When an activity modi.es a graph element, a copy of that element is made to enable rollbacks. \n Aborted activities: When an activity is aborted, the com\u00adputational work performed up to that point \nby that activ\u00adity is wasted. Furthermore, the runtime system needs to take corrective action to roll \nback the activity, which adds to the overhead.  Dynamic assignment of work: Threads go to the central\u00adized \nwork-list to get work. This requires synchronization; moreover, if there are many threads and the computation \nperformed in each activity is small, contention between threads will limit speedup.   Figure 9. Con.icts \noccur when two neighborhoods contain the same node. 5. Exploiting structure to optimize parallel execution \nReducing the overheads of the baseline system requires ex\u00adploiting algorithmic structure in general. \nFor example, when the graph can be partitioned between the threads, abstract locks can be assigned to \npartitions rather than to graph ele\u00adments, and separate work-lists can be used for each partition; this \nreduces the number of aborted activities, contention be\u00adtween threads for the work-list and the overhead \nof lock\u00ading [20]. However, partitioning a graph is an overhead in it\u00adself. Fortunately, constraint graph \nrewriting has structure of its own that can be exploited to reduce or even completely eliminate most \nof these overheads. 5.1 Optimizations Eliminating abstract locks: To reduce the overhead of en\u00adforcing \nneighborhood constraints, we observe .rst that if the constraint graph were a read-only data structure, \nwe would not need any abstract locks to ensure commutativity of graph API operations. Unfortunately, \nthe rewrite rules of Figure 6 do update the constraint graph, so this simple optimization cannot be used. \nHowever, notice that these rewrite rules never remove nodes or edges from the graph; they can only add \nedges to the graph. This means that the two edges in the precondition of each rewrite rule will exist \npermanently in the graph regardless of what other rewrites are performed, so it is not necessary to use \nabstract locks to ensure this. As a consequence, when executing a rewrite rule, we need to ac\u00adquire an \nabstract lock only for the node at which the edge is added. This reduces the number of abstract lock \nacquisitions and releases by two-thirds. While this optimization is relatively straight-forward, a more \ncareful analysis shows that we do not need an abstract lock even at the node at which the edge is added. \nThe only concurrent activities that can happen when an activity I is adding a copy or points edge (a, \nv) are the following. Another activity reads an edge that starts at node a. This edge cannot be removed \nby I.  Another activity adds an edge (a, w) such that v w.  = This new edge cannot affect the update \nrule executed by I, because it does not depend on that edge.  Another activity tries to add the same \nedge (a, v). The two rules can be interleaved in any fashion and the .nal state will be the same, provided \nthe concrete representa\u00adtion of the edge set is properly synchronized to ensure that the edge is added \nonly once. The work performed by one of the activities is redundant, but it is irrelevant which one actually \nperforms the addition of the edge. It is important to note that the elimination of abstract locks depends \ncritically on the fact that the operators (rewrite rules) are extensive3: they only add edges, and never \nremove nodes or edges from the graph4. Eliminating copying of data: Since there are no con.icts between \nconcurrently executing rewrite rules, there are no rollbacks, so it is unnecessary to make backup copies \nof modi.ed data. Aborted activities: There are no aborted activities. Some activities may perform a small \namount of redundant work since the edge they want to add may have been added by a concurrent activity, \nwhich cannot happen in a sequential implementation. Therefore, the remaining overhead is the dynamic \nassign\u00adment of work. Since each activity performs a relatively small computation, the overhead of adding \nand removing work from the centralized work-list can be substantial. To reduce this performance penalty, \nwe use iteration coalescing [23], which can be viewed as a data-centric version of loop chunk\u00ading [27]. \nWhen an activity adds an edge to the graph, it checks to see if the new edge violates any invariants \nat the source node. If so, it puts the work on a local work-list of its own, and performs those activities \nitself, rather than putting them on the global work-list. Intuitively, the optimization in\u00adcreases the \ngranularity of the work and reduces the perfor\u00admance impact of having a global work-list.  5.2 Discussion \nIn our current implementation of inclusion-based points-to analysis, the optimizations described in this \nsection are im\u00adplemented by hand. However, we believe it should be pos\u00adsible to automate these optimizations. \nThe key observation is that the rewrite rules of Figure 6 are extensive operators, and this is straight-forward \nfor a static analysis to deduce from the Galois code since this code uses the Galois graph API, and properties \nof API methods are available to the static analysis. Note that this kind of analysis would be almost \nim\u00ad 3 In mathematics, if D is a partially ordered set, f : D . D is extensive if x = f(x). 4 If the baseline \nsystem used commutativity of method invocations to detect con.icts [18], the system would correctly report \nthat concurrent applica\u00adtions of the rewrite rules do not con.ict with each other. However, checking \ncommutativity of graph API method invocations is expensive in general. To eliminate these checks for \nthis particular problem, the system needs global information that all graph API method invocations in \nthe body of the opera\u00adtor commute, which is similar to what is needed when con.icts are checked using \nexclusive abstract locks. EdgeName equivalence hcd Figure 10. Additional edge types to support merging \n possible if the code were written in a language like C without using data abstractions. A second issue \nis whether it is worth implementing this analysis and these optimizations in a general-purpose sys\u00adtem. \nIn other words, how common are extensive operators in practice? Notice that any .ow-insensitive data.ow \nanalysis involves monotone and extensive operators, so these opti\u00admizations are useful for all such analyses. \n6. Node coalescing Fahndrich et al [7] introduced a key optimization of the basic algorithm for inclusion-based \npoints-to analysis described in Section 2. They observed that the constraint systems of many programs \ncan be reduced substantially by eliminating cycles of copy constraints. For example, a pair of statements \nof the form a = b; b = a; produces a cycle of constraints that imply that pts(a) = pts(b). We call these \nequivalent nodes; coalescing equivalent nodes into a single node reduces the size of the problem. Unfortunately, \ncycles can also arise dynamically during the constraint process, so we cannot .nd all equivalent nodes \nduring preprocessing. In the literature, cycle detection comes in two .avors: of.ine methods [29] look \nfor cycles during a preprocess\u00ading phase, while online methods [7] look for cycles dur\u00ading the constraint \nsolving process. Some intermediate tech\u00adniques, such as Hybrid Cycle Detection [8] (HCD), com\u00adbine the \ntwo: potential cycles are identi.ed in the of.ine phase, and these are collapsed during analysis. Potential \ncy\u00adcles arise from statements of the form *a = b; b = *a;. With\u00adout knowing pts(a), we do not know the \nnodes that partici\u00adpate in cycles with b, so these cycles cannot be eliminated during preprocessing, \nbut we can remove them during the constraint solving process whenever we add nodes to pts(a). We experimented \nwith these alternatives, and ultimately set\u00adtled on combining of.ine techniques [9] with HCD. Our ex\u00adperiments, \nwhich are described in more detail in Section 7, showed that this is faster than online cycle detection \neven for serial execution; moreover, HCD is easier to parallelize, making it the preferred choice for \na parallel implementation. In this section, we focus on the parallelization of the on\u00adline phase of HCD, \nwhich coalesces nodes that must have the same point-to sets. In principle, all that needs to be done \nis to replace the two nodes being coalesced with a single node that inherits all the edges incident on \nthe eliminated nodes. However, removing nodes from a graph in a parallel imple\u00admentation can be expensive, \nso we perform this operation Invariant Rewrite rule -. v ..a -. b = ..b -- v  (a) detect rule h p \n.a ure 10. An hcd edge a p  - . v, = and a is equivalent to b, then there must be a points edge - . \nv. p The push rule states that if there is a points edge a .a . v ..a . .b-  . v p p- -- b b The transitive \nrule ensures the correct propagation of equivalences throughout the analysis graph. It exploits (b) \npush rule the transitive nature of the given equivalence: if a is equivalent to b, and b is equivalent \nto c, then a is equiva\u00adlent to c. == .a -- b ..a -- v = ..b -- v  (c) transitive rule Figure 11. Rewriting \nrules to support node coalescing p- While the detection rule is particular to HCD, the push and transitive \nrules are applicable to the coalescing phase of any other algorithm that exploits cycles. . x p- lent \nvariables are identical, thereby adding the edges a . x. and v implicitly in a way that permits coalescing \nto happen in par\u00adallel with graph rewriting for constraint solving. A simple scheme that accomplishes \nthis is described in Section 6.1. Our actual implementation is a re.nement of this scheme, as described \nin Section 6.2. 6.1 Rewrite rules for equivalent nodes We start by adding two new types of edges, shown \nin Fig\u00ad b indicates that b and all the  6.2 Parallelization and optimizations Like the constraint graph \nrewriting rules of Section 3, the new rewrite rules are extensive operators and do not delete nodes or \nedges from the graph. Therefore, we can apply the same reasoning as in Section 5 to devise a parallel \nimple\u00admentation of the extended algorithm that does not require any abstract locking. On the other hand, \nthis baseline merging scheme has the disadvantage that a points edge is propagated to all equiva\u00ad h \n-. variables in *a can be coalesced; these edges are added by lent nodes. A better approach is to implement \na union-.nd data structure [34] to track equivalent nodes, and propagate points edges only to representative \nnodes wherever possi\u00ad ble. In our implementation, the union-.nd data structure is overlayed with the \nconstraint graph by making equivalence edges directional (these are called representative edges). The \nrepresentative for a set of equivalent nodes is the node with the highest ID. Because of concurrent graph \nrewriting, a node may have several outgoing representative edges. These the HCD preprocessing phase. \nAn equivalence edge such as = a -- b indicates that the points-to sets of a and b are equal, so they \ncan be coalesced. The analysis graph is initialized as in the basic algorithm in Section 3, except that \nhcd edges are added by the of\u00ad .ine detection phase of HCD. This initial graph contains no equivalence \nedges. The set of basic graph rewrite rules in Figure 6 is augmented with three additional rules, shown \nin Figure 11. edges are kept sorted by the node IDs of the destination nodes of these edges; when propagating \npoints edges, we The detect rule implements the HCD cycle detection pol-only propagate edges along the \n.rst edge in this sequence. A icy: if there is an hcd edge between a and b, then every detailed description \nof a concurrent union-.nd data structure variable v pointed by a is equivalent to b. implemented along \nthese lines will appear elsewhere. 7. Experimental evaluation Our implementation of the parallel pointer \nanalysis is written in Java. To provide a reasonable comparison with an existing state-of-art analyzer, \nour code is structured along the lines of the C++ serial implementation of [9], which is available at \nhttp://www.cs.ucsb.edu/ benh/. For the rest of this section, we will refer to this C++ implementation \nas the reference implementation. 7.1 Key data structures The points-to algorithm uses two key data structures \nfor the representation of the constraint graph: the Binary Decision Diagram [5] (BDD), used for storing \nthe points-to edges, and the sparse bit vector, used for storing all the other types of edges. In addition \nto concurrent versions of these data struc\u00adtures, we implemented a thread-safe version of the work-list. \nA BDD is a rooted acyclic directed graph. We represent the graph nodes using objects, which are stored \nin a concur\u00adrent hash table. Another concurrent hash table is used for implementing the cache that stores \nthe results of previous operations on the BDD, as it is done in the popular BuDDy library [22]. Synchronization \non the hash table is achieved by protecting segments of contiguous buckets with a reen\u00adtrant lock. Memory \nspace occupied by BDD nodes that are no longer in use is reclaimed by the Java garbage collector, because \nthe hash table has support for weak references. In Java, objects that are reachable only by weak references \ncan be collected by the GC, as opposed to objects reachable by at least one strong reference. By relying \non the automatic memory man\u00adagement provided by the virtual machine, our BDD imple\u00admentation does not \nrequire the use of reference counts, as it is needed in C implementations like BuDDy. Another (non concurrent) \nBDD package that takes advantage of the JVM s memory management is SableJBDD [28]. Previous studies have \nhighlighted the importance of vari\u00adable ordering in BDD implementations [4]. Suboptimal or\u00adderings result \nin a BDD with a large number of nodes, con\u00adsuming more memory and slowing down set operations. In this \nwork, we used the same variable ordering as the one used by the reference implementation. A sparse bit \nvector is another standard representation used for the compact encoding of a (sparse) set of elements. \nIn our case, the different sets of outgoing edges associated with each node are represented using sparse \nbit vectors based on concurrent doubly linked lists. Each node in the linked list contains a double word \nthat can store up to 64 elements on it. The work-list is represented using a concurrent vector. We use \na dual [24] work-list, which is divided into two sections: current and next. Active nodes are selected \nfrom the current section and pushed onto next, and the two are swapped when current becomes empty. The \ndivided work\u00adlist not only results on better performance than a single work-list, but also in less contention: \na thread that wants to add an element to a work-list does not have to wait for threads that are getting \nelements from the work-list. In fact, retrievals could proceed without synchronization, given that the \ncurrent work-list size is known when the two work-lists are swapped: we could simply assign each thread \nto a certain index range in the work-list. However, this approach does not perform well in practice because \nit results on poor load balancing. Variables Constraints Program initial reduced initial reduced perl \n53,358 6,031 68,645 10,926 nh 97,933 21,943 114,459 38,178 gcc 120,867 20,053 156,276 32,193 vim 246,941 \n22,420 108,271 37,422 python 92,596 23,595 111,531 43,534 svn 107,705 30,482 139,848 59,939 gdb 232,811 \n51,044 241,592 84,238 pine 612,913 60,657 315,900 120,840 php 339,535 59,960 325,891 108,156 gimp 558,864 \n153,231 649,590 284,252 Figure 13. Benchmark suite: number of variables and con\u00adstraints.  As in other \nunordered algorithms, any order of process\u00ading active nodes is legal but the amount of computation performed \nby different orders may be different. Our imple\u00admentation uses the same LRF (Least Recently Fired) strat\u00adegy \nas the reference implementation. This scheduling pol\u00adicy gives priority to nodes processed furthest back \nin time, thereby promoting a breadth-.rst propagation of information through the graph. Although the \nJava library provides con\u00adcurrent sets based on skip lists, our experiments showed that a simpler alternative \nperforms better in practice: we keep an atomic .ag for every node in the graph to indicates whether it \nis in the work-list or not (so the work-list becomes a work\u00adset), and the LRF order is enforced by sorting \nthe next work\u00adlist at swap time. 7.2 Experimental setup Figure 13 shows the benchmark suite used in \nour experi\u00adments. It consists of ten C programs ranging from 53K-558K variables and 68K-649K constraints. \nMost of the programs in our benchmark suite have been used by other researchers in this area [9, 25]. \nThe input programs are parsed using the LLVM compiler to generate the initial variables and constraints. \nIn Fig 13, the numbers of variables and constraints are shown before and after of.ine analysis. In our \nexperiments, we use the three of.ine algorithms available in the reference implementation: HVN, HRU [9], \nand HCD. The of.ine phase results on a constraint graph at least 50% smaller than the original. The machine \nused in our experiments is a Sun Fire X2270 (Nehalem server) running Ubuntu Linux version 8.06. The system \ncontains two quad-core 2.93 GHz Intel Xeon pro\u00adcessors. The 8 CPUs share 24 GB of main memory. Each core \nhas two 32 KB L1 caches and a uni.ed 256 KB L2 cache. Each processor has an 8 MB L3 cache that is shared \namong the cores. The Java Virtual Machine is the 32-bit Sun HotSpot server virtual machine version 1.6.0 \n20.  7.3 Justifying the use of HCD Our .rst set of experiments evaluates the performance of the (sequential) \nreference implementation using the HCD and LCD approaches. The objective is to justify the choice of \nthe HCD algorithm for the parallel implementation. The analysis times (in milliseconds) are shown in \nFig\u00adure 14. Each benchmark was run three times, and the av\u00aderage runtime is reported. For every benchmark \nin the input suite, we evaluated the effect of the two online optimizations present in the reference \nimplementation: HCD and LCD [8]. All the four possible combinations obtained by enabling/dis\u00adabling these \ntechniques are shown in the table of results. The analysis time is divided into two columns. The of.ine \ncolumn refers to the time spent in the HVN, HRU, and HCD (if applicable) of.ine algorithms. The online \ncolumn refers to the time consumed by constraint solving. The total column is the sum of the of.ine and \nonline runtimes. We omitted the time spent loading the constraints from disk and initializing the BDD \n(these consume less than 10% of the total analysis time). To facilitate the comparison between the different \ncon\u00ad.gurations we show a impr (improvement) column, which is the result of dividing the total runtime \nwith no optimiza\u00adtions (HCD=no, LCD=no) by the total runtime for the given con.guration. Higher values \nof impr indicate better per\u00adformance. For example, in the case of the perl benchmark we get impr = 1608/428 \n= 3.8 by using only HCD, and impr = 1608/536 = 3.0 by using only LCD. The results show the following. \nThe application of any of the cycle detection techniques drastically reduces the overall analysis time \nfor all the inputs. This effect is more evident for large benchmarks. For instance, gimp takes 12 minutes \nto complete in the absence of any optimization, but when LCD or HCD are applied, the analysis .nishes \nin less than 30 seconds.  We can achieve nearly optimal runtimes just by apply\u00ading HCD. The addition \nof Lazy Cycle Detection results in improvements in performance for half of the bench\u00admarks, but the gain \nis very small except in the case of python. These results justify the choice of HCD for node coalesc\u00ading \nin our parallel implementation.  7.4 Results of parallel implementation Figure 15 show the analysis \ntimes (in milliseconds) for each program in the benchmark suite, and different numbers of threads. To \naccount for the effects of JIT compilation, each benchmark was run nine times, and the average runtime \nis reported. We veri.ed the output (points-to of every variable in the original program) after the .rst \nrun against the refer\u00adence implementation. Finally, we minimize the in.uence of garbage collection by \nmaximizing the size of the heap used by the JVM. As in Figure 14, the analysis time is divided into two \nphases. Because the of.ine phase is sequential, the time spent on it does not change for different numbers \nof threads. The online column refers to the time taken by our parallel implementation of constraint solving, \nwhich uses HCD but not LCD. The .rst observation about the results in Figure 15 is that the analysis \ntimes are not always proportional to the input size. This also applies to the reference implementation. \nFor instance, the number of variables and constraints in the python benchmark is similar to that of vim, \nbut the analysis of python takes twice as much time to complete. Clearly the structure of the original \nconstraint graph plays a fundamental role in the analysis time. Sparse constraint graphs demand little \npoints-to propagation, so the algorithm will converge rapidly. Figure 15 shows the runtimes of our parallel \nimplemen\u00adtation on different numbers of threads. The scaling of a par\u00adallel program is the execution \ntime for one thread divided by the execution time for x threads. The scaling achieved is dependent on \nthe amount of computation that needs to be done, and tends to improve when the one-thread execution times \nget bigger. For small benchmarks (perl, nh and gcc) the performance degrades when we use many threads, \nsince the amount of computation to be done is so small -the online analysis of nh .nishes within 273ms \nwith one thread-that the overheads of parallelization dominate the total runtime. For all the other inputs \nwe achieve a good scaling by us\u00ading two threads, although the best total runtime is achieved with eight \nthreads. It is interesting to see that the best scal\u00ading (2.85x) is obtained for python, while for our \nbiggest input program (gimp) the scaling is almost 2x. Because we only parallelized the constraint solving \nphase of the analysis, the of.ine section now becomes a perfor\u00admance bottleneck. For example, the of.ine \nphase of php rep\u00adresents the 20% of the total runtime if we use one thread, but the ratio increases to \n35% with eight threads. We leave the parallelization of the of.ine phase as future work. 7.5 Comparative \nstudy We now describe a comparative study of the results obtained for the parallel and reference implementations, \nusing only HCD. While we con.gured the two implementations so they use the same online and of.ine techniques, \nour analysis (as the rest of the Galois framework) is written in Java, while Hardekopf s implementation \nis written in C++. Therefore, establishing a completely fair comparison is extremely dif\u00ad.cult: one language \nis interpreted, the other is compiled; the reference implementation uses the BuDDy BDD library, while \nwe implemented a pure Java BDD package that re\u00adlies on the garbage collector for eliminating dead nodes, \netc.   Nevertheless, the value of comparing the runtimes of the two analyses lies then in showing that \nour parallel implementa\u00adtion is competitive with a highly tuned, state-of-the-art C++ implementation; \nin most cases, our parallel Java implemen\u00adtation achieves a speed-up over the optimized C++ sequen\u00adtial \nimplementation. We plotted the overall runtime for the two analyses in Figure 16. Each dark triangle \nmarker (.) in a plot corre\u00adsponds to the online runtime of our analysis for that partic\u00adular combination \nof benchmark and number of threads (on\u00adline column in Figure 15). Each bright square marker (_) in a \nplot corresponds to the online runtime of the reference im\u00adplementation for that particular benchmark \nin the HCD=yes, LCD=no row of Figure 14. The overheads of the parallel analyzer can be observed when \nonly one thread is used: our implementation can be twice as slow as the reference program, even though \nour (se\u00adquential) of.ine phase usually takes less time to complete. The bene.ts of using a parallel implementation \nbecome clear for larger thread counts since we usually achieve a signi.\u00adcant speedup over the C++ implementation. \nFigure 17 provides a comparison between the total analy\u00adsis times for the parallel and reference implementations. \nThe times shown for the parallel version correspond to the fastest execution. The best speedup achieved \nfor the whole analysis is 2.5x for the python benchmark (note that Figure 16 shows only the online analysis \ntimes).  7.6 Memory consumption Figure 18 shows the memory consumption (in megabytes) of the parallel \nand reference implementations among the ten in\u00adput benchmarks. The numbers in the table correspond to \nthe maximum amount of memory required by each analyzer. We measured the memory usage of the Java (parallel) \nprogram using the tools provided by the Hot Spot virtual machine; the reference implementation uses the \npro.ler available in the Google Performance Tools [31]. The memory consumption of our code is signi.cantly \nhigher (a factor of 1.5-2.5x). The BDD and the sparse bit Figure 18. Memory usage, in megabytes. vectors \ndominate the memory usage of both applications. Since we are using the same data structures (but concurrent \nversions) and cache policies as the reference implementa\u00adtion, we believe these differences might arise \nfrom the fact that our implementation is in Java, while the reference im\u00adplementation is in C++. 8. Conclusions \nand future work This paper describes the .rst parallel implementation of a state of the art inclusion-based \npoints-to analysis. Irregular algorithms of this kind are very dif.cult to parallelize be\u00adcause dependences \nbetween computations are functions of runtime data, so speculation is needed in general to exploit the \nparallelism. However, the overheads of speculation can be reduced dramatically by exploiting algorithmic \nstructure. In our experience, reasoning about parallelism in terms of the operator formulation of algorithms \ngreatly simpli.es the task of devising an ef.cient version of the algorithm. The operator formulation \nexposed algorithmic structure that we exploited in optimizations to reduce the overheads of specu\u00adlative \nexecution. Our parallelization of Andersen s algorithm exploits the fact that the state of the analysis \ngrows monotonically during the execution of the algorithm; tree-building algorithms such as Prim s MST \nalgorithm and n-body simulation algorithms also have this structure, which can be exploited for ef.cient \nparallel implementation. The experimental results in Section 7 con.rm that our ap\u00adproach is not only \nfeasible, but can also be competitive with highly tuned sequential implementations of pointer analysis. \nFurther improvements require attention to parallelizing the of.ine component of the analysis. Our implementation \ndepends on two data structures: the hash table used for representing the BDD, and the doubly linked list \nused for representing a sparse bit vector. The con\u00adcurrent versions of these two popular data types use \nlocking (mutexes). It would be interesting to study the performance of the analysis when lock-free implementations \n[11, 33] of the two data structures are used. Acknowledgments The authors would like to thank Ben Hardekopf \nfor gener\u00adously providing the benchmarks used in our experiments and clarifying many aspects of his implementation. \nReferences [1] Galois website. http://iss.ices.utexas.edu/ galois/. [2] A. Aho, R. Sethi, , and J. Ullman. \nCompilers: principles, techniques, and tools. Addison Wesley, 1986. [3] L. O. Andersen. Program Analysis \nand Specialization for the C Programming Language. PhD thesis, DIKU, University of Copenhagen, May 1994. \n(DIKU report 94/19). [4] Marc Berndl, Ondrej Lhot\u00b4 ak, Feng Qian, Laurie Hendren, and Navindra Umanee. \nPoints-to analysis using BDDs. In PLDI 03: Proceedings of the ACM SIGPLAN 2003 confer\u00adence on Programming \nlanguage design and implementation, pages 103 114, New York, NY, USA, 2003. ACM. [5] Randal E. Bryant. \nGraph-based algorithms for boolean func\u00adtion manipulation. IEEE Transactions on Computers, 35:677 691, \n1986. [6] H. Ehrig and M. L\u00a8owe. Parallel and distributed derivations in the single-pushout approach. \nTheoretical Computer Science, 109:123 143, 1993. [7] Manuel F\u00a8ahndrich, Jeffrey S. Foster, Zhendong Su, \nand Alexander Aiken. Partial online cycle elimination in inclusion constraint graphs. In PLDI 98: Proceedings \nof the ACM SIG-PLAN 1998 conference on Programming language design and implementation, pages 85 96, New \nYork, NY, USA, 1998. ACM. [8] Ben Hardekopf and Calvin Lin. The ant and the grasshopper: fast and accurate \npointer analysis for millions of lines of code. In PLDI, 2007. [9] Ben Hardekopf and Calvin Lin. Exploiting \npointer and loca\u00adtion equivalence to optimize pointer analysis. In SAS, pages 265 280, 2007. [10] Nevin \nHeintze and Olivier Tardieu. Ultra-fast aliasing analysis using cla: a million lines of c code in a second. \nSIGPLAN Not., 36(5):254 263, 2001. [11] Maurice Herlihy and Eric Koskinen. Transactional boosting: a \nmethodology for highly-concurrent transactional objects. In PPoPP 08: Proceedings of the 13th ACM SIGPLAN \nSym\u00adposium on Principles and practice of parallel programming, pages 207 216, New York, NY, USA, 2008. \nACM. [12] Maurice Herlihy and J. Eliot B. Moss. Transactional memory: architectural support for lock-free \ndata structures. In ISCA, 1993. [13] Michael Hind. Pointer analysis: haven t we solved this prob\u00adlem \nyet? In PASTE 01: Proceedings of the 2001 ACM SIGPLAN-SIGSOFT workshop on Program analysis for soft\u00adware \ntools and engineering, pages 54 61, New York, NY, USA, 2001. ACM. [14] Vineet Kahlon. Bootstrapping: \na technique for scalable .ow and context-sensitive pointer alias analysis. In PLDI 08: Pro\u00adceedings of \nthe 2008 ACM SIGPLAN conference on Program\u00adming language design and implementation, pages 249 259, New \nYork, NY, USA, 2008. ACM. [15] Ken Kennedy and John Allen, editors. Optimizing compil\u00aders for modern \narchitectures: a dependence-based approach. Morgan Kaufmann, 2001. [16] J. W. Klop, Marc Bezem, and R. \nC. De Vrijer, editors. Term Rewriting Systems. Cambridge University Press, New York, NY, USA, 2001. [17] \nVenkata Krishnan and Josep Torrellas. A chip-multiprocessor architecture with speculative multithreading. \nIEEE Trans. Comput., 48(9):866 880, 1999. [18] M. Kulkarni, K. Pingali, B. Walter, G. Ramanarayanan, \nK. Bala, and L. P. Chew. Optimistic parallelism requires abstractions. SIGPLAN Not. (Proceedings of PLDI \n2007), 42(6):211 222, 2007. [19] Milind Kulkarni, Martin Burtscher, Rajasekhar Inkulu, Ke\u00adshav Pingali, \nand Calin Casc\u00b8aval. How much parallelism is there in irregular applications? In Proceedings of the 14th \nACM SIGPLAN symposium on Principles and practice of par\u00adallel programming, pages 3 14, New York, NY, \nUSA, 2009. ACM. [20] Milind Kulkarni, Keshav Pingali, Ganesh Ramanarayanan, Bruce Walter, Kavita Bala, \nand L. Paul Chew. Optimistic par\u00adallelism bene.ts from data partitioning. SIGARCH Comput. Archit. News, \n36(1):233 243, 2008. [21] Chris Lattner and Vikram Adve. LLVM: A Compilation Framework for Lifelong Program \nAnalysis &#38; Transformation. In Proceedings of the 2004 International Symposium on Code Generation \nand Optimization (CGO 04), Palo Alto, Califor\u00adnia, Mar 2004. [22] Jorn Lind-Nielsen. Buddy, a Binary \nDecision Diagram pack\u00adage. http://www.itu.dk/research/buddy/. De\u00adpartment of Information Technology, \nTechnical University of Denmark. [23] Mario Mendez-Lojo, Donald Nguyen, Dimitrios Prount\u00adzos, Xin Sui, \nM. Amber Hassaan, Milind Kulkarni, Martin Burtscher, and Keshav Pingali. Structure-driven optimizations \nfor amorphous data-parallel programs. In Proceedings of the 15th ACM SIGPLAN symposium on Principles \nand practice of parallel programming, pages 3 14, January 2010. [24] Flemming Nielson, Hanne R. Nielson, \nand Chris Hankin. Principles of Program Analysis. Springer-Verlag New York, Inc., Secaucus, NJ, USA, \n1999. [25] Fernando Magno Quintao Pereira and Daniel Berlin. Wave propagation and deep propagation for \npointer analysis. In CGO 09: Proceedings of the 2009 International Symposium on Code Generation and Optimization, \npages 126 135, Wash\u00adington, DC, USA, 2009. IEEE Computer Society. [26] Keshav Pingali, Milind Kulkarni, \nDonald Nguyen, Martin Burtscher, Mario Mendez-Lojo, Dimitrios Prountzos, Xin Sui, and Zifei Zhong. Amorphous \ndata-parallelism in irregular al\u00adgorithms. regular tech report TR-09-05, The University of Texas at Austin, \n2009. [27] C. D. Polychronopoulos and D. J. Kuck. Guided self\u00adscheduling: A practical scheduling scheme \nfor parallel super\u00adcomputers. IEEE Trans. Comput., 36(12):1425 1439, 1987. [28] Feng Qian. SableJBDD, \na Java Binary Decision Diagram Package. http://www.sable.mcgill.ca/ fqian/ SableJBDD/. [29] Atanas Rountev \nand Satish Chandra. Off-line variable substi\u00adtution for scaling points-to analysis. In PLDI 00: Proceedings \nof the ACM SIGPLAN 2000 conference on Programming lan\u00adguage design and implementation, pages 47 56, New \nYork, NY, USA, 2000. ACM. [30] Erik Ruf. Partitioning data.ow analyses using types. In POPL 97: Proceedings \nof the 24th ACM SIGPLAN-SIGACT symposium on Principles of programming languages, pages 15 26, New York, \nNY, USA, 1997. ACM. [31] Craig Silverstein. Google Performance Tools. http:// code.google.com/p/google-perftools/. \n[32] Bjarne Steensgaard. Points-to analysis in almost linear time. In POPL 96: Proceedings of the 23rd \nACM SIGPLAN- SIGACT symposium on Principles of programming languages, pages 32 41, New York, NY, USA, \n1996. ACM. [33] H\u00b0akan Sundell and Philippas Tsigas. Lock-free deques and doubly linked lists. J. Parallel \nDistrib. Comput., 68(7):1008 1020, 2008. [34] Robert Endre Tarjan. Ef.ciency of a good but not linear \nset union algorithm. J. ACM, 22(2):215 225, 1975. [35] John Whaley and Monica S. Lam. Cloning-based context\u00adsensitive \npointer alias analysis using binary decision dia\u00adgrams. In PLDI 04: Proceedings of the ACM SIGPLAN 2004 \nconference on Programming language design and implemen\u00adtation, pages 131 144, New York, NY, USA, 2004. \nACM. [36] Sean Zhang, Barbara G. Ryder, and William Landi. Program decomposition for pointer aliasing: \na step toward practical analyses. SIGSOFT Softw. Eng. Notes, 21(6):81 92, 1996.    \n\t\t\t", "proc_id": "1869459", "abstract": "<p>Inclusion-based points-to analysis provides a good trade-off between precision of results and speed of analysis, and it has been incorporated into several production compilers including gcc. There is an extensive literature on how to speed up this algorithm using heuristics such as detecting and collapsing cycles of pointer-equivalent variables. This paper describes a complementary approach based on exploiting parallelism. Our implementation exploits two key insights. First, we show that inclusion-based points-to analysis can be formulated entirely in terms of graphs and graph rewrite rules. This exposes the amorphous data-parallelism in this algorithm and makes it easier to develop a parallel implementation. Second, we show that this graph-theoretic formulation reveals certain key properties of the algorithm that can be exploited to obtain an efficient parallel implementation. Our parallel implementation achieves a scaling of up to 3x on a 8-core machine for a suite of ten large C programs. For all but the smallest benchmarks, the parallel analysis outperforms a state-of-the-art, highly optimized, serial implementation of the same algorithm. To the best of our knowledge, this is the first parallel implementation of a points-to analysis.</p>", "authors": [{"name": "Mario M&#233;ndez-Lojo", "author_profile_id": "81388602261", "affiliation": "Institute for Computational Engineering and Sciences, University of Texas, Austin, TX, USA", "person_id": "P2354087", "email_address": "", "orcid_id": ""}, {"name": "Augustine Mathew", "author_profile_id": "81470645380", "affiliation": "Dept. of Computer Science. University of Texas, Austin, TX, USA", "person_id": "P2354088", "email_address": "", "orcid_id": ""}, {"name": "Keshav Pingali", "author_profile_id": "81100554731", "affiliation": "Dept. of Computer Science. University of Texas, Austin, TX, USA", "person_id": "P2354089", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1869459.1869495", "year": "2010", "article_id": "1869495", "conference": "OOPSLA", "title": "Parallel inclusion-based points-to analysis", "url": "http://dl.acm.org/citation.cfm?id=1869495"}