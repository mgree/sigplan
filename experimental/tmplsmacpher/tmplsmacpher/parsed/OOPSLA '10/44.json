{"article_publication_date": "10-17-2010", "fulltext": "\n Task Types for Pervasive Atomicity Aditya Kulkarni Yu David Liu Scott F. Smith SUNY Binghamton SUNY \nBinghamton The Johns Hopkins University akulkar1@cs.binghamton.edu davidL@cs.binghamton.edu scott@cs.jhu.edu \n Abstract Atomic regions are an important concept in correct con\u00adcurrent programming: since atomic regions \ncan be viewed as having executed in a single step, atomicity greatly re\u00adduces the number of possible \ninterleavings the programmer needs to consider. This paper describes a method for building atomicity \ninto a programming language in an organic fash\u00adion. We take the view that atomicity holds for whole threads \nby default, and a division into smaller atomic regions occurs only at points where an explicit need for \nsharing is needed and declared. A corollary of this view is every line of code is part of some atomic \nregion. We de.ne a polymorphic type system, Task Types, to enforce most of the desired atomicity properties \nstatically. We show the reasonableness of our type system by proving that type soundness, isolation invariance, \nand atomicity enforcement properties hold at run time. We also present initial results of a Task Types \nimplementation built on Java. Categories and Subject Descriptors D.3.3 [Programming Languages]: Language \nConstructs and Features Concurrent Programming Structures General Terms Design, Languages, Theory Keywords \nPervasive Atomicity, Type Systems, Sharing-Aware Programming 1. Introduction In an era when multi-core \nprogramming is becoming the rule not the exception, the property of atomicity that pro\u00adgram execution \nin the presence of interleavings has the same effect as a sequential execution is a crucial in\u00advariant. \nSome programming languages now support a no\u00adtion of atomic block, requiring the block to be viewable \nas executing atomically; this means there will not be any interleavings violating the sequential view \nof that block and program meaning is greatly clari.ed. One weakness of atomic blocks however is that \nguarantees of atomicity hold Permission to make digital or hard copies of all or part of this work for \npersonal or classroom use is granted without fee provided that copies are not made or distributed for \npro.t or commercial advantage and that copies bear this notice and the full citation on the .rst page. \nTo copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c \npermission and/or a fee. OOPSLA/SPLASH 10, October 17 21, 2010, Reno/Tahoe, Nevada, USA. Copyright &#38;#169; \n2010 ACM 978-1-4503-0203-6/10/10. . . $5.00 only in so-marked blocks, and code outside of the marked \nblocks may well have anomalous behaviour upon interleav\u00ading. When the atomicity-enforcing code interleaves \nwith the non-atomicity-enforcing code, a weaker guarantee known as weak atomicity [CMC+06; SMAT+07; ABHI08] \nmay happen. This paper built on top of our previous Coqa language [LLS08] takes the opposite route \nto address atomicity. In\u00adstead of indicating which subparts of a thread should be atomic, a programmer \nof our language divides the thread into subzones of atomic execution, and every single line of code must \nbe part of some atomic zone. This design prin\u00adciple, which we call pervasive atomicity, eliminates weak \natomicity by design. The programming approach is the op\u00adposite of atomic zones: by default threads are \ncompletely atomic and uncommunicative, and speci.c atomicity break points are then inserted where the \nthread needs to commu\u00adnicate with other threads. In addition, since the number of zones is much smaller \nthan the number of program instruc\u00adtions, the conceptual number of program interleavings is sig\u00adni.cantly \nreduced, a boon for program analysis and testing, and ultimately for the deployment of more reliable \nsoftware. Unfortunately, Coqa is not ideal: it requires dynamic monitoring to limit object sharing between \nthreads. Dynamic monitoring mechanisms are known to incur heavy overhead, which can be particularly bad \nhere since every object may need to be monitored. They can also suffer from problems of deadlock or livelock. \nOur initial Coqa compiler relied on ad hoc optimizations to achieve tolerable performance. What is needed \nis a principled means to keep the bene.ts of pervasive atomicity, but without the high cost. This paper \nanswers this need by developing a static, declarative method for dividing objects between threads, Task \nTypes. It is common knowledge [CGS+99; WR99; Bla99] that if an object is accessed only by one thread, \nthen dynamic atomicity enforcement on that object is unneces\u00adsary. Task Types lift this simple notion \nto the programming level, effectively enforcing a non-shared memory model by default at compile time. \nFigure 1(a) illustrates how objects are statically localized in threads via Task Types. Here the two \nrectangular boxes represent two runtime threads, called tasks in our language. The solid black objects \nare special task objects which launch a new task every time they are invoked, while the white objects \nare the non-shared ordinary objects, the vast ma\u00ad  Figure 1. Isolation and Sharing at Run Time jority \nof objects that are accessed by only one task. Our type system statically guarantees this picture of \nisolation. Making non-shared-memory the default case has parallels with Actor-based languages [Agh90; \nArm96; SM08], MPI [GLS94] and DPJ [BAD+09] amongst others. Compared to these approaches, Task Types aim \nto get the bene.ts of these models while offering a more familiar setting for program\u00admers: only minor \nchanges need to be made to most Java pro\u00adgrams to make them compilable by the Task Types compiler. One \nreason why fewer program changes are required is that we support limited inter-task sharing, unlike the \nabove mod\u00adels. Fig. 1(b) and Fig. 1(c) are examples of forms of inter\u00adtask object sharing that we support. \nIn Fig. 1(b), two tasks communicate through shared task objects, special objects which may themselves \nhold a set of non-shared ordinary objects and serve as the rendezvous point for other tasks. An important \nbene.t of limited sharing is the degree to which atomicity properties are preserved: the sending task \nhas one zone of atomicity from the start to the shared task object invocation, the shared task itself \nis an atomic zone, and the task execution after the invocation is .nished is a third zone of atomicity. \nSo, our approach is a compromise between the extreme lack of sharing of Ac\u00adtor or Actor-like languages \n[Agh90; Arm96; SM08; GLS94; BAD+09] and the uncontrolled sharing of current multi\u00adthreaded languages; \nby aiming in the middle we can achieve a reasonable compromise between ease of programmability and the \nproduction of reliable code. Fig. 1(c) shows an additional form of sharing that we support, the shared \nordinary objects. These objects allow Coqa-style sharing to be used in limited cases within Task Types: \nonly one task may use a shared ordinary object at a time, so no atomicity of tasks is ever violated due \nto access of these objects. As in Coqa, run-time support is needed to ensure two different tasks never \naccess such an object at the same time. The programming choice between using a shared task object or \na shared ordinary object re.ects a clear choice between more parallelism or more atomicity. Ownership \ntypes [CPN98; Cla01; BLR02] and region types [TT97; Gro03; CCQR04] are well-known type-based techniques \nfor static partitioning of memory. Task Types are strongly related to such systems, but differ in two \nimpor\u00adtant aspects: explicit sharing exceptions are allowed in Task Types, and static type variables \nmust invariably align with runtime tasks. 2. Informal Discussion In this section, we highlight a number \nof features of our lan\u00adguage, focusing on its type system. We use a simpli.ed Map-Reduce algorithm [DG04] \nto illustrate basic language fea\u00adtures; the code is in Fig. 2. Map-Reduce represents a com\u00admon type of \nmulti-core algorithm, of the embarrassingly parallel style. Later in this section, we will discuss a \npro\u00adgram of the opposite nature, a high-contention PuzzleSolver [LLS08]. Together, we aim to provide \nreaders a real feel of programming in Task Types. 2.1 Sharing-Aware Programming Task Types encourage \nprogrammers to make upfront shar\u00ading decisions, by associating class modi.ers \u00b5 with classes. The default \nchoice here (\u00b5 = E) aligns precisely with the principle of having non-shared memory as the default. The \nobjects instantiated from these classes are non-shared ordi\u00adnary objects, and messaging to these objects \nuses the stan\u00addard Java dot (.) invocation symbol. For instance, the code for MapReduce in Fig. 2 indicates \nWorkUnit is not shared. The WorkUnit object encapsulates data and a unit of work that needs to be done \non the data, and each such instance is indeed exclusively used by each Mapper. Here Mapper is declared \nas a task, meaning each Mapper object is a (non-shared) task object. Each such object spawns a new task \n(thread) when sent a message, in analogy to how an actor handles a message [Agh90]. Non\u00adshared tasks \nare simply threads with a distinguished object representative, and the execution of the body of the invoked \nmethod constitutes the lifetime of a task. Note that the com\u00adpletion of the task does not end the lifetime \nof the task object or any state associated with it the object may later receive and handle another message \nfollowing the Actor model. Here the main method s expression m -> map(ul, r) creates a non-shared task \nby sending a map message to the entry object m. Such invocations are asynchronous and have no interesting \nreturn value. Here, twenty Mapper threads are executing in parallel. Individually, each task object keeps \na queue of all received messages and processes them serially, following Actors. This sequential processing \nconstraint is to preserve atomicity; if multiple tasks need to run in paral\u00adlel, the programmer multiply \ninstantiates the same task class multiple times, and this is illustrated by the twenty Mapper task objects \nin the example. Any shared classes must be explicitly declared, and the exclamation mark (!) also must \nbe used in the sym\u00ad  task class Mapper { task class Main { void map(Loader ul,Reducer r) { void main() \n{ WorkUnit wu = ul !->loadWorkUnit(); int NUM = 20; r !->reduce(wu.work()); CtrFactory cf = new CtrFactory(NUM); \n} r= new Reducer(cf); }ul = new Loader(cf); shared task class Reducer { for(i=1; i <= NUM; i++) { int \nsum=0; Mapperm= new Mapper(); Counter toReduce; void Reducer(CtrFactory cf) { toReduce = cf !.newCtr(); \n} void reduce(int rt) { sum = sum + rt; toReduce.dec(); if(toReduce.val() == 0) { ...output sum ...} \n}}shared task class Loader { Counter toLoad; Loader(CtrFactory cf) { toLoad = cf !.newCtr(); } WorkUnit \nloadWorkUnit() { toLoad.dec(); return new WorkUnit(this); }}class WorkUnit { Loader l; WorkUnit(Loader \nl) { l = l; } int work() { ...return result ... }}shared class CtrFactory { int i; CtrFactory(int i) \n{ i = i; } Counter newCtr() {return new Counter(i); }}class Counter { int v = 0; Counter(int v) { v \n= v; } void dec() { v--; } int val() {return v; } }  Figure 2. A Simpli.ed Map-Reduce Example  bol \nfor sending messages to these objects so that sharing is highlighted in the source code. In the example, \nboth Reducer and Loader are shared task objects: in partic\u00adular, all twenty Mapper objects share a single \nReducer object to sum up the numerical results, by the invoca\u00adtion r !-> reduce(wu.work()) one by one. \nShared task messaging does not run in parallel with the invoker o!->m(v) is synchronous and blocking \nand does not spawn a thread. This point can be made clear by looking at the ul !-> loadWorkUnit() expression \n the Mapper object has to wait for the return of the WorkUnit. What makes a shared task a task is its \nability to build its own atomicity zone: the shared task maintains its own objects and frees them all \nwhen the shared task ends, i.e. the method returns. This nature of shared tasks helps improve system \nperformance, by not holding onto objects for too long, and makes rendezvous between two live tasks possible. \nShared ordinary objects, such as CtrFactory, can only be accessed over the lifetime of one task at a \ntime, and as such will never break the atomicity of a task. However, they may be accessed by one task \n.rst, and then accessed by another when the .rst task has completed. As a result, shared ordinary objects \nmust be dynamically monitored. Shared task objects and shared ordinary objects are related, but their \neffects on atomicity and parallelism are clearly different. Fig. 3 summarizes the four kinds of objects \nand their mes\u00adsaging. To represent four possibilities, we use the presence or absence of two modi.ers, \ntask and shared; the .rst differ\u00adentiates the execution policy (a task or not), and the second distinguishes \nthe access policy (shared or non-shared). For convenience, we will equivalently view \u00b5 as consisting \nof set of 0-2 keywords. Thus for instance, task ./\u00b5 matches either case in the right column of the .rst \ntable.  2.2 Static Enforcement of Atomicity Our previous Coqa compiler [LLS08] implemented atomic\u00adity \nsolely with locks, and we proved that a running task is atomic when it locks every (ordinary) object \nit accesses; if a task attempts to access an object which is already locked Access Execution atomicity \nzone entry (task . \u00b5) resource (task /. \u00b5) shared (shared . \u00b5) \u00b5 = shared task \u00b5 = shared (shared task \nobject o3) (shared ordinary object o4) isolated (shared /. \u00b5) \u00b5 = task \u00b5 = E (non-shared task object \no2) (non-shared ordinary object o1)  messaging what it is why you should use it o1 .m(v) intra-task \nmessaging promotes mutual exclusion and atomicity o2 ->m(v) task creation promotes parallelism by starting \nup a new thread o3 !->m(v) shared service allows for atomicity-breaking sharing; promotes parallelism \nwith early free o4 !.m(v) shared data allows for atomicity-preserving sharing Figure 3. Four Kinds of \nObjects, Four Kinds of Messaging by another task, it blocks until the locking task execution is completed. \nThe main property of Task Types is that non-shared ordi\u00adnary objects are provably isolated inside at \nmost one runtime task throughout their lifetime, and thus need no mechanism to support mutually exclusive \naccess at runtime. The main goal of the type system is to prove such isolation indeed holds at compile-time, \nand so the run-time lock monitoring can be removed. We now give a high-level description of how limited, \nsafe sharing of ordinary objects is supported in the type system. A key structure involved in typechecking \nis the static access graph, a static directed graph with edges for object access via .eld read/write \nor being sent a message. Since some data sharing needs to be supported, the static ac\u00adcess graph is not \na strict hierarchy; in particular, nodes repre\u00adsenting shared task objects are sharing points. In the \nMapRe\u00adduce example, Loader and Reducer for example share a CtrFactory. The MapReduce static access graph \nis illustrated in a box inside Fig. 2. The graph generated by the type system is the bottom one labeled \nStatic Access Graph. For the purpose of presenting basic ideas however, let us .rst focus on a simpli.ed \nversion of that graph, labeled Pre-Twinned Access Graph in the Figure. Over this graph we de.ne an access \npath to be a path on this graph from a non-shared task object to a non-shared ordinary object; then we \nsay a cut vertex exists in the graph for some non-shared ordinary object o iff all distinct access paths \nto o in fact go through a single cut node in the static access graph. The name of this property comes \nfrom a graph-theoretic property of this invariant, and is formalized as predicate cutExists in Sec. 3.5. \nTo see a potential violation of isolation, suppose we removed the shared task modi.er from class Loader \n(and changed all !-> symbols to . for messaging to its instances). The resulting program is obviously \ntroublesome at runtime different Mapper instances would race to mutate .eld toLoad in class Loader, violating \nits mutual exclusive access and invalidating our atomicity model. Such a bug would be caught by our type \nsystem because there is no cut vertex for access paths of Loader, in this case from Main and from Mapper. \n Note that the typechecking described above subsumes aggregate locking, i.e. if a large data structure \nneeds to be lock protected, there should not be a need to lock each and every element. To see how this \ncan be supported, observe that when the cut vertex above is a shared ordinary object, all non-shared \nordinary objects can hide behind it, and the type system still typechecks. The example above also shows \nthe need for a polymor\u00adphic type system. Two different Counter s, toLoad and toReduce, are created there \nby two invocations of the factory method newCtr. Even though there is only one new Counter statement \nin the program, two instances are created at runtime. Task Types aim to be maximally expres\u00adsive in this \ncase, and use context-sensitivity to give unique typings to each counter, as is re.ected in the diagram \nin Fig. 2. The form of context-sensitivity employed in Task Types follows [WS01; EGH94; MRR05; WL04]. \nWe now summarize why the MapReduce example type\u00adchecks. The Mapper map method can freely access its work \nunit wu; it was created by the UnitsLoader and thus the object is passed across a task boundary, but \nthe only task with access to it is the particular mapper task itself; this means the mapper is the cut \nvertex, i.e. the cutExists typ\u00ading predicate holds for wu. The reducer, ul, and all the mappers are tasks \nand so can be shared freely. Task Twinning We now explain why the Pre-Twinned Access Graph is not good \nenough for preserving isola\u00adtion of non-shared ordinary objects. Observe in that graph that only one \nnode is created for Mapper, even though their might be 20 Mapper s at run time. The fundamen\u00adtal problem \nhere is static approaches have to .nitely ap\u00adproximate the in principle unbounded tasks that arise in \nthe presence of recursion. Since different runtime contexts must share static representations for the \nanalysis to remain .nite, we need to make sure this approximation will not intro\u00adduce errors, and it \ndoes in fact introduce some very subtle complications. Suppose we added an additional .eld called secretShare \nof type WorkUnit to class Loader, and we changed the return statement at the end of that method to return \nsecretShare. It is not hard to see this is a prob\u00adlem program as all Mapper instances would be sharing \nthe same WorkUnit stored in secretShare; furthermore, the cut vertex predicate above would be unable \nto detect this problem if we chose to use the Pre-Twinned Access Graph for analysis. The root of this \nproblem is that the type sys\u00adtem as described up to now created only one instantiation of Mapper to model \nthe many created at runtime, and that approximation does not soundly model sharing between the different \nMapper instances at runtime. To address this case we invent a technique called task twinning: we make \ntwo static instances instead of a sin\u00adgle task instance, which here means two Mapper s, two Loader s \nand two Reducer s. The type system in fact produces the graph at the bottom of Fig. 2. This technique \nintuitively captures the fact that every program point for task instantiation potentially may lead to \nmore than one task instantiation at run time, so it directly uses two distinct static type variables \nto split all possible runtime objects into two subsets. Combining this with our polymorphic treat\u00adment \nof method invocations, each object instantiated inside the scope of twinned task objects is also multiplied \nin the graph there are two Counter instances for toReduce (one for each twinned Reducer), two Counter \ninstances for toLoad (one for each twinned Loader), and four WorkUnit instances (from each twinned Mapper \ninvok\u00ading loadWorkUnit of each twinned Loader). Our formal system is constructed to handle the case where \nevery line of code may potentially be involved in recursion, and hence every task object needs to be \ntwinned. In practice, mechanical application of twinning is not al\u00adways necessary. For instance, the \nReducer and Loader objects are in fact instantiated in the bootstrapping main method, so the system would \nstill be sound even if our static access graph had not twinned them. We treat simpli.cations in this \n.avor as implementation-level optimizations, and do not model them in the theory, except that the bootstrapping \ntask Main is trivially singular, so we do not twin it. Now, looking at the actual Static Access Graph \nthe type system generates in Fig. 2 we see that the cut ver\u00adtex property still holds for every non-shared \nordinary ob\u00adject in the graph. If, however, the program incorporated the secretShare modi.cation above, \nthere would be two WorkUnit instances in the full static graph, and each WorkUnit would be accessed by \nboth Loader s in ad\u00addition to both Mapper s. Such a graph would violate the cut vertex condition since \nthere is an access path to each WorkUnit from each Mapper and there is no cut vertex dominating both \nMapper s. Properties For the simplicity of the presentation, the static access graph drawn in Fig. 2 \ndoes not distinguish be\u00adtween read and write access. Our formal type system is more re.ned, and is constructed \nusing a non-exclusive-read\u00adexclusive-write principle. Additionally, we can prove there is no atomicity \nviolation when a .eld write in constructors are treated as non-exclusive. Immutable objects consequently \ncan be freely shared. In Section 4 we prove type soundness and decidability of type inference for Task \nTypes. We also will show how Task Types preserve a non-shared memory model for all objects declared as \nnon-shared and ordinary. Since these objects do not need lock protection to preserve atomicity, Task \nTypes will have the same pervasive atomicity properties as did Coqa but with a lower run-time cost. Pervasive \natomicity also provably subsumes race condition freedom [LLS08], so programmers also will be spared from \nrace conditions for example, in the reduce method, sum = sum + rt is guaranteed to compute predictable \nresults.  2.3 Programmability Effective Task Types programming requires the program\u00admer to develop \na careful plan for object sharing. Declaring as many objects as possible to be non-shared ordinary ob\u00adjects, \n\u00b5 = E, will increase run-time performance, and at the same time increase the size of atomic zones, so \nthe goal of the programmer is simply to maximize the non-shared ordi\u00adnary classes but still allow the \nprogram to typecheck. Ob\u00adjects that cannot typecheck as unshared ordinary should be typechecked as shared \nordinary (\u00b5 = shared) or shared task (\u00b5 = shared ->). Fortunately, problems with typing a non\u00adshared \nordinary object are always solved by hoisting to a shared one since the type system imposes no additional \ncon\u00adstraints on shared objects. So, this is always available as a last resort; the key is to hoist up \nas few objects as possible, to obtain the largest zones of atomicity and highest perfor\u00admance. Both shared \nordinary and shared task objects are locked for mutual exclusion, but shared ordinary objects do not \nadd new atomicity break points into tasks and are also more appropriate when a task makes continual use \nof an object. So, shared is the preference over shared task if the object really should be local to the \ntask, but the type system is too weak to realize that. A good situation to use shared tasks is when the \nclass wraps up a relatively independent service, so that when the service is completed, partial victory \ncan be declared. Examples include Reducer and Loader in the MapReduce example. We believe this additional \nprogrammer focus on object sharing is time well-spent if the .nal goal is production of reliable software. \nIt is our belief that the vast majority of ob\u00adjects in a vast range of applications are ordinary objects \nnot shared across tasks, and they can be programmed normally, so additional planning is required only \non the shared object portions of the code. We don t expect this paradigm will extend to every line of \nevery single application just as there is a rare need to escape to C in Java there will be rare cases \nwhere for ef.ciency the Task Type framework needs to be bypassed. One such example is data structure \nimplementations that use hand-over-hand locking. We next describe a benchmark program with signi.cant \ncontention. Intuitively, this is precisely the category of pro\u00adgrams one would expect Task Types to be \nuncomfortable with, so it will be more of a stress test of our language. 2.3.1 The PuzzleSolver Benchmark \nIn Section 5 we discuss the implementation and a few bench\u00admarks of its performance. One benchmark, PuzzleSolver, \nsolves a generalized version of the 15-puzzle, the famous 4x4 sliding puzzle where numbers 1-15 must \nbe slid to ar\u00adrive at numerical order. The primary worker tasks of the pro\u00adgram are n SolverTasks, each \nof which in parallel takes existing legal gameplay move sequences (PuzzleMove s) from a central work \nqueue (PuzzleTaskQueue) and puts back on the queue all gameplay moves extending the play sequence grabbed \nby one game step, if any exist. All worker tasks loop on this activity until there are no more move se\u00adquences \nto extend on the shared queue. In order to avoid re\u00adpeating play, all visited board states (PuzzlePositions) \nare centrally logged in a PositionSeen object so no worker SolverTask will add an already investigated \npo\u00adsition to the PuzzleTaskQueue. Lastly there are classes Block and Puzzle, the former containing the \nID and size of a block (the generalization supports n \u00d7 m blocks), and the latter which checks for legal \npuzzle moves. The primary interest for Task Types is what sharing declarations are placed on the classes. \nSolverTask is obviously a task and therefore declared a task. Classes PuzzleTaskQueue and PositionSeen \nare data struc\u00adtures that must be shared by all tasks and so logically are de\u00adclared as shared task which \nalso implicitly guarantees their mutually exclusive access. Classes Block and Puzzle are not changed \nafter they are constructed, and even though they are shared across worker tasks it is still possible \nto declare them as ordinary unshared classes since they are known to be immutable this is an example \nof the practical usefulness of the non-exclusive read we support in the type system, as was discussed \nin the previous subsection. The only class which the typechecker is less than op\u00adtimal on is PuzzlePosition \nwhich must be declared shared task in order to typecheck. The PuzzlePosition objects are used by SolverTask \ns to replay PuzzleMove sequences on the grid, and each worker task has its own pri\u00advate PuzzlePosition \nwhich logically is unshared. So, it sounds like PuzzlePosition could be declared unshared ordinary, but \nthere is a subtle problem: there is also a distin\u00adguished PuzzlePosition called initial position which \nholds the initial board con.guration and is set up by the main task that launches the worker tasks. This \ninitial position is passed to each worker task so they can set up their own PuzzlePosition (by copying \ninitial position), but even though they are only read\u00ading it, the main task had written to initial position \nwhen the data was read from a .le and so it is consid\u00adered owned by the main task and so cannot be read \nby the worker tasks. If Task Types were to support a per-instance declaration of sharing policy, the \nprivate per-worker-task PuzzlePosition objects could be declared unshared or\u00addinary, and only the initial \nposition would need to be a shared task. A .ow-sensitive typing would also sup\u00adport this since initial \nposition is not mutated after it is read from the .le. And, a call-by-copy syntax as discussed in Sec. \n7 would solve this problem as well. For simplicity we elected not to include any of these three extensions \nin the current design, but conceptually one will likely be needed in a future extension to increase expressiveness. \n 3. The Formal System 3.1 Abstract Syntax The core syntax of our language is formalized in Fig. 4, where \nnotation X is used to represent a sequence of X s. As a convention, metavariable c is used for class \nnames, m for method names, f for .eld names, and x for vari\u00adable names. Special class names include Object, \nthe root \u00b5 classes for the inheritance hierarchy, one for each class mod\u00adi.er \u00b5. The bootstrapping code \nis located inside the body of class Main s no-argument method main. The class directly inherits from \nclass Objecttask with no additional .elds. We def encode unit/void type via class name Unit = Objecte. \nDefault constructors are supported: a constructor for class c is syntactically a method in c with method \nname also c. To make the formalism more uniform, we assume all construc\u00adtors always return this as the \nreturn value. In this formalism\u00adfriendly syntax, .eld read/write expressions are annotated with a scope \nmodi.er ., denoting whether the expression is lexically scoped in a constructor (. = cons) or not (. \n= reg). Parametric Polymorphism Support The formal syntax differs from the programmer syntax in that \nseveral pro\u00adgram annotations are included to help streamline the formal presentation. Programmer syntax \nnew c(e) is formally ex\u00adpressed as newA c(e) when class c has modi.er \u00b5/. task (i.e. resource object \ninstantiation), and as newA1,A2 c(e) otherwise (i.e. concurrency unit object instantiation). The associated \nsubscript A serves to distinguish different instan\u00adtiation sites. Structurally, each A represents a list \nof type variables a (of set SlV). The .rst type variable in A rep\u00adresents the object instantiated at \nthe speci.c program point where the expression occurs, and each of the rest represents an object that \ncan be stored in a .eld of the instantiated object. For any two distinct new expressions in the source \ncode, we require their respective A s have distinct elements. The newA1,A2 c(e) form for concurrency \nunit object instan\u00adtiation is present to realize our need for task twinning to model self-sharing, as \nwas outlined in Sec. 2.2. To support context sensitivity, each call site is differentiated by asso\u00adciating \na singleton list [a] with each messaging expression, e* [a]m(e'). For any two distinct messaging expressions \nin the source code, we require their respective [a] s to be dis\u00adtinct. As examples, the several expressions \nin the Loader class of Fig. 2 is automatically annotated as follows: cf!.newCtr() as cf!.[a100 ]newCtr(null) \ntoLoad.dec() as toLoad.[a101 ]dec(null) new WorkUnit(this) as new[a102 ,a103 ] WorkUnit(this) For each \nclass de.nition \u00b5 class c extends c' {FM } in\u00adside codebase C and each method de.nition c' m(c'' x){e} \ninside M , we de.ne function mbody(p)= x.e to return the method body and deterministic function mtype(p)= \n '' . c .A.(c') to return the signature for method index p = (c; m). Both functions are implicitly parameterized \nby the .xed codebase C . The type variable list above A =[a'',a'] includes two distinct type variables \nrepresenting the argu\u00adment and the return value respectively. To illustrate, selected methods of Fig. \n2 have the following signatures: mtype((Loader; Loader))=.[a200,a201]. (CtrFactory . Loader) mtype((Loader; \nloadWorkUnit))=.[a202,a203]. (Unit . WorkUnit) mtype((CtrFactor; newCtr))=.[a204,a205]. (Unit . Counter) \nmtype((Counter; dec))=.[a206,a207]. (Unit . Unit) To model inheritance, we further de.ne mtype((c0; m))= \nmtype((c; m)) for any c0 a subclass of c. This is the only case where mtype may compute overlapping A \ns for different p s, i.e. for all other cases, if mtype(p1) and mtype(p2) compute A1 and A2 respectively \nand p1 p2, = A1 and A2 have disjoint elements. mtype((Main; main))= .A.(Unit . Unit) for some (uninteresting) \nA. Auxiliary De.nitions For class \u00b5 class c extends c' {FM }in C , we further de.ne modi.er(c)= \u00b5 and \nsupers(c)= {c}. supers(c'). Here we assume Object = c and \u00b5 there is no cycle on the inheritance chain \ninduced by C . In addition, we de.ne modi.er(Object)= \u00b5 and \u00b5\u00b5\u00b5 supers(Object)= Object. These functions \nare also implicitly parameterized by C . We now de.ne standard mathematical notation used in this paper. \n[] is used to represent an empty sequence, and x : def [x1,...,xn]=[x, x1,...,xn], and |[x1,...,xn]| \n= n. When ordering of a sequence does not matter, we liberally consider them convertible to their unordered \ncounterpart (a set) and standard set operators such as . and . will be used on it. A special kind of \nsequence, a mapping sequence, is denoted as x . y and de.ned as [x1 . y1,...,xn . yn] for some unspeci.ed \nlength n. Given . being the mapping def def sequence above, dom(.)= {x1,...xn} and range(.)= {y1,...yn}. \nWe write .[x . y] for mapping update: . and .[x . y] are identical except that .[x . y] maps x to y. \nUpdateable mapping concatenation [ is de.ned as def .1 [ . = .1[x1 . y1] ... [xn . yn]. We also write \n.1 [ . as .1 I ., except the latter function requires the pre\u00adcondition of dom(.1) n dom(.)= \u00d8. Given \ntwo sequences, a zip like operator -. produces a mapping sequence: def [x1,...,xn] -. [y1,...,yn]=[x1 \n. y1,...,xn . yn].  3.2 A Bird s Eye View of the Type System Task Types are a constraint-based type \nsystem with (T-Program) as the top-level typing rule:  ' C ::= c . \u00b5 class c extends c {FM } classes \n\u00b5 ::= E | shared | task | shared task class modi.er F ::= cf .elds ' M ::= m . cm(c x){e} methods e \n::= x | null | this | f. | f. :=e | (c)e | e * [a] m(e) | newA c(e) | newA,A' c(e) expression * ::= !-> \n| !. | -> | . method invocation symbol a . SlV annotated type variable A ::= aa sequence . ::= reg | \ncons scope modi.er p ::= (c; m) method index Figure 4. Abstract Syntax fcls C (ci)\\C(ci) for all ci . \ndom(C ) WF (0 C) (T-Program) fp C : C This rule de.nes typechecking in two phases: First, constraints \nare collected for each method of each class modularly, and inconsistencies are detected as early as possible. \nThe per-class typing rule fcls and the related expression typing rules are de.ned in Fig. 5. At the end \nof this phase, constraints are stored in a per-class, per\u00admethod fashion, in constraint store C.  Second, \na closure phase propagates inter-procedural in\u00adformation. The resulting constraint closure is computed \nby the 0 function, as de.ned in Sec. 3.4. Afterwards the static access graph represented in the closure \nis checked by a simple WF () function in Sec. 3.5, determining whether static isolation for non-shared \nordinary objects holds.  We favor a phased de.nition to be more realistic with object-oriented languages. \nToday s OO languages often rely on a modular phase to .nd as many bugs as possible (either via source \ncode compilation or bytecode veri.cation), and delays as few as possible non-modular constraint solving \nto dynamic class loading time (such as those related to Java subtyping [LB98]). Presenting Task Types \nin phases de facto describes how it can be constructed in a language with dy\u00adnamic class loading. Notably, \nextracting a modular phase out of an inter-procedural algorithm is no trivial task for object\u00adoriented \nprograms, as the latter are fundamentally mutu\u00adally recursive: class c1 might contain expression new \nc2(e1) whereas class c2 might contain expression new c1(e2); or class c1 s method m1 might invoke c2 \ns m2 which in turn in\u00advokes c1 s m1. By constructing an explicit formal de.nition of the modular phase \nhere, we gain con.dence in the ability to construct a practical and decidable typechecking process. \n 3.3 Modular Class Typing Expressions are typed via the f rules of Fig. 5. Classes are typed via judgment \nfcls, and method bodies via fm. All typing rules are implicitly parameterized by the .xed codebase C \n. Types are always of the form c@a, with c the class name analogous to Java s object type, and a the \ntype variable associated with a speci.c object instance, needed for the polymorphic type system. Typing \nenvironment G maps variables x, .eld names f, and this to their types. We delay the discussion of class-indexed \nconstraint store C and method-indexed constraint store M until judgments fcls and fm are explained. Per-method \nconstraint store K is a set of constraints collected for each method body. Each constraint is represented \nby metavariable .. The meanings of speci.c constraints will be clari.ed later, but in general a . = constraint \nis intuitively a constraint recording .ow, a --\u00adconstraint is intuitively a constraint recording access. \nSpecial variables a are used only in = constraints. Access Constraints A main goal of our type system \nis to generate the static access graph that was informally de\u00adscribed in Section 2. The nodes of such \na graph are type variables representing individual objects. The edges, as col\u00adlected in the modular type \nchecking phase, are called access . constraints. They are of the form thost ---a, meaning an object represented \nby type variable a is accessed. Con\u00adstraint label ., also called access mode, ranges over three values: \nwhen . = R, the constraint asserts that object a is non-exclusively read ; when . = W, a is exclusively \nwrit\u00adten ; when . = T, a is the entry/facade object for a pro- T tected zone of sharing. In our type \nsystem, ---constraints are generated when a is a shared task object or a shared ordinary object. Observe \nthat both kinds of objects are dy\u00adnamically protected upon entry, and the objects completely hiding behind \nthem need not be dynamically protected. The T ---constraints make the static access graph aware of the \ndynamic protection points and reason accordingly. Placeholder type variable thost denotes the accessor. \nIntuitively, the accessor should have been a type variable  G(f)= c@a G(f)= c@a1 G f e : c@a2\\K (T-Read)(T-Write) \nG f f. : c@a\\aC (R, G(this),.)G f f. :=e : c@a1\\K . {a2 = a1}. aC (W, G(this),.) G f e : t\\K t = c@a0 \nmtype((c; m))= .A.(c1 . c2) ' G f e : c1@a ' \\K ' * matches modi.er(c) returns c2 . =[a]a0,m,a ' any \n. (T-Msg) G f e * [a]m(e '): c2@a\\K . K ' .{.}. aC (T, t, .) G f e : c0@a0\\K0 A =[a, . . . ] mtype((c; \nc))= .A ' .(c0 . c) . =[a]a,c,a0 t = c@a any . (T-New) G f newA c(e): t \\K0 . {Ac = a, .}. aC (T, t, \n.) G f newA1 c(e): c@a1\\K1 G f newA2 c(e): c@a2\\K2 task . modi.er(c) (T-NewTask) G f newA1,A2 c(e): \nc@a1\\K1 .K2 .{a1 = a2,a2 = a1} G f e : c0@a\\K c . supers(c0)G f e : c0@a\\K (T-Sub)(T-Cast) G f e : c@a\\K \nG f (c) e : c@a\\K (T-Var)G f x : G(x)\\\u00d8 (T-This)G f this : G(this)\\\u00d8 (T-Null)G f null : t\\\u00d8 fcls \u00b5 class \nc0 ... \\M .elds(c)= .A.GG fm mbody(pj ): mtype(pj)\\.Aj ..Sj.Kj for all mj . dom(M ),pj = (c; mj) (T-Cls) \n fcls \u00b5 class c extends c0 {FM }\\.A.(M [ mj . .Aj ..Sj.Kj) .elds(Object)= .A.G \u00b5 (T-ClsTop) fcls \u00b5 class \nObject\\.A.[] \u00b5 G I [x . c1@a1] f e : c2@a3\\K A =[a1,a2] S = labels(e) (T-Md) G fm x.e : .A.(c1 . c2)\\.A..S.(K.{a3 \n= a2}) . matches E returns c !. matches shared returns c !-> matches shared task returns c -> matches \ntask returns Unit t ::= c@a types G ::= t . t typing environment t ::= x | this | f environment variable \nS ::= AA sequence K ::= . per-method constraint store . . ::= a = a |Aa,m,a ' | thost ---a constraint \na ::= a |Ac .ow element . ::= T | R | W access mode C ::= c . .A.M class-indexed constraint store M ::= \nm . .A..S.K method-indexed constraint store . {thost ---a} if modi.er(c)= E, . = R or W,. = reg def \n aC (., c@a, .)= or modi.er(c) . shared,. = T \u00d8 otherwise . .. .. Figure 5. Typing Rules representing \nthe entry facade object for a protected zone. A placeholder is used because no concrete type variable \nnam\u00ading this object is known during the modular type checking phase. Consider for example the case when \nthe toLoad Counter object is written by object Loader because toLoad s .eld v is written in method dec. \nWhen class Counter is typed, we cannot directly determine in what protected zone method dec is invoked \nfrom one needs to backtrack on all possible call paths to .nd the .rst non\u00adordinary object. We therefore \nput thost here and rely on the closure phase (Sec. 3.4) to instantiate it; such a scheme is common in \ninter-procedural analyses with a modular phase. Access constraints are collected by typing rules depen\u00addent \non aC (., t, .). This convenience function collects . constraints when the type of the accessed entity \nis t and the scope of access is .. The function is de.ned in Fig. 5. (T-Msg) puts aC (T, t, .) into the \nconstraint set, i.e. it collects T ---constraints when the message receiver is a shared task object or \na shared ordinary object (regardless of the scope). This is consistent with our previous discussion of \nprotected zones. The same constraint is collected in (T-New) for con\u00adstructor calls. For accessing non-shared \nordinary objects, an access occurs when the .eld of that object is read or written. Related constraints \nare collected in (T-Read) and (T-Write) respectively. Observe that when the .eld read/write happens in \na constructor, the access is not recorded as a constraint. This is sound because, when an object is constructed, \nits ref\u00aderence is not yet created, let alone leaked to another task and accessed by it such read/write \naccess fundamentally does not lead to atomicity violations. Expressions with Polymorphic Typing In principle, \npoly\u00admorphic typing behaves rather like let-polymorphism: the type constraints of the polymorphic code \n the invokee s method body here should be refreshed and added to the invoker s constraint set. What \ncomplicates matters here is the fundamentally recursive nature of OO programs: naively merging the invokee \ns constraints may lead to non\u00adtermination due to an in.nite regress of refreshing along a recursive call. \nIt is for this reason our type system in the modular phase only places a delayed contour marker in the \nconstraint set, and delays the task of refreshing and merging to the phase of closure. Marker [a]a0,m,a \n' added in (T-Msg) indicates the need to merge (at closure time) the constraints of method m of object \na0, with argument being a ' and return value being a. For instance, typing the annotated expression cf!.[a100 \n]newCtr(null) in class Loader generates the fol\u00adlowing marker constraint: = {[a100]a200,newCtr,a100 } \nKloader1 where a200 is the type variable associated with variable cf the latter is the argument of the \nconstructor; its associated type variable is computed by mtype((Loader; Loader)) earlier. The choice \nof type variables to type null is irrele\u00advant; we use a100 here. (T-Msg) also contains a predicate * \nmatches \u00b5 returns c , which matches different method in\u00advocation symbols (*) with class modi.ers \u00b5. In \naddition, it requires that asynchronous top-level task creation has no in\u00adteresting return values. In \n(T-New), the type variable representing the object in\u00adstantiated by newAc(e) is the .rst element of A, \nconsis\u00adtent with how A is constructed in the formal syntax. Since all such annotations include disjoint \ntype variables, new ex\u00adpressions at different program points are given different type variables. Rule \n(T-New) also contains a .ow constraint of the form Ac = a. This constraint says that instantiation site \nAc .ows into type variable a. This constraint, together with the transitivity of = as de.ned in the closure \nphase, is used to trace back any type variable to its concrete instan\u00adtiation point(s) a concrete type \nanalysis scheme essential for languages with aliases. Lastly, a similar delayed contour marker is added \nto a constructor call. For instance, typing the annotated expression new[a102 ,a103 ] WorkUnit(this) \nin class Loader leads to the following constraints: = {[a102,a103]WorkUnit = Kloader2 a102, [a102]a102,WorkUnit,a300 \n} assuming this was given type a300. (T-NewTask) types expression newA1,A2 c(e), which is used when modi.er(c) \n. task. This is how task twinning is re.ected in the type system: for each statically known task object, \nthe type system instantiates them twice, with A1 and A2 respectively. Other Rules Standard nominal subtyping \nis supported by (T-Sub). Casting is typed by (T-Cast); we do not single out stupid cast as warnings [IPW99] \nas this does not affect soundness. (T-Read), (T-Write), (T-This), (T-Var) rely on the typing environment. \nGiven a class \u00b5 class c extends c ' {FM }in codebase C where F =[c1 f1,..., cn fn], a typing envi\u00adronment \nwith .eld and this type information is prepared via the following deterministic function: def .elds(c)= \n.A.((G I [f1 . c1@a1,... fn . cn@an]) [[this . c@a0]) if .elds(c ')= .A ' .G A = A ' I [a1,...,an] G(this)= \nc '@a0,a1,...,an distinct and the base case is .elds(Object)= .[a].[this . Object@a]. This function deterministically \nassigns each \u00b5 \u00b5 .eld a distinct type variable, as well as assigning one for this. The function is implicitly \nparameterized by the .xed codebase C . It is able to support .eld inheritance but dis\u00adallows .eld shadowing \nfor simplicity. As an example, the function has the following behavior on class Loader : .elds(Loader)= \n.[a300,a301]. ' toLoad . Counter@a301 this . Loader@a300  To make the parametric nature of constraints \nmore ex\u00adplicit, a class-indexed constraint store C computed in (T-Cls) and (T-ClsTop) is always of the \nform .A.M, where A is computed by the previous .elds function. A method\u00adindexed constraint store M computed \nin (T-Md) is always of the form .A..S.K, with A containing the two type vari\u00adables representing the argument \nand the return value of the method, and S being the set of A labels appearing in the body of the method. \nS is computed by the following func\u00adtion: def labels(x)= \u00d8 def labels(newA c(e)) = {A} . labels(e) def \nlabels(newA1,A2 c(e)) = {A1, A2}. labels(e) def labels(e * A m(e ')) = {A} . labels(e) . labels(e ') \ndef labels((c)e)= labels(e) ... We require the type variables computed by .elds, by mtype, and by labels \nto be pairwise disjoint. We now provide a complete picture of all the constraints produced for class \nLoader: We .rst de.ne re.exive and transitive binary relation O ' C(Loader) = .[a300, a301].M M(Loader) \n= .[a200, a201]..[[a100]]. Kloader1 . {a300 = a201}T M(loadWorkUnit) = .{thost ---a200}.[a202, a203]. \n.[[a101], [a102, a103]]. Kloader2 . {[a101]a301,dec,a101 }.{a102 = a203} 3.4 Type Closure . . O' by \nthe proof system in Fig. 6. This relation denotes O . closes to O' under calling context '.. A calling \ncontext . is represented as a sequence of tuples d, each of the form (\u00df; c; m) denoting a call to method \nm of object \u00df of class c on the call chain. We use \u00df, B, ., O to represent the closure-time counterparts \nof modular-typing-time a, A, ., K, respectively. We differentiate these syntactic entities to highlight \nthe fact that \u00df s and a s are chosen to be disjoint. The set of all \u00df s are denoted IlV. It includes \none special type variable tmain to represent the bootstrapping task. Type closure 0 C, as used in (T-Program), \nis de.ned as [] the largest set O where relation boot ' . O holds under implicit C, and boot is sugar \nfor {[tmain]tmain,main,tmain , [tmain]Main = tmain}. Intuitively the delayed contour marker in this set \nindicates that the main method of the Main class is invoked, with tmain representing the main task and \nirrelevant arguments and return values. The overall goal of type closure is to merge local per-class \nand per-method constraints into one global . \u00df ' set, so that all access constraints \u00df ---can form one \nstatic access graph for the key well-formedness check. Rule (C-Canon) canonizes access constraints, i.e. \nit traces back the chain of the .ow constraints so that the type variable generated at instantiation \npoint is used as a canonical name for the object. This is de facto applying a concrete type analysis \nto the object aliases, expressed as type variables here, appearing in access constraints. We use constraint \nform . . \u00df ' \u00df -to represent the canonized version. To facilitate the process of instantiation-point \nback-tracing, rules (C-Flow=) and (C-Flow+) assert that .ow constraints = are re.exive and transitive. \nThe transitivity rule (C-Flow+) enables any type variable to ultimately .nd its instantiation point(s) \nvia the previously explained .ow constraint placed in (T-New). This is why in the map function of the \nMapper class, our type system ultimately will .nd out what objects .ow into ul, even though ul is not \ninstantiated in its scope. (C-Task=) and (C-Task+) de.ne re.exivity and transitivity for T -. if a protected \nzone \u00df1 encloses protected zone \u00df2, and \u00df2 encloses \u00df3, then \u00df1 can be viewed as enclosing \u00df3. The main \ncomplexity of the closure algorithm arises from context sensitivity, captured by (C-Contour). Recall \nthat for a context-sensitive algorithm, the type constraints associated with the method body need to \nbe refreshed according to the speci.c calling context. We de.ne a function for picking type variables: \ndef gen(., A)= generate(collapse(.), A) where generate is a deterministic function de.ned as fol\u00adlows: \ndef generate(., A)= B where |A| = |B|, all elements in B distinct with the additional requirement that \nfor any A1, A2, the type variables in sequences generate(., A1) and generate(.' , A2) are disjoint if \ncollapse(.) = collapse(.') or A1 = A2 Note this requirement can be concretely satis.ed by index\u00ading the \nvariables on the call string. Function collapse determines whether a recursive invoca\u00adtion has been made, \nand if so, reuses the results of the initial invocation: def collapse([]) = [] def collapse(d :.) =[d, \nd1,...,dn] if collapse(.) = [. . . , d, d1,...,dn] def collapse(d :.) = d : collapse(.) if d/. collapse(.) \n Given calling context .=[(\u00df1; c1; m1),..., (\u00dfn; cn; mn)], partial function pzone(.) is used to compute \nthe current protected zone. It is de.ned as \u00dfi for some i . [1..n] where modi.er(ci)= E, and for any \nj such that 1 = j<i, modi.er(ci)= E. Substitution notation [s] replaces all type variables a . dom(s) \nin with s(a). Let us now illus\u00adtrate one inductive step of closure when the constraints associated \nwith Mapper class s map method have been  . . .. (C-Canon) {\u00df ' ---\u00df2' , (\u00df1 : B1)c1 1, (\u00df2 : B2)c2 \n2}.{\u00df1 . \u00df2} (C-Flow=) \u00d8.{\u00df = \u00df}= \u00df ' = \u00df ' '-' 1 .. T (C-Flow+) {\u00df 1 = \u00df, \u00df = \u00df2} '.{ \u00df 1 = \u00df2} (C-Task=) \n\u00d8 '.{\u00df -. \u00df} TT. T (C-Task+) {\u00df1 -. \u00df2,\u00df2 -. \u00df3} '.{\u00df1 -. \u00df3} d = (\u00df; c; m) .' = d :. C(c)= .A1.M  M(m)= \n.A2..S.K s =(A1 -. B) I (A2 -. [\u00dfarg,\u00dfret]) I A -. gen(.' , A) A.S (C-Contour) . {[\u00dfret]\u00df,m,\u00dfarg , Bc \n= \u00df} '. {(K[s])d} . (C-GlobalIntro) {\u00df = \u00df, (O)d} '.(O .{ \u00df = \u00df})d . B\u00dfret,m,\u00dfarg . O' /O1 '. O2 (C-GlobalElim)(C-Union) \n.. {(O . O' )d} '. O'[thost . pzone(d : .)] . {(O)d} O . O1 '. O . O2 . d:. O '. O1 . O2 O1 '. O2 (C-Subset)(C-Context) \n.. O '. O1 (O1)d '.(O2)d \u00df . IlV type variables in closure B ::= \u00df\u00df sequence . ::= d calling context \nd ::= (\u00df; c; m) call site O ::= . closure .. . \u00df ' . ::= \u00df = \u00df |B\u00df,m,\u00df ' | \u00df ---\u00df ' | \u00df -|(O)d constraints \nin closure  \u00df ::= \u00df |Bc .ow elements in closure s ::= a . \u00df substitution def W. WF (O) = .\u00df.\u00df ' -. \u00df \n. O=. cutExists(O, {\u00df ' |\u00df ' -. \u00df}) def TTT cutExists(O, B)= .\u00dfc .(\u00dfc -. \u00df) . (.\u00df ' = \u00dfc .(\u00df ' -. \u00df)=. \n(\u00df ' -. \u00dfc )) cc c \u00df.B \u00df.B Figure 6. Type Constraint Closure and Isolation Preservation merged, we show \nhow (C-Contour) helps merge in the con\u00adstraints for the method body of loadWorkUnit. Let us assume the \nclosure at that step includes: [\u00df401]\u00df402,loadWorkUnit,\u00df403 which results from typing ul!->[a401]loadWorkUnit() \nin Mapper and [\u00df404,\u00df405]Loader = \u00df402 from typing new[a404,a405] Loader(cf) in Main and by .ow transitivity. \nUsing the de.nition for C(Loader) given previously, the substitution built up in (C-Contour) thus is \n' a300 . \u00df404 A1 -. B is a301 . \u00df405 ' a202 . \u00df403A2 -. [\u00dfarg,\u00dfret] is a203 . \u00df401 .. a101 . \u00df601 A.S \nA -. gen(.' , A) is . a102 . \u00df602 . a103 . \u00df603 and given \u00df406 represents the Mapper object, and .' = \n[(\u00df402; Loader; loadWorkUnit), (\u00df406; Mapper; map), (tmain; Main; main)], the gen function is: gen(.' \n, [a101]) = [\u00df601] gen(.' , [a102,a103]) = [\u00df602,\u00df603]  There are two interesting points here. First, \nif function loadWorkUnit were invoked via different call chains, the gen function would map .' to different \ntype variable lists, so that different instances of WorkUnit instantiated from loadWorkUnit can be differentiated. \nThis is also why different Counter instances in Fig. 2 can be approx\u00adimated statically even though they \nare all instantiated from one program point. Second, rather than immediately merg\u00ading the substituted \nconstraints into the type closure, a spe\u00adcial contextual constraint of the form (O)d is used, a marker \ndenoting constraints O in the calling context of d is to be merged to the type closure. The real merging \nhappens at (C-GlobalElim). Here any constraint other than delayed contour markers can be yanked out of \nthe contextual constraint in other words, these constraints are not context-sensitive . Note that when \nthis happens, the placeholder for the current protected zone, thost, needs to be properly replaced. Con\u00adtinuing \nwith the example above, any constraint inside the ()of the contextual constraint obtained via (C-Contour) \ncan be yanked out with thost replaced with \u00df402. It represents the Loader object itself. This is the \nvalue of thost here be\u00adcause shared tasks create protected zones of their own. Other rules related to \ncontextual constraints, (C-GlobalIntro) and (C-Context), are self-explanatory.  3.5 Isolation Preservation \nIsolation is enforced by the WF function in Fig. 6. It checks that for any non-shared ordinary object, \neither it is only accessed once, all accesses are reads, or the accessing tasks as in B must satisfy \ncutExists(O, B). The cutExists(O, B) function in Fig. 6 formally de.nes the notion of cut vertex alluded \nto in Sec. 2.2: the subgraph including all static access paths ending with a type variable in B must \nhave a cut vertex (or articulation point), and if there is more than one cut vertex for that subgraph, \nthere must be a least upper bound of them. The de.nition here is phrased so as to allow a shared task \n(or the ordinary objects it owns) to access the objects belonging to its ancestor tasks, as long as the \ncut vertex invariant is not violated. 4. Operational Semantics and Formal Properties In this section \nwe brie.y describe the operational semantics of our language, with a focus on features that are related \nto stating the proven formal properties. Small-step reductions S . are de.ned over con.gurations S = \n(H; S; e) S ' for H the object heap, S the dynamic constraint set, and e the expression. The reductions \nare implicitly parameterized by class list C . We use S .* S ' to represent multi-step reduction, which \nis de.ned as the transitive closure of .. We use .C S to represent a computation of program C starting \nin the initial state and computing in multiple steps to S. We use S . to mean there is some computation \nof S that computes forever. H ::= o . (Bc; Fd) heap .. S ::= p -e o . o | p dynamic constraint set \u00df \n::= \u00b7\u00b7\u00b7 | o extended type variable . . ::= \u00b7\u00b7\u00b7 | \u00df e \u00df ' extended constraint Fd ::= f . v .eld store \nv ::= o | null value o, p, q . 1m object ID o e ::= \u00b7\u00b7\u00b7 | (e)d || e; e extended expressions . | v | post \ne | e I e E ::= | E I e | e I E evaluation context | f. :=E | (c)E | E * [\u00df] m(e) | v * [\u00df] m(E) | newB \nc(E) | newB1,B2 c(E) | E; e |(E)d Figure 7. Dynamic Semantics De.nitions Fig. 7 gives de.nes related \ndata structures. To reuse data structures that have been de.ned for static semantics, we extend type \nvariables \u00df to include o s as well. As a result, the previous de.nition of calling context a sequence \nof call sites in the form of (\u00df; c; m) s can also be viewed as that for the dynamic calling context, \nin the form of a list of (o; c; m) s. H is a mapping from objects o to .eld stores (Fd), and its program \npoint information Bc of their instantiation. Expressions are extended with values v, which are either \nobject ID s or null. Auxiliary expression (e)d is used to represent an expression e evaluated inside \na dynamic o call site d. Expression is a helper expression to indicate o . is accessed in mode ., and \npost e is a helper expression to post a root task for later execution. Parallel operator e I ' e is commutative. \nDynamic constraint set S includes two ... kinds of constraints: -e. We reuse the . constraint . and -from \nthe static type system, which is computed only for stating theorems and is not collected by the compiler. \nWe use E to represent evaluation contexts. The initial con.guration is (Hinit; \u00d8; einit) where Hinit \n=[o .([tmain]Main ;[])] for some o and einit = o->[tmain]main(null). Operational Semantics A complete \nde.nition of opera\u00adtional semantics can be found online [TT]. We now only focus on what is relevant for \nstating meta-theories, espe\u00adcially access invariants. First, reductions under a particular . dynamic \ncalling context . is represented by the S =. S ' , and this relation is connected with . by a self-evident \ncon\u00adtext rule. cxt(E) ' '' H, S, E[ e ] . H, S' , E[ e ' ] if H, S,e =. H, S' ,e where  def T modi.er(c)= \ntask, and there exists some n = 0, {o0 . cxt( ) [] = e T def o}. S. o def = cxt(E):[d] Let us now study \nliveness. The reduction for shows . cxt([ E ]o) cxt(E) o1,... on-1 on, on ee = cxt((E)d) cxt((c)E) \ndef .eld read/write is never blocked, and messaging to non\u00ad = cxt(E) shared ordinary objects is never \nblocked. This is precisely ... The reductions related to access take the following shape, '' where H \nand e are irrelevant information we elide here: H, S, freg:=v '' . o =. H, S, ; e W if .= (o; c; m) :.' \n. o H, S, freg ' =. H, S, ; e R if .= (o; c; m) :.' o . ' H, S, o * [\u00df] m(v)=. H, S, ; e T o . ' H, S, \n=. H, S . dset(\u00b5, ., p, o),e . if progressable(H, S, \u00b5, ., p, o),p = pzone(.) . . . H, S -{o1 H, S, (v)(o;c;m) \no2 | o1 or o2 is o},v = e why declaring objects as non-shared ordinary objects im\u00adproves performance. \nWhen the messaging target is a shared ordinary/task object, the execution is not blocked iff the tar\u00adget \nis not accessed by any task (roots(H, S, o)= \u00d8) or it is a reentrant access (roots(H, S, o)= roots(H, \nS,p)). These two cases, combined with the fact that |roots(H, S,p)| =1 (by the nature of how call stacks \ngrow and shrink), can be summarized with predicate roots(H, S, o) . roots(H, S,p). If the target is a \nnon-shared task object, messages are pro\u00adcessed one at a time, as we discussed in Sec. 2. This is why \nT pre-condition (oo) ./S is used in that case. e Properties We next discuss the properties of our language, \nstarting with some de.nitions. With a language runtime with if H(o)= (Bc; Fd), modi.er(c) . task . blockings, \ndeadlocks are possible: H, S, (v)(o;c;m) =. H, S,v o0 if H(o)= (Bc; Fd), modi.er(c) ./task De.nition \n2 (Deadlock). S = (H; S; (E[])(p0;c0;m0) I T on In essence, all object access controls (.eld read/write \nor \u00b7\u00b7\u00b7 I (E[])(pn;cn;mn) I e) is a deadlock con.guration iff o T messaging) have been delegated to the \nreduction of , where for i =0..n, H(oi)= (Bci ; Fdi), and .i function progressable(H, S, \u00b5, ., p, o) \nis de.ned as: pi . roots(H, S, o(i+1) mod (n+1)) \u00b5 . T R or W E true true shared roots(H, S, o) . roots(H, \nS, p) true shared task roots(H, S, o) . roots(H, S, p) true task (o T eo) /. S true The de.nition here \nshows how Task Types programming can help programmers reduce the likelihood of deadlocks by encouraging \nthe default non-shared memory model. Dead\u00adlock cannot happen on non-shared ordinary objects: if there \nare no locks, there are no deadlocks. Next, some run-time ex\u00adceptions standard in Java-like languages \nare possible in our language: and function dset(\u00b5, ., p, o) is de.ned as De.nition 3 (Null Pointer Exception). \nS leads to a null pointer exception iff S = (H; S; E[null * [\u00df] m(v)]). De.nition 4 (Bad Cast). Con.guration \nS leads to a bad cast exception iff S = (H; S; E[(c ')o]) where H(o)= (Bc; Fd) ' and c ./supers(c). The \ntype soundness property is as follows. Theorem 1 (Type Soundness). If fp C : C, then there exists S such \nthat .C S, where either S ., or S = (H; S; v), or S is a deadlock con.guration, or it leads to a null \npointer or \u00b5 . T R or W E \u00d8 {p . eo, p .-. o} shared {p T eo, p T-. o} \u00d8 shared task {p T eo, p T-. o}, \n\u00d8 task T eo, o T- \u00d8 {o . o} . -bad cast exception for some H, S. . constraints monotonically grow in \nthe dynamic constraint set, recording the entire history of the dynamic access This Theorem states that \nthe execution of a statically . . By now the difference between -. . and should be clear: e typed program \neither diverges, leads to a deadlock con.gu\u00adsince the program is bootstrapped. constraints on the e \nother hand are removed whenever the invocation to a (shared ration, leads to a standard exception, or \ncomputes to a value. or non-shared) task object ends intuitively, a task frees The Theorem is established \nby showing Lemmas of Subject all its accessing objects at the end of its execution. For Reduction, Progress, \nand the trivial fact that the bootstrap\u00ad that reason, . e only records the access relation at a speci.c \nping process leads to a well-typed initial state. runtime snapshot. We next de.ne a function to compute \nthe set of root task objects (i.e. non-shared task objects) that are currently accessing o. De.nition \n1 (Roots). Function roots(H, S, o) is de.ned as the largest set of o0 such that H(o0)= (Bc; Fd), Theorem \n2 (Type Decidability). For any program C it is decidable whether there exists a C such that fp C : C. \nWe now move on to state theorems related to the non\u00adshared memory model. Let us .rst introduce the de.nition \nof a well-partitioned heap:  De.nition 5 (Well Partitioned Heap). partitioned(H, S) W holds iff for \nall o such that o0 e o . S, |roots(H, S, o)| = 1. The predicate says that at runtime, if a non-shared \nordi\u00adnary object is write accessed, then all accesses must be ini\u00adtiated by only one non-shared task. \nWe now describe some properties related to task isolation. Theorem 3 (Static Task Isolation). If .C (H; \nS; e), then partitioned(S). This theorem says that a written non-shared ordinary object cannot be accessed \nby more than one root task at the same time. This theorem may appear appealing, but it in fact is weaker \nthan what Task Types enforce for non\u00adshared ordinary objects, because it cannot prevent a non\u00adshared \nordinary object from .rst being accessed by one task, released, and then accessed by another. The theorem \nthat fully re.ects the spirit of non-shared ordinary objects is the following: Theorem 4 (Thread Locality). \nIf fp C : C and .C (H; S; e), then WF (S). This crucial theorem says the run-time constraint set we are \ncomputing in fact conforms to the statically checked one, in that cut vertices still exist for all non-shared \nordinary . objects. Note that WF works on - . constraints, which in this case include the entire history \nof access. This theorem implies each non-shared ordinary object is accessed by at most one task over \nits entire lifetime. Let us now state some concurrency properties: Theorem 5 (Race Freedom). fp C : C, \nthen there are no race conditions for .eld access in any execution of C . This theorem can be easily \nderived from Thm. 3, which states the most important subcase that .elds with non-shared ordinary objects \ncan never be accessed by more than one root tasks. For .elds of other kinds of objects, the pre\u00adcondition \nprogressable() suf.ces to guarantee race freedom. Last, we can prove Task Type programs preserve atomicity: \nTheorem 6 (Atomicity). fp C : C, then pervasive quan\u00adtized atomicity de.ned in [LLS08] is preserved for \nall exe\u00adcutions of C . Since this property is identical to the one de.ned in [LLS08], we defer it to \nan accompanying technical report online [TT]. Informally it is the pervasive, partitioned atom\u00adicity \nproperty described in Sec. 2.2. The de.nition of that property is based on the Theory of Reductions [Lip75], \na standard method for proving atomicity properties. 5. Implementation and Evaluation A prototype implementation \nof Task Types has been built on top of the CoqaJava compiler [LLS08], a compiler built using the Polyglot \nJava framework [NCM03]. We support the core syntax presented in Fig. 4, plus standard Java fea\u00adtures \nincluding primitive values, local variable declarations and assignment, local method invocations, public \n.eld ac\u00adcess, multiple-argument methods, return statement, .eld ini\u00adtializers, arrays, static classes \nand methods, method and con\u00adstructor overloading, super invocations in constructors, super invocations \nfor regular methods, and control .ow expres\u00adsions (loops, branches, and exception handling). We rely \non Polyglot 2.4 s built-in inner class remover to process inner classes, and conservatively wrap static \n.elds as shared ordi\u00adnary objects (with lock protection) among all instances since they are global variables \nthat any access to may constitute an atomicity break point. The current implementation does not support \nre.ection or native methods. We now report some preliminary benchmarks. We picked two programs on the \nopposite ends of the contention spec\u00adtrum: the low-contention RayTracer and the high-contention PuzzleSolver. \nAll benchmarks were performed on a 4 x 6-Core Intel E7450 2.4GHz CPU machine (24 cores total) us\u00ading \n64GB RAM, running Debian GNU/Linux 2.6.26. RayTracer RayTracer is a computationally intensive algo\u00adrithm \nfor rendering 3D images and is taken from the Java Grande Suite [SBO01]. We tweaked the program to .t \nthe new Task Types syntax. In the test runs we created 12 non\u00adshared tasks to process individual Scene \nobjects concur\u00adrently; each scene contains 150 * 500 pixels. The programs were executed 50 times and \nthe .rst 2 runs were discarded. Table. 1 below reports the elapsed time in milliseconds, which is the \nmedian of the 48 hot runs. # Cores 1 2 4 6 12 Coqa 741 593 450 364 340 Task Types 622 540 279 264 289 \n Table 1. RayTracer Benchmark: Coqa vs. Task Types In the table, the Task Types results are an implemen\u00adtation \nof RayTracer with the minimal sharing declarations needed for the program to pass the Task Types typechecker. \nThe program contains 1955 lines of code in 16 classes. We are able to declare 12 classes out of them \nto be non-shared ordinary classes (\u00b5 = E); two are non-shared task classes, i.e. thread launchers, and \ntwo needed to be declared shared. The fact that the vast majority of classes are typable as non\u00adshared \nordinary classes con.rms our earlier assertion that most objects can be coded normally no special sharing \ndeclaration is needed. Coqa is a re-implementation of the same program with all 16 ordinary objects above \nexplicitly declared as shared, meaning the program will follow the Coqa model and all non-task objects \nwill be guarded with runtime locks so at most one task can use them at a time. As can be seen, the use \nof non-shared objects led to a nontrivial performance improvement, averaging about 20% faster across \nthe cases here. This preliminary result con.rms that a nontrivial speedup will be obtained compared to \nthe purely dynamic approach of Coqa.  PuzzleSolver The PuzzleSolver benchmark was discussed earlier \nin Sec. 2.3.1. We benchmarked both the Task Types re-implementation of PuzzleSolver as well as the original \nJava implementation. To give meaningful multicore data, we created 1 task in the 1-core run, 2 tasks \nin the 2-core run, and so on. The programs were executed 20 times and the .rst 2 runs were discarded. \nTable. 2 below reports the elapsed time in milliseconds, which is the median of the rest of 18 hot runs. \n# Cores 1 2 4 6 12 Task Types 123 617 94 99 100 Java 80 564 91 74 82 Table 2. PuzzleSolver Benchmark \nResults Observe that for a single-core execution, the Task Type implementation is about 35% slower. As \nthe number of cores increases, the slowdown decreases to around 20%. Observe also that due to high contention \nthis benchmark is not ap\u00adpreciably faster with multiple cores; it also exhibits an odd slowdown at two \ncores which is hard to explain but is in the underyling Java concurrency implementation since both implementations \nexhibit it. Our implementation of runtime shared task locking is still inef.cient and can be improved, \nand we are also unnecessarily locking PuzzlePosition accesses in this benchmark as was pointed out in \nSec. 2.3.1. So, we expect these numbers to improve considerably in a production implementation. That \nis not to say we expect there will be no runtime penalty at all; it is unlikely to go all the way to \nzero as some of the locking will not be necessary for correctness. Compilation Time Compilation of both \nbenchmarks took only a few seconds; the constraint closure step took around 1/3 sec. in each case, which \nshows it is not a signi.cant factor here. More ef.cient implementations of context\u00adsensitive/polymorphic \ntype closure have been investigated [MRR05; EGH94; Age96; WS01; WL04]. As we move on to larger code samples, \nwe will consider implementing var\u00adious optimizations such as BDDs [BLQ+03; WL04] and constraint garbage \ncollection [WS01] if they are needed. 6. Related Work Type Systems, Static Analyses, and Logics Flanagan \nand Qadeer [FQ03] .rst explored type system support for atom\u00adicity; their work takes a shared-memory \nmodel as basis, and supports reasoning about the composibility of atomic blocks over the shared memory, \nwhereas Task Types can be viewed as carving out a non-shared-memory model in which atom\u00adicity violations \ncannot arise. STM systems primarily use dynamic means to enforce atomicity [HF03; WJH04; CMC+06]. The \nproblem of weak atomicity has attracted signi.cant interest recently, and a number of static or hybrid \nsolutions exist to ensure transac\u00adtional code and non-transactional code do not interfere with each other. \nSuch a property is enforced by Shpeisman et. al. [SMAT+07] via a hybrid approach with dynamic escape \nanalysis and a not-accessed-in-transaction static analysis. The latter has the good property of allowing \ndata handoff : a transaction can pass along an object without accessing it. Data handoff is useful for \ndifferent threads sharing a data structure (say a queue) but not the elements in it. This prop\u00aderty also \nholds for Task Types. Passing along an object refer\u00adence or storing the reference is not considered access \nsince atomicity is not violated. AME [ABHI08] describes a con\u00adceptually static procedure to guarantee \nviolation-freedom of transactional code and non-transactional code. Harris et. al. [HMPJH05] used the \nmonads of Haskell to separate compu\u00adtations with effects and those without. Constructing type systems \nto assure race condition free\u00addom is a well-explored topic. These systems work under signi.cantly different \nassumptions than we do: they typi\u00adcally assume a Java-like shared memory with explicit lock acquire/release \nexpressions. Given the non-shared memory assumption and protected access to shared task objects, race \nconditions do not occur in our system. Static inference techniques have been designed to auto\u00admatically \ninsert locks to enforce atomicity for atomic blocks [MZGB06; HFP06; EFJM07; CCG08]. These techniques \nas\u00adsume that atomic blocks are a fundamentally local con\u00adstruct, and make assumptions that are unrealistic \nfor sup\u00adporting the larger atomic regions we aim for: Autolocker [MZGB06] requires all invocations in \nan atomic block to be inlined, a static bound on lock resources is needed in others [EFJM07; CCG08; HFP06], \nand some require that all ob\u00adjects accessed in the atomic block not be accessed elsewhere [HFP06]. Our \nlocality enforcement algorithm is related to owner\u00adship/region types, and particularly their inference \n[CCQR04; Wre03; DM07; LS08]. Cut vertices are evocative of domina\u00adtors in the owners-as-dominators ownership \ntype systems; in Task Types, DAGs can be freely formed inside the bound\u00adary of the dominator. A major \nchallenge of our design the problem that motivated task twinning is not an is\u00adsue for ownership/region \ntypes. When recursion happens, there is nothing wrong for ownership/region type declara\u00adtion/inference \nsystems to assign the same type variable to all recursive occurrences, but this would lead to atomicity \nviola\u00adtion in our situation; two copies of the task static instances are needed. . There are many static \nanalysis algorithms for tracking the .ow of objects. Closest to our work are several thread es\u00adcape analyses \n[CGS+99; WR99; Bla99] for object-oriented languages, which use reachability graphs to prevent or track \nalias escape from threads. Task Types share a focus on thread locality with this work, but differ in \ntwo important aspects:  (1) the pervasive atomicity of our language requires shared task-style sharing, \nwhich is a partial escape that should be allowed but has no representation in these analyses; (2) Fun\u00addamentally, \nobject references do not need to be con.ned to guarantee atomicity: it is perfectly .ne for a task to \ncreate an object, pass it over to another task, which in turn stores it or passes it further to a third \ntask. The key to atomicity is there is no con.icting object access. The effect of these dif\u00adferences \nis to produce unique issues that escape analyses do not encounter and we need to solve here. A type system \nis a logical system and our work has a dis\u00adtant relation to program logics such as separation logic. \nIn separation logic, local properties of the heap can be guar\u00adanteed by partitioning the heap, reasoning \nabout the com\u00adponents, and then soundly re-composing to get the property over the full heap. Task Types \nshare this philosophy of heap partitioning in how the access relations partition objects. Al\u00adthough there \nare currently no separation logics for concur\u00adrent object-oriented languages as we know of, there has \nbeen work on separation logics for Java [PB05; PB08] and for concurrency [OHe07; Bro07]. We believe the \nlocality prop\u00aderty of relation cutExists is dif.cult to express with heap partitioning since they are \na subtle form of heap sharing; these may be useful areas on which to focus extensions to separation logic. \nLanguage Designs for Atomicity The shortcomings of ex\u00adplicitly declared atomic blocks are summarized \nin [VTD06]. In that work, a data-centric approach is taken: the .elds of an object are partitioned into \nstatically labelled atomic sets, and access to .elds in the same set is guaranteed to be atomic, analogous \nto declaring different shared tasks for different atomic sets in Task Types. Their data-centric approach \nis a step forward compared with atomic blocks, but the de\u00adsign philosophy is still no-atomicity-unless-you-declare-it, \nand hence fundamentally different from our notion of perva\u00adsive atomicity. The Actor model and related \nmessage-passing languages [Agh90; Arm96; SM08] achieve atomicity by imposing stronger limitations on \nobject sharing: threads communi\u00adcate only at thread launch. A primary appeal of this model is each actor \nmessage handler thread executes atomically. If a synchronous communication is needed however, the sender \nneeds to have an explicit message handler for processing the return value. The synchronous sender must \nthus be coded as two handlers, the code for actions up to and including the send, and the post-send return \nvalue handler (the contin\u00aduation); this breaks the sender into two different atomicity regions. Additionally, \nthis breaks the obvious control .ow that would be apparent in synchronous messaging syntax, making programming \nmore dif.cult. Some implemented Actor-based languages do include implicit CPS transfor\u00admation syntax \nto ease coding, but that convolutes the code meaning (variable scoping for example) and does not repair \nthe fact that the span of an atomic region was broken. Kilim [SM08] is a more recent actor-like language, \nwith a focus on providing re.ned message passing mechanisms with\u00adout sacri.cing the isolation property \nof Actors. The Kilim type system relies on extra programmer declarations called isolation modi.ers to \ndenote how each parameter can be passed/used in the inter-actor context. Kilim and Task Types have the \nsame focus on object isolation, but in orthogonal design spaces: Kilim on message passing, and Task Types \non atomicity. A recent work that is closer to our spirit of pervasive atomicity is AME [IB07; ABHI08]. \nThe language constructs of AME are along the lines of Actors, where an async e expression starts up an \natomicity-preserving thread. AME is different from our work in its support of an expression unprotected \ne, meaning atomicity is not preserved for e. This is fundamentally different from pervasive atomicity. \nIn addition, AME does not support synchronous messaging, and does not overlap with the static type system \naspect of our work. A more conservative approach than atomicity is to de\u00adsign determinism into a concurrent \nlanguage, such as build\u00ading a type and effect system to guarantee threads are de\u00adterministic [BAD+09]. \nWhile this system is very useful for some algorithms, it is surprisingly subtle the properties that make \nalgorithms deterministic and their system is often too weak to detect the determinism. Also, many algorithms \nare in fact not deterministic because some choices can be arbi\u00adtrary. Note that both of our benchmarks \nare fundamentally arbitrary the PuzzleSolver for example has tasks compet\u00ading for next moves and the \nprocessor timing will dictate which task wins). The pervasive atomicity of this paper is a less rigid \nnotion for the programmer in that threads must be divided into deterministic segments but need not be \nwholly deterministic. The two approaches are compatible; perhaps an ideal language would use the DPJ \napproach when deter\u00adminism was feasible to guarantee, and fallback to Task Types when not. 7. Discussion \nIn our initial experience we have not found it dif.cult to port basic concurrency patterns to Task Types, \nbut further work is needed to increase accuracy of the system to cover the full spectrum of programming \npatterns. Since increased accuracy also brings increased complexity, there needs to be a strong justi.cation \nfor an extension before it is made. We left out several potentially useful extensions in this paper, \nwhich we discuss now. Simple Extensions Several features are left out from the formal system for simplicity, \nbut can be added with minimal change. The .rst feature is per-object sharing declarations (rather than \nper-class sharing declarations as found in the core calculus). For instance, the PuzzlePosition class \nof Sec. 2.3.1 is a case where a per-object declaration of the sharing policy, in place of the current \nper-class declaration, would have allowed the program to typecheck without the need for runtime locks. \nThis extension is technically trivial. Because our polymorphic type system is able to differenti\u00adate \nobjects of the same class anyways, the only change to implement it would be (1) turning types from c@a \nto a more verbose form such as c@a of \u00b5, and (2) whenever a .ow constraint is generated, a constraint \nasserting both sides have compatible modi.ers is collected.  A second extension which would have allowed \nthe same program above to typecheck is a call-by-copy parameter passing mechanism. This is a variation \non immutability which allows object data to be freely passed from one task to another since the callee \ngets a fresh copy and thus no back-channel will be created. A third extension in which the granularity \nof sharing could be made .ner is to take individual .elds of objects as the atomic units of sharing, \nmeaning some .elds could be owned by one thread and other .elds by another thread. This approach may \nindeed lead to more programs to typecheck and it is not dif.cult to implement, but it is not clear to \nus that this is a good idea for object-oriented programmers (as opposed to C programmers). It violates \na principle of object\u00adbased design in how an object has dual allegiance; in fact this may very well be \na sign that such an object needs to be refactored into two different objects. On the design spectrum \nwith full declaration on one end and full inference on the other, Task Types lie somewhere in the middle: \nit is a type inference system but shared and task classes and messaging must be explicitly declared. \nIn general, the design principle here is that sharing policies and execution policies for classes are \nessential and should be de\u00adclared, but enforcement can be the job of a program analy\u00adsis, in this case \na polymorphic type inference algorithm, for programmability reasons. The ideas here are to some degree \nindependent of this spectrum: it would be possible to infer sharing when needed, and it would also be \npossible to have an explicit declarative type system to capture the necessary well-formedness properties. \nNon-Trivial Extensions Task Types disallow a non-shared ordinary object to be used in phases , each of \nwhich be\u00adlongs to a different task: from bootstrapping to a partic\u00adular point of execution, object o \nbelongs to task X; from that point on however, o will never be used by X again, and can be grabbed by \ntask Y. To see why this could in some cases be a useful feature, consider the Swing AWT invokeAndWait \nmethod. This method allows one thread to pass some GUI-manipulating code to the AWT thread where it will \nbe queued up as an event and handled. Since Swing is not thread-safe this is the only safe way another \nthread can interact with Swing widgets outside of listen\u00aders. Consider the following example, taken from \nthe Swing threading documentation: void printTextField() throws Exception { final String[] myStrings \n= new String[2]; Runnable getTextFieldText = new Runnable() { public void run() { myStrings[0] = textField0.getText(); \nmyStrings[1] = textField1.getText(); } }; SwingUtilities.invokeAndWait(getTextFieldText); System.out.println(myStrings[0] \n+ \" \" + myStrings[1]); } Here invokeAndWait will pass the code snippet in run() for execution on the \nAWT thread; the current thread will block until run() completes. If this Java code was ported directly \nto Task Types, the myStrings array would want to be statically owned by both the main task and the AWT \ntask and so typing would fail; not even making the array a shared ordinary object would help since even \nafter run() completes the AWT thread will not release the array since it has not completed. A call-by-copy \nsemantics, as dis\u00adcussed above, would restore typability since the original task will have a fresh copy \nof the data; however, this adds unnec\u00adessary overhead. This example shows it may be useful to add an \nobject transfer feature. As can be seen in the above ex\u00adample, tasks hold on to shared ordinary objects \neven if they are in fact .nished with them, but early release at an exist\u00ading atomicity break point would \nnot increase the number of atomic regions and would allow programs such as the above to statically typecheck: \nat the point where run() completes, the myStrings array can safely be transferred back to the original \ntask. For the above example a .ow-insensitive type analysis should be able to determine that myStrings \ndoes not escape run() and if a singular capability to run() were passed to the AWT task, object transfer \nupon return is sound. In more complex cases it may be necessary to use a .ow-sensitive analysis to detect \nwhen an object is no longer used. We discussed task twinning in Sec. 2.2. This method is relatively easy \nto prove correct, but in practice this conser\u00advative de.nition can be re.ned in several situations. such \nas a program point is obviously not instantiating more than one task, or if it does, the instantiated \ntasks are obviously not accessing the same objects. One re.nement was mentioned in Sec. 2.2: twinning \nis not needed when a simple analysis determines a program point is not in a recursive context. An\u00adother \nre.nement would be to track information .ow in and out of the task object: no sharing can happen between \ntwo tasks instantiated from the program point twinning is not needed as a result if there is no .ow \nleading them to share. Limitations A common complaint of constraint-based type systems such as this one \nis they are non-modular, since the whole program is needed to compute the constraint clo\u00adsure. This mainly \nleads to two problems: (1) the burden of typechecking overhead at dynamic loading time a la Java, and \n(2) the dif.culty of printing precise error messages. As was discussed in Sec. 3, problem (1) motivated \nus to for\u00admulate our system in two phases, so that many type errors can in fact be discovered modularly. \nFurthermore, if dynam\u00adically loaded code was certi.ed by signature to be identical to the compile-time \ncode, no additional dynamic link check\u00ading would be needed. We agree that problem (2) is a real challenge. \nIndeed, any system with type inference across modularity boundaries such as the type inference of ML \n is faced with this challenge. As our compiler matures, we are interested in studying various error localization \napproaches.  8. Conclusion We have presented an approach to achieve atomicity in mul\u00adtithreaded object-oriented \nprograms, by making the follow\u00ading contributions: We develop a top-down, mostly static approach to en\u00adforce \natomicity as opposed to the usual bottom-up, dy\u00adnamic approach. Atomicity with Task Types is top-down \nin the sense that it is pervasive instead of building islands of atomic blocks. Principled language constructs \nfor shar\u00ading between tasks are provided, and strong atomicity is achieved for all code regardless of \nsharing.  We provide a programming model whose language syn\u00adtax requires only minor changes to standard \nobject syn\u00adtax, but the philosophy of a non-shared memory model as a default, and explicit support for \ndata sharing between threads, is signi.cantly different than the norm. As a con\u00adsequence, the object \nsharing between tasks is brought front and center for the programmer, where it should be.  Task Types \nincorporate a precise and provably sound polymorphic/context-sensitive analysis which statically veri.es \nthat non-shared ordinary objects are appropri\u00adately partitioned between tasks. Since the partitioning \nis veri.ed statically, there is no need for dynamic partition\u00ading of non-shared objects between tasks, \nmeaning there is no additional runtime overhead.  The viability of Task Types has been con.rmed by a \npro\u00adtotype compiler, and initial benchmark results are reason\u00adable.  Task Types are a complex type system; \nwe believe this stems from complexity inherent in writing shared-memory concurrent programs correctly. \nAlthough it could be argued that means shared-memory concurrency needs to be aban\u00addoned, we are perhaps \ntoo far down that road to turn around, and Task Types represent a compromise between the Wild West of \nJava programming today and the rigid straight\u00adjacket of non-shared memory concurrency. Programmers also \nneed not understand every detail of the type system, only that some object is shared more than the type \nsystem is allowing, and either its use must be restricted or its class must be lifted to be a shared \nclass. As in many type system approaches, the programmer bur\u00adden of typing under Task Types will be nontrivial. \nHowever, that is not necessarily a bad thing: the ML language by anal\u00adogy has a type system that is challenging \nfor new program\u00admers, but greatly increases productivity in the long term, be\u00ad cause the type system \ncatches what would have been dif.cult run-time bugs and so the extra time spent typechecking is more \nthan made up in the time saved in later debugging. We believe Task Types will offer a similar advantage \nfor concur\u00ad rent programming. The webpage of Task Types [TT] includes all reduction rules, a complete \nset of proofs for all the theorems presented in this paper, as well as the source code of the compiler. \nReferences [ABHI08] Mart\u00b4in Abadi, Andrew Birrell, Tim Harris, and Michael Isard. Semantics of transactional \nmemory and automatic mutual exclusion. In POPL 08, pages 63 74, 2008. [Age96] Ole Agesen. Concrete type \ninference: delivering object-oriented applications. PhD thesis, Stanford University, Stanford, CA, USA, \n1996. [Agh90] Gul Agha. ACTORS : A model of Concurrent com\u00adputations in Distributed Systems. MITP, Cambridge, \nMass., 1990. [Arm96] J. Armstrong. Erlang a Survey of the Language and its Industrial Applications. \nIn INAP 96 The 9th Exhibitions and Symposium on Industrial Appli\u00adcations of Prolog, pages 16 18, Hino, \nTokyo, Japan, 1996. [BAD+09] Robert L. Bocchino, Jr., Vikram S. Adve, Danny Dig, Sarita V. Adve, Stephen \nHeumann, Rakesh Komurav\u00adelli, Jeffrey Overbey, Patrick Simmons, Hyojin Sung, and Mohsen Vakilian. A type \nand effect system for deterministic parallel java. In OOPSLA 09, pages 97 116, 2009. [Bla99] Bruno Blanchet. \nEscape analysis for object-oriented languages: application to java. SIGPLAN Not., 34(10):20 34, 1999. \n[BLQ+03] Marc Berndl, Ondrej Lhot\u00b4ak, Feng Qian, Laurie Hen\u00addren, and Navindra Umanee. Points-to analysis \nusing bdds. In PLDI 03, pages 103 114, 2003. [BLR02] Chandrasekhar Boyapati, Robert Lee, and Martin Ri\u00adnard. \nOwnership types for safe programming: pre\u00adventing data races and deadlocks. In OOPSLA 02, pages 211 230, \n2002. [Bro07] Stephen Brookes. A semantics for concurrent sepa\u00adration logic. Theoretical Computer Science, \n375(1\u00ad3):227 270, 2007. [CCG08] Sigmund Cherem, Trishul Chilimbi, and Sumit Gul\u00adwani. Inferring locks \nfor atomic sections. In PLDI 08, pages 304 315, 2008. [CCQR04] Wei-Ngan Chin, Florin Craciun, Shengchao \nQin, and Martin Rinard. Region inference for an object\u00adoriented language. In PLDI 04, pages 243 254, \n2004. [CGS+99] Jong-deok Choi, Manish Gupta, Mauricio Serrano, Vugranam C. Sreedhar, and Sam Midkiff. \nEscape analysis for java. In OOPSLA 99, pages 1 19, 1999. [Cla01] Dave Clarke. Object Ownership and Containment. \nPhD thesis, University of New South Wales, July 2001.  [CMC+06] B. CarlStrom, A. McDonald, H. Cha., \nJ. Chung, C. Minh, C. Kozyrakis, and K. Olukotun. The atomos transactional programming language. In PLDI \n06, June 2006. [CPN98] David G. Clarke, John M. Potter, and James Noble. Ownership types for .exible \nalias protection. In In OOPSLA 98, pages 48 64. ACM Press, 1998. [DG04] Jeffrey Dean and Sanjay Ghemawat. \nMapReduce: Simpli.ed data processing on large clusters. In OSDI 04, 2004. [DM07] W. Dietl and P. M\u00a8uller. \nRuntime universe type infer\u00adence. In IWACO 07, 2007. [EFJM07] Michael Emmi, Jeffrey S. Fischer, Ranjit \nJhala, and Rupak Majumdar. Lock allocation. In POPL 07, pages 291 296, 2007. [EGH94] Maryam Emami, Rakesh \nGhiya, and Laurie J. Hen\u00addren. Context-sensitive interprocedural points-to analysis in the presence of \nfunction pointers. In PLDI 94, pages 242 256, 1994. [FQ03] Cormac Flanagan and Shaz Qadeer. A type and \neffect system for atomicity. In PLDI 03, pages 338 349, 2003. [GLS94] William D. Gropp, Ewing Lusk, and \nAnthony Skjel\u00adlum. Using MPI Portable Parallel Programming with the Message Passing Interface. MIT Press, \nCam\u00adbridge, MA, 1994. [Gro03] Dan Grossman. Type-safe multithreading in cyclone. In TLDI 03, pages 13 \n25, 2003. [HF03] Tim Harris and Keir Fraser. Language support for lightweight transactions. In OOPSLA \n03, pages 388 402, 2003. [HFP06] Michael Hicks, Jeffrey S. Foster, and Polyvios Prat\u00adtikakis. Lock inference \nfor atomic sections. In TRANSACT 06, June 2006. [HMPJH05] Tim Harris, Simon Marlow, Simon Peyton-Jones, \nand Maurice Herlihy. Composable memory transactions. In PPoPP 05, pages 48 60, 2005. [IB07] Michael Isard \nand Andrew Birrell. Automatic mutual exclusion. In HOTOS 07: Proceedings of the 11th USENIX workshop \non Hot topics in operating sys\u00adtems, pages 1 6, Berkeley, CA, USA, 2007. USENIX Association. [IPW99] \nAtsushi Igarashi, Benjamin Pierce, and Philip Wadler. Featherweight java -a minimal core calculus for \njava and gj. In ACM Transactions on Programming Lan\u00adguages and Systems, pages 132 146, 1999. [LB98] Sheng \nLiang and Gilad Bracha. Dynamic class load\u00ading in the java virtual machine. In In OOPSLA 98, pages 36 \n44. ACM Press, 1998. [Lip75] Richard J. Lipton. Reduction: a method of prov\u00ading properties of parallel \nprograms. Commun. ACM, 18(12):717 721, 1975. [LLS08] Yu David Liu, Xiaoqi Lu, and Scott F. Smith. Coqa: \nConcurrent objects with quantized atomicity. In CC 08, March 2008. [LS08] Yu David Liu and Scott Smith. \nPedigree types. In 4th International Workshop on Aliasing, Con.ne\u00ad ment and Ownership in object-oriented \nprogramming (IWACO), pages 63 71, July 2008. [MRR05] Ana Milanova, Atanas Rountev, and Barbara G. Ry\u00adder. \nParameterized object sensitivity for points-to analysis for java. ACM Trans. Softw. Eng. Methodol., 14(1):1 \n41, 2005. [MZGB06] Bill McCloskey, Feng Zhou, David Gay, and Eric Brewer. Autolocker: synchronization \ninference for atomic sections. In POPL 06, pages 346 358, 2006. [NCM03] Nathaniel Nystrom, Michael R. \nClarkson, and An\u00addrew C. Myers. Polyglot: An extensible compiler framework for java. In CC 03, volume \n2622, pages 138 152, NY, April 2003. Springer-Verlag. [OHe07] Peter W. OHearn. Resources, concurrency, \nand local reasoning. Theoretical Computer Science, 375(1\u00ad3):271 307, 2007. [PB05] Matthew Parkinson and \nGavin Bierman. Separation logic and abstraction. In POPL 05, pages 247 258, 2005. [PB08] Matthew J. Parkinson \nand Gavin M. Bierman. Sep\u00adaration logic, abstraction and inheritance. In POPL 08, pages 75 86, New York, \nNY, USA, 2008. ACM. [SBO01] L. A. Smith, J. M. Bull, and J. Obdrz\u00b4 alek. A parallel java grande benchmark \nsuite. In Supercomputing 01: Proceedings of the 2001 ACM/IEEE conference on Supercomputing, 2001. [SM08] \nSriram Srinivasan and Alan Mycroft. Kilim: Isolation-typed actors for java. In ECOOP 08, 2008. [SMAT+07] \nTatiana Shpeisman, Vijay Menon, Ali-Reza Adl-Tabatabai, Steven Balensiefer, Dan Grossman, Richard L. \nHudson, Katherine F. Moore, and Bratin Saha. Enforcing isolation and ordering in stm. In PLDI 07, pages \n78 88, 2007. [TT] http://www.cs.binghamton.edu/ davidL/ tasktypes. [TT97] Mads Tofte and Jean-Pierre \nTalpin. Region-based memory management. Information and Computation, 1997. [VTD06] Mandana Vaziri, Frank \nTip, and Julian Dolby. As\u00adsociating synchronization constraints with data in an object-oriented language. \nIn POPL 06, pages 334 345, 2006. [WJH04] Adam Welc, Suresh Jagannathan, and Antony L. Hosking. Transactional \nmonitors for concurrent ob\u00adjects. In ECOOP 04, pages 519 542, 2004. [WL04] John Whaley and Monica S. \nLam. Cloning-based context-sensitive pointer alias analysis using binary decision diagrams. In PLDI 04, \npages 131 144, 2004. [WR99] John Whaley and Martin Rinard. Compositional pointer and escape analysis \nfor java programs. In OOPSLA, pages 187 206, 1999. [Wre03] Alisdair Wren. Ownership type inference. Master \ns thesis, Imperial College, 2003. [WS01] Tiejun Wang and Scott F. Smith. Precise constraint\u00adbased type \ninference for Java. In ECOOP 01, pages 99 117, 2001.   \n\t\t\t", "proc_id": "1869459", "abstract": "<p>Atomic regions are an important concept in correct concurrent programming: since atomic regions can be viewed as having executed in a single step, atomicity greatly reduces the number of possible interleavings the programmer needs to consider. This paper describes a method for building atomicity into a programming language in an organic fashion. We take the view that atomicity holds for whole threads by default, and a division into smaller atomic regions occurs only at points where an explicit need for sharing is needed and declared. A corollary of this view is every line of code is part of some atomic region. We define a polymorphic type system, <i>Task Types</i>, to enforce most of the desired atomicity properties statically. We show the reasonableness of our type system by proving that type soundness, isolation invariance, and atomicity enforcement properties hold at run time. We also present initial results of a Task Types implementation built on Java</p>", "authors": [{"name": "Aditya Kulkarni", "author_profile_id": "81470649141", "affiliation": "SUNY Binghamton, Binghamton, NY, USA", "person_id": "P2354125", "email_address": "", "orcid_id": ""}, {"name": "Yu David Liu", "author_profile_id": "81414612105", "affiliation": "SUNY Binghamton, Binghamton, NY, USA", "person_id": "P2354126", "email_address": "", "orcid_id": ""}, {"name": "Scott F. Smith", "author_profile_id": "81381593469", "affiliation": "The Johns Hopkins University, Baltimore, MD, USA", "person_id": "P2354127", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1869459.1869514", "year": "2010", "article_id": "1869514", "conference": "OOPSLA", "title": "Task types for pervasive atomicity", "url": "http://dl.acm.org/citation.cfm?id=1869514"}