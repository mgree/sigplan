{"article_publication_date": "10-17-2010", "fulltext": "\n Language Virtualization for Heterogeneous Parallel Computing Hassan Cha.* Zach DeVito* Adriaan Moors \nTiark Rompf Arvind K. Sujeeth* Pat Hanrahan* Martin Odersky Kunle Olukotun* *Stanford University: {hcha., \nzdevito, asujeeth, hanrahan, kunle}@stanford.edu EPFL: {.rstname.lastname}@ep..ch Abstract As heterogeneous \nparallel systems become dominant, appli\u00adcation developers are being forced to turn to an incompatible \nmix of low level programming models (e.g. OpenMP, MPI, CUDA, OpenCL). However, these models do little \nto shield developers from the dif.cult problems of parallelization, data decomposition and machine-speci.c \ndetails. Most pro\u00adgrammers are having a dif.cult time using these program\u00adming modelseffectively.Toprovidea \nprogramming model that addresses the productivity and performance require\u00adments for the average programmer, \nwe explore a domain\u00adspeci.c approach to heterogeneous parallel programming. We propose language virtualization \nas a new principle that enables the construction of highly ef.cient parallel do\u00admain speci.c languages \nthat are embeddedinacommon host language.We de.ne criteria for language virtualization and present techniquesto \nachieve them.We presenttwoconcrete case studies of domain-speci.c languages that are imple\u00admented using \nour virtualization approach. Categories and Subject Descriptors D.1.3[Programming Techniques]: Concurrent \nProgramming Parallel program\u00adming; D.3.4[Programming Languages]:Processors Code generation, Optimization, \nRun-time environments General Terms Languages, Performance Keywords Parallel Programming, Domain Speci.c \nLan\u00adguages, Dynamic Optimizations 1. Introduction Until the early 2000s, advances in out-of-order (OOO) \nsu\u00adperscalar processors provided applications with improved performance by increasing the CPU core clock \nfrequency Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page.To copyotherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. Onward! \n2010, October 17 21, 2010, Reno/Tahoe, Nevada, USA. Copyright c &#38;#169; 2010ACM 978-1-4503-0236-4/10/10... \n$10.00 andthe numberofinstructionsperclockcycle(IPC).With each newgeneration of processors, software \ndevelopers were able to leverage this performance increase to create more compelling applications without \nchanging their program\u00adming model. Furthermore, existing applications also bene\u00ad.tedfromthis performance \nincreasewithnoextraeffort.The inability of processor vendors to deliver higher performance froma single \ncore withoutexceedinga reasonablepower en\u00advelope has brought this so called free lunch era to an end \n[47]. Power ef.ciencyis now the most dominant design driver for microprocessors. Power ef.cient microprocessor \ndesign favors chip-multiprocessors consisting of simpler cores[24, 32] and heterogeneous systems consisting \nof general pur\u00adpose processors, SIMD units and special purpose accelerator devices such as graphics processing \nunits (GPUs)[2, 44] and cryptographic units. Existing applications can no longer take advantageofthe \nadditional computepoweravailablein these newand emerging systems withoutasigni.cant parallel pro\u00adgramming \nand program specialization effort. However, writ\u00ading parallel programs is not straightforward because \nin con\u00adtrast to the familiar and standard von Neumann model for sequential programming, a variety of \nincompatible parallel programming models are required, each with their own set of trade-offs. The situation \nis even worse for programming heterogeneous systems where each accelerator vendor usu\u00adally provides a \ndistinct driver API and programming model to interface with the device. Writing an application that di\u00adrectlytargetsthevarietyofexisting \nsystems,let alone emerg\u00ading ones, becomes a very complicated affair. While one can hope that a few parallel \nprogramming ex\u00adperts will be able to tackle the complexity of developing parallelheterogeneous software,expecting \ntheaverage pro\u00adgrammer to deal with all this complexity is simply not realis\u00adtic.Moreover,exposingthe \nprogrammer directlytothevari\u00adous programming models supportedbyeach computedevice will impact application \nportability as well as forward parallel scalability; as new computer platforms emerge, applications will \nconstantly need to be rewritten to take advantage of any new capabilities and increased parallelism. \nFurthermore, the most ef.cient mapping of an application to a heterogeneous parallel architecture occurs \nwhen the characteristics of the application are matched to the different capabilities of the ar\u00adchitecture. \nThis represents a signi.cant disadvantage of this approach: for each application and for each computing \nplat\u00adform a specialized mapping solution must be created by a programmer that is an expert in the speci.c \ndomain as well as in the targeted parallel architecture. This creates an ex\u00adplosion and fragmentation \nof mapping solutions and makes it dif.cult to reuse good mapping solutions created by ex\u00adperts.  1.1 \nThe Needfor DSLs One way to capture application-speci.c knowledge for a whole class of applications and \nsimplify application devel\u00adopmentatthe sametimeisto usea domain speci.c language (DSL).A DSLisa concise \nprogramming language witha syntax that is designed to naturally express the semantics of a narrow problem \ndomain [49]. DSLs, sometimes called little languages [4] or telescoping languages [29], have beeninuse \nfor quite some time.Infact,itis likely that most application developers have already used at least one \nDSL. Examples of commonly used DSLs areTeX and LaTeX for typesetting academic papers, Matlab for testing \nnumerical linear algebra algorithms, and SQL for querying relational databases. One can also view OpenGL \nasa DSL.Byexposing an in\u00adterface for specifying polygons and the rules to shade them, OpenGL created \na high-level programming model for real\u00adtime graphics decoupled from the hardware or software used to \nrender it, allowing for aggressive performance gains as graphics hardware evolves. Even the Unix shell \ncan be con\u00adsidereda DSL that providesa command-line interface to un\u00adderlying operating system functions \nsuch as .le and process management.TheuseofDSLscanprovide signi.cantgains in the productivity and creativity \nof application developers, the portability of applications, and application performance. Thekeybene.tsof \nusinga domain-speci.c approach for enhancing application performance are using domain knowl\u00adedgefor staticand \ndynamic optimizationsofaprogram writ\u00adten using a DSL and the ability to reuse expert knowledge for mapping \napplications ef.ciently to a specialized archi\u00adtecture. Most domain speci.c optimizations would not be \npossibleifthe programwas writtenina general purposelan\u00adguage. General-purpose languages are limited when \nit comes to optimization for at least two reasons. First, theymust pro\u00adduce correct code across a very \nwide range of applications. This makes it dif.cult to apply aggressive optimizations compiler developers \nmust err on the side of correctness. Sec\u00adond, because of the general-purpose nature needed to sup\u00adportawiderangeof \napplications(e.g. .nancial,gaming,im\u00adage processing, etc.), compilers can usually infer little about \nthe structure of the data or the nature of the algorithms the code is using. In contrast, DSL compilers \ncan use aggressive optimization techniques using knowledge of the data struc\u00adtures and algorithms derived \nfrom the DSL. This makes it possible to deliver good performance on heterogeneous ar\u00adchitectures using \nDSLs. Using DSLs naturally divides the application develop\u00adment processintotwophases.First,aDSLdeveloperdesigns \naDSLfora speci.c domain.Then,amuchlarger numberof domain experts make use of the DSL to develop applica\u00adtions. \nThe ideal DSL developer would be a domain expert, a parallelism expert, and a language and compiler expert. \nSuch developers are rare and so there is a need to simplify the process of developing DSLs for parallelism. \nTraditionally,DSLs havebeen developed from the ground\u00adup using custom compiler infrastructure. This is \ncalled the external DSL approach . There are two problems with this approach to DSLs. First, developing \nthese new languages to a suf.cient degree of maturity is an enormous effort. This investment wouldhave \nto include not just language speci.\u00adcations and construction of their optimizing compilers and libraries, \nbut also all the other aspects of modern tooling including IDEs, debuggers, pro.lers,build tools as well \nas documentation and training. It is hard to see how such an investment can be made repeatedly for each \nspecialized do\u00admain. Second, DSLsdo notexistinavacuumbuthaveto interfaceto other partsofa system.For \ninstance,a climate simulation program could have a visualization component that is based on a graphics \nDSL. It is not clear how multiple separate DSLs would inter-operate without creating a new DSL that integrates \nthe desired combination of the others.  1.2 Embedded DSLs and LanguageVirtualization Embedded DSLs [23] \novercome the problems with external DSLs and make DSL development more tractable. An em\u00adbedded DSL lives \ninside of a host language. It is quite like a framework or a library, consisting of a set of classes \nand operations on objects of those types. There are several ad\u00advantages to using embedded DSLs for application \nwriters. First, programmers do not have to learn a completely new syntax. Second, multiple DSLs can be \ncombined in the same application. Third, the DSL infrastructure can be shared be\u00adtween different DSLs \nand DSL developers will all use the same infrastructure forbuilding their DSLs. The main problem with \nembedded DSLs is that theycan\u00adnot exploit domain knowledge to ef.ciently map programs to specialized \narchitectures because they live inside a gen\u00aderal purpose language. There is no obvious solution to this \nproblem apart from replicating much of the effort ofbuild\u00ading a stand-alone DSL implementation. To overcome \nthis problem, we need embedding languages that are particularly suitedtothetaskofservingasahost languagetoDSLimple\u00admentations. \nA language with this capability supports what we call language virtualization.Alanguage is virtualizable \nif and only if it can provide an environment to embedded lan\u00adguages that makes them essentially identical \nto correspond\u00ading stand-alone language implementations in terms of (1) expressiveness being able to \nexpress a DSL in a way which is natural to domain specialists,\n\t\t\t", "proc_id": "1869459", "abstract": "<p>As heterogeneous parallel systems become dominant, application developers are being forced to turn to an incompatiblemix of low level programming models (e.g. OpenMP, MPI, CUDA, OpenCL). However, these models do little to shield developers from the difficult problems of parallelization, data decomposition and machine-specific details. Most programmersare having a difficult time using these programming models effectively. To provide a programming modelthat addresses the productivity and performance requirements for the average programmer, we explore a domainspecificapproach to heterogeneous parallel programming.</p> <p>We propose language virtualization as a new principle that enables the construction of highly efficient parallel domain specific languages that are embedded in a common host language. We define criteria for language virtualization and present techniques to achieve them.We present two concrete case studies of domain-specific languages that are implemented using our virtualization approach.</p>", "authors": [{"name": "Hassan Chafi", "author_profile_id": "81100275129", "affiliation": "Stanford University, Stanford, CA, USA", "person_id": "P2354158", "email_address": "", "orcid_id": ""}, {"name": "Zach DeVito", "author_profile_id": "81470655829", "affiliation": "Stanford University, Stanford, CA, USA", "person_id": "P2354159", "email_address": "", "orcid_id": ""}, {"name": "Adriaan Moors", "author_profile_id": "81381603100", "affiliation": "EPFL, Lausanne, Switzerland", "person_id": "P2354160", "email_address": "", "orcid_id": ""}, {"name": "Tiark Rompf", "author_profile_id": "81442614474", "affiliation": "EPFL, Lausanne, Switzerland", "person_id": "P2354161", "email_address": "", "orcid_id": ""}, {"name": "Arvind K. Sujeeth", "author_profile_id": "81456635415", "affiliation": "Stanford University, Stanford, CA, USA", "person_id": "P2354162", "email_address": "", "orcid_id": ""}, {"name": "Pat Hanrahan", "author_profile_id": "81100482576", "affiliation": "Stanford University, Stanford, CA, USA", "person_id": "P2354163", "email_address": "", "orcid_id": ""}, {"name": "Martin Odersky", "author_profile_id": "81100056476", "affiliation": "EPFL, Lausanne, Switzerland", "person_id": "P2354164", "email_address": "", "orcid_id": ""}, {"name": "Kunle Olukotun", "author_profile_id": "81100603292", "affiliation": "Stanford University, Stanford, CA, USA", "person_id": "P2354165", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1869459.1869527", "year": "2010", "article_id": "1869527", "conference": "OOPSLA", "title": "Language virtualization for heterogeneous parallel computing", "url": "http://dl.acm.org/citation.cfm?id=1869527"}