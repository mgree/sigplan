{"article_publication_date": "10-17-2010", "fulltext": "\n Do I Use the Wrong De.nition? DefUse: De.nition-Use Invariants for Detecting Concurrency and Sequential \nBugs . Yao Shi Soyeon Park Zuoning Yin Tsinghua University shiyao00@mails.tsinghua.edu.cn University \nof California, San Diego soyeon@cs.ucsd.edu University of Illinois at Urbana-Champaign zyin2@uiuc.edu \nShan Lu Yuanyuan Zhou Wenguang Chen University of Wisconsin-Madison University of California, San Diego \nTsinghua University shanlu@cs.wisc.edu yyzhou@cs.ucsd.edu cwg@tsinghua.edu.cn Weimin Zheng Tsinghua University \nzwm-dcs@tsinghua.edu.cn Abstract Software bugs, such as concurrency, memory and seman\u00adtic bugs, can \nsigni.cantly affect system reliability. Although much effort has been made to address this problem, there \nare still many bugs that cannot be detected, especially concur\u00adrency bugs due to the complexity of concurrent \nprograms. Effective approaches for detecting these common bugs are therefore highly desired. This paper \npresents an invariant-based bug detection tool, DefUse, which can detect not only concurrency bugs (in\u00adcluding \nthe previously under-studied order violation bugs), but also memory and semantic bugs. Based on the obser\u00advation \nthat many bugs appear as violations to program\u00admers data .ow intentions, we introduce three different \ntypes of de.nition-use invariants that commonly exist in both sequential and concurrent programs. We \nalso design an algorithm to automatically extract such invariants from programs, which are then used \nto detect bugs. Moreover, . This work is supported by IBM CAS Canada research fellowship May\u00adHappen-In-Parallel \nAnalysis for POSIX Thread and Intel-MOE joint re\u00adsearch award Program Analysis for Parallel Applications \n. Permission to make digital or hard copies of all or part of this work for personal or classroom use \nis granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA/SPLASH \n10, October 17 21, 2010, Reno/Tahoe, Nevada, USA. Copyright c . 2010 ACM 978-1-4503-0203-6/10/10. . . \n$10.00 DefUse uses various techniques to prune false positives and rank error reports. We evaluated DefUse \nusing sixteen real-world applica\u00adtions with twenty real-world concurrency and sequential bugs. Our results \nshow that DefUse can effectively detect 19 of these bugs, including 2 new bugs that were never re\u00adported \nbefore, with only a few false positives. Our training sensitivity results show that, with the bene.t \nof the prun\u00ading and ranking algorithms, DefUse is accurate even with insuf.cient training. Categories \nand Subject Descriptors D.2.4 [Software Engi\u00adneering]: Software/Program Veri.cation Reliability; D.2.5 \n[Software Engineering]: Testing and Debugging Diagnostics General Terms Reliability Keywords Concurrency \nBug, Sequential Bug, Atomicity Violation, Order Violation 1. Introduction 1.1 Motivation As software \nhas grown in size and complexity, the dif.culty of .nding and .xing bugs has increased. Inevitably, many \nbugs leak into deployed software, contributing to up to 27% of system failures [22]. Among the different \ntypes of bugs, concurrency bugs are the most notorious, due to their non-deterministic nature. Unlike \nsequential bugs, concurrency bugs depend not only on inputs and execution environments, but also on thread\u00adinterleaving \nand other timing-related events that are to be manifested [3, 18, 27, 30, 34, 40]. This makes them hard \nto be exposed and detected during in-house testing. Although concurrency bugs may appear less frequently \nthan sequential bugs, they can cause more severe consequences, such as data corruption, hanging systems, \nor even catastrophic disasters (e.g., the Northeastern Electricity Blackout Incident [35]). With the \npervasiveness of multi-core machines and concur\u00adrent programs, this problem is becoming more and more \nse\u00advere.  Recent research works have made signi.cant advances in detecting concurrency bugs, especially \nfor data races [10, 23, 34, 39] and atomicity violations 1 [9, 11, 19, 33, 42, 44], but order violation \nbugs have been neglected. An order violation occurs if a programming assumption on the order of certain \nevents is not guaranteed during the implementation [18] 2. According to a recent real-world concurrency \nbug character\u00adistic study [18], order violations account for one third of all non-deadlock concurrency \nbugs. Figure 1(a) shows an order violation bug in HTTrack. In this example, the programmer incorrectly \nassumes that S2 in Thread 1 is always executed after S3 in Thread 2 (probably due to some misunderstanding \nin the thread creation process and scheduling). Unfortunately, this order assumption is not guaranteed \nin the implementation. In some cases, S2 may be executed before S3 and results in a null-pointer parameter, \nwhich may lead to a null-pointer dereference, crashing the program later. Besides concurrency bugs, semantic \nbugs are another form of bugs that is hard to detect. To date, only a few tools have been proposed to \ndetect semantic bugs due to their ver\u00adsatility and lack of general patterns. Figure 2(a) shows such an \nexample from gzip. This bug is caused by the missing reassignment of variable ifd before S2. With certain \nin\u00adputs, ifd can incorrectly reuse the value set by S3, causing the program to misbehave. This bug is \nprogram-speci.c and cannot be detected easily. Finally, memory corruption bugs such as buffer over.ow \nand dangling pointer are also important since they can be exploited by malicious users. Many tools have \nbeen built for detecting those bugs but they each target only a special subtype of bugs. Figure 2(c) \nand 2(d) show two real-world memory corruption bugs.  1.2 Commonality among Bug Types Interestingly, \nregardless of the difference between these bugs root causes, many of them share a common charac\u00adteristic: \nwhen triggered, they usually are followed by an in\u00adcorrect data .ow, i.e., a read instruction uses the \nvalue from 1 An atomicity violation bug is a software error occurring when a set of opera\u00adtions/accesses \nthat is supposed to be atomic is not protected with appropriate syn\u00adchronizations. 2 Even though order \nviolations can be .xed in ways similar to atomicity violation bugs (i.e., by adding locks), their root \ncauses are different. Order is not essential in atomicity violation, as long as there is no intervening \nremote access breaking the assumed atomicity. an unexpected de.nition 3. We refer to such a de.nition \nas an incorrect de.nition. Figure 1 and 2 use eight real-world bugs to demonstrate such commonality. \nAlthough these bugs have different root causes, when they are manifested, they invariably result in a \nread (highlighted in each .gure) that uses a value from an incorrect de.nition. For example, in the HTTrack \nexample (Figure 1(a)), dur\u00ading correct execution, S2 always uses the de.nition from S3. However, in a \nbuggy run, S2 uses the de.nition from S1, re\u00adsulting in a null-pointer dereference later. Similarly, \nin the atomicity violation bug example from Apache (Figure 1(b)), during correct execution, S2 always \nuses the de.nition from S1. However, in a buggy run, S3 is interleaved between S1 and S2. Thus S2 uses \nthe de.nition from S3 which causes both threads to call cleanup cash obj(). This will lead to a null-pointer \ndereference inside this function. The other two bugs in Figure 1(c)(d) are also similar. Some sequential \nbugs also share a similar characteristic. For example, in the semantic bug example shown in Fig\u00adure 2(a), \nin correct runs, S2 always uses the de.nition from S1. However, when the bug manifests, S2 can read the \ndef\u00adinition from S3 from the last loop iteration. The other three bugs shown in Figure 2, including one \nsemantic bug and two memory bugs, are similar. The above commonality indicates that, if we can detect \nsuch incorrect de.nition-use data .ow, it is possible to catch these bugs, regardless of their different \nroot causes. In order to detect incorrect de.nition-use .ow, we .rst need to determine what de.nitions \na correct instruction should use. Unfortunately, it is not easy to get such infor\u00admation. Firstly, it \nis tedious to ask programmers to provide such information (if they were aware of such information consciously, \nthey would not have introduced such bugs in the .rst place). Secondly, it is also hard to extract such \nin\u00adformation by a static analysis. Though an advanced static analysis could reveal all possible de.nitions \nthat an instruc\u00adtion could use, it cannot differentiate the correct de.nitions from incorrect ones. For \nexample, in Figure 2(a), static anal\u00adysis tools would report the de.nitions from both S1 and S3 can reach \nS2, but it cannot identify S3 as an incorrect de.nition for S1. In this paper, we use a different but \ncomplementary technique to address this problem for both concurrent and sequential programs written in \nunsafe programming lan\u00adguages.  1.3 Our Contributions The technique proposed in this paper is a new \nclass of pro\u00adgram invariants called de.nition-use invariants, which can capture the inherent relationships \nbetween de.nitions and 3 When a variable, v, is on the left-hand side of an assignment statement, this \nstatement is a de.nition of v. If variable v is on the right-hand-side of a statement, v has a use at \nthis statement. A reaching de.nition for a given instruction is another instruction, the target variable \nof which reaches the given instruction without an intervening assignment [1]. For simplicity, we refer \nto a use s reaching de.nition as its de.nition.   ........................................................................................... \n ..................................................... ................................................................. \n.................................................................................................................................. \n.............. ............. ................................................................... ................................................................................................................................................ \n................... ................................................. . ............................................. \n.......................................... . ................................................................. \n................................................... . . .......................... ................. \n...................................................................... ....................................................................... \n....................... ........... ...................................................... ......................................................................... \n................. ............................................................. ................... ........................... \n .......... ......................................................... ............................ .......................................................................................... \n............................ ...................................... . . ................. .................................................................. \n.............. ....................... ............................................ ..................................................................... \n........... ................. ........................................................................ \n.............................................................................. ........................................ \n................... ......... ................. ................................................................... \n ................................................................................ ................................................................. \n....................................................................................................... \n.. Figure 1. Real-world concurrency bug examples, de.nition-use (DefUse) invariants and violations. \n............. ............. ............................... ................. .............. ................................................................ \n.................................................. ................................................................ \n. . ....................... ........... ........................................... ....................................................................... \n ................. ............................................................... ....................................... \n................... .................................  .......................... .......................... \n...................................... ......................................................................... \n. . ............. . ................................ ............................... ............. ................. \n....................................................................... ........................................................... \n....................... ........... ................................................................. \n.............................................................................. ................. ...................................................................... \n........................................................ ................... ............. uses for \nconcurrent and sequential programs. A de.nition\u00aduse invariant re.ects the programmers assumptions about \nwhich de.nition(s) a read should use. Speci.cally, we have observed the following three types of de.nition-use \ninvari\u00adants. We combine all the three types into DefUse Invariants. Local/Remote(LR) invariants. A read \nonly uses de.ni\u00adtions from either the local thread (e.g., Figure 1(b)); or a remote thread (e.g., Figure \n1(a)). LR invariants are useful in detecting order violations and read-after-write atomic\u00adity violations \nin multi-threaded programs.  Follower invariants. When there are two consecutive reads upon the same \nvariable from the same thread, the second always uses the same de.nition as the .rst. For example, in \nFigure 1(c), S2 should use the same de.ni\u00adtion of buf index as S1. Follower invariants are useful for \ndetecting read-after-read atomicity violations and cer\u00adtain memory bugs.  . . De.nition Set(DSet) invariants. \nA read should always use de.nition(s) from a certain set of writes. For example, in Figure 2(a), S1 is \nthe only correct de.nition for S2 to use. It can be useful for detecting both concurrency and sequential \nbugs but this would introduce more false positives. We have designed a tool, called DefUse that automati\u00adcally \nextracts de.nition-use invariants from programs writ\u00adten in C/C++ and uses them to detect software bugs \nof var\u00adious types. To the best of our knowledge, this is one of the .rst techniques that can be used \nto detect a large variety of software bugs, including concurrency bugs (both order vio\u00adlations and atomicity \nviolations) as well as sequential bugs. Similar as many previous invariant-based bug detection studies \n[8, 13, 19, 37, 44, 45], DefUse leverages in-house testing as training runs to extract the de.nition-use \ninvari\u00adants, which are then used to detect concurrency and sequen\u00ad  ........................................................ \n. ........................................................................ ........................................ \n............................................................................... .............. ...................................... \n..................................................................... ............................................................ \n............................................. ........................................................................................ \n................ . . .................................................................... ....................... \n.............................................................................. ....................................................... \n.......................................................... ........... .................. ............................................................................. \n....................................................................... . ................... ............. \n .................................................... .................................................................... \n......................................................................................... ........................................................................................ \n................................................................................. ............................................................................................................. \n....................................................................... ........................................... \n ........................................................................ Figure 2. Real-world sequential \nbug examples, de.nition-use (DefUse) invariants and violations. .............................................. \n.................................... ..................................................................................................... \n...................................................................................................... \n............................................................... ..................... ........................................... \n......................................... ............. . . ................. .......................................................................... \n....................................... ........................ .......... ................................................................................. \n.............................................................................. .............................................. \n.............................................. .................. .......................................................................... \n................... ........................................................... ................................. \n........................................................................................ ................................................................................. \n................................................... . ........................................ ............................................................................................ \n. .......... ............................................ ................. .................................. \n.................................................................. . ....................... ..................................................................... \n............................................ .......................................................................... \n........... .................. ................... ................................................................ \n................... .................................................................... ........................................................................................................ \n.......................................................................................... ............................................ \n.............. ................................................................................................ \n............... ................................................................................................. \n. ............. ................. ....................................... ........................................................................... \n........................ .......... ........................................................................ \n.......................................................... ............................................... \n............................................................. .................. .................................................................... \n................... ...................................................... tial bugs. To tolerate insuf.cient \ntraining, DefUse automati\u00adcally prunes out unlikely invariants and violations, and ranks remaining violations \nbased on con.dence. Our invariant ex\u00adtraction algorithm also takes possible training noises (which may \nbe incorrectly labeled training runs) into consideration. We envision our tool being used in two scenarios: \n(1) General bug detection during testing: Once the invariants are extracted, DefUse can be used to monitor \ntesting runs for possible violations. (2) Post-mortem diagnosis: When a programmer tries to diagnose \na bug, he can use DefUse to see if any DefUse invariant is violated, and which incor\u00adrect de.nition-use \ndata .ow is detected. Such information provides useful clues for narrowing down the root cause. For such \nusage, we assume a deterministic replay support like . .. . the Flight Data Recorder [41] or others [27] \nto reproduce the failure occurrence. We have evaluated DefUse with twenty representative real-world concurrency \nand sequential bugs 4, including or\u00adder violation, atomicity violation, memory corruption, and semantic \nbugs, from sixteen server and desktop applications such as Apache, MySQL, Mozilla, HTTrack, GNU Linux \ncoreutils, and so on. DefUse successfully detects 19 of them 4 Like other dynamic tools such as AVIO \n[19] or Purify [15], our bug detection tool requires the bug to be manifested during execution (either \na monitored run for post\u00admortem diagnosis or a production run). Hence, evaluation for a dynamic tool \nis usually done with mostly known bugs whether it can detect these bugs or not. This does not mean that \nit can only detect known bugs. When such a tool (e.g., the commonly-used Purify [15] or our tool) is \nused in .eld, it can also help programmers to detect unknown bugs. Furthermore, during our evaluation \nprocess, we discovered two new bugs that were never reported before.  including 2 new bugs that were \nnever reported before. Fur\u00adthermore, DefUse reports only a few (0-3) false positives for our evaluated \napplications, bene.ting from DefUse s prun\u00ading algorithm that can reduce the number of false posi\u00adtives \nby up to 87%. Our training sensitivity results show that DefUse is reasonably tolerant of insuf.cient \ntraining and achieves acceptable false positive rates after only a few (around 20) training runs.  \n2. De.nition-Use Invariants 2.1 Overview Classi.cation of Program Invariants. Invariants are pro\u00adgram \nproperties that are preserved during correct executions. They re.ect programmer intentions. Invariants \nare tradition\u00adally used for compiler optimization (e.g., loop invariants). Recent works automatically \ninfer likely invariants of certain types from training runs and use them to detect software bugs. Previously \nproposed invariants include value-range in\u00advariants [8, 13], access-set invariants [45], access interleav\u00ading \ninvariants [19] and others [6, 17]. Since few previous studies have classi.ed program invari\u00adants, we \nsummarize them into three categories as below: Characteristic-based Invariants: Sometimes, a program \nelement s state may change, but all states share a com\u00admon characteristic. Examples include variable \nx s value should be less than 5 , function f s return value should be positive , basic block b should \nbe atomic , and so on.  Relation-based Invariants: Some invariants are about the relationship among \nmultiple program elements. An example is the multi-variable invariant discussed in MUVI [17]. It captures \nthe relations between multiple variables that are always accessed together.  Value-based Invariants: \nIn some cases, certain program elements always have a .xed set of values. In this case, we use value \nin a broad sense. The value can be a variable value, or an access set, and so on. We call such invariants \nas value-based. The access set invariants discovered in AccMon [45] is such an example. It captures the \nset of instructions (program counters) that access a target memory location.  De.nition-Use Invariants. \nAs discussed in Section 1.2, de.nition-use invariants widely exist in both sequential and concurrent \nprograms, but have not been studied before. They re.ect programmers intention on data .ows and are closely \nrelated to many software bugs, such as those shown in Fig\u00adure 1 and Figure 2. From many real-world programs, \nwe have observed that common data .ow intention can be captured by the follow\u00ading three types of de.nition-use \ninvariants, with each be\u00adlonging to a category discussed above. Local/Remote (LR) is a type of characteristic-based \nin\u00advariants for concurrent programs. An LR invariant re\u00ad.ects programmers intention on an important property \nof a read instruction: should it use only de.nitions from a local (respectively remote) thread? It is \nuseful for detect\u00ading order violation bugs, and read-after-write atomicity violation bugs in concurrent \nprograms. Follower is a type of relation-based invariant that is espe\u00adcially useful for concurrent programs. \nIt re.ects the rela\u00adtion between two consecutive reads to the same mem\u00adory location from the same thread: \nshould the second read use the same de.nition as the .rst one? It captures the atomicity assumption between \ntwo consecutive reads, and can be used to detect read-after-read atomicity viola\u00adtion bugs.  De.nition \nSet (DSet) is a value-based invariant and cap\u00adtures the set of de.nitions (i.e., write instructions) \nthat a read instruction should use. Unlike LR and Follower, DSet is applicable to both concurrent and \nsequential pro\u00adgrams. It is useful for detecting concurrency bugs (atom\u00adicity violations, especially \nread-after-write atomicity vi\u00adolations, and order violations), as well as memory bugs (dangling pointers, \nbuffer over.ow, and so on) and cer\u00adtain semantic bugs which are introduced by incorrect data .ow.  The \nabove types of de.nition-use invariants capture dif\u00adferent data .ow intentions and complement each other \nwell. When used for detecting bugs, each has different trade-offs between detection power and the false \npositive rate. There\u00adfore, we combine these three types into a single de.nition\u00aduse framework, called \nDefUse. In the rest of this section, we .rst discuss each one with real-world examples, followed by the \nrationale for combin\u00ading all three of them in DefUse. 2.2 Local/Remote (LR) Invariants In concurrent \nprograms, certain reads may use only local de.nitions (maybe due to atomicity) or only remote de.ni\u00adtions \n(maybe for the purpose of communication or synchro\u00adnization). LR invariants can capture these properties. \nFormally speaking, an LR invariant, LR(r), at a read, r, equals to LOCAL (respectively REMOTE ) if r \nuses only de.nitions from the local (respectively remote) thread. We denote them as LR-Local and LR-Remote, \nrespectively. If r can read from either a local or a remote de.nition, r has no LR invariant. Figure \n3(a) illustrates the basic idea of LR invariants. Figure 1(a) and (b) demonstrate two different real-world \nexamples of LR invariants from HTTrack and Apache and their corresponding violations. As we explained \nbefore in the introduction, in the example shown in Figure 1(a), the global opt pointer at S2 in thread \n1 should always use a de.nition from a remote thread, i.e., a child thread of thread 1, not from the \nlocal thread. Otherwise, it can lead to a null\u00adpointer dereference. In other words, it has an LR-Remote \n  Thread 1 Thread 1 Thread 2 Thread 1 Thread 2 Thread 1 Thread 2 W1 W1 W1 W1  R1 R1 R1  definition \nR1 R2 set= {W1} R1 should read a va lue R1 should read a va lue defined R2 (a follower) should read \nthe R1 should read the value defined defined by a local writer W1. by a remote writer W1. same value \nas what R1 reads by a write in its definition set (a) Local/Remote (LR) (b) Follower (c) Definition Set \n(DSet) Figure 3. Examples of real-world de.nition-use invariants and their violations. invariant but \nunfortunately it is not guaranteed by the im\u00adplementation (probably due to the programmer s wrong as\u00adsumption \nabout the execution order after thread creation). Conversely, in the Apache example shown on Figure 1(b), \nS2 always needs to read the refcount value decremented by the local thread, i.e., S2 holds a LR-Local \ninvariant. Fur\u00adthermore, due to the incorrect implementation, the above as\u00adsumption is not guaranteed. \nTherefore, S2 and S1 can be in\u00adterleaved by S3, which causes S2 to read the de.nition from a remote write \nS3. This violates S2 s LR-Local invariant.  2.3 Follower Invariants In concurrent programs, programmers \nfrequently make as\u00adsumptions about the atomicity of certain code regions. The LR invariants already captures \nthe case of read-after-write data .ow relation in an assumed atomic region, but not the read-after-read \ncase, which can be captured by using a Fol\u00adlower invariant. Speci.cally, for two consecutive reads, r1 \nand r2, to the same location from the same thread, if r2 always uses the same de.nition as r1, we say \nr2 has a Follower invariant. Follower is different from LR because as long as r2 uses the same de.nition \nas r1, the de.nition can come from either local or remote. Figure 3(b) demonstrates Follower invariants. \nFigure 1(c) shows a real-world example of Follower in\u00advariants from Apache. As long as S2 reads the same \nde.ni\u00adtion as S1, wherever buf index is updated, the execution is correct, i.e., S2 holds a Follower \ninvariant. However, when such an assumption is not guaranteed, the sequence of S1 and S2 may be interrupted \nby the interleaved remote access S3, making S2 read a different value from S1. Therefore, S2 s Follower \ninvariant is violated. This is an atomicity vio\u00adlation bug. 2.4 De.nition Set (DSet) Invariants While \nconcurrent programs have special inter-thread data .ows, de.nition-use is not speci.c to only concurrent \npro\u00adgrams. Our third type of invariants, De.nition Set (DSet), is suitable for both sequential and concurrent \nprograms. A DSet invariant at a read is de.ned as the set of all writes whose de.nitions this read may \nuse. Figure 3(c) shows a DSet invariant at R1. Every read instruction has such a DSet. When it consumes \na value de.ned by an instruction outside its DSet, a DSet invariant is violated, indicating a likely \nbug. Figure 1(d) shows a real-world example of a DSet invari\u00adant in a concurrent program, Transmission. \nS3 is supposed to use h->bandwidth s value de.ned only by S2. If the thread creation and initialization \nin S2 takes a longer time than the programmer s assumption, S3 may use the de.ni\u00adtion from S1, violating \nthe DSet invariant of S1. It is of note that there is no Follower invariant here. Even though there is \nan LR-Remote invariant, the bug s manifestation condi\u00adtion does not violate LR-Remote at S3 because the \nwrong de.nition S1 also comes from a remote thread (Thread 1). Therefore, LR or Follower invariants are \nnot effective in de\u00adtecting this bug, while DSet is. Apart from concurrent programs, DSet is also applicable \nto sequential programs, as shown in Figure 2. The DSet invariant at S2 in Figure 2 (a) captures the set \nof valid de.nitions for S2 to use: {S1}. When the semantic bug is triggered, S2 would use a de.nition \nfrom S3, violating S2 s DSet invariant. Hence, this bug can be detected. The other three sequential bug \nexamples are similar.  2.5 DefUse: Combining All Three Invariants The above three types of invariants \ncapture different aspects of de.nition-use properties. Although DSet invariants ap\u00adpear more general \nthan LR and Follower, they do not nec\u00adessarily subsume LR and Follower since they do not capture inter-thread \ninformation. In a de.nition set, we do not ex\u00adpress whether this de.nition is local or remote. As a result, \nin some concurrency bug cases, DSet may not be violated, but LR or Follower invariants are, as shown \nin the examples below. Figure 1(b) shows such an example from Apache Httpd. The variable refcount de.nition \nused at S2 is always from atomic decrease() at S1. However, when the bug is trig\u00adgered, S2 can use a \nde.nition from a remote thread also ex\u00adecuting atomic decrease(). Therefore, the DSet invariant at S2 \nis not violated during such abnormal execution. How\u00adever, the LR invariant is violated. Similarly, DSet \ninvariants do not completely cover Fol\u00adlower invariants either. For example, in Figure 1(c), there are \ntwo de.nition-use chains, S3-S1 and S3-S2, which can be recorded in two DSets. However, the dependence \nrelation   Symbol Description I IU ID i iu id T (id) or T (iu) a static instruction a read instruction \n(i.e., a use) a write instruction (i.e., a de.nition) the latest dynamic instance of I the latest dynamic \ninstance of IU the latest dynamic instance of ID id s or iu s thread identi.er Figure 4. Overview of \nDefUse system. between the two reads de.nitions are not captured by DSet. In an incorrect run, if another \ninstance of S3 is interleaved between S1 and S2, no DSet invariant is violated, although the Follower \ninvariant at S2 is. In addition, since LR and Follower invariants capture general properties instead \nof concrete values, they provide fewer false positives in bug detection. It follows that their generality \nmay also lead to failure to detect the actual bugs. In particular, LR and Follower invariants are not \nsuitable for capturing sequential data .ow, whereas DSet is. Therefore, DSet can be used for detecting \nmemory and semantic bugs. Due to the above reasons, our work combines all three invariants into a single \nframework, called DefUse.  3. Invariant Extraction DefUse has two phases: (1) an extraction phase for \ninferring de.nition-use invariants; and (2) a detection phase for de\u00adtecting invariant violations and \nreporting potential bugs af\u00adter pruning and ranking. DefUse uses four components for these two phases: \nExtractor, Detector, Invariant Database, and Pruning/Ranking Engine, which are shown in Figure 4. This \nsection discusses how to automatically infer pro\u00adgrams de.nition-use invariants. It .rst describes the \ngen\u00aderal idea and then provides the extraction algorithms for LR, Follower and DSet invariants. The next \nsection will discuss violation detection, pruning and ranking. 3.1 The Main Idea De.nition-use invariant \nare hard to extract using static code analysis, especially from concurrent programs. It is also too tedious \nto require programmers to write down all invariants. In addition, programmers may not be consciously \naware of many such invariants. Otherwise, they probably would have implemented the code correctly without \nintroducing such bugs. Similar to the previous works on extracting program in\u00advariants, such as Daikon \n[8], DIDUCE [13], AVIO [19], Ac\u00adcMon [45], and so on, we also rely on correct runs from in\u00adhouse testing \nto statistically infer de.nition-use invariants. Although it is possible to combine this with static \nanalysis, it is dif.cult to use such information especially for concurrent programs written in C/C++, \nfor which it is hard to statically determine which code can be executed in parallel. Similar to the previous \nworks, we also assume that the target program Table 1. Symbols used in this paper. All instruction symbols \nare for the memory address m by default. is reasonably mature and thereby feasible for extracting in\u00advariants \nstatistically. As pointed out by the previous works [19, 44], concurrent programs provide a unique challenge \nfor generating different training runs. Different runs, even with the same input, can have different \ninterleavings. A systematic concurrency test\u00ading framework such as CHESS [25, 26] or CTrigger [30] can \nbe used to systematically explore different interleavings for each input. Insuf.cient training problem \nand training sensitivity. It is conceivable that training runs may not cover all execution paths. Therefore, \nthe extracted invariants may suffer from the problem of insuf.cient training. To address this, both our \ninvariant extraction and bug detection algorithms explore various techniques to .lter unlikely invariants \nor violations, and devalue uncon.dent invariants and bug reports (see Sec\u00adtion 4.2). Moreover, the extracted \ninvariants can be re.ned after each detection run based on the programmer s feed\u00adback on bug reports. \nFinally, in our experiments, we studied the training sensitivity by testing with the inputs that are \nvery different from that used in the training (see Section 5.5).  3.2 Extraction Algorithms In this \nsection, we describe in detail the algorithms to extract de.nition-use invariants. Due to page limitations, \nwe present only the algorithms for concurrent programs. The extraction for DSet invariants works for \nsequential programs as well. For simplicity, we use the symbols explained in Table 1. DSet invariant \nextraction. Our DefUse extraction algo\u00adrithm obtains DSet by collecting all de.nitions that are used \nby IU during training runs. In our implementation, DefUse uses the Pin [21] binary instrumentation framework \nto in\u00adstrument accesses to every monitored memory location. For each memory location, DefUse stores its \nmost recent write instruction in a global hash-table, called De.nition-Table. At a read iu, DefUse gets \nits de.nition id from the De.nition-Table and uses id s corresponding static instruction ID to update \nIU s DSet: DSet(IU ) . DSet(IU ) .{ID } (1) After the training phase, information for every instruc\u00adtions \nDSet is stored into the invariant database along with some statistical information such as the number \nof times IU and ID are executed, and so on, for the purpose of pruning and ranking.  LR invariant extraction. \nIn order to infer LR invariants, DefUse .rstly needs to know which thread provides the de.nition for \na read. This can be easily obtained through the De.nition-Table. Speci.cally, when iu is executed, DefUse \n.nds its de.nition id from the De.nition-Table. DefUse then compares T (iu) and T (id) to determine whether \nthey are from the same thread or not. Finally, it compares the answer with the LR(IU ) associated with \nIU . If they are different, LR(IU ) is set to NO INV to indicate that there is no LR invariant at IU \nand this read is no longer monitored for LR invariant extraction. LR(IU ) is initialized based on the \nde.nition source (either REMOTE or LOCAL) on the .rst time IU is executed. This process can be formalized \nas follows: . LOCAL if LR(IU )= LOCAL . . . . . T (id)= T (iu) LR(IU ) . REMOT E if LR(IU )= REMOT E \n. . . . T (id) <> T (iu) . NO INV Otherwise (2) Follower invariant extraction. In order to infer Follower \ninvariants, DefUse needs to store its recent access history to determine whether an instruction and its \npredecessor use the same de.nition. To achieve this, DefUse maintains a bit\u00advector for every memory location \nm, called has read(m). A bit in the vector has read(m,t) indicates whether the current de.nition to memory \nlocation m has already been used by thread t. By checking this bit-vector before every read instruction \niu, DefUse can easily determine whether iu and its predecessor use the same de.nition. Speci.cally, bit-vector \nhas read(m) is initialized as zero. Every write to m sets all bits of has read(m) to zero. After any \nread from m in thread t, has read(m, t) is set to one. In order to know whether an instruction iu uses \nthe same de.nition as its predecessor, before executing iu, DefUse checks if the corresponding bit (i.e., \nhas read(m, t)) is one. If it is, it means that there is no new de.nition since thread T (iu) s last \nuse to m. In other words, iu shares the same de.nition with its predecessor. To maintain and update the \nFollower invariant informa\u00adtion for IU , DefUse associates F ollower(IU ) with it. This .ag is set to \nTRUE if IU s dynamic instances always share de.nition with their predecessors. Whenever a dynamic in\u00adstance \nof IU uses a different de.nition from its predecessor, the .ag is set to FALSE and IU is no longer monitored \nfor Follower invariant extraction. F ollower(IU ) . F ollower(IU ) . has read(m, T (iu)) (3)  3.3 Design \nand Implementation Issues Our implementation is based on Pin [21], a dynamic binary instrumentation framework. \nIn the following paragraphs, we discuss several detailed implementation issues. Monitored memory locations. \nThe invariant extraction algorithms can be applied to any memory unit (byte, word, and so on) and any \nmemory region (stack, heap, global data region, and so on). We use byte granularity in our imple\u00admentation \nfor the greatest accuracy. Currently, our DefUse prototype monitors heap and global data region. Stack \nis ig\u00adnored because variables stored on stack seldom involve in concurrency bugs [18]. Nevertheless, \nsome sequential bugs may not be detected because of this choice. We will extend DefUse in the future \nto cover these stack related sequential bugs. External de.nitions. Programs may use external libraries \nand system calls. We term the de.nitions from those external codes as external de.nitions. The absence \nof these external de.nitions could result in some de.nition-use invariants not being detected. We use \ntwo approaches to solve this problem. Firstly, we annotate some system calls and widely used library \nfunc\u00adtions, such as memset and memcpy which can create de.\u00adnitions. We regard these functions as black \nboxes and their call-sites as de.nitions. Of course, it is impractical to annotate all external func\u00adtions \nand system calls. Therefore, our second approach is to introduce . =(I. ,T.) to represent unannotated \nexter\u00adnal de.nitions. When DefUse meets a use and does not .nd any preceding de.nition to the same memory \nlocation, the symbol I. is introduced to represent the virtual external def\u00adinition. It is of note that \nsince we instrument the code at binary level, we can always detect every user-level de.nition and use, \nregardless of whether or not it is from libraries. How\u00adever, annotation may improve context-sensitivity \ncompared to directly processing the de.nitions within them. Virtual address recycle. We also need to \nconsider vir\u00adtual address recycling caused by memory allocation and de\u00adallocation. In other words, some \ninstructions may access the same memory location simply due to memory recycling rather than for some \ninherent data .ow relation. To address this problem, we intercept every de-allocation function, and delete \nall the previous information for accesses to the future\u00addeallocated memory region. Context-sensitivity. \nSmall un-inlined functions may have many call-sites. As a result, a use in such a function may have many \nde.nitions from different call-sites, which can lead to inaccurate invariants. This issue can be addressed \nin two ways. The .rst is to inline this small function, and treat each call-site s use as different reads. \nIn other words, we can make it context-sensitive. A simpler approach is to just prune such use from the \ninvariant database since it has too many de.nitions.  Training noise. Although existing in-house testing \noracles are relatively accurate in labeling incorrect runs from cor\u00adrect ones in most cases, we also \nsuffer from the problem of training noise (i.e., a buggy run is incorrectly labeled as cor\u00adrect), as \nexperienced with all previous works in invariant\u00adbased bug detection [8, 13, 19, 45]. To handle this \nprob\u00adlem, our extraction algorithm can be extended by relaxing the 100% support constraint. For example, \nif an invariant is supported by 95% of the training runs, this invariant is not pruned and is still kept \nto detect potential violations dur\u00ading monitored runs. Intuitively, violations to such invariants should \nbe ranked lower than those for invariants with 100% support from training runs.  4. Bug detection 4.1 \nDetection Algorithms DefUse detects bugs by checking violations against the ex\u00adtracted DSet, LR, and \nFollower invariants. At every read instruction, if the monitored properties do not match the associated \ninvariants, DefUse reports such violation to its Rank/Pruning Engine for further analysis. The detection \nalgorithms are similar to the extraction algorithms discussed in Section 3. In the below section, we \nbrie.y describe the basic detection algorithms against each type of de.nition-use invariant violations. \nWe will discuss how to prune and rank bug reports in Section 4.2. To detect DSet invariant violation, \nDefUse maintains a De.nition-Table at runtime so that it knows which instruc\u00adtion provides a de.nition \nfor each use. If the de.nition for a use is not in this use s DSet, a violation is issued. Formally speaking, \nthe violation condition is: ID ./DSet(IU ) (4) Detecting violations against LR invariants is also straight\u00adforward. \nAt a monitored read, DefUse .rst checks whether this read has an LR invariant or not. If it does, DefUse \nex\u00adamines whether the monitored read and its de.nition come from the same thread and matches the monitored \ncondition with the type of LR invariant (LOCAL and REMOTE) extracted at this read. If there is a violation, \nit will be reported to the Pruning/Ranking Engine: {LR(IU )= LOCAL . T (id) <> T (iu)}. (5) {LR(IU )= \nREMOT E . T (id)= T (iu)} The basic idea of detecting violations to Follower invari\u00adants is to check \nwhether an instruction with a Follower in\u00advariant shares the same de.nition with its predecessor (by \nleveraging the has read(m) vector similar to that used in extraction). If not, a violation will be reported \nfor pruning and ranking: F ollower(IU ) . has read(m, T (iu)) (6)  4.2 Pruning and Ranking For training-based \napproaches, it is usually hard to know whether the training is suf.cient. Without ranking and prun\u00ading, \nbug detection using such approaches may generate a sig\u00adni.cant number of false positives due to insuf.cient \ntraining. Pruning. DefUse automatically prunes the following cases in bug reports: Barely exercised \nuses: For reads that are never covered during training, we do not report any violations since we do not \nextract any invariants associated with them. It is also useful to extend this strategy to those uses \nthat we have only seen once or twice during training runs and therefore have little con.dence of in the \nextracted invariants.  Barely exercised de.nitions: Similarly, for de.nitions that are never exercised \nduring training runs, if we en\u00adcounter them during detection, we are not con.dent whether they are violations. \nIn this case, we also prune them from the bug reports.  Popular uses: Some uses, such as those in a \nsmall func\u00adtion called from multiple call-sites, are very popular and have a large de.nition set. In \nthis case, we also prune it as it might be perfectly acceptable to have yet another de.nition for this \nuse during detection runs.  Ranking. After pruning the above cases, DefUse ranks ev\u00adery unpruned violations \nbased on its con.dence. In this way, users can start with the top-ranked bug reports and gradu\u00adally move \ndown the list. In addition, DefUse also provides an optional pruning phase, called con.dence-based pruning, \nwhich prunes the violations whose con.dence is lower than a certain threshold. By default, we do not \nuse such an option for our experiments unless explicitly mentioned. Intuitively, the following conditions \nincrease the con.\u00addence of a true bug:  many dynamic instances of the de.nition (#ID ) and the use (#IU \n) during training;  no signi.cant difference between the number of instances of the de.nition and instances \nof the use (|#ID - #IU |) during training;  small de.nition set (|DSet(IU )|);  few instances of \nthis violation pair (#violationDSet(ID ,IU )) during detection.  For a DSet invariant violation, if \nthe invariant is supported by many instances of de.nitions and uses during training, any violation to \nit has a high probability of being a bug. Meanwhile, if the size of DSet(IU ) is large, or the same violation \nis detected many times, the violation is unlikely to be a bug. As such, the con.dence is computed as \nfollows.  confDSet = #ID \u00d7#IU (7) (|#ID -#IU |+1)\u00d7|DSet(IU )|\u00d7#violationDSet (ID ,IU ) For LR and Follower \nviolations, the con.dence is com\u00adputed based only on the number of dynamic instances of a use during \ntraining and the number of violations occurring during detection, as follows: confLR =#IU /#violationLR(IU \n) (8) confF =#IU /#violationF (IU ) (9) Violations to multiple invariants. In some cases, a bug may violate \nmultiple invariants (e.g., both the DSet and LR invariants). In these case, DefUse uses a geometric mean \nof the con.dence of all violations as DefUse s con.dence. As an alternative approach, we can also weigh \neach invariant differently. For example, violations to LR and Follower in\u00advariants may have higher con.dence \nvalues, since they are coarse-grained invariants and hence can gain more statisti\u00adcal support from training \nruns.  5. Experimental Evaluation 5.1 Test Platform, Applications and Bugs We implemented DefUse using \nIntel s dynamic instrumenta\u00adtion tool, Pin [21], for invariant extraction and bug detection. All experiments \nare conducted on an 8-core Intel Xeon ma\u00adchine (2.33GHz, 4GB of memory) running Linux 2.6.9. We used \na total of sixteen representative real-world ap\u00adplications (shown in Table 2), including 8 multi-threaded \napplications and 8 sequential applications. Concurrent ap\u00adplications include 2 widely used servers (Apache \nHttpd and MySQL), 6 desktop/client applications (Mozilla JavaScript engine, HTTrack, Transmission, PBZip2, \nand so on). We used the original applications from their of.cial websites and did not modify any of them \nfor our evaluation. We evaluated DefUse with 20 real-world software bugs (including 11 concurrency bugs \nand 9 sequential bugs) of various root causes, including atomicity violations, order violations, memory \nbugs and semantic bugs.  5.2 Training Process In our experiments, we tried our best to emulate how pro\u00adgrammers \nwould use DefUse in practice, especially for in\u00advariant extraction from training runs. The following \nis our training setup. Like all previous invariant-based works, we leverage the common available testing \noracle from in-house regression testing to label correct testing runs for training. Invariants are extracted \nsystematically. Speci.cally, we used multiple different inputs for training, which are cru\u00adcial for sequential \napplications. All inputs used for training are from standard test cases released together with the cor\u00adresponding \nsoftware. In other words, training inputs are se\u00adlected in a way that is oblivious to the tested bug. \nIn some Bug Type Atomicity Violation Bugs Order Violation Bugs Semantics Unbounded memory read Type \nApplications LOC Descriptions Server Apache 345K Web server Con-App. MySQL 1.9M Database server current \nMozilla 3.4M Web browser suite App. HTTrack 54.8K Website copier Desktop Transmission 86.4K BitTorrent \nclient App. PBZip2 2.0K Parallel .le compressor x264 29.7K H.264 codec ZSNES 37.3K Nintendo game emulator \nUtility gzip 14.6K Compression tool tar 41.2K File archiving tool seq 1.7K Print number sequence paste \n1.4K Merge lines of .les Sequen-GNU sort 4.3K Sort lines in .les tial Linux ptx 5.7K Index of .le contents \nApp. coreutils cut 3.3K Remove sections from .les pr 6.3K Convert .les for printing Table 2. Evaluated \napplications.  ID Bug description Apache#1 Random crash in cache management Apache#2 Non-deterministic \nLog-.le corruption Apache#3 Random crash in worker-queue accesses Mozilla Wrong results of JavaScript \nexecution MySQL#1 Non-deterministic DB log disorder MySQL#2 Read after free object HTTrack Transmi -ssion \nPBZip2 x264 ZSNES gzip seq tar#1 paste sort Read before object creation A shared variable is read before \nit is properly assigned by a child thread Random crash in .le decompression Read after .le closed Inquiry \nlock before initialization Use wrong .le descriptor Read wrong string terminator Read out of buffer Read \nout of buffer Read unopened .le Dangling pointer Buffer over.ow Buffer over.ow Overwrite wrong buffer \n.elds Table 3. Evaluated real-world bugs. experiments (for studying training sensitivity), we even man\u00adually \nexamined the training inputs to ensure that they are dif\u00adferent from the input used for detection (as \nexplained in the next subsection). For concurrent applications, in each input, training also needs to \ncover various interleavings [25, 26, 36]. There\u00adfore, we executed the tested program with each input \nfor 10 times 5 on our 8-core machine. It is conceivable to use more advanced interleaving testing tools \n[25, 26, 30, 36] to im\u00adprove DefUse training process to cover more interleavings. 5 In the case of Apache \nserver, we de.ne one run as the duration to service for 100 different client requests, and for HTTrack, \nweb crawling time for one run is set to .ve minutes. Dangling Pointer cut Memory ptx corruption pr tar#2 \n   5.3 Two Sets of Experiments DefUse is designed for two different usage scenarios (post\u00admortem diagnosis \nand general bug detection during testing) as mentioned in the introduction. During post-mortem diag\u00adnosis, \ndevelopers usually know which input triggers the bug so that they can easily design a good training input \nset for DefUse. For general bug detection, programmers can rely only on in-house testing suites, and \nso it is possible that some monitored runs may have a very different input. In such cases, it would be \ndesirable for a bug detector to not report many false positives. Considering the above two different \nusage scenarios, we design two sets of experiments to thoroughly evaluate DefUse. 1. DefUse bug detection \nwith suf.cient training inputs. In this set of experiments, DefUse s training input set in\u00adcludes all \nsuitable inputs provided by the software devel\u00adopers or those generated by representative tools(e.g., \nfor MySQL, we used the mysql test suite; for Apache, we used Surge [2] and httperf [24]). For some applications \n(e.g., tar, sort), we generated many different inputs. The results with this setting are shown in Table \n4 and Table 5. 2. DefUse bug detection with insuf.cient training inputs. We use two experiments to show \nhow DefUse would perform with insuf.cient trainings: how the number of false positives changes with inputs \ndifferent from those used in training (Figure 5); and the amount of training runs/inputs needed to achieve \na good detection accuracy (Figure 6).  In all our experiments, con.dence is only used for rank\u00ading, \nbut not for pruning, unless speci.cally mentioned (Fig\u00adure 5(b) and Figure 6(b)). 5.4 Overall Results \nTable 4 shows DefUse s detection capability and the number of false positives, as well as the bug rankings \nin DefUse s violation report. Bug detection. As shown on Table 4, DefUse can detect 19 of the 20 tested \nbugs in the 16 applications. It is of note that these bugs are of various types including atomicity violations, \norder violations, semantic bugs, buffer over.ows, dangling pointers, unbound reads, and so on. Section \n5.6 explains why DefUse misses the bug of tar#2. The results also show that the three types of de.nition-use \ninvariants (LR, Follower and DSet) are very complementary. DSet can detect 7 of the 11 concurrency bugs \nand all the tested sequential bugs, but misses 4 concurrency bugs which can be detected by LR or Follower. \nFor example, Apache#1 and Mozilla are detected only by using LR invariants, and Apache#2 is detected \nonly by using Follower. This justi.es the bene.t of combining all three invariants in DefUse. DefUse \ncan also detect bugs that would be missed by pre\u00advious tools. One is an order violation bug in HTTrack \nand the other two are semantic bugs in gzip and seq. Speci.cally, the recently proposed invariant-based \nbug detector, PSet [44], cannot detect the HTTrack bug, since their detection is not strictly based on \nde.nition-use relations. Figure 8 in Sec\u00adtion 6 shows the simpli.ed codes of HTTrack, and compares DefUse \nwith PSet s behavior. It can be noted that DefUse can detect the bug with both DSet and LR invariants. \nA more detailed comparison with PSet with a bug example is in Sec\u00adtion 6. The evaluated two semantic \nbugs are caused only by a se\u00admantically wrong de.nition-use pair and not by accessing an invalid memory \nlocation. Hence, they would not be detected by previous memory checkers such as Valgrind [28]. False \npositives. In Table 4, for all the applications, the number of false positives is fewer than four. Moreover, \nal\u00admost all bugs ranked top in the violation report, which helps programmers easily identify bugs. The \nlow false positive numbers are a result of our pruning scheme described in Section 4.2 as well as suf.cient \ntrain\u00ading (the results of insuf.cient training are shown later). Ta\u00adble 5 demonstrates the effectiveness \nof our basic pruning algorithm (please note that no con.dence-based pruning is used here). Overall, the \nnumber of false positives is reduced by up to 87% by pruning. Comparing LR and Follower with DSet, the \n.rst two have much fewer false positives because they are coarse-grained (i.e., they look at only high-level \nde.nition-use properties instead of concrete de.nitions) and have more statistical supports from training \nruns.  5.5 Training Sensitivity As discussed in Section 5.3, in some scenarios and especially if used \nfor general bug detection instead of post-mortem analysis, it is possible that training is not suf.cient. \nThat is, the training does not have a good coverage of execution paths or inputs. Our second set of experiments \naim to answer the following related questions: What if detection runs are dramatically different fromtraining \nruns? To show DefUse s training sensitivity, we .rst analyze the effect of training inputs as shown in \nFig\u00adure 5. For detection, we use various inputs that are different from training inputs. To quantitatively \nmeasure their differ\u00adences, we use the number of basic blocks newly explored in a detection run (but \nnot in any training run). Without any con.dence-based pruning, as shown on Fig\u00adure 5(a)(c), when a detection \nrun is very different from train\u00ading runs (e.g., for MySQL, with 835 number of untrained ba\u00adsic blocks), \nDefUse can introduce a few (e.g., 58 in MySQL) false positives. Fortunately, this problem can be addressed \nby applying con.dence-based pruning, i.e., pruning out those with low con.dence. In Figure 5(b)(d), after \ncon.dence\u00adbased pruning, even with 700 untrained basic blocks in a detection run, the number of false \npositives is always below 30. However, con.dence-based pruning may also increase  Bug Type Applications \nBug Detected? Apache#1 Atomicity Apache#2 Violation Apache#3 Bugs Mozilla MySQL#1 MySQL#2 HTTrack * Order \nTransmission * Violation PBZip2 Bugs x264 ZSNES Semantics gzip seq Unbounded tar#1 memory paste read \nsort Dangling Pointer cut Memory ptx corruption pr tar#2  0 no 0 no 1 Yes (2) 1 no 2 Yes(1) 0 no 3 Yes(1) \n3 0 Yes(1) 0 no 0 Yes(1) 0 0 no 0 no 3 Yes (1) 3 no 0 no 1 Yes(1) 0 Yes(2) 1 no 0 no 1 Yes(1) 2 Yes(1) \n3 0 no 0 Yes(1) 0 Yes(1) 0 no 0 no 0 Yes(1) 0 Yes(1) 0 no no 0 0 Yes(1) no 0 0 Yes(1) Yes(1) 0 0 Yes(1) \nYes(1) 0 0 0 no 0 Yes(1) 0 Yes(1) 0 Yes(1) Yes(1) Yes (1) Yes(1) Yes(1) - - - - - - - - -LR Follower \n False Bug False Positives Detected? Positives Concurrent Applications Sequential Applications - - - \n- - - - - -- - - - - - - - -- - - - - - - - -DSet Bug Detected Yes(1) Yes(1) Yes(1) Yes(1) Yes(2) Yes(1) \nYes(1) Yes(1) no False Positives 0 0 0 0 1 1 0 0 0 DefUse Bug False Detected? Positives Yes(1) 0 Yes(1) \n0 Yes(1) 0 Yes(1) 0 Yes(2) 1 Yes(1) 1 Yes(1) 0 Yes(1) 0 no 0  Table 4. Overall results from DefUse with \ninvariants extracted from suf.cient training runs described in Section 5.3. The number inside each parenthesis \nis the bug s ranking in the violation report of DefUse. The bugs marked * are new bugs that were never \nreported before. . .. . . . . . . . .. . ....................... ... . .. . .. . .. . . . . . . . . . \n. . . . . . . . . . . . . . . . .. . . . . . . . .. . ....................... ... . .. . .. . .. . . \n. . . . . . . . . . . . . . . . . . . . . Threshold = 5 . . . . . . . . . . .. . . . . ....................... \n. . . .. . . . . .. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .. . . . . . . . .. . .. \n. ....................... .. . .. . .. ... ... . . . . . . . . . . . . . . . . . . . . . . . . Threshold \n= 5 (a) MySQL (b) MySQL (c) HTTrack (d) HTTrack before confidence-based pruning after confidence-based \npruning before confidence-based pruning after confidence-based pruning Figure 5. Training sensitivity \nin terms of input difference. Each dot is one detection run. The detection runs use various inputs that \ndiffer from those used in training runs by various degrees. For each detection, its difference from the \ntraining runs is re.ected by the number of basic blocks that are not covered by the training runs. The \nx-axis is the number of such basic blocks. The y-axis is the number of false positives occurred in each \ndetection run. the chance of pruning out some true bugs, even though this was not demonstrated in our \nexperiments. How many training runs/inputs are suf.cient? Figure 6 shows how the number of training inputs/runs \naffects the number of false positives. Without con.dence-based prun\u00ading (shown on Figure 6(a)), after \nDefUse training with 13 inputs, the number of false positives is gradually reduced, and .nally reaches \nto 1. Con.dence-based pruning (shown on Figure 6(b)) makes the false positive curves drop faster. In \ncomparison, LR and Follower s false positives are al\u00adways fewer than DSet. DefUse has fewer false positives \nthan DSet with con.dence pruning, because its con.dence is cal\u00adculated using a geometric mean of the \ncon.dences from all three types of invariants.  5.6 Bug Missed by DefUse Even though DefUse can detect \nvarious types of concur\u00adrency and sequential bugs, it still misses one tested bug (tar#2), which is described \nin Figure 7. If a .le name (i.e., read s name) is longer than posix header.name, S1 cor\u00adrupts the following \n.elds, e.g., header.mode, and dumps the wrong data to fd. Note that the wrong write S1 and its use S3 \ndo not violate the DSet invariant held by S3, since this de.nition-use relation has already been captured \nfrom header.name during training. Interestingly, other existing memory checkers (i.e., Valgrind [28]) \ncannot detect it as well, since S1 always writes to valid location, even when this bug is manifested. \n    Application LR Follower DSet DefUse BP AP BP AP BP AP BP AP Apache#1 0 0 0 0 2 1 2 1 Apache#2 \n2 2 0 0 3 3 3 3 Apache#3 0 0 0 0 0 0 0 0 Mozilla 0 0 1 0 4 3 5 3 MySQL#1 0 0 1 1 4 0 5 1 MySQL#2 0 0 \n1 1 22 2 23 3 HTTrack 0 0 0 0 1 0 1 0 Transmission 0 0 0 0 0 0 0 0 PBZip2 0 0 0 0 0 0 0 0 x264 0 0 0 \n0 1 0 1 0 ZSNES 0 0 0 0 0 0 0 0       Table 5. Comparison on the number of false positives before \npruning (BP) and after pruning (AP). AP prunes barely exer\u00adcised uses and de.nitions, and popular uses, \nas described in Section 4.2. The results show the worst case from .ve detection runs. The evaluation \nsetup is the same as Table 4.  Figure 6. False positives with increasing number of training runs. The \nplain-line curve shows the numbers of untrained basic blocks in the detection runs. To address this problem, \nDefUse needs to be extended to consider other types of de.nition-use invariants such as whether a de.nition \nis used at least once before it is re-de.ned. In Figure 7, when the bug is manifested, header.typeflag \nis re-de.ned by S2 without any uses after S1, which can be detected by the extension. Like previous invariant-based \napproaches, DefUse would miss bugs where there is no statistical support to extract any invariants, and \nbugs that do not violate any invariants.  5.7 Overhead of DefUse In Table 6, the slowdown of DefUse \nis less than 5X in most cases and up to 20X for memory intensive concurrent appli\u00adcations such as application \nx264 . Compared to some other  Figure 7. A memory corruption bug in GNU tar that cannot be detected \nby DefUse. Mozilla MySQL Transmission PBZip2 x264 gzip 3.26X 5.53X 1.03X 3.39X 20.26X 4.12X  seq tar \npaste sort cut ptx 1.87X 2.18X 2.71X 2.45X 2.39X 2.41X Table 6. Overhead of DefUse. software only concurrency \nbug detection tools such as Val\u00adgrind (which includes a lockset data race detector and incurs 10X-100X \noverhead) [28], and the AVIO software imple\u00admentation (which has 15X-40X slowdowns) [19], DefUse is much \nfaster. Even though DefUse overhead may be high for some applications, it is not critically important \nbecause we expect Defuse to be used during post-mortem diagnosis and in-house testing/debugging, similar \nto Valgrind.  6. Related work Program invariants. Our work was inspired by many pre\u00advious works on \nprogram invariants including Daikon [8], DIDUCE [13], AccMon [45], AVIO [19], DITTO [37], MUVI [17], \nand so on. Similar to these studies, we also extract invariants from training runs, and dynamically detect \nviolations to report potential bugs. Unlike these works, our invariants focus on data .ow. More speci.cally, \nthe focus is on de.nition-use invariants, which can be used to detect both concurrency and sequential \nbugs. Few previous methods can detect both types. Furthermore, our tool is also one of the .rst that \ncan detect order violations in concurrent programs. De.nition-use and data .ow. A recent study [4] used \ndata .ow to detect security vulnerabilities, i.e., buffer over.ow attacks. It was based on the insight \nthat if an intended data .ow is subverted with a unexpected use, there may lead to an attack. Our work \ndiffers from this as we determine whether a use reads from a wrong de.nition by extracting de.nition-use \ninvariants from both concurrent and sequen\u00adtial programs, and report potential concurrency (including \natomicity and order violations) and sequential bugs. De.nition-use relations have also been used for \nsoftware testing. Many works [14, 16, 43] have focused on testing de.nition-use relations to increase \ntest coverage. Data .ow has many other usages. For example, EIO [12] uses data .ow analysis to see whether \nlow-level error codes (e.g., I/O error ) are correctly propagated to .le systems so       thread \n1 thread 2 thread 1 thread 2 thread 1 thread 2 I0: write I0: write I0: write I2: read I2: read I2: read \n I3: read I2: read I2: read I2: read I1: write Correct run 1 Correct run 2 Incorrect run PSet[I0]={ }; \nPSet[I2] = {I1} DSet[I2] = {I0, I1} PSet[I1]={I0,I2}; PSet[I3] = {I1} DSet[I3] = {I1} Figure 8. Simpli.ed \ncode of HTTrack showing the difference between PSet and DSet. In the incorrect run (the execution order \nof I1 and I3 does not match with the assumption), I3 obtains the de.nition from I0, violating DSet(I3). \nthat recovery mechanisms can handle them. In [5], data .ow analysis is used to detect memory leak and \ndouble free. Concurrency bug detection and avoidance. There have been a lot of works on detecting data \nraces [10, 23, 39]. The happens-before [31], lockset algorithms [7, 34, 38], and many of the extensions \n[29, 32] have been proposed for data races detection. However, there are two problems for data race detectors: \n(1) many data races in real-world applications are benign such that race detectors may have too many \nfalse alarms; and (2) some wrong interleavings and orders are not related to data races. Therefore, concurrency \nbugs caused by atomicity violations and order violations are introduced [18]. Atomicity violations have \nbeen well studied [9, 11, 19, 33, 42, 44], but order violations are, so far, rarely studied. Much research \nhas been conducted on detecting or avoid\u00ading concurrency bugs. Recently, [44] proposed an innovative \nmethod to avoid concurrency bugs by using data dependency information collected during correct runs. \nTheir approach encodes the set of tested correct interleavings in a program s binary executable, and \nenforces only those interleavings at runtime. They use Predecessor Set(PSet) constraints to cap\u00adture \nthe tested interleavings between two dependent memory instructions. Speci.cally, for each shared memory \ninstruc\u00adtion, PSet speci.es the set of all valid remote memory in\u00adstructions that it can be immediately \ndependent upon 6. Their work is quite different from DefUse in the follow\u00ading aspects. (1) Their work \nfocuses on runtime fault toler\u00adance and recovery, while DefUse is tailored to bug detec\u00adtion and post-mortem \nfailure diagnosis. (2) PSet does not di\u00adrectly target de.nition-use relations even though it re.ects \nsome aspect of them. Speci.cally PSet does not capture such de.nition-use relations as it cares about \nonly remote, preceding instructions. Figure 8 shows the simpli.ed code from HTTrack, where PSet does \nnot capture the de.nition\u00aduse relation on I3 and I1, and misses the order violation bug. DefUse (with \nDSet or LR invariants) can detect it. (3) Our work also includes LR and Follower invariants and thereby \ncan check other properties of concurrent execution. (4) DefUse can also detect sequential bugs and work \nfor se\u00adquential programs.  Bugaboo [20] uses context-aware communication graphs for concurrency bug \ndetection. However, it is limited by the context size due to the exponential cost. If the granularity \nof the bug manifestation is larger than the context size, 5 by default, the detection may be inaccurate. \nFor example, in Figure 1(a), if the interval between S1 and S3 in execution is larger than the context \nsize, it may miss the bug or may report the false positive. In addition, Bugaboo needs the spe\u00adcial hardware \nextension because of the expensive software implementation even for the default context size. Our tool, \nDefUse, can detect not only atomicity viola\u00adtions, but also order violations that have been seldom stud\u00adied \nin previous research. Using the same framework, we can also detect memory bugs and semantic bugs.  7. \nConclusion This paper proposed de.nition-use invariants which can be used to detect a wide spectrum of \nsoftware bugs, including both concurrency bugs (atomicity and order violations) and sequential bugs (memory \ncorruptions and certain semantic bugs). Our experimental results with 16 real-world applica\u00adtions and \n20 real-world bugs of different types have shown that DefUse was effective in detecting 19 of them, including \n2 new bugs that were never reported before, while introduc\u00ading only 0-3 false positives. Our training \nsensitivity exper\u00adiments showed that DefUse can reasonably tolerate insuf.\u00adcient training, especially \nwith con.dence-based pruning. There are several possible directions for the future work. Firstly, as \nshown in our experiments, DefUse still fails to detect one tested bug (Section 5.6). To handle such bugs, \nwe need to extend DefUse invariants to include constraints. Sec\u00adondly, it is interesting to consider \nother types of de.nition\u00aduse invariants. Thirdly, DefUse is currently implemented en\u00adtirely in software, \nbut it is conceivable that some simple hardware extensions can signi.cantly reduce DefUse s run\u00adtime \noverhead and make it possible to use DefUse to monitor production runs.  References [1] A. V. Aho, R. \nSethi, and J. D. Ullman. Compilers: Principles, Techniques, and Tools. Addison Wesley, 1986. [2] P. Barford \nand M. Crovella. Generating representative web workloads for network and server performance evaluation. \nIn ACM SIGMETRICS, June 1998. [3] M. Burrows and K. R. M. Leino. Finding stale-value errors in concurrent \nprograms. Concurrency and Computation: 6 The PSet of a static memory instruction M includes another \ninstruction P only if Practice &#38; Experience, 16(12):1161 1172, 2004. (i) P and M are in two different \nthreads; (ii) M is immediately dependent on P; (iii) [4] M. Castro, M. Costa, and T. Harris. Securing \nsoftware by neither of the two threads executes a read or a write (to the same memory location) that \ninterleaved between P and M [44]. enforcing data-.ow integrity. In OSDI, 2006.  [5] S. Cherem, L. Princehouse, \nand R. Rugina. Practical memory leak detection using guarded value-.ow analysis. In PLDI, 2007. [6] T. \nChilimbi and V. Ganapathy. HeapMD: Identifying heap\u00adbased bugs using anomaly detection. In ASPLOS, 2006. \n[7] J.-D. Choi, K. Lee, A. Loginov, R. O Callahan, V. Sarkar, and M. Sridharan. Ef.cient and precise \ndatarace detection for multithreaded object-oriented programs. In PLDI, 2002. [8] M. D. Ernst, J. H. \nPerkins, P. J. Guo, S. McCamant, C. Pacheco, M. S. Tschantz, and C. Xiao. The Daikon system for dynamic \ndetection of likely invariants. Science of Computer Programming, 69(1 3):35 45, Dec. 2007. [9] C. Flanagan \nand S. N. Freund. Atomizer: a dynamic atomicity checker for multithreaded programs. In POPL, 2004. [10] \nC. Flanagan and S. N. Freund. FastTrack: ef.cient and precise dynamic race detection. In PLDI, 2009. \n[11] C. Flanagan and S. Qadeer. A type and effect system for atomicity. In PLDI, pages 338 349, 2003. \n[12] H. S. Gunawi, C. Rubio-Gonzaiz, A. C. Arpaci-Dusseau, R. H. Arpaci-Dusseau, and B. Liblit. EIO: \nError handling is occasionally correct. In FAST, 2008. [13] S. Hangal and M. S. Lam. Tracking down software \nbugs using automatic anomaly detection. In ICSE, 2002. [14] M. J. Harrold and B. A. Malloy. Data .ow \ntesting of parallelized code. In ICSM, 1992. [15] R. Hastings and B. Joyce. Purify: Fast detection of \nmemory leaks and access errors. In Usenix Winter Technical Confer\u00adence, 1992. [16] S. Lu, W. Jiang, and \nY. Zhou. A study of interleaving coverage criteria. In FSE, 2007. [17] S. Lu, S. Park, C. Hu, X. Ma, \nW. Jiang, Z. Li, R. A. Popa, and Y. Zhou. MUVI: Automatically inferring multi\u00advariable access correlations \nand detecting related semantic and concurrency bugs. In SOSP, 2007. [18] S. Lu, S. Park, E. Seo, and \nY. Zhou. Learning from mistakes a comprehensive study of real world concurrency bug characteristics. \nIn ASPLOS, 2008. [19] S. Lu, J. Tucek, F. Qin, and Y. Zhou. AVIO: Detecting atomicity violations via \naccess interleaving invariants. In ASPLOS, 2006. [20] B. Lucia and L. Ceze. Finding concurrency bugs \nwith context\u00adaware communication graphs. In MICRO, 2009. [21] C.-K. Luk, R. Cohn, R. Muth, H. Patil, \nA. Klauser, G. Lowney, S. Wallace, V. J. Reddi, and K. Hazelwood. Pin: building customized program analysis \ntools with dynamic instrumen\u00adtation. In PLDI, 2005. [22] E. Marcus and H. Stern. Blueprints for high \navailability (2nd edition). John Wiley and Sons, 2003. [23] D. Marino, M. Musuvathi, and S. Narayanasamy. \nLiteRace: effective sampling for lightweight data-race detection. In PLDI, 2009. [24] D. Mosberger and \nT. Jin. httperf a tool for measuring web server performance. Performance Evaluation Review, 26(3): 31 \n37, 1998. [25] M. Musuvathi and S. Qadeer. Iterative context bounding for systematic testing of multithreaded \nprograms. In PLDI, 2007. [26] M. Musuvathi, S. Qadeer, T. Ball, and G. Basler. Finding and reproducing \nheisenbugs in concurrent programs. In OSDI, 2008. [27] S. Narayanasamy, C. Pereira, and B. Calder. Recording \nshared memory dependencies using strata. In ASPLOS, 2006. [28] N. Nethercote and J. Seward. Valgrind: \nA framework for heavyweight dynamic binary instrumentation. In PLDI, 2007. [29] R. O Callahan and J.-D. \nChoi. Hybrid dynamic data race detection. In PPoPP, 2003. [30] S. Park, S. Lu, and Y. Zhou. CTrigger: \nExposing atomicity violation bugs from their hiding places. In ASPLOS, 2009. [31] D. Perkovic and P. \nJ. Keleher. Online data-race detection via coherency guarantees. In OSDI, 1996. [32] E. Pozniansky and \nA. Schuster. Ef.cient on-the-.y data race detection in multithreaded C++ programs. In PPoPP, 2003. [33] \nA. Sasturkar, R. Agarwal, L. Wang, and S. D. Stoller. Automated type-based analysis of data races and \natomicity. In PPoPP, pages 83 94, 2005. [34] S. Savage, M. Burrows, G. Nelson, P. Sobalvarro, and T. \nAnderson. Eraser: A dynamic data race detector for multithreaded programs. ACM TOCS, 1997. [35] SecurityFocus. \nSoftware bug contributed to blackout. http://www.securityfocus.com/news/8016. [36] K. Sen. Race directed \nrandom testing of concurrent programs. In PLDI, 2008. [37] A. Shankar and R. Bodik. DITTO: Automatic \nincremental\u00adization of data structure invariant checks (in Java). In PLDI, 2007. [38] C. von Praun and \nT. R. Gross. Object race detection. In OOPSLA, 2001. [39] C. von Praun and T. R. Gross. Static con.ict \nanalysis for multi-threaded object oriented programs. In PLDI, 2003. [40] M. Xu, R. Bodik, and M. Hill. \nA regulated transitive reduction for longer memory race recording. In ASPLOS, 2006. [41] M. Xu, R. Bodik, \nand M. D. Hill. A .ight data recorder for enabling full-system multiprocessor deterministic replay. In \nISCA, 2003. [42] M. Xu, R. Bodik, and M. D. Hill. A serializability violation detector for shared-memory \nserver programs. In PLDI, pages 1 14, 2005. [43] C.-S. D. Yang, A. L. Souter, and L. L. Pollock. All-du-path \ncoverage for parallel programs. In ISSTA, 1998. [44] J. Yu and S. Narayanasamy. A case for an interleaving \nconstrained shared-memory multi-processor. In ISCA, 2009. [45] P. Zhou, W. Liu, F. Long, S. Lu, F. Qin, \nY. Zhou, S. Midkiff, and J. Torrellas. AccMon: Automatically Detecting Memory-Related Bugs via Program \nCounter-based Invariants. In MICRO, 2004.   \n\t\t\t", "proc_id": "1869459", "abstract": "<p>Software bugs, such as concurrency, memory and semantic bugs, can significantly affect system reliability. Although much effort has been made to address this problem, there are still many bugs that cannot be detected, especially concurrency bugs due to the complexity of concurrent programs. Effective approaches for detecting these common bugs are therefore highly desired.</p> <p>This paper presents an invariant-based bug detection tool, DefUse, which can detect not only concurrency bugs (including the previously under-studied order violation bugs), but also memory and semantic bugs. Based on the observation that many bugs appear as violations to programmers' data flow intentions, we introduce three different types of definition-use invariants that commonly exist in both sequential and concurrent programs. We also design an algorithm to automatically extract such invariants from programs, which are then used to detect bugs. Moreover, DefUse uses various techniques to prune false positives and rank error reports.</p> <p>We evaluated DefUse using <i>sixteen</i> real-world applications with twenty real-world concurrency and sequential bugs. Our results show that DefUse can effectively detect 19 of these bugs, including 2 new bugs that were never reported before, with only a few false positives. Our training sensitivity results show that, with the benefit of the pruning and ranking algorithms, DefUse is accurate even with insufficient training.</p>", "authors": [{"name": "Yao Shi", "author_profile_id": "81470646757", "affiliation": "Tsinghua University, Beijing, China", "person_id": "P2354030", "email_address": "", "orcid_id": ""}, {"name": "Soyeon Park", "author_profile_id": "81331501572", "affiliation": "University of California, San Diego, San Diego, CA, USA", "person_id": "P2354031", "email_address": "", "orcid_id": ""}, {"name": "Zuoning Yin", "author_profile_id": "81455605430", "affiliation": "University of Illinois at Urbana-Champaign, Urbana, IL, USA", "person_id": "P2354032", "email_address": "", "orcid_id": ""}, {"name": "Shan Lu", "author_profile_id": "81100052818", "affiliation": "University of Wisconsin-Madison, Madison, WI, USA", "person_id": "P2354033", "email_address": "", "orcid_id": ""}, {"name": "Yuanyuan Zhou", "author_profile_id": "81451594551", "affiliation": "University of California, San Diego, San Diego, CA, USA", "person_id": "P2354034", "email_address": "", "orcid_id": ""}, {"name": "Wenguang Chen", "author_profile_id": "81100108264", "affiliation": "Tsinghua University, Beijing, China", "person_id": "P2354035", "email_address": "", "orcid_id": ""}, {"name": "Weimin Zheng", "author_profile_id": "81326494227", "affiliation": "Tsinghua University, Beijing, China", "person_id": "P2354036", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1869459.1869474", "year": "2010", "article_id": "1869474", "conference": "OOPSLA", "title": "Do I use the wrong definition?: DeFuse: definition-use invariants for detecting concurrency and sequential bugs", "url": "http://dl.acm.org/citation.cfm?id=1869474"}