{"article_publication_date": "10-17-2010", "fulltext": "\n G-Finder: Routing Programming Questions Closer to the Experts * Wei Li 1,2,3 Charles Zhang 2 Songlin \nHu 1 1 Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China 2 Department of \nComputer Science and Engineering, Hong Kong University of Science and Technology, Hong Kong, China 3 \nGraduate University of Chinese Academy of Sciences, Beijing, China liwei01@ict.ac.cn charlesz@cse.ust.hk \nhusonglin@ict.ac.cn Abstract Programming forums are becoming the primary tools for programmers to .nd \nanswers for their programming prob\u00adlems. Our empirical study of popular programming forums shows that \nthe forum users experience long waiting period for answers and a small number of experts are often over\u00adloaded \nwith questions. To improve the usage experience, we have designed and implemented G-Finder, both an algo\u00adrithm \nand a tool that makes intelligent routing decisions as to which participant is the expert for answering \na particular programming question. Our main approach is to leverage the source code information of the \nsoftware systems that forums are dedicated to, and discover latent relationships between forums users. \nOur algorithms construct the concept networks and the user networks from the program source and the fo\u00adrum \ndata. We use programming questions to dynamically in\u00adtegrate these two networks and present an adaptive \nranking of the potential experts. Our evaluation of G-Finder, us\u00ading the data from three large programming \nforums, takes a retrospective view to check if G-Finder can correctly pre\u00addict the experts who provided \nanswers to programming ques\u00adtions. The evaluation results show that G-Finder improves the prediction \nprecision by 25% to 74%, compared to related approaches. Categories and Subject Descriptors D.2.6 [Software \nEn\u00adgineering]: Programming Environments General Terms Design, Human Factors * In programming communities, \nexperts are synonymous to both Gurus and Geeks. Permission to make digital or hard copies of all or part \nof this work for personal or classroom use is granted without fee provided that copies are not made or \ndistributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. OOPSLA/SPLASH 10, October 17 21, 2010, Reno/Tahoe, Nevada, USA. \nCopyright c . 2010 ACM 978-1-4503-0203-6/10/10. . . $10.00 Keywords Expert Search, Social Networks, Programming \nForums 1. Introduction Programming forums are becoming a major means for pro\u00adgrammers to get help for \ntheir programming tasks, especially when open source software packages developed by the com\u00admunity effort \nare involved. The basic unit of programming forums is a thread, consisting of a chain of messages, start\u00ading \nwith a question message from a user (questioner) and a number of reply messages from other users, as \nillustrated in Figure 1. If a message m1 is to reply another message m2, m2 can be considered as the \nparent of m1. A message has one parent at most in a thread. Programming forums are being actively used. \nThe Java Develop Forum1, an online system from Sun, contains 87 sub-forums that focus on various topics \nconcerning Java pro\u00adgramming [17]. Just one of these sub-forums, Java Program\u00adming2 (referred to as the \nJava Forum hereon), contains about 77000 threads or 490000 messages. In this forum, there are over 200 \nparticipants online and 200 messages posted per day. The Eclipse Forum3, a platform for users to discuss \nprogramming problems using the Eclipse platform, contains about 80 sub-forums, and some of these sub-forums \ncontain more than 20000 threads. All these forums have thousands of users registered, and they interact \nwith each other by post\u00ading messages and waiting for replies from other users. Despite the popularity \nof programming forums, the fo\u00adrum participants often experience a few common problems. First, the time \nspan between raising the initial question and getting the satisfactory answer is often hard to predict \nand longer than expected. Through our investigation of the Java Forum, we found that the time taken for \na question to be 1 Java Develop Forum. URL: http://forums.sun.com/index.jspa 2 Java Programming Forum. \nURL: http://forums.sun.com/forum. jspa?forumID=31 3 Eclipse Forum. URL: http://www.eclipse.org/forums/index. \n php?t=index&#38;cat=1&#38;  Figure 1: An Example of Thread It contains seven messages and four users. \nThe arrows show the post-reply relation answered can range from less than an hour to 3 or 4 days or \neven longer. A question lingering for several days will undoubtedly slow down the programming productivity. \nSec\u00adond, experts in forums are overloaded with questions, which affects the quality and timeliness of \nthe answers. We ran\u00addomly sampled about 500 questions and 600 participants from the Java Forum. We found \nthat only one third of partic\u00adipants give replies, and 14% of these participants have given 61% of the \nreplies. All of the observations we made above call for an intelligent information processing tool that \ncan make good decisions with respect to who is the right person to answer a programming question. The \nproblem of expert searching has been extensively ex\u00adplored in the information retrieval research. We \nclassify the previous work using the simple criteria of whether they use the content of the messages \nor the post-reply relationships between participants or both types of knowledge in the anal\u00adysis. The \nlanguage model proposed in [2][3] computes the probabilities of the term appearance in plain texts of \nthe par\u00adticipants. Another line of work [6][17] computes the global rankings of the forum participants \nthrough the construction of the participant graphs and the use of well-known algo\u00adrithms such as the \nPageRank [5] and HITS [7]. The con\u00adstructed participant graphs encode the post-reply relation\u00adships of \nparticipants in the message threads. The content of the messages is, however, ignored. A recent method \n[18] combines the language model and the post-reply structure to improve the precision and recall of \nthe expert searching in forums. It uses term probabilities to pro.le the forum par\u00adticipants. These pro.les \nare combined with a global ranking approach computing from the post-reply graphs to produce the .nal \nranking of experts. This combination is reported to have improved both the precision and the recall of \nthe expert recommendation in forum systems. Compared to the general forums that the related ap\u00adproaches \nare concerned with, programming forums (PFs) share a certain degree of similarity and, at the same time, \nexhibit some important differences. One difference is that the messages in PFs contain not only texts \nbut also, in most cases, fragments of code. It is problematic for the term probability calculation because \nit treats every word non-discriminatively. For instance, a forum question ask\u00ading about how to use I/O \noperations in Java may present a piece of code fragment containing the Java I/O class: BufferedReader. \nThe word, BufferedReader, despite having the same appearance weight as many other words in the message, \nis more important than others in repre\u00adsenting the nature of the question and much more help\u00adful in .nding \nthe right expert. Another difference is re\u00adgarding the topics of the threads. In previous approaches \n[17] [18], different threads are considered representing independent topics. But for PFs, many threads \nhave im\u00adplicit relationships among each other. For example, ex\u00adperts participating a discussion thread \ninvolving the type, java.io.BufferedInputStream, are very likely capable of contributing to another thread \ninvolving java.io.Filt\u00aderedInputStream, a super type of BufferedInputStream. Considering them as independent \nthreads may lead to impre\u00adcision in the identi.cation of experts. Leveraging these observations, we have \ndesigned, imple\u00admented, and evaluated G-Finder, both an algorithm and a tool that uses the semantic information \nof forum threads, in addition to the post-reply relationships, to locate experts in programming forums. \nWe encode the relationships of con\u00adcepts and participants as graphs or networks . Meanwhile, we associate \nforum threads and querying questions with a set of Java class types, we hereon referred to as concepts, \nusing a few effective heuristics based on the observed pat\u00adterns. We then construct the concept networks \nby extracting the relationships of concepts from either the source code or the bytecode of the system \nthat the forum is dedicated to. We also build the user networks based on the post-reply re\u00adlationships. \nHowever, unlike the related work, our user net\u00adwork is not static but computed on demand according to \nthe concept-mappings of the queries. Therefore, our prediction is quite adaptive to the semantics of \nthe queries and, hence, more accurate. The evaluation of our algorithm is based on the real fo\u00adrum data \ncrawled from three popular online programming forums. From these forums, we .rst extract predictable \ndata by selecting questions that are answered by participants who also answered multiple questions. We \nthen randomly divide the data into two sets, where the .rst set is used to construct both the concept \nand the user networks, and the second to test if G-Finder can correctly identify the experts who also \nanswered the questions in the .rst set. We sampled about three to four thousand threads from two general \nJava programming forums and one speci.c programming forum from Eclipse, and evaluated the precision of \nour algorithm. Our evaluation shows that, compared to the previous work, G-Finder improves the prediction \naccuracy by 25% to 74%.  The main contributions of our paper are: 1. We present an empirical study of \nthe post-reply relation\u00adships on three popular programming forums to further motivate the need for the \nmore accurate and balanced se\u00adlection of experts, for answering questions in online fo\u00adrums. 2. We present \nthe design and the implementation of an algo\u00adrithm that can be used by programming forums to route programming \nquestions closer to the experts. This algo\u00adrithm uses the semantic information extracted from the source \ncode as the main insight for improving the predic\u00adtion accuracy. 3. We evaluate the prediction quality \nof our approaches using the data from active programming forums. We also quantify our improvement with \nrespect to the state of the art by implementing and comparing to the other related expert searching algorithms. \n  The rest of the paper is organized as follows: Section 2 shows the related research on expert searching; \nSection 3 presents some analysis that motivates our design principles. Section 4 shows our algorithm \nof expert searching on pro\u00adgramming forums. Section 5 describes the implementation of our model. Section \n6 shows the evaluation of our algo\u00adrithms and discusses some observations on programming fo\u00adrums. Section \n7 gives the conclusion and future work. 2. Related Work The research of expert searching is originally \nbased on the scienti.c and enterprise data. The research in [9] studied a topic model that matches papers \nwith reviewers, which uses the method of Latent Dirichlet Allocation (LDA) [4] to ob\u00adtain the distribution \nof documents over topics and the distri\u00adbution of topics over the vocabulary of topics. Our method is \ndifferent in following aspects. First, the topic model de\u00adscribes topics by the most probable words in \nthe paper cor\u00adpus, whereas, in PFs, the concepts are described by words concerning with the source code. \nSecond, LDA is not suit\u00adable for PFs, as PFs have explicit word vocabularies of con\u00adcepts. Third, the \npost-reply structures do not exist in the pa\u00adper corpus. The signi.cant research effort4 is devoted to \nex\u00adpert searching based on the enterprise data that re.ects the experiences of users in real organizations, \nsuch as intranet pages, email archives, and document repositories, to identify experts. For this type \nof data, language models are the dom\u00adinating techniques [2][3][11], which calculate the probabil\u00adities \nof term generation of the users through the documents associated with the users. The probability that \na user will be an expert on a question is calculated through computing the probability that the user \ngenerates the set of terms in the question. Language models have sound foundations in the\u00ad 4 Enterprise \nTrack of TREC. URL: http://trec.nist.gov/data/ enterprise.html ory [12][16] and perform well on the document \ncorpus. Our algorithm is different from those that use language model in two aspects. First, our algorithm \nanalyzes both the message content and the code structures of the target software system, hence, uses \nmore information to improve the precision of the prediction. Second, the document corpus have no post-reply \nstructures. Regarding the expert search using forum data, Zhang et al [17] analyzed the characteristics \nof post-reply structures of the Java Forum, and compared some ranking methods in\u00adcluding ranking using \nthe PageRank values and the HITS au\u00adthority values. Another method proposed by Jurczyk et al [6] constructs \nthe ask-reply structures in Question/Answering portals, such as Yahoo!Answers5, and uses link analysis \nmethods, such as HITS, to .nd the users with high author\u00adity values or users answering a large proportion \nof questions. These methods ignore the content of the posts and rank users globally. Consequently, questions \nwill always be routed to top ranked users, which exacerbates the overloading prob\u00adlem. In addition, global \nranks also do not recognize the se\u00admantic differences of the questions and, consequently, result in low \nprecision in identifying experts. Zhou et al [18] proposed three models to route ques\u00adtions to proper \nusers, including the pro.le-based model, the thread-based model, and the cluster-based model. These methods \nbuild language models with three different policies, which are integrated with a global ranking of users \nusing the authority values, computed by the HITS-based algorithm on the post-reply graph. Their algorithms \nmade signi.cant im\u00adprovement on precision and recall on general forums. Com\u00adpared to this approach, we \nleverage the source code informa\u00adtion of the target software system of the forum. There is also signi.cant \nresearch effort in the expertise recommendation in the software engineering area [10][1]. Mockus et al \nused the experience atoms [10] to identify the expertise. John Anvik et al [1] proposed methods on deciding \nwho should .x bugs based on text classi.cation methods. G-Finder aims to search experts in PFs, which \nis complimentary to these approaches. 3. An Empirical Study of Program Forums To further motivate our \nresearch, we have conducted an em\u00adpirical study on the post-reply structures of online program\u00adming forums. \nA special web crawler is implemented to crawl three popular forums, including the Java Forum, hosted \nby Sun6 for developers to ask general JDK related questions, the Java DevShed Forum7, another Java forum \nfor general Java programming problems with about 22000 threads, and the 5 Yahoo Answer. URL: http://answers.yahoo.com/ \n6 Java Forum. URL: http://forums.sun.com/forum.jspa?forumID= 31 7 Java DevShed Forum. URL: http://forums.devshed.com/ \n java-help-9/  GEF Forum8, where the Eclipse9 users search for help with the programming tasks using \nGEF. The size of the crawled data is about 2GB, consisting of 23000 threads from the Java Forum, 21000 \nthreads from the Java DevShed Forum, and 7000 threads from the GEF Forum. As summarized in Ta\u00adble 2, \nwe analyzed about 4000 threads from two Java forums and 3000 threads from the GEF Forum. We also collect \nthe number of users as well as the number of distinctive con\u00adcepts as the measures of the discussion \ndiversity. In Table 2, we report the number of active users for each PF. The active users are de.ned \nas those giving more than 10 replies in our data set. We formulate our analysis of the crawled forum \ndata as the following observations: Observation 1: The life span of discussions is long. Table 1 shows \nthe distribution of the life span of the threads on the three investigated forums, where the life span \nis de.ned as the time period from the .rst message to the last message. It shows that about 43% of the \nthreads last for more than 4 hours and about 21% of them last for more than one day on average. A signi.cant \nnumber of discussions in the forums last for a long time, while the programmers want to solve their problems \nas soon as possible. Forum 0 -4h 4 -24h 24 -48h 48h - Java Forum 52.4% 23.1% 10.3% 14.2% Java Devshed \n53% 20.3% 17% 9.7% GEF 65.7% 20% 4.1% 10.2% Table 1: Overview of PFs Observation 2: Most of the questions \nare answered by a small number of active users. Table 2 reports the number of concepts, the number of \nusers, and the number of active users in the three forums. It shows that the active users only constitute \nabout 13% of all of the users. In Figure 2, we show the distribution of the number of users with respect \nto the number of replies given by a single user. Each point in the graph represents the distribution \nof the total number of users who provided more than the number of replies represented by the X-axis. \nFor all of the three PFs, we observed that the measured overall activeness of users in providing answers \nand participants in discussions is distributed very unevenly, as most of the users provide a small number \nof replies, and the active users carry the majority of the workload. This type of imbalance detriments \nthe usage experience of these forums. We believe that this power-law distribution is not a measure of \nthe willingness of users to provide answers. Routing questions to proper users can not only lower the \nloads of active users, but also increase the participation of otherwise less active users, with the help \nof some incentive 8 GEF is an open source framework for GUI programming. URL: http: //www.eclipse.org/forums/index.php?t=thread&#38;frm_id=81&#38; \n9 Eclipse Platform. URL: http://www.eclipse.org/ mechanisms, such as the money rewards in the Amazon \ns Mechanical Turk Scheme10. Forum Thread Num Concepts Users Active User Java Forum 4000 814 3439 502 \nJava Devshed 4000 753 1549 186 GEF 3000 43 1373 110 Table 2: Overview of PFs Figure 2: User/Replies \nCount Distribution Observation 3: Experts answer questions in semantic clusters. We found that experts \nwho answer multiple ques\u00adtions tend to answer questions that are related to each other. We analyze the \nrelationships of concepts that participants give replies to on the Java Forum. It is not surprising that \nthe replies of some users involve more than one class types. In fact, about 68% of the users participate \nin the questions related to at least 2 class types. What is more interesting is that about 75% of these \nparticipants give replies on the class types that have relationships. For example, in Table 3, a user \ncalled jverd in the Java Forum replies to ques\u00adtions on HashMap and Map, where the former is a subclass \nof the latter. JComboBox and JTable of the user PhHein are inherited from the same class type. The fact \nthat one class type inherits from another, or two class types inherit from a third class type, which \nwe refer to as the type hierarchy relationship, is common in the set of classes that each user answers. \nThe relationships that span more than one level of inheritance is rare in our observation. Another implicit \nrela\u00adtionship among the class types is the call graph relationship. The fact that experts answer questions \nin semantic clusters gives us the inspiration to construct a relation network of the class types discussed \nby the threads, which is used to improve the quality of expert recommendation. 10 Amazon s Mechanical \nTurk. URL: https://www.mturk.com/mturk/ welcome  Table 3: Distribution on Concepts of Users UserId \nConcepts DarrylBurke JList,JFrame,JButton,JScrollBar JoachimSauer String,Integer,List,ArrayList, Collection, \nHashSet, Collections jverd String,Integer,Map,Short, Boolean, ArrayList, HashMap DeltaGeek Array,Iterator,ArrayList, \nCollection PhHein JComboBox,JTable DrClap String,JTextField,List,Iterator, ArrayList From the observation \n1 and 2 above, we believe that for the programming forums, an expert searching tool is use\u00adful in shortening \nthe time span for the questions to be an\u00adswered. Meanwhile, this tool should be capable of locating experts \nadaptively with respect to the semantics of the ques\u00adtions to release the load of experts in general. \nObservation 3 shows that some users tend to be local experts , mean\u00ading that users tend to only answer \nquestions on related con\u00adcepts. For a speci.c concept, .nding out potential experts on related concepts \nmay help to retrieve more experts due to the clustering phenomenon. Driven by this objective, we have \ndeveloped an adaptive expert searching algorithm that simultaneously considers message semantics and \nthe social networks of users. We explain our algorithm in detail next. 4. Algorithm The goal of the expert \nrecommendation is to identify a list of participants who are knowledgeable about a given question. In \nG-Finder, this is achieved in the following steps, shown in Figure 3. First, we crawl down the pages \nof threads from the online forums. Second, we extract the concepts that each thread discusses. The concepts \nof a thread represent the main semantic content of the thread. Third, we build the concept networks to \npresent the relationships among concepts, and the user networks to describe the post-reply structures \nof users. To search experts, we map the queries onto concepts and return a ranked list of users as recommended \nexperts for each query. Our algorithm is underpinned by a probabilistic model that calculates the probability \nof a user being an expert on a speci.c question. Given a question(or a thread), the proba\u00adbility of a \nparticipant being an expert on this question can be represented as the conditional probability P (user|question). \nP (user|question) can be calculated by multiplying the probability of user being an expert of a particular \nconcept, P (user|concept), with the probability of question belong\u00ading to the concept, P (concept|question), \nsumming over all the concepts: (1) P (concept|question) is calculated using some heuristic methods, \nand P (user|concept) is computed by constructing the user network on each concept, and integrating the \nuser network with concept network. 4.1 Map Threads to Concepts Concepts are the semantic foci of a thread. \nIn PFs, many threads contain the class type information, and we believe that these class types, either \nreferred to in the titles as well as the texts of messages, or embedded in the source code, can be considered \nas the main semantic narrative of the thread. We de.ne the concept universe of a program forum as all \nthe class types in the source code of the target software system. For example, for a Java forum, the \nconcepts are referred to as the class names in JDK packages, and, in GEF, the concepts are the classes \nin the source code of the GEF packages. We use a set of heuristics to calculate which of the con\u00adcepts \nin the concept universe are relevant to a particular thread. The probability of a question belonging \nto a con\u00adcept, P (concept|question), is computed using the follow\u00ading heuristics: 1. Title: If the title \nof a thread contains n concepts, we de.ne: P (concept|question)=1/n (2) Usually, the title of a thread \nis a short description of the question. The class types appearing in the title are strong indicators \nof what the question is concerned with.  Figure 4: Map from Thread to Concept Equation 2 states that \na thread is represented by all the concepts in the title with equal probabilities. 2. Source Code: If \nthe thread contains fragments of source code, we use a method similar to the calculation of the tf/idf \n[13] value. The tf/idf value shows the strength of the relevance of a term with respect to a document. \nTf is the frequency of a term appearing in this docu\u00adment, and idf is the inverse of the frequency a \nterm appears in all the documents. The tf/idf value of a term in a document is proportional to the relevance \nbe\u00adtween the term and the document. In our context, we just consider the terms involving concepts and \ncalcu\u00adlate the tf/idf values for each concept. We de.ne tf as the frequency of a concept appearing in \na thread among all the concepts in the thread and idf the in\u00adverse of frequency that a concept appears \nin all threads. Let vconcept question be the tf/idf value of a concept in a thread, which indicates the \nrelevance between the concept and the thread. If vconcept question is higher, the thread is more likely \nconcerned with the concept. We de.ne P (concept|question) as: vconcept question P (concept|question)= \nvconcept question .concept (3) 3. Content: In some threads, concepts are mentioned not in the source \ncode but in the plain text of the messages. In this case, we consider the probability of the thread belonging \nto the concept, P (concept|question), will be also 1/n, where n is the number of concepts appearing in \nthe content of the thread. If a thread satis.es more than two criteria above, we just use one heuristic \nfollowing the priority order of title, source code, and content. Figure 4 shows a simple example, in \nwhich four threads on the left have concepts in their titles, and are mapped to corresponding concepts. \nThe values of P (concept|question) are presented above the arrows in Figure 4. This example will be used \nthrough out this section to illustrate the complete calculation. 4.2 Probabilities of Participants on \nConcepts The probability P (user|concept) is calculated through two steps. We .rst establish the user \nnetwork for each concept and compute the probability of a particular user being an expert on the concept, \nPsep(user|concept), based on the user network of the concept. In this step, the relationships of concepts \nare not involved. In the second step, we take the relationships among concepts, represented by concept \nnetwork, into consideration to compute P (user|concept) of a user, based on the semantic clustering phenomenon \nof concepts. Experiments show that this step can signi.cantly improve the quality of expert recommendation. \n 4.2.1 User network In forums, the post-reply structures of messages can be viewed as probabilistic voting \nnetworks for experts. For ex\u00adample, if user A replies to user B, user B is likely to vote user A as an \nexpert. If multiple users reply, user B is equally likely to vote any of the repliers as an expert. If \nthe user A replies multiple times, this will increase the probability of being voted as an expert. We \nextract this post-reply in\u00adformation from each thread and construct the user network G(V, E) as follows. \nV is a set of nodes representing partic\u00adipants. A directed edge e from A to B signi.es that user B has \nprovided replies to user A. Each edge has a weight label, wAB, which is the count of the number of replies. \nAfter constructing the user network for each thread, we apply a consolidation step across all networks \nas follows. We merge the common nodes, i.e., the node representing the same participants, of the networks, \nand connect all the edges to the new node. If the edges are common, we do not add new edges and instead, \nsimply update the weight of the common edge to be the sum of the weights of all the merged edges. This \nmerging process is depicted in Figure 5, where Figure 5(a) shows the thread-speci.c user networks in \nthe example of Figure 4 and Figure 5(b) shows the merged network. Figure 5: User Network The concepts \nand the users in ellipses are merged. The numbers above the arrows are the counts of replies.  Based \non the merged user network, we can compute the ranking of expert participants based on the voting scheme. \nAssume that the expertise value of a user u on a particular concept is Psep(u|concept). The probability \nof u voting v as an expert is Puv, the fraction of the edge weight over the sum of weights of all the \noutgoing edges of u. Therefore, the probability of participant v being an expert on the same concept \nis Puv * Psep(u|concept). Considering all the con\u00adnected participants in the network, the complete calculation \nof Psep(v|concept) is as follows: Psep(v|concept)= .(Puv * Psep(u|concept)) (4) u Let EV be the vector \nrecording the likelihood probability for all users being an expert on a particular concept, and M be \nthe voting probabilities matrix between all pairs of participants, Equation 4 encodes the voting calculations \nof the user network, which is a Markov process. EV = MT * EV (5) With some standard manipulations, we \ncan use the PageR\u00adank algorithm [5] to compute Psep(u|concept) for each user. The column of Psep(u|concept) \nin Table 4 shows the prob\u00adability values of users for the example in Figure 5. Table 4: User Probabilities \nUsers:( u1: luck2000@gmx.at u2: JoachimSauer u3 : ejp u4 : thomas.behr u5: kazenofairy u6: JosAH u7: \ngogo u8: pm kirkham u9 : en.end u10: sabre150 u11: kimos2 u12: pieblok), Concepts:(c1: FileInputStream \nc2: InputStream) (u5|c2) = 0.050 (u12|c2) = 0.064 In the previous work [17], the post-reply structure \nis con\u00adstructed globally and used to compute PageRank or author\u00adity values as users prior probabilities \nof being experts. On the contrast, our user network is built separately for each main concept of the \nqueries. As a result, our user network is adaptive to queries, as the concept mappings of the different \nqueries will vary. In the next step, we consider the relation\u00adships among the concepts and integrate \nthe user networks adaptively according to the speci.c queries. 4.2.2 Concept Network As we mentioned, \nparticipants tend to answer questions of related concepts. We consider two types of relationships in \nthis paper and use them to construct the concept network. Psep(u|concept) Psep(u1|c1) = 0.069 Psep(u6|c2) \n= 0.072 Psep(u2|c1) = 0.088 Psep(u7|c2) = 0.088 Psep(u3|c1) = 0.088 Psep(u8|c2) = 0.074 Psep(u4|c1) = \n0.088 Psep(u9|c2) = 0.051 Psep(u3|c2) = 0.087 Psep(u10|c2) = 0.064 Psep(u4|c2) = 0.064 Psep(u11|c2) = \n0.064 Psep Psep 1. Type Hierarchy: The .rst type of relationship is the type hierarchy. We consider \ntwo classes are related by the type hierarchy if one inherits another or both inherit the same super \ntype. However, the transitivity of type hierarchy is not considered, as in Observation 3, we .nd the \ntransitiv\u00adity of type hierarchy rarely observed in related concepts. Given a class type A, the type hierarchy \nrelation we con\u00adsider is just the super classes, the children and the siblings of A in type hierarchy \ntree. 2. Call Graph: The second type of relationship we consider is the call graph relation. If a class \nA calls some methods of a class B in the source code, these two classes have the call graph relationship. \nWe do not consider the entire call graph of the whole concept universe, only the concepts involved in \nthe threads, generated by the code analysis tools11.  After de.ning the relationships between concepts, \nwe fur\u00adther de.ne the weights of these relationships, to show the likelihood probability of a participant \nbeing an expert on both the related concepts. The weight of type hierarchy rela\u00adtionship, wh(a, b, u), \nand the weight of call graph relation\u00adship, wc(a, b, u), are de.ned as the likelihood probabilities of \nthe user u being an expert on concept B, when the user is also an expert on concept A. These likelihood \nprobabilities are determined by the speci.c data sets of forums. For exam\u00adple, if a user replies to three \nconcepts that have the type hier\u00adarchy relationships with concept A, and there are totally ten concepts \nthat have type hierarchy relationships with A, we consider the probability of this user to answer the \nconcepts related to A to be 3/10. We formally de.ne the weights as follows: | conceptsu reply to | wh(a, \nb, u)= (6)| conceptstypeHierarchy-with-A | And wc(a, b, u) can also be calculated similarly, with the \nrelation type being the call graph relation: | conceptsu reply to | wc(a, b, u)= (7) | conceptscallGraph-with-A \n| With the weights of these two relationships, the total weight between concept A and B of the user u \nis the sum of these two weights when the relationships exist. De.ne hab =1 if type hierarchy relation \nexists and hab =0 else from A to B, and de.ne cab =1 if call graph relation exists and cab =0 else from \nA to B, the weight w(a, b, u) from A to B of u is: w(a, b, u)= wh(a, b, u) * hab + wc(a, b, u) * cab \n(8) The Concept Network is de.ned as a graph of concepts: CN = {V, E, P }, while the vertex set V represents \nthe set 11 In this research, we use the tool Wala. URL: http://wala. sourceforge.net/wiki/index.php/Main_Page \n 1. b Figure 6: Concept Network  4.2.3 Integration of User Network and Concept Network The concept \nnetwork de.nes that a participant u, an expert on concept A with the probability Psep(u|concepta), is \nalso likely an expert on a related concept B with the likelihood probability of P (a, b, u). So it can \nbe considered that the expertise probability of the participant on concept B, as a result of being an \nexpert on concept A, is P (a, b, u) \u00d7 Psep(u|concepta). Taking the relationships of concepts into consideration, \nwe calculate the probability of a participant being an expert on a speci.c concept A, P (user|concepta), \nby accumulat\u00ading all the Psep(u|concept) of u on related concepts of A, shown in concept networks. As \nthe user tends to answer a related concept B with the likelihood probability, P (b, a, u), we determine \nP (u|concepta) in following Equation: P (u|concepta)= .P (b, a, u) \u00d7 Psep(u|conceptb) (10) b When a = \nb, we consider P (a, a, u)=1. The probabil\u00adity P (u|concepta) can be viewed as combining the proba\u00adbilities, \nPsep(u|conceptb), of the same user being an expert on all of the concepts associated with A with the \nstrength, P (b, a, u), of the associations.  4.3 Query Processing We describe how we leverage both \nconcept networks and user networks to process queries. A query q is usually a post with source code. \nThe query is .rst mapped to a set of concepts as described in Section 4.1, in which P (concept|q) is \ncalculated. The probability of user being an expert on concept is calculated by Equation 10. With Equation \n1, the probability of the user being an expert of the question q is calculated through following equation: \nP (user|q)= . (. P (b, a, user)P (user|conceptb)) concepta b \u00d7 P (concepta|q) (11) For the example in \nFigure 5, assuming a query, q, the title of which is I have a question on how to use FileIn\u00adputStream \n, is analyzed by G-Finder. First, q is mapped to only one concept, c1, with the probability of 1. Accord\u00ading \nto Equation 11, the probability P (u3|c1) is the sum of the probability of u3 being an expert on c1, \nand the probability of u3 on related concept c2 multiplied by the likelihood probability P (c2,c1,u3). \nTherefore, the calcula\u00adtion is 0.088 + 0.087 \u00d7 0.5=0.1315. If another query is mapped to c2 with the \nprobability of 1, P (u3|c2) would be 0.087 + 0.088 \u00d7 0.5=0.131. Our model produces adaptive results for \neach query, as each query has different probabilities P (concept|q) on con\u00adcepts according to their differences \nin the attached source code or their contents. 5. Implementation We have implemented our algorithm in \na tool called G-Find\u00ader, the architecture of which is shown in Figure 7. The wide arrows show the .ow \nof data preprocessing: the forum data and the software source code that the forum is dedicated to are \ndownloaded by the crawler, and the raw data preproces\u00adsor takes the raw forum data and outputs the threads \ninto a relational database. These two components are described below: 1. Crawler: The crawler downloads \nthe pages of threads from online forums, given the seed links of the forums. The seed links are the initial \nlinks that the crawler uses to download pages, of which the links are extracted for further downloading. \nAs a forum always has .xed site structures and page formats, our crawler ef.ciently ex\u00adtracts new links \nof pages in the seed pages using regular expressions.  2. Figure 7: Architecture The thin arrows show \nthe steps of the model construction. The key module is the router, which extracts the threads from the \ndatabase and constructs both the concept networks and the user networks. The router also processes queries \nand returns the ranked user lists. All these functions are achieved through four sub-components of the \nrouter described below: 1. Mapper: The mapper maps the threads to concepts, tak\u00ading the threads in the \ndatabase and the code of software system pertaining to the forum as input. The threads are mapped to \nconcepts with certain probabilities using the heuristics explained in Section 4. The code of software \nsystem of forum is downloaded separately. 2. Concept network constructor: The concept network constructor \nbuilds the concept networks by construct\u00ading the relationships of the concepts and computing the weights \namong concepts using the threads in the database. The type hierarchy relationship and the call graph \nrelationship between classes are obtained using the code analysis tool Wala.12 3. User network constructor: \nThe user network construc\u00adtor builds the user networks for each concept, and cal\u00adculate the probability \nP (user|concept) using the post\u00adreply structures in the threads. 4. Query handler: The query handler, \ntaking queries as input, calculates the probabilities P (concept|question) by invoking the functionalities \nof the mapper, and then returns a list of ranked users.  12 Wala. URL: http://wala.sourceforge.net/wiki/index.php/ \nMain_Page 6. Experiment Our evaluation of G-Finder aims at the following objec\u00adtives: Generality. Does \nour algorithm effectively work on pro\u00adgramming forums in general?  Effectiveness. Is our technique more \neffective compared to related approaches?  Scalability Does our modeling technique scale with the size \nof the forum data?  We crawl the data of discussion threads from three fo\u00adrums: the Java Forum, the \nJava DevShed Forum, and the GEF Forum. Our main evaluation method is to .rst create a manually veri.ed \nhistorical data set as the oracle. We then implement an array of related approaches and compare the performance \nof G-Finder against them. In the rest of the section, we .rst describe the oracle data set, followed \nby the details of the evaluation. 6.1 Oracle Data Set Unlike the earlier evaluation method [18], which \nis based on the manual judgement to assess the precision, we use the historical data to quantify the \nprediction effectiveness of our algorithm. We divide the concepts concerned in the threads into two parts \nrandomly, one called PA, used to construct both the user and concept networks, and the other called PB, \nused to test the results. After division, we only reserve the predictable data in these two sets, such \nthat the threads in one set have participants who also provided answers to the questions in the other \nset. We compute the probabilities that the user can answer the associated questions in PB, according \nto the model built using PA. The returned user list is compared with the actual expert set, de.ned as \nAUconcept. The expert set, AUconcept, is obtained through a consen\u00adsus voting process on each thread. \nSix graduate students were involved to go through the questions in PB and vote which user is the expert. \nWe pick the users with more than 4 votes as the experts and add them into AUconcept. Then the precision \nof our algorithm is computed by comparing the re\u00adturned user list with AUconcept. The manual judgement \nof experts can not be avoided for the forum systems that do not tag who have given the right answers \nto questions. However, the voting approach downplays the subjectivity in our exper\u00adiments.  6.2 Metrics \nIn the expert searching area, the metrics of mean of average precision (MAP) [14][15] and Precision@N \n(P@N) [8] are widely used to evaluate the precision of expert searching. They are de.ned as follows: \nDEFINITION 1. MAP: MAP is the mean of the average of precisions over a set of query questions. The average \nof precisions for a query is the average of precision at each correctly retrieved answer(expert).  Table \n5: Performance on Three Java Forums Method Java Forum Java Devshed GEF MAP P@5 P@10 MAP P@5 P@10 MAP \nP@5 P@10 G-Finder 0.73 0.65 0.73 0.74 0.65 0.72 0.67 0.61 0.63 Pro.le-based 0.58 0.55 0.53 0.61 0.57 \n0.62 0.57 0.54 0.55 Language Model 0.53 0.53 0.54 0.55 0.56 0.57 0.52 0.51 0.53 Replies Count 0.43 0.41 \n0.4 0.51 0.51 0.52 0.33 0.31 0.34 DEFINITION 2. P@N is the percentage of top N candidate answers who \nare correct. 6.3 Benchmarked Approaches We compare our model to three methods, including the lan\u00adguage \nmodel [2], the pro.le-based method [18], and the method of ranking using the reply count [18]. The detailed \ndescriptions on these three methods are below: 1. language model: The language model calculates the probability \nof a user to generate the terms in a partic\u00adular question, based on the historical documents created \nby the users. First, the term probabilities of the users are computed and smoothed according to the pro.les \nof users. Then the probability of generating a question is obtained through multiplying probabilities \nof each term in the question. 2. pro.le-based method: The pro.le-based method com\u00adbines the language \nmodels and the graph-based methods. First, the messages posted by a particular participant are considered \nas the pro.le of the user. Second, the prob\u00adability of a user as a candidate of expert is calculated \nby multiplying the authority value, computed using the HITS algorithm from the post-reply network of \nthreads, with the probability computed from the language model of the user. 3. reply count: Ranking \naccording to reply count is to rank the users according to the numbers of threads users give replies \nto. This is a straight forward approach for predict\u00ading experts.  6.4 Generality Study We .rst test \nthe general effectiveness of our algorithm on the three PFs. The results are shown below in Table 6. \nTable 6: Generality Study Forum MAP P@1 P@3 P@5 P@10 Java Forum 0.73 0.58 0.6 0.65 0.73 Java Devshed \n0.74 0.6 0.62 0.65 0.72 GEF 0.67 0.53 0.56 0.61 0.63  The results show that, on average, our algorithm \ncan achieve about 70% on MAP and 65% on P@N in all the three PFs. The best precision G-Finder can achieve \nis about 74%, and the worst is about 53% on P@1. For the case of Top-1 users, the returned user set is \nsmall, which makes the percentage of experts small. However, G-Finder can still predict the right experts \nfor about 60% of the times on average in such situations. The prediction results on two Java forums are \na little higher than that on the GEF Forum. We think that, compared with the general Java programming \nproblems, the problems in the GEF Forum is more dif.cult to answer and the percentage of users that give \nanswer is relatively smaller, resulting lower MAPs and P@Ns on the GEF Forum. 6.5 Performance In this \nexperiment, we show the performance of our model compared with three other methods mentioned above on \nthe three forums. The results are shown in Table 5. The results show that, based on semantic information, \nG-Finder can improve the performance of expert searching by 21% on average on MAP, and 23% on P@10, compared \nto the pro.le-based model. The best case for MAP is on the Java Forum, and the improvement is about 25%. \nThe worst case is on the GEF Forum with the improvement of 17%. From the result, we conclude that the \nG-Finder improves the precision of expert searching signi.cantly.  6.6 Effectiveness of Concept Network \nThe main difference in our approach, as compared to the re\u00adlated work, is that we use the concept network \nin G-Finder to locate experts. So are the relationships among concepts re\u00adally effective in improving \nthe quality of prediction? In this experiment, we compare the prediction quality of G-Finder with (G-Finder) \nand without (Single Concept) the concept network. The result, presented in Table 7, shows that using \nconcept network improves precision by about 50% on MAP com\u00adpared to Single Concept Method. For the top \nten users re\u00adturned, the use of concept networks returns two more rec\u00adommended experts on average. Returning \nmore experts can help to lower the work load of experts. Table 7: Effectiveness of Concept Network Method \nMAP P@5 P@10 G-Finder 0.73 0.65 0.73 Single Concept 0.50 0.45 0.49  by Figure 8: Time Cost/Count of \nThreads We do not evaluate the query time. The probabilities, Psep(u|concept), and the likelihood probabilities, \nP , are computed and stored in the database after building the con\u00adcept networks and the user networks. \nThe cost of querying is mainly the time of mapping the query to the concepts, a tiny period of time that \ncan be ignored. 6.8 Limitations of G-Finder In this section, we discuss the limitation of G-Finder: \n No Concepts in Threads: Our heuristic-based concept mapping method in some cases fails to extract concepts. \nWhen these threads are in the test data set, our model can not return a reasonable result. However, in \nPFs, the per\u00adcentage of threads that cannot be handled by our heuris\u00adtics is usually low. Second, when \nthese threads are in the set for building models, we can just remove these threads because the remaining \nthreads in the set are of a healthy number to build our model.  Precision of Mapping from Thread to \nConcept: In the algorithm, we map the threads to concepts using a heuristic-based approach. So the threads \nmay be mapped to concepts they do not semantically belong to. We do not calculate the mapping precision \nin the experiment, but just calculate the .nal result. In fact, we can not quantify this error, as in \nthe current forums no labeled mapping  between threads and concepts exists. In some forums, a question \nis marked with tags, such as java or perl . However, these tags are too high-level and not sensible to \nconstruct concept networks. So we just compute the .nal precision to evaluate the performance of our \nalgorithm. The probabilistic nature of our algorithm requires a sig\u00adni.cant amount of data to build both \nthe user and the concept networks. Active forums usually have suf.cient data for our algorithm to be \napplied.  6.9 Analysis of Prediction Errors We carefully examined the evaluation results and found that \nG-Finder fails to .nd experts on the following three types: 1. General Experts: From the perspective \nof the concept network, we .nd that a certain type of users can be ob\u00adserved as general experts , as \nthe concepts of the ques\u00adtions they answer have no explicit or implicit semantic relationships with each \nother. 2. Random Experts: Another type of false positives is what we refer to as the random experts. \nThey actively par\u00adticipate in the threads with comments and suggestions but rarely with answers. Our \nalgorithm makes correct guesses that they are likely to give the answers but un\u00adable to distinguish between \nreal answers and commentary texts. 3. New Experts: The third type of false positives belongs to users \nwho never reply to anything before. Their .rst posts on the forums give the right answers to the questions. \nWe refer to these users as new experts.  These three types of users do not have high rankings in our \nresults, but provide the right answers in the veri.ed data set. This is due to the special participation \nbehavior of these users on the forums. As users are free to post anything to any threads, there are probably \nother latent patterns and relationships that are not captured by our model. We plan to address these \nissues in our future work. 7. Conclusions and Future Work We have presented the design and the evaluation \nof G-Finde\u00adr, both an algorithm and a tool that locates the most appro\u00adpriate user of programming forums \nfor answering a partic\u00adular programming question. G-Finder makes use of the source code information of \nthe target software system of a particular forum to discover additional latent relation\u00adships among forum \nthreads and, consequently, signi.cantly improves the quality of the expert search. We evaluated G-Finder \nusing the real world data from active program\u00adming forums and compared our approach to the state of the \nart forum analysis techniques. As the future work, we plan to employ more sophisticated techniques to \nmore accurately extract the semantic characteristics of the threads. We also seek more latent relationships \namong forum threads in addi\u00adtion to the source code based ones that we currently use.  8. Acknowledgements \nThe authors would like to thank Prem Devanbu and the anonymous reviewers for their encouragement and \nconstruc\u00adtive feedbacks. This research is supported by the National Basic Research Program of China under \nGrant No.2007CB3\u00ad10805, the Beijing science and technology plan projects un\u00adder Grant No.Z09000100960907, \nthe Beijing Natural Sci\u00adence Foundation project under Grant No. 4092043, and the Hong Kong RGC GRF Grant \nNo.622208. References [1] J. Anvik, L. Hiew, and G. C. Murphy. Who should .x this bug? In 28th International \nConference on Software Engineer\u00ading (ICSE 2006), Shanghai, China, May 20-28, 2006, pages 361 370. ACM, \n2006. ISBN 1-59593-375-1. [2] K. Balog, L. Azzopardi, and M. de Rijke. Formal models for expert .nding \nin enterprise corpora. In SIGIR 06: Proceed\u00adings of the 29th annual international ACM SIGIR conference \non Research and development in information retrieval, pages 43 50, New York, NY, USA, 2006. ACM. ISBN \n1-59593\u00ad369-7. doi: http://doi.acm.org/10.1145/1148170.1148181. [3] K. Balog, T. Bogers, L. Azzopardi, \nM. de Rijke, and A. van den Bosch. Broad expertise retrieval in sparse data environments. In SIGIR 07: \nProceedings of the 30th an\u00adnual international ACM SIGIR conference on Research and development in information \nretrieval, pages 551 558, New York, NY, USA, 2007. ACM. ISBN 978-1-59593-597-7. doi: http://doi.acm.org/10.1145/1277741.1277836. \n[4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent dirichlet allo\u00adcation. J. Mach. Learn. Res., 3:993 \n1022, 2003. ISSN 1532\u00ad4435. doi: http://dx.doi.org/10.1162/jmlr.2003.3.4-5.993. [5] S. Brin and L. Page. \nThe anatomy of a large-scale hypertextual web search engine. Comput. Netw. ISDN Syst., 30(1-7):107 117, \n1998. ISSN 0169-7552. doi: http://dx.doi.org/10.1016/ S0169-7552(98)00110-X. [6] P. Jurczyk and E. Agichtein. \nDiscovering authorities in ques\u00adtion answer communities by using link analysis. In CIKM 07: Proceedings \nof the sixteenth ACM conference on Conference on information and knowledge management, pages 919 922, \nNew York, NY, USA, 2007. ACM. ISBN 978-1-59593-803-9. doi: http://doi.acm.org/10.1145/1321440.1321575. \n[7] J. M. Kleinberg. Authoritative sources in a hyperlinked en\u00advironment. In SODA 98: Proceedings of \nthe ninth annual ACM-SIAM symposium on Discrete algorithms, pages 668 677, Philadelphia, PA, USA, 1998. \nSociety for Industrial and Applied Mathematics. ISBN 0-89871-410-9. [8] C. Manning, P. Raghavan, and \nH. Schtze. Introduction to Information Retrieval. Cambridage University Press, 2008. [9] D. Mimno and \nA. McCallum. Expertise modeling for match\u00ading papers with reviewers. In KDD 07: Proceedings of the 13th \nACM SIGKDD international conference on Knowledge discovery and data mining, pages 500 509, New York, \nNY, USA, 2007. ACM. ISBN 978-1-59593-609-7. doi: http: //doi.acm.org/10.1145/1281192.1281247. [10] A. \nMockus and J. D. Herbsleb. Expertise browser: a quan\u00adtitative approach to identifying expertise. In Proceedings \nof the 22rd International Conference on Software Engineering, ICSE 2002, 19-25 May 2002, Orlando, Florida, \nUSA, pages 503 512. ACM, 2002. [11] D. Petkova and W. B. Croft. Hierarchical language models for expert \n.nding in enterprise corpora. In ICTAI 06: Pro\u00adceedings of the 18th IEEE International Conference on \nTools with Arti.cial Intelligence, pages 599 608, Washington, DC, USA, 2006. IEEE Computer Society. ISBN \n0-7695-2728-0. doi: http://dx.doi.org/10.1109/ICTAI.2006.63. [12] J. M. Ponte and W. B. Croft. A language \nmodeling approach to information retrieval. In SIGIR 98: Proceedings of the 21st annual international \nACM SIGIR conference on Research and development in information retrieval, pages 275 281, New York, NY, \nUSA, 1998. ACM. ISBN 1-58113-015-5. doi: http://doi.acm.org/10.1145/290941.291008. [13] G. Salton and \nC. Buckley. Term weighting approaches in automatic text retrieval. Technical report, Ithaca, NY, USA, \n1987. [14] C. Shah and W. B. Croft. Evaluating high accuracy retrieval techniques. In SIGIR 04: Proceedings \nof the 27th annual international ACM SIGIR conference on Research and de\u00advelopment in information retrieval, \npages 2 9, New York, NY, USA, 2004. ACM. ISBN 1-58113-881-4. doi: http: //doi.acm.org/10.1145/1008992.1008996. \n[15] E. Voorhees and D. Tice. The trec-8 question answering track evaluation. In Proceedings 8th Text \nREtrieval Conference (TREC-8), pages 83 105, 1999. [16] C. Zhai and J. Lafferty. A study of smoothing \nmethods for language models applied to information retrieval. ACM Trans. Inf. Syst., 22(2):179 214, 2004. \nISSN 1046-8188. doi: http://doi.acm.org/10.1145/984321.984322. [17] J. Zhang, M. S. Ackerman, and L. \nAdamic. Expertise net\u00adworks in online communities: structure and algorithms. In WWW 07: Proceedings of \nthe 16th international conference on World Wide Web, pages 221 230, New York, NY, USA, 2007. ACM. ISBN \n978-1-59593-654-7. doi: http://doi.acm. org/10.1145/1242572.1242603. [18] Y. Zhou, G. Cong, B. Cui, C. \nS. Jensen, and J. Yao. Routing questions to the right users in online communities. In ICDE 09: Proceedings \nof the 2009 IEEE International Conference on Data Engineering, pages 700 711, Washington, DC, USA, 2009. \nIEEE Computer Society. ISBN 978-0-7695-3545-6. doi: http://dx.doi.org/10.1109/ICDE.2009.44.   \n\t\t\t", "proc_id": "1869459", "abstract": "<p>Programming forums are becoming the primary tools for programmers to find answers for their programming problems. Our empirical study of popular programming forums shows that the forum users experience long waiting period for answers and a small number of experts are often overloaded with questions. To improve the usage experience, we have designed and implemented G-Finder, both an algorithm and a tool that makes intelligent routing decisions as to which participant is the expert for answering a particular programming question. Our main approach is to leverage the source code information of the software systems that forums are dedicated to, and discover latent relationships between forums users. Our algorithms construct the concept networks and the user networks from the program source and the forum data.We use programming questions to dynamically integrate these two networks and present an adaptive ranking of the potential experts. Our evaluation of G-Finder, using the data from three large programming forums, takes a retrospective view to check if G-Finder can correctly predict the experts who provided answers to programming questions. The evaluation results show that G-Finder improves the prediction precision by 25% to 74%, compared to related approaches.</p>", "authors": [{"name": "Wei Li", "author_profile_id": "81537250256", "affiliation": "Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China", "person_id": "P2354009", "email_address": "", "orcid_id": ""}, {"name": "Charles Zhang", "author_profile_id": "81435599989", "affiliation": "Hong Kong University of Science and Technology, HongKong, Hong Kong", "person_id": "P2354010", "email_address": "", "orcid_id": ""}, {"name": "Songlin Hu", "author_profile_id": "81451597178", "affiliation": "Institute of Computing Technology, Chinese Academy of Sciences, Beijing, China", "person_id": "P2354011", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1869459.1869466", "year": "2010", "article_id": "1869466", "conference": "OOPSLA", "title": "G-Finder: routing programming questions closer to the experts", "url": "http://dl.acm.org/citation.cfm?id=1869466"}