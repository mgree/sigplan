{"article_publication_date": "10-17-2010", "fulltext": "\n Automatic Atomic Region Identi.cation in Shared Memory SPMD Programs * Gautam Upadhyaya, Samuel P. Midkiff \nand Vijay S. Pai Purdue University {gupadhya, smidkiff, vpai}@purdue.edu Abstract This paper presents \nTransFinder, a compile-time tool that automatically determines which statements of an unsynchro\u00ad nized \nmultithreaded program must be enclosed in atomic regions to enforce con.ict-serializability. Unlike previous \ntools, TransFinder requires no programmer input (beyond the program) and is more ef.cient in both time \nand space. Our implementation shows that the generated atomic re\u00ad gions range from being identical to, \nor smaller than, the programmer-speci.ed transactions in the three Java Grande benchmarks considered, \nand in .ve of the eight STAMP benchmarks considered, while still providing identical syn\u00ad chronization \nsemantics and results. The generated atomic regions are between 5 and 38 lines larger in the three re\u00ad \nmaining STAMP benchmarks. In the most conservative case, TransFinder can, based on the program structure, \nsuccess\u00ad fully identify and suggest an alternative that conforms ex\u00ad actly to the programmer-speci.ed \natomic regions. By gener\u00ad ating small, highly-targeted, con.ict-serializable atomic re\u00ad gions, TransFinder \nallows the programmer to focus further tuning efforts on only a small portion of the code (when fur\u00ad \nther tuning is needed). Categories and Subject Descriptors D.1.3 [Programming Techniques]: Parallel programming \nGeneral Terms Algorithms, Performance, Design Keywords Automatic transactional region identi.cation, \ncon.ict-serializability, parallel programming 1. Introduction A major dif.culty when programming shared \nmemory par\u00adallel machines is identifying shared variables (whose stor\u00adage is accessed by many threads), \nand controlling the ac\u00adcess to those variables across the threads. When writing *This work is supported \nin part by the National Science Foundation under Grant Nos. CCF-0532448, CNS-072212,and CNS-0751153. \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. a parallel \nprogram, the programmer must both determine the shared variables and analyze the interactions among them \nto determine when sequences of shared memory ac\u00adcesses must execute as a single atomic region, when they \nmay execute as a sequence of atomic regions, or when some (or all) of them can execute outside of any \natomic re\u00adgion. Once the programmer identi.es the code s atomic re\u00adgions, they may be enforced using \neither transactions [18 20, 26, 33, 34], user-generated locks, or locks automatically generated from \nuser-identi.ed atomic regions or transac\u00adtions [7, 14, 17, 21, 25, 35, 41]. All of these techniques require \nthe programmer to un\u00adderstand the interactions between different threads and the .ow of data across threads. \nErrors in generating atomic sec\u00adtions and locks can lead to non-determinate bugs that are ex\u00adtremely \ndif.cult to correct. This paper describes the Trans-Finder tool that attacks these fundamental problems \nhead\u00adon. TransFinder assumes that programmers desire operations on shared data within a thread to be \nperformed without inter\u00adference from other threads; that is, without the values of the shared data being \nread or written by other threads while the operations are performed. Using compiler analyses, some of \nwhich are inspired by concepts from database theory, Trans-Finder automatically identi.es regions in \nJava programs that need to be executed as atomic regions to enforce the seman\u00adtics implied by the above \nconstraint. TransFinder produces as output a C++ program with the atomic regions marked. Unlike the only \nother known attempt at automatically identi\u00adfying atomic regions in programs [37, 38], TransFinder does \nnot require user annotations of the code. This makes Trans-Finder easy to use. The close match of atomic \nregions iden\u00adti.ed by TransFinder, and hand-inserted atomic regions in our benchmark programs indicates \nthat the semantics Trans-Finder ascribes to multithreaded programs are valid over the benchmarks examined, \nand useful. The primary goal of TransFinder is to suggest atomic re\u00adgions to a programmer, and allow \nthe programmer to de\u00adtermine the .nal scope of the atomic regions. It does this by analyzing cycles of \ncon.icting operations and encapsu\u00adlating those cycles in atomic regions. In most cases stud\u00adied, however, \nthe atomic regions speci.ed by TransFinder are close enough to hand generated atomic regions to be used \nunchanged. For certain program control .ow structures loops in particular TransFinder can suggest alterna\u00adtive \natomic regions in cases where the .rst choice suggested OOPSLA/SPLASH 10, October 17 21, 2010, Reno/Tahoe, \nNevada, USA. by TransFinder is too conservative. In one additional bench-Copyright . 2010 ACM 978-1-4503-0203-6/10/10. \n. . $10.00 c  mark, this alternative was close enough to hand-generated code to be used unchanged. Moreover, \nit is always the case that the atomic regions suggested by TransFinder allow the programmer to focus \non a small subset of the program (less than 7% of the code in the benchmarks studied) to evaluate the \natomic regions. TransFinder is conservative in that the atomic regions speci.ed include all variable \nreferences that might need to be in some atomic region. We use programs from two benchmarks suites: we \nuse the three multithreaded full-scale applications from the Java Grande Benchmarks [15] and the STAMP \nbenchmarks [6]. All of these benchmarks are characterized by having a sin\u00adgle thread type, of which multiple \ninstances simultaneously execute. Following the example of [22] we refer to these as SPMD (Single Program, \nMultiple Data) programs. Our anal\u00adysis can be applied to programs with multiple thread types. Finally, \nTransFinder does not check for code within trans\u00adactions that might lead to deadlocks or similar behaviors \nas discussed in [39, 42]. This paper s contributions are as follows: It describes the TransFinder system \nthat automatically detects atomic regions in shared memory SPMD pro\u00adgrams;  It describes the correctness \ncriteria used by TransFinder, and its relationship to con.ict-serializability;  It describes the analyses \nused by TransFinder to enforce these correctness criteria;  It provides experimental data showing how \nclose Trans-Finder comes to hand-generated atomic regions in the three full-scale applications among \nthe multithreaded Java Grande benchmarks [15] and in the STAMP benchmarks [6]. It does this by both comparing \nlines of code contained within atomic regions, and by mea\u00adsuring the performance of programs using hand-coded \nand automatically-generated atomic regions. Experimen\u00adtal results are provided for executions using versions \nof the programs where the atomic regions are enforced us\u00ading software transactional memory.  It discusses \nextensions of the TransFinder technique to non-SPMD shared memory programs.  The rest of the paper is \norganized as follows. Section 2 provides an overview of our approach. Section 3 describes the theoretical \nunderpinnings of our approach and the analy\u00adses we use. Section 4 describes the implementation of these \nprinciples in our system. Section 5 describes our evaluation using the STAMP and Java Grande benchmark \nsuites. Sec\u00adtions 6 and 7 describe related work and conclusions. Ap\u00adpendix A contains a detailed example. \n2. Overview In this section we introduce, at a fairly high level, our ap\u00adproach to identifying atomic \nregions in shared memory pro\u00adgrams. We introduce the concepts and some useful de.ni\u00adtions, leaving the \nmore formal analysis for Section 3. Our analysis is built on the notion of serializability. Se\u00adrializability \nis a universally recognized correctness criterion that has its roots in database theory. To motivate \nthe rest of this paper, we brie.y explain the necessary de.nitions here. A fuller exploration of some \nof these topics can be found in [13]. A trace of a program involving multiple transactions is called \na schedule. A schedule is said to be serial if statements in an individual transaction execute one after \nthe other, with no interference from any statements in other transactions. A schedule is serializable \nif it is equivalent to a serial schedule (the resultant database state is the same as that of some serial \nschedule). Operations in different transactions are said to con.ict if they access the same memory location(s), \nand at least one of the operations is a write. Broadly speaking, there are two widely-used de.ni\u00adtions \nof serializability: view-serializability and con.ict\u00adserializability. View-serializability matches the \ngeneral def\u00adinition of serializability above. Con.ict-serializability is a conservative approximation \nto serializability that focuses on the respective order of con.icting operations. In particular, a schedule \nis con.ict-serializable if the respective order of mu\u00adtually con.icting operations is the same as that \nof some se\u00adrial schedule (non-con.icting operations may be reordered). We note that con.ict-serializability \nis more restrictive than serializability, in that any con.ict-serializable schedule is also serializable, \nbut the converse does not necessarily hold. In the rest of the paper, we will use the terms serializable \nand con.ict-serializable interchangeably. For a schedule involving multiple transactions to be con.ict-serializable, \ntherefore, only non-con.icting opera\u00adtions may be interleaved (here we consider an operation in its entirety, \nand do not break it down into its constituent reads and writes). This leads to the following insight: \nIf two transactions have multiple operations on a given (shared) memory location, and if these oper\u00adations \nmutually con.ict, then all of the operations within a given transaction which access that location must \nbe allowed to execute without the possibility of an interleaving from statements in other transactions. \n Intuitively, if such an interleaving were to be allowed to exist (which ordered con.icting operations \nin a certain manner), then it would also be possible for an alternate interleaving to exist which ordered \nthese operations in a different manner. The end result would, by de.nition, not be con.ict-serializable. \nAdding synchronization constructs eliminates the possibility of such an interleaving, and forces a con.ict-serializable \nschedule. We note that con.ict-serializability generally conforms to a user s de.nition of correctness: \na sequence of operations on shared storage within a thread generally should appear to complete as if \nthe sequence executed without interference from other threads. Thus, shared variable state after such \na sequence should be the same as if there was no interference from other threads. When analyzing multithreaded \nshared memory programs, the TransFinder compile-time tool initially considers each possible runtime thread \nto correspond to a single, large trans\u00adaction. In other words, the TransFinder tool assumes the transaction \nis the entire thread. With these semantics, and given our discussion so far, any interleavings of con.ict\u00ading \noperations can lead to non-con.ict-serializable execution runs. Clearly such interleavings may be avoided \nby forc\u00ad Thread 1 Thread 2 x=1 x=2 y=x y=x (a) Program Fragment Thread 1 Thread 2 begin atomic() begin \natomic() x=1 x=2 y=x y=x end atomic() end atomic() (d) Synchronized Code (b) Invalid Interleaving Thread \n1 Thread 2 x=1 x=2 y=x y=x Thread 1 Thread 2 x=1 y=x x=2 y=x (e) Serializable Schedule write xT1 . \nwrite xT2 . (read xT1 ,write yT1 ) . (read xT2 ,write yT2 ) (c) Order of con.icting operations (Si . \nSj denotes Si executes before Sj ) write xT1 . (read xT1 ,write yT1 ) . write xT2 . (read xT2 ,write \nyT2 ) (f) Order of con.icting operations (Si . Sj denotes Si executes before Sj ) Figure 1. Valid and \ninvalid interleavings: x and y are variables shared by both threads. ing the threads to execute sequentially. \nHowever, such an arrangement would defeat the purpose of a multithreaded program. In this work, our purpose \nis to protect against only those interleavings which would lead to non-con.ict\u00adserializable schedules. \nTo do so, we .rst identify con.icting operations, and then determine which interleavings of those operations \nare disallowed. We then prevent the possibility of such interleavings by synchronizing only the offending \nblocks of code via atomic regions, and thus enforce con.ict\u00adserializability. Consider the example of \nFigure 1, where two threads at\u00adtempt to .rst write to a (shared) variable x, and then use the value of \nx to update a different shared variable y. The in\u00adput to TransFinder consists of program source code \nwith no synchronization constructs. Executing such code could lead to any and all interleavings, including \nthose that may lead to non-con.ict-serializable schedules. Figure 1(b) shows one such invalid interleaving \nof the statements in the program fragment from Figure 1(a). The order of con.icting opera\u00adtions in the \nschedule is shown in Figure 1(c). Note that this order is inconsistent with that of the serial schedule \nfrom Figure 1(e), (shown in Figure 1(f)). In fact, the ordering shown in Figure 1(c) is inconsistent \nwith that of any se\u00adrial schedule. The invalid interleaving of Figure 1(b) can be prohibited by enclosing \nthe con.icting operations within an atomic block, as shown in Figure 1(d). Doing so leads to con.ict-serializable \nschedules. Intuitively, when the threads have .nished executing the program fragments shown, it might \nreasonably be expected that the values assigned to x and y have been assigned in a single thread. Enclosing \nthe relevant operations in atomic regions ensures this. We now formalize the criteria we use for enclosing \ncon\u00ad.icting references within an atomic region. DEFINITION 1. Two threads, Ti,Tj have a non-serializable \nexecution run if there exists some interleaving of the indi\u00advidual memory operations of the two threads \nwhich is not con.ict-serializable. Again, Figure 1(a) gives an example code fragment and Figure 1(b) \nshows one possible invalid interleaving. The interleaving is possible because of a lack of synchronization \non the two globally-scoped variables, x and y. Inserting synchronization (Figure 1(d)) leads to a serializable \nschedule (Figure 1(e)). Con.ict-serializability can be tested using a precedence graph. A precedence \ngraph of a schedule is a directed graph with a node for each (completed) atomic region in the sched\u00adule. \nAn edge exists between two transactions, Xi and Xj if an action of Xi precedes and con.icts with an action \nof Xj . The following theorem illustrates the use of precedence graphs to test the con.ict-serializability \nof a schedule: THEOREM 1. A schedule is con.ict-serializable if, and only if, its precedence graph is \nacyclic. This theorem is a fundamental and well-known result [13]. Figure 2 shows the precedence graph \nfor the schedule shown in Figure 1(b). An edge exists from the node for Thread 1 to the node for Thread \n2 because the update of the shared variable x occurs in Thread 1 before it occurs in Thread 2. In addition, \nan edge exists from Thread 2 to Thread 1 because the update of x by Thread 2 precedes and con.icts with \nthe read of x during the assignment to the shared variable y by Thread 1. Therefore, a cycle exists, \nand the schedule shown in Figure 1(b) is not con.ict-serializable. Thus, detection of non-serializable \nschedules is a matter of cycle detection in the precedence graphs of shared mem\u00adory parallel programs. \nNotice that the cycles in the graph of Figure 2 could have been avoided by forcing one thread to execute \nthe relevant con.icting instructions in toto, with\u00adout any possibility of interference from the other \nthread. Doing so forces a total ordering of the instructions in the program fragment, and therefore disallows \ncycle formation. Figure 1(d) shows correctly synchronized code (through the use of an atomic region), \nand Figure 1(e) shows one possible resultant schedule. To summarize, we have the following observations: \n Cycles in a precedence graph of a schedule indicate a lack of con.ict-serializability.  may precede \nSy is written: Sx . Sy. If the statements may execute in parallel, then Sx . Sy and Sy . Sx and is writ\u00adten \nSx . Sy. DEFINITION 4. Two threads, Ti,Tj have an interleaving if there exist statements Si,x,Si,z . \nTi and Sj,y . Tj such that Si,x . Si,z, Si,x . Sj,y and Sj,y . Si,z. The thread interleaving relation \nis written: Ti .. Tj. Given these de.nitions, we de.ne a Con.ict Interleaving (CI) as follows: DEFINITION \n5. Threads T1,T2,...,TN have a Con.ict In\u00adterleaving (CI) if the following condition holds: Let Si = \n(Si,1,Si,2,...,Si,mi ) be the set of state\u00adments of each thread Ti, 1 = i = N, and let {Si,j . j=(mi-1) \nSi,(j+1)}.Si. Then j=1 (S1,a . S2,b), (S1,a . S2,b), (S2,c . S3,d), (S2,c . S3,d)|c = b, ...... (SN,y \n. S1,z), (SN,y . S1,z)|z>a The Con.ict Interleaving relationship is written as: T1 .. . T2 ... .. . \nTN Intuitively, threads have a con.ict interleaving if there exists some interleaving of the threads \ninvolving mutually con.icting operations. We next explore the role of con.ict interleavings in de\u00adtecting \nnon-con.ict-serializable schedules. We then discuss how to detect con.ict interleavings in shared memory \npro\u00adgrams, with a special emphasis on SPMD programs that is, programs where all threads are of the same \ntype. Finally, we show how to insert atomic regions into SPMD programs by eliminating con.ict interleavings. \nThe following theorems illustrate how con.ict interleav\u00adings may be used to detect non-con.ict-serializable \nexecu\u00adtions. THEOREM 2. In a shared memory program, any con.ict in\u00adterleaving will result in a non-con.ict-serializable \nexecu\u00adtion. Proof: Let the CI be between threads T1,T2 ...TN , and let Si = (Si,1,Si,2,...,Si,mi ) be \nthe set of statements of j=(mi-1) each thread Ti, where {Si,j . Si,(j+1)}.Si. Then, j=1 from De.nition \n5, we have: (S1,a . S2,b), (S1,a . S2,b) for some S1,a . S1,S2,b . S2. In the precedence graph of the \nprogram, therefore, an edge exists from the ver\u00adtex corresponding to thread T1, to the vertex correspond\u00ading \nto thread T2. Similarly, (S2,c . S3,d), (S2,c . S3,d) for some S2,c . S2 ,S3,d . S3, leading to an edge \nfrom T2 to T3, and so on for all N threads. Finally, we have: (SN,y . S1,z), (SN,y . S1,z), leading to \nan edge from TN to T1, thus completing the cycle. From Theorem 1, therefore, the schedule is non-serializable. \nTHEOREM 3. If an execution run is non-serializable then there exists at least one con.ict interleaving. \n Proof: Let threads T1,T2,...,TN have a non\u00adserializable run, and let Si = (Si,1,Si,2,...,Si,mi ) be \nthe set of statements of each thread Ti, where j=(mi-1) Because the execu\u00ad {Si,j . Si,(j+1)}j=1 .Si. \ntion is non-serializable, there must exist a cycle in the precedence graph of the program. Without loss \nof generality, assume the path traced by the cycle is [T1,T2,...,TN ,T1] i.e., an edge exists from the \nvertex corresponding to any thread Ti, to the vertex correspond\u00ading to thread Ti+1. Furthermore, let \nthere be statements (S1,a,S1,z) . S1, (S2,b,S2,c) . S2,...,SN,y . SN . Be\u00adcause an edge exists from T1 \nto T2,wehave (S1,a . S2,b), and because the only edges allowed in the precedence graph are con.ict edges, \nwe have (S1,a . S2,b). Similarly, the edge from T2 to T3 gives us: (S2,c . S3,d) , (S2,c . S3,d), and \nso on for the other threads. And .nally, the edge from TN to T1 gives us: (SN,y . S1,z) , (SN,y . S1,z) \n(here we assume, without loss of generality, that S1,a . S1,z). From De.nition 5, therefore, we have \na con.ict interleaving. THEOREM 4. Threads, T1,T2,...,TN have a non\u00adserializable execution run if, and \nonly if, there is a con.ict interleaving between them. Proof: Follows from Theorems 2 and 3. COROLLARY \n1. Eliminating con.ict interleavings in a shared memory program will result in a con.ict-serializable \nexecution. Proof: Follows from Theorem 4. 3.2 Detecting Con.ict Interleavings in general shared memory \nprograms The previous section established the relationship between cycles in the precedence graph of \na program, and con.ict in\u00adterleavings. The precedence graph is a runtime construct: it depicts precedence \norders between committed transactions. As such, it is not directly usable in a static context because \nit is impossible, in general to determine a priori in what order the individual statements in a multithreaded \nprogram will execute. TransFinder, on the other hand, is a tool that, at compile-time, determines the \nsynchronization constructs necessary to achieve con.ict-serializability. In order to al\u00adlow TransFinder \nto achieve its goal, we require a construct to statically represent the various possible (runtime) inter\u00adleavings \nin the program. This section introduces the Con.ict Graph, which is just such a construct. We begin by \nstating some useful de.nitions: DEFINITION 6. A Concurrent Shared Basic Block (CSBB) is a basic block \nwith the added constraint that there is at most one occurrence, within the block, of a variable that \nreferences shared memory locations. We note that the shared variable within a CSBB may, over time, reference \ndifferent memory locations. We de.ne con.icting CSBBs as follows: DEFINITION 7. Two Concurrent Shared \nBasic Blocks con\u00ad.ict if they contain statements that con.ict with one an\u00adother. More precisely, let \nthere be two CSBBs, Bi,Bj.Then Bi . Bj if {.Si . Bi,Sj . Bj|Si . Sj }. Moreover, a class Thread int []data \nprocedure Thread(int []d) // Thread constructor data = d end procedure procedure void run() // run \nfunction for the thread int x = d[0] d[0] = 3 int y = d[1] ... end procedure end class ... procedure \nmain() int []a = new int[10] Thread T1 = new Thread(a), T2 = new Thread(a) T1.run(); T2.run() end procedure \nFigure 3. A Typical SPMD program. concurrent shared basic block Bi may precede another, Bj , if any \nstatement in Bi may precede any statement in Bj, i.e. Bi . Bj if {.Si . Bi,Sj . Bj|Si . Sj}. Note that \nwe conservatively assume that if two CSBBs may con.ict then they do con.ict. We rely on the con.ict graph \n(CG) to identify CIs in multithreaded programs. The CG is closely related to the Concurrent Control Flow \nGraph (CCFG) of Lee et al. [23]. A CG is de.ned as follows: DEFINITION 8. A Con.ict Graph (CG) is a directed \ngraph G =(V, E, Entry, Exit), where: 1. V is the set of vertices in G. Each vertex is a CSBB, denoted \nB. 2. E = EF . EC . EF is the set of control .ow edges, in\u00adcluding transitive edges. EC is the set of \ncon.ict edges. An undirected con.ict edge exists between any two CS-BBs that con.ict. Only con.ict edges \nmay exist between CSBBs in different threads. 3. Entry is the CSBB through which all control .ow enters \nthe graph. 4. Exit is the CSBB through which all control .ow leaves the graph.  It is sometimes desirable \nto refer to the smallest subgraph of a CG that contains a speci.c thread. We denote the sub\u00adgraph of \nsome CG that contains the nodes of thread Ti as CGi. Figure 3 gives an example of a typical SPMD program, \nand Figure 4 shows the CG for the program of Figure 3. The Thread Start and Thread Stop nodes indicate \nthread initial\u00adizations (one per outgoing edge) and destructions, respec\u00adtively. Thread run and Thread \njoin are special nodes which indicate where the thread starts and stops executing, respec\u00adtively. In \nwhat follows, we will assume the existence of the Entry and Exit nodes and will generally omit mentioning \nthem. Figure 11 in the appendix contains a more detailed example of a CG. The CG can be analyzed to determine \nCIs by detecting cycles involving control .ow edges and inter-thread con.ict edges. In general, a con.ict \noccurs because of multiple ac\u00adcesses to shared memory locations. These accesses are either    Figure \n4. The Con.ict Graph of the SPMD program of Figure 3. read accesses or write accesses. We note that while \nindivid\u00adual CSBBs may contain a single access to any shared mem\u00adory location, these accesses do not necessarily \ncon.ict with the accesses from other CSBBs. To detect con.icts between CSBBs, therefore, we must .rst \ndetect the set of accesses to shared memory locations. The read set of a CSBB Bi in the con.ict graph \nCGi for thread Ti is the set of shared memory locations being read by the statements in Bi, and is denoted \nRi. Similarly, the write set, Wi, is the set of shared memory locations being written by the statements \nin Bi. Two CSBBs, Bi and Bj , con.ict if any of the following conditions hold: (a) Ri n Wj \u00d8, (b) Wi \nn Rj = \u00d8 or (c) Wi n Wj \u00d8. The read-write set of CSBB Bi is the set of shared memory locations being \nread from or written to by the statements in Bi, and is denoted RWi where RWi = Ri . Wi. We are now ready \nto de.ne a con.ict cycle: DEFINITION 9. Let Bi = (Bi,1,Bi,2,...,Bi,mi ) . CGi be the CSBBs of some thread \nTi, and let {(Bi,j,Bi,(j+1)) . j=(mi-1) EF }.Bi. Then threads [T1,T2,...,TN ] have a j=1 con.ict cycle \nif the following condition holds: there exists a path [B1,a,B2,b,B2,c,B3,d,...,BN,y,B1,z] such that: \n(B1,a,B2,b) . EC ,B1,a . B2,b (B2,c,B3,d) . EC |c = b, B2,c . B3,d, ...... (BN,y,B1,z) . EC |z = a, BN,y \n. B1,z Intuitively, a con.ict cycle is a cycle in the CG involving both control .ow and con.ict edges. \nTHEOREM 5. An inter-thread con.ict cycle in a shared memory program indicates a CI. Proof: Let the inter-thread \ncon.ict cycle oc\u00adcur between threads [T1,T2,...,TN ]. Let Bi = (Bi,1,Bi,2,...,Bi,mi ) . CGi be the CSBBs \nof thread Ti, j=(mi-1) and let {(Bi,j ,Bi,(j+1)) . EF }j=1 .Bi. From De.ni\u00adtion 9, we know that (B1,a,B2,b) \n. EC ,B1a . B2,b. But (B1,a,B2,b) . EC if B1,a . B2,b and, from De.nition 3, B1a . B2,b if B1,a . B2,b \nand B1,a . B2,b. Similarly, (B2,c,B3,d) . EC if B2,c . B3,d, and B2,c . B3,d if B2,c . B3,d and B2,c \n. B3,d. Extending this to all N threads gives us: (B1,a . B2,b), (B1,a . B2,b), (B2,c . B3,d), (B2,c \n. B3,d)|c = b, ...... (BN,y . B1,z), (BN,y . B1,z)|z>a which, from De.nitions 5 and 7, is a con.ict \ninterleaving. To detect con.ict interleavings in multithreaded pro\u00adgrams, then, we detect inter-thread \ncon.ict cycles in the con\u00ad.ict graph of the program. 3.3 Detecting Con.ict Interleavings in SPMD programs \nIn Section 3.2, we noted the equivalence of inter-thread con\u00ad.ict cycles with con.ict interleavings. \nDetecting and remov\u00ading con.ict interleavings is therefore a matter of detecting con.ict cycles in the \nCG of the program. However, such cy\u00adcle detection can be resource-intensive (common techniques for cycle \ndetection are derived from Depth First Search (DFS) algorithms, which have a worst case performance of \nO(|V |+|E|); for a large program with many con.icts, E . V 2). It is possible to optimize this cycle \ndetection process for SPMD programs. This is done by projecting the effects of every thread onto a single \nthread, and then identifying the Strongly Connected Components of the resultant con.ict graph (the resulting \ntime complexity is still O(|V | + |E|), but the number of edges, |E|, is much smaller). To that end, \nwe .rst de.ne a Projection Con.ict Graph (PCG) and then show how the PCG of an SPMD program enables con.ict \ninterleaving detection. DEFINITION 10. Two CSBBs, Bi,Bj . CG are equivalent (written Bi = Bj ) if they \nare isomorphic (contain the same set of edges) and contain the same set of statements. DEFINITION 11. \nLet Bx . CGi and By . CGj.If Bx = By then the projection of By onto Bx is the vertex Bxy . CGi,Bxy = \nBx, with RWxy = RWx . RWy. More precisely, . RWxy = RWx RWx . RWy if Bx = By otherwise Intuitively, \nthe projection of By . CGj onto Bx . CGi merges the read-write sets of the two nodes, but leaves CGi \nstructurally unchanged (i.e. Bxy = Bx). The projection of thread Tj onto thread Ti is the projec\u00adtion \nof the individual CSBBs of CGj onto CGi. 3.3.1 The Projection Con.ict Graph (PCG) DEFINITION 12. The \nProjection Con.ict Graph, or PCG, of an SPMD program with threads T1,T2 ...,TN is the con.ict graph of \nany one thread, say T1, on which the con.ict graphs of the other N-1 threads have been projected. Note \nthat there can be only one PCG for any SPMD pro\u00adgram. To see why this is so, consider an SPMD program \nwith threads T1,T2 ...,TN . Let the PCG for this program be constructed by projecting every thread onto \nthe CG of thread T1. Call this graph PCG.. Note that, by de.nition, PCG. = CG1. Construct another PCG \nby projecting the con.ict graphs of threads T1,T3 ...TN onto the con.ict graph of thread T2 and call \nit PCG... Then PCG.. = CG2. But, because this is an SPMD program, CG1 = CG2 and therefore PCG. = PCG... \nThus, the resultant PCGs are structurally equivalent. The equivalence of the read-write sets may be demonstrated \nin a similar fashion. We can use the PCG to detect con.ict interleavings in the program. The following \ntheorems illustrate how inter-thread con.ict cycles in the CG have equivalent cycles in the PCG: THEOREM \n6. Every inter-thread con.ict edge in the CG of an SPMD program has an equivalent con.ict edge in the \nPCG for the program. Proof: Let the inter-thread con.ict edge occur between nodes Bi . CGi and Bj . CGj \n(that is, (Bi,Bj ) . EC ). In other words, one or more of the following is true: (a) Ri n Wj = \u00d8, (b) \nWi nRj = \u00d8 or (c) Wi n Wj = \u00d8. Without loss of generality, assume condition (a) above is true. Let Bi.,B. \nbe the projections of Bi,Bj onto the PCG. Then, by j de.nition, Ri . Ri . and Wj . Wj.. Thus, if Ri n \nWj = \u00d8 then R. n W . =\u00d8, and therefore (B.,B. ) . EC . ij ij COROLLARY 2. Every inter-thread con.ict \ncycle in the CG of an SPMD program has an equivalent con.ict cycle in the PCG for the program. Proof: \nAn inter-thread con.ict cycle consists of control .ow edges and inter-thread con.ict edges. Control .ow \nedges are always present in the PCG (because the PCG is structurally equivalent to the CGs of the threads). \nFrom Theorem 6, every inter-thread con.ict edge in the CG has an equivalent con.ict edge in the PCG. \nTherefore, every edge in the inter-thread con.ict cycle has an equivalent edge in the PCG, and the cycle \nitself is in the PCG. Figure 5(a) shows the projection con.ict graph of the con.ict graph depicted in \nFigure 4. Here, thread T2 has been projected onto thread T1. The read-write sets have also been shown: \nan RTi indicates a read by thread Ti, while a WTi indicates a write by thread Ti.  3.4 Inserting atomic \nregions into SPMD programs So far we have focused on detecting con.ict interleav\u00adings (CIs). Once CIs \nare detected, the next step is to insert atomic regions that will ensure that the CIs cannot lead to \nnon-con.ict serializable executions. In what follows, we as\u00adsume that the Projection Con.ict Graph (PCG) \nhas been con\u00adstructed.      (a) Projection Con.ict Graph: (b) SCC contraction of PCG Thread T2 projected \nonto T1   Figure 5. The Projection Con.ict Graph and SCC Contrac\u00adtion of the SPMD program of Figure \n3. class Thread ... procedure void run() // run function for the thread atomic region begin() { // Generated \ntransactional region int x = d[0] d[0] = 3 } atomic region end() int y = d[1] ... end procedure end \nclass Figure 6. Program with Atomic Regions Added. We will .rst consider programs without loops, i.e. \nwith\u00adout back-edges. Later we discuss how to insert atomic re\u00adgions into SPMD programs with loops. Corollary \n1 states that eliminating con.ict interleavings will lead to a con.ict-serializable execution run, while \nThe\u00adorem 5 shows that an inter-thread con.ict cycle indicates a con.ict interleaving. Corollary 2 states \nthat every inter\u00adthread con.ict cycle in the CG has an equivalent con.ict cycle in the PCG. Consider \none such con.ict cycle in the PCG. The cycle consists of control .ow edges and con.ict edges. Intuitively, \nthe con.ict cycle exists because two nodes that are transitively reachable via control .ow edges are \nalso connected via a (by de.nition, bidirectional) con.ict edge. Eliminating these con.ict edges will \nthen eliminate the con\u00ad.ict cycle, and will lead to a serializable execution run. Note that because \ncon.ict edges are bidirectional, any node in the con.ict cycle is reachable from every other node in \nthe cycle, thus creating a Strongly Connected Component (SCC). Therefore, to eliminate the con.ict cycle, \nwe contract the SCC into a single node. The contraction is accomplished by merging the individual nodes \nin the SCC, with the merged node containing every statement contained in the individual nodes. The merged \nnode also inherits the read-write sets (and therefore, the con.ict edges to nodes not contained in the \nSCC) of the constituent nodes. Figure 5(b) shows the SCC contraction of the PCG from Figure 5(a). The \ncontracted nodes now constitute the .nal atomic regions in the program. Protecting against mutual access \nacross these regions (e.g., through the use of lock sets or transactional memory implementations) will \nnow prevent all interleavings which could result in a non-serializable sched\u00adule. Figure 6 shows the \nthe input program from Figure 3 annotated with the (automatically generated) atomic region boundary markers. \n  3.5 Using semantic information in our analysis Commutative Operations By de.nition any interleaving \ninvolving commutative operations leaves the program in the same state (and is therefore serializable), \nregardless of whether the memory accesses performed by the operations con.ict. Note that this is true \nonly if we treat the operation in toto and do not decompose the operation into its compo\u00adnent reads and \nwrites (an increment of a shared variable may .rst read the value of that variable and then write the \nincre\u00admented value back into the variable). TransFinder supports commutative operations in three ways \n(i) it detects commu\u00adtative operations on shared memory locations, (ii) it treats such operations in \ntheir entirety (by merging the CSBBs which contain the component reads and writes comprising the operation) \nand (iii) it ensures that no con.ict cycles can form as a result of con.ict edges between any two commuta\u00adtive \noperations by deleting such edges. We currently only de\u00adtect a small class of commutative operations \n(namely, arith\u00admetic increments and decrements). This is suf.cient for our purposes but support for detecting \na larger class of such op\u00aderations (using, e.g., the commutativity analysis of Rinard and Diniz [29]) \ncould be added to TransFinder s analysis, if necessary. Loops Loops have control .ow back-edges. Thus, \neach loop is a strongly connected component. To avoid detect\u00ading and contracting these components, therefore, \nwe avoid traversing control .ow back-edges. In addition, the read\u00adwrite sets of the nodes within a loop \nmay contain accesses to shared arrays whose subscript functions do not cause loop\u00adcarried dependences, \nand other arrays and scalars that lead to loop-carried dependences. These references lead to two kinds \nof con.ict cycles: (a) cross-iteration (or loop-carried) cycles, where the con.ict arises from accesses \nto shared scalars or array references that have loop-carried depen\u00addences, and (b) intra-iteration cycles \non array references in\u00advolved in no loop-carried dependences. SCC detection and contraction resolves \nintra-iteration con.ict cycles. Cross\u00aditeration con.ict cycles remain after SCC contraction; in such \ncases, the generated atomic region boundary markers are hoisted out of the loop so that the entire loop \nis in an atomic region. This is because the con.icts are loop-carried and TransFinder conservatively \nassumes every iteration is a part of the con.ict cycle. Conditional Branches Con.ict cycles occur because \nnodes that are transitively reachable via control .ow edges are also connected by a con.ict edge. The \ncase of conditional branches, such as if or switch statements, must be handled carefully. If the conditional \nbranch is not contained within a loop, or the branch condition is invariant in all enclosing loops, then \nwhen a thread executes statements in one clause of the conditional branch it will never execute statements \nin the other clauses of that instance of the conditional branch since if one branch of the instance is \ntaken then the other is not. Thus, in the compile-time representation of the program (the PCG), no strongly \nconnected component is allowed to form across multiple clauses of such a conditional branch statement, \nand an atomic region will either span the entire branch (because a statement external to the branch con.icts \nwith a statement contained within the branch) or will be con\u00ad.ned to the statements within the individual \nclauses. When the conditional branch is within one or more loops and the branch condition is not loop \ninvariant, a thread may execute different clauses of the conditional branch in different itera\u00adtions \nof the loop. In this case statements in different clauses of a conditional branch are transitively reachable \nfrom each other along control .ow edges. To handle this case, Trans-Finder allows the traversal of control \n.ow back-edges of the loop when detecting SCCs whenever the compiler can\u00adnot guarantee the thread will \nexecute only one clause of the branch within the loop. Traversing the back-edge ensures that every statement \nin the loop is transitively reachable from every other statement, and allows statements in the different \nclauses of the conditional branch to be reachable from one another. 4. Implementation The ideas presented \nin Section 3 have been implemented in TransFinder and evaluated. TransFinder is a source-to\u00adsource compiler \nwhose input is an SPMD shared memory Java program and whose output is C++ code that has been annotated \nwith atomic regions inserted using the analyses and optimizations of Section 3. We now describe the imple\u00admentation \nof TransFinder. We .rst describe, in Section 4.1, the escape analysis that TransFinder uses to conservatively \ndetermine what variable references in the program are to storage that is accessed in more than one thread, \nand how the results of this analysis are combined with the results of Section 3 to build a PCG. We then \ndescribe situations that lead to TransFinder insert\u00ading atomic regions that are different than the hand-coded \nre\u00adgions, and how TransFinder can help the programmer gen\u00aderate better atomic regions in these cases. \n4.1 Escape analysis and detecting atomic regions The TransFinder compiler uses the points-to analysis \nand code of [35], which builds upon the SPARK analysis [24] procedure GenCG(CFG) input CFG: Control \nFlow Graph for the program returns cg: Con.ict Graph cg = new CG( ) curr = new CSBB( ), prev = NULL for \neach Node Ni . CFG do if Ni does not contain a shared memory access then add all statements in Ni to \ncurr else add curr to cg if prev . = NULL then add program edge (prev, curr) to cg end if prev = curr, \ncurr = new CSBB( ) end if end for return cg end procedure procedure ConstructPCG(CG, L) input CG: Con.ict \nGraph for the program input L: List of Threads returns pcg: Projection Con.ict Graph of the program CG1 \n= CG of thread T1 for each Thread Ti . L, 1 <i = N do CGi = CG of thread Ti for each CSBB Ni . CGi do \n Project Ni onto CG1 end for end for return CG1 end procedure procedure ConstructCG(CFG, L, T) input \nCFG: Control Flow Graph for the program input L: List of allocation sites input T: List of Threads returns \ncg: Con.ict Graph cg = GenCG(CFG) for each ai . L do TW = list of CSBBs in CG writing to ai TR = list \nof CSBBs in CG reading from ai for each wi . TW do for each ri . TR do add con.ict edge (wi,ri) to cg \nend for end for end for return cg end procedure procedure doSCC(PCG) input PCG: Projection Con.ict Graph \nof program // Perform SCC detection and contraction stack = new Stack( ) root = root of PCG SCC = tarjan(stack, \nroot, 0) for each List L . SCC do // Merge all nodes into a single node // This node is now the SCC contraction \nend for end procedure procedure TransFind(CFG, L, T) input CFG: Control Flow Graph of the program input \nL: List of allocation sites input T: List of threads cg = ConstructCG(CFG, L, T) pcg = ConstructPCG(cg, \nL) doSCC(pcg) end procedure Figure 7. Algorithm to identify atomic regions in code. found in Soot [36]. \nThe points-to analysis annotates allo\u00adcation sites with the various contexts where they are read or written. \nThe TransFinder compiler uses these read-write sets to perform an escape analysis as follows: if multiple \nthreads have entries in the read-write sets of an allocation site then that site is considered to have \nthread-escaped. After the compiler has determined thread-escaping locations, the Con.ict Graph (CG) is \nconstructed from the Control Flow Graph (CFG) by using the thread-escape and read-write in\u00adformation \nobtained from the earlier phases to construct con\u00ad.ict edges between the various Concurrent Shared Basic \nBlocks (CSBBs). The Projection Con.ict Graph (PCG) is then constructed and SCC detection and contraction \nproceed as described in Section 3.4. We use a modi.cation of Tar\u00adjan s algorithm to detect SCCs. We modify \nthe algorithm by considering only those previously visited successors of a given node which are connected \nto the node via a con\u00ad.ict edge. Figure 7 gives the complete algorithm for iden\u00adtifying an atomic region \n(the modi.ed Tarjan s algorithm is not shown). In the interest of brevity, we omit the special cases \nmentioned in Section 3.5 from the algorithms.  4.2 Program tuning and optimization The goal of TransFinder \nis to facilitate the insertion of op\u00adtimized atomic regions or locks by identifying sequences of statements \nthat must be protected against concurrent access to ensure con.ict-serializable execution runs. In this \nsection we discuss TransFinder s utility when the identi.ed regions are different from those that a skilled \nprogrammer might identify. TransFinder s atomic regions are smaller than manually identi.ed atomic regions \nIn some programs TransFinder identi.es atomic regions that are too .ne grained, leading to too-large \noverheads from starting and committing the trans\u00adactions, or acquiring and releasing the locks used to \nenforce the atomic region. In these situations, TransFinder assists the programmer by focusing attention \non the small percent\u00adage of program statements that likely should be protected by atomic regions, allowing \nthe programmer to incrementally adjust and merge the suggested regions to create a correct and ef.cient \nprogram. TransFinder s atomic regions are larger than manually identi.ed atomic regions TransFinder s \natomic regions are sometimes larger than the hand-coded atomic regions, particularly when memory accesses \nwith inter-thread con\u00ad.icts create loop carried dependences. In this case, Trans-Finder will hoist the \natomic region out of the loop and en\u00adclose the entire loop in the atomic region, as described in Section \n3.5. The atomic regions can be overly conservative for three reasons. First, the alias, escape and dependence \nanalysis used by TransFinder are, like all such analyses, con\u00adservative, and thus the loop-carried dependence \nmay not ac\u00adtually exist. Second, the semantics of the computation real\u00adized by the program may allow \n(or require) the accesses of for i =1 to numOps do op = opType[i] transaction begin( ) if op == A then \n{ transaction begin( ) for i =1 to numOps do { op = opType[i] // Perform op A if op == A then }// Perform \nop A transaction end( ) else else // Perform op B transaction begin( ) end if {end for // Perform op \nB }} transaction end( ) transaction end( ) end if end for (a) Atomic regions considering (b) Atomic regions \nignoring back\u00adback-edges edges Figure 8. Benchmark tuning example taken from the Vaca\u00adtion benchmark. \ndifferent iterations to be in different atomic regions. Third, the computations performed by different \niterations may be commutative, but this commutativity may not be recognized by TransFinder. As a heuristic \nto give the programmer insights into the behavior of the program, TransFinder .rst inserts atomic regions \ninto the entire program and presents the results to the user. Next, TransFinder can optionally ignore \nthe effects of the loop back-edges, starting with the outermost loop, then the outermost two loops, and \nso forth until all loops are ignored. The programmer is then presented with the atomic regions that result \nfrom each of these cases. By examining the identi.ed atomic regions the programmer can see the effect \nthat the different loops have on the atomic regions. From these results, the programmer can either pick \nthe set of atomic regions that properly re.ects the program semantics, or can incrementally adjust the \natomic regions using domain speci.c semantic knowledge about the computation. The Vacation benchmark \nin the STAMP benchmark suite [6] exhibits this behavior. A succession of operations on a database are \nsimulated by a loop that picks, at random, different operations on different data. Because the data in\u00adduces \na loop-carried dependence, TransFinder initially gen\u00aderates an atomic section that encloses the entire \nloop, as shown in Figure 8(a). By ignoring the back-edge for the loop, the atomic regions in Figure 8(b) \nare produced, and are exactly those present in the original, hand-transformed code. 5. Experimental Evaluation \nThis section presents the experimental evaluation of Trans-Finder, including the experimental methodology, \nquantita\u00adtive performance results, and detailed analysis. 5.1 Methodology The strategies discussed in \nthis paper are evaluated in two ways: by comparing the similarity of the generated atomic regions to \nhand-generated versions and by comparing the performance of the programs using automatically-generated \natomic regions with respect to programs with hand-coded atomic regions. We use the Stanford Transactional \nApplica\u00ad tions for Multi-Processing (STAMP) benchmark suite and the multithreaded version of the Java \nGrande benchmark suite for our experiments [6, 15]. The STAMP suite is a set of benchmarks originally \nde\u00ad signed to evaluate the ef.cacy of various transactional mem\u00ad ory implementations. However, its use \nof atomic regions provides an objective way to evaluate the ability of Trans- Finder to identify atomic \nregions. The original STAMP benchmarks are C programs synchronized with explicit atomic regions. TransFinder, \non the other hand, accepts as input un-annotated Java code. Accordingly, we modify the original STAMP \nbenchmark programs by stripping out the begin and end statements of the various atomic regions and manually \nconverting the code to Java, before feeding it to TransFinder. The STAMP benchmark suite consists of: \n(1) Genome, a gene sequencing program; (2) Intruder, a network intru\u00ad sion detection program; (3) Kmeans, \na K-means clustering program1; (4) Labyrinth, a maze routing program; (5) Va\u00ad cation, a travel reservation \nprogram; (6) Bayes, a bayesian network learning algorithm implementation2 (7) Yada,an implementation \nof a Delauney mesh re.nement algorithm and (8) Ssca2, a program which implements Kernel 1 of the Scalable \nSynthetic Compact Applications 2 (SSCA2) graph\u00ad based benchmark [2]. The Java Grande benchmark suite \nis a collection of low\u00ad level kernels and applications for scienti.c and numerical computing. We use \nthe suite s three large-scale applica\u00ad tions to test the applicability and performance of our sys\u00ad tem \non non-transactional programs. The three Java Grande programs considered were: (1) Moldyn, a molecule \ndynam\u00ad ics simulation; (2) Raytracer, a ray-tracing simulation and (3) Montecarlo, a Monte Carlo simulation. \nThe Moldyn and Montecarlo codes do not use synchronized blocks, rely\u00ading on a careful thread-local partitioning \nof data to ensure con.ict-serializability we use these codes to determine if TransFinder generates unnecessary \natomic regions for large codes. For Raytracer, we manually remove all synchronized blocks and regions \nbefore inputting the code into Trans-Finder, just as with the STAMP benchmarks. Similarity of the TransFinder \ngenerated programs to the original Native programs is measured in three ways. First, we give the number \nof atomic regions that are present in both forms of the program. Second, we give the total number of \nlines contained within atomic regions in each program. Third, we give the percentage of code, i.e. the \npercentage of the lines of program code, within atomic regions. Only non\u00adcomment lines of code are used \nin our measurements, and we used the sclc line-counting tool [4] for measuring the non\u00adcomment, source \ncode size of the generated atomic regions with respect to the original program. For the TransFinder versions \nof the STAMP benchmarks, line number measure\u00adments were conducted on the generated C++ code. For the \nJava Grande benchmarks, we .rst analyzed the programs 1We converted the TL2 version of Kmeans to use \ndouble precision arithmetic because of a bug in TL2 which caused spurious writes to successive float \narray elements within tight loops. 2We use the CM DELAY contention manager for the TinySTM versions of \nBayes; the default contention manager caused an inordinate number of con.icts in the TransFinder version \nof the benchmark. Applications Program Version # static atomic regions total lines in atomic regions \n% of code in atomic regions Bayes Native 15 52 1.70 TransFinder 5 80 2.62 Genome Native 5 50 3.91 TransFinder \n5 44 3.44 Intruder Native 3 8 0.63 TransFinder 2 13 1.02 Kmeans Native 3 14 2.26 TransFinder 4 12 1.94 \nLabyrinth Native 3 18 1.75 TransFinder 3 18 1.75 Ssca2 Native 3 21 0.61 TransFinder 5 18 0.53 Vacation-untuned \nNative 3 83 4.71 TransFinder 1 121 6.87 Vacation-tuned Native 3 83 4.71 TransFinder 3 83 4.71 Yada Native \n6 11 0.69 TransFinder 6 9 0.57 Moldyn Native 0 0 0 TransFinder 0 0 0 Monte carlo Native 0 0 0 TransFinder \n0 0 0 Raytracer Native 1 2 0.29 TransFinder 1 2 0.29 Table 1. Comparison of atomic regions generated \nby TransFinder to atomic regions in the original benchmarks. with TransFinder and then manually mapped \nthe indicated atomic regions (if any) onto the original (Java) programs to conduct line number measurements. \nThe performance tests in this paper use software trans\u00adactional memory (STM) as the concurrency control \nmech\u00adanism. We consider three such STM systems: the TL2 sys\u00adtem [11], the TinySTM system [16] and the \nSwissTM sys\u00adtem [12]. Each application that includes atomic regions is evaluated with two different versions: \na version that uses the default atomic regions in the benchmarks (called Na\u00adtive ) and a version that \nuses the atomic regions generated by TransFinder (called TransFinder ). All experiments are performed \non a Dell Poweredge 2950 server with two 1.8 GHz quad-core Intel Xeon E5320 pro\u00adcessors based on the \nCore 2 microarchitecture (8 cores to\u00adtal). This system has 16 GB of system RAM and 4 MB L2 caches shared \nbetween pairs of processors. This sys\u00adtem runs Linux kernel 2.6.18 and GNU C library 2.3.6 with the Native \nPOSIX Threads Library in the Debian AMD64 distribution. All C/C++ programs were compiled using the GNU \ngcc compiler at optimization level: -O3. We used Java Grande Thread Version 1.0 and STAMP version 0.9.10 \nfor the benchmarks. For the STM backends we used version 0.9.6 of the x86 port of TL2, version 0.9.9 \nof TinySTM and version 2009-09-10 of SwissTM.  5.2 Analysis Running Times Table 2 details the analysis \ntimes for the TransFinder tool for the eight STAMP benchmarks considered. The TransFinder\u00adspeci.c phase \nof the analysis took a negligible amount of Applications Alias Analysis Time (secs) TransFinder-speci.c \nTime (secs) Bayes 23.2777 5.772 Genome 12.21067 1.36233 Intruder 11.676033 0.488667 Kmeans 10.421033 \n0.370667 Labyrinth 16.48837 2.86633 Ssca2 13.8843 8.734 Vacation-untuned 22.171 6.36667 Vacation-tuned \n15.89603 7.15667 Yada 15.20433 1.85067 Table 2. Analysis times for the TransFinder tool time for the \nthree Java Grande programs and they are not presented here. The .rst column gives the name of the appli\u00adcation. \nThe second column depicts the average time taken by the alias analyser to perform the points-to analysis \nof [35], while the last column shows the average time taken to per\u00adform the phases of the analysis speci.c \nto TransFinder (in\u00adcluding the construction of the PCG and the SCC detection and contraction phases). \nAs can be seen, the TransFinder\u00adspeci.c phase of the analysis incurs only a minimal over\u00adhead, ranging \nfrom a minimum of 0.37 seconds (Kmeans) to a maximum of 7.16 seconds (Vacation Tuned). In addition, the \nloop elision technique described in Section 4.2 adds an average overhead of less than 1 second. 16 \n18  70 14 16 60 12 14 50 Time (secs) Time (secs) Time (secs) Time (secs) Time (secs) Time (secs) 10 \n12 40 8 10 30 6 8 20 4 6 10 0 4 2 0 1 2 4 8 1 2 4 8 1 2 4 8 # Threads # Threads # Threads (a) Genome(b) \nIntruder(c) Kmeans 5.5 7 5.5 5 56   4.5 4 3.5 3 2.5 2 4.5 4 3.5 3 2.5 2 5 4 3 2 1 1.5 1.5 1 0 1 \n# Threads # Threads # Threads (d) Labyrinth(e) Vacation Tuned(f) Yada # Threads # Threads (g) SSCA2(h) \nBayes (i) Legend Figure 9. STAMP benchmark performance and scalability results of the various STM backends. \n 5.3 Code Quality Results Table 1 gives a measure of the effectiveness of TransFinder in determining \natomic regions. The .rst two columns give the name and version of the program for which metrics are provided \nin the other columns. The Native version is the original benchmark, and the TransFinder version is the \nbenchmark transformed by TransFinder. The # static atomic regions column presents the number of atomic \nregions in the Native and TransFinder versions of the programs. The total lines in atomic regions column \npresents the total number of non-comment source lines of code in the atomic regions for each of the versions \nof the benchmark programs, while the % of code in atomic regions column expresses this number as a percentage \nof the total (non-library) lines of code in the respective programs. The three applications at the bottom \npresent results for the three Java Grande benchmarks considered. All three are in 100% agreement with \nthe original atomic regions. Among the STAMP benchmarks, Labyrinth shows the best result, with the TransFinder \nversion of the benchmark having the same number of atomic regions as the Native version. In addition, \neach of these regions has exactly the same code in both versions. The TransFinder version of Intruder, \non the other hand, generates a smaller number of larger atomic blocks than are present in the Native \nversion of the bench\u00admark. A closer inspection reveals that conservatism in the thread-escape analysis \nleads to an over-estimation of the size of the generated atomic regions. Genome, Kmeans, Yada and Ssca2 \nare interesting cases: the generated atomic regions are, on average, actually smaller than those in the \noriginal STAMP benchmarks. In the case of Genome, an atomic re\u00adgion which should properly contain only \na single hashmap insertion within a loop has been hoisted out of the loop by the benchmark designers, \npresumably to minimize trans\u00adaction startup and shutdown times. This pattern recurs in Ssca2, where three \nseparate atomic blocks containing code accessing disjoint memory locations have been agglomer\u00adated into \na single atomic block, and in Yada, where atomic blocks containing accesses to two disjoint memory locations \nhave been agglomerated into a single block. In Kmeans, the original benchmark encloses an entire inner \nloop within an atomic region, even though the shared memory accesses in different loop iterations are \nindependent of each other. TransFinder, on the other hand, recognizes the iterations as independent and \nthus generates atomic regions only large enough to enclose the shared memory accesses within a sin\u00adgle \niteration. As in the other benchmarks, the atomic regions in the original benchmark version seem to have \nbeen coded to reduce the transaction startup and shutdown time.  In the two remaining STAMP benchmarks, \nVacation and Bayes, the number of atomic regions indicated by Trans-Finder was less than the number of \nsuch regions in the original programs. Of these, the least similar was Vacation, where the TransFinder \nversion of the benchmark agglomer\u00adated three separate atomic regions into a single block (al\u00adthough even \nin this case, the absolute difference in the size of the atomic regions between the two versions of the \nbench\u00admark is only 38 lines). Vacation, however, is amenable to the tuning techniques detailed in Section \n4.2. Accordingly, we present two results for Vacation: a version which uses atomic regions generated \nafter the outermost loop has been elided using the methodology described in Section 4.2 (called Va\u00adcation \nTuned) and a version which uses the atomic regions generated without the benchmark tuning (called Vacation \nUntuned). As we can see, TransFinder can properly iden\u00adtify benchmark artifacts that preclude correct \nidenti.cation of independent atomic regions, and can suggest alternatives with a high degree of accuracy: \nthe tuned version of the benchmark is 100% conformant with the STAMP version. The number of atomic blocks \nin the TransFinder version of the Bayes benchmark is a third of the number present in the original benchmark. \nA closer inspection of the code reveals the cause: two switch statements (with common case expres\u00adsions) \ncause multiple con.icting statements to be transitively reachable from each other. This causes the SCC \ncontraction phase to agglomerate 11 different atomic regions into a sin\u00adgle, large atomic block, and \nserves to illustrate a shortcom\u00ading in this, and indeed, any static analysis, namely, an in\u00adability to \nincorporate complex semantic information in the decision making process. The user-synchronized code, \non the other hand, takes advantage of application-speci.c in\u00adformation to break up the large block of \ncode into multiple, (semantically) independent chunks. The differences in the Native and TransFinder \nversions are also shown by providing the percentage of each version of the program that is contained \nin atomic blocks. The per\u00adcentages and the absolute number of lines in atomic blocks show that even when \nTransFinder s code differs from the hand-tuned code, both the absolute and relative amount of code that \nneeds to be examined by the programmer when doing performance tuning of atomic regions is small.  5.4 \nPerformance Results Figure 9 presents experimental results showing the perfor\u00admance and scalability of \nthe eight STAMP benchmarks un\u00adder each of the implementations described above. We do not show the Java \nGrande benchmarks because, as discussed in Section 5.3, the TransFinder code and original code are both \nidentical. In each case, the X axis represents the number of threads while the Y axis shows the execution \ntime, in seconds, on the given platform. We note that numbers for Vacation Un\u00adtuned are not presented: \nas discussed in Section 4.2, the presence of loop-carried dependences results in a single, se\u00adrialized \nblock. Accordingly, the results for Vacation use the loop elision technique of Section 4.2, as described \nabove. In six of the benchmarks (Genome, Intruder, Labyrinth, Vaca\u00adtion, Yada, and Ssca2), the performance \nof the TransFinder versions that use TinySTM and SwissTM for concurrency control closely track that of \nthe respective Native versions. This is to be expected, since the generated atomic regions have a high \ndegree of conformity to the default atomic re\u00adgions in the original STAMP benchmarks (the least accurate, \nIntruder, generates an atomic region which is only 5 lines larger than the original). In addition, the \nTransFinder-TL2 version of the benchmark performs similarly to the Native version for Intruder, Labyrinth, \nVacation and Yada. In the case of Genome and Ssca2, however, the TransFinder-TL2 version does not scale \npast 4 threads. A detailed analysis of Genome revealed high overheads for a transaction contain\u00ading a \nsingle hashmap insertion call within a loop: in the origi\u00adnal version of the benchmark the transaction \nis hoisted out of the loop, thus minimizing transaction startup and shutdown times. Similarly, an analysis \nof Ssca2 revealed higher over\u00adheads due to the added number of transaction startups and shutdowns in \nthe TransFinder generated code as discussed in Section 5.3. The TransFinder versions of the Kmeans bench\u00admark \ndid not scale well because of the added overheads of the extra transaction begin and end function calls \n(the Trans-Finder versions execute 32 pairs of these calls for each pair executed by the Native versions). \nWe do not present TL2 results for Bayes here: both the Native-TL2, and the TransFinder-TL2 versions of \nthe benchmark terminated with a segmentation fault. Figure 9(h) shows the results of the other versions \nof the benchmarks. In general, the TransFinder versions of the benchmark run slower than the corresponding \nnative versions. This is likely due to the agglomeration of multiple atomic regions into a single, large \natomic block, as described in Section 5.3: an analysis revealed a large number of con.icts within this \nre\u00adgion. It should be noted, however, that the TransFinder ver\u00adsions follow the same trend as the Native \nversions of the benchmark. Where there are differences in either code quality or per\u00adformance between \nthe results of TransFinder and the origi\u00adnal benchmarks, these differences stem primarily from situ\u00adations \nwhere benchmarks have been tuned for performance by using atomic regions that are not the minimal set \nof state\u00adments that must be protected against concurrent access. In these situations, TransFinder enables \nthe programmers tun\u00ading efforts to be focused on only the small part of the pro\u00adgram that must be covered \nby atomic regions. The program\u00admer can then use additional semantic knowledge about the program to determine \nif those generated atomic regions can be altered without compromising correctness. These ideas form the \nbasis of the TransFinder approach to generating atomic regions: .rst automatically producing atomic regions \nwith con.ict-serializability, and then allowing the program\u00admer to focus code tuning efforts only on \nthose regions that appear to be bottlenecks. 6. Related Work Vechev et al. have also attempted to analyze \nshared mem\u00adory programs with a view to enforcing whole-program cor\u00adrectness criteria [37, 38]. In [37] \nthe authors parameterize (.nite-state) programs through the use of an n-tuple char\u00adacterized by the values \nof the various shared variables, and the program counters of the various threads participating in the \nprogram. The authors then enumerate all possible pro\u00adgram states and attempt to successively remove states \nwhich would cause an incorrect program condition, for some (user\u00adde.ned) correctness criterion. The authors \nextend this ap\u00adproach in [38] by allowing in.nite-state programs (pro\u00adgrams which may iterate inde.nitely). \nThis work may be viewed as a program veri.cation tool which veri.es pro\u00adgrams under a (user-supplied) \ncorrectness criterion and, if necessary, modi.es programs (by introducing atomicity con\u00adstraints) to \nconform to the correctness conditions. The ap\u00adproach is similar to their previous work in that programs \nare represented as a set of states, and invalid transitions between these states (more properly, transitions \nwhich may lead to in\u00advalid interleavings) are eliminated. However, there are some signi.cant differences: \nthe program states are parameterized through the use of various abstractions (more precisely, they use \nabstract representations to parameterize the shared vari\u00adable values in the state tuples). This allows \nthem to ver\u00adify in.nite-state programs. Moreover, at every stage, when faced with a possibly invalid \nprogram state, the algorithm chooses to either (a) remove the underlying interleaving by adding atomicity \nconstraints to an (evolving) atomicity for\u00admula or (b) re-verify the program under a new, more re.ned \nabstract representation. Our approach differs from these works in the follow\u00ading, signi.cant ways: (1) \nthey use a user-supplied correct\u00adness condition, whereas we rely on con.ict-serializability as our (.xed) \nconsistency criterion and (2) they use an enu\u00admeration of program states, each characterized via the \nuse of an abstract representation, whereas we rely on cycles in a Con.ict Graph to determine violations \nof serializability. Providing a user-supplied whole-program correctness con\u00addition has the advantage \nof generality: their approach works equally well whether analyzing the program with a view to\u00adwards enforcing \nserializability, or some other correctness condition. However, the speci.c bene.ts to this approach are, \nin our opinion, somewhat muted by the fact that the onus of responsibility for providing the (sometimes \nconvo\u00adluted) correctness criteria now falls on the end-users. Indeed, [38] cites cases where these conditions \nwere too long and tedious to reproduce. Also, using a set of program states allows them to exercise greater \ncontrol over the granular\u00adity of the various correctness criteria, but leads to a larger search space \nof invalid interleavings (possibly exponential in the number of shared variables and threads), whereas \nour Projection Con.ict Graph approach takes up no more space than is required to express the control \n.ow of a single thread. In addition, these works target numerical programs, whereas our approach applies \nto general shared memory parallel pro\u00adgrams. The problem of serializability-preserving transaction op\u00adtimization \nhas received much attention in the database com\u00admunity [1, 3, 5, 31, 40]. Of these, the work most relevant \nto this paper is the transaction chopping approach of Shasha et al. [31], in which the authors attempt \nto reduce (or chop ) the individual transactions in a database program while guar\u00adanteeing serializable \nexecutions. They do so by identify\u00ading a set of primitive database operations, by determining the resultant \nconnected components and then by enforcing a precedence order to determine the .nal chopping. However, \ntheir work, unlike ours, targets database transactions and not general shared memory programs. Also, \ntheir tool operates on programs with known transactional regions, unlike ours. Finally, their work is \npredicated on the assumption that the user of the tool can characterize all of the transactions that \nmay run in some time interval, whereas we make no such assumption (indeed, it can be argued that the \nproblem of a priori transactional region identi.cation in shared memory parallel programs is hard precisely \nbecause of the lack of such semantic information). There exists a rich body of work exploring the problem \nof concurrency control in shared memory programs. Given some user-identi.ed atomic scopes, researchers \nhave pro\u00adposed solutions to ef.ciently guard against concurrent mod\u00adi.cations in those sections, either \nby providing mutual ex\u00adclusion (compiler-directed lock generation [7, 14, 17, 21, 25, 35, 41]) or by \nusing optimistic techniques (transactional memory [18 20, 26, 33, 34]). We view these approaches as being \ncomplementary to ours: the techniques discussed in this paper may be used to identify the atomic regions \nthat must be guarded by the various concurrency control mecha\u00adnisms. Shasha and Snir [32] provide a qualitative \ngraph-based analysis of the requirements for enforcing sequential con\u00adsistency in shared memory programs \nunder a given set of atomicity constraints. Given a shared memory program, their work attempts to de.ne \nthe set of delays and locks which must be used to enforce sequential consistency. Krishna\u00admurthy and \nYelick [22] present a set of optimizations for SPMD programs that extend the delay set analysis of Shasha \nand Snir by incorporating synchronization analysis and op\u00adtimizing the resultant cycle analysis for SPMD \nprograms. Both of these works detect violations of sequential consis\u00adtency in shared memory programs \nby detecting cycles in their underlying con.ict graphs. In addition, Krishnamurthy and Yelick improve \nthe accuracy of the delay set analysis by incorporating additional information from an analysis of the \nsynchronization constructs in the program. They, like us, are optimizing based on a con.ict graph of \na shared mem\u00adory SPMD program. We differ in two signi.cant ways: .rst, they project all threads onto \ntwo, rather than one, thread, and second, their goal is to detect violations of sequential consis\u00adtency, \nwhich is a weaker correctness criterion than con.ict\u00adserializability. Moreover, they rely on user-supplied \ntransac\u00adtional boundary information rather than attempting to auto\u00admatically discover this information. \nData race detection tools [8, 9, 25, 27, 28, 30] analyze programs to determine data races: unsynchronized \naccesses to the same memory location by more than one thread, where at least one access is a write. They \ndo so by using static analyses, at runtime, or both. Unlike our work, none of these tools attempts to \nenforce any whole-program correctness criteria such as serializability. Our use of the con.ict graph \nis inspired by the Concurrent Control Flow Graph of Lee et al. [23]. In that work, the authors present \na mechanism for extending the SSA form of Cytron et al. [10] to parallel shared memory programs. They \nthen show how the resultant Concurrent Static Single Assignment (CSSA) form can be used for optimizations \nsuch as constant propagation in parallel programs. They do not, however, optimize the .ow graph for SPMD \nprograms. Nor do they attempt to discover atomic regions in the code. 7. Conclusions This paper presented \nan effective method for identifying atomic regions in multithreaded SPMD programs. The pa\u00adper systematically \nmapped the con.ict-serializability prob\u00adlem onto the space of multithreaded programs by qualita\u00adtively \ndemonstrating the equivalence of con.ict graphs and precedence graphs. It then presented optimizations \nof con\u00ad.ict graphs for SPMD programs and demonstrated how, in this special case, it is possible to project \nthe effects of multi\u00adple threads onto a single thread, thus reducing the space and time requirements \nfor the atomic region identi.cation pro\u00adcess. Implementing these ideas allowed us to reproduce the atomic \nregions with 100% accuracy in the three Java Grande benchmarks considered. The generated atomic regions \nwere also either equal to or smaller than the default atomic re\u00adgions in a majority of the STAMP benchmarks, \nand were never more than 38 lines larger in the remaining STAMP benchmarks. In addition, this paper also \npresented a method\u00adology to aid in benchmark tuning via the use of iterative loop elision and demonstrated \nhow this technique is partic\u00adularly helpful in situations where benchmark design dictates the placement \nof multiple independent atomic regions within large loops. In general, atomic region identi.cation will \nalways in\u00advolve some programmer effort due to the semantic nature of the problem. However, we have demonstrated, \nfor the .rst time, how it is possible to automatically identify atomic re\u00adgions in multithreaded SPMD \nprograms, using only an un\u00adannotated program as input, with a surprising degree of ac\u00adcuracy for a varied \nclass of benchmarks. Much work still needs to be done in this area but we believe that, with the ideas \nmentioned in this paper, we have provided an impor\u00adtant stepping stone in the process. References [1] \nD. Agrawal, J. L. Bruno, A. El Abbadi, and V. Krishnaswamy.Relative serializability (extended abstract): \nan approach for relaxing the atomicity of transactions. In PODS 94: Pro\u00adceedings of the ACM Symposium \non Principles of Database Systems, pages 139 149, New York, NY, USA, 1994. ACM. [2] D. A. Bader and K. \nMadduri. Design and implementation ofthe hpcs graph analysis benchmark on symmetric multipro\u00adcessors. \nIn HiPC 05: Proceedings of the High Performance Computing Conference, pages 465 476, 2005. [3] R. Bayer. \nConsistency of transactions and random batch. ACM Transactions on Database Systems, 11(4):397 404, 1986. \n [4] Brad Appleton. Sclc and Cdiff: Perl scripts for ClearCase. At http://www.cmcrossroads.com/ broadapp/clearperl/sclc-cdiff.html. \n[5] M. J. Cahill, U. R\u00a8 ohm, and A. D. Fekete. Serializable isolation for snapshot databases. In SIGMOD \n08: Proceedings of the ACM International Conference on Management of Data, pages 729 738, New York, NY, \nUSA, 2008. ACM. [6] C. Cao Minh, J. Chung, C. Kozyrakis, and K. Oluko\u00adtun. STAMP: Stanford transactional \napplications for multi\u00adprocessing. In IISWC 08: Proceedings of The IEEE Interna\u00adtional Symposium on Workload \nCharacterization, September 2008. [7] S. Cherem, T. M. Chilimbi, and S. Gulwani. Inferring locksfor atomic \nsections. In PLDI 08: Proceedings of the ACM Conference on Programming Language Design and Imple\u00admentation, \npages 304 315, 2008. [8] J.-D. Choi, K. Lee, A. Loginov, R. O Callahan, V. Sarkar, and M. Sridharan. \nEf.cient and precise datarace detection formultithreaded object-oriented programs. In PLDI 02: Pro\u00adceedings \nof the ACM Conference on Programming Language Design and Implementation, pages 258 269, New York, NY, \nUSA, 2002. ACM. [9] M. Christiaens and K. De Bosschere. Trade, a topologicalapproach to on-the-.y race \ndetection in Java programs. In JVM 01: Proceedings of the 2001 Symposium on JavaTM Virtual Machine Research \nand Technology Symposium, pages15 15, Berkeley, CA, USA, 2001. USENIX Association. [10] R. Cytron, J. \nFerrante, B. K. Rosen, M. N. Wegman, and F. K.Zadeck. Ef.ciently computing static single assignment formand \nthe control dependence graph. ACM Transactions on Programming Languages and Systems, 13(4):451 490, 1991. \n[11] D. Dice, O. Shalev, and N. Shavit. Transactional Locking II.In Proceedings of the International \nSymposium on Distributed Computing, pages 194 208, 2006. [12] A. Dragojevi\u00b4c, R. Guerraoui, and M. Kapalka. \nStretchingtransactional memory. In PLDI 09: Proceedings of the ACM Conference on Programming Language \nDesign and Imple\u00admentation, pages 155 165, New York, NY, USA, 2009. ACM. [13] R. Elmasri and S. B. Navathe. \nFundamentals of Database Systems (5th Edition). Addison-Wesley Longman Publishing Co., Inc., Boston, \nMA, USA, 2006. [14] M. Emmi, J. S. Fischer, R. Jhala, and R. Majumdar. Lock allocation. In POPL 07: Proceedings \nof the ACM Symposium on Principles of Programming Languages, pages 291 296, New York, NY, USA, 2007. \nACM. [15] EPCC. The Java Grande Forum Benchmark Suite. At http://www.epcc.ed.ac.uk/research/java-grande/. \nLast ac\u00adcessed March 24, 2010. [16] P. Felber, C. Fetzer, and T. Riegel. Dynamic performance tun\u00ading \nof word-based software transactional memory. In PPoPP 08: Proceedings of the ACM Symposium on Principles \nand Practice of Parallel Programming, pages 237 246, New York, NY, USA, 2008. ACM. [17] R. L. Halpert, \nC. J. Pickett, and C. Verbrugge. Component\u00adbased lock allocation. In PACT 07: Proceedings of the In\u00adternational \nConference on Parallel Architectures and Compi\u00adlation Techniques, pages 353 364, Los Alamitos, CA, USA,2007. \nIEEE Computer Society. [18] T. Harris, S. Marlow, S. Peyton-Jones, and M. Herlihy. Com\u00adposable memory \ntransactions. In PPoPP 05: Proceedings of the Tenth ACM SIGPLAN Symposium on Principles and Prac\u00adtice \nof Parallel Programming, pages 48 60, New York, NY, USA, 2005. ACM Press. [19] M. Herlihy, V. Luchangco, \nM. Moir, and W. N. Scherer. Soft\u00adware transactional memory for dynamic-sized data structures.In PODC \n03: Proceedings of the Symposium on Principles of Distributed Computing, pages 92 101, New York, NY, \nUSA, 2003. ACM Press. [20] M. Herlihy, J. E. B. Moss, J. Eliot, and B. Moss. Transactionalmemory: Architectural \nsupport for lock-free data structures. In Proceedings of the 20th Annual International Symposium on Computer \nArchitecture, pages 289 300, 1993. [21] M. Hicks, J. S. Foster, and P. Prattikakis. Lock inference foratomic \nsections. In Proceedings of the First ACM SIGPLAN Workshop on Languages, Compilers, and Hardware Support \nfor Transactional Computing, June 2006. [22] A. Krishnamurthy and K. Yelick. Analyses and optimizationsfor \nshared address space programs. Journal of Parallel and Distributed Computing, 38(2):130 144, 1996. [23] \nJ. Lee, D. A. Padua, and S. P. Midkiff. Basic compiler algo\u00adrithms for parallel programs. In PPoPP 99: \nProceedings of the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, pages 1 \n12, 1999. [24] O. Lhot\u00b4ak and L. Hendren. Scaling Java points-to analysisusing Spark. In G. Hedin, editor, \nProceedings of the Interna\u00adtional Conference on Compiler Construction, volume 2622 of LNCS, pages 153 \n169, Warsaw, Poland, April 2003. Springer. [25] B. McCloskey, F. Zhou, D. Gay, and E. Brewer. Autolocker: \nsynchronization inference for atomic sections. In POPL 06: Proceedings of the ACM Symposium on Principles \nof Pro\u00adgramming Languages, pages 346 358, New York, NY, USA, 2006. ACM. [26] M. Moir, K. Moore, and D. \nNussbaum. The adaptive trans\u00adactional memory test platform: a tool for experimenting withtransactional \ncode for rock (poster). In SPAA 08: Proceedings of the Symposium on Parallelism in Algorithms and Architec\u00adtures, \npages 362 362, 2008. [27] M. Naik, A. Aiken, and J. Whaley. Effective static race detec\u00adtion for java. \nIn PLDI 06: Proceedings of the ACM Confer\u00adence on Programming Language Design and Implementation, pages \n308 319, New York, NY, USA, 2006. ACM. [28] R. O Callahan and J.-D. Choi. Hybrid dynamic data race detection. \nIn PPoPP 03: Proceedings of the ACM Symposium on Principles and Practice of Parallel Programming, pages \n167 178, New York, NY, USA, 2003. ACM. [29] M. C. Rinard and P. C. Diniz. Commutativity analysis: A newanalysis \ntechnique for parallelizing compilers. ACM Transac\u00adtions on Programming Languages and Systems, 19(6):1 \n47, 1997. [30] S. Savage, M. Burrows, G. Nelson, P. Sobalvarro, and T. An\u00adderson. Eraser: a dynamic data \nrace detector for multi\u00adthreaded programs. ACM Transactions on Computer Systems, 15(4):391 411, 1997. \n[31] D. Shasha, F. Llirbat, E. Simon, and P. Valduriez. Transactionchopping: algorithms and performance \nstudies. ACM Trans\u00adactions on Database Systems, 20(3):325 363, 1995. [32] D. Shasha and M. Snir. Ef.cient \nand correct execution of parallel programs that share memory. ACM Transactions on Programming Languages \nand Systems, 10(2):282 312, April 1988. [33] N. Shavit and D. Touitou. Software transactional memory. \nInProceedings of the Symposium on Principles of Distributed Computing, pages 204 213, 1995. [34] T. Shpeisman, \nV. Menon, A.-R. Adl-Tabatabai, S. Balensiefer, D. Grossman, R. L. Hudson, K. F. Moore, and B. Saha. En\u00adforcing \nisolation and ordering in STM. In PLDI 07: Proceed\u00adings of the ACM Conference on Programming Language \nDe\u00adsign and Implementation, pages 78 88, New York, NY, USA, 2007. [35] G. Upadhyaya, S. P. Midkiff, \nand V. S. Pai. Using datastructure knowledge for ef.cient lock generation and strongatomicity. In PPoPP \n10: Proceedings of the ACM Symposium on Principles and Practice Of Parallel Programming, pages 281 292, \n2010. [36] R. Vallee-Rai, E. Gagnon, L. J. Hendren, P. Lam, P. Pom\u00adinville, and V. Sundaresan. Optimizing \nJava bytecode using the Soot framework: Is it feasible? In Proceedings of the In\u00adternational Conference \non Compiler Construction (CC 09), pages 18 34, 2000. [37] M. T. Vechev, E. Yahav, and G. Yorsh. Inferring \nsynchro\u00adnization under limited observability. In TACAS 09: Proceed\u00adings of the International Conference \non Tools and Algorithms for the Construction and Analysis of Systems, pages 139 154, 2009. [38] M. T. \nVechev, E. Yahav, and G. Yorsh. Abstraction-guidedsynthesis of synchronization. In POPL 10: Proceegings \nof the ACM Symposium on Principles of Programming Languages, pages 327 338, 2010. [39] H. Volos, N. Goyal, \nand M. Swift. Pathological in\u00adteraction of locks with transactional memory. In Proceedings of the ACM \nWorkshop on Transactional Computing (TRANSACT) 08, 2008. article avail\u00adable at http://www.unine.ch/transact08/papers/Volos\u00adPathological.pdf. \nURL last checked on Nov. 19, 2009. [40] O. Wolfson. The virtues of locking by symbolic names.Journal \nof Algorithms, 8(4):536 556, 1987. [41] Y. Zhang, V. C. Sreedhar, W. Zhu, V. Sarkar, and G. R. Gao.Minimum \nlock assignment: A method for exploiting concur\u00adrency among critical sections. In Proceedings of the \n21st Annual Workshop on Languages and Compilers for Parallel Computing (LCPC 08), 2008. [42] L. Ziarek, \nA. Welc, A.-R. Adl-Tabatabai, V. Menon, T. Shpeis\u00adman, and S. Jagannathan. A uniform transactional executionenvironment \nfor Java. In ECOOP 08: Proceedings of the Eu\u00adropean Conference on Object-Oriented Programming, pages \n129 154, 2008. A. Example This section presents a more detailed example of an SPMD program, and its associated \ncon.ict and projection con.ict graphs. Figure 10(a) shows a code fragment adapted from the Kmeans benchmark \nin the STAMP transactional memory benchmark suite [6] (shared variables in the .g\u00adure have been annotated \nfor clarity; the code analyzed by TransFinder does not contain any annotations). Figure 11 shows the \nresultant Con.ict Graph (CG). Figure 12(a) shows the Projection Con.ict Graph (PCG), with Thread T2 pro\u00adjected \nonto Thread T1, and Figure 12(b) shows the result af\u00adter SCC contraction. The CSBBs with a dashed outline \nrep\u00adresent atomic regions. Blue dashes indicate CSBBs which were formed as a result of an SCC contraction, \nwhile red dashes indicate CSBBs which were carried over unchanged from the PCG. Figure 10(b) shows the \ncode fragment from Figure 10(a) with the appropriate atomic synchronization constructs added (only the \nrun procedure is shown). class Thread // Constructor and members elided procedure void run() // Local \nvariable declarations elided while start < npoints do int stop = ... for i = start to stop do index \n= foo(...) if shared mem[i] = index then delta ++ end if shared mem[i] = index shared arr1[index][0] \n= shared arr1[index][0] + 1 for j=0to nfeatures do shared arr2[index][j] shared arr2[index][j] shared \nfeat[i][j] end for end for start = shared i shared i = start + CHUNK end while shared delta = shared \ndelta + delta end procedure end class ... procedure main() // Initialization code elided Thread T1 = \nnew Thread(...) Thread T2 = new Thread(...) T1.run(); T2.run() end procedure (a) Original Code procedure \nvoid run() // Local variable declarations elided while start < npoints do int stop = ... for i = start \nto stop do index = foo(...) atomic {if shared mem[i] = index then delta ++ end if shared mem[i] = index \n} // end atomic atomic { shared arr1[index][0] = shared arr1[index][0] + 1 } // end atomic = + for j=0to \nnfeatures do atomic {shared arr2[index][j] = shared arr2[index][j] + shared feat[i][j] } // end atomic \nend for end for atomic {start = shared i shared i = start + CHUNK } // end atomic end while atomic {shared \ndelta = shared delta + delta } // end atomic end procedure ... (b) Synchronized Code Figure 10. Kmeans \nbenchmark code: globally-scoped variables are annotated with shared . Figure 11. Con.ict Graph of program \nfragment from Figure 10. The Thread Stop and Exit nodes are not shown.  Entry Entry  (a) Projection \nCon.ict Graph of CG shown in Figure 11.   \n\t\t\t", "proc_id": "1869459", "abstract": "<p>This paper presents <i>TransFinder</i>, a compile-time tool that automatically determines which statements of an unsynchronized multithreaded program must be enclosed in atomic regions to enforce <i>conflict-serializability</i>. Unlike previous tools, TransFinder requires no programmer input (beyond the program) and is more efficient in both time and space.</p> <p>Our implementation shows that the generated atomic regions range from being identical to, or smaller than, the programmer-specified transactions in the three Java Grande benchmarks considered, and in five of the eight STAMP benchmarks considered, while still providing identical synchronization semantics and results. The generated atomic regions are between 5 and 38 lines larger in the three remaining STAMP benchmarks. In the most conservative case, TransFinder can, based on the program structure, successfully identify and suggest an alternative that conforms exactly to the programmer-specified atomic regions. By generating small, highly-targeted, conflict-serializable atomic regions, TransFinder allows the programmer to focus further tuning efforts on only a small portion of the code (when further tuning is needed).</p>", "authors": [{"name": "Gautam Upadhyaya", "author_profile_id": "81325490588", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P2354122", "email_address": "", "orcid_id": ""}, {"name": "Samuel P. Midkiff", "author_profile_id": "81100421918", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P2354123", "email_address": "", "orcid_id": ""}, {"name": "Vijay S. Pai", "author_profile_id": "81350595263", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P2354124", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1869459.1869513", "year": "2010", "article_id": "1869513", "conference": "OOPSLA", "title": "Automatic atomic region identification in shared memory SPMD programs", "url": "http://dl.acm.org/citation.cfm?id=1869513"}