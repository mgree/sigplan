{"article_publication_date": "10-17-2010", "fulltext": "\n Cross-Language, Type-Safe, and Transparent Object Sharing For Co-Located Managed Runtimes Michal Wegiel \nComputer Science Department University of California, Santa Barbara mwegiel@cs.ucsb.edu Abstract As \nsoftware becomes increasingly complex and dif.cult to analyze, it is more and more common for developers \nto use high-level, type-safe, object-oriented (OO) programming languages and to architect systems that \ncomprise multiple components. Different components are often implemented in different programming languages. \nIn state-of-the-art multi\u00adcomponent, multi-language systems, cross-component com\u00admunication relies on \nremote procedure calls (RPC) and mes\u00adsage passing. As components are increasingly co-located on the same \nphysical machine to ensure high utilization of multi-core systems, there is a growing potential for using \nshared memory for cross-language cross-runtime communi\u00adcation. We present the design and implementation \nof Co-Located Runtime Sharing (CoLoRS), a system that enables cross\u00adlanguage, cross-runtime type-safe, \ntransparent shared mem\u00adory. CoLoRS provides object sharing for co-located OO run\u00adtimes for both static \nand dynamic languages. CoLoRS de\u00ad.nes a language-neutral object/class model, which is a static\u00addynamic \nhybrid and enables class evolution while maintain\u00ading the space/time ef.ciency of a static model. CoLoRS \nuses type mapping and class versioning to transparently map shared types to private types. CoLoRS also \ncontributes a syn\u00adchronization mechanism and a parallel, concurrent, on-the\u00ad.y GC algorithm, both designed \nto facilitate cross-language cross-runtime object sharing. We implement CoLoRS in open-source, production\u00adquality \nruntimes for Python and Java. Our empirical eval\u00aduation shows that CoLoRS extensions impose low overhead. \nWe also investigate RPC over CoLoRS and .nd that using shared memory to implement co-located RPC signi.cantly \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA/SPLASH \n10, October 17 21, 2010, Reno/Tahoe, Nevada, USA. Copyright c . 2010 ACM 978-1-4503-0203-6/10/10. . . \n$10.00 Chandra Krintz Computer Science Department University of California, Santa Barbara ckrintz@cs.ucsb.edu \nimproves both communication throughput and latency by avoiding data structure serialization. Categories \nand Subject Descriptors D.3.3 [Programming Languages]: Language Constructs and Features Dynamic Storage \nManagement, Classes and Objects; D.3.4 [Pro\u00adgramming Languages]: Processors Run-Time Environ\u00adments, \nMemory Management (Garbage Collection), Com\u00adpilers, Optimization General Terms Design, Languages, Management, \nMea\u00adsurement, Performance 1. Introduction Large, scalable software systems are increasingly being built \nusing collections of components to better manage software complexity through reusability, modularity, \nand fault isola\u00adtion. Since each programming language has its own unique combination of performance, \nspeed of development, and library support, different software components are often implemented in different \nlanguages. As evidence of this, Thrift [35] and Protocol Buffers [32] have been developed by engineers \nat Facebook and Google, respectively, to en\u00adable more ef.cient interoperation across multi-language components \nemployed within their applications and back\u00adend services. For web applications, different languages are \nbetter suited for the implementation of different tiers: Ruby, Python, Java, and JavaScript facilitate \nfast development of the presentation layer, Java, PHP, Perl, Python, and Ruby components commonly implement \nserver-side logic, and Java, query languages, and C++ are used for a wide range of backend database technologies. \nThe components of these multi-language, multi-component applications and mashups typically execute within \nindependent runtime systems (lan\u00adguage virtual machines (VMs), interpreters, etc.) and com\u00admunicate and \ninteroperate via remote procedure calls (RPC) and message passing. Increasingly, administrators co-locate \nruntimes to bet\u00adter utilize multi-core resources. This makes it possible to use shared memory for such \ncross-component communica\u00adtion as well as for a cross-runtime language-neutral trans\u00adparent object storage. \nHowever, despite its growing practi\u00adcal value, shared memory has not yet been investigated in either \nof these contexts. To evaluate the potential of us\u00ading shared memory for cross-language, safe, transparent \ncommunication and object storage, we design and imple\u00adment Co-Located Runtime Systems (CoLoRS). CoLoRS \npro\u00advides direct object sharing across static and dynamic, object\u00adoriented (OO) languages.  CoLoRS virtualizes \nVM components that assume a lang\u00aduage-speci.c object/class/memory model. In CoLoRS, shared objects retain \ntheir language-speci.c behavior, including the semantics of virtual method calls, locking, and .eld access. \nIn addition, builtin/library data structures, such as collec\u00adtions, transparently map to their shared \ncounterparts in the CoLoRS object model. Our key hypothesis is that sharing objects across static/dy\u00adnamic \nOO languages using shared memory can be safe, transparent, and ef.cient. Our main contributions include: \n An object and memory model that enable language\u00adneutral object and class sharing across dynamic and \nstatic languages. The CoLoRS object model is a static-dynamic hybrid, which provides the ef.ciency of \na static model with the .exibility of dynamic class modi.cations. To enable this, CoLoRS uses an extensible \nstatic model with versioning and type mapping.  A parallel, concurrent, and on-the-.y GC that is better \nsuited for multi-VM memory management than extant GCs. CoLoRS GC is simpler than state-of-the-art on-the\u00ad.y \nGCs, does not require tight integration into a runtime, and imposes no system-wide pauses.  A synchronization \nmechanism that avoids the complexi\u00adties of conventional approaches to monitor synchroniza\u00adtion, while \nproviding the same semantics and comparable performance.  CoLoRS implementation for Java and Python. \nTo inves\u00adtigate object sharing between dynamic and static OO languages, we integrate CoLoRS support within \nopen\u00adsource, production-quality runtimes: HotSpot JVM and cPython.  CoLoRS experimental evaluation. \nWe have evaluated CoLoRS ef.cacy using standard Java and Python bench\u00admarks and found that CoLoRS extensions \nimpose low execution time overhead. We also provide experimental results for the CoLoRS GC algorithm \nand CoLoRS syn\u00adchronization.  RPC as a CoLoRS use-case. We have found empirically that CoLoRS can signi.cantly \n(up to 2 orders of mag\u00adnitude) improve the performance of cross-language RPC systems, such as CORBA [13], \nREST [23], Thrift [35], and Protocol Buffers [32]. This is because using shared memory in the co-located \ncase avoids expensive ob\u00adject serialization. The improvements in communication  ...... .............. \n..... .......  co-located on a multi-core system Figure 1. CoLoRS architecture. There is exactly one \nCoL\u00adoRS server process, which manages the shared memory seg\u00adment and runs concurrent GC. Runtimes for \ndifferent lan\u00adguages (Java and Python in this case) attach to the shared memory segment and allocate/use \nobjects in the shared heap. throughput and latency due to CoLoRS signi.cantly in\u00adcrease end-to-end transaction \nperformance in Cassan\u00addra [1] (a key-value database), and the Hadoop Dis\u00adtributed File System (HDFS) \nserver [26]. In the sections that follow, we present the design and ar\u00adchitecture of CoLoRS, describe \nthe key contributions of our system, including a language-neutral object/memory model, memory management, \ngarbage collection, and synchroniza\u00adtion support, as well as transparent object sharing via run\u00adtime/library \nvirtualization. We then discuss CoLoRS im\u00adplementation and its empirical evaluation, compare/contrast \nCoLoRS with related work, and conclude.  2. CoLoRS Overview A primary design goal of CoLoRS is to provide \ntype-safe, transparent, direct object sharing between co-located man\u00adaged runtimes for different OO languages. \nThis includes both statically-typed (e.g. Java) and dynamically-typed (e.g. Python) languages. The key \nchallenge with providing such support are the major differences between language imple\u00admentations, including \nobject/class models, memory models, type systems, builtin types, standard libraries, and memory management \n(GC). For instance, dynamic languages sup\u00adport attribute (member) addition at runtime, while static lan\u00adguages \npermit class changes at compile-time only. Figure 1 shows a high-level view of a CoLoRS system. In this \nexample, two VM processes (one for Java and one for Python) are co-located on a multi-core system. There \nis ex\u00adactly one CoLoRS server process which manages the shared heap (this includes the setup of the shared \nmemory segment, data structure initialization, as well as support for garbage collection). Each VM process \nhas its own private heap and a private object/class model and runs its applications threads. In the shared \nheap, there is a CoLoRS object/class model which is transparently translated to a private object/class \nmodel in each VM. All VMs map the shared memory seg\u00adment at the same address in the virtual address space \nand use shared objects directly via pointers.  CoLoRS does not allow pointers from the shared heap to \nany private heap because of memory/type safety. In our experience, this restriction is rarely violated \nin standard li\u00adbraries and most existing classes can be shared without any modi.cations. Static (class) \n.elds are not subject to sharing because they often represent local resources and sharing them would \nbreak resource isolation. For instance (object) .elds, how\u00adever, CoLoRS supports fully transparent sharing \nwith regard to allocation, GC, .eld access, (virtual) method invocation, monitor synchronization, standard \nlibraries, and class load\u00ading. We do not support code sharing because that would re\u00adquire de.ning a VM-neutral \nlanguage and checking whether two methods are equivalent, which in general is undecid\u00adable. Instead, \nCoLoRS guarantees type-safety for data/state sharing only. To reduce the programming effort associated \nwith ensuring that the code/behavior matches across differ\u00adent languages, methods can be translated between \nlanguages automatically. Note that it is sometimes desirable to have different class implementations/interfaces \nin different VMs: standard libraries differ across languages and we do not want to unify them because \nprogrammers are used to existing li\u00adbraries and there is a lot of legacy code written to them. Shar\u00ading \nonly instance .elds makes CoLoRS more practical as the code and static data do not have to match across \nlanguages. A general approach we take in CoLoRS is to de.ne a language-neutral, shared object model (with \nrespect to non\u00adstatic data) and then dynamically map it to each runtime\u00adspeci.c object model. To implement \nthis, we virtualize all runtime components that rely on a speci.c object model. Modi.cations to runtimes \nare necessary to make object shar\u00ading transparent. In particular, CoLoRS needs to intercept all .eld \naccesses to handle shared objects correctly. 2.1 CoLoRS Usage CoLoRS provides a simple application programming \ninter\u00adface (API) for developers. The CoLoRS API for Java com\u00adprises the following methods in the SharedMemory \nclass (Python has equivalent API): Object copyToSharedMemory(Object root); Object allocate(Class objectClass); \nObject allocate(Class containerClass, int length); boolean isObjectShared(Object object); ObjectRepository \n.ndOrCreateRepository(String key); ObjectChannel .ndOrCreateChannel(String key); Type getSharedType(Object \nobject); CoLoRS supports two ways of creating shared objects: via direct object allocation (the allocate \nmethod) and via deep copying of a private object graph to shared memory (the copyT oSharedMemory method). \nThe allocate method has two variants: one for allocation of .xed-size objects and one for allocation \nof container objects (which takes the initial size of a container as a parameter). Note that we do not \nsupport a state model where a thread can switch to the shared mode and issue regular object al\u00adlocations \nto allocate in shared memory (as is done in related work on cross-JVM sharing [37]). The reason is that \nthe state model requires complex rules specifying which allocations should target shared memory. For \ninstance, in a JVM, we must exclude class loading, static initializers, and exception handling from leaking \nobjects into shared memory. CoLoRS provides two mechanisms to initiate commu\u00adnication between two runtimes: \nchannels and repositories, both of which are named entities enabling exchange of a reference to a shared \nobject. The ObjectRepository class provides nonblocking get/set functionality while the ObjectChannel \nclass supports blocking send/receive cross-VM semantics. The following code fragments show an id\u00adiomatic \nrepository usage for two Java processes. The client process: ObjectRepository r = SharedMemory..ndOrCreateRepository( \ndb ); synchronized(r) { while(r.get() == null) r.wait(); }The server process: ObjectRepository r = SharedMemory..ndOrCreateRepository( \ndb ); synchronized(r) { r.set(root); r.notifyAll(); }For object channels, we have a similar pattern but \nsynchro\u00adnization/waiting is not necessary because of the blocking behavior of send and receive. Each \nrepository holds a reference to its root object. Each channel has a .xed capacity for messages and blocks \nthe sender when full. As long as a shared object is reachable from any repository, channel, or any VM, \nit stays alive. Unreachable shared objects are garbage collected. Channels and repositories are identi.ed \nby a key (string). The CoLoRS API enables re.ective inspection of the shared type of a shared object \nvia the getSharedT ype method. We need this API method because in CoLoRS, ex\u00adpressions that evaluate \nto an object class, e.g. object.getClass() in Java, retrieve a private class to which a speci.c shared \nclass currently maps. To see the shared class before map\u00adping to a private class occurs, getSharedT ype \nis used. Shared classes are regular objects CoLoRS uses a three\u00adlevel circular meta-data hierarchy that \nis fully traversable by programs wishing to inspect it. A programmer can check whether an object is in \nshared memory via the isObjectShared method. The system throws a SharedMemoryException to prevent shared-to\u00adprivate \npointers as well as to signal type mapping failures, out-of-memory errors, and locking issues.  3. \nCoLoRS Design and Architecture CoLoRS uses a dedicated process (CoLoRS server) to man\u00adage shared memory. \nThere is one CoLoRS server per OS instance. This server creates, initializes, and destroys the shared \nmemory segment, as well as runs concurrent, par\u00adallel GC. That is, GC continues to function even when \nno runtimes are currently attached. CoLoRS was designed to be scalable (GC, repositories) therefore having \none server per host is not a limitation.  To use shared memory, runtimes attach to the shared memory \nsegment (by mapping it to their virtual address space at the pre-de.ned, .xed address). The shared memory \nsegment contains three spaces: metadata space (for state variables and synchronization), classes space \n(for shared types, repositories, and channels), and objects space (for garbage-collected shared objects). \nEach VM runs a separate CoLoRS thread which is responsible for collaboration with the CoLoRS server during \nGC. CoLoRS intercepts all .eld accesses in the VMs and handles shared and private data differently. Private \n.elds are read/written in a VM-speci.c way while shared .elds use CoLoRS accessors. 3.1 The CoLoRS Object \nModel (OM) CoLoRS employs an OM that aims at transparent and ef.\u00adcient cross-language object sharing, \nwhile supporting both static and dynamic languages. Our primary goal is main\u00adtaining the language-speci.c \nOM and object/class semantics while a VM interacts with shared objects. The rationale be\u00adhind this is \nto avoid introducing a new unfamiliar program\u00adming model. In addition, CoLoRS combines certain charac\u00adteristics \nof static and dynamic OMs in order to support the .exibility of a dynamic model while providing the ef.ciency \nand simplicity of a static model. 3.1.1 CoLoRS Type System CoLoRS preserves language-speci.c type-safety \nwithout de.ning new typing rules by mapping shared types to private types. When mapping a shared type \nS to a private type P1 in one VM and to private type P2 in another VM, we guarantee that any .eld access \npermitted by P1 does not violate the .eld typing constraints imposed by P2 (and vice versa). In the CoLoRS \ntype system, every value is an object (there are no primitive types like in Java or C#). This is mo\u00adtivated \nby dynamic languages like Python and Ruby which treat everything as an object and therefore require that \neach value have a unique identity (address). Unlike extant systems for cross-language data sharing, CoLoRS \ndoes not specify its own data de.nition language (DDL). Conventional approaches have resulted in a number \nof domain-speci.c DDLs, e.g., SQL in relational databases, WSDL in web services, and IDL in CORBA. The \nprimary limitation of DDLs is their static nature and the necessity for a programmer to master another \nlanguage. Instead, CoLoRS generates the shared data model automatically from the na\u00adtive language data \nmodel de.ned by the programmer. More\u00adover, this happens dynamically at runtime and only for types that \nare used in shared memory. The CoLoRS OM strives to strike a balance between supporting diverse languages \n(both static and dynamic) and staying suf.ciently close to each individual language so that costly runtime \ndata conversions are avoided if possible. An\u00adother key design tradeoff is to support the .exibility of \ndy\u00adnamic languages while leveraging the bene.ts provided by static typing. In fully static OMs (e.g. \nJava), object layout is completely described by classes, .elds are ef.ciently ac\u00adcessed via offsets, \neach object consumes only as much mem\u00adory as necessary for its attribute values, and the data model is \nfully documented by classes. On the other hand, in fully dynamic OMs (e.g. Python) classes do not describe \nobject attributes, each object maintains a dictionary mapping at\u00adtribute names to values, .eld access \nis expensive as it takes place via names, and space usage is suboptimal due to the redundancy across \nattribute dictionaries. However, unlike static OMs, dynamic OMs support dynamic attribute addi\u00adtion/removal \nas well as per-object attributes. Several hybrid models have been introduced to mitigate the static-dynamic \ntradeoffs. A partially static/dynamic OM is used by Google AppEngine, where each object has a static \npart (.elds described by a class) and a dynamic part (per\u00adobject dictionary). On attribute access, the \nsystem .rst tries to use a static .eld then falls back to an object dictionary on failure. Dynamically \ncreated attributes do not become part of the static model. A similar concept has been introduced to Python \n(via the slots declaration). The JavaScript V8 runtime implements hidden classes to enable fast, offset\u00adbased \nattribute lookup while supporting dynamic attribute addition and deletion.  3.1.2 Hybrid OM and Versioning \nCoLoRS OM is a static-dynamic hybrid, which can be de\u00adscribed as an extensible static model with versioning \nand type mapping. Our goal is to keep CoLoRS OM as static as possible but still allow the .exibility \nof modi.cations (add/remove/change name/type of a .eld). Shared classes are always created based on private \nclasses when a private object gets allocated in (or copied to) shared memory. On each allocation in shared \nmemory, we inspect the .elds of the allocated object and look for a shared class being an exact match \nfor a given type name and .eld set. If we do not .nd an exact match, we create a new class (or if a class \nwith this name already exists, we create a new shared class version, having the same class name but a \ndifferent .eld set). For example, suppose that we have the following class in Java: class Employee { \nString name; double salary; }and we perform shared allocation using: Employee e = (Employee)SharedMemory.allocate(Employee.class); \nIf no Employee class is present in shared memory yet, we create one, with two .elds that correspond to \nthe private Employee class. Now assume that we add a new .eld to the Employee class, say Employee manager; \nand we repeat the shared allocation as shown above. This time, CoLoRS   shared memory private objects \nspace memory Private class A with class field float b pointer added class pointer  will create a \nnew version of the shared Employee class, with three .elds. Note that at any point in time there is exactly \none private Employee class (which may evolve in time) and there may be multiple versions of shared Employee \nclass (re.ecting the schema evolution). Field removal is handled in a similar way. Shared objects use \nshared classes to describe their lay\u00adout. Different versions of a single shared class may have dif\u00adferent \nlayouts in memory and .eld sets. Shared classes are read-only, they do not change. However, shared objects \nmay change their class pointers (from one version of a particular class to another version of the class). \nThis can happen both in static and dynamic languages. For example, the following code in Python, which \nuses our two-.eld Employee class: e = shared memory.copy to(Employee( Smith , 100)) e.state = NY adds \na new .eld (called state) dynamically. To support this in shared memory, CoLoRS creates a new version \nof the Employee class and changes the current class of the e ob\u00adject to the new class version. Dynamic \n.eld removal (via del in Python) is handled similarly. The advantage of versioning over a pure OO model \nis lower space consumption. In conventional OO systems, class evolution takes place via subclassing: \nto add or hide a .eld a new class is created that inherits from the old class. As a result, it is not \npossible to remove any attribute and space is consumed forever by unused .elds. In contrast, with version\u00ading, \neven if classes evolve, the newly-created objects always consume the optimal amount of space.  3.1.3 \nType Mapping To correctly handle multiple class versions in shared mem\u00adory, CoLoRS uses type mapping. \nEach private class P in a VM always has exactly one version which, at any given mo\u00adment, may be mapped \nto several different versions of class P in shared memory (a one-to-many relationship). Except for builtins \n(e.g. Integer, String), mapping only occurs be\u00adtween classes with the same name programs in different \nlanguages must agree on package/module and class names. We map a shared .eld to a private .eld if and \nonly if both have the same name and the same (or convertible) type. In dynamic languages, we map solely \non the .eld name basis as there are no static types available. Since type mapping is a relatively expensive \nprocess, we perform it lazily, once per shared-class-version, and maintain the mapping in a private hash \ntable in each VM. We also use a reverse mapping table, to avoid shared-type lookup/matching on every \nallocation in shared memory. Note that on allocation, we need to obtain the shared type based on a private \ntype. In contrast, when accessing a .eld in a shared object, we perform the mapping from a shared type \nto the private type. When CoLoRS allocates a new object in shared memory, it tries to .nd a shared class \nversion that exactly matches the private .eld set of the newly-allocated object. If no Figure 2. An example \nillustrating CoLoRS versioning and type mapping as private class A evolves by having a .eld added.  \n exact match is found, it creates a new shared class version. Consequently, newly-created objects do \nnot contain .elds that were removed from a private class due to its evolution. The rationale behind this \nis that we want to keep the object size in shared memory optimal. However, when mapping a shared class \nto a private class in a context other than allocation, we allow both private and shared .elds to remain \nunmapped (if they do not have a match). When a VM uses an unmapped .eld in a shared object, we dynamically \nadd a .eld to a class. To do so, we create a new shared class version that contains the previously unmapped \n.eld, and change the shared object s class pointer to point to the new class version. Note that the shared \nobject s type does not change, as seen from the VM s perspective all versions of a shared class always \nmap to the same private class (with the same name). Although CoLoRS supports dynamic changes, once the \ndata model is stable, both space usage and .eld access work exactly like a fully static model. Also, \nin the CoLoRS OM, all object attributes are always present in its class and can be introspected via re.ection. \nSome VMs, such as Java, support class loading that makes it possible to have multiple classes with the \nsame fully-quali.ed name. CoLoRS supports this via type map\u00adping. One shared class can map to multiple \nprivate classes (e.g. we can map a single shared class named a.b.C to all pri\u00advate classes named a.b.C \nloaded by different class loaders). Figure 2 shows an example where private class A evolves from a single-.eld \nclass containing int a into a class with two .elds, int a and .oat b . Private class A has exactly one \nversion (the newest one with both .elds). Shared class A has two versions. Both shared versions are mapped \nto the private class A so that they can be uniformly used, despite being distinct types in shared memory. \nThe shared objects space contains two objects of class A one allocated for the old version of A and \none allocated for the new version of A. Note that each shared object uses only as much space as necessary \nfor its attribute set. Both objects have the same type in a VM, and the VM may access both .elds (a and \nb) in both objects. On access to a non-existent .eld (b in this case) in older shared objects, CoLoRS \nwill expand the object to make room for the new .eld (initializing the new .eld to 0).  Reconsidering \nthe example in Figure 2 in the case when class A evolves by having the b .eld removed, we have a similar \nsituation. Private class A again has exactly one version (the newest one, with one .eld a). Shared class \nA has two versions, both mapped to the same private type A. Field b remains unmapped as it can never \nbe used by the VM and this .eld is simply ignored in those shared objects that have it. Note that newly-allocated \nshared objects do not reserve a slot for .eld b, thus using optimal amount of space. In contrast, OO \ninheritance does not allow removal of a .eld from an object (unused inherited .elds continue to consume \nslots in objects). Field renaming is equivalent to .eld removal followed by a .eld addition. Note that \nusing CoLoRS cannot lead to broken program invariants because matching .elds can never remain un\u00admapped. \nThus, if class implementations across languages match and preserve some invariant in each language, CoL\u00adoRS \nwill preserve this invariant too.  3.1.4 Built-In Types and Libraries CoLoRS provides full transparency \nfor builtin types (e.g. strings, integers, lists, and sets). Builtin types differ sig\u00adni.cantly across \nlanguages and at the same time are fre\u00adquently used by programs and libraries. CoLoRS preserves language-speci.c \ninterfaces for builtin types by virtualizing the builtin implementation and/or standard libraries in \neach runtime. Library virtualization amounts to modifying the code of library methods so that these methods \ncheck whether any of the method arguments (including the receiver, if any) is a shared object and, if \nso, to execute a different implemen\u00adtation of the method. CoLoRS de.nes a set of builtin types which \nwe identify in Table 1 with their mappings in Java and Python. We support 64-bit integers, which can \nbe mapped to Python int and to any integer type in Java, both primitive, e.g. int, short, and reference, \ne.g. Long, Integer.Having only one integer type allows us to avoid complex rules for .eld mapping during \nschema evolution. For example, if we supported int and short as distinct integer types in shared memory, \nthen we would have to de.ne complex semantics for changing the .eld type from int to short and vice versa, \ni.e. when we create a new .eld dynamically and when we reuse existing integer .eld. We use a similar \napproach in case of .oating-point types, supporting only 64-bit IEEE .oats. The CoLoRS 64-bit .oat can \nbe used in Java as any .oating point type, e.g. double or Float. We do over.ow/under.ow checks when read\u00ading/writing \ninteger/.oat .elds requires conversion. For non-container types, we also provide boolean and string. \nAs in Thrift [35], CoLoRS de.nes three container types: list, set,and map. Containers are untyped (i.e. \nmay Shared Java Python integer .oat boolean string binary list set map byte, short, int, long, char, \nByte, Short, Integer, Long, Character .oat, double, Float, Double boolean, Boolean String byte[] List, \nArrayList, Object[], int[], .oat[], T[], ... Set, HashSet Map, HashMap int .oat bool str bytearray list, \ntuple set, frozenset dict Table 1. Builtin types supported by CoLoRS and their map\u00adpings to Java and \nPython builtin types. For transparent and convenient use by programmers, multiple mappings are pos\u00adsible \nper shared type. contain objects of different types at the same time). This is because we cannot automatically \ninfer the container element type (at least in Java and Python), even if the container is not empty. To \nsupport a compact byte array representation we provide the binary type, suitable for blobs. Note that \nin Java, a shared list can be used as an array (of any type) and as a List. The rationale behind this \nis transparency we want to support Java arrays even though CoLoRS and Python do not have arrays so that \nwe do not change the Java programming model. Non-container types (integer, .oat, boolean, and string) \nare immutable. Builtin objects always have exactly one version, exactly one mapping to a private type, \nand do not have any programmer-visible .elds. In order to use shared objects along with private objects \nin a single hash-based container, hash codes and equal-to methods must agree across runtimes. We unify \nthem for Java and Python builtin types. For shared objects, CoLoRS provides default hash code generation, \nequal-to methods, and less-then methods (all based on object addresses). They can be overridden by programmers. \nFor programmer convenience, CoLoRS automatically copies non-container types (e.g. integer, string) to \nshared memory. On .eld assignment/array store, the system checks whether the assignment uses a private \nr-value and a shared l-value. If so, and the r-value is of a non-container type, CoLoRS silently calls \nthe copyT oSharedMemory method on the r-value, instead of throwing an exception. This mech\u00adanism is particularly \nuseful for constructors.  3.1.5 Static Languages In static languages, object .elds are typed and typically \nac\u00adcessed using .eld offsets. Since CoLoRS uses a mostly\u00adstatic OM, it also identi.es .elds in shared \nobjects by their offsets. Private and shared .eld offsets may differ so it is nec\u00adessary to map between \nthem. Unidirectional mapping from the private offset to the shared offset is suf.cient because VMs always \naccess shared .elds using the context of a pri\u00advate type. To make this mapping ef.cient, we associate \na .eld-offset-table with each pair (S,P) where S is a shared type mapped to private type P. Whenever \nwe access a shared .eld in a shared object, we index the appropriate .eld-offset\u00adtable with the private \n.eld offset and obtain the shared .eld offset.  When inspecting a class of a shared object (e.g. via \nob\u00adject.getClass() in Java) we always get a unique private class as a result. For example, integer maps \nto SharedInteger while list maps to SharedList. However, to ensure trans\u00adparency, shared builtins can \nmap to multiple different private types. In OO languages, this can be implemented via multi\u00adple inheritance. \nFor instance, if we can make SharedList inherit from List, Object[], ArrayList, etc. then represent\u00ading \nshared list as private SharedList is correct in all pos\u00adsible mappings. However, some languages (e.g. \nJava) do not support multiple inheritance or inheritance of array types. We instead simulate both by \nmodifying the runtime so that SharedList can be cast to any of the private types that shared list maps \nto. We apply a similar approach for integer and float. Each private class maps to a unique shared class. \nA gen\u00aderal rule that we use is that whenever we allocate private type P as shared type S, we must later \nbe able to use the shared type S as P . Type mapping may cause class loading in a VM. This is because \nwhenever we encounter an instance of a shared type T , which maps to a private type U, we must load class \nU. Thus, CoLoRS introduces a new class loading barrier (in VMs that use dynamic loading). Since in static \nlanguages, the static type of a .eld is avail\u00adable, we permit certain conversions while mapping shared \n.elds to private .elds. Let us denote any private class to which a shared class S maps as map(S). For \na given .eld of shared type S and of private type P , CoLoRS allows both upcasts and downcasts during \nmapping. Upcasts occur if class P is a superclass of class map(S) or class map(S) implements interface \nP . For instance, we have an upcast when we map a .eld of shared type list to a .eld of private type \nList (because map(list)= SharedList and SharedList implements the List interface). Or we have an upcast \nwhen we map a .eld of shared type string to a .eld of private type Object, because Object is a superclass \nof class map(string)= String. Upcasts are most useful to support interface-type private .elds, such as \nList in Java. Downcasts take place if class P subclasses map(S). For example, there is a downcast if \na .eld of shared type list is mapped to a .eld of private type String[], because String[] subclasses \nObject[] =map(list). Thanks to downcasts, pri\u00advate arrays (whose elements are typed) can conveniently \nac\u00adcess shared lists (whose elements are untyped). To ensure type safety, downcasts require a read barrier \nwhich checks the actual object type on each read access. Upcasts represent a covariant type operator \n(analogous to the array upcasts in Java) and therefore require a write barrier that checks the type of \nthe stored object against the expected static type.  3.1.6 Dynamic Languages In dynamic languages, .elds \nare accessed by name (not by offsets) and static .eld types are not available. Therefore, when creating \na new shared class or comparing to an existing one, CoLoRS relies on actual types of all non-null attributes \nin a particular object (i.e. the one being copied to shared memory). This results in type concretization \n shared classes created by dynamic runtimes always have the most derived .eld types. Such concretized \ntypes can be later used by static runtimes without any problems because static runtimes allow upcasts \nduring type mapping. We ignore NULL .elds as for them no static (concretized) type can be inferred. When \nlooking for an exact type match (during copying to shared memory), we allow type conver\u00adsions (upcasts \nand downcasts). No read barrier is necessary as dynamic languages do not guarantee any particular type \nfor any .eld. However, each .eld store must verify the type of the stored object against an appropriate \nstatic shared type (via a write barrier). When mapping a shared type S to a private type P ,we do not \nmap .elds, as we do not have .eld types and offsets in P . Instead, we just create a hash table mapping \n.eld names to shared offsets. This speeds up attribute access (which is done via names). Since multiple \nprivate types can be mapped to a single shared type (e.g. list and tuple in Python both map to shared \nlist), we employ multiple inheritance if possible (e.g. in Python) or we extend the runtime to simulate \nit for the types in question. CoLoRS uses reverse mapping to avoid shared class lookup on each allocation. \nReverse mapping can improve performance only if private instances of a single private class have similar \nattribute sets (a natural property but one that is not always enforced by dynamic languages). Otherwise, \nthe system might end up relying on dynamic .eld addition fre\u00adquently as some objects types may be mapped \nto static types that have too few static attributes.  3.2 The CoLoRS Memory Model (MM) CoLoRS de.nes \na memory model that builds on and sim\u00adpli.es memory models supported by mainstream languages. CoLoRS \nMM is equivalent to the Java MM for programs that do not contain data races. Java programs that rely \non volatile and final .elds or other race-related aspects of the Java MM may work incorrectly with CoLoRS \nbecause shared object .elds drop their Java-speci.c modi.ers. Python does not de\u00ad.ne any MM so using \nCoLoRS cannot break extant Python programs. Following the Java Memory Model (JMM) approach and recent \nstandardization effort for the C++ MM [9], CoL\u00adoRS guarantees sequentially consistent semantics only \nto programs that are properly synchronized (i.e. those that do not contain data races). A data race occurs \nwhen multiple threads can access the same object .eld at the same time and at least one of them performs \na write.  Similarly to Java and C#, CoLoRS provides monitor syn\u00adchronization. Monitors provide mutual \nexclusion for threads and restrict re-ordering of memory accesses. Monitor entry has load acquire semantics \n(downward fence) while monitor exit has store release semantics (upward fence). Full mem\u00adory fence is \nnot supported in CoLoRS (following Java and C# design) a pair of downward and upward fences does not \nconstitute a full fence. In CoLoRS, monitors are fault\u00adtolerant: if a VM dies while holding a monitor, \nsubsequent acquisitions of this monitor do not result in a deadlock or ac\u00adcess to corrupted data, but \nthrow a runtime exception before entering a critical section. Like the JMM (and unlike the C++ MM), CoLoRS \nmust guarantee basic type-and memory-safety even in the pres\u00adence of data races. Therefore, in CoLoRS, \nall pointer stores and loads are always safe (even with data races). This prop\u00aderty is relatively easy \nto implement (an aligned machine\u00adword-wide load/store is atomic on most architectures). This property \nis not strictly necessary for type-safety in case of primitive values, like integer or .oat, and therefore \nCoLoRS does not guarantee it for non-pointer .elds. Operations like shared class creation or dynamic \n.eld addition are always thread-safe because they are rare and can be internally pro\u00adtected by a lock. \nNote that CoLoRS MM avoids many of the complexities of the JMM by supporting only instance .eld sharing \n(no statics, no methods, no constructors) and ignoring .eld mod\u00adi.ers like final and volatile. Unlike \nC++ MM, CoLoRS MM does not support atomic operations and the trylock functionality, which simpli.es the \nmodel signi.cantly.  3.3 Monitor Synchronization The CoLoRS synchronization mechanism is an adapta\u00adtion \nand simpli.cation of extant, commonly-used schemes, which are inadequate for CoLoRS because of their \ncomplex\u00adity, tight integration with VM services, and reliance on the ability to stop all the threads. \nState-of-the-art high-performance VMs, like HotSpot JVM, use biased locking [33] to avoid atomic CAS \noper\u00adations in the common case. However, biased locking re\u00adquires safepoint support it occasionally \nneeds to stop all the threads to recover from its speculative behavior. Safe\u00adpoints are needed for bias \nrevocation (when a thread must manipulate the stack of the current bias owner) as well as for bulk rebiasing \n(to walk all thread stacks to search for cur\u00adrently held monitors). One of the design goals in CoLoRS \nis to avoid stopping all VMs at once such system-wide safepoints are inherently unscalable and introduce \nlengthy pauses. Therefore, biased locking is not suitable for CoL\u00adoRS. Another commonly-used locking \nscheme is lightweight locking [33], which strives to avoid using OS primitives in the common case by \nrelying on atomic CAS operations. We have investigated the ef.cacy of this approach and found that in \nmodern OSes that provide futexes (fast user-mode locking primitives), lightweight locking performs worse \nthat an OS mutex. In older OSes, OS-backed synchronization was slow because it required kernel entry/exit. \nLinux im\u00adplements futexes that in the uncontended case perform one atomic CAS in user-mode for each pair \nof lock and un\u00adlock operations. In contrast, lightweight locking needs two atomic CASes [33] per uncontended \nlock-unlock pair. We have compared the performance of pthread-based locking and lightweight locking in \nthe uncontended case. We mea\u00adsured the time needed to do one lock and one unlock. Our results show that \nlightweight locking is slower: on a dual\u00adcore Intel Core2 by 31%, and on a quad-core Intel Xeon by 45%. \nTherefore, we have designed CoLoRS to use OS prim\u00aditives (POSIX mutexes based on futexes) directly. Most \nextant monitor implementations (e.g. HotSpot JVM) reserve a word in the object header to assign a lock \npointer to an object once a lock is needed. The presence of such a pointer leads to signi.cant design \ncomplexity in ex\u00adtant systems because once the pointer is set, one can only clear it when all threads \nare stopped or the object has be\u00adcome unreachable. CoLoRS does not ever stop-the-world (halt/safepoint \nall threads in the system), hence we take a different approach. Instead of using a pointer to a monitor, \nwe hash the object address (shared objects do not move in CoLoRS) into a .xed-size table of monitors \nkept in shared memory. Since few objects are used as monitors at a time, it is unlikely that multiple \nsimultaneously-locked objects will ever hash to the same monitor-table entry (i.e. hash con.icts are \nrare). To avoid deadlocks and decreasing concurrency level, we detect con.icts in the hash table and \nuse a collision chain to ensure that each object gets a unique monitor. Hash-based locking is also used \nin GCJ [24] (GNU static Java compiler) in order to reduce the object header size. GCJ, however, uses \nboth light-and heavy-weight locks. We use mutex trylock() to avoid blocking the acquiring thread in case \nthere is a con.ict in the lock-hash-table. We also tag lock-hash-table entries with an object pointer, \nonce a lock is successfully acquired via mutex trylock(). Each thread locking object O .rst checks if \na hash-table entry is tagged with O. If so, the thread proceeds to mutex trylock(). Otherwise, if the \nentry is tagged with P != O, we re-hash to .nd another entry. If there is no tag there, we proceed to \nmu\u00adtex trylock(). Dead-object tags are cleared asynchronously by GC for each con.ict chain, GC creates \nand locks a new untagged chain entry, thus temporarily stopping chain expansion (all threads will block \non mutex trylock() in that GC-created entry). GC then clears the dead tags in the chain, and .nally, \nnoti.es the blocked threads to repeat their lock\u00ading from scratch (the re-do .ag is set on the GC-created \nentry and the GC releases the mutex).  The above synchronization scheme can be transparently integrated \ninto Java based on Java monitors. Python does not support the monitor abstraction (locks are not associ\u00adated \nwith objects) and therefore needs to be extended with dedicated API for monitors (similar to Java). \n 3.4 Garbage Collection Since CoLoRS targets multi-and many-core systems and avoids system-wide safepoints, \nthe most appropriate GC al\u00adgorithm for shared objects is parallel (i.e. using multiple GC threads), concurrent \n(i.e. performing most work with\u00adout stopping the application), and on-the-.y (i.e. stopping at most one \nthread at a time) GC. In addition, CoLoRS needs a non-moving, mark-sweep-style GC because some runtimes \n(e.g. Python) assume that objects do not move and other ones (e.g. Mono for C#) use conservative stack \nscanning.  We have found extant on-the-.y mark-sweep GCs to be unsuitable given the CoLoRS architecture \nand requirements. Therefore, we have designed a variation of snapshot-at-the\u00adbeginning (SATB) GC, which \nis parallel, concurrent, and on\u00adthe-.y. The state-of-the-art in on-the-.y GC systems include those that \nemploy the Doligez-Leroy-Gonthier [16, 17] algo\u00adrithm and its extensions by Domani et al. [18, 19] for \ngener\u00adational heap layout and multiprocessors without sequential consistency. State-of-the-art, snapshot-based, \non-the-.y GC algo\u00adrithms require multiple (three to start the collection cycle) system-wide handshakes \nwith all the threads. The mutators must check whether they need to respond to handshakes regularly during \ntheir normal operation. For scalability, we designed CoLoRS to work at the granularity of VMs, not individual \nthreads. The handshakes would require keeping track of all threads in all VMs. In addition, we do not \nwant to require VMs to implement the per-thread handshake-polling mechanism, as it is not generally supported \nin VMs. A design goal of CoLoRS GC is to abstract away private VM memory management to one operation: \nshared root re\u00adport, without imposing any speci.c implementation details. As a result, we have designed \nan on-the-.y GC that does not use handshakes and works at the VM level (not thread level). In addition, \nthe CoLoRS GC is simpler (as it does not have any phase transitions) and guarantees termination (some \npre\u00advious algorithms unreliably depend on the relative speed of the collector and mutation rate for termination). \nCoLoRS uses thread-local allocation buffers (TLABs) to reduce allocation cost. Each thread performs bump\u00adpointer \nunsynchronized allocation in its own TLAB. Once the TLAB is exhausted, it is retired, and the thread \nrequests a new one. VMs request TLAB-and large-object-allocation directly from the object space. The \nfreelist contains all un\u00adallocated blocks whose size is at least the TLAB size. The freelist is protected \nby a lock. 3.4.1 GC Algorithm Our GC comprises four concurrent phases: .ag clearing, root report, marking, \nand sweeping. The CoLoRS server initiates a new GC cycle as soon as the heap usage crosses a speci.ed \nthreshold. The main GC thread is awoken by an allocating thread once this happens. CoLoRS GC imposes \nno pauses. If a VM is capable of reporting shared roots without causing internal pauses (e.g. as Python \ncan), then the system never needs to pause any threads. Flag clearing. The main GC thread .rst clears \nall GC\u00adrelated .ags in the heap. This operation is fully concurrent. Each object has three GC .ags: pending \n(i.e. it needs to be recursively marked), marked (i.e. it has been recursively marked), and recent (it \nhas been recently allocated). Unlike in extant SATB GCs, in CoLoRS, the snapshot mode is active all the \ntime. This simpli.es the algorithm as it avoids complex state transitions and handshakes. The snapshot \nmode means that all objects are allocated live (i.e. with the recent .ag set) and mutators use a write \nbarrier: on pointer stores they mark the overwritten pointer as live (i.e. they set the pending .ag). \nWhen GC scans a live object it sets its marked .ag. During the .ag-clearing heap scan, the main GC thread \nalso computes a fully-balanced heap partitioning that is used later on for parallel scanning. The key \nsystem invariant is that it is always possible to sequentially scan all blocks in the heap, without any \nsynchronization. We carefully design allocation procedures so that we do not break this invariant. GC \n.ag clearing has a similar effect to activating the snapshot mode from scratch in other algorithms, but \ndoes not require handshakes. Once GC .ags are cleared, the main GC thread requests root dumps from all \nattached VMs. Root report. Each VM must be able to identify pointers into shared memory in its private \nheap/stacks in an ef.cient way. In VMs using tracing GC this is straightforward we either scan the whole \nheap (non-generational GC) or use a card table (generational GC). In the latter case (e.g. in Java), \nwe extend the card table so that we can quickly .nd not only pointers from the old generation(s) to the \nyoung gen\u00aderation but also pointers from the old generation(s) to shared memory. To report shared roots \nin this case, we simply trig\u00adger a fast minor collection and ef.ciently .nd all pointers to shared memory. \nIn VMs which use reference counting GC (e.g. cPython), CoLoRS can track shared roots as they are created \nand de\u00adstroyed, thus being able to report them any time without any processing. For each shared reference, \nwe create a small proxy object in private memory with reference count set to one. Once the proxy object \nbecomes unreachable (which we know immediately thanks to reference counting) we reclaim it and forget \nthe shared root. Note that only private refer\u00adences can exist to the proxy object since there are no \nshared\u00adto-private pointers.  CoLoRS requests roots from each VM and waits until all reports arrive. \nTo report a shared root, a VM sets the object s pending .ag. To ensure store visibility, a memory fence \ntakes place on both sides once the reporting completes. CoLoRS does not use timeouts because it detects \nVM termination in a reactive way via TCP/IP sockets. Termination is noticed right away and the exited \nVM is removed from the waiting\u00adfor-roots list. Marking. As soon as all roots are reported, the main GC \nthread initiates parallel, concurrent marking done by several worker GC threads. Each worker thread scans \nits own heap partition looking for pending objects, and recursively marks them using depth-.rst search. \nTo ensure dynamic load bal\u00adancing during marking, worker GC threads employ random\u00adized work stealing. \nGC threads use barrier synchronization to meet at subsequent GC phases. Once .rst marking completes, \nthe main GC thread enters a loop. During each iteration, CoLoRS performs parallel, concurrent marking \nfrom pending objects. However, this time it stops marking the object graph once its sees an object with \nthe recent .ag set. The loop terminates when no new objects have been marked. Stopping marking on recently\u00adallocated \nobjects guarantees GC termination there is a .nite number of old objects in the heap when the GC starts, \nand all the newly-allocated objects are being .agged as recent. Therefore, GC must .nish in a .nite number \nof steps. This scheme is correct because after the 1st iteration, a recently-allocated object cannot \nhave a pointer to an object that is live but otherwise unreachable and invisible to GC (and thus it cannot \nbe incorrectly left unmarked). Note that such a situation may occur during the .rst marking pass, which \nmarks from the VM roots. Our snapshot write barrier (SATB WB) does not capture root pointer updates \nit only captures heap stores. Suppose that root r points to object O, and a new object N is allocated \nhaving its only pointer set to O. If root r is later updated to point to N, we end up with a newly-allocated \nobject N that has a pointer to a live object O that is reachable only through N. The reason for this \nis that we do not notice root updates. Such a situation is impossible from the second marking on, as \nduring 2nd and subsequent markings we ignore roots and mark from the pending .ags only (i.e. from heap \nobjects that are protected by SATB WB). Reconsidering our example in the heap context: object O is marked \nas pending on r update, and will be marked/scanned even if we stop marking on object N (which has its \nrecent .ag set). Sweeping. As soon as the marking loop terminates, CoL\u00adoRS moves on to concurrent, parallel \nsweep. Each worker GC thread scans its heap chunk trying to .nd the .rst potentially-free (candidate) \nblock. This scan is done with\u00adout synchronizing with mutators that are actively allocating objects. Once \na GC thread .nds a candidate block, it ac\u00adquires the freelist lock and continues the scan as long as \nit encounters reclaimable blocks. Finally, it removes all found dead blocks from the freelist and inserts \none coalesced block into the freelist. The GC thread releases the freelist lock and looks for the next \ncandidate block. Our GC-mutator contract guarantees that all block headers are always parsable.   4. \nCoLoRS Implementation CoLoRS can work under any OS that supports adequate IPC functionality. We have \nimplemented CoLoRS in HotSpot JVM 1.6 and cPython 3.1 under Linux. The .rst step in the process of extending \na VM with CoLoRS support is to determine the VM object/class model, its relationship to the CoLoRS OM, \nmemory management (GC) algorithm(s), and operations that use objects, typically .eld access, method calls, \nsynchronization, etc. Next, we de.ne type mapping for builtins and user-de.ned types, and add any runtime \nextensions (such as multiple inheritance) to support it. The next step is heap access virtualization \nwhich amounts to extending an interpreter, a JIT compiler, or both, to provide a separate control path \nfor handling shared objects. Depending on a VM, other components may need similar extensions, e.g. the \nGC subsystem. Typically, we must intercept all program instructions that read/write heap objects. Next, \nwe insert calls to the CoLoRS API along the newly added control paths. This step translates VM\u00adspeci.c \noperations into VM-neutral operations (e.g. getting an attribute by name into getting a .eld by offset). \nLastly, we add GC runtime support we implement a dedicated CoLoRS thread and the shared-root-dump operation \nin the private GC system. 4.1 Shared Memory Segment The CoLoRS shared memory segment contains three \nspaces: metadata, classes, and objects. The objects space is a garbage\u00adcollected mark-sweep heap with \nTLAB/free-list allocation. The classes space is a bump-pointer space for immortal ob\u00adjects that contains \nshared classes, class version lists, and registered object repositories/channels. The metadata space \ncontains pointers to all builtin types (in the classes space), pointers to the repositories/channels \nhash tables (mapping names to repositories/channels), a pointer to class versions hash table (mapping \nnames to class version lists), as well as user-level monitors, internal system locks, the freelist head, \nspace usage statistics, and the bump-pointer top (for the classes space). Each CoLoRS monitor has its \nPOSIX mutex and condi\u00adtion variable. We use the PTHREAD PROCESS SHARED .ag to make the POSIX mutexes \nand conditions work across OS processes. In addition, monitors use the recursion count (to avoid re-locking \nby the same thread) as well as owner ID (VM ID plus thread ID). The CoLoRS server maintains additional \nstate (metadata) in private memory to manage GC threads, and to track the attached VMs. For each attached \nVM, there is a dedicated monitoring thread, which detects VM termination using an open TCP/IP connection \nto a VM. On VM termination, the monitoring thread receives an error when reading from a closed socket. \nNote that OS-level IPC (e.g. sockets) is the only reliable way of detecting process termination without \nresorting to timeout/keep-alive solutions. This is because in Unix systems certain signals (e.g. the \nKILL signal) cannot be intercepted.  We group class versions into lists based on their name. Object \nrepositories/channels and classes are permanent enti\u00adties we do not collect them as they are small and \nreusable. Object repositories/channels are treated as GC roots during GC. GC .ags are implemented as \none-byte-wide .elds be\u00adcause of concurrent access. We assume that writes issued by a particular thread \nare visible to other threads in the order they are issued (sequential consistency guarantees this). The \nobjects space is a contiguous sequence of blocks. Each block can be an object, a free chunk (part of \nthe freel\u00adist), or a TLAB. The block header contains two .elds: block length and block type. This enables \nquick traversal of the heap without parsing actual objects a key property for our concurrent GC. TLAB \nblocks contain an owner ID, which identi.es the VM that is currently using the TLAB. This enables us \nto reclaim TLABs orphaned by asynchronously terminated VMs. To provide transparent object sharing, CoLoRS \nintercepts all VM operations that access heap memory. To ef.ciently check whether an object is shared, \nCoLoRS uses a constant border between private and shared area in the virtual mem\u00adory. Each memory-related \noperation, such as .eld access, compares the pointer value against this constant border.  4.2 HotSpot \nJVM In static runtimes with high-performance, adaptively opti\u00admizing compilers, border-checks may be \nexpensive as they make the intermediate code larger and more dif.cult to op\u00adtimize. Therefore, in our \nCoLoRS implementation in the HotSpot JVM server compiler, we compile methods in two modes: CoLoRS-aware \nand CoLoRS-safe. The CoLoRS\u00adaware mode is used for methods in which shared mem\u00adory has been determined \n(via pro.ling during interpretation) to be commonly-used. For such methods, border-checking overhead \nand the additional code that handles the shared pointers are acceptable. The remaining methods (a vast \nmajority in practice) are compiled in the CoLoRS-safe mode, where private pointers are the common case. \nThe CoLoRS-safe methods contain only the minimum number of border-checks needed to take a trap on shared \npointers. Such traps deoptimize the method and recompile it as CoLoRS-aware, running the method in the \ninterpreted mode in the meantime. The CoLoRS-aware methods use fast upcalls to C to handle shared pointers \n(CoLoRS is implemented in C). If fast upcalls fail (e.g. be\u00adcause class loading is needed), we bail out \nto the interpreter. In CoLoRS-safe methods, we combine null checks with shared-border checks. Assuming \nthat shared memory area is at lower virtual addresses than the private area, check\u00ading if a pointer is \nbelow the border detects both NULL pointers and shared pointers. If the check passes, we trap to the \ninterpreter, which .nds the actual cause of a trap itself (the trap cost is not a problem as it is the \nuncommon case path). In CoLoRS-aware methods we guard virtual method calls to prevent calling into a \nCoLoRS-safe method with a shared receiver (such calls need a trap). CoLoRS-safe meth\u00adods must translate \nuser-provided null checks into null-and\u00adborder checks to avoid eliding border checks along with null \nchecks. We also perform approximate data .ow analysis which conservatively computes all methods which \ncan operate on a pointer to a shared object. The analysis exploits the fact that shared pointers can \nonly be produced by the meth\u00adods from the CoLoRS API. We dynamically and incre\u00admentally build the call \ngraph as classes are loaded. In the graph, nodes represent methods and there is an edge from node m to \nn, if method m can pass/return a reference to method n. In case of interface methods, we have ad\u00additional \nedges leading to all implementors of a particular method. We divide all loaded methods into two classes: \npri\u00advate and potentially-shared. Private methods can never reach shared objects. If any potentially-shared \nmethod contains the putstatic bytecode, then we assume all methods containing the getstatic bytecode \nto be potentially-shared. Otherwise, if a method is reachable from a potentially-shared method in the \ncall graph, that method is also considered potentially\u00adshared. Potentially-shared methods are compiled \nas either CoLoRS-aware or CoLoRS-safe, depending on the pro.ling data. Private methods do not contain \nany instrumentation. If class loading makes a previously-private method potentially\u00adshared, we make the \nmethod non-entrant and recompile it. CoLoRS intercepts all bytecode instructions that access objects \nin the heap (both .elds and object header): put\u00ad.eld, get.eld, arrayload, arraystore, invoke, monitor-related \nones, arraylength, and objectclass. We extend the HotSpot template interpreter and the server compiler \n(both targeting amd64). In addition, we virtualize the HotSpot runtime writ\u00adten in C (biased locking, \nGC, class loading, JNI, JVM, JMM, JVMTI). Several internal classes are not allowed to be in in\u00adstantiated \nin shared memory (e.g. Thread, ClassLoader) they are VM-speci.c and do not make sense in the context \nof other VMs.  4.3 cPython Runtime We virtualize shared objects via private proxy objects, each containing \na forwarding pointer to a shared object and a nor\u00admal Python header (comprising private type and a reference \ncount). This design choice is dictated by the fact that Python uses reference counting GC and CoLoRS \nuses tracing GC (so there is no reference counts in shared object headers). The cost of one level of \nindirection is compensated by the fact that we do not need to perform type mapping on each shared object \naccess proxy objects have their private type computed once. All proxy objects have the same size and \nare bucket-allocated in a dedicated memory region (for fast bor\u00adder checks). Deallocation takes place \nonce a reference count drops to zero. Thus, the number of proxies never exceeds the number of private-to-shared \npointers. Finding shared roots in such a setting is fast and amounts to a linear scan of the proxy object \nregion.  Proxy objects also simplify Python runtime virtualiza\u00adtion, as the Python interpreter dispatches \nbasic operations such as .eld access, method call, and operator evaluation, based on object type (note \nthat proxies already have the proper private type set). We provide a new private type for each builtin \nshared type, and the interpreter automatically in\u00advokes the right implementation (shared/private). Python \nVM allocates only one global TLAB because the interpreter is single-threaded and simulates multi-threading \nby context\u00adswitching between program threads. The Python runtime component most complex to virtualize \nare standard libraries and builtin types, which provide rich, complex interfaces (e.g. for sorting, concatenation, \nset algebra, etc).  5. Related Work CoLoRS is unique in that it supports type-safe, transparent, and \ndirect object sharing via shared memory between man\u00adaged runtimes for different static/dynamic object-oriented \nlanguages. To enable this, CoLoRS contributes a language\u00adneutral object/memory model as well as a synchroniza\u00adtion \nmechanism and concurrent/on-the-.y GC, all designed speci.cally for multi-VM cross-language object sharing. \nCoLoRS takes a top-down approach to object sharing. That is, we assume full isolation between the runtimes \nvia operating system (OS) process semantics and provide a mechanism for object sharing within this context. \nSeveral previous systems [6, 15, 22] took a bottom-up approach by executing multiple applications in \na single OS process and providing software-based isolation between them. State-of-the-art systems that \nsupport type-safe, cross\u00adlanguage communication for OO languages, such as OMG CORBA [13], Apache Thrift \n[35], Google Protocol Buffers [32], SOAP, and REST, target distributed systems and rely on message-passing \nand data serialization. CoLoRS differs from these systems in that it targets co-location and transpar\u00adent \nshared memory (as opposed to explicit message passing). Although one can use CoLoRS to implement an ef.cient \ncross-language RPC for the co-located case (similar in spirit to LRPC [7]), CoLoRS is more general than \nRPC systems and differs from them in terms of both architecture and pro\u00adgramming model. The XMem system \nby Wegiel and Krintz [37] is most related to ours. XMem provides direct object sharing be\u00adtween JVMs. \nXMem also takes a top-down and transpar\u00adent approach, but does not support sharing between het\u00aderogeneous \nlanguages and requires global synchronization across runtimes (which CoLoRS avoids) for such opera\u00adtions \nas garbage collection, class loading, shared memory attach/detach, and communication channel establishment. \nSystems supporting communication between isolated tasks within a single-language, single-process runtime \nin\u00adclude Erlang [3], KaffeOS [6], MVM [15], Alta [5], GVM [5], and J-Kernel [36]. These systems take \na bottom-up ap\u00adproach which provides weaker isolation (i.e. weaker pro\u00adtection guarantees than the CoLoRS \napproach) and is more complex to implement. Unlike CoLoRS, they replicate OS mechanisms within a single \nOS process instead of leverag\u00ading existing hardware-assisted inter-process isolation. Language-based \noperating systems also provide mecha\u00adnisms for communication and interoperation between pro\u00adcesses [8, \n20, 22, 25, 27 29, 34, 38]. Such systems typically implement support for light-weight processes that \nshare a single address space and provide compiler support to guar\u00adantee type and control safety within \nand between processes. To facilitate the latter, these systems require that the compo\u00adnents (processes/tasks) \nbe written in the same safe/checkable language. In addition, since CoLoRS is not an operating sys\u00adtem, \nit is signi.cantly simpler. Some concurrent languages provide direct support for inter-process communication \nbetween light-weight pro\u00adcesses [4, 20, 31] written in the same language. The key difference between \nthese systems and CoLoRS is that they employ share-nothing semantics for message-based commu\u00adnication \nwhereas CoLoRS provides support for direct object sharing when runtimes are co-located on the same physical \nmachine. CoLoRS is also distinct from distributed shared mem\u00adory and single system image runtimes for \nclusters such as MultiJav [10], cJVM [2], JESSICA [30], Split-C [14], and UPC [21]. In contrast to them, \nCoLoRS provides a uni\u00adform cost for accessing all objects (private and shared) and does not target distributed \ncomputing. These systems pro\u00advide sharing between code written in the same language, and focus on guaranteeing \nmemory consistency and cache coherence for concurrent access to objects across multiple machines.  6. \nExperimental Evaluation An important practical use case for CoLoRS is improving communication performance \nof RPC in the co-located case. We evaluate CoLoRS in this context because there are cross\u00adlanguage RPC \nframeworks, such as CORBA, Thrift, Proto\u00adcol Buffers, and REST, to which we can compare. CoLoRS, however, \nprovides signi.cantly more functionality over ex\u00adtant cross-language RPC systems by enabling direct, \ntype\u00adsafe, and transparent object sharing. We compare CoLoRS-based RPC against extant RPC frameworks \nin terms of communication performance (i.e. latency and throughput). We also evaluate end-to-end server\u00adclient \nperformance (response time and transaction rate) for two applications: Cassandra and HDFS. Finally, we \nmeasure the overhead of CoLoRS in programs that do not employ shared memory, using standard community \nbenchmarks for Java and Python.  6.1 Methodology Our experimental platform is a dedicated machine with \na quad-core Intel Xeon and 8GB main memory. Each core is clocked at 2.66GHz and has 6MB cache. We run \n64-bit Ubuntu Linux 8.04 (Hardy) with the 2.6.24 SMP kernel. We use HotSpot JVM from OpenJDK 6 build \n16 (April 2009) compiled with GCC 4.2.4 in the 64-bit mode. Our con\u00ad.guration employs the server (C2) \ncompiler, biased locking, and parallel GC (copying in the young generation and com\u00adpacting in the old \ngeneration). For the Python runtime we use the open-source cPython 3.1.1 (released August 2009) compiled \nwith GCC 4.2.4 in the 64-bit mode. To measure CoLoRS overhead in Java, we use Da\u00adCapo 08 and SPECjbb \n( 00 and 05). We set the heap size to 3.5x the live data size so that GC activity does not dominate performance \nand so that we capture all sources of overhead. We use the default input for DaCapo and 5 warehouses, \nwith 90s runs, for SPECjbb. In Python, we evaluate CoLoRS overhead using PyBench (a collection of tests \nthat provides a standardized way to measure the performance of Python implementations), a set of Shootout \ncPython benchmarks (from [12]), and PyStone (a standard synthetic Python benchmark). In all experiments, \nwe repeat each measurement a min\u00adimum of seven times. For experiments that employ shared memory, we perform \nsuf.cient iterations to guarantee that GC is performed by CoLoRS. We report average values. The standard \ndeviation is below 5% in all cases. CoLoRS reserves 256MB in shared memory for objects and 64MB for classes. \nWe use 32KB TLABs, and 2 parallel GC threads. In each experiment, we employ two co-located runtimes: \nPython and Java. Whenever running an unmodi\u00ad.ed (CoLoRS-unaware) JVM, we set its heap size to 300MB so \nthat its private memory is comparable in size to the shared memory. Note that our results underestimate \nCoLoRS potential since we implement CoLoRS in Python 3.1 and compare its communication performance with \nRPCs running on Python 2.6. This is because the RPC frameworks that we use have not yet been ported to \nPython 3.1. To quantify this differ\u00adence we evaluate the performance of Python 3.1 relative to Python \n2.6. The last Column in Table 5 shows the overhead of Python 3.1 relative to Python 2.6 across our set \nof bench\u00admarks. On average, Python 3.1 is slower by 20%.  6.2 CoLoRS Impact on Communication Performance \nWe .rst evaluate the performance potential of CoLoRS\u00adbased RPC using communication microbenchmarks with \na range of message types and sizes. We implement equiva-Figure 3. Average execution time (in seconds) \nfor CoLoRS (left) and CORBA (right) experiments.  lent microbenchmarks using RPC frameworks for CORBA, \nThrift, Protocol Buffers, and REST. We compare RPC la\u00adtency and throughput (call rate). For the implementation \nof the microbenchmarks, we use a Python client and a Java server. Whenever possible we em\u00adploy RPC methods \nwith fully symmetric input and output (i.e. returning a data structure similar to the data structure \npassed in as an argument). This ensures that the server and the client exercise data structure (de-)serialization \nin a sym\u00admetric way. To evaluate RPC throughput, we vary method input/output size between 1 to 1024 units \nand measure mean time per method call. Next, we use least-squares linear regression to compute throughput \nfrom the coef.cients in the equation time = latency + size/throughput. We calculate latency as the mean \ntime needed per call for unit input/output. We employ this methodology because we have observed that \nfor small input sizes the function time(size) is sometimes non\u00adlinear and approximating it by a line \nleads to an inaccurate latency estimation. Each RPC method call takes a list as input and returns a list \nas output. List sizes vary between 1 and 1024. For each list size we do 10 experiments and use their \naverage in the calculation above. We use several different objects as list elements, including built-in \nprimitive types (string, integer, .oat, and boolean) and user-de.ned types. For the latter we employ \nbinary trees, the depth for which ranges between 1 and 4 levels, and each node contains 4 primitive .elds. \nThis enables us to investigate both shallow-and deeply\u00adlinked data structures. The above choice is also \ndictated by the limitations of extant RPC frameworks which support a small set of builtins and do not \nsupport recursive data structures. (Note that CoLoRS provides a richer and more .exible object model \nthan these RPC systems.) We implement an RPC endpoint in CoLoRS as a message queue on which a server \nwaits for messages (call requests). Each message is an object encapsulating input and output. A client \nissues a call by allocating a message object (and the associated input) in shared memory, enqueuing it, \nand Table 2. Throughput for microbenchmarks. For each data type, we show the throughput in calls per \nmillisecond; in parentheses, we show the CoLoRS/RPC throughput ratio. tree : n means the type is a full \nbinary tree of depth n.  RPC boolean Throughput in calls/millisecond; CoLoRS/RPC in parenthesis integer \n.oat string tree:1 tree:2 tree:3 tree:4 CORBA 173.22 (11) 82.67 (26) 83.20 (27) 75.96 (15) 14.67 (13) \n4.68 (15) 1.83 (17) 0.86 (17) ProtoBuf 31.73 (59) 30.98 (70) 34.32 (65) 26.43 (43) 2.85 (68) 0.88 (78) \n0.36 (85) 0.17 (91) REST 23.17 (81) 22.45 (97) 21.89 (102) 22.94 (50) 8.73 (22) 2.66 (26) 0.91 (34) 0.31 \n(49) Thrift 237.04 (8) 283.23 (8) 274.37 (8) 149.08 (8) 15.38 (13) 4.27 (16) 1.80 (17) 0.87 (17) CoLoRS \n1876.08 (1) 2175.32 (1) 2231.45 (1) 1144.87 (1) 193.66 (1) 68.61 (1) 30.61 (1) 15.08 (1) RPC boolean \nLatency in milliseconds; RPC/CoLoRS in parenthesis integer .oat string tree:1 tree:2 tree:3 tree:4 CORBA \n0.62 (14) 0.65 (19) 0.62 (14) 0.63 (14) 0.68 (17) 0.82 (15) 1.13 (17) 1.92 (19) ProtoBuf 0.22 (5) 0.31 \n(9) 0.21 (5) 0.23 (5) 0.55 (14) 1.32 (23) 2.90 (44) 6.02 (58) REST 3.89 (90) 3.89 (113) 4.00 (89) 3.92 \n(90) 4.07 (101) 4.80 (85) 7.35 (111) 9.94 (96) Thrift 0.09 (2) 0.10 (3) 0.11 (3) 0.12 (3) 0.19 (5) 0.35 \n(6) 0.74 (11) 1.38 (13) CoLoRS 0.04 (1) 0.03 (1) 0.04 (1) 0.04 (1) 0.04 (1) 0.06 (1) 0.07 (1) 0.10 (1) \n Table 3. Latency for microbenchmarks. For each data type, we show the latency in milliseconds; in parentheses, \nwe show the RPC/CoLoRS latency ratio. tree : n means the type is a full binary tree of depth n. notifying \nthe server. The server removes the request from the queue and generates the output in shared memory. \nFinally, the server noti.es the client that the result is ready (as the output .eld in the message object). \nFor all experiments, we report throughput as the number of calls per millisecond, and latency in milliseconds. \nDue to space constraints, we only present timings graphs that com\u00adpare CORBA to CoLoRS. This data is \nshown in Figure 3. The x-axis is message size and the y-axis is time in seconds. This data is representative \nof all of the RPC experiments. We summarize the latency and throughput of each below. Table 2 shows throughput \nacross all microbenchmarks and RPC systems. We report both absolute values and rel\u00adative improvement \ndue to CoLoRS. Table 3 uses a similar format but presents results for our latency measurements. CORBA. \nThe Common Object Request Broker Archi\u00adtecture (CORBA) [13] standardizes object-oriented RPC across different \nplatforms, languages, and network proto\u00adcols. A client and a server use automatically-generated stubs \nand skeletons to (de)marshall arguments and return values for methods speci.ed in the Interface De.nition \nLanguage (IDL). To implement our CORBA benchmarks, we use the org.omg.CORBA package and the idlj compiler \nin Java and the Fnorb module and the fnidl compiler in Python. Our measurements indicate that, compared \nto CORBA, CoLoRS achieves 11 27 times better throughput and 14 19 times lower latency. Thrift. Thrift \nis a framework originally developed at Face\u00adbook for scalable cross-language RPC. Like CORBA, Thrift \nrequires a language-neutral interface speci.cation from which it generates client/server template code. \nHowever, Thrift is simpler and much more lightweight than CORBA. We use Apache Thrift version 2008/04/11. \nOur experiments show that CoLoRS improves throughput by 8 17 times and latency by 2 13 times, over Thrift. \nWe also .nd that Thrift achieves much better performance for builtin types than for user-de.ned types. \nProtocol Buffers. Protocol Buffers (PB) are a language\u00adneutral, platform-neutral, extensible mechanism \nfor serial\u00adizing structured data, developed by Google engineers as a more ef.cient alternative to XML \n[32]. To use PB, develop\u00aders specify message types in a .proto .le, and a PB compiler generates data \naccess classes that allow to parse/encode ob\u00adjects into a bytes buffer/stream. We use PB version 2.2.0, \nwhich includes message parsers and builders but does not support RPC. Therefore, we implement RPC on \ntop of PB by using PB serialization and communication over TCP/IP sockets. We maintain a single TCP connection \nthroughout each experiment. Each message that we send from a client to a server, contains a method tag, \nmessage length, and PB\u00adserialized data structure (method input). CoLoRS improves the throughput of PB-RPC \nby 43 91 times and latency by 5 58 times. REST. REpresentational State Transfer (REST) [23] is a client-server \narchitecture based on HTTP/1.0 where requests and responses are built around the transfer of representations \nof resources. REST provides stateful RPC by exchanging documents that capture the current or intended \nstate of a resource. Individual resources are identi.ed in requests by URIs. In our benchmarks, we de.ne \na single resource stored on a server and identi.ed by http://localhost:8080/ db/items. A representation \nof this resource is an XML document containing all stored items. Clients send GET requests to the resource \nURI, and parse the resulting XML document. This document contains a varying number of items (1 1024), \nwhere each item is either a primitive or a user-de.ned object. We employ the Python restful lib to implement \nthe client and the Java restlet (version 1.1.6) for the server. Relative to REST, CoLoRS throughput is \n22 102 times higher and latency is 85 113 times lower. REST has the highest latency among all of the \nRPC technologies that we investigate because of the verbose data format and parsing overhead of XML. \n   6.3 CoLoRS GC We gathered basic GC statistics for our Java-Python mi\u00adcrobenchmarks. The results \nare similar across all the pay\u00adloads that we use (described in the previous Section). Below we discuss \nthe experimental data obtained for 4-level binary trees. We set the GC triggering threshold to 70%. Average \ntime between subsequent GC cycles is 1458ms while average GC cycle time is 325ms (GC is active 18% of \nthe time). Note that GC runs concurrently in a separate process. The clearing phase takes 94ms on average \n(29% GC cycle). The root dump phase was 1.2ms on average (below 0.4% GC cycle). In the HotSpot JVM, each \nroot dump request causes a STW pause which averages at 0.8ms (with the maximum pause of 2.9ms). In cPython \nthere is no pauses. The marking phase takes 116ms on average (36% GC cycle). Two object graph scanning \niterations suf.ce on average (the maximum is 3). The sweep phase averages at 113ms (35% GC cycle). The \ndominating GC phases are marking, sweeping, and clearing, each taking around 1/3 of each GC cycle.  \n6.4 CoLoRS Impact on End-to-End Performance To lend insight into the CoLoRS potential when used by actual \napplications, we investigate two popular server-side software systems: Cassandra [1] version 0.4.1 and \nHDFS [26] version 0.20.1. Cassandra is a highly scalable, eventu\u00adally consistent, distributed, structured, \npeer-to-peer, key\u00advalue store developed by Facebook engineers. HDFS is the Hadoop Distributed File System \n a .le system server that provides replicated, reliable storage of .les across cluster resources. Both \nof these systems are employed for a wide range of web applications, e.g. MapReduce, HBase (open\u00adsource \nBigTable implementation), email search, etc. Cassandra and HDFS both expose Thrift-based inter\u00adfaces. \nThese interfaces provide a set of query/update meth\u00adods which use relatively complex data structures \n(e.g. maps). Query methods are natural candidates for in-memory result caching, recently a common approach \nto scaling up servers (e.g. MemchacheD, MySQL cache). If caching is used, then in the common case (i.e. \non cache hit), server processing is minimal and therefore communication constitutes a large portion of \nthe end-to-end performance. In systems with in-memory caching, CoLoRS can im\u00adprove performance in two \nways. First, it can reduce RPC cost by avoiding serialization. Second, part of the in-memory Figure 4. \nAverage execution time (in seconds) for Cassan\u00addra (left) and HDFS (right) vs. CoLoRS .  cache can be \nkept in shared memory immutable objects such as strings can be shared by multiple clients without the \nrisk of interference. As a result, CoLoRS can provide copy semantics without actually copying data. To \ninvestigate both these scenarios, we extend Cassandra and HDFS with in\u00admemory caches for particular queries \nand evaluate the ef.\u00adcacy of using CoLoRS for these queries, on end-to-end per\u00adformance. Note that when \ncaching is used, the benchmarks exercise not only copying to shared memory but also fre\u00adquent access \nto shared objects (which includes translation overhead). For Cassandra, we implement caching for the \nget key range query (parameterized by table name, column family, start value, end value, maximum keys \ncount, and consistency level). The query returns a list of keys matching the given criteria. Updaters, \nsuch as insert and remove, detect con.ict\u00ading modi.cations and invalidate the cache accordingly. The \ncache is kept on the server and maps inputs (serialized to a string) to responses. Cached responses are \npartially in shared memory (strings are immutable). Thus, CoLoRS has the po\u00adtential for improving performance \nby avoiding serialization and reducing copying overhead. For HDFS, we implement an in-memory cache for \nthe listStatus call, which, given a directory name, generates a list of FileStatus objects, each describing \n.le attributes, name, owner, permissions, length, and modi.cation time. The cache is a map from path \nname to responses, which we partially store in shared memory. Cache invalidation hap\u00adpens on con.icting \n.le system operations: create, append, write, rm, rename, mkdirs, chmod, and chown. Figure 4 presents \nthe timing data for Cassandra and CoL\u00adoRS (left graph) and HDFS and CoLoRS (right graph). The x-axis \nis message size and the y-axis is time in seconds. We use this data to compute latency and throughput, \nwhich we summarize in Table 4. Columns 2 3 show transaction rate (per millisecond) while Columns 4 5 \npresent response time (in ms). We use one cache warmup iteration followed by 10 iterations during each \nof which we vary the query result size between 1 and 1024 entries. In each Column group, we re\u00adTable \n4. End-to-end performance for Cassandra and HDFS with caching. The third and .fth Column show number \nof times improvement due to CoLoRS for throughput and la\u00adtency, respectively.  Throughput Latency Server \nqueries CoLoRS in App/ Application per ms /App ms CoLoRS Cassandra 249.50 19 0.12 3 HDFS 12.03 20 0.19 \n3 Bench\u00admark Python 3.1 time (s) CoLoRS 3.1 % OHead Python 2.6 %Impr binary-trees 6.79 3.39 -0.44 fannkuch \n1.97 4.57 24.68 mandelbrot 15.32 7.18 66.52 meteor-contest 2.25 1.78 32.35 n-body 8.67 2.08 7.04 spectral-norm \n14.31 5.73 18.85 pybench 3.92 5.20 1.18 pystone 4.09 5.87 12.98 Average 7.17 4.48 20.40 Table 5. The \noverhead of CoLoRS support for Python (and for the use of Python v3.1 over v2.6). Column 2 is execution \ntime in seconds. Column 3 shows the percent degradation due to CoLoRS. Column 4 shows the percent improvement \nin performance when we use Python 2.6 (over 3.1). port measurements for the server without CoLoRS and \nthe relative improvement due to CoLoRS. For cache-enabled Cassandra, CoLoRS improves transaction rate \nby 19 times and reduces response time by 3 times. For cache-enabled HDFS, CoLoRS improves transaction \nrate by 20 times and decreases response time by 3 times.  6.5 CoLoRS Overhead To implement CoLoRS, we \nvirtualize components of Java and Python runtimes. This includes standard libraries, ob\u00adject .eld access, \nsynchronization, method dispatch, inter\u00adpreter, dynamic compiler, allocation, and GC. Doing so pro\u00advides \ntransparency, but introduces execution time overhead. To evaluate this overhead, we compare unmodi.ed \nrelease versions of Python 3.1 and Java 1.6 with their CoLoRS coun\u00adterparts. Table 5 shows Python results. \nIn Column 2, we report per-benchmark execution times for unmodi.ed Python 3.1. Next, in Column 3, we \npresent the CoLoRS overhead percentage increase in execution times relative to Column 2. Across our \nbenchmarks, the average CoLoRS overhead is 4%. Note that scripting languages are not concerned with enabling \nhigh-performance (they are interpreted and much slower than statically compiled code). Table 6 shows \nthe Java results. For each benchmark, we report its heap size and execution time (for DaCapo the top \nTable 6. The overhead of CoLoRS runtime support for Java. Column 3 is execution time (ET) in seconds \nfor all but jbb 00 and jbb 05 for which we report throughput (TP). Column 4 shows the percent degradation \ndue to CoLoRS. Bench\u00admark Heap Size ET or TP CoLoRS Support % Overhead antlr 7 2.40 8.4 bloat 28 6.34 \n6.3 chart 42 6.19 6.1 eclipse 115 24.54 4.7 fop 28 2.11 7.7 hsqldb 280 3.35 3.6 jython 3 8.35 4.5 luindex \n7 7.50 9.0 lusearch 45 4.25 1.4 pmd 56 6.92 8.6 xalan 105 5.97 -0.6 jbb 00 900 112726 5.3 jbb 05 900 \n54066 1.3 11 benchmarks) or throughput (for SPECjbb), and percent\u00adage CoLoRS overhead (Column 4). Across \nthe benchmarks, the average CoLoRS overhead is 5%.  6.6 Sockets vs. Shared Memory We also investigate \nthe relative performance of shared\u00admemory-based transport (SMTx) and local-socket-based transport (LSTx). \nThis enables us to determine how much performance improvement is due to the use of shared mem\u00adory versus \nof sockets and due to avoiding object serializa\u00adtion. In this experiment, we extend the Thrift RPC framework \nfor Java with SMTx and compare it with the LSTx already built into Thrift (using our microbenchmarks \ndescribed in Section 6.2). We have implemented SMTx in Thrift on top of a bidirectional FIFO channel \nin a shared memory segment and POSIX mutexes/conditions. We focus on Java and Thrift here because of \ntheir high-performance characteristics. We observe that Thrift over LSTx attains better through\u00adput \nthe improvement ranges from 1.7x (for the integer payload) to 3.2x (for 4-level binary trees) and averages \nat 2.7x. At the same time, Thrift over SMTx has lower latency for small messages (by up to 29% for the \ninteger payload) and higher latency for larger payloads (by up to 0.8x for 4\u00adlevel binary trees), while \naveraging at 9% lower latency than Thrift over LSTx. The fact that Thrift/LSTx achieves better overall \ncommu\u00adnication performance than Thrift/SMTx can be attributed to a more ef.cient sockets implementation \n(in the kernel) than our shared-memory queue implementation (in user-land). In the kernel, there is more \ncontrol over memory mapping and thread scheduling, both of which can be used to optimize sockets implementation \n(e.g. to reduce the amount of copy\u00ading and thread context switching).  Based on this experiment, we \ncan conclude that CoLoRS improves throughput and latency because it avoids serializa\u00adtion and not because \nit uses shared memory instead of sock\u00adets.  6.7 Summary CoLoRS can improve communication performance \nsigni.\u00adcantly when runtimes executing interoperating components (potentially written in different languages) \nare co-located on the same physical system, compared to extant type-safe cross-language RPCs (latency \n2 113 times and throughput 8 102 times). In systems with short request processing times (e.g. servers \nwith caches) this improvement can translate to large end-to-end performance gains (19 20x for transac\u00adtion \nrates and 3x for response times). As more and more components are co-located on multi-cores and caches \nbe\u00adcome prevalent in servers, object sharing systems like CoL\u00adoRS have a growing potential for increasing \nperformance of multi-component, multi-language systems.  7. Conclusions We present the design and implementation \nof CoLoRS, a system supporting cross-language, cross-VM, type-safe shared memory for co-located VMs. \nCoLoRS contributes a language-neutral object/class/memory model for static and dynamic OO languages, \nas well as an on-the-.y, concurrent GC and a monitor synchronization mechanism both adapted and extended \nto support language-and runtime-independent object sharing. We implement and evaluate CoLoRS within runtimes \nfor Python and Java. CoLoRS imposes low overhead when there is no use of shared memory (4% for Python \nand 5% Java) due to virtualization of runtime services and libraries. An important use case for CoLoRS \nis improving the perfor\u00admance of RPC protocols in the co-located case. We have found that for microbenchmarks \nCoLoRS increases through\u00adput by 8 102 times and reduces latency by 2 113 times. CoLoRS improves the performance \nfor the cache-enabled Cassandra database and HDFS by 19 20 times for through\u00adput and 3 times for latency. \nIn summary, CoLoRS enables type-safe, object sharing across OO languages in a transparent and ef.cient \nway. As part of future work, we are extending CoLoRS support to other OO languages (C++ and Ruby) and \nare investigating its use within multi-language distributed cloud systems such as AppScale [11] for which \ncomponents migrate dynamically (co-location is intermittent).  References [1] Apache Cassandra Project. \nhttp://cassandra.apache. org. [2] Y. Aridor, M. Factor, and A. Teperman. cJVM: A single system image \nof a JVM on a cluster. In ICPP, 1999. [3] J. Armstrong. Erlang a survey of the language and its industrial \napplications. In 9th ESIAP, 1996. [4] J. Armstrong, R. Virding, C. Wikstrom, and M. Williams. Concurrent \nProgramming in Erlang. Prentice-Hall, 1996. [5] G. Back, P. Tullmann, L. Stoller, W. C. Hsieh, and J. \nLepreau. Java operating systems: Design and implementation. Techni\u00adcal report, Univ. of Utah, 1998. [6] \nG. Back, W. C. Hsieh, and J. Lepreau. Processes in KaffeOS: Isolation, resource management, and sharing \nin Java. In OSDI, 2000. [7] B. N. Bershad, T. E. Anderson, E. D. Lazowska, and H. M. Levy. Lightweight \nremote procedure call. ACM Trans. Com\u00adput. Syst., 8(1), 1990. [8] B. N. Bershad, S. Savage, P. Pardyak, \nE. G. Sirer, M. E. Fi\u00aduczynski, D. Becker, C. Chambers, and S. J. Eggers. Extensi\u00adbility, safety and \nperformance in the SPIN operating system. In SOSP, 1995. [9] H.-J. Boehm and S. V. Adve. Foundations \nof the C++ concur\u00adrency memory model. In PLDI, 2008. [10] X. Chen and V. H. Allan. MultiJav: A distributed \nshared memory system based on multiple Java virtual machines. In PDPTA, 1998. [11] N. Chohan, C. Bunch, \nS. Pang, C. Krintz, N. Mostafa, S. Soman, and R. Wolski. AppScale: Scalable and Open AppEngine Application \nDevelopment and Deployment. In ICCC, 2009. [12] Computer Language Benchmarks Game. Language Perfor\u00admance \nComparisons. http://shootout.alioth.debian. org/. [13] CORBA Speci.cation. http://www.omg.org. [14] D. \nE. Culler, A. C. Arpaci-Dusseau, S. C. Goldstein, A. Kr\u00adishnamurthy, S. Lumetta, T. von Eicken, and K. \nA. Yelick. Parallel programming in Split-C. In SC, 1993. [15] G. Czajkowski and L. Daynes. Multitasking \nwithout compro\u00admise: A virtual machine evolution. In OOPSLA, 2001. [16] D. Doligez and G. Gonthier. Portable, \nunobtrusive garbage collection for multiprocessor systems. In POPL, 1994. [17] D. Doligez and X. Leroy. \nA concurrent, generational garbage collector for a multithreaded implementation of ml. In POPL, 1993. \n[18] T. Domani, E. K. Kolodner, and E. Petrank. A generational on-the-.y garbage collector for Java. \nSIGPLAN Not., 35(5), 2000. [19] T. Domani, E. K. Kolodner, E. Lewis, E. E. Salant, K. Barabash, I. Lahan, \nY. Levanoni, E. Petrank, and I. Yanorer. Implementing an on-the-.y garbage collector for Java. SIG-PLAN \nNot., 36(1), 2001. [20] S.Dorward, R.Pike,D.L.Presotto,D.Ritchie,H.Trickey, and P. Winterbottom. Inferno. \nIn COMPCON, 1997. [21] T. El-Ghazawi, W. Carlson, and J. Draper. UPC Language Speci.cations V, 2001. \nhttp://upc.gwu.edu. [22] M. Fahndrich, M. Aiken, C. Hawblitzel, O. Hodson, G. C. Hunt, J. R. Larus, and \nS. Levi. Language support for fast and reliable message-based communication in Singularity OS. In EuroSys, \n2006.  [23] R. T. Fielding. Architectural styles and the design of network\u00adbased software architectures. \nTechnical report, Univ. of Cali\u00adfornia, Irvine, 2000. [24] GCJ. The GNU Compiler for the Java Programming \nLan\u00adguage. http://gcc.gnu.org/java. [25] M. Golm, M. Felser, C. Wawersich, and J. Kleinoder. The JX operating \nsystem. In USENIX Annual Technical Conference, 2002. [26] Hadoop File System (HDFS). http://hadoop.apache. \norg. [27] G. C. Hunt and J. R. Larus. Singularity: Rethinking the soft\u00adware stack. Operating Systems \nReview, 41(2):37 49, 2007. [28] JavaOS : A Standalone Java Environment. Sun Microsystems, 1996. [29] \nJNode. http://www.jnode.org. [30] M. J. M. Ma, C.-L. Wang, and F. C. M. Lau. JESSICA: Java-enabled single-system-image \ncomputing architecture. J. Parallel Distrib. Comput., 60(10), 2000. [31] Occam Programming Manual. 1984. \nInmos Corporation. [32] Protocol Buffers. Google s Data Interchange Format. http: //code.google.com/p/protobuf. \n[33] K. Russell and D. Detlefs. Eliminating synchronization\u00adrelated atomic operations with biased locking \nand bulk rebi\u00adasing. SIGPLAN Not., 41(10), 2006. [34] F. B. Schneider, G. Morrisett, and R. Harper. A \nlanguage\u00adbased approach to security. Lecture Notes in CS, 2001. [35] M. Slee, A. Agarwal, and M. Kwiatkowski. \nThrift: Scalable Cross-Language Services Implementation. 2007. [36] T. von Eicken, C.-C. Chang, G. Czajkowski, \nC. Hawblitzel, D. Hu, and D. Spoonhower. J-Kernel: A capability-based operating system for Java. In Secure \nInternet Programming, 1999. [37] M. Wegiel and C. Krintz. XMem: Type-Safe, Transparent, Shared Memory \nfor Cross-Runtime Communication and Co\u00adordination. In PLDI, 2008. [38] N. Wirth and J. Gutknecht. Project \nOberon: the design of an operating system and compiler. ACM Press/Addison-Wesley, 1992.  \n\t\t\t", "proc_id": "1869459", "abstract": "<p>As software becomes increasingly complex and difficult to analyze, it is more and more common for developers to use high-level, type-safe, object-oriented (OO) programming languages and to architect systems that comprise multiple components. Different components are often implemented in different programming languages. In state-of-the-art multicomponent, multi-language systems, cross-component communication relies on remote procedure calls (RPC) and message passing. As components are increasingly co-located on the same physical machine to ensure high utilization of multi-core systems, there is a growing potential for using shared memory for cross-language cross-runtime communication.</p> <p>We present the design and implementation of Co-Located Runtime Sharing (CoLoRS), a system that enables cross-language, cross-runtime type-safe, transparent shared memory. CoLoRS provides object sharing for co-located OO runtimes for both static and dynamic languages. CoLoRS defines a language-neutral object/classmodel,which is a static-dynamic hybrid and enables class evolution while maintaining the space/time efficiency of a static model. CoLoRS uses type mapping and class versioning to transparently map shared types to private types. CoLoRS also contributes a synchronization mechanism and a parallel, concurrent, on-the-fly GC algorithm, both designed to facilitate cross-language cross-runtime object sharing.</p> <p>We implement CoLoRS in open-source, production-quality runtimes for Python and Java. Our empirical evaluation shows that CoLoRS extensions impose low overhead. We also investigate RPC over CoLoRS and find that using shared memory to implement co-located RPC significantly improves both communication throughput and latency by avoiding data structure serialization.</p>", "authors": [{"name": "Michal Wegiel", "author_profile_id": "81351596018", "affiliation": "University of California, Santa Barbara, CA, USA", "person_id": "P2354049", "email_address": "", "orcid_id": ""}, {"name": "Chandra Krintz", "author_profile_id": "81100025597", "affiliation": "University of California, Santa Barbara, CA, USA", "person_id": "P2354050", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1869459.1869479", "year": "2010", "article_id": "1869479", "conference": "OOPSLA", "title": "Cross-language, type-safe, and transparent object sharing for co-located managed runtimes", "url": "http://dl.acm.org/citation.cfm?id=1869479"}