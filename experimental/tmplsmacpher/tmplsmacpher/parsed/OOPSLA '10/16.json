{"article_publication_date": "10-17-2010", "fulltext": "\n A Study of Java s Non-Java Memory Kazunori Ogata Dai Mikurube Kiyokuni Kawachiya Scott Trent Tamiya \nOnodera IBM Research Tokyo 1623-14, Shimo-tsuruma, Yamato, Kanagawa 242-8502, Japan ogatak@jp.ibm.com \n Abstract A Java application sometimes raises an out-of-memory exception. This is usually because it \nhas exhausted the Java heap. However, a Java application can raise an out-of\u00admemory exception when it \nexhausts the memory used by Java that is not in the Java heap. We call this area non-Java memory. For \nexample, an out-of-memory exception in the non-Java memory can happen when the JVM attempts to load too \nmany classes. Although it is relatively rare to ex\u00adhaust the non-Java memory compared to exhausting the \nJava heap, a Java application can consume a considerable amount of non-Java memory. This paper presents \na quantitative analysis of non-Java memory. To the best of our knowledge, this is the first in\u00addepth \nanalysis of the non-Java memory. To do this we cre\u00adated a tool called Memory Analyzer for Redundant, \nUn\u00adused, and String Areas (MARUSA), which gathers memory statistics from both the OS and the Java virtual \nmachine, breaking down and visualizing the non-Java memory usage. We studied the use of non-Java memory \nfor a wide range of Java applications, including the DaCapo bench\u00admarks and Apache DayTrader. Our study \nis based on the IBM J9 Java Virtual Machine for Linux. Although some of our results may be specific to \nthis combination, we believe that most of our observations are applicable to other plat\u00adforms as well. \nCategories and Subject Descriptors C.4 [Programming .......... Dai Mikurube is currently affiliated with \nGoogle Inc. Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for profit or commercial \nadvantage and that copies bear this notice and the full citation on the first page. To copy otherwise, \nor republish, to post on servers or to redistribute to lists, requires prior specific permission and/or \na fee. OOPSLA/SPLASH 10 October 17 21, 2010, Reno/Tahoe, Nevada, USA. Copyright &#38;#169; 2010 ACM \n978-1-4503-0203-6/10/10 $10.00. Languages]: Measurement techniques, D.2.5 [Software Engineering]: Testing \nand Debugging debugging aids. General Terms Measurement, Experimentation. Keywords Java, memory footprint \nanalysis, non-Java memory, Java native memory.  1. Introduction A Java application sometimes raises \nan out-of-memory exception. This is usually because it has exhausted the Java heap. A large application \nmay use gigabytes of Java heap due to memory leaks or bloat [22]. With varying degrees of sophistication, \nmany tools are available for analyzing the Java heap and for debugging the out-of-memory exceptions [21, \n30]. However, a Java application can sometimes raise an out\u00adof-memory exception because it has exhausted \nnon-Java memory, the memory region outside the Java heap. For example, this can happen when it attempts \nto load too many classes into the virtual machine. Although running out of non-Java memory is rare compared \nto running out of the Java heap, a typical Java application actually consumes a considerable amount of \nnon-Java memory. As we will show later, the non-Java memory usage is as large as the Java heap for more \nthan half of the DaCapo benchmarks [6] when the heap sizes are twice the minimum size re\u00adquired for each \nbenchmark. A Java Virtual Machine (JVM) uses non-Java memory for various purposes. It holds shared libraries, \nthe class metadata for the loaded Java classes, the just-in-time (JIT) compiled code for Java methods, \nand the dynamic memory used to interact with the underlying operating system. In\u00adterestingly, modern \nvirtual machines tend to use more and more non-Java memory. For instance, beginning with Ver\u00adsion 1.4.0, \nSun's HotSpot Virtual Machine [29] optimizes reflective invocations [27] by dynamically generating classes, \nwhich consumes non-Java memory. The same ver\u00adsion also introduced direct byte buffers to improve I/O \nper\u00adformance [28]. These buffers typically reside in non-Java memory. Out-of-memory exceptions can result \nfrom such implicit use of non-Java memory, even though Java pro\u00adgrammers are often unaware of the specifics \nof such over\u00adhead. For testing, we used a micro-benchmark that repeatedly allocates and deallocates direct \nbyte buffers with multiple threads in three implementations of the Sun Hot-Spot Java VM, the IBM J9 Java \nVirtual Machine [3, 10], and the Jikes RVM [17]. We confirmed that this micro\u00adbenchmark caused out-of-memory \nerrors (or segmentation fault crashes) in tens of seconds, even though we allocate sufficiently large \nJava heaps. (For the Sun HotSpot VM, we also allocated a large amount of memory reserved for direct byte \nbuffers.)  This paper presents a quantitative analysis of non-Java memory. While there are numerous \nreports and publica\u00adtions that analyze Java heaps written by researchers and practitioners [21, 22, 30], \nto the best of our knowledge this is the first study that analyzes non-Java memory. To do this, we built \na tool called Memory Analyzer for Redundant, Unused, and String Areas (MARUSA), which gathers mem\u00adory \nstatistics from both the Java virtual machine and the operating system, using this data to visualize \nthe non-Java memory usage. We modified the IBM J9 Java VM for Linux to efficiently gather fine-grained, \nJVM-level statis\u00adtics. We studied the usage of non-Java memory for a wide range of Java applications, \nincluding the DaCapo bench\u00admarks [6] and WebSphere Application Server [15] running Apache DayTrader [2]. \nWe ran them with the modified IBM J9 Java VM under Linux. Note that the use of non-Java memory inevitably \ndepends on both the Java virtual machine and the operating system. Although some of our results may be \nspecific to our JVM and Linux, we believe that most of our observations are relevant to other plat\u00adforms. \nMore specifically, in this paper, we focus on the Java Standard and Enterprise Editions (Java SE and \nEE), rather than the Java Micro Edition (Java ME). Today the majority of Java virtual machines for Java \nSE and EE are written in C and C++, run on general-purpose operating systems, and include adaptive JIT \ncompilers with multiple optimization levels [3, 10, 22, 29]. We believe that our observations are also \nsubstantially relevant to these plat\u00adforms. Our contributions in this paper are: We quantitatively analyzed \nthe usage of non-Java mem\u00adory for a variety of Java programs, including the DaCapo benchmarks and WebSphere \nApplication Server running Apache DayTrader. We ran them on a modified version of IBM s production virtual \nmachine for Linux on x86 [16] and POWER [25] processors, and divided non-Java memory into eight components, \nsuch as class metadata, JIT-compiled code, and JIT work ar\u00adeas. We measured the amount of resident memory \nthese components consume over a period of time. We found that non-Java memory usage exceeds the Java \nheap for more than half of the DaCapo benchmarks when the heap size was set to be twice as large as the \nminimum heap size necessary to run each benchmark.  We found that, in all of the programs studied, the \nJIT work area fluctuates greatly, while memory usage for the remaining components stabilizes soon. This \nis be\u00adcause the JIT compiler from time to time demands sig\u00adnificantly more memory for its work area when \ncompiling methods at aggressive levels of optimization.  We observed that the behaviors of the libc \nmemory management system (MMS), the malloc and free rou\u00adtines, have a strong impact on the usage of non-Java \nmemory. Typically, a JVM-level MMS is built on top of the libc MMS, which in turn is built on top of \nthe OS\u00adlevel MMS. Even if the JVM-level MMS returns a chunk of memory to the libc MMS, this may not lead \nto reduced resident memory, since the libc MMS may fail to return it to the OS-level MMS.  We evaluated \na technique to effectively manage mem\u00adory by directly telling the OS-level MMS to remove memory pages \neven when libc MMS fails to remove it.  The rest of the paper is organized as follows. Section 2 presents \nan anatomy of non-Java memory. Section 3 de\u00adscribes our methodology, including our tool, MARUSA. Section \n4 shows the results of the micro-benchmarks, while Section 5 presents the results of the macro-benchmarks. \nSection 6 discusses a technique to improve memory man\u00adagement. Section 7 discusses related work. Finally, \nSection 8 offers conclusions.  2. An Anatomy of Non-Java Memory Figure 1 shows a breakdown for the non-Java \nmemory of an enterprise Java application, WebSphere Application Server (WAS) running Apache DayTrader \nfor 9 minutes. About 210 MB of non-Java memory was used, which is almost the same as the default WAS \nJava heap size, 256 MB (not shown in Figure 1). However, Java programmers are typically unaware of such \nsituations. For deeper quantitative analysis, we divided the non-Java memory into eight categories. Table \n1 summarizes these categories and their typical data types. This section describes each of these memory \nareas. In the example in Figure 1, five categories consume most of the non-Java memory.   JIT-compiled \nMalloc\u00adcode then-freed  ManagementClass overhead metadata JIT work 0 50 100 150 200 250 MB Figure \n1. Breakdown of non-Java memory when Apache DayTrader [2] is running on WebSphere Appli\u00adcation Server \n[15]. This is the annotated output from MARUSA, showing the resident set size but the Java heap. Category \nTypical data Code area Code loaded from the executable files Shared libraries Data areas for shared \nlibraries JVM work area Work area for the JVM Areas allocated by Java class li\u00adbraries Class metadata \n Java classes JIT compiled code Native code generated by the JIT Runtime data for the generated code \nJIT work area Work areas for the JIT compiler Malloc-then\u00adfreed area The areas that were once allocated \nby malloc(), then free()ed, and still residing in memory (typically held in a free list) Management overhead \n The unused portion of a page where only a part of a page isused, or the area used to manage an artifact, \nsuch as the malloc header Stack C stack Java stack Table 1. Categories of non-Java memory. 2.1 Code \narea Code area memory holds the native code from executable files and libraries, and the data loaded \nfrom shared libraries. This area does not include any of the code generated by the JIT compiler. The \nsize of code area increases when the code or data in an executable file or a library is loaded and actually \nused. 2.2 JVM work area JVM work area memory holds the data used by the JVM itself and the memory allocated \nby a Java class library or user-defined JNI methods. The memory used for direct byte buffers is an example \nof memory allocated by a Java class library. This area does not include class metadata or the other JIT-related \nareas. The size of this area increases when the JVM needs more working storage or when a Java application \nallocates more memory through a Java class library. 2.3 Class metadata Class metadata is a memory area \nfor the data loaded from Java class files, such as bytecode, UTF-8 literals, the con\u00adstant pool, and \nmethod tables. The JVM creates metadata upon loading a Java class. While there is no explicit alloca\u00adtion \nfor this in Java applications, using a class is not free, but does require some memory. This overhead \nmemory can become significant for large applications using thousands of classes. 2.4 JIT compiled code \nJIT compiled code memory area stores native code gener\u00adated by the JIT compiler and the data for the \ngenerated code. The size of this area increases as the JIT compiler compiles more methods. Some JIT compilers \ncan recom\u00adpile methods to optimize them more aggressively and gen\u00aderate new versions of the compiled \ncode, which usually consume even more memory. If a JIT compiler supports unloading of the generated code, \nthe size of this area can decrease. 2.5 JIT work area JIT work area memory contains data used by the \nJIT com\u00adpiler, such as the intermediate representations of a method being compiled. The size of this \narea increases when the intermediate representation is large (perhaps as methods are inlined) or when \nthe JIT does aggressive optimizations. The size of this area decreases when the compilation of a method \nis completed, though some of the data may remain in memory for inter-procedural analysis or profiling. \nNote that the JIT compiler can use aggressive optimizations de\u00adpending on the amount of available work \narea memory, so the JIT compiler will function correctly even when it is unable to allocate the desired \namount of work area memory.  2.6 Malloc-then-freed areas Malloc-then-freed memory areas are allocated \nusing mal\u00adloc() by the JVM or JIT, and then deallocated using free(). The malloc library typically manages \nsuch areas by holding them in a free list or returning them to the OS. If held in the free list, then \nthe deallocated memory resides in the non-Java memory in this malloc-then-freed area. If returned to \nthe OS, then the deallocated memory can be removed from the process memory. Therefore, the size of this \nnon-Java memory depends on how the standard C library (libc) and OS handle deallocated memory. We include \nmalloc-then-freed areas as part of the non-Java memory, since it remains in the resident memory of the \nprocess and consumes actual memory pages. This area sometimes becomes quite large, as shown in Figure \n1. Note that this large malloc-then-freed area is not a unique prob\u00adlem for JVMs, but can also affect \ntraditional C programs.  2.7 Management overhead Management overhead memory is implicitly used by OS \nor system libraries to manage process memory. A malloc header is an example of this kind of data. The \nunused parts of allocated pages are also included in this category. 2.8 Stack Stack memory area is used \nfor the Java stack and the C stack. We combined these stacks into the same category because both can \nbe used to store the stack frames of Java methods corresponding to the implementation of the JVM. The \nsize of this area increases when many stack frames are allocated in nested calls, when a stack frame \ncontains many local variables, or when threads are created.  3. Methodology to Measure Non-Java Memory \nThis section describes the analysis methodology used to divide the non-Java memory into these eight categories. \n 3.1 Our approach The philosophical key to our memory analysis is to fully identify the usage of the \nresident memory of a JVM process based on these eight categories (plus the Java heap). The underlying \nOS manages the address ranges of a process s resident memory, while the JVM controls the actual mem\u00adory \nusage. Thus, we need to gather memory management information at both the OS and JVM levels. We use three \nsteps to categorize non-Java memory: 1. Gather OS-level information to enumerate all of the memory ranges \nowned by a JVM process and identify the attributes of each range. 2. Gather JVM-level information to \nidentify the use of each area based on the component that allocated it. 3. Combine these two levels \nof information and summa\u00adrize the data using the eight categories.  Large modern programs, including \nJVMs, may have their own internal memory managers, which allocate chunks of memory from a pool, dividing \nthem into smaller pieces to handle memory allocation requests from other components. Therefore, we also \nneed to identify each com\u00adponent that requested memory from the internal memory manager. Tracing only \nthe memory allocation API calls, such as malloc() and free(), is insufficient to identify the memory \nusage in such a program because it only captures Software layers Application JVM Systemlibrary OS Categoriesof \nnon-Java memory Management overhead Malloc-then-freed when these areas are freed and held in the free \nlist Figure 2. Correspondence between memory allocation paths and the eight categories of non-Java memory. \nthe operations of the internal memory manager, without identifying how the pool is used by those components. \nFigure 2 shows examples of the correspondence be\u00adtween the memory allocation paths and the eight categories \nof non-Java memory. Since a memory requestor at a higher layer has more detailed knowledge about how \nthe memory is used, we need to gather information from all layers and combine it carefully, avoiding \nduplication. For that purpose, we built a tool called MARUSA, which gathers two levels of memory information, \ninterprets it, and then visualizes the breakdown of non-Java memory usage. Our tool can also analyze \nthe Java heap area [19], though we focus on non-Java memory in this paper. 3.2 Gathering OS-level memory \nmanagement information We first need to know the sizes and attributes of the mem\u00adory blocks assigned \nto the JVM process. These attributes typically include access permission, mapped file flag, and the file \npath if the memory is mapped to a file, though the specific attributes available depend on the OS. In \nthis study, we focus on the resident set size of process memory, where physical memory is assigned. Therefore, \nwe also need to gather information on which of the pages in the memory blocks of the process have physical \npages. Under Linux, MARUSA uses maps in the /proc file sys\u00adtem to gather address ranges and their attributes. \nFor Linux kernels version 2.6.25 or later, we can collect the physical page states using pageinfo in \nthe /proc file system. For older kernels, we can use a kernel module included in the open source software \nexmap [5].  3.3 Gathering memory usage in JVM If the JVM provides detailed information about its memory \nusage for debugging the JVM, we can use it to categorize non-Java memory. If the information is insufficient, \nwe need to add probes to the JVM by using plug-ins or by modifying JVM source code. MARUSA uses a mix \nof these approaches. We use de\u00adbugging information from the IBM J9 Java VM to get the sizes of the class \nmetadata and the JIT compiled code, and we modified IBM JVM to gather detailed information about memory \nallocations and deallocations, including re\u00adquests to the internal memory manager. This fine-grained \ndata allows us to capture full information regarding mem\u00adory usage of the JVM work area. 3.4 Computing \nnon-Java memory usage To combine both OS-level and JVM-level information, the MARUSA analyzer uses a \nmap structure that holds all of the gathered information for each memory byte in the JVM process. This \nmap uses the virtual address of each byte as a key to combine the information gathered from different \nsources. We call this map the memory attribute map. For example, it can identify that a memory byte was \nallocated using malloc() by the internal memory manager for loading class metadata, and that it is in \na page that is allocated in physical memory. To compute the breakdown of the non-Java memory us\u00adage, \nMARUSA counts the bytes with the same memory attributes. MARUSA uses a prioritized list of attributes \nto avoid counting any bytes twice. It first sums the bytes with the highest priority, and then sums the \nbytes with the sec\u00adond highest priority among the bytes still uncounted, and so on. We can create other \nviews of the memory break\u00addown by changing the ordering of the list.  4. Micro-Benchmarks This section \ndescribes the relation between the size of the non-Java memory and the operations in Java programs. Although \nthis correspondence depends on the implementa\u00adtion of the Java VM, many other implementations of the \nJava VM should show similar trends. Actually, we meas\u00adured the total resident set size of the Sun HotSpot \nJVM process running the same micro-benchmarks using the ps command, and confirmed that the resident set \nsize followed the same trend as that of the IBM J9 Java VM. We developed several micro-benchmarks to \nanalyze non-Java memory, and evaluated them using the IBM J9 Java VM for Java 6 in Linux on x86 and POWER \nma\u00adchines. Tables 2 and 3 describe our measurement environ\u00adment. In these measurements, we show the size \nof the non-Java memory where physical memory is actually allocated. Since no memory was swapped out during \nthese measure- Hardware environment Machine IBM BladeCenter LS21 CPU Dual-core Opteron (2.4 GHz), 2 sockets \nRAM size 4 GB Software environment OS SUSE Linux Enterprise Server 10.0 Kernel version 2.6.16 JVM IBM \nJ9 Java VM for Java 6 (SR7), 32bit Table 2. Execution environment for x86. Hardware environment Machine \nIBM BladeCenter JS21 CPU Dual-core PowerPC 970MP (2.5 GHz), 2 sockets RAM size 8 GB CPU and memory allocated \nto the tested virtual machine CPU 2 CPUs Memory 2 GB Software environment OS RedHat Enterprise Linux \n5.4 Kernel version 2.6.18 JVM IBM J9 Java VM for Java 6 (SR7), 32bit Table 3. Execution environment \nfor POWER. ments, this is the same as the RSS (Resident Set Size) of each JVM process after subtracting \nthe size of its Java heap area. 4.1 Micro-benchmark for the class metadata The first micro-benchmark \nshows how the size of the class metadata changes when reflective method invocation is heavily used. We \ncreated a micro-benchmark that invokes a getter and a setter for each of 6,000 fields by using a java.lang.reflect.Method \nobject for each of them. We measured the memory usage when these 12,000 methods were invoked 10 times, \n100 times, and 2,000 times. Figure 3 shows the results for this micro-benchmark on x86. The class metadata \narea was 3.9 MB when each method was invoked 10 times and 21.8 MB when each method was invoked 100 or \n2,000 times. This is because the JVM dynamically generated a method for each Method object to optimize \nthe reflective invocations, and loaded those generated methods and their containing classes [27]. For \nour micro-benchmark, this generated 12,002 classes for the tests with 100 and 2,000 invocations, while \nonly two  MB Figure 3. Changes in non-Java memory due to repeated reflective invocation on x86. Class \n Management metadata Invoke 10 times overhead JIT work Invoke 100 times  Invoke 2,000 times   \n0 20 40 60 80 100 MB Figure 4. Changes in non-Java memory due to repeated reflective invocation on POWER. \nclasses were generated for the test with 10 invocations. These two classes were always generated by a \nJava class library. When each method was invoked 2,000 times, the size of JIT compiled code grew from \n0.8 MB to 12.3 MB. This is because the methods in the generated classes were JIT compiled after they \nwere invoked many times. The total memory increase in the class metadata and the JIT compiled code was \n29.2 MB. This extra memory con\u00adsumption caused by reflective invocation is 43% of the resident set size \nwhen reflective invocation was used 2,000 times, but many programmers do not worry about such a large \namount of overhead. In addition, since this memory consumption is a result of optimization by the JVM, \na Java program may suddenly raise an out-of-memory error even though it has been running without problem \nfor a while. As modern programs are becoming more dynamic and reflec\u00adtive method invocations are more \nheavily used for their flexibility, the likelihood of such errors is increasing and programmers need \nto monitor their use of non-Java mem\u00adory. Figure 4 shows the results for this micro-benchmark on POWER. \nThe growth trend of the resident set size was the same as for x86. However, the code areas and the JIT\u00adcompiled \ncode areas were notably larger. The reason for the larger code areas was the difference in the base page \nsize [32]. In RedHat Enterprise Linux 5 for POWER, the base page size is 64 KB [33]. This change can \nimprove performance by reducing the number of TLB MB Figure 5. Change in non-Java memory due to allocating \nand freeing direct byte buffers on x86. After Allocation After GC MB Figure 6. Change in non-Java memory \ndue to allocating and freeing direct byte buffers on POWER. misses [33], but may increase memory usage \nbecause of internal fragmentation. The larger JIT-compiled code area was due to a differ\u00adence in the \nimplementations. Since the size when allocating a new chunk of memory for JIT-compiled code is larger \nin the POWER implementation, the initial size of this area is larger than for x86, and it grows in larger \nsteps. 4.2 Micro-benchmark for the JVM work and malloc\u00adthen-freed areas Next we studied how the size \nof the JVM work area changes due to the allocations of direct byte buffers. We created a micro-benchmark \nthat allocates and deallocates a specified number of direct byte buffers. For these meas\u00adurements, the \nsize of each byte buffer was set to 32 KB, the Java heap size was set to 8 MB, and no allocation failure \nGC occurred during the test runs. Figure 5 shows the mem\u00adory usage on x86 when 10,000 direct byte buffers \nwere created and then garbage collected by invoking System.gc(). Although the Java heap was as small \nas 8 MB, the JVM work area was 354.3 MB, and 351.7 MB in that JVM work area was used as memory for the \nactual buffers of the direct byte buffers. This large memory consumption could cause an unexpected out-of-memory \nerror because it is invisible to Java programs and Java debugging tools. The memory consumption after \ngarbage collection was unchanged because the malloc-then-freed area increased by 350 MB, while the size \nof the JVM work area was reduced by 352 MB. This suggests that all of the memory for the direct byte \nbuffers was retained in the free list, even though the Java programs and the JVM were unaware of its \nexis\u00adtence in the process memory. This memory in the malloc\u00adthen-freed area is also invisible to Java \nprograms and other tools, and thus, it can cause problems for Java program\u00admers and system maintainers \nbecause of unexpectedly high memory consumption.  Figure 6 shows the results for the same scenario on \nPOWER. The size of the JVM work area was 355.2 MB. The memory used for the actual buffers of the direct \nbyte buffers was exactly the same as the memory used on x86, because the Java program specified the amount \nof memory. However, the JVM work area after garbage collection was 31.4 MB, while it was reduced to 2.7 \nMB on x86. This is because about 900 direct byte buffer objects still remain in Java heap even after \ngarbage collection, while the direct byte buffers were completely collected on x86.  5. Macro-Benchmarks \nThis section shows our experimental results using larger programs. We evaluated WebSphere Application \nServer (WAS) 7.0 [15] running Apache DayTrader [2] and the DaCapo benchmarks [6]. For DaCapo, we present \nand dis\u00adcuss only the results of the benchmark named bloat, be\u00adcause the other programs showed the similar \ntrends in non-Java memory use. The hardware environments for these measurements were the same as shown \nin Table 2 for the micro\u00adbenchmarks. 5.1 WAS 7.0 running Apache DayTrader Figure 7 shows how the non-Java \nmemory use changes during the execution of Apache DayTrader in WAS 7.0 on x86. This graph shows the non-Java \nmemory at fourteen points in a single invocation of the server: (1) just after starting the server, (2) \nafter the first access to the scenario page of the DayTrader application, and then (3-14) at 12 times \nup to 10 minutes while DayTrader is accessed by a load generator using 30 threads. Note that the measurement \nintervals are not equal. The maximum heap size was set to 256 MB, but the Java heap area is not shown \nin the graph. In this application, the class metadata was the largest memory area just after startup, \nand the JVM work area in\u00adcreased by 27.4 MB to 37.8 MB at 30 seconds. The cause of this increase was \nthe memory for the direct byte buffers. Then the malloc-then-freed area grew, and these three areas became \nthe major areas in the non-Java memory. The JIT work area occasionally became large, but it was small \nat many of the measurement points. We will discuss the JIT work and malloc-then-freed areas in Section \n5.2. Figure 8 shows the same scenario on POWER. For these measurements, we only used 20 threads on the \nload genera\u00adtor because this POWER machine was slower than the x86 MB Figure 7. Non-Java memory for \nWAS 7.0 running Apache DayTrader on x86. MB Figure 8. Non-Java memory for WAS 7.0 running Apache DayTrader \non POWER. machine, but the CPU utilization was still more than 90% during the measurements. The size \nof the JVM work area grew up to 30 seconds, but decreased at 1 minute and grew again at 3 minutes. In \nthis execution, these fluctuations are caused by the combi\u00adnation of two memory allocation activities. \nOne is the in\u00adcrease of the direct byte buffers, which were more numerous at 30 seconds and at 3 minutes, \nusing 13.2 MB and 5.1 MB, respectively. The other is the allocation of temporary data structures at 30 \nseconds and their dealloca\u00adtion at 1 minute, which resulted in shrinking the JVM work area. The stack \narea is also 4 MB larger than on x86. The rea\u00adson is the larger base page size, which increased the unused \nmemory in the pages allocated for the stack. In this meas\u00adurement, WAS ran 155 threads. A JVM typically \nallocates at least one separate page as the stack for each thread, so that it can use guard pages to \ndetect stack overflows. This means that threads whose largest stacks are small will not use memory efficiently. \n  5.2 DaCapo We analyzed the non-Java memory use of the DaCapo benchmarks. We will only discuss the \nresults for bloat since the other benchmarks showed similar trends in their non-Java memory use. Table \n4 describes the configurations of the DaCapo benchmark and the Java heap size. We measured the memory \nuse at 20 points in a single execution of the benchmark to see how the non-Java memory changed during \nthe execution of a single iteration of the benchmark. Figure 9 shows how the size of the non-Java memory \nchanges during the execution of bloat on x86. The vertical axis is the percentage of the total object \nallocations in the benchmark. For example, the first bar shows the memory usage when the JVM had allocated \nobjects whose cumula\u00adtive size was 5% of the total allocation in bloat, which was about 990 MB. We call \nthis point the 5% allocation point. As shown in Table 4, the maximum heap size was set to 13 MB for this \nbenchmark. The non-Java memory consump\u00adtion was much larger than the Java memory. The sizes of the JIT \nwork areas and the malloc-then\u00adfreed areas varied widely within a single execution. Note that our measurement \napproach captures snapshots of the memory as it changes continuously during the execution of the program. \nTherefore, the sizes shown in Figure 9 do not necessarily show the maximum sizes in each period. The \nJIT compiler uses a large work area when it com\u00adpiles a large method, which may be due to inlining many \nmethods or due to aggressive optimization. The largest JIT work areas in this measurement were about \n20 MB in most of the intervals after the 25% allocation point. This is the reason the malloc-then-freed \narea increased after the 30% allocation point. The size of the malloc-then-freed area occasionally in\u00adcreased, \nthough it was around 9 MB for most of the inter\u00advals after the 30% allocation point. This is still under \ninvestigation, but we believe most of the malloc-then-freed area was the same memory used for the JIT \nwork area. Since the JIT work area was large in some compilations, the size of the malloc-then-freed \narea increases after those compilations. However, as we noted in Section 4.2, not all of the freed memory \nis held in the malloc-then-freed area. The size of this area is the result of the interactions be\u00adtween \nthe memory allocation and deallocation in the JIT compiler, and the algorithm used to maintain the free \nlist in libc.   More aggressive JIT optimization on POWER resulted in the larger malloc-then-freed \narea.  6. Reducing the Resident Set Size of the Malloc-then-Freed Area In this section, we discuss \nthe problems with the malloc\u00adthen-freed area and evaluate a technique to reduce the resi\u00addent set size \nwith the macro-benchmark we used in Sec\u00adtions 5.1 and 5.2. 6.1 Run-to-run fluctuation of the resident \nset size of the malloc-then-freed area As shown in Sections 4 and 5, the malloc-then-freed area consumes \na large amount of resident memory. The problem with this freed area is not just the extra memory consump\u00adtion, \nbut the difficulty of evaluating the true memory con\u00adsumption of a program. We found the size of the \nfreed area fluctuates widely during a single execution, and also varies significantly between runs of \nthe same program. Even if the resident set size reported by the ps command changes after a program is \nmodified, that does not prove that the code modification affected the memory use. Figure 11 shows a breakdown \nof the memory usage in another execution of DaCapo bloat. The total resident set size was about 37 MB \nor more throughout the execution after the 10% allocation point, while it was around 28 MB at many points \nin the results of Figure 9. 6.2 Reducing the resident set size of the malloc-then\u00adfreed area As described \nin Section 2.6, the malloc-then-freed area is the memory held in the free list managed by libc, and this \namount of memory depends on the algorithm used in the library. Since there is no API to tell the library \nabout the intention of the memory usage in a program, applications have no control over whether a freed \nchunk should be kept in the free list for reuse in the near future, or whether it should be returned \nto OS. This lack of any API between libc and applications prevents effective memory manage\u00adment between \nthe application and libc. We can reduce the resident set size of the malloc-then\u00adfreed area by directly \ntelling the OS to remove physical memory pages from the process memory. In Linux, we can use the madvise() \nsystem call for this purpose. Although the memory areas in the free list will still occupy address space, \nthe size of resident memory can be reduced. This technique has been applied to general memory manage\u00admeny \nsystems [8, 9] and a Java heap [12]. We applied this technique to the JIT work area because it produced \nmost of the malloc-then-freed area. We reduced system call over\u00adhead by limiting the target memory area \nto the JIT work MB size of malloc-then-freed area is large. (Graph scale is the same as Figure 9.) area, \nexploiting the knowledge of the implementation of the Java VM. 6.2.1 The madvise system call in Linux \nThis section briefly describes the madvise() system call in Linux and many UNIX-like operating systems. \nIt advises the kernel how to handle paging input and output. An ap\u00adplication can tell the kernel how \nit expects to use specific mapped or shared memory areas. The kernel can then choose appropriate read-ahead \nor caching techniques, though the kernel is also free to ignore this advice from the application. The \navailable options and their behaviors dif\u00adfer among operating systems. For example, MADV_DONTNEED is \nan option for mad\u00advise() indicating that the specified pages will not be ac\u00adcessed in the near future. \nIn Linux, the kernel immediately releases the physical memory pages, but continues to re\u00adserve the virtual \naddresses. Subsequent accesses to the re\u00adleased pages will succeed, but the pages will be zeroed out. \nThis behavior is specific to Linux, while some other oper\u00adating systems (such as FreeBSD) require both \nthe MADV_FREE and MADV_DONTNEED options for this effect. Since no application code can access the content \nof the freed areas, we can safely call madvise(MADV_DONTNEED) to remove such memory pages from process \nmemory.  6.2.2 Calling the madvise system call from the JVM We modified the IBM JVM to call the madvise() \nsystem call whenever a chunk of memory is freed. We actually need to call madvise() before the chunk \nis freed because another thread might reuse that memory when free() re\u00adturns. Since our JVM has its own \ninternal memory manager, we modified the memory manager to call madvise() just  JVM work JIT work Code \nMalloc-then- Just after startup freed After 1st access 30sec Management 1min overhead 1min 30sec 2min \n Stack 3min 4min 5min 6min 7min 8min 9min 10min      0 50 100 150 200 250 MB Figure 13. Non-Java \nmemory breakdown for WAS 7.0 running Apache DayTrader on POWER when madvise() is called as the JIT work \nareas are freed. (Graph scale is the same as Figure 8.) before calling free() when it is requested to \nfree a memory chunk by other JVM components. Note that we can call madvise() only when the size of a \nfreed chunk includes an entire page within the address range of the freed chunk. We cannot call madvise() \nfor a page that is only partially included in the range, because removing a memory page with madvise() \nerases all of the data in that page. To avoid performance degradation, we should call mad\u00advise() only \nfor memory chunks that will not be reused in the near future, such as a JIT work area. Therefore we modified \nthe JVM to call madvise() only when a JIT work area is freed. 0 5 1015 2025 3035 40 45 50556065 MB Figure \n14. Non-Java memory breakdown for DaCapo bloat on x86 when madvise() is called as the JIT work areas \nare freed. (Graph scale is the same as Figure 9.) JVM work metadata Class JIT-compiled code Malloc\u00adthen-freed \n MB  6.3 Savings by calling the madvise system call Figures 12 and 13 show the resident set size of \nApache Day-Trader in WAS 7.0 on x86 and POWER, respectively. We modified our JVM to use the madvise() \nsystem call upon freeing the JIT work areas. We measured the same scenario as in Section 5.1. Using the \nmadvise() system call, we reduced the resident set size of the malloc-then-freed area to around 10-15% \non both x86 and POWER. Figures 14 and 15 show the resident set size of the DaCapo bloat benchmark on \nx86 and POWER, respec\u00adtively. Using the madvise() system call, we reduced the resident set size of the \nmalloc-then-freed area to 16% on x86. In comparison, the reduction on POWER was smaller than on x86, \nonly 41% of the size without madvise(). This variation is again explained by differences in the base \npage size. As described in Section 6.2.2, an entire page needs to be included in the address range of \nthe freed area when the page is released with madvise(). Thus, the physical pages can be released only \nwhen the JIT compiler frees a memory chunk larger than 64 KB on POWER, while it is possible for a chunk \nlarger than 4 KB on x86.  6.4 Performance impact of calling the madvise system call Figure 16 shows \nthe relative performance when the JVM calls madvise() when freeing the JIT work areas compared to the \nJVM without madvise(). We used DayTrader throughput and DaCapo benchmark execution times. Throughput \nwas measured with Apache JMeter [1], and the number of iterations for the DaCapo benchmark was set to \none. The differences of the performance between the JVM that calls madvise() when freeing the JIT work \narea and the JVM without madvise() were up to 1.0% and 1.6% on x86 and POWER, respectively. The geometric \nmeans of the differences on x86 and POWER were 0.1% and 0.2%, re\u00adspectively. This measurement shows our \napproach has very small impact on performance.  6.5 Discussion Since the malloc-then-freed area will \neventually be re\u00adclaimed by the OS, it seems that we do not have to worry about this area even if a large \namount of memory is allo\u00adcated. However, since the OS does not know whether or not the content of the \npage will be used, it must swap the unnecessary data out to disk, and then swap it in when the malloc \nMMS touches the swapped-out pages while han\u00addling an allocation request. This can cause unnecessary thrashing \nin high-memory-use situations. We measured the number of bytes swapped in and out for a three minute \nof execution of two WAS processes, both running DayTrader. Table 5 describes the settings of this test \nenvironment. We used the Xen hypervisor to pro\u00adduce a high-memory-use situation by allocating a small \namount of memory to the tested guest virtual machine. Figure 17 shows the swapping activity when madvise() \nwas not called. In this case, large amounts of swapping out occurred periodically during execution, and \nthe total amount of swap space increased by 118 MB during this period. Swapping in also occurred continuously \nduring this period. Figure 18 shows the results when madvise() was used. In this case, swapping was greatly \nreduced, and the increase in the total size of the swap space was only 14.5 MB. This indicates that deleting \nunused data from the proc\u00adess memory and releasing the corresponding physical pages Relative performance \n(taller is better) 105% 100% 95% DayTraderantlrbloatcharteclipsefophsqldbjythonluindexlusearchpmdxalan \nGeo. Mean Figure 16. Relative performance of a JVM that calls madvise() when freeing the JIT work areas \ncompared to the JVM without madvise(). Hardware environment Machine IBM BladeCenter LS21 CPU Dual-core \nOpteron (2.4 GHz), 2 sockets RAM size 8 GB Hypervisor Xen 3.1.0 CPU and memmachine ory allocated to the \ntested virtual CPU 1 CPU Memory 1 GB Software envir onment OS RedHat Enterprise Linux 5.3 Kernel version \n2.6.18 JVM IBM Java J9 VM for Java 6 (SR7), 32bit Java heap size 333 MB (1/3 of allocated memory) Table \n5. Execution environment for measuring disk I/O for swap-in and swap-out. prevents unnecessary swapping \nand to retain good per\u00adformance.  7. Related Work There have been numerous papers and reports analyzing \nthe Java heap, so we will only review a few of the most important ones. Sun's Java Development Kit Version \n1.2 introduced the Java Virtual Machine Profiler Interface (JVMPI), and included the HPROF agent that \ninteracts with the JVMPI to profile the use of the Java heap and the CPU [20]. For example, this agent \ncan generate a heap al\u00adlocation profile that shows the numbers and sizes in bytes of the allocated and \nlive objects for each allocation site. The agent relates the allocation sites to the source code by \n Swap in / out [MB/sec] Swapped data [MB] [sec] Swap in / out [MB/sec] Swapped data [MB] [sec] during \nexecution of two WAS processes running Apache DayTrader when madvise() was called for the JIT work area. \ntracking the dynamic stack traces that led to the allocations. The HPROF agent can also generate a complete \nheap dump to find unnecessary object retentions or memory leaks. In JDK 5.0, the JVMPI was replaced by \nthe Java Virtual Ma\u00adchine Tool Interface (JVMTI) [31], and the HPROF [26] agent was re-implemented using \nthe JVMTI. The IBM Dump Analyzer for Java [4] analyzes the dump produced by a JVM, helping developers \nidentify common problems such as memory shortages, deadlocks, and crashes. It provides basic support \nfor diagnosing mem\u00adory problems, such as showing statistics for the live objects in a Java heap and the \nclass metadata. The tool is available as a plug-in for the IBM Support Assistant (ISA) [14], a free software \nserviceability workbench. Even if complete heap dumps are available and there are available tools for \nviewing the dumps, diagnosing memory leaks is a significant challenge for developers. The Java Heap Analysis \nTool, jhat, supports an SQL-like query lan\u00adguage to query the heap dumps, and allows developers to browse \nheap dumps with Web browsers [30]. Beginning in JDK 6.0, jhat is included in the standard distribution. \nMitchell and Sevitsky [21] proposed an automated and lightweight tool, LeakBot, for diagnosing memory \nleaks. It ranks data structures by their likelihood of containing leaks, identifies suspicious regions, \ncharacterizes the expected evolution of memory use, and tracks the actual evolution at run time. LeakBot \nwas incorporated into another tool named Memory Dump Diagnostic for Java (MDD4J) [24], which is also \navailable as a plug-in for ISA. Jump and McKinley [18] proposed an accurate, scalable, online, and low-overhead \nleak detector called Cork. They introduced a new heap summarization technique based on types. They build \na type points-from graph to summarize, identify and report on the data structures with systematic heap \ngrowth. Mitchell and Sevitsky [22] did an analysis of Java heaps, focusing on the overhead of collections. \nThey introduced a health signature to distinguish the roles of the bytes based on the roles of the objects \nin the collections, and provide concise and application-neutral summaries of the heap us\u00adage. Kawachiya \net al. [19] did another analysis of Java heaps, focusing on Java strings. Analyzing Java heap snap\u00adshots, \nthey found that there are many identical strings, and proposed three different savings techniques, including \none to \"unify\" the duplicates at garbage collection time. Java's non-Java memory, also called Java's \nnative heap, is not well described or documented. Chawla [7] provides a brief overview of how IBM's 32-bit \nJava virtual machine uses the address space in AIX, though IBM s JVM for 1.4.2 can behave differently \nfrom IBM s Java 5 and Java 6 VMs. Hanik [11] describes the memory layout of a JVM process, and considers \nthe causes of and solutions for out of memory errors.  8. Conclusion We quantitatively analyzed the \nusage of non-Java memory for a wide range of Java applications. Using a modified version of a production \nJava virtual machine for Linux, we verified that a Java application consumes a considerable amount of \nnon-Java memory. We found that non-Java memory could become as large as the Java heap in many Java programs. \nA Java virtual machine uses non-Java memory for vari\u00adous purposes. The non-Java memory holds shared libraries, \nbuilds the class metadata, provides the work area for gen\u00aderating the JIT-compiled code, and has the \ndynamic memory used to interact with the operating system. Although a plethora of memory problems affect \nthe Java heap, similar problems can also appear in the non-Java memory. For example, an out-of-memory \nexception will be raised when the virtual machine loads or dynamically generates too many classes based \non the requests from an many classes based on the requests from an application. Modern Java virtual machines \ntend to use more non-Java memory. For example, they may dynamically generate classes to optimize reflective \ninvocations, while also allo\u00adcating direct byte buffers to improve I/O performance. In addition, a trend \nto build scripting language runtimes on top of JVMs also tends to use more non-Java memory by generating \nJava classes dynamically. Examples include JRuby, Jython, and Groovy.  Through time series analysis, \nwe observed that the JIT work area had significant fluctuations in the use of non-Java memory, because \nthe JIT compiler intermittently re\u00adquires large amounts of temporary memory for aggressive optimizations. \nWe also observed that the libc memory management system (MMS) has a profound impact on the resident memory \nof non-Java memory, because it may re\u00adtain the memory chunks freed by an upper-level MMS. This suggests \nthat the layers of MMSes should be more carefully integrated. For example, an upper-level MMS may need \nthe ability to force the libc MMS to return free memory to the OS-level MMS. In this paper, we evaluated \na technique to compensate for the lack of integration be\u00adtween libc MMS and upper-level MMS by directly \ntelling the OS-level MMS to remove memory pages. We also verified that this technique reduced swapping \nactivity during the execution of two WAS processes in a high-memory-use situation. Since virtualized \ncomputation environments on a hypervisor, such as the servers in a cloud data center, are becoming popular, \nsuch high\u00admemory-use situations will be more common. Our tech\u00adnique for in-depth analysis of non-Java \nmemory is also useful for improving effectiveness of the memory over commitment by identifying unnecessary \nmemory use.  Acknowledgments We would like to thank the members of the IBM J9 Java VM and the TR JIT \ncompiler development teams in IBM Canada, especially Andrew Low, Trent Gray-Donald, and Mark Stoodley, \nfor helpful discussions on the idea and im\u00adplementation of MARUSA. We also thank Shannon Jacobs of IBM \nJapan HRS for helpful comments on the descrip\u00adtion of the applicability of our study in an earlier version \nof this paper.  References [1] The Apache Software Foundation. Apache JMeter. http://jakarta.apache.org/jmeter/ \n[2] The Apache Software Foundation. Apache DayTrader Benchmark Sample. http://cwiki.apache.org/GMOxDOC20/daytrader.html. \n[3] Chris Bailey. Java technology, IBM style: Introduction to the IBM Developer Kit, 2006. http://www.ibm.com/ \ndeveloperworks/java/library/j-ibmjava1.html. [4] Helen Beeken, Daniel Julin, Julie Stalley and Martin \nTrotter. Java diagnostics, IBM style, Part 1: Introducing the IBM Di\u00adagnostic and Monitoring Tools for \nJava - Dump Analyzer. http://www.ibm.com/developerworks/java/library/ j-ibmtools1/index.html [5] John \nBerthels. Exmap memory analysis tool. http://www.berthels.co.uk/exmap/ [6] Stephen M. Blackburna, et \nal. The DaCapo Benchmarks: Java Benchmarking Development and Analysis. In Proceed\u00adings of the 21st ACM \nConference on Object-Oriented Pro\u00adgramming, Systems, Languages, and Applications (OOPSLA '06), pp. 169-190, \n2006. [7] Sumit Chawla. Getting more memory in AIX for your Java applications, 2003. http://www.ibm.com/developerworks/ \neserver/articles/aix4java1.html [8] Yi Feng and Emery D. Berger. A Locality-Improving Dy\u00adnamic Memory \nAllocator. In Proceedings of the 2005 work\u00adshop on Memory system performance (MSP 05), pp. 68-77, 2005. \n[9] Sanjay Ghemawat. TCMalloc : Thread-Caching Malloc. 2007. http://google-perftools.googlecode.com/svn/trunk/doc/ \ntcmalloc.html [10] Nikola Grcevski, Allan Kielstra, Kevin Stoodley, Mark Stoodley, and Vijay Sundaresan. \nJava Just-In-Time Compiler and Virtual Machine Improvements for Server and Middle\u00adware Applications. \nIn Proceedings of the 3rd USENIX Vir\u00adtual Machine Research and Technology Symposium (VM '04), pp. 151-162, \n2004. [11] Filip Hanik. Inside the Java Virtual Machine, 2007 http://www.springsource.com/files/Inside_the_JVM.pdf \n[12] Matthew Hertz, Yi Feng, and Emery D. Berger. Garbage Collection Without Paging. In Proceedings of \nthe 2005 ACM SIGPLAN conference on Programming language design and implementation (PLDI 05), pp. 143-153, \n2005. [13] IBM Corporation. AIX 6.1 information, Multiple page size support. http://publib.boulder.ibm.com/infocenter/aix/v6r1/index.jsp? \ntopic=/com.ibm.aix.prftungd/doc/prftungd/ multiple_page_size_support.htm [14] IBM Corporation. IBM Support \nAssistant. http://www.ibm.com/software/support/isa/ [15] IBM Corporation. WebSphere Application Server. \nhttp://www.ibm.com/software/webservers/appserv/was/. [16] Intel Corporation. Intel 64 and IA-32 Architectures \nSoftware Developer s Manual. Volume 1: Basic Architecture. Order Number: 253665-033US. 2009. [17] The \nJikes RVM Project. Jikes RVM. http://jikesrvm.org/ [18] Maria Jump and Kathryn S. McKinley. Cork: Dynamic \nMemory Leak Detection for Garbage-Collected Languages.  In Proceedings of the 34th ACM Symposium on \nPrinciples of Programming Languages (POPL '07), pp. 31-38, 2007. [19] Kiyokuni Kawachiya, Kazunori Ogata, \nand Tamiya Onodera. Analysis and Reduction of Memory Inefficiencies in Java Strings. In Proceedings of \nthe 23rd ACM Conference on Ob\u00adject-Oriented Programming, Systems, Languages, and Appli\u00adcations (OOPSLA \n'08), pp. 385-401, 2008. [20] Sheng Liang and Deepa Viswanathan. Comprehensive Profil\u00ading Support in \nthe Java Virtual Machine. In Proceedings of the 5th USENIX Conference on Object-Oriented Technolo\u00adgies \nand Systems (COOTS '99), pp. 229-242, 1999. [21] Nick Mitchell and Gary Sevitsky. LeakBot: An Automated \nand Lightweight Tool for Diagnosing Memory Leaks in Large Java Applications. In Proceedings of the 17th \nEuro\u00adpean Conference on Object-Oriented Programming (ECOOP '03), pp. 351-377, 2003. [22] Nick Mitchell \nand Gary Sevitsky. The Causes of Bloat, The Limits of Health. In Proceedings of the 22nd ACM Confer\u00adence \non Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA '07), pp. 245-260, 2007. \n[23] Oracle. JRockit. http://www.oracle.com/appserver/jrockit/index.html [24] Indrajit Poddar and Robbie \nJohn Minshall. Memory leak detection and analysis in WebSphere Application Server: Part 1: Overview of \nmemory leaks. http://www.ibm.com/developerworks/websphere/library/ techarticles/0606_poddar/0606_poddar.html \n[25] Power.org. http://www.power.org/ [26] Sun Microsystems. HPROF: A Heap/CPU Profiling Tool in J2SE \n5.0. http://java.sun.com/developer/technicalArticles/Programming /HPROF.html [27] Sun Microsystems. The \nJava HotSpot Virtual Machine, v1.4.1. http://java.sun.com/products/hotspot/docs/whitepaper/ Java_Hotspot_v1.4.1/Java_HSpot_WP_v1.4.1_1002_1.html \n[28] Sun Microsystems. Java API reference, java.nio.ByteBuffer. http://java.sun.com/j2se/1.4.2/docs/api/java/nio/ \nByteBuffer.html [29] Sun Microsystems. Java SE HotSpot at a Glance. http://java.sun.com/javase/technologies/hotspot/ \n[30] Sun Microsystems. jhat - Java Heap Analysis Tool. http://java.sun.com/javase/6/docs/technotes/tools/share/ \njhat.html [31] Sun Microsystems. JVM Tool Interface (JVM TI). http://java.sun.com/j2se/1.5.0/docs/guide/jvmti/ \n[32] Madhusudhan Talluri and Mark D. Hill. Surpassing the TLB Performance of Superpages with Less Operating \nSystem Support. In Proceedings of the sixth international conference on Architectural support for programming \nlanguages and operating systems (ASPLOS-VI), pp. 171-182, 1994. [33] Peter W. Wong and Bill Buros. A \nPerformance Evaluation of 64KB Pages on Linux for Power Systems. http://www.ibm.com/developerworks/wikis/display/ \nhpccentral/A+Performance+Evaluation+of+64KB+Pages+ on+Linux+for+Power+Systems  \n\t\t\t", "proc_id": "1869459", "abstract": "<p>A Java application sometimes raises an out-of-memory ex-ception. This is usually because it has exhausted the Java heap. However, a Java application can raise an out-of-memory exception when it exhausts the memory used by Java that is not in the Java heap. We call this area non-Java memory. For example, an out-of-memory exception in the non-Java memory can happen when the JVM attempts to load too many classes. Although it is relatively rare to ex-haust the non-Java memory compared to exhausting the Java heap, a Java application can consume a considerable amount of non-Java memory.</p> <p>This paper presents a quantitative analysis of non-Java memory. To the best of our knowledge, this is the first in-depth analysis of the non-Java memory. To do this we cre-ated a tool called Memory Analyzer for Redundant, Unused, and String Areas (MARUSA), which gathers memory statis-tics from both the OS and the Java virtual machine, break-ing down and visualizing the non-Java memory usage.</p> <p>We studied the use of non-Java memory for a wide range of Java applications, including the DaCapo benchmarks and Apache DayTrader. Our study is based on the IBM J9 Java Virtual Machine for Linux. Although some of our results may be specific to this combination, we believe that most of our observations are applicable to other platforms as well.</p>", "authors": [{"name": "Kazunori Ogata", "author_profile_id": "81100139883", "affiliation": "IBM Research - Tokyo, Kanagawa, Japan", "person_id": "P2354042", "email_address": "", "orcid_id": ""}, {"name": "Dai Mikurube", "author_profile_id": "81470653969", "affiliation": "Google, Inc., Tokyo, Japan", "person_id": "P2354043", "email_address": "", "orcid_id": ""}, {"name": "Kiyokuni Kawachiya", "author_profile_id": "81100038759", "affiliation": "IBM Research - Tokyo, Kanagawa, Japan", "person_id": "P2354044", "email_address": "", "orcid_id": ""}, {"name": "Scott Trent", "author_profile_id": "81100129457", "affiliation": "IBM Research - Tokyo, Kanagawa, Japan", "person_id": "P2354045", "email_address": "", "orcid_id": ""}, {"name": "Tamiya Onodera", "author_profile_id": "81100474003", "affiliation": "IBM Research - Tokyo, Kanagawa, Japan", "person_id": "P2354046", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1869459.1869477", "year": "2010", "article_id": "1869477", "conference": "OOPSLA", "title": "A study of Java's non-Java memory", "url": "http://dl.acm.org/citation.cfm?id=1869477"}