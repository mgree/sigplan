{"article_publication_date": "10-17-2010", "fulltext": "\n Performance Analysis of Idle Programs Erik Altman Matthew Arnold Stephen Fink Nick Mitchell IBMT.J.Watson \nResearch Center {ealtman,marnold,sjfink, nickm}@us.ibm.com Abstract This paper presents an approach \nfor performance analysis of mod\u00adern enterprise-class server applications. In our experience, perfor\u00admance \nbottlenecks in these applications differ qualitatively from bottlenecksinsmaller, stand-alone systems. \nSmall applicationsand benchmarks often suffer from CPU-intensive hot spots. In contrast, enterprise-class \nmulti-tier applications often suffer from problems that manifest not as hot spots, but as idle time indicating \na lack of forward motion. Manyfactors can contribute to undesirable idle time, including locking problems, \nexcessive system-level activities likegarbage collection,various resource constraints, and problems driving \nload. We present the design and methodology for WAIT, a tool to diagnosis the root cause of idle time \nin server applications. Given lightweight samples of Java activity on a single tier, the tool can often \npinpoint the primary bottleneck on a multi-tier system. The methodology centers on an informative abstraction \nof the states of idleness observedinarunning program.This abstractionallowsthe tool to distinguish, for \nexample, between hold-ups on a database machine, insuf.cient load, lock contention in application code, \nand a conventional bottleneck due to a hot method. To compute the abstraction, we present a simple expert \nsystem based on an extensible set of declarative rules. WAIT can be deployed on the .y, without modifying \nor even restarting the application. Many groups in IBM have applied the tool to diagnosis performance \nproblems in commercial systems, and we present a number of examples as case studies. Categories and Subject \nDescriptors Software [Software Engi\u00adneering]: Metrics, Performance Measures General Terms Performance \nKeywords bottlenecks, performance analysis, multi-tier server applications, idle time 1. Introduction \nShun idleness. It is a rust that attaches itself to the most brilliant metals. Voltaire. This paper addresses \nthe challenges of performance analysis for modern enterprise-class server applications. These systems \nrun across multiple physical tiers, and their software comprises many Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page.To copyotherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA/SPLASH 10, October 17 21, 2010, Reno/Tahoe, \nNevada, USA. Copyright c &#38;#169; 2010ACM 978-1-4503-0203-6/10/10... $10.00 components from different \nvendors and middleware stacks. Many of these applications support a high degree of concurrency, serving \nthousands or even millions of concurrent user requests. Theysup\u00adport rich and frequent interactions with \nother systems, with no in\u00adtervening human think time. Many server applications manipulate large data \nsets, requiring substantial network and disk infrastructure to support bandwidth requirements. With these \nrequirements and complexities, such applications face untold dif.culties when attempting to scale for \nheavy produc\u00adtion loads.In ourexperience with dozensof industrial applications, every individual deployment \nintroduces a unique set of challenges, due to issues speci.c to a particular con.guration. Anychange \nto keycon.guration parameters, such as machine topology, applica\u00adtion parameters, code versions, and \nload characteristics, can cause severe performance problems due to unanticipated interactions. Part of \nthe challenge arises from the sheer diversity of poten\u00adtial pitfalls. Even a single process can suffer \nfrom any number of bottlenecks, including concurrency issues from thread locking behavior,excessivegarbage \ncollection load due to temporary ob\u00adject churn[9,21,24], and saturating the machine s memory band\u00ad width[24]. \nAny of these problems may appear as a serialization bottleneck in that the applicationfails to use multiple \nthreads ef\u00adfectively; however, one must drill down further to .nd the root cause. Other problems can \narise from limited capacity of physi\u00adcal resources includingdiskI/Oandnetwork links.Aload balancer may \nnot effectively distribute load to application clones. When per\u00adformance testing, testers often encounter \nproblems generating load effectively. In such cases, the primary bottleneck may be processor or memory \nsaturation on a remote node, outside the system-under\u00adtest. Furthermore, many pro.ling and performance \nunderstanding tools are inappropriate for commercial server environments. Many tools[2,3,5,6,11,13,14,16,18,19]rely \non restarting or instru\u00ad menting an application, which is often forbidden in commercial deployment environments. \nSimilarly, many organizations will not deploy anyunapproved monitoring agents, nor tolerate anysignif\u00adicant \nperturbation of the running system. In practice, diagnosing performance problems under such constraints \nresembles detective work, where the analyst must piece together clues from incomplete information. Addressing \nperformance analysis under these constraints, we presentthedesignand methodologyofatoolcalled WAIT. WAIT \ns methodology centers on Idle Time Analysis: rather than analyze what an application is doing, the analysis \nfocuses on explaining idle time. Speci.cally, the tool tries to determine the root cause that leads to \nunder-utilized processors. Additionally, the tool must present informationinawaythatiseasily consumable,and \noperate under restrictions typical of production deployment scenarios. The main contributions of this \npaper include: ahierarchical abstractionofexecution state:Wepresentanovel abstraction of concrete execution \nstates, which provides a hier\u00adFigure 1. Multi-tier applications route data between clients and back-end \nstorage. The Java component often acts as a hub that can be sampled to give insight into larger-scale \nperformance problems.  archy of abstract states corresponding to different sources of idle time. In \ncontrast to traditional pro.ling tools, we present a high-level characterization of application behavior \ncomputed by an expert system. The expert system is codi.ed by a set of declarative rules, which a practitioner \ncan easily customize. a methodology to infer behavior based on ubiquitous sampling mechanisms.We show \nthat a tool can analyze performance ef\u00adfectively based on lightweight, non-intrusivesampling informa\u00adtion \navailable by default from standard JavaVirtual Machines and operating systems. Notably, the end user \nneed not restart, recompile, or otherwise modify the running application. In our experience, this feature \nis crucial for wide-scale adoption of a tool in this space, and rules out most previous approaches de\u00adscribed \nin the literature. Wealso presentanumberof case studies from real deployments illustrating how the methodology \napplies in practice for a number of different types of performance problems. The remainder of this paper \nproceeds as follows. Section 2 presentsa high-leveloverviewof the methodology. Section3 de\u00ad scribes details \nof how the system collects monitoring data. Sec\u00adtions4and5describe the abstractionof idle states and \nthe analysis that computes this abstraction. Section6 discusses the user inter\u00ad face and implementation. \nSection7reviews case studies illustrat\u00adingvariousexamples from the .eld. Section8reviews relatedwork, \nand Section9concludes. 2. Overview We consider, as a motivating example, performance diagnosis for aJava \nEnterprise Edition (JEE) application. Figure1illustrates the typical structure of a JEE application. \nA JEE application server, running Java code, sits in the middle of several communicating tiers of machines; \nthese tiers include clients, relational databases, and directory and caching services. To understand \nperformance of this application, WAIT uses a Hub Sampling approach.With the approach, we demonstrate \nhow the analysis of only the Java tier can provide insight into bottle\u00adnecks in the system as a whole. \nHub Sampling To identify primary bottlenecks in Java-hub ap\u00adplications, we collect samples of processor \nutilization and samples of the state of the Java threads. Production environments impose severe constraints \non the types of monitoring and tools deemed acceptable. For example, code instrumentation is often a \nnon-starter: many organizations sim\u00adply will not rebuild an application with instrumentation, deploy \na non-standard runtime system, or enable anynon-trivial monitoring agent. Many organizations will not \ntolerate any observable perfor\u00admance overhead, except perhaps under limited and carefully con-Figure \n2. Data, sampled from a running JVMs, that we use for inferring the existence and nature of performance \nbottlenecks. trolled guidance. Additionally,manyorganizations will not tolerate large trace .les, and \nwill not allow any interactive access to the monitored systems. To work within these constraints, we \nmust rely on ubiquitous monitoring technology, without requiring instrumentation or non\u00adtrivial agents.We \nmust also makedo witha relatively small corpus sample-based monitor data, collected during a small window \nand processed of.ine. Fortunately, most production JVMs provide built-in sampling mechanisms, whereby \nthe JVM will respond to signals and dump relatively small core ( javacore ) .les with data representing \nthe current JVM state. Figure2illustrates the data thatis readilyavail\u00ad able from most commercial JVMs, \nwithout requiring anychanges to an application s deployed con.guration: the monitor graph, which speci.es \nthe ownership and queuing relationships between threads and monitors; thread stack samples; the conventional \nrun state of each thread (i.e. Runnable, CondWait, Blocked,Parked); anda windowofgarbage collectionevents. \nWe have found that even infrequent samples, acquired once or twice per minute, can yield surprising insight \ninto the primary system bottlenecks. Idle Time Analysis Even with relatively infrequent samples,java\u00adcore \ndumps of JEE server applications can carry a tremendous vol\u00adume of information. Consider simple stack \nsamples: often stacks in a JEE application extend to several hundred stack frames spanning dozensoflogical \ncomponentsfromdifferentvendors,as illustrated in Figure 4. Understanding the relevant information from \neven a single such stack requires a lot of work. Now consider that an ap\u00adplication will typically have \nmany dozens of threads performing variousactivities,andthatto understandperformancechangesover time, \nwe must inspect samples from at least several points in time. In this common scenario,atool can easilyoverwhelmahuman \nwith too much information. The fundamental problem is that the pro.le data lacks abstrac\u00adtion: there \nare too many distinct concrete methods in play, and a user can not easily digest pro.le information spanning \nthousands of methods. While an expert with experience and intuition can prob\u00adably navigate the raw data \nand diagnosis a problem, this task is usually too dif.cult for mere mortals. To address this problem, \nWAIT analyzes the sample data and produces an abstract model of the application behavior, designed to \nilluminate bottlenecks. The analysis uses a set of expert rules to inferahierarchicalcategorizationofthe \nstateof threads acrosstime (c.f.[6,7,10,23]).The rules dependonthe participationofathread in the monitor \ngraph, and the names of methods on its call stack. The analysis machinery is exceedingly simple and runs \nquickly, relying primarily on simple pattern-matching and decision trees. However, the expert rules embody \nsophisticated understanding of various Java frameworks. At its coarsest level, the analysis assigns each \nthread an abstract state called a Wait State.Athread sWait State speci.es whether it is able to make \nforward progress, and if not, the nature of the hold up. Each state, such as Blocked , Disk , GC , and \nNetwork , represents a general class of delays, independent of application\u00adFigure 3. From information \nthat includes monitor states and stack sample context inferences, WAIT infers a Wait State for every \nsample.  level details.In thisway, theWait States serve the same purpose as the conventional run states,but \nprovidea richer semantics that helps identify bottlenecks. Figure3givesexample inferencesof Wait States.Forthe \nstackin Figure4,the analysiswould inferthe Wait State Network , as the stack matches the pattern shown \nin the middleexampleof Figure3. For each stack frame, we compute an abstraction called aCate\u00adgory that \nrepresents the code or activity being performed by that method invocation. For example, a method invocation \ncould in\u00addicate application-level activity such as Database Query, Client Communication, or JDBC Overhead.We \nfurther label each stack sample with a Primary Category which best characterizes the ac\u00adtivitybeing performedbyan \nentire stackatthe sampled momentin time. The abstract modelof activityformsa hierarchy:aWait State givesa \ncoarsebut meaningful abstractionofa classof behaviors. For more information, one can drill down through \nthe model to see .ner distinctions based on Primary Categories, stacks of Cate\u00adgories, and .nally concrete \nstack samples without abstraction. This hierarchyprovides a natural model for an intuitive user interface, \nwhich provides a high-level overview and the ability to drill down through layers of abstraction to pinpoint \nrelevant details. Tool Wehaveimplementedatoolbasedontheabove abstraction, which is deployed as a service \nwithin IBM. The tool computes the abstraction described above based on a simple set of rules, de.ned \ndeclarativelyby anexpert based on knowledgeof common methods in standard library and middleware stacks. \nWe present some statistics and case studies that indicate the methodology is practical, and successfully \nidenti.es diverse sources of idle time. 3. Hub Sampling WAIT relies on samples of processor activity \nand of the state of threads in a JVM. The system typically takes samples from the hub process(e.g.,application \nserver)ofamulti-tier application,but can also collect data from anystandard Java environment. Despite \ncollecting no data from the other tiers, information from a hub process frequently illuminates multi-tier \nbottlenecks. We next describe howWAIT collects information from a Java hub, and discuss challenges due \nto limitations in available data. 3.1 Sampling Mechanisms We have found a low barrier to entry to be \na .rst-order require\u00adment for tools in this space. Many, if not most, potential users will reject any \nchanges to deployment scripts, root permissions, kernel changes, speci.c software versions, or specialized \nmonitor\u00ading agents. Instead, WAIT collects samples of processor utiliza\u00adtion, process utilization, and \nsnapshotsofJava activity usingbuilt\u00adin mechanisms that are available on nearly every deployed Java system.Table1summarizes \nthe mechanismsby which the system collects data. Figure4. Partofa105-deepcallstack,fromtheDayTrader \nbench\u00admark (a con.guration of which is now part of the DaCapo Suite).  data UNIX Windows machine utilization \nvmstat typeperf process utilization ps tasklist Java state kill -3 sendsignal Table 1. Thebuilt-in mechanisms \nwe use to sample the Java hub. Note that kill -3 does not terminate the signaled process, and 3 is the \nnumeric code for SIGQUIT. sampling interval (seconds) throughput slowdown 1 59% 5 24% 10 13% 20 8% 30 \n2% 1000 unmeasurable Table 2. Throughput perturbationversus sampling interval, froma document processing \nsystem running IBM s 1.5 JVM. The system can also produce meaningful results with partial data. In practice, \ndata sometimes arrives corrupted or prematurely terminated,duetoa myriadof problems.Forexample,thetarget \nmachine may run out of disk space while writing out data, the target JVM may havebugs in its data collection, \nor there may be simple user errors. If anyof the sources of data described are incomplete, the system \nwill produce the best possible analysis based on the data available. Processor Utilization Most operating \nsystems support non-intrusive processor utilization sampling.We attemptto collect time seriesof processor \nutilization at three levels of granularity: (1) for the whole machine, (2) for the process being monitored, \nand (3) for the indi\u00advidual threads within the process.Forexample, on UNIX platforms we use vmstat and \nps to collect this data. Java Thread Activity To monitor the state of Java threads, we rely on the support \nbuilt into most commercial JVMs to dump javacore .les. Our implementation currently supports (parses) \nthe javacore format produced by IBM JVMs, and contains prelim\u00adinary support for the HotSpot JVM. This \ndata, originally intended to help diagnose processfailures and deadlock, canbe sampledby issuing a signal \nto a running JVM process.1 Upon receiving this signal, the JVM stops running threads, and then writes \nout the in\u00adformation speci.edinFigure2.TheJVM forces threadstoquiesce using the same safepoint mechanism \nthat is used by other stan\u00addard JVM mechanisms, such as thegarbage collector. IBM JVMs can producejavacore \nsamples withfairlylow per\u00adturbation.Fora large application with several hundred threads with deep call \nstacks, writing out a javacore .le can pause the applica\u00adtion for several hundred milliseconds. As long \nas samples occur infrequently, writing javacores has a small effect on throughput, but an unavoidable \nhit on latencyfor requests in .ight at the mo\u00admentof sample.Table2 shows that samples taken onceevery30 \nseconds resultina2% slowdown.For server applications, where operations repeat inde.nitely, perturbation \ncan be dialed down as low as needed, at least concerning throughput. When the hub of a multi-tier application \nspans multiple pro\u00adcesses, possibly running on multiple machines, we currently choose one at random. \nIn the future, we will tackle multi-process environ\u00ad 1There are some nominal differences between this \nformat and that provided by a Sun JVM. Also, Sun provides a .ner granularity for data acquisition, via \nthe jstack, jstat, and relatedfamilyof commands. ments more generally, including cloud environments. \nHowever, we note thatin manyenterpriseworkloads, the bottlenecksfacing all JVMs are similar, and thus \nour approach provides substantial ben\u00ade.t even now.  3.2 Dif.culties with Thread States The run states \nprovidedby the JVM and operating system are often inconsistent or imprecise, due to several complications. \nThe .rst problem is that manyJVM implementations quiesce threads at safe\u00adpoints before dumping the javacore. \nThreads that are already qui\u00adesced(e.g., waiting to acquire a monitor) will be reported correctly as \nhaving a conventional run state of Blocked. However, anythread that was Runnable before triggering the \ndump will be reported to haveafalse run stateof CondWait, since the threadwas stoppedby the JVM before \nwriting the javacore .le. The boundary between the JVM and the operating system intro\u00adduces further dif.culties \nwith thread run states. The JVM and OS each track the run state of a thread. The JVM may think a thread \nis Blocked, while the OS reports the same thread Runnable, in the midst of executing a spinlock. Spinning \nis sometimes a detail out\u00adside the JVM s jurisdiction, implemented in a native library called by the \nJVM. Similarly, the JVM may report a thread in a Cond-Waitstate, even though the thread is executing \nsystem code such as copying data out of networkbuffers or traversing directory entries in the .lesystem \nimplementation. Even if conventional run states were perfectly accurate, they often help little in diagnosing \nthe nature of a bottleneck. Consider the conventional CondWait run state. One such thread may be waiting \nat a join point, in a fork-join style of parallelism. Another thread, with the same CondWait run state, \nmay be waiting for data froma remote source, such asa database.Athird such thread may be a worker thread, \nidle only for want of work. For these reasons, we instead compute on a richer thread state abstraction \nthat distinguishes between these different types of states. 4. AHierarchical Abstraction of Execution \nState WAIT s analysis maps concrete program execution states into an abstract model, designed to illuminate \nroot causes of idle time. In this Section, we describe the conceptsin the abstraction. Section5 later \ndescribes how the analyzer computes the abstraction, and how details of the abstraction hierarchy arise \nfrom a declarative speci.cation. The analysis maps each sampled thread into an abstract state, which \nconsists of a pair of two elements called the Wait Stateand a stack of Categories.A Wait State encapsulates \nthe status of a thread regarding its potential to make forward progress, while the a Category represents \nthe code or activity being performed by a particular method invocation. We next describe the hierarchical \nabstraction in more detail. 4.1 TheWait State Abstraction The Wait Stateabstraction groups thread samples, \nassigning each sample a label representing common cases of forward progress (or thelack thereof).Figure5(a)showsthethe \nhierarchyofWait States which cover all possible concrete thread states. The analysis maps each concrete \nthread sample into exactly one node in the tree. The hierarchy has proven to be stable over time; we \nhave not needed to add additional states beyond those shown in the hierarchy of Figure 5(a), even as \nusage of the WAIT tool has expanded to included diverse workloads. At the coarsestlevel, theWait Stateofa \nsampled thread indi\u00adcates whether that thread is currently held up or making forward progress: Java threads \nmay be either Waitingor Runnable.Athird  (a) TheWait StateTree (b) TheCategoryTree Figure 5. Every \nconcrete state,a sampled thread, mapstoapairof abstract states:aWait Stateanda stackof Categories(oneper \nframe). Every frame in a call stack has a default name based on the invoked package; e.g. com.MyBank.login() \nwould be named MyBank Code. possibility coversathreadexecuting native(non-Java)code that the toolfailsto \ncharacterize,in which casethe threadis assignedWait State Native Unknown. For Java threads, the analysis \npartitionsWaitingand Runnable into .ner abstractions, which convey more information regarding sourcesof \nidle time.Forexample,a Waitingthread might be wait\u00ading for data from some source (Awaiting Data), blocked \non lock contention(Contention), or have putitself to sleep(Sleeping). As shown in the Figure, .ner distinctions \nare also possible. Consider a Sleeping thread: this could be part of a polling loop that contains a call \nto Thread.sleep (Poll); it could be the join in a fork-join style of parallelism(Join); or it could be \nthat the thread is a worker in a pool of threads, and is waiting for new work to arrive in the work queue(Awaiting \nNoti.cation). We claim that in many cases, distinctions in Wait States give a good .rst approximation \nof common sources of idle time in server applications. Furthermore, we claim that differencesinWait States \nindicate fundamentally different types of problems that lead to idle time. A server application suffering \nfrom low throughput due to insuf.cient load would have many threads in the Awaiting Noti.cation state. \nThe solution to this problem might, for exam\u00adple, be to tune the load balancer. A system that improperly \nuses Thread.sleep suffers from a problem of a completely different nature. Similarly, having a preponderance \nof threads waiting for data fromadatabase has radically different implications onthe sys\u00adtem health than \nmany threads, also idle, suffering from lock con\u00adtention. Thus, theWait Stategivesa high-level descriptionof \nthe root cause of idle time in an application. The second part of the abstrac-Figure 6. The call stack \nis mapped to a stack of Categories, from which a primary Category (Database Communication) is chosen. \n tion, the Category stack, gives a .ner abstraction for pinpointing root causes.  4.2 The Category Abstraction \nThe Category abstraction assigns each stack frame a label repre\u00adsenting the code or activity being performed \nby that method invo\u00adcation. Category names provide a convenient abstraction that sum\u00admarizes nested method \ninvocations that implement larger units of functionality[10]. Note that since each stack frame maps to \na Category, each stack will contain representatives from several Categories.Tounderstand behavior of \nmany stacks at a glance, it is useful to assign each stack a primary Category, which represents the Category \nwhich provides the best high-level characterization of the activity of the entire stack.Forexample,in \nFigure4, the JDBC Category would be chosen as the primaryCategory, based on priority logic that de\u00adtermines \nthat the JDBC Category label conveys more information than other Categories in the stack, such as Networking \nor WebCon\u00adtainer.  Figure 5(b) shows a subset of the Categories that we currently model.AswithWait States,theCategory \nabstract statesformatree. The primary distinction is drawn between activities, which name what a method \nis doing, and nicknames for common libraries and frameworks. Common activities include sorting and marshalling \ndata, such as occurs in the handling of the XML data of SOAP requests. It is common for server applications \nto have dozens of administrative(Admin, in the .gure) activities. These activities include background \nlogging threads, cache eviction threads, and alarm threads that periodically probe for changes of .les \nstored on disk. Since the Category abstraction in Figure 5(b) re.ects activities in well-known software \ncomponents, the Category abstraction is bound to be incomplete with respect to non-framework application \ncode. This incompleteness stands in contrast to the theWait State abstraction in Figure 5(a) which describes \nmore general conditions of all systems. Since the Category abstraction is incomplete, it is crucial to \nhave a fallback Category assignment, in the case of insuf.cient coverage of Category names. We have found \nthat the method s package .lls this coverage hole effectively, and as such, we assign the package as \nthe Code Nickname , shown at the upper right of Figure 5(b). Section5illustrateshowa practitionercan \nde.nenewCategory abstractions declaratively,inthe speci.cationofthe analysisexpert rules system. Figure \n6 gives an example call stack, and the correspond\u00ading Category stack. For example, a call to the socketRead0 \nmethod belongs to the code that has been nicknamed Network. The com.mybank code has no nickname, nor \nknown activity, and so is assigned its default name MyBank Code. The primary Cate\u00adgory of this call stack \nis the highlighted Database Communication activity. Declarative rules (described shortly) indicate priorities \nused to choose the primary label for a stack. Sometimes the appropriate choice of priorities varies depends \non who views the report. For examplein Figure6,ifa network analyst were the primary viewer of the output, \nNetwork might be a better choice as the primary Category.  4.3 Wait State Analysis De.nition Having \ninformally introduced the abstraction, we can now state more precisely the analysis performed by WAIT: \nDEFINITION 1 (Wait State Analysis). Let k be the maximum stack depth of sampled threads, W be the tree \nofWait States and C be thetreeof Categories.We de.nea wait state analysis as a function that maps eachsampled \nconcrete thread state to an abstract state (w, c) . W\u00d7Ck.We say that w is the Wait State of a sampled \nthread, and c, its Category Stack, is a tuple whose components correspond to frames in the sampled call \nstack. DEFINITION 2 (Category Priorities and Primary Category). Let the call stackof a sampled thread \ncontain methods m1,...,mk, and the output of a wait state analysis be (w, c), where the Categories are \nc =[c1,...,ck]. Each element of the Category mapping, mi . ci, has a priority pi. The primary Category \nused by the sampled thread is that ci with maximum priority, and, in the case of ties, the one closest \nto the leaf invocation m1. The abstract model provides a backbone for progressive disclo\u00adsure of details \nregarding thread activity, at a sampled moment in time. In practice, we have found that it is most useful \nto start by clustering stack samples according toWait State.A user request to focuson particularWait \nStates resultsinthenavigationtoaview Figure7. The rawinput from stack sampling provides this data for \neach sampled thread. Each monitor and thread has a hexadecimal address and a name (not shown).  Figure \n8. The input concrete data model, after clustering and opti\u00admization. Thisexample shows that cluster \nc1,which occurred 3592 times across all application samples, was invoked in thread t3 in the second application \nsample (sample index 2). that clustersthe stack samplesinthatWait State accordingto their primary Category. \nIn this way, navigation of a user interface corre\u00adsponds directly to traversals of the tree-structuredabstract \nmodel. Section 6.2 describes the user interface in more detail. 5. Analyzer We now describe a relatively \nsimple engine that computes the analysis just de.ned. The analysis engine has three steps: 1) parse and \ntransform the raw data, 2) infer Categories, and then 3) infer Wait States. 5.1 ETL Step As described \nearlier, WAIT takes input that includes raw javacore samples and raw outputof machine utilization utilities.Apre-pass \nto the analysis performs an Extract-Transform-Load (ETL) step that parses the raw data and transforms \nit to a canonical form, which abstracts away irrelevant details that vary from platform to platform. \nFigure 7 provides a more detailed summary of the raw data available in a javacore thread sample, including \nthe name of thread sampled, its native thread identi.er (e.g. on UNIX, this turns out to be the address \nof the corresponding pthread data structure),the callstackasalistof method names,and informationabouthowthis \nthread interacts with the monitors. The ETL step consumes this data and produces the data model shownin \nFigure8. First,we computeequivalence classesof thread samples, where two thread samples are considered \nequivalent if theyrepresent the same stack of methods and locking status, The Figure labels these equivalence \nclasses as call stackclusters. The output model represents the dataina tabularfashion, simi\u00adlartoa relational \ndatabase.Viewingthe count row of the call stack  com/mysql/jdbc/ConnectionImpl.commit . Database Commit \n.../XATransactionWrapper.rollback . Database Rollback .../SQLServerStatement.doExecuteStatement . Database \nQuery .../WSJdbcStatement.executeBatch . Database Batch .../OracleResultSetImpl.next . Database Cursor \nFigure 9. Rules that de.ne aspects of the Database Category. clusters table in Figure 8 reveals that \ncluster c1 occurred 3592 times across all application samples.Viewing table blocked in Fig\u00ad ure8indicates \nthatin the .rst and last application samples, cluster c1 was waiting to enter the critical section guarded \nby monitor m2. Viewing theowned by table in turn reveals that m2 was owned by cluster c2. In other words, \nthread stack c1 was blocked on a moni\u00adtor held by thread stack c2.  5.2 Category Analysis WAIT relies \non a simple pattern-matching system, driven by a set of rules characterizing well-known methods and packages, \nto determine the Category label for each stack frame. The system relies on a simple declarative speci.cation \nof textual patterns that de.ne Categories. The declarative rules that de.ne the Category analysis de.ne \ntwo models. The .rst model is a CategoryTree, such as the one shown in 5(b). A Category Tree provides \nthe namespace, inheri\u00ad tance structure, and prioritization of the Category abstractions that are available \nas method labels. The second model is a set of rules. Each rule maps a regular expression over method \nnames to a node in the CategoryTree.Forexample, Figure9 shows rules that,in part, de.ne Database activity. \nThe rules distinguish between .ve aspects of database activity: queries, batch queries, commits, roll\u00adbacks, \niteration over result sets. This example illustrates how it is easy to de.ne a Category Tree that is \nmore precise than the one shown in 5(b). Given these rules, the Category analysis is simple and straight\u00adforward. \nThe analysis engine iterates over every frame of ev\u00adery call stack cluster, looking for the highest-priority \nrule that matches each frame.As describedin Section4,every methodhas an implicit Category, its package \nwhich is assigned to the Cat\u00adegory s Code Nickname. Thus, if no Category rule applies to a frame, then \nwe form an ad hoc Category for that frame: a method P1/P2/P3/.../Class.Method receives the Code Nickname \nP2 Code.  5.3 Wait State Analysis In addition to inferring Categories, WAIT infers a Wait State as illustrated \nin Figure 5(a). Our analysis to infer Wait States com\u00ad bines three sources of information: processor \nutilization, the con\u00adcrete data model previously presentedin Figure8, and rules based on method names. \nThe rules over method names, in some cases, require the inspection of multiple frames in stack. This \ndiffers from the Category analysis, where each frame s Category is independent of other frames. The main \nchallengein using method namesto inferaWait State concerns handling imperfect knowledge of an application \ns state. As discussedin Section3.2,the trueWait Stateofa sampled thread is, in many cases, not knowable. \nTo .ll this knowledge gap, we use expert knowledge about the meaning of activities, based on method names. \nFortunately, many aspects of Wait States depend on the meaning of native methods, and the use of native \nmethods does not vary greatly from application to application. The design of Java seems actively to discourage \nthe use of native methods. Nevertheless our conclusions are always subject to the inevitable hole in \nthe rule set. The algorithm proceeds asa sieve, looking for theWait State that can be inferred with the \nmost certainty. The algorithm uses data from the concrete data model presented in Figure 8, as well as \na set of rules over method names, speci.ed declaratively with patterns (analogous to the Category Analysis). \nTheWait Stateofagiven call stack cluster c at sample index i is the .rst match found when traversing \nthe following conditions, in order: 1. Deadlock: if this stack cluster participates in a cycle of lock \ncontention in the monitor graph; i.e., there is a cycle in the Blocked and Owned By relations. 2. Lock \nContention:if the stack cluster, at the moment in time of sample i, has an entry in the Blocked relation. \n 3. Awaiting Noti.cation: if the stack cluster, at the moment in time of sample i, has an entryin theWaiting \nrelation. 4. Spinlock: if the Wait State rule set de.nes a method in c that with high certainty, implies \nthe use of spinlocking. Many methods in the java.util.concurrent library fall in this high-certainty \ncategory. 5. Awaiting Data from Disk, Network: if the rule set matches c as a use of a .lesystem or \na network interface. Most such rules need only inspect the leaf invocation of the stack, e.g. a socketRead \nnative invocation is a very good indication that this stack sample cluster is awaiting data from the \nnetwork.2 In some cases, requests for data are dispatched to a stub/tie method, as is common in LDAP \nor ORB implementations. 6. ExecutingJava Code:if the method invoked by the top of the stack is not a \nnative method, then c is assumed to be Runnable, executing Java code. 7. Executing NativeCode:ifthe \nmethodinvokedbythetopofthe stack is a native method, and the rule set asserts that this native method \nis truly running, then we infer that the native method is Runnable.We treatnativeandJavainvocations asymmetrically, \nto increase robustness. A Java method, unless it participates in the monitor graph, is almost certain \nto be Runnable. The same cannot be said of native methods. In our experience, native methods more often \nthan not, serve the role of fetching data, rather than executing code. Therefore, we require native methods \nto be whitelisted in order to be considered Runnable. 8. JVM Services: if c has no call stack, it is \nassumed to be exe\u00adcuting nativeservices. Anycompilation and Garbage Collection threads,spawnedbytheJVM \nitself,fallintothiscategory.Even though these call stack samples have no call stacks, and unreli\u00adable \nthread states, theyparticipate in the monitor graph. Thus, unlessthey are observedtobeina ContentionorAwaitingNo\u00adti.cation \nstate, we assume they are Runnable, executing JVM Services. 9. Poll, IOWait,JoinPoint: if there exists \na rule that describes thenative methodatthetopofthestackasoneofthesevariants of Sleeping. 10. NativeUnknown: \nany call stack cluster with a native method at the top of the stack and not otherwise classi.ed is placed \ninto the NativeUnknownWait State. This classi.cation is in contrast to call stack clusters with Java \nleaf invocations, which are assumed to be Runnable. For robustness, the algorithm requires call stacks \nwith native leaf invocations to be speci.ed by rules to be in some particularWait State. This allows \ntool users to quickly spot de.ciencies in the rule set. In practice, we  2Thisislikely,butnotan absolute \ncertainty.TheJVMand operatingsystem spend some time copying networkbuffers, etc., as part of fetching \nremote data.Weare currentlyworkingto incorporatenative stacksintotheruleset, will allow fora more re.nedWait \nState inference.  java/net/SocketInputStream.socketRead0 . Network com/ibm/as400/NativeMethods.socketRead \n. Network org/apache/.../OSNetworkSystem.write . Network sun/nio/ch/SocketChannelImpl.write0 . Network \njava/lang/Object.wait . %Condvar java/util/concurrent/locks/ReentrantLock . %Condvar (a) Declarative \nRules Based on One Leaf Invocation %Condvar &#38; ObjectInputStream.readObject . Network (b) ADeclarative \nRule Based on Several Frames in a Stack Wait State Rule #Rules Waiting on ConditionVariable 26 Native \nRunnable 22 Awaiting Data from Disk,Network 16 Spinlock 12 Table 3. The76Wait State rules, grouped accordingtoWait \nState. Only 5 of these rules use semantics from outside the Standard Library and Apache Commons. havefound \nthat our handling of nativemethods is robust enough that this state .res rarely. Rules for Wait States \nThe syntax for declaringWait State rules is more general than that for Category rules, which depend on \nex\u00adactly one method name. In particular, rules can specify antecedents which depend on a conjunction \nof frame patterns appearing to\u00adgether in a single stack, as illustrated in Figure 10. For convenience, \nthe rules engine allows a declarative speci.\u00adcation of tags, which are auxiliary labels which can be \nattached to a stack to encode information consumed by other rules which pro\u00adduceWait States. Figure 10(b) \nshows an example that matches against two frames, and relies on an auxiliary tag(%CondVar)which labels \nvarious manifestations of waiting on a condition variable.  5.4 Rule Coverage A key hypothesis in any \nrule-based system is that a stable, and hopefully small, set of rules can achieve good coverage on range \nof diverse inputs. This sectionevaluates thishypothesis for the WAIT rule-based analysis. WAIT has been \nlive within IBM for 10 months, and over that time has received over 1,200 submissions, 830 of which were \nsub\u00admittedby 151 unique users who were notinvolvedin anyway with WAIT development. The submissions came \nfrom multiple divisions within IBM and cover a wide range of Java applications, including document processing \nsystems, e-commerce systems,business an\u00adalytics, software con.guration and management, mashups, object \ncaches,and collaboration services.To handle these reportswehave encodeda totalof76Wait State rules. This \nset of rules has proven to be extremely stable. When our tool sees code fromanew framework, these rules \nmust change only to the extent that the framework interacts with the world outside of Javainnewways.In \npractice,werarelyseenew.avorsof crossings between Java application code and the native environment. In \nthe case of a new cryptographic library, or a new in-Java caching framework, noneof theseWait State rules \nneed change. Figure 10. SomeexampleWait State rules.(a) Simple rules spec\u00adifyaWait State(e.g., Network)or \nan auxiliary tag(e.g.,%CondVar) based on a single method frame. (b) Complex rules can use con\u00adjunctions \nof antecedents to match stacks which satisfy multiple conditions at different layers in the stack. (a) \nRules, by Category (b) Database, by provider Category #Rules Category #Rules Database 72 DB2 18 Administrative \nClient Communication 59 41 MySQL Oracle 14 12 Disk, Network I/O Waiting for Work Marshalling JEE 46 30 \n30 22 Apache SqlServer 8 6 Classloader 13 Logging LDAP 12 6 Table 4. It typically takes only a small \nnumber of rules to cover the common Categories. #Thread Category Package #Reports Stacks Rule Nickname \nWAIT team 378 605,460 87.7% 12.2% External users 830 1,391,033 77.3% 22.7% Total 1208 1,996,493 80.4% \n19.5% Table 5. Rule coverage statistics onWAIT reports submitted. Data shows what percent of thread stacks \nare labeled explicitly by the Category rules, vs falling back to nickname based on the Java package. \nFortheCategory analysiswehavefoundthatonlyasmall num\u00adber of rules are necessary to capture a wide range \nof Categories. Table4a characterizes mostof the 387 Category rules that wehave currently de.ned.Forexample,our \ncurrentrulesetcovers.vecom\u00admon JDBC libraries, including IBM DB2 andMicrosoft SqlServer, with only 72 \nrules. The number of rules speci.c to a particular JDBC implementation lies on the order of 10 20, as \nshown inTa\u00ad ble4b.Wehavealsofoundtherulestobe stable acrossversionsof any one implementation.Forexample, \nthe same setof rules cov\u00aders all known versions and platforms of the DB2 JDBC driver that we have tested. \nThis testing includes three versions of the code and four platforms. It is dif.cult to prove that the \nexisting rules are suf.cient other than to observe the eventual success of the tool within IBM. One concrete \nmetric to assess rule coverage is to observe how often a thread stack does not match any Category, thus \ndefaults to a nickname based on the Java package. Table 5 presents this metricfortheWAIT submissionswehavereceivedsofar.The1208 \nsubmissions contain 1,996,493 thread samples; 80.5% of these stacks are labeled by our Category analysis. \nThe remaining 19.5% fallbacktobeing namedby package. Ignoringthe378 submissions byWAITteam members,thecoveragedropsonly \nslightlyto77.3% of stacks being labeledby rules, and 22.7%by package name. 6. The WAIT Tool Based on \nthe abstractions and analyses presented throughout the paper, we have implemented WAIT as a software-as-a-service \nde\u00adployed in IBM. In this section, we describe the overall design of the tool, present the user interface, \nand discuss some implementa\u00adtion choices.  6.1 WAIT Tool Architecture We designedWAIT with a primary \nrequirement of a low barrier to entry: the tool must be simple and easy to use. Anycomplex install procedure \nor steep learning curve will inhibit adoption, particularly when targeting large commercial systems deployments. \nTo meet this goal, we implementedWAIT asa service. Using WAIT involves three steps: 1. Collectoneormorejavacores.Thiscanbedonemanually,orby \nusing a data collection script we provide that collects machine utilization and process utilization, \nas discussed in Section 3.1. 2. Upload the collected data to the WAIT server through a web interface. \n 3. View the report in a browser.  Aservice architecture offers the following advantages: Zero-install. \nThe user can use WAIT without having to install anysoftware.  Easy to collaborate.A WAIT report can \nbe shared and dis\u00adcussed by forwarding a single URL.  Incrementally re.ned knowledge base. By having \naccess to the data submitted to the WAIT service, the WAIT team can mon\u00aditor the reports being generated \nand continually improve the knowledge base when existing rules prove insuf.cient. This in\u00adcremental re.nement \napproach has proved particularly bene.\u00adcial during the early stages of development and allowed us to \nrelease the toolfar earlier thanwouldhave been possible witha standalone tool.  Cross-report analysis. \nHaving access to a large number of re\u00adports allows looking for trends that may not stand out clearly \nin a single report.  The main disadvantageofa service-based toolis thatit requires a network connection \nto the server, which is sometimes not avail\u00adablewhendiagnosingaproblemata customersite.Theremayalso be \nprivacy concerns with uploading the data to a central server, al\u00adthough these concerns are mitigatedbya \nserver behinda corporate .rewall. One IBM organization has deployedaclone WAIT service on their own server, \nto satisfy more strict privacyrequirements.  6.2 User Interface Figure 11 shows a screenshot of a WAIT \nreport being viewed in Mozilla Firefox. The report is intended to be scanned from top to bottom, as this \norder aligns with the logical progression of ques\u00adtions an expert would likely ask when diagnosing a \nperformance problem. Activity Summary The top portion of the report present a high\u00adlevel view of the \napplication s behavior. The pie charts on the left present data averaged over the whole collection period, \nwhile timelines on the right show how the behavior changed over time. The top row shows the machine utilization \nduring the collection period, breaking down the activity into four possible categories: Your Application \n(the Java program being monitored), Garbage Collection, Other Processes, and Idle. This overview appears \nin the WAIT report .rst because it represents the .rst property one usually checks when debugging a performance \nproblem. In this particular report, the CPU utilization drops to zero roughly 1/3 of the way through \nthe collection period, a common occurrence when problems arise in a multi-tier application. The second \nand third rows report theWait State (described in Section 4.1)of all threads found running in the JVM. \nThe second row shows threads that are Runnable, while the third row shows threads that are Waiting. Each \nbar in the timeline represents the data from one Javacore. This example shows as many as 65 Runnable \nthreads for the .rst8javacores taken, at which point all runnable activity ceased,andthe numberofWaiting \nthreadsshotupto140, allinWait State Delayed by Remote Request.3 Skimming the top portion of the WAIT \nreport enables a user to quickly see that CPU idle time follows from threads backing up in a remote request \nto another tier. Category Viewer The lower left hand pane of the WAIT report showsa breakdownof most \nactive Categories (technically, primary Categoriesfrom Section4.2)executinginthe application. Clicking \non a pie slice or bar in the above charts causes the Category pane to drill down, showing the Category \nbreakdown for theWait State that was clicked. This report shows that allbut one of the threads inWait \nState DelayedbyRemote Request wereexecuting Category Getting Data from Database. This indicates that \nthe source of this problem stems from the database becoming slow or unresponsive. WAIT does not currently \nsupport further detailed diagnosis of problems from the database tier itself; WAIT s utility stems from \nthe ease with which the user can narrow down the problem to the database, without having even looked \nat logs from the database machine. Stack Viewer Glancing at the commonly occurring Wait States and Category \nactivity often suf.ces to rapidly identify bottlenecks; however, WAIT provides one additional level of \ndrilldown. Select\u00ading a bar in the report opens a stack viewer pane to display all call stacks that match \nthe selectedWait State and Category. Stacks are sorted by most common occurrence to help identify the \nmost im\u00adportant bottlenecks.Havingfull stack samplesavailablehasproven valuable not only for understanding \nperformance problems,but for .xing them. The stacks allow mapping back to source code with full context \nand exact lines of code where the backups are occur\u00adring.Passing this information on to the application \ndevelopers is often suf.cient for them to identify a .x. The presence of thread stacks makes WAIT useful \nnot only for analyzingwaiting threads,but also for identifying program hot spots. Clicking on the Runnable \nThreads pie slice causes the Stack Viewer to display the most commonly occurring running threads. Browsing \nthe top candidates often produces surprising results such as seeing logging activity or date formatter \nappear near the top, suggesting wasted cycles and easy opportunities for streamlining the code. Discussion \nThe WAIT user interface takes an intentionally min\u00adimalistic approach, striving to present a small amount \nof semanti\u00adcally rich datato users rather thanoverloading them with mountains of raw data. Although the \ninput to WAIT is signi.cantly lighter weight and more sparse than of manyother tools, particularly those \nbased on tracing, we have found that WAIT is often more effective for quick analysis of performance problems. \nThe pairing of WAIT s analyses together with drilldown to full stack traces has proven to be a powerful \ncombination. The WAIT abstractions guide the user s focus in the right direction, and presents a set \nof concrete thread stacks that can be used to con.rm thehypotheses. If the WAIT analysis is incorrect, \nor is simply not trusted by a skeptical user, viewing the stacks quickly con.rms or denies their suspicion. \n3This label corresponds to the Awaiting Data node in Figure 5(a). The UI does not always present the \nsame text as used to de.ne the underlying model,but the mappingis straightforward.For the remainderof \nthis paper, when discussing the UI, we simply refer to text labels as presented in the UI.   application \nthreads Analysis time (milliseconds) samples sampled zipped input clustered output Firefox 3.6 Safari \n4.05 A1 1 228 156kB 50kB A1 140 23 A2 1 206 447kB 40kB A2 90 16 A3 6 90 112kB 22kB A3 53 13 A4 10 356 \n449kB 15kB A4 68 28 A5 499 27,638 27MB 56kB A5 560 204 A6 946 127,536 120MB 499kB A6 720 110 Table 6. \nSize of the data model communicated from server to the client s browser. This data is a representative \nsample from actual IBM customer data that was submitted, by support personnel, to the WAIT service. \n 6.3 Implementation WAIT is coded in a combination of Java and Javascript. The ETL step of parsing the \nraw data and producing the data model (Sec\u00adtion5.1)runsinJavaandexecutesonthe server once,whenareport \nis created. The remaining analyses (Wait State analysis and Cate\u00adgory analysis) run in Javascript and \nexecute in the browser each time a report is loaded. This somewhat controversial design allows users \nto modify the rules or select alternate rules con.gurations without a round trip to the server. This \ndesign also allows WAIT reports, once generated, to be viewed in headless mode without a server present; \nthe browser can load the report off a local disk and maintain full functionality of the report. This \ndesign presents two performance constraints on the WAIT analysis: (1) the data models must be suf.ciently \nsmall to avoid excessive network transfer delays, and (2) the analysis written in Javascript mustbefast \nenoughtoavoid intrusive pauses,and more\u00adoveravoidJavascript timeouts.Forexample, Firefox 2.5warns the \nuser that something might be wrong with the page after roughly .ve seconds of Javascript execution. Table6reports \ndata model sizes for six representative IBM cus\u00adtomer applications, labeled A1 A6. The clustering and \noptimiza\u00adtion steps described in Section 5.1 produce a signi.cant reduction in space for the data model. \nFor larger inputs, the clustered data model is multiple orders of magnitude smaller than the zipped raw \ndata input .les. The largest submission received to date contained 946samples(Javacores),withatotalof \n127,536stacksamples.The data modelforthis reportwas stillamanageable 499kB.Moretyp\u00adically,we havefound \nthat the clustered output consumes fewer than 50kB. The performance of the WAIT analyses is reasonable, \neven though executed within the browser. Table 7 shows the analysis runtime for the applications from \nTable 6. The table shows that, even with a large number of application samples (e.g. the 946 of application \nA6), the analysis completes in less than one second. As discussed later in Section7, most typical uses \nof the tool have in\u00ad puts withasmaller numberof samples, more similarto applications A1 through A4.For \nthese typical cases, the analysis usually com\u00adpletes in less than 100ms. In general, analysis takes less \ntime than general widget-related rendering work. 7. Case Studies Wenowshowsix additional case studiesto \ndemonstrate howWAIT illuminates various performance problems. All these case studies represent real problems \ndiscovered when analyzing IBM systems. To conserve space, we show only the relevant portions of the UI \nfor each case study. Table 7. Execution time for the inference engine, as Javascript on a2.4GHz Intel \nCore2 Duo with 4GB of memory,on MacOS 10.6.2. The applications, A1 A6, are the same as thoseinTable6. \n7.1 Lock Contention and Deadlock Figure12 depictsa WAIT report ona 48-core system. TheWaiting Threads \ntimeline shows a sudden and sustained surge of Blocked on Monitor; i.e. threads seeking a lock and not \nreceiving it, and thus being blocked from making progress. Looking at the Cate\u00adgory breakdown suggests \nthat lock contention comes from miscel\u00adlaneous APACHE OPENJPACode. The threadstacks from this cat\u00adegory \nidentify the location of the lock contention as line 364 of getMetaDataLocking(). Sometimes locking issues \ngo beyond contention to the point of deadlock orlivelock. WhenWAIT detectsacyclein the monitor graph, \ntheirWait State is Blocked on Deadlock. This situation ap\u00adpearsinFigure13.Lookingatthe thread stacksatthelowerrightof \nthereport suggeststhatthe threadsarewaitingforalockinthelog\u00adging infrastructure method SystemOutStream.processEvent() \nline 277. Armed with this information,a programmer could lookat the code and try to determine the reason \nfor the deadlock. 7.2 Not Enough Load TheWAIT report in Figure 14 shows that the machine was 67% idle,andthe \ntimeline con.rmsthatutilizationwas consistentlylow during the entire monitoring period. TheWaiting Threads \npiechart indicates that threads spend most of their time Delayed by Remote Request. Digging more deeply, \nthe Category viewindicates that the remote request on which the threads are delayed is client communi\u00adcation \nusing an HTTP protocol. In this case, the performance prob\u00adlem is not with the server machine, but that \nthe amount of data beingreceivedfromthe client machineisnotsuf.cienttokeepthe server busy. Indeed it \nis possible that there is no problem in this situation, other than that the server may be over provisioned \nfor thisworkload.WAIT cannot directly determine whether the client is generating a small amount data \nor if the network between the client and server is under-provisioned to handle the request traf.c. To \nanswer this question, a network utility such as netstat could be employed, or the CPU utilization of \nclient machines could be investigated.  7.3 Memory Leak WAIT can also detect memory leaks. As shown \nin Figure 15, a memory leak can be detected by looking at just the .rst timeline, whichshowsgarbage collectionactivityovertime.Inthisexample, \ninitially the non-GC work dominates, but over time the garbage collection activity increases until it \neventually consuming most of the non-idle CPUcycles. The large increaseingarbage collection as time passes \nis strong evidence that the heap is inadequate for the amount of live memory; either the heap size is \nnot appropriate for the workload, or that the application has a memory leak.   7.4 Database Bottleneck \nFigure16 presentsanexampleofadatabase bottleneck. UnlikeFig\u00ad ure 11 where the database became completely \nunresponsive, in this case the database is simply struggling tokeep up with the appli\u00adcation server s \nrequests. Over time the server s utilization varies between approximately 10% and 85%, and these dips \nin utilization correlate roughly with the spikesin the numberof threadsinWait\u00ading state Delayed by Remote \nRequest and Category GettingData from Database, thus pointing to the likely source of the delay. Clicking \non the orange bar for Getting Data from Database reveals the thread stacks that a developer can analyze \nto determine key parts of the application delayed by the database, and try to reduce the load generated \nagainst the database. Alternatively, the database couldbe optimized or deployed onfaster hardware.  \n7.5 Disk I/O Affecting Latency Figure17showsaWAITreportwhere .lesystemactivityislimiting performance. \nThe top two pie charts and timelines show that there isenoughJavacodeactivitytokeeptheCPUs well-utilized.How\u00adever, \ntheWaiting activity showa signi.cant numberof threadsin Wait StateDelayedby Disk I/O, and CategoryFilesystem \nMetadata Operations, suggesting room for improvement withfasterdisks, or by restructuring the code to \nperform fewer disk operations. Reducing these delays would clearly improve latency, since each transaction \nwould spend less time waiting on Disk I/O, but such improvement would have other bene.ts as well. Even \nthough the four CPUs on this machine are currently well utilized, this application will likely scale \npoorly on larger machines. As the number of processors increases, the frequent disk access delays will \neventually become a scaling bottleneck. WAIT can help identify these scaling limiters early in the development \nprocess. 8. RelatedWork Most conventional gprof-style performance analysis tools(e.g., [19, 20]) provide \na pro.le which indicates in which methods a pro\u00ad gram spends time. Often, these tools provide a hierarchical \ntree view, in which an analyst can recursively drill down to .nd time spent in subroutines. These tools \nalso often provide a summary view which indicates the sum totals of execution time over a run. Since \nperformance characteristics vary over time, a summary view isoftenlesshelpfulthan snapshotviewsofpro.ledataover \nsmaller windows. Manytools(e.g.,[12, 15, 20]) provide snapshot or win\u00ad dow views. A few previous works \nhave applied abstraction to pro.le in\u00adformation, in order to infer higher-level notions of bottlenecks. \nHollingsworth[7]identi.ed classes of performance bottlenecks in parallel programs, and introduced a methodology \nto dynamically insert instrumentation to test for bottlenecks. Srinivas and Srini\u00advasan [10] present \na tool design that allows the user to de.ne Components , abstract names for code from particular packages \nor archives. This paper presents a new methodology to present lay\u00aders of abstraction when monitoring \nperformance, suitable for pro\u00adduction enterprise deployment scenarios.   Afew previous works have been \ndeveloped to mine log infor\u00admation in large systems[22, 23]. However, unlike the approach proposed here, \nboth require access to source code that is often unavailable in enterprise environments. In addition,[22]employs \nmachine-learning to determine important characteristics to report as opposed to the expert rule structure \nemployed here.[23] uses analysis of source code control paths to determine precise causes of error conditions. \nThere are heavier-weight alternatives to sampling that we chose to avoid. Performance tools exist that \ncollect detailed information about method invocations[2,3,5,6, 11, 13, 19], or that stitch together an \nend-to-end trace of call .ows across a multi-tier sys\u00adtem[1,4, 14, 16, 18]. We also note that performance \nexperts have been using Java core dumpsfor manyyears,but generallyinan ad-hocwayusing theireyesand brainsto \nsummarizethedata insteadofhavingatool to do so. Though other tools like Thread Analyzer [17]takeJava \ncores as input, the tools of which we are aware are interactive, and requireanexpert userforeffectiveresults.Asaresult,existing \nJava core tools are generally not suitable for use by less expert users for quick triage. Others have \nattempted to use method names to help diagnose software problems[8]. However, unlike our approach which \nem\u00ad ploys method names to diagnose functional issues in the system, [8]uses method name analysis to promote \nadherence to naming conventions and readable, maintainable software. 9. Conclusion We have presented \nthe design and methodology behind WAIT,a tool designed to pinpoint performance bottlenecks in server \nappli\u00adcations based on idle time analysis. WAIT has been used widely throughout IBM, in both development \nand production deploy\u00adments.Throughthesedeployments,wehave learnedhowtoexploit ubiquitous sample-based \nmonitoring to infer rich semantic infor\u00admation about server application performance. Although much of \nthe literature explores intrusive monitoring based on instrumenta\u00adtion and monitoring agents, we have \nshown effective performance analysis is possible with a lower barrier to entry.  References [1]M.K. \nAguilera,J.C.Mogul,J.L.Wiener,P.Reynolds,andA. Muthi\u00adtacharoen. Performance debugging for distributed \nsystems of black boxes. In Symposium on Operating System Principles.ACM, 2003. [2] W. P. Alexander, R. \nF. Berry, F. E. Levine, and R. J. Urquhart. A unifying approach to performance analysis in the java environment. \nIBM SystemsJournal, 39(1), 2000. [3] G. Ammons, J.-D. Choi, M. Gupta, and N. Swamy. Finding and removing \nperformance bottlenecks in large systems. In The European Conference on Object-Oriented Programming. \nSpringer, 2004. [4] B. Darmawan, R. Gummadavelli, S.Kuppusamy, C.Tan, D. Rintoul, H. Anglin, H. Chuan, \nA. Subhedar, A. Firtiyan,P. Nambiar,P. Lall, R. Dhall, and R. Pires. IBMTivoli Composite Application \nManager Family: Installation, Con.guration, and Basic Usage. http://www. redbooks.ibm.com/abstracts/sg247151.html?Open. \n[5] W. De Pauw, E. Jensen, N. Mitchell, G. Sevitsky, J. Vlissides, and J. Yang. Visualizing the execution \nof Java programs. In Software Visualization, State-of-the-art Survey, volume 2269 of Lecture Notes in \nComputer Science. Springer-Verlag, 2002. [6] R. J. Hall. Cpprofj: Aspect-capable call path pro.ling of \nmulti\u00adthreadedjavaapplications. In Automated Software Engineering,pages 107 116. IEEE Computer Society \nPress, 2002. [7] J. K. Hollingsworth. Finding Bottlenecks in Large-scale Parallel Programs. PhD thesis, \nUniversityofWisconsin, Aug. 1994. [8] E.W. Host and B. M. Ostvold. Debugging method names. In The European \nConference on Object-Oriented Programming. Springer, 2009. [9] N. Mitchell, G. Sevitsky, and H. Srinivasan. \nModeling runtime behav\u00adior in framework-based applications. The European Conference on Object-Oriented \nProgramming, 2006. [10] K. Srinivas and H. Srinivasan. Summarizing application performance fromacomponents \nperspective. Foundations of SoftwareEngineering, 30(5):136 145, 2005. [11] Borland Software Corporation. \nOptimizeItTM Enterprise Suite. http://www.borland.com/us/products/optimizeit, 2005. [12] Compuware. Compuware \nVantage Analyzer. http://www. compuware.com/solutions/e2e brochures factsheets.asp. [13] Eclipse. EclipseTest&#38;PerformanceTools \nPlatform Project. http: //www.eclipse.org/tptp. [14] HP. HP Diagnostics for J2EE. [15] IBM. Compuware \nVantage Analyzer. http://alphaworks.ibm. com/tech/dcva4j/download. [16] IBM. IBM OMEGAMON XE for WebSphere. \nhttp://www-01. ibm.com/software/tivoli/products/omegamon-xe-was. [17] IBM. Thread and Monitor Dump Analyzer \nfor Java. http://www. alphaworks.ibm.com/tech/jca. [18] IBM. Tivoli Monitoring for Transaction Performance. \nhttp://www-01.ibm.com/software/tivoli/products/ monitor-transaction. [19] Sun Microsystems. HPROF JVM \npro.ler. http://java.sun. com/developer/technicalArticles/Programming/HPROF. html, 2005. [20] Yourkit \nLLC. Yourkit pro.ler. http://www.yourkit.com. [21] G. Xu, M. Arnold, N. Mitchell, A. Rountev, and G. \nSevitsky. Go with the .ow: pro.ling copies to .nd runtime bloat. In PLDI 09: Proceedingsof the 2009ACM \nSIGPLAN conference onProgramming language design and implementation, pages 419 430,NewYork,NY, USA, 2009.ACM. \n  [22]W.Xu,L. Huang,A.Fox,D.Patterson,andM.I. Jordan. Detecting [24]Y.Zhao,J.Shi,K.Zheng,H.Wang,H.Lin,andL.Shao. \nAllocationlarge-scale system problemsby mining consolelogs. In Symposium wall: a limiting factor of java \napplications on emerging multi-core on Operating System Principles.ACM, 2009. platforms. In OOPSLA 09: \nProceeding of the 24thACM SIGPLAN [23] D. Yuan, H. Mai, W. Xiong, L. Tan, Y. Zhou, and S. Pasupathy. \nconference on Object oriented programming systems languages and Sherlog: Error diagnosis by connecting \nclues from run-time logs. In applications, pages 361 376,NewYork,NY, USA, 2009.ACM. Architectural Support \nfor Programming Languages and Operating Systems, Mar. 2010.    \n\t\t\t", "proc_id": "1869459", "abstract": "<p>This paper presents an approach for performance analysis of modern enterprise-class server applications. In our experience, performance bottlenecks in these applications differ qualitatively from bottlenecks in smaller, stand-alone systems. Small applications and benchmarks often suffer from CPU-intensive hot spots. In contrast, enterprise-class multi-tier applications often suffer from problems that manifest not as hot spots, but as <i>idle time</i>, indicating a lack of forward motion. Many factors can contribute to undesirable idle time, including locking problems, excessive system-level activities like garbage collection, various resource constraints, and problems driving load.</p> <p>We present the design and methodology for WAIT, a tool to diagnosis the root cause of idle time in server applications. Given lightweight samples of Java activity on a single tier, the tool can often pinpoint the primary bottleneck on a multi-tier system. The methodology centers on an informative abstraction of the states of idleness observed in a running program. This abstraction allows the tool to distinguish, for example, between hold-ups on a database machine, insufficient load, lock contention in application code, and a conventional bottleneck due to a hot method. To compute the abstraction, we present a simple expert system based on an extensible set of declarative rules.</p> <p>WAIT can be deployed on the fly, without modifying or even restarting the application. Many groups in IBM have applied the tool to diagnosis performance problems in commercial systems, and we present a number of examples as case studies.</p>", "authors": [{"name": "Erik Altman", "author_profile_id": "81100139431", "affiliation": "IBM T.J. Watson Research Center, Hawthorne, NY, USA", "person_id": "P2354138", "email_address": "", "orcid_id": ""}, {"name": "Matthew Arnold", "author_profile_id": "81100021720", "affiliation": "IBM T.J. Watson Research Center, Hawthorne, NY, USA", "person_id": "P2354139", "email_address": "", "orcid_id": ""}, {"name": "Stephen Fink", "author_profile_id": "81100118324", "affiliation": "IBM T.J. Watson Research Center, Hawthorne, NY, USA", "person_id": "P2354140", "email_address": "", "orcid_id": ""}, {"name": "Nick Mitchell", "author_profile_id": "81100359733", "affiliation": "IBM T.J. Watson Research Center, Hawthorne, NY, USA", "person_id": "P2354141", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1869459.1869519", "year": "2010", "article_id": "1869519", "conference": "OOPSLA", "title": "Performance analysis of idle programs", "url": "http://dl.acm.org/citation.cfm?id=1869519"}