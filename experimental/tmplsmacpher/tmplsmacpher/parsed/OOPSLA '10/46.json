{"article_publication_date": "10-17-2010", "fulltext": "\n SPUR: A Trace-Based JIT Compiler for CIL Michael Bebenita * Florian Brandner Manuel Fahndrich Francesco \nLogozzo Wolfram Schulte Nikolai Tillmann Herman Venter Microsoft Research {maf,logozzo,schulte,nikolait,hermanv}@microsoft.com \nAbstract Tracing just-in-time compilers (TJITs) determine frequently executed traces (hot paths and loops) \nin running programs and focus their optimization effort by emitting optimized machine code specialized \nto these traces. Prior work has established this strategy to be especially bene.cial for dy\u00adnamic languages \nsuch as JavaScript, where the TJIT inter\u00adfaces with the interpreter and produces machine code from the \nJavaScript trace. This direct coupling with a JavaScript interpreter makes it dif.cult to harness the \npower of a TJIT for other components that are not written in JavaScript, e.g., the DOM implemen\u00adtation \nor the layout engine inside a browser. Furthermore, if a TJIT is tied to a particular high-level language \ninterpreter, it is dif.cult to reuse it for other input languages as the opti\u00admizations are likely targeted \nat speci.c idioms of the source language. To address these issues, we designed and implemented a TJIT \nfor Microsoft s Common Intermediate Language CIL (the target language of C#, VisualBasic, F#, and many \nother languages). Working on CIL enables TJIT optimizations for any program compiled to this platform. \nIn addition, to vali\u00addate that the performance gains of a TJIT for JavaScript do not depend on speci.c \nidioms of JavaScript that are lost in the translation to CIL, we provide a performance evaluation of \nour JavaScript runtime which translates JavaScript to CIL and then runs on top of our CIL TJIT. Categories \nand Subject Descriptors D.3.4 [Programming Languages]: Processors Code generation; Optimization; Run-time \nenvironments * Contributed to this project during an internship at Microsoft Research. Contributed to \nthis project during an internship at Microsoft Research. Permission to make digital or hard copies of \nall or part of this work for personal or classroom use is granted without fee provided that copies are \nnot made or distributed for pro.t or commercial advantage and that copies bear this notice and the full \ncitation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to \nlists, requires prior speci.c permission and/or a fee. OOPSLA/SPLASH 10, October 17 21, 2010, Reno/Tahoe, \nNevada, USA. Copyright c . 2010 ACM 978-1-4503-0203-6/10/10. . . $10.00 General Terms Design, Languages, \nPerformance Keywords dynamic compilation, tracing, just-in-time, CIL, JavaScript 1. Introduction Tracing \njust-in-time compilers (TJITs) determine frequently executed traces (hot paths and loops) in running \nprograms and focus their optimization effort by emitting optimized machine code specialized to these \ntraces. Tracing JITs have a number of advantages over traditional JITs and static com\u00adpilers in that \nthey are able to harness runtime information, such as indirect jump and call targets, dynamic type special\u00adization, \nand the ability to inline calls and unroll loops more aggressively due to the focus on hot traces [4]. \nPrior work has established this strategy to be bene.cial for dynamic languages such as JavaScript, where \nthe TJIT interfaces with an interpreter and produces machine code from the JavaScript trace [18]. In \nsuch a setting, the TJIT works directly on the high-level programming constructs from the original source \nlanguage (e.g., JavaScript) and can take advantage of that information for its optimizations. It is not \nclear whether similar performance bene.ts can be achieved by .rst using standard compilation of the high-level \ndynamic language to an intermediate typed language such as Microsoft s Common Intermediate Language [16] \n(CIL) and applying trace jitting to this intermediate language instead. In this paper we thus examine \nthe following hypothesis: Trace jitting a typed intermediate language (such as CIL) and compiling high-level \ndynamic languages and their run\u00adtimes to that intermediate level, enables similar performance gains as \ndirectly trace jitting the high-level language. To con.rm this hypothesis, we designed and imple\u00admented \nSPUR, a trace JIT for CIL (the target language of C#, VisualBasic, F#, and many other languages). Working \non CIL enables TJIT optimizations for any program com\u00adpiled to this platform. To validate that the performance \ngains of a trace JIT for JavaScript do not depend on speci.c id\u00adioms of JavaScript that are lost in the \ntranslation to CIL, we provide a performance evaluation of our JavaScript run\u00adtime which translates JavaScript \nto CIL and then runs on top of our CIL trace JIT. Our evaluation shows that the per\u00adformance of the .nal \nmachine code generated for standard JavaScript benchmarks on SPUR is competitive with the best current \nJavaScript engines.  1.1 Basic Insight Statically compiling dynamic languages, such as JavaScript, to \nef.cient code ahead of runtime is dif.cult due to the dy\u00adnamic nature of the operations in the language. \nFor instance, the addition operation + of JavaScript has numerous seman\u00adtics, based on the actual runtime \ntypes of its arguments. It can be the addition of numbers, or concatenation of strings, along with conversions \nof operands to appropriate types. E.g., adding a string and a number causes the number to be converted \nto a string and the operation ends up being string concatenation. Traditional TJITs such as [18] produce \nef.cient code for such an addition operation by leveraging built-in knowledge of the operations of a \nparticular target language. They ob\u00adserve the particular types of the operands at runtime and then specialize \nthe operation to those operand types, thereby sav\u00ading many otherwise necessary tests to determine what \nop\u00aderation to apply. Note that some tests still remain as guards in the trace to determine if the special \ncase applies on future executions, but the number and placement of guards can usu\u00adally be optimized, \nor guards can even be hoisted out of inner loops. In contrast, our TJIT knows nothing about the intricate \nsemantics of JavaScript s addition operation. Instead, our JavaScript runtime (written in C# and compiled \nto CIL with the standard C# compiler) simply contains methods for each primitive JavaScript operation \nsuch as addition. These meth\u00adods perform the necessary tests to determine the types of the operands and \nwhat operation to actually perform. In effect, our runtime breaks down the high-level JavaScript semantics \nto the comparatively low-level CIL type system and opera\u00adtions. If a JavaScript program translated to \nCIL contains an ad\u00addition operation, it will show up as a call into the JavaScript runtime s addition \nmethod. Our TJIT then traces from the CIL instructions resulting from the JavaScript code into the CIL \ninstructions of the JavaScript runtime s addition opera\u00adtion. As a result, our TJIT does not depend on \nany JavaScript semantics but still specializes the dynamic behavior of the CIL code just as a TJIT working \nat the source language level would.  1.2 Motivation Coupling a TJIT with a high-level language makes \nit dif.cult to harness the power of the TJIT for other components that are not written in that language, \ne.g., when a JavaScript application in the browser is executed by a JavaScript TJIT, then JavaScript \ncode that calls out to the browser s native DOM implementation or the layout engine inside a browser \ncannot be optimized fully. Furthermore, if a TJIT is tied to a particular high-level language, it is \ndif.cult to reuse it for other input languages as the instructions traced and optimized are language \nspeci.c. Instead, our approach of adding trace jitting to a language neutral intermediate language provides \na basis for reaping the bene.t of trace compilation for an entire platform and multiple languages compiled \nonto the platform. The ability to trace through multiple software components and abstrac\u00adtion layers \nprovides an excellent way to optimize away the abstraction tax of good software engineering, which calls \nfor many small components and abstraction barriers. As part of the SPUR project, we are also in the process \nof imple\u00admenting a browser DOM and layout engine in C#, which will enable us in the future to optimize \ntraces that span JavaScript code, JavaScript runtime code, DOM, and layout code. In comparison to previous \nwork on trace jitting interme\u00addiate languages such as Java byte code [6, 20, 30], our work differentiates \nitself by having no interpreter for CIL at all, and having practically no overhead when transferring \ncon\u00adtrol between different code versions. We use multiple levels of jitting: 1) a fast pro.ling JIT producing \nunoptimized code that determines hot paths, 2) a fast tracing JIT producing in\u00adstrumented code for recording \ntraces, and 3) the optimizing JIT for traces itself. Using multiple levels of compilation and no interpretation \nis similar to the approach of Jalape no [2] and Jikes RVM [3]. The main difference of SPUR with respect \nto those systems is that our optimizations are based solely on tracing and trace optimizations, whereas \nJalape no and Jikes use online pro.le gathering and traditional optimization techniques, but not tracing. \nOur JITs are not based on the production Microsoft CLR JIT; instead, they have been written from scratch \nfor the SPUR project; besides our trace optimizations, our JITs do not perform any optimizations, and \nemploy a simple register allocator. One design goal was to enable ef.cient transitions between the different \ncode versions, e.g. by sharing calling conventions and stack layouts. Having all code compiled and run \nas machine code in a common environment allows our JIT to support scenarios not handled by previous work, \nsuch as calling into pre\u00adcompiled runtime code from an optimized trace without any additional overhead, \neven if that code may throw exceptions or causes garbage collection. In fact, there are no inherent operations \nthat force us to abort a trace. Our contributions in this paper are: We provide a positive answer to \nthe hypothesis that trac\u00ading of a typed intermediate language results in similar performance gains when \nJavaScript is compiled to that level, as trace jitting JavaScript directly.  To our knowledge, SPUR \nis the .rst tracing JIT based on jitting alone, without the need for an interpreter and without the need \nfor adapter frames to mediate between different code versions. We provide details of how to   transition \nbetween the different code versions emitted by our system. Our approach is also unique in that we have \nboth pre\u00adcompiled x86 and CIL code for some components (e.g., the JavaScript runtime) available at runtime. \nThis allows us to either call into precompiled code, or record a trace at the CIL level, in order to \noptimize and recompile the original CIL intermediate form for particular calling con\u00adtexts.  Our approach \navoids duplicating the JavaScript seman\u00adtics in both an interpreter and a compiler, as we use a single \ncompiler-runtime implementing the JavaScript se\u00admantics.  The machine code generated for optimized traces \nseam\u00adlessly integrates with other machine code, including stat\u00adically precompiled machine code, and code \njitted at run\u00adtime. This enables dif.cult scenarios not found in pre\u00advious tracing JITs, such as uni.ed \ncalling conventions, garbage collection, and exception handling, without hav\u00ading to exit the trace. \n We present a number of novel optimizations for traces, such as store-load elimination (or sinking), \nand specula\u00adtive guard strengthening.  The rest of the paper is organized as follows. Section 2 gives \na motivating example of a loop in C#, Section 3 describes the SPUR architecture and engine, Section 4 \ndescribes our various versions of just-in-time compilers and transitions be\u00adtween them, whereas Section \n5 focuses on our trace record\u00ading and optimizations. Section 6 shows how we compile JavaScript to CIL \nand Section 7 re-examines our motivat\u00ading example, which we re-write in JavaScript, showing how it is \ntranslated and eventually optimized. Section 8 contains our performance evaluation, and Section 9 discusses \nrelated work. 2. Example 1: A Loop in C# Consider the following example, which is written in C#, but \nshows the basic problem of the abstraction tax paid in JavaScript programs, where all values are (potentially \nboxed) objects. interface IDictionary { int GetLength(); object GetElement(object index); void SetElement(object \nindex, object value); } ... void ArraySwap(IDictionary a) { for (int i = 0; i < a.GetLength() -1; i++) \n{ var tmp = a.GetElement(i); a.SetElement(i, a.GetElement(i + 1)); a.SetElement(i + 1, tmp); } } In this \ncode, the interface IDictionary describing a general mapping from objects to objects is used in method \nAr\u00adraySwap with the implicit assumption that it is a dense array, mapping integers to doubles. Furthermore, \nlet us assume that ArraySwap is most com\u00admonly used with the following implementation of IDic\u00adtionary, \nwhich can only store .oating point values in a dense array: class DoubleArray : IDictionary { private \ndouble[] _elements; ... public int GetLength() { return _elements.Length; } public object GetElement(object \nindex) { return (object)_elements[(int)index]; } public void SetElement(object index, object value) { \n_elements[(int)index] = (double)value; } } Without tracing through the virtual calls in ArraySwap to \nthe interface methods, the arguments and results of GetEle\u00adment and SetElement are continually boxed, \ntype-checked for proper types, and unboxed. With tracing, only a few sim\u00adple checks, called guards, need \nto remain in place which en\u00adsure that all explicit and implicit branching decisions that were observed \nduring tracing will repeat later on. If not, then the optimized code will jump back into unoptimized \ncode. Here, we need guards to ensure that the actual dictionary is not null and actually has type DoubleArray, \nthat a. elements is not null, and also that i is a valid index, and ful.lls the loop condition. All virtual \nmethod calls can be inlined, all box\u00ading, type-checks and unboxing code eliminated. Here, most guards \ncan be hoisted out of the loop, and code similar to the following is produced by SPUR for the loop, preceeded \nby the hoisted guards: if (a == null || a.GetType() != typeof(DoubleArray)) { ... /* transfer back to \nunoptimized code */ } var elements = a._elements; if (elements == null) { ... /* transfer back to unoptimized \ncode */ } var length1 = elements.Length -1; while (true) { if(i<0|| i>=length1) { ... /* transfer back \nto unoptimized code */ } double tmp = elements[i]; elements[i] = elements[i + 1]; elements[i + 1] = tmp; \ni++; } We ran this C# ArraySwap benchmark program with a Dou\u00adbleArray initialized to 1000 elements on \na Intel Core2 Duo CPU, P9500 @ 2.5 Ghz, 4 GB Ram, Windows 7 Enterprise, 32-bit version. Running it 100, \n000 times in a loop took  Figure 2. The SPUR architecture. 5.7 seconds with the regular Microsoft .NET \nv4.0 CLR JIT, compared to 0.8 seconds using the SPUR tracing JIT. For both JITs, the overhead for jitting \nand tracing is less than 0.1 seconds. Thus, the SPUR JIT improved performance of this C# benchmark by \na factor of around 7. We will revisit this example in Section 7, where we see in more detail how a similar \nmethod written in JavaScript source gets compiled, traced, and optimized by SPUR. 3. SPUR The basic input \nprogram to run on top of SPUR is any pro\u00adgram compiled to Microsoft s Common Intermediate Lan\u00adguage CIL \n[16]. CIL is a stack based intermediate language with support for object-oriented features such as inheritance, \ninterfaces, and virtual dispatch. It also supports value types (like C structs), pointers to object .elds \nand to locals on the runtime stack, and method pointers (usually wrapped in CIL delegate types) with \nindirect method calls. Instructions oper\u00adate on the evaluation stack by popping operands and pushing \nresults. CIL is the target for languages such as C#, Visual-Basic, F#, and in this paper, JavaScript. \n3.1 Architecture Figures 1 and 2 show the architecture of SPUR con.gured to run JavaScript code. JavaScript \nsource code is compiled by the JavaScript compiler which produces CIL. The SPUR engine jits this CIL \nto pro.ling code that we run on the Bartok runtime. Bartok is a managed runtime and static compiler developed \nat Microsoft Research. When pro.ling determines a hot path, tracing code is jitted and executed. The \nrecorded trace is then optimized and jitted. The native bridge in the .gures refers to linking informa\u00adtion \nthat is generated ahead-of-time via the Bartok compiler. This linking information consists of runtime \nentry-points as well as object layout information of runtime types. It is used in order to make dynamically \ncompiled X86 interoperate with ahead-of-runtime produced X86. In the following, we often refer to ahead-of-runtime \ncode as native code. Figure 1 elucidates what code is produced ahead-of\u00adruntime (native code), what code \nis produced dynamically (jitted code), and whether the code is CIL or X86. The upper part of the .gure \nrefers to code produced ahead-of-runtime, while the bottom of the .gure shows code produced during SPUR \ns execution. The left side of the .gure shows code in the form of X86 binary instructions (our native \ntarget), while the right side of the .gure shows code in the form of CIL instructions. The four quadrants \nthus give rise to all combi\u00adnations of ahead-of-runtime or dynamically generated, X86 or CIL code. Examples \nof ahead-of-runtime code in X86 form are the JavaScript compiler code, the garbage collector (GC), and \nour X86 code generator itself. An example of ahead-of\u00adruntime code in CIL form is the CIL for the JavaScript \nruntime, produced by compiling our C# sources to CIL. CIL code generated at runtime consists of the code \npro\u00adduced by the JavaScript compiler for JavaScript source code. X86 code generated at runtime is code \nproduced by our JIT for CIL instructions from the right-side of the architecture, both from CIL generated \nahead-of-runtime and CIL gener\u00adated by the JavaScript compiler at runtime.  3.2 Runtime Our runtime \nfor CIL code consists of an adaption of the pro\u00adduction generational mark and sweep garbage collector \nof the Microsoft CLR, and the Bartok runtime. The garbage collector maintains three separate generations, \nand a sepa\u00adrate large-object heap; all write accesses of references re\u00adquire a write barrier. The Bartok \nsystem also provides a static compiler de\u00adveloped at Microsoft Research, compiling CIL to X86 code ahead-of-time. \nMost of the Bartok runtime, including the base class li\u00adbraries is written in C# itself and compiled \nto CIL via C# s compiler and then to X86 code via the Bartok compiler. The same holds for the JavaScript \ncompiler and runtime. As Fig\u00adure 1 shows, we actually have both X86 and CIL versions  Figure 1. Code \nQuadrants of SPUR. of the Bartok runtime code and the JavaScript runtime code available at runtime. Having \nthe CIL form may seem unnec\u00adessary at .rst as we can just call directly into the X86 form. However, in \norder to optimize traces that span calls into the runtime, we use the CIL of runtime methods and dynami\u00adcally \ncompile them into a form that contains pro.ling coun\u00adters or callbacks to our trace recorder. This way, \nthe recorded trace contains CIL instructions of any inlined runtime meth\u00adods along with CIL instructions \nfrom compiled JavaScript code. This is crucial in order to optimize traces that span runtime code, in \nparticular for the JavaScript runtime, as its methods contain most of the specialization opportunities. \n3.2.1 Data Representations CIL objects (instances of classes) are represented at runtime using two header \nwords, followed by the .eld representa\u00adtions. The header contains a pointer to the virtual dispatch table \nof the type and the other header word is used during garbage collection. Value types, including all primitive \ntypes such as Boolean values, integers, and .oating point values, are not repre\u00adsented as heap objects, \nbut stored either in registers, the run\u00adtime stack, or as .elds of other objects. Pointers in our runtime \nare not tagged, meaning that there are no unions encompassing pointers and primitive types in the same \nstorage location. The garbage collector depends on GC tables of each type to identify the offsets of \nmanaged pointers within the object layout. Garbage collector tables also exist to identify the off\u00adsets \nof managed pointers within stack frames. Exception handlers are identi.ed via exception tables identifying \npro\u00adtected code regions and the appropriate handlers are found via lookup operations. 3.2.2 Code Representations \nIn a preprocessing step before jitting code, CIL is trans\u00adformed into an equivalent representation where \nall implicit operand stack accesses have been made explicit, using aux\u00adiliary local variables. This simpli.es \nSPUR s JIT, and the transfers into and out of optimized traces.  3.3 New Instructions SPUR adds a few \ninstructions to CIL to aid in tracing. 3.3.1 Trace Anchors A trace anchor is an instruction representing \na potential start point of a trace. Trace anchors are inserted in a preprocessing step for all targets \nof loop back-edges, as trace compilation is most pro.table for loops. We also place trace anchors af\u00adter \nall potential exit points from existing traces in order to combine multiple traces together into trace \ntrees. Trace an\u00adchors are also inserted at entry points of potentially recursive methods. While such \nautomatically inserted trace anchors can be seen as an implementation detail of our tracing JIT, mak\u00ading \nthis instruction explicit allows other high-level language compilers to emit explicit hints as to where \npotentially prof\u00aditable traces might start.  3.3.2 Transfers A transfer instruction is the only way \noptimized trace code may give control back to regular pro.ling code. A transfer instruction details the \nlayout of nested stack frames that may have to be reconstructed, corresponding to methods that were inlined \nduring tracing.   3.4 Execution Modes At any point in time, a thread running under SPUR is either in \nnative mode (running code pre-compiled with Bartok, e.g., the X86 code generator itself), or in one of \nthe following JIT modes depicted in Figure 3: Pro.ling: runs unoptimized code with counters to iden\u00adtify \nhot loops and paths.  Tracing: runs unoptimized code with call-backs to record a trace.  On-trace: \nruns the optimized trace code.  The execution mode is uniquely determined by the kind of code pointed \nat by the program counter. Thus, no extra state for maintaining the mode is used. A transition from pro.ling \nmode to tracing mode occurs when a hot loop or path is identi.ed at a trace-anchor. At trace anchors, \nthe code can be patched to jump to optimized trace code. If such an anchor is reached while in pro.ling \nmode, the thread transitions to on-trace mode. During tracing mode, execution transitions back to pro.ling \nmode when one of the following cases occurs: The initial trace anchor is reached again. This means, \nwe identi.ed a loop trace.  For traces starting from a loop anchor, we end the trace when it leaves \nthe loop body. For nested loops within a single method, we consider the outer loop part of the inner \nloop as well, in order to trace entire loop nests.  When a trace anchor is reached for which we already \nhave an optimized trace.  When the trace length exceeds a con.gurable threshold (currently 100k instructions). \n When the stack depth through inlining exceeds a con.g\u00adurable threshold (currently 20).  When an instruction \nthrows an exception.  When a non-loop trace ends, we determine via heuristics if it is pro.table to \noptimize the trace and patch it as a non-loop trace into the trace anchor. When running optimized trace \ncode, execution remains on-trace as long as all guards hold. (Guards arise from con\u00additional control \n.ow decisions that were recorded earlier during tracing.) Otherwise, a trace exit is reached, at which \npoint the execution transfers back to pro.ling. 4. Just-in-Time Compilers SPUR uses different JITs to \nproduce different code versions. The JITs are not based on the production Microsoft CLR JIT, but have \nbeen developed from scratch to enable ef.cient transitions between the different code version. The current \nimplementation of SPUR targets the 32-bit X86 architecture.  Figure 3. JIT version, code, and execution \nmodes. All JITs are derived by subclassing a basic visitor pattern over CIL instructions. In addition \nto the three main JITs depicted in Figure 3, there is one additional specialized JIT called the transfer-tail \nJIT. Its job is to bridge the execution from an arbitrary instruction within a block from tracing or \non-trace mode to the next safe point to transfer to the pro.ling code. We use an abstract JIT base class \nwithout a register al\u00adlocator from which the tracing, transfer-tail, and an abstract base class for JITs \nwith a register allocator are derived. From the latter, the pro.ling JIT and the optimizing JIT are de\u00adrived, \nboth of which perform register allocation. The register allocator is very simple. We distinguish ba\u00adsic \nblocks with multiple predecessors as head basic blocks from basic blocks with a single predecessor. At \nall head ba\u00adsic block boundaries, all registers are spilled. We don t allo\u00adcate registers across head \nbasic blocks in order to simplify the transition into and out of pro.ling mode. For each ba\u00adsic block, \nthe register allocator performs one backward scan to determine if generated values will be used. Then \nit per\u00adforms a forward register allocation, allocating up to three callee-save X86 registers (EBX, ESI, \nEDI). EBP serves as the frame pointer, ESP is the stack pointer, and the caller\u00adsave X86 registers (EAX, \nECX, EDX) are used as scratch registers in the compilation of the individual CIL instruc\u00adtions. The X86 \n.oating-point stack is used to allocate .oat\u00ading point values in registers. Write barriers for the garbage \ncollector are inlined within loops. 4.1 Code Transitions The virtual dispatch table of any object at \nruntime always points to the native version of a method (ahead-of-time X86, see Figure 1). This invariant \nis trivially maintained as SPUR does not support dynamic type creation at the moment. In the future, \nwhen types are created dynamically, their dispatch tables would point to pro.ling versions of the methods. \nDelegate objects represent closures and consist of a method pointer and an optional target object. Delegates \ncreated in native mode point to the native mode version, whereas delegates created in any JIT mode always \npoint to the pro.ling version. The initial execution mode is native. Transitions from na\u00adtive mode can \nonly occur via indirect calls through method pointers, and thus only to pro.ling mode. Otherwise execu\u00adtion \nremains in native mode.  Transitions from JIT modes to native mode occur only through indirect method \ncalls through delegates created in native mode, or when calling methods marked with [Native-Call] attributes. \nSuch annotations are useful to prevent trac\u00ading through runtime methods that were written in C, con\u00adtain \nunsafe pointer manipulations, or whose trace expansion would be non-bene.cial. Execution may transfer \nfrom tracing or optimized trace code back to pro.ling code at any instruction in a basic block. Instead \nof persisting and restoring register allocation decisions at every instruction, we only do so at head \nbasic block boundaries; we use transfer-tail code jitted without register allocation to execute the instructions \nup to the start of the next head basic block. Thus the actual transitions from tracing and on-trace mode \nin Figure 3 goes through a small portion of transfer-tail code.  4.2 Pro.ling JIT For each dynamically \ngenerated CIL method an X86 stub is put in place. The purpose of the stub is to determine if an X86 version \nof the method has been compiled and is ready to call. If not, the stub invokes the pro.ling JIT (when \nin pro.ling mode) to translate the CIL to X86. The pro.ling JIT is a very simple and direct JIT perform\u00ading \nno optimizations, but it does employ a register allocator. The task of the pro.ling JIT is to allow us \nto start running dynamic code immediately and to emit pro.ling counters to identify hot paths. For each \ntrace anchor, the pro.ling JIT inserts a counter variable. For loop-related trace anchors, these counters \nare local variables. Counters for individual trace exits and counters for trace anchors for tracing potentially \nrecursive method calls are global. A counter decrement operation and a conditional branch is inserted \nat each trace anchor. The counter starts at a certain threshold (default: 3 for lo\u00adcal counter, and 317 \nfor global counters); when it reaches zero, execution transitions to tracing mode, which involves branching \nto the corresponding basic block in the tracing code version of the current method. The tracing code \nver\u00adsion is emitted by the tracing JIT. Direct calls always invoke the pro.ling code of the target method, \nexcept if the target method is marked [NativeCall]. In the latter case, execution proceeds in native \nmode until the call returns, at which point it reverts back to pro.ling. At virtual call sites, the pro.ling \nJIT produces code that .nds the pro.ling version of the target method via table lookups. To amortize \nthe lookup cost, we employ inline caches in the code. Finally, at indirect call sites through method \npointers, the method pointer is directly invoked. According to our invariants, the method pointer points \nto native code (in which case we temporarily transition to native mode), or the pointer points to the \npro.ling code version of the method. Note that we jit pro.ling versions of methods in the runtime using \nthe CIL versions depicted in the upper right quadrant of Figure 1. As a result, pro.ling might determine \nthe start of a hot loop or path inside the runtime. 4.3 Tracing JIT In tracing mode, we need X86 code \nthat performs call-backs to our trace infrastructure in order to record the sequence of CIL instructions \nbeing executed on the current path. We call this code tracing code and it is emitted by the tracing JIT \n(Figure 3). The tracing JIT handles method calls similarly to the pro.ling JIT but with the goal of remaining \nin tracing mode. At direct call sites, the JIT simply emits a direct call to the tracing version of the \ntarget method, unless the method is marked [NativeCall]. At virtual call sites, we determine the target \nmethod via a table lookup and use inline caches to amortize the cost. Indirect calls operate on method \npointers. As a convention, method pointers created for jitted code always point to pro.ling code. Thus, \nthe tracing JIT needs to do more work than the pro.ling JIT to perform an indirect call: The tracing \nJIT emits code that uses such a method pointer as an index into a hashtable that maps pro.ling code to \ntracing code. If the table contains an entry, control is transferred to the corresponding tracing version \nof the method. Otherwise, the method pointer must be a native method and it is called directly, transitioning \ntemporarily out of tracing mode until the native method returns. Thus, by default, most calls are inlined \nand traces extend through runtime methods, except when the target method is marked as [NativeCall] or \ncannot be determined due to an indirect call with a native code target. The ability to selectively inline \nruntime methods into traces is the major advantage of our approach and enables on-demand specialization \nof complicated runtime methods to particular calling contexts.  4.4 Optimizing JIT for On-Trace Code \nVia call-backs, the tracing code can record a trace of CIL instructions. When a pro.table trace has been \nrecorded, it is optimized by our optimizing trace compiler, which produces a special CIL method for the \ntrace. The optimizing JIT for on-trace code compiles this trace method to machine code. A trace method \nis special in several ways: It inherits the arguments and local variables of the method where the trace \nstarted.  It does not contain a regular return instruction, and its control-.ow must not allow the termination \nof the method via an exception. Instead, execution may only leave this method via a special transfer \ninstruction.  Even though the code generated by the Optimizing JIT has these unusual entry and exit \nsemantics, the result is a seam\u00adless integration into the existing calling conventions and stack layouts, \ngarbage collector and exception handling ta\u00adbles.  Because our trace recorder inlines method calls aggres\u00adsively \nand a trace may exit in the middle of many inlined calls, a transfer instruction must recover all of \nthe stack frames of all active inlined methods of the current trace. This involves reconstructing the \nlocal variables and architecture dependent state, such as frame pointers, return addresses, registers, \netc. When transferring out of the trace method, transfer-tail code is employed. Once machine code for \nthe optimized trace code has been generated, a jump to the resulting X86 code is patched into the trace \nanchor code of the pro.ling code, overriding the original counting logic. The optimizing JIT uses the \nstandard register allocator. Note that a trace tree with a loop has exactly one head basic block which \nacts as the loop head. Thus, in this case, register allocation happens globally over the entire tree. \n 4.5 Transfer-Tail JIT Transfer-tail code enables transitions from tracing or op\u00adtimized trace code \nback to pro.ling code. At the transfer source, all registers are spilled onto the stack. Transfer-tail \ncode itself does not use any register allocation, and so exe\u00adcution can start at any instruction. At \nthe beginning of each head basic block, control transfers back to regular pro.ling code. Transfer-tail \ncode is jitted on demand.  4.6 Design Implications This section brie.y touches on a few consequences \nof our design. 4.6.1 Stack Reconstruction At trace exits, we have to reconstruct the stack layout of \ninlined active methods. This reconstruction is complicated by the fact that CIL supports managed pointers \ninto the runtime stack, e.g., for passing the address of a local to another method. Consider the following \nexample method: void SwapWithSmaller(ref int x, int[] data) { for (int i=0; i< data.Length; i++) { if \n(data[i] < x) { int temp = data[i]; data[i] = x; x = temp; return; } } } void Use(int[] data) { int a \n= 55; SwapWithSmaller(ref a, data); } SwapWithSmaller takes an address to an integer x and an array of \nintegers and swaps the .rst element in the array that is smaller than the current value of x. The runtime \nstack for the call from within Use looks as follows: top-of-stack SwapWithSmaller x &#38;a data p (fp,sp,...) \nUse a 55 data p (fp,sp,...) Here, p refers to the address of the array and (sp,fp,...) refers to calling \nconvention registers and saved registers. Note how x points to the stack location of a. If a trace is \ncompiled that inlines the call from Use and the trace needs to be exited in the middle of the execution, \nthen the exact stack con.guration above must be produced. The optimized trace code typically needs a \nfraction of the original stack space. E.g., it doesn t need duplicate locations for data and stack elements \nfor calling conventions can also be saved. If we naively optimize the stack frame of the trace we have \nthe problem that the address of local a in the opti\u00admized frame is different from the address in the \ncon.gura\u00adtion above, which implies that we have to adjust all locations containing pointers to a, such \nas x. Worse, the CIL execution model assumes and requires that stack locations do not move at runtime. \nThus, such an approach isn t viable. To address this issue, our trace code uses stack frames that are \nas large as the reconstructed stack frames it might need to produce on trace exit. This allows us to \nplace stack locals whose address is taken into the exact position they occupy in the reconstructed frames. \nThe resulting optimized frame simply contains gaps of memory that may remain unused until a trace exit \nforces the reconstruction of the stack.  4.6.2 Stacks of Trace Contexts Allowing native calls from within \nour jitted code results in interesting runtime con.guration that may not be immedi\u00adately apparent. Recall \nthat indirect calls via delegates from native contexts may enter pro.ling code if the delegate was constructed \nin jitted code. As native contexts can be invoked from any mode (pro.ling, tracing, on-trace), it is \npossible to build up a stack of distinct trace contexts at runtime, sepa\u00adrated by native stack frames \nas shown below: context 3 context 2 context 1 . . . pro.ling native tracing pro.ling native on-trace \npro.ling native In principle any combination of active modes is possible at runtime, in particular, \nthere might be multiple contexts ac\u00adtively recording a trace. In order to simplify our infrastruc\u00adture, \nwe currently allow only a single trace to be recorded at a time. Thus, if a nested context wants to start \ntracing, but an outer context is already recording a trace, we remain in pro.ling mode instead of entering \ntracing mode.  5. Trace Recording and Optimization The trace tree optimizer performs many standard optimiza\u00adtions, \nsuch as constant folding, common subexpression elim\u00adination, arithmetic simpli.cations, and dead code \nelimi\u00adnation. In addition, we perform some novel optimizations which we describe in more detail. 5.1 \nSimplifying Assumptions The SPUR infrastructure is not a full implementation of Microsoft s .NET platform. \nIn particular, we don t support the following core features: Runtime re.ection is limited to the generation \nof new methods. In particular, new types cannot yet be gener\u00adated dynamically and metadata cannot be \ninspected at runtime.  We do not yet support multiple user threads, which sim\u00adpli.es stub patching and \ncounter management.  We do not yet support tracing of code involving typed references, which are used \nto realize C-like  vararg calling conventions (ordinary managed point\u00ad ers realizing interior pointers \nare supported), multi-dimensional arrays (jagged arrays are supported), or unsafe code. The addition \nof these missing features does not require a change in the architecture and we don t anticipate any major \nissues. The addition of threading will impact certain opti\u00admizations as mentioned below.  5.2 Growing \nTrace Trees Figure 4 illustrates the structure of trace trees in SPUR. A trace tree is created when the \n.rst trace is recorded for a par\u00adticular trace anchor in the pro.ling code. Along the recorded trace, \nguards make sure that future executions will take the previously recorded path. If not, the trace is \nexited. After optimized trace tree code has been jitted, a jump to the op\u00adtimized code is patched into \nthe original trace anchor in the pro.ling code. Later, when the optimized trace tree code is executed, \na particular guard may fail frequently. In that case, a trace for the continuation after the corresponding \ntrace exit is recorded; it is attached to the previously recorded trace at the previously failing guard, \nwhich now selects between the old and the new trace continuation. All associated recorded traces are \ncomposed to form the trace tree. Individual traces may or may not loop back to the trace anchor. SPUR \ndoes not currently support nested trace trees. Up to .ve tracing attempts are made from each trace anchor; \nif each attempt is aborted, e.g. because execution left Figure 4. Trace Trees in SPUR. the relevant \nloop body, or an exception is thrown, or because a non-loop trace was not pro.table, then the trace anchor \nis retired, and the trace anchor code is patched with X86 no\u00adoperation (NOP) instructions. During trace \nrecording, some trivial optimizations, in par\u00adticular constant folding, are already applied to reduce \nthe amount of collected data. Every time a new trace is recorded and incorporated into a trace tree, \nthe entire trace tree is recompiled by the opti\u00admizing JIT.  5.3 Trace Intermediate Representation We \nrepresent recorded traces in an SSA form. Whenever possible, indirect memory accesses, which can occur \nin CIL via managed pointers, are resolved. We represent updates of value types and selection of .elds \nfrom values types in SSA form as well. Most instructions which can throw an exception implicitly a null-dereference, \na bounds check at an array access, or an arithmetic exceptions are split into guard instructions to ascertain \nthat the operation won t fail, and the actual operation without the check. All guard instructions are \nannotated with the live stack state that needs to be reconstructed on exiting the trace. However, some \ninstructions for which the check for the exceptional case would be too complicated or redundant, e.g. \na multiplication with over.ow check, or a call to a native method, are instead annotated with live stack \nstate to recon\u00adstruct inlined stack frames, so that they can be reconstructed when an exception is actually \nthrown. Besides spilling of registers around the protected region with exception han\u00addling, there is \nno overhead for exception handling.  5.4 Guards A recorded trace typically contains many guards, i.e., \nin\u00adstructions corresponding to conditional branches and other conditions which would force a deviation \nof the execution from the recorded trace (e.g., via an exception). A typical place where a guard is needed \nis at a virtual call instruction. The guard makes sure the target method is the same as the one recorded \nduring tracing. We perform aggressive guard optimizations, which are crucial for gaining performance. \nThe main optimization per\u00adformed is evidently guard implication, i.e., guards that have been checked \nearlier and are thus redundant don t need to be checked again. In simple cases, this corresponds to common \nsubexpression elimination combined with constant propaga\u00adtion and dead code elimination. SPUR can also \ndetect non\u00adtrivial implications by analyzing the relationship of integer equalities and inequalities. \nTo further reduce the number of guards and possibly hoist them out of loops, we perform novel guard strengthening \noptimizations. These optimizations are speculative (but safe) in that they might cause a trace to be \nexited earlier than absolutely necessary. Guard strengthening identi.es guards implied by later guards \nand then strengthens earlier guards to the stronger guard occurring later in the trace. This change makes \nthe later guard redundant but might cause execution to leave the optimized trace at the earlier guard \nrather than the later guard. Leaving a trace earlier isn t really a problem, as the trace would be left \nanyway in the same loop iteration. There is a small performance loss in the .nal iteration where the \ntrace is left, but no semantic problem. If a particular trace exit is triggered frequently, SPUR will \nrecord a trace starting from that trace exit. If the guard that caused the trace exit was involved in \nspeculative guard strengthening, then it might be the case that the recorded trace initially takes an \nalready known path after the failing guard through the trace tree, and only later deviates at the later \nguard that was made redundant by the speculative guard strengthening. SPUR detects this scenario, and \nthen only attaches the new part of the trace at the later guard. Once a new trace has been attached, \nspeculative guard strengthening will no longer be performed at or above the branching point. 5.5 Hints \nand Guarantees SPUR supports annotations in the traced code to guide trac\u00ading and enable further optimizations: \nSome JavaScript run\u00adtime methods are annotated with a [TraceUnfold] attribute, that indicates that contained \nloops should be unfolded in outer traces, and no local trace trees should be constructed. This annotation \nis used mainly on those runtime methods that implement traversals of JavaScript prototype chains in a \nloop, as these prototype chains tend to be stable at particular callsites. SPUR also supports annotations \nto propagate safety guarantees of the JavaScript runtime. In particular, we use [DoNotCheckArrayAccesses] \nattributes to eliminate bounds checks on certain JavaScript runtime functions written in C# which access \narray elements holding object properties which are guaranteed to be present according to the JavaScript \ntype of the object. 5.6 Store-Load Propagation We use an alias analysis to perform a store-load propaga\u00adtion \nalong the trace to avoid (or delay) writes and reads to the heap and the runtime stack. Recall that CIL \nallows man\u00adaged pointers into the stack and call-by-reference parame\u00adters. Store-load propagation enables \nshort-circuiting such in\u00addirect stores and loads when inlining methods with by-ref parameters. When applied \nto heap locations, this optimization relies on the single-threaded assumption of our current infrastruc\u00adture. \nIn the presence of threads, the alias analysis will need to restrict this optimizations according to \nthe CIL memory model, and it can only be applied fully to thread local data (typically allocated on the \ntrace). Note that the optimization applied to stack locations remains fully applicable even in the presence \nof multiple threads. Store-load optimizations are often able to eliminate tem\u00adporary object allocations \nin a trace. Object .elds may be kept in registers, and the allocation of value types may be delayed until \na trace exits.  5.7 Invariant code motion We .rst determine which local variables are guaranteed to \nnot change during execution along any looping path of a trace tree. We mark those memory locations as \ninvariant. All pure non-exception-throwing computations which depend only on invariant values are marked \ninvariant as well. All invariant computations are hoisted. Invariant code motion is extended to guards, \nload oper\u00adations from the heap, and memory allocations as well: If a guard only depends on invariant \nvalues, and if it appears somewhere along all looping paths of the trace tree, then the guard is hoisted. \nThis might disable non-looping paths as a result (similarly to guard strengthening), but optimizing the \nlooping paths is more important. Similarly, if a load operation from the heap only de\u00adpends on invariant \nreferences and indices, and it appears somewhere along all looping paths of the trace tree, and any required \nimplicit null and/or type check has been al\u00adready hoisted, and no looping path can possibly overwrite \nthe memory to be loaded, then the load operation is hoisted. Memory allocations which do not escape are \nalso hoisted; in the case of an array, the array size must be invariant. In that case, at the end of \neach path that may write to a .eld or element of a hoisted memory allocation, an instruction is inserted \nto .ll all .elds or elements of the new object with a zero bit pattern to restore the initial state of \nthe object as if it were re-allocated.  5.8 Loop Unrolling For looping trace trees with limited branches, \nwe aggres\u00adsively unroll the loop. Our heuristic tries to keep the size of the unrolled trace tree reasonable, \nas it potentially explodes with the number of branches in the trace tree. We only unroll the most frequently \ntaken branches.  5.9 Delayed Computations in Transfers Besides optimizing the trace, the optimizer is \nresponsible for emitting guard checks at each potential trace exit, and transfer instructions to exit \nthe trace. When a guard fails, some values used in the transfer instruction to reconstruct nested stack \nframes might still have to be computed, as they were not used in the main execution of the optimized \ntrace code. All pure and non-exceptional computations might be delayed until a transfer requires them. \n6. JavaScript Compiler On startup, SPUR starts executing the JavaScript compiler, which in turn reads \nthe JavaScript source, compiles it to CIL in memory. Then SPUR jits the CIL to machine code, which .nally \ngets invoked. Our JavaScript compiler evolved from the JScript.NET code base [1], a JavaScript compiler \nfor the standard .NET platform. Before targeting SPUR, we put a fair amount of work into the compiler \nand the JavaScript runtime to in\u00adcrease performance when running on Microsoft s standard .NET platform. \n6.1 Argument and Variables of Primitive Types Arguments and local variables of functions are specialized \nto be of type bool, int, or double, if the static type inference can prove that the values of these arguments \nor variables will always be in the range of the specialized type. 6.2 Function Calls When a function \nis called that was de.ned in JavaScript code, the function is type-specialized according to the statically \nknown argument types at the call site. A cache maintains jitted variations of a function, one per type\u00adspecialization. \nWhen the function being called is implemented as a na\u00adtive runtime method in C#, rather than as a JavaScript \nfunc\u00adtion, then the native function cannot be specialized and jitted to suit the call site. Instead, \nan adaptor stub is generated to convert the call site arguments to the type signature expected by the \nnative method. 6.3 Object Representation JavaScript objects are represented as CIL objects, deriving \nfrom a common base class. In this CIL base class, one .eld holds a reference to a type object, and another \n.eld holds a reference to an array of property values. (Fields of an object in JavaScript are called \nproperties.) The type object maps property and function names to indices into the property array. When \na JavaScript object gets a new property, or loses a property, its type object is replaced with another \ntype object and its property value array is resized (if need be). When a JavaScript object is used as \na hash table from strings to values, a naive version of this scheme would gen\u00aderate a new type object \nevery time a new key is added. To avoid this proliferation of types, any JavaScript object whose (non-array \nindex) properties are created via the [] syntax gets a mutable (unshared) copy of its type object and \nthus all fur\u00adther additions of properties to the object are done in place in the mutable type object. \nIn effect, the type object itself becomes a non-shared hash table. JavaScript arrays are derived fom \nJavaScript objects that in our implementation additionally have a native .eld that tracks the types of \narray elements (bool, int, double or any). Depending on the element type of the array one of four ad\u00additional \n.elds is initialized with a native array with the ap\u00adpropriate element type. JavaScript arrays can be \neither dense or sparse. When sparse, the .rst n entries are represented densely in the native array and \nthe rest are stored in a special\u00adized hash table. Note that whenever the name of a property is an array \nindex, the property is not added to the type ob\u00adject. It either goes to the native array or the specialized \n(per instance) hash table. If the object is not a JavaScript array, it only maintains the specialized \nhash table. 6.4 Lookup Implementation Every type object is a mapping from property name to prop\u00aderty \noffset. Every code location where an object property is accessed has a dedicated cache object1. When \ncontrol reaches the access, the current type of the object is extracted and compared to the type stored \nin the cache. If there is a hit, the cache contains the corresponding property offset. If there is a \nmiss, the type object is asked to map the property name to an offset and the cache is updated with the \ntype object and offset. In this context property offset means index of the value in the array of property \nvalues.  6.5 Eval Since eval needs to be able to inspect and modify the stack frame of a function, a \nfunction containing eval cannot store its locals in true locals, but uses a heap allocated activation \nobject to represent locals as properties. These properties are modeled and accessed exactly like the \nproperties of normal JavaScript objects. The code generated by eval is turned into a function whose (heap \nallocated) scope chain is the same as the calling function. Global code is also represented as a function \nwith a heap allocated activation object. Thus, global variables live in properties of an implicit global \nJavaScript object, whose type evolves just like the types of all other JavaScript objects. 1 In the current \nSPUR implementation, these caches are two-way, out-of\u00adline.  Note that other constructs, such as nested \nfunctions and with statements also cause a function to allocate its locals on the heap.  6.6 Static \nAnalysis We use abstract interpretation on the JavaScript code to perform sound type-specialization by \ninferring ranges for atomic local variables and function arguments [23]. Expres\u00adsions that involve constants \nor local variables and function arguments that are type-specialized are compiled into typed CIL, rather \nthan into calls to polymorphic helper functions. This results in shorter traces and fewer guards. 6.7 \nBoxed Values JavaScript object properties, elements of polymorphic Java-Script arrays, and local variables \nand arguments whose type could not be restricted by the static analysis, hold boxed values. However, \nin the context of SPUR s runtime, boxed objects are not allocated on the heap. Instead, they are stored \nin a 16-byte value type (in a 32-bit environment), which is usually passed around by-reference (and sometimes \nby\u00advalue). This value type effectively implements a union via an explicit struct layout in C#: [StructLayout(LayoutKind.Explicit)] \nstruct ValueWrapper { [FieldOffset(0)] object wrappedObject; [FieldOffset(4)] TypeCode typeCode; [FieldOffset(8)] \nbool wrappedBool; [FieldOffset(8)] char wrappedChar; [FieldOffset(8)] double wrappedDouble; [FieldOffset(8)] \nint wrappedInt; [FieldOffset(8)] uint wrappedUInt; [FieldOffset(8)] long wrappedLong; [FieldOffset(8)] \nulong wrappedULong; } enum TypeCode { Empty, Int32, Double, String, Object, DBNull, Boolean, Char, UInt16, \nUInt32, Missing, }  6.8 Tweaking the JavaScript Compiler for Tracing We found a few idioms in the generated \ncode of the JavaScript compiler that needed to be changed in order to get the best performance out of \nthe TJIT. Our caches for property lookups are realized as objects which live on the heap. However, only \npersistent data should be stored in such a cache object, and all temporary data involved in cache lookups \nshould be passed via the stack.  We use a high-water mark for non-empty slots in arrays; this often \nallows hoisting checks for empty elements out of loops.  void arraySwap(ref ValueWrapper a) { for (double \ni = 0.0; Less(i, Minus(GetProperty(c, a, \"length\"), 1.0)); i += 1.0) { ValueWrapper tmp = GetElement(a, \ni); SetElement(a, i, GetElement(a, i + 1.0)); SetElement(a, i + 1.0, tmp); } } Figure 5. C# corresponding \nroughly to JavaScript example General guidelines to improve performance of object ori\u00adented programs \napply here as well, in particular avoiding casts to non-sealed classes. 7. Example 2: A Loop in JavaScript \nIn this section, we will revisit the C# example from Sec\u00adtion 2, which we now rewrite in JavaScript as \nthe source lan\u00adguage: function arraySwap(a) { for(vari =0;i<a.length-1;i++){ var tmp = a[i]; a[i] = a[i \n+ 1]; a[i +1] =tmp; } } Static analysis determines that i is a number, but a could be any object (it \ndoesn t have to be an array), so the type of a.length is unknown statically, so the arithmetic expression \na.length-1 as well as the comparison against i has to be computed with boxed values. The types of the \narray elements are also not known. The CIL code generated by our JavaScript compiler is roughly the code \ncorresponding to the C# in Figure 5. Note that JavaScript values of unknown types are stored in the value \ntype called ValueWrapper shown in Section 6.7. We don t write out the implicit boxing of double values \nto Val\u00adueWrapper nor do we show that value wrappers are actually passed by reference. The JavaScript \nruntime helper methods Less and Minus implement the corresponding JavaScript se\u00admantics for < and -; \nGetProperty implements the retrieval of an object property; a cache object c stores information about \nthe index of the property given a particular JavaScript object type; GetElement and SetElement implement \narray accesses. The unoptimized trace recorded for this loop, inlining all helper methods, has 210 instructions, \nincluding guards, and is not shown. After optimization, 37 loop invariant instructions and 13 loop variant \ninstructions remain, some of which can be de\u00adlayed until an exit is triggered. Figure 6 shows the optimized \ntrace intermediate representation, omitting delayed instruc\u00adtions. In the .rst part, the hoisted loop \ninvariant computa\u00ad  ---------hoisted loop invariant code loop: guard a Cne null // guard i Clt (34) \n(2) = a.typeCode fld qword ptr [ebp-1Ch] guard (2) Ceq TypeCode.Object fld qword ptr [ebp-17Ch]  (6) \n= a.wrappedObject fucomip st,st(1) (7) = (6) as MS.JScript.ObjectInstance jp exit1 (23) = (6) as MS.JScript.ArrayInstance \njbe exit1 guard (6) Ceq (23) fstp st(0) guard (6) Cne null // (60) = Conv_I4 i  (12) = (6).type cvttsd2si \nebx,mmword ptr [ebp-1Ch] (13) = c.type1 // guard (60) Clt_Un (69) guard (12) Ceq (13) mov esi,dword \nptr [ebp-16Ch]  (15) = c.dynamicPropertyOffset1 cmp ebx,esi guard (15) Clt 0 jae exit2  (17) = c.property1 \n// (72) = (71) [(60)] guard (17) Cne null mov edi,dword ptr [ebp-164h]  (21) = call (17).System.Object::GetType() \nfld qword ptr [edi+ebx*8+0Ch] guard (21) Ceq MS.JScript.ArrayLengthProperty // (121) = i Add 1.0D  (27) \n= (6).len fld qword ptr [ebp-1Ch] (28) = Conv_R_Un (27) fadd qword ptr ds:[3D80008h] (34) = (28) Sub \n1.0D fstp qword ptr [ebp-194h] guard (7) Cne null // (127) = Conv_I4 (121)  (58) = call (7).System.Object::GetType() \nmov dword ptr [ebp-180h],ebx guard (58) Ceq MS.JScript.ArrayInstance cvttsd2si ebx,mmword ptr [ebp-194h] \n (62) = (7).denseArrayLength // guard (127) Clt_Un (69) (65) = (7).elementTypeCode cmp ebx,esi guard \n(65) Ceq TypeCode.Double jae exit3  (68) = (7).indexOfFirstMissingEntry // (139) = (71) [(127)] (71) \n= (7).doubleArray fld qword ptr [edi+ebx*8+0Ch]  (143) = (6) as System.String // (71) [(60)] = (139) \nguard (143) Ceq null mov ecx,dword ptr [ebp-180h] (69) = (68) Min_Un (62) fstp qword ptr [edi+ecx*8+0Ch] \n--------- loop body // (71) [(127)] = (72) guard i Clt (34) // exit1 fst qword ptr [edi+ebx*8+0Ch] (60) \n= Conv_I4 i // loop tail state: i = (121), tmp = (83) guard (60) Clt_Un (69) // exit2 fld qword ptr [ebp-194h] \n (72) = (71) [(60)] fstp qword ptr [ebp-1Ch]  (121) = i Add 1.0D // (81)=update default(.).wrappedDouble=(72) \n(127) = Conv_I4 (121) // (82)=update (81).typeKind = Double guard (127) Clt_Un (69) // exit3 // (83)=update \n(82).wrappedObject = null (139) = (71) [(127)] lea ebx,[ebp-2Ch] (71) [(60)] = (139) fst qword ptr [ebx+8] \n (71) [(127)] = (72) mov dword ptr [ebx+4],2  (81) = update default(ValueWrapper).wrappedDouble=(72) \nxor eax,eax (82) = update (81).typeKind = TypeCode.Double mov dword ptr [ebx],eax (83) = update (82).wrappedObject \n= null fstp st(0) loop tail state: i = (121), tmp = (83) // book-keeping how often loop gets executed \n inc dword ptr [ebp-12Ch] Figure 6. The trace intermediate representation. jmp loop Figure 7. The optimized \ntrace loop body X86 code. tions are shown. Numbers in parentheses, e.g. (2), repre\u00adsent SSA values that \nare de.ned along the hoisted or loop body code. The instructions shown include .eld accesses casts that \nyield null on failure ((x) as T), calls to native ((x) = (y).f), guards (guard (x) op (y)) where the \noper-methods (call (x).f()), conversion from unsigned integers to ator op can be equality (Ceq), disequality \n(Cne), inequality .oating point (Conv R Un), conversion from .oating point (Clt for signed integers/ordered \n.oating point values, Clt Un to signed integer (Conv I4), arithmetic operations ((x) op for unsigned \nintegers/unordered .oating point values), type (y)) where the operator op can be subtraction (Sub), addition \n V8 TM SPUR with traces SPUR CLR SPUR w/o traces IE8 IE9 (Pre\u00adview) 3d-cube 13 25 14 42 63 112 37 3d-morph \n19 36 10 26 47 103 42 3d-raytrace 15 43 16 50 72 165 24 acc-bin-tree 1 28 22 37 50 128 19 acc-fannkuch \n10 47 15 84 174 280 13 acc-nbody 11 13 6 36 56 148 28 acc-nsieve 2 8 4 13 33 90 8 bitops-3bit 2 1 1 5 \n7 77 1 bitops-bits 6 7 6 6 8 79 5 bitops-and 6 2 1 15 12 186 3 bitops-nsiev 12 17 4 32 57 135 15 control-rec \n2 31 9 17 22 106 2 crypto-aes 75 15 14 40 75 113 12 crypto-md5 7 2 2 16 20 70 10 crypto-sha1 7 3 2 15 \n21 70 11 date-tofte 20 56 49 67 92 165 42 date-xparb 26 69 66 65 67 161 41 math-cordic 13 21 6 48 66 \n143 2 math-partial 15 11 13 71 105 100 31 math-spectr 5 3 2 16 22 99 16 regexp-dna 12 37 616 528 585 \n204 31 string-b64 14 7 10 32 38 560 19 string-fasta 21 42 64 75 112 163 39 string-tagcl 22 45 123 109 \n160 129 43 string-unpac 43 58 382 294 397 124 68 string-valid 22 19 50 114 82 113 31 Figure 8. Steady-state \nexecution times for SunSpider benchmarks in milliseconds (Add), minimum of unsigned integers (Min Un), \nstruct up\u00addates (update (x).f = (y)), and default values (default(T)), i.e. the struct value where are \n.elds are .lled with a zero bit pattern. The Min Un instruction was not present in the origi\u00adnally recorded \ntrace, but resulted from a speculative guard strengthening optimization, in which two separate guards \nwere combined. The accesses to type1, dynamicProperty-O.set1, and property1 are related to the access \nof a.length, which refers to the access of the array length property in this calling context. Figure \n7 shows the resulting machine code for the loop body. We show the loop body here without loop unrolling, \nbut SPUR will actually unroll the loop body at least once. The generated machine code is not optimal, \nand there is room for further improvement: The value (60) is spilled and later reloaded, and the temporary \ntmp of type ValueWrapper is fully constructed and written back at the end of the loop; this is because \nSPUR currently does not perform a liveness analysis of local variables typed as ValueWrapper. 8. Evaluation \nFigure 8 shows steady-state execution times for the SunSpi\u00adder benchmarks in milliseconds, on a Intel \nCore2 Quad CPU, Figure 9. Normalized execution time over V8 4.0.249.89.  Q9650 @ 3.00 GHz, 8 GB Ram, \nWindows 7 Enterprise, 64\u00adbit version, running 32-bit versions of all browsers. Each benchmark was embedded \nin a loop iteration 30 times; the lowest reported execution time is quoted in the table. As a result, \njitting or tracing overhead is effectively not included in the quoted times. Note that we used abbreviated \nbench\u00admark names to reduce the size of the table. V8 refers to V8 in Chrome 4.0.249.89 (38071), TM refers \nto TraceMonkey in Firefox 3.6, IE8 refers to Internet Explorer 8, which runs JavaScript with an interpreter \nwithout jitting. IE9 refers to an early version of Internet Explorer 9 (Platform Preview v1.9.7745.6019), \nwhich interprets JavaScript bytecodes with selective jitting. SPUR with tracing is the SPUR engine as \ndescribed in this paper, SPUR without tracing is using SPUR s JIT, but without tracing, and SPUR-CLR \nis using SPUR s JavaScript compiler, but using the production Mi\u00adcrosoft CLR v3.5 JIT (and runtime) instead \nof SPUR s JIT, again without tracing. Comparing SPUR without tracing to SPUR-CLR shows that the code \ngenerated by SPUR s JIT is on average 1.4 times slower than the CLR JIT. The same JIT is used when running \nSPUR with tracing, and yet with exception of certain string-heavy benchmarks it consistently per\u00adforms \nfaster than SPUR-CLR, faster than TraceMonkey, and quite often faster than V8. Excluding the string-related \nbenchmarks regexp-dna, string-b64, string-fasta, string\u00adtagcl, string-unpack, string-valid, then the \ncode generated by SPUR with tracing only takes 59% percent of the time to execute compared to the code \ngenerated by TraceMonkey. When further excluding the recursion-heavy benchmarks acc-bin-tree, control-rec, \nthen SPUR s code completes exe\u00adcution in 87% of the time it takes V8.  Trees Loops Traces Traces /Tree \nLoop Traces /Loop Instrs. /Trace 3d-cube 96 8 156 1.6 5.1 863 3d-morph 9 3 10 1.1 1.3 380 3d-raytrace \n62 8 102 1.6 4.0 1360 acc-bin-tree 38 0 47 1.2 - 650 acc-fannkuch 20 7 47 2.4 2.1 161 acc-nbody 15 4 \n21 1.4 1.8 2542 acc-nsieve 6 3 12 2.0 1.0 128 bitops-3bit 3 1 3 1.0 1.0 218 bitops-bits 6 1 15 2.5 8.0 \n55 bitops-and 3 1 3 1.0 1.0 376 bitops-nsiev 7 3 17 2.4 2.3 112 control-rec 45 0 56 1.2 - 260 crypto-aes \n81 26 143 1.8 2.7 455 crypto-md5 16 4 17 1.1 1.3 4254 crypto-sha1 17 4 26 1.5 3.0 898 date-tofte 38 2 \n43 1.1 1.0 1345 date-xparb 41 3 63 1.5 3.0 1754 math-cordic 5 1 9 1.8 5.0 654 math-partial 8 1 9 1.1 \n2.0 1980 math-spectr 21 4 25 1.2 2.0 317 regexp-dna 3 2 3 1.0 1.0 2010 string-b64 7 3 8 1.1 1.3 2566 \nstring-fasta 14 3 18 1.3 2.3 668 string-tagcl 19 2 30 1.6 3.0 460 string-unpac 42 6 78 1.9 6.3 1221 string-valid \n16 3 34 2.1 3.0 416 Figure 10. Tracing statistics of SPUR on SunSpider bench\u00admarks  In its current implementation, \nSPUR does not have an optimized implementation of the string functions, and no optimized regular expression \nlibrary. This prevents SPUR from performing better on the string-related benchmarks. Figure 9 compares \nthe execution time of TraceMonkey, SPUR with tracing, SPUR-CLR, and SPUR without trac\u00ading code against \nV8. Execution times are normalized against V8 s time. Thus, bars extending to the right of 1, are factors \nslower than V8. Bars extending to the left of 1 are fractions of execution time of V8 code, thus faster. \nIn addition to the individual benchmarks, the geometric mean over all bench\u00admarks is shown. The graph \nillustrates several points. First, code produced by SPUR without tracing runs only slightly slower than \nSPUR-CLR. This validates the code generated by our unop\u00adtimized JIT as relatively competitive to a commercial \nimple\u00admentation. In light of that, the graph shows that tracing does dramatically improve the performance \nof code generated by SPUR over SPUR without tracing, often by over a factor of 10. Thus our speedups \nreally do come from tracing, and not just a better back-end or other effect. Observe that where TraceMonkey \nbeats V8, SPUR does so too and with similar magnitude. Vice-versa, where Trace-Monkey is slower than \nV8, SPUR usually is as well. The graph thus substantiates our hypothesis that trace optimiza\u00adtions that \nwork well for a dynamic source language tracer like TraceMonkey, also work well when tracing the code \nob\u00adtained by translating the dynamic language and its runtime to CIL. Overall, V8 is still a formidable \nengine to beat. In 14/26 benchmarks V8 wins, often dramatically, whereas SPUR wins in 11/26 benchmarks. \nThe current overhead of SPUR s trace optimization and compilation is signi.cant. The numbers reported \nin Figure 8 effectively do not include jit or tracing overhead for any JavaScript engine. We have not \nyet attempted to reduce the overhead. For example, in the crypto-md5 benchmark, the main loop results \nin a trace tree with only a single trace, consisting of 60891 instructions; total processing of this \ntrace, including trace recording, optimization, and compila\u00adtion takes around 1.5 seconds, and the resulting \njitted code size is increased by a factor 13.5. Figure 10 shows some statistics about the trace trees \nSPUR builds for the SunSpider benchmarks. Trees shows the number of trace trees created from distinct \ntrace anchors, Loops represents the number of trace trees which have looping paths, Traces represents \nthe number of recorded traces, or in other words, the number of paths through all trace trees. Traces/Tree \nshows the average number of traces per tree, Loop Traces/Loop shows the average number of looping paths \nper tree with any looping path. Instrs/Trace shows the average number of instructions in recorded traces, \nwhere we count the number of instructions in our trace intermediate representation, before any opti\u00admization \nsuch as constant folding is applied. Note that acc\u00adbin-tree and control-rec do not contain any loops. \nCom\u00adpared to the statistics reported for TraceMonkey (Figure 13, Detailed trace recording statistics \nin [18]), SPUR tends to record more trees and loops. There are two main reason for this tendency: SPUR \ns JavaScript compiler specializes methods according to types for arguments that can be in\u00adferred statically \nat call sites. This causes the generation of different type-specialized method bodies, which in turn \nget separate trace trees. Also, SPUR does not only record traces for proper loops, but SPUR also starts \ntracing from trace anchors placed after all potential exit points from existing traces, and at entry \npoints of potentially recursive methods. However, SPUR records fewer loops than TraceMonkey for some \nbenchmarks. The reason for this phenomenon is likely to be found in the different approaches for nested \nloops, which SPUR does not support directly. The number of loop\u00ading traces per loop also varies widely \nbetween SPUR and TraceMonkey, which is probably due to differences in the tracing systems runtimes, whose \ninternal branching over different object types are responsible for a signi.cant num\u00adber of traces.  \n9. Related Work Several state-of-the-art JavaScript implementations employ just-in-time compilation, \ne.g. Apple s SquirrelFish Extreme and Google s V8. Compiling multiple specialized code versions was pio\u00adneered \nby [10] in the context of the SELF language. Dy\u00adnamic trace compilation was attempted systematically \nby Dynamo [4], which operates at the machine code level. It interprets machine code, records traces, \noptimizes them, and maintains a trace cache of optimized traces. Other binary optimization systems include \n[9, 15]. YETI [30] is a simple TJIT for Java, focusing on an ef.\u00adcient integration with an interpreter. \nHotPathVM [20] is an\u00adother TJIT for Java. It attaches secondary (child) traces to primary traces, in \norder to form trace trees [19]. Traces are represented in SSA form. Child traces may connect different \nloops, effectively supporting loop nests. The goal of the Hot-PathVM was to provide a small footprint \nJVM with better performance than an interpreter. Compared to a state of the art JVM, HotPathVM still \ntrails far behind in performance. Tamarin-Tracing [12], and TraceMonkey [18] are two JavaScript TJITs \nbased on the ideas in the HotPathVM. They use an interpreter to gather traces of high-level JavaScript \noperations and optimize these traces directly before com\u00adpiling them to machine code. To save time recompiling \ntrace trees, these TJITs use trace stitching, where opti\u00admized branch traces are stitched onto the main \ntrunk (or side branch) without recompiling the entire tree. This approach saves compilation time, but \ndoes not permit aggressive opti\u00admizations like global register allocation (described in [17]). Suganuma \net. al. [28] propose an inter-procedural region selection scheme similar to our tracing approach. They \nuse pro.ling and instrumentation to drive region selection and rely on on-stack replacement (OSR) to \ntransfer out of com\u00adpiled regions. Unlike traces, regions are arbitrary subsets of methods. They often \ncontain join nodes, and thus require correspondingly more complex pro.ling and optimization algorithms. \nClosely related to our translation of JavaScript into CIL is earlier work by Chang et.al. [11], where \nthey embed JavaScript in Java, combined with a TJIT. Their system still contains a Java interpreter, \nwhereas SPUR is based on jitting only. Our JavaScript translation can take advantage of value types in \nthe native runtime. More general trace-optimizations of hierarchically lay\u00adered virtual machines (VMs) \nhave been described recently [7, 27, 29]. However, while our approach is based on compila\u00adtion at all \nlevels (JavaScript to CIL, and CIL to machine code), these approaches are based on interpreters at all \nlev\u00adels. As the interpreter loop of the guest VM would appear to be the hot loop and optimized, instead \nof the loops of the interpreted programs, they propose special hints to guide the trace optimization \nof the host VM, essentially to unroll the guest interpreter loop. While we do not have a general prob\u00adlem \nwith an interpreter loop, we do employ similar hints to unfold loops in the JavaScript runtime that perform \ndynamic type lookups at runtime. The Dynamic Language Runtime (DLR) [26] is a library on top of .NET \nwhich simpli.es the implementation of dy\u00adnamic languages. Our JavaScript runtime can be seen as a specialized \nimplementation of the DLR. The DLR does not provide any support for tracing. Cuni et.al. [13] describe \nhow to use the built-in dynamic code generation features of the Microsoft CLR, namely the Re.ection.Emit \nlibrary, to realize a TJIT. As a consequence, their approach incurs a relatively high overhead for enter\u00ading \nand leaving traces, as their optimized traces are regular methods. SPUR has the concept of trace methods, \nwhich in\u00adherit the stack frame of the originating method, and SPUR supports a transfer instruction that \nallows reconstructing in\u00adlined method calls. Merrill et.al [25] present a system which compiles each \nmethod into two equivalent binary representations: a low .\u00addelity region with counters to pro.le hot \nloops and a high\u00ad.delity region that has instrumentation to sample every code block reached by a trace. \nWhen a hot loop has been iden\u00adti.ed, the low-.delity code transfers control to the high\u00ad.delity region \nfor trace formation. Upon conclusion of a trace, execution jumps back to the appropriate low-.delity \nregion. We have earlier reported on the static analysis based on abstract interpretation which SPUR performs \nto infer types of local variables and method arguments [23]. The SPUR system, written mainly in C#, having \na static compiler (Bartok) and a JIT at runtime, is similar in na\u00adture to the Maxine system for Java \n[24], which was re\u00adcently extended for tracing [6]. In contrast to the tracing Maxine system, SPUR s \noptimized traces seamlessly inte\u00adgrate with other precompiled or jitted code, without the need for adapter \nstack frames to mediate between different code versions, and SPUR supports any .NET language, including \nJavaScript. 10. Conclusion and Future Work We presented a tracing just-in-time compiler (TJIT) for CIL, \nwhich signi.cantly improves the runtime performance of JavaScript programs on the .NET platform. Our \napproach is novel in that it does not employ an interpreter at any lan\u00adguage level. We con.rmed our hypothesis \nthat the perfor\u00admance bene.ts of tracing for dynamic languages is not lost when the tracing is performed \non an intermediate level of the compiled dynamic language source, rather than at the source level. The \nkey to make our approach work in practice is the ability to trace not only the target code produced by \nthe dy\u00adnamic language compiler, but also the runtime code of the dynamic language.  In future work, \nwe want to reduce the overhead of trace optimization and compilation by revisiting our algorithms to \navoid any non-linear computations (currently, our alias analysis, invariant code motion, and guard strengthening \nare potentially quadratic in the number of instructions). We also want to defer that workload into a \nparallel thread, so that the main program execution can continue undisturbed, similar to [5, 21]. As \nan alternative, the code generation process could be performed of.ine, leveraging previously recorded \ntraces. We also want to garbage collect the jitted code when it is no longer needed. SPUR can currently \ndetect implied guards by checking built-in patterns over equalities, disequalities, and inequali\u00adties. \nWe plan to use symbolic execution [22] along the traces combined with an SMT solver such as Z3 [14] to \ndetect all implied guards, and to perform further optimizations. We also plan to investigate if precise \nsymbolic knowledge about a loop body at runtime can be leveraged for automatic paral\u00adlelization [8]. \nSome virtual machines, including Google s V8, use tagged pointers to compactly represent 31-bit integers \nwith\u00adout boxing. We want to investigate if such a value represen\u00adtation can further improve performance. \nWe plan to improve our static analysis on JavaScript code, to track liveness of local variables, and \nto perform an inter\u00adprocedural analysis when possible. References [1] Andrew Clinick. Introducing JScript \n.NET, 2000. http://msdn.microsoft.com/ms974588.aspx. [2] M. Arnold, S. Fink, D. Grove, M. Hind, and P. \nF. Sweeney. Adaptive optimization in the Jalape In OOPSLA no JVM. 00: Proceedings of the 15th ACM SIGPLAN \nconference on Object-oriented programming, systems, languages, and appli\u00adcations, pages 47 65, New York, \nNY, USA, 2000. ACM. [3] M. Arnold, M. Hind, and B. G. Ryder. Online feedback\u00addirected optimization of \nJava. In OOPSLA 02: Proceedings of the 17th ACM SIGPLAN conference on Object-oriented programming, systems, \nlanguages, and applications, pages 111 129, New York, NY, USA, 2002. ACM. [4] V. Bala, E. Duesterwald, \nand S. Banerjia. Dynamo: a trans\u00adparent dynamic optimization system. In PLDI 00: Proceed\u00adings of the \nACM SIGPLAN 2000 conference on Programming language design and implementation, pages 1 12, New York, \nNY, USA, 2000. ACM. [5] M. Bebenita, M. Chang, A. Gal, and M. Franz. Stream\u00adbased dynamic compilation \nfor object-oriented languages. In TOOLS (47), pages 77 95, 2009. [6] M. Bebenita, M. Chang, K. Manivannan, \nG. Wagner, M. Cin\u00adtra, B. Mathiske, A. Gal, C. Wimmer, and M. Franz. Trace based compilation in interpreter-less \nexecution environments. Technical Report ICS-TR-10-01, University of California, Irvine, March 2010. \n[7] C. F. Bolz, A. Cuni, M. Fijalkowski, and A. Rigo. Tracing the meta-level: Pypy s tracing jit compiler. \nIn ICOOOLPS 09: Proceedings of the 4th workshop on the Implemen\u00adtation, Compilation, Optimization of \nObject-Oriented Lan\u00adguages and Programming Systems, pages 18 25, New York, NY, USA, 2009. ACM. [8] B. \nJ. Bradel and T. S. Abdelrahman. Automatic trace-based parallelization of java programs. In ICPP 07: \nProceedings of the 2007 International Conference on Parallel Processing, page 26, Washington, DC, USA, \n2007. IEEE Computer Soci\u00adety. [9] D. Bruening, T. Garnett, and S. Amarasinghe. An infrastruc\u00adture for \nadaptive dynamic optimization. In CGO 03: Proceed\u00adings of the international symposium on Code generation \nand optimization, pages 265 275, Washington, DC, USA, 2003. IEEE Computer Society. [10] C. Chambers and \nD. Ungar. Customization: optimizing com\u00adpiler technology for self, a dynamically-typed object-oriented \nprogramming language. In PLDI 89: Proceedings of the ACM SIGPLAN 1989 Conference on Programming language \ndesign and implementation, pages 146 160, New York, NY, USA, 1989. ACM. [11] M. Chang, M. Bebenita, A. \nYermolovich, A. Gal, and M. Franz. Ef.cient just-in-time execution of dynamically typed languages via \ncode specialization using precise runtime type inference. Technical Report ICS-TR-07-10, University of \nCalifornia, Irvine, 2007. [12] M. Chang, E. Smith, R. Reitmaier, M. Bebenita, A. Gal, C. Wimmer, B. Eich, \nand M. Franz. Tracing for web 3.0: trace compilation for the next generation web applications. In VEE \n09: Proceedings of the 2009 ACM SIGPLAN/SIGOPS inter\u00adnational conference on Virtual execution environments, \npages 71 80, New York, NY, USA, 2009. ACM. [13] A. Cuni, D. Ancona, and A. Rigo. Faster than c#: ef.cient \nimplementation of dynamic languages on .net. In ICOOOLPS 09: Proceedings of the 4th workshop on the Implemen\u00adtation, \nCompilation, Optimization of Object-Oriented Lan\u00adguages and Programming Systems, pages 26 33, New York, \nNY, USA, 2009. ACM. [14] L. De Moura and N. Bj\u00f8rner. Z3: an ef.cient SMT solver. In TACAS 08/ETAPS 08: \nProceedings of the Theory and prac\u00adtice of software, 14th international conference on Tools and algorithms \nfor the construction and analysis of systems, pages 337 340, Berlin, Heidelberg, 2008. Springer-Verlag. \n[15] J. C. Dehnert, B. K. Grant, J. P. Banning, R. Johnson, T. Kistler, A. Klaiber, and J. Mattson. The \ntransmeta code morphingTMsoftware: using speculation, recovery, and adap\u00adtive retranslation to address \nreal-life challenges. In CGO 03: Proceedings of the international symposium on Code gener\u00adation and optimization, \npages 15 24, Washington, DC, USA, 2003. IEEE Computer Society.  [16] ECMA. International standard ECMA-355, \nCommon Lan\u00adguage Infrastructure, June 2006. [17] A. Gal. Ef.cient bytecode veri.cation and compilation \nin a virtual machine. PhD thesis, Long Beach, CA, USA, 2006. Adviser Franz, Michael. [18] A. Gal, B. \nEich, M. Shaver, D. Anderson, D. Mandelin, M. R. Haghighat, B. Kaplan, G. Hoare, B. Zbarsky, J. Oren\u00addorff, \nJ. Ruderman, E. W. Smith, R. Reitmaier, M. Bebenita, M. Chang, and M. Franz. Trace-based just-in-time \ntype spe\u00adcialization for dynamic languages. In PLDI 09: Proceedings of the 2009 ACM SIGPLAN conference \non Programming lan\u00adguage design and implementation, pages 465 478, New York, NY, USA, 2009. ACM.  [19] \nA. Gal and M. Franz. Incremental dynamic code generation with trace trees. Technical Report ICS-TR-06-16, \nUniversity of California, Irvine, 2006. [20] A. Gal, C. W. Probst, and M. Franz. HotpathVM: an effective \njit compiler for resource-constrained devices. In VEE 06: Proceedings of the 2nd international conference \non Virtual execution environments, pages 144 153, New York, NY, USA, 2006. ACM. [21] J. Ha, M. Arnold, \nS. M. Blackburn, and K. S. McKinley. A concurrent dynamic analysis framework for multicore hard\u00adware. \nIn OOPSLA 09: Proceeding of the 24th ACM SIG-PLAN conference on Object oriented programming systems languages \nand applications, pages 155 174, New York, NY, USA, 2009. ACM. [22] J. C. King. Symbolic execution and \nprogram testing. Com\u00admun. ACM, 19(7):385 394, 1976. [23] F. Logozzo and H. Venter. RATA: Rapid atomic \ntype analysis by abstract interpretation application to JavaScript optimiza\u00adtion. In Compiler Construction, \nvolume 6011 of LNCS, pages 66 83. Springer-Verlag, 2010. [24] B. Mathiske. The maxine virtual machine \nand inspector. In OOPSLA Companion 08: Companion to the 23rd ACM SIG-PLAN conference on Object-oriented \nprogramming systems languages and applications, pages 739 740, New York, NY, USA, 2008. ACM. [25] D. \nMerrill and K. Hazelwood. Trace fragment selection within method-based jvms. In VEE 08: Proceedings of \nthe fourth ACM SIGPLAN/SIGOPS international conference on Virtual execution environments, pages 41 50, \nNew York, NY, USA, 2008. ACM. [26] Microsoft. Dynamic Language Runtime (DLR), 2010. http://www.codeplex.com/dlr/. \n[27] A. Rigo and S. Pedroni. PyPy s approach to virtual machine construction. In OOPSLA 06: Companion \nto the 21st ACM SIGPLAN symposium on Object-oriented programming sys\u00adtems, languages, and applications, \npages 944 953, New York, NY, USA, 2006. ACM. [28] T. Suganuma, T. Yasue, and T. Nakatani. A region-based \ncompilation technique for dynamic compilers. ACM Trans\u00adactions on Programming Languages and Systems, \n28(1):134 174, 2006. [29] A. Yermolovich, C. Wimmer, and M. Franz. Optimization of dynamic languages \nusing hierarchical layering of virtual machines. In DLS 09: Proceedings of the 5th symposium on Dynamic \nlanguages, pages 79 88, New York, NY, USA, 2009. ACM. [30] M. Zaleski, A. D. Brown, and K. Stoodley. \nYeti: a gradu\u00adally extensible trace interpreter. In VEE 07: Proceedings of the 3rd international conference \non Virtual execution environ\u00adments, pages 83 93, New York, NY, USA, 2007. ACM.     \n\t\t\t", "proc_id": "1869459", "abstract": "<p>Tracing just-in-time compilers (TJITs) determine frequently executed traces (hot paths and loops) in running programs and focus their optimization effort by emitting optimized machine code specialized to these traces. Prior work has established this strategy to be especially beneficial for dynamic languages such as JavaScript, where the TJIT interfaces with the interpreter and produces machine code from the JavaScript trace.</p> <p>This direct coupling with a JavaScript interpreter makes it difficult to harness the power of a TJIT for other components that are not written in JavaScript, e.g., the DOM implementation or the layout engine inside a browser. Furthermore, if a TJIT is tied to a particular high-level language interpreter, it is difficult to reuse it for other input languages as the optimizations are likely targeted at specific idioms of the source language.</p> <p>To address these issues, we designed and implemented a TJIT for Microsoft's Common Intermediate Language CIL (the target language of C#, VisualBasic, F#, and many other languages). Working on CIL enables TJIT optimizations for any program compiled to this platform. In addition, to validate that the performance gains of a TJIT for JavaScript do not depend on specific idioms of JavaScript that are lost in the translation to CIL, we provide a performance evaluation of our JavaScript runtime which translates JavaScript to CIL and then runs on top of our CIL TJIT.</p>", "authors": [{"name": "Michael Bebenita", "author_profile_id": "81338487543", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P2354128", "email_address": "", "orcid_id": ""}, {"name": "Florian Brandner", "author_profile_id": "81314487481", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P2354129", "email_address": "", "orcid_id": ""}, {"name": "Manuel Fahndrich", "author_profile_id": "81100288438", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P2354130", "email_address": "", "orcid_id": ""}, {"name": "Francesco Logozzo", "author_profile_id": "81100572523", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P2354131", "email_address": "", "orcid_id": ""}, {"name": "Wolfram Schulte", "author_profile_id": "81100279001", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P2354132", "email_address": "", "orcid_id": ""}, {"name": "Nikolai Tillmann", "author_profile_id": "81100175291", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P2354133", "email_address": "", "orcid_id": ""}, {"name": "Herman Venter", "author_profile_id": "81384610324", "affiliation": "Microsoft Research, Redmond, WA, USA", "person_id": "P2354134", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1869459.1869517", "year": "2010", "article_id": "1869517", "conference": "OOPSLA", "title": "SPUR: a trace-based JIT compiler for CIL", "url": "http://dl.acm.org/citation.cfm?id=1869517"}