{"article_publication_date": "10-17-2010", "fulltext": "\n Scalable and Systematic Detection of Buggy Inconsistencies in Source Code Mark Gabel1 Junfeng Yang2 \n* Yuan Yu3 Moises Goldszmidt3 Zhendong Su1 1 University of California, Davis 2 Columbia University 3 \nMicrosoft Research, Silicon Valley {mggabel, su}@ucdavis.edu junfeng@cs.columbia.edu {yuanbyu, moises}@microsoft.com \nAbstract Software developers often duplicate source code to replicate functionality. This practice can \nhinder the maintenance of a software project: bugs may arise when two identical code segments are edited \ninconsistently. This paper presents DejaVu, a highly scalable system for detecting these general syntactic \ninconsistency bugs. DejaVu operates in two phases. Given a target code base, a parallel inconsistent \nclone analysis .rst enumerates all groups of source code fragments that are similar but not identical. \nNext, an extensible buggy change analysis framework re.nes these results, separating each group of inconsistent \nfragments into a .ne-grained set of inconsistent changes and classifying each as benign or buggy. On \na 75+ million line pre-production commercial code base, DejaVu executed in under .ve hours and produced \na report of over 8,000 potential bugs. Our analysis of a sizable random sample suggests with high likelihood \nthat at this report contains at least 2,000 true bugs and 1,000 code smells. These bugs draw from a diverse \nclass of software defects and are often simple to correct: syntactic inconsistencies both indicate problems \nand suggest solutions. Categories and Subject Descriptors D.2.5 [Software En\u00adgineering]: Testing and \nDebugging; D.2.7 [Software En\u00adgineering]: Distribution, Maintenance, and Enhancement restructuring, reverse \nengineering, and reengineering General Terms Languages, Reliability, Algorithms, Exper\u00adimentation * Junfeng \nYang was supported in part by NSF Grant Nos. CNS-1012633 and CNS-0905246. Zhendong Su was supported in \npart by NSF CAREER Grant No. 0546844, NSF CyberTrust Grant No. 0627749, NSF CCF Grant No. 0702622, NSF \nTC Grant No. 0917392, and US Air Force Grant No. FA9550-07-1-0532. Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA/SPLASH 10, October 17 21, 2010, Reno/Tahoe, \nNevada, USA. Copyright &#38;#169; 2010 ACM 978-1-4503-0203-6/10/10. . . $10.00 1. Introduction Software \ndevelopers often duplicate source code to replicate functionality. This duplication may be performed \nsimply for convenience, or it may represent more thoughtful solutions to dif.cult engineering problems. \nIn either case, copied code can be dif.cult to maintain. Identical code is often expected to evolve in \nsync, with feature additions and bug .xes applied consistently across copies. In practice, this can be \nquite challenging. For instance, copied code may lack annotations that point a would-be editor to all \nother identical code segments, so consistent updates become dependent on a developer s memory and/or \ndiligence. Large systems with multiple authors further complicate the situation. For example, a new developer \nof an operating system may use a copy of an existing device driver as a template without informing the \noriginal author. Lacking communication, a bug in the shared portions of the code would not likely be \n.xed consistently. Researchers have recognized these dif.culties and have studied the duplicate code \nproblem extensively. Examples of technical work include the formalization of the notion of similar code \nfragments, referred to as code clones, and the de\u00advelopment of ef.cient algorithms for enumerating all \ncopied code in a software system, implemented as clone detection tools [2, 3, 15, 18, 21]. Other, more \nempirical work has stud\u00adied the incidence of code cloning both qualitatively [19, 20] and quantitatively \n[22, 25]. Most of the research on code clones advocates some form of proactive solution to the clone \nmaintenance problem; that is, the research introduces solutions that aim to prevent the introduction \nof bugs to copied code. For example, much of clone-related research makes the explicit or tacit suggestion \nthat cloning code should be avoided and that cloned code itself should be factored out into a single, \nparameterized copy whenever possible: Baker s seminal paper on the topic [2] dis\u00adcusses the total reduction \nof a program s size given a complete factorization of all cloned code, putting the code in a certain \nnormal form. Other research provides support for managing existing code clones, providing tools to track \ncopied portions of code [6, 23] and update them consistently [27].  Comparatively few projects have \nprovided support for some form of retroactive detection and repair of clone\u00adrelated bugs, the two notable \nexamples being Li et al. s CP-Miner [21] and Jiang et al. s context-based clone bug detector [16]. Though \neffective, these bug .nding tools have fundamental limitations. Recall: Quantity and Variety of Bugs \nBoth of these tools focus on a speci.c class of clone-related bugs we call copy\u00adtime bugs. These bugs \nare introduced when code is copied: they are a result of improperly adapting copied code into its new \nenvironment. Each tool is further restricted to a spe\u00adci.c subclass of copy-time bugs: CP-Miner .nds \ninstances of copied code with inconsistently renamed identi.ers, while Jiang et al. s tool extends that \napproach by additionally locat\u00ading code clones with con.icting surrounding contexts (e.g. an if statement \nversus a standard code block), which may indicate that the preconditions for execution of the code were \nnot understood when copying. These tools do not suf.ciently address the general and likely much more \ncommon case of bugs in inconsistent edits to clones, which usually occur after the initial copy. The \nediting of code clones, be it bug .xes other evolutionary edits, involves the insertion, deletion, and/or \nreplacement of arbitrary code. As these specialized tools are based on an analysis of exact code clones, \nonly the rare instances of inconsistent edits that .t their respective models will be detected the majority \nwill likely be missed. Precision: Quality of Bug Reports Despite their focus on speci.c classes of bugs, \nexisting tools suffer from a large number of false bug reports: both Li et al. and Jiang et al. report \nthe precision of their tools as approximately 10%, i.e., 90% of their output consists of false alarms. \nWhile one must grant a certain amount of latitude to tools based solely on the analysis of syntax, the \nsignal to noise ratio of these tools is likely at or below many developers thresholds for tolerance. \nScalability While existing clone bug detection tools are suf.ciently scalable to moderately large (e.g. \nseveral million lines of code) software systems, none have been demonstrated to scale to very large projects \n(e.g. tens to hundreds of millions of lines). This level of scale would hardly be a purely academic exercise: \ncommercial code bases can grow to this size for a single large project. DejaVu In this paper, we address \nall of these limitations with DejaVu, a highly scalable and precise system for a detecting buggy inconsistencies \nin source code. DejaVu implements a static source code analysis that captures both the class of copy-time \nclone bugs those found in poorly\u00adadapted copies of source code and the more general class of inconsistent \nedits to clones. Our work is inspired by and builds on previous work on the detection of near-miss clones \nin software: a limited number of existing clone detection tools [15, 21] are able to boost recall (the \namount of copied code found) by allowing for minor differences in clones, e.g. clones differing by a \nfew tokens. At a high level, DejaVu functions quite similarly: we also search for near miss clones, but \nwe treat the inconsistencies as potential bugs. Though conceptually straightforward, this approach presents \nseveral key challenges: Focusing on near misses is too restrictive. In order to detect a broad class \nof inconsistent edits, we must push the concept of near miss clones to its logical limit and allow for \nhighly divergent clones. Discrepancies between pairs of code fragments may exceed a few tokens: inconsistent \nedits may involve several entire code blocks, and the changes need not necessarily be contiguous.  Precision \nis paramount. Relaxing the de.nition of simi\u00adlarity creates a natural precision problem by potentially \nadmitting many spurious, non-copied clones. There is a more subtle issue as well: a pair of inconsistent \ncode fragments makes for a rather crude bug report, as any given pair of fragments may contain several \nclasses of changes: some will be intentional and adaptive, others will be unintentional but innocuous, \nand only a fraction of them, if any, will be buggy.  No clone detection tool capable of .nding divergent \nclones has been demonstrated to scale beyond inputs of a few to several million lines of code. DejaVu \ns primary target is a 75+ million line pre-production commercial code base that is anecdotally believed \nto contain a signi.cant amount of copied code.  DejaVu contains two principal components. While each \nis separable as a unique contribution, we believe our most impor\u00adtant contribution is our demonstration \nof their effectiveness as a practical, integrated system. Large-scale Inconsistent Clone Analysis We \nimplement an inconsistent clone detection tool for C and C++ programs. It is both parallel and distributed, \ntaking advantage of both local and network computing power. Using four quad-core systems, DejaVu s clone \ndetection component is able to enumerate all clones in a 75+ million line system using a quite liberal \ndef\u00adinition of similarity in just a few hours. The system is based on a signi.cant reworking and an independent \nimplementa\u00adtion of the current state-of-the-art algorithm, Jiang et al. s DECKARD [15]. Our extensions, \namong other things, add the aforementioned parallelization and a guarantee of maximal\u00adity of the returned \nclone sets: we report the largest possible clones without reporting any redundant constituent compo\u00adnents, \ngreatly improving the precision and completeness of the bug reports. Buggy Change Analysis Framework \nOur clone analysis is tuned for extreme recall; at its most aggressive settings, the raw clones are barely \nusable on their own. In spite of this, our system maintains a high degree of precision through our buggy \nchange analysis framework. This framework which can be bolted on to any clone detection tool uses a combination \nof .exible lexical analysis, sequence differ\u00adencing and similarity algorithms, and the extraction of \nother  1 int status = OK; 2 int *resdi = NULL; 3 int res = NULL; 4 AnsiString resourcePath; 5 RESOURCEINFO \nresinfo; 6 FRE fre; 7 bool isReady = false; 8 9 Check(resourcePath.Set(paramPath, CP_UTF8)); 10 CheckTrue(res \n= RsrcOpen(resourcePath, BINARY | RDONLY | SEQUENTIAL, 0), E_FAIL); 11 12 // Bug Ver 17, Bug #12743: \nWe no longer need to wrap malloc and free. 13 CheckTrue(resdi = RsrcCreate(malloc, free, RsrcOpen, RsrcRead, \nRsrcWrite, RsrcClose, RsrcSeek, UNKNWN, &#38;fre), E_FAIL); 14 isReady = RsrcIsReady(resdi, res, &#38;resinfo); \n15 16 Error: 17 if (resdi) 18 RsrcCleanup(resdi); 19 if (res) 20 RsrcClose(res); 21 return isReady; 1 \nint status = OK; 2 int *resdi = NULL; 3 int res = -1; // Invalid resource handle 4 AnsiString resourcePath; \n5 RESOURCEINFO resinfo; 6 FRE fre; 7 bool isReady = false; 8 9 Check(resourcePath.Set(paramPath, CP_UTF8)); \n10 res = RsrcOpen(resourcePath, BINARY | RDONLY | SEQUENTIAL, 0); 11 12 13 CheckTrue(res != -1, E_FAIL); \n14  15 16  17 18 // Bug Ver 17, Bug #12743: We no longer need to wrap malloc and free. 19 CheckTrue(resdi \n= RsrcCreate(malloc, free, RsrcOpen, RsrcRead, RsrcWrite, RsrcClose, RsrcSeek, UNKNWN, &#38;fre), E_FAIL); \n20 isReady = RsrcIsReady(resdi, res, &#38;resinfo); 21 22 Error: 23 if (resdi) 24 RsrcCleanup(resdi); \n25 if (res != -1) 26 RsrcClose(res); 27 return isReady; Figure 1: Motivating Example. A clearly annotated \nexample of an inconsistent .x to a pair of copied code segments. clone-related features to provide a \ncon.gurable framework for re.ning the inconsistent clones and isolating the buggy changes. This step \nis computationally expensive: executing it in a brute force manner over the entire target code base would \nbe intractable; it is only in linking it with the inconsistent clone analysis which serves to massively \nprune the search space that it becomes ef.cient. Our experiments indicate that DejaVu is an effective \nbug .nding system: on our 75+ million line pre-production code base, DejaVu produced 8,103 bug reports. \nWe sampled and veri.ed a random sample of 500 of these reports and found 149 very likely bugs and 109 \nother code smells. Using standard techniques, we estimate with high con.dence that DejaVu has found 2,070 \n2,760 new bugs in this code base. These bugs are varied in nature, often dif.cult to detect using other \ntechniques, and, given that the .xes are often embedded in the bug reports, simple to repair. This paper \nis organized as follows. In the next section (Section 2), we provide a motivating example and a high-level \noverview of DejaVu. In the following two sections (Sections 3 and 4) we describe our system s two main \ncomponents in detail. Section 5 describes our experimental results, and Sections 6 and 7 discuss related \nwork and our plans for continuing this research, respectively. 2. System Overview Our presentation of \nDejaVu begins with a motivating example that highlights the advantages of our approach and provides insight \ninto our design decisions. We continue with a high\u00adlevel architectural view of the system. 2.1 Motivating \nExample Consider the code fragments in Figure 1. These fragments are clones1 that perform the same function: \nthey are part of separate modules that provide object-oriented wrappers around the same system resource. \nThe left fragment contains a critical bug: on line 10, the programmer mistakenly checks the return value \nof RsrcOpen as a Boolean, assuming true indicates a successful return. (The Check and CheckTrue identi.ers \nrefer to control .ow macros that jump to the Error: label, commonly used for cleanup code, if a condition \nis false.) However, this particular system function returns a negative nonzero value on error, which \nis incorrectly interpreted as success. This is a real bug that was located by our system. It exhibits \nseveral interesting properties: 1 All bug examples in this paper are taken directly from a snapshot of \nour commercial code base. They have been anonymized, but we have made an effort to guide the anonymization \nsuch that the apparent semantics of each code fragment remains clear.  Figure 2: DejaVu system architecture. \n On line 12 of the right fragment, a comment clearly shows that this programming error had actually caused \na runtime crash and had been entered into an issue tracking database as a numbered bug. The programmer \ncorrected the con\u00addition of the CheckTrue call (and added an unnecessary second check) but neglected \nto propagate the change to the fragment s twin. This is congruent with the intuitive notion of a copy-paste \nbug.  The presence of other identical comments (lines 12 and 18 of the left and right fragments, respectively) \nsuggests that the fragments either were at one point edited consistently or were duplicated after the \ncomment was added.  The fragments exhibit both inter-line and intra-line struc\u00adtural differences, including \nthe insertion of complete state\u00adments and the modi.cation of a condition, an initializer, and a function \ncall. DejaVu handles these general differ\u00adences while still maintaining a high degree of precision. \n The fragments contain both buggy and irrelevant changes. The initializations on line 3 of both fragments \nare both redundant dead stores and are not relevant to the bug.  Though not depicted in the .gure, these \ncode fragments currently have different owners and are separated by several levels in the project directory \nstructure. This pattern of copying code across large distances is not uncommon and necessitates DejaVu \ns global analysis.  This example clearly motivates our technique: a scalable, global static analysis \nthat enumerates divergent copies of otherwise duplicate code and isolates and classi.es their essential \ndifferences.  2.2 System Architecture This section provides a high-level overview of each of De-jaVu \ns components in the context of the implemented system; we will later present each component s design \nand implemen\u00adtation in detail in Sections 3 and 4. DejaVu s architecture appears in Figure 2. At a high \nlevel, DejaVu consists of two components: the inconsistent clone detector and the buggy change analysis \nframework. Clone Detection Front End The front end of our clone analysis is responsible for reading the \ninput source code and producing a set of clone candidates, abstractions of code fragments that can be \npotentially grouped together as clones. It is implemented within the front end of a commercial C++ compiler \nand thus easily integrates with a variety of build systems. Aside from simplicity of execution, the integration \nwith build systems also allows incremental clone detection. Clone Detection Back End After DejaVu produces \na set of candidate code fragments, the back end of our clone analysis clusters the candidates to form \na set of clones. This step is the dominant computation especially when we increase the computational \ncost by tuning the analysis for recall and it is parallelized on a compute cluster. Though omitted from \nthe Figure 2 for brevity, a single master node coordinates the computation and handles data distribution \nand aggregation. Each node in our compute cluster is equipped with a multicore processor, which we also \nfully utilize. Buggy Change Analysis Framework Upon completion, the clone analysis yields a large, but \npotentially noisy, set of inconsistent clones. We re.ne this set and distill the essential buggy changes \nwith an extensible framework. Brie.y, our change analysis framework operates in three steps. 1. We .rst \nprovide a mechanism for applying .lters to the input clones. In our default implementation, we implement \na single main .lter: a check based on a document similarity algorithm that ensures (with high probability) \nthat each clone is the product of an intentional copy and paste action. 2. Next, we use a sequence differencing \nalgorithm in a series of iterations to align the clones and trim their potentially spuriously-matching \nprologues and epilogues. This focuses our analysis on the essential differences. 3. Using the results \nof the previous step, we distill the difference between the two clones into a set of .ne-grained   \nchanges, which we then categorize and .lter based on their characteristics. If essential, likely-buggy \nchanges remain after this step, we output a bug report consisting of a) the pair s source code fragments \nalong with b) a concise diff, highlighting just the changes DejaVu .nds suspicious. These steps are potentially \nquite computationally expensive, and attempting to use this framework in isolation by feeding an entire \nproject through it would be cost-prohibitive. When acting as part of the complete DejaVu system, however, \nthe clone analysis handles the scalability problem in stride, transforming a large quadratic-time problem \na similarity search over a large code base into a much smaller (by orders of magnitude, but still large), \nlinear-time one the re.nement and classi.cation of a set of syntactic inconsistencies. 3. Detecting Inconsistent \nClones Our clone analysis is based on a reworking and an entirely independent, from scratch implementation \nof Jiang et al. s DECKARD algorithm [15]. DECKARD is a general algorithm that operates over tree structures: \ngiven a tree model, it scal\u00adably enumerates all pairs of similar subtrees and/or subforests, where similarity \nis de.ned by tree edit distance. When ap\u00adplied to trees of source code (either concrete or abstract syn\u00adtax) \nit functions as a highly effective clone detection algo\u00adrithm. The algorithm operates in two high level \nphases. It .rst enumerates all possible clone candidates and abstracts them into characteristic vectors. \nIt then uses locality sensitive hashing [12] to scalably group the vectors by similarity and enumerate \nthe similar sets. This separation of concerns allows a great deal of .exi\u00adbility. In our case, this design \nallowed us to readily adapt the algorithm to our requirements: DejaVu implements an entirely new front \nend, which generates a complete set of candidates, allowing us to .nd maximal clones (Section 3.1). We \nwere then able to independently optimize and rework the back end (Section 3.2) to allow for parallel \nexecution, a more principled treatment of inconsistent clones, and several performance optimizations. \n 3.1 Front End: Candidate Selection and Abstraction Candidate Selection Our algorithm operates over abstract \nsyntax trees. A natural choice for candidate code fragments are those contained by subtrees, the most \nrelevant of which are function de.nitions, blocks, and control .ow constructs (e.g. loops and conditionals). \nThis is a sound .rst step; DejaVu does include all subtree-delineated candidates. However, this set is \nrather coarse grained and likely to miss important instances of copying. Consider again the code fragments \nin Figure 1. While these fragments comprise complete function de.nitions, it is conceivable perhaps even \nlikely that a third similar code fragment might exist in a form like Figure 3. Note that this code fragment \ncontains at least a portion of the same bug: res should be checked against -1 on line 8, not as a 1 // \n... Several lines of unrelated code ... 2 CheckTrue(resdi = RsrcCreate(malloc, free, RsrcOpen, RsrcRead, \nRsrcWrite, RsrcClose, RsrcSeek, UNKNWN, &#38;fre), E_FAIL); 3 isEOF = RsrcIsEOF(resdi, res, &#38;resinfo); \n4 5 Error: 6 if (resdi) 7 RsrcCleanup(resdi); 8 if (res) 9 RsrcClose(res); 10 return isEOF; Figure 3: \nA fragment exhibiting a similar bug. simple Boolean test. As it is a subsequence of a much larger .at \nlist of statements, this code fragment has no minimal surrounding construct; i.e., it does not represent \na subtree. With the use of subtree-only candidates, this bug would likely remain undiscovered. Jiang \net al. provide a strategy for mitigating this risk in the original implementation of DECKARD [15] by \nserializing the syntax tree, effectively creating an unstructured token se\u00adquence, and using a small \n(30 to 50 token), .xed-size sliding window to generate additional candidates. For our bug.nd\u00ading application, \nthis strategy is unsatisfactory. It biases the algorithm toward .nding many small clones, possibly with \nmany repeated lines. We require maximal clones: around each potentially bug-inducing inconsistency, we \nmust .nd as much consistent context as possible to be able to state with con.dence that two code fragments \nare nearly identical. Our system s output must also be as free of duplicate bug reports as possible. \nOur solution to this problem is a somewhat brute force approach: for each node (equivalently, subtree) \nin the abstract syntax tree, we generate additional merged candidates n for all 2 subsequences of the \nnode s children. Put more concretely, for each block, control structure, or function de.nition, we generate \na candidate for each contiguous subsequence of top-level statements. Example Consider this arti.cial \ncode snippet: 1 int main() { 2 inti=0; 3 for(;i<10;++i){ 4 cout << \"Hello.World!\"; 5 if (i >= 10) 6 i=2; \n7 global = i; 8} 9 return 0; 10 } and its associated (simpli.ed) abstract syntax tree:  Assuming we \nset a minimum candidate size of two state\u00adments, we would .rst generate candidates for the subtrees rooted \nat nodes (eq. lines) 1, the enclosing function and 3, the loop. We would then generate merged candidates \nfor all subsequences of each node with at least two children. For the enclosing function de.nition, we \nwould generate the addi\u00adtional candidates consisting of nodes (2 3), (3 9), and (2 3 9). Similarly, we \nrecursively apply the same procedure to the loop (node 3) and generate sequences (4 5), (5 7), and (4 \n5 7). We are, in effect, enumerating every syntactically valid code fragment that a developer could have \ncopied and pasted. DejaVu s front end theoretically runs the risk of producing an inordinately large \nnumber of candidates: at each level of the abstract syntax tree, we are enumerating a quadratic nn(n-1) \nnumber ( 2 = 2 ) of candidates. In practice, this is not a problem: for our 75+ million line code base, \nwe generate fewer than 50 million potential candidates. There are several reasons for this. Minimum \nSize We set our minimum candidate size at 6 statements or statement-expressions. Thus, many small blocks \nare never merged, and there are fewer (but still an asymptotically quadratic number of) subsequences. \n Canonicalization We implemented the ability to canon\u00adicalize the abstract syntax tree with con.gurable \ntrans\u00adformations. At present, DejaVu only performs a single transformation: contiguous sequences of simple \ndeclara\u00adtions, usually found at the top of a given code block, are collapsed into a single logical subtree, \nrooted with an arti\u00ad.cial null node. In our early experiments, considering all subsequences of simple \ndeclarations caused a blowup in the number of candidates with little quantitative (e.g., clone coverage \nmetrics) impact on the result set.  Con.guration of Merging We have also implemented the ability to \ndisable merging on certain parent node types. As implemented, we only add this restriction to a single \ncase: we do not generate candidates for all possible subsequences of cases of a switch. Note, though, \nthat we still generate candidates for the subsequences of statements within the blocks delineated by \nthe case labels.  Abstraction We have described our enumeration strategy for a complete set of clone \ncandidates. Following DECKARD, we abstract each candidate into a characteristic vector, to be later clustered \nby the back end with respect to a similarity measure. Brie.y, a characteristic vector is a point in n-dimensional \nspace, where n is the number of tree node types.2 In a given point, the value of each coordinate counts \nthe number of occurrences of a given node type in the clone candidate. In our small example earlier, \nthe following vector characterizes the entire fragment: .1 FUNCDEFN,1DECL,1FOR ,1FUNCALL,1IF,2ASGN ,1RETURN \n. 2 Our C++ abstract syntax trees have 124 different node types. Figure 4: Clone detection back end. \nNote that this characterization necessarily involves the full abstraction of all constant values and \nidenti.er names. An important property of these vectors is that despite their heavy abstraction, the \nEuclidean distance between any two points correlates strongly with the tree edit distance between the \ntwo clone candidates [28]; thus, clustering with respect to Euclidean distance yields similar clones. \nFor a more detailed presentation of this abstraction, see Jiang et al. s paper [15]. It is important \nto note that both layers of abstraction abstract syntax trees and characteristic vectors are sound: two \nidentical (syntactically complete) code fragments will always correspond to identical abstract syntax \ntrees, and two identical abstract syntax trees will always correspond to identical characteristic vectors. \nEach layer of abstraction provides us with an opportunity to .nd more clones (i.e., increase recall) \nin a more scalable manner (with a reduction in the problem space) at the possible expense of precision. \n 3.2 Back End: Grouping The back end of our clone analysis clusters the abstracted clone candidates \n(i.e., the vectors) into sets of clones. The key enabling technology that allows this to be performed \nscalably is locality sensitive hashing [12]. Brie.y, LSH is a highly scalable probabilistic technique \nfor hashing similar items to the same bucket, thus transforming a linear search for similar objects into \nsimple collision detection (which takes constant time). The DECKARD approach involves using an instance \nof LSH to solve a near neighbor problem with the candidate clones, thus grouping similar code fragments. \nJiang et al. use Andoni s e2LSH package [1] to implement this solver; we also use Andoni s implementation \nin binary form. LSH forms the basis for the scalability of our approach, but several challenges remain \nin attempting to scale the analysis up to 75+ million lines of code while continuing to produce accurate \nresults that are meaningful for bug detection. Figure 4 displays the interaction of these components, \nwhich are described in detail in the following sections. Parallelization A very large code fragment is \nnever a clone of a very small code fragment, and larger code fragments should tolerate proportionally \nmore edits (inconsistencies) than smaller ones. Jiang et al. formalize this notion and pro\u00advide a formula \nfor dispatching the vectors into overlapping subgroups based on their magnitude, which closely corre\u00adsponds \nto code size. Each subgroup has a progressively larger average magnitude and is assigned increasingly \nmore relaxed clustering parameters (i.e., allowing for more inconsistency).  We adapt and implement \na similar formula. In addition, we take the novel step of dispatching the subgroups to nodes in a compute \ncluster and processing them in parallel. For moderately large target projects (around 5 million lines \nof code), this clustering step completes in around 10 to 20 minutes and does not dominate the overall \nprocess. However, for our very large code base, clustering can take tens of hours. Partitioning the work \nallows a near linear speedup in the number of nodes. With the use of a cluster of four quad core servers, \nour entire clone analysis completes in a few hours rather than (up to) several days. Coordination is \nhandled on a single master node. We schedule the clustering of the subgroups in decreasing order of size \nto minimize overall latency. In addition, we have implemented basic fault tolerance; the master node \ndetects and retries any failed or incomplete clustering jobs. Filtering of Identical Vectors As a performance \noptimiza\u00adtion, we preprocess each clustering task by condensing all fragments with identical vectors \n(i.e., the exact clones) into sets of fragments mapping to a single vector. We implement this in linear \ntime using standard hashing. In practice, this yields a 20 to 30% reduction in the number of vectors \nthat the LSH library must consider. As this optimization is local to a given task (vector group), we \ndistribute this work to the cluster as well. Local Post-Processing A given set of similar clone can\u00addidates \nis unlikely to be succinct and meaningful without further processing. Consider a pair of exactly duplicate \ncode fragments, surrounded by inconsistent context: 1 RSLT result = *gResult; 1 object->get(&#38;result,0); \n2 2 3 min = result.xMin; 3 min = result.xMin; 4 max = result.xMax; 4 max = result.xMax; 5 } 5 } 6 6 \n7 min = result.yMin; 7 min = result.yMin;  8 max = result.yMax; 8 max = result.yMax; 9} 9} 10 int \nmid = min/2+max/2; 10 printf(\"min=%d\\n\",min); Lines 2-9 of each fragment are clearly exact clones. However, \nwhen allowing for an inconsistency of approximately one statement, lines 2-9 of the .rst fragment are \nalso duplicates of a) lines 1-9 and 2-10 of the right fragment and b) lines 1-9 and 2-10 of the same \n(left) segment. Intuitively, we see a pair of exact clones, but using this relaxed de.nition of similarity, \nthe tool incorrectly detects a group of 6 similar code fragments. This highlights a major problem with \na very common case: nearly every exact clone can be trivially expanded to form an inexact clone, albeit \nnot in a meaningful way that .nds inconsistent edits. In general, we would like to extract from each \ngroup the most alike, non-overlapping subset of code fragments. We implement a fast, approximate solution \nto this problem that makes use of each fragment s characteristic vectors: we favor the most alike code \nfragments by prioritizing those whose vectors appear most tightly clustered in the Euclidean space. For \na given clone set, we .rst calculate the geometric centroid point of its characteristic vectors. Next, \nwe calculate the distance between the centroid and each individual code fragment s point. After sorting \nby this distance, ascending, we greedily prune overlapping code fragments. In general, exact clones if \nthey exist fall naturally toward the center of the cluster and are listed .rst. The trivial inconsistent \nclones are sorted later in the sequence and are pruned due to overlap. In our experience, this re.nement \ntremendously improved the quality of the results. With ad-hoc random pruning of the overlapping portion \nof groups, over half (extrapolating from random samples) of the inconsistent clones were trivial extensions \nof exact clones. When using this technique, this proportion dropped to below one percent. Global Post-Processing: \nMaximal Clones As a .nal step, we prune all fully subsumed clone groups from the result set. For example, \nif two functions are complete clones, the subsumed sub-clones consist of all pairs of their constituent \nblocks. These subsumed groups are quite common: because we generate a complete set of candidates, the \npresence of a larger clone group in the result set implies the existence of many smaller, subsumed clones. \nWe accomplish this pruning in amortized O(nlogn) time by maintaining a priority heap of clone groups, \nsorted lexicographically (after grouping those with equivalent cardinality) .rst ascending by their members \nstarting lines and then descending by their members ending lines. Sorting in this way ensures that subsumed \ngroups are adjacent, so pruning them is a linear time operation. To minimize memory overhead, we implemented \nan immutable string library with internalization. Note that because we generate a complete set of candidates, \nthis pruning phase yields a minimal set of maximal clones ideal for bug .nding. 4. Buggy Change Analysis \nDejaVu s buggy change analysis framework takes a large, potentially noisy set of inconsistent clones \nand extracts from it a set of bug reports in the form of speci.c, likely-buggy inconsistent changes. \nThough this component is an integral part of our system as a whole, it is separable and usable with any \nclone detection tool, albeit less effectively if provided with fewer potential clones than our own recall-tuned \nclone analysis is able to detect. The .rst stage of this process (Section 4.1) provides an interface \nfor the coarse-grained .ltering of clones, which we utilize sparingly. The second stage (Section 4.2) \nre.nes each clone pair by performing a series of abstraction and sequence alignment operations. The .nal, \nmost important  1 if (cd != cdNone) goto Error; 1 if (cd != cdNone) goto Error; 2 if (cdDEF != cdNone) \n{ cd = cdDEF; goto Error; } 2 if (cdDEF != cdNone) { cd = cdDEF; goto Error; } 3 3 4 // read data 4 // \nread data types 5 cd = ReadData(stream, false, &#38;cnt, &#38;data); 5 cd = ReadTypes(stream, &#38;ntypes, \n&#38;buffer); 6 if (cd != cdNone) goto Error; 6 if (cd != cdNone) goto Error; 7 7 8 // hand back results. \n8 // hand back results. 9 *pcnt_output = cnt; 9 *pntypes = ntypes; 10 *pdata_output = data; 10 *pbuffer \n= buffer; 11 data = NULL; 11 buffer = NULL; 12 12 13 Error: 13 Error: 14 if (data != NULL) 14 if (buffer \n!= NULL) 15 free(&#38;data); 15 free(buffer); 16 Release(stream); 16 Release(stream); 17 Release(requestData); \n17 Release(requestData); (a) A bug: the accidental freeing of a stack address. Though they contain several \ndifferences, the fragments textual similarity is > 0.5. 1 if (flags &#38; BORDER) 1 if (flags &#38; RowsBit) \n1 if ID &#38; ID 1 if ID &#38; ID 2 obj->OptBorder = true; 2 obj->OptRows = true; 2 ID -> ID = BOOL 2 \nID -> ID = BOOL 3 if (flags &#38; SHADING) 3 if (flags &#38; LastBit) 3 if ID &#38; ID 3 if ID &#38; \nID 4 obj->OptShading = true; 4 obj->OptLast = true; 4 ID -> ID = BOOL 4 ID -> ID = BOOL 5 if (flags &#38; \nFONT) 5 if (flags &#38; ColsBit) 5 if ID &#38; ID 5 if ID &#38; ID 6 obj->OptFont = true; 6 obj->OptCols \n= true; 6 ID -> ID = BOOL 6 ID -> ID = BOOL 7 if (flags &#38; COLOR) 7 if (flags &#38; FirstBit) 7 if \nID &#38; ID 7 if ID &#38; ID 8 obj->OptColor = true; 8 obj->OptFirst = true; 8 ID -> ID = BOOL 8 ID -> \nID = BOOL 9 if (flags &#38; SCROLL) 9 if (!(flags &#38; NoTabs)) 9 if ID &#38; ID 9 if ! ID &#38; ID \n10 obj->OptScroll = true; 10 obj->OptTabs = true; 10 ID -> ID = BOOL 10 ID -> ID = BOOL (b) A false \nclone with text similarity < 0.5 (c) The abstraction that led to the false clone. Figure 5: Examples \nthat motivate our use of textual similarity as a .lter. stage (Section 4.3) extracts the syntactic differences \nbetween each clone as a set of .ne-grained change operations, which we then classify as benign or buggy, \nthe latter of which we present as bug reports. 4.1 Filtering The most straightforward way of improving \nDejaVu s pre\u00adcision is through coarse-grained .ltering of the inconsistent clones; that is, one can simply \ndiscard clones that do not meet certain criteria. In terms of recall, however, reckless and overly coarse \n.ltering is antithetical to our goal of .nding a large number of general clone bugs; overly selective \n.lters are both dif.cult to generalize and often biased toward low hanging fruit. With this in mind, \nwe limit DejaVu s coarse .ltering to a single rule, a minimum textual similarity threshold that allows \nus to establish an important foundational assumption: that each clone pair we examine very likely originates \nfrom an intentional copy and paste operation. Our intuition is that inconsistent changes between incidentally \nsimilar code fragments are far less likely to be buggy than those between true deliberate clones. Concrete \nmotivating examples for this .lter appear in Figure 5. Textual Similarity On manual inspection, a developer \nmay be able to quickly dismiss false clones due to the (somewhat nebulously de.ned) intuition that the \nfragments do not look similar at all. We codify and automate this process using a notion from the .eld \nof information retrieval: document similarity. A similarity index of two documents is a value in the \nclosed interval [0, 1] that describes their relative resemblance. A variety of approaches exist, but \nmany follow a similar pat\u00adtern: generate representative sets from each document and compare them using \na set similarity measure. To compute tex\u00adtual similarity of source code, DejaVu generates for each code \nfragment a w-shingling [4], a set of contiguous, .xed-length subsequences of tokens the program s text, \nand compares the sets using the Jaccard index: |X nY | J(X,Y )= |X .Y | Example: Consider the following \ntwo lines of source code: int res = NULL; int res = 1; These lines correspond to the following shinglings \nof length three ( trigrams ): F1 = { [ int, res, = ], [ res, =, NULL ], [ =, NULL, ; ] } F2 = { [ int, \nres, = ], [ res, =, 1 ], [ =, 1, ; ] }  Their intersection consists of the .rst element of each, and \ntheir union is of size .ve. The similarity of the two fragments is: 1 J(F1, F2)= = 0.2 5 For our experiments, \nwe use sequences of length .ve (5\u00adgrams), a length near the median number of tokens per line, and we \nset our threshold to the fairly permissive value of 0.5, which we arrived at after a series of exploratory \nexperiments. There are more formal ways of .tting these thresholds: we can treat them as parameters and \nthen use methods from statistics such as cross-validation and ROC curves to estimate the effectiveness \nof a setting and make a decision on the optimal values. For the purposes of our concrete dataset, the \nvalues above were suf.cient. Though we only implement a single .lter, this stage is a natural extension \npoint for project-speci.c tuning. For example, we noted in our experiments that a disproportionate number \nof false alarms came from a single large .le: a remote procedure call client interface completely comprised \nof idiomatic marshaling and network code. Had the goal of our experiments been to produce as precise \na report as possible rather than to evaluate our system, we could have implemented a .lter to exclude \nthis .le and others like it.  4.2 Re.nement The next stage of our buggy change analysis is the re.nement \nof individual clone pairs. The goal of this phase is twofold: we prune inconsistent, but likely spuriously-matching \npro\u00adlogues and epilogues from each pair, and we simultaneously establish a common abstraction over which \nto compare the clones. We illustrate this process in Figure 6 and describe it here. We .rst establish \nan abstraction for each code fragment using .exible lexical analysis, con.gured for the C++ lexicon (with \npreprocessor directives) with the following options: We fully abstract literals by type. All Boolean \nliterals become a token marked $BOOL, for example.  We perform consistent mapping of identi.ers to normal\u00adized \nvalues. DejaVu greedily normalizes identi.ers as they occur: the .rst referenced identi.er of each fragment \nbecomes $0ID, the second $1ID, and so on. For equal but consistently renamed code fragments, this generaliza\u00adtion \nstrategy produces a perfect substitution of identi.er names. However, this strategy performs poorly in \nthe presence of minor structural changes that introduce or ref\u00aderence new identi.ers in otherwise equivalent \nfragments: it incorrectly causes a cascading chain of differences. We mitigate this effect by not normalizing \nthe shared (i.e., not renamed) identi.ers and greedily renaming the remaining set.  Whitespace and comment \ntokens are discarded, and key\u00adwords are left intact.  An example of this normalization appears in the \nlower row of Figure 6: note the lack of abstraction on the shared identi.er free. After abstraction, \nwe align the two normalized sequences using a sequence differencing algorithm (Ratcliff/Obershelp [24]). \nIf we .nd that a pair of inconsistent clones starts or ends with an inconsistent prologue or epilogue, \nrespectively, we prune it: as described in Section 3, trivial extensions of clones are usually uninteresting. \nAlthough our earlier-described postprocessing solves this problem for the common case of trivial extensions \nto exact clones, we .nd that these spurious matches are quite common in highly divergent clones. Note \nthat we are not throwing out the entire clones, we are merely re.ning trimming them to their essential \ncore. Figure 6 illustrates an additional subtlety: after prun\u00ading, the abstraction namely, the identi.er \nrenaming is no longer valid and must be repeated. We iterate this abstrac\u00adtion and alignment process \nuntil each clone is surrounded by a user-con.gurable amount of exact context, which we currently set \nto .ve tokens. To evaluate our system s general applicability, we limited our con.guration of the lexer \nto the three rules described above. However, additional project-speci.c con.guration is possible. An \nembedded system project, for example, might selectively choose to abstract all bit-manipulating operators \nto a single value, while an enterprise system may prefer to not abstract string literals so as to identify \nembedded SQL bugs.  4.3 Buggy Change Isolation At this step in the analysis, DejaVu has produced a set \nof inconsistent code fragments that are 1) very likely deliberately copied and 2) abstracted, aligned, \nand trimmed; that is, they are focused on their essential differences. The .nal, and arguably most important, \nphase of our analysis takes these aligned, abstracted token sequences and distills them into a sequence \nof .ne-grained change operations: insertions, deletions, and replacements that are capable of transforming \none fragment in the clone pair into its inconsistent twin. DejaVu then classi.es each of these individual \nchanges as benign or buggy. We perform this classi.cation with a series of syntactic .lters. Each .ltering \nfunction has access to a variety of infor\u00admation: 1) the source text and abstracted tokens involved in \nthe change, 2) the complete source text of both fragments, and 3) the set of all changes (for making \ncontextual judgments). While we have implemented a small set of general default .lters, we expect this \nnatural extension point to be commonly utilized in practice: it provides a simple, intuitive, and fast \npoint at which users can .t DejaVu to a speci.c project and focus precisely on bugs of interest. Our \nthree default .lters are de.ned as follows:  Figure 6: An example of our re.nement process. Matching \nfragments are typeset in black and differences are highlighted in grey. Punctuation Our .rst .lter is \nsimple: we discard changes that consist solely of punctuation (e.g. semicolons and braces), as they are \nusually not semantics-affecting. Parameter Additions This .lter controls for a speci.c class of adaptive \nchange: the addition of new arguments to a function. An example of this .lter s applicability appears \nin Figure 5a: on line 5 of each fragment, note that each calls a different function. This particular \nchange is adaptive and a result of differing functionality, while the change at line 15 is the more relevant, \nsuspicious change. Identi.er Scope This .lter affects only changes that contain identi.ers; all others \nimplicitly pass. For any change that contains at least one identi.er, at least one identi.er must be \nshared with the pair clone, and at most one identi.er can be unshared. Our intuition here is that divergent \nbug .xes are most likely applicable when their referenced variables are in scope. Note that we do not \nrequire all variables to be in scope: a common form of bug is that of a missing but necessary function \ncall. Our experiments were con.gured with exactly these three change .lters. However, throughout our \nevaluation of the bug reports, we were without a shortage of ideas for additional .lters, and our extensible \nframework simpli.es the process of implementing and evaluating them. Reporting Once the buggy changes \nhave been identi.ed, DejaVu outputs each clone pair containing at least one suspi\u00adcious change as a bug \nreport. Along with the source text, the report entries include a rendered diff of the speci.c changes \nDejaVu .nds suspicious: DejaVu maps each change back to the relevant source lines, displaying 1) any \nline in its entirety that contains at least one structural difference and 2) between lines, precise intra-line \nmarkers highlighting the locations of relevant structural differences. Note that in our evaluation, we \nonly marked a bug as true if DejaVu identi.ed both the relevant clone pair and the relevant buggy change(s). \nThese differences highlight the essence of the inconsistent edit and facilitate inspection: in our evaluation \n(cf. Section 5), each bug report took an average of 2.4 minutes to verify, albeit with a somewhat high \nvariance. Note that DejaVu often .nds bugs that have already been .xed at least once, so the solution \nto the problem is often encoded in the bug report. Over.tting and Generality A natural objection to the \nspe\u00adci.c instances of the techniques described above is over.tting: we may have developed .lters and \nrules that are speci.c to our current target. While the particular rules we present are arguably general, \nwe do not see generality as an absolute requirement: our framework has several extension points, and \nproject-speci.c .lters are simple to write and try. The development of a purely syntactic bug .nding \nanalysis that functions equally well on all projects is likely a futile exer\u00adcise: in its most practical \nsetting, we envision DejaVu s use in a work.ow in which aggressive, project-speci.c .lters are used and \nre.ned frequently. Scalability These operations are potentially expensive to compute: each involves numerous \nstring and set operations that are potentially quadratic in the size of the code fragments. However, \nin practice, the buggy change analysis module executes quickly. The key factor in.uencing its scalability \nis the massive reduction in problem size afforded by the scalable clone analysis. In our experiments, \nthe clone detection front end enumerates O(108) source code fragments. Pairwise analysis of these fragments \nfor buggy changes would require O(1016) expensive executions of the buggy change analysis framework. \nInstead, the clone analysis despite being tuned for recall reduces this to a much more manageable set \nof O(104) clone pairs, each requiring only a single expensive change analysis. Nonetheless, this module \nhas numerous opportunities for optimization. First, it is implemented in Python and would achieve a signi.cant \ngain by moving to a more performance\u00adoriented language. Second, this task is easily parallelized, as \nit involves only local comparisons between two code frag\u00adments. Third, more scalable but less precise \nsimilarity tech\u00adniques like winnowing [26] may provide a further increase in performance by quickly removing \nthe least similar clones.  Parameter Value Minimum code fragment size 6 statements DECKARD similarity \nvalue 0.8 Minimum textual similarity 0.5 Textual similarity n-gram length 5 tokens Alignment: min. exact \ncontext 5 tokens Table 1: DejaVu s parameter settings. 5. Experimental Results An execution of DejaVu \non our large commercial code base produced 8,103 bug reports in under .ve hours. We estimate that 2,070 \n2,760 of these very likely represent true, previously-unknown bugs. The following sections provide a \ndetailed presentation of this evaluation as well as a quantitative and qualitative assessment of the \nresults. 5.1 Methodology and Timing We evaluated DejaVu on a code base with 100+ million lines of C and \nC++ source code.3 75+ million lines are contained in actively built source .les and 25+ million lines \nreside in header .les. This code base comprises approximately 450 projects that form a single larger \nproduct family. Despite this unifying relationship, the individual projects cover a diverse range of \nfunctionality. The front end of our tool resides within the compiler and thus considers only actively \ncompiled code. This is an interesting property of DejaVu, which can be considered both a strength and \na weakness. On the positive side, errors in uncompiled code are likely to be of low severity and may \nbe considered noise. However, should we need to process .les outside of a functional build system, the \ncompiler that hosts our front end does have the ability to run its parsing stage in isolation (i.e., \nsyntax check only mode), which does not check dependencies and can be run with a minimum of effort. In \npractice, we intend to use DejaVu only on actively compiled code as part of the regular development process. \nTiming We deployed DejaVu on a single master node and four cluster nodes. The master node is equipped \nwith a dual core Intel processor and Windows Server 2008. Each cluster node is equipped with two dual \ncore AMD Opteron processors and a 64 bit version of Windows Server 2003. The experimental parameters \nused for DejaVu s various compo\u00adnents are described throughout this paper; we consolidate them for reference \nin Table 1. We ran DejaVu s front end separately, during a normal build of the code base. The integration \nwith the custom build system rendered precise overhead measurements dif.cult, but we observed that the \ngeneration of clone candidates added a negligible amount of time to the overall build. In addition, future \nexecutions of this step, which are executed 3 We have also performed an additional experiment on an open \nsource code base; it is included as Appendix A. DejaVu Execution Time (Elapsed) Component Time (hh:mm) \nSequential Portion 0:45 Clone Detection Parallel Portion 2:05 Buggy Change Analysis Framework 1:30 Total \n4:20 Table 2: Execution time on our 75+ million line code base. incrementally through a build system, \nisolate this slight amount of overhead to changed .les only. The rest of the process, which includes \nclone detection, change analysis, and report generation, took a total of 4 hours and 20 minutes. A breakdown \nof the time spent in individual components appears in Table 2. The clone detection phase was con.gured \nwith a fairly liberal similarity setting: the mean edit distance between the clones in our result set \nis 28 tokens, and controlled for size, the clone pairs differ by approximately 14% on average (with a \nfairly high variance). Despite this potential scalability trap, the clone detection phase completes in \nunder 3 hours and is dominated by the parallel step, which we execute on all 16 available cores (4 nodes \n\u00d7 2 CPUs/node \u00d7 2 cores/CPU). Adding more nodes to the cluster is likely to improve performance further, \nand as discussed in Section 4, the .nal re.nement step contains numerous opportunities for optimization. \n 5.2 Error Reports DejaVu produced 8,103 bug reports, which were beyond our resources to verify fully. \nWe instead sampled 500 reports uniformly at random a signi.cant sample size and divided the set into \n.ve categories: Bugs Very likely bugs. Examples are found throughout this paper. Code Smells Suspicious \nbut ultimately harmless differences that are often inconsistently .xed. Example: a dead store to a local \nvariable that one programmer notices is redundant and deletes from his or her copy. Style Smells Changes \nin style that leave the semantics of the code fragments unaffected but may hurt clarity. For example, \nsome code fragments contain clever uses of as\u00adsignments as expressions. When copied, a programmer may \nadapt the fragment for clarity using temporary vari\u00adables. Unknown There is nothing apparently false \nabout these reports, and the necessary conditions for their validity are there; the changes apparently \napply. However, we could not con.dently judge them as bugs without further domain knowledge. False False \nreports. These consist mainly of adaptive changes we fail to account for and .lter, but also occasion\u00ad \n Category Count Rate Extrapolated Count Bugs 149 30% 2,070 2,760 Code Smells 49 10% 450 1,140 Style \nSmells 60 12% 630 1,320 Unknown 58 12% 600 1,280 False Reports 184 37% 2,640 3,330 Table 3: Categorization \nof sampled bug reports and estimates of total counts with 95% con.dence. ally include pathological cases \nin which every inconsistent edit is intentional. The results of this evaluation appear in Table 3. Our \nsample size of 500 allows us to estimate the rate of any one of these parameters in our larger report \nwith a \u00b14.25% margin of error with 95% con.dence (equivalently, a \u00b13.56% margin with 90% con.dence). \nConservatively assuming every Unknown report is false, our overall true-bug precision is very likely \nbetween 26 and 34%: triple that of previous work, and our set of bug reports very likely contains 2,070 \n2,760 true bugs. Weakening our de.nition of bug to include code smells but still conservatively assuming \nall Unknown reports to be false raises our precision to between 48 and 56%. Qualitative Assessment The \npotential bugs are quite varied: Figures 7 and 8 contain two of the more interesting examples. In the \n.rst example (Figure 7), locking around accesses to a .eld are added to otherwise identical functions. \nIn the second example (Figure 8), a .le resource is leaked along an error path. The bugs presented earlier \nin this paper (Figures 1 and 5a) are equally diverse. These examples highlight particularly well DejaVu \ns novelty: we are able to detect a very general class of clone bugs. Many of the bugs are more mundane. \nFigure 9 contains three common classes of bugs that we observed repeatedly in the result set. The .rst \n(Figure 9a) is a general error related to input validation: the author mistakenly joined the two disjoint \nerror conditions with the logical and operator. This example is particularly obvious; line 2 of the .rst \nfragment simpli.es to false. The second example (Figure 9b) involves a com\u00admon idiom: function error \ncodes are commonly collected in a single result variable and returned upon exit. We observed many cases \nwhere this was omitted likely by mistake and .xed inconsistently. The .nal example (Figure 9c) is the \nubiquitous null check: we found numerous examples where inconsistent null checks were added to otherwise \ntextually identical code fragments even within a single .le. Code Smells and Style Smells also comprised \na signi.cant proportion of the results and are worth exploring in that they can often lead to future \nbugs. Three common examples appear in in Figure 10. Other examples include unnecessary declarations, \ninconsistent type quali.ers (e.g.. const and unsigned), and unnecessary explicit conversions from non-Boolean \ntypes to Boolean values. False Positives At present, about 50% of the potential bug reports are either \nobviously false or unveri.able by us without more extensive domain knowledge. As we noted in Section \n4, many of these could be .ltered at the risk or perhaps the bene.t of over.tting DejaVu to our current \ntarget code base. This number is quite manageable, and unlike in many forms of traditional static analysis, \nthese false reports are bene.cial to investigate: duplicate code is a code smell in its own right. Our \nanalysis framework ensures that all the false positive bug reports are still almost certainly derived \nfrom true clone reports, so the false positives are often viable refactoring candidates. Lastly, as reports \nare con.rmed by the development teams, we expect to implement several .lters that provide more relevant \nresults; that is, we intend to tune DejaVu to not only .nd true bugs, but to also focus on the bugs the \ndevelopers believe are severe. Such .lters would act at the expense of recall; we present only the most \ngeneral results in this paper. 6. Discussion and Related Work In this section, we provide a detailed \ncomparison with other copy-paste bug detection tools as well as a brief overview of other related work. \n 6.1 Copy-Paste Bug Detection Tools Despite a vast amount of research on clone detection, compar\u00adatively \nfew tools actively detect clone-related bugs; Li et al. s CP-Miner [21] and Jiang et al. s context-based \nbug detection tool [16] are the best-known examples. CP-Miner CP-Miner [21] detects identi.er renaming \nbugs. Identi.er renaming bugs occur when a developer copies a code fragment and intends to rename one \nor more identi.ers consistently but omits a case. Though these bugs are often caught by the compiler, \nthey are quite dif.cult to trace when they do occur. Our tool succinctly captures this class of bugs \nas well: we start by fully abstracting identi.er names, .nding both consistently and inconsistently renamed \nidenti.ers, and later detect inconsistent renaming problems as structural differences in the token streams. \nThe method by which we unify variable names is quite similar to that used by Li et al., though their \nmore speci.c heuristics are likely much more precise for this class of bugs. For comparison, we conducted \nan additional simple study: we excluded from our report clones who differ only in the consistent naming \nof identi.ers. We noted only an approxi\u00admate 4% decrease in our report size, which suggests that our \ngeneral technique is .nding a signi.cantly larger new class of bugs. Within that 4%, we noted many of \nthe same types of false positives reported by Li et al., including simple reorder\u00adings. As part of our \nongoing work, we intend to integrate their heuristics for this class of bugs into our own tool. Context-Based \nJiang et al. developed a tool [16] that looks for exact clones that are used inconsistently. The intuition \nis that duplicate code should be used in similar contexts, e.g.  1 CS.Enter(); 2 request = m_request; \n3 m_request = 0; 4 CS.Leave(); 5 6 if (request) { 7 CloseRequest(request); 8 request = 0; 9} 10 /* \nBody of method omitted for space (20 lines) */ 11 CS.Enter(); 12 if (IsTaskCancelled()) 13 Log(USER_CANCEL); \n14 15 m_request = request; 16 request = 0; 17 CS.Leave(); 18 19 Error: 20 CS.Leave(); // Force leave \nhere if still inside CS. 21 22 CloseRequest(request); 23 return res; 1 request = m_request; 2 m_request \n= 0; 3 4 if (request) { 5 CloseRequest(request); 6 request = 0; 7} 8 /* Body of method omitted for space \n(20 lines) */  9 if (Cancelled()) 10 Log(USER_CANCEL); 11 12 m_request = request; 13 request = 0; 14 \n15 Error: 16 CloseRequest(request); 17 return res; Figure 7: A concurrency bug. Though in a different \nclass, the right fragment contains the critical section .eld and guards other accesses of m_request with \nit. 1 FILE file = Open(fName, WRITE, CREATE, NORMAL); 2 if(file == INVALID_FILE) return E_FAIL; 3 4 int \nsize = SizeofResource(mod, rsc); 5 int written; 6 RESOURCE rGlobal = LoadResource(mod, rsc); 7 8 if(!rGlobal) \n9 return E_FAIL; 18 19 void *data = AccResource(rGlobal); 20 if (data == NULL) { 21 res = E_FAIL; 22 \ngoto exit; 23 } 24 if (!WriteFile(file, data, size, &#38;written, NULL)) { 25 res = E_FAIL; 26 goto exit; \n27 } 28 exit: 29 if(file != INVALID_FILE_VALUE) 30 Close(file); 1 FILE file = Open(fileName, WRITE, CREATE, \nNORMAL); 2 if(file == INVALID_FILE) return E_FAIL; 3 4 int size = SizeofResource(mod, rsc); 5 int written; \n6 RESOURCE rsrc = LoadResource(mod, rsc); 7 if(rsrc == NULL) { 8 res = E_FAIL;  9 10 }  11 void *data \n= AccResource(rsrc); 12 if (data == NULL) { 13 res = E_FAIL; 14 goto Cleanup; 15 } 16 if (!WriteFile(file, \ndata, size, &#38;written, NULL)) { 17 res = E_FAIL; 18 goto Cleanup; 19 } 20 Cleanup: 21 if(file != INVALID_FILE_VALUE) \n22 Close(file); Figure 8: A resource leak. The .le is not closed on all paths. a null check surrounding \none clone should also surround another. The authors report about a 10% true positive rate overall. Though \nwe .nd many of the same kinds of bugs indeed, local instances of context inconsistencies are a special \ncase of our general inconsistencies the tool is complemen\u00adtary to our own in general, as the surrounding \ncontext of a clone can often be quite distant and thus undetectable as a single inexact clone. Like this \ntool, DejaVu operates over syntax trees, so we can integrate this technique with our own by recording \nthe enclosing syntactic context of each clone and using it as a clone-related feature for .ltering. 6.2 \nOther Related Work Clone Detection Tools Many tools have been developed for detecting similar code fragments, \neach with varying levels of scalability, abstraction, and granularity; we brie.y present a sampling of \nthe more scalable techniques for comparison. Baker s tool dup [2] was one of the .rst clone detection \ntools and it remains one of the most scalable. It operates at line granularity and allows for consistent \nrenaming of tokens via parameterized string matching. It is incapable, however, of .nding clones with \nstructural differences.  1 // Check level function argument 2 if( level < 1 &#38;&#38; level > NUM_LEVELS \n) 3 return INVALID_ARG; 1 // Check level function argument 2 if( level < 1 || level > NUM_LEVELS ) 3 \nreturn INVALID_ARG; (a) Poor argument checking. NUM_LEVELS is a positive integer constant. 1 // Finally, \nsubmit the message ! 2 SubmitMessage(msgStatus); 3 /* ... */ 4 return result; 1 // Finally, submit the \nmessage ! 2 result = SubmitMessage(msgRoute); 3 /* ... */ 4 return result; (b) Not saving the result \nof an important function call. 1 if (!AddItem(psp, pspDest, *desc, AFTER)) 2 return false; 1 if (!desc \n|| !AddItem(psp, pspDest, *desc, AFTER)) 2 return false; (c) An added null check to an otherwise identical \npair of larger functions. Figure 9: Three examples of common classes of simple bugs. Context is omitted \nfor brevity. As discussed throughout this paper, Deckard [15] forms the basis of our approach. Gabel \net al. later extended this technique [11] to scalably .nd clones within subgraphs of program dependence \ngraphs [10], which encode data and control .ow information. The data.ow information encoded in PDGs may \nimprove the precision of DejaVu s bug reports by .ltering non-semantics-affecting changes in a more principled \nway. Li s CP-Miner [21], discussed earlier as a bug .nding tool, is also based on a novel clone detection \nalgorithm. Like De\u00adjaVu, CP-Miner enumerates a set of possible clone candidates and later groups them \ntogether. CP-Miner .rst enumerates and abstracts the basic blocks of the target code base and then uses \na data mining technique, frequent subsequence mining, to enumerate the possible clones. This approach \nallows for the detection of clones with an inserted or removed statement (referred to as a gap). To output \nmaximal clones, CP-Miner assembles the smaller basic block-level clones into larger, more complete copied \ncode segments. CCFinder [18] uses a suf.x tree-based algorithm to .nd clones within token streams. Though \nasymptotically quadratic, it scales well to software projects of a few million lines of code, though \nit is incapable .nding of any form of structurally inconsistent clone. A later parallelized extension, \nD-CCFinder [22], scales this approach to a 400 million line code base, which takes 2 days to complete \non 80 nodes. The 1 res = chunk->put_Context(chunkContext); 2 ON_ERR_RETURN(res); 3 res = chunk->put_Locale(chunkId); \n4 ON_ERR_RETURN(res); 1 res = ((Chunk*) chunk)->put_Context(chunkContext); 2 ON_ERR_RETURN(res); 3 res \n= chunk->put_Locale(chunkId); 4 ON_ERR_RETURN(res); (a) A strange extraneous cast. 1 PSESSION pSession; \n2 if (NULL == (pSession = GetActiveSession())) 3 return; 1 PSESSION pSession = NULL; 2 if (NULL == (pSession \n= GetActiveSession())) 3 return; (b) A dead store to a local variable. 1 len = StringLen(bstr); 2 if \n(wz == NULL || *outlen < (len + 1)) 3{ 4 *outlen = StringLen(bstr); 5 return wz == NULL; 6} 1 len = \nStringLen(bstr); 2 if (wz == NULL || *outlen < (len + 1)) 3{ 4 *outlen = len; 5 return wz == NULL; 6} \n (c) Redundant recomputation of a value. Figure 10: Three code smells. extension is straightforward: \nthey divide the problem into n n smaller chunks and .nd clones within the 2 possible pair\u00adings. Though \non the same level of scale as DejaVu, it does not .nd inconsistent clones or attempt to .nd clone bugs. \nSchleimer et al. s MOSS [26] is a highly scalable system for detecting software plagiarism. It utilizes \na document similarity technique related to the algorithms we apply in our inspection framework. However, \nit is quite coarse grained, operating at the program or .le level, and is thus unsuitable for .nding \nclone related bugs. Studies of Cloning Each of the above tools reports that a signi.cant amount (5-30%) \nof their target systems consist of copied code, which motivates well our technique. Kim et al. [20] study \nthe nature of code clone genealogies by utilizing a project s version control history to track the in\u00adtroduction, \nevolution, and removal of clones over time. In a portion of the work that is relevant to our own, they \n.nd that a signi.cant proportion of all code clones over the history of two software projects evolved \nin sync and are thus prone to the general class of errors that we detect. In a concurrently developed \nstudy [17], Juergens et al. extend a suf.x tree-based clone detection algorithm to detect clone pairs \nwith a small number of edits. They report the inconsistent clones in their entirety to the developers \nof several proprietary software projects and study the fault rate of the inconsistent clone fragments. \nThey .nd that these fragments are more failure-prone than non-copied code, which validates our own approach. \nHowever, their algorithm is asymptotically quadratic and appears to hit its scaling limit around 5 million \nlines of code.  Lightweight Bug Finding Engler et al. [8] introduced the idea of using behavioral inconsistencies \nto .nd bugs. For example, if a pointer is dereferenced unconditionally on one program path but is checked \nfor validity on another, their tool outputs a warning: either the check is unnecessary or a bug exists. \nThis is quite similar in spirit to our own work: we search for bugs using syntactic inconsistencies. \nLike Engler et al. s work, our work inherently exposes the nature of the inconsistencies, greatly simplifying \nthe repair of the defects. Other tools, like the historical LCLint [9], FindBugs for Java code [14], \nand Engler et al. s Metal speci.cation language and checker for systems code [7, 13] are all capable \nof using syntactic characteristics and/or a local static analysis to .nd bugs in large software systems. \n(Interestingly, in a later empirical study [5], Chou et al. note that a large proportion of the errors \nfound by their tool are propagated by copying and pasting source code.) These tools are all alike in \nthat they require as input explicit speci.cations for speci.c classes of bugs. In our scenario, we expect \ncopied code to form a type of meta-level implicit speci.cation: duplicate code is generally intended \nto remain identical. We extract violations of this implicit speci.cation by leveraging an ef.cient, large \nscale clone analysis, eliminating the need to specify and check instances of duplication manually. Though \nthis higher level speci.cation is weaker, admitting many false positives, it captures a much more general \ncategory of bugs. 7. Conclusions and Future Work In this paper, we have presented DejaVu, a system for \nlocating general copy-paste bugs. We demonstrated that DejaVu is highly scalable: a full analysis completes \nin under .ve hours on 75+ million lines of source code. Our system located 2,070 2,760 very likely bugs \nand we have shown that these bugs are general and varied, affecting resource leakage, concurrency, memory \nsafety, and other important classes of software errors. Our most immediate future work involves continued \neval\u00aduation of our tool, the .rst aspect of which is continuing our communication with the developers \nof our target system. We are currently gathering con.rmation of bug reports and so\u00adliciting feedback \non the tool s output. We also intend to run DejaVu against several other large code bases, both commer\u00adcial \nand open source. References [1] A. Andoni and P. Indyk. Near-optimal hashing algorithms for approximate \nnearest neighbor in high dimensions. In Proceedings of FOCS 06, 2006. [2] B. S. Baker. On .nding duplication \nand near-duplication in large software systems. In WCRE 95: Proceedings of the Second Working Conference \non Reverse Engineering, 1995. [3] I. D. Baxter, A. Yahin, L. Moura, M. Sant Anna, and L. Bier. Clone \ndetection using abstract syntax trees. In ICSM, 1998. [4] A. Z. Broder, S. C. Glassman, M. S. Manasse, \nand G. Zweig. Syntactic clustering of the web. In Selected papers from the sixth international conference \non World Wide Web, pages 1157 1166, 1997. [5] A. Chou, J. Yang, B. Chelf, S. Hallem, and D. Engler. An \nempirical study of operating systems errors. In SOSP 01: Proceedings of the eighteenth ACM symposium \non Operating systems principles, 2001. [6] E. Duala-Ekoko and M. P. Robillard. Tracking code clones in \nevolving software. In ICSE 07: Proceedings of the 29th international conference on Software Engineering, \n2007. [7] D. Engler, B. Chelf, A. Chou, and S. Hallem. Checking system rules using system-speci.c, programmer-written \ncompiler extensions. In OSDI, 2000. [8] D. R. Engler, D. Y. Chen, and A. Chou. Bugs as inconsistent behavior: \nA general approach to inferring errors in systems code. In SOSP, 2001. [9] D. Evans, J. Guttag, J. Horning, \nand Y. M. Tan. Lclint: a tool for using speci.cations to check code. In SIGSOFT FSE, 1994. [10] J. Ferrante, \nK. J. Ottenstein, and J. D. Warren. The program dependence graph and its use in optimization. ACM Trans. \nProgram. Lang. Syst., 9(3), 1987. [11] M. Gabel, L. Jiang, and Z. Su. Scalable detection of semantic \nclones. In ICSE 08: Proceedings of the 30th international conference on Software engineering, 2008. [12] \nA. Gionis, P. Indyk, and R. Motwani. Similarity search in high dimensions via hashing. In Proc. VLDB, \n1999. [13] S. Hallem, B. Chelf, Y. Xie, and D. Engler. A system and language for building system-speci.c, \nstatic analyses. In Proc. PLDI 02, 2002. [14] D. Hovemeyer and W. Pugh. Finding bugs is easy. In OOPSLA \n04, 2004. [15] L. Jiang, G. Misherghi, Z. Su, and S. Glondu. Deckard: Scalable and accurate tree-based \ndetection of code clones. In Proceedings of ICSE, 2007. [16] L. Jiang, Z. Su, and E. Chiu. Context-based \ndetection of clone-related bugs. In ESEC-FSE 07, 2007. [17] E. Juergens, F. Deissenboeck, B. Hummel, \nand S. Wagner. Do code clones matter? In ICSE 09: Proceedings of the 31st international conference on \nSoftware engineering, 2009. [18] T. Kamiya, S. Kusumoto, and K. Inoue. CCFinder: a multilinguistic token-based \ncode clone detection system for large scale source code. TSE, 28(7), 2002.  [19] C. Kapser and M. W. \nGodfrey. \"Cloning considered harmful\" considered harmful. In Proc. WCRE 06, pages 19 28, Washington, \nDC, USA, 2006. IEEE Computer Society. [20] M. Kim, V. Sazawal, D. Notkin, and G. Murphy. An empirical \nstudy of code clone genealogies. In ESEC/FSE-13, 2005. [21] Z. Li, S. Lu, S. Myagmar, and Y. Zhou. CP-Miner: \nA tool for .nding copy-paste and related bugs in operating system code. In OSDI, 2004. [22] S. Livieri, \nY. Higo, M. Matushita, and K. Inoue. Very-large scale code clone analysis and visualization of open source \nprograms using distributed CCFinder: D-CCFinder. In ICSE 07, 2007. [23] T. T. Nguyen, H. Nguyen, N. Pham, \nJ. Al-Kofahi, and T. Nguyen. Cleman: Comprehensive clone group evolution management. Automated Software \nEngineering (ASE), 2008., Sept. 2008. [24] J. W. Ratcliff and D. Metzener. Pattern matching: The gestalt \napproach. Dr. Dobb s Journal, July 1988. [25] C. K. Roy and J. R. Cordy. An empirical study of function \nclones in open source software. In WCRE 08: Proceedings of the 2008 15th Working Conference on Reverse \nEngineering, 2008. [26] S. Schleimer, D. S. Wilkerson, and A. Aiken. Winnowing: local algorithms for \ndocument .ngerprinting. In SIGMOD, 2003. [28] R. Yang, P. Kalnis, and A. K. H. Tung. Similarity evaluation \non tree-structured data. In SIGMOD, 2005. A. Appendix: Experiment on Mozilla To further demonstrate \nDejaVu s general effectiveness as a bug .nding tool, we performed an additional experiment on an open \nsource code base owned by the Mozilla Foundation. This appendix summarizes the results.    A.1 Setup \nCode Base We gathered current Mercurial (a distributed version control system) snapshots of the source \ncode for the Mozilla Firefox web browser (http://mozilla.com/ firefox), the Thunderbird email client \n(http://mozilla. com/thunderbird), and their supporting libraries. Using a na\u00efve methodology, we counted \napproximately two million lines of non-header C and C++ source code. Building After gathering the source \ncode, we integrated DejaVu s front end into the Mozilla build process and ran two standard builds, one \neach for Firefox and Thunderbird. In this con.guration, DejaVu only scanned code actively compiled during \na standard Windows platform build of each project. Con.guration We used the same experimental parameters \nas those described in our formal evaluation (Section 5), with one notable exception: due to the much \nsmaller size of this code base, we performed the entirety of the experiment on a single node, a 2.4 GHz \nCore 2 Duo system with 4 GB of RAM running Microsoft Windows 7. We con.gured DejaVu s clone analysis \nto use both available cores.  A.2 Results Timing Running on a single, node, DejaVu s clone detection \nback end completed in 40 minutes and the buggy change analysis framework completed completed in four \nminutes. For a code base of this size, scalability is not an issue. Results DejaVu produced 136 bug reports. \nWe categorized each report using the same methodology and classi.cation scheme as in our formal evaluation. \n(Any statistically signi.\u00adcant random sample of a population this small would include nearly every report; \nwe thus opted to verify them all.) The results appear in the following table. [27] M. Toomim, A. Begel, \nand S. Graham. Managing duplicated code with linked editing. In Proc. IEEE Symp. Visual Languages: Human \nCentric Computing, 2004.  Category Count Rate Bugs Code Smells 45 21 33% 15% Style Smells Unknown 11 \n33 8% 24% False 26 19% Table 4: Categorization of Mozilla bug reports. We note a comparable rate of \ntrue bugs and similar rates for the other categories: DejaVu appears to be generally effective on this \ncode base as well. However, we tended to annotate bugs as Unknown at nearly twice the rate as in our \ncommercial code base. This is likely due to our lack of previous exposure to the Mozilla source code. \nThe complete experimental results, including DejaVu s output and our classi.cations, can be found online \nat the fol\u00adlowing address: http://wwwcsif.cs.ucdavis.edu/~gabel/ research/dejavu_mozilla.zip  \n\t\t\t", "proc_id": "1869459", "abstract": "<p>Software developers often duplicate source code to replicate functionality. This practice can hinder the maintenance of a software project: bugs may arise when two identical code segments are edited inconsistently. This paper presents DejaVu, a highly scalable system for detecting these general syntactic inconsistency bugs.</p> <p>DejaVu operates in two phases. Given a target code base, a parallel /inconsistent clone analysis/ first enumerates all groups of source code fragments that are similar but not identical. Next, an extensible /buggy change analysis/ framework refines these results, separating each group of inconsistent fragments into a fine-grained set of inconsistent changes and classifying each as benign or buggy.</p> <p>On a 75+ million line pre-production commercial code base, DejaVu executed in under five hours and produced a report of over 8,000 potential bugs. Our analysis of a sizable random sample suggests with high likelihood that at this report contains at least 2,000 true bugs and 1,000 code smells. These bugs draw from a diverse class of software defects and are often simple to correct: syntactic inconsistencies both indicate problems and suggest solutions.</p>", "authors": [{"name": "Mark Gabel", "author_profile_id": "81350577026", "affiliation": "University of California at Davis, Davis, CA, USA", "person_id": "P2354037", "email_address": "", "orcid_id": ""}, {"name": "Junfeng Yang", "author_profile_id": "81502714673", "affiliation": "Columbia University, New York, NY, USA", "person_id": "P2354038", "email_address": "", "orcid_id": ""}, {"name": "Yuan Yu", "author_profile_id": "81100472712", "affiliation": "Microsoft Research, Silicon Valley, Mountain View, CA, USA", "person_id": "P2354039", "email_address": "", "orcid_id": ""}, {"name": "Moises Goldszmidt", "author_profile_id": "81100295078", "affiliation": "Microsoft Research, Silicon Valley, Mountain View, CA, USA", "person_id": "P2354040", "email_address": "", "orcid_id": ""}, {"name": "Zhendong Su", "author_profile_id": "81100108298", "affiliation": "University of California at Davis, Davis, CA, USA", "person_id": "P2354041", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1869459.1869475", "year": "2010", "article_id": "1869475", "conference": "OOPSLA", "title": "Scalable and systematic detection of buggy inconsistencies in source code", "url": "http://dl.acm.org/citation.cfm?id=1869475"}