{"article_publication_date": "10-17-2010", "fulltext": "\n Back to the Futures: Incremental Parallelization of Existing Sequential Runtime Systems James Swaine \nKevin Tew Peter Dinda Northwestern University University of Utah Northwestern University JamesSwaine2010@u.northwestern.edu \ntewk@cs.utah.edu pdinda@northwestern.edu Robert Bruce Findler Northwestern University robby@eecs.northwestern.edu \nAbstract Many language implementations, particularly for high-level and scripting languages, are based \non carefully honed run\u00adtime systems that have an internally sequential execution model. Adding support \nfor parallelism in the usual form as threads that run arbitrary code in parallel would require a major \nrevision or even a rewrite to add safe and ef.cient locking and communication. We describe an alternative \nap\u00adproach to incremental parallelization of runtime systems. This approach can be applied inexpensively \nto many sequen\u00adtial runtime systems, and we demonstrate its effectiveness in the Racket runtime system \nand Parrot virtual machine. Our evaluation assesses both the performance bene.ts and the developer effort \nneeded to implement our approach. We .nd that incremental parallelization can provide useful, scalable \nparallelism on commodity multicore processors at a frac\u00adtion of the effort required to implement conventional \nparallel threads. Categories and Subject Descriptors D.3.4 [Programming Languages]: Processors Run-time \nenvironments General Terms Languages, Performance Keywords Racket, functional programming, parallel pro\u00adgramming, \nruntime systems 1. Runtime Systems from a Sequential Era Many modern high-level or scripting languages \nare imple\u00admented around an interpretive runtime system, often with a Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA/SPLASH 10, October 17 21, 2010, Reno/Tahoe, \nNevada, USA. Copyright c &#38;#169; 2010 ACM 978-1-4503-0203-6/10/10. . . $10.00 Matthew Flatt University \nof Utah m.att@cs.utah.edu JIT compiler. Examples include the Racket [11] (formerly PLT Scheme) runtime \nsystem, the Parrot virtual machine, and the virtual machines underlying Perl, Python, Ruby, and other \nproductivity-oriented languages. These runtime sys\u00adtems are often the result of many man-years of effort, \nand they have been carefully tuned for capability, functionality, correctness, and performance. For the \nmost part, such runtime systems have not been de\u00adsigned to support parallelism on multiple processors. \nEven when a language supports constructs for concurrency, they are typically implemented through co-routines \nor OS-level threads that are constrained to execute one at a time. This limitation is becoming a serious \nissue, as it is clear that ex\u00adploiting parallelism is essential to harnessing performance in future processor \ngenerations. Whether computer architects envision the future as involving homogeneous or heteroge\u00adneous \nmulticores, and with whatever form of memory co\u00adherence or consistency model, the common theme is that \nthe future is parallel and that language implementations must adapt. The essential problem is making \nthe language imple\u00admentation safe for low-level parallelism, i.e., ensuring that even when two threads \nare modifying internal data structures at the same time, the runtime system behaves correctly. One approach \nto enabling parallelism would be to al\u00adlow existing concurrency constructs to run in parallel, and to \nrewrite or revise the runtime system to carefully employ locking or explicit communication. Our experience \nwith that approach, as well as the persistence of the global interpreter lock in implementations for \nPython and Ruby, suggests that such a conversion is extremely dif.cult to perform correctly. Based on \nthe even longer history of experience in parallel systems, we would also expect the result to scale poorly \nas more and more processors become available. The alternative of simply throwing out the current runtime \nand re-designing and implementing it around a carefully designed concur\u00adrency model is no better, as \nit would require us to discard years or decades of effort in building an effective system, and this approach \nalso risks losing much of the language s momentum as the developers are engaged in tasks with little \nvisible improvement for a long period. In this paper, we report on our experience with a new technique \nfor parallelizing runtime systems, called slow\u00adpath barricading. Our technique is based on the observa\u00adtion \nthat the core of many programs and particularly the part that runs fast sequentially and could bene.t \nmost from parallelism involves relatively few side effects with respect to the language implementation \ns internal state. Thus, in\u00adstead of wholesale conversion of the runtime system to sup\u00adport arbitrary \nconcurrency, we can add language constructs that focus and restrict concurrency where the implementa\u00adtion \ncan easily support it. Speci.cally, we partition the set of possible operations in a language implementation \ninto safe (for parallelism) and unsafe categories. We then give the programmer a mecha\u00adnism to start \na parallel task; as long as the task sticks to safe operations, it stays in the so-called fast path of \nthe imple\u00admentation and thus is safe for parallelism. As soon as the computation hits a barricade, the \nruntime system suspends the computation until the operation can be handled in the more general, purely \nsequential part of the runtime system. Although the programming model allows only a subset of language \noperations to be executed in parallel, this subset roughly corresponds to the set of operations that \nthe pro\u00adgrammer already knows (or should know) to be fast in se\u00adquential code. Thus, a programmer who \nis reasonably capa\u00adble of writing a fast programs in the language already pos\u00adsesses the knowledge to \nwrite a program that avoids unsafe operations and one that therefore exhibits good scaling for parallelism. \nFurthermore, this approach enables clear feed\u00adback to the programmer about when and how a program uses \nunsafe operations. We continue our presentation by discussing parallel com\u00adputation in Racket and introducing \nour variant of futures (Section 2), which we use as the programmer s base mech\u00adanism for spawning a parallel \ntask. We then explain in gen\u00aderal terms our approach to incremental parallelization (Sec\u00adtion 3). The \ncore of our work is the implementation of fu\u00adtures in the Racket runtime system (Section 4) and Parrot \nvirtual machine (Section 5). Our Racket implementation is more mature and is included in the current \nrelease. We eval\u00aduate the implementations both in terms of developer effort to create them, and in terms \nof their raw performance and scaling for simple parallel benchmarks, including some of the NAS parallel \nbenchmarks (Section 6). The contributions of this work are the slow-path barricad\u00ading technique itself, \nas well as an evaluation of both how dif\u00ad.cult it is to add to Racket and Parrot and the performance \nand scalability of the resulting runtime systems. Our eval\u00aduation suggests that the developer effort \nneeded to use the approach is modest and that the resulting implementations have reasonable raw performance \nand scaling properties. 2. Parallelism through Futures Racket provides future to start a parallel computation \nand touch to receive its result: future :(. a) . a-future touch : a-future . a The future function takes \na thunk (i.e., a procedure with no arguments) and may start evaluating it in parallel to the rest of \nthe computation. The future function returns a future descriptor to be supplied to touch. The touch function \nwaits for the thunk to complete and returns the value that the thunk produced. If touch is applied to \nthe same future descriptor multiple times, then it returns the same result each time, as computed just \nonce by the future s thunk. For example, in the function (define (f x y) (* (+ x y) (-x y)))  the expressions \n(+ xy) and (-xy) are independent. They could be could be computed in parallel using future and touch \nas follows: (define (f x y) (let ([s (future (lambda () (+ x y)))] [d (future (lambda () (-x y)))]) (* \n(touch s) (touch d))))  The main computation can proceed in parallel to a future, so that the variant \n(define (f x y) (let ([d (future (lambda () (-x y)))]) (* (+ x y) (touch d))))  indicates as much parallelism \nas the version that uses two futures, since the addition (+ x y) can proceed in parallel to the future. \nIn contrast, the variant (define (f x y) (let ([s (future (lambda () (+ x y)))]) (* (touch s) (-x y)))) \n includes no parallelism, because Racket evaluates expres\u00adsions from left to right; (-x y) is evaluated \nonly after the (touch s) expression. A future s thunk is not necessarily evaluated in parallel to other \nexpressions. In particular, if the thunk s computa\u00adtion relies in some way on the evaluation context, \nthen the computation is suspended until a touch, at which point the computation continues in the context \nof the touch. For ex\u00adample, if a future thunk raises an exception, the exception is raised at the point \nof the touch. (If an exception-raising (define MAX-ITERS 50) (define MAX-DIST 2.0) (define N 1024) (define \n(mandelbrot-point x y) (define c (+ (-(/ (* 2.0 x) N) 1.5) (* +i (-(/ (* 2.0 y) N) 1.0)))) (let loop \n((i 0) (z 0.0+0.0i)) (cond [(> i MAX-ITERS) (char->integer #\\*)] [(> (magnitude z) MAX-DIST) (char->integer \n#\\space)] [else (loop (add1 i) (+ (* z z) c))]))) (for ([y (in-range N)]) (for ([x (in-range N)]) (write-byte \n(mandelbrot-point x y))) (newline)) Figure 1. Sequential Mandelbrot plotting future is touched a second \ntime, the second attempt raises a fresh exception to report that no value is available.) A future s thunk \ncan perform side effects that are visible to other computations. For example, after (define x 0) (define \n(inc!) (set! x (+ x 1))) (let ([f1 (future inc!)] [f2 (future inc!)]) (touch f1) (touch f2)) the possible \nvalues of x include 0, 1, and 2. The future and touch operations are intended for use with thunks that \nperform independent computations, though possibly storing results in variables, arrays or other data \nstructures using side effects. The f example above has no useful parallelism, because the work of adding \nor subtracting numbers is far simpler than the work of creating a parallel task and communicating the \nresult. For an example with potentially useful parallelism, Figure 1 shows a simple Mandelbrot-set rendering \nprogram, a classic embarrassingly parallel computation. In an ideal language implementation, the Mandelbrot \ncomputation could be parallelized through a future for each point. Figure 2 shows such an implementation, \nwhere for/list is a list-comprehension form that is used to create a list of list of futures, and then \neach future is touched in or\u00adder. This approach does not improve performance, however. Although a call \nto mandelbrot-point involves much more computation than (+x y), it is still not quite as much as the \ncomputation required for future plus touch. Figure 3 shows a per-line parallelization of the Mandel\u00adbrot \ncomputation. Each line is rendered independently to a (define fss (for/list ([y (in-range N)]) (for/list \n([x (in-range N)]) (future (lambda () (mandelbrot-point x y)))))) (for ([fs (in-list fss)]) (for ([f \n(in-list fs)]) (write-byte (touch f))) (newline))) Figure 2. Naive Mandelbrot parallelization (define \nfs (for/list ([y (in-range N)]) (let ([bstr (make-bytes N)]) (future (lambda () (for ([x (in-range N)]) \n(bytes-set! bstr x (mandelbrot-point x y))) bstr))))) (for ([f (in-list fs)]) (write-bytes (touch f)) \n(newline))) Figure 3. Per-line Mandelbrot parallelization buffer, and then the buffered lines are written \nin order. This approach is typical for a system that supports parallelism, and it is a practical approach \nfor the Mandelbrot program in Racket. Perhaps surprisingly, then, the per-line refactoring for Mandelbrot \nrendering runs much slower than the sequential version. The problem at this point is not the decomposition \napproach or inherent limits in parallel communication. In\u00adstead, the problem is due to the key compromise \nbetween the implementation of Racket and the needs of program\u00admers with respect to parallelization. Speci.cally, \nthe prob\u00adlem is that complex-number arithmetic is currently treated as a slow operation in Racket, and \nthe implementation makes no attempt to parallelize slow operations, since they may manipulate shared \nstate in the runtime system. Programmers must learn to avoid slow operations within parallel tasks at \nleast until incremental improvements to the implementation allow the operation to run in parallel. A \nprogrammer can discover the slow operation in this case by enabling debugging pro.ling, which causes \nfuture and touch to produce output similar to future: 0 waiting for runtime at 126.741: * future: 0 waiting \nfor runtime at 126.785: /  The .rst line of this log indicates that a future computation was suspended \nbecause the * operation could not be exe\u00ad (define (mandelbrot-point x y) (define ci (fl-(fl/ (fl* 2.0 \n(->fl y))(->fl N)) 1.0)) (define cr (fl-(fl/ (fl* 2.0 (->fl x))(->fl N)) 1.5)) (let loop ((i 0) (zr 0.0) \n(zi 0.0)) (if (> i MAX-ITERS) (char->integer #\\*) (let ((zrq (fl* zr zr)) (ziq (fl* zi zi))) (if (fl> \n(fl+ zrq ziq) (expt MAX-DIST 2)) (char->integer #\\space) (loop (add1 i) (fl+ (fl-zrq ziq) cr) (fl+ (fl* \n2.0 (fl* zr zi)) ci))))))) Figure 4. Mandelbrot core with .onum-speci.c operations cuted in parallel. \nA programmer would have to consult the documentation to determine that * is treated as a slow oper\u00adation \nwhen it is applied to complex numbers. Another way in which an operation can be slow in Racket is to \nrequire too much allocation. Debugging-log output of the form future: 0 waiting for runtime at 126.032: \n[acquire_gc_page] indicates that a future computation had to synchronize with the main computation to \nallocate memory. Again, the prob\u00adlem is a result of an implementation compromise, because Racket s memory \nallocator is basically sequential, although moderate amounts of allocation can be performed in parallel. \nFigure 4 shows a version of mandelbrot-point for which per-line parallelism offers the expected performance \nim\u00adprovement. It avoids complex numbers, and it also uses .onum-speci.c arithmetic (i.e., operations \nthat consume and produce only .oating-point numbers). Flonum-speci.c op\u00aderations act as a hint to help \nthe compiler unbox interme\u00addiate .onum results keeping them in registers or allocating them on a future-local \nstack, which avoids heap allocation. (In a language with a static type system, types provide the same \nhint.) As a result, in sequential mode, this version runs about 30 times as fast as the original version; \na program\u00admer who needs performance will always prefer it, whether using futures or not. Meanwhile, for \nmuch the same reason that it can run fast sequentially, this version also provides a speed-up when run \nin parallel. All else being equal, obtaining performance through par\u00adallelism is no easier in our design \nfor futures than in other systems for parallel programming. The programmer must still understand the \nrelative cost of computation and com\u00admunication, and the language s facilities for sequential per\u00adformance \nshould be fully deployed before attempting par\u00adallelization. All else is not equal, however; converting \nthe initial Racket program to one that performs well is far sim\u00adpler than, say, porting the program to \nC. For more sophisti\u00adcated programs, where development in Racket buys produc\u00adtivity from the start, futures \nprovide a transition path to paral\u00adlelism that keep those productivity bene.ts intact. Most im\u00adportantly, \nour approach to implementing futures makes these bene.ts available at a tractable cost for the implementer \nof the programming language. 3. Implementing Futures Since future does not promise to run a given thunk \nin parallel, a correct implementation of future and touch is easily added to any language implementation; \nthe result of future can simply encapsulate the given thunk, and touch can call the thunk if no previous \ntouch has called it. Of course, the trivial implementation offers no parallelism. At the opposite extreme, \nin an ideal language implementation, future would immediately fork a parallel task to execute the given \nthunk giving a programmer maximal parallelism, but placing a large burden on the language implementation \nto run arbitrary code concurrently. The future and touch constructs are designed to accom\u00admodate points \nin between these two extremes. The key is to specify when computations can proceed in parallel in a way \nthat is (1) simple enough for programmers to reason about and rely on, and (2) .exible enough to accommodate \nim\u00adplementation limitations. In particular, we are interested in starting with an implementation that \nwas designed to support only sequential computation, and we would like to gradually improve its support \nfor parallelism. To add futures to a given language implementation, we partition the language s set of \noperations among three cate\u00adgories: A safe operation can proceed in parallel to any other com\u00adputation \nwithout synchronization. For example, arith\u00admetic operations are often safe. An ideal implementation \ncategorizes nearly all operations as safe.  An unsafe operation cannot be performed in parallel, ei\u00adther \nbecause it might break guarantees normally provided by the language, such as type safety, or because \nit de\u00adpends on the evaluation context. Its execution must be de\u00adferred until a touch operation. Raising \nan exception, for example, is typically an unsafe operation. The simplest, most conservative implementation \nof futures categorizes all operations as unsafe, thus deferring all computation to touch.  A synchronized \noperation cannot, in general, run in par\u00adallel to other tasks, but by synchronizing with other tasks, \nthe operation can complete without requiring a touch. It thus allows later safe operations to proceed \nin parallel.   Operations that allocate memory, for example, are often synchronized. In a language \nlike Racket, the key to a useful categoriza\u00adtion is to detect and classify operations dynamically and \nat the level of an operator plus its arguments, as opposed to the operator alone. For example, addition \nmight be safe when the arguments are two small integers whose sum is another small integer, since small \nintegers are represented in Racket as immediates that require no allocation. Adding an integer to a string \nis unsafe, because it signals an error and the cor\u00adresponding exception handling depends on the context \nof the touch. Adding two .oating-point numbers, meanwhile, is a synchronized operation if space must \nbe allocated to box the result; the allocation will surely succeed, but it may require a lock in the \nallocator or a pause for garbage collection. This partitioning strategy works in practice because it \nbuilds on an implicit agreement that exists already between a programmer and a language implementation. \nProgram\u00admers expect certain operations to be fast, while others are understood to be slow. For example, \nprogrammers expect small-integer arithmetic and array accesses to be fast, while arbitrary-precision \narithmetic or dictionary extension are rel\u00adatively slow. From one perspective, implementations often \nsatisfy such expectations though fast paths in the inter\u00adpreter loop or compiled code for operations \nthat are expected to be fast, while other operations can be handled through a slower, more generic implementation. \nFrom another per\u00adspective, programmers learn from experimentation that cer\u00adtain operations are fast, \nand those operations turn out to be fast because their underlying implementations in the runtime have \nbeen tuned to follow special, highly optimized paths. The key insight behind our work is that fast paths \nin a language implementation tend to be safe to run in parallel and that it is not dif.cult to barricade \nslow paths, preventing them from running in parallel. An implementation s exist\u00ading internal partitioning \ninto fast and slow paths therefore provides a natural .rst cut for distinguishing safe and un\u00adsafe operations. \nOur implementation strategy is to set up a channel from future to the language implementation s fast \npath to execute a future in parallel. If the future s code path departs from the fast path, then the \ndeparting operation is considered unsafe, and the computation is suspended until it can be completed \nby touch. The details of applying our technique depend on the lan\u00adguage implementation. Based on our \nexperience converting two implementations and our knowledge of other imple\u00admentations, we expect certain \ndetails to be common among many implementations. For example, access to parallelism normally builds on \na POSIX-like thread API. Introducing new threads of execution in a language implementation may require \nthat static variables within the implementation are converted to thread-local variables. The memory manager \nmay need adjustment to work with multiple threads; as a .rst cut, all allocation can be treated as an \nunsafe slow path. To Figure 5. Incremental parallelization methodology support garbage collection and \nsimilar global operations, the language implementation s fast path needs hooks where the computation \ncan be paused or even shifted to the slow path. Figure 5 illustrates our general methodology. We start \nby adding low-level support for parallelism, then experiment\u00ading with the paths in the implementation \nthat are affected. Based on that exploration, we derive a partitioning of the lan\u00adguage s operations \ninto safe and unsafe. Having partitioned the operations, we must implement a trapping mechanism capable \nof suspending a parallel task before it executes an unsafe operation. Finally, we re.ne our partitioning \nof the operations (perhaps designating and implementing some op\u00aderations as synchronized) as guided by \nthe needs of applica\u00adtions. In the following two sections, we report in detail on two experiments adding \nfutures to an existing language. Our .rst and more extensive experiment is in adding futures to Racket. \nWe also added a simple version of futures to the Parrot virtual machine in order to test whether the \nlessons from Racket generalized. 4. Adding Futures to Racket The Racket runtime system is implemented \nby roughly 100k lines of C code. It includes a garbage collector, macro ex\u00adpander, bytecode compiler, \nbytecode interpreter, just-in-time (JIT) compiler, and core libraries. The core libraries include support \nfor threads that run concurrently at the Racket level, but internally threads are implemented as co-routines \n(i.e., they are user threads ). Execution of a program in the virtual machine uses a stack to manage \nthe current continuation and local bind\u00adings. Other execution state, such as exception handlers and dynamic \nbindings, are stored in global variables within the virtual-machine implementation. Global data also \nincludes the symbol table, caches for macro expansion, a registry of JIT-generated code, and the garbage \ncollector s meta-data. In addition, some primitive objects, such as those representing I/O streams, have \ncomplex internal state that must be man\u00adaged carefully when the object is shared among concurrent computations. \n The virtual machine s global and shared-object states present the main obstacles to parallelism for \nRacket pro\u00adgrams. An early attempt to implement threads as OS-level threads which would provide access \nto multiple processors and cores as managed by the operating system failed due to the dif.culty of installing \nand correctly managing locks within the interpreter loop and core libraries. Since that early attempt, \nthe implementation of Racket has grown even more complex. A related challenge is that Racket offers .rst-class \ncon\u00adtinuations, which allow the current execution state to be cap\u00adtured and later restored, perhaps in \na different thread of ex\u00adecution. The tangling of the C stack with execution state means that moving \na continuation from one OS-level thread to another would require extensive changes to representation \nof control in the virtual machine. The design of futures side-steps the latter problem by des\u00adignating \noperations that inspect or capture the current ex\u00adecution state as usafe; thus, they must wait until \na touch. Meanwhile, the notions of unsafe and synchronized opera\u00adtions correspond to using a single big \nlock to protect other global state in the virtual machine. The following sections provide more details \non how we adjusted the implementation of execution state and catego\u00adrized operations in Racket while \nlimiting the changes that were needed overall. 4.1 Compilation, Execution, and Safety Categorization \nExecution of a Racket program uses two phases of compila\u00adtion. First, a bytecode compiler performs the \nusual optimiza\u00adtions for functional languages, including constant and vari\u00adable propagation, constant \nfolding, inlining, loop unrolling (a special case of inlining in Racket), closure conversion, and unboxing \nof intermediate .oating-point results. The byte\u00adcode compiler is typically used ahead of time for large \npro\u00adgrams, but it is fast enough for interactive use. Second, when a function in bytecode form is called, \na JIT compiler con\u00adverts the function into machine code. The JIT creates inline code for simple operations, \nincluding type tests, arithmetic on small integers or .onums, allocations of cons cells, array accesses \nand updates, and structure-.eld operations. When the JIT compiler is disabled, bytecode is interpreted \ndirectly. The .rst step in supporting useful parallelism within Racket was to make the execution-state \nvariables thread\u00adlocal at the level of OS threads, so that futures can be ex\u00adecuted speculatively in \nnew OS-level threads. To simplify this problem, we con.ned our attention to the execution state that \nis used by JIT-generated code. Consequently, our .rst cut at categorizing operations was to de.ne as \nsafe any operation that is implemented directly in JIT-generated code (e.g. any operation that can be \ntranslated by the JIT compiler typedef (*primitive)(int argc, Scheme_Object **argv); Scheme_Object *handler(int \nargc, Scheme_Object **argv, primitive func) { Scheme_Object *retval; if (pthread_self() != g_runtime_thread_id) \n{ /* Wait for the runtime thread */ retval = do_runtimecall(func, argc, argv); return retval; } else \n{ /* Do the work directly */ retval = func(argc, argv); return retval; } } Figure 6. Typical primitive \ntrap handler into machine instructions which do not include function calls back into runtime code), \nand any other operation was unsafe. This .rst-cut strategy offered a convenient starting point for the \nincremental process, in which we modify performance\u00adcritical unsafe operations to make them future-safe. \nWhen a future is created in Racket, the corresponding thunk is JIT-compiled and then added to a queue \nof ready futures. The queue is served by a set of OS-level future threads, each of which begins execution \nof the JIT-generated code. At points where execution would exit JIT-generated code, a future thread suspends \nto wait on the result of an unsafe operation. The operation is eventually performed by the original runtime \nthread when it executes a touch for the future. In our current implementation, a future thread remains \nblocked as long as it waits for the runtime thread to complete an unsafe operation. Since the JIT compiler \nwas designed to work for a non\u00adparallelized runtime system, the code that it generates uses several global \nvariables to manage execution state. In some cases, state is kept primarily in a register and occasionally \nsynchronized with a global variable. To allow multiple JIT\u00adgenerated code blocks to execute in parallel, \nwe changed the relevant global variables to be thread-local variables in the C source of the Racket implementation.1 \nWe then adjusted the JIT compiler to access a global variable through an thread-speci.c indirection. \nThe thread-speci.c indirection is supplied on entry to JIT-generated code. 4.2 Handling Unsafe Operations \nWhen JIT-generated code invokes an operation that is not implemented inline, it invokes one of a handful \nof C func\u00ad 1 On some platforms, we could simply annotate the variable declaration in C with thread. On \nother platforms, we use pre-processor macros and inline assembly to achieve similar results.  Figure \n7. Timeline for a future with an unsafe operation Figure 8. Timeline for a synchronized operation tions \nthat call back into the general interpreter loop. When a future thread takes this path out of JIT-generated \ncode, the call is redirected to send the call back to the runtime thread and wait for a response. Figure \n6 illustrates the general form of such functions. Each .rst checks whether it is already ex\u00adecuting on \nthe runtime thread. If so, it performs the external call as usual. If not, the work is sent back to the \nruntime thread via do runtimecall. The runtime thread does not execute the indirect call until touch \nis called on the corresponding future. Figure 7 illustrates the way that an unsafe operation suspends \na future thread until its value can be computed by the runtime thread in response to a touch. Note that \nthe touch function itself is considered unsafe, so if touch is called in a future thread, then it is \nsent back to the runtime thread. Thus, the touch function need only work in the runtime thread.  4.3 \nSynchronized Operations Like unsafe operations, synchronized operations always run on the runtime thread. \nUnlike unsafe operations, however, the runtime thread can perform a synchronized operation on a future \nthread s behalf at any time, instead of forcing the future thread to wait until touch is called. As part \nof its normal scheduling work to run non-parallel threads, the runtime system checks whether any future \nthread is waiting on a synchronized operation. If so, it im\u00admediately performs the synchronized operation \nand returns the result; all synchronized operations are short enough to be performed by the scheduler \nwithout interfering with thread scheduling. Currently, the only synchronized operations are alloca\u00adtion \nand JIT compilation of a procedure that has not been called before. More precisely, allocation of small \nobjects usually can be performed in parallel (as described in the next section), but allocation of large \nobjects or allocation Figure 9. Per-future allocation of a fresh page for small objects requires cooperation \nand synchronized with the memory manager. Figure 8 illustrates the synchronized allocation a new page \nwith the help of the runtime thread. 4.4 Memory Management Racket uses a custom garbage collector that, \nlike the rest of the system, was written for sequential computation. Specif\u00adically, allocation updates \nsome global state and collection stops the world. As in many runtime systems, the virtual machine and \nits garbage collector cooperate in many small ways that make dropping in a third-party concurrent garbage \ncollector prohibitively dif.cult. Similarly, converting the garbage collector to support general concurrency \nwould be dif.cult. Fortunately, adapting the collector to support a small amount of concurrency is relatively \neasy. The garbage collector uses a nursery for new, small ob\u00adjects, and then compacting collection for \nolder objects. The nursery enables inline allocation in JIT-generated code by bumping a pointer. That \nis, a memory allocation request takes one of the following paths: Fast Path the current nursery page \nhas enough space to accommodate the current request. In this case, a page pointer is incremented by the \nsize of the object being allocated, and the original page pointer is returned to the caller. This path \nis executed purely in JIT-generated code.  Slow Path the current page being used by the allo\u00adcator \ndoes not have enough space to accommodate the current request. In this case, a new page must either be \nfetched from either the virtual machine s own inter\u00adnal page cache, or must be requested from the operating \nsystem. If the entire heap space has been exhausted, a garbage collection is triggered.  The nursery \nitself is implemented as a collection of pages, so adding additional thread-speci.c pages was straightfor\u00adward. \nAs long as it is working on its own page, a future thread can safely execute the inline-allocation code \ngen\u00aderated by the JIT compiler. Figure 9 illustrates the use of future-speci.c nurseries. Acquiring \na fresh nursery page, in contrast, requires syn\u00adchronization with the runtime thread, as described in \nthe pre\u00advious section. The size of the nursery page adapts to the amount of allocation that is performed \nby the future request\u00ading the page. Garbage collection still requires stopping the world, which includes \nall future threads. The JIT compiler gen\u00aderates code that includes safe points to swap Racket-level threads. \nWhen JIT-generated code is running in a future, it never needs to stop for other Racket threads, but \nthe same safe points can be repurposed as garbage-collection safe points. That is, the inlined check \nfor whether the compu\u00adtation should swap threads is instead used as a check for whether the future thread \nshould pause for garbage collec\u00adtion. Meanwhile, garbage collection in the runtime thread must not be \nallowed unless all future threads are blocked at a safe point. Besides providing support for thread-speci.c \nnursery pages, the garbage collector required minor adjustments to support multiple active execution \ncontexts to be treated as roots. Roughly, the implementation uses fake Racket threads that point to the \nexecution state of a computation in a future thread.  4.5 Implementing touch To tie all of the preceding \npieces together, the implementa\u00adtion of touch is as follows: If the future has produced a result already, \nreturn it.  If a previous touch of the future aborted (e.g., because the future computation raised an \nexception), then raise an exception.  If the future has not started running in a future thread, remove \nit from the queue of ready futures and run it directly, recording the result (or the fact that it aborts, \nif it does so).  If the future is running in a future thread, wait until it either completes or encounters \nan unsafe operation:  If the future thread has encountered an unsafe oper\u00adation, perform the unsafe \noperation, return the result, and wait again. If performing the unsafe operation re\u00adsults in an exception \nor other control escape, tell the future thread to abort and record the abort for the fu\u00adture. If the \nfuture completes in a future thread, record and return the result. In addition, the scheduler loop must \npoll future threads to see if any are waiting on synchronized operations; if so, the operation can be \nperformed and the result immediately returned to the future thread. By de.nition, a synchronized operation \ncannot raise an exception. 5. Adding Futures to Parrot Parrot is a register-based virtual machine with \nheap-allocated continuation frames. Compilers target Parrot by emitting programs in the Parrot intermediate \nlanguage, which is a low-level, imperative assembly-like programming language, but with a few higher-level \nfeatures, including garbage col\u00adlection, subroutines, dynamic container types, and a extensi\u00adble calling \nconvention. Three key characteristics made adding futures to the Par\u00adrot VM machine relatively easy: \n1. an existing abstraction and wrapping of OS-level threads; 2. a concurrency or green thread implementation \nthat ab\u00adstracts and encapsulates thread of execution state; and 3. a pluggable runloop (i.e., interpreter \nloop) construct that allows switching between different interpreter cores.  With such groundwork in \nplace, the following enhancements to the Parrot C implementation were needed: refactoring of representation \nof threads to allow it to be reused for futures,  allowing an un.nished future computation to be com\u00adpleted \nby the main interpreter after a OS-level join, and  creating a new runloop that executes only safe (for \nparal\u00adlelism) operations and reverts back to the main thread for unsafe operations.  In Parrot, spawning \na future consists of creating a new interpreter and specifying what data, if any, to share between the \nparent and child OS-level threads. Parrot futures have their own execution stack, but they share the \nsame heap and bytecodes. To implement touch, Parrot waits for the future thread to return and then checks \nto see if the future returned a value, in which case the value is returned. If the future encountered \nan unsafe instruction, the future thread returns a computation, which is completed in the caller. Parrot \ns runloop is the core of the interpreter, where byte\u00adcodes are fetched and executed. Parrot has several \ndifferent runloops that provide debugging, execution tracing, pro.l\u00ading, and experimental dispatching. \nParallel futures adds a future runloop that checks each bytecode just before it is ex\u00adecuted to see if \nit is safe to execute or if the future needs to be suspended and executed sequentially in the main inter\u00adpreter. \nThis runtime safety checking includes argument type checking and bounds checking on container data structures. \nParrot is a highly dynamic virtual machine. Many byte\u00adcodes are actually virtual function calls on objects \nthat can be de.ned by the users. For example, Parrot has typical array get and set opcodes, but the opcode \nare translated into vir\u00adtual method calls on a wide variety of container object types. Some container \nobject types are a .xed size at construction Person hours Task expert non-expert General Steps Naive \nimplementation 6 40 Exploration and discovery - 480 Unsafe-operation handling 6 16 Blocked-future logging \n1 - Total General: 13 536 Implementation-speci.c Step s Thread-local variables 8 - Future-local allocation \n8 - Garbage-collection sync 6 - Thread-local performance 6 - Total Speci.c: 28 - Overall Total: 41 536 \nFigure 10. Racket implementation effort by task. Adding a release-quality futures implementation to Racket \nusing our approach required only one week of expert time, and one academic quarter of non-expert time. \n time; others grow dynamically. This indirection inherent in the bytecode makes compile-time safety \nchecking dif.cult. To compensate, the future runloop does checks at run time that an operand container \nobject is of the .xed-size variety (and thus safe to run in parallel), not a dynamically growing variant \n(which would not be safe to run in parallel). For our Parrot experiment, we stopped short of imple\u00admenting \nany operations as synchronized or implementing future-local allocation, and the Parrot future runloop \ntreats most opcodes as unsafe operations. Arithmetic, jump, and .xed-container access and update are \nthe only operations designated as safe enough to run a parallel benchmark and check that it scales with \nmultiple processors. Our experience working with the Parrot VM and Racket VM suggest that the work required \nto add those features to the Parrot VM would be very similar to the work we did to add them to the Racket \nVM and would achieve similar improvements in performance. 6. Evaluation Our evaluation supports the hypothesis \nthat our approach to incremental parallelization of existing, sequential runtime systems is (a) developer-ef.cient, \nand (b) leads to reason\u00adable performance gains for parallelized programs. Our eval\u00aduation of the .rst \nclaim is based on measurements of the overall amount of developer time (person-hours) invested in Racket \nand the Parrot VM implementation work. Our evalu\u00adation of the second claim is based on measuring both \nraw performance and scaling of several benchmarks, focusing primarily on the Racket implementation. Person \nhours Task expert General Steps Naive implementation 8 Exploration and discovery 24 Unsafe-operation \nhandling 8 Total General: 40 Implementation-speci.c Steps Wrapping OS-level threads 8 Future runloop \n4 Total Speci.c: 12 Overall Total: 52 Figure 11. Parrot implementation effort by task. Adding a proof-of-concept \nimplementation of futures to Parrot using our approach required only about a week of expert time. 6.1 \nDeveloper Effort is Low Because our approach was developed in the context of the Racket runtime system, \nour parallel Racket implementation is the most mature. Given that, one might expect to see substantial \ndevelopment effort; however, this is not the case. Figure 10 lists the overall development costs (in \nperson\u00adhours) required to apply our approach to Racket. Costs are partitioned into two categories: general \nand implementation\u00adspeci.c. The general category re.ects the .rst three steps described in Figure 5; \nthe implementation-speci.c category re.ects the incremental parallelization step, which includes work \nthat was necessary for Racket, but may not apply in other runtime adaptation work. The columns in Figure \n10 show the efforts of two de\u00advelopers, the expert being the designer and primary imple\u00admenter of the \nRacket runtime and the non-expert being a .rst-year graduate student (working in a different city). No\u00adtice \nthat with roughly one week of expert time, and one aca\u00addemic quarter of non-expert time, it was possible \nto apply our approach to a widely used,2 mature3 runtime system and achieve reasonable performance results. \nFurthermore, the ef\u00adfort produced a parallel futures implementation that is now a part of the main-line \nrelease of Racket. Having gained experience with our approach, we also applied it to the Parrot VM to \nensure our experience is not Racket-speci.c. Figure 11 lists the development time we required to add \na .rst-cut futures implementation. The Parrot effort was undertaken by a third-year Ph.D. student who, \nwhile not a core developer of Parrot, is intimately familiar with its internals. He also was familiar \nwith the Racket effort. The upshot of Figure 11 is that adding a proof\u00adof-concept implementation of futures \nto Parrot using our approach required only about a week of expert time. 2 The Racket distribution is \ndownloaded more than 300 times per day. 3 The runtime system has been in continuous development since \n1995. Penghu Cosmos OS OS X 10.6.2 CentOS 5.4 Processor Type Intel Xeon AMD Opteron 8350 Processors \n2 4 Total Cores 8 16 Clock Speed 2.8 GHz 2.0 GHz L2 Cache 12 MB 4x512 KB Memory 8 GB 16 GB Bus Speed \n1.6 GHz 1 GHz Racket 5.0 5.0 32-bit mode 64-bit mode GCC 4.2.1 (Apple) 4.1.2 (Red Hat) Java Java SE 1.6 \nOpenJDK 1.6 Figure 12. Machine con.gurations used for benchmarks  That the same approach has been applied \nsuccessfully and ef.ciently to two very different runtime systems suggests that it is quite general. \n 6.2 Testbed, Metrics, and Benchmarks We evaluated the performance of our futures implementa\u00adtions using \ntwo different machines and .ve benchmarks. We use the commonplace parallel systems performance metrics \nof strong scalability and raw performance, and we also com\u00adpare our results against the same algorithms \nimplemented in other languages. Machines We conducted performance evaluations on both a high\u00adend desktop \nworkstation with two quad-core processors (8 cores), and a mid-range server machine with four quad-core \nprocessors (16 cores). The detailed con.guration of these machines is given in Figure 12. During the \nexecution of a given benchmark, no other signi.cant load was placed on the machine. We veri.ed that separate \nthreads of execution used by the runtime system were in fact mapped to separate processing cores. Metrics \nThe number of threads used by each of our benchmarks is a runtime parameter. We measured the wall-clock \nexecution time of each benchmark as a function of this parameter, and we present both the raw numbers \nand a speedup curve. The speedup curve shows the wall-clock time of the parallel implementation using \na single thread divided by the wall\u00adclock time of the parallel implementation using the indicated number \nof threads. The problem size remains constant as we increase the number of threads; thus the speedup \ncurve measures strong scaling. For several benchmarks, we also measured the wall-clock time of sequential \nimplementations in various languages, including optimized C and Java. Program Implementation Microbenchmarks \n(self-developed) MV-Sparse Mergesort Signal Convolution Racket Racket Racket, Parrot NAS Parallel Benchmarks \n[4] Integer Sort Fourier Transform Racket, Java Racket, Java  Figure 13. Benchmarks, sources, and parallel \nimplementa\u00adtions. Sequential implementations in C, Java, Racket, and Parrot are also used for comparison. \nBenchmarks Figure 13 lists the benchmarks we used to evaluate the per\u00adformance of parallel futures in \nthe Racket and Parrot VM implementations. Several of the evaluation programs are not drawn from a particular \nbenchmark suite; rather, they are common implementations of well-known algorithms. Note that not all \nprograms have a Parrot VM implementation. Signal convolution is a signal-processing algorithm used to \ndetermine the output signal of a system given its impulse response or kernel, which de.nes how the system \nwill re\u00adspond given an impulse function applied to the input signal. For any input signal x, we can compute \neach value in the corresponding output signal y using the following equation: k nk \u00b7 hn-k y = x -k \nwhere k is time and h is the impulse response/kernel. Our implementation computes an output signal given \na one\u00addimensional input signal and kernel, both of which are made up of .oating-point values. We have \nimplemented signal convolution in sequential Racket, Racket with futures, sequential Parrot, Parrot with \nfutures, and sequential C. Mergesort sorts a vector of .oating-point numbers. We consider two variants \nof the mergesort algorithm, one that is readily parallelizable, and one that is not, but runs sig\u00adni.cantly \nfaster than the parallel version on one proces\u00adsor [16]. We implemented both of the algorithms in sequen\u00adtial \nRacket, Racket with futures, and sequential C. MV-Sparse does sparse matrix-vector multiplication us\u00ading \nthe compressed row format to store the matrix. For those unfamiliar with compressed row format, the essential \nidea is to .atten the matrix into a single 1D array and combine it with parallel 1D arrays indicating \nboth where rows begin and what the column indices are. We implemented MV-Sparse in sequential Racket, \nRacket with futures, and sequential C. Our Racket version employs the higher-level nested data par\u00adallel \nprimitive called a gather, which we have implemented using futures. The NAS Parallel Benchmarks [4] \nare a suite of bench\u00admarks derived from computational .uid dynamics applica\u00adtions of interest to NASA. \nThey are widely used in the paral\u00adlel systems community as application benchmarks. We con\u00adsider two of \nthem here. NAS Integer Sort (IS) sorts an array of integer keys where the range of key values is known \nat compile-time. Sorting is performed using the histogram-sort variant of the bucket sort algorithm. \nWe implemented NAS IS in sequen\u00adtial Racket and Racket with futures. We compare against the publicly \navailable sequential and parallel Java reference im\u00adplementations [13]. NAS Fourier Transform (FT) is \nthe computational ker\u00adnel of a 3-dimensional Fast Fourier Transform. Each itera\u00adtion performs three sets \nof one-dimensional FFT s (one per dimension). We implemented NAS FT in sequential Racket and Racket with \nfutures. We again compare against the pub\u00adlicly available sequential and parallel Java reference imple\u00admentations. \n 6.3 Performance is Reasonable Using futures implemented via our approach to incremen\u00adtally parallelizing \nexisting sequential runtime systems, it is possible to achieve both reasonable raw performance and good \nscaling for the benchmarks we tested. Racket Figure 14 shows running time and speedup curves for the \nRacket implementations of the three microbenchmarks listed in Figure 13. The results con.rm that using \nfutures, im\u00adplemented using our developer-ef.cient incremental paral\u00adlelization approach, it is feasible \nto achieve reasonable par\u00adallel performance on commodity desktops and servers, both in terms of raw performance \nand speedup. Though the Racket implementations are slower than the optimized C versions in the sequential \ncase, all three par\u00adallel Racket versions are able to yield better performance than sequential C after \nemploying relatively small numbers of processors (2 for both convolution and MV-sparse, and 6 for mergesort). \nThe parallel convolution implementation ex\u00adhibits good scaling through the maximum number of proces\u00adsors \navailable on both machine con.gurations, owing to the tight nested-loop structure of the algorithm, which \ninvolves only .oating-point computations. Here the parallel convolu\u00adtion is able to avoid slow-path exits \nby using Racket s .oat\u00ading point-speci.c primitives, as discussed in Section 2. The Racket benchmarks \nalso use unsafe versions of the arith\u00admetic and array indexing operations. Figure 15 shows running time \nand speedup curves for the Racket implementations for the NAS IS and FT benchmarks. We compare performance \nwith both sequential and parallel Java implementations. While sequential Racket implementations for these \nbench\u00admarks are considerably slower than the Java implementa\u00adtions, the parallel implementations scale \nbetter. However, this scaling does not allow us to catch up with the paral\u00adlel Java implementation in \nabsolute terms. We suspect that, especially in the case of the IS benchmark, this is due to the majority \nof work being performed in the parallel por\u00adtion of the benchmark being array accesses (e.g. vector-ref \nand vector-set! in Racket), operations, which are more heavily optimized in the Java runtime system. \nThe Racket with futures implementation of NAS FT, however, is able to outperform sequential Java after \n3 processors (on the Cos\u00admos machine con.guration), and generally exhibits similar scaling characteristics \nto the reference parallel Java imple\u00admentation. As with the self-developed benchmarks (Signal Convo\u00adlution, \nMergesort, and MV-Sparse), the results demonstrate that our approach to incremental parallelization of \nsequential runtimes can lead to reasonable parallel performance. Parrot We tested our prototype implementation \nof Parrot with fu\u00adtures using only the convolution benchmark. The results, as can be seen in Figure 16, \nare comparable to those we saw with Racket with futures in terms of speedup (compare to Figure 14(a)-(d)). \nThis is supportive of our claim that our approach leads to reasonable parallel performance. It is important \nto point out that the raw performance is not comparable, however. This is the result of our implementa\u00adtion \nbeing preliminary (contrast Figure 10 and Figure 11). More speci.cally, our current implementation is \nbased on a version of Parrot in which the JIT is in a state of .ux. Our benchmark results re.ect interpreted \nperformance without the JIT, and thus should be taken with a grain of salt. Our current implementation \nof futures in Parrot only supports op\u00aderations on unboxed .oating-point numbers. Caveats notwithstanding, \nour results for the Parrot with futures proof-of-concept implementation suggest that our approach can \nbe generalized to other high level language implementations. 7. Related Work Our work builds on the ideas \nof futures from MultiLisp [14], a parallel dialect of Scheme. Parallelism in MultiLisp is also expressed \nvia future. However, MultiLisp does not require an explicit touch on the part of the programmer. Instead, \ntouches are implicitly performed whenever the value of a future is needed. Also unlike our work, futures \nin Multilisp always execute in parallel, whereas ours only execute in parallel when it is safe (based \non the constraints of the runtime system). Many language communities face the problem of retro.tting \ntheir implementations to support parallelism. The typical approach is to allow arbitrary threads of computation \nto run in parallel, and to adapt the runtime system as neces\u00adsary. Some succeed in the transition through \nsubstantial re\u00adimplementation efforts; threads in the original Java 1.1 vir\u00ad Wall-clock time Speedup \nConvolution (a) Cosmos (b) Penghu (c) Cosmos (d) Penghu Mergesort  (e) Cosmos (f) Penghu (g) Cosmos \n(h) Penghu MV-Sparse  (i) Cosmos (j) Penghu (k) Cosmos (l) Penghu  Wall-clock time Speedup NAS Integer \nSort (a) Cosmos (b) Penghu (c) Cosmos (d) Penghu NAS Fourier Transform  (e) Cosmos (f) Penghu (g) Cosmos \n(h) Penghu tual machine where implemented as user threads, but many re-implementations of Java now support \nthreads that use hardware concurrency. Others succeed in the transition with the help of their language \ndesigns; Erlang and Haskell are prime examples of this category, where the purely functional nature of \nthe language (and much of the language implemen\u00adtation) made a transition to support for parallelism \neasier, through it required substantial effort [3, 15]. Finally, many continue to struggle with the transition; \nour own attempts to map Racket-level threads to OS-level threads failed due to the complexity of the \nruntime system, and frequent attempts to rid the main Python and Ruby implementations of the global interpreter \nlock (GIL) have generally failed [1, 17]. An attempt to support OS-level threads in OCaml has so far \nproduced an experimental system [7]. Our approach of introducing parallelism through con\u00adstrained futures \nis somewhat similar to letting external C code run outside the GIL (and therefore concurrently) in Python \nor Ruby. Instead of pushing parallelism to foreign li\u00adbraries, however, our approach draws parallelism \ninto a sub\u00adset of the language. Our approach is also similar to adding special-purpose parallel operators \nto a language, as in data\u00adparallel operations for Data Parallel Haskell [8]; instead of constraining \nthe set of parallel operations a priori, however, our approach allows us to gradually widen the parallelism \navailable through existing constructs. In adding support for parallelism to Racket, we hope to move toward \nthe kind of support for parallelism that is pro\u00advided by languages like NESL [5], X10 [10], Chapel [9], \nFortress [2], Manticore [12], and Cilk [6], which were all designed to parallelism from the start. Adding \nparallelism to a sequential run-time system is a different problem than designing a parallel language \nfrom scratch, but we take in\u00adspiration from designs that largely avoid viewing parallelism as concurrent \nthreads of arbitrary computation. Wall-clock time Speedup Convolution (a) Cosmos (b) Penghu (c) Cosmos \n(d) Penghu Figure 16. Raw performance and speedup of microbenchmarks using Parrot with futures: dotted \nline is sequential Parrot, dots indicate Parrot with futures. The solid lines in speedup graphs indicate \nideal speedup.  8. Conclusion We have proposed and described slow-path barricading,a new approach for \nincrementally parallelizing sequential run\u00adtime systems, and we have implemented and benchmarked it in \ntwo real-world runtime systems. In both cases, slow\u00adpath barricading allowed programs to safely take \nadvantage of all the processor cores available in commodity multicore machines. These efforts required \nmodest amounts of devel\u00adoper effort, and the scalability of parallel benchmarks that we wrote using the \nfutures implementations was surprisingly good. Using our approach, implementers can quickly pro\u00adduce \na mature parallel adaptation of an existing sequential runtime system. The key insight behind our approach \nis that in existing se\u00adquential runtime systems, fast-path operations are generally already safe to run \nin parallel, while slow-path ones may not be. Furthermore, we can use this decomposition to in\u00adstrument \nthe codebase to .nd and improve problematic fast\u00adpaths. Finally, the application developer interested \nin paral\u00adlelizing a part of a program is already likely using fast-path constructs for maximum sequential \nperformance, or should be. In essence, we leverage both the runtime system imple\u00admenters extensive efforts \nto optimize fast-path operations and the application developers use of these fast-path opera\u00adtions in \noptimized sequential code. In this paper, we focus on adding the low-level paral\u00adlel construct of futures \nto existing runtime systems. While rectly with futures (much like one can write them in a lower\u00adlevel \nlanguage with threads), we envision futures as only the starting point. We have already used futures \nin Racket to construct a prototype library of higher-level parallel con\u00adstructs, speci.cally nested data-parallel \nconstructs. We are currently working on expanding that library and determining which higher-level constructs \nneed to be implemented in the runtime system for performance reasons. Efforts are also in place to improve \nthe futures implementation in Racket, par\u00adticularly to enhance support for nested futures and schedul\u00ading/mapping \nof futures. Another effort involves the design and implementation of parallel constructs for NUMA archi\u00adtectures \nand distributed memory machines. We believe that our approach could be applied to a wide range of runtime \nsystems, and obvious possibilities are scripting languages such as Perl, Python, and Ruby, as well as \nJavaScript/ECMAScript. Acknowledgments We would like to thank Steven P. Tarzia for generously granting \nus access to Cosmos, allowing us to expand our measurements to 16 cores. Thanks also to Nikos Hardavellas \nfor discussions on the work in general and on multi-core architectures speci.cally. References [1] Python \ndesign note on threads, 2008. http: //www.python.org/doc/faq/library/ #can-t-we-get-rid-of-the-global-interpreter-lock. \n[2] ALLEN, E., CHASE, D., HALLETT, J., LUCHANGCO, V., MAESSEN, J., AND STEELE, G. The Fortress language \nspec\u00adi.cation. http://research.sun.com/projects/plrg/  it is clearly feasible to write to write parallel \nprograms di-fortress.pdf, 2005. [3] ARMSTRONG, J. A history of Erlang. In HOPL III: Pro\u00adceedings of \nthe third ACM SIGPLAN conference on History of programming languages (2007), pp. 6 1 6 26. [4] BAILEY, \nD., BARTON, J., LASINSKI, T., AND SIMON, H. The NAS parallel benchmarks. Tech. Rep. RNR-91-002, NASA \nAmes Research Center, August 1991. [5] BLELLOCH, G. Implementation of a portable nested data\u00adparallel \nlanguage. Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Program\u00adming \n(1993), 102 111. [6] BLUMOFE, R., JOERG, C., KUSZMAUL, B., LEISERSON, C., RANDALL, K., AND ZHOU, Y. Cilk: \nAn ef.cient mul\u00adtithreaded runtime system. ACM SIGPLAN Notices (1995), 207 216. [7] BOURGOIN, M., JONQUET, \nA., CHAILLOUX, E., CANOU, B., AND WANG, P. OCaml4Multicore, 2007. http://www. algo-prog.info/ocmc/web/. \n[8] CHAKRAVARTY, M. T., LESHCHINSKIY, R., JONES, S. P., KELLER, G., AND MARLOW, S. Data Parallel Haskell: \nA status report. Proceedings of the 2007 Workshop on Declara\u00adtive Aspects of Multicore Programming (2007), \n10 18. [9] CHAMBERLAIN, B. L., CALLAHAN, D., AND ZIMA, H. P. Parallel programming and the Chapel language. \nInternational Journal of High Performance Computing Applications (Au\u00adgust 2007), 291 312. [10] CHARLES, \nP., GROTHOFF, C., SARASWAT, V., DONAWA, C., KIELSTRA, A., EBCIOGLU, K., VON PRAUN, C., AND SARKER, V. \nX10: An object-oriented approach to non\u00aduniform cluster computing. Proceedings of the 20th Annual ACM \nSIGPLAN Conference on Object-Oriented Program\u00adming, Systems, Languages, and Applications (2005), 519 \n538. [11] FLATT, M., AND PLT. Reference: Racket. http://www. racket-lang.org/tr1/. [12] FLUET, M., RAINEY, \nM., REPPY, J., SHAW, A., AND XIAO, Y. Manticore: A heterogeneous parallel language. Proceed\u00adings of the \n2007 Workshop on Declarative Aspects of Multi\u00adcore Programming (2007), 37 44. [13] FRUMKIN, M., SCHULTZ, \nM., JIN, H., AND YAN, J. Im\u00adplementation of NAS parallel benchmarks in Java. Tech. Rep. NAS-02-009, Ames \nResearch Center, Moffett Field, CA, USA, 2002. [14] HALSTEAD, JR., R. H. MULTILISP: A language for concur\u00adrent \nsymbolic computation. ACM Transactions on Program\u00adming Languages and Systems (TOPLAS) (October 1985), \n501 538. [15] JONES, S., GORDON, A., AND FINNE, S. Concurrent Haskell. Conference Record of the 23rd \nACM SIGPLAN-SIGACT Symposium on Principles of Programming Lan\u00adguages (POPL 96) (January 1996), 295 308. \n[16] JUSZCZAK, C. Fast mergesort implementation based on half\u00adcopying merge algorithm, 2007. http://kicia.ift.uni. \nwroc.pl/algorytmy/mergesortpaper.pdf. [17] Thread state and the global interpreter lock. http://docs.python.org/c-api/init.html# \nthread-state-and-the-global-interpreter-lock, March 2010.   \n\t\t\t", "proc_id": "1869459", "abstract": "<p>Many language implementations, particularly for high-level and scripting languages, are based on carefully honed runtime systems that have an internally sequential execution model. Adding support for parallelism in the usual form -- as threads that run arbitrary code in parallel -- would require a major revision or even a rewrite to add safe and efficient locking and communication. We describe an alternative approach to incremental parallelization of runtime systems. This approach can be applied inexpensively to many sequential runtime systems, and we demonstrate its effectiveness in the Racket runtime system and Parrot virtual machine. Our evaluation assesses both the performance benefits and the developer effort needed to implement our approach. We find that incremental parallelization can provide useful, scalable parallelism on commodity multicore processors at a fraction of the effort required to implement conventional parallel threads.</p>", "authors": [{"name": "James Swaine", "author_profile_id": "81470655349", "affiliation": "Northwestern University, Chicago, IL, USA", "person_id": "P2354107", "email_address": "", "orcid_id": ""}, {"name": "Kevin Tew", "author_profile_id": "81100269625", "affiliation": "University of Utah, Salt Lake City, UT, USA", "person_id": "P2354108", "email_address": "", "orcid_id": ""}, {"name": "Peter Dinda", "author_profile_id": "81100591810", "affiliation": "Northwestern University, Chicago, IL, USA", "person_id": "P2354109", "email_address": "", "orcid_id": ""}, {"name": "Robert Bruce Findler", "author_profile_id": "81100028925", "affiliation": "Northwestern University, Chicago, IL, USA", "person_id": "P2354110", "email_address": "", "orcid_id": ""}, {"name": "Matthew Flatt", "author_profile_id": "81100490544", "affiliation": "University of Utah, Salt Lake City, UT, USA", "person_id": "P2354111", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1869459.1869507", "year": "2010", "article_id": "1869507", "conference": "OOPSLA", "title": "Back to the futures: incremental parallelization of existing sequential runtime systems", "url": "http://dl.acm.org/citation.cfm?id=1869507"}