{"article_publication_date": "10-17-2010", "fulltext": "\n Monitor Optimization via Stutter- Equivalent Loop Transformation Rahul Purandare, Matthew B. Dwyer, \nSebastian Elbaum Dept. of Computer Science and Engineering, University of Nebraska -Lincoln {rpuranda,dwyer,elbaum}@cse.unl.edu \nAbstract There has been signi.cant interest in equipping programs with runtime checks aimed at detecting \nerrors to improve fault detection during testing and in the .eld. Recent work in this area has studied \nmethods for ef.ciently monitoring a program execution s conformance to path property speci.\u00adcations, \ne.g., such as those captured by a .nite state automa\u00adton. These techniques show great promise, but their \nbroad applicability is hampered by the fact that for certain combi\u00adnations of programs and properties \nthe overhead of checking can slow the program down by up to 3500%. We have observed that, in many cases, \nthe overhead of runtime monitoring is due to the behavior of program loops. We present a general framework \nfor optimizing the mon\u00aditoring of loops relative to a property. This framework al\u00adlows monitors to process \na loop in constant-time rather than time that is proportional to the number of loop iterations. We present \nthe results of an empirical study that demonstrates that signi.cant overhead reduction that can be achieved \nby applying the framework to monitor properties of several large Java programs. Categories and Subject \nDescriptors D.2.5 [Software En\u00adgineering]: Testing and Debugging monitors, tracing General Terms Reliability, \nVeri.cation Keywords optimization, runtime monitoring, stutter invari\u00adance, loop transformation 1. Introduction \nThere is a rich history of applying static analysis techniques to check programs for conformance with \ncorrectness proper\u00adties that de.ne the proper sequencing of statements along a Permission to make digital \nor hard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. program path, e.g., [13, 18, 26, 29]. In recent \nyears, a com\u00adplementary line of research has developed that explores tech\u00adniques for monitoring the behavior \nof programs at runtime for conformance with such path properties, e.g., [1, 2, 11]. The hope is that \nthese dynamic techniques will overcome the scalability challenges that have held back the broad applica\u00adbility \nof the static techniques. If that can be achieved then programmers will have a much richer set of assertion-like \nchecks that they can embed into their programs to enhance bug .nding and to allow deployed systems to \ncatch and cor\u00adrect errors in the .eld [11, 17]. Assertions are predicates that are evaluated on the state \nof the program at a speci.c location [27] and modern lan\u00adguages, like Java, have evolved to enforce a \nnumber of asser\u00adtions, e.g., null dereference and array index bounds checks. To minimize the runtime \noverhead of checks for such state properties researchers have applied static analyses to deter\u00admine when \na check is guaranteed to succeed and then remove that check, e.g., [10, 23]. Following the same line \nof reasoning, researchers have attempted to apply static analyses targeting path properties to reduce \nthe cost of monitoring such properties at runtime [6, 7, 9, 14]. While these techniques have shown some \npromise in reducing monitoring costs, there remain relatively simple properties whose monitoring can \nincur overheads well in excess of 100%. The reason for this is simple it is relatively common for property \nmonitoring to instrument statements in loops and that instrumentation can execute millions of times. \nIn this paper, we leverage the observation that path prop\u00aderties are typically insensitive to the number \nof times that a loop iterates. In other words, a monitor must observe a few iterations of the loop in \norder to judge whether the program violates or satis.es the property, but it can effectively ignore the \nrest of the loop iterations. The contributions of this paper are threefold: 1) We build on the theory \nof stutter-invariance [21] to formalize a general framework for analyzing and transforming loops that \noffers the potential to signi.cantly reduce the overhead of runtime monitoring; (2) We de.ne an instance \nof that framework that realizes a common special case arising in Java programs and OOPSLA/SPLASH 10, \nOctober 17 21, 2010, Reno/Tahoe, Nevada, USA. implement a property-driven loop optimization; and (3) \nWe Copyright &#38;#169; 2010 ACM 978-1-4503-0203-6/10/10. . . $10.00 c  Figure 2. HasNext property, \nfit, as an FSA evaluate the effectiveness of that optimization on a collection of properties and programs \nthat are drawn from previous runtime monitoring work and that exhibit high monitoring overhead. The evaluation \nresults provide strong preliminary evidence of the potential of the technique for reducing cost of monitoring \nfor path property conformance. The next section provides an overview of our approach and highlights each \nof these contributions. 2. Overview We illustrate the principles of our approach by way of of an example. \nFigure 1 shows an excerpt of a method from the bloat Dacapo benchmark [4]. This is one of the 424 call \nsites within the bloat code base that creates an Iterator to process the contents of a collection. A \nwell-known, and widely studied [2, 5, 11], HasNext property of the Iterator interface is that a call \nto the hasNext() method returning true should precede each call to next(), which returns the next element. \nSuch a property, often referred to as a path property, object protocol or typestate property, can be \nformulated as a deterministic .nite state automaton (FSA). Figure 2 depicts the FSA this HasNext property, \nwhich we denote fit. In de.ning the property we use hasNextt to denote a call to hasNext() that returns \ntrue. The code in Figure 1 creates the iterator by a call to the iterator() method and the while loop \ncondition guards calls to next() in the loop body to ensure that they always follow hasNext(). This example \nis simple enough that by studying the code one can see that all possible sequences of calls on the iterator \nare consistent with the HasNext property. When calls occur in multiple methods or in complex control \n.ow constructs it becomes much more dif.cult to determine that a program is consistent with a property. \nIn recent years, a number of researchers [1, 2, 11, 20] have proposed the use of runtime monitors to \ncheck program executions for conformance with such properties. A simple such analysis would create an \ninstance of the property FSA on each call to iterator(), which would be de.ned as the property creation \nevent [11], and then for each relevant call on the Iterator instance the monitor would update the current \nFSA state based on the FSA s state transition function. This requires, of course, that all potentially \nrelevant calls be instrumented to generate symbols in the language of the FSA.1 There are two dominant \ncomponents of the runtime over\u00adhead introduced by monitoring: (1) the number of FSA instances needed \nand (2) the number of transitions sim\u00adulated for each instance. Except in rare circumstances, whose details \nare elided in Figure 1, every time method visitPhiStmt() is called an instance of fit is created and \nif the Collection iterates over has k elements, then 2k symbols are generated. The overhead adds up quickly. \nWhen using the default Dacapo input load for bloat, mon\u00aditoring for this property generates more than \n211 million symbols generated for nearly a million FSA instances. To reduce monitoring overhead while \npreserving the se\u00admantics of the original monitors, techniques such as Trace\u00admatches [2] and JavaMOP \n[11] heavily optimize the .rst overhead component, e.g., by eliminating FSA instances when they are no \nlonger needed. The second overhead com\u00adponent has been addressed using static analysis to remove instrumentation \nof statements that do not contribute to the monitoring problem [6, 9, 14]. Unfortunately, these tech\u00adniques \ndo not help much with bloat because most of its monitoring overhead is incurred in loops that generate \nmil\u00adlions of events that change the FSA state, like the one in Figure 1, and consequently the best reported \nresults in the literature still have unacceptable overhead 154% [11] and 258% [5]. An alternative dynamic \napproach to reduce the second source of overhead consists of inserting and removing in\u00adstrumentation \non the .y during program execution [1, 16]. Such optimization, however, requires potentially frequent \ndynamic re-writing of program code to insert and remove instrumentation, and it only applies when cyclic \npatterns of behavior are comprised of symbols that do not cause the property state to change. Again, \nthe code in Figure 1 would cause such techniques to perform very poorly, since on ev\u00adery iteration the \nloop body would be re-written twice and no occurrences of the statements whose instrumentation was removed \nwill ever be executed. 2.1 Our Approach Independent of whether the approach is static or dynamic, a common \nobjective across the existing body of work on prop\u00aderty monitoring is the determination of whether instrumen\u00adtation \nfor generating a symbol can be completely removed without impacting the monitoring results. We believe \nthat aiming for complete removal has limited optimization op\u00adportunities that might be applied to monitoring \ninstrumenta\u00adtion where a majority, but not all, of its executions can be removed. 1 In our presentation, \nwe use the terms symbol when discussing automata\u00adtheoretic concepts and observable when discussing program \nstatements that are related to a property; the term event is used in [11] to describe the same concept. \n public void visitPhiStmt ( final PhiStmt stmt ) {... final Iterator it = stmt.operands(). iterator(); \nouter : {inner : {public void visitPhiStmt ( while (it .hasNext()) { final PhiStmt stmt ) { // instrumentation: \nhasNext(it):t ... final Expr op = (Expr) it.next(); final Iterator it = // instrumentation: next(it) \n stmt.operands(). iterator (); if ( op instanceof VarExpr ) while (it .hasNext()) { if (op.def() != null \n) // instrumentation: hasNext(it):t phiRelatedUnion(op.def(), stmt.target()); final Expr op = (Expr) \nit.next(); break inner ; // instrumentation: next(it) } if ( op instanceof VarExpr ) break outer ; if \n(op.def() != null ) }phiRelatedUnion(op.def() , // uninstrumented remainder of loop stmt.target ()); \nwhile (it .hasNext()) {} final Expr op = (Expr) it.next(); } if ( op instanceof VarExpr ) if (op.def() \n!= null ) phiRelatedUnion(op.def(), stmt.target()); }}} Figure 1. Example from class SSAPRE: original \n(left) and transformed (right) Our technique leverages this realization by performing a semantics preserving \nprogram transformation to reduce the frequency of executing instrumentation in a loop. Our key insight \nis to calculate the loop iteration after which the behavior of any subsequent iteration is guaranteed \nto preserve the state of the monitored property at that iteration the remaining loop iterations are \nsaid to stutter relative to the property [21]. The technique has several desirable properties: (i) it \ndoes not require dynamic re-writing of the program; (ii) it accom\u00admodates complex behaviors within loop \nbodies; (iii) it con\u00adsiders each loop independently and thus scales well; and (iv) it preserves the semantics \nof runtime property monitoring. Consider the loop in Figure 1 and the HasNext property. Our analysis \ncalculates all possible sequences of symbols in the property alphabet that can be executed by a loop \niteration on the same object in this case it is simply the sequence hasNextt; next. Table 1 illustrates \nthe analysis of the .rst two loop itera\u00adtions for each possible state of fit on entry to the loop. When \nentering the loop in state 1, an execution of the loop body as\u00adsures that monitor transitions to state \n2 (after hasNextt) and then back to 1 (after next). Since subsequent loop iterations are guaranteed to \npreserve the state of this property, we can assert that from state 1 the loop stutters immediately on \nits .rst iteration; state identi.ers written on a black background indicate the point at which stuttering \nbegins. Whenever the monitor is in the err state the remainder of the program is trivially stuttering. \n When entering the loop in state 2, the .rst iteration transi\u00adtions the monitor to state 2 (via the self-loop \non hasNextt) and then to 1. Since state 1 was determined to immediately stutter, we can conclude that \nwhen initially entering the loop in state 2 the loop stutters on the second iteration. In fact, this \nis the longest iteration distance at which the loop stutters for any entry state. For the given code \nand property, the analysis has deter\u00admined that only the .rst loop iteration that generates the se\u00adquence \nhasNextt; next needs to be monitored since subse\u00adquent iterations cannot reveal anything new about that \nprop\u00aderty. Hence, only the .rst iteration of the loop needs to be instrumented and only that iteration \nwill introduce runtime overhead. Figure 1 illustrates, on the right, how the loop is transformed based \non the analysis results. Note that in this case it appears as if we have simply unrolled the loop one \ntime, but in general, the transformation must account for the fact that multiple loop iterations may \nbe performed that by\u00adpass the instrumented statements in the original loop. The correctness of our transformation \nrequires that instrumented statements be executed a speci.c number of times before falling through to \nthe uninstrumented loop. Clearly, this technique offers signi.cant potential for overhead reduction for \nruntime monitoring. The transformed loop in Figure 1 will always require processing of 2 sym\u00ad  Table \n1. Stutter distance for fit on hasNextt; next bols, rather than 2k, regardless of the size of the collection. \nIn the context of the bloat benchmark nearly all of the more than 900 thousand dynamic instances of Iterators \nare processed in similar loops, consequently, monitoring using our loop optimization will result in fewer \nthan 2.5 million, rather than 211 million, symbols that need processing a signi.cant overhead reduction. \nIn the next Section we formalize the key concepts be\u00adhind our transformation and establish a suf.cient \ncondition on loop transformation that if met ensures the preservation of runtime property checking. Following \nthat we detail, in Section 4, the design and implementation of our stutter\u00adequivalent loop transformation \nfor optimizing runtime mon\u00aditoring of Java programs. We then present, in Section 5, the results of an \nevaluation of the effectiveness of that optimiza\u00adtion. Sections 6 and 7 discuss related and future work, \nre\u00adspectively. 3. Monitor Preserving Program Transformation In this section, we de.ne the requirements \non program trans\u00adformations that ensure that they preserve soundness and completeness of runtime monitoring \nof path properties.2 We begin with a brief overview of path property monitoring. 3.1 Runtime Path Property \nMonitoring Path properties can be expressed in a variety of formalisms [15]. Regardless of the formalism, \ndevelopers de.ne proper\u00adties in terms of observations of a program s behavior. In gen\u00aderal, an observation \nis de.ned in terms of a program state\u00adment, such as a method call or return, coupled optionally with \na predicate de.ned over program variables. In our pre\u00adsentation, we abstract away from such details by \nassuming the de.nition of a property alphabet, S, which is a set of symbols that encode observations \nof program behavior that are relevant to a property. For run-time monitoring, the most common form of \npath property speci.cation used is a deterministic .nite state au\u00adtomaton (FSA) [19]. An FSA is a tuple \nf =(S, S, d, s0,A) where: S is a set of states, S is the alphabet of symbols, s0 . S is the initial state, \nA . S are the accepting states 2 The transformation can be unsound and incomplete in the presence of \nunchecked exceptions if certain conditions described in Section 4.4 are met. Henceforth, in this paper, \nwherever we use the terms sound and complete, we mean sound and complete in the absence of unchecked \nexceptions. and d : S \u00d7 S . S is the state transition function. We use .: S \u00d7 S+ . S to de.ne the composite \nstate transition for a sequence of symbols from S; we refer to such a sequence as a trace and denote \nit p. We lift the transition function from traces to sets of traces, ., and de.ne .(s, .) = {s'|.p . \n. : .(s, p)= s'}3, i.e., the set of states reached from s via any trace in .. We de.ne an error state \nas err . S such that \u00ac.p . S* : .(err,p) . A. A property de.nes a language L(f)= {p | p . S* . .(s0,p) \n. A}; for convenience we overload L so that L(r) denotes the language de.ned by regular expression r. \nFSA monitoring involves instrumenting a program to de\u00adtect each occurrence of an observation, a . S.A \nsimple runtime monitor stores the current state, sc . S, which is initially s0, and at each occurrence \nof an observation a, it updates the state to sc = d(sc,a) to track the progress of the FSA in recognizing \nthe trace of the program execution. We say that a program execution violates a property, f, if the generated \ntrace, p, ends in a non-accepting state, i.e., .(s0,p) . A; violations can be detected as soon as the \nmonitor enters an error state, i.e., sc = err. DEFINITION 3.1 (Monitor Correctness). A runtime moni\u00adtor \nfor property f observing execution trace p is sound if it reports a violation if p . L(f), and complete \nif it reports a violation only if p . L(f). Soundness guarantees that no observed violation will be missed, \nwhereas completeness guarantees that false reports of violations will not occur. The simple runtime monitoring \napproach described above is both sound and complete. 3.2 Stutter Equivalence Our objective is to reduce \nmonitoring overhead by trans\u00adforming a program that would generate a trace, p, to gener\u00adate a shorter \ntrace, p', such that the two traces are equivalent relative to the property, i.e., p . L(f) . p' . L(f). \nWe achieve this by building on Lamport s notion of stuttering equivalence [21], which he introduced as \na means of describ\u00ading desirable limitations on the expressive power of tempo\u00adral logics and which has \nbeen leveraged as a basis for partial order reductions in model checking [3]. A program execution de.nes \na sequence of concrete pro\u00adgram states, s = c0 ..., where ci records the values of pro\u00adgram variables, \ncall stacks, execution locations, etc. Reason\u00ading about sequences of states at runtime is prohibitively \nex\u00adpensive, so most existing research [2, 11, 20] instead reasons about a trace of observations, p = \na0 ..., where ai . S, generated by the program execution. We note that such an approach abstracts a concrete \nstate sequence to a sequence of property states, i.e., states of f, such that sf = s0,..., where si . \nS and d(si,p[i]) = si+1; we refer to the se\u00adquence of property states for a trace as states(p). 3 . inside \nthe set comprehension corresponds to the composite state transi\u00adtion for a sequence of symbols from S. \n An adjacent pair of states in states(p), i.e., si,si+1, is a stutter step if si = si+1; one can also \nde.ne stutter steps in terms of the observation in the trace that transitions from si, i.e., d(si,p[i]) \n= si+1 = si. Two traces are stutter equivalent if their state sequences differ only in stutter steps. \nSince we are concerned with runtime monitoring, we restrict our attention to stuttering equivalence of \n.nite traces by adapting the de.nition of Baier and Katoen [3, De.nition 7.86]. DEFINITION 3.2 (Stutter \nEquivalent Traces). Traces p and f p ' are stutter equivalent for a given property f, denoted p = p ', \nif there exists a .nite sequence s0s1 ...sn, where si . S, ++ + such that states(p) . L(s ...s) and states(p \n' ) . 0 s1 n +++ + L(s0 s ...s), where s is a sequence of one or more 1 ni occurrences of state si. In \nthis de.nition, we judge trace equivalence by relat\u00ading sequences of automaton states reached throughout \nthe trace. We characterize such sequences as regular expressions over state values, si. Such an expression \ngives rise to a lan\u00ad ++ guage, e.g., L(s ), de.ning a set of state sequences, e.g., 0 s1 {[s0,s1], [s0,s0,s1], \n[s0,s1,s1],...}, where each sequence satis.es the constraints of the expression, e.g., that one or more \ns0 precede one or more s1. Judging trace equivalence based on states is powerful as it permits traces \ninvolving different symbols to be judged equivalent, e.g., if f is de\u00ad.ned such that d(si,a)= d(si,b) \nthen two equivalent traces reaching state si may extend the trace by a and b, respec\u00adtively. For runtime \nmonitoring, this focus on states is appropriate since judgement about property violations is based solely \non property states. We note that property states, S, play the role of atomic propositions in our adaptation \nof traditional stuttering frameworks, e.g., [3, Chapter 7].  3.3 Phrase Stuttering A stutter step re.ects \nthe fact that an observation is irrelevant with respect to a property since it does not cause a state \nchange. We extend this notion to phrases, or sequences, of symbols, i.e., elements of S+, that do not \ncause a state change. A phrase, denoted a, is de.ned as a regular language over the observation alphabet. \nA sequence of states si,...,sj is a stutter step for phrase a if .p . L(a) : (.(si,p)= sj) . (si = sj). \nPhrase stuttering trace equivalence is concerned only with the dif\u00adferences among non-stuttering steps \nin the trace. DEFINITION 3.3 (Phrase-Stutter Equivalent Traces). Traces p and p ' are stutter equivalent \nfor phrase a and prop\u00ad a:f erty f, denoted p = p ', if there exists a .nite sequence s0s1 ...sn, where \nsi is a sequence of states in S, such that the following three conditions hold .pa . L(a) : .(si[1],pa)= \nsi[|si|] (1) states(p) . L(s+s+ ...s+) (2) 01 n states(p ' ) . L(s+s+ ...s+) (3) 01 n In the remainder \nof the paper, we only consider phrase stuttering since it generalizes the simpler stuttering de.ni\u00adtions \nin which the phrases are individual symbols in S. 3.4 Stutter Equivalent Loops and Programs Since a \nsigni.cant source of overhead in monitoring arises due to observations processed repeatedly in loops, \nwe start by focusing on de.ning loops that are guaranteed to be phrase stutter equivalent. A loop is \nan iterative structure in a program, i.e., every while, do and for statement in a Java program. A loop, \nl, may perform a wide a variety of different computations on any given iteration and consequently it \nmay generate a variety of different phrases. We de.ne the phrases that a loop body may generate as .(l) \nwhich can be encoded as a regular expression or an FSA. We note that in general it is possible that E \n. L(.(l)), i.e., a loop with observables may not execute them. A loop, l, stutters for property f if, \nregardless of the state on entry to the loop and the instance of the phrase generated by the loop iteration, \nthe iteration can be viewed as a stutter step, i.e., .s . S : .p . .(l) : .(s, p)= s. In our experience, \nit is uncommon for a loop to stutter immediately on the .rst iteration for a non-trivial property. As \nwe have observed in bloat, however, it is frequently the case that after the execution of several iterations \ninvolving observations the possible states of the monitor will converge to a set of states from which \nthe remainder of the loop iterations stutter. DEFINITION 3.4 (Loop Stutter Distance). A loop, l, stut\u00adters \nat distance d for property f if '' .s . S : .s . .(s, (.(l) -{E})d) : .(s, .(l)) = {s ' } Note that \nthis de.nition does not count iterations of the loop that do not execute any observations, i.e., we remove \nE from the set of loop phrases then take the d closure of the remaining phrases; we refer to these as \nnon-trivial iterations. We are interested in the minimum loop stutter distance which de.nes the number \nof non-trivial loop iterations after which the loop will stutter regardless of the property state on \nentry to the loop; henceforth when we refer to stutter distance we always mean the minimum such distance. \n.(l) de.nes the set of traces that a loop body may ex\u00adecute. To reason about the equivalence of loops, \nwe must relate possible execution paths through the loop to the ele\u00adments of .(l) in more detail. DEFINITION \n3.5 (Path Traces). Given a program fragment f, let paths(f) de.ne the set of input constraints that force \nexecution along each path through f. Given a constraint, c . paths(f), the execution of f on any input \nsatisfying c generates a trace p(f, c) for a given property f. We .rst consider the equivalence of fragments \ncomprised just of loops, then extend our notion of equivalence to pro\u00adgrams. PROPOSITION 3.1 (Stutter \nEquivalent Loops). Given a loop l that stutters at distance d for property f. Loops l and l ' are phrase \nstutter equivalent if they perform identical compu\u00adtations on the .rst d non-trivial iterations, and \non all sub\u00adsequent iterations l ' performs the same computation as l except that it generates no observations. \nPROOF 3.1. The fact that l and l ' perform the same compu\u00adtation, other than the generation of observations, \nmeans that paths(l)= paths(l ' ), yet for some constraints c . paths(l) p(l, c) may not be identical \nto p(l ' ,c). By De.nition 3.4, we can write p(l, c)= pdprest, where pd . L((.(l)-{E})d). In this case, \np(l ' ,c)= pd. Moreover from any state, s, of f on entry to the loop, .(s, pd) is a state from which \nany additional instances of a phrase of l is a stutter step; note that the phrase E produces a (trivial) \nstutter step. Thus, .(.(s, pd),prest) = .(.(s, pd),E) and .(l):f we conclude that .c . paths(l): p(l, \nc)= p(l ' ,c). D When a loop that iterates n times can be replaced by a stutter-equivalent loop, the \ncost of monitoring the loop is reduced from O(n) to O(d). In many cases d is very small and the cost \nof monitoring a loop is signi.cantly reduced and independent of the number of iterations of the loop. \nWhile we focus only on analyzing and transforming loops in this paper, stutter equivalence naturally \nextends to entire programs. We de.ne a loop partition of a trace p as a se\u00adquence of traces [p1,...,pn] \nsuch that p1 ...pn = p and where each pi consists of symbols that are either all gen\u00aderated within a \nsingle execution of a loop or are generated outside of any loop; note that a loop execution may be com\u00adprised \nof multiple iterations. Let l(p) be a function that maps p to the loop that generated all of the symbols \nin p or . if p was not generated by a loop. DEFINITION 3.6 (Stutter Equivalent Programs). Two pro\u00ad ' \ngrams p and p are stutter equivalent for a given property f, if for every execution path, c, each program \ngenerates a trace that can be loop partitioned into s =[p(p, c)1,...,p(p, c)n] and '' s ' =[p(p ,c)1,...,p(p \n,c)n] such that .1 =i = n : .(l(s[i])):f ((l(s[i]) = .)=. s[i]= s ' [i]) . (l(s[i]) = .)=. s ' [i]= s \n' [i]) Intuitively, this de.nition requires an exact equivalence of corresponding traces that do not \ninvolve loops and phrase stuttering equivalence for corresponding traces from loops. In the next section, \nwe present an algorithm that transforms a program to a phrase stutter equivalent by only transforming \nobservables that lie within loops that satisfy De.nition 3.4. Our focus on loops re.ects our insights \nabout the current dominant source of monitoring overhead. As monitoring be\u00adcomes more prevalent we expect \nthat other program struc\u00adtures, e.g., recursion, may lead to signi.cant overhead. We leave such extensions \nto future work. 4. Detecting and Transforming Stutter-equivalent Loops The framework presented in the \nprevious section is quite general, but we have found that focusing on even the sim\u00adplest instance of \nthe framework, where d =1, can yield sig\u00adni.cant reductions in the cost of monitoring real programs. \nWe present an analysis to detect whether a loop enjoys this property, which we term unit stuttering, \nand prove that it calculates a suf.cient condition for De.nition 3.4. We then present a program transformation \nthat eliminates instrumen\u00adtation from all but the .rst iteration of such loops thereby assuring that \nProposition 3.1 holds and that the resulting pro\u00adgram meets De.nition 3.6. 4.1 Static Analysis We begin \nwith a set of static analyses that calculate three properties that are necessary to ensure the soundness \nand completeness of the transformation process. These analyses assure that (1) all observables that can \nbe executed within a loop have been identi.ed, (2) those observables are related to a common set of objects, \nand (3) all paths involving those observables can be safely and ef.ciently approximated. The .rst of \nthese is assured by scanning the program for syntactic occurrences of observables. Standard techniques \nare used to construct a program call-graph. Methods are vis\u00adited in reverse topological order for further \nanalysis. For each method, an intra-procedural .ow-insensitive analysis checks for the existence of observables \ninside the method; the methods that do not have any observables are not ana\u00adlyzed further. For methods \nwith observables, an abstract syn\u00adtax tree like representation is constructed and it is analyzed to determine \nif any observables lie within loops. This anal\u00adysis is an ef.cient .ow-insensitive check within each \nloop body, since the analysis tool that we use directly provides an AST-like data structure. When loops \nwith observables are found in a method, an exceptional control .ow graph is built and additional analyses \nare run to assure the second and third properties. First, a must points-to analysis is calcu\u00adlated within \nthe scope of the method to determine whether all observable method calls within a loop, including the \nones that occur in the loop condition, involve the same object; for multi-object properties [25] we perform \nthe points-to analy\u00adsis for all of the object references related to the property ob\u00adservable. Second, \na side-effects analysis is performed within loop bodies to determine that they are free of statements \nthat reference any object involved in the property being checked. While restrictive, these conditions \nhelp to ensure the preservation of monitor correctness in our transformation process. Several of these \nlimitations can be relaxed, e.g., at the end of this section we describe how nested loops that contain \nmultiple receiver objects that satisfy certain condi\u00adtions can be transformed. In spite of these restrictions \nthat help to ensure the correctness of the transformation process, the analysis can still yield property \nmonitors that produce false positives and false negatives results for programs that use unchecked exceptions \nin certain ways we explain this in detail in Section 4.4.  4.2 Checking for Unit Stuttering We assume \nat this point that a loop l containing observables has been detected and determined to meet the conditions \nde\u00adscribed above. The .rst step in our process is to summarize the phrases that l may generate. We do \nthis by constructing an NFA from the control-.ow graph (CFG) of l s body. CFG nodes whose statements \ncontain an observable are mapped to an NFA transition labeled with a symbol for that observ\u00adable and \nall other nodes generate E-labelled transitions. In general, a loop s observables may be executed condi\u00adtionally \non an iteration. De.nition 3.4 requires that we only consider iterations that execute an observable. \nTo distinguish such iterations, we extend the NFA with an additional ini\u00adtial symbol, m, that marks the \nbeginning of each loop itera\u00adtion. The NFA is determinized and minimized to produce fl which compactly \nencodes the possible loop phrases. We note that since this process ignores the semantics of branch con\u00additions \nwithin the loop body, fl overapproximates the actual phrases that can be generated by loop executions. \nThe top of Figure 3 illustrates this construction for the loop on the left of Figure 1. The algorithm \nin Figure 4 checks whether a loop capable of generating phrases fl stutters at unit distance for a prop\u00aderty \nf. The algorithm operates by relating fl to a series of closely related variants of f. Those variants \nare constructed in lines 1 8, with their common elements de.ned in lines 1 6. Lines 1 3, de.ne a set \nof shadow states for each non\u00aderror state of f. Lines 4 5 extend the alphabet of f with the marker symbol, \nm, from fl and de.ne all states of f as ac\u00adcept. Line 6 de.nes the transition function for the variants \nby extending f s function with transitions from each state of f to its shadow on m and self-loop transitions \non m for all shadows. For each state in f, its transitions are mirrored by its shadow state. Figure 5 \nillustrates the shadow property automaton constructed for the property in Figure 2. The loop on lines \n7 11 considers each state of f and constructs a variant of the automaton with that state as the start \nstate. It forms the product of that automaton with fl and minimizes the product. Intuitively, the accept \nstates of  Figure 3. Loop phrase automaton construction UnitStutteringCheck(fl,f =(S, S, d, s0,A)) \n1 let S ' be a set of states, such that S ' n S = \u00d8 2 let R|S -{err}. S ' be a bijective map 3 let Ss \n= S . S ' 4 let Ss =S .{m} where m . Sl 5 let As = S 6 let ds = d. s.S-{err}{((s, m),R(s)), ((R(s),m),R(s))}.((R(s),a),d(s, \na)) a.S,s.S-{err} 7 for s . S do 8 let fs =(Ss, Ss,ds, s, As) 9 let fp = fl \u00d7 fs 10 minimize(fp) 11 \nif |Ap| =1 then return false 12 return true Figure 4. Checking a Loop for Unit Stuttering Distance. \nthe product de.ne sequences of symbols that are common to both the loop and a sub-sequence of a string \nof the property that begins in state s. If there is exactly one such state, then all of the strings in \nL(fl) drive fs to a single accept state. If that property holds when starting in all states of f, then \nthe loop stutters after its .rst non-trivial iteration. To illustrate, consider the loop and property \nautomata in Figures 3 and 5, respectively. There are two non-error states in f that may correspond to \nmonitor states on entry to the loop; the error state is trivially stuttering. Tracing the product fp \n= fl \u00d7 fs, starting with s0 = (1, 1) on the sequence of symbols m, hasNextt, next results in the se\u00adquence \nof states (1, 1), (2, 1 ' ), (3, 2), (1, 1) where (1, 1) .  Figure 5. Shadow property automaton Ap; \nall other traces fail to reach an accept state. Tracing fp, starting with s0 = (1, 2) on the sequence \nof sym\u00adbols m, hasNextt, next results in the sequence of states (1, 2), (2, 2 ' ), (3, 2), (1, 1); all \nother traces fail to reach an accept state. Thus, the resulting products con.rm that a sin\u00adgle accept \nstate is present in the minimized products for each start state of f. THEOREM 4.1 (Suf.cient Unit Stutter \nDistance Check). If the algorithm in Figure 4 returns true for loop l and prop\u00aderty f, then l stutters \nat distance 1 for f. PROOF 4.1. When d =1 De.nition 3.4 requires that .s . '' S : .s . .(s, .(l) -{E}) \n: .(s, .(l)) = {s ' }. Consider a single state s . S. The algorithm calculates .(s, .(l) -{E}) by forming \nthe product of fl, which over\u00adapproximates the phrases m.(l), and fs, with start state s. That product \naccepts every string that is accepted by both fs and fl. Accepting strings of fl correspond to a sequence \nof iter\u00adations of the loop. An initial sequence of k trivial iterations generates the string mk, which \ndrives fs into the shadow state of s a non-accepting state. The .rst non-trivial itera\u00adtion produces \na string ma, where a = E, which drives fs to state .(s, mkma) which by de.nition is an accepting state. \nConsequently, the product is guaranteed to have at least one accept state (s0, .(s, mkma)). A subsequent \nnon-trivial iteration, perhaps preceded by k ' trivial iterations, may produce a string m\u00df. If (s0, .(s, \nmkma)) = (s0, .(s, mkmamk! m\u00df) then the product has at least two accept states. Clearly, if this is the \ncase then the property reaches one state after the .rst non-trivial iteration and another state after \nthe second, thus it does not stutter at distance one. If the two states are equal, then there is a single \naccept state and the loop stutters at distance one. The product construction considers all possible loop \nit\u00aderations, i.e., all possible a and \u00df. Moreover, the loop at line 7 considers each state in turn varying \nfs appropriately, thereby enforcing the single product accept state test for all states of f. D We note \nthe algorithm in Figure 4 may return false for loops that stutter at distance one. We discuss the generaliza\u00adtion \nof the algorithm in Section 4.5. 4.3 Transforming Unit Stuttering Loops A loop that satis.es the unit \nstuttering condition de.ned above may be transformed to eliminate instrumentation for any iteration after \nthe .rst non-trivial iteration. To achieve this we must (1) insert a dynamic test to determine when a \nnon-trivial iteration has been executed and (2) when that test is true we must transfer control to an \nuninstrumented version of the loop. These must be achieved in a manner that preserves the semantics of \nthe loop. It is easy to determine when a non-trivial iteration has been executed simply detect when \nsome observable in the loop has been executed and exit the instrumented loop after the iteration completes. \nHowever, such an approach would incur overhead even on iterations of the instrumented loop that do not \nexecute any instrumentation. We have developed a transformation that completely avoids overhead on trivial \nloop iterations. The key to this approach is the calculation of a loop trailer for an observable. The \ntrailer for an observable statement, trailer(o), is de.ned by the control .ow sub\u00adgraph rooted at the \nstatement and ending at a loop backedge, continue, or break statement. We clone the trailer for each \nobservable statement, re-target selected branches within the trailer, and splice the trailer into the \nloop immediately after the observable. All continue statements that lie within a trailer are replaced \nwith break statements that transfer control to the succeeding uninstrumented loop. A loop backedge, i.e., \nthe fall through case at the end of a loop body, is also replaced with a similar break statement. Explicit \nbreak statements are retargeted to skip the succeeding uninstrumented loop. There may be many observables \nin a loop, but only a subset need to have trailers created for them. Speci.cally, for ' any pair of \nobservables o and o ', if o post-dominates o then only a trailer for o is needed. We collect all observables \nthat are not post-dominated by another into the set postDom. Figure 6 de.nes the steps in transforming \na unit stuttering loop l. Lines 1-3 create and populate a pair of nested blocks. Within the inner block \na clone of l, linst, is inserted followed by a break statement this skips execution of the successor \nloop when the .rst linst exits. The inner block and a clone of l are inserted into the outer block. Lines \n4-5 instrument all observables within linst; no in\u00adstrumentation is inserted into the other loop clone. \nThe loop in lines 6-10 handles each of the post-dominating observables in linst. The trailer for observable \nstatement o is calculated and spliced into linst between o and the next statement n. The trailer is then \nprocessed to replace all continue statements and backedges with break state\u00adments targeted at the inner \nblock, and all break statements are retargeted to exit the outer loop. T ransformLoop(l) 4.4 Supporting \nexceptions 1 linst = l.clone() 2 blockinner = block(linst, breakouter) 3 blockouter = block(blockinner, \nl.clone()) 4 for o . obs(linst) do 5 instrument(o) 6 for o . postDom . o . obs(linst) do 7 replace o, \nn with o, trailer(o),n 8 for s . trailer(o) do 9 if (s = continue) . (s = goto)). target(s)= entrylinst \nthen 10 replace s with break blockinner 11 else if s = break then 12 replace s with break blockouter \nFigure 6. Unit Stuttering Loop Transformation 1 2 3 while Cond1 do S1 if Cond2 then 1 2 3 outer : {inner \n: {while Cond1 do 4 S2 4 S1 5 o.a()* 5 if Cond2 then 6 S3 6 S2 7 o.b()* 7 o.a()* 8 S4 8 S3 9 S5 9 o.b()* \n10 S4 11 S5 12 break inner // if ends 13 S5 // while ends 14 break outer } // inner ends 15 while Cond1 \ndo 16 S1 17 if Cond2 then 18 S2 19 o.a() 20 S3 21 o.b() 22 S4 23 S5 } // while and outer end Figure 7. \nOriginal loop (left) and Transformed loop (right). Figure 7 shows the pseudo-code for a simple loop and \nits transformed version. The calls to methods a and b are ob\u00adservables. The * indicates that the call \nis instrumented. The trailer consisting of S4, S5 and the loop backedge is inserted after o.b(); it post-dominates \no.a(). The backedge has been converted to a break. Note that this transformed loop only incurs instrumentation \noverhead when it executes an observ\u00adable and upon completion of that iteration it transitions to the \nuninstrumented copy of the loop. Java provides checked as well as unchecked exceptions. Al\u00adthough the \nproblems associated with both types of excep\u00adtions are similar, the analysis handles them differently \nbe\u00adcause methods are not required to declare unchecked excep\u00adtions, and such exceptions may be thrown \nby many different statements within a program. For the purpose of the analysis, it only matters if a \nstate\u00adment that may throw an exception lies inside a loop that con\u00adtains at least one observable. Any \nother statement will not be considered in the loop analysis and transformation process, and hence cannot \naffect the soundness or completeness of the resulting property monitor. Moreover, a statement that lies \ninside a loop that is processed may impact soundness or completeness only if (1) there are at least two \nobservables in\u00adside the loop, and (2) the statement lies on a path within the loop that connects a pair \nof observables where that path does not include a loop back-edge. The second condition restricts consideration \nto paths arising in a single loop iteration. The occurrence of such a statement may introduce a new loop \nphrase, but that new phrase impacts the loop transfor\u00admation process only if it causes the failure of \nthe unit stut\u00adtering distance check. This depends on both the loop phrase and the property being monitored. \nWe illustrate how excep\u00adtions can impact the operation of our monitor optimization approach through examples \nin the remainder of this subsec\u00adtion, and conclude with a discussion of the limitations of our current \nanalysis implementation. 4.4.1 Checked exceptions Our analyses operate on exceptional CFGs for each method \nin the program that includes a loop containing property ob\u00adservables. These CFGs, in addition to edges \nthat represent normal control .ow, include an edge to represent the excep\u00adtional control .ow introduced \nby statements that are declared to throw an exception.4 A throw of a checked exception must be explicitly \ndeclared in Java. The loop phrase automaton construction process overap\u00adproximates the execution paths \nbased on this CFG which en\u00adsures that the loop phrases corresponding to the loop execu\u00adtions that throw \nexceptions are taken into account. Conse\u00adquently, the algorithm in Figure 4 that checks for unit stut\u00adtering \ndistance handles checked exceptions directly. Figure 8 shows a simple loop inside a try block that has \na statement which invokes method throwsException() and catches CheckedException, which is a subtype of \nException but not of RuntimeException in other words it is a checked exception. Here we consider the \ncase where the method may throw a checked exception. Suppose the property being moni\u00adtored is the HasNext \nproperty in Figure 2 for which both 4 Our discussion in this section is based on the exceptional CFG \nprovided by the Soot Java analysis framework [28] which we use in our implementation as discussed in \nSection 5 try { while (it .hasNext()) { x. throwsException (); it .next().doSomething(); ... } } catch \n(CheckedException e) { } Figure 8. Stutter distance checking in the presence of ex\u00adceptions. hasNextt \nand next are observables. The invocation of throwsException() lies on the path that connects the two \nobservables and the exceptional control .ow makes the loop phrase hasNextt possible; since the call to \nnext() may be bypassed. The loop phrase automaton construction process en\u00adsures that all possible loop \nphrases, i.e., the exception re\u00adlated phrase hasNextt and the non-exceptional phrase hasNextt; next, \nare taken into account. The unit stuttering check calculates that the loop does not have a unit stutter\u00ading \ndistance, due to the fact that the two phrases drive the the property automaton into distinct states, \nand hence this loop would not be transformed. Note that whether method doSomething() might throw an exception \nhas no impact on the loop transformation process since it does not lie on a back-edge free path connecting \ntwo observables.  4.4.2 Unchecked exceptions In Java, methods are not required to declare that they \nthrow unchecked exceptions. This makes construction of an ex\u00adceptional CFG a challenge. Many different \nJava statements, in addition to method invocation, may potentially throw unchecked exceptions. An extremely \nconservative analysis may overapproximate exceptional control .ow by including an exceptional edge at \nevery such statement. This approach, however, would make for a very imprecise CFG and signi.\u00adcantly degrade \nthe precision of any analysis performed using it. We adopt the approach used in recent work [6, 9] which \nhandles many common uses of unchecked exceptions. The exceptional CFG we use includes an exceptional \nedge when\u00adever the program includes a throw statement regardless of whether the exception thrown is checked \nor unchecked. In addition, it conservatively adds an exceptional .ow edge for every statement that may \nthrow an unchecked exception, if an appropriate catch target is speci.ed in the same method body. If \nneither of these cases hold, then our exceptional CFG may not model all possible exceptional transfers \nof control, since it does not determine whether method invo\u00adcations may throw an unchecked exception. \nIn general, if a transformed loop contains more than one observable and a statement that lies on a path \nbetween any two observables can throw an unchecked exception that is not declared and is not caught in \ncontaining method, then the property monitor may be in an incorrect state as a result of our transformation. \nIn this situation, our analysis is not guaranteed to produce property monitors that are sound and complete. \n We illustrate using the HasNext property and the exam\u00adple in Figure 8, but this time we consider the \ncase where throwsException() may throw an unchecked excep\u00adtion. By not modeling this unchecked exception, \nour ap\u00adproach would transform the loop. During any iteration af\u00adter the .rst one, if throwsException() \nactually throws an unchecked exception, then that exception would not be caught by the catch of CheckedException. \nIn this case, the loop exits and the program continues execution with the property monitor in state 2, \nsince the hasNextt observable has been processed. This is potentially problem\u00adatic because our unit stutter \ndistance check, which was mis\u00adinformed about exceptional control .ow, calculated that the loop is always \nexited in state 1. An incorrect property monitor state can give rise to ei\u00adther false or missing reports \nof property violations. Thus the treatment of undeclared unchecked exceptions means that our analysis \nand transformation approach may produce mon\u00aditors that are unsound and incomplete. In future work, we \nplan to investigate the use of additional analyses that focus on the set of unchecked exceptions that \nare explicitly caught in the program, but that are not declared as thrown by methods in the program. \nUnchecked exceptions that are not caught will abnormally terminate program exe\u00adcution thereby indicating \na failure that the developer should investigate. Unchecked exceptions that are caught may be problematic \nfor our analysis, but only if methods that throw them are called within loops that our analysis targets \nfor transformation. Information about the absence of such meth\u00adods within loops can be used to con.rm \nthe soundness and completeness of our monitor optimization. 4.4.3 Try-catch blocks inside loops The \npresence of exception throw statements and try-catch blocks containing observables inside of loops complicates \nthe detection of stuttering loops and their transformation. Moreover, the computation of trailers for \nobservable state\u00adments within try-catch blocks inside of loops is also complicated and we leave its general \ntreatment for future work. At present, even if a loop is determined to unit stutter, we do not transform \nit if it contains a try-catch block containing an observable statement.  4.5 Generalizations The approach \ndescribed in the preceding section handles a limited, but valuable set of special cases. In this section, \nwe describe several extensions that we have developed to gener\u00adalize the set of loops that are amenable \nto stutter-equivalent optimization. Supporting nested loops In certain cases, loop nesting can make \nstutter-equivalent transformation very complex. We identify two commonly occurring cases that can be \nsup\u00adported safely; in all other cases we do not transform the loop nest. We can distinguish each loop \nin the nest by its depth. When observable statements only occur at one depth in the loop nest we can \ntransform that loop to a stutter-equivalent. Any loops that are deeper in the nest are treated as any \nother code in our transformation, i.e., trailers are computed branches are retargeted. Any loops that \nare shallower remain unchanged and simply enclose the now transformed loop. When observables can occur \nat multiple depths, it must be the case that the objects involved at each depth are provably distinct. \nIf this condition is met, transformation can proceed from the inner-most loop with observables outwards. \nStuttering at d> 1 The algorithm in Figure 4 only treats a stutter distance of one. It can be generalized \nto arbitrary distance d by iterating the product construction in lines 9 10 until Ap reaches a .xed point. \nGeneralizing the loop transformation process to distance d requires the introduction of d copies of the \nloop where those loops are nested within blocks in a cascading fashion. For example, for loop l with \nd =2 one would produce: d0 : { d1 : { d2 : { l2 } l1 } l } where the trailers in the l2 copy of l break \nusing label d2 and transition to l1. The staging of loop copies serves to perform a count down of the \nnon-trivial loop iterations, until the original uninstrumented loop l is reached. Supporting distinct \nloop traces The algorithm in Figure 4 is too restrictive in that it requires all loop phrases, .(l), \nto lead to the same state, i.e., the second component of the single product accept state. This can be \nrelaxed by using the iterated product construction approach described above and, particularly, the Ap \n.xed point test. This approach for unit stutter distance checking requires two products to be constructed, \nthe .rst to generate an Ap and the second to con.rm that it is a .xed point. In addition, we observed \na high frequency of cases with unit distance. Hence, we favored the algorithm in Figure 4 which is more \nef.cient for the unit stuttering case. 5. Evaluation In this section we investigate the potential of \nthe proposed technique and assess its performance against existing moni\u00adtoring approaches. More speci.cally, \nwe wish to explore two questions: (RQ1) How effective is the optimization in reduc\u00ading the number of \nevents processed?, and (RQ2) How well does the optimization perform when the monitoring problem is scaled? \n 5.1 Artifacts To assess RQ1 we consulted [11] which reports data on the number of events generated and \nmonitoring overhead incurred when checking 6 .nite-state properties against 11 Dacapo [4] benchmarks. \nOf the 66 program-property pairs reported in Table 3 of [11], we systematically selected the ones that \nincurred overhead of greater than 25% when us\u00ading JavaMOP (the ones where further optimization seemed \nvaluable). This selection resulted in 4 pairs comprised of the combination of two programs, bloat and \npmd, and two prop\u00aderties, HasNext (shown in Figure 2) and FailSafeIter which checks that no call to update \na collection is made between any pair of calls to next on an iterator for that collection. The FailSafeIter \nproperty is a multi-object property and the analysis must keep track of events on both an iterator ob\u00adject \nand the collection being iterated over. The latter are gen\u00aderally spread across multiple methods making \nthem a chal\u00adlenge for the analysis. To expand the scope of our study, we selected an an ad\u00additional open \nsource artifact for which monitoring overhead promised to be high. JGraphT version 0.8.1 is a substantial \nlibrary, consisting of 172 classes, that is designed for manip\u00adulating large graphs. It exhibits a number \nof design features that promote reuse, con.gurability, and high-performance. JGraphT includes a load \ntest which we analyzed against both properties selected above. 5.2 Design The overhead of runtime monitoring, \nand the effectiveness of our optimization depend on the program, the property, and the input to the monitored \nprogram execution. There are some programs that never instantiate the type(s) that are related to the \nproperty overhead in this case is trivial. In other situations, only speci.c program inputs will give \nrise to signi.cant monitoring overhead. Finally, the details of the property encoding can give rise to \nvariation in overhead. For RQ1 we consider three monitoring treatments. The .rst treatment, original, \nconsists of no monitoring at all, serving as a baseline to assess the other approaches. The second treatment, \ncontrol, consists of inserting the property monitor into the application, representing the state of the \npractice and an upper bound on performance overhead. The third treatment, optimized, corresponds to applying \nour opti\u00admization technique, then inserting the property monitor after the program transformation. For \neach combination of program and property, we apply each of the three treatments to integrate the property \nmonitor into the program, and execute the program with its default input. This design results in the \ncollection of 6 observations (3 programs and 2 properties) per treatment, 18 for the whole study. For \nRQ2 we consider two dimensions of scalability: the program input size and the per-event monitor processing. \n For input size, we investigated the use of large Dacapo inputs, but found that those inputs are not \nsimply larger ver\u00adsions of the default input. They are structurally quite dif\u00adferent and thus one cannot \ncompare performance on default and large inputs directly to infer a technique s scalability. For jgrapht, \nwe are able to manipulate the size of the input while maintaining its structure. We use three input size \ntreatments: 0.25 times the default size, the default size, and 4 times the default size, where size is \nde.ned by the number of nodes and edges of the graph that serves as input to jgrapht. For monitor processing, \nwe used three treatments to vary the per-event processing in monitoring. The low cost monitor does the \nabsolute minimum processing and corresponds to the properties used in [11]. In reality, it is likely \nthat some in\u00adformation will be recorded during monitoring to aid in fault diagnosis and debugging. We \nuse our instrumented aspect as a representative of a high cost monitor. Finally, we consider a monitor \nthat records signi.cantly less information, but still permits the developer to identify the source locations \nthat in\u00adcur high overload. We refer to this as a medium cost monitor.  5.3 Measures Our primary measure \nis the number of events generated dur\u00ading the execution of a benchmark. This measure is valuable since \nit represents a platform independent measure of the work performed during monitoring. Reducing the number \nof events generated is guaranteed to reduce monitoring over\u00adhead. We also report the runtime overhead \nincurred due to monitoring. We consider this a secondary measure, since variations in the execution platform \ncan give rise to variation in measured overhead that is not due to our optimization. For example, when \ntwo measures of monitoring overhead are within 3% of one another the bene.t of the optimization is obscured; \ncomparing events permits that bene.t to be observed. So as to not con.ate these measures, we use two \nvariants of each property to collect data. To record overhead we use the exact same property encodings \nas were used in [11]. To record events, we customize the property to record the number of events during \na run and the source line of the program at which each event is generated. For a given program and property, \na subset of the program loops will involve events appearing in the property these are candidates for \nour optimization. We record the number of candidate loops that are transformed. This provides an indirect \nindication of the potential for our optimization to reduce monitoring costs across a range of input values; \nthe higher the percentage of transformed loops the larger the space of inputs our technique is effective \non.  5.4 Infrastructure The technique presented in Section 4 could be implemented in any number of compiler \ninfrastructures. It requires access to a high-level representation of the program, to ease the de\u00adtection \nand analysis of loops, and a low-level representation, to enable analyses of the semantics of instructions \nwithin the loops. We chose to prototype the analysis in Soot [28], since we were familiar with it and \nit is a rich analysis infras\u00adtructure, and used its Dava [24] decompilation framework to provide the \nhigh-level information we required. The Dava toolchain works in multiple phases. First, Soot processes \na .class .le to produce Jimple, a 3-address rep\u00adresentation of bytecodes, and call graph construction \nis per\u00adformed, using either class hierarchy analysis (CHA) or the more precise Spark [22] points-to analysis \nframework. We used Spark for bloat, but were forced to revert to CHA for pmd and jgrapht to control analysis \ntime. Second, Dava con\u00adstructs an AST-like representation of the program based on Grimp, another higher \nlevel representation of code. It is after this second step where our analyses is inserted. The trans\u00adformed \nloops are encoded in Dava s AST representation, with appropriately cloned versions of the underlying \nGrimp representation. The remainder of Dava involves analysis of the AST, optimization of its structure \nfor pretty printing, and generation of Java source. Dava was not designed to be used as a high-level \nprogram representation and, unlike the rest of Soot, it has not been actively maintained since 2006. \nThis presented two signif\u00adicant problems: (1) Dava includes signi.cant functionality for creating programmer \nfriendly decompiled output and this leads to poor runtime performance, and (2) Dava s evalua\u00adtion did \nnot include the Dacapo benchmarks and those pro\u00adgrams exposed numerous latent bugs in its implementation. \nThese problems impacted our study in two ways. First, bugs in Dava caused our transformation to fail, \nthrough no fault of our technique, and we had to detect and workaround those problems. We did this in \ntwo ways: (a) We instrumented our analysis and transformation implementa\u00adtions to log information about \nthe methods they processed so that we could detect when a Dava bug aborted that process. In such cases, \nwe simply output the original method leav\u00ading it untransformed. This means that limitations in Dava force \nus to underestimate the bene.t of our optimization. (b) For each program-property pair we manually inspected \nthe transformed program comparing it to the original program to ensure that the program semantics were \npreserved. Note that the transformation log allowed us to focus on just the meth\u00adods that were transformed, \nbut in some of the benchmarks there were hundreds of such methods. We also executed each transformed \nprogram and compared its output to the original program to con.rm that it computed the same result. Second, \nthe cost of running Dava and, especially, the ef\u00adforts we went to con.rm the correct operation of Dava \nand of the transformed programs made performing our experi\u00adments quite expensive. The combination of \nbloat and has-Next is representative of the cost of analyzing all of the pro\u00adgram property pairs. Running \nSoot s points-to and call-graph construction takes 150 seconds for this program, a necessary Program \nProperty Control events Optimized events ratio bloat HasNext FailSafeIter 211455226 98158809132 2466093 \n11367199 0.01 0.0001 pmd HasNext FailSafeIter 4457941 2261768 120399 72576 0.03 0.03 jgrapht HasNext \nFailSafeIter 16120016 8040006 80024 8040006 0.005 1.0 Table 2. Events processed during monitoring Program \nProperty Original time Control time Optimized time loops bloat 5.2 HasNext FailSafeIter 208% 3496% 67% \n797% 232/293 192/349 pmd 5.7 HasNext FailSafeIter 8.8% 3.5% 3.5% 3.5% 108/131 122/164 jgrapht 4.8 HasNext \nFailSafeIter 808% 594% 69% 594% 29/31 5/38 Table 3. Monitoring time and loops optimized precursor of \nour analysis which takes 45 seconds, includ\u00ading the processing related to the logs described above. The \nrest of Dava s execution involves constructing and optimiz\u00ading ASTs and generating Java code which takes \nabout 20 to 25 minutes. Auditing the log .les and working around Dava bugs takes approximately 2 hours \nper program-property pair. In the future work, we plan to shift our analysis to a framework that eliminates \nthe limitations we encountered with Dava which will allow us to signi.cantly scale our evaluation. We \nuse JavaMOP 2.0 as the mechanism for encoding properties and generating them in aspectJ. Those aspects \nare then woven, using ajc 1.6.8, and the resulting Java program is executed on an Opteron 250 running \nCentOS 5.2 and JVM 1.6.0 with 16 Gigabytes of memory. For the Dacapo benchmarks, we run them using the \n-converge option until the variation in execution time is less than 3%. We report the average execution \ntime and use it to compute runtime overhead. For jgrapht, we take a less rigorous approach. We run the \nprogram .ve times and then use the average execution time.  5.5 Results Table 2 reports, for each program-property \npair, the number of events generated by JavaMOP on the original program and on the optimized program. \nIt also reports the ratio of events reported under optimization to those reported without optimization. \nIn 5 of the 6 pairs, we observe reductions in generated events ranging from between 1 and 4 orders of \nmagnitude. The data for jgrapht-FailSafeIter indicates that the optimization was completely ineffective \nin this case; we discuss this in detail below. Table 3 reports the execution time of the original program \nexecution in seconds, with no monitoring, and the overhead of the control and optimized treatments. For \nthe optimized treatment, we also report the fraction of candidate loops that could be optimized. We observe \nthat in 4 out of 6 cases the use of optimization signi.cantly reduces overhead. It is interesting to \nnote that for pmd-FailSafeIter the over\u00adhead of optimized and control treatments are the same, de\u00adspite \na signi.cant reduction in events processed. We discuss this issue in detail below. As a .nal observation, \nwe see in Table 3 that the per\u00adformance of jgrapht-FailSafeIter was due to the fact that its execution \ndid not transit any of the 5 loops in the program that were transformed. We present a more detailed analysis \nof this case below. Table 4 reports on our limited study of scalability. The second and third columns \npresent data on how the number of events generated varies with input size. The event measure is independent \nof monitor processing, but overhead is not. The fourth column shows the execution time of the unmonitored \nprogram as input scales. The remaining columns report, for each monitoring cost treatment, how overhead \nvaries with input size. The event data indicates that the number of events scales linearly with input \nsize for the optimized program, whereas in the control treatment the number of events appears to grow \nexponentially with input size. For low cost monitors, the trend of reduced overhead appears to be somewhat \nobscured by measurement noise, but for higher cost monitors it is quite clear. The signi.cant reduction \nin execution time when using monitoring gives rise to a strong downward trend in overhead as input size \nscales; this is due in no small part to the fact that program execution time appears to be growing super-linearly. \n 5.6 Analysis and Discussion The data suggest that stutter-equivalent loop transformation can signi.cantly \nreduce the number of events generated dur\u00ading program monitoring. That reduction can yield signi.cant \nreductions in runtime overhead and, based on limited data, those reductions appear to increase as programs \nrun longer and when more realistic monitors that record data, e.g., for fault localization, are used. \n  Scaling events per-event monitor cost factor ctl. opt. orig. low med. high ctl. opt. ctl. opt. \nctl. opt. 0.25 4060016 40024 1.0 34.8% 18.9% 84.4% 44.6% 91.5% 41.1% 1.0 16120016 80024 4.6 15.5% 3.5% \n80.2% 21.4% 88.9% 22.7% 4.0 64240016 160024 29.2 7.7% 6.3% 70.6% 4.1% 83.7% 6.7% Table 4. Effects of \nscaling on jgrapht-HasNext While generally positive, these results point to the need for additional optimization \nof program monitoring if it is to become a technology that is widely deployed. In addition to the number \nof events generated, the number of objects that are monitored is a signi.cant source of overhead and \none that our optimization does not directly target. Our record\u00ading aspects provided data on the number \nof monitors cre\u00adated during monitoring. We found that bloat had upwards of 1 million, pmd had approximately \n58 thousand, and jgrapht approximately 25 thousand monitors generated for each pro\u00adgram run. Unlike processing \nan event, the time required to create a monitor is non-trivial and we conjecture that for pmd-FailSafeIter \nthe bulk of the overhead is comprised of monitor creation, thus obscuring differences related to re\u00adductions \nof generated events. Research targeting at reducing the number of monitors is sorely needed. Several \nresearchers have investigated object sampling techniques [1, 8] that make a randomized choice when object \ncreation events occur and either create a mon\u00aditor or not. These can dramatically reduce monitoring cost, \nbut they sacri.ce fault detection our event reduction opti\u00admization does not. There is an existing line \nof research that has explored the use of static analysis to reduce the cost of property monitor\u00ading. \nEric Bodden s doctoral thesis [5, 6], reports the results of a sequence of different static analysis \nculminating in a Nop-shadows analysis. We cannot directly compare to that analysis since is designed \nspeci.cally for reasoning about properties expressed as Tracematches [2]. We note, how\u00adever, that the \napplication of Nop-shadows to the four Dacapo program-property pairs that we studied led to modest reduc\u00adtions \n(see Table 5.6 in [5]) whereas we observed greater than 2 times overhead reduction on three of those \nfour programs. We observed that in the data we report the overhead of monitoring program-property pairs \nwith JavaMOP differs, in some cases signi.cantly, with the data reported in [11]. Our platform differs \nfrom the one they used in two signi.cant ways: we use linux and they used Windows and we use JavaMOP \n2.0 their latest release. We have shared our experimental results with the developers of JavaMOP to \ndetermine whether the performance differences are related to optimizations in their latest release or \nwhether some other factor is the cause. We will investigate this in the future. Of the 38 loops in jgrapht \nthat involve events in the FailSafeIter property our technique transformed 5. Unfortu\u00adnately the application \nwe studied only executed 4 of the 38 loops and none were one s that were transformed. Moreover, a single \nloop was responsible for generating over 8 million events 99.5% of the events generated during monitoring. \nThat loop is part of a custom iteration framework that sup\u00adports the ef.cient iteration over graphs using \ndifferent strate\u00adgies, e.g., breadth-.rst or depth-.rst. That framework imple\u00adments a next method that \ntests and updates internal collec\u00adtions to ensure that a vertex, or edge, is only visited once dur\u00ading \nan iterator traversal. The points-to analyses that support our optimization are unable to determine that \nthe updates to the internal collections are not updates to the collection be\u00ading iterated over and thus, \nthe loop is not transformed. A modest extension of the points-to analysis would allow this loop to be \ntransformed, but we refrained from implementing problem-driven extensions to our framework to better \nunder\u00adstand its strengths and weaknesses. We plan to explore two such extensions that we believe will \nallow our technique to perform signi.cantly better on properties like FailSafeIter and other properties \nthat follow the structure of a constrained-response pattern [15]. We will report our .ndings in the future. \nOur implementation currently handles only two com\u00admonly occurring special cases of nested loops. Moreover, \nit does not support transformation of loops when the ob\u00adservables within loops are associated with possibly \nmore than one receiver objects. This indicates that our results un\u00adderestimate the potential overhead \nreduction that could be achieved given a more comprehensive engineering of our analysis implementation. \nWe did not perform a detailed anal\u00adysis of the optimization opportunities that were lost due to this \nlimitation. However, we studied our log data for PMD-FailSafeIter and determined 19 nested loops were \nnot op\u00adtimized and 9 loops with multiple monitored objects were not optimized. Clearly, handling those \nloops would make it possible to achieve better overhead reduction, but for the Da\u00adcapo workloads those \n28 loops contributed a total of just 24 of the 2.2 million events produced when performing moni\u00adtoring; \nso for this program, property, and workload the lost optimization opportunity was negligible. We intentionally \nperformed a focused evaluation to target the most costly program-property pairs reported in the liter\u00adature, \nbut the scope of the evaluation is limited. Our .ndings should be interpreted as providing preliminary \nevidence that the number of events generated in runtime monitoring can be signi.cantly reduced through \nthe use of stutter-equivalent loop transformation. Followup studies that consider more programs, properties, \nand a diversity of program inputs must be performed to establish the breadth of applicability and ef\u00adfectiveness \nof the technique. 6. Related Work We focus here on the literature on runtime monitoring of path properties, \ni.e., sequences of observations, while ac\u00adknowledging that there is a rich literature on monitoring for \nstate property conformance, e.g., [12]. Path properties present a very different set of challenges to \nef.cient imple\u00admentation. Research on runtime path property monitoring has been very active over the \npast decade. A number of frameworks for monitoring properties of Java programs have been devel\u00adoped, \ne.g., Java-Mac [20], tracematches [2], JavaMOP [11], and QVM [1]. These techniques have developed multiple \nef\u00ad.cient implementation mechanisms for performing runtime monitoring on large numbers of objects. Much \nof that bene.t has been achieved by analyzing the property under analysis and is independent of the program \nunder analysis. Additional opportunities for optimization present themselves when both the program and \nproperty are analyzed together. Our work on residual analysis [14] investigated how the results of a \nstatic analysis for a path property can be lever\u00adaged to remove instrumentation that would otherwise \nbe needed for monitoring. As previously formulated residual analysis requires a whole-program typestate \nanalysis, such as the one implemented in [18], to be run on the program such analyses are known to scale \npoorly and be quite ex\u00adpensive. The cost of such analyses limit the application of residual analysis \nto large programs; for instance they cannot be applied cost-effectively to the programs we studied in \nthis paper. Bodden and colleagues have investigated a series of lighter weight static analysis designed \nto reduce runtime monitoring overhead [6, 9]. These techniques have proven effective in eliminating instrumentation \nfor certain program structures, but they have not been successful in optimiz\u00ading iterative behavior. \nIn fact, the inability of these analyses to mitigate loop-related monitoring overhead was one of the \nmotivations for our pursuing the stutter-equivalent loop transformation approach. In comparison to previous \nefforts that use static analysis to reduce the cost of runtime monitoring, the approach in this paper \nfocuses on iterative execution of instrumentation. This focus has three advantages: (1) it permits a \nmore scalable analysis since we only analyze methods with loops that contain observations relevant to \nthe property, (2) we can afford to employ more precise analyses at the method level since their scope \nis limited, and (3) as our study bears out, in many cases, the bulk of the monitoring overhead is due \nto instrumentation in loops. We have not studied the combination of the above opti\u00admizations, except \nfor the optimizations built into JavaMOP which we use in our study. Several of Bodden et al s analy\u00adses, \nresidual analyses, and other analyses [16] can effectively reduce instrumentation outside of loops and \nwe believe they can provide additional overhead reduction when combined with stutter-equivalent loop \ntransformation. 7. Conclusion In this paper we have introduced a general framework for reducing the overhead \nof performing runtime monitoring of programs with loops. The framework exploits the natural ab\u00adstraction \nof program traces that arises when monitoring path properties expressed as an automaton. More speci.cally, \nit detects when a loop is guaranteed to stutter relative to a prop\u00aderty and at that point it eliminates \nthe instrumentation in sub\u00adsequent loop iterations. In our evaluation, when applying an implementation \nof our unit-stuttering analysis and transfor\u00admation we observed orders of magnitude reductions in the \nnumber of events generated during monitoring and in many cases this led to signi.cant reductions in runtime \noverhead. We believe that the study of methods to co-optimize the performance of programs and their runtime \nmonitors holds promise for making runtime monitoring more widely de\u00adployable. In future work, we plan \nto generalize our frame\u00adwork along several dimensions, including those presented in Section 4.5 and develop \nsupport for overhead reduction of recursive program structures. In addition, we would like to perform \na comprehensive empirical study to understand the costs, bene.ts, and limitations of stutter-equivalent \ntransfor\u00admation across a broad range of programs and properties. We observed the presence of very large \nnumber of run\u00adtime monitors and believe that monitoring performance can be further improved if this number \ncan be controlled. In fu\u00adture, we plan to invest our efforts to develop techniques that achieve this \nreduction without sacri.cing much effective\u00adness. Acknowledgments This material is based in part upon \nwork supported by the National Science Foundation under Awards CCF-0541263, CNS-0720654, CCF-0915526, \nAir Force Of.ce of Scien\u00adti.c Research under Award #9550-09-1-0687, and by the National Aeronautics and \nSpace Administration under grant number NNX08AV20A. References [1] M. Arnold, M. Vechev, and E. Yahav. \nQvm: An ef.cient runtime for detecting defects in deployed systems. In Conf. on Obj. Oriented Prog. Sys. \nLang. and App., pages 143 162, 2008.  [2] P. Avgustinov, J. Tibble, and O. de Moor. Making trace monitors \nfeasible. In Conf. on Obj. Oriented Prog. Sys. Lang. and App., pages 589 608, 2007. [3] C. Baier and \nJ.-P. Katoen. Principles of Model Checking. MIT Press, 2009. [4] S. M. Blackburn, R. Garner, C. Hoffman, \nA. M. Khan, K. S. McKinley, R. Bentzur, A. Diwan, D. Feinberg, D. Frampton, S. Z. Guyer, M. Hirzel, A. \nHosking, M. Jump, H. Lee, J. E. B. Moss, A. Phansalkar, D. Stefanovi\u00b4 c, T. VanDrunen, D. von Dincklage, \nand B. Wiedermann. The DaCapo benchmarks: Java benchmarking development and analysis. In Proc. of the \n21st ACM SIGPLAN Conf. on Object-Oriented Programing, Systems, Languages, and Applications, pages 169 \n190, Oct. 2006. [5] E. Bodden. Verifying Finite-State Properties of Large-Scale Programs. PhD thesis, \nMcGill University, June 2009. [6] E. Bodden. Ef.cient hybrid typestate analysis by determining continuation-equivalent \nstates. In Int l. Conf. on Soft. Eng., 2010. [7] E. Bodden, L. Hendren, and O. Lhotak. A staged static \npro\u00adgram analysis to improve the performance of runtime monitor\u00ading. In Euro. Conf. on Obj. Oriented \nProg., pages 525 549, July 2007. [8] E. Bodden, L. J. Hendren, P. Lam, O. Lhot\u00b4ak, and N. A. Naeem. Collaborative \nruntime veri.cation with tracematches. In Works. on Runtime Verif., pages 22 37, March 2007. [9] E. Bodden, \nP. Lam, and L. Hendren. Finding programming errors earlier by evaluating runtime monitors ahead-of-time. \nIn Int l Symp on Found. of Soft. Eng., pages 36 47, New York, NY, USA, 2008. [10] R. Bod\u00b4ik, R. Gupta, \nand V. Sarkar. Abcd: eliminating array bounds checks on demand. In Proc. of the ACM SIGPLAN 2000 Conf. \nProg. lang. design and impl., pages 321 333, 2000. [11] F. Chen and G. Ros\u00b8u. Mop: an ef.cient and generic \nruntime veri.cation framework. In Conf. on Obj. Oriented Prog. Sys. Lang. and App., pages 569 588, 2007. \n[12] L. A. Clarke and D. S. Rosenblum. A historical perspective on runtime assertion checking in software \ndevelopment. SIG-SOFT Softw. Eng. Notes, 31(3):25 37, 2006. [13] M. Das, S. Lerner, and M. Seigle. Esp: \npath-sensitive pro\u00adgram veri.cation in polynomial time. In Proc. of the ACM SIGPLAN 2002 Conf. Prog. \nlang. design and impl., pages 57 68, 2002. [14] M. Dwyer and R. Purandare. Residual dynamic typestate \nanalysis. In Int l. Conf. on Aut. Soft. Eng., pages 124 133, 2007. [15] M. Dwyer, G. Avrunin, and J. \nCorbett. Patterns in property speci.cations for .nite-state veri.cation. In Int l. Conf. on Soft. Eng., \npages 411 420, May 1999. [16] M. Dwyer, A. Kinneer, and S. Elbaum. Adaptive online program analysis. \nIn Int l. Conf. on Soft. Eng., pages 220 229, 2007. [17] M. B. Dwyer, M. Diep, and S. Elbaum. Reducing \nthe cost of path property monitoring through sampling. In Int l. Conf. on Aut. Soft. Eng., pages 228 \n237, 2008. [18] S. J. Fink, E. Yahav, N. Dor, G. Ramalingam, and E. Geay. Effective typestate veri.cation \nin the presence of aliasing. ACM Trans. Softw. Eng. Methodol., 17(2):1 34, 2008. [19] J. E. Hopcroft \nand J. D. Ullman. Introduction to Automata Theory, Languages and Computation. Addison-Wesley, 1979. [20] \nM. Kim, M. Viswanathan, S. Kannan, I. Lee, and O. V. Sokol\u00adsky. Java-MaC: A run-time assurance approach \nfor Java pro\u00adgrams. Formal Meth. Sys. Design, 24(2):129 155, 2004. [21] L. Lamport. What good is temporal \nlogic? In IFIP Congress, pages 657 668, 1983. [22] O. Lhot\u00b4ak. Spark: A .exible points-to analysis framework \nfor Java. Master s thesis, McGill University, Dec 2002. [23] S. P. Midkiff, J. E. Moreira, and M. Snir. \nOptimizing array reference checking in java programs. IBM Syst. J., 37(3):409 453, 1998. [24] N. A. Naeem. \nProgrammer-friendly decompiled java. Mas\u00adter s thesis, McGill University, Aug 2006. [25] N. A. Naeem \nand O. Lhotak. Typestate-like analysis of mul\u00adtiple interacting objects. In Proceedings of the ACM SIG-PLAN \nconference on Object-oriented programming systems languages and applications, pages 347 366, 2008. [26] \nK. M. Olender and L. J. Osterweil. Cecil: A sequencing constraint language for automatic static analysis \ngeneration. IEEE Trans. Softw. Eng., 16(3):268 280, 1990. [27] D. S. Rosenblum. A practical approach \nto programming with assertions. IEEE Trans. Softw. Eng., 21(1):19 31, 1995. [28] Soot. http://www.sable.mcgill.ca/soot/. \n[29] R. E. Strom and S. Yemini. Typestate: A programming lan\u00adguage concept for enhancing software reliability. \nIEEE Trans. Softw. Eng., 12(1):157 171, 1986.   \n\t\t\t", "proc_id": "1869459", "abstract": "<p>There has been significant interest in equipping programs with runtime checks aimed at detecting errors to improve fault detection during testing and in the field. Recent work in this area has studied methods for efficiently monitoring a program execution's conformance to path property specifications, e.g., such as those captured by a finite state automaton. These techniques show great promise, but their broad applicability is hampered by the fact that for certain combinations of programs and properties the overhead of checking can slow the program down by up to 3500%.</p> <p>We have observed that, in many cases, the overhead of runtime monitoring is due to the behavior of program loops. We present a general framework for optimizing the monitoring of loops relative to a property. This framework allows monitors to process a loop in constant-time rather than time that is proportional to the number of loop iterations. We present the results of an empirical study that demonstrates that significant overhead reduction that can be achieved by applying the framework to monitor properties of several large Java programs.</p>", "authors": [{"name": "Rahul Purandare", "author_profile_id": "81330497442", "affiliation": "University of Nebraska - Lincoln, Lincoln, NE, USA", "person_id": "P2354060", "email_address": "", "orcid_id": ""}, {"name": "Matthew B. Dwyer", "author_profile_id": "81100013564", "affiliation": "University of Nebraska - Lincoln, Lincoln, NE, USA", "person_id": "P2354061", "email_address": "", "orcid_id": ""}, {"name": "Sebastian Elbaum", "author_profile_id": "81100007035", "affiliation": "University of Nebraska - Lincoln, Lincoln, NE, USA", "person_id": "P2354062", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1869459.1869483", "year": "2010", "article_id": "1869483", "conference": "OOPSLA", "title": "Monitor optimization via stutter-equivalent loop transformation", "url": "http://dl.acm.org/citation.cfm?id=1869483"}