{"article_publication_date": "10-17-2010", "fulltext": "\n Concurrency by Modularity: Design Patterns, a Case in Point Hridesh Rajan Steven M. Kautz Wayne Rowcliffe \nDept. of Computer Science Iowa State University 226 Atanasoff Hall, Ames, IA, 50010, USA {hridesh,smkautz,wrowclif}@iastate.edu \nAbstract General purpose object-oriented programs typically aren t embarrassingly parallel. For these \napplications, .nding enough concurrency remains a challenge in program de\u00adsign. To address this challenge, \nin the Pa\u00af\u00af nini project we are looking at reconciling concurrent program design goals with modular program \ndesign goals. The main idea is that if programmers improve the modularity of their programs they should \nget concurrency for free. In this work we de\u00adscribe one of our directions to reconcile these two goals \nby enhancing Gang-of-Four (GOF) object-oriented design pat\u00adterns. GOF patterns are commonly used to improve \nthe mod\u00adularity of object-oriented software. These patterns describe strategies to decouple components \nin design space and spec\u00adify how these components should interact. Our hypothesis is that if these patterns \nare enhanced to also decouple compo\u00adnents in execution space applying them will concomitantly improve \nthe design and potentially available concurrency in software systems. To evaluate our hypothesis we have \nstud\u00adied all 23 GOF patterns. For 18 patterns out of 23, our hy\u00adpothesis has held true. Another interesting \npreliminary result reported here is that for 17 out of these 18 studied patterns, concurrency and synchronization \nconcerns were completely encapsulated in our concurrent design pattern framework. Categories and Subject \nDescriptors D.2.10 [Software Engineering]: Design; D.1.5 [Programming Techniques]: Object-Oriented Programming; \nD.2.2 [Design Tools and Techniques]: Modules and interfaces, Object-oriented design methods; D.2.3 [Coding \nTools and Techniques]: Object-Oriented Programming; D.3.3 [Programming Languages]: Language Constructs \nand Features General Terms Design, Human Factors, Languages Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. Onward! 2010, October 17 21, 2010, Reno/Tahoe, \nNevada, USA. Copyright c &#38;#169; 2010 ACM 978-1-4503-0236-4/10/10. . . $5.00. Keywords Modularity, \nconcurrency, ease of program de\u00adsign, design patterns, synergistic decoupling 1. Introduction A direct \nresult of recent trends in hardware design towards multicore CPUs with hundreds of cores is that the \nneed for scalability of today s general-purpose programs can no longer be simply ful.lled by faster CPUs. \nRather, these pro\u00adgrams must now be designed to take advantage of the inher\u00adent concurrency in the underlying \ncomputational model. 1.1 The Problems and their Importance Scalability of general-purpose programs faces \ntwo major hurdles. A .rst and well-known hurdle is that writing cor\u00adrect and ef.cient concurrent programs \nhas remained a chal\u00adlenge [2, 22, 24, 32]. A second and less explored hurdle is that unlike in scienti.c \napplications, in general-purpose pro\u00adgrams potential concurrency isn t always obvious. We believe that \nboth these hurdles are in part due to a signi.cant shortcoming of the current concurrent language features \nor perhaps the design discipline that they promote. These features treat modular program design and concurrent \nprogram design as two separate and orthogonal goals. As a result, concurrent program design as a goal \nis often tackled at a level of abstraction lower than modular program design. Synchronization defects \narise when developers work at low abstraction levels and are not aware of the behavior at a higher level \nof abstraction [23]. This unawareness also limits potentially available concurrency in resulting programs. \nTo illustrate consider a picture viewer. The overall re\u00adquirement for this program is to display the \ninput picture raw, hereafter referred to as the display concern. To enhance the appearance of pictures \nthis program is also required to apply such transformations as red-eye reduction, sharpening, etc, to \nraw pictures, henceforth referred to as the processing concern. The listing in Figure 1 shows snippets \nfrom a sim\u00adple implementation of this picture viewer. This listing shows a GUI-related class Display \nthat is responsible for drawing borders, captions and displaying pictures. The key method of the picture \nviewer is show, which given a raw picture processes it, draws a border and  1 class Display { 2 Picture \npic = null; 3 void show(Picture raw) { 4 Picture tmp = crop(raw); 5 tmp = sharpen(raw, tmp); 6 pic = \nredeye(raw, tmp); 7 8 9 10  11 12 13 14 15 16 17 18 19 20 21 22 //Elided code for displaying border, \ncaption, and pic. 23 } Figure 1: A Picture Viewer and its two Tangled Concerns caption around it, and \ndraws the framed image on the screen. In this listing, lines 1-3, 7-10 and 22 implement the display concern \nand other lines implement the processing concern. The class Display thus tangles these two concerns. \nThe al\u00adgorithms for red eye reduction, sharpening and cropping an image are not relevant to this discourse \nand are omitted, al\u00adthough their basic ideas are summarized in Figure 1. In later .gures for this running \nexample, we omit these descriptions. 1.1.1 Improving Modularity of Picture Viewer To enhance the reusability \nand separate evolution of these components, it would be sensible to separate and modularize the implementation \nof the display concern and the process\u00ading concern. Driven by such modularity goals a programmer may \nseparate out the implementation of these two concerns. Such implementation is shown in Figure 2.  7 \ndisplayCaption(); 8 displayPicture(pic); 9 }} 22 Picture sharpen(Picture raw, Picture pic){...} 23 Picture \ncrop(Picture raw){...} 24 } Figure 2: Modularizing Viewer using Template Method The class Processor \nnow implements the processing concern using the Template Method design pattern [12]. It provides a new \ntemplate method process for its client [12]. This allows independent evolution of var\u00adious parts of the \nprocessing concern s implementation, e.g. a new algorithm for red eye reduction could be added without \nextensively modifying the clients. Concrete processing algorithms are implemented in the subclass BasicProcessor. \nThe class Display creates and uses an instance of BasicProcessor on lines 4 and 5 respec\u00adtively but remains \nindependent of its implementation.  1.1.2 Improving Concurrency of Picture Viewer On another day we \nmay want to enhance the responsiveness of the picture viewer. An approach to do that could be to render \nborders, captions, etc, concurrently with picture pro\u00adcessing, which may take a long time to .nish. This \nwould prevent such common nuisances as the frozen user inter\u00adface . Starting with the listing in Figure \n1, we could make picture processing concurrent using standard thread creation and synchronization discipline \nas shown in Figure 3. 1 class Display { 2 Picture pic = null; 3 void show(final Picture raw) { 4 Thread \nt = new Thread( 5 new Runnable(){ 6 void run() { 7 Picture tmp = crop(raw); 8 tmp = sharpen(raw,tmp); \n9 pic = redeye(raw,tmp); 10 } 11 }); 12 t.start(); 13 displayBorders(); // do other things 14 displayCaption(); \n// while rendering 15 try { t.join(); } 16 catch(InterruptedException e) { return;} 17 displayPicture(pic); \n18 } 19 Picture redeye(Picture raw, Picture pic){...} 20 Picture sharpen(Picture raw, Picture pic){...} \n21 Picture crop(Picture raw){...} 22 } Figure 3: Improving Responsiveness of Picture Viewer In this listing \nthe code for processing pictures is wrapped in a Runnable interface (lines 6, 10-11). An instance of \nthis class is then passed to a newly created thread (lines 4 and 5). This thread is then started on line \n12. As a result, picture processing may proceed concurrently with border and frame drawing (on lines \n13 and 14). Since it is possible that the thread drawing the border and caption may complete its task \nbefore the thread processing the picture or vice versa, synchronization code is added on lines 15 and \n16 to ensure that an attempt to draw the picture is made only when both picture processing and caption \ndrawing tasks are complete. Two hurdles mentioned before are apparent in this ex\u00adample. First, in this \nexplicitly concurrent implementation, synchronization problems can arise if the developer inadver\u00adtently \nomits the join code on lines 15-16 and/or incorrectly accesses the .eld pic creating data races. Second, \nfrom this solution it is not obvious whether there is any potential con\u00adcurrency between various algorithms \nfor processing pictures because the concurrent solution is essentially a boiler-plate adaptation of the \nsequential solution.  1.1.3 Similarities between Modularity and Concurrency Improvement Goals To address \nthe modularity goal as described in Section 1.1.1, we managed the explicit and implicit dependence between \nthe Display and Processor modules that decreases modularity. For example, instead of accessing the picture \n.eld of the class Display directly for sharpening and red eye reduction as in Figure 1, in Figure 2 this \ndependence is made explicit as part of the interface of method process. Similarly to address the concurrency \ngoal as described in Section 1.1.2, we managed the explicit and implicit depen\u00addence between the display-task \nand the picture-processing task that decreases parallelism. For example, we added the synchronization \ncode on lines 15 and 16 in Figure 3 and ex\u00adplicitly avoided data races between the processing thread \nand the display thread. It is surprising that even though the tasks necessary to explicitly address these \ngoals appear to be strikingly similar, we did not take advantage of this similarity in practice. The \nnet effect was that the modularity and concurrency goals were tackled mutually exclusively. Making progress \ntowards one goal did not naturally contribute towards the other.  1.2 Contributions to the State-of-the-Art \nThe goal of the P\u00afa\u00afnini project [25] is to explore whether modularity and concurrency goals can be reconciled. \nThis work, in particular, focuses on cases where programmers ap\u00adply GOF design patterns [12] to improve \nmodularity of their programs. GOF design patterns are design structures com\u00admonly occurring in and extracted \nfrom real object-oriented software systems. Thus the bene.ts observed in the context of these models \ncould be to some extent extrapolated to concurrency bene.ts that might be perceived in real systems that \nemploy these patterns. To that end, we are developing a concurrent design pat\u00adtern framework that provides \nimplicitly concurrent versions of GOF patterns for Java programs. These enhanced patterns decouple components \nin both design and execution space, si\u00admultaneously. Figure 4 shows an example of its use.  The listing \nin this .gure is adapted from that in Figure 2. The only change is the additional code on line 5, where \nthe asynchronous template method generator from our frame\u00adwork is used to create an asynchronous version \nof the basic picture processor. We also added a Java import statement to add our library to the program \nthat is not shown here. The rest of the picture viewer remains unchanged. 1.2.1 Hiding behind a Line \nof Code Brie.y, the asynchronous template method generator takes a template method interface (here Processor) \nand an instance of its concrete implementation (here p1, which is an instance of BasicProcessor), produces \nan asyn\u00adchronous concrete implementation of the template method automatically, and returns a new instance \nof this asyn\u00adchronous implementation. Creation of this asynchronous implementation involves several checks \nthat we will not discuss in detail here, how\u00adever, the basic idea is that this generator utilizes the \nwell\u00adknown protocol of the template design pattern to identify potentially concurrent tasks during the \nexecution of the tem\u00adplate method, dependencies between these tasks, and gen\u00aderates an implementation \nthat exposes this concurrency and implements the synchronization discipline to respect depen\u00addencies \nbetween the sub-tasks of the algorithm implemented using the template design pattern. As a result, the \nasynchronous template method instance returned on line 5 by our framework implements the inter\u00adface Processor \nand encapsulates p1. For each method in the interface Processor it provides a method that creates a task \nto run the corresponding sequential method concur\u00adrently with p1 as the receiver object. If the method \nin the interface has a non-void return type, then a proxy for return value is returned immediately. For \nexample, for the method crop the return type is Picture so the asynchronous ver\u00adsion or crop returns \na proxy object of type Picture. This proxy object encapsulates a Future for the concurrent task and imposes \nthe synchronization discipline behind the scene.  1.2.2 Software Engineering Bene.ts The bene.ts in \nease of implementation are quite visible by comparing two implementations in Figure 3 and Figure 4. Instead \nof explicitly creating and starting threads and writing out potentially complex code for synchronization \nbetween threads, our framework is used to replace the sequential template method instance with an asynchronous \ntemplate method instance on line 5 in Figure 4. Thus, much of the thread class code, spawning of threads, \nand synchronization code is eliminated, which reduces the potential of errors. Additional software engineering \nadvantages are in code evolution and maintenance. Imagine a case where the pro\u00adgram in Figure 3 evolves \nto a form where, inadvertently, additional code is inserted to change the argument raw be\u00adtween lines \n12 14. Such an inadvertent error, potentially cre\u00adating race conditions, would not be detected by a typical \nJava compiler. On the other hand, our framework automati\u00adcally checks and enforces isolation by suitable \ninitialization and cloning of objects to minimize (but not eliminate) object sharing between the Processor \nand the Display code.  This relieves the programmer from the burden of explic\u00aditly creating and maintaining \nthreads and managing locks and shared memory. Thus it avoids the burden of reasoning about the usage \nof locks. Incorrect use of locks may create safety problems and may degrade performance because ac\u00adquiring \nand releasing a lock has some overhead. Our concurrent design pattern framework also takes ad\u00advantage \nof the task execution facilities in the Java concur\u00adrency utilities (java.util.concurrent package), which \nmini\u00admizes the overhead of thread creation and excessive context switching. These bene.ts make our work \nan interesting point in the space of modular and concurrent program design. 2. Related Work There is \nan important body of prior work on patterns for par\u00adallel programming and distributed systems. The books \nby Mattson et al. on parallel patterns [20], by Lea on concur\u00adrent programming in Java [15], and by Schmidt \net al. on pat\u00adterns for distributed systems [29] are some well-known rep\u00adresentative examples. Mattson \net al. s work (as well as recent work of McCool [21]) describes methodologies for discover\u00ading and analyzing \nparallelism in an application and provides guidelines on how to structure a parallel application. Pro\u00adgrammers \nare responsible for deciding where concurrency can be introduced and for creating threads and using appro\u00adpriate \nlocking mechanisms. Lea [15] used a pattern-oriented approach to develop a set of concurrency utilities \nfor the Java language that were subsequently incorporated into the stan\u00addard libraries. These utilities \nprovide a substantial improve\u00adment over the primitive features originally included with the Java platform, \nbut the programmer must still identify poten\u00adtial concurrency and choose how to create and manage tasks \nand their synchronization mechanisms. Schmidt et al. [29] describe a set of patterns for struc\u00adturing \ndistributed (and thus potentially concurrent) appli\u00adcations and provides a reusable library of components, \nthe ACE framework. There is an intriguing analogy between dis\u00adtributed systems and systems that exploit \nconcurrency on a single host, for example, concepts such as messages, futures, completion tokens, and \nevent handlers can inspire designs for concurrent applications. However, it is worth emphasiz\u00ading that \nthe fundamental challenges are quite different, and a framework such as ACE is not generally useful for \nstructur\u00ading a concurrent application on a single host. In a distributed system, the cost of a remote \ncall is several orders of magni\u00adtude greater than the cost of a context switch on any given platform, \nso it is almost always bene.cial to perform remote calls asynchronously, it is usually easy to identify \nwhere re\u00admote calls occur, and there is no shared memory between the local and remote host that must \nbe protected with syn\u00adchronization locks. In contrast, for a concurrent system on a single host, it is \nnot generally obvious where to introduce concurrency and whether the performance bene.t of doing so will \noutweigh the overhead associated with task or thread creation and context switching, and the programmer \nis usu\u00adally faced with the dif.cult task of using locks to manage access to data shared between threads. \nThe most signi.cant difference between the approaches to parallel patterns discussed above and the current \nwork is that the former introduce and document idioms for constructing explicitly parallel applications, \nwhile we are proposing to exploit the existing use of well-known design idioms to automatically discover \nand expose potential im\u00adplicit concurrency. We believe that the training efforts to use our framework \nwill be minimal because OO programmers are typically already familiar with GOF patterns. Our work is \nphilosophically close to the Galois system by Kulkarni et al. [14], in which high-level abstractions \nare used to help the framework to discover potential implicit concurrency in certain classes of algorithms \nthat operate on data structures such as sets and graphs. Our concurrent iter\u00adators can be viewed as a \nsimple case of their approach; note that we also handle many additional cases besides iterators. Also \nrelated is our own work on the design of the Pa\u00af\u00af nini language, which has the same goals of reconciling \nmodu\u00adlarity and concurrency in program design [18]. Pa\u00af\u00af nini s de\u00adsign provides developers with asynchronous, \ntyped events, a new language feature that is useful for modularizing be\u00adhavioral design patterns. The \nadvantage of Pa\u00af\u00af nini s design over the present work is that its type system ensures that programs are \ndata race free, deadlock free and have a guar\u00adanteed sequential semantics. Furthermore, since Pa\u00af\u00af nini \nhas a dedicated compiler infrastructure it can provide a more ef\u00ad.cient implementation of many idioms, \nwhereas our current work will have to rely on runtime or compile-time code gen\u00aderation to implement some \nof these idioms. The advantage of the current work over Pa\u00af\u00af nini s language design is that it doesn \nt require programmers to change their compilers, in\u00adtegrated development environments, and to learn new \nlan\u00adguage features. An additional advantage is that we have also considered structural and creational \npattern in this work. This work is also closely related to and makes use of Doug Lea s Fork-join framework \n[16] implementation in Java and several concurrency utilities in Java 1.5 and 1.6 that share similar \ngoals. It is similar to the work on the Task Parallel Library (TPL) [17], TaskJava [9], Tame [13] and \nTasks [7] in that it also proposes means for improv\u00ading concurrency in program design. However, the underlying \nphilosophy of our work is signi.cantly different. While our work provides means to expose concurrency \nin programs via good modular design, the Task Parallel Library and related projects promote exposing \nconcurrency via explicit features. Many of the implementation ideas behind these projects can be used \nfor more ef.cient implementation of our concurrent  Design Pattern Potentially Concurrent Tasks Usage \nNotes  Abstract factory Product creation Use when creating expensive products, and when there are clear \ncreation and use phases. Builder Product component creation Use when creating expensive parts of a multi-part \nproduct. Factory method Product creation Use when creating expensive products, and when there are clear \ncreation and use phases. Prototype Prototype creation and usage Use when prototype is either readonly \nor temporarily immutable. Singleton \u00d7 No concurrency is generally available.  Adapter Multiple client \nrequests to adaptee Use only when Adapter properly encapsulates Adaptee s internal states. Bridge \u00d7 Potential \nconcurrency depends on the abstraction and the implementation. Composite Operations on parts of composite. \nUse when the order of applying operations on composites and leafs is irrelevant. Decorator Multiple decorations \nof a component Use when decorators implement independent functionality Facade Multiple client requests \nto facade Use only when facade s internal states are properly encapsulated from clients. Flyweight \u00d7 \nNo concurrency is generally available. Proxy Client processing and proxy request Use when proxy is indeed \nenabling logical separation.  Chain of responsibility Checking handler applicability Use when any handler \nmay apply with equal probability. Command Command creation and execution Use when only the command object \nencapsulates the receiver object s state. Interpreter Interpretation of subexpressions Use when expression \nevaluations are mostly independent. Iterator Operation on iterated components Use when operations are \nmostly independent (like parallel for and map operations). Mediator Mediator integration logic Use when \nmediators do not have recursive call backs Memento \u00d7 May be useful, when creating memento is expensive \n(See creational). Observer Observation by multiple observers Use with orthogonal observers that do not \nshare state with subjects/other observers. State \u00d7 No concurrency is generally available. Strategy When \nstrategy has independent steps Also see template method description. Visitor Visiting subtrees of an \nabstract syntax tree node Use when visits are not context-sensitive. Template method Steps of the algorithm \nimplemented as template Use when majority of these steps have little dependence. Figure 5: An Overview \nof GOF design patterns, shows possibly concurrent interactions between participants. design pattern framework, \nso in that sense they are comple\u00admentary to this work. Our work is also related to work on implicitly \nparal\u00adlel languages such as Jade [26], POOL [1], ABCL [33], Concurrent Smalltalk [34] BETA [30], Cilk \n[4, 10], and C. [2], though not related to explicitly concurrent ap\u00adproaches such as Grace [3], X10 [6], \nand deterministic par\u00adallel Java (DPJ) [5]. Unlike implicitly concurrent language\u00adbased approaches we \ndo not require programmers to learn new features and to change tools; however, our approach provides \nless stringent guarantees compared to language\u00adbased approaches. Lopes s D Language [19] has goals similar \nto ours. D aims to separate the concurrency concerns from the appli\u00adcation s concerns, whereas we aim \nto eliminate concurrency concerns altogether. Unlike D, we do not provide a general\u00adpurpose mechanism \nfor writing distributed programs; rather, we provide few specialized idioms based on GOF patterns. Moreover, \nas discussed previously, the fundamental chal\u00adlenges for distributed programming (which D targets) and \nconcurrent programming on a single host (which we target) are signi.cantly different. Dig et al. [8] \nhave proposed a tool that allows pro\u00adgrammers to refactor sequential code into concurrent code that uses \nJava utilities for concurrency. The advantage of their tool is that it does not require the use of annotations \nand can be used by programmers to convert existing code to use classes AtomicInteger, ConcurrentHashMap \nand FJTask in the Java concurrency library. Compared to their work, our pattern-based concurrency library \noperates at a higher-level of abstraction. We also encapsulate the usage of concurrency utilities in \nour library code. Some of our pattern library is modeled after the Future construct in Multilisp [27], \nand uses Java s current adoption of Futures along with the Fork-join framework [16]. Outline. The rest \nof this paper is organized as follows. In the next section, we describe the design and implementa\u00adtion \nof our concurrent design pattern framework using sev\u00aderal examples. Section 4 analyzes key software engineering \nproperties of our framework. Section 5 presents a prelimi\u00adnary performance evaluation. Section 6 concludes \nthe paper and outlines future directions for investigation. Framework Version. The examples presented \nin this pa\u00adper use version 0.1 of our design pattern framework. 3. Reconciling Modularity and Concurrency \nby Exploiting Protocols of GOF Patterns In Section 1, we have illustrated that with suitable disci\u00adpline \nand tools, improving modularity of a software system through the use of the template method design pattern \n[12] can immediately introduce concurrency bene.ts. To further study the extent to which modularity and \ncon\u00adcurrency goals can be treated as synergistic, we conducted an investigation into the remaining GOF \npatterns. For the cases in which the use of the pattern provides opportunities to introduce potential \nconcurrency, we provide utilities for a transformation of the pattern into a concurrency-friendly form \nalong with guidelines for recognizing when the trans\u00ad formation is applicable.  In all, we have suggested \ntransformations for the major\u00adity of the GOF patterns as summarized in Figure 5. A selec\u00adtion of these \nis discussed in detail below. Latest code for all the examples, as well as for the remaining patterns \nwe have adapted, can be found at the URL http://paninij. org. In the rest of this section, we discuss \nthe design and im\u00adplementation of our concurrency-enhanced framework start\u00ading with the key decisions \nin its design. 3.1 Overarching Design Decisions As noted in Section 1, one of our objectives is to .nd \nways to introduce concurrency in general-purpose applications with\u00adout burdening the developer with the \nlow-level details of synchronization. Another objective in adapting the GOF pat\u00adterns is to minimize \nthe impact on client code of using the implicitly concurrent versions of the patterns. 3.1.1 Lightweight \nBackend In the concurrent adaptations of design patterns we strenu\u00adously avoid the explicit creation \nof threads and the use of synchronization locks. In order to do so we take advantage of some library \nclasses in the package util.concurrent that allow us to think in terms of tasks rather than threads. \nAn Executor provides an abstraction of a task execution envi\u00adronment, and a Future is an abstraction \nof a handle for the result of executing a task [27]. Executors and Futures, along with related concrete \nclasses, were introduced in Java ver\u00adsion 5. The fork-join framework [16] is scheduled for release in \nJava version 7. The fork-join framework is an extremely lightweight con\u00adcurrent task execution framework \ndesigned to ef.ciently handle large numbers of small tasks with very few threads (typically the number \nof threads is the same as the num\u00adber of cores) [16]. It is ideal, in particular, for recursive or divide-and-conquer \nstyle algorithms, such as tree traversals. A task associated with a ForkJoinPool can be sched\u00aduled for \nconcurrent execution with a call to the fork() method, and the result is returned by a corresponding \ncall to join(), which does not return until the task is complete. The similarity in nomenclature to the \nfork() and join() system calls in Unix is only super.cial, however. A key feature is the ef.cient use \nof the underlying thread pool; the invocation of join() on a task, though it does not return until the \ntask is complete, does not actually block the calling thread the thread remains free to .nd other tasks \nto execute using a strategy called work-stealing. Likewise, invoking fork() on a new task does not necessarily \ntrigger a context switch; if no thread in the pool is available to execute the new task before the caller \ninvokes join(), the call to join() will simply cause the caller to directly execute the task. A question \nto be addressed in determining the applicabil\u00adity of the transformations we describe is whether the over\u00adhead \nof thread creation, context switching, and loss of local\u00adity will overwhelm the potential performance \ngains due to concurrency. The use of a lightweight execution framework mitigates some of this overhead, \nand is a step toward a more ideal situation in which the programmer describes the design using appropriate \nlanguage constructs and lets the compiler and runtime environment decide how to most ef.ciently ex\u00adecute \nthe necessary tasks. 3.1.2 Code Generation to Avoid Client Modi.cation A recurring question arising \nin concurrent programming is the following: suppose a method m() returns an object of type T, and we \nwish m() to run asynchronously, that is, to return immediately and allow the actual result to be pro\u00adduced \nin a separate thread. How does the caller eventually obtain the result? One answer is to return a Future \nthat serves as a placeholder for the result and provides an implicit syn\u00adchronization point; the caller \ncan perform other work until the result is actually needed, and then claim the Future, i.e., obtain the \nactual result by invoking a special method on the Future (such as the get() method of the Java implemen\u00adtations), \nblocking if necessary until the result is produced. Note that this approach requires a change in the \nreturn type of m() and requires the caller to explicitly claim the future in order to synchronize before \nusing the result. In order to minimize the impact on client code, we take a different approach. Assuming \nthat the type T is an interface, we can autogenerate a class that serves as a proxy for the result and \nthat also implements the interface T so that it can be used by the client just like the any other concrete \ntype that would normally be returned by m(). The proxy encapsulates a Future for the result that the \nclient never needs to explicitly claim; synchronization occurs implicitly when the client invokes one \nof the methods of T on the proxy object. An example of such a proxy is the Picture object returned on \nline 6 of Figure 4, and Figure 17 shows in pseudocode how such a proxy can be implemented. The current \nimplementation of the framework supports both static and dynamic mechanisms for the autogeneration of \ncode. If the participants of a GOF pattern are appropri\u00adately annotated using a set of annotations that \nwe de.ne, our annotation processor generates the required classes at compile time. If the annotations \nare not present, the required classes are dynamically generated, compiled, and loaded at runtime. Note \nthat in addition to the proxy objects discussed above, many other classes used by clients of the framework \nare generated automatically, for example, the asynchronous implementation of the template method class \nProcessor created on line 5 of Figure 4 is also an autogenerated class.  3.2 Chain of Responsibility \nPattern The intent of the chain of responsibility (COR) pattern is to decouple a set of components that \nraise requests and another set of components that may handle such requests (handlers). These handlers \nare typically organized in a chain. In some variations of this pattern other structures such as trees \ncan also be used for organizing handlers, which can be treated in exactly the same manner, so we omit \nthese variations here.  3.2.1 Search in an Address Book Application To illustrate our work on the chain \nof responsibility design pattern, we show snippets from an address book application. This application \nimplements functionality for a uni.ed search from one or more types of address books for a user. A user \nmay choose to add several address books of speci.ed types, e.g. an address book stored as an XML .le, \nCSV .le, Excel spreadsheet, relational database, or even third party services such as Google contacts. \nFurthermore, a user can order these address books from most preferred to least preferred. New address \nbooks can be added and preferences can be changed at runtime. Such change has an effect from the next \nsearch onwards. Once the address book is set up, it can be used to search for addresses by providing \nthe .rst name and the last name of the person. This search proceeds by .rst looking at the most preferred \naddress book. If the requested person is not found, the next preferred address book is searched, and \nso on. If the requested person s address is not found in the least preferred address book, a dialog box \nis displayed informing the user that the search has failed.  3.2.2 Modularizing Address Book Search \nA modular design of this application can be created by applying the chain of responsibility pattern. \nSuch a design would, for example, allow the application to support new type of address books without \nhaving to change other parts of the application. Furthermore, the ability to change the preference of \naddress books dynamically would be naturally supported in this design of the application. 1 interface \nRequest { } 2 class AddressRequest implements Request { 3 AddressRequest(String first, String last){ \n4 this.first = first; this.last = last; 5 } 6 String getLast() { return last; } 7 String getFirst() { \nreturn first; } 8 private String first, last; 9 } Figure 6: The Address Request To that end, the request \nclass is shown in Figure 6. It en\u00ad capsulates the .rst and the last name of the person being searched. \nThe abstract class Handler for all request han\u00ad dlers is shown in Figure 7. Since the overriding by subclasses \nis signi.cant here, we show modi.ers in the .gure. This class implements the standard chain of responsi\u00ad \nbility protocol on lines 7 14. Basically it checks whether the current handler can handle the request \nand if not tries to forward the request to its successor. If forwarding fails because the successor is \nnull, it throws an excep\u00ad tion on line 11. Clients interact with handlers by invoking 1 public abstract \nclass Handler <T extends Request,R> { 2 protected abstract boolean canHandle(T request); 3 protected \nabstract R doHandle(T request); 4 public Handler(Handler<T,R> successor){ 5 this.successor = successor; \n6 } 7 public final R handle(T request){ 8 if(this.canHandle(request)){ 9 return this.doHandle(request); \n 10 } else if (successor==null){ 11 throw new CORException(); 12 } 13 return successor.handle(request); \n14 } 15 private Handler<T,R> successor; 16 public final Handler<T,R> getSuccessor(){ 17 return successor; \n18 } 19 private Handler(){} 20 } Figure 7: The Abstract Request Handler the handle method and concrete \naddress books implement methods canHandle and doHandle. 1 class XMLHandler 2 extends Handler <AddressRequest,Address>{ \n3 boolean canHandle(AddressRequest r) { 4 return contains(r.getFirstname(),r.getLastname()); 5 } 6 Address \ndoHandle(AddressRequest r) { 7 return search(r.getFirstname(),r.getLastname()); 8 } 9 XMLHandler(Handler<AddressRequest,Address> \nsuccessor){ 10 super(successor); 11 initDB(\"AddressBook.xml\"); // Elided below. 12 } /* ... */ } Figure \n8: A Concrete Handler: XML Address Book A concrete handler is shown in Figure 8. On initialization this \nhandler reads its entries from an XML database. It also provides an implementation of the methods canHandle \nand doHandle that search the requested name in the database. If the name is found, the address is also \nobtained from the corresponding database tables Given a search request and several address books, search\u00ading \ninvolves sequentially invoking the canHandle method in the chain of address books until the address is \nfound. Each address book search is, however, independent of the others. Thus, to decrease the search \ntime, it would be sensible to try to make the searches concurrent.  3.2.3 Reaping Concurrency Bene.ts \nGiven a modular implementation of the address book, reaping the concurrency bene.ts is very easy using \nour adaptation of the chain of responsibility design pattern. The changed code for the concrete handler \nis shown be\u00adlow, which is now changed to inherit from the class CORHandler in our framework s library. \nThe library class CORHandler is similar to the abstract request handler discussed above but it also takes \nadvantage of the chain of responsibility protocol to expose potential concurrency. The method handle \nin this class traverses the chain of successors and creates a task for each handler  1 class XMLHandler \n2 extends CORHandler <AddressRequest,Address>{ 3 // Rest of the code same as before. 4 } Figure 9: A \nConcrete Handler: XML Address Book in the chain. This task runs the canHandle method for that handler. \nThis causes search tasks to run concurrently in our example. After these concurrent tasks are .nished, \nthe method doHandle is run with the .rst handler to return true as the receiver object. If canHandle \nmethod for no handler returns true an exception is thrown as speci.ed by the chain of responsibility \nprotocol. The library class CORHandler does use locks behind the scene, however, the application code \nremains free of any explicit concurrency constructs. Furthermore, no mod\u00ad i.cation is necessary for clients \nand minimal modi.cation is necessary for the handler classes. Thus, for the chain of re\u00ad sponsibility \npattern, applying our adapted version to improve modularity of an application results in concomitant \nconcur\u00ad rency in that application. For this pattern, modularity and concurrency goals appear to be synergistic. \n  3.3 Observer Design Pattern The observer pattern improves modularity of such concerns (observers) \nthat are coupled to another set of concerns (sub\u00ad jects) due to the fact that the functionality speci.ed \nby ob\u00ad servers happens in response to state changes in subjects. The intention of this pattern is to \ndecouple subjects from ob\u00ad servers so that they can evolve independently. A typical use of the observer \npattern relies on creating an abstraction event to represent state changes in subjects. Subjects explicitly \nannounce events. Observers register with subjects to receive event noti.cations. Upon a state change, \na subject implicitly invokes registered observers without de\u00ad pending on their names. 3.3.1 Value Computation \nin a Chess Application Our example for this section (snippets shown in Figures 10 and 11) is an application \nthat assists human players in a game of chess. It provides a model for the board (Board concern), a view \nfor displaying the current board position and for allowing users to make and undo a chess move (BoardUI \nconcern). A requirement for this application is to compute and show the value of each move (Value concern). \nThis value is computed using a min-max algorithm. This algorithm computes value of a move by searching \nthe game tree up to a given depth. The value concern is not central to the Board concern or BoardUI concerns. \nThus, it would be sensible to decouple the implementation of the value concern from the Board and BoardUI \nconcerns. This would, for example, allow other methods of computing the value of a move to be added to \nthe application or for the implementation of the value concern to be reused in other games. This decoupling \nis achieved using the observer design pattern.  3.3.2 Modularizing Value Computation The BoardUI concern \nin this example is implemented by the class Chess in this implementation. To decouple the Value concern \nand other similar observers, this class declares and explicitly announces an abstract event PieceMoved \n. 1 class Chess extends BoardUI implements MouseListener{ 2 void announcePieceMoved(Board b, Move m){ \n3 for(PieceMovedListener l : pmlisteners){ 4 l.notify(b, m); 5 } 6 } 7 //other irrelevant code elided. \n8 } 9 public interface PieceMovedListener { 10 void notify(Board b, Move m); 11 } Figure 10: Listener \nInterface and Subject Chess Board. As shown in Figure 10 the subject class Chess main\u00adtains a list of \nobservers (pmlisteners). All of these ob\u00adservers implement the interface PieceMovedListener shown on \nlines 9-11. This interface provides a single method notify with the changing board model (b) and move \n(m) as parameters. An event is announced by calling the method announcePieceMoved (lines 2-6), which \niterates over the list of registered observers and noti.es them of the event occurence by calling the \nmethod notify (line 4). 1 public class MinMax extends PieceMovedListener { 2 public MinMax(int d) { \nthis.depth = d; } 3 private int depth = 0; 4 void notify(Board bOrig, Move m) { 5 Board bNew = bOrig.getBoardWithMove(m); \n6 Piece p = bOrig.getPieceAt(m.getSource()); 7 boolean wMoved = p.isWhite(); 8 int val = minmax(bNew, \nwMoved, depth); 9 } 11 private int minmax(Board bOrig, boolean wMoved, int d){ 12 if(d==0) return computeValue(bOrig,wMoved); \n13 List<Board> nextLevel = nextBoards(bOrig,wMoved); 14 int val = 0; 15 for(Board bNext: nextLevel) * \n16 val += (-1 minmax(bNext, 17 return val; 18 } 19 // Other methods elided. 20 } !wMoved, d-1)); Figure \n11: Sequential Min-max Computation. As shown in Figure 11 the min-max algorithm is imple\u00admented as an \nobserver. The method notify of this class creates a new board with this move on line 5, computes whether \nthe white player moved on lines 6 and 7, and calls the recursive method minmax to compute this move s \nvalue. The modularity advantages of the observer design pattern are clear in this example. The BoardUI \nconcern modeled by the class Chess is not coupled with the Value concern mod\u00adeled by the class MinMax, \nwhich improves its reusability. Furthermore, class MinMax is also independent of the UI class Chess, \nwhich allows other potential implementations of the BoardUI concern to be used in the application without \naffecting the implementation of the Value concern.  A problem with this implementation strategy is that \ncom\u00adputation of the min-max value is computationally intensive. Thus, in the implementation above the \ndepth of the min-max game tree affects the responsiveness of the chess UI.  3.3.3 Reaping Concurrency \nBene.ts Fortunately this problem can be easily addressed with our concurrent adaptation of the observer \npattern as we show below. In the concurrent version of the observer pattern, the listener interface is \nimplemented as shown in Figure 12. 1 public abstract class PieceMovedListener 2 extends ConcurrentObserver<PieceMovedListener.Context> \n{ 3 public class Context { 4 public Context(final Board b, final Move m) { 5 this.b = b.clone(); 6 this.m \n= m.clone(); 7 } 8 protected Board b; 9 protected Move m; 10 } 11 void notify(Board b, Move m){ notify(new \nContext(b,m)); } 12 } Figure 12: Concurrent PieceMoved Listener Interface. Unlike its sequential counterpart, \nthis interface inher\u00adits from a library class ConcurrentObserver that we have provided to encapsulate \nthe concurrency concern. The class ConcurrentObserver takes a generic argument. This argument de.nes \nthe context available at the event and it de.nes the type of argument for an abstract method notify that \nclass ConcurrentObserver provides. The PieceMovedListener declares an inner class on lines 22-29, which \nencapsulates the changing board and the move. This class is used as the generic parameter for the li\u00adbrary \nclass ConcurrentObserver. The method notify on line 11 in Figure 12 calls a method of the same name de.ned \nin the library class. This library method enqueues this observer as a task and returns. 1 public class \nMinMax extends PieceMovedListener { 2 public MinMax(int d) { this.depth = d; } 3 private int depth = \n0; 4 void subNotify(Context c) { 5 Board b = c.ps.getBoardWithMove(c.m); 6 Piece p = c.ps.getPieceAt(c.m.getSource()); \n7 boolean wMoved = p.isWhite(); 8 int val = minmax(b, wMoved, depth); 9 } 10 /* minmax method same as \nbefore. */ Figure 13: Concurrent Min-max Computation. The implementation of observers only changes slightly. \nThis change is in the signature of the method notify, which must be renamed to subNotify as shown on \nline 4 in Figure 13. The only argument to this method is a Context as declared in Figure 12. So in the \nbody of this method, arguments must be explicitly accessed from the .elds of the argument c (on lines \n5 and 6). The implementation of subjects remain unaffected. For example, the concurrent version of the \nclass Chess is the same as in Figure 10. To summarize, the class ConcurrentObserver in our framework \nallows developers that are modularizing their object-oriented software using the observer pattern to \nmake execution of all observers concurrent. The use of our frame\u00adwork does not require any changes in \nsubjects and only mi\u00adnor modi.cations in observers. Furthermore, developers do not have to write any \ncode dealing with thread creation and synchronization. Rather they should simply ensure that sub\u00adjects \nand observers remain decoupled.  3.3.4 Applicability In descriptions of the observer pattern, two implementations \nare common, the push model and the pull model. In the former, the subject state is passed to the observer \nwith the notify method. In the latter, the observer must query the subject regarding the change in state. \nThe concurrent version we describe is only applicable to the push model. It should not be used in cases \nwhere the observers call back into the subject. Moreover, developers must ensure that the context (subject \nstate) passed to the observers is not modi.ed, or else that (as in the preceding example) the data in \nthe context object are properly cloned before noti.cation. The use of this adaptation of the pattern \nalso assumes that observers are independent of one another.  3.4 Abstract Factory Design Pattern The \nAbstract Factory pattern uses an interface for creating a family of related objects, called products, \nthat are them\u00adselves described as interfaces. At runtime, a system binds a concrete implementation of \na factory to create concrete instances of the products. The primary bene.t is to decou\u00adple the system \nfrom the details of specifying which products are created and how they are created; new behavior can \nbe introduced by instantiating a different concrete factory that produces a possibly different family \nof concrete products. 3.4.1 Image Carousel Using a Sequential Factory The example for this section is \nan application that displays a carousel (a scrollable sequence) of images obtained by applying a .xed \nset of possible convolution transformations to a given source image. Thus, the transformed images are \nthe products produced by the factory. Figure 14 shows the interfaces for the abstract factory ImageToolkit \nand the product TransformedImage. On starting the application, a concrete implementation of ImageToolkit \nis created and bound to the instance variable factory. The handler for a Load button then executes a \nsequence as shown in Figure 15.  1 // Abstract factory for transformed images 2 public interface ImageToolkit \n{ 3 TransformedImage createEmbossedImage(BufferedImage src); 4 TransformedImage createBrightImage(BufferedImage \nsrc); 5 TransformedImage createBlurredImage(BufferedImage src); 6 // other examples omitted 7 } 8 // \nProducts produced by the factory 9 public interface TransformedImage { 10 BufferedImage getImage(); \n11 BufferedImage getThumbnail(); 12 } Figure 14: Abstract Factory and Product Interfaces. 1 private \nImageToolkit factory = 2 new ConcreteConvolutedImageFactory(); 3 ... 4 ImageCarousel carousel = new ImageCarousel(); \n5 carousel.addImage(factory.createEmbossedImage(src)); 6 carousel.addImage(factory.createBrightImage(src)); \n7 carousel.addImage(factory.createBlurredImage(src)); Figure 15: Using the Concrete Factory.  3.4.2 \nImage Carousel Using a Concurrent Factory If the creation is computationally expensive, it makes sense \nto create products asynchronously. One reasonably clean way to do this is to explicitly create a task \nfor submission to an executor, which is an abstraction of a thread pool, and then use the returned Future \nas a handle for the product to be created. An example is shown in Figure 16 (in this example, as in those \nthat follow, we omit the handling of exceptions that may be thrown by the get() method). 1 ExecutorService \nexecutor = 2 Executors.newFixedThreadPool(1); 3 //... 4 Callable<TransformedImage> c = 5 new Callable<TransformedImage>{ \n6 public TransformedImage call() { 7 return factory.createEmbossedImage(image); 8 } 9 }; 10 Future<TransformedImage> \nfuture = executor.submit(c); 11 // possibly do other work 12 TransformedImage result = future.get(); \n13 carousel.addImage(result); Figure 16: Creating Products Using Explicit Tasks. The strategy shown in \nFigure 16 is as elegant as one can expect using standard libraries, yet still requires signi.cant modi.cation \nto the client code. In addition, the potential concurrency bene.t is only realized if the developer for \nthe client code remembers to place the invocation of get() just prior to the .rst use of the product, \nsince it is the call to get() that potentially blocks. We instead provide a utility that generates an \nasyn\u00adchronous wrapper for the factory itself. The AsyncFactory uses the class token for the factory interface, \nalong with the desired concrete factory implemen\u00ad tation, to generate the following: 1. An implementation \nof a proxy class for each product interface. The proxy encapsulates a Future representing an instance \nof the concrete product to which each method call is delegated. 2. An implementation of the abstract \nfactory interface, each method of which returns a proxy for the appropriate prod\u00aduct and initiates execution \nof the encapsulated Future to create the product instance.  The proxy object implements the product \ninterface and can be used by the client as usual, with one difference in behav\u00adior: the .rst time a method \nis invoked on it, the method may block if creation of the underlying concrete product is not yet complete. \nThe pseudo code for the proxy object is as shown in Figure 17. 1 class _AsyncProxy_TransformedImage \n2 implements TransformedImage { 3 private FutureTask<TransformedImage> task; 4 public _AsyncProxy_TransformedImage( \n5 final ImageToolkit factory, 6 Executor executor, 7 final BufferedImage image) { 8 Callable<TransformedImage> \nc = 9 new Callable<TransformedImage>() { 10 public TransformedImage call() { 11 return factory.createEmbossedImage(image); \n12 } 13 }; 14 executor.submit(c); 15 } 16 public BufferedImage getThumbnail() { 17 TransformedImage result \n= task.get(); 18 return result.getThumbnail(); 19 } 20 public BufferedImage getImage() { 21 TransformedImage \nresult = task.get(); 22 return result.getImage(); 23 } 24 } Figure 17: Example of Proxy for Concrete \nProduct. This autogenerated class essentially facilitates implicit synchronization without requiring \nmodi.cations in client code. In practice, we generate code that encourages just-in-time (JIT) compiler \nto inline methods such as getThumbnail in Figure 17 that signi.cantly reduces the overhead of this indirection. \nThe main advantage of this scheme is that no changes to the client code are required except for the creation \nof the factory. In this case, line 1 of Figure 15 would be replaced by the call shown in Figure 18. A \nsecond bene.t is that the proxy for the concrete product is obtained immediately without blocking. The \nget() method of the Future is only invoked upon the .rst attempt to call a method on the proxy. 1 private \nImageToolkit factory = 2 AsyncFactory.createAsyncFactory(ImageToolkit.class, 3 new ConcreteConvolutedImageFactory()); \n Figure 18: Creating the Asynchronous Factory. To summarize, the class AsyncFactory in our frame\u00adwork \nallows developers that are modularizing their object\u00adoriented software using the factory pattern to make \nproduct creation concurrent. The use of our framework does not re\u00adquire any changes in the code for abstract \nor concrete factory and only minor modi.cations to the clients that use this fac\u00adtory. Furthermore, developers \ndo not have to write any code dealing with thread creating and synchronization. Thus for this pattern \nas well our framework enables synergy between modularity and concurrency goals.  3.4.3 Applicability \nArguments passed in to the factory methods must not be modi.ed. Creational patterns such as Abstract \nFactory are good targets for introducing concurrency, since newly cre\u00adated objects are generally not \nsharing state.  3.5 Composite Pattern The Composite pattern is used to represent hierarchical structures \nin such a way that individual elements of the struc\u00adture and compositions of elements can be treated \nuniformly. Both individual and composite elements implement a com\u00admon interface representing one or more \noperations on the structure. A client can invoke one of the operations without knowledge of whether an \nobject is an individual or compos\u00adite element. Operations on composites typically involve traversing \nthe subtree rooted at some element to gather information about the structure. The value at a node often \ndepends on the values computed from child nodes but generally not on the values of sibling nodes, a fact \nwhich suggests an opportunity for concurrency. In this example we discuss an adaptation of the Compos\u00adite \npattern that supports concurrent traversals using the fork\u00adjoin framework. 3.5.1 A File Hierarchy as \na Sequential Composite A simple and familiar composite structure is a .le system; the individual elements \nare .les and the composite elements are directories. An example of such a structure is shown in Figure \n19. Performing an operation on the structure involves a re\u00adcursive traversal such as the getTotalSize() \nmethod in Figure 20.  3.5.2 A File Hierarchy as a Concurrent Composite To adapt the .le hierarchy structure \nfor concurrent opera\u00adtions we let the element types extend the generic library class ConcurrentComponent \nfrom our framework. This class implements the general mechanism for adding and remov\u00ading children along \nwith the method operation() shown in Figure 21, where Result and Arg are generic type pa\u00adrameters representing \na result type and argument type for the operation. The operation() method initiates the concur\u00adrent traversal \nby creating the initial task and submitting it to the ForkJoinPool for execution. 1 // Interface for \nall elements 2 interface FileSystemComponent{ 3 4 // An operation on the int sizeOperation(); structure \n6 7 8 9 10 // Child-related methods void add(FileSystemComponent compvoid remove(FileSystemComponent \ncFileSystemComponent getChild(int int getChildCount(); onent); omponent); i); 11 } 13 // Composite element \n14 class Directory implements FileSystemComponent{ 15 protected List<FileSystemComponent> children = \n... 17 // Directories have size 0 18 public int sizeOperation() { return 0; } 20 // Methods for adding \nand removing 21 // children, etc., not shown 22 } 24 // Leaf element 25 class File implements FileSystemComponent \n{ 26 protected int size; 28 public int sizeOperation() { return size; } 29 // Other methods not shown \n30 } Figure 19: Composite Elements and Individual Elements. 1 int getTotalSize(FileSystemComponent \nc){ 2 int size = c.sizeOperation(); 3 for (int i = 0; i < c.getChildCount(); i++){ 4 size += getTotalSize(c.getChild(i)); \n5 } 6 return size; 7 } Figure 20: Recursive Traversal of Composite Structure. Application-speci.c behavior \nis added by implementing the abstract methods shown in Figure 21. In particular, the sequentialOperation \nmethod represents the actual operation to be performed on leaf nodes, and the combine method determines \nhow the results of performing the opera\u00adtion on child nodes are assembled into a result for the parent \nnode. The ConcurrentComponentTask class is a subtype of RecursiveTask from the fork-join framework. The \nkey method is compute(), which is executed in the fork\u00adjoin thread pool and returns a result via the \njoin() method. For leaf nodes, the compute() method simply returns the value of sequentialOperation. \nFor composite nodes, a new ConcurrentComponentTask is created for each child, and the results are assembled \nusing the combine() method when they become available. The major details of the compute() method are \nshown in Figure 22.  3.5.3 Applicability The operation to be performed on the structure must be side-effect \nfree, since the actual order in which nodes are  1 public abstract class ConcurrentComponent <Arg,Result>{ \n3 private static ForkJoinPool pool = new ForkJoinPool(); 5 // Returns COMPOSITE or LEAF 6 protected abstract \nComponentType getKind(); 8 // Performs operation on a leaf 9 protected abstract Result sequentialOperation(Arg \nargs); 11 // Distributes args value for child nodes 12 protected abstract Arg[] split(Arg args); 14 \n// Assembles the results from child nodes 15 // into a result for the node 16 protected abstract Result \ncombine(List<Result> results); 18 // Performs the operation on this structure 19 public Result operation(Arg \nargs){ 20 ConcurrentComponentTask<Arg,Result> task = 21 new ConcurrentComponentTask<Arg,Result>(this, \nargs); 22 return pool.invoke(t); 23 } 25 // other details elided 26 } Figure 21: Abstract Methods of \nthe library class ConcurrentComponent. 1 protected Result compute(){ 2 if(component.getKind() == ComponentType.Leaf) \n3 return component.sequentialOperation(args); 5 Arg[] a = component.split(args); 6 ConcurrentComponentTask<Arg,Result>[] \ntasks = 7 new ConcurrentComponentTask[a.length]; 8 int i = 0; 9 for(ConcurrentComponent<Arg,Result> c: \n 10 component.components){ 11 tasks[i] = 12 new ConcurrentComponentTask<Arg,Result>(c, a[i]); 13 tasks[i].fork(); \n14 ++i; 15 } 17 List<Result> results = new ArrayList<Result>(); 18 for(ConcurrentComponentTask<Arg,Result> \nt : tasks){ 19 results.add(t.join()); 20 } 21 return component.combine(results); 22 } Figure 22: The \ncompute method for the task. visited is not deterministic. It follows that the argument to the operation() \nmust not be modi.ed. However, the re\u00adsult can enforce any desired ordering on the results obtained from \nchild nodes, since child tasks always complete before the execution of the combine() method. 4. Analysis \nof Framework s design So far we have shown that for several design patterns, our concurrent adaptation \nprovides synergistic modularity and concurrency bene.ts. It is important to note, however, in the absence \nof programming language-based extensions and compilers most of the correctness guarantees are dependent \nupon developers strictly following our design rules for ap\u00adplying the concurrent adaptation of a pattern. \nFor example, for observers that are not orthogonal to subjects and that share state with subjects or \nwith each other, use of the con\u00adcurrent observer pattern may lead to data races. In comple\u00admentary work, \nwe have also explored a language-based so\u00adlution to this problem [18]. However, developers unable to \nadopt new language features and those willing to follow our design rules carefully can still reap both \nmodularity and con\u00adcurrency bene.ts from our concurrent object-oriented pat\u00adtern framework. Figure 23 \nsummarizes the concurrent pattern adaptations in our framework and their impact on components and clients. \nAs indicated in Section 3.4, for the abstract fac\u00adtory pattern the only impact on client code is that \nat the point where the factory is instantiated, the factory must be wrapped by the AsyncFactory proxy \nclass. No changes to the component itself are required. The concurrency con\u00adcern is fully modularized \nin the library class, and the library class is fully reusable. These observations are summarized in the \n.rst line of Figure 23. For the other creational patterns Builder, Factory method, and Prototype the \nconclusions are similar; Template Method and Strategy are also imple\u00admented the same way. In the case \nof the structural patterns, Adapter, Facade, and Proxy are similar to the creational patterns in that \nthe only change required is that the client code use one of our framework library classes to wrap the \ninstance of the pat\u00adtern class. No changes are required for the component it\u00adself. For the Composite \npattern, in order to use the frame\u00adwork library the composite implementation classes must ex\u00adtend ConcurrentComposite \nas described in Section 3.5. This change, however, is transparent to the client code. The case for the \nDecorator pattern is essentially the same. In section 3.3 we described how the observer implemen\u00adtation \nclass must extend our library class, but that otherwise there is no impact on subjects and minimal impact \non ob\u00adservers (changing the name of one method). The same is true for the Chain of Responsibility, Command, \nInterpreter, Me\u00addiator, and Visitor patterns. Note that for a few of the patterns, there is an X in the \nReusable Pattern column even though the second column is checked. For the Interpreter pattern we cannot \nprovide a reusable library because it is impossible to know a priori in which cases subtrees can be evaluated \nconcurrently. For in\u00adstance, in the sample code we have an interpreter for regular expressions; while \nalternation subexpressions can be eval\u00aduated concurrently, sequence subexpressions cannot. Simi\u00adlarly, \nfor the Visitor pattern, the general scenario is that the methods of the visitor class correspond to \nthe concrete types in the hierarchy to be visited, and the visitor may handle each concrete type differently. \nTherefore, our library class can be used as a guide and will be adequate for simple cases, but in most \ncases it will have to be tailored to the speci.c hierarchy.  Design Pattern Modularized Concurrency \nConcern Reusable Pattern Impact on Client Code Impact on Component Code  Abstract factory v v Must wrap \nconcrete factory instance. None Builder v v Must wrap builder instance. None Factory method v v Must \nwrap factory instance. None Prototype Must wrap prototype instance. None Singleton \u00d7 \u00d7 N/A N/A  Adapter \nMust wrap adapter instance. None. Bridge \u00d7v \u00d7v N/A N/A Composite v v None Composite must extend library \nclass. Decorator v v None Decorator must extend library class. Facade Must wrap facade instance None \nFlyweight \u00d7v \u00d7v N/A N/A Proxy Clients must wrap proxy instance. None  Chain of responsibility v v None \nHandlers must extend library class. Command v None Command must extend library class. Interpreter v \u00d7v \nNone Concrete interpreter must extend library class. Iterator v Clients must provide iterative code in \na closure. None. Mediator \u00d7 No impact on colleagues. Mediator abstract class must extend library class. \nMemento \u00d7v \u00d7v N/A N/A Observer No impact on subjects. Observer abstract class must extend library class. \nState \u00d7v \u00d7v N/A N/A Strategy v Must wrap strategy instance. None Visitor v \u00d7v None Concrete visitor must \nextend library class. Template method Must wrap template method instance. None Figure 23: An Analysis \nof the Impact of Concurrent Design Pattern Framework on Program Code.  5. Preliminary Evaluation In \nthis section, we describe our initial evaluation of the concurrent pattern framework. So far we have \napplied our framework to four real world applications: Writer2LaTeX, a utility for converting OpenOf.ce \ndocuments to LaTeX; Adaptive Archiver, a utility for selecting and performing the best compression strategy \nfor an archive; Grader, a JUnit-based [11] automatic grading framework; and BiNA, a biomolecular network \nalignment toolkit [31]. For all of these applications, with fairly small and local changes we were able \nto expose implicit concurrency in their designs. For Writer2LaTeX we saw roughly 2X speedup using the \nimplicitly concurrent Command pattern, and for Adaptive Archiver, we observed approximately 4X speedup \nusing the implicitly concurrent Strategy pattern. In the re\u00admainder of this section we describe our efforts \nfor Grader and BiNA in greater detail. 5.1 Grader: JUnit-based Automated Grading Grader is built around \nthe JUnit framework [11] with facil\u00adities for assigning scores and generating feedback for pro\u00adgramming \nassignments based on the number of unit tests the code base passes. The results are then assembled into \na re\u00adport for students. Grader signi.cantly simpli.es the task of tallying points and indicating areas \nwhere the code both suc\u00adceeded and failed to perform as expected. The main pattern that we applied to \nGrader was the Builder pattern. This pattern is generally useful for construc\u00adtion of multi-part objects. \nIt allows construction algorithms for these parts to vary while providing clients a uniform in\u00adterface \nto construct the multi-part object. In the GOF illustra\u00adtion of this pattern [12] there are three main \nroles: a director component that is responsible for managing the correct se\u00adquence of creation steps \nalong with a builder interface that hides the details of concrete builder components. 5.1.1 Sequential \nGrade Book Builder The multi-part object constructed in this application is the grade report. It consists \nof sub-reports for groups of unit tests. The builder pattern plays a key role in this application as \nit allows tests that constitute the report to vary while providing a consistent way to compose test results \ninto a .nal report for students. Relevant parts from this application are shown in Figure 24. We elide \nirrelevant details to focus on the usage of the builder pattern in this application. 2 public interface \nGrader { //Builder 3 public GradeReport grade(); 4 } 6 public class GraderSet { //Director 7 private \nList<Grader> tests; 8 // Other details elided. 9 public void go() { 10 GradeReport[] reports = new GradeReport[tests.size()]; \n11 for(int i = 0; i < reports.length; i++) { 12 reports[i] = tests.get(i).grade(); 13 } 14 // Other details \nelided .. 15 } 16 /* Other details elided. */ } Figure 24: The Sequential Grade Report Builder The interface \nGrader in Figure 24 plays the role of builder in this application and the class GraderSet plays the role \nof director. Concrete builder instances are contained in the list tests on line 7. The director iterates \nthrough the builders asking each to build its part of the grade report.  The main advantage of this \ndesign is that it allows instruc\u00ad tors and teaching assistants to write tests that are concrete builders \nwithout having to modify the rest of the application.  5.1.2 Implicitly Concurrent Grade Book Builder \nGiven a modular implementation of the grading functional\u00adity, reaping the concurrency bene.ts turned \nout to be very easy using our adaptation of the builder design pattern. Fig\u00adure 25 highlights the only \nlines that changed for this adapta\u00adtion in the entire application. 1 @Builder 2 public interface Grader \n{ 3 public GradeReport grade(); 4 } 6 public class GraderSet { //Director 7 private List<Grader> tests; \n8 // Other details elided. 9 public void go() { 10 GradeReport[] reports = new GradeReport[tests.size()]; \n11 for(int i = 0; i < reports.length; i++) { 12 reports[i] = AsyncUtil.createAsyncBuilder( 13 Grader.class,tests.get(i)).grade(); \n14 } 15 // Other details elided .. 16 } 17 /* Other details elided. */} Figure 25: Implicitly Concurrent \nGrade Report Builder We added line 1 to the interface declaration Grader, which uses an annotation to \ndeclare that this interface plays the role of a builder. This annotation is provided by our framework \nto mark roles that classes play in the design pat\u00adtern implementation. As discussed in Section 3.1.2, \nthis an\u00adnotation causes two classes to be autogenerated at compile time, a proxy class of type GradeReport \nthat encapsu\u00adlates a future for the result, and an asynchronous builder class of type Grader. The proxy \nclass is similar to the TransformedImage proxy discussed in Section 3.4 and the asynchronous builder \nclass is similar to the autogener\u00adated class for the asynchronous factory of Section 3.4 and to the template \nmethod of Section 1.2.1. We modi.ed line 12 in Figure 24 to wrap the concrete builder instance returned \nby tests.get(i) inside an asynchronous builder instance. This is accomplished by us\u00ading the createAsyncBuilder \nmethod provided by our framework. We then call the method grade as before, but now with the asynchronous \nbuilder instance as the receiver object (instead of the concrete builder instance). The net effect of \nthese changes is that on lines 12 13 in Figure 25, creation tasks for all reports are queued for asyn\u00adchronous \nexecution and can complete concurrently. This is unlike Figure 24, where creation of reports is sequential. \nBe\u00adsides these two lines the rest of this application remain the same; thus the impact of applying our \nimplicitly concurrent builder pattern on client code is fairly minimal.  5.1.3 Performance Results To \nanalyze the bene.ts of these two lines of changes in the grading application, we compared the performance \nof the enhanced version with the original sequential version. All experiments in this paper were run \non a system with a total of 12 cores (two 6-core AMD Opteron 2431 chips) running Fedora GNU/Linux. Both \nthe original and the enhanced version of the grad\u00ading framework computed 12 identical grade reports. \nThese results are presented in Figure 26. The performance bene.ts in this case were almost as good as \ncould be expected. In\u00adcreasing the number of threads used consistently improved the runtime roughly in \nkeeping with the 1/x curve. It should however be noted that there are plateaus in the runtimes which \ncan be easily explained. The concurrency in the pro\u00adgram is achieved by building different reports concurrently. \nSince there were 12 identical reports in the benchmark, these were spread over the different threads. \nSigni.cant drops can be seen at 2, 3, 4, 6, 12 threads. These are the even multiples of 12. Unless the \nnumber of threads divides 12 evenly, the best expected runtime is that achieved by the previous even \nmultiple (because of unbalanced load on threads). 5.1.4 Summary To summarize, for the JUnit-based grading \napplication, we observed that exploiting the builder design pattern to expose potential concurrency shows \nsigni.cant scalability bene.ts. For this application, adaptation efforts were minimal a to\u00adtal of two \nlines were changed. However, in general we do expect these costs and performance gains to vary substan\u00adtially \nbased on the application.  5.2 Biomolecular Network Alignment Toolkit (BiNA) The Biomolecular Network \nAlignment (BiNA) Toolkit is a framework for studying biological systems at the molecular\u00adlevel such as \ngenes, proteins and metabolites [31, pp.345]. For these systems, of particular interest to molecular \nbiol\u00adogists is their interaction patterns. In practice, multiple ver\u00adsions of interaction patterns can \nbe observed between molec\u00adular participants based on observation conditions. BiNA is used to compare \nand align these interaction patterns among large number of molecular participants.  Unlike the applications \ndiscussed so far, the original im\u00adplementation of BiNA used explicitly created threads. Thus, our challenge \nwas to match or exceed the performance of this explicitly tuned concurrent version of BiNA. 5.2.1 Application \nof Pattern Framework We .rst removed explicit threading from BiNA s original implementation to create \na sequential version of BiNA. We then used two design patterns in BiNA: abstract factory and iterator. \nBiNA constructs a network of interaction patterns based on input .les before computing alignment of these \nnetworks. Based on our inspection, construction of these network objects appeared to be an expensive \noperation and thus a good candidate for an asynchronous factory. We also modi.ed two existing applications \nof the iterator pattern to use our implicitly concurrent versions. 5.2.2 Performance Results To analyze \nthe bene.ts of these changes in BiNA, we com\u00adpared the performance of the enhanced version with the orig\u00adinal \nconcurrent version. All experiments in this paper were run on a system with a total of 12 cores (two \n6-core AMD Opteron 2431 chips) running Fedora GNU/Linux. For this multicore CPU, the original version \nof BiNA performed best when we set the total number of threads to 12. Alignments were computed using \ndata for six different protein-protein interaction networks (mouse, human, .y, and yeast) using both \nthe original and the enhanced versions of BiNA. These data sets are small to medium in size. The comparative \nresults are presented in Figure 27. Depending on the data, we observed improvements of 8.5% to 34.5% \nover the manually-tuned concurrent version of BiNA.  5.2.3 Summary For BiNA we observed that exploiting \nthe abstract fac\u00adtory pattern and the iterator design pattern to expose poten\u00adtial concurrency showed \nsigni.cant scalability bene.ts. The adaptation effort was also fairly small. 6. Conclusion and Future \nWork With increasing emphasis on multiple cores in computer ar\u00adchitectures, improving scalability of \ngeneral-purpose pro\u00adgrams requires .nding potential concurrency in their de\u00adsign. Existing proposals \nto expose potential concurrency rely on explicit concurrent programming language features. Pro\u00adgrams \ncreated with such language features are hard to reason about and building correct software systems in \ntheir presence is dif.cult [22, 28]. In this work, we presented a concurrent design pattern framework \nas a solution to both of these problems. Our so\u00adlution attempts to unify program design for modularity \nwith program design for concurrency. Our framework exploits design decoupling between components achieved \nby a pro\u00adgrammer using GOF design patterns to expose potential con\u00adcurrency between these components. \nWe have studied all 23 GOF design patterns and found that for 18 patterns, synergy between modularity \ngoals and concurrency goals is achievable. Since these design patterns are widely used in object-oriented \nsoftware, we expect our results to be similarly widely applicable. Our framework relies on Java s existing \ntype system and libraries to enforce concurrency and synchronization disci\u00adpline behind the scenes. We \nhave had much success with this approach; however, completely enforcing usage policies such that resulting \nprograms are free of data races and dead\u00adlocks, and have a guaranteed sequential semantics, doesn t appear \nto be possible with the library-based solution that we propose in this work. In a synergistic work, we \nare also ex\u00adploring novel language features and type systems [18] that allows sound determination of \nthese properties. We expect this work to inform the design of such language features. Based on our current \nefforts, we have come to an un\u00adderstanding that a sophisticated runtime system as a back\u00adend will be \nnecessary to completely abstract from the con\u00adcurrency concern. For example, performance evaluation of \nseveral patterns suggest the need to support load-balancing in our framework. Similarly all pattern implementations \ncan bene.t from better support for race detection and avoidance as well as a cost-bene.t analysis to \ndetermine applicability. We plan to continue to investigate these issues. Finally, we would like to apply \nour concurrent design pattern frame\u00adwork to larger case studies to gain insights into problems that might \narise due to scale. Acknowledgments This work was supported in part by the US National Science Foundation \nunder grant CCF-08-46059. We are thankful to Fadi Tow.c and Vasant Honavar for answering questions on \nthe BiNA implementation and for providing data sets for experiments. Sean Mooney helped with the build \nsystem for this project and with some of the performance evaluation, Titus Klinge helped with the implementation \nof the builder pattern, Brittin Fontenot helped with the interpreter pattern, and Spencer Morrison helped \nimprove several examples.  References [1] P. America. Issues in the design of a parallel object-oriented \nlanguage. Formal Aspects of Computing, 1(4):366 411, 1989. [2] N. Benton, L. Cardelli, and C. Fournet. \nModern concurrency abstractions for C#. ACM Trans. Program. Lang. Syst., 26(5):769 804, 2004. [3] E. \nD. Berger, T. Yang, T. Liu, and G. Novark. Grace: safe multithreaded programming for C/C++. In Proceedings \nof the Conference on Object Oriented Programming Systems, Languages and Applications, pages 81 96, 2009. \n[4] R. D. Blumofe, C. F. Joerg, B. C. Kuszmaul, C. E. Leiserson, K. H. Randall, and Y. Zhou. Cilk: an \nef.cient multithreaded runtime system. In PPOPP, pages 207 216, 1995. [5] R. L. Bocchino, Jr., V. S. \nAdve, D. Dig, S. V. Adve, S. Heumann, R. Komuravelli, J. Overbey, P. Simmons, H. Sung, and M. Vakilian. \nA type and effect system for deterministic parallel java. In Proceedings of the Conference on Object \nOriented Programming Systems, Languages and Applications, pages 97 116, 2009. [6] P. Charles, C. Grothoff, \nV. Saraswat, C. Donawa, A. Kielstra, K. Ebcioglu, C. von Praun, and V. Sarkar. X10: an object-oriented \napproach to non-uniform cluster computing. In Proceedings of the Conference on Object Oriented Programming \nSystems, Languages and Applications, pages 519 538, 2005. [7] R. Cunningham and E. Kohler. Tasks: language \nsupport for event-driven programming. In Conference on Hot Topics in Operating Systems, Berkeley, CA, \nUSA, 2005. [8] D. Dig, J. Marrero, and M. D. Ernst. Refactoring sequential java code for concurrency \nvia concurrent libraries. In ICSE, pages 397 407, 2009. [9] J. Fischer, R. Majumdar, and T. Millstein. \nTasks: language support for event-driven programming. In PEPM, pages 134 143, 2007. [10] M. Frigo, C. \nE. Leiserson, and K. H. Randall. The implementation of the Cilk-5 multithreaded language. In the ACM \nconference on Programming language design and implementation (PLDI), pages 212 223, 1998. [11] E. Gamma \nand K. Beck. JUnit. http://www.junit. org. [12] E. Gamma, R. Helm, R. Johnson, and J. Vlissides. Design \nPatterns: Elements of Reusable Object-Oriented Software. Addison-Wesley Longman Publishing Co., Inc., \n1995. [13] M. Krohn, E. Kohler, and M. F. Kaashoek. Events can make sense. In USENIX, 2007. [14] M. Kulkarni, \nK. Pingali, B. Walter, G. Ramanarayanan, K. Bala, and L. P. Chew. Optimistic parallelism requires abstractions. \nIn PLDI, pages 211 222, 2007. [15] D. Lea. Concurrent Programming in Java. Second Edition: Design Principles \nand Patterns. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 1999. [16] D. Lea. A Java \nFork/Join Framework. In The Java Grande Conference, pages 36 43, 2000. [17] D. Leijen, W. Schulte, and \nS. Burckhardt. The design of a task parallel library. In Proceedings of the Conference on Object Oriented \nProgramming Systems, Languages and Applications, pages 227 242, 2009. [18] Y. Long, S. L. Mooney, T. \nSondag, and H. Rajan. Implicit invocation meets safe, implicit concurrency. In GPCE 10: Ninth International \nConference on Generative Programming and Component Engineering, October 2010. [19] C. Lopes. D: A language \nframework for distributed programming. PhD thesis, Northeastern University, 1997. [20] T. G. Mattson, \nB. A. Sanders, and B. L. Massingill. A Pattern Language for Parallel Programming. Addison Wesley Software \nPatterns Series, 2004. [21] M. D. McCool. Structured parallel programming with deterministic patterns. \nIn 2nd USENIX workshop on Hot Topics in Parallelism, 2010. [22] J. Ousterhout. Why threads are a bad \nidea (for most purposes). In ATEC, January 1996. [23] V. Pankratius, C. Schaefer, A. Jannesari, and W. \nF. Tichy. Software engineering for multicore systems: an experience report. In IWMSE workshop, pages \n53 60, 2008. [24] P. Pratikakis, J. Spacco, and M. Hicks. Transparent Proxies for Java Futures. In OOPSLA, \npages 206 223, 2004. [25] H. Rajan, Y. Long, S. Kautz, S. Mooney, and T. Sondag. Panini project s website. \nhttp://paninij. sourceforge.net, 2010. [26] M. C. Rinard and M. S. Lam. The design, implementation, and \nevaluation of Jade. ACM Trans. Program. Lang. Syst., 20(3):483 545, 1998. [27] J. Robert H. Halstead. \nMultilisp: A Language for Concurrent Symbolic Computation. ACM Trans. Program. Lang. Syst., 7(4):501 \n538, 1985. [28] Saha, B. et al.. McRT-STM: a high performance software transactional memory system for \na multi-core runtime. In PPoPP, pages 187 197, 2006. [29] D. C. Schmidt, H. Rohnert, M. Stal, and D. \nSchultz. Pattern-Oriented Software Architecture: Patterns for Concurrent and Networked Objects. John \nWiley &#38; Sons, Inc., New York, NY, USA, 2000. [30] B. Shriver and P. Wegner. Research directions in \nobject\u00adoriented programming, 1987. [31] F. Tow.c, M. Greenlee, and V. Honavar. Aligning biomolec\u00adular \nnetworks using modular graph kernels. In 9th Interna\u00adtional Workshop on Algorithms in Bioinformatics \n(WABI 09), 2009. [32] A. Welc, S. Jagannathan, and A. Hosking. Safe Futures for Java. In OOPSLA, pages \n439 453, 2005. [33] A. Yonezawa. ABCL: An object-oriented concurrent system, 1990. [34] A. Yonezawa and \nM. Tokoro. Object-oriented concurrent programming, 1990.     \n\t\t\t", "proc_id": "1869459", "abstract": "<p>General purpose object-oriented programs typically aren't embarrassingly parallel. For these applications, finding enough concurrency remains a challenge in program design. To address this challenge, in the Panini project we are looking at reconciling concurrent program design goals with modular program design goals. The main idea is that if programmers improve the modularity of their programs they should get concurrency for free. In this work we describe one of our directions to reconcile these two goals by enhancing Gang-of-Four (GOF) object-oriented design patterns. GOF patterns are commonly used to improve the modularity of object-oriented software. These patterns describe strategies to decouple components in design space and specify how these components should interact. Our hypothesis is that if these patterns are enhanced to also decouple components in execution space applying them will concomitantly improve the design and potentially available concurrency in software systems. To evaluate our hypothesis we have studied all 23 GOF patterns. For 18 patterns out of 23, our hypothesis has held true. Another interesting preliminary result reported here is that for 17 out of these 18 studied patterns, concurrency and synchronization concerns were completely encapsulated in our concurrent design pattern framework.</p>", "authors": [{"name": "Hridesh Rajan", "author_profile_id": "81323495185", "affiliation": "Iowa State University, Ames, IA, USA", "person_id": "P2354149", "email_address": "", "orcid_id": ""}, {"name": "Steven M. Kautz", "author_profile_id": "81453629989", "affiliation": "Iowa State University, Ames, IA, USA", "person_id": "P2354150", "email_address": "", "orcid_id": ""}, {"name": "Wayne Rowcliffe", "author_profile_id": "81470654373", "affiliation": "Iowa State University, Ames, IA, USA", "person_id": "P2354151", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1869459.1869523", "year": "2010", "article_id": "1869523", "conference": "OOPSLA", "title": "Concurrency by modularity: design patterns, a case in point", "url": "http://dl.acm.org/citation.cfm?id=1869523"}