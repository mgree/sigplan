{"article_publication_date": "10-17-2010", "fulltext": "\n Composable Speci.cations for Structured Shared-Memory Communication Benjamin P. Wood Adrian Sampson \nLuis Ceze Dan Grossman University of Washington {bpw,asampson,luisceze,djg}@cs.washington.edu Abstract \nIn this paper we propose a communication-centric approach to specifying and checking how multithreaded \nprograms use shared memory to perform inter-thread communication. Our approach complements past efforts \nfor improving the safety of multithreaded programs such as race detection and atom\u00adicity checking. Unlike \nprior work, we focus on what pieces of code are allowed to communicate with one another, as opposed to \ndeclaring what data items are shared or what code blocks should be atomic. We develop a language that \nsupports composable speci.cations at multiple levels of ab\u00adstraction and that allows libraries to specify \nwhether or not shared-memory communication is exposed to clients. The precise meaning of a speci.cation \nis given with a formal se\u00admantics we present. We have developed a dynamic-analysis tool for Java that \nobserves program execution to see if it obeys a speci.cation. We report results for using the tool on \nseveral benchmark programs to which we added speci.\u00adcations, concluding that our approach matches the \nmodular structure of multithreaded applications and that our tool is performant enough for use in development \nand testing. Categories and Subject Descriptors D.2.4 [Software Engi\u00adneering]: Software/Program Veri.cation \nreliability; D.2.5 [Software Engineering]: Testing and Debugging monitors, testing tools; D.3.2 [Programming \nLanguages]: Language Classi.cations Concurrent, distributed, and parallel lan\u00adguages; F.3.1 [Logics and \nMeanings of Programs]: Speci\u00adfying and Verifying and Reasoning about programs speci\u00ad.cation techniques \nGeneral Terms Languages, Veri.cation, Reliability Keywords concurrency, software reliability, bug detection, \nannotation, speci.cation, shared memory Permission to make digital or hard copies of all or part of this \nwork for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. OOPSLA/SPLASH 10, October 17 21, 2010, Reno/Tahoe, Nevada, USA. Copyright \nc &#38;#169; 2010 ACM 978-1-4503-0203-6/10/10. . . $10.00 1. Introduction With the move to multicore \narchitectures, more and more ap\u00adplications are being written with multiple threads that com\u00admunicate \nvia shared memory. While many high-level pro\u00adgramming languages, Java being a canonical example, pro\u00advide \nbuilt-in support for shared memory, developers still struggle to build robust and effective multithreaded \npro\u00adgrams. On the one hand, shared memory provides a sim\u00adple abstraction because inter-thread communication \nis im\u00adplicit, so programmers need not move data explicitly. On the other hand, a key reason why shared-memory \nprograms are so dif.cult to write and understand is precisely that inter\u00adthread communication or its \nabsence is implicit. The vast amount of programming-languages research on static and dynamic analyses \nto detect programming errors such as data races, deadlocks, and atomicity violations has helped to ad\u00address \nthis problem. In this work, we provide a new and complementary ap\u00adproach to specifying and checking multithreaded \nsafety properties. We believe our speci.cations more directly match the code structure of programs. Prior \nwork has fo\u00adcused either on data invariants, such as object o1 is thread\u00adlocal or object o2 is always \nprotected by lock l, or on iso\u00adlation invariants, such as statement s appears to execute atomically. \nWe focus on code-communication invariants, such as if method m1 writes to memory, another thread will \nread that write only when executing m2 or m3. For exam\u00adple, a partial speci.cation for a queue library \ncould state that writes by enqueue should be read only by enqueue (for the queue size) or dequeue (for \nthe size and the data). To keep speci.cations simple, we do not describe what data is communicated between \nthreads, only which meth\u00adods communicate between threads. In this way, our new communication-centric \napproach is complementary to ex\u00adisting approaches. In a sense, this captures some of the ex\u00adplicitness \nof a message-passing model while preserving the conveniences of shared memory. An analysis tool can then \ncheck these speci.cations. In this work, we develop a dynamic-analysis (i.e., debugging) tool to determine \nif a program execution violates the pro\u00adgram s speci.cation. For the example above, this checking would \nensure that the queue abstraction is not violated via unexpected inter-thread communication.  A naive \nimplementation of this approach to speci.cation and checking would work as follows: For every method \nm, have developers list every method m' that can read data writ\u00adten by m when m and m' run in different \nthreads. The dy\u00adnamic analysis would then record the relevant metadata with each write and check it for \neach read. There are several rea\u00adsons this description is naive, and overcoming these chal\u00adlenges is \nthe primary research contribution in our design: Method calls: Methods often use callees to perform \nthe actual writes and reads of shared memory. We use a notion of inlining in our speci.cations to indicate \nthat memory accesses are performed on behalf of the caller.  Conciseness: If many methods all communicate \nwith each other, the speci.cations could suffer a quadratic blow-up. A notion of communication groups \navoids this problem.  Local speci.cations: Modern applications are too large for anyone to have a single \nglobal view of all inter\u00adthread communication. We design method annotations that need to describe communication \nonly within a con\u00adceptual module boundary.  Layered communication: Speci.cations must capture the intuition \nthat communication can be described at multi\u00adple levels of abstraction. For example, a producer may pass \ndata to a consumer via a queue library. We can spec\u00adify and check communication at the producer/consumer \nlevel and the queue level simultaneously. Naturally, our approach supports an arbitrary number of layers. \n Encapsulated communication: Libraries often perform communication that is abstracted away from clients. \nFor example, inside a queue library dequeue communicates to enqueue (via the queue-size .eld), but the \nspeci.\u00adcations used to check clients of the library should not consider this communication. We de.ne \ncommunication modules and interface groups to address this essential abstraction issue.  Overall, the \nspeci.cation language has several essential, sub\u00adtle, and synergistic features. We motivate these features \nwith canonical examples and de.ne them precisely with a for\u00admal semantics. This semantics formally describes \nwhen a dynamic memory operation violates the speci.cation of al\u00adlowed communication. We have also implemented \na real dynamic checker that processes speci.cations written as Java annotations and uses Java bytecode-instrumentation \nto perform checking. As will be clear after describing our speci.cation language, check\u00ading in the general \ncase requires storing the call-stack with each memory write and comparing it to the call-stack at each \nmemory read. By storing only the portion of the call-stack relevant to a program s speci.cations and \naggressively us\u00ading memoization for everything related to call-stack check\u00ading, our tool can run programs \nwith an overhead of approx\u00adimately 5 10x for most programs too slow for deployed software, but acceptable \nfor a debugging tool. We describe how to use the tool to identify a program s communication and how to \nuse it interactively to help develop speci.cations for legacy (already written, but unspeci.ed) programs. \nWe have evaluated our tool by annotating small programs from the Java Grande benchmark suite as well \nas three large applications from the DaCapo suite. We measure the pre\u00adcision and conciseness of annotations \nas well as our tool s performance. We conclude that specifying legacy applica\u00adtions is dif.cult but informative, \nand we believe that co\u00addeveloping new applications and their speci.cations will be even more helpful. \nIn summary, our contributions are: A new communication-centric approach to specifying shared-memory \ncommunication  A speci.cation language that naturally supports modular\u00adity and shared-memory communication \nat multiple layers of abstraction  A formal semantics for our language  A dynamic checker that is performant \nas a debugging tool  An evaluation of our language and checker on benchmark programs  The rest of this \npaper proceeds as follows. The next sec\u00adtion presents an example to motivate our communication\u00adcentric \napproach and to distinguish it from other approaches. Section 3 describes our speci.cations and additional \nex\u00adamples. Section 4 provides a formal semantics of speci\u00ad.cation checking to remove any ambiguity regarding \nthe meaning of our modular speci.cations. Section 5 describes our dynamic-analysis tool for Java. Section \n6 evaluates our tool on benchmark applications we annotated. Finally, Sec\u00adtions 7, 8, and 9 discuss future \nwork, related work, and con\u00adclusions, respectively. 2. Illustrative Example In this section, we use a \nshort example to give a basic sense of our communication-centric speci.cations and how they complement \nother approaches. This section does not present the full speci.cation language nor does it provide precise \nde.nitions. The Code. The code skeleton for our example appears in Figure 1. It depicts a hypothetical \nimage-rendering ap\u00adplication where we imagine the render method was re\u00adcently changed to parallelize \nthe application so that each of nthreads threads now processes an equal fraction of the image s pixel \nrows. (Calls to render by each thread are not shown.) We further assume a separate thread initiates the \nrendering and then calls getImage to obtain the result. The  @Group(\"Image\") public class Renderer { \nvolatile int curLine; final int totalLines; ConcurrentMap<Integer,Pixel[]> outputImage; // Render line \n(nthreads * n + tid) for every n. @Writer({\"Image\"}) void render(int tid, int nthreads) { for (curLine \n= tid; curLine < totalLines; curLine += nthreads) outputImage.put(curLine, expensiveCall(...)); } // \nReturn after rendering is finished @Reader({\"Image\"}) ConcurrentMap<Integer,Pixel[]> getImage() { while \n(curLine < totalLines) /*spin*/ ; return outputImage; } } Figure 1. A simple concurrency bug that can \nbe caught using code-centric communication speci.cation. getImage method observes the curLine .eld in \norder to wait for rendering to .nish. Unfortunately, the naive parallelization of the code intro\u00adduced \na bug: curLine is a .eld now shared among the calls to render, leading to potentially wrong output and \nloop con\u00additions. Our Speci.cations. Figure 1 also includes the method an\u00adnotations we use to specify \nthe allowed inter-thread commu\u00adnication. Overall, these speci.cations indicate that getImage can read \nmemory written by render. We place no restric\u00adtions on intra-thread communication; we always mean that \nthe read takes place in a different thread from the write. Be\u00adcause the speci.cations do not allow render \nto read mem\u00adory written by render, any execution that called render from multiple threads would violate \nthe speci.cation (via curLine) and our dynamic analysis would report an error. Because our speci.cation \nlanguage was designed for larger programs, annotations use several features that are needed less for \ntiny examples. First, rather than specify directly the write-to-read communication from render to getImage, \nwe de.ne a communication group (Image) and specify that render is a write-member of the group (but cru\u00adcially \nnot a read-member) and getImage is a read-member. Second, the group itself is (implicitly) private, meaning \nthe communication it speci.es is not propagated to callers out\u00adside of the module (given our Java substrate, \nwe by default equate packages with modules for our purposes). That way, a caller of a method like createScene \n(not shown) need not be aware that the callee is using concurrency. The use of a ConcurrentMap in our \nexample provides another motivating example for distinguishing communi\u00adcation internal to a module from \nexternal communication. Internally, concurrent calls to put do potentially commu\u00adnicate (e.g., if the \nkeys are the same, the later call must detect this by reading shared memory internal to the data structure \nand overwrite the .rst mapping). But external clients should see shared-memory communication only from \nput to get (and similar methods). Modular speci.ca\u00adtions for ConcurrentMap would make this distinction \n(see Sections 3.4 and 3.5); no additional annotations are needed for callers outside the library. Other \nApproaches. In many cases, our speci.cations may detect the same concurrency errors as other approaches \nsuch as race detectors and atomicity checkers, which is interesting in and of itself since what we are \nspecifying is fundamentally different. In other cases, the errors detected are complemen\u00adtary. In fact, \nfor our example, these approaches are unlikely to identify the problem: Race detectors: The program \nhas no data race. The pro\u00adgrammer correctly declared curLine as volatile to allow the asynchronous polling \nin getImage. Unfortu\u00adnately, this masks the higher level race in render. The accesses to the concurrent \nmap are properly synchronized by the library implementation.  Atomicity checkers: Calls to render are \nnot necessar\u00adily atomic, but they are also not atomic in a correct ver\u00adsion of the code. Since each call \nto render makes sev\u00aderal calls to outputImage.put, mutating the concurrent map, these put operations \nmay interleave when multi\u00adple threads run render concurrently, meaning render is not strictly atomic. \nrender is only atomic at a higher level of abstraction that accounts for the fact that calls to outputImage.put \nwith distinct keys commute with each other. Therefore, an atomicity checker reporting that render is \nnot atomic is not helpful; it does not distin\u00adguish correct from incorrect code.  3. Communication Speci.cations \nIn this section we present the fundamental concepts of inter\u00adthread communication we use and how to specify \nthem, re\u00advising our de.nitions as we introduce each new concept. Section 3.1 .rst gives a simple de.nition \nof what it means for one method to communicate to another, in terms of dynamic memory operations. It \nthen considers naive approaches for specifying all possible communication. We use these ob\u00adservations \nto motivate communication inlining (Section 3.2) and communication groups (Section 3.3). These two con\u00adcepts \nsuf.ce for annotating small programs, but larger pro\u00adgrams bene.t from modular speci.cations, using features \npresented in Sections 3.4 and 3.5. Table 1 summarizes the main concepts our speci.cations embody.  Concept \nPurpose Section Communication inlining Methods that perform communication solely on behalf of their callers \nare allowed 3.2 to communicate whenever their callers are. Communication groups Concisely specify many \nrelated communicating method pairs. 3.3 Communication modules Build communication abstractions to avoid \nwhole-program speci.cations. 3.4 Stack segments Enforce communication abstractions by partitioning each \ncommunicating call 3.4 stack to isolate communication in distinct modules. Interface groups Encapsulate \nor expose communication at module boundaries. 3.5 Table 1. Overview of communication speci.cation concepts \npresented in this paper. 3.1 Method Communication We consider inter-thread communication only. If thread \ntw executes a memory write operation in the dynamic scope of a call to some method mw and the result \nof this operation is later read by a memory read operation executed in the dy\u00adnamic scope of a call to \nmethod mr by a different thread tr, then we say that mw communicates to mr. (The same principles apply \nto synchronization: when a thread acquires a lock last released by another thread, communication oc\u00adcurs.) \nGeneralizing to nested method calls, it is clear that every method on the call stack of tw at the time \nof its write operation communicates to every method on the call stack of tr at the time of its read operation. \nA key insight in this de.nition is that the communi\u00adcation effects of memory and synchronization operations \nare dynamically not statically scoped. A method m may communicate through a memory operation in m or \nany tran\u00adsitive callee of m. This distinction captures a common id\u00adiom that is not speci.c to shared-memory \nprograms: many methods execute memory operations on the behalf of others. (We will exploit this relationship \nfurther to develop modular communication abstractions in Section 3.4.) For example, consider the simple \nvector implementation outlined in Figure 2. Clients are responsible for synchroniz\u00ading access to the \nvector. The add method calls Util.expand to expand the underlying array if it is already full when try\u00ading \nto add a new item. Assume one thread calls add and trig\u00adgers an expansion with expand, which reads all \nthe items in the current array and writes them into a new larger array be\u00adfore add writes an item into \nthe new array. Next, if another thread calls get, requesting a different index than that of the newly \nadded item, it reads an element in the array, reading the result of a memory write operation executed \nin expand. This single write-read pair causes both expand and add to communicate to get, since the write \noperation executed in the dynamic scope of both methods. Naive Approaches to Speci.cation. Two naive \nways to specify communication are immediately obvious from our de.nition of communication. The .rst is \nto specify every pair of call stacks that is allowed to communicate. Although this approach gives fully \ncontext-sensitive precision to speci\u00ad @Group(\"Vector\") class SimpleVector { Item[] elements = new Item[10]; \nint size = 0; @Writer({\"Vector\"}) @Reader({\"Vector\"}) void add(Item item) { if (size == elements.length) \nelements = Util.expand(elements); elements[size++] = item; } @Reader({\"Vector\"}) Item get(int i) { return \nelements[i]; } @Writer({\"Vector\"}) @Reader({\"Vector\"}) Item replace(int i, Item item) { Item old = elements[i]; \nelements[i] = item; return old; } ... } class Util { static Item[] expand(Item[] array) { Item[] tmp \n= new Item[array.length * 2]; for (int i = 0; i < array.length; i++) tmp[i] = array[i]; return tmp; } \n} Figure 2. A simple vector implementation. .cations, it would be combinatorial in size and would yield \na whole-program speci.cation, clearly a non-starter. The sec\u00adond approach is to enumerate all pairs of \nmethods that are allowed to communicate. A memory read operation is valid under such a speci.cation when \nfor all methods mw on the call stack at the last write to its target and all methods mr on the call stack \nat the read, (mw,mr) is in the speci.cation. While this approach is less expensive than enumerating pairs \nof call stacks, it yields whole-program speci.cations that are still quadratic in size. In the remainder \nof this section, we harness several important observations on program and com\u00admunication structure to \nimplement speci.cations that over\u00adcome the limitations of these naive speci.cations.  3.2 Communication \nInlining Many methods communicate only incidentally, when their callers use them to operate on shared \ndata. For exam\u00adple, there is nothing meaningful about communication per\u00adformed by Util.expand (introduced \nin Section 3.1 and shown in Figure 2) except in the context of its callers. While it is obvious that \na useful speci.cation must allow com\u00admunication between expand and the vector get method, a speci.cation \nthat explicitly declares communication be\u00adtween expand and get is misleading and unwieldy outside the \nvector implementation. The speci.cation would likely need pairs containing expand and many other methods. \nA better way to understand communication in expand is that expand is allowed to communicate whenever \nits caller is. We refer to this as communication inlining. Communica\u00adtion due to memory operations in \nexpand is simply treated as though expand is inlined into its caller. Methods that communicate only on \nbehalf of their callers are so prevalent that all methods are communication-inlined by default unless \nthe speci.cation places explicit restrictions on their communication. In practice, this convention alone \nhas a signi.cant simplifying impact on the complexity of a communication speci.cation. In the vector \nimplementation, for example, the naive pairwise speci.cation requires that expand communicate nearly \neverywhere add does. Leaving expand inlined removes the need for all these extra pairs. In the remainder \nof this paper, when we refer to a stack or a call stack, we mean the version with all inlined methods \nremoved. Inlining yields an interesting property of speci.ca\u00adtions: the speci.cation in which all methods \nare inlined al\u00adlows all communication in a program. Since the call stack is conceptually empty at every \ncommunicating write and read operation, the set of methods that communicate as a result is empty. This \nproperty becomes particularly useful for devel\u00adoping speci.cations incrementally once we introduce more \nmodular speci.cation features in Section 3.4. 3.3 Communication Groups Programs often contain sets of \nmethods where all or nearly all pairs of methods within the set communicate. The vec\u00adtor implementation \nin Figure 2 is a prime example of this pattern. The three methods shown, in addition to others that are \nomitted (e.g., contains, find, and remove), commu\u00adnicate with each other (through the elements array \nand the size .eld). For a set of n related methods like this, a naive pairwise speci.cation would include \nO(n2) annotations. The Communication Group Primitive. The communica\u00adtion group is the basic unit of a \ncommunication speci.ca\u00adtion, and serves to express many communicating pairs in a set of related methods \nconcisely. A group G =(W, R) is a pair of the set W of the group s writer methods and the set R of its \nreader methods, representing the set of pairs in the cross product W \u00d7 R. The writers and readers of \na group are collectively referred to as its members. Separating the two types of members facilitates \nthe expression of common patterns where certain methods should read values written by others in the group \nbut should not write values that the others may read (or vice versa). The @Group, @Writer, and @Reader \nannotations in the vector implementation are the Java annotation equivalent of the following group: GVector \n=({add, replace}, {add, get, replace}) The get method is a reader but not a writer in this group. For \nconvenience reasons, our Java speci.cations use decen\u00adtralized notation: @Group(\"Vector\") declares a \ngroup by name; each method is annotated as a writer or reader in zero or more groups: @Writer({\"Vector\"}) \n@Reader({\"Vector\"}) Checking the communication resulting from a memory read operation against a speci.cation \nis simple to de.ne: De.nition 1 (Valid Simple Communication). A read of x is always valid if x was last \nwritten in the same thread. If x was last written by a different thread tw, then it is valid for a thread \ntr to read x if for all methods mw that were on tw s call stack when it wrote x and all methods mr on \ntr s call stack when it reads x there exists some group (W, R) in the speci.cation such that mw . W and \nmr . R.  3.4 Modularity: Communication Modules Speci.cations composed of the communication inlining \nand group primitives suf.ce for simple programs, but for larger programs it is natural to specify communication \nat multi\u00adple layers of abstraction. Consider the simple producers\u00adconsumers pipeline sketched in Figure \n3. Producer threads (not shown) call produce to produce items and enqueue them in a bounded buffer, and \nconsumer threads (also not shown) call consume to dequeue and process items for the next stage of the \npipeline. Communication in this program occurs at two levels of abstraction. At a low level, the bounded \nbuffer meth\u00adods enqueue and dequeue communicate through a shared buffer representation: GBu.er =({enqueue, \ndequeue}, {enqueue, dequeue}) At a higher level, produce communicates to consume in the pipeline by enqueueing \nitems in a bounded buffer from which consume later dequeues them: GPipe =({produce}, {consume}) However, \nsince the bounded buffer s size .eld is both read and written when produce calls enqueue and when consume \ncalls dequeue, then, for example, consume may communicate to produce, so we must settle on the following \nspeci.cation until we introduce encapsulation in Section 3.5:  GPipe =({produce, consume}, {produce, \nconsume}) With only inlining and groups, we are stuck with three unsatisfactory speci.cations: 1. Specify \nonly the low-level bounded-buffer abstraction by inlining the pipeline methods; 2. Specify only the \nhigher-level pipeline abstraction by in\u00adlining the bounded buffer methods; or 3. Specify that all four \nmethods are writers and readers in a single group that lacks any notion of abstraction at all:  GPipe \n=({produce, consume, enqueue, dequeue}, {produce, consume, enqueue, dequeue}) To specify communication \nat multiple levels of abstraction, we introduce communication modules. The Communication Module Primitive. \nA communica\u00adtion module consists of a set of related methods and a set of groups whose members are drawn \nfrom these methods. The methods in a module interact with each other and perform communication described \nby the module s groups to im\u00adplement a communication abstraction such as the bounded buffer. The pipeline \nprogram in Figure 3 has two mod\u00adules, which are aligned by default with Java packages. Ex\u00adplicit annotations \nof arbitrary modules are also supported; in this case the default suf.ces. The module Mp contains the \nmethods and groups for the item processing pipeline, and the module Mb contains the methods and groups \nfor the bounded buffer: Mp =({produce, consume}, {GPipe}) GPipe =({produce, consume}, {produce, consume}) \nMb =({enqueue, dequeue}, {GBu.er }) GBu.er =({enqueue, dequeue}, {enqueue, dequeue}) Section 3.5 discusses \nthe @InterfaceGroup annotation. The levels of communication abstraction in the pipeline program are well \naligned with the program representation, but we must map them clearly to the dynamic communica\u00adtion behavior \nof the program as well. Recall our description in Section 3.3 that when the result of a write operation \nis read by a read operation in another thread, all methods on the call stack at the write operation communicate \nto all methods on the call stack at the read operation. This de.nition assumes a single monolithic communication \nabstraction, so we now extend it to support modularity. To support layered abstrac\u00adtions, we divide communicating \nstacks into segments corre\u00adsponding to each layer. A stack segment is a maximal contiguous sequence of \nmethods on a single call stack that all belong to the same package p; @Group(\"Pipe\") class ItemProcessingPipeline \n{ b.BoundedBuffer pipe = new b.BoundedBuffer(); @Writer({\"Pipe\"}) void produce() { ...; pipe.enqueue(...); \n... } @Reader({\"Pipe\"}) void consume() { ...; ... = pipe.dequeue(); ... } } package b; @InterfaceGroup(\"BufferClient\") \n@Group(\"Buffer\") public class BoundedBuffer { Item[] buffer = new Item[10]; int size = 0; ... @Writer({\"Buffer\", \n\"BufferClient\"}) @Reader({\"Buffer\"}) public synchronized void enqueue(Item i) { while (size == buffer.length) \nwait(); buffer[...] = i; size++; notifyAll(); } @Writer({\"Buffer\"}) @Reader({\"Buffer\", \"BufferClient\", \n}) public synchronized Item dequeue() { while (size == 0) wait(); size--; notifyAll(); return buffer[...]; \n } } Figure 3. A pipeline application that uses a simpli.ed bounded buffer to communicate between stages. \nmodule. As an example, Figure 4(a) shows the segmented call stacks at the time of a pair of write and \nread operations in the pipeline program. On the writer stack, the lower segment, consisting of enqueue, \nbelongs to the module Mb, while the upper segment, consisting of produce, belongs to Mp. The reader stack \nhas corresponding segments, containing dequeue and consume, respectively. The fact that the segments \nin these two communicating stacks segments align by module is key. We say that two stacks Sw and Sr have \nequivalent segmentations if both stacks have n segments and for all i . 1 ...n the ith segment on Sw \nbelongs to the same module as the ith segment on Sr. Together, the ith segments on a pair of communicating \ncall stacks with equivalent segmentations form a layer of com\u00admunication abstraction. When two communicating \nstacks do not have equivalent segmentations, either the communica\u00ad  (a) Communication exposed by Mb \n (b) Communication encapsulated by Mb Figure 4. Segmented call stacks for a communication in the pipeline \nprogram from Figure 3. Each box is a segment. In this example all segments have exactly one method. tion \nis in error, or the speci.cation is in error and the mis\u00adalignment of segments should be resolved by \ninlining more methods (recall that stacks contain no inlined methods). De.nition 2 (Valid Modular Communication). \nA memory write operation is allowed to communicate to a memory read operation if the stack at the write \noperation and the stack at the read operation have equivalent segmentations and for each segment on the \nwriter stack, all methods in the segment are allowed to communicate to all methods in the corresponding \nsegment on the reader stack, established by writer-reader group membership, as in De.nition 1. Thus the \npair of stacks in Figure 4(a) represents a valid communication, since enqueue is allowed to communicate \nto dequeue according to GBu.er and produce is allowed to communicate to consume according to GPipe. Communication \nmodules express communication ab\u00adstractions naturally without specifying extraneous communi\u00adcation across \nabstraction boundaries. Furthermore, modules allow for incremental and composable speci.cations. Any \nprogram may use the bounded buffer implementation with\u00adout any additional speci.cation, leaving all other \nmethods inlined. The bounded buffer is still checked for valid com\u00admunication without placing restrictions \non communication performed in the rest of the program.  3.5 Encapsulation: Interface Groups With inlining, \ngroups, and communication modules, we are still unable to express the ideal speci.cation for the pipeline \nabstraction, re.ecting our intuition that produce communi\u00adcates to consume and no other communication \nis possible: GPipe =({produce}, {consume}) The missing link is the encapsulation of communication from \ndequeue to enqueue (via the size .eld and the this lock) in the bounded buffer module. To encapsulate \ncommu\u00adnication in modules, we introduce interface groups. The Interface Group Primitive. Interface groups \nare a primitive for communication encapsulation in modules. We extend communication modules to include \na set of interface groups in addition to their member methods and communi\u00adcation groups. Like communication \ngroups, interface groups are composed of writer and reader methods within the mod\u00adule. While communication \ngroups describe what communi\u00adcation is allowed among methods in the module, interface groups describe \nwhat communication is exposed to external callers of the module s methods. Returning to the pipeline \nexample, we declare an inter\u00adface group for the bounded buffer: @InterfaceGroup(\"BufferClient\") We annotate \nenqueue as a writer and dequeue as a reader in this group, meaning that communication from enqueue to \ndequeue will be exposed to their callers, but all other communication (e.g., from dequeue to enqueue) \nis encap\u00adsulated by the module and not exposed to callers. The result yields a clean interface for the \nbounded buffer and the intu\u00aditive speci.cation for the pipeline: Mp =({produce, consume}, {GPipe }, \u00d8) \nGPipe =({produce}, {consume}) Mb =({enqueue, dequeue}, {GBu.er }, {IBu.erClient }) GBu.er =({enqueue, \ndequeue}, {enqueue, dequeue}) IBu.erClient =({enqueue}, {dequeue}) At the pipeline level, we can now \nregard the bounded buffer just as we do memory. An enqueue operation and a dequeue operation may result \nin communication just as a write operation and read operation would. Alternatively, memory may now be \nregarded as simply one more lowest layer of communication abstraction with an interface dictat\u00ading that \nthe write location method (i.e., writes) communi\u00adcates to the read location method (i.e., reads). We \nextend De.nition 2 to de.ne when a communication violates a speci.cation in the presence of encapsulation: \nDe.nition 3 (Valid Communication). We say that communi\u00adcation is encapsulated by a corresponding pair \nof segments if the pair of segments are at the roots of their stacks or if no interface group contains \nthe deepest (caller-most) method in the writer segment as a writer and the deepest method on the reader \nsegment as a reader. Communication between two stacks Sw and Sr is allowed by a speci.cation if there \nexist stack pre.xes S ' and S ' wr composed of segments 1 ...k of Sw and Sr, respectively, such that \ncommunication is encapsulated by the kth pair of segments and communication from S ' to S ' is allowed \nby wr De.nition 2. When dequeue communicates to enqueue, the commu\u00adnication is encapsulated by module \nMb. As shown in Fig\u00adure 4(b), the communication is not exposed above Mb s layer on the two stacks; even \nthough consume is not allowed to communicate to produce, this communication is valid. In this case, it \nhappens that the entire stacks have equivalent segmentations, even above the encapsulation boundary. \nIn general, however, this is not required.  In reality, some communication abstractions, such as those \ninvolving callbacks, do not map so clearly to layered abstractions. Callbacks may cause communication \nbetween a stack with direct control and a stack with inverted control that has an extra segment for the \ncallback caller at its root. While not a perfect match, we .nd that inlining callback sys\u00adtems is a reasonable \nway to address this pattern. We discuss one example in Section 6.3. 4. Formal Semantics In this section \nwe present a formal semantics for simple multithreaded programs to gain a precise de.nition of when a \nprogram execution satis.es or violates a communica\u00adtion speci.cation, and brie.y discuss salient properties \nof communication-checked programs. We observe that shared\u00admemory communication has a very restricted \ninteraction with program semantics: only memory accesses and method entry and exit are relevant. As a \nresult, we use a simpli.ed view of the execution of multithreaded programs, in which each thread is reduced \nto a trace of operations on global and local state, eliding details that do not affect inter-thread communication. \nOur formalization has three key parts. The .rst describes a simple operational semantics for the execution \nof multi\u00adthreaded programs. The second part is an operational se\u00admantics for the execution of communication-checked \nmulti\u00adthreaded programs that effectively instruments the simple se\u00admantics with the necessary bookkeeping \nand checking. This instrumented semantics is de.ned in terms of the third com\u00adponent of the semantics: \na separate judgment that captures the semantics of when a dynamic memory write operation is allowed to \ncommunicate to a dynamic memory read opera\u00adtion, based on the call stacks when the operations occurred. \n4.1 Standard Multithreaded Semantics A multithreaded program is a set of threads executing con\u00adcurrently, \neach identi.ed by a unique identi.er t and accom\u00adpanied by a thread state p, representing all thread-local \nstor\u00adage and state information. The thread state store . maps each thread s identi.er to its state. The \nthreads share a heap that maps each variable x to a value v and each lock l to the iden\u00adti.er of the \nthread that holds it, or . if no thread does. A program state s =(H, .) is comprised of a heap and a \nthread state store. Threads change the program state by performing operations a that update the heap \nand the thread state. These operations are reading from or writing to a shared variable x in the heap \n(rd(x, v) and wr(x, v)), ac\u00adquiring or releasing a lock l (acq(l) and rel(l)), and entering or exiting \na method m (enter(m) and exit(m)). Programs: Thread ID t Thread State p Method ID m Variable x Lock l \nValue v Address p ::= x | l Holder ls ::= t | . Heap H ::= \u00b7| H, x . v | H, l . ls Thread Map . ::= \n\u00b7| ., t . p State s ::= (H, .) Operation a ::= wr(x, v) | rd(x, v) | acq(l) | rel(l) | enter(m) | exit(m) \nInstrumentation: Shadow Stack S ::= \u00b7| S, m Instrumented Thread Map T ::= \u00b7| T,t . (p, S) Last Writer \nMap f ::= \u00b7| f, p . (t, S) Instrumented State S ::= (H, f, T) Speci.cations and checking: Method Set \n\u00b5, R, W ::= {m1,...,mn} Stack Segment S ::= {m1,...,mn}Group G ::= (W, R) Group Set . ::= {G1,...,Gn}Module \nM ::= (\u00b5, .C ,.I ) Speci.cation G ::= {M1,...,Mn} Figure 5. Domains. Operations a executed by thread \nt may update the heap ' as shown in the judgment H; t; a . H in Figure 6. Reads and writes act as expected; \nlock acquire and release update the lock s heap entry to re.ect its holder or its unheld sta\u00adtus. Method \nentry and exit have no effect on the heap. We represent constraints on the possible steps a thread can \ntake, such as program order, control .ow, and data .ow, by a re\u00adlation Program over thread identi.ers, \ninitial thread states, operations, and the resulting thread states. Thread t, starting in state p, can \nexecute operation a, ending in state p ' when Program(t, p, a, p ' ) holds. A program can step from state \ns to s ' by nondeterministi\u00adcally selecting a thread to perform an operation that satis.es the Program \nrelation and can execute on the current heap, as shown in the judgment Program f s . s ' in Figure 6. \n 4.2 Communication-Checked Semantics To check communication in a program against a speci.ca\u00adtion G, \nwe instrument the standard multithreaded semantics  ' H; t; a . H READ ACQUIRE RELEASE WRITE H(x) = \nv H(l) = . H(l) = t H; t; wr(x, v) . (H, x . v) H; t; rd(x, v) . H H; t; acq(l) . (H, l . t) H; t; rel(l) \n. (H, l . .) ENTER EXIT H; t; enter(m) . H H; t; exit(m) . H Program f s . s ' STEP ' Program(t, .(t), \na, p ' ) H; t; a . H ' Program f (H, .) . (H, (., t . p ' )) Figure 6. Operational semantics for multithreaded \nprograms. G f f; t; S; a . f ' ; S ' INS ENTER (\u00b5, .C ,.I ) . G m . \u00b5 G f f; t; S; enter(m) . f; S, m \nINS INLINED EXIT .(\u00b5, .C ,.I ) . G .m /. \u00b5 G f f; t; S; exit(m) . f; S INS COMMUNICATING READ ' t f(x)=(t \n' ,S ' ) = t G f S ' INS EXIT (\u00b5, .C ,.I ) . G m . \u00b5 G f f; t; S, m; exit(m) . f; S INS WRITE G f f; \nt; S; wr(x, v) . (f, x . (t, S)); S INS RELEASE . S G f f; t; S; rd(x, v) . f; S G f f; t; S; rel(l) \n. (f, l . (t, S)); S INS COMMUNICATING ACQUIRE ' t f(l)=(t ' ,S ' )G f S ' = t . S G f f; t; S; acq(l) \n. f; S Program;G f S . S ' INS INLINED ENTER .(\u00b5, .C ,.I ) . G .m /. \u00b5 G f f; t; S; enter(m) . f; S INS \nTHREAD-LOCAL READ f(x)=(t, S ' ) G f f; t; S; rd(x, v) . f; S INS THREAD-LOCAL ACQUIRE f(l)=(t, S ' ) \nG f f; t; S; acq(l) . f; S CHECKED STEP ' T(t)=(p, S) Program(t, p, a, p ' ) H; t; a . H G f f; t; S; \na . f ' ; S ' Program;G f (H, f, T) . (H ' ,f ' , (T,t . (p ' ,S ' ))) Figure 7. Operational semantics \nfor communication-checked multithreaded programs. to maintain a shadow stack S for each thread and a \nglobal map f recording the last writer of every variable (and the last releaser of every lock). A speci.cation \nG is a set of modules M =(\u00b5, .C ,.I ), where \u00b5 is the set of methods that belong to the module. Speci.cations \nand modules are discussed in more detail in Section 4.3. For now it suf.ces to understand that inlined \nmethods do not belong to any module. The judgment G f f; t; S; a . f ' ; S ', in Figure 7, describes \nthe effects of operations on shadow stacks and the last-writers map. A program can take a step according \nto this judgment if the step will not violate the speci.cation. A shadow stack S represents a thread \ns call stack, with inlined methods elided. When thread t enters the non-inlined method m, it pushes m \nonto its shadow stack, popping it off when it exits the method. Inlined methods are ignored. (See rules \nINS ENTER, INS EXIT, INS INLINED ENTER, and INS INLINED EXIT.) When a thread s current shadow stack is \n\u00b7,m1,m2, its program counter is in m2 (or an inlined transitive callee of m2), where m2 was called by \nm1 (or an inlined transitive callee of m1), and m1 was the thread s entry point, or a transitive callee \nof an inlined entry point. The last-writers map f stores for each variable and lock the last thread to \nwrite or release it and that thread s shadow stack at the time of the operation. The entry in the last\u00adwriters \nmap for a variable x or lock l is updated with the executing thread and its current stack on every write \nto x or release of l, as shown in rules INS WRITE and INS RELEASE.  \u00b5; m f S = S ' ; S COLLECT BORDER \nEND ' '' m . \u00b5\u00b5; m f S = S ' ; S m . \u00b5S = S ' ,m m ./\u00b5 m . \u00b5 ' \u00b5; m f S, m = S ' ; S .{m} \u00b5; m f S, \nm = S; {m} \u00b5; m f\u00b7,m = \u00b7; {m} .C f S w S r. f mw mr CHECK METHODS CHECK SEGMENTS (W, R) . .mw . Wmr . \nR .mw . S w . .mr . S r ..C f mw mr . f mw mr .C f S w Sr EXPOSED LAYER (\u00b5, .C ,.I ) . G \u00b5; mw f Sw \n= S ' ; S w \u00b5; mr f Sr = S ' ; S r .C f S w S r .I f mw mr G f S ' S ' wrwr G f Sw Sr G f Sw Sr ENCAPSULATED \nLAYER = S ' ; = S ' ; .C f EMPTY (\u00b5, .C ,.I ) . G \u00b5; mw f Sw Sw \u00b5; mr f Sr Sr Sw Sr .I mw mr wr G f \nSw Sr G f\u00b7 \u00b7 Figure 8. Stack checking semantics. Read and acquire operations are instrumented to check \nthat any communication they complete is allowed by the speci.cation G. Thread-local reads and acquires \nare always allowed. When tr reads from variable x, if the last thread to write to x was tr, then the \nread is allowed to proceed, as shown in rule INS THREAD-LOCAL READ. If the last thread tw to write to \nx was not the same as the thread tr executing the read operation, then the read is only allowed to proceed \nif the speci.cation G allows communication from the stack Sw of tw at the time it wrote to x to tr s \ncurrent stack, Sr, according to the judgment G f Sw Sr, in Figure 8. (Speci.cations and stack checking \nare described in detail in Section 4.3.) The same logic applies to lock acquires, with respect to the \nlast release of the same lock. An instrumented program state S consists of a heap H,a last-writers map \nf, and an instrumented thread state store T, which maps each thread identi.er to the associated thread \nstate, instrumented with a shadow stack. An instrumented program is allowed to step from one instrumented \nstate to another under the communication-checked semantics, as shown in the judgment Program;G f S . \nS ', in Figure 7, if it can step under the simple semantics and the operation is allowed under the instrumented \nsemantics by satisfying the judgment G f f; t; S; a . f ' ; S '. Speci.cally, the rule CHECKED STEP ensures \nthat read and acquire operations are only possible when the communication they complete is thread-local \nor allowed by the speci.cation.  4.3 Speci.cation Semantics A communication speci.cation G is a set \nof modules. Each module M =(\u00b5, .C ,.I ) consists of a set of methods \u00b5 that does not overlap with that \nof any other module in the spec\u00adi.cation, a set of communication groups .C that describes what communication \nis allowed among methods in \u00b5, and a communication interface .I that describes what communi\u00adcation among \nmethods in \u00b5 is visible to callers outside the module. A communication group G is a pair (W, R) of sets \nof methods, denoting communication from every method in the writer set W to every method in the reader \nset R. A stack segment S is the set of methods appearing in a maximal contiguous subsequence of a stack \nsuch that all of the methods in the subsequence belong to the same mod\u00adule. A stack segment represents \na conceptual layer of com\u00admunication abstraction in a communicating stack. The judg\u00adment \u00b5; m f S = S \n' ; S , in Figure 8, states that a stack S is pre.xed by a maximal non-empty sequence of methods comprising \nthe segment S , where S . \u00b5, m is the method in S that is deepest in the stack pre.x, and S ' is the \nsuf\u00ad ' .x of S starting with the shallowest method m on S such ' that m ./\u00b5. If all methods on S are \nin \u00b5, then S ' is the empty stack. Though the rules implementing this judgment are somewhat subtle, with \na recursive case (COLLECT), and two base cases (BORDER and END) for collecting stack seg\u00adments, they \nwork together to select one segment of contigu\u00adous methods belonging to one module from the callee end \nof the stack. Applied recursively, this judgment just segments a stack as described in Section 3.4. Communication \nfrom writer stack Sw to reader stack Sr is checked recursively by the judgment G f Sw Sr in Figure 8 \nby peeling stack segments S w and S r from the tips of Sw and Sr, respectively, where mw and mr are the \ndeepest methods in the two segments, and S ' and S ' are the wr suf.xes of the two stacks, respectively. \n We check that S w and S r belong to the same module M =(\u00b5, .C ,.I ) and that for all methods m ' . \nand w Sw all methods mr ' . S r, there exists some group in .C in '' which m is a writer and m is a reader. \n(Recall that stacks wr contain no inlined methods.) These checks are described by '' the judgments .C \nf Sw Sr and . f mw mr. If this segment matching fails, then communication from Sw to Sr is prohibited \nby the speci.cation. Otherwise, if mw and mr, the segment boundary methods, are not writer and reader \nrespectively in any group in the module s communi\u00adcation interface .I , then communication is hidden \nfrom S ' w and S ' by this pair of segments, so no more checking is nec\u00ad r essary; communication from \nSw to Sr is allowed. Otherwise, if some group in the module s communication interface does describe the \nwriter-reader pair (mw,mr), then communica\u00adtion is exposed by these segments and we must recursively \ncheck that the two stack suf.xes S ' and S ' are allowed to wr communicate.  4.4 Properties of Communication-Checked \nPrograms Although the main purpose of our formalization is to de.ne precisely when a dynamic memory operation \nsatis.es or violates a communication speci.cation, an ancillary ben\u00ade.t is that we can reason about important \nproperties of communication-checked program executions. In this sec\u00adtion, we sketch, but for brevity \ndo not prove, meta-theorems on the equivalence of communication-checked and uninstru\u00admented program executions \n(with a special equivalence case for empty speci.cations) and the soundness and precision of communication \nchecking. When describing program executions, we use .* and .* to denote the re.exive and transitive \nclosure of . and ., respectively. To compare uninstrumented and communi\u00adcation-checked executions, we \nmust .rst de.ne the equiva\u00adlence of uninstrumented and instrumented program states. If .t. .S. T(t)=(.(t),S), \nthen (H, .) is equiva\u00adlent to (H, f, T) and we write (H, .) = (H, f, T) or (H, f, T) = (H, .). The initial \nprogram state s0 and the initial instrumented program state S0 are equivalent. Both hold the empty heap \nand the same initial thread states. S0 also holds the empty last-writers map, and stores the empty stack \nfor each thread. Equivalence of Semantics. All executions admitted by a communication-checked program \nare also admitted by the uninstrumented program; given a speci.cation, all execu\u00adtions admitted by the \nuninstrumented program are either ad\u00admitted by the communication-checked program too or con\u00adtain communication \nthat is invalid under the speci.cation: 1. If Program;G f S0 .* S then .s = S such that Program f s0 \n.* s. 2. Given G, if Program f s0 .* s then either:  (a) .S = s such that Program;G f S0 .* S and all \ncommunication in Program f s0 .* s is valid under G, or (b) .S = s such that Program;G f S0 .* S and \nProgram f s0 .* s contains communication that is invalid under G.  Equivalence of Semantics Under the \nEmpty Speci.cation. A special case is that, given a speci.cation where all meth\u00adods are inlined, a communication-checked \nprogram admits all the executions admitted by the uninstrumented program: If Program f s0 .* s, then \n.S = s such that Program; \u00d8f S0 .* S. Soundness. If an uninstrumented program execution per\u00adforms communication \nthat is invalid under a given speci.ca\u00adtion, then the communication-checked version does not ad\u00admit that \nexecution, given the speci.cation: Given G, if Program f s0 .* s and the uninstrumented program execution \nperforms communication that is invalid under G then .S = s such that Program;G f S0 .* S. Precision. \nA communication-checked program admits all executions admitted by the uninstrumented version that do \nnot perform invalid communication under its speci.cation: Given G, if Program f s0 .* s and .S = s such \nthat Program;G f S0 .* S, then Program f s0 .* s contains communication that is invalid under G. 5. Implementation \nIn this section we describe OSHAJAVA,1 our prototype implementation of communication speci.cations for \nJava. OSHAJAVA speci.cations are expressed by Java annotations. At class load time, we use bytecode instrumentation \nto in\u00adstrument each write operation with a communication state update and each read operation with a \ncheck to see if the method communication it causes obeys the speci.cation. The instrumentation causes \nthe program to throw a commu\u00adnication exception if its next step would violate its speci.ca\u00adtion. Though \ndeeper compiler or virtual machine integration might afford more optimization opportunities, our imple\u00admentation \noffers the following useful properties: Portability: OSHAJAVA programs compile and run with any Java \n1.6-compatible compiler and virtual machine.  Interoperability: Every valid Java program is also a valid \nOSHAJAVA program (and vice versa) at both the source and bytecode levels. Every Java program can run \nunmodi.ed and without recompilation under OSHAJAVA (modulo performance overhead) and every OSHAJAVA program \ncan run unmodi.ed and without recompilation under Java (without runtime speci.cation checking).  Flexibility: \nProgrammers can annotate programs incre\u00admentally and mix unannotated and OSHAJAVA-annotated  1 OSHA \nstands for Organized Sharing, the project s original working title.  components indiscriminately in \nprograms running under OSHAJAVA or the standard unchecked Java runtime. The remainder of this section \ndescribes the annotation sys\u00adtem and the runtime system in more detail. 5.1 Speci.cation Annotation System \nBy default, each Java package is a communication module. In practice, we observe that most modules align \nwith pack\u00adages or classes. However, programmers may de.ne their own modules comprised of arbitrary sets \nof methods. In the pres\u00adence of method overloading, group identi.ers are simpler to express than method \nidenti.ers. The annotation processor, run as a plugin to the Java compiler, compiles the speci.ca\u00adtion \nfor each module to an ef.cient form for runtime use. The task of instrumenting the program with speci.cation \nchecks is deferred to runtime to avoid compiling both instrumented and uninstrumented versions of programs. \nSubtyping and Dynamic Dispatch. Our speci.cation lan\u00adguage and dynamic checker work on methods that are \nactu\u00adally called at run-time, so we do not need any special sup\u00adport for method overriding. As a program \ndesign matter, one could argue that an overriding method should perform no more communication external \nto callers than is speci.ed by the overridden method this is just an instance of behavioral subtyping \n[25] but we do not require the speci.cations for the methods to obey this relation. As a practical matter, \na @Super annotation to indicate, the same speci.cation as the method being overridden would work .ne, \nbut we have not suffered from its absence in our experience.  5.2 Runtime System At runtime, OSHAJAVA \ninstruments each class as it is loaded by the JVM. Each .eld f of each object o is tracked by a communication \nstate .eld, inserted by the instrumentor, that stores the last thread to write a value o.f and the call \nstack under which the write was performed. When a thread reads o.f, the runtime .rst checks the communication \nstate for o.f to see if the last write was performed by the same thread or if communication is allowed \nfrom the last-writer call stack to the current call stack. Checking the latter property uses the natural \nalgorithmic version of stack checking as described in De.nition 3 and Section 4.3, incrementally checking \nthat the two stacks have equivalent segmentations and that all communication in each corresponding segment \nis allowed by some group in the speci.cation. Performing the full stack check on every read is prohibitively \nexpensive; fortunately, most memory reads can be checked by simpler means. Checking Optimizations. To \navoid the high cost of a full stack walk for every communicating read, we employ the following series \nof progressively more expensive checks. Each stack has an integer ID and a bit set of the IDs of other \nstacks that are allowed to communicate to this stack. This set is populated lazily as the program runs. \nIn addition to the speci.cation, the runtime maintains a global hash table\u00adbased memo table of pairs \nof stacks that have previously been checked. The checks proceed as follows: 1. If the last write was \ndone by the same thread, the read does not communicate, and is trivially valid. 2. If the writer stack \ns ID is a member of the reader stack s bit set of valid writers, then the communication is valid. 3. \nIf the pair of writer and reader stacks is in the global memo table of valid communicating stacks, then \nthe com\u00admunication is valid. If the number of checks of this pair of stacks that have been satis.ed by \nthe global memo table reaches a certain threshold (currently, 8), then the writer stack is given the \nnext available non-zero ID, and this ID is added to the reader stack s bit set of valid writers. ID and \nbit set updates are synchronized with respect to each other on a given stack, but not with respect to \nbit set membership tests. At worst a membership test that should succeed races with an update and fails, \nreverting to a more expensive check. 4. If the writer stack has never communicated to the reader stack \nbefore, a full stack walk is performed. If this check fails, a communication exception is thrown, otherwise \nthe pair is added to the global memo table.  Section 6.2 discusses the frequency with which each of \nthese stages is used in practice. In summary, nearly all read oper\u00adations are thread-local or validated \nby the bit sets. In typical programs, pairs of call stacks communicate repeatedly, so memoization quickly \npays off. Allocating IDs lazily keeps the bit sets small. In practice, at most 41 inlined call stacks \nreceived IDs in any single execution. Our implementation uses bit sets that can grow to arbitrary size, \nbut for all of the executions we have observed, a single 64-bit long would suf.ce. The thread-local check \nand bit set test are inlined into the body of the method performing the checked read. Other Optimizations. \nA program may generate very large numbers of call stacks, visit the same call stack repeatedly, and visit \nmany call stacks with differing tips but identical tails. Omitting inlined methods from shadow stacks \nreduces the size and number of shadow stacks we need to store and a hash-consing representation limits \ntheir duplication. Each .eld and array element is tracked by a communica\u00adtion state storing the last \nwriter to that .eld or to any element in that array. A runtime option enables array-level tracking for \narray accesses, with one communication state per array, trading precision for memory overhead. Object-level \ncom\u00admunication tracking is not yet implemented, mainly because the memory overheads we have observed \nwith .eld-level tracking are reasonable. Tracking at the object and array granularities is sound and \nprecise if, when thread tr reads an element, all elements in the array are guaranteed to have been written \nlast by the same thread tw. This property is neither uncommon nor pervasive; where it holds, array-level \ntracking can save memory.  Atomicity of Checks and Accesses. Our tool relaxes the soundness requirement \nthat the checks it inserts be atomic with the memory accesses they check. As a result, it is only fully \nsound and precise on race-free programs. How\u00adever, the possibility for unsound communication checking \nbehavior (based on out-of-date communication state) is lim\u00adited to those data that were targets of races. \nOur experiences with various dynamic race detectors suggest that, in practice, forgoing strictly atomic \ncheck-access sequences yields sub\u00adstantial performance bene.ts, while unsound behavior oc\u00adcurs rarely \nif at all, an acceptable trade-off for a debugging tool. Ideally, synchronization is an orthogonal and \nseparately checked concern. 6. Evaluation The goals of our evaluation are to characterize our anno\u00adtation \nlanguage, understand the performance and memory overheads of the OSHAJAVA checking tool, and discuss \ncase studies. We used the multithreaded benchmarks from the Java Grande suite [37] and selected programs \nfrom version 9.12 of the DaCapo benchmark suite [5]. Table 2 shows the applications we annotated for \nour eval\u00aduation. The Java Grande benchmarks are relatively small (at most 1.2K lines of code), but exercise \na variety of communi\u00adcation structures. From the DaCapo benchmark suite, which contains larger-scale \nparallel and concurrent programs, we examine a subset of the applications that exhibit signi.\u00adcant communication \nin contrasting patterns. These applica\u00adtions were selected because they are representative of differ\u00adent \npatterns of communication. Avrora exhibits frequent and complex communication that often crosses module \nbound\u00adaries due to callback patterns. Batik is embarrassingly par\u00adallel; the only communication occurs \nat the top level in the test harness. Xalan communicates frequently but in a more modular way than Avrora. \nWhile there are many possible valid speci.cations for a given program (even the empty speci.cation all \nmeth\u00adods inlined suf.ces), we have attempted to annotate as thoroughly as possible. In particular, we \nhave never inlined methods where meaningful communication seems to occur. However, programmers more familiar \nwith the applications may construct speci.cations differently. For the small Java Grande benchmarks as \nwell as Batik, we are very con.\u00addent that we have annotated all meaningful communication; for the more \ncomplex Avrora and Xalan benchmarks, while there is greater chance that we have missed some meaning\u00adful \ncommunication, we believe we have covered the entire program, especially since unspeci.ed communication \nwould lead to exceptions. We now present evaluations of: (1) speci.cation size, or how many annotations \nwere inserted; (2) speci.cation preci\u00adsion, measuring how much of the speci.ed communication was actually \nexercised; (3) runtime overheads of the check\u00ading tool in both time and space; and (4) a case study of \nXalan and Avrora, the most complex applications we annotated. 6.1 Speci.cation Size and Precision Size \nof Annotations. Table 2 lists the number of anno\u00adtations used in our communication speci.cations for \neach benchmark. The annotation count includes all of the anno\u00adtations described in Section 3: group declarations, \ngroup memberships (multiple-group @Reader and @Writer dec\u00adlarations are considered multiple annotations), \nand explicit module memberships. The Java Grande benchmarks require between 0.5 and 1.5 annotations per \nmethod. These applica\u00adtions are very small and therefore an arti.cially large portion of their methods \nare involved in communication. In contrast, the DaCapo applications have a much lower frequency of annotations: \none for every 50 to 100 methods. Due to their size, our annotations of the Java Grande benchmarks consist \nof just one to three communication mod\u00adules. In the DaCapo suite, Avrora has 7 non-empty modules (those \nwith at least one non-inlined method) with an average of 12 non-inlined methods per module. Xalan has \n6 modules with 7 non-inlined methods per module. As expected, the majority of the methods in the bench\u00admarks \nwe examined could be inlined (and thus unannotated). In the Java Grande benchmarks, about 85% of the \nmeth\u00adods are inlined. Both Avrora and Xalan from DaCapo have greater than 99% of their methods inlined. \nBecause our an\u00adnotation system inlines methods by default, speci.cations can be created by identifying \nthe small set of methods that must be annotated. Dynamic Communication Characteristics. Our measure of \nspeci.cation precision is the proportion of the static spec\u00adi.cation dynamically exercised during a given \nexecution of the program. Our annotation system allows a trade-off be\u00adtween conciseness and precision \nby adding more annota\u00adtions, the programmer can more tightly constrain the com\u00admunication behavior of \nthe program. A good annotation system would provide high precision using a small number of annotations. \nHowever, complete precision may not be attainable or even desirable for all ap\u00adplications. Some looseness \nin speci.cations allows for vari\u00adation in communication patterns across inputs. Loose spec\u00adi.cations \nmay also allow valid communication that does not currently occur but may start to occur as the program \nchanges. Speci.cally, it may be helpful to allow communi\u00adcation between methods that share data and could, \nbut never do, run on different threads. Such a speci.cation corresponds well with intuition regarding \nthe program s behavior and makes the speci.cation robust to future changes in the pro\u00adgram that run the \nmethods on separate threads. Figure 9 shows the proportion of methods and commu\u00adnicating method pairs \ndeclared in the speci.cation that ac\u00adtually communicated at runtime. This is a direct measure  Lines \nof Annotated Non-Empty Total Name Description Code Methods Methods Groups Modules Annotations Crypt IDEA \nencryption 300 17 5 4 1 16 LUFact LU factorization 500 29 6 4 2 15 MolDyn N-body simulation 500 27 16 \n6 2 39 MonteCarlo Financial simulation 1200 172 11 3 1 19 RayTracer 3D ray-tracing 700 77 15 6 3 37 SOR \nLinear system solver 200 13 5 3 1 14 Series Fourier transform 200 15 6 2 1 10 SparseMatmult Matrix multiplication \n200 12 4 2 1 9 Avrora Sensor network micro\u00ad 70,000 9775 85 17 7 175 controller simulator Batik SVG image \nrenderer 190,000 15547 8 2 2 16 Xalan XSLT/XPath interpreter 180,000 7854 42 7 6 90 Table 2. Summary \nof benchmarks and their annotations. Total Annotations counts all the annotations described in Section \n3. Non-empty modules have at least one non-inlined method. Lines of code were counted by David A. Wheeler \ns SLOCCount.  Figure 9. Proportion of the speci.cation exercised during a single run of each benchmark \non 8 threads. The .rst bar indicates the percentage of non-inlined methods that actu\u00adally communicated \nwith at least one other method. The sec\u00adond indicates the percentage of pairs of methods allowed to communicate \nthat actually communicated. of precision. As expected, the simpler applications (Java Grande) had a much \nhigher proportion ( 80% vs. 30% on average) of communicating methods than the larger applica\u00adtions (DaCapo). \nThe same applies to communicating pairs of methods ( 60% vs. 10% on average). This is largely due to \nthe effect described earlier: the speci.cations conserva\u00adtively allow communication between pairs of \nmethods that would communicate if they ever ran on different threads. In Batik, only 1 method pair communicates \nout of 19 pairs al\u00adlowed to communicate; all of the unexercised method pairs fall into the above category \nof methods that would communi\u00adcate if they did not always run on the same thread. Recall that the annotations \nmeasured are .rst impressions by program\u00admers unfamiliar with the applications details: further study \ncould likely improve precision. In order to make speci.cation feasible, the number of methods in each \nstack segment (see Section 3.4) should be small: the programmer must allow every pair of reader and writer \nmethods in a pair of stack segments to communi\u00adcate, and the number of method pairs grows quickly with \nthe size of module segments. A programmer using OSHAJAVA can keep these all-to-all checks small by dividing \nunrelated groups of methods into communication modules and by in\u00adlining most methods. For the Java Grande \nbenchmarks, the average number of methods per stack segment is between 1 and 1.5. For two of the benchmarks \n(Crypt and LUFact), ev\u00adery segment had exactly 1 method. In DaCapo, Avrora has 1.8 methods per stack \nsegment; Xalan s average is close to 1 while Batik s is exactly 1. Communication modules and in\u00adlining \neffectively keep OSHAJAVA s all-to-all checks small.  6.2 Performance We ran performance and pro.ling \nexperiments for the OSHAJAVA runtime on an 8-core 2.8GHz Intel Xeon E5462 machine with 10GB of memory, \nrunning Ubuntu GNU/Linux 8.10 and the HotSpot 64-bit client VM 1.6.0 with maximum heap size set to 8GB. \nWe ran each benchmark 10 times in each con.guration, measuring execution time and memory usage and preceding \neach set of 10 by a warmup run. For the Java Grande benchmarks, we used the largest available inputs; \nfor DaCapo we used the default inputs. Execution Time. Figure 10 shows the average execution time of \nbenchmarks run on OSHAJAVA, con.gured to use 1, 2, 4, or 8 threads with element-and array-level communica\u00adtion \ntracking, normalized to the average execution time on Java with the same number of threads. The DaCapo \nbench\u00admarks were run with default options (yielding 7 threads for Avrora, 8 for Batik, and 9 for Xalan). \nSingle-threaded exe\u00adcutions are included to demonstrate the baseline overheads introduced by OSHAJAVA. \nOverall, the performance is quite reasonable for a debugging tool. Most benchmarks experi\u00adence 5-15x \nslowdown, trending towards the lower half of this interval. In general, performance with element-level \ntracking is on par with array-level tracking.  There are four major exceptions to these trends: Series, \nwhich has overheads between 0.1% and 2%, spends nearly all of its time inside a tight loop performing \n.oating point operations on local variables, so slow .eld or array accesses are a non-issue. SparseMatmult \nscales very poorly with array-level communication tracking, running 24.1 times as long as under Java \nwith 8 threads. However, it scales nicely with element-level tracking, suggesting that contention on \nshared array states is a bottleneck. Indeed, sparse matrix multiplication is known to be highly cache-sensitive. \nWhen many threads concurrently access different elements of an array under array-level tracking, caches \ncontend heavily on the array s state, causing increased cache-coherence traf.c, and likely affecting \nthe scaling properties of SparseMat\u00admult. MonteCarlo and Xalan run 1.5 to 2 times as slow with element-level \ntracking as with array-level tracking. This slowdown is due in part to garbage collection burden: MonteCarlo \nspends almost 4 and Xalan roughly 7 times longer garbage collecting with element-level tracking than \nwith array-level tracking. Memory Overhead. Figure 11 shows the average peak memory usage of benchmarks \nrun on OSHAJAVA with array-level and array element-level communication track\u00ading, normalized to the average \npeak memory usage on Java.2 Memory overhead is fairly consistent across all numbers of threads; these \ndata represent averages over runs with 8 threads. Array-level tracking overheads are quite low for all \nof the Java Grande benchmarks except MolDyn and Monte-Carlo. Since many of these programs use arrays \nheavily, the overhead of element-level tracking is larger. Crypt s high memory overhead of 34.3x (roughly \n5.4GB total) with element-level tracking results from its allocation of three 50,000,000-element byte \narrays for the large input we tested. 2 We believe RayTracer has a lower memory footprint running under \nOSHAJAVA because garbage collection is triggered more frequently. Figure 11. Average peak memory usage \nof instrumented executions with array-and element-level communication tracking, normalized to average \npeak memory usage of unin\u00adstrumented executions. Effectiveness of Optimizations. We also pro.led commu\u00adnication \nand checking in each of the benchmarks. For sim\u00adplicity, we present this data aggregated across all thread \ncon\u00ad.gurations for each benchmark. Figure 12 shows the distri\u00adbution of successful communication checks \nover the stage in the checking algorithm at which they succeeded. Communi\u00adcation checking slow paths \nare elided from this plot because they are not even visible when included. In fact, fewer than 1 in 10,000,000 \nreads were not thread-local or validated by the bit set memo table; less than two-thirds of these required \nstack walks. Across all benchmarks, the largest number of stack walks in a single execution was 697, \nwhile the number of communications ranged from a few thousand to 6 billion, with most executions performing \ntens of millions to hun\u00addreds of millions of communications. It is clear that aggres\u00adsive memoization \nof valid pairs of communicating stacks makes dynamic communication checking tractable. Indeed, checking \ncommunication between deep stacks is no more costly than checking communication between empty stacks \nsince stack walks are performed exceedingly rarely. In the programs we have considered, performance is \nnot affected by the depth or precision of a speci.cation.  Figure 12 also illustrates soundness and \nprecision is\u00adsues with array-level communication tracking (as discussed in Section 5.2). In most of the \nJava Grande multithreaded benchmarks, array-level is imprecise. Communication struc\u00adture in these programs \nis quite stable, so a difference in the rate of apparent communication under array-vs. element\u00adlevel \nis a rough indicator of how much false communication or false non-communication occurs with array-level \ntrack\u00ading. SOR observes more communications with array-level tracking, suggesting false communication. \nRayTracer falls at the opposite extreme: in reality, 97% of its reads commu\u00adnicate, but only 1% appear \nto communicate with array-level tracking. The explanation is simple: Under array-level track\u00ading, when \na thread t writes an element, it updates the array s last writer to t. If no other threads write elements \nof the array before t s next element read, it appears thread-local, regard\u00adless of which thread last \nwrote that particular element. This pattern is common in RayTracer. Thus if memory is not a concern, \nthere is no reason to sacri.ce soundness and precision for array-level tracking. As observed previously, \nelement-level tracking has insignif\u00adicant performance impact in most of our benchmarks. The two exceptions \nto this trend, MonteCarlo and Xalan, exhibit little or no false communication or non-communication with \narray-level tracking.  6.3 Case Studies We now discuss some of our experience annotating Xalan and Avrora, \nthe two most complex applications we consid\u00adered. We discovered communication properties by introduc\u00ading \nrestrictive speci.cations and successively relaxing them based on observed violations. We also used a \nsimple tool that enumerates pairs of communicating methods. While pro\u00adgrammers may approach the process \ndifferently when an\u00adnotating programs incrementally as they are constructed, we believe our experience \nis representative of the process of an\u00adnotating existing programs. @Group(\"ListLinks\") class TransactionalList \n{ Link head; Link tail; @Reader({\"ListLinks\"}) @Writer({\"ListLinks\"}) void add(Object value) { ... tail.next \n= new Link(value); tail = tail.next; } } class EventList extends TransactionalList { @Reader({\"ListLinks\"}) \nvoid fireAll() { for (Link pos = head; pos != null; pos = pos.next) ((Event)pos.object).fire(); } } Figure \n13. A class in the Avrora benchmark from DaCapo that causes communication across callbacks. The desired \nannotations shown would prohibit this communication. Simple Speci.cations. One advantage of our communi\u00adcation-centric \napproach to speci.cation is its ability to dis\u00adtinguish which methods can write shared data. In data\u00adcentric \nsystems such as SharC [3], shared data can be de\u00adclared read-only to prevent modi.cation after initialization, \nbut the programmer cannot selectively allow read write access to certain methods. The benchmarks we examined \ncontained several methods that needed @Reader but not @Writer annotations, suggesting that this distinction \nis use\u00adful in specifying real communication patterns. The large Java programs we examined often exhibited \ncommunication of object references that does not intuitively correspond to communication of data. For \ninstance, many threads in Avrora use a global screen writer object. With strictly communication-centric \nannotations, every method that uses the screen writer must be allowed to read from the method that creates \nit. While this annotation strategy pre\u00adcisely describes the communication pattern, a system com\u00adbining \ncommunication-and data-centric speci.cation styles might allow the annotation to be more succinct by \nsimply declaring the screen writer object read-only. Modules and Abstraction. We found that, in the common \ncase, communicating pairs of stacks have equivalent seg\u00admentations, as described in Section 3.4. Mismatched \nmod\u00adule sequences are in our experience con.ned to callback sit\u00aduations, in which a module appears once \non one stack but twice on the other (i.e., two methods from one module are separated by at least one \nmethod from a different module). Avrora serves as a case study for this communication pattern.  In Figure \n13, an EventList class extends a linked list class to invoke methods on simulation event objects at a \nlater time. It would be desirable to place the list class in a separate module to avoid exposing incidental \ncommunication through its private .elds. However, using this strategy, because the events .red by the \nEventList read data written outside of events, communication occurs from a stack without methods from \nthe list module to one with such methods. This situation violates the assumption in Section 3.4 that \ncommunicating stack pairs have equivalent segmentations. This assumption prevents us from isolating EventList \nin a communication module, but we can still create a correct speci.cation by inlining the list methods. \nThis strategy yields a less precise speci.cation but permits communication across callbacks. In contrast \nto Avrora, Xalan exhibits modular, isolated communication that rarely crosses module boundaries. For \nexample, Xalan includes a class called FastStringBuffer that matches well with OSHAJAVA s distinction \nbetween communication and interface groups. The buffer s append method must be allowed to communicate \nwith itself because it both reads and writes private bookkeeping data. However, semantic communication \noccurs only from append to out\u00adput methods like toString; clients of the class that only append to the \nbuffer should not also be allowed to read it. We were able to succinctly specify this constraint by using \ndistinct communication and interface speci.cations for the buffer module. Because of the modular nature \nof commu\u00adnication in Xalan, our speci.cation for the benchmark does not need the inlining strategy described \nabove for Avrora. 7. Future Work While our approach to communication checking is entirely dynamic, an \nobvious area for future work is developing a sound, conservative static analysis for checking speci.ca\u00adtions. \nOur primary goal has been to develop a checkable and useful speci.cation language that captures important \nsafety properties of shared-memory programs in ways that match the program structure. We did not allow \nchecking technology to unduly affect (e.g., reduce the expressiveness of) the spec\u00adi.cations. Indeed, \nthe need to check entire call stacks was a challenge our dynamic analysis had to overcome. Static analysis \nwith reasonable precision will also face signi.cant challenges, notably alias analysis for thread-shared \ndata. We believe our program annotations and checking tool are valu\u00adable even without a static-analysis \ncounterpart. Currently, use of callbacks presents an inconvenience for our annotation system, requiring \nthat methods be inlined when they should intuitively be encapsulated in a commu\u00adnication module. Extensions \nto our speci.cation language may allow more precise speci.cation in the case of call\u00adbacks and other \nsituations when communicating stacks do not have equivalent segmentations. In addition, our language \ncurrently has no notion of logical threads code units that should be considered communicating even when \nrun in the same Java thread. As a result, our system does not check communication between distinct tasks \nthat are multiplexed onto the same Java thread (e.g., in a worker thread pool). An additional language \nconstruct could address this issue. Our speci.cations express properties distinct from those addressed \nby other approaches such as data-centric sharing speci.cations. Combining these approaches may offer \nop\u00adportunities for more precise speci.cations, including con\u00adstraints that a given pair of methods may \nonly communicate through a given shared .eld. Communication speci.cations could be used for opti\u00admization \nof shared-memory programs. An operating system could use communication speci.cations to schedule com\u00admunicating \npieces of code to nearby processing units. Or, a compiler could better decide what optimizations to apply \nin threaded code if it knew the communication pattern. Signi.cant avenues exist for optimizing our runtime \nchecker. Static escape analysis and other data .ow analyses could likely identify several opportunities \nfor sound instru\u00admentation elision, improving performance signi.cantly. A JIT compiler that performs \nthread-escape analysis will pro\u00advide some of these bene.ts, but more static analysis could further reduce \nthe overhead of instrumentation. 8. Related Work Much of the work on checking properties of multithreaded \nsoftware has focused on race detection and atomicity check\u00ading. Detecting data races is not a program-speci.c \nissue, so general tools requiring no annotations are successful, using either static [8, 12, 24, 29, \n30, 33, 42, 43] or dy\u00adnamic [9, 17, 36, 38, 41, 48, 49] analysis. That said, type sys\u00adtems and related \nannotation systems can make checking sim\u00adpler and more modular [1, 6, 7, 13 15, 23]. Similarly, atom\u00adicity \nis a general property amenable to static [2, 18, 19, 44] and dynamic [16, 21, 32] approaches. Our communication-centric \napproach complements this work in two ways: (1) it speci.es how shared-memory com\u00admunication occurs rather \nthan data or isolation properties and (2) it focuses on program-speci.c properties with mul\u00adtiple levels \nof abstraction rather than generic properties. A generic property like data-race free or atomic in our \ncommunication-centric view would be does no inter\u00adthread communication, which we can easily specify. \nSuch a speci.cation and .nding violations of it could well prove useful, but we have focused instead \non speci.cations for methods that do communicate and checking that the speci\u00ad.ed communication encompasses \nall actual communication. Also note that does no communication is incomparable to both data-race free \n(write/write races do not communi\u00adcate and communicating methods may or may not be racy) and atomic (an \natomic method may not communicate and a communicating method may not be atomic). In the sense of capturing \nsimple program-speci.c multi\u00adthreading properties, some recent work shares many of our goals despite \nbeing data-centric. For example, SharC [3] lets programmers specify data-sharing rules at an object gran\u00adularity: \nfor example, read-only and read-write. Shoal [4] builds on SharC and lets programmers assign rules to \nen\u00adtire data-structures, as opposed to individual objects. More recently, Martin et al. [27] present \nan annotation technique for C/C++ programs that lets programmers declare data\u00adownership properties (which \nthreads own which data). As in our work, they use a dynamic tool to check speci.cations.  Approaches \nto verifying more sophisticated properties of multithreaded programs include abstraction and model\u00adchecking \n(e.g., [10, 46]) and modular theorem-prover tech\u00adniques using assume-guarantee reasoning (e.g., [11, \n20]). Richer notions of program veri.cation naturally require more sophisticated annotations than our \nspeci.cations; our goal is to focus on properties, namely communication, unique to multithreading, rather \nthan more general program veri.cation. Our work is complementary to model-checking techniques based on \nexhaustive concurrent program test\u00ading [22, 28, 40]: one could use exhaustive testing to deter\u00admine (non)conformance \nto our speci.cations. Thread col\u00adoring [39] uses statically checked annotations to specify which threads \nare allowed to execute what code based on the roles they ful.ll. The code communication properties that \nare speci.ed and checked in our system are comple\u00admentary to those veri.ed by this approach. In a rough \nsense, static pointer analysis for multithreaded programs [31, 34, 35] is related: If no memory accesses \nin methods m1 and m2 or any methods they call have over\u00adlapping points-to sets, then m1 and m2 de.nitely \ndo not communicate via shared memory. However, points-to sets do not distinguish intra-thread and inter-thread \ncommunica\u00adtion, which is central to our work. Pointer analysis is also not naturally suited to working \nfor multiple levels of abstraction and encapsulating communication within modules. Recent work on code-based \ninter-thread communica\u00adtion invariants has employed hardware to record and ana\u00adlyze instruction-level \ninter-thread communication patterns in program executions for debugging [26] or inference of likely intended \ncommunication invariants to enforce in sub\u00adsequent program executions [47]. Another approach records \nfunction-level communication for program understanding and characterization [45]. Our system is pure \nsoftware and employs explicit speci.cations to de.ne precisely what com\u00admunication is allowed at the \nmethod level. 9. Conclusions We have developed new communication-centric, simple, partial speci.cations \nfor shared-memory multithreaded pro\u00adgrams. The key idea is to specify which methods commu\u00adnicate with \neach other across threads. Essential to our tech\u00adnique is a treatment of (transitive) callees that is \nmodular and allows speci.cations at multiple levels of abstraction. We have implemented a dynamic-analysis \ntool to check our speci.cations and shown that it is effective at checking non\u00adtrivial speci.cations \nfor executions of benchmark programs. Acknowledgments This work was supported in part by an Anne Dinning \n-Michael Wolf Endowed Regental Fellowship, an ARCS Foundation Fellowship, a Hacherl Endowed Graduate \nFel\u00adlowship, and a Microsoft Faculty Fellowship. We thank Tom Bergan and Marius Nita for thoughts on \nmodularity, and the SAMPA and WASP groups at the University of Wash\u00adington for useful discussions. We \nalso thank Steve Freund for helpful discussions on dynamic analysis techniques for concurrent programs. \nReferences [1] M. Abadi, C. Flanagan, and S. N. Freund. Types for Safe Locking: Static Race Detection \nfor Java. ACM Transactions on Programming Languages and Systems, 28(2), 2006. [2] R. Agarwal, A. Sasturkar, \nL. Wang, and S. D. Stoller. Op\u00adtimized Run-time Race Detection and Atomicity Checking Using Partial Discovered \nTypes. In IEEE/ACM International Conference on Automated Software Engineering, 2005. [3] Z. Anderson, \nD. Gay, R. Ennals, and E. Brewer. SharC: Checking Data Sharing Strategies for Multithreaded C. In ACM \nConference on Programming Language Design and Im\u00adplementation, 2008. [4] Z. Anderson, D. Gay, and M. \nNaik. Lightweight Annota\u00adtions for Controlling Sharing in Concurrent Data Structures. In ACM Conference \non Programming Language Design and Implementation, 2009. [5] S. M. Blackburn, R. Garner, C. Hoffman, \nA. M. Khan, K. S. McKinley, R. Bentzur, A. Diwan, D. Feinberg, D. Framp\u00adton, S. Z. Guyer, M. Hirzel, \nA. Hosking, M. Jump, H. Lee, J. E. B. Moss, A. Phansalkar, D. Stefanovi\u00b4c, T. VanDrunen, D. von Dincklage, \nand B. Wiedermann. The DaCapo Bench\u00admarks: Java Benchmarking Development and Analysis. In ACM Conference \non Object-Oriented Programming, Systems, Languages, and Applications, 2006. [6] C. Boyapati and M. Rinard. \nA Parameterized Type System for Race-Free Java Programs. In ACM Conference on Object-Oriented Programming, \nSystems, Languages, and Applica\u00adtions, 2001. [7] C. Boyapati, R. Lee, and M. Rinard. Ownership Types \nfor Safe Programming: Preventing Data Races and Deadlocks. In ACM Conference on Object-Oriented Programming, \nSystems, Languages, and Applications, 2002. [8] G.-I. Cheng, M. Feng, C. Leiserson, K. Randall, and A. \nStark. Detecting Data Races in Cilk Programs that Use Locks. In ACM Symposium on Parallel Algorithms \nand Architectures, 1998. [9] J.-D. Choi, K. Lee, A. Loginov, R. O Callahan, V. Sarkar, and M. Sridharan. \nEf.cient and Precise Datarace Detection for Multithreaded Object-Oriented Programs. In ACM Confer\u00adence \non Programming Language Design and Implementation, 2002.  [10] M. Dwyer, J. Hatcliff, R. Joehanes, S. \nLaubach, C. Pasareanu, Robby, W. Visser, and H. Zheng. Tool-supported Program Abstraction for Finite-state \nVeri.cation. In ACM/IEEE Inter\u00adnational Conference on Software Engineering, 2001. [11] T. Elmas, S. Qadeer, \nand S. Tasiran. A Calculus of Atomic Actions. In ACM Symposium on Principles of Programming Languages, \n2009. [12] D. Engler and K. Ashcraft. RacerX: Effective, Static Detec\u00adtion of Race Conditions and Deadlocks. \nIn ACM Symposium on Operating Systems Principles, 2003. [13] C. Flanagan and M. Abadi. Object Types Against \nRaces. In International Conference on Concurrency Theory, volume 1664 of Lecture Notes in Computer Science. \nSpringer-Verlag, 1999. [14] C. Flanagan and M. Abadi. Types for Safe Locking. In Eu\u00adropean Symposium \non Programming, volume 1576 of Lecture Notes in Computer Science. Springer-Verlag, 1999. [15] C. Flanagan \nand S. N. Freund. Type-based Race Detection for Java. In ACM Conference on Programming Language Design \nand Implementation, 2000. [16] C. Flanagan and S. N. Freund. Atomizer: A Dynamic Atomic\u00adity Checker for \nMultithreaded Programs. In ACM Symposium on Principles of Programming Languages, 2004. [17] C. Flanagan \nand S. N. Freund. FastTrack: Ef.cient and Precise Dynamic Race Detection. In ACM Conference on Program\u00adming \nLanguage Design and Implementation, 2009. [18] C. Flanagan and S. Qadeer. A Type And Effect System For \nAtomicity. In ACM Conference on Programming Language Design and Implementation, 2003. [19] C. Flanagan \nand S. Qadeer. Types for Atomicity. In ACM Workshop on Types in Language Design and Implementation, 2003. \n[20] C. Flanagan, S. N. Freund, S. Qadeer, and S. A. Seshia. Modu\u00adlar Veri.cation of Multithreaded Programs. \nTheoretical Com\u00adputer Science, 338(1 3), 2005. [21] C. Flanagan, S. N. Freund, and J. Yi. Velodrome: \nA Sound And Complete Dynamic Atomicity Checker for Mul\u00adtithreaded Programs. In ACM Conference on Programming \nLanguage Design and Implementation, 2008. [22] P. Godefroid. Model Checking for Programming Languages \nUsing Verisoft. In ACM Symposium on Principles of Pro\u00adgramming Languages, 1997. [23] D. Grossman. Type-Safe \nMultithreading in Cyclone. In ACM Workshop on Types in Language Design and Implementation, 2003. [24] \nT. A. Henzinger, R. Jhala, and R. Majumdar. Race Checking by Context Inference. In ACM Conference on \nProgramming Language Design and Implementation, 2004. [25] B. H. Liskov and J. M. Wing. A Behavioral \nNotion of Sub\u00adtyping. ACM Transactions on Programming Languages and Systems, 16(6), 1994. [26] B. Lucia \nand L. Ceze. Finding Concurrency Bugs with Context-Aware Communication Graphs. In ACM/IEEE Inter\u00adnational \nSymposium on Computer Architecture, 2009. [27] J.-P. Martin, M. Hicks, M. Costa, P. Akritidis, and M. \nCas\u00adtro. Dynamically Checking Ownership Policies in Concurrent C/C++ Programs. In ACM Symposium on Principles \nof Pro\u00adgramming Languages, 2010. [28] M. Musuvathi, S. Qadeer, T. Ball, G. Basler, P. Nainar, and I. \nNeamtiu. Finding and Reproducing Heisenbugs in Concur\u00adrent Programs. In USENIX Symposium on Operating \nSystems Design and Implementation, 2008. [29] M. Naik and A. Aiken. Conditional Must Not Aliasing for \nStatic Race Detection. In ACM Symposium on Principles of Programming Languages, 2007. [30] M. Naik, A. \nAiken, and J. Whaley. Effective Static Race Detection for Java. In ACM Conference on Programming Language \nDesign and Implementation, 2006. [31] M. G. Nanda and S. Ramesh. Pointer Analysis of Multi\u00adthreaded Java \nPrograms. In ACM Symposium on Applied Computing, 2003. [32] C.-S. Park and K. Sen. Randomized Active \nAtomicity Vio\u00adlation Detection in Concurrent Programs. In ACM Interna\u00adtional Symposium on the Foundations \nof Software Engineer\u00ading, 2008. [33] P. Pratikakis, J. S. Foster, and M. Hicks. LOCKSMITH: Context-Sensitive \nCorrelation Analysis for Race Detection. In ACM Conference on Programming Language Design and Implementation, \n2006. [34] R. Rugina and M. C. Rinard. Pointer Analysis for Structured Parallel Programs. ACM Transactions \non Programming Lan\u00adguages and Systems, 25(1), 2003. [35] A. Salcianu and M. Rinard. Pointer and Escape \nAnalysis for Multithreaded Programs. In ACM Symposium on Principles and Practice of Parallel Programming, \n2001. [36] S. Savage, M. Burrows, G. Nelson, P. Sobalvarro, and T. An\u00adderson. Eraser: A Dynamic Data \nRace Detector for Multi\u00adthreaded Programs. ACM Transactions on Computer Systems, 15(4), 1997. [37] L. \nA. Smith, J. M. Bull, and J. Obdrz\u00b4alek. A Parallel Java Grande Benchmark Suite. In ACM/IEEE International \nCon\u00adference for High Performance Computing and Networking, 2001. [38] N. Sterling. A Static Data Race \nAnalysis Tool. In USENIX Winter Technical Conference, 1993. [39] D. F. Sutherland and W. L. Scherlis. \nComposable Thread Coloring. In ACM Symposium on Principles and Practice of Parallel Programming, 2010. \n[40] W. Visser, G. P. B. Klaus Havelund, and S. Park. Model Checking Programs. In IEEE/ACM International \nConference on Automated Software Engineering, 2000. [41] C. von Praun and T. Gross. Object Race Detection. \nIn ACM Conference on Object-Oriented Programming, Systems, Languages, and Applications, 2001. [42] C. \nvon Praun and T. R. Gross. Static Con.ict Analysis for Multi-Threaded Object-Oriented Programs. In ACM \nConfer\u00adence on Programming Language Design and Implementation, 2003.  [43] J. Voung, R. Jhala, and S. \nLerner. RELAY: Static Race Detec\u00adtion on Millions of Lines of Code. In ACM International Sym\u00adposium on \nthe Foundations of Software Engineering, 2007. [44] L. Wang and S. D. Stoller. Accurate and Ef.cient \nRuntime Detection of Atomicity Errors in Concurrent Programs. In ACM Symposium on Principles and Practice \nof Parallel Pro\u00adgramming, 2006. [45] B. P. Wood, J. Devietti, L. Ceze, and D. Grossman. Code-Centric \nCommunication Graphs for Shared-Memory Multi\u00adthreaded Programs. Technical Report UW-CSE-09-05-02, University \nof Washington, 2009. [46] E. Yahav. Verifying Safety Properties of Concurrent Java Pro\u00adgrams Using 3-value \nLogic. In ACM Symposium on Principles of Programming Languages, 2001. [47] J. Yu and S. Narayanasamy. \nA Case for an Interleaving Constrained Shared-Memory Multi-Processor. In ACM/IEEE International Symposium \non Computer Architecture, 2009. [48] Y. Yu, T. Rodeheffer, and W. Chen. RaceTrack: Ef.cient Detection \nof Data Race Conditions via Adaptive Tracking. In ACM Symposium on Operating Systems Principles, 2005. \n[49] P. Zhou, R. Teodorescu, and Y. Zhou. HARD: Hardware-Assisted Lockset-based Race Detection. In International \nSym\u00adposium on High-Performance Computer Architecture, 2007.    \n\t\t\t", "proc_id": "1869459", "abstract": "<p>In this paper we propose a communication-centric approach to specifying and checking how multithreaded programs use shared memory to perform inter-thread communication. Our approach complements past efforts for improving the safety of multithreaded programs such as race detection and atomicity checking. Unlike prior work, we focus on what pieces of code are allowed to communicate with one another, as opposed to declaring what data items are shared or what code blocks should be atomic. We develop a language that supports composable specifications at multiple levels of abstraction and that allows libraries to specify whether or not shared-memory communication is exposed to clients. The precise meaning of a specification is given with a formal semantics we present. We have developed a dynamic-analysis tool for Java that observes program execution to see if it obeys a specification. We report results for using the tool on several benchmark programs to which we added specifications, concluding that our approach matches the modular structure of multithreaded applications and that our tool is performant enough for use in development and testing.</p>", "authors": [{"name": "Benjamin P. Wood", "author_profile_id": "81470647599", "affiliation": "University of Washington, Seattle, WA, USA", "person_id": "P2354026", "email_address": "", "orcid_id": ""}, {"name": "Adrian Sampson", "author_profile_id": "81470649493", "affiliation": "University of Washington, Seattle, WA, USA", "person_id": "P2354027", "email_address": "", "orcid_id": ""}, {"name": "Luis Ceze", "author_profile_id": "81100112680", "affiliation": "University of Washington, Seattle, WA, USA", "person_id": "P2354028", "email_address": "", "orcid_id": ""}, {"name": "Dan Grossman", "author_profile_id": "81405594870", "affiliation": "University of Washington, Seattle, WA, USA", "person_id": "P2354029", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1869459.1869473", "year": "2010", "article_id": "1869473", "conference": "OOPSLA", "title": "Composable specifications for structured shared-memory communication", "url": "http://dl.acm.org/citation.cfm?id=1869473"}