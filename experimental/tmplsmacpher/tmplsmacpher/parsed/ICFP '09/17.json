{"article_publication_date": "08-31-2009", "fulltext": "\n Beautiful Differentiation Conal M. Elliott LambdaPix conal@conal.net Abstract Automatic differentiation \n(AD) is a precise, ef.cient, and conve\u00adnient method for computing derivatives of functions. Its forward\u00admode \nimplementation can be quite simple even when extended to compute all of the higher-order derivatives \nas well. The higher\u00addimensional case has also been tackled, though with extra complex\u00adity. This paper \ndevelops an implementation of higher-dimensional, higher-order, forward-mode AD in the extremely general \nand ele\u00adgant setting of calculus on manifolds and derives that implementa\u00adtion from a simple and precise \nspeci.cation. In order to motivate and discover the implementation, the paper poses the question What \ndoes AD mean, independently of imple\u00admentation? An answer arises in the form of naturality of sampling \na function and its derivative. Automatic differentiation .ows out of this naturality condition, together \nwith the chain rule. Graduat\u00ading from .rst-order to higher-order AD corresponds to sampling all derivatives \ninstead of just one. Next, the setting is expanded to arbi\u00adtrary vector spaces, in which derivative values \nare linear maps. The speci.cation of AD adapts to this elegant and very general setting, which even simpli.es \nthe development. Categories and Subject Descriptors G.1.4 [Mathematics of Com\u00adputing]: Numerical Analysis \nQuadrature and Numerical Differ\u00adentiation General Terms Algorithms, Design, Theory Keywords Automatic \ndifferentiation, program derivation 1. Introduction Derivatives are useful in a variety of application \nareas, including root-.nding, optimization, curve and surface tessellation, and com\u00adputation of surface \nnormals for 3D rendering. Considering the use\u00adfulness of derivatives, it is worthwhile to .nd software \nmethods that are simple to implement,  simple to prove correct,  convenient,  accurate,  ef.cient, \nand  general.  Permission to make digital or hard copies of all or part of this work for personal \nor classroom use is granted without fee provided that copies are not made or distributed for pro.t or \ncommercial advantage and that copies bear this notice and the full citation on the .rst page. To copy \notherwise, to republish, to post on servers or to redistribute to lists, requires prior speci.c permission \nand/or a fee. ICFP 09, August 31 September 2, 2009, Edinburgh, Scotland, UK. Copyright c &#38;#169; 2009 \nACM 978-1-60558-332-7/09/08. . . $5.00 d (u + v) = du + dv d (u \u00b7 v) = dv \u00b7 u + du \u00b7 v d (-u) =-du d \n(e u) = du \u00b7 e u d (log u) = d u/u vv d ( u) = d u/(2 \u00b7 u) d (sin u) = du \u00b7 cos u d (cos u) = du \u00b7 (- \nsin u) v d (sin-1 u) = d u/ 1 - u2 v 2 d (cos-1 u) =-d u/ 1 - u d (tan-1 u) = d u/(u 2 + 1) d (sinh u) \n= du \u00b7 cosh u d (cosh u) = du \u00b7 sinh u v d (sinh-1 u) = du/ u2 +1 v d (cosh-1 u) =-du/ u2 - 1 d (tanh-1 \nu) = d u/(1 - u 2) Figure 1. Some rules for symbolic differentiation One differentiation method is numeric \napproximation, using simple .nite differences. This method is based on the de.nition of (scalar) derivative: \nf(x + h) - fx df x = lim (1) h.0 h (The left-hand side reads the derivative of f at x .) The approxi\u00admation \nmethod uses f(x + h) - fx df x h for a small value of h. While very simple, this method is often inaccurate, \ndue to choosing either too large or too small a value for h. (Small val\u00adues of h lead to rounding errors.) \nMore sophisticated variations improve accuracy while sacri.cing simplicity. A second method is symbolic \ndifferentiation. Instead of using the limit-based de.nition directly, the symbolic method uses a collection \nof rules, such as those in Figure 1 There are two main drawbacks to the symbolic approach to dif\u00adferentiation. \nOne is simply the inconvience of symbolic methods, requiring access to and transformation of the source \ncode of com\u00adputation, and placing restrictions on that source code. A second drawback is that implementations \ntend to be quite expensive and in particular perform redundant computation. A third method is the topic \nof this paper (and many others), namely automatic differentiation (also called algorithmic differ\u00adentiation \n), or AD . There are forward and reverse variations ( modes ) of AD, as well as mixtures of the two. \nThis paper con\u00adsiders only the forward-mode. The idea of AD is to simultane\u00adously manipulate values and \nderivatives. Overloading of the stan\u00addard numerical operations makes this combined manipulation as data \nDa = Daa deriving (Eq, Show) constD :: Num a . a . Da constD x = Dx 0 idD :: Num a . a . Da idD x = Dx \n1 instance Num a . Num (Da) where fromInteger x = constD (fromInteger x) '' Dxx' + Dyy' = D (x + y)(x+ \ny) Dxx' * Dyy' = D (x * y)(y' * x + x' * y) negate (Dxx')= D (negate x)(negate x') signum (Dx )= D (signum \nx)0 abs (Dxx')= D (abs x)(x' * signum x) instance Fractional x . Fractional (Dx) where fromRational \nx = constD (fromRational x) ' recip (Dxx')= D (recip x)(x/ sqr x) sqr :: Num a . a . a sqr x = x * x \ninstance Floating x . Floating (Dx) where p = constD p exp (Dxx')= D (exp x)(x' * exp x) ' log (Dxx')= \nD (log x)(x/ x) ' sqrt (Dxx')= D (sqrt x)(x/ (2 * sqrt x)) sin (Dxx')= D (sin x)(x' * cos x) cos (Dxx')= \nD (cos x)(x' * (-sin x)) asin (Dxx')= D (asin x)(x' / sqrt (1 - sqr x)) ' acos (Dxx')= D (acos x)(x/ \n(-sqrt (1 - sqr x))) ... Figure 2. First-order, scalar, functional automatic differentiation convenient \nand elegant as manipulating values without derivatives. Moreover, the implementation of AD can be quite \nsimple as well. For instance, Figure 2 gives a simple, functional (foward-mode) AD implementation, packaged \nas a data type D and a collection of numeric type class instances. Every operation acts on a regular \nvalue and a derivative value in tandem. (The derivatives for abs and signum need more care at 0.) As \nan example, de.ne f1 :: Floating a . a . a f1 z = sqrt (3 * sin z) and try it out in GHCi: *Main> f1 \n(D 2 1) D 1.6516332160855343 (-0.3779412091869595) To test correctness, here is a symbolically differentiated \nversion: f2 :: Floating a . a . Da f2 x = D (f1 x) (3 * cos x / (2 * sqrt (3 * sin x))) Try it out in \nGHCi: *Main> f2 2 D 1.6516332160855343 (-0.3779412091869595) This AD implementation satis.es most of \nour criteria very well: It is simple to implement. The code matches the familiar laws given in Figure \n1. There are, however, some stylistic improve\u00ad ments to be made in Section 4.  It is simple to verify \ninformally, because of its similarity to the differentiation laws.  It is convenient to use, as shown \nwith f1 above.  It is accurate, as shown above, producing exactly the same result as the symbolic differentiated \ncode, f2.  It is ef.cient, involving no iteration or redundant computation.  The formulation in Figure \n2 does less well with generality: It computes only .rst derivatives.  It applies (correctly) only to \nfunctions over a scalar (one\u00addimensional) domain.  Moreover, proving correctness is hampered by lack \nof a precise speci.cation. Later sections will address these shortcomings. This paper s technical contributions \ninclude the following. A prettier formulation of .rst-order and higher-order forward\u00admode AD using function-based \noverloading (Sections 2, 3 and 4).  A simple formal speci.cation for AD (Section 5).  A systematic \nderivation of .rst-order and higher-order forward\u00admode AD from the speci.cation (Sections 5.1 and 6). \n Reformulation of AD to general vector spaces including (but not limited to) Rm . Rn, from the perspective \nof calculus on manifolds (CoM) (Spivak 1971), and adaptation of the AD derivation to this new setting \n(Section 10).  General and ef.cient formulations of linear maps and bases of vector spaces (using associated \ntypes and memo tries), since the notion of linear map is at the heart of CoM (Appendix A).  2. Friendly \nand precise To start, let s make some cosmetic improvements, which will be carried forward to the more \ngeneral formulations as well. Figure 1 has an informality that is is typical of working math notation, \nbut we can state these properties more precisely. For now, give differentiation the following higher-order \ntype: d :: (a . a) . (a . a) --.rst attempt Then Figure 1 can be made more precise. For instance, the \nsum rule is short-hand for d (.x . ux + vx) = .x . dux + dvx and the log rule means d (.x . log (ux)) \n= .x . dux / ux These more precise formulations are tedious to write and read. Fortunately, there is \nan alternative to replacing Figure 1 with more precise but less human-friendly forms. We can instead \nmake the human-friendly form become machine-friendly. The trick is to add numeric overloadings for functions, \nso that numeric operations apply point-wise. For instance, u + v = .x . ux + vx log u = .x . log (ux) \n Then the informal laws in Figure 1 turn out to be well-de.ned and exactly equivalent to the more precise \nlong-hand versions above. The Functor and Applicative (McBride and Paterson 2008) in\u00ad stances of functions \n(shown in Figure 3) come in quite handy. Figure 4 shows the instances needed to make Figure 1 well\u00ad de.ned \nand correct exactly as stated, by exploiting the Functor and Applicative instances in Figure 3. In fact, \nthese instances work instance Functor ((.) t) where fmapf g = f . g instance Applicative ((.) t) where \npure = const f \u00ae g = .t . (ft)(gt) Consequently, liftA2 huv = .x . h (ux)(vx) liftA3 huvw = .x . h (ux)(vx)(wx) \n... Figure 3. Standard Functor and Applicative instances for func\u00adtions instance Num b . Num (a . b) \nwhere fromInteger = pure . fromInteger negate = fmap negate (+) = liftA2 (+) (*)= liftA2 (*) abs = fmap \nabs signum = fmap signum instance Fractional b . Fractional (a . b) where fromRational = pure . fromRational \nrecip = fmap recip instance Floating b . Floating (a . b) where p = pure p sqrt = fmap sqrt exp = fmap \nexp log = fmap log sin = fmap sin ... Figure 4. Numeric overloadings for function types for any applicative \nfunctor a point that will become important in Section 10. We ll soon see how to exploit this simple, \nprecise notation to improve the style of the de.nitions from Figure 2. 3. A scalar chain rule Many of \nthe laws in Figure 1 look similar: d (fu)= du * f ' u for some function f '. The f ' is not just some \nfunction; it is the derivative of f. The reason for this pattern is that these laws follow from the scalar \nchain rule for derivatives. d (g . f ) x = dg (fx) * df x Using the (*) overloading in Figure 4, the \nchain rule can also be written as follows: d (g . f ) = (dg . f ) * df All but the .rst two rules in \nFigure 1 then follow from the chain rule. For instance, d (sin u) ={ sin on functions } d (sin . u) ={ \nreformulated chain rule } (d sin . u) * du ={ d sin = cos } (cos . u) * du ={ cos on functions } cos \nu * du  The .rst two rules cannot be explained in terms of the scalar chain rule, but can be explained \nvia the generalized chain rule in Section 10. We can implement the scalar chain rule simply, via a new \nin.x operator, (N ), whose arguments are a function and its derivative. in.x 0 N (N ) :: Num a . (a . \na) . (a . a) . (Da . Da) '' '' (f N f )(Daa )= D (fa)(a * fa) This chain rule removes repetition from \nour instances. For in\u00adstance, instance Floating x . Floating (Dx) where p = D p 0 exp = exp N exp log \n= log N recip sqrt = sqrt N .x . recip (2 * sqrt x) sin = sin N cos cos = cos N .x .-sin x asin = asin \nN .x . recip (sqrt (1 - sqr x)) acos = acos N .x .-recip (sqrt (1 - sqr x)) ... 4. Prettier derivatives \nvia function overloading Section 2 gave numeric overloadings for functions in order to make the derivative \nlaws in Figure 1 precise, while retaining their simplicity. We can use these overloadings to make the \nderivative implementation simpler as well. With the help of (N ) and the overloadings in Figure 4, the \ncode in Figure 2 can be simpli.ed to that in Figure 5. 5. What is automatic differentiation, really? \nThe preceding sections describe what AD is, informally, and they present plausible implementations. Let \ns now take a deeper look at AD, in terms of three questions: 1. What does it mean, independently of implementation? \n 2. How do the implementation and its correctness .ow gracefully from that meaning? 3. Where else might \nwe go, guided by answers to the .rst two questions?  5.1 A model for automatic differentiation How do \nwe know whether this AD implementation is correct? We can t even begin to address this question until \nwe .rst answer a more fundamental one: what exactly does its correctness mean? In other words, what speci.cation \nmust our implementation obey? AD has something to do with calculating a function s values and derivative \nvalues simultaneously, so let s start there. toD :: (a . a) . (a . Da) toD f = .x . D (fx)(df x) Or, \nin point-free form, toD f = liftA2 Df (df ) thanks to the Applicative instance in Figure 3. We have \nno implementation of d, so this de.nition of toD will serve as a speci.cation, not an implementation. \n instance Num a . Num (Da) where fromInteger = constD . fromInteger '' '' Dx0 x + Dy0 y = D (x0 + y0)(x \n+ y ) x@(Dx0 x ' ) * y@(Dy0 y ' )= D (x0 * y0)(x ' * y + x * y negate = negate N -1 abs = abs N signum \nsignum = signum N 0 instance Fractional a . Fractional (Da) where fromRational = constD . fromRational \nrecip = recip N -sqr recip instance Floating a . Floating (Da) where p = constD p exp = exp N exp log \n= log N recip sqrt = sqrt N recip (2 * sqrt) sin = sin N cos cos = cos N -sin asin = asin N recip (sqrt \n(1 - sqr)) acos = acos N recip (-sqrt (1 - sqr)) atan = atan N recip (1 + sqr) sinh = sinh N cosh cosh \n= cosh N sinh asinh = asinh N recip (sqrt (1 + sqr)) acosh = acosh N recip (-sqrt (sqr - 1)) atanh = \natanh N recip (1 - sqr) Figure 5. Simpli.ed derivatives using the scalar chain rule and function overloadings \nSince AD is structured as type class instances, one way to spec\u00adify its semantics is by relating it to \na parallel set of standard in\u00adstances, by a principle of type class morphisms, as described in (El\u00ad liott \n2009c,b), which is to say that the interpretation preserves the structure of every method application. \nFor AD, the interpretation function is toD. The Num, Fractional, and Floating morphisms provide the speci.cations \nof the instances: toD (u + v) = toD u + toD v toD (u * v) = toD u * toD v toD (negate u) = negate (toD \nu) toD (sin u) = sin (toD u) toD (cos u) = cos (toD u) ... Note here that the numeric operations are \napplied to values of type a . a on the left, and to values of type a . Da on the right. These (morphism) \nproperties exactly de.ne correctness of any implementation of AD, answering the .rst question: What does \nit mean, independently of implementation?  5.2 Deriving an AD implementation Equipped with a simple, \nformal speci.cation of AD (numeric type class morphisms), we can try to prove that the implementation \nabove satis.es the speci.cation. Better yet, let s do the reverse, us\u00ading the morphism properties to \nderive (discover) the implementa\u00adtion, and prove it correct in the process. The derivations will then \nprovide a starting point for more ambitious forms of AD. 5.2.1 Constants and identity function Value/derivative \npairs for constant functions and the identity func\u00adtion are speci.ed as such: ' ) constD :: Num a . \na . Da idD :: Num a . a . Da constD x = toD (const x) . idD = toD id  To derive implementations, expand \ntoD and simplify. constD x ={ speci.cation }toD (const x) . ={ de.nition of toD }D (const x .)(d (const \nx) .) ={ de.nition of const and its derivative }Dx 0 idD x ={ speci.cation } toD (id x) ={ de.nition \nof toD } D (id x)(d id x) ={ de.nition of id and its derivative } Dx 1  In (Karczmarczuk 2001) and elsewhere, \nidD is called dVar and is sometimes referred to as the variable of differentiation, a term more suited \nto symbolic differentiation than to AD. 5.2.2 Addition Specify addition on D by requiring that toD preserves \nits structure: toD (u + v) = toD u + toD v Expand toD, and simplify both sides, starting on the left: \ntoD (u + v) ={ de.nition of toD } liftA2 D (u + v)(d (u + v)) ={ d (u + v) from Figure 1 } liftA2 D \n(u + v)(du + dv) ={ liftA2 on functions from Figure 3 } .x . D ((u + v) x) ((du + dv) x) ={ (+) on functions \nfrom Figure 4 } .x . D (ux + vx)(dux + dvx)  Then start over with the right-hand side: toD u + toD v \n={ (+) on functions from Figures 3 and 4 }.x . toD u x + toD v x ={ de.nition of toD }.x . D (ux)(dux)+ \nD (vx)(dvx) We need a (+) on D that makes these two .nal forms equal, i.e., .x . D (ux + vx)(dux + dvx) \n= .x . D (ux)(dux)+ D (vx)(dvx) An easy choice is '' ' Daa + Dbb = D (a + b)(a + b ' ) This de.nition \nprovides the missing link, completing the proof that toD (u + v) = toD u + toD v The multiplication \ncase is quite similar (Elliott 2009a).  5.2.3 Sine The speci.cation: toD (sin u) = sin (toD u) Simplify \nthe left-hand side: toD (sin u) ={ de.nition of toD } liftA2 D (sin u)(d (sin u)) ={ d (sin u) } liftA2 \nD (sin u)(du * cos u) ={ liftA2 on functions } .x . D ((sin u) x) ((du * cos u) x) ={ sin, (*) and cos \non functions } .x . D (sin (ux)) (dux * cos (ux))  and then the right: sin (toD u) ={ sin on functions \n} .x . sin (toD u x) ={ de.nition of toD } .x . sin (D (ux)(dux)) So a suf.cient de.nition is sin (Daa \n' )= D (sin a)(a ' * cos a) Or, using the chain rule operator, sin = sin N cos The whole implementation \ncan be derived in exactly this style, answering the second question: How does the implementation and \nits correctness .ow gracefully from that meaning? 6. Higher-order derivatives Let s now turn to the \nthird question: Where else might we go, guided by answers to the .rst two questions? Our next destination \nwill be higher-order derivatives, followed in Section 10 by derivatives over higher-dimensional domains. \nJerzy Karczmarczuk (2001) extended the D representation above to an in.nite lazy tower of derivatives \n. data Da = Da (Da) The toD function easily adapts to this new D type: toD :: (a . a) . (a . Da) toDf \nx = D (fx)(toD (df ) x) or toD f = liftA2 Df (toD (df )) The de.nition of toD comes from simplicity \nand type-correctness. Similarly, let s adapt the previous derivations and see what arises. 6.1 Addition \nSpeci.cation: toD (u + v) = toD u + toD v Simplify the left-hand side: toD (u + v) ={ de.nition of toD \n} liftA2 D (u + v)(toD (d (u + v))) ={ d (u + v) }liftA2 D (u + v)(toD (du + dv)) ={ induction for toD \n/ (+) }liftA2 D (u + v)(toD (du)+ toD (dv)) ={ de.nition of liftA2 and (+) on functions }.x . D (ux + \nvx)(toD (du) x + toD (dv) x) and then the right: toD u + toD v ={ (+) on functions }.x . toD u x + toD \nv x ={ de.nition of toD }.x . D (ux)(toD (dux)) + D (vx)(toD (dvx)) Again, we need a de.nition of (+) \non D that makes the LHS and RHS .nal forms equal, i.e., .x . D (ux + vx)(toD (du) x + toD (dv) x) = .x \n. D (ux)(toD (du) x)+ D (vx)(toD (dv) x) Again, an easy choice is '' '' Da0 a + Db0 b = D (a0 + b0)(a \n+ b ) The induction step above can be made more precise in terms of .xed-point introduction or the generic \napproximation lemma (Hutton and Gibbons 2001). Crucially, the morphism properties are assumed more deeply \ninside of the representation. 6.2 Multiplication Simplifying on the left: toD (u * v) ={ de.nition of \ntoD }liftA2 D (u * v)(toD (d (u * v))) ={ d (u * v) }liftA2 D (u * v)(toD (du * v + dv * u)) ={ induction \nfor toD / (+) }liftA2 D (u * v)(toD (du * v)+ toD (dv * u)) ={ induction for toD / (*) }liftA2 D (u * \nv)(toD (du) * toD v + toD (dv) * toD u) ={ liftA2, (*), (+) on functions }.x . liftA2 D (ux * vx)(toD \n(du) x * toD v x + toD (dv) x * toD u x) and then on the right: toD u * toD v ={ de.nition of toD }liftA2 \nDu (toD (du)) * liftA2 Dv (toD (dv)) ={ liftA2 and (*) on functions }.x . D (ux)(toD (du) x) * D (vx)(toD \n(dv) x) A suf.cient de.nition is a@(Da0 a ' ) * b@(Db0 b ' )= D (a0 + b0)(a ' * b + b ' * a) because \ntoD u x = D (ux)(toD (du) x) toDv x = D (vx)(toD (dv) x)  Note the new element here. The entire D value \n(tower) is used in building the derivative. 6.3 Sine As usual sin shows a common pattern that applies \nto other unary functions. Simplifying on the left-hand side: toD (sin u) ={ de.nition of toD } liftA2 \nD (sin u)(toD (d (sin u))) ={ d (sin u) } liftA2 D (sin u)(toD (du * cos u)) ={ induction for toD / (*) \n} liftA2 D (sin u)(toD (du) * toD (cos u)) ={ induction for toD / cos } liftA2 D (sin u)(toD (du) * cos \n(toD u)) ={ liftA2, sin, cos and (*) on functions } .x . D (sin (ux)) (toD (du) x * cos (toD u x))  \nand then the right: sin (toD u) ={ de.nition of toD } sin (liftA2 Du (toD (du))) ={ liftA2 and sin on \nfunctions } .x . sin (D (ux)(toD (du) x))  To make the left and right .nal forms equal, de.ne sin a@(Da0 \na ' ) = D (sin a0)(a ' * cos a)  6.4 A higher-order, scalar chain rule The derivation above for sin \nshows the form of a chain rule for scalar derivative towers. It is very similar to the formulation in \nSection 3. The only difference are that the second argument (the derivative) gets applied to the whole \ntower instead of a regular value, and so has type Da . Da instead of a . a. in.x 0 N (N ) :: (Num a) \n. (a . a) . (Da . Da) . (Da . Da) '' ' (f N f ) a@(Da0 a )= D (fa0)(a ' * fa) With this new de.nition \nof (N ), all of the chain-rule-based de.ni\u00adtions in Figure 5 (.rst-order derivatives) carry over without \nchange to compute in.nite derivative towers. For instance, instance Floating a . Floating (Da) where \nexp = exp N exp log = log N recip sqrt = sqrt N recip (2 * sqrt) sin = sin N cos cos = cos N -sin ... \n Now the operators and literals on the right of (N ) are overloaded for the type Da . Da. For instance, \nin the de.nition of sqrt, 2 :: Da . Da recip :: (Da . Da) . (Da . Da) (*) ::(Da . Da) . (Da . Da) . \n(Da . Da) 7. Optimizing zeros The derivative implementations above are simple and powerful, but have \nan ef.ciency problem. For polynomial functions (constant, linear, quadratic, etc), all but a few derivatives \nare zero. Consider\u00adable wasted effort can go into multiplying and adding zero deriva\u00adtives, especially \nwith higher-order derivatives. To optimize away the zeros, wrap Maybe around the derivative in D. data \nDa = Da (Maybe (Da)) The implementation can stay very simple and readable, as shown in (Elliott 2009a). \n8. What is a derivative, really? Section 6 showed how easily and beautifully one can construct an in.nite \ntower of derivative values in Haskell programs, while computing plain old values. The trick (from (Karczmarczuk \n2001)) was to overload numeric operators to operate on the following (co)recursive type: data Db = Db \n(Db) This representation, however, works only when differentiating functions from a one-dimensional \ndomain. The reason for this limitation is that only in those cases can the type of derivative values \nbe identi.ed with the type of regular values. Consider a function f :: R2 . R. The value of f at a do\u00admain \nvalue (x, y) has type R, but the derivative of f consists of two partial derivatives. Moreover, the second \nderivative consists of four partial second-order derivatives (or three, depending how you count). A function \nf :: R2 . R3 also has two partial derivatives at each point (x, y), each of which is a triple. That pair \nof triples is commonly written as a three-by-two matrix. Each of these situations has its own derivative \nshape and its own chain rule (for the derivative of function compositions), using plain\u00adold multiplication, \nscalar-times-vector, vector-dot-vector, matrix\u00adtimes-vector, or matrix-times-matrix. Second derivatives \nare more complex and varied. How many forms of derivatives and chain rules are enough? Are we doomed \nto work with a plethora of increasingly complex types of derivatives, as well as the diverse chain rules \nneeded to accommodate all compatible pairs of derivatives? Fortunately, not. There is a single, simple, \nunifying generalization. By reconsidering what we mean by a derivative value, we can see that these various \nforms are all representations of a single notion, and all the chain rules mean the same thing on the \nmeanings of the representations. Let s now look at unifying view of derivatives, which is taken from \ncalculus on manifolds (Spivak 1971). To get an intuitive sense of what s going on with derivatives in \ngeneral, we ll look at some examples. 8.1 One dimension Start with a simple function on real numbers: \nf1 :: R . R f1 x = x 2 +3 * x +1 Writing the derivative of a function f as df , let s now consider the \nquestion: what is df1? We might say that df1 x =2 * x +3 so e.g., df1 5 = 13. In other words, f1 is \nchanging 13 times as fast as its argument, when its argument is passing 5. Rephrased yet again, if dx \nis a very tiny number, then f1 (5 + dx) - f1 5 is very nearly 13 * dx. If f1 maps seconds to meters, \nthen df1 5 is 13 meters per second. So already, we can see that the range of f (meters) and the range \nof df (meters/second) disagree. 8.2 Two dimensions in and one dimension out As a second example, consider \na two-dimensional domain: f2 :: R2 . R f2 (x, y)=2 * x * y +3 * x +5 * y +7 Again, let s consider some \nunits, to get a guess of what kind of thing df2 (x, y) really is. Suppose that f2 measures altitude of \nterrain above a plane, as a function of the position in the plane. You can guess that df (x, y) is going \nto have something to do with how fast the altitude is changing, i.e. the slope, at (x, y). But there \nis no single slope. Instead, there s a slope for every possible compass direction (a hiker s degrees \nof freedom). Now consider the conventional answer to what is df2 (x, y). Since the domain of f2 is R2, \nit has two partial derivatives: df2 (x, y) = (2 * y +3, 2 * x + 5) In our example, these two pieces \nof information correspond to two of the possible slopes. The .rst is the slope if heading directly east, \nand the second if directly north (increasing x and increasing y, respectively). What good does it do \nour hiker to be told just two of the in.nitude of possible slopes at a point? The answer is perhaps magical: \nfor well-behaved terrains, these two pieces of information suf.ce to calculate all (in.nitely many) slopes, \nwith just a bit of math. Every direction can be described as partly east and partly north (negatively \nfor westish and southish directions). Given a direction angle . (where east is zero and north is 90 degrees), \nthe east and north components are cos . and sin ., respectively. When heading in the direction ., the \nslope will be a weighted sum of the north-going slope and the east-going slope, where the weights are \nthese north and south components. Instead of angles, our hiker may prefer thinking directly about the \nnorth and east components of a tiny step from the position (x, y). If the step is small enough and lands \ndx to the east and dy to the north, then the change in altitude, f2 (x +dx, y+dy)-f2 (x, y) is very nearly \nequal to (2 * y + 3) * dx + (2 * x + 5) * dy. If we use (<\u00b7>) to mean dot (inner) product, then this \nchange in altitude is df2 (x, y) <\u00b7> (dx, dy). From this second example, we can see that the derivative \nvalue is not a range value, but also not a rate-of-change of range values. It s a pair of such rates \nplus the know-how to use those rates to determine output changes. 8.3 Two dimensions in and three dimensions \nout Next, imagine moving around on a surface in space, say a torus, and suppose that the surface has \ngrid marks to de.ne a two-dimensional parameter space. As our hiker travels around in the 2D parameter \nspace, his position in 3D space changes accordingly, more .exibly than just an altitude. The hiker s \ntype is then f3 :: R2 . R3 At any position (s, t) in the parameter space, and for every choice of direction \nthrough parameter space, each of the coordinates of the position in 3D space has a rate of change. Again, \nif the function is mathematically well-behaved (differentiable), then all of these rates of change can \nbe summarized in two partial derivatives. This time, however, each partial derivative has components \nin X, Y , and Z, so it takes six numbers to describe the 3D velocities for all possible directions in \nparameter space. These numbers are usually written as a 3-by-2 matrix m (the Jacobian of f3). Given a \nsmall parameter step (dx, dy), the resulting change in 3D position is equal to the product of the derivative \nmatrix and the difference vector, i.e., m timesVec (dx, dy). 8.4 A unifying perspective The examples \nabove use different representations for derivatives: scalar numbers, a vector (pair of numbers), and \na matrix. Common to all of these representations is the ability to turn a small step in the function \ns domain into a resulting step in the range. In f1, the (scalar) derivative c means (c*), i.e., multiply \nby c.  In f2, the (vector) derivative v means (v<\u00b7>).  In f3, the (matrix) derivative m means (m timesVec \n).  So, the common meaning of these derivative representations is a function, and not just any function, \nbut a linear function often called a linear map or linear transformation . Now what about the different \nchain rules, saying to com\u00adbine derivative values via various kinds of products (scalar/scalar, scalar/vector, \nvector/vector dot, matrix/vector)? Each of these prod\u00aducts implements the same abstract notion, which \nis composition of linear maps. 9. The general setting: vector spaces Linear maps (transformations) lie \nat the heart of generalized differ\u00adentiation. Talking about linearity requires a few simple operations, \nwhich are encapsulated in the the abstract interface known from math as a vector space. Vector spaces \nspecialize the more general notion of a group which as an associative and commutative binary operator, \nan iden\u00adtity, and inverses. For convenience, we ll specialize to an additive group which provides addition-friendly \nnames: class AdditiveGroup v where 0 :: v (+) :: v . v . v negate :: v . v  Next, given a .eld s,a vector \nspace over s adds a scaling operation: class AdditiveGroup v . Vector s v where (\u00b7) :: s . v . v  Instances \ninclude Float, Double, and Complex, as well as tuples of vectors, and functions with vector ranges. (By \nvector here, I mean any instance of Vector, recursively). For instance, here are instances for functions: \ninstance AdditiveGroup v . AdditiveGroup (a . v) where 0= pure 0 (+) = liftA2 (+) negate = fmap negate \ninstance Vector s v . Vector s (a . v) where (\u00b7) s = fmap (s\u00b7)  These method de.nitions have a form \nthat can be used with any applicative functor. Other useful operations can be de.ned in terms of these \nmeth\u00adods, e.g., subtraction for additive groups, and linear interpolation for vector spaces. Several \nfamiliar types are vector spaces:  Trivially, the unit type is an additive group and is a vector space \nover every .eld.  Scalar types are vector spaces over themselves, with (\u00b7) = (*).  Tuples add and \nscale component-wise.  Functions add and scale point-wise, i.e., on their range.  Appendix A gives \nan ef.cient representation of linear maps via an associated type (Chakravarty et al. 2005) of bases of \nvector spaces. Without regard to ef.ciency, we could instead represent lin\u00adear maps as a simple wrapper \naround functions, with the invariant that the contained function is indeed linear: newtype u -v = LMap \n(u . v) --simple &#38; inef.cient deriving (AdditiveGroup, Vector) Assume the following abstract interface, \nwhere linear and lapply convert between linear functions and linear maps, and idL and (.\u00b7) are identity \nand composition. linear :: (Vector s u, Vector s v) . (u . v) . (u -v) lapply :: (Vector s u, Vector \ns v) . (u -v) . (u . v) idL :: (Vector s u) . u -u  (.\u00b7) ::(Vector s u, Vector s v) . (v -w) . (u -v) \n. (u -w)  Another operation plays the role of dot products, as used in com\u00adbining partial derivatives. \n(0) :: (Vector s u, Vector s v, Vector s w) . (u -w) . (v -w) . ((u, v) -w)  Semantically, (l 0 m) lapply \n(da, db) = l lapply da + m lapply db which is linear in (da, db). Compare with the usual de.nition of \ndot products: ' '' (s , t ' ) <\u00b7> (da, db)= s \u00b7 da + t \u00b7 db Dually to (0), another way to form linear \nmaps is by zipping : (*) :: (w -u) . (w -v) . (w -(u, v)) which will reappear in generalized form in \nSection 10.3. 10. Generalized derivatives We ve seen what AD means and how and why it works for a specialized \ncase of the derivative of a . a for a one-dimensional (scalar) type a. Now we re ready to tackle the \nspeci.cation and derivation of AD in the much broader setting of vector spaces. Generalized differentiation \nintroduces linear maps: d :: (Vector s u, Vector s v) . (u . v) . (u . (u -v))  In this setting, there \nis a single, universal chain rule (Spivak 1971): d (g . f ) x = dg (fx) .\u00b7 df x where (.\u00b7) is composition \nof linear maps. More succinctly, d (g . f ) = (dg . f ) .\u00b7 df using lifted composition: ( .\u00b7)= liftA2 \n(.\u00b7) 10.1 First-order generalized derivatives The new type of value/derivative pairs has two type parameters: \ndata a N b = Db (a -b) As in Section 5.1, the AD speci.cation centers on a function, toD, that samples \na function and its derivative at a point. This time, it will be easier to swap the parameters of toD: \ntoD :: (Vector s u, Vector s v) . u . (u . v) . u N v toDx f = D (fx)(df x) In Sections 5 and 6, AD \nalgorithms were derived by saying that toD is a morphism over numeric types. The de.nitions of these \nmorphisms and their proofs involved one property for each method. In the generalized setting, we can \ninstead specify and prove three simple morphisms, from which all of the others follow effortlessly. We \nalready saw in Figure 4 that the numeric methods for func\u00ad tions have a simple, systematic form. They \nre all de.ned using fmap, pure, or liftA2 in a simple, regular pattern, e.g., fromInteger = pure . fromInteger \n(*)= liftA2 (*) sin = fmap sin ... Numeric instances for many other applicative functors can be given \nexactly the same method de.nitions. For instance, Maybe a, Either a b, a . b, tries, and syntactic expressions \n(Elliott 2009b). Could these same de.nitions work on a N b, as an implementa\u00adtion of AD? Consider one \nexample: sin = fmap sin For now, assume this de.nition and look at the corresponding numeric morphism \nproperty, i.e., toD x (sin u) = sin (toDx u) Expand the de.nitions of sin on each side, remembering \nthat the left sin is on functions, as given in Figure 4. toD x (fmap sin u) = fmap sin (toDx u) which \nis a special case of the Functor morphism property for toD x. Therefore, proving the Functor morphism \nproperty will cover all of the de.nitions that use fmap. The other two de.nition styles (using pure and \nliftA2) work out similarly. For example, if toD x is an Applicative morphism, then toD x (fromInteger \nn) ={ fromInteger for a N b } toD x (pure (fromInteger n)) ={ toD x is an Applicative morphism } pure \n(fromInteger n) ={ fromInteger for functions } fromInteger n toD x (u * v) ={ (*) for a N b } toD x \n(liftA2 (*) uv) ={ toD x is an Applicative morphism } liftA2 (*)(toDx u)(toDx v) ={ (*) on functions \n} toDx u * toDx v  Now we can see why these de.nitions succeed so often: For applicative functors F \nand G, and function \u00b5 :: Fa . Ga, if \u00b5 is a morphism on Functor and Applicative, and the numeric instances \nfor both F and G are de.ned as in Figure 4, then \u00b5 is a numeric morphism. Thus, we have only to come \nup with Functor and Applicative instances for (N) u such that toD x is a Functor and Applicative morphism. \n10.2 Functor First look at Functor. The morphism condition (naturality), .\u00adexpanded, is fmap g (toDx \nf ) = toD x (fmapg f ) Using the de.nition of toD on the left gives fmap g (D (fx)(df x)) Simplifying \nthe RHS, toD x (fmapg f ) ={ de.nition of toD } D ((fmapg f ) x)(d (fmapg f ) x) ={ de.nition of fmap \nfor functions } D ((g . f ) x)(d (g . f ) x) ={ generalized chain rule } D (g (fx)) (dg (fx) .\u00b7 df x) \n So the morphism condition is equivalent to fmap g (D (fx)(df x)) = D (g (fx)) (dg (fx) .\u00b7 df x)  Now \nit s easy to .nd a suf.cient de.nition: fmap g (D fx dfx)= D (g fx)(d g fx .\u00b7 dfx) This de.nition is \nnot executable, however, since d is not. Fortu\u00adnately, all uses of fmap in the numeric instances involve \nfunctions g whose derivatives are known statically and so can be statically substituted for applications \nof d. To make the static substitution more apparent, refactor the fmap de.nition, as in Section 3. instance \nFunctor ((Na)) where fmap g = g N dg (N ) :: (Vector s u, Vector s v, Vector s w) . (v . w) . (v . (v \n-w)) . (u N v) . (u N w) (g N dg)(D fx dfx)= D (g fx)(dg fx .\u00b7 dfx) This new de.nition makes it easy \nto transform the fmap-based de.nitions systematically into effective versions. After inlining this de.nition \nof fmap, the fmap-based de.nitions look like sin = sin N d sin sqrt = sqrt N d sqrt ...  Every remaining \nuse of d is applied to a function whose derivative is known, so we can replace each use. sin = sin N \ncos sqrt = sqrt N recip (2 * sqrt) ...  Now we have an executable implementation again. 10.3 Applicative/Monoidal \nFunctor is handled, which leaves just Applicative (McBride and Paterson 2008): class Functor f . Applicative \nf where pure :: a . fa (\u00ae) :: f (a . b) . fa . fb  The morphism properties will be easier to demonstrate \nin terms of a type class for (strong) lax monoidal functors: class Monoidal f where unit :: f () (*) \n:: fa . fb . f (a, b)  For instance, the function instance is instance Monoidal ((.) a) where unit = \nconst () f * g = .x . (fx, gx)  The Applicative class is equivalent to Functor+Monoidal (McBride and \nPaterson 2008, Section 7). To get from Functor and Monoidal to Applicative, de.ne pure a = fmap (const \na) unit fs \u00ae xs = fmap app (fs * xs)  where app :: (a . b, a) . b app (f , x)= fx  I ve kept Monoidal \nindependent of Functor, unlike (McBride and Paterson 2008), because the linear map type has unit and \n(*) but is not a functor. (Only linear functions can be mapped over a linear map.) The shift from Applicative \nto Monoidal makes the speci.ca\u00adtion of toD simpler, again as a morphism: unit = toD x unit toDx f * toDx \ng = toD x (f * g)  Filling in the de.nition of toD, unit = D (unit x)(d unit x) D (fx)(df x) * D (gx)(dgx) \n= D ((f * g) x)(d (f * g) x)  The reason for switching from Applicative to Monoidal is that differentiation \nis very simple with the latter: d unit = const 0 d (f * g) = df * dg The ( *) on the right is a variant \nof (*), lifted to work on functions (or other applicative functors) that yield linear maps: ( *)= liftA2 \n(*) We cannot simply pair linear maps to get a linear map. Instead, (*) pairs linear maps point-wise. \nNow simplify the morphism properties, using unit and (*) for functions, and their derivatives: unit = \nD () 0 D (fx)(df x) * D (gx)(dgx) = D (fx, gx)(df x * dgx)  So the following simple de.nitions suf.ce: \nunit = D () 0 D fx dfx * D gx dgx = D (fx, gx)(dfx * dgx) The switch from Applicative to Monoidal introduced \nfmap app (in the de.nition of (\u00ae)). Because of the meaning of fmap on u Nv, we will need a derivative \nfor app. Fortunately, app is fairly easy to differentiate. Allowing only x to vary (while holding f constant), \nfx changes just as f changes at x, so the second partial derivative of app at (f , x) is df x. Allowing \nonly f to vary, fx is linear in f , so it (considered as a linear map) is its own derivative. That is, \nusing ($) as in.x function application, d ($ x) f = linear ($ x) d (f $) x = df x As mentioned in Section \n9, (0) takes the place of dot product for combining contributions from partial derivatives, so d app \n(f , x)= linear ($ x) 0 df x Alternatively, de.ne liftA2 via fmap and (*) instead of app. Either way, \nthe usual derivative rules for (+) and (*) follow from liftA2, and so needn t be implemented explicitly \n(Elliott 2009a). 10.4 Fun with rules Let s back up to our more elegant method de.nitions: (*)= liftA2 \n(*) sin = fmap sin sqrt = fmap sqrt ... Section 10.2 made these de.nitions executable in spite of their \nappeal to the non-executable d by (a) refactoring fmap to split the d from the residual function (N ), \n(b) inlining fmap, and (c) rewriting applications of d with known derivative rules. Instead, we could \nget the compiler to do these steps for us, by specifying the derivatives of known functions as rewrite \nrules (Peyton Jones et al. 2001), e.g., d log = recip d sqrt = recip (2 * sqrt) ...  Notice that these \nde.nitions are simpler and more modular than the standard differentiation rules, as they do not have \nthe chain rule mixed in. With these rules in place, we can use the incredibly simple fmap-based de.nitions \nof our methods. The de.nition of fmap must get inlined so as to reveal the d applications, which then \nget rewritten according to the rules. For\u00adtunately, the fmap de.nition is tiny, which encourages its \ninlining. The current implementation of rewriting in GHC is somewhat fragile, so it may be a while before \nthis sort of technique is robust enough for every day use. 10.5 A problem with type constraints There \nis a problem with the Functor and Monoidal (and hence Applicative) instances derived in above. In each \ncase, the method de.nitions type-check only for type parameters that are vector spaces. The standard \nde.nitions of Functor and Applicative in ..D ((f ))(toDx (d (f )))gxg Generalizedchainrule ={}. .D ((f \n))(toDx ((dg f ) df ))\u00b7gxmorphism,={ .=.}liftA() liftA()\u00b7\u00b722 .. D ((f ))(toDx (dg f ) toDx (df )))\u00b7gxonfunctions \n={=.}fmap () . D ((f ))(toDx (fmap (dg) f ) toDx (df )))\u00b7gx* ={ fmap morphism }D (g (fx)) (fmap (dg)(toDx \nf ) .\u00b7 toD x (df )) Summarizing, toD x is a Functor morphism iff fmap g (D (fx)(toD x (df ))) = D (g \n(fx )) (fmap (dg)(toDx f ) .\u00b7 toD x (df )) Given this form of the morphism condition, and recalling \nthe de.\u00adnition of toD, it s easy to .nd a fmap de.nition for (N ): Haskell do not allow for such constraints. \nThe problem is not in the categorical notions, but in their specialization (often adequate and convenient) \nin the standard Haskell library. For this reason, we ll need a variation on these standard classes, either \nspeci.c for use with vector spaces or generalized. The de.nitions above work with the following variations, \nparameterized over a scalar type s: class Functor s f where fmap :: (Vector s u, Vector s v) . (u . v) \n. (fu . fv) class Functor s f . Monoidal s f where fmap g fxs@(D fx dfx)= D (g fx)(fmap (dg) fxs .\u00b7 \ndfx) The only change from (N) is ( .\u00b7) in place of (.\u00b7). Again, this de.nition can be refactored, followed \nby replacing the non-effective applications of d with known derivatives. Alter\u00adnatively, replace arbitrary \nfunctions with differentiable functions (u '. v), as in Section 10.5, so as to make this de.nition exe\u00ad \ncutable as is. The Monoidal derivation goes through as before, adding an induction step. The instance \nde.nition is exactly as with (N) above. The only difference is using ( *) in place of (*). * instance \nVector s u . Monoidal s ((N ) u) whereunit :: f () (*) ::(Vector s u, Vector s v) . unit = D () 0 ' \n' ' * v ' ) fu . fv . f (u, v) Duu * Dvv = D (u, v)(u While we are altering the de.nition of Functor, \nwe can make another change. Rather than working with any function, limit the *Tooptimizeoutzerosineither \nNNoruvu around the derivative part of the representation, and mentioned in v, add a Maybe class to accepting \nonly differentiable functions. A simple represen-Section 7 and detailed in (Elliott 2009a). The zero-optimizations \ntation of a differentiable function is a function and its derivative: are entirely localized to the de.nitions \nof fmap and (*). To handle the Nothing vs Just, add an extra fmap in the fmap de.nition, data u '. v \n= FD (u . v)(u . (u -v)) and add another liftA2 in the (*) de.nition. This representation allows a simple \nand effective implementation of d: 11. Related work d :: (Vector s u, Vector s v) . Jerzy Karczmarczuk \n(2001) .rst demonstrated the idea of an in.\u00ad (u '. v) . (u . (u -v)) nite lazy tower of derivatives \n, giving a lazy functional implemen\u00ad ' ' d (FD f )= f With these de.nitions, the simple numeric method \nde.nitions (via fmap and liftA2) are again executable, provided that the functions passed to fmap are \nreplaced by differentiable versions. 10.6 Generalized derivative towers To compute in.nitely many derivatives, \nbegin with a derivative tower type and a theoretical means of constructing towers from a function: tation. \nHis .rst-order warm-up was similar to Figure 2, with the higher-order (tower) version somewhat more complicated \nby the introduction of streams of derivatives. Building on Jerzy s work, this paper implements the higher-order \ncase with the visual sim\u00adplicity of the .rst-order formulation (Figure 2). It also improves on that simplicity \nby means of numeric instances for functions, lead\u00ading to Figure 5. Another improvement is optimizing \nzeros without cluttering the implementation (Section 7). In contrast, (Karczmar\u00ad czuk 2001) and others \nhad twice as many cases to handle for unary methods, and four times as many for binary. * * data u N \nv = Dv (u N (u -v)) Jerzy s AD implementation was limited to scalar types, al\u00ad though he hinted at a \nvector extension in (Karczmarczuk 1999), using an explicit list of partial derivatives. These hints were \nlater .eshed out for the higher-order case in * toD :: (Vector s u, Vector s v) . u . (u . v) . u N \nv toDx f = D (fx)(toD x (df )) The naturality (functor morphism) property is fmap g . toD x = toD x . \nfmap g As before, let s massage this speci.cation into a form that is easy to satisfy. First, .-expand, \nand .ll in the de.nition of toD: fmap g (D (fx)(toD x (df ))) = D ((fmapg f ) x)(toD x (d (fmapg f ))) \n Next, simplify the right side, inductively assuming the Functor and Applicative morphisms inside the \nderivative part of D. (Foutz 2008), replacing lists with (nested) sparse arrays (repre\u00ad sented as fast \ninteger maps). The constant-optimizations there com\u00adplicated matters but had an advantage over the version \nin this paper. In addition to having constant vs non-constant constructors (and hence many more cases \nto de.ne), each sparse array can have any subset of its partial derivatives missing, avoiding still more \nmutipli\u00adcations and additions with zero. To get the same bene.t, one might use a linear map representation \nbased on partial functions. Pearlmutter and Siskind (2007) also extended higher-order forward-mode AD \nto the multi-dimensional case. They remarked: The crucial additional insight here, both for developing \nthe extension and for demonstrating its correctness, involves re\u00adformulating Karczmarczuk s method using \nTaylor expan\u00adsions instead of the chain rule. The expansions involve introducing non-standard e values, \nwhich come from dual numbers. Each e must be generated, managed, and and carefully distinguished from \nothers, so as to avoid problems of nested use described and addressed in (Siskind and Pearlmutter 2005, \n2008). In contrast, the method in this paper is based on the chain rule, while still handling multi-dimensional \nAD. I don t know whether the tricky nesting problem arises with the formulation in this paper based linear \nmaps. Henrik Nilsson (2003) extended higher-order AD to work on a generalized notion of functions that \nincludes Dirac impulses, allowing for more elegant functional speci.cation of behaviors involving instantaneous \nvelocity changes. These derivatives were for functions over a scalar domain (time). Doug McIlroy (2001) \ndemonstrated some beautiful code for manipulating in.nite power series. He gave two forms, Horner and \nMaclaurin, and their supporting operations. The MacLaurin form is especially similar, under the skin, \nto working with lazy derivative towers. Doug also examined ef.ciency and warns that the product operators \nfor Maclaurin and Horner forms respectively take O(2n) and O(n 2) coef.cient-domain operations to compute \nn terms. He goes on to suggest computing products by conversion to and from the Horner representation. \nI think the exponential complexity can apply in the formulations in (Karczmarczuk 2001) and in this paper. \nI am not aware of work on AD for general vector spaces, nor on deriving AD from a speci.cation. 12. Future \nwork Reverse and mixed mode AD. Forward-mode AD uses the chain rule in a particular way: in compositions \ng . f , g is always a primi\u00adtive function, while f may be complex. Reverse-mode AD uses the opposite \ndecomposition, with f being primitive, while mixed-mode combines styles. Can the speci.cation in this \npaper be applied, as is, to these other AD modes? Can the derivations be successfully adapted to yield \ngeneral, ef.cient, and elegant implementations of reverse and mixed mode AD, particularly in the general \nsetting of vector spaces? Richer manifold structure. Calculus on vector spaces is the foun\u00addation of \ncalculus on rich manifold strucures stitched together out of simpler pieces (ultimately vector spaces). \nExplore differentiation in the setting of these rich structures. Ef.ciency analysis. Forward-mode AD \nfor Rn . Ris described as requiring n passes and therefore inef.cient. The method in this paper makes \nonly one pass. That pass manipulates linear maps instead of scalars, which could be as expensive as n \npasses, but it might not need to be. 13. Acknowledgments I m grateful for comments from Anonymous, Barak \nPearlmutter, Mark Rafter, and Paul Liu. References Manuel M. T. Chakravarty, Gabriele Keller, and Simon \nPeyton-Jones. Associated type synonyms. In ICFP 05: Proceedings of the tenth ACM SIGPLAN international \nconference on Func\u00adtional programming, pages 241 253. ACM Press, 2005. Conal Elliott. Beautiful differentiation \n(extended version). Tech\u00ad nical Report 2009-02, LambdaPix, March 2009a. URL http: //conal.net/papers/beautiful-differentiation. \n Conal Elliott. Denotational design with type class morphisms. Technical Report 2009-01, LambdaPix, March \n2009b. URL http://conal.net/papers/type-class-morphisms. Conal Elliott. Push-pull functional reactive \nprogramming. In Proceedings of the Haskell Symposium, 2009c. Jason Foutz. Higher order multivariate automatic \ndif\u00adferentiation in haskell. Blog post, February 2008. URL http://metavar.blogspot.com/2008/02/ higher-order-multivariate-automatic.html. \nRalf Hinze. Generalizing generalized tries. Journal of Functional Programming, 10(04):327 351, 2000. \nGraham Hutton and Jeremy Gibbons. The generic approximation lemma. Information Processing Letters, 79(4):197 \n201, 2001. Jerzy Karczmarczuk. Functional coding of differential forms. In Scottish Workshop on Functional \nProgramming, 1999. Jerzy Karczmarczuk. Functional differentiation of computer pro\u00adgrams. Higher-Order \nand Symbolic Computation, 14(1), 2001. Conor McBride and Ross Paterson. Applicative programming with \neffects. Journal of Functional Programming, 18(1):1 13, 2008. M. Douglas McIlroy. The music of streams. \nInformation Process\u00ading Letters, 77(2-4):189 195, 2001. Henrik Nilsson. Functional automatic differentiation \nwith Dirac impulses. In Proceedings of the Eighth ACM SIGPLAN Interna\u00adtional Conference on Functional \nProgramming, pages 153 164, Uppsala, Sweden, August 2003. ACM Press. Barak A. Pearlmutter and Jeffrey \nMark Siskind. Lazy multivariate higher-order forward-mode AD. In Proceedings of the 2007 Symposium on \nPrinciples of Programming Languages, pages 155 60, Nice, France, January 2007. Simon Peyton Jones, Andrew \nTolmach, and Tony Hoare. Playing by the rules: rewriting as a practical optimisation technique in ghc. \nIn In Haskell Workshop. ACM SIGPLAN, 2001. Jeffrey Mark Siskind and Barak A. Pearlmutter. Nesting forward\u00admode \nAD in a functional framework. Higher Order Symbolic Computation, 21(4):361 376, 2008. Jeffrey Mark Siskind \nand Barak A. Pearlmutter. Perturbation con\u00adfusion and referential transparency: Correct functional imple\u00admentation \nof forward-mode AD. In Implementation and Appli\u00adcation of Functional Languages, IFL 05, September 2005. \nMichael Spivak. Calculus on Manifolds: A Modern Approach to Classical Theorems of Advanced Calculus. \nHarperCollins Publishers, 1971. A. Ef.cient linear maps   A.1 Basis types A basis of a vector space \nV is a subset B of V , such that the ele\u00adments of B span V and are linearly independent. That is to say, \nev\u00adery element (vector) of V is a linear combination of elements of B, and no element of B is a linear \ncombination of the other elements of B. Moreover, every basis determines a unique decomposition of any \nmember of V into coordinates relative to B. Since Haskell doesn t have subtyping, we can t represent \na ba\u00adsis type directly as a subset. Instead, for an arbitrary vector space v, represent a distinguished \nbasis as an associated type (Chakravarty et al. 2005), Basis v, and a function that interprets a basis \nrepre\u00adsentation as a vector. Another method extracts coordinates (coef.\u00adcients) for a vector with respect \nto basis elements. class (Vector s v, Enumerable (Basis v)) . HasBasis s v where type Basis v :: * basisValue \n:: Basis v . v coord :: v . (Basis v . s)  The Enumerable constraint enables enumerating basis ele\u00adments \nfor application of linear maps (Section A.2). It has one method that enumerates all of the elements in \na type: class Enumerable a where enumerate :: [a ] A.1.1 Primitive bases Since () is zero-dimensional, \nits basis is the Void type. The distinguished basis of a one-dimensional space has only one element, \nwhich can be represented with no information. Its corresponding value is 1. instance HasBasis Double \nDouble where type Basis Double = () basisValue () =1 coord s = const s  A.1.2 Composing bases Given \nvector spaces u and v, a basis element for (u, v) will be one basis representation or the other, tagged \nwith Left or Right. The vectors corresponding to these basis elements are (ub, 0) or (0, vb), where ub \ncorresponds to a basis element for u, and vb for v. As expected then, the dimensionality of the cross \nproduct is the sum of the dimensions. The decomposition of a vector (u, v) contains left-tagged decompositions \nof u and right-tagged decompositions of v. instance (HasBasis s u, HasBasis s v) . HasBasis s (u, v) \nwhere type Basis (u, v)= Basis u Either Basis v basisValue (Left a)=(basisValue a, 0) basisValue (Right \nb) =(0, basisValue b) coord (u, v)= coord u either coord v Triples etc, can be handled similarly or \nreduced to nested pairs. Basis types are usually .nite and small, so the decompositions can be memoized \nfor ef.ciency, e.g., using memo tries (Elliott 2009b).  A.2 Linear maps Semantically, a linear map is \na function f :: a . b such that, for all scalar values s and vectors u, v :: a, the following properties \nhold: f (s \u00b7 u) = s \u00b7 fu f (u + v) = fu + fv By repeated application of these properties, f (s1 \u00b7 u1 \n+ ... + sn \u00b7 un) = s1 \u00b7 fu1 + ... + sn \u00b7 f un Taking the ui as basis vectors, this form implies that \na linear function is determined by its behavior on any basis of its domain type. Therefore, a linear \nfunction can be represented simply as a func\u00adtion from a basis, using the representation described in \nSection A.1. type u -v = Basis u . v The semantic function converts from (u -v) to (u . v). It decomposes \na source vector into its coordinates, applies the basis function to basis representations, and linearly \ncombines the results. lapply :: (Vector s u, Vector s v) . (u -v) . (u . v) lapply uv u = sumV [coord \nu e \u00b7 uv e | e . enumerate ] or lapply lm = linearCombo . fmap (.rst lm) . decompose The inverse function \nis easier. Convert a function f , presumed linear, to a linear map representation: linear :: (Vector \ns u, Vector s v, HasBasis u) . (u . v) . (u -v)  It suf.ces to apply f to basis values: linear f = f \n. basisValue The coord method can be changed to return v -s, which is the dual of v. A.2.1 Memoization \nThe idea of the linear map representation is to reconstruct an entire (linear) function out of just a \nfew samples. In other words, we can make a very small sampling of function s domain, and re-use those \nvalues in order to compute the function s value at all domain values. As implemented above, however, \nthis trick makes function application more expensive, not less. If lm = linear f , then each use of lapply \nlm can apply f to the value of every basis element, and then linearly combine results. A simple trick \n.xes this ef.ciency problem: memoize the linear map. We could do the memoization privately, e.g., linear \nf = memo (f . basisValue) If lm = linear f , then no matter how many times lapply lm is applied, the \nfunction f can only get applied as many times as the dimension of the domain of f . However, there are \nseveral other ways to make linear maps, and it would be easy to forget to memoize each combining form. \nSo, instead of the function representation above, ensure that the function be memoized by representing \nit as a memo trie (Hinze 2000; Elliott 2009b). M type u -v = Basis u . v The conversion functions linear \nand lapply need just a little tweak\u00ading. Split memo into its de.nition untrie . trie, and then move untrie \ninto lapply. We ll also have to add HasTrie constraints: linear :: (Vector s u, Vector s v , HasBasis \ns u, HasTrie (Basis u)) . (u . v) . (u -v) linear f = trie (f . basisValue) lapply :: (Vector s u, Vector \ns v , HasBasis s u, HasTrie (Basis u)) . (u -v) . (u . v) lapply lm = linearCombo . fmap (.rst (untrie \nlm)) . decompose Now we can build up linear maps conveniently and ef.ciently by using the Functor and \nApplicative instances for memo tries (Elliott 2009b).  \n\t\t\t", "proc_id": "1596550", "abstract": "<p>Automatic differentiation (AD) is a precise, efficient, and convenient method for computing derivatives of functions. Its forward-mode implementation can be quite simple even when extended to compute all of the higher-order derivatives as well. The higher-dimensional case has also been tackled, though with extra complexity. This paper develops an implementation of higher-dimensional, higher-order, forward-mode AD in the extremely general and elegant setting of <i>calculus on manifolds</i> and derives that implementation from a simple and precise specification. In order to motivate and discover the implementation, the paper poses the question \"What does AD mean, independently of implementation?\" An answer arises in the form of <i>naturality</i> of sampling a function and its derivative. Automatic differentiation flows out of this naturality condition, together with the chain rule. Graduating from first-order to higher-order AD corresponds to sampling all derivatives instead of just one. Next, the setting is expanded to arbitrary vector spaces, in which derivative values are linear maps. The specification of AD adapts to this elegant and very general setting, which even <i>simplifies</i> the development.</p>", "authors": [{"name": "Conal M. Elliott", "author_profile_id": "81339498305", "affiliation": "LambdaPix, San Andreas, USA", "person_id": "P1614022", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1596550.1596579", "year": "2009", "article_id": "1596579", "conference": "ICFP", "title": "Beautiful differentiation", "url": "http://dl.acm.org/citation.cfm?id=1596579"}