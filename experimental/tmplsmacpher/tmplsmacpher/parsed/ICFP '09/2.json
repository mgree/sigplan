{"article_publication_date": "08-31-2009", "fulltext": "\n Safe Functional Reactive Programming through Dependent Types Neil Sculthorpe Henrik Nilsson School of \nComputer Science University of Nottingham United Kingdom {nas,nhn}@cs.nott.ac.uk Abstract Functional \nReactive Programming (FRP) is an approach to reactive programming where systems are structured as networks \nof func\u00adtions operating on signals. FRP is based on the synchronous data\u00ad.ow paradigm and supports both \ncontinuous-time and discrete-time signals (hybrid systems). What sets FRP apart from most other lan\u00adguages \nfor similar applications is its support for systems with dy\u00adnamic structure and for higher-order reactive \nconstructs. Statically guaranteeing correctness properties of programs is an attractive proposition. \nThis is true in particular for typical applica\u00adtion domains for reactive programming such as embedded \nsystems. To that end, many existing reactive languages have type systems or other static checks that \nguarantee domain-speci.c properties, such as feedback loops always being well-formed. However, they are \nlimited in their capabilities to support dynamism and higher-order data-.ow compared with FRP. Thus, \nthe onus of ensuring such properties of FRP programs has so far been on the programmer as established \nstatic techniques do not suf.ce. In this paper, we show how dependent types allow this concern to be \naddressed. We present an implementation of FRP embedded in the dependently-typed language Agda, leveraging \nthe type sys\u00adtem of the host language to craft a domain-speci.c (dependent) type system for FRP. The \nimplementation constitutes a discrete, operational semantics of FRP, and as it passes the Agda type, \ncov\u00aderage, and termination checks, we know the operational semantics is total, which means our type system \nis safe. Categories and Subject Descriptors D.3.2 [Programming Lan\u00adguages]: Language Classi.cations applicative \n(functional) lan\u00adguages, data-.ow languages, specialized application languages General Terms Languages \nKeywords dependent types, domain-speci.c languages, DSELs, FRP, functional programming, reactive programming, \nsynchronous data-.ow 1. Introduction Functional Reactive Programming (FRP) grew out of Conal El\u00adliott \ns and Paul Hudak s work on Functional Reactive Animation Permission to make digital or hard copies of \nall or part of this work for personal or classroom use is granted without fee provided that copies are \nnot made or distributed for pro.t or commercial advantage and that copies bear this notice and the full \ncitation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to \nlists, requires prior speci.c permission and/or a fee. ICFP 09, August 31 September 2, 2009, Edinburgh, \nScotland, UK. Copyright c &#38;#169; 2009 ACM 978-1-60558-332-7/09/08. . . $10.00 [Elliott and Hudak \n1997]. The idea of FRP is to allow the full power of modern Functional Programming to be used for imple\u00admenting \nreactive systems: systems that interact with their environ\u00adment in a timely manner. This is achieved \nby describing systems in terms of functions mapping signals (time-varying values) to sig\u00adnals, and combining \nsuch signal functions into signal processing networks. The nature of the signals depends on the application \ndo\u00admain. Examples include input from sensors in robotics applications [Peterson et al. 1999], video streams \nin the context of graphical user interfaces [Courtney and Elliott 2001] and games [Courtney et al. 2003, \nCheong 2005], and synthesised sound signals [Giorgidze and Nilsson 2008b]. A number of FRP variants exist. \nHowever, the synchronous data-.ow principle, and support for both continuous and discrete time (hybrid \nsystems), are common to most of them. There are thus close connections to synchronous data-.ow languages \nsuch as Es\u00adterel [Berry and Gonthier 1992], Lustre [Halbwachs et al. 1991], and Lucid Synchrone [Caspi \nand Pouzet 1996, Pouzet 2006]; hy\u00adbrid automata [Henzinger 1996]; and languages for hybrid mod\u00adelling \nand simulation, such as Simulink [Simulink]. However, FRP goes beyond most of these approaches by supporting \ndynamism (highly-dynamic system structure), and .rst-class signal functions (also known as higher-order \ndata-.ow). Dynamism and higher-order data-.ow are becoming important aspects of reactive programming \nas they are essential for imple\u00admenting recon.gurable systems, including systems that receive software \nupdates whilst running, which are increasingly preva\u00adlent [Colac\u00b8o et al. 2004]. Statically guaranteeing \ncentral domain\u00adspeci.c correctness properties is consequently also becoming much more important, as dynamism \nand higher-order data-.ow add levels of system complexity which make it correspondingly harder to test \nsystems suf.ciently thoroughly. Moreover, in many reactive appli\u00adcation scenarios, the cost of failure \nis very high (for example, man\u00adual intervention may not be feasible: consider updating the software of \na robot on Mars), thereby making it imperative to statically guar\u00adantee that the system will not fail. \nYampa [Nilsson et al. 2002] is an embedding of FRP in Haskell that supports dynamism (more so than previous \nHaskell-based FRP implementations) and .rst-class signal functions. However, from the perspective of \nreactive programming, the Haskell-based type system of Yampa is arguably not safe, as it does not enforce \nimpor\u00adtant domain-speci.c correctness properties. For example, there is nothing that prevents ill-formed \nfeedback loops, which, if present, can cause deadlock. Furthermore, even if a Yampa program is ini\u00adtially \nwell-formed, there are no guarantees that it will remain so after dynamic recon.guration. Conversely, \nthere are reactive lan\u00adguages that statically do enforce such domain-speci.c properties (Lustre and Lucid \nSynchrone, for example), but their support for dynamism or higher-order data-.ow is limited.  To address \nthis problem, we develop a domain-speci.c type system for FRP that guarantees two central domain-speci.c \ncor\u00adrectness properties, well-formed feedback loops and proper initial\u00adisation, while still allowing \nfor dynamism and .rst-class reactive entities. The type system is safe in that it guarantees that reactive \nprograms are productive (guaranteed to deliver output at all points in time), under the assumption that \nthe pure functional code em\u00adbedded in the signal-processing network is total and terminating. This is \naccomplished through the domain-speci.c type system be\u00ading dependent [Thompson 1991, Pierce 2002]: the \ntypes of signal functions are indexed on speci.c properties that they satisfy, al\u00adlowing the corresponding \nproperties of composite networks to be established compositionally through type-level computations. The \ntype system has been realised in the context of a pro\u00adtotype FRP implementation embedded in Agda [Norell \n2007], a dependently-typed functional language. Agda bears many similari\u00adties to Haskell, but requires \nall functions to be total and terminating. The central part of the implementation is a function that \nconstitutes a discretised operational semantics: given the time passed since the previous step and the \ncurrent input, this semantic function maps a well-typed term representing the current con.guration of \n(part of) a signal function network to the current output and a new, well-typed term representing the \nupdated con.guration. Because the seman\u00adtic function is total and terminating, it constitutes a proof \nthat the embedded type system guarantees the productivity of well-typed signal-function networks, which \nis the safety property with which we are concerned here. A further bene.t of making domain-speci.c properties \nmanifest in the types of signal functions is that this clari.es their semantics, which, in turn, offers \nstrong guidance as to their proper use. This is in stark contrast to Yampa, where subtle but crucial \nproperties are often implicit, possibly leading to confusion about the exact rela\u00adtion between differently \nnamed combinators with the same type. There are a couple of other innovative aspects to the FRP ver\u00adsion \ndeveloped in this paper. Firstly, there is a clear type-level distinction between continuous-time and \ndiscrete-time signals. In Yampa, the latter are just continuous-time signals carrying an op\u00adtion type. \nAs a result, certain signal functions, such as the various delays, that in order to guarantee desirable \nsemantical properties would have to treat continuous-time and discrete-time signals dif\u00adferently, actually \ntreat them uniformly. This is another source of subtle bugs that can be eliminated by the more precise \ntype system presented in this paper. Secondly, our development is structured around n-ary signal functions, \nthrough the notion of signal vectors. This enables a number of important optimisations, such as change \npropagation, to an extent that is not possible in Yampa [Sculthorpe and Nilsson 2008]. Note that our \nFRP type-system is, in principle, independent of the Agda-based FRP implementation presented here: it \ncould be used with other realisations of FRP. In summary, the main contributions of this paper are: \nA type system for FRP that  enforces well-formed feedback loops and proper initialisa\u00adtion;  guarantees \nproductivity if all pure functions embedded in a network are total and terminating;  makes a clear \ntype-level separation between continuous\u00adtime and discrete-time signals, ruling out additional kinds \nof ill-formed programs.   A discrete, operational semantics for the version of FRP used in this paper. \n A machine-checked proof of the safety of the type system carried out through an embedding of the type \nsystem and the operational semantics in the dependently-typed language Agda. We outline the proof in \nthe present paper; the full code is available from the .rst author s website1. The rest of the paper \nis structured as follows. Section 2 explains the fundamental concepts of FRP. Section 3 describes a new \nconceptual model that addresses some of the limitations in previous FRP mod\u00adels. Sections 4 and 5 demonstrate \nhow the new conceptual model allows us to include feedback loops and uninitialised signals in an FRP \nprogram, whilst guaranteeing productivity at the type level. Finally, Section 6 describes a prototype \nFRP implementation using this type system, and gives its operational semantics.  2. FRP Fundamentals \nFRP programs can be considered to have two levels to them: a func\u00adtional level and a reactive level. \nThe functional level is a pure, func\u00adtional language. FRP implementations are usually embedded in a host \nlanguage, and in these cases the functional level is provided entirely by the host. In the case of Yampa, \nthe host language is Haskell. The reactive level is concerned with time-varying values, which we call \nsignals. At this level, combinators are used to con\u00adstruct synchronous data-.ow networks by combining \nsignal func\u00adtions. The levels are, however, interdependent: the reactive level re\u00adlies on the functional \nlevel for carrying out arbitrary pointwise com\u00adputations on signals, while reactive entities, such as \nsignal func\u00adtions, are .rst class entities at the functional level. 2.1 Continuous-Time Signals The \ncore conceptual idea of FRP is that time is continuous. Signals are modelled as functions from time to \nvalue, where we take time to be the set of non-negative real numbers: Time = {t . R | t . 0 } Signal \na Time . a This conceptual model provides the foundation for an ideal FRP se\u00admantics. Of course, any \ndigital implementation of FRP will have to execute over a discrete series of time steps and will consequently \nonly approximate the ideal semantics. The advantage of the con\u00adceptual model is that it abstracts away \nfrom such implementation details. It makes no assumptions as to the rate of sampling, whether this sampling \nrate is .xed, nor how this sampling is performed. It also avoids many of the problems of composing subsystems \nthat have different sampling rates. The ideal semantics is helpful for understanding FRP programs, at \nleast to a .rst approximation. It is also abstract enough to leave FRP implementers considerable free\u00addom. \nThat said, implementing FRP completely faithfully to the ideal semantics is challenging. At the very \nleast, a faithful implementa\u00adtion should, for reasonable programs , converge to the ideal se\u00admantics \nin the limit as the sampling interval tends to zero [Wan and Hudak 2000]. But even then it is hard to \nknow how densely one needs to sample before an answer is acceptably close to the ideal. However, the \nfocus of this paper is not directly the faithfulness of FRP implementations to any ideal semantics. Instead, \nour inter\u00adest is to statically rule out programs that are bad; either because they lack meaning, or because \nthey would be hard to run faithfully. This, in turn, is one step towards making it easier to implement \nFRP faithfully and allowing programmers to reason in terms of the ideal semantics with greater con.dence. \nThus, in this paper, we only pro\u00advide a discrete operational FRP semantics as this is what we need 1 \nhttp://www.cs.nott.ac.uk/~nas/icfp09.html  for our purposes. But we will continue to refer to the ideal, \nconcep\u00adtual model when it is expedient for providing the right intuitions.  2.2 Signal Functions Signal \nfunctions are conceptually functions from signal to signal: SF a b Signal a . Signal b In Yampa, signal \nfunctions, rather than signals, are .rst class enti\u00adties. Signals have no independent existence of their \nown; they exist only indirectly through the signal functions. To make it possible to implement signal \nfunctions in such a way that output is produced in lock-step with the input arriving, as is required \nfor a system to be reactive, we insist that signal functions are causal. Causal Signal Function. A signal \nfunction is causal if, at any given time, its output can depend upon its past and present inputs, but \nnot its future inputs: SF a b = {sf : Signal a . Signal b |. (t : Time)(s1 s2 : Signal a). (. t' : t. \ns1 t' = s2 t') . (sf s1 t = sf s2 t)} In an implementation, signal functions that depend upon past inputs \nneed to record past information in an internal state. For this reason, they are often called stateful \nsignal functions. Some signal functions are such that their output only depends on their input at the \ncurrent point in time. We refer to these as stateless signal functions, as they require no internal state \nto be implemented: SF stateless ab = {sf : Signal a . Signal b |. (t : Time)(s1 s2 : Signal a). (s1 t \n= s2 t) . (sf s1 t = sf s2 t)} The terms sequential and combinatorial are also used for the same notions \nas stateful and stateless, respectively. 2.3 Why Not First Class Signals? In Classic FRP (CFRP) [Elliott \nand Hudak 1997, Wan and Hudak 2000], the .rst class entities are behaviours, which are time-varying values \ncorresponding to signals: Behaviour a Time . a CFRP programs are constructed by applying functions to \nbe\u00adhaviours, making CFRP programs look more like conventional functional programs than Yampa programs \ndo. This is appealing in many ways. However, unless great care is exercised, .rst-class behaviours can \nlead to a number of performance problems. There are also thorny semantic problems related to composing \nbehaviours temporally by switching from one to another [Nilsson et al. 2002]. In part to avoid these \nissues, the notion of signal function was adopted as the core concept for Yampa. The absence of .rst \nclass signals makes it simple to process input as it arrives, which is the norm for synchronous data-.ow \nlanguages. The semantics of switching also becomes obvious, paving the way for supporting structural \ndynamism [Nilsson et al. 2002]. This is not to say that .rst-class behaviours cannot be a viable approach \nin many cases: Elliott s recent work on the Reactive library has clearly shown this is not so [Elliott \n2008]. However, we have chosen to stay with signal functions as the core concept because of its simplicity, \nrobustness, and demonstrated .exibility.  3. The New Conceptual Model In this section, we introduce \na new conceptual FRP model that ad\u00addresses some limitations of the Yampa design. We have discussed these \nproblems in earlier work [Sculthorpe and Nilsson 2008], along with an initial version of this new model. \nIn the following, we brie.y review the problems of the Yampa design, and then in\u00adtroduce a re.ned version \nof the new model adapted to the setting of the present paper. With the new model as a basis, we then \ncon\u00adtinue to develop a type system guaranteeing safety in the following sections. 3.1 Limitations of \nthe Yampa Design In Yampa, multiple signals are combined by tupling them together. There is no distinction \nbetween a pair of signals and a signal carrying a pair. For example, a signal function that conceptually \nmaps a pair of signals carrying doubles to another pair of signals carrying doubles has the type: SF \n(Double, Double)(Double, Double) This is exactly the same type as a signal function that maps a signal \ncarrying pairs of doubles to another signal carrying pairs of doubles. Routing of signals between signal \nfunctions is mostly carried out at the functional level by lifting pure routing functions to the reactive \nlevel. Unfortunately, this approach hides the routing from the reac\u00adtive level, making it dif.cult to \nimplement Yampa in a way that scales well (such as through direct point-to-point communication or change \npropagation [Sculthorpe and Nilsson 2008]). To overcome this, routing needs to be internalised at the \nreactive level, and the signal function notion needs to be re.ned so that a signal function truly maps \nmultiple individual input signals to multiple individual output signals. Another characteristic aspect \nof the Yampa design is that discrete-time signals are realised by continuous-time signals carry\u00ading an \noption type (Signal (Maybe A)). This is very convenient, as continuous-time and discrete-time signals \ncan be freely mixed, but alas not suf.ciently abstract: the ideal semantics of discrete-time signals \ncannot really be enforced, nor can it be exploited for opti\u00admising the implementation. It is thus desirous \nto make a clear type\u00adlevel distinction between continuous-time signals and discrete-time signals, while \nretaining the convenience of the Yampa approach.  3.2 Signal Descriptors and Signal Vectors To address \nthe limitations of Yampa, we introduce the notion of a signal vector, a heterogeneous vector of signals, \nand rede.ne the conceptual notion of signal function to be a function on signal vec\u00adtors. We also introduce \ntwo distinct kinds of signals: continuous\u00adtime signals, de.ned as before; and discrete-time, or event, \nsignals, which are only de.ned at countably many points in time. Each point at which an event signal \nis de.ned is known as an event occurrence. The crucial point is that we de.ne these notions of different \nkinds of signals, and vectors of such signals, only as an integral part of the signal function abstraction: \nthey have no independent existence of their own and are thus completely internalised at the reactive \nlevel. This means that the FRP implementer has great freedom in choosing representations and exploiting \nthose choices. We proceed as follows. First we de.ne signal descriptors.A signal descriptor is a type \nthat describes key characteristics of a signal. Signal descriptors only exist at the type-level: there \nare no values having such types; in particular, a signal descriptor is not the (abstract) type of any \nsignal. Initially, we are interested in the time domain and the type (of the values carried by) the signal. \nThus we introduce one descriptor for each kind of signal, each parametrised on the signal type: data \nSigDesc : Set where E : Set . SigDesc --discrete-time signals (events) C : Set . SigDesc --continuous-time \nsignals    Figure 1. The Sequential ( \u00bb ) and Parallel ( *** ) Composition Combinators Note that Set \nis the type of types in Agda (similar to kind * in Haskell)2. Next we introduce signal vector descriptors. \nA signal vector descriptor is simply a (type level) list of signal descriptors: SVDesc : Set SVDesc = \nList SigDesc For the purpose of stating the new conceptual de.nition of signal functions, and for use \nin semantic de.nitions later, we postulate a function (SVRep) that maps a signal vector descriptor to \nsome suitable type for representing a sample of signal vectors of that description, and use this to de.ne \nsignal vectors: SVRep : SVDesc . Set SigVec : SVDesc . Set SigVec as Time . SVRep as However, we do \nnot require the existence of such a function: an implementation may opt to not represent signal vectors \nexplicitly at all. Finally, we re.ne the conceptual de.nition of signal functions: SF : SVDesc . SVDesc \n. Set SF as bs SigVec as . SigVec bs  3.3 Example Combinators and Primitives To demonstrate the new \nconceptual model, we de.ne some com\u00admon primitive signal functions and combinators from Yampa. These \nprimitives either operate at the reactive level, or mediate between the functional and reactive levels. \n3.3.1 Sequential and Parallel Composition Signal functions can be composed sequentially (\u00bb)orin parallel \n(* ) (seeFigure1): \u00bb : {as bs cs : SVDesc }. SF as bs . SF bs cs . SF as cs * : {as bs cs ds : SVDesc \n}.SF as cs . SF bs ds . SF (as + bs)(cs + ds) (In Agda, is used to indicate the argument positions for \nin.x and mix.x operators, while the curly braces are used to enclose implicit arguments: arguments that \nonly have to be provided at an application site if they cannot be inferred from the context.) Note that \n*** composes two signal functions that take different inputs. For parallel composition where both signal \nfunctions take the same input, there is the &#38; combinator: &#38;  : {as bs cs : SVDesc }. SF as \nbs . SF as cs . SF as (bs + cs) 2 Strictly speaking, SigDesc should have type Set1 (the type of Set). \nHowever, for clarity, we use the Agda option that accepts Set as the type of Set. We have successfully \nimplemented the type system without this option, but, because Agda does not support universe polymorphism, \nthe result is very repetitive code and loss of conceptual clarity. Figure 2. The Feedback Combinator \n(loop) 3.3.2 Switches Signal function networks are made dynamic through the use of switches. Basic switches \nhave the following type: switch : .{as bs }.{e : Set }. SF as (E e :: bs) . (e . SF as bs) . SF as bs \ndswitch : .{as bs }.{e : Set }. SF as (E e :: bs) . (e . SF as bs) . SF as bs (Agda allows the type of \nan implicit argument to be omitted when it is clear from the context. In the de.nitions above, both as \nand bs are clearly of type SVDesc as they are used as arguments to the type constructor SF .) The behaviour \nof a switch is to run the subordinate signal function (the .rst explicit argument), emitting all but \nthe head (the event) of the output vector as the overall output. When there is an event occurrence in \nthe event signal, the value of that signal is fed into the function (the second explicit argument) to \ngenerate a residual signal function. The entire switch is then removed from the network and replaced \nwith this residual signal function. The difference between a switch and a dswitch (decoupled switch) \nis whether, at the moment of switching, the overall output is the output from the residual signal function \n(switch), or the output from the subordinate signal function (dswitch).3 A key point regarding switches \nis that the residual signal func\u00adtion does not start running until it is applied to the input signal \nat the moment of switching. Consequently, rather than having a single global Time, each signal function \nhas its own local time. Local Time. The time since this signal function was applied to its input signal. \nThis will have been either when the entire system started, or when the sub-network containing the signal \nfunction in question was switched in. 3.3.3 Loops The loop primitive provides the means for introducing \nfeedback loops into signal function networks. A loop consists of two signal functions: a subordinate \nsignal function (the .rst explicit argument) and a feedback signal function (the second explicit argument). \nThe input of the feedback signal function is a suf.x of the output of the subordinate signal function, \nand the output of the feedback signal function is a suf.x of the input to the subordinate signal function: \nloop : .{as bs cs ds }. SF (as + cs)(bs + ds) . SF ds cs . SF as bs Intuitively, we use the feedback \nsignal function to connect some of the output signals of the subordinate signal function to some of its \ninput signals, forming a feedback loop (see Figure 2).  3.3.4 Primitive Signal Functions We can lift \npure functions to the reactive level using the primitives pure and pureE 4. Such lifted signal functions \nare always stateless: 3 In Yampa, dswitch also decouples part of its input from part of its output, but \nwe do not assume any such behaviour here. 4 It is possible to have one pure primitive that is overloaded \nto operate on either time domain, but we do not do so here for clarity.  pure : {ab : Set }. (a . b) \n. SF [C a ][C b ] pureE : {ab : Set }. (a . b) . SF [E a ][E b ] Note that we are using [s ] as a synonym \nfor (s :: []). We can lift values to the reactive level using the primitive constant. This creates a \nsignal function with a constant, continuous\u00adtime, output: constant : .{as }.{b : Set }. b . SF as [C \nb ] Events can only be generated and accessed by event processing primitives. Examples include edge, \nwhich produces an event whenever the boolean input signal changes from false to true;  hold, which emits \nas a continuous-time signal the value carried by its most recent input event;  never, which outputs \nan event signal containing no event oc\u00adcurrences;  now, which immediately outputs one event, but never \ndoes so again.  edge : SF [C Bool ][E Unit ] hold : {a : Set }. a . SF [E a ][C a ] never : .{as }.{b \n: Set }. SF as [E b ] now : .{as }. SF as [E Unit ] The primitive pre conceptually introduces an in.nitesimal \ndelay: pre : .{a }. SF [C a ][C a ] To make this precise, the ideal semantics of pre is that it outputs \nwhatever its input was immediately prior to the current time; that is, the left limit of the input signal \nat all points: ' . (t : Time+)(s : Signal a). pre s t = lim - st t.t Here, Time+ denotes positive time. \nConsequently, at any given point, the output of pre does not depend upon its present input, which is \nthe crucial property of pre: see Section 4. The primitive pre is usually implemented as a delay of one \ntime step. Of course, this only approximates the ideal semantics. However, if the length of the time \nsteps tends to zero, the semantics of such an implementation of pre converges to the ideal semantics. \nNote that pre is only de.ned for continuous-time signals. This is because the left limit at any point \nof a discrete-time signal (a signal de.ned only at countably many points in time) is unde.ned. In our \nsetting, this amounts to an event signal without any occurrences; which is a signal equivalent to the \noutput from never. Applying pre to an event signal would thus be pointless (use never instead), and any \nattempt to do so would likely be a mistake stemming from a misunderstanding of the semantics of pre. \nDisallowing pre on events thus eliminates a potential source of programming bugs. In contrast, Yampa, \nbecause discrete-time signals are realised as continuous-time signals carrying an option type (see Section \n3.1), cannot rule out pre being applied to event signals, nor can it guarantee the proper semantics of \nsuch an application. Note also that pre is only de.ned for positive time. When the local time is zero \n(henceforth referred to as time0), the output of pre is necessarily unde.ned as there are no prior points \nin time. Thus we need an initialise combinator that de.nes a signal function s output at time0: initialise \n: .{as b }. b . SF as [C b ] . SF as [C b ] Initialisation is discussed further in Section 5.  3.4 \nExample Let us illustrate the concepts and de.nitions that have been intro\u00adduced thus far by constructing \na simple signal function network. Its purpose is to monitor a real-valued continuous-time input sig\u00adnal \nand output the same signal until the input dips below 0. At this point, the output should be clamped \nto 0, and then remain at 0 from then on. clamp : SF [C R ][C R ] clamp = switch ((pure (.x . x < 0) \u00bb \nedge)&#38;&#38; pure id) (. . constant 0 )  4. Decoupled Signal Functions As previously discussed, \nthe loop combinator allows feedback to be introduced into a network. This is an essential capability, \nas feedback is widely used in reactive programming. However, feedback must not cause deadlock due to \na signal function depending on its own output in an unproductive manner. To guarantee this, we conservatively \nprohibit instantaneous cycles in the network. This is a common design choice in reactive lan\u00adguages, \nbut our way of enforcing it is different. We identify de\u00adcoupled signal functions, essentially a class \nof signal functions that can be used safely in feedback loops, and index the type of a signal function \nby whether or not it is decoupled. Decoupled Signal Function. A signal function is decoupled if, at any \ngiven time, its output can depend upon its past inputs, but not its present and future inputs: SF dec \nas bs = {sf : SF as bs |. (t : Time)(sv1 sv2 : SigVec as). '' (. t < t. sv1 t = sv2 t ' ) . (sf sv1 \nt = sf sv2 t)} Decoupled Cycle. A cycle is decoupled if it passes through a decoupled signal function. \nInstantaneous Cycle (Algebraic Loop). A cycle is instantaneous if it does not pass through a decoupled \nsignal function. In Yampa, the onus is on the programmer to ensure that all cy\u00adcles are correctly decoupled. \nAn instantaneous cycle will not be detected statically, and the program could well loop at run-time. \nMany reactive languages deal with this problem by requiring a speci.c decoupling construct (a language \nprimitive) to appear syn\u00adtactically within the de.nition of any feedback loops. This works in a .rst \norder setting, but becomes very restrictive in a higher order setting as decoupled signal functions cannot \nbe taken as parameters and used to decouple loops. Our solution is to encode decoupledness information \nin the types of signal functions. This allows us to statically ensure that a well-typed program does \nnot contain any instantaneous cycles. Furthermore, the decoupledness of a signal function will be visible \nin its type signature, providing guidance to an FRP programmer. 4.1 Decoupledness Descriptors We introduce \na data type of decoupledness descriptors: data Dec : Set where dec : Dec --decoupled signal functions \ncau : Dec --causal signal functions We then index SF with a decoupledness descriptor: SF : SVDesc . SVDesc \n. Dec . Set We can now enforce that the feedback signal function within a loop is decoupled: loop : .{as \nbs cs ds }.{d : Dec }. SF (as + cs)(bs + ds) d . SF dscs dec . SF as bsd The primitive signal functions \nnow need to be retyped to include appropriate decoupledness descriptors:  pure : .{ab }. (a . b) . SF \n[C a ][C b ] cau pureE : .{ab }. (a . b) . SF [E a ][E b ] cau constant : .{as b }. b . SF as [C b ] \ndec edge : SF [C Bool ][E Unit ] cau hold : .{a }. a . SF [E a ][C a ] cau never : .{as b }. SF as [E \nb ] dec now : .{as }. SF as [E Unit ] dec pre : .{a }. SF [C a ][C a ] dec initialise : .{as b }.{d : \nDec } . b . SF as [C b ] d . SF as [C b ] d Notice that, from the de.nition of decoupled signal functions, \nit is evident that they are a subtype of causal signal functions (dec <: cau). This means that we can \ncoerce a decoupled sig\u00adnal function into a causal one. Intuitively, we do this by forgetting that the \ndecoupled signal function does not depend upon the present input. We provide a weakening primitive that \nperforms this coer\u00adcion: ' weaken : .{as bsd d }. ' ' d <: d . SF as bs d . SF as bs d The primitive \ncombinators compute the decoupledness descriptor of their composite signal function from the descriptors \nof their components. To do this, we use the join ( . ) and meet ( . )of the decoupledness descriptors \n(with respect to subtyping): \u00bb : .{as bscs d1 d2 }.SF as bs d1 . SF bs cs d2 . SF as cs (d1 . d2) * : \n.{as bscs ds d1 d2 }. SF as cs d1 . SF bs ds d2 . SF (as + bs)(cs + ds)(d1 . d2) &#38; : .{as bscs \nd1 d2 }. SF as bs d1 . SF as cs d2 . SF as (bs + cs)(d1 . d2) switch : .{as bs ed1 d2 }. SF as (E e :: \nbs) d1 . (e . SF as bs d2) . SF as bs (d1 . d2) dswitch : .{as bs ed1 d2 }. SF as (E e :: bs) d1 . (e \n. SF as bs d2) . SF as bs (d1 . d2) We have now ensured that all feedback in the network is well de.ned. \nIn Yampa, badly de.ned cycles can cause the execution to loop at run-time, something that our type system \nguarantees will not occur. Note that without indexing signal functions by their decoupled\u00adness, an Agda \nimplementation using this type system (such as the one described in Section 6) would not pass Agda s \ntermination checker.  4.2 Example: Switching Integration Methods To demonstrate the usefulness of decoupledness \ndescriptors, we give here a small example of how they can be used to allow dy\u00adnamic switching between \nseveral integration signal functions. This is inspired by an example from Lucid Synchrone [Colac\u00b8o et \nal. 2004], the synchronous data-.ow language with the greatest sim\u00adilarity to FRP. Our example differs \nin that the integration signal function is being used to decouple a loop, allowing the decision of whether \nto switch integration functions to depend upon the current output. To our knowledge, there is no other \nreactive language that could accept this program while also guaranteeing the absence of dead\u00adlock at \nrun-time.   4.2.1 Recurring Switches For this example we need to introduce an additional class of \nswitch\u00ading combinators: recurring switches (similar to every in Lucid Syn\u00adchrone). The behaviour of a \nrecurring switch is to apply its subor\u00addinate signal function to the tail of its input, producing the \noverall output. Whenever an event (the head of the input) occurs, the signal function carried by that \nevent replaces the subordinate signal func\u00adtion. Recurring switches come in two varieties: like basic \nswitches, they differ in whether the output at the instant of switching is from the new (rswitch)orold \n(drswitch) subordinate signal function. rswitch : .{as bs d1 d2 }. SF as bs d1 . SF (E (SF as bs d2):: \nas) bs cau drswitch : .{as bs d1 d2 }. SF as bs d1 . SF (E (SF as bs d2):: as) bs (d1 . d2)  4.2.2 Dynamic \nIntegrator There are many different ways to de.ne an integration signal func\u00adtion, usually involving \na trade-off between ef.ciency and accuracy. Some de.nitions of integration allow for decoupled behaviour, \nwhile others do not. We .rst introduce some synonyms. We assume the signal to be integrated (Input) is \na .oating point number, as is the integrated signal (Output). An integrator (Intgr) is a causal signal \nfunction that integrates a signal, and a decoupled integrator (dIntgr) is one that is decoupled. We assume \nintegral is an existing decoupled integrator. Input = Float Output = Float Intgr = SF [C Input ][C Output \n] cau dIntgr = SF [C Input ][C Output ] dec integral : dIntgr Sometimes it can be advantageous to swap \nbetween integrators dynamically if the behaviour of the input signal changes radically (making sure to \ntransfer the state). For the purposes of this example, we assume that we have a signal function (intgrDecider)that \ncontains several integrators, and is capable of deciding, given the current input and output, whether \nto switch to one of them. For this, we require all such integrators to be decoupled (as the output is \nrequired to decide which integrator to use to compute that very output), and use them as the feedback \nsignal function within a loop. The intgrDecider also receives as input new integrators, along with some \ndecision rules that allow it to determine when they should be used. We can now de.ne a dynamic integrator \n(dynIntgr)that uses a loop to connect intgrDecider with a drswitch containing an initial decoupled integrator: \nDecisionRules : Set NewIntgr = dIntgr \u00d7 DecisionRules intgrDecider : SF (E NewIntgr :: C Input :: C Output \n:: []) (C Output :: E dIntgr :: C Input :: []) cau dynIntgr : SF (E NewIntgr :: C Input :: []) [C Output \n] cau dynIntgr = loop intgrDecider (drswitch integral) This program could be de.ned in Yampa. However, \nthere would be no restriction on the decoupledness of the integrators. It would be possible to provide \na new, non-decoupled integrator as input, which would then cause deadlock if it was ever used. Here, \nthe types ensure this will never happen.   5. Uninitialised Signals Recall the pre signal function \nfrom Section 3.3. This primitive is very common in reactive programming, and is the standard means of \ndecoupling feedback. However, the output of pre is unde.ned at time0. It is for this reason that the \ninitialise primitive exists, which de.nes the output of a signal function at time0. It is possible to \nsidestep this issue by combining pre and initialise into one combinator (such as the iPre primitive in \nYampa). We do not do this for several reasons: We may not want, or be able (if no initialisation value \nis avail\u00adable), to initialise the signal at the usage of pre, but only else\u00adwhere in the program.  An \nuninitialised signal can pass through a stateless signal func\u00adtion without causing an error, it just \nproduces an uninitialised output signal.  Some signals may not require initialising, such as within \nthe residual signal function of a dswitch (see Section 5.2).  In Yampa, the onus is on the programmer \nto ensure that any unini\u00adtialised signals are correctly initialised where necessary. When this is not \ndone correctly, it can cause run-time errors. 5.1 Initialisation Descriptors Our solution is to add initialisation \ninformation to the signal de\u00adscriptors. This guides the FRP programmer when writing programs, and allows \nthe type checker to reject any programs where unini\u00adtialised signals could cause a run-time error. (Without \nthis addition we could not implement uninitialised signals in Agda using this type system; the totality \nchecker would reject it.) Note that the property of a signal being de.ned or not at time0 is only of \ninterest for continuous-time signals. Event signals are, by de.nition, only de.ned at discrete points \nin time, and thus there is no need to initialise them if they are not de.ned at time0. data Init : Set \nwhere ini : Init --initialised signals uni : Init --uninitialised signals data SigDesc : Set where E \n: Set . SigDesc C : Init . Set . SigDesc  The primitive signal functions that mention continuous-time \nsig\u00adnals in their types now need to be retyped: pure : .{ab }.{i : Init }. (a . b) . SF [C ia ][C ib \n] cau constant : .{as b }. b . SF as [Cini b ] dec edge : SF [Cini Bool ][E Unit ] cau hold : .{a }. \na . SF [E a ][Cini a ] cau pre : .{a }. SF [Cini a ][C uni a ] dec initialise : .{as b d }.{i : Init \n}. b . SF as [C ib ] d . SF as [Cini b ] d Initialised signals are subtypes of uninitialised signals \n(ini <: uni), as they can be coerced by forgetting the value at time0.We extend the weaken primitive \nto re.ect this: ' ' ' weaken : .{asas bsbs dd }. ' '' as <: as . bs <: bs . d <: d . ' '' SF as bs d \n. SFas bs d Note that, as is usual for function types, the subtyping is contravari\u00adant on the input signal \nvector. A signal vector is a subtype of an\u00adother if all signals in the former are subtypes of the respective \nsig\u00adnals in the latter (we do not use any width subtyping). 5.2 Switching into Uninitialised Signals \nOur signal descriptors allow us to deal with uninitialised signals at time0, but, crucially, not at any \nother time. As previously dis\u00adcussed, signal functions exist in their own local time. What is time0 for \none part of the system may not be for another part. We must ensure that an uninitialised signal does \nnot escape from a sub\u00adnetwork that locally is at time0, into an outer network that is not. For example, \nconsider switch. When the residual signal func\u00adtion is switched in, it could produce uninitialised output \nat its (lo\u00adcal) time0. But this uninitialised signal then escapes, potentially causing a run-time error. \nIn fact, switches are the only place that this can occur, as it is only switches that create sub-networks \nat a different local time. We resolve this by requiring that all output signals from the residual signal \nfunction are initialised, enforcing this at the type level: initc : SVDesc . SVDesc initc = map initcAux \nwhere initcAux : SigDesc . SigDesc initcAux (E a)= E a initcAux (C a)= Cini a switch : .{as bs ed1 d2 \n}. SF as (E e :: bs) d1 . (e . SF as (initc bs) d2) . SF as bs (d1 . d2) We do not require this constraint \nfor decoupled switches, as their output at time0 is de.ned as being the output from the subordi\u00adnate \nsignal function. The (time0) output from the residual signal function is discarded, so it does not matter \nif it is uninitialised. Furthermore, this means that the initialisation of the residual signal function \ns output does not affect the overall initialisation of the switch construct. We rede.ne dswitch to re.ect \nthis .exibility: ' dswitch : .{asbsbs ed1 d2 }. SF as (E e :: bs) d1 . ' (e . SFasbs d2) . ' initc bs \n<: bs . SF as bs (d1 . d2)   6. Safety and Semantics Agda has completeness and termination checkers, \nensuring that Agda programs are total and terminating. Thus our FRP embedding within Agda has these assurances \nas well. To run signal functions, our prototype implementation operates by running the network iteratively \nover a discrete sequence of time steps. At each time step, the input is sampled and fed into the network, \nalong with the time delta since the preceding time step. The network then updates any internal state, \nand produces an output sample. This is the same approach as taken by Yampa and many other reactive languages. \nWe give the operational semantics of this in Figure 4. These semantics correspond directly to the Agda \nfunction we use to execute one step of a network (=.): .t : Set .t = Time =. : .{as bs d }. .t \u00d7 SF \nas bs d \u00d7 SVRep as . SF as bs d \u00d7 SVRep bs This (one time step) evaluation function is accepted by Agda; \nthere\u00adfore the evaluation of each time step is guaranteed to terminate, producing output. Although execution \nof an FRP program is usu\u00adally modelled as non-terminating (an in.nite sequence of steps), we can nevertheless \nguarantee that they are productive because each individual step is productive.  data SF : SVDesc . SVDesc \n. Dec . Set where prim : .{as bs State }. (.t . State . SVRep as . State \u00d7 SVRep bs) . State . SF asbs \ncau dprim : .{as bs State }. (.t . State . (SVRep as . State) \u00d7 SVRep bs) . State . SF as bs dec \u00bb : \n.{as bs cs d1 d2 }. SF as bsd1 . SF bs cs d2 . SF as cs (d1 . d2) * : .{as bs cs ds d1 d2 }. SF as cs \nd1 . SF bs ds d2 . SF (as + bs)(cs + ds)(d1 . d2) loop : .{as bs cs ds d }. SF (as + cs)(bs + ds) d . \nSF dscs dec . SF asbs d switch : .{as bs e d1 d2 }. SF as (E e :: bs) d1 . (e . SF as bs d2) . SF as \nbs (d1 . d2) dswitch : .{as bs e d1 d2 }. SF as (E e :: bs) d1 . (e . SF as bs d2) . SF as bs (d1 . d2) \n  Figure 3. Implementation Core Primitives ' f dt sas . (s , bs) PRIM (primf s, as)=dt.(primf s ' \n, bs) ' f dt s . (g, bs) gas . s DPRIM (dprim f s, as)=dt.(dprim f s ' , bs) dt ' dt ' (s., as)=.(s. \n, bs)(sfr, bs)=.(sfr , cs) SEQ dt '' (s. \u00bb sfr, as)=.(s. \u00bb sfr , cs) dt ' dt ' svsplit asbs . (as, bs)(s., \nas)=.(s. , cs)(sfr, bs)=.(sfr , ds) cs + ds . csds PAR dt '' (s. * sfr, asbs)=.(s. * sfr , csds) dt \ndt ' dt ' s. =.(s.f, cs) as + cs . ascs (sfs, ascs)=.(sfs , bsds) svsplit bsds . (bs, ds)(s.f, ds)=.s. \nf1 f2 LOOP dt '' (loop sfs s. , as)=.(loop sfs s. , bs) (sf , as)=dt.(sf ' , NoEvent :: bs) SW-NOEV \n (switch sf f , as)=dt.(switch sf ' f , bs) dt ' 0 ' (sfs, as)=.(sfs , Event e :: bss) fe . sfr (sfr, \nas)=.(sfr , bsr) SW-EV (switch sfs f , as)=dt.(sfr ' , bsr) (sf , as)=dt.(sf ' , NoEvent :: bs) DSW-NOEV \n (dswitch sf f , as)=dt.(dswitch sf ' f , bs) dt ' 0 ' (sfs, as)=.(sfs , Event e :: bss) fe . sfr (sfr, \nas)=.(sfr , bsr) DSW-EV (dswitch sfs f , as)=dt.(sfr ' , bss) Figure 4. Operational Semantics for the \n=dt.Evaluation Relation As our implementation allows feedback loops and uninitialised signals, this \ndemonstrates that our type system allows for the con\u00adstruction of safe programs with these features. \n6.1 Prototype Implementation We will not discuss the implementation in detail, but give the key points \nthat are required to understand the semantics. Signal functions are represented internally as a data \ntype, of which the constructors are .ve primitive combinators and two primitive signal functions (see \nFigure 3). We will call these the core primitives. Note that this implementation does not allow arbitrary \nrouting to be expressed at the reactive level; routing at the functional level is required for some network \nstructures. This design choice was made for simplicity of presentation, as routing is not the primary \nconcern of this paper. We stress that this is a limitation of this particular implementation, not of \nthe type system in general. The two core signal functions are prim (for causal signal func\u00adtions) and \ndprim (for decoupled signal functions). The internal structures of these two primitives re.ect the properties \nwe require them to have. A causal signal function must be able to produce output at the current time, \nprovided it has access to all past and present inputs. We realise this by giving prim an internal state \n(with which to record past inputs), and a function that maps the time delta (since the preceding time \nstep), state and input to an updated state and output. A decoupled signal function must be able to produce \noutput at the current time, provided it has access to all past inputs. Thus its internal function requires \nonly the time delta and state to produce an output. In order to manage updating the state, it also produces \na function mapping input to an updated state. The key point here is that while the input will be required \nto fully evaluate the signal function at the current time step, the output can be produced before that \ninput is provided. Be aware that prim and dprim are intended to be hidden from the FRP programmer. The \nFRP primitives (such as edge and pre, and many more not described in this paper) are de.ned in terms \nof them internally. Notice that the semantics of loop (rule LOOP in Figure 4) require the use of two \nauxiliary evaluation relations. These are phase 1 evaluation (=.f1 )and phase 2 evaluation (=.f2 ), both \nof which are only de.ned on decoupled signal functions. They are required to allow partial evaluation \nof a decoupled signal function, producing its output without requiring its current input (phase 1), and \nthen to allow this partially evaluated signal function (denoted SFf, see Figure 5) to be updated by providing \nthe input (phase 2). The semantics of these two relations are given in Figures 6 and 7. The types of \ntheir corresponding Agda functions are: =.: .{as bs }. .t \u00d7 SF as bs dec . f1 SFf as bs \u00d7 SVRep bs =.: \n.{as bs }. .t \u00d7 SFf as bs \u00d7 SVRep as . f2 SF as bs dec The semantics also make use of svsplit, an auxiliary \nfunction that splits a signal vector into two parts, determined by the required type of the output: svsplit \n: .{as bs }. SVRep (as + bs) . SVRep as \u00d7 SVRep bs We use .. to denote evaluation at the functional \nlevel. Finally, we let the representation of an event signal be either NoEvent or Event e; in the latter \ncase e is the value carried by the event. 6.2 Semantics of Uninitialised Signals The semantics given \nhere omit any mention of signal initialisation. This is because these semantics only apply to time steps \nafter the initialisation step (at time0). The initialisation step requires a slightly different (and \nmore complicated) set of semantic rules that we do not give in this paper. Be aware that when our evaluation \nrules are used with a zero time delta (as in SW-EVENT), we should actually be using the time0 semantics. \n  7. Related Work The synchronous data-.ow languages [Benveniste et al. 2003, Halbwachs 1993, 1998] \nhave long modelled reactive programs as synchronous data-.ow networks. These languages usually have static \n.rst-order structures, allowing them to prevent undesirable network structures by performing a static \nanalysis at compile time. While not having the expressiveness of FRP, they can provide strong space and \ntime guarantees on their programs. A typical example is Lucid Synchrone [Pouzet 2006], which does not \nallow instantaneous feedback loops, nor the construction of any nodes (signal functions) that produce \nuninitialised output. To guarantee that this is not the case, the decoupling and initiali\u00adsation primitives \n(pre and.) have to appear syntactically within a recursive node de.nition. There has been recent work \nto extend Lucid Synchrone with higher order features [Colac\u00b8o et al. 2004], setting it apart from the \nother synchronous languages. However, higher order nodes cannot be used to decouple feedback loops due \nto the aforementioned syntactic requirements. FRP approaches the problem from the other direction. Most \nFRP implementations are highly expressive, but lack termination guarantees, and space and time bounds. \nReal-Time FRP (RT-FRP) [Wan et al. 2001], a small and experimental CFRP variant, is a no\u00adtable exception \nthat does provide some guarantees. RT-FRP disal\u00adlows instantaneous feedback through the type system, \nwhich, like Lucid Synchrone, insists on the explicit insertion of the decoupling primitive (delay) into \nany recursive calls. However, RT-FRP has very limited capabilities for abstracting over and combining \nreac\u00adtive entities, essentially only being concerned with monolithic reac\u00adtive expressions. Also, there \nare no uninitialised signals in RT-FRP, as delay requires an initialisation value as an additional parameter \n(like iPre in Yampa). FrTime [Cooper and Krishnamurthi 2006] is another recent FRP incarnation, though \ndistinguished in that it is embedded in DrScheme rather than Haskell. FrTime takes the same approach \nto decoupling as Lucid Synchrone, requiring explicit decoupling and initialisation within a node de.nition. \n 8. Further Work We have focused primarily on basic switches in this paper, giving brief mention to \nrecurring switches. However, as demonstrated by Yampa [Nilsson et al. 2002], much more general switches \nare possible that allow for greater dynamism of network structure. Our next task is to extend our implementation \nwith such switches. Yampa is structured using Arrows [Hughes 2000], and thus many of its (and our) combinators \nare arrow combinators. Program\u00adming directly with arrow combinators is awkward for more compli\u00adcated \narrows, and so a syntactic sugar has been devised to aid writ\u00ading of arrow code [Paterson 2001] (similar \nto monadic do notation). A recent development has been the Arrow Calculus [Lindley et al. 2008], an alternative \n(and equivalent) notation for arrows, with a structure bearing more resemblance to the lambda calculus. \nWe are interested in using the arrow calculus for programming at the reac\u00adtive level (where it could \nreplace our core routing combinators).  data SFf : SVDesc . SVDesc . Set where dprimf : .{as bs State \n}. (.t . State . (SVRep as . State) \u00d7 SVRep bs) . (SVRep as . State) . SFf as bs : .{as bs cs d }. SFf \nas bs . SF bs cs d . SFf as cs \u00bbfr \u00bbfl  : .{as bs cs d }. SF as bs d . SFf bs cs . SFf as cs : .{as \nbs cs ds }. SFf as cs . SFf bs ds . SFf (as + bs)(cs + ds) loopf : .{as bs cs ds }. SFf (as + cs)(bs \n+ ds) . SF ds cs dec . SVRep cs . SFf as bs switchf : .{as bs e }. SFf as (E e :: bs) . (e . SF as bs \ndec) . SFf as bs dswitchf : .{as bs e }. SFf as (E e :: bs) . (e . SF as bs dec) . SFf as bs * f  Figure \n5. Partially Evaluated Signal Functions f dt s . (sf, bs) F1-DPRIM dt dprim f s =.(dprimf fsf, bs) f1 \ndt dt ' s. =.(s.f, bs)(sfr, bs)=.(sfr , cs) f1 F1-SEQ-L dt ' s. \u00bb sfr =.f1 (s.f \u00bbfl sfr , cs) sfr =dt.f1 \n(sfrf, cs) F1-SEQ-R dt s. \u00bb sfr =.f1 (s. \u00bbfr sfrf, cs) dt dt s. =.(s.f, cs) sfr =.(sfrf, ds) cs + ds \n. csds f1 f1 F1-PAR dt s. * sfr =.(s.f * f sfrf, csds) f1 dt dt ' sfs =.(sfsf, bsds) svsplit bsds . \n(bs, ds)(s. , ds)=.(s. , cs) f1 F1-LOOP loop sfs s. =dt.f1 (loopf sfsf s. ' cs, bs) sf =dt.f1 (sff, \nNoEvent :: bs) F1-SW-NOEV dt switch sf f =.(switchf sff f , bs) f1 dt 0 sfs =.(sfsf, Event e :: bss) \nfe . sfr sfr =.(sfrf, bsr) f1 f1 F1-SW-EV dt switch sfs f =.(sfrf, bsr) f1 sf =dt.(sff, NoEvent :: bs) \nf1 F1-DSW-NOEV dt dswitch sf f =.(dswitchf sff f , bs) f1 dt 0 sfs =.(sfsf, Event e :: bss) fe . sfr \nsfr =.(sfrf, bsr) f1 f1 F1-DSW-EV dt switch sfs f =.(sfrf, bss) f1 Figure 6. Operational Semantics for \nthe =dt.f1 Evaluation Relation ' sf as . s F2-DPRIM (dprimf fsf, as)=dt.f2 dprim f s ' (s.f, as)=dt.f2 \ns. ' F2-SEQ-L ' dt '' (s.f \u00bbfl sfr , as)=.f2 s. \u00bb sfr dt ' dt ' (s., as)=.(s. , bs)(sfrf, bs)=.sfr f2 \nF2-SEQ-R dt '' (s. \u00bbfr sfrf, as)=.f2 s. \u00bb sfr dt ' dt ' svsplit asbs . (as, bs)(s.f, as)=.s. (sfrf, \nbs)=.sfr f2 f2 F2-PAR dt '' (s.f * f sfrf, asbs)=.s. * sfr f2 as + cs . ascs (sfsf, ascs)=dt.sfs ' \nf2 F2-LOOP ' dt '' (loopf sfsf s. cs, as)=.loop sfs s. f2 (sff, as)=dt.f2 sf ' F2-SW-NOEV (switchf \nsff f , as)=dt.f2 switch sf ' f (sff, as)=dt.f2 sf ' F2-DSW-NOEV (dswitchf sff f , as)=dt.dswitch sf \n' f f2 Figure 7. Operational Semantics for the =dt.f2 Evaluation Relation An important aspect of FRP \nis its capacity for dynamism and higher-order data-.ow. It should be possible to exploit this by receiving \nnew programs (signal functions) as system inputs at run\u00adtime. This is not yet possible in any FRP implementations, \nbut there has been work in Haskell to allow dynamic loading of new code, which would make a good starting \npoint for future work in this direction [Pang et al. 2004]. In this paper, we keep track of decoupledness \nby noting which signal functions are decoupled. We could take a more .ne grained approach by recording \nthe decoupledness of each output signal with respect to each input signal. This would give each signal \nfunction a matrix of decoupledness descriptors, allowing for far more pre\u00adcise tracking of decoupledness \nthan the comparatively conserva\u00adtive method we use in this paper. For example, the following signal function \nwould have all output signals decoupled from all input signals in such a setting, whereas here it is \ntyped as causal: parPre : SF (Cini R :: Cini R :: []) (C uni R :: C uni R :: []) cau parPre =(pre * pure \nid) \u00bb (pure id * pre) Such a setting would allow us, for example, to de.ne a loop com\u00adbinator similar \nto Yampa s, in which it is not necessary to explic\u00aditly separate a feedback signal function from the \nsubordinate signal function. Finally we note that our current prototype is a proof of con\u00adcept implementation \nonly. The long term aim of our work is to de\u00advelop an ef.cient scalable implementation of FRP that incorporates \nthe safety properties described in this paper. For example, while it would not be possible to prove the \nsafety of an embedded imple\u00admentation in Haskell in the manner we have done here, it might be possible \nto encode the domain-speci.c type constraints in a Haskell embedding. Note that it would not matter to \nthe end use that the implementation does not carry the proof as long as the implemen\u00adtation is correct. \nHowever, earlier attempts of ours to work inside Haskell were discouraging (for example, there were problems \nen\u00adcoding associativity of vector concatenation). That was in part what prompted us to switch to a framework \nwhere we would not have to worry about language restrictions to realise our design. But it may be that \nrecent Haskell extensions such as type-level functions would suf.ce. The alternative would be to implement \na more con\u00adventional stand-alone type checker. It may still be possible to do that in an embedded setting \nusing quasiquoting [Mainland et al. 2008, Giorgidze and Nilsson 2008a].  9. Conclusions In this paper \nwe presented a domain-speci.c type system for FRP. This type system ensures that signal function networks \nwith feed\u00adback loops and uninitialised signals will be productive, without sacri.cing support for higher-order \ndata-.ow and structural dy\u00adnamism.  The type system also makes a distinction between the functional \nand reactive levels of an FRP program, restricting some concepts (such as time domain) to the reactive \nlevel. We demonstrated the expressiveness of the type system with a simple example of a well formed program \n(Section 4.2) that would either be statically rejected, or permit non-productive networks to be dynamically \nconstructed, in any other reactive language we are aware of. We have implemented a prototype interpreter \nfor our FRP pro\u00adgrams in Agda. We have given a simpli.ed version of the opera\u00adtional semantics of this \ninterpreter in this paper (omitting initiali\u00adsation at time0), and the full implementation is available \nfrom the .rst author s website. As our implementation is accepted by Agda, we know it is safe and productive.5 \nThe implementation also constitutes a machine\u00adchecked proof of the safety of our type system for FRP, \nin principle allowing it to be used with con.dence by other FRP implementa\u00adtions.  Acknowledgments \nWe would like to thank George Giorgidze, Nils Anders Daniels\u00adson and the anonymous reviewers for their \nhelpful comments and feedback.  References Albert Benveniste, Paul Caspi, Stephen Edwards, Nicolas Halbwachs, \nPaul Le Guernic, and Robert de Simone. The synchronous languages twelve years later. Proceedings of the \nIEEE, Special issue on embedded systems, 91(1):64 83, 2003. G\u00b4erard Berry and Georges Gonthier. The Esterel \nsynchronous programming language: Design, semantics, implementation. Science of Computer Programming, \n19(2):87 152, 1992. Paul Caspi and Marc Pouzet. Synchronous Kahn networks. In International Conference \non Functional Programming (ICFP 96), pages 226 238. ACM, 1996. Mun Hon Cheong. Functional programming \nand 3D games. BEng thesis, University of New South Wales, Sydney, Australia, 2005. Jean-Louis Colac\u00b8o, \nAlain Girault, Gr\u00b4egoire Hamon, and Marc Pouzet. To\u00adwards a higher-order synchronous data-.ow language. \nIn Embedded Software (EMSOFT 04), pages 230 239. ACM, 2004. Gregory H. Cooper and Shriram Krishnamurthi. \nEmbedding dynamic data.ow in a call-by-value language. In European Symposium on Pro\u00adgramming (ESOP 06), \npages 294 308. Springer-Verlag, 2006. Antony Courtney and Conal Elliott. Genuinely functional user interfaces. \nIn Haskell Workshop (Haskell 01), pages 41 69. Elsevier, 2001. Antony Courtney, Henrik Nilsson, and John \nPeterson. The Yampa arcade. In Haskell Workshop (Haskell 03), pages 7 18. ACM, 2003. Conal Elliott. Simply \nef.cient functional reactivity. http://conal.net/papers/simply-reactive/, 2008. Conal Elliott and Paul \nHudak. Functional reactive animation. In Interna\u00adtional Conference on Functional Programming (ICFP 97), \npages 263 273. ACM, 1997. George Giorgidze and Henrik Nilsson. Embedding a functional hybrid modelling \nlanguage in Haskell. In Implementation and Application of Functional Languages (IFL 08), 2008a. To appear. \nGeorge Giorgidze and Henrik Nilsson. Switched-on Yampa: Declarative programming of modular synthesizers. \nIn Practical Aspects of Declara\u00adtive Languages (PADL 08), pages 282 298. Springer-Verlag, 2008b. Nicolas \nHalbwachs. Synchronous Programming of Reactive Systems.The Springer International Series in Engineering \nand Computer Science. Springer-Verlag, 1993. Nicolas Halbwachs. Synchronous programming of reactive systems, \na tutorial and commented bibliography. In Computer Aided Veri.cation (CAV 98), pages 1 16. Springer-Verlag, \n1998. Nicolas Halbwachs, Paul Caspi, Pascal Raymond, and Daniel Pilaud. The synchronous data-.ow programming \nlanguage Lustre. Proceedings of the IEEE, 79(9):1305 1320, 1991. Thomas A. Henzinger. The theory of hybrid \nautomata. In Logics in Computer Science (LICS 96), pages 278 292. IEEE Computer Society, 1996. John Hughes. \nGeneralising monads to arrows. Science of Computer Programming, 37(1 3):67 111, 2000. Sam Lindley, Philip \nWadler, and Jeremy Yallop. The arrow calculus. Technical report, University of Edinburgh, School of Informatics, \n2008. Geoffrey Mainland, Greg Morrisett, and Matt Welsh. Flask: Staged func\u00adtional programming for sensor \nnetworks. In International Conference on Functional Programming (ICFP 08), pages 335 345. ACM, 2008. \nHenrik Nilsson, Antony Courtney, and John Peterson. Functional reactive programming, continued. In Haskell \nWorkshop (Haskell 02), pages 51 64. ACM, 2002. Ulf Norell. Towards a Practical Programming Language Based \non De\u00adpendent Type Theory. PhD thesis, Chalmers University of Technology, 2007. Andr\u00b4e Pang, Don Stewart, \nSean Seefried, and Manuel M. T. Chakravarty. Plugging Haskell in. In Haskell Workshop (Haskell 04), pages \n10 21. ACM, 2004. Ross Paterson. A new notation for arrows. In International Conference on Functional \nProgramming (ICFP 01), pages 229 240. ACM, 2001. John Peterson, Paul Hudak, and Conal Elliott. Lambda \nin motion: Control\u00adling robots with Haskell. In Practical Aspects of Declarative Languages (PADL 99), \npages 91 105. Springer-Verlag, 1999. Benjamin C. Pierce. Types and Programming Languages. MIT Press, \n2002. Marc Pouzet. Lucid Synchrone, version 3.0: Tutorial and reference man\u00adual.Universit\u00b4e Paris-Sud, \nLRI, 2006. http://www.lri.fr/~pouzet/lucid\u00adsynchrone. Neil Sculthorpe and Henrik Nilsson. Optimisation \nof dynamic, hybrid signal function networks. In Trends in Functional Programming (TFP 08), pages 97 112. \nIntellect, 2008. Simulink. Using Simulink, Version 7.3. 3 Apple Hill Drive, Natick, MA, 2009. http://www.mathworks.com. \nSimon Thompson. Type Theory and Functional Programming. Addison-Wesley, 1991. Zhanyong Wan and Paul Hudak. \nFunctional reactive programming from .rst principles. In Programming Language Design and Implementation \n(PLDI 00), pages 242 252. ACM, 2000. Zhanyong Wan, Walid Taha, and Paul Hudak. Real-time FRP. In Interna\u00adtional \nConference on Functional Programming (ICFP 01), pages 146 156. ACM, 2001. 5 Assuming there are no .aws \nin the Agda system itself.  \n\t\t\t", "proc_id": "1596550", "abstract": "<p>Functional Reactive Programming (FRP) is an approach to reactive programming where systems are structured as networks of functions operating on signals. FRP is based on the synchronous data-flow paradigm and supports both continuous-time and discrete-time signals (hybrid systems). What sets FRP apart from most other languages for similar applications is its support for systems with dynamic structure and for higher-order reactive constructs.</p> <p>Statically guaranteeing correctness properties of programs is an attractive proposition. This is true in particular for typical application domains for reactive programming such as embedded systems. To that end, many existing reactive languages have type systems or other static checks that guarantee domain-specific properties, such as feedback loops always being well-formed. However, they are limited in their capabilities to support dynamism and higher-order data-flow compared with FRP. Thus, the onus of ensuring such properties of FRP programs has so far been on the programmer as established static techniques do not suffice.</p> <p>In this paper, we show how dependent types allow this concern to be addressed. We present an implementation of FRP embedded in the dependently-typed language Agda, leveraging the type system of the host language to craft a domain-specific (dependent) type system for FRP. The implementation constitutes a discrete, operational semantics of FRP, and as it passes the Agda type, coverage, and termination checks, we know the operational semantics is total, which means our type system is safe.</p>", "authors": [{"name": "Neil Sculthorpe", "author_profile_id": "81442614500", "affiliation": "University of Nottingham, Nottingham, United Kingdom", "person_id": "P1613978", "email_address": "", "orcid_id": ""}, {"name": "Henrik Nilsson", "author_profile_id": "81100060854", "affiliation": "University of Nottingham, Nottingham, United Kingdom", "person_id": "P1613979", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1596550.1596558", "year": "2009", "article_id": "1596558", "conference": "ICFP", "title": "Safe functional reactive programming through dependent types", "url": "http://dl.acm.org/citation.cfm?id=1596558"}