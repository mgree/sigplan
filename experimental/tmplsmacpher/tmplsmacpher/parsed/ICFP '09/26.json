{"article_publication_date": "08-31-2009", "fulltext": "\n Control-Flow Analysis of Function Calls and Returns by Abstract Interpretation JanMidtgaard RoskildeUniversity \n jmid@ruc.dk Abstract Wederive a control-.ow analysis thatapproximates theinterproce\u00adduralcontrol-.ow \nofbothfunction calls and returnsinthepresence of .rst-class functions and tail-call optimization. In \naddition to an abstract environment, our analysis computes for each expression an abstract control stack, \neffectively approximating where func\u00adtion calls return across optimized tail calls. The analysis is sys\u00adtematically \ncalculated by abstract interpretation of the stack-based CaEK abstract machine of Flanagan et al. using \na series of Galois connections. Abstract interpretation provides a unifying setting in which we 1) prove \nthe analysis equivalent to the composition of a continuation-passing style(CPS)transformationfollowedby \nan ab\u00adstractinterpretation of a stack-lessCPS machine, and 2) extract an equivalent constraint-based \nformulation, therebyproviding a ratio\u00adnal reconstruction of a constraint-based control-.ow analysisfrom \nabstractinterpretationprinciples. Categories and Subject Descriptors F.3.2[Logics and Mean\u00adings of Programs]: \nSemantics of Programming Languages Pro\u00adgramAnalysis GeneralTerms Languages,Theory,Veri.cation Keywords \nControl .owanalysis,abstractinterpretation,tail-call optimization, continuation-passing style, direct \nstyle, constraint\u00adbased analysis 1. Introduction The control .ow of a functional program is expressed \nin terms of function calls and returns. As a result, iteration in functional pro\u00adgrams is expressed using \nrecursive functions. In order for this ap\u00adproach to be feasible, language implementations perform tail-call \noptimization of function calls [Clinger, 1998], by not pushing a stack frame on the control stack at \ncall sites in tail position. Con\u00adsequentlyfunctionsdo not necessarily return control to their caller. \nControl-.ow analysis(CFA)haslongbeen a staple ofprogram op\u00adtimization and veri.cation. Surprisingly, \nresearch on control-.ow analysis has focused on calls: A textbook CFA will determine where the .ow of \ncontrol may be transferred to in the case [...] of a function application. [Nielson et al., 1999]. Our \nsystematic approximation of a known operational semantics leads to a CFA Permission to make digital or \nhard copies of all or part of this work for personal or classroomuseisgranted withoutfeeprovided that \ncopiesarenot madeordistributed forpro.tor commercial advantage andthat copiesbearthis notice andthefull \ncitation onthe .rstpage.Tocopy otherwise,torepublish,topostonserversortoredistribute tolists, requiresprior \nspeci.cpermission and/or afee. ICFP 09, August31 September2,2009,Edinburgh,Scotland,UK. Copyright c &#38;#169; \n2009ACM978-1-60558-332-7/09/08. . .$10.00 ThomasP.Jensen CNRS thomas.jensen@irisa.fr let g z= zin let \nfk =if bthen k 1else k 2in let y= f(fn x => x) in gy (a)Exampleprogram call call return call return \ncall return call call (c)Optimizedcallgraph Figure1:The corresponding callgraphs that will determine \nwhere the .ow of control may be transferred to in the case of a function return. The resulting analysis \nthereby approximates both call and return information for a higher-order, direct-stylelanguage.Interestinglyitdoes \nsobyapproximating the control stack. Consider the example program in Fig. 1(a). The program con\u00adtains \nthree functions: two named function g and f and an anony\u00admousfunction fn x=> x.A standarddirect-styleCFA \ncandeter\u00admine that the applications of k in each branch of the conditional will call the anonymous function \nfn x => x at run time. Build\u00ading a call-graph based on this output gives rise to Fig. 1(b), where wehave \nnamed the main expression of theprogram main.In addi\u00adtion to the above resolved call, our analysis willdetermine \nthat the anonymous function returns to the let-binding of y in main upon completion, rather than to its \ncaller. The analysis hence gives rise to the callgraphinFig.1(c). On a methodological level, we derive \nthe analysis systemati\u00adcally by Cousot-Cousot-style abstract interpretation. The analysis approximates \nthe reachable states of an existing abstract machine from the literature: the CaEK machine ofFlanagan \net al.[1993]. We obtain the analysis as the result of composing the collecting semantics induced by the \nabstract machine with a series of Galois connections that each speci.es one aspect of the abstraction \nin the analysis. We show how the abstract interpretation formulation lends it\u00adself to a lock-step equivalence \nproof between our analysis and a previously derived CPS-based CFA. More precisely, we de.ne a relationbetween \nthe abstractdomains of the analyses thatis a sim\u00adulationbetweenthetwo,reducing theproof toa .xpointinduction \nover the abstractinterpretations. To sum up, the main contributions of this article are: An abstract \ninterpretation-derivation of a CFA for a higher\u00adorder functional language from a well-known operational \nse\u00admantics,  a resultingCFA with reachability which computesboth call and return control-.ow,  aproof \nof equivalence ofthe analysis ofprogramsindirect style and theCPS analysis of theirCPS counterparts, \n an equivalent constraint-based analysis extracted from the above.  1.1 Related work We separate the \ndiscussion of related analyses in two: direct-style analyses and analysesbased onCPS. Direct-styleCFA \nhas along researchhistory.Jones[1981]ini\u00adtially developed methods for approximating the control .ow of \nlambda terms.Since thenSestoft[1989] conceived the related clo\u00adsure analysis.Palsberg[1995] simpli.edthe \nanalysis andformu\u00adlated an equivalent constraint-based analysis. At the same time Heintze[1994] developed \na related set-based analysisformulated in terms of set constraints. For a detailed account of related \nwork, we referto a recent survey ofthe area[Midtgaard,2007].Itis worth emphasizing that all of the above \nanalyses focus on calls, in that they approximate the sourcelambdasbeing called at each call site. Assuch \ntheydonotdirectlydeterminereturn .owforprogramsin direct style. CPS-basedCFA waspioneered byShivers[1988] \nwhoformu\u00adlatedcontrol-.ow analysisforScheme.Sincethen several analyses havebeenformulatedforCPS[Ayers,1992,Ashley \nandDybvig, 1998, Might and Shivers,2006]. In CPS all calls are tail calls, and even returns are encoded \nas callstothe current continuation.Byde\u00adtermining call.ow andhencethereceiverfunctionsof such con\u00adtinuation \ncalls, a CPS-based CFA thereby determines return .ow without additional effort. The impact of CPS transformation \non static analyses orig\u00adinates in binding-time analysis, for which the transformation is known to have \napositive effect[Consel and Danvy, 1991, Damian andDanvy,2003].Asto theimpact ofCPStransformation onCFA \nwe separate theprevious work on the subjectin two: 1. results relating an analysis specialized to the \nsource language to an analysis specialized to the targetlanguage(CPS), and 2. results relating the analysis \nof a program to the same analysis of theCPS transformedprogram.  Sabry and Felleisen [1994] designed \nand compared specialized analyses and hence falls into the .rst category as does the present paper.Damian \nandDanvy[2003] related the analysis of aprogram and its CPS counterpart for a standard .ow-logic CFA \n(as well asfortwobinding-time analyses), andPalsberg andWand[2003] related the analysis of a program \nand its CPS counterpart for a standard conditional constraintCFA.Hencethelattertwofallinto the second \ncategory. We paraphrase the relevant theorems of Sabry and Felleisen [1994], of Damian and Danvy [2003], \nof Palsberg and Wand [2003], and of the present paper in order to underline the differ\u00adencebetween the \ncontributions(C refers to non-trivial,0-CFA-like analysesde.nedin the citedpapers, pranges overdirect-stylepro\u00adgrams, \ncps denotes CPS transformation, and ~ denotes analysis equivalence). Ourformulations should notbe read \nas aformal sys\u00adtem,but only as a meansfor elucidating thedifferencebetween the contributions. Sabry and \nFelleisen[1994]: exists analyses C1, C2 : exists p, C1( p) .C2(cps( p)) Damian and Danvy[2003], Palsberg \nand Wand[2003]: exists analysis C :for all p, C( p) ~ C(cps( p)) Presentpaper,Theorem 5.1: exists analyses \nC1, C2 :for all p, C1( p) ~ C2(cps( p)) Our work relates to all of the above contributions. The dis\u00adciplined \nderivation of specialized CPS and direct-style analyses results in comparable analyses, contrary to Sabry \nand Felleisen [1994]. Furthermore our equivalence proof extends the results of Damian andDanvy[2003] \nandPalsberg andWand[2003] inthat we relate both call .ow, return .ow, and reachability, contrary to their \nrelating only the call .ow of standard CFAs. In addition, the systematic abstract interpretation-based \napproach suggests a strat\u00adegyfor obtaining similarequivalence resultsfor otherCFAsderived in thisfashion. \nFormulating CFA in the traditional abstract interpretation framework was stated as an openproblembyNielson \nandNielson [1997].Ithasbeen a recurringthemeinthe work of thepresent au\u00adthors.In an earlierpaperSpoto \nandJensen[2003]investigated class analysis of object-oriented programs as aGalois connection-based abstraction \nof a trace semantics. In a recent article[Midtgaard and Jensen, 2008a], the authors systematically derived \na CPS-based CFAfromthecollecting semanticsof astack-lessmachine.While investigatinghowtoderive a correspondingdirect-style \nanalysis we discovered a mismatchbetween the computed returninformation. As tail calls areidenti.ed syntactically, \nthe additional informa\u00adtion could also have been obtained by a subsequent analysis af\u00adter a traditional \ndirect-style CFA. However we view the need for such a subsequent analysis as a strongindication of a \nmismatchbe\u00adtween the direct-style and CPS analysis formulations. Debray and Proebsting[1997]haveinvestigated \nsuch a return analysis for a .rst-orderlanguage with tail-call optimization.Thispaperbuilds a semantics-based \nCFA that determines such information, and for a higher-orderlanguage. The systematic design of constraint-based \nanalyses is agoal shared with the .ow logic framework of Nielson and Nielson [2002]. In .ow logic an \nanalysis speci.cation can be systemat\u00adically transformed into a constraint-based analysis. The present \npaper instead extracts a constraint-based analysis from an analysis developed in the original abstractinterpretationframework. \nThe idea of CFA by control stack approximation, applies equally well to imperative or object-oriented \nprograms, but it is beyond the scope of this paper to argue this point. Due to space limitations most \ncalculations andproofs are also omitted.We refer the reader to the accompanying technical report [Midtgaard \nand Jensen,2008b].  2. Language and semantics Our sourcelanguageis a simple call-by-value corelanguageknown \nas administrative normalform (ANF).Thegrammar ofANFterms is given in Fig. 2(a). Following Reynolds, the \ngrammar distin\u00adguishes serious expressions, i.e., terms whose evaluation may di\u00adverge, from trivial expressions, \ni.e., terms without risk of diver\u00adgence. Trivial expressions include constants, variables, and func\u00adtions, \nand serious expressions include returns, let-bindings, tail calls, and non-tail calls.Programs are serious \nexpressions. The analysis is calculated from a simple operational semantics in the form of an abstract \nmachine. We use the environment-based CaEK abstract machine ofFlanagan et al.[1993]giveninFig.2in which \nfunctional values are represented using closures, i.e., pairs of a lambda-expression and an environment. \nThe environment\u00adcomponent capturesthe(values ofthe)free variables ofthelambda. Machine states are triples \nconsisting of a serious expression, an P. p ::= s (programs) T . t ::= c | x | fnx =>s (trivialexpressions) \nC . s ::= t | letx=t ins | t0 t1 | let x =t0 t1 in s (serious expressions) (a)ANFgrammar '' Val. w ::= \nc | [fnx =>s, e] (t , e, [x, s , e ' ] :: k ')-. (s , e ' [x . \u00b5(t,e)], k ') Env. e ::= | e[x . w] (letx=t \nins , e, k)-. (s, e[x . \u00b5(t ,e)], k) ' K . k ::= stop | [x, s , e] :: k (t0 t1 , e, k)-. (s , e ' [x \n. w], k) ' (b)Values,environments,andstacks if [fnx =>s , e ' ]= \u00b5(t0 ,e) and w = \u00b5(t1 ,e) ' (let x \n=t0 t1 in s , e, k)-. (s , e ' [y . w], [x, s, e] :: k) \u00b5 : T\u00d7 Env. Val if [fny =>s ' , e ' ]= \u00b5(t0 ,e) \nand w = \u00b5(t1 ,e) \u00b5(c,e)= c (d)Machine transitions \u00b5(x ,e)= e(x) \u00b5(fnx =>s,e)=[fnx =>s , e] eval(p)= w \niff (p, , [xr , xr , ] :: stop) -.* (xr , [xr . w], stop) (c)Helperfunction (e)Machineevaluation Figure2:TheCaEK \nabstract machine environment and a control stack. The control stack is composed of elements( stackframes \n) of theform [x, s, e] where x is the variable receiving the return value w of the current function call, \nand s is a serious expression whose evaluation in the environment e[x . w] represents the rest ofthe \ncomputationinthat stackframe. The empty stackis representedby stop.The machinehas ahelper function \u00b5 \nfor evaluation of trivial expressions. The machine is initializedwiththeinputprogram, with an empty environment, \nand with an initial stack, that will bind the result of the program to a special variable xr before halting. \nEvaluation follows by repeated application of the machine transitions.  3. Abstractinterpretationbasics \nWe assume some familiarity with the basic mathematical facts re\u00adcalled in Appendix A. Canonical abstract \ninterpretation approx\u00adimates the collecting semantics of a transition system [Cousot, 1981]. A standard \nexample of a collecting semantics is the reach\u00adable states from a given set of initial states I. Given \na transition '' function T de.ned as: T(S)= I.{s |.s . S : s . s }, we can compute the reachable states \nof T as the least .xed-point lfpT of T. The collecting semantics is ideal, in that it is the most precise \nanalysis.Unfortunately itisingeneral uncomputable.Abstractin\u00adterpretationtherefore approximates the collecting \nsemantics,byin\u00adstead computing a .xed-point overanalternativeandperhapssim\u00adplerdomain.For this reason, \nabstractinterpretationis also referred to as a theory of .xed-point approximation. Abstractions are formally \nrepresented as Galois connections which connect completelattices through apair of adjointfunctions a \nand . (seeAppendixA).Galoisconnection-basedabstractinter\u00adpretation suggests that one may derive an analysis \nsystematically by composing the transitionfunction with these adjoints: a . T . .. In this setting Galois \nconnections allow us to gradually re.ne the collecting semantics into a computable analysis function \nby mere calculation. An alternative recipe consists in rewriting the com\u00adposition of the abstraction \nfunction and transition function a . T into something of the form T. . a, from which the analysis func\u00adtion \nT. canbe read off[Cousot andCousot,1992a].Cousot[1999] has shown how to systematically construct a static \nanalyser for a .rst-order imperative language using calculational abstract inter\u00adpretation.  4. ApproximatingtheCaEK \ncollecting semantics As our collecting semantics we consider the reachable states of the CaEK machine, \nexpressed as the least .xed point lfpF of the following transitionfunction. F :P(C\u00d7 Env\u00d7 K) . P(C\u00d7 Env\u00d7 \nK) '' F(S)= Ip .{s |.s . S : s -. s} where Ip = {(p , , [xr , xr , ] :: stop)} First weformulateinFig.3(a) \nan equivalent helperfunction \u00b5c extended to work on sets of environments. Lemma4.1. .t,e : {\u00b5(t ,e)} \n= \u00b5c(t ,{e}) The equivalence of the two helper functions follow straightfor\u00adwardly. This lemma enables \nus to express an equivalent collecting semanticsbased on \u00b5c, which appearsinFig.3.The equivalence of \nF and Fc followsfrom thelemma andbyunfolding thede.nitions. The abstraction of the collecting semantics \nis staged in several steps. Figure 4 provides an overview. Intuitively, the analysis ex\u00adtracts threepieces \nofinformationfrom the set of reachable states. 1. An approximation of the set of reachable expressions. \n 2. A relation between expressions and control stacks that repre\u00adsents where the values of expressions \nare returned to. 3. An abstract environment mapping variables to the expressions that maybebound to \nthat variable.Thisis standardinCFA and allows to determine which functions are called at a given call \nsite.  Keeping an explicit set of reachable expressions is more precise than leaving it out, once we \nfurther approximate the expression\u00adstack pairs. Alternatively the reachable expressions would be ap\u00adproximatedbythe \nexpressionspresentinthe expression-stack rela\u00adtion. However expressions may be in the expression-stack \nrelation without everbeing reached.An examplehereof wouldbe adiverg\u00ading non-tail call. Toformalizethisintuition,we \n.rstperformaCartesianabstrac\u00adtion of the machine states, however keeping the relation between expressions \nandtheir corresponding control stacks.The second step in the approximation consistsin closingthe triplesby \na closure op\u00aderator, to ensure that (a) any saved environment on the stack or nested within another environmentisitselfpart \nof the environment \u00b5c : T\u00d7P(Env) . P(Val) \u00b5c(c,E)= {c} \u00b5c(x,E)= {w |.e . E: w = e(x )} \u00b5c(fnx =>s,E)= \n{[fnx =>s, e] |.e . E} (a)Helperfunction Fc :P(C\u00d7 Env\u00d7 K) . P(C\u00d7 Env\u00d7 K) Fc(S)= Ip . {(s ' , e ' [x . \nw], k ')} '' (t,e,[x ,s ,e ]::k ').S w.\u00b5c(t ,{e})  . {(s, e[x . w], k)} (letx =t ins ,e,k).S w.\u00b5c(t \n,{e}) . {(s ' , e ' [x . w], k)} (t0 t1 ,e,k).S '' [fnx =>s ,e ].\u00b5c(t0 ,{e}) w.\u00b5c(t1 ,{e}) ' . {(s \n, e ' [y . w], [x, s , e] :: k)} (let x =t0 t1 in s ,e,k).S '' [fny =>s ,e ].\u00b5c(t0 ,{e}) w.\u00b5c(t1 ,{e}) \n(b)Transitionfunction Figure3:Collecting semantics P(C\u00d7 Env\u00d7 K) coll. sem. Fc   .\u00d7 a\u00d7 H  P(C) \u00d7P(C\u00d7 \nK) \u00d7P(Env) -F\u00d7   1 . H  .(P(C) \u00d7P(C\u00d7 K) \u00d7P(Env)) -F.   .. a. H  P(C) \u00d7 (C/=. P(K.)) \u00d7 Env. 0-CFA \nF. Figure4:Overview of abstraction set, and(b) that all expression-control stackpairsthat appearfur\u00adther \ndown in a control stack are also contained in the expression\u00adstack relation.We explainthisin moredetailbelow(Section4.2). \nFinally as a third step we approximate stacks by their top element, we merge expressions with the same \nreturn point into equivalence classes, and we approximate closure valuesbytheirlambda expres\u00adsion. In \nthe following sections we provide a detailed explanation of each abstractionin turn. 4.1 Projecting \nmachine states The mapping that extracts the threekinds ofinformationdescribed aboveisde.nedformally \nasfollows. .\u00d7 --- P(C\u00d7 Env\u00d7 K) -.--.P(C) \u00d7P(C\u00d7 K) \u00d7P(Env) a\u00d7 a\u00d7(S)= (p1S,{(s , k)|.e : (s , e, k). S},p2S) \n.\u00d7((C, F, E))= {(s , e, k)| s . C .(s, k). F . e . E} Lemma4.2. a\u00d7, .\u00d7 is aGalois connection. The above \nGalois connection and the proof hereof closely re\u00adsembles the independent attributes abstraction, which \nis a known Galois connection.We use the notation .\u00d7 and .\u00d7 for the compo\u00adnentwisejoin and componentwiseinclusion \nof triples. As traditional[CousotandCousot,1979,1992a,1994], we will assume that the abstract product \ndomains throughout this article have been reduced, i.e., all triples (A, B, C) with a bottom compo\u00adnent(A \n= .a . B = .b . C = .c )have been eliminated and replacedby a singlebottom element (.a, .b, .c). Based \non this abstraction we can now calculate a new transfer functionF\u00d7.The resulting transitionfunction appearsinFig.5.By \nconstruction,thetransitionfunction satis.esthefollowingtheorem. Theorem4.1. .C,F,E: a\u00d7(Fc(.\u00d7((C, F, E)))) \n= F\u00d7((C, F, E))  4.2 A closure operator on machine states For the .nal analysis, we are only interested \nin an abstraction of theinformationpresentin an expression-stackpair.Moreprecisely, we aim at only keeping \ntrack of the link between an expression and the top stack frame in effect during its evaluation, throwing \naway everythingbelow.However, we needto makethisinformation explicit for all expressions appearing on \nthe control stack, i.e., '' for a pair (s , [x, s , e] :: k) we also want to retain that s will be evaluated \nwith control stack k. Similarly, environments can be stored on the stack orinside other environments \nand willhavetobe extracted. We achieve this by de.ning a suitable closure operator on these nested structures. \nFor environments, we adapt the de.nition of a constituent re\u00adlationduetoMilner andTofte[1991] Wesaythateach \ncompo\u00adnent xi of a tuple (x0,...,xn) is a constituent of the tuple, written (x0,...,xn). xi. For a partial \nfunction1 f =[x0 . w0,...,xn . wn], we say that each wi is a constituent of the function, written f . \nwi.We write .* for the re.exive, transitive closure ofthe con\u00adstituent relation. Todealwiththe control \nstack, wede.ne an order on expression\u00adstackpairs.Twopairsare orderedif(a) the stackcomponent of the second \nis the tail of the .rst s stack component, and (b) the expression component ofthe second, resides onthetop \nstackframe '' * of the .rst pair: (s , [x, s , e] :: k) . (s , k). We write . for the re.exive, transitive \nclosure of the expression-stackpair ordering. Next, we consider an operator ., de.ned in terms of the \ncon\u00adstituent relation and the expression-stackpair ordering.The opera\u00adtor . ensures that all constituent \nenvironments will themselves be\u00ad long to the set of environments, and that any structurally smaller expression-stack \npairs are also contained in the expression-stack relation. De.nition4.1. '' .((C, F, E))= (C,{(s , k) \n| .(s , k '). F:(s , k '). *(s, k)}, '' {e | .(s, k). F : (s , k) . * e ..e . E : e .* e}) We need to \nrelate the expression-stack ordering to the con\u00ad ' stituentrelation.By caseanalysis one canprovethat \n.(s, k),(s , k ') : ' (s, k) . (s , k ') =. k . k ' . By structural induction (on the '* stack component) \nit now follows that .(s, k),(s , k ') : (s, k) . ' (s , k ') =. k .* k ' .Based on these results we can \nverify that . is a closure operator andformulate an abstraction on the triples: 1 .-- .(P(C)\u00d7P(C\u00d7 K)\u00d7P(Env)) \nP(C)\u00d7P(C\u00d7 K)\u00d7P(Env) --. . 1Milner andToftede.netheconstituent relationfor .nitefunctions. F\u00d7 :P(C) \n\u00d7P(C\u00d7 K) \u00d7P(Env) . P(C) \u00d7P(C\u00d7 K) \u00d7P(Env) F\u00d7((C, F, E))= ({p }, {(p , [xr , xr , ] :: stop)}, { }) .\u00d7 \n({s '}, {(s ' , k ')}, {e ' [x . w]}) \u00d7 '' ({t},{(t ,[x ,s ,e ]::k ')},{e}).\u00d7(C,F,E) w.\u00b5c(t,{e})  \n .\u00d7 ({s }, {(s, k)}, {e[x . w]}) \u00d7 ({letx =t ins },{(letx =t ins ,k)},{e}).\u00d7(C,F,E) w.\u00b5c(t,{e}) .\u00d7 ({s \n'}, {(s ' , k)}, {e ' [x . w]}) \u00d7 ({t0 t1 },{(t0 t1 ,k)},{e}).\u00d7(C,F,E) '' [fnx =>s ,e ].\u00b5c(t0 ,{e}) \nw.\u00b5c(t1 ,{e}) .\u00d7 ({s '}, {(s ' , [x, s, e] :: k)}, {e ' [y . w]}) \u00d7 ({let x =t0 t1 in s },{(let x =t0 \nt1 in s ,k)},{e}).\u00d7(C,F,E) '' [fny =>s ,e ].\u00b5c(t0 ,{e}) w.\u00b5c(t1 ,{e}) Figure5:Abstract transitionfunction \nWe use the notation .. forthejoin operation . X..(.\u00d7X) on theclosureoperator-induced completelattice.First \nobservethatin our case: .. = . X..( Xi)= . X. .(Xi)= . X. Xi = .\u00d7 \u00d7\u00d7 \u00d7 ii i Based on the closure operator-based \nGalois connection, we can calculate a new intermediate transfer function F. . The resulting transfer \nfunction appears in Fig. 6. This transfer function differs only minimally from the one in Fig. 5, in \nthat (a) the signature has changed, (b) the set of initial states has been closed and now contains the \nstructurally smaller pair (xr , stop), and(c)the fourindexedjoinsnoweachjoin closed triplesintheimageof \nthe closure operator. By construction, the new transition function satis.es thefollowing theorem. Theorem4.2. \n.C,F,E: . . F\u00d7. 1((C, F, E))= F. ((C, F, E))  4.3 Abstracting the expression-stack relation Since stacks \ncan grow unbounded (for non-tail recursive pro\u00adgrams), we need to approximate the stack component and \nhereby theexpression-stack relation.We .rstformulateagrammarof ab\u00adstract stacks and an elementwise operator \n@ : C\u00d7 K . C\u00d7 K. operating on expression-stackpairs. K. . k. ::= stop | [x, s] @((s, stop))= (s, stop) \n' @((s, [x, s , e] :: k))= (s, [x, s ' ]) Basedonthe elementwise operator we can now use an elementwise \nabstraction. Elementwise abstraction[Cousot andCousot,1997]: Agiven elementwise operator@:C . A induces \naGalois connection: .@ .--- (P(A);.)(P(C);.) ---. a@ a@(P)= {@( p) | p. P} .@(Q)= {p| @( p) . Q} Noticehow \nsome expressions sharethe same returnpoint(read: same stack):the expressions letx=t ins ands sharethe \nsame re\u00adturnpoint, andlet x =t0 t1 in s ands sharethe same returnpoint. In order to eliminate such redundancy \nwede.ne an equivalence re\u00adlation on serious expressionsgroupingtogether expressions sharing the same \nreturn point. We de.ne the smallest equivalence relation = satisfying: letx =t ins =s let x=t0 t1 in \ns =s Based hereon we de.ne a second elementwise operator @ ' : C\u00d7 K. . C/=\u00d7 K. mappingthe .rstcomponent \nofanexpression\u00adstackpairto a representative ofitscorrespondingequivalence class: @ ' ((s, k.))= ([s]=, \nk.) We can choose the outermost expression as a representative for each equivalence class by a linear \ntop-down traversal of the input program. Pointwise coding of a relation[Cousot andCousot,1994]: A relation \ncanbeisomorphically encoded as a set-valuedfunctionby aGalois connection: (P(A\u00d7 B);.) -. ----. (A. P(B);. \n.. - .) -- a. a. (r)= . a.{b |(a, b). r} .. ( f)= {(a, b)| b. f(a)} By composing the three above Galois \nconnections we obtain our abstraction of the expression-stack relation: .st P(C\u00d7 K) ---- C/=. P(K.) .--. \nast where ast = a. . a@ '. a@ = . F. .(s,k).Fa. ({@ '. @((s, k))}) and .st = .@ . .@ '. .. . We can \nnow prove a lemma relating the concrete and abstract expression-stack relations. Lemma4.3. Control stack \nand saved environments Let(C, F, E). .(P(C) \u00d7P(C\u00d7 K) \u00d7P(Env)) begiven. '' (s, [x, s , e] :: k). F =. \ne . E . {(s , k)} . F .{[x , s ' ]}. ast(F)([s]=) Proof. The .rsthalffollowsfromtheassumptions.Thesecondhalf \nfollowsfrom monotonicity of ast, and thede.nitions of ast, ..,@, @ ' , a. , and ... 4.4 Abstractingenvironments \nWe also abstract values using an elementwise abstraction. Again we formulate a grammar of abstract values \nand an elementwise F. : .(P(C) \u00d7P(C\u00d7 K) \u00d7P(Env)) . .(P(C) \u00d7P(C\u00d7 K) \u00d7P(Env)) F. ((C, F, E))= ({p }, {(p \n, [xr , xr , ] :: stop),(xr , stop)}, { }) .\u00d7 .(({s '}, {(s ' , k ')}, {e ' [x . w]})) \u00d7 '' ({t },{(t \n,[x ,s ,e ]::k ')},{e}).\u00d7(C,F,E) w.\u00b5c(t ,{e})   .\u00d7 .(({s }, {(s , k)}, {e[x . w]})) \u00d7 ({letx =t ins \n},{(letx =t ins ,k)},{e}).\u00d7(C,F,E) w.\u00b5c(t ,{e}) .\u00d7 .(({s '}, {(s ' , k)}, {e ' [x . w]})) \u00d7 ({t0 t1 \n},{(t0 t1 ,k)},{e}).\u00d7(C,F,E) ' [fnx =>s ,e ' ].\u00b5c(t0 ,{e}) w.\u00b5c(t1 ,{e})  .\u00d7 .(({s '}, {(s ' , [x, \ns , e] :: k)}, {e ' [y . w]})) \u00d7 ({let x =t0 t1 in s },{(let x =t0 t1 in s ,k)},{e}).\u00d7(C,F,E) ' [fny \n=>s ,e ' ].\u00b5c(t0 ,{e}) w.\u00b5c(t1 ,{e})  Figure6:The second abstract transitionfunction operator@: Val. \nVal. mapping concrete to abstract values. . Val. . w ::= c | [fnx =>s] @(c)= c @([fnx =>s , e])= [fnx \n=>s] The abstraction of environments, which are partial functions, canbe composedby a series of well-knownGalois \nconnections. Pointwise abstraction of a set offunctions[Cousot andCousot, 1994]: A given Galois connection \non the co-domain (P(C);.) . .-- (C.;.) induces aGalois connection on a setoffunctions: --. a .. (P(D. \nC);.) -.--.(D. C.;. --- .) a. a.(F)= . d.a({ f(d) | f . F}) ..(A)= { f |.d : f(d) . .(A(d))} Subset abstraction[Cousot \nandCousot,1997]: Given a set C anda strictsubset A. Chereof,the restrictiontothe subsetinduces aGalois \nconnection: .. .---- (P(C);.) ----. (P(A);.) a. a.(X)= Xn A ..(Y)= Y . (C\\ A) A standard trick is to \nthink of partial functions r : D . C as total functions r. : D . (C..) where .... c, for all c . C. Consider \nenvironments e . Var . Val to be total functions Var . (Val..) usingthisidea.In this context thebottom \nelement . will denote variable lookup failure. Now compose a subset abstraction .. .---- P(Val..) ----. \nP(Val) with the above value abstraction, and a. feed the result to the pointwise abstraction above. The \nresult is a pointwise abstraction of a set of environments, not explicitly .. --- modelling variablelookupfailure:P(Env) \n-.--.Var . P(Val.). a. By considering only closed programs, we statically ensure against failure of variable-lookup, \nhence disregarding . loses noinforma\u00adtion.  4.5 Abstracting thehelperfunction We can calculate an abstract \nhelper function, by pushing a s under the function de.nition, and reading off a resulting abstract de.nition. \n Lemma4.4. .t,E : a@(\u00b5c(t,E)) = \u00b5.(t,a.(E)) . The resultinghelperfunction \u00b5: T\u00d7 Env. . P(Val.) reads: \n\u00b5 .(c,E.)= {c} \u00b5 .(x ,E.)= E.(x) \u00b5 .(fnx =>s ,E.)= {[fnx =>s ]} where we write Env. as shorthand for \nVar . P(Val.). We shall needalemma relatingthetwohelperfunctionde.nitions on closed environments. Lemma4.5. \nHelperfunction on closed environments(1) Let(C, F, E). .(P(C) \u00d7P(C\u00d7 K) \u00d7P(Env)) begiven. {[fnx =>s , \ne]}. \u00b5c(t ,E)=. e . E .{[fnx =>s]}. \u00b5 .(t ,a.(E)) The abovelemmais easilyextendedto capture nestedenvironments \nin allvalues returnedby thehelperfunction: Lemma4.6. Helperfunction on closed environments(2) Let(C, \nF, E). .(P(C) \u00d7P(C\u00d7 K) \u00d7P(Env)) begiven. '' '' {w}. \u00b5c(t,E) . w .* e =. e . E  4.6 Abstractingthe machine \nstates We abstract the triplet of setsinto abstract triplesby a component\u00adwise abstraction. Componentwise \nabstraction[Cousot andCousot,1994]: As\u00ad .i --  suming a series of Galois connections: P(Ci) .- -.Ai \nfor i . ai {1,...,n},their componentwise compositioninduces aGalois con\u00adnection on tuples: .. (P(C1) \n\u00d7 ... \u00d7P(Cn);.\u00d7) ---- (A1\u00d7 ... \u00d7 An;..) .--. a. a.((X1, ..., Xn))= (a1(X1), ..., an(Xn)) ..((x1, ..., \nxn))= (.1(x1), ..., .n(xn)) We write .. and .. for componentwise join and inclusion, re\u00adspectively. \nFor the set of expressions P(C) we use the identity abstrac\u00adtion consisting of two identity functions. \nFor the expression-stack F. : P. P(C) \u00d7 (C/=. P(K.)) \u00d7 Env. . P(C) \u00d7 (C/=. P(K.)) \u00d7 Env. F. ((C, F., \nE.))= ({p}, [[p]= .{[xr , xr ]},[xr ]= .{stop}], . _.0/) p .. ({s '}, F., E. ..[x . \u00b5 .(t ,E.)]) . {t \n}.C ' {[x ,s ]}.F.([t]= ) .. ({s}, F., E. ..[x . \u00b5 .(t ,E.)]) . {letx =t ins }.C  .. ({s '}, F. ..[[s \n' ]= . F.([t0 t1 ]=)], E. ..[x . \u00b5 .(t1 ,E.)]) . {t0 t1 }.C {[fnx =>s ' ]}.\u00b5. (t0 ,E.)  .. ({s '}, F. \n..[[s ' ]= .{[x, s]}], E. ..[y . \u00b5 .(t1 ,E.)]) . {let x =t0 t1 in s }.C {[fny =>s ' ]}.\u00b5. (t0 ,E.)  \nFigure7:The resulting analysisfunction relation P(C\u00d7 K) we use the expression-stack abstraction ast de\u00advelopedinSection4.3.For \nthe set of environments P(Env) we use the environment abstraction a. developedinSection4.4. Using the \nalternative recipe we can calculate the analy\u00adsis by pushing a s under the intermediate transition function: \na.(F. ((C, F, E))) .. F.((C, ast(F), a.(E))) from which the .nal de.nition of F. can be read off. The \nresulting analysis ap\u00adpears in Fig. 7. The alert reader may have noticed that this .nal abstraction is \nnot complete in that the above equation contains an inequality. Completeness is a desirable goal in an \nabstract inter\u00adpretation but unfortunately it is not possible in general without re.ning the abstract \ndomain [Giacobazzi et al., 2000]. Consider for example the addition operator over the standard sign-domain: \n0 = a(1+(-1)) . a(1)+ a(-1)= .. As traditional [Cousot, 1999], weinsteadlimitupwardjudgements to a minimum. \nAs a corollary of the construction, the analysis safely approxi\u00admates the reachable states of the abstract \nmachine. Corollary 4.1. a.. . . a\u00d7(lfpF) .. lfpF. 4.7 Characteristics of the analysis First of all the \nanalysis incorporates reachability: it computes an approximate set of reachable expressions and will \nonly analyse those reachable program fragments. Reachability analyses have previously been discovered \nindependently[Ayers, 1992, Palsberg and Schwartzbach, 1995, Gasser et al., 1997]. In our case they arise \nnaturally from a projecting abstraction of a reachable states collecting semantics. Second the formulation \nmaterializes monomorphism into two mappings:(a) onemapping merging allbindingstothesamevari\u00adable, and(b) \none mapping merging all calling contexts of the same function.Both characteristics are wellknown,but \nourpresentation is novelin thatitliterallycaptures thisphenomenonin two approx\u00adimationfunctions. Thirdthe \nanalysishandles returnsinside-out( callee-restore ), in that the called function restores control from \nthe approximate control stack andpropagatesthe obtained return values.Thisdiffers from thetraditionalpresentations[Palsberg,1995,Nielson \net al., 1999] thathandle returns outside-in( caller-restore ) where the caller propagates the obtained \nreturn values from the body of the function tothe call site(typicallyformulated as conditional con\u00adstraints). \nCProg. p ::= fnk =>e (CPSprograms) SExp. e ::= t0t1c | ct (seriousCPSexpressions) TExp. t ::= x | v | \nfn x,k =>e (trivialCPSexpressions) CExp. c ::= fnv =>e | k (continuation expressions) Figure8:BNFofCPSlanguage \n  5. Analysis equivalence In previous work [Midtgaard and Jensen, 2008a] we derived an initialCFAwithreachabilityforaCPSlanguagefromthe \nstack-less CE-machine[Flanagan et al.,1993].Inthis section we showthat the present ANF analysis achieves \nthe same precision as obtained by .rst transforming a program into CPS and then using the CPS analysis. \nThis is done by de.ning a relation that captures how the direct-style analysis and theCPS analysis operateinlock-step. \nThe grammar of CPS terms is given in Fig. 8. The grammar distinguishes variables in the original sourceprogram \nx . X,from intermediate variables v . V and continuation variables k . K.We assume the three classes \nare non-overlapping. Their union consti\u00adtute thedomain ofCPS variables Var = X . V . K. 5.1 CPS transformationandback \nagain In order to state the relation between the ANF and CPS analyses we .rst recall the relevant program \ntransformations. The below presentationisbased onDanvy[1991],Flanagan et al.[1993], and Sabry andFelleisen[1994]. \nThe CPS transformation given in Fig. 9(a) is de.ned by two mutually recursive functions one for serious \nand trivial expres\u00adsions.A continuation variable k isprovidedin theinitialcall to F . Afresh k isgenerated \nin V slambda abstraction case. To ease the expression of the relation, we choose k unique to the serious \nex\u00adpression s ks . It follows that we only need one k per lambda abstraction in the original program \n+ an additional k in the initial case. Itisimmediatefromthede.nition of F that the CPS transfor\u00admation \nof a let-binding letx =t ins and the CPS transformation ofitsbody s sharethesamecontinuationidenti.er \nand similarly for non-tail calls.Hence we shall equate the two: ' De.nition5.1. ks = ks ' iff s = s \n D : CProg. P C : P. CProg D[fnk =>e]= U [e] C [p]= fn kp => Fkp [p] U : SExp. C F : K . C . SExp U \n[kt]= P[t] Fk [t]= k V [t] U [(fnv =>e ) t]= let v =P[t] in U [e] Fk [letx=t ins]=(fnx => Fk [s]) V [t] \nU [t0t1k]= P[t0] P[t1] Fk [t0 t1 ]= V [t0 ] V [t1 ] k U [t0t1(fnv =>e)] = let v =P[t0] P[t1] in U [e] \nFk [let x=t0 t1 in s]= V [t0 ] V [t1 ](fnx => Fk [s]) P : TExp. T V : T . TExp P[x]= x V [x]= x P[v]= \nv V [fnx =>s]= fn x,ks => Fks [s] P[fn x ,k =>e]= fnx => U [e] (a)CPStransformation (b)Direct-styletransformation \nFigure9:Transformations to andfromCPS The direct-style transformation given in Fig. 9(b) is de.ned by \ntwo mutually recursive functions over serious and trivial CPS expressions.Wede.nethedirect-styletransformation \nof aprogram fnk =>e as thedirect-style transformation ofitsbody U [e]. Transforming a program, a serious \nexpression, or a trivial ex\u00adpression toCPSandback todirect styleyields the original expres\u00adsion, which \ncanbe con.rmedby(mutual) structuralinduction on trivial and serious expressions. Lemma 5.1. D[C [p]] \n= p . U [Fk [s]] = s . P[V [t]] = t  5.2 CPS analysis We recall the CPS analysis of Midtgaard and Jensen \n[2008a] in Fig. 10. It is de.ned as the least .xed point of a program speci.c transfer function Tp . \n. The de.nition relies on two helper functions \u00b5 . and \u00b5c . fortrivialandcontinuation expressions, respectively.The \nt analysis computes apair consisting of(a) a set of serious expres\u00adsions(the reachable expressions) and(b) \nan abstract environment. Abstract environments map variables to abstract values. Abstract values canbe \neithertheinitial continuation stop,function closures [fn x,k =>e], or continuation closures [fnv =>e]. \nThede.nition relies on two special variables kr and vr, the .rst of which names the initial continuation \nand the second of which namesthe resultoftheprogram.Toensurethe mostprecise analysis result,variablesinthe \nsourceprogram canbe renamedtobedistinct asis traditionalin control-.ow analysis[Nielson et al.,1999]. \n 5.3 Analysis equivalence Beforeformallystatingthe equivalence ofthetwo analyses we will study an example \nrun. As our example we use the ANF program: let f=fnx =>x in leta1 =fcn1 in let a2 =fcn2 in a2 , taken \nfromSabry andFelleisen[1994] where wehaveChurch encoded the integer literals. We write cn1 for fns =>fnz \n=>sz and cn2 forfns =>fnz =>lett1 =sz inst1 .The analysistrace appears in theleftcolumn ofTable1. Similarly \nwe study the CPS analysis of the CPS transformed program.The analysis trace appearsin the right column \nofTable1 where we have written ccn1 for V [cn1] and ccn2 for V [cn2]. Contrary toSabry andFelleisen[1994]both \ntheANF and theCPS analyses achieve the same precision on the example, determining that a1 willbebound \nto one of the twointegerliterals. We are now in position to state our main theorem relating the ANFanalysis \nto theCPS analysis.Intuitively the theorem relates: reachabilityinANF toCPS reachability  abstract \nstacksinANF toCPS continuation closures  abstract stackbottominANF toCPSinitial continuation  ANFclosures \ntoCPSfunction closures  . Theorem 5.1. Let p be given. Let (C, F., E.) = lfpFp and . (Q., R.) = lfpT.Then \nC [p ] s . C .. Fks [s] . Q. . [x, s ' ] . F.(s) .. [fnx => Fks [s ' ]] . R.(ks ) . ' stop . F.(s) .. \nstop . R.(ks ) . [fnx =>s] . E.(y) .. [fn x,ks => Fks [s]] . R.(y) For the purpose of the equivalence \nwe equate the special vari\u00adables xr and vr both naming the result of the computations. We prove the theorem \nby combining an implication in each direction with the identity from Lemma 5.1. We formulate both implication \nas relations andprovethatboth relations arepreservedbythetrans\u00adferfunctions. 5.4 ANF-CPSequivalence \nWe formally de.ne a relation RANF that relates ANF analysis CPS triples toCPS analysispairs. , E.) RANF \n(Q.  De.nition5.2. (C, F., R.) iff .s : CPS s . C =. Fks [s] . Q. . [x, s ' ] . F.(s )=. [fnx => Fks \n[s ' ]] . R.(ks ) . ' stop . F.(s )=. stop . R.(ks ) . [fnx =>s ] . E.(y )=. [fn x ,ks => Fks [s]] . \nR.(y) First we need a small lemma relating the ANF helper function to one of theCPShelperfunctions. \n Lemma5.2. , E.) RANF [fnx =>s] . \u00b5 .(t,E.) .(C, F. CPS (Q., R.) . =. [fn x,ks => Fks [s]] . \u00b5 (V [t],R.) \nt  Env. = Var . P(Val.) (abstractenvironment) . Val. . w ::= stop | [fn x,k =>e] | [fnv =>e ] (abstractvalues) \n(a)Abstract domains . \u00b5 : TExp\u00d7 Env. . P(Val.) t T. : CProg. P(SExp) \u00d7 Env. . P(SExp) \u00d7 Env.. \u00b5 (x ,R.)= \nR.(x ) t . T((Q., R.))= ({e }, [kr .{stop},k .{[fn vr => kr vr]}]) . fnk =>e \u00b5 (v ,R.)= R.(v ) t .. \n.. ({e '}, R. ..[x . \u00b5t (t1,R.),k ' . \u00b5c(c,R.)]) . . \u00b5 (fn x,k =>e ,R.)= {[fn x ,k =>e]} t t0t1c .Q. \n. [fn x ,k ' => e ' ].\u00b5t (t0,R.) . \u00b5 : CExp\u00d7 Env. . P(Val.) . c .. ({e '}, R. ..[v . \u00b5 (t,R.)]) t . \n. \u00b5 (k,R.)= R.(k ) c ct .Q. . . [fnv =>e ' ].\u00b5c (c ,R.) \u00b5c(fnv =>e,R.)= {[fnv =>e]} (b)Abstracttransitionfunction \n(c)Abstracthelperfunctions Figure10:CPS analysis The relationispreservedby the transferfunctions.  Theorem5.2. \n, E.) RANF (C, F., R.) CPS (Q. , E.)) RANF . =. F. ((C, F. ((Q., R.)) p CPS TC [p ] Proof. First we name \nthe individual triples of the union in the function body of F. . We name the .rst triple of results as \nini\u00ad.. tial: (CI, FI, E) = ({p }, [p .{[xr , xr ]},xr .{stop}], . _.0/). I The results of the second, \nthird, fourth, and .fth joined triples corresponding to return, binding, tail call, and non-tail call \nare .. .... named (Cret, Fret), (Cbind, F, E), (Ctc, Ftc) and ret, Ebindbindtc, E .. (Cntc, Fntc, E), \nrespectively. Similarly we name the .rst re\u00ad ntc sult pair in the function body of the CPS analysis as \ninitial: .. (QI, R) = ({e }, [kr .{stop},k .{[fn vr => kr vr]}]). The re- I sults of the second and thirdjoinedpair \ncorresponding to call and .. .. return are named (Q, R) and (Q), respectively. callcallret, Rret Theproofproceedsby \nverifying.verelations: ..) RANF .. (CI, FI, EI, R) (1) ICPS (QI.. ) RANF .. (Cret, F) (2) ret, EretCPS \n(Qret, Rret.. ) RANF .. (Cbind, F, Eret, R) (3) bindbindCPS (Qret.. ) RANF .. , R) (4) (Ctc, Ftc, EtcCPS \n(Qcallcall.. ) RANF .. , R) (5) (Cntc, Fntc, EntcCPS (Qcallcall Realizing that the union of related triples \nand pairs are related we obtain thedesired result. After realizingthatthebottom elements are relatedbythe \nabove relation, it follows by .xed point induction that their least .xed points(andhence the analyses)are \nrelated. . RANF . Corollary 5.1. lfpFp lfpT CPS C [p ] 5.5 CPS-ANFequivalence Again weformallyde.ne a \nrelation now relatingCPSanalysispairs toANF analysis triples. , R.) RCPS  De.nition5.3. (Q. (C, F., \nE.) iff .e : ANF e . Q. =. U [e] . C . [fnx =>e] . R.(ks )=. [x, U [e]] . F.(s) . stop . R.(ks )=. stop \n. F.(s) . [fn x ,ks => e] . R.(y)=. [fnx => U [e]] . E.(y )  We again need ahelperlemma relating thehelperfunctions. \n Lemma5.3. ., R.) RCPS [fn x,ks => e] . \u00b5 (t,R.) .(Q. ANF (C, F., E.) t =. [fnx => U [e]] . \u00b5 .(P[t],E.) \n This relationis alsopreservedby the transferfunctions. Theorem5.3. (Q., R.) RCPS , E.) ANF (C, F. ., \nR.)) RCPS =. T((Q. ((C, F., E.)) p C [p ] ANF F. Proof. The proof follows a similar structure to the \nearlier proof. The bottom elements are related by the relation and it follows by .xedpointinductionthattheirleast \n.xedpoints(andhencethe analyses) are related. . RCPS .  Corollary5.2. lfpTlfpFp C [p ] ANF  6. Extractingconstraints \nThe resultinganalysis may appear complex at.rstglance.However we can express the analysis in the popular \nconstraint formulation, extracted from the obtained de.nition.The formulation shown be\u00adlow is in terms \nof program-speci.c conditional constraints. Con\u00adstraintshave a(possibly empty)list ofpreconditions and \na conclu\u00adsion[Palsberg andSchwartzbach,1995,Gasser et al.,1997]: {u1}. rhs1 . ... .{un}. rhsn . lhs. \nrhs The constraints operate on the same three domains as the above analysis. Left-hand sides lhs can \nbe of the form {u}, F.([s]=), or E.(x), right-hand sides rhs can be of the form C, F.([s ]=), or E.(x),and \nsingleton elements u canbe oftheform s, c, [fnx =>s], or [x, s ].FromFig.7 wedirectly read offthefollowing \nconstraints. .. .. i ANFtrace: (Ci, Fi, E) CPStrace: (Qi, R) i i {let f =fn x => x in let a1=f cn1 in \nlet a2=f cn2 in a2 } {(fnf =>fccn1 (fn a1 => f ccn2 (fn a2 =>kp a2))) (fn x ,kx =>kxx )} [[xr ] ]= .{stop}, \n 0 [] kr .{stop}, [let f =fn x => x in let a1=f cn1 in let a2=f cn2 in a2 ]= .{[xr , xr ]} kp .{[fn \nvr => kr vr]} . _.0/ C0 .{let a1=f cn1 in let a2=f cn2 in a2 } . Q . {f ccn1 (fn a1 => f ccn2 (fn a2 \n=>kp a2))} 0 1 F0 . . [] R ..f .{[fn x ,kx =>kxx ]} 0 . [] E ..f .{[fnx =>x ]} 0 C1 .{x } . Q .{kx \nx } 1 . 1 2 F..[[x ]= .{[a1 , let a2=f cn2 in a2 ]}] [kx .{[fn a1 => f ccn2 (fn a2 =>kp a2)]}] R. .. \n1 . [] x .{ccn1} E ..x .{cn1} 1 C2 .{let a2=f cn2 in a2 } . Q. {f ccn2 (fn a2 =>kp a2)} 3 F2 . 2 . \n[] R ..a1 .{ccn1} 2 . [] E ..a1 .{cn1} 2 C3 . Q3 F ..[x ]= .{[a1 , let a2=f cn2 in a2 ],[a2 , a2 ]} \n 43 . [ ][kx .{[fn a1 => f ccn2 (fn a2 =>kp a2)],[fn a2 =>kp a2]}] . R3 .. . [] x .{ccn1,ccn2} E ..x \n.{cn1,cn2} 3 C4 .{a2 } . Q . {kp a2} 4 . F 54 [] a1 .{ccn1,ccn2} . [] R.. a1 .{cn1,cn2} 4 . a2 .{ccn1,ccn2} \na2 .{cn1,cn2} C5 .{xr } E4 .. . Q .{kr vr} 5 6 F5 . . [] R ..vr .{ccn1,ccn2} 5 E5 ..xr .{cn1,cn2} \n. [] .. .. 7 C6 FE QR 66 66 Table1:Analysis traces of letf =fnx =>x inleta1 =fcn1 in let a2 =fcn2 in \na2 anditsCPS transformed counterpart For theprogram p: {p}. C {[xr , xr ]}. F.([p]=) {stop}. F.([xr ]=) \nFor eachreturn expression t and non-tail call let x=t0 t1 in s ' in p: {s '} . C . {t }. C .{[x, s ' \n]}. F.([t]=) .\u00b5sym(t,E.) . E.(x) For eachlet-binding letx=t ins in p : {s }. C . {letx=t ins }. C .\u00b5sym(t,E.) \n. E.(x ) ' For each tail call t0 t1 andfunction fnx =>s in p : . {s '} . C . {t0 t1 }. C . . . . F.([t0 \nt1 ]=) . F.([s ' ]=) . {[fnx =>s ' ]}. \u00b5sym(t0 ,E.) .(t1 ,E.) . E.(x) .\u00b5sym ' For eachnon-tail call let \nx =t0 t1 in s andfunction fny =>s in p: . {s '} . C . {let x =t0 t1 in s }. C . . . .{[x, s ]}. F.([s \n' ]=) . {[fny =>s ' ]}. \u00b5sym(t0 ,E.) .(t1 ,E.) . E.(y) .\u00b5sym where we partially evaluate the helper \nfunction, i.e., interpret the helperfunction symbolically at constraint-generation time, togen\u00aderate \na lookup for variables, and a singleton for constants and lambda expressions.Thede.nition ofthe symbolichelperfunction \notherwise coincides with the abstracthelperfunction \u00b5. . We maygenerate constraints {[fnx =>s]}.{[fny \n=>s ' ]} of a form not covered by the above grammar. We therefore .rst pre\u00adprocess the constraints in \nlinear time, removing vacuously true inclusions {[fnx =>s]}.{[fnx =>s ]} from each constraint, and removing \nconstraints containing vacuously falsepreconditions . {[fnx =>s]}.{w .}, where [fny =>s ' ] = w . The \nresulting constraint system is formally equivalent to the control .ow analysis in the sense that all \nsolutions yield correct control .ow information and that the best (smallest) solution of the constraints \nis as precise as the information computed by the analysis.Moreformally: Theorem6.1. Asolution to theCFA \nconstraints ofprogram p is a safeapproximationof theleast .xpoint of theanalysisfunction F. inducedbyp \n.Furthermore,theleast solutiontotheCFA constraints isequal totheleast .xpoint of F. . Implemented naively, \na single constraint may take O(n) space alone. However by usingpointers or by labelling each sub-expres\u00adsion \nand using thepointer/labelinstead ofthe sub-expressionitself, a single constraint takes only constant \nspace.Bylinearlydetermin\u00ading a representative for each sub-expression, by generating O(n 2) constraints,linearpost-processing, \nanditeratively solving them us\u00ading a well-known algorithm [Palsberg and Schwartzbach, 1995, Nielson et \nal., 1999], we can compute the analysis in worst-case O(n 3) time. The extractedconstraintsbear similaritiesto \nexisting constraint\u00adbased analyses in the literature. Consider, e.g., calls t0 t1 , which usuallygivesrisetotwoconditional \nconstraints[Palsberg,1995, Nielson et al.,1999]:(1) {[fnx =>s ' ]}. C.(t0 ) . C.(t1 ) . E.(x ) and(2) \n{[fnx =>s ' ]}. C.(t0 ) . C.(s ' ) . C.(t0 t1 ).The .rst con\u00adstraint resemblesourthird constraintfortail \ncalls.Thesecond re\u00adturn constraint differsin thatithas ainside-out(or caller-restore) nature, i.e., propagation \nof return-.ow from the function body is handled at the call site. The extracted reachability constraints \nare similar to Gasser et al. [1997] (modulo an isomorphic encoding P(C) . C . P({on}) ofpowersets). \n7. Conclusion Wehavepresented a control-.ow analysis determining interproce\u00addural control-.ow of both \ncalls and returns for a direct-style lan\u00adguage. Existing CFAs have focused on analysing which functions \nare called at agiven call site.In contrast, the systematicderivation of ourCFAhaslead toan analysisthatprovides \nextrainformation about where afunction returns to at no additional cost.In thepres\u00adence oftail-calloptimization, \nsuchinformation enablesthe creation of moreprecise callgraphs. Theanalysis wasdevelopedsystematically \nusingGaloisconnec\u00adtion-based abstract interpretation of a standard operational seman\u00adtics for that language: \nthe CaEK abstract machine of Flanagan et al. In addition to being more principled, such a formulation \nof theanalysisispedagogicallypleasing sincemonomorphism of the analysis is made explicit through two \nGalois connections: one lit\u00aderally merges all bindings to the same variable and one merges all calling \ncontexts of the samefunction. The analysis has been shown to provide a result equivalent to what can \nbe obtained by .rst CPS transforming the program and then running a control .ow analysis derived from \na CPS-based operational semantics. This extends previous results obtained by Damian and Danvy, and Palsberg \nand Wand. The close correspon\u00addencebetweenthe way thatthe analyses operate(asillustratedby the analysis \ntraceinTable1)leads usto conjecture that such equiv\u00adalence results canbe obtainedfor otherCFAsderived \nusing abstract interpretation. The functional, derived by abstract interpretation, that de.nes the analysis \nmay appear rather complex at .rst glance. As a .nal result, we have shown how to extract from the analysis \nan equiv\u00adalent constraint-based formulation expressed in terms of the more familiar conditionalconstraints.Nevertheless, \nwe stressthatthede\u00adrivedfunctional canbe useddirectlytoimplement the analysis.We havedeveloped aprototypeimplementationof \ntheresulting analy\u00adsisinOCaml.2 2available at http://www.brics.dk/~jmi/ANF-CFA/ The analysis has been \ndeveloped for a minimalistic functional language in order to be able to focus on the abstraction of the \ncontrol structureinducedbyfunction calls and returns.An obvious extension is to enrich the language with \nnumerical operators and study how our Galois connections interact with abstractions such as theinterval \norpolyhedral abstraction of numerical entities. The calculationsinvolvedinthederivation ofaCFA arelengthy \nand wouldbene.t enormouslyfrom someform of machine support. Certi.ed abstract interpretation [Pichardie, \n2005, Cachera et al., 2005]has sofarfocused onproving the correctness of the analysis inside aproof assistantby \nusing the concretization(.)component of the Galois connection to prove the correctness of an already \nde.ned analysis. Further work should investigate whether proof assistants such as Coq are suitable for \nconducting the kind of reasoningdeveloped in thispaperin a machine-checkable way.  Acknowledgments The \nauthors thank Matthew Fluet, Amr Sabry, Mitchell Wand, Daniel Damian, Olivier Danvy, and the anonymous \nreferees for comments on earlier versions. Part of this work was done with the support of theCarlsbergFoundation. \n References J.M.Ashley andR.K.Dybvig. Apractical and .exible .owanalysisfor higher-orderlanguages. ACMTransactions \nonProgrammingLanguages andSystems,20(4):845 868, 1998. A. E. Ayers. Ef.cient closure analysis with reachability. \nIn M. Billaud, P. Cast\u00e9ran, M.-M. Corsini, K. Musumbu, and A. Rauzy, editors, Actes WSA 92Workshop onStatic \nAnalysis,Bigre,pages126 134,Bordeaux, France,Sept.1992.Atelier Irisa,IRISA,CampusdeBeaulieu.  D. Cachera, \nT. Jensen, D. Pichardie, and V. Rusu. Extracting a data .ow analyser in constructive logic. Theoretical \nComputer Science, 342(1): 56 78,2005.  W.D.Clinger. Propertail recursion and spaceef.ciency. InK.D.Cooper, \neditor, Proc. of the ACM SIGPLAN 1998 Conference on Program\u00admingLanguagesDesign andImplementation,pages174 \n185,Montr\u00e9al, Canada,June1998. C. Consel and O. Danvy. For a better support of static data .ow. In \nJ. Hughes, editor, Proc. of the Fifth ACM Conference on Functional Programming and Computer Architecture, \nvolume 523 of LNCS, pages 496 519,Cambridge, Massachusetts, Aug.1991.Springer-Verlag.  P. Cousot. The \ncalculational design of a generic abstract interpreter. In M. Broy and R. Steinbr\u00fcggen, editors, Calculational \nSystem Design. NATOASISeriesF.IOSPress,Amsterdam,1999.  P. Cousot. Semantic foundations of program analysis. \nIn S. S. Muchnick andN.D.Jones, editors, Program Flow Analysis: Theory and Applica\u00adtions, chapter 10,pages303 \n342.Prentice-Hall, 1981. P. Cousot and R. Cousot. Abstract interpretation of algebraic polynomial systems. \nInM.Johnson, editor, Proc. of the Sixth International Confer\u00adence on Algebraic Methodology and Software \nTechnology, AMAST 97, volume 1349 of LNCS, pages 138 154, Sydney, Australia, Dec. 1997. Springer-Verlag. \n P. Cousot and R. Cousot. Higher-order abstract interpretation (and ap\u00adplication to comportment analysis \ngeneralizing strictness, termination, projection and PER analysis of functional languages), invited paper. \nIn  H.Bal, editor,Proc. oftheFifthIEEEInternational Conference onCom\u00adputer Languages,pages95 112,Toulouse,France,May1994. \n P. Cousot and R. Cousot. Abstract interpretation frameworks. Journal of Logic andComputation, 2(4):511 \n547, Aug.1992a. P. Cousot and R. Cousot. Abstract interpretation and application to logic programs. Journal \nofLogicProgramming,13(2 3):103 179, 1992b. P. Cousot and R. Cousot. Systematic design of program analysis \nframe\u00adworks. In B. K. Rosen, editor, Proc. of the Sixth Annual ACM Sym\u00ad posium on Principles of Programming \nLanguages, pages 269 282, San Antonio,Texas,Jan.1979. D.Damian andO.Danvy. Syntactic accidentsinprogram \nanalysis:Onthe impact oftheCPStransformation. Journal ofFunctionalProgramming, 13(5):867 904,2003. Apreliminary \nversion waspresented atthe2000 ACMSIGPLANInternationalConference onFunctionalProgramming. O.Danvy. ThreestepsfortheCPS \ntransformation. TechnicalReportCIS\u00ad92-2,KansasStateUniversity,Manhattan, Kansas,Dec.1991. B. A. Davey \nand H. A. Priestley. Introduction to Lattices and Order. CambridgeUniversityPress,Cambridge,England, \nsecondedition,2002.  S.K.Debray andT.A.Proebsting.Interprocedural control .owanalysisof .rst-order programs \nwith tail-call optimization. ACM Transactions on Programming Languages and Systems,19(4):568 585, 1997. \n C. Flanagan, A. Sabry, B. F. Duba, and M. Felleisen. The essence of compiling with continuations. In \nD. W. Wall, editor, Proc. of the ACM SIGPLAN 1993 Conference on Programming Languages Design and Implementation,pages237 \n247,Albuquerque,NewMexico,June1993. K. L. S. Gasser, F. Nielson, and H. R. Nielson. Systematic realisation \nof control .owanalysesforCML. InM.Tofte,editor, Proc. of the Second ACM SIGPLAN International Conference \non Functional Programming, pages38 51,Amsterdam,TheNetherlands, June1997.  R.Giacobazzi, F.Ranzato, \nandF.Scozzari. Makingabstractinterpretations complete. J.ACM,47(2):361 416, 2000. N.Heintze. Set-basedprogram \nanalysis ofMLprograms. InC.L.Talcott, editor, Proc. of the 1994 ACM Conference on Lisp and Functional \nProgramming,LISPPointers,Vol.VII,No.3,pages 306 317,Orlando, Florida,June1994. N.D.Jones.Flowanalysisoflambdaexpressions(preliminary \nversion).In S.Even andO.Kariv, editors, Automata, Languages and Programming, 8th Colloquium, Acre (Akko), \nvolume 115 of LNCS, pages 114 128, Israel,July1981.Springer-Verlag. J. Midtgaard. Control-.ow analysis \nof functional programs. Technical Report BRICS RS-07-18, Dept. of Comp. Sci., University of Aarhus, Aarhus, \nDenmark, Dec. 2007. Accepted for publication in ACM Com\u00adputing Surveys. J. Midtgaard and T. Jensen. \nA calculational approach to control-.ow analysisbyabstractinterpretation. InM.Alpuente andG.Vidal, editors, \nStatic Analysis, 15th International Symposium, SAS 2008, volume5079 of LNCS,pages347 362,Valencia, Spain,July2008a.Springer-Verlag. \n J. Midtgaard and T. P. Jensen. Control-.ow analysis of function calls and returns by abstract interpretation. \nRapport de Recherche RR-6681, INRIARennes BretagneAtlantique,Oct.2008b.  M.Might andO.Shivers. Environmental \nanalysis via .CFA. InS.Peyton Jones, editor,Proc. ofthe33rdAnnualACMSymposium onPrinciples of Programming \nLanguages, pages 127 140, Charleston, South Carolina, Jan.2006. R.Milner andM.Tofte. Co-inductionin relational \nsemantics. Theoretical Computer Science, 87(1):209 220, 1991. F.NielsonandH.R.Nielson. In.nitary control \n.owanalysis:acollecting semanticsfor closure analysis. InN.D.Jones, editor, Proc. of the 24th Annual \nACM Symposium on Principles of Programming Languages, pages332 345,Paris,France,Jan.1997. F.Nielson,H.R.Nielson, \nandC.Hankin. Principles ofProgramAnalysis. Springer-Verlag, 1999. H.R.NielsonandF.Nielson.Flowlogic:amulti-paradigmatic \napproach to static analysis.InT.\u00c6.Mogensen,D.A.Schmidt, andI.H.Sudborough, editors, TheEssence ofComputation:Complexity,Analysis,Transforma\u00adtion. \nEssays Dedicated to Neil D. Jones, volume 2566 of LNCS, pages 223 244.Springer-Verlag, 2002. J. Palsberg. \nClosure analysis in constraint form. ACM Transactions on Programming Languages and Systems,17(1):47 62, \n1995.  J.Palsberg andM.I.Schwartzbach. Safety analysis versustypeinference. Information andComputation, \n118(1):128 141, 1995. J.PalsbergandM.Wand.CPStransformationof.owinformation. Journal ofFunctional Programming,13(5):905 \n923, 2003. D. Pichardie. Interpr\u00e9tation abstraite en logique intuitioniste: extraction d analyseurs Java \ncerti.\u00e9s. PhD thesis, Universit\u00e9 de Rennes 1, Sept. 2005. A. Sabry and M. Felleisen. Is continuation-passing \nuseful for data .ow analysis? In V. Sarkar, editor, Proc. of the ACM SIGPLAN 1994 Con\u00adference onProgramming \nLanguagesDesign andImplementation, pages 1 12,Orlando,Florida,June1994. P.Sestoft. Replacing functionparametersbyglobal \nvariables. InJ.E.Stoy, editor, Proc. of the Fourth International Conference on Functional Pro\u00adgramming \nand Computer Architecture, pages 39 53, London, England, Sept.1989. O. Shivers. Control-.ow analysis \nin Scheme. In M. D. Schwartz, editor,  Proc. of the ACM SIGPLAN 1988 Conference on Programming Lan\u00adguages \nDesign and Implementation, pages 164 174, Atlanta, Georgia, June1988. F.SpotoandT.P.Jensen.Classanalysesasabstractinterpretations \noftrace semantics. ACMTransactions onProgrammingLanguages andSystems, 25(5):578 630, 2003. A. Underlyingmathematical \nmaterial This sectionisbasedonknown material[Cousot andCousot,1979, Cousot,1981,Cousot andCousot, 1992b, \n1994, Davey andPriest\u00adley,2002]. A complete lattice is a partially ordered set (C;.,.,.,.,.) (poset), \nsuch that the least upper bound .S and the greatest lower bound .S exists for every subset S of C. . \n= .C de\u00adnotes the in.mum of C and . = .C denotes the supremum of C. The set of total functions D . C, \nwhose domain is a complete lattice (C;.,.,.,.,.), is itself a complete lattice (D. C;..,..,.., ..) under \nthe pointwise ordering f .' .. ., .. f .x. f(x) . f ' (x), withbottom,top,join, and meet extended simi\u00adlarly. \nThe powersetsP(S) of a set S ordered by set inclusion is a completelattice (P(S);.,0/,S,.,n). A Galois \nconnection is a pair of functions a, . between two posets (C;.) and (A;=) such that for all a . A,c . \nC : a(c) = a .. c . .(a).Equivalently a Galois connection can be de.ned as a pair of functions satisfying \n(a) a and . are monotone, (b) a . . is reductive, and(c) . . a is extensive. Galois connections . .-- \n are typeset (C;.) --.(A;=). We omit the orderings when they a are clear from the context. For a Galois \nconnection between two complete lattices a is a complete join-morphism (CJM) and . is a complete meet \nmorphism. The composition of two Galois .1 .2 -- --  connections (C;.) .- -.(B;.) and (B;.) .- -.(A;=) \nis itself a1 a2 .1..2 a Galois connection (C;.) .---- (A;=). Galois connections in - ---. a2.a1 which \na is surjective (or equivalently . is injective) are typeset . .---  (C;.) ---. (A;=). Galois connections \nin which . is surjective a . (or equivalently a is injective) are typeset (C;.) . --- (A;=). - --. \na Whenbotha and. are surjective,thetwodomains areisomorphic. A(n upper) closure operator . is map . : \nS . S on a poset '' (S;.),thatis(a)monotone:(for all s,s . S : s . s =. .(s) . .(s ' )), (b) extensive \n(for all s . S : s . .(s)), and (c) idempo\u00adtent, (for all s . S : .(s)= .(.(s))). A closure operator \n. in\u00ad 1 .-- duces a Galois connection (S;.) --.(.(S);.), writing .(S) for . {.(s) | s . S} and 1 for \nthe identity function. Furthermore the im\u00adage ofa completelattice (C;.,.,.,.,.) byan upper closure op\u00aderatorisitselfa \ncompletelattice (.(C);.,.(.),.,. X..(.X),.).  \n\t\t\t", "proc_id": "1596550", "abstract": "<p>We derive a control-flow analysis that approximates the interprocedural control-flow of both function calls and returns in the presence of first-class functions and tail-call optimization. In addition to an abstract environment, our analysis computes for each expression an abstract control stack, effectively approximating where function calls return across optimized tail calls. The analysis is systematically calculated by abstract interpretation of the stack-based CaEK abstract machine of Flanagan et al. using a series of Galois connections. Abstract interpretation provides a unifying setting in which we 1) prove the analysis equivalent to the composition of a continuation-passing style (CPS) transformation followed by an abstract interpretation of a stack-less CPS machine, and 2) extract an equivalent constraint-based formulation, thereby providing a rational reconstruction of a constraint-based control-flow analysis from abstract interpretation principles.</p>", "authors": [{"name": "Jan Midtgaard", "author_profile_id": "81100381173", "affiliation": "Roskilde University, Roskilde, Denmark", "person_id": "P1613953", "email_address": "", "orcid_id": ""}, {"name": "Thomas P. Jensen", "author_profile_id": "81100640174", "affiliation": "CNRS, Rennes, France", "person_id": "P1613954", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1596550.1596592", "year": "2009", "article_id": "1596592", "conference": "ICFP", "title": "Control-flow analysis of function calls and returns by abstract interpretation", "url": "http://dl.acm.org/citation.cfm?id=1596592"}