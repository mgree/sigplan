{"article_publication_date": "08-31-2009", "fulltext": "\n Purely Functional Lazy Non-deterministic Programming Sebastian Fischer Oleg Kiselyov Chung-chieh Shan \nChristian-Albrechts University, Germany FNMOC, CA, USA Rutgers University, NJ, USA sebf@informatik.uni-kiel.de \noleg@pobox.com ccshan@cs.rutgers.edu Abstract Functional logic programming and probabilistic programming \nhave demonstrated the broad bene.ts of combining laziness (non-strict evaluation with sharing of the \nresults) with non-determinism. Yet these bene.ts are seldom enjoyed in functional programming, be\u00adcause \nthe existing features for non-strictness, sharing, and non\u00addeterminism in functional languages are tricky \nto combine. We present a practical way to write purely functional lazy non-deterministic programs that \nare ef.cient and perspicuous. We achieve this goal by embedding the programs into existing lan\u00adguages \n(such as Haskell, SML, and OCaml) with high-quality im\u00adplementations, by making choices lazily and representing \ndata with non-deterministic components, by working with custom monadic data types and search strategies, \nand by providing equational laws for the programmer to reason about their code. Categories and Subject \nDescriptors D.1.1 [Programming Tech\u00adniques]: Applicative (Functional) Programming; D.1.6 [Program\u00adming \nTechniques]: Logic Programming; F.3.3 [Logics and Mean\u00adings of Programs]: Studies of Program Constructs \nType structure General Terms Design, Languages Keywords Monads, side effects, continuations, call-time \nchoice 1. Introduction Non-strict evaluation, sharing, and non-determinism are all valu\u00adable features \nin functional programming. Non-strict evaluation lets us express in.nite data structures and their operations \nin a modular way (Hughes 1989). Sharing lets us represent graphs with cycles, such as circuits (surveyed \nby Acosta-G\u00b4omez 2007), and express memoization (Michie 1968), which underlies dynamic program\u00adming. \nSince Rabin and Scott s Turing-award paper (1959), non\u00addeterminism has been applied to model checking, \ntesting (Claessen and Hughes 2000), probabilistic inference, and search. These features are each available \nin mainstream functional languages. A call-by-value language can typically model non\u00adstrict evaluation \nwith thunks and observe sharing using reference cells, physical identity comparison, or a generative \nfeature such as Scheme s gensym or SML s exceptions. Non-determinism can be achieved using amb (McCarthy \n1963), threads, or .rst-class con\u00adtinuations (Felleisen 1985; Haynes 1987). In a non-strict language \nlike Haskell, non-determinism can be expressed using a list monad Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. ICFP 09, August 31 September 2, 2009, Edinburgh, \nScotland, UK. Copyright c &#38;#169; 2009 ACM 978-1-60558-332-7/09/08. . . $5.00 (Wadler 1985) or another \nMonadPlus instance, and sharing can be represented using a state monad (Acosta-G\u00b4 omez 2007; \u00a72.4.1). \nThese features are particularly useful together. For instance, sharing the results of non-strict evaluation \nknown as call-by-need or lazy evaluation ensures that each expression is evaluated at most once. This \ncombination is so useful that it is often built-in: as delay in Scheme, lazy in OCaml, and memoization \nin Haskell. In fact, many programs need all three features. As we illustrate in \u00a72, lazy functional logic \nprogramming (FLP) can be used to express search problems in the more intuitive generate-and-test style \nyet solve them using the more ef.cient test-and-generate strategy, which is to generate candidate solutions \nonly to the extent demanded by the test predicate. This pattern applies to property\u00adbased test-case generation \n(Christiansen and Fischer 2008; Fischer and Kuchen 2007; Runciman et al. 2008) as well as probabilistic \ninference (Goodman et al. 2008; Koller et al. 1997). Given the appeal of these applications, it is unfortunate \nthat combining the three features naively leads to unexpected and un\u00addesired results, even crashes. For \nexample, lazy in OCaml is not thread-safe (Nicollet et al. 2009), and its behavior is unspeci.ed if the \ndelayed computation raises an exception, let alone backtracks. Although sharing and non-determinism can \nbe combined in Haskell by building a state monad that is a MonadPlus instance (Hinze 2000; Kiselyov et \nal. 2005), the usual monadic encoding of non\u00addeterminism in Haskell loses non-strictness (see \u00a72.2). \nThe triple combination has also been challenging for theoreticians and prac\u00adtitioners of FLP (L\u00b4opez-Fraguas \net al. 2007, 2008). After all, Algol has made us wary of combining non-strictness with any effect. The \nFLP community has developed a sound combination of laziness and non-determinism, call-time choice, embodied \nin the Curry language. Roughly, call-time choice makes lazy non-deter\u00administic programs predictable and \ncomprehensible because their declarative meanings can be described in terms of (and is often same as) \nthe meanings of eager non-deterministic programs. 1.1 Contributions We embed lazy non-determinism with \ncall-time choice into main\u00adstream functional languages in a shallow way (Hudak 1996), rather than, say, \nbuilding a Curry interpreter in Haskell (Tolmach and Antoy 2003). This new approach is especially practical \nbecause these languages already have mature implementations, because functional programmers are already \nknowledgeable about lazi\u00adness, and because different search strategies can be speci.ed as MonadPlus instances \nand plugged into our monad transformer. Furthermore, we provide equational laws that programmers can \nuse to reason about their code, in contrast to previous accounts of call-time choice based on directed, \nnon-deterministic rewriting. The key novelty of our work is that non-strictness, sharing, and non-determinism \nhave not been combined in such a general way before in purely functional programming. Non-strictness \nand non-determinism can be combined using data types with non\u00addeterministic components, such that a top-level \nconstructor can be computed without .xing its arguments. However, such an encod\u00ading defeats Haskell s \nbuilt-in sharing mechanism, because a piece of non-deterministic data that is bound to a variable that \noccurs multiple times may evaluate to a different (deterministic) value at each occurrence. We retain \nsharing by annotating programs explic\u00aditly with a monadic combinator for sharing. We provide a generic \nlibrary to de.ne non-deterministic data structures that can be used in non-strict, non-deterministic \ncomputations with explicit sharing.  Our library is implemented as a monad transformer and can, hence, \nbe combined with arbitrary monads for non-determinism. We are, thus, not restricted to the list monad \n(which implements depth-.rst search) but can also use monads that backtrack more ef.ciently or provide \na complete search strategy. The library does not directly support logic variables perhaps the most conspicu\u00adous \nfeature of FLP and the associated solution techniques of nar\u00adrowing and residuation, but logic variables \ncan be emulated using non-deterministic generators (Antoy and Hanus 2006) or managed using an underlying \nmonad of equality and other constraints. We present our concrete code in Haskell, but we have also im\u00adplemented \nour approach in OCaml. Our monadic computations per\u00adform competitively against corresponding computations \nin Curry that use non-determinism, narrowing, and uni.cation.  1.2 Structure of the paper In \u00a72 we describe \nnon-strictness, sharing, and non-determinism and why they are useful together. We also show that their \nnaive combi\u00adnation is problematic, to motivate the explicit sharing of non-deter\u00administic computations. \nIn \u00a73 we clarify the intuitions of sharing and introduce equational laws to reason about lazy non-determinism. \nSection 4 develops an easy-to-understand implementation in sev\u00aderal steps. Section 5 generalizes and \nspeeds up the simple imple\u00admentation. We review the related work in \u00a76 and then conclude. 2. Non-strictness, \nsharing, and non-determinism In this section, we describe non-strictness, sharing, and non-deter\u00adminism \nand explain why combining them is useful and non-trivial. 2.1 Lazy evaluation Lazy evaluation is illustrated \nby the following Haskell predicate, which checks whether a given list of numbers is sorted: isSorted \n:: [Int] -> Bool isSorted (x:y:zs) = (x <= y) &#38;&#38; isSorted (y:zs) isSorted _ = True In a non-strict \nlanguage such as Haskell, the arguments to a func\u00adtion are only evaluated as much as demanded by the \nde.nition of the function. The predicate isSorted only demands the complete input list if it is sorted. \nIf the list is not sorted, then it is only de\u00admanded up to the .rst two elements that are out of order. \nAs a consequence, we can apply isSorted to in.nite lists and it will yield False if the given list is \nunsorted. Consider the following function that produces an in.nite list: iterate :: (a -> a) -> a -> \n[a] iterate next x = x : iterate next (next x) The test isSorted (iterate ( div 2) n) yields the result \nFalse if n>0. It does not terminate if n<=0 because an in.nite list cannot be identi.ed as being sorted \nwithout considering each of its elements. In this sense, isSorted is not total (Escard\u00b4 o 2007). A lazy \nevaluation strategy is not only non-strict. Additionally, it evaluates each expression bound to a variable \nat most once even if the variable occurs more than once. For example, the variable x occurs twice in \nthe right-hand side of the de.nition of iterate. Although iterate never evaluates its argument x, the \nduplication of a computation bound to x does not cause it to be evaluated twice. In a lazy language, \nthe value of the expression factorial 100 would only be computed once when evaluating iterate ( div 2) \n(factorial 100). This property called sharing makes lazy evaluation strictly more ef.cient than eager \nevaluation, at least on some problems (Bird et al. 1997).  2.2 Non-determinism Programming non-deterministically \ncan simplify the declarative formulation of an algorithm. For example, many languages are eas\u00adier to \ndescribe using non-deterministic rather than deterministic au\u00adtomata. As logic programming languages \nsuch as Prolog and Curry have shown, the expressive power of non-determinism simpli.es programs because \ndifferent non-deterministic results can be viewed individually rather than as members of a (multi-)set \nof possible re\u00adsults (Antoy and Hanus 2002). In Haskell, we can express non-deterministic computations \nus\u00ading lists (Wadler 1985) or, more generally, monads that are in\u00adstances of the type class MonadPlus. \nA monad m is a type con\u00adstructor that provides two polymorphic operations: return::a->m a (>>=) ::ma->(a->mb)->mb \nThe operation return builds a deterministic computation that yields a value of type a, and the operation \n>>= ( bind ) chains computations together. Haskell s do-notation is syntactic sugar for long chains of \n>>=. For example, the expression dox<-e1 e2 desugars into e1>>= \\x->e2. If a monad m is an instance of \nMonadPlus, then two additional operations are available: mzero :: m a mplus::ma ->ma->ma Here, mzero \nis the primitive failing computation, and mplus chooses non-deterministically between two computations. \nFor the list monad, return builds a singleton list, mzero is an empty list, and mplus is list concatenation. \nAs an example, the following monadic operation computes all permutations of a given list non-deterministically: \nperm :: MonadPlus m => [a] -> m [a] perm [] = return [] perm (x:xs) = do ys <-perm xs zs <-insert x \nys return zs insert :: MonadPlus m => a -> [a] -> m [a] insert x xs = return (x:xs) mplus case xs of \n[] -> mzero (y:ys) -> do zs <-insert x ys return (y:zs) The operation perm permutes a list by recursively \ninserting ele\u00adments at arbitrary positions. To insert an element x at an arbitrary position in xs, the \noperation insert either puts x in front of xs or recursively inserts x somewhere in the tail of xs if \nxs is not empty. Non-determinism is especially useful when formulating search algorithms. Following the \ngenerate-and-test pattern, we can .nd solutions to a search problem by non-deterministically describing \ncandidate solutions and using a separate predicate to .lter results. It is much easier for a programmer \nto express generation and testing separately than to write an ef.cient search procedure by hand, because \nthe generator can follow the structure of the data (to achieve completeness easily) and the test can \noperate on fully determined candidate solutions (to achieve soundness easily).  We demonstrate this \ntechnique with a toy example, permutation sort, which motivates our combination of non-strictness, sharing, \nand non-determinism. Below is a simple declarative speci.cation of sorting. In words, to sort a list \nis to compute a permutation of the list that is sorted. The convenience function guard is semi-deter\u00administic: \nit yields () if its argument is True and fails otherwise. sort :: MonadPlus m => [Int] -> m [Int] sortxs=doys \n<-permxs guard (isSorted ys) return ys Unfortunately, this program is grossly inef.cient, because it \niterates through every permutation of the list. It takes about a second to sort 10 elements, more than \n10 seconds to sort 11 elements, and more than 3 minutes to sort 12 elements. The inef.ciency is mostly \nbe\u00adcause we do not use the non-strictness of the predicate isSorted. Although isSorted rejects a permutation \nas soon as it sees two el\u00adements out of order, sort generates a complete permutation before passing it \nto isSorted. Even if the .rst two elements of a permu\u00adtation are already out of order, exponentially \nmany permutations of the remaining elements are computed. In short, the usual, naive monadic encoding \nof non-determinism in Haskell loses non-strictness.  2.3 Retaining non-strictness by sacri.cing sharing \nThe problem with the naive monadic encoding of non-determinism is that the arguments to a constructor \nmust be deterministic. If these arguments are themselves results of non-deterministic computa\u00adtions, \nthese computations must be performed completely before we can apply the constructor to build a non-deterministic \nresult. To overcome this limitation, we can rede.ne all data structures such that their components may \nbe non-deterministic. A data type for lists with non-deterministic components is as follows: dataListm \na= Nil |Cons(ma)(m(List ma)) We de.ne operations to construct such lists conveniently: nil :: Monad m \n=> m (List m a) nil = return Nil cons :: Monad m => ma -> m(Listma) -> m(Listma) cons x y = return (Cons \nx y) We rede.ne the non-strict isSorted to test non-deterministic lists: isSorted :: MonadPlus m => m \n(List m a) -> m Bool isSorted ml = ml >>= \\l -> case l of Cons mx mxs -> mxs >>= \\xs -> case xs of Consmymys-> \nmx>>=\\x->my>>=\\y-> if x <= y then isSorted (cons (return y) mys) else return False _ -> return True _ \n-> return True By generating lists with non-deterministic arguments, we can de\u00ad.ne a lazier version of \nthe permutation algorithm. perm :: MonadPlus m => m (List m a) -> m (List m a) permml=ml>>=\\l -> case \nl of Nil -> nil Cons mx mxs -> insert mx (perm mxs) Note that we no longer evaluate (bind) the recursive \ncall of perm in order to pass the result to the operation insert, because insert now takes a non-deterministic \nlist as its second argument. insert :: MonadPlus m =>m a-> m(Listma) ->m (Listm a) insert mx mxs = cons \nmx mxs mplus do Cons my mys <-mxs cons my (insert mx mys) The operation insert either creates a new list \nwith the non\u00addeterministic mx in front of the non-deterministic mxs or it inserts mx somewhere in the \ntail of mxs. Here, the pattern match in the do-expression binding is non-exhaustive. If the computation \nmxs returns Nil, the pattern-match failure is a failing computation. Now, we can de.ne a permutation-sort \nalgorithm that lazily checks whether generated permutations are sorted: sort :: MonadPlus m => m (List \nm Int) -> m (List m Int) sortxs =letys =permxsin do True <-isSorted ys ys Unfortunately, this version \nof the algorithm does not sort. It yields every permutation of its input, not only the sorted permutations. \nThis is because the shared variable ys in the new de.nition of sort is bound to the non-deterministic \ncomputation yielding a permuta\u00adtion of the input rather than to the result of this computation. Con\u00adsequently, \nisSorted checks whether there is a sorted permutation and, if so, sort yields an arbitrary permutation. \nIn short, the presence of non-deterministic components in data structures con.icts with the intuition \nthat shared variables such as ys denote values, fully determined if not yet fully computed. In order \nfor sort to work, the shared non-deterministic computation ys, used twice in sort, must yield the same \nresult each time.  2.4 Explicit sharing Our new approach to non-determinism is lazy in that it preserves \nboth non-strictness and sharing. We provide a combinator share for explicit sharing, which can be used \nto introduce variables for non-deterministic computations that represent values rather than computations. \nThe combinator share has the signature1 share::ma ->m(m a) where m is an instance of MonadPlus that supports \nexplicit sharing. (We describe the implementation of explicit sharing in \u00a7\u00a74 5.) The function sort can \nthen be rede.ned to actually sort: sort xs = do ys <-share (perm xs) True <-isSorted ys ys In this version \nof sort, the variable ys denotes the same permuta\u00adtion wherever it occurs but is nevertheless only computed \nas much as demanded by the predicate isSorted. 3. Programming with lazy non-determinism In this section \nwe formalize the share combinator and specify equational laws with which a programmer can reason about \nnon\u00addeterministic programs with share and predict their observations. Before the laws, we .rst present \na series of small examples to clarify how to use share and what share does. 3.1 The intuition of sharing \nWe de.ne two simple programs. The computation coin .ips a coin and non-deterministically returns either \n0 or 1. coin :: MonadPlus m => m Int coin = return 0 mplus return 1 1 In fact, the signature has additional \nclass constraints; see \u00a75.  The function duplicate evaluates a given computation a twice. duplicate \n:: Monad m => m a -> m (a, a) duplicate a = do u <-a v <-a return (u,v) 3.1.1 Sharing enforces call-time \nchoice We contrast three ways to bind x: dup_coin_let = let x = coin in duplicate x dup_coin_bind = do \nx <-coin duplicate (return x) dup_coin_share = do x <-share coin duplicate x The programs dup_coin_let \nand dup_coin_bind do not use share, so we can understand their results by treating m as any non-determinism \nmonad, such as the set monad. The program dup_coin_let binds the variable x to the non-deterministic \ncomputation coin. The function duplicate executes x twice performing two independent coin .ips so dup_coin_let \nyields four answers, namely (0,0), (0,1), (1,0), and (1,1). In con\u00adtrast, dup_coin_bind binds x, of type \nInt, to share not the coin computation but its result. The function duplicate receives a de\u00adterministic \ncomputation return x, whose two evaluations yield the same result, so dup_coin_bind yields only (0,0) \nand (1,1). The shared computation x in dup_coin_share behaves like return x in dup_coin_bind: both arguments \nto duplicate are deterministic computations, which yield the same results even when evaluated multiple \ntimes. As in \u00a7\u00a72.3 2.4, we wish to share the results of computations, and we wish variables to denote \nvalues. In dup_coin_bind, x has the type Int and indeed represents an in\u00adteger. In dup_coin_share, x \nhas the type m Int, yet it represents one integer rather than a set of integers. Thus dup_coin_share \nyields the same two results as dup_coin_bind. 3.1.2 Sharing preserves non-strictness Shared computations, \nlike the lazy evaluation of pure Haskell ex\u00adpressions, take place only when their results are needed. \nIn particu\u00adlar, if the program can .nish without a result from a shared compu\u00adtation, then that computation \nnever happens. The sorting example in \u00a72 shows how non-strictness can improve performance dramati\u00adcally. \nHere, we illustrate non-strictness with two shorter examples: strict_bind = do x <-undefined :: m Int \nduplicate (const (return 2) (return x)) lazy_share = do x <-share (undefined :: m Int) duplicate (const \n(return 2) x) The evaluation of strict_bind diverges, whereas lazy_share yields (2,2). Of course, real \nprograms do not contain undefined or other intentionally divergent computations. We use undefined above \nto stand for an expensive search whose results are unused. Alternatively, undefined above may stand for \nan expensive search that in the end fails to .nd any solution. If the rest of the program does not need \nany result from the search, then the shared search is not executed at all. Thus, if we replace undefined \nwith mzero in the examples above, strict_bind would fail, whereas lazy_share would yield (2,2) as before. \n 3.1.3 Sharing recurs on non-deterministic components We turn to data types that contain non-deterministic \ncomputations, such as Listm a introduced in \u00a72.3. We de.ne two functions for illustration: the function \nfirst takes the .rst element of a List; the function dupl builds a List with the same two elements. first \n:: MonadPlus m => m (List m a) -> m a firstl =l>>=\\(Consxxs)->x dupl:: Monadm=>m a-> m(Listma) dupl x \n= cons x (cons x nil) The function dupl is subtly different from duplicate: whereas duplicate runs a \ncomputation twice and returns a data structure with the results, dupl returns a data structure containing \nthe same computation twice without running it. The following two examples illustrate the bene.t of data \nstruc\u00adtures with non-deterministic components. heads_bind = do x <-cons coin undefined dupl (first (return \nx)) heads_share = do x <-share (cons coin undefined) dupl (first x) Despite the presence of undefined, \nthe evaluation of both exam\u00adples terminates and yields de.ned results. Since only the head of the list \nx is needed, the unde.ned tail of the list is not evaluated. The expression cons coin undefined above \ndenotes a de\u00adterministic computation that returns a data structure contain\u00ading a non-deterministic computation \ncoin. The monadic bind in heads_bind shares this data structure, coin and all, but not the result of \ncoin. The monad laws entail that heads_bind yields cons coin (cons coin nil). When we later execute the \nla\u00adtent computations (to print the result, for example), the two copies of coin will run independently \nand yield four outcomes [0,0], [0,1], [1,0], [1,1], so heads_bind is like dup_coin_let above. Informally, \nmonadic bind performs only shallow sharing, which is not enough for data with non-deterministic components. \nOur share combinator performs deep sharing: all components of a shared data structure are shared as well.2 \nFor example, the variable x in heads_share stands for a fully determined list with no latent non-determinism. \nThus, heads_share yields only two outcomes, [0,0] and [1,1]. 3.1.4 Sharing applies to unbounded data \nstructures Our .nal example involves a list of non-deterministic, unbounded length, whose elements are \neach also non-deterministic. The set of possible lists is in.nite, yet non-strictness lets us compute \nwith it. coins :: MonadPlus m => m (List m Int) coins = nil mplus cons coin coins dup_first_coin = do \ncs <-share coins dupl (first cs) The non-deterministic computation coins yields every .nite list of zeroes \nand ones. Unlike the examples above using undefined, each possible list is fully de.ned and .nite, but \nthere are an in.nite number of possible lists, and generating each list requires an un\u00adbounded number \nof choices. Even though, as discussed above, the shared variable cs represents the fully determined result \nof such an unbounded number of choices, computing dup_first_coin only makes the few choices demanded \nby dupl (first cs). In partic\u00adular, first cs represents the .rst element and is demanded twice, each \ntime giving the same result, but no other element is demanded. Thus, dup_first_coin produces two results, \n[0,0] and [1,1].  3.2 The laws of sharing We now formalize the intuitions illustrated above in a set \nof equa\u00adtional laws that hold up to observation as detailed in \u00a73.3. We show 2 Applying share to a function \ndoes not cause any non-determinism in its body to be shared. This behavior matches the intuition that \ninvoking a function creates a copy of its body by substitution.  here how to use the laws to reason \nabout in particular, predict the results of non-deterministic computations with share, such as the examples \nabove. In \u00a74, we further use the laws to guide an implementation. The laws of our monad are shown in \nFigure 1. We write ret for return, /0 for mzero, . for mplus , and . for undefined. First of all, our \nmonad satis.es the monad laws, (Lret), (Rret) and (Bassc). Our monad is also a MonadPlus instance, but \nthe laws for MonadPlus are not agreed upon (MonadPlus 2008); we include two commonly accepted laws, (Ldistr) \nand (Lzero). We do not however require that . be associative or that /0 be a left or right unit of ., \nso that our monad can be a weighted non-determinism monad, for which . means averaging weights. Weighted \nnon-determinism is useful for probabilistic inference. Using the laws in Figure 1, we can reduce a computation \nexpres\u00adsion in our monad to an expression like (0/.\u00b7\u00b7\u00b7) . (ret v1 .\u00b7\u00b7\u00b7),a (potentially in.nite) tree \nwhose branches are . and whose leaves are ., /0, or ret v. To observe the computation, we apply a func\u00adtion \nrun to convert it to another MonadPlus instance such as the set monad. Figure 2 gives the laws of run. \nThe right-hand sides use primes (ret', \u00bb=', /0', .') to refer to operations of the target monad. Using \nthe (Lret) and (Ldistr) laws, we can compute the result of the example dup_coin_bind above, which does \nnot use share. (ret 0 . ret 1) \u00bb= . x.ret x \u00bb= . u.ret x \u00bb= . v.ret (u,v) =(ret 0 . ret 1) \u00bb= . x. ret \n(x,x) =(ret 0 \u00bb= . x.ret (x,x)) . (ret 1 \u00bb= . x.ret (x,x)) = ret (0,0) . ret (1,1) To show how the laws \nenforce call-time choice, we derive the same result for dup_coin_share, which is share (ret 0 . ret 1) \n\u00bb= . x.x \u00bb= . u.x \u00bb= . v.ret (u,v). We .rst use the (Choice) law to reduce share (ret 0 . ret 1) to share \n(ret 0) . share (ret 1). To proceed further, we reduce share (ret 0) to ret (ret 0) (and share (ret 1) \nto ret (ret 1)) using the (HNF) law (HNF is short for head normal form ). In the law, c stands for a \nconstructor with n non-deterministic components. Since 0 has no non-deterministic components, n = 0 and \nwe have ret (ret 0) \u00bb= . x. x \u00bb= . u.x \u00bb= . v.ret (u, v) = ret 0 \u00bb= . u.ret 0 \u00bb= . v.ret (u,v)= ret (0,0) \nThe overall result is thus the same as that for dup_coin_bind. The preservation of non-strictness is \nillustrated by lazy_share. After reducing const (ret 2) x there to ret 2, we obtain share .\u00bb= . x.duplicate \n(ret 2). Because x is unused, the result can be computed without evaluating the shared expression. And \nit is, as assured by the (Bot) law, which reduces share. to ret. (not to .). The (Lret) law then reduces \nthe expression to duplicate (ret 2), and the .nal result is ret (2,2). The next section discusses (Bot) \nfurther. The (Fail) law works similarly. We turn to data structures with non-deterministic components. \nUsing the monad laws, the heads_bind example easily reduces to ret (Cons coin (ret (Cons coin (ret Nil)))), \nin which the construc\u00adtor Cons takes two non-deterministic computations as arguments. Whereas applying \nrun to observe results without non-deterministic components is a trivial matter of replacing /0 by /0', \n. by .', and ret by ret' (so trivial as to be glossed over above), observing the result of heads_bind \nrequires using the (rRet) law in Figure 2 in a non-trivial way, with c being Cons and n being 2. The \nresult is run coin \u00bb=' . y1. run (ret (Cons coin (ret Nil))) \u00bb=' . y2. ret' (Cons (ret' y1)(ret' y2)), \nret x \u00bb= k = kx (Lret) a \u00bb= ret = a (Rret) (a \u00bb= k1) \u00bb= k2 = a \u00bb= . x.k1x \u00bb= k2 (Bassc) 0/\u00bb= k = 0/(Lzero) \n(a . b) \u00bb= k =(a \u00bb= k) . (b \u00bb= k) (Ldistr) share (a . b)= share a . share b (Choice) share 0/= ret 0/(Fail) \nshare . = ret . (Bot) share (ret (cx1 ...xn)) = share x1 \u00bb= . y1.... (HNF) share xn \u00bb= . yn. ret (ret \n(cy1 ...yn)) where c is a constructor with n non-deterministic components Figure 1. The laws of a monad \nwith non-determinism and sharing run 0/= 0/' (rZero) run (a . b)=(run a) .' (run b) (rPlus) run (ret \n(cx1 ...xn)) = run x1 \u00bb=' . y1.... (rRet) '' run xn \u00bb=' . yn. ret' (c (rety1)...(retyn)) Figure 2. The \nlaws of observing a monad with non-determinism and sharing in another monad with non-determinism which \neventually yields four solutions due to two independent ob\u00adservations of coin. In general, (rRet) ensures \nthat the .nal observa\u00adtion yields only fully determined values. To predict the result of heads_share, \nwe need to apply the (HNF) law in a non-trivial way, with c being Cons and n being 2: share (ret (Cons \ncoin .)) = share coin \u00bb= . y1.share .\u00bb= . y2.ret (ret (Cons y1 y2)) = share coin \u00bb= . y1.ret (ret (Cons \ny1 .)) = share (ret 0 . ret 1) \u00bb= . y1.ret (ret (Cons y1 .)) =(share (ret 0) . share (ret 1)) \u00bb= . y1. \nret (ret (Cons y1 .)) =(share (ret 0) \u00bb= . y1.ret (ret (Cons y1 .))) . (share (ret 1) \u00bb= . y1.ret (ret \n(Cons y1 .))) = ret (ret (Cons (ret 0) .)) . ret (ret (Cons (ret 1) .)) This derivation shows that applying \nshare to ret (Cons coin .) ex\u00adposes and lifts the latent choice coin in the list to the top level. Therefore, \nsharing a list that contains a choice is equivalent to shar\u00ading a choice of a list, so heads_share yields \nonly two outcomes. The (Bot) law and our discussion of lazy_share above suggest a more general law share \na \u00bb= . . b = b, (Ignore) which says that any unused shared computation, not just ., can simply be skipped. \nThis law implies that . is idempotent: b.b = b. The proof of the implication is that b . b =(share (ret \n0) \u00bb= . . b) . (share (ret 1) \u00bb= . .b) = share coin \u00bb= . .b = b. Idempotence is justi.ed if we observe \na non-deterministic com\u00adputation as a set of outcomes, that is, if we care only whether a particular \nresult is produced, not how many times or in what order. This perspective on non-deterministic programming \nis popular; for instance, it is customary in FLP (L\u00b4 opez-Fraguas et al. 2007, 2008).  The (Ignore) \nlaw enables a simpler analysis of our last example program dup_first_coin, which creates an in.nite number \nof choices but demands only a few of them. Without (Ignore), we can only reduce the program using (Choice) \nand (HNF) to /0 . (a . b), where a = d0 . (a . a), b = d1 . (b . b), and di = ret (Cons (ret i)(ret (Cons \n(ret i)(ret Nil)))). Using (Ignore), we can arrive at a simpler result with no duplicate solutions, namely \n/0 . (d0 . d1).  3.3 Intuitions behind our laws Call-time choice makes shared non-determinism feel like \nfamil\u00adiar call-by-value evaluation in monadic style, except /0 and . are treated like values. Indeed, \nthe laws (Fail) and (Bot) would be sub\u00adsumed by (HNF) if . and /0 were ret c for some c. The intuition \nof treating divergence like a value to express laziness guides stan\u00addard formalizations of FLP (Gonz\u00b4alez-Moreno \net al. 1999; L\u00b4opez-Fraguas et al. 2007, 2008) that inspired our laws. Still, for an equational law, \n(Bot) is unusual in two ways. First, (Bot) is not constructive: its left-hand side matches a computation \nthat diverges, which is in general not decidable. Therefore, it does not correspond to a clause in the \nimplementation of share, as we detail in \u00a7\u00a74 5 below. Second, the function share is computable and thus \nmonotonic in the domain-theoretic sense, so (Bot) entails that share a = share . = ret . for all a. In \nparticular, we have by (Choice) that share a . share b = share (a . b) = ret .. How can a non-deterministic \nvalue share a . share b possibly be as de.ned as the deterministic value ret .? The key is that our laws \nhold only up to observation. That is, they only say that replacing certain expressions by others does \nnot affect the (non)termination of well-typed programs3 when the monad type constructors are held abstract \n(Hinze 2000; Lin 2006). Observing a computation in our monad requires applying run then observing the \nresult in the target monad. Each of the two steps may identify many computations. For example, the order \nof choices may be unobservable because the target monad is the set monad.4 Also, we may be unable to \ndisprove that share a .share b is as de.ned as ret . because run (ret .) diverges. A positive example \nof our laws holding up to observation lies in Constructor-Based Rewriting Logic (CRWL) (Gonz\u00b4 alez-Moreno \net al. 1999), a standard formalization of FLP. To every term e (which we assume is closed in this informal \nexplanation), CRWL assigns a denotation [e], the set of partial values that e can reduce to. A partial \nvalue is built up using constructors such as Cons and Nil, but any part can be replaced by . to form \na lesser value. A denotation is a downward-closed set of partial values. L\u00b4 opez-Fraguas et al. s Theorem \n1 (2008) is a fundamental prop\u00aderty of call-time choice. It states that, for every context C and term \ne, the denotation [C[e]] equals the denotation t.!e][C[t]], i.e., the union of denotations of C[t] where \nt is drawn from the deno\u00adtation of e. Even if e is non-deterministic, the denotation of a large term \nthat contains e can be obtained by considering each partial value e can reduce to. Especially, if e is \nan argument to a function that duplicates its argument, this argument denotes the same value wherever \nit occurs. The monadic operation . for non-deterministic choice resembles the CRWL operation ? de.ned \nas follows: x?y->x x?y->y Using the theorem above, we conclude that [C[a?b]] = [C[a]?C[b]], which inspired \nour (Choice) law. 3 without selective strictness via seq 4 The set monad can be implemented in Haskell \njust like the list monad, with the usual Monad and MonadPlus instances that do not depend on Eq or Ord, \nas long as computations can only be observed using the null predicate. 4. Implementing lazy non-determinism \nWe start to implement share in this section. We begin with a very speci.c version and generalize it step \nby step. Revisiting the equational laws for share, we show how memoization can be used to achieve the \ndesired properties. First, we consider values without non-deterministic components, namely values of \ntype Int. We then extend the approach to non-deterministic components, namely lists of numbers. An implementation \nfor arbitrary user-de.ned non\u00addeterministic types in terms of a transformer for arbitrary instances of \nMonadPlus is given in \u00a75. 4.1 The tension between late demand and early choice Lazy evaluation means \nto evaluate expressions at most once and not until they are demanded. The law (Ignore) from the previous \nsection, or more speci.cally, the laws (Fail) and (Bot) from Figure 1 formalize late demand. In order \nto satisfy these laws, we could be tempted to implement share as follows: share::Monadm =>ma->m(ma) share \na = return a and so share undefined is trivially return undefined, just as the law (Bot) requires; (Fail) \nis similarly satis.ed. But (Choice) fails, because ret (a . b) is not equal to ret a . ret b. For example, \nif we take dup_coin_share from \u00a73.1.1 and replace share with return, we obtain dup_coin_let which, as \nexplained there, shares only a non-deterministic computation, not its result as de\u00adsired. Instead of \nre-making the choices in a shared monadic value each time it is demanded, we must make the choices only \nonce and reuse them for duplicated occurrences. We could be tempted to try a different implementation \nof share that ensures that choices are performed immediately: share::Monadm =>ma->m(ma) share a = a >>= \n\\x -> return (return x) This implementation satis.es the (Choice) law, but it does not sat\u00adisfy the (Fail) \nand (Bot) laws. The (Lzero) law of MonadPlus shows that this implementation renders share mzero equal \nto mzero, which is observationally different from the return mzero required by (Fail). This attempt ensures \nearly choice using early de\u00admand, so we get eager sharing, rather than lazy sharing as desired.  4.2 \nMemoization We can combine late demand and early choice using memoization. The idea is to delay the choice \nuntil it is demanded, and to remem\u00adber the choice when it is made for the .rst time so as to not make \nit again if it is demanded again. To demonstrate the idea, we de.ne a very speci.c version of share that \n.xes the monad and the type of shared values. We use a state monad to remember shared monadic values. \nA state monad is an instance of the following type class, which de.nes operations to query and update \na threaded state component. class MonadState s m where get :: m s put :: s -> m () In our case, the threaded \nstate is a list of thunks that can be either unevaluated or evaluated. data Thunk a = Uneval (Memo a) \n| Eval a Here, Memo is the name of our monad. It threads a list of Thunks through non-deterministic computations \nrepresented as lists. newtype Memo a = Memo { unMemo :: [Thunk Int] -> [(a, [Thunk Int])] }  The instance \ndeclarations for the type classes Monad, MonadState, and MonadPlus are as follows: instance Monad Memo \nwhere return x = Memo (\\ts -> [(x,ts)]) m>>=f = Memo (concatMap (\\(x,ts) -> unMemo (f x) ts) . unMemo \nm) instance MonadState [Thunk Int] Memo where get = Memo (\\ts -> [(ts,ts)]) put ts = Memo (\\_ -> [((),ts)]) \ninstance MonadPlus Memo where mzero = Memo (const []) a mplus b = Memo (\\ts -> unMemo a ts ++ unMemo \nb ts) It is crucial that the thunks are passed to both alternatives separately in the implementation \nof mplus. The list of thunks thus constitutes a .rst-class store (Morrisett 1993) using mutable global \nstate to store the thunks would not suf.ce because thunks are created and evaluated differently in different \nnon-deterministic branches. We can implement a very speci.c version of share that works for integers \nin the Memo monad. share :: Memo Int -> Memo (Memo Int) share a = memo a memoa = do thunks <-get let \nindex = length thunks put (thunks ++ [Uneval a]) return (do thunks <-get case thunks!!index of Eval x \n-> return x Uneval a -> dox<-a thunks <-get let (xs,_:ys) = splitAt index thunks put (xs ++ [Eval x] \n++ ys) return x) This implementation of share adds an unevaluated thunk to the current store and returns \na monadic action that, when executed, queries the store and either returns the already evaluated result \nor evaluates the unevaluated thunk before updating the threaded state. The argument a given to share \nis not demanded until the inner action is performed. Hence, this implementation of share satis.es the \n(Fail) and (Bot) laws. Furthermore, the argument is only evaluated once, followed by an update of the \nstate to remember the computed value. Hence, this implementation of share satis.es the (Choice) law (up \nto observation, as de.ned in \u00a74.4). If the inner action is duplicated and evaluated more than once, then \nsubsequent calls will yield the same result as the .rst call due to memoization.  4.3 Non-deterministic \ncomponents The version of share just developed memoizes only integers. How\u00adever, we want to memoize data \nwith non-deterministic components, such as permuted lists that are computed on demand. So instead of \nthunks that evaluate to numbers, we rede.ne the Memo monad to store thunks that evaluate to lists of \nnumbers now. newtype Memo a = Memo { unMemo :: [Thunk (List Memo Int)] -> [(a, [Thunk (List Memo Int)])] \n} The instance declarations for Monad and MonadPlus stay the same. In the MonadState instance only the \nstate type needs to be adapted. We also reuse the memo function, which has now a different type. We could \ntry to de.ne share simply as a renaming for memo again: share :: Memo (List Memo Int) -> Memo (Memo (List \nMemo Int)) share a = memo a However, with this de.nition lists are not shared deeply. This be\u00adhavior \ncorresponds to the expression heads_bind where the head and the tail of the demanded list are still executed \nwhenever they are demanded and may hence yield different results when duplicated. This implementation \ndoes not satisfy the (HNF) law. We can remedy this situation by recursively memoizing the head and the \ntail of a shared list: share :: Memo (List Memo Int) -> Memo (Memo (List Memo Int)) sharea =memo(dol \n<-a case l of Nil -> nil Consxxs->doy <-sharex ys <-share xs cons y ys) This implementation of share \nmemoizes data containing non\u00addeterministic components as deeply as demanded by the compu\u00adtation. Each \ncomponent is evaluated at most once and memoized individually in the list of stored thunks.5  4.4 Observing \nnon-deterministic results In order to observe the results of a computation that contains non\u00addeterministic \ncomponents, we need a function (such as run in Fig\u00adure 2) that evaluates all the components and combines \nthe resulting alternatives to compute a non-deterministic choice of deterministic results. For example, \nwe can de.ne a function eval that computes all results from a non-deterministic list of numbers. eval \n:: List Memo Int -> Memo (List Memo Int) eval Nil = return Nil eval (Cons x xs) = doy <-x >>=eval ys \n<-xs >>= eval return (Cons (return y) (return ys)) The lists returned by eval are fully determined. Using \neval, we can de.ne an operation run that computes the results of a non\u00addeterministic computation: run \n:: Memo (List Memo Int) -> [List Memo Int] run m = map fst (unMemo (m >>= eval) []) In order to guarantee \nthat the observed results correspond to pre\u00addicted results according to the laws in \u00a73.2, we place two \nrequire\u00adments on the monad used to observe the computation ([] above). (In contrast, the laws in \u00a73.2 \nconstrain the monad used to express the computation (Memo above).) Idempotence of mplus The (Choice) \nlaw predicts that the com\u00adputation run (share coin \u00bb= . . ret Nil) gives ret ' Nil .' ret ' Nil. However, \nour implementation gives a single solution ret ' Nil (fol\u00adlowing the (Ignore) law, as it turns out). \nHence, we require .' to be idempotent; that is, m .' m = m. This requirement is satis.ed if we abstract \nfrom the multiplic\u00adity of results (considering [] as the set monad rather than the list 5 This implementation \nof share does not actually type-check because share x in the body needs to invoke the previous version \nof share, for the type Int, rather than this version, for the type List Memo Int. The two versions can \nbe made to coexist, each maintaining its own state, but we develop a polymorphic share combinator in \n\u00a75 below, so the issue is moot.  monad), as is common practice in FLP, or if we treat .' as averag\u00ading \nthe weights of results, as is useful for probabilistic inference. Distributivity of bind over mplus According \nto the (Choice) law, the result of the computation run (share coin \u00bb= . c.coin \u00bb= . y.c \u00bb= . x. ret (Cons \n(ret x)(ret (Cons (ret y)(ret Nil))))) is the following non-deterministic choice of lists (we write (x,y) \nto denote ret ' (Cons (ret ' x)(ret ' (Cons (ret ' y)(ret ' Nil))))). ((0,0). ' (0,1)) .' ((1,0). ' (1, \n1)) However, our implementation yields ((0,0).' (1,0)) .' ((0, 1).' (1,1)). In order to equate these \ntwo trees, we require the following dis\u00adtributive law between \u00bb= ' and .' . '' a \u00bb= ' . x.( fx .' gx)=(a \n\u00bb= f ) .' (a \u00bb= g) If the observation monad satis.es this law, then the two expressions above are equal \n(we write coin ' to denote ret ' 0 .' ret ' 1): ((0, 0).' (0,1)) .' ((1,0).' (1, 1)) =(coin '\u00bb= ' . y. \n(0,y)) .' (coin '\u00bb= ' . y. (1,y)) = coin '\u00bb= ' . y. ((0,y). ' (1,y)) =((0,0).' (1,0)) .' ((0, 1).' (1,1)). \nHence, the intuition behind distributivity is that the observation monad does not care about the order \nin which choices are made. This intuition captures the essence of implementing call-time choice: we can \nperform choices on demand and the results are as if we performed them eagerly. In general, it is .ne \nto use our approach with an observation monad that does not match our requirements, as long as we are \nwilling to abstract from the mismatch. For example, the list monad satis.es neither idempotence nor distributivity, \nyet our equational laws are useful in combination with the list monad if we abstract from the order and \nmultiplicities of results. We also do not require that .' be associative or that /0' be a left or right \nunit of .' . 5. Generalized, ef.cient implementation In this section, we generalize the implementation \nideas described in the previous section such that 1. arbitrary user-de.ned types with non-deterministic \ncomponents can be passed as arguments to the combinator share, and 2. arbitrary instances of MonadPlus \ncan be used as the underlying search strategy.  We achieve the .rst goal by introducing a type class \nwith the inter\u00adface to process non-deterministic data. We achieve the second goal by de.ning a monad \ntransformer Lazy that adds sharing to any instance of MonadPlus. After describing a straightforward imple\u00admentation \nof this monad transformer, we show how to implement it differently in order to improve performance signi.cantly. \nBoth of these generalizations are motivated by practical appli\u00adcations in non-deterministic programming. \n1. The ability to work with user-de.ned types makes it easier to compose deterministic and non-deterministic \ncode and to draw on the sophisticated type and module systems of existing functional languages. 2. The \nability to plug in different underlying monads makes it possible to express techniques such as breadth-.rst \nsearch (Spivey 2000), heuristics, constraint solving (Nordin and Tol\u00admach 2001), and weighted results. \n For example, we have applied our approach to express and sample from probability distributions as OCaml \nprograms in direct style (Filinski 1999). With less development effort than state-of-the\u00adart systems, \nwe achieved comparable concision and performance (Kiselyov and Shan 2009). The implementation of our \nmonad transformer is available as a Hackage package at: http://hackage.haskell.org/cgi-bin/ hackage-scripts/package/explicit-sharing-0.1 \n 5.1 Non-deterministic data We have seen in the previous section that in order to share nested, non-deterministic \ndata deeply, we need to traverse it and apply the combinator share recursively to every non-deterministic \ncom\u00adponent. We have implemented deep sharing for the type of non\u00addeterministic lists, but want to generalize \nthis implementation to support arbitrary user-de.ned types with non-deterministic compo\u00adnents. It turns \nout that the following interface to non-deterministic data is suf.cient: class MonadPlus m => Nondet \nm a where mapNondet ::(forallb .Nondet mb =>m b->m(mb)) -> a -> m a A non-deterministic type a with non-deterministic \ncomponents wrapped in the monad m can be made an instance of Nondet m by implementing the function mapNondet, \nwhich applies a monadic transformation to each non-deterministic component. The type of mapNondet is \na rank-2 type: the .rst argument is a polymorphic function that can be applied to non-deterministic data \nof any type. We can make the type List m Int, of non-deterministic num\u00adber lists, an instance of Nondet \nas follows. instance MonadPlus m => Nondet m Int where mapNondet _ c = return c instance Nondet m a => \nNondet m (List m a) where mapNondet _ Nil = return Nil mapNondet f (Cons x xs) = do y <-f x ys <-f xs \nreturn (Cons y ys) The implementation mechanically applies the given transformation to the non-deterministic \narguments of each constructor. In fact the implementation is so regular that we believe it can be easily \nautomated using generic programming (L\u00a8ammel and Peyton Jones 2003) or Template Haskell. We plan to investigate \nthis possibility. An example for the use of mapNondet is the following oper\u00adation, which computes the \nfully determined values from a non\u00addeterministic value. eval:: Nondetm a=>a->ma eval = mapNondet (\\a \n-> a>>=eval>>=return.return) This operation generalizes the speci.c version for lists given in \u00a74.4. \nIn order to determine a value, we determine values for the arguments and combine the results. The bind \noperation of the monad nicely takes care of the combination. Our original motivation for abstracting \nover the interface of non-deterministic data was to de.ne the operation share with a more general type. \nIn order to generalize the type of share to allow not only different types of shared values but also \ndifferent monad type constructors, we de.ne another type class. class MonadPlus m => Sharing m where \nshare ::Nondetma=>ma->m(ma) Non-determinism monads that support the operation share are instances of \nthis class. We next de.ne an instance of Sharing with the implementation of share for arbitrary non-deterministic \ntypes.  5.2 State monad transformer The implementation of memoization in \u00a74 uses a state monad to thread \na list of thunks through non-deterministic computations. The straightforward generalization is to use \na state monad transformer to thread thunks through computations in arbitrary monads. A state monad transformer \nadds the operations de.ned by the type class MonadState to an arbitrary base monad. The type for Thunks \ngeneralizes easily to an arbitrary monad: data Thunk m a = Uneval (m a) | Eval a Instead of using a list \nof thunks, we use a ThunkStore with the following interface. Note that the operations lookupThunk and \ninsertThunk deal with thunks of arbitrary type. emptyThunks :: ThunkStore getFreshKey :: MonadState ThunkStore \nm => m Int lookupThunk :: MonadState ThunkStore m => Int -> m (Thunk m a) insertThunk :: MonadState ThunkStore \nm => Int -> Thunk m a -> m () There are different options to implement this interface. We have implemented \nthunk stores using the generic programming features provided by the Data.Typeable and Data.Dynamic modules \nbut omit corresponding class contexts for the sake of clarity. Lazy monadic computations can now be performed \nin a monad that threads a ThunkStore. We obtain such a monad by apply\u00ading the StateT monad transformer \nto an arbitrary instance of MonadPlus. type Lazy m = StateT ThunkStore m For any instance m of MonadPlus, \nthe type constructor Lazy m is an instance of Monad, MonadPlus, and MonadState ThunkStore. We only need \nto de.ne the instance of Sharing ourselves, which implements the operation share. instance MonadPlus \nm => Sharing (Lazy m) where share a = memo (a >>= mapNondet share) The implementation of share uses the \noperation memo to memoize the argument and the operation mapNondet to apply share recur\u00adsively to the \nnon-deterministic components of the given value. The function memo resembles the speci.c version given \nin \u00a74.2 but has a more general type. memo :: MonadState ThunkStore m => m a -> m (m a) memoa = do key \n<-getFreshKey insertThunk key (Uneval a) return (do thunk <-lookupThunk key case thunk of Eval x -> return \nx Uneval b -> do x <-b insertThunk key (Eval x) return x) The only difference in this implementation \nof memo from before is that it uses more ef.cient thunk stores instead of lists of thunks. In order to \nobserve a lazy non-deterministic computation, we use the functions eval to compute fully determined values \nand evalStateT to execute actions in the transformed state monad. run:: Nondet(Lazy m)a =>Lazyma->m a \nrun a = evalStateT (a >>= eval) emptyThunks This function is the generalization of the run function to \narbitrary data types with non-deterministic components that are expressed in an arbitrary instance of \nMonadPlus. This completes an implementation of our monad transformer for lazy non-determinism, with all \nof the functionality motivated in \u00a7\u00a72 3.  5.3 Optimizing performance We have applied some optimizations \nthat improve the performance of our implementation signi.cantly. We use the permutation sort in \u00a72 for \na rough measure of performance. The implementation just presented exhausts the search space for sorting \na list of length 20 in about 5 minutes.6 The optimizations described below reduce the run time to 7.5 \nseconds. All implementations run permutation sort in constant space (5 MB or less) and the .nal implementation \nexecutes permutation sort on a list of length 20 roughly three times faster than the fastest available \ncompiler for Curry, the M\u00a8unster Curry Compiler (MCC). As detailed below, we achieve this competitive \nperformance by 1. reducing the amount of pattern matching in invocations of the monadic bind operation, \n 2. reducing the number of store operations when storing shared results, and 3. manually inlining and \noptimizing library code.  5.3.1 Less pattern matching The Monad instance for the StateT monad transformer \nperforms pattern matching in every call to >>= in order to thread the store through the computation. \nThis is wasteful especially during com\u00adputations that do not access the store because they do not perform \nexplicit sharing. We can avoid this pattern matching by using a dif\u00adferent instance of MonadState. We \nde.ne the continuation monad transformer ContT:7 newtype ContT m a = C { unC::forallw.(a-> mw)->mw} runContT \n:: Monad m => ContT m a -> m a runContT m = unC m return We can make ContT m an instance of the type \nclass Monad without using operations from the underlying monad m: instance Monad (ContT m) where returnx= \nC(\\c->cx) m>>=k =C(\\c->unCm(\\x->unC(kx)c)) An instance for MonadPlus can be easily de.ned using the corre\u00adsponding \noperations of the underlying monad. The interesting ex\u00adercise is to de.ne an instance of MonadState using \nContT. When using continuations, a reader monad a monad where actions are functions that take an environment \nas input but do not yield one as output can be used to pass state. More speci.cally, we need the following \noperations of reader monads: ask :: MonadReader s m => m s local::MonadReader sm=>(s->s)->ma->ma The \nfunction ask queries the current environment, and the func\u00adtion local executes a monadic action in a \nmodi.ed environment. In combination with ContT, the function local is enough to im\u00adplement state updates: \ninstance Monad m => MonadState s (ContT (ReaderT s m)) where get =C(\\c->ask>>=c) put s = C (\\c -> local \n(const s) (c ())) 6 We performed our experiments on an Apple MacBook with a 2.2 GHz Intel Core 2 Duo \nprocessor using GHC with optimizations (-O2). 7 This implementation differs from the de.nition shipped \nwith GHC in that the result type w for continuations is higher-rank polymorphic.  With these de.nitions, \nwe can de.ne our monad transformer Lazy: type Lazy m = ContT (ReaderT ThunkStore m) We can reuse from \n\u00a75.2 the de.nition of the Sharing instance and of the memo function used to de.ne share. After this optimization, \nsearching all sorted permutations of a list of length 20 takes about 2 minutes rather than 5.  5.3.2 \nFewer state manipulations The function memo just de.ned performs two state updates for each shared value \nthat is demanded: one to insert the unevaluated shared computation and one to insert the evaluated result. \nWe can save half of these manipulations by inserting only evaluated head-normal forms and using lexical \nscope to access unevaluated computations. We use a different interface to stores now, again abstracting \naway the details of how to implement this interface in a type-safe manner. emptyStore :: Store getFreshKey \n:: MonadState Store m => m Int lookupHNF :: MonadState Store m => Int -> m (Maybe a) insertHNF :: MonadState \nStore m =>Int->a->m () Based on this interface, we can de.ne a variant of memo that only stores evaluated \nhead normal forms. memo :: MonadState Store m => m a -> m (m a) memoa = do key <-getFreshKey return (do \nhnf <-lookupHNF key case hnf of Just x -> return x Nothing -> do x <-a insertHNF key x return x) Instead \nof retrieving a thunk from the store on demand if it is not yet evaluated, we can use the action a directly \nbecause it is in scope. As a consequence, a cannot be garbage collected as long as the computation returned \nby share is reachable, which is a possible memory leak. We did not experience memory problems during \nour experiments, however. After this optimization, searching all sorted permutations of a list of length \n20 takes 1.5 minutes rather than 2.  5.3.3 Mechanical simpli.cations The .nal optimization is to 1. \nexpand the types in ContT (ReaderT State m), 2. inline all de.nitions of monadic operations, 3. simplify \nthem according to monad laws, and 4. provide a specialized version of memo that is not overloaded. \n This optimization, like the previous ones, affects only our library code and not its clients; for instance, \nwe did not inline any de.\u00adnitions into our benchmark code. Afterwards, searching all sorted permutations \nof a list of length 20 takes 7.5 seconds rather than 1.5 minutes. This is the most impressive speedup \nduring this se\u00adquence of optimizations, even though it is completely mechanical and should ideally be \nperformed by the compiler. Surprisingly, our still high-level and very modular implementa\u00adtion (it works \nwith arbitrary monads for non-determinism and ar\u00adbitrary types for nested, non-deterministic data) outperforms \nthe fastest available Curry compiler. A Curry program for permutation sort, equivalent to the program \nwe used for our benchmarks, runs for 25 seconds when compiled with MCC and -O2 optimizations. We have \nalso compared our performance on deterministic monadic computations against corresponding non-monadic \npro\u00adgrams in Haskell and Curry. Our benchmark is to call the naive reverse function on long lists, which \ninvolves a lot of deterministic pattern-matching. In this benchmark, the monadic code is roughly 20% \nfaster than the corresponding Curry code in MCC. The over\u00adhead compared to a non-monadic Haskell program \nis about the same order of magnitude. Our library does not directly support narrowing and uni.ca\u00adtion \nof logic variables but can emulate it by means of lazy non\u00addeterminism. We have measured the overhead \nof such emulation using a functional logic implementation of the last function: last l | l =:= xs ++ \n[x] = x where x,xs free This Curry function uses narrowing to bind xs to the spine of the init of l and \nuni.cation to bind x and the elements of xs to the elements of l. We can translate it to Haskell by replacing \nx and xs with non-deterministic generators and implementing the uni.\u00adcation operator =:= as equality \ncheck. When applying last to a list of determined values, the monadic Haskell code is about six times \nfaster than the Curry version in MCC. The advantage of uni.cation shows up when last is applied to a \nlist of logic variables: in Curry, =:= can unify two logic variables deterministically, while an equal\u00adity \ncheck on non-deterministic generators is non-deterministic and leads to search-space explosion. More \nef.cient uni.cation could be implemented using an underlying monad of equality constraints. All programs \nused for benchmarking are available under: http: //github.com/sebfisch/explicit-sharing/tree/0.1.1 6. \nRelated work In this section we compare our work to foundational and practical work in various communities. \nWe refer to other approaches to im\u00adplementing monads for logic programming. We also point out sim\u00adilarities \nto the problems solved for hardware description languages. 6.1 Functional logic programming The interaction \nof non-strict and non-deterministic evaluation has been studied in the FLP community, leading to different \nseman\u00adtic frameworks and implementations. They all establish call-time choice, which ensures that computed \nresults correspond to strict evaluation. An alternative interpretation of call-time choice is that variables \ndenote values rather than (possibly non-deterministic) computations. As call-time choice has turned out \nto be the most intuitive model for lazy non-determinism, we also adopt it. Unlike approaches discussed \nbelow, however, we do not de\u00ad.ne a new programming language but implement our approach in Haskell. In \nfact, functional logic programs in Curry or Toy can be compiled to Haskell programs that use our library. \nSemantic frameworks There are different approaches to formal\u00adizing the semantics of FLP. CRWL (Gonz\u00b4alez-Moreno \net al. 1999) is a proof calculus with a denotational .avor that allows to rea\u00adson about functional logic \nprograms using inference rules and to prove program equivalence. Let Rewriting (L\u00b4opez-Fraguas et al. \n2007, 2008) de.nes rewrite rules that are shown to be equivalent to CRWL. It is more operational than \nCRWL but does not de.ne a constructive strategy to evaluate programs. Deterministic proce\u00addures to run \nfunctional logic programs are described by Albert et al. (2005) in the form of operational big-step and \nsmall-step semantics. We de.ne equational laws for monadic, lazy, non-deterministic computations that \nresemble let rewriting in that they do not .x an evaluation strategy. However, we provide an ef.cient \nimplemen\u00adtation of our equational speci.cation that can be executed using an arbitrary MonadPlus instance. \nHence, our approach is a step towards closing the gap between let rewriting and the operational semantics, \nas it can be seen as a monadic let calculus that can be executed but does not .x a search strategy. \n Implementations There are different compilers for FLP lan\u00adguages that are partly based on the semantic \nframeworks discussed above. Moreover, the operational semantics by Albert et al. (2005) has been implemented \nas Haskell interpreters by Tolmach and An\u00adtoy (2003) and Tolmach et al. (2004). We do not de.ne a compiler \nthat translates an FLP language; nor do we de.ne an interpreter in Haskell. We rather de.ne a monadic \nlanguage for lazy FLP within Haskell. Instead of de.ning data types for every language construct as the \ninterpreters do, we only need to de.ne new types for data with non-deterministic components. Instead \nof using an untyped representation for non-deterministic data, our approach is typed. This tight integration \nwith Haskell lets us be much more ef.\u00adcient than is possible using an interpreter. The KiCS compiler \nfrom Curry to Haskell (Bra\u00dfel and Huch 2009) also aims to exploit the fact that many functional logic \nprograms contain large determinis\u00adtic parts. Unlike our approach, KiCS does not use monads to imple\u00adment \nsharing but generates unique identi.ers using impure features that prevent compiler optimizations on \nthe generated Haskell code. Naylor et al. (2007) implement a library for functional logic programming \nin Haskell which handles logic variables explicitly and can hence implement a more ef.cient version of \nuni.cation. It does not support data types with non-deterministic components or user-de.ned search strategies. \nThe authors discuss the con.ict between laziness and non-determinism in \u00a75.4 without resolving it. Monad \ntransformers Hinze (2000) derived monad transformers for backtracking from equational speci.cations. \nSpivey (2000) and Kiselyov et al. (2005) improved the search strategy in monadic computations to avoid \nthe de.ciencies of depth-.rst search. How\u00adever, we are the .rst to introduce laziness in non-deterministic \ncomputations modeled using monads in Haskell. Any instance of MonadPlus including those developed in \nthese works can be used in combination with our approach.  6.2 Call-by-need calculi The combination \nof sharing and non-strictness known as call-by\u00adneed or lazy evaluation has been extensively investigated \ntheo\u00adretically. The .rst natural semantics for call-by-need evaluation (Launchbury 1993; Seaman 1993) \nboth rely on heaps, which store either evaluated or unevaluated bindings of variables. Later, Ariola \net al. (1995), Ariola and Felleisen (1997), and Maraist et al. (1998) proposed call-by-need calculi to \navoid the explicit heap: Maraist et al. s sequence of let-bindings and Ariola and Felleisen s bind\u00ading \ncontext play the role of a heap but use bindings present in the original program instead of creating \nfresh heap references. The calculi developed equational theories to reason about call\u00adby-need programs. \nHowever, the laws presented in these calculi are quite different from ours (Figure 1). Although Ariola \net al. add constructors as an extension of their calculus, constructed values cannot have non-value components. \nTo construct data lazily, one must explicitly let-bind computations of all components, however deeply \nnested. They do not have an analogue of our (HNF) law. Maraist et al. brie.y discuss an extension for \nconstructed values with non-value components; their law VK corresponds to our law (HNF). Our laws (Choice), \n(Fail) and (Bot) are not re.ected in any call-by-need calculus. Ariola et al. mention our law (Ignore) \nas a potential addition (adopted by Maraist et al. later). Unlike these call-by-need calculi, we do not \nneed a special syntactic category of answers, since we introduce the notion of observation (Figure 2). \nSince our implementations are based on store passing, they closely correspond to Launchbury s natural \nsemantics (1993). Eval\u00aduating memo a returns a computation that behaves like a variable reference in \nLaunchbury s semantics. Evaluating the variable refer\u00adence for the .rst time evaluates the associated \nterm and updates the store by binding the variable to the resulting value. The main dif\u00adference of our \nevaluator is non-determinism. We cannot get by with a single global heap we need .rst-class stores (Morrisett \n1993), one for each branch of the non-deterministic computation. Garcia et al. (2009) recently reduced \ncall-by-need (again, with\u00adout non-determinism) to stylized uses of delimited continuations. In particular, \nthey simulate call-by-need in a call-by-value calcu\u00adlus with delimited control. We have similarly (Kiselyov \nand Shan 2009) embedded lazy probabilistic programs (Koller et al. 1997) in OCaml, a call-by-value language \nwith delimited control. Like Gar\u00adcia et al., we use .rst-class control delimiters to represent shared \nvariables on the heap. A useful (Goodman et al. 2008) and straight\u00adforward generalization is to memoize \nprobabilistic functions.  6.3 Hardware description languages The problem of explicit sharing has also \nbeen addressed in the context of hardware description languages (Acosta-G\u00b4omez 2007; Bjesse et al. 1998). \nIn order to model a circuit as an algebraic data type in a purely functional language, one needs to be \nable to identify shared nodes. The survey by Acosta-G\u00b4omez (2007; \u00a72.4.1) discusses four different solutions \nto this problem: Explicit labels clutter the code with identi.ers that are apart from expressing sharing \nunrelated to the design of a circuit. Moreover, the programmer is responsible for passing unique labels \nin order to correctly model the nodes of a circuit. State monads can be used to automate the creation \nof unique la\u00ad bels. However, writing monadic code is considered such a ma\u00ad jor paradigm shift in the \ncontext of circuit description that, for example, Lava (Bjesse et al. 1998) resorts to the next solution. \nObservable sharing is the preferred solution because it maintains the usual recursive structure of the \ncircuit description, but it re\u00ad quires impure features that often make it extremely complicated to reason \nabout or debug programs (de Vries 2009). Source transformations can also label the nodes of a circuit \nau\u00ad tomatically. For example, Template Haskell can be used to add unique labels at compile time to unlabeled \ncircuit descriptions. Observable sharing is very similar to the approach used cur\u00adrently in KiCS (Bra\u00dfel \nand Huch 2009). The problem of im\u00adpure features especially their hindering compiler optimizations seems \nmuch more severe in FLP than in hardware description. As non-deterministic computations are usually expressed \nmonadically in Haskell anyway, there is no paradigm shift necessary to use our monadic approach to sharing. \nIt integrates smoothly by introducing a new operation to share the results of monadic computations. 7. \nConclusions We have presented an equational speci.cation and an ef.cient implementation of non-strictness, \nsharing, and non-determinism embedded in a pure functional language. Our speci.cation (Figure 1) formalizes \ncall-time choice, a com\u00adbination of these three features that has been developed in the FLP community. \nThis combination is intuitive and predictable because the results of computations resemble results of \ncorresponding ea\u00adger computations and shared variables represent fully determined values as opposed to \npossibly non-deterministic computations. Our equational laws for lazy non-determinism can be used to \nreason about the meaning of non-deterministic programs on a high level. They differ from previous formalizations \nof call-time choice, which use proof calculi, rewriting, or operational semantics. We describe intuitively \nthe correspondence of L\u00b4 opez-Fraguas et al. s formaliza\u00adtions (2007, 2008) with our laws as well as \nwhy our implementation satis.es them. A more formal treatment is left as future work. Our implementation \nis novel in working with custom monadic data types and search strategies; in expressing the sharing of \nnon\u00addeterministic choices explicitly; and in implementing the sharing using .rst-class stores of typed \ndata.  Our high-level monadic interface was crucial in order to op\u00adtimize our implementation as described \nin \u00a75.3. Initial compar\u00adisons of monadic computations with corresponding computations in Curry that use \nnon-determinism, narrowing, and uni.cation are very promising. We outperform the currently fastest Curry \ncompiler (MCC) on the highly non-deterministic permutation sort algorithm. In our deterministic benchmark \nwe incur acceptable overhead com\u00adpared to pure Haskell. Simulated narrowing turned out competitive while \nsimulated uni.cation can lead to search space explosion. Our results suggest that our work can be used \nas a simple, high-level, and ef.cient implementation target for FLP languages. Acknowledgments We thank \nGreg Morrisett for pointing us to .rst-class stores, and Bernd Bra\u00dfel and Michael Hanus for examining \ndrafts of this paper. References Acosta-G\u00b4 omez, Alfonso. 2007. Hardware synthesis in ForSyDe. Master \ns thesis, Dept. of Microelectronics and Information Technology, Royal Institute of Technology, Stockholm, \nSweden. Albert, Elvira, Michael Hanus, Frank Huch, Javier Oliver, and German Vidal. 2005. Operational \nsemantics for declarative multi-paradigm languages. Journal of Symbolic Computation 40(1):795 829. Antoy, \nSergio, and Michael Hanus. 2002. Functional logic design patterns. In FLOPS, 67 87. . 2006. Overlapping \nrules and logic variables in functional logic programs. In ICLP, 87 101. Ariola, Zena M., and Matthias \nFelleisen. 1997. The call-by-need lambda calculus. JFP 7(3):265 301. Ariola, Zena M., Matthias Felleisen, \nJohn Maraist, Martin Odersky, and Philip Wadler. 1995. The call-by-need lambda calculus. In POPL, 233 \n246. Bird, Richard, Geraint Jones, and Oege de Moor. 1997. More haste, less speed: Lazy versus eager \nevaluation. JFP 7(5):541 547. Bjesse, Per, Koen Claessen, Mary Sheeran, and Satnam Singh. 1998. Lava: \nHardware design in Haskell. In ICFP, 174 184. Bra\u00dfel, Bernd, and Frank Huch. 2009. The Kiel Curry System \nKiCS. In WLP 2007, 195 205. Christiansen, Jan, and Sebastian Fischer. 2008. EasyCheck test data for \nfree. In FLOPS, 322 336. Claessen, Koen, and John Hughes. 2000. QuickCheck: A lightweight tool for random \ntesting of Haskell programs. In ICFP, 268 279. Escard\u00b4o, Mart\u00b4in H. 2007. In.nite sets that admit fast \nexhaustive search. In LICS, 443 452. Felleisen, Matthias. 1985. Transliterating Prolog into Scheme. Tech. \nRep. 182, Computer Science Dept., Indiana University. Filinski, Andrzej. 1999. Representing layered monads. \nIn POPL, 175 188. Fischer, Sebastian, and Herbert Kuchen. 2007. Systematic gener\u00adation of glass-box \ntest cases for functional logic programs. In PPDP, 63 74. Garcia, Ronald, Andrew Lumsdaine, and Amr Sabry. \n2009. Lazy evaluation and delimited control. In POPL, 153 164. Gonz\u00b4alez-Moreno, Juan Carlos, Maria Teresa \nHortal\u00b4a-Gonz\u00b4alez, Francisco Javier L\u00b4opez-Fraguas, and Mario Rodr\u00b4iguez-Artalejo. 1999. An approach \nto declarative programming based on a rewriting logic. Journal of Logic Programming 40(1):47 87. Goodman, \nNoah, Vikash K. Mansinghka, Daniel Roy, Keith Bonawitz, and Joshua B. Tenenbaum. 2008. Church: A language \nfor generative models. In UAI, 220 229. Haynes, Christopher T. 1987. Logic continuations. Journal of \nLogic Programming 4(2):157 176. Hinze, Ralf. 2000. Deriving backtracking monad transformers (functional \npearl). In ICFP, 186 197. Hudak, Paul. 1996. Building domain-speci.c embedded languages. ACM Computing \nSurveys 28(4es):196. Hughes, John. 1989. Why functional programming matters. The Computer Journal 32(2):98 \n107. Kiselyov, Oleg, and Chung-chieh Shan. 2009. Embedded proba\u00adbilistic programming. In Working conf. \non domain speci.c lang. Kiselyov, Oleg, Chung-chieh Shan, Daniel P. Friedman, and Amr Sabry. 2005. Backtracking, \ninterleaving, and terminating monad transformers (functional pearl). In ICFP, 192 203. Koller, Daphne, \nDavid McAllester, and Avi Pfeffer. 1997. Effective Bayesian inference for stochastic programs. In AAAI, \n740 747. L\u00a8Scrap your ammel, Ralf, and Simon L. Peyton Jones. 2003. boilerplate: A practical design pattern \nfor generic programming. In TLDI, 26 37. Launchbury, John. 1993. A natural semantics for lazy evaluation. \nIn POPL, 144 154. Lin, Chuan-kai. 2006. Programming monads operationally with Unimo. In ICFP, 274 285. \nLopez-Fraguas, Francisco Javier, \u00b4Juan Rodr\u00b4iguez-Hortal\u00b4 a, and Jaime S\u00b4anchez-Hern\u00b4andez. 2007. A simple \nrewrite notion for call-time choice semantics. In PPDP, 197 208. . 2008. Rewriting and call-time choice: \nThe HO case. In FLOPS, 147 162. Maraist, John, Martin Odersky, and Philip Wadler. 1998. The call\u00adby-need \nlambda calculus. JFP 8(3):275 317. McCarthy, John. 1963. A basis for a mathematical theory of computation. \nIn Computer programming and formal systems, 33 70. North-Holland. Michie, Donald. 1968. Memo functions \nand machine learning. Nature 218:19 22. MonadPlus. 2008. MonadPlus. http://www.haskell.org/ haskellwiki/MonadPlus. \nMorrisett, J. Gregory. 1993. Re.ning .rst-class stores. In Proceed\u00adings of the workshop on state in programming \nlanguages. Naylor, Matthew, Emil Axelsson, and Colin Runciman. 2007. A functional-logic library for Wired. \nIn Haskell workshop, 37 48. Nicollet, Victor, et al. 2009. Lazy and threads. http: //caml.inria.fr/pub/ml-archives/caml-list/2009/ \n02/9fc4e4a897ce7a356674660c8cfa5ac0.fr.html. Nordin, Thomas, and Andrew Tolmach. 2001. Modular lazy search \nfor constraint satisfaction problems. JFP 11(5):557 587. Rabin, Michael O., and Dana Scott. 1959. Finite \nautomata and their decision problems. IBM Journal of Research and Development 3:114 125. Runciman, Colin, \nMatthew Naylor, and Fredrik Lindblad. 2008. SmallCheck and Lazy SmallCheck: Automatic exhaustive test\u00ading \nfor small values. In Haskell symposium, 37 48. Seaman, Jill M. 1993. An operational semantics of lazy \nevaluation for analysis. Ph.D. thesis, Pennsylvania State University. Spivey, J. Michael. 2000. Combinators \nfor breadth-.rst search. JFP 10(4):397 408. Tolmach, Andrew, and Sergio Antoy. 2003. A monadic semantics \nfor core Curry. In WFLP, 33 46. Valencia, Spain. Tolmach, Andrew, Sergio Antoy, and Marius Nita. 2004. \nImple\u00admenting functional logic languages using multiple threads and stores. In ICFP, 90 102. de Vries, \nEdsko. 2009. Just how unsafe is unsafe. http://www.haskell.org/pipermail/haskell-cafe/ 2009-February/055201.html. \nWadler, Philip L. 1985. How to replace failure by a list of suc\u00adcesses: A method for exception handling, \nbacktracking, and pat\u00adtern matching in lazy functional languages. In FPCA, 113 128.     \n\t\t\t", "proc_id": "1596550", "abstract": "<p>Functional logic programming and probabilistic programming have demonstrated the broad benefits of combining laziness (non-strict evaluation with sharing of the results) with non-determinism. Yet these benefits are seldom enjoyed in functional programming, because the existing features for non-strictness, sharing, and non-determinism in functional languages are tricky to combine.</p> <p>We present a practical way to write purely functional lazy non-deterministic programs that are efficient and perspicuous. We achieve this goal by embedding the programs into existing languages (such as Haskell, SML, and OCaml) with high-quality implementations, by making choices lazily and representing data with non-deterministic components, by working with custom monadic data types and search strategies, and by providing equational laws for the programmer to reason about their code.</p>", "authors": [{"name": "Sebastian Fischer", "author_profile_id": "81330491065", "affiliation": "Christian-Albrechts University, Kiel, Germany", "person_id": "P1613975", "email_address": "", "orcid_id": ""}, {"name": "Oleg Kiselyov", "author_profile_id": "81100177557", "affiliation": "FNMOC, Monterey, CA, USA", "person_id": "P1613976", "email_address": "", "orcid_id": ""}, {"name": "Chung-chieh Shan", "author_profile_id": "81100400956", "affiliation": "Rutgers University, Piscataway, NJ, USA", "person_id": "P1613977", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1596550.1596556", "year": "2009", "article_id": "1596556", "conference": "ICFP", "title": "Purely functional lazy non-deterministic programming", "url": "http://dl.acm.org/citation.cfm?id=1596556"}