{"article_publication_date": "08-31-2009", "fulltext": "\n A Concurrent ML Library in Concurrent Haskell Avik Chaudhuri University of Maryland, College Park \navik@cs.umd.edu Abstract In Concurrent ML, synchronization abstractions can be de.ned and passed as values, \nmuch like functions in ML. This mecha\u00adnism admits a powerful, modular style of concurrent programming, \ncalled higher-order concurrent programming. Unfortunately, it is not clear whether this style of programming \nis possible in lan\u00adguages such as Concurrent Haskell, that support only .rst-order message passing. Indeed, \nthe implementation of synchronization abstractions in Concurrent ML relies on fairly low-level, language\u00adspeci.c \ndetails. In this paper we show, constructively, that synchronization ab\u00adstractions can be supported in \na language that supports only .rst\u00adorder message passing. Speci.cally, we implement a library that makes \nConcurrent ML-style programming possible in Concurrent Haskell. We begin with a core, formal implementation \nof synchro\u00adnization abstractions in the p-calculus. Then, we extend this imple\u00admentation to encode all \nof Concurrent ML s concurrency primitives (and more!) in Concurrent Haskell. Our implementation is surprisingly \nef.cient, even without pos\u00adsible optimizations. In several small, informal experiments, our li\u00adbrary \nseems to outperform OCaml s standard library of Concurrent ML-style primitives. At the heart of our implementation \nis a new distributed syn\u00adchronization protocol that we prove correct. Unlike several previ\u00adous translations \nof synchronization abstractions in concurrent lan\u00adguages, we remain faithful to the standard semantics \nfor Concurrent ML s concurrency primitives. For example, we retain the symme\u00adtry of choose, which can \nexpress selective communication. As a corollary, we establish that implementing selective communication \non distributed machines is no harder than implementing .rst-order message passing on such machines. Categories \nand Subject Descriptors D.1.3 [Programming Tech\u00adniques]: Concurrent Programming; D.3.3 [Programming Lan\u00adguages]: \nLanguage Constructs and Features Concurrent program\u00adming structures; D.3.1 [Programming Languages]: Formal \nDe.\u00adnitions and Theory Semantics General Terms Algorithms, Languages Keywords Concurrent ML, synchronization \nabstractions, dis\u00adtributed synchronization protocol, p-calculus, Concurrent Haskell Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for pro.t or commercial advantage and that copies bear \nthis notice and the full citation on the .rst page. To copy otherwise, to republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. ICFP 09, August 31 September \n2, 2009, Edinburgh, Scotland, UK. Copyright c &#38;#169; 2009 ACM 978-1-60558-332-7/09/08. . . $10.00 \n1. Introduction As famously argued by Reppy (1999), there is a fundamental con\u00ad.ict between selective \ncommunication (Hoare 1978) and abstrac\u00adtion in concurrent programs. For example, consider a protocol \nrun between a client and a pair of servers. In this protocol, selec\u00adtive communication may be necessary \nfor liveness if one of the servers is down, the client should be able to interact with the other. This \nmay require some details of the protocol to be exposed. At the same time, abstraction may be necessary \nfor safety the client should not be able to interact with a server in an unexpected way. This may in \nturn require those details to be hidden. An elegant way of resolving this con.ict, proposed by Reppy \n(1992), is to separate the process of synchronization from the mech\u00adanism for describing synchronization \nprotocols. More precisely, Reppy introduces a new type constructor, event, to type syn\u00adchronous operations \nin much the same way as -> ( arrow ) types functional values. A synchronous operation, or event, describes \na synchronization protocol whose execution is delayed until it is explicitly synchronized. Thus, roughly, \nan event is analogous to a function abstraction, and event synchronization is analogous to function application. \nThis abstraction mechanism is the essence of a powerful, mod\u00adular style of concurrent programming, called \nhigher-order concur\u00adrent programming. In particular, programmers can describe sophis\u00adticated synchronization \nprotocols as event values, and compose them modularly. Complex event values can be constructed from simpler \nones by applying suitable combinators. For instance, se\u00adlective communication can be expressed as a choice \namong event values and programmer-de.ned abstractions can be used in such communication without breaking \nthose abstractions (Reppy 1992). Reppy implements events, as well as a collection of such suit\u00adable combinators, \nin an extension of ML called Concurrent ML (CML) (Reppy 1999). We review these primitives informally \nin Section 2; their formal semantics can be found in (Reppy 1992). The implementation of these primitives \nin CML relies on fairly low-level, language-speci.c details, such as support for continu\u00adations and signals \n(Reppy 1999). In turn, these primitives immedi\u00adately support higher-order concurrent programming in CML. \nOther languages, such as Concurrent Haskell (Peyton-Jones et al. 1996), seem to be more modest in their \ndesign. Following the p-calculus (Milner et al. 1992), such languages support only .rst-order message \npassing. While functions for .rst-order mes\u00adsage passing can be encoded in CML, it is unclear whether, \ncon\u00adversely, the concurrency primitives of CML can be expressed in those languages. Contributions In \nthis paper, we show that CML-style concur\u00adrency primitives can in fact be implemented as a library, in \na lan\u00adguage that already supports .rst-order message passing. Such a library makes higher-order concurrent \nprogramming possible in a language such as Concurrent Haskell. Our implementation has fur\u00adther interesting \nconsequences. For instance, the designers of Con\u00adcurrent Haskell deliberately avoid a CML-style choice \nprimitive (Peyton-Jones et al. 1996, Section 5), partly concerned that such a primitive may complicate \na distributed implementation of Con\u00adcurrent Haskell. By showing that such a primitive can be encoded \nin Concurrent Haskell itself, we eliminate that concern.  At the heart of our implementation is a new \ndistributed protocol for synchronization of events. Our protocol is carefully designed to ensure safety, \nprogress, and fairness. In Section 3, we formalize this protocol as an abstract state machine, and prove \nits correctness. Then, in Section 4, we describe a concrete implementation of this protocol in the p-calculus, \nand prove its correctness as well. This implementation can serve as a foundation for other implementa\u00adtions \nin related languages. Building on this implementation, in Sec\u00adtions 5, 6, and 7, we show how to encode \nall of CML s concurrency primitives, and more, in Concurrent Haskell. Our implementation is very concise, \nrequiring less than 150 lines of code; in contrast, a related existing implementation (Russell 2001) \nrequires more than 1300 lines of code. In Section 8, we compare the performance of our library against \nOCaml s standard library of CML-style primitives, via sev\u00aderal small, informal experiments. Our library \nconsistently runs faster in these experiments, even without possible optimizations. While these experiments \ndo not account for various differences be\u00adtween the underlying language implementations, especially those \nof threads, we think that these results are nevertheless encouraging. Finally, we should point out that \nunlike several previous im\u00adplementations of CML-style primitives in other languages, we re\u00admain faithful \nto the standard semantics for those primitives (Reppy 1999). For example, we retain the symmetry of choose, \nwhich can express selective communication. Indeed, we seem to be the .rst to implement a CML library \nthat relies purely on .rst-order message passing, and preserves the standard semantics. We defer a more \nde\u00adtailed discussion on related work to Section 9. 2. Overview of CML In this section, we present a brief \noverview of CML s concurrency primitives. (Space constraints prevent us from motivating these primitives \nany further; the interested reader can .nd a comprehen\u00adsive account of these primitives, with several \nprogramming exam\u00adples, in (Reppy 1999).) We provide a small example at the end of this section. Note \nthat channel and event are polymorphic type construc\u00adtors in CML, as follows: The type channel tau is \ngiven to channels that carry values of type tau.  The type event tau is given to events that return \nvalues of type tau on synchronization (see the function sync below).  The combinators receive and transmit \nbuild events for syn\u00adchronous communication. receive : channel tau -> event tau transmit : channel tau \n-> tau -> event () receive c returns an event that, on synchronization, accepts a message M on channel \nc and returns M. Such an event must synchronize with transmit c M.  transmit c M returns an event that, \non synchronization, sends the message M on channel c and returns () (that is, unit ). Such an event must \nsynchronize with receive c.  Perhaps the most powerful of CML s concurrency primitives is the combinator \nchoose; it can nondeterministically select an event from a list of events, so that the selected event \ncan be synchronized. In particular, choose can express selective communication. Several implementations \nneed to restrict the power of choose in order to tame it (Russell 2001; Reppy and Xiao 2008). Our implementation \nis designed to avoid such problems (see Section 9). choose : [event tau] -> event tau choose V returns \nan event that, on synchronization, synchro\u00adnizes one of the events in list V and aborts the other events. \nConversely, the combinator wrapabort can specify an action that is spawned if an event is aborted by \na selection. wrapabort : (() -> ()) -> event tau -> event tau wrapabort f v returns an event that either \nsynchronizes the event v, or, if aborted, spawns a thread that runs the code f (). The combinators guard \nand wrap can specify pre-and post\u00adsynchronization actions. guard : (() -> event tau) -> event tau wrap \n: event tau -> (tau -> tau ) -> event tau guard f returns an event that, on synchronization, synchro\u00adnizes \nthe event returned by the code f ().  wrap v f returns an event that, on synchronization, synchro\u00adnizes \nthe event v and applies the function f to the result.  Finally, the function sync can synchronize an \nevent and return the result. sync : event tau -> tau By design, an event can synchronize only at some \npoint , where a message is either sent or accepted on a channel. Such a point, called the commit point, \nmay be selected among several other candidates at run time. Furthermore, some code may be run before, \nand after, synchronization as speci.ed by guard functions, by wrap functions that enclose the commit \npoint, and by wrapabort functions that do not enclose the commit point. For example, consider the following \nvalue of type event (). (Here, c and c are values of type channel ().) val v= choose [guard (fn () -> \n...; wrapabort ... (choose [wrapabort ... (transmit c ()); wrap (transmit c ()) ... ] ) ); guard (fn \n() -> ...; wrap (wrapabort ... (receive c)) ... ) ] The event v describes a fairly complicated protocol \nthat, on syn\u00adchronization, selects among the communication events transmit c (), transmit c (), and receive \nc, and runs some code (elided by ...s) before and after synchronization. Now, suppose that we run the \nfollowing ML program. val _= spawn (fn () -> sync v); sync (receive c ) This program spawns sync v in \nparallel with sync (receive c ). In this case, the event transmit c () is selected inside v, so that \nit synchronizes with receive c . The .gure below depicts sync v as a tree. The point marked is the commit \npoint; this point is selected among the other candidates, marked ., at run time.  Furthermore, (only) \ncode speci.ed by the combinators marked in boxes are run before and after synchronization, following \nthe semantics outlined above. wrapabort . guard wrapabort choose < wrap choose < guard wrap wrapabort \n. 3. A distributed protocol for synchronization We now present a distributed protocol for synchronizing \nevents. We focus on events that are built with the combinators receive, transmit, and choose. While the \nother combinators are important for describing computations, they do not fundamentally affect the nature \nof the protocol; we consider them later, in Sections 5 and 6. 3.1 A source language For brevity, we simplify \nthe syntax of the source language. Let . c range over channels. We use the following notations: -.i is \na sequence of the form .1,...,.n, where e . 1..n; furthermore, .- . {-.i} is the set {.1,...,.n}, and \n[.i] is the list [.1,...,.n]. The syntax of the language is as follows. Actions a, \u00df, . . . are of the \nform c or c (input or output on c). Informally, actions model communication events built with receive \nand transmit.  Programs are of the form S1 | ... | Sm (parallel composition of S1,...,Sm), where each \nSk (k . 1..m) is either an action  - . a, or a selection of actions, select(ai). Informally, a selection \nof actions models the synchronization of a choice of events, following the CML function select. select \n: [event tau] -> tau select V = sync (choose V) Further, we consider only the following local reduction \nrule: - . . c . {-ai} c .{ \u00dfj } (SEL COMM) - .- . select(ai) | select(\u00dfj ) -. c | c This rule models \nselective communication. We also consider the usual structural rules for parallel composition. However, \nwe ignore reduction of actions at this level of abstraction.  3.2 A distributed abstract state machine \nfor synchronization Our synchronization protocol is run by a distributed system of prin\u00adcipals that include \nchannels, points, and synchronizers. Informally, every action is associated with a point, and every select \nis associ\u00adated with a synchronizer. The reader may draw an analogy between our setting and one of arranging \nmarriages, by viewing points as prospective brides and grooms, channels as matchmakers, and synchronizers \nas parents whose consents are necessary for marriages. We formalize our protocol as a distributed abstract \nstate ma\u00adchine that implements the rule (SEL COMM). Let s range over states of the machine. These states \nare built by parallel composi\u00adtion |, inaction 0, and name creation . (Milner et al. 1992) over various \nstates of principals. States of the machine s s ::= states of the machine s | s' parallel composition \n0 inaction - . (.pi ) s name creation . state of principals The various states of principals are shown \nbelow. Roughly, prin\u00adcipals in speci.c states react with each other to cause transitions in the machine, \nfollowing rules that appear later in the section. States of principals . .p ::= states of a point p . \na active .p matched a released .c ::= states of a channel 8c free .c(p, q) announced .s ::= states of \na synchronizeropen s s closed .s(p) selected \u00d7(p) refused .. .s (p) con.rmed .. .s canceled Let p and \ns range over points and synchronizers. A synchronizer can be viewed as a partial function from points \nto actions; we represent this function as a parallel composition of bindings of the form p . a. Further, \nwe require that each point is associated with a '' unique synchronizer, that is, for any s and s, s = \ns. dom(s) n dom(s')= \u00d8. The semantics of the machine is described by the local transition rules shown \nbelow, plus the usual structural rules for parallel com\u00adposition, inaction, and name creation as in the \np-calculus (Milner et al. 1992). Operational semantics s -. s' (1) p . c | q . c |8c -. .p |.q |.c (p, \nq) |8c p . dom(s) (2.i) .p | s -. .s(p) | s p . dom(s) (2.ii) .p | s -. \u00d7 (p) | s .... (3.i) .s(p) | \n.s' (q) |.c (p, q) -. .s (p) | .s' (q) .. (3.ii) .s(p) |\u00d7 (q) |.c (p, q) -. .s .. (3.iii) \u00d7(p) | .s(q) \n|.c (p, q) -. .s (3.iv) \u00d7(p) |\u00d7 (q) |.c (p, q) -. 0 s(p)= a (4.i) .. .s (p) -. a ..-. . (4.ii) . (.pi \n)( s | s) where dom(s)= {- .s pi } Intuitively, these rules may be read as follows. (1) Two points p \nand q, bound to complementary actions on chan\u00adnel c, react with c, so that p and q become matched (.p \nand .q) and the channel announces their match (.c(p, q)). (2.i ii) Next, p (and likewise, q) reacts with \nits synchronizer s. If the synchronizer is open ( s), it now becomes closed ( s), and p is declared selected \nby s (.s(p)). If the synchronizer is already closed, then p is refused (\u00d7(p)).  (3.i iv) If both p and \nq are selected, c con.rms the selections to both parties ( .. s (p) and .. s ' (q)). If only one of them \nis selected, c cancels that selection ( .. s). (4.i ii) If the selection of p is con.rmed, the action \nbound to p is released. Otherwise, the synchronizer reboots with fresh names for the points in its domain. \n 3.3 Compilation Next, we show how programs in the source language are com\u00adpiled on to this machine. \nLet . denote indexed parallel compo\u00adsition; using this notation, for example, we can write a program \nS1 | ... | Sm as .k.1..mSk. Suppose that the set of channels in a program .k.1..mSk is C. We compile \nthis program to the state ~ .c.C 8c | .k.1..m Sk, where 8 <a if S = a ~ -. - . S (.pi )( s | s) if S \n= select(ai), i . 1..n, and : . s =.i.1..n (pi . ai) for fresh names - pi 3.4 Correctness We prove that \nour protocol is correct, that is, the abstract machine correctly implements (SEL COMM), by showing that \nthe compila\u00adtion from programs to states satis.es safety, progress, and fairness. Roughly, safety implies \nthat any sequence of transitions in the state machine can be mapped to some sequence of reductions in \nthe lan\u00adguage. Furthermore, progress and fairness imply that any sequence of reductions in the language \ncan be mapped to some sequence of transitions in the state machine. (The formal de.nitions of these properties \nare complicated because transitions in the machine have much .ner granularity than reductions in the \nlanguage; see below.) Let a denotation be a list of actions. The denotations of pro\u00adgrams and states \nare derived by the function '\u00b7', as follows. (Here l denotes concatenation over lists.) (Progress) if \nP -. , then s -.+ s ' and P -.P ' for some s ' and P ' such that P ' ~ s ' ; (Fairness) if P -.P ' for \nsome P ', then s0 -. ... -. sn for some s0,...,sn such that sn = s, P~ si for all 0 = i<n, and s0 -.+ \ns ' for some s ' such that P ' ~ s ' . Informally, the above theorem guarantees that any sequence of \nprogram reductions can be simulated by some sequence of state \u00af transitions, and vice versa, such that \n\u00af from any intermediate program, it is always possible to simulate any transition of a related intermediate \nstate;  from any intermediate state,  it is always possible to simulate some reduction of a related \nintermediate program;  further, by backtracking, it is always possible to simulate any reduction of \nthat program.   3.5 Example Consider the program select(x, y) | select(y, z) | select(z) | select(x) \nBy (SEL COMM), this program can reduce to x | z | z | x with denotation [x, z, z, x], or to y | y | select(z) \n| select(x) with denotation [y, y]. The original program is compiled to the following state. x,py\u00af)( \nx .x | py .y) | p\u00af 8x |8y |8z | (.p\u00af| (.py,pz x . x | py\u00af . y) . y | pz (p . z) )( | (.pz\u00af| (.px (py \n.y | pz .z) | py . z) )( (p \u00af z .z) | p\u00af )( (px .x) | px . x) z Denotations of programs and states '\u00b7' \n\u00af \u00af \u00af 'S1 | ... | Sm' = 'S1' l \u00b7 \u00b7\u00b7l 'Sm' This state describes the states of several principals: 'a' \n=[a] channels 8x, 8y, 8z; - 'select(.ai)' = [] points px\u00af . x, py\u00af . y, py . y, pz . z, pz\u00af . z, px \n. x; x .x | py .y), z .z), 's | s '' = 's' l 's '' synchronizers '0' (py .y | pz .z), (p(p (px .x). \n= [] This state can eventually transition to - . '(.pi ) s' = 's' j '.' = [a] if . = a 8x |8y |8z | \nx | z | z | x | sgc [] otherwise with denotation [x, z, z, x], or to 8x |8y |8z | y | y | sgc Informally, \nthe denotation of a program or state is the list of released actions in that program or state. Now, if \na program is com\u00ad piled to some state, then the denotations of the program and the .z) | pz\u00af . z) .x) \n| px z)( (p \u00af z(px with denotation [y, y]. In these states, sgc can be garbage-collected, | (.p\u00af| (.px)( \n. x) state coincide. Furthermore, we expect that as intermediate pro\u00adand is separated out for readability. \nsgc (.px\u00af,py\u00af,py,pz,pz\u00af,px) grams and states are produced during execution (and other actions are released), \nthe denotations of those intermediate programs and states remain in coincidence. Formally, we prove the \nfollowing the\u00ad \u00af \u00af \u00af x .x | py .y) | z .z) | THEOREM 3.1 (Correctness of the abstract state machine). \nLet C Let us examine the state with denotation [y, y], and trace the be the set of channels in a program \n.k.1..mSk. Then transitions to this state. In this state, the original synchronizers are ~ all closed \n(see sgc). We can conclude that the remaining points (py .y | pz .z) | ( (px .x)) orem (Chaudhuri 2009). \n(p(p .k.1..mSk ~ .c.C 8c | .k.1..m Sk and px . x and their synchronizers and . pz\u00afz (p \u00af z .z) (px .x) \nwere produced by rebooting their original synchronizers with fresh names pz\u00afand px. Indeed, in a previous \nround of the pro\u00ad where ~ is the largest relation such that P~ s iff (Invariant) s -.* s ' for some s \n' such that 'P' = 's ''; tocol, the original points pz\u00af . z and px . x were matched with (Safety) if \ns -. s ' for some s ', then P -.* P ' for some P ' the points pz . z and px . x, respectively; however, \nthe lat\u00adsuch that P ' ~ s ' ; ter points were refused by their synchronizers (py .y | pz .z) and x.x \n| py.y) (to accommodate the selected communication on y in that round); these refusals in turn necessitated \nthe cancellations .. .. Each channel c is identi.ed with a pair of fresh names z.z) and \u00af\u00af\u00af Each point \nis identi.ed with a fresh name p. (p (px.x). (i[c][c] , o (p ), on which it accepts messages from points \nthat are 4. Higher-order concurrency in the p-calculus While we have an abstract state machine that correctly \nimplements (SEL COMM), we do not yet know if the local transition rules in Section 3.2 can be implemented \nfaithfully, say by .rst-order message-passing. We now show how these rules can be imple\u00admented concretely \nin the p-calculus (Milner et al. 1992). The p-calculus is a minimal concurrent language that allows processes \nto dynamically create channels with fresh names and communicate such names over channels. This language \nforms the core of Concurrent Haskell. Let a, b, x range over names. The syntax of processes is as follows. \nProcesses p p ::= processes p | p ' parallel composition 0 inaction (.a) p name creation a(b).p output \na(x).p input !p replication Processes have the following informal meanings. p | p ' behaves as the parallel \ncomposition of p and p ' .  0 does nothing.  (.a) p creates a channel with fresh name a and continues \nas p; the scope of a is p.  a(b).p sends the name b on channel a, and continues as p.  a(x).p accepts \na name on channel a, binds it to x, and contin\u00adues as p; the scope of x is p.  !p behaves as the parallel \ncomposition of an unbounded number of copies of p; this construct, needed to model recursion, can be \neliminated with recursive process de.nitions.  A formal operational semantics can be found in (Milner \net al. 1992). Of particular interest are the following reduction rule for communication: a(x).p | a(b).p \n' -. p{b/x}| p ' and the following structural rule for scope extrusion: a is fresh in p p | (.a) p ' \n= (.a)(p | p ' ) The former rule models the communication of a name b on a channel a, from an output \nprocess to an input process (in parallel); b is substituted for x in the remaining input process. The \nlatter rule models the extrusion of the scope of a fresh name a across a parallel composition. These \nrules allow other derivations, such as the following for communication of fresh names: b is fresh in \na(x).p a(x).p | (.b) a(b).p ' = (.b)(p{b/x}| p ' ) 4.1 A p-calculus model of the abstract state machine \nbound to input or output actions on c. Each synchronizer is identi.ed with a fresh name s, on which it \naccepts messages from points in its domain. Informally, the following sequence of messages are exchanged \nin any round of the protocol. A point p (at state p . c or p . c) begins by sending a [c]; message \nto c on its respective input or output name i[c] or othe message contains a fresh name candidate[p] on \nwhich p expects a reply from c.  When c (at state 8c) gets a pair of messages on i[c] and o[c], say \nfrom p and another point q, it replies by sending messages on candidate[p] and candidate[q] (reaching \nstate .c(p, q) |8c); these messages contain fresh names decision[p] and decision[q] on which c expects \nreplies from the synchroniz\u00aders for p and q.  On receiving a message from c on candidate[p], p (reaching \nstate .p) tags the message with its name and forwards it to its synchronizer on the name s.  If p is \nthe .rst point to send such a message on s (that is, s is at state s), a pair of fresh names (con.rm[p], \ncancel[p]) is sent back on decision[p] (reaching state .s(p) | s); for each sub\u00adsequent message accepted \non s, say from p ', a blank message is sent back on decision[p ' ] (reaching state \u00d7(p ' ) | s).  On \nreceiving messages from the respective synchronizers of p and q on decision[p] and decision[q], c inspects \nthe messages and responds.  If both (con.rm[p] , ) and (con.rm[q] , ) have come in, signals are sent \nback on con.rm[p] and con.rm[q].  If only ( , cancel[p]) has come in (and the other message is blank), \na signal is sent back on cancel[p]; likewise, if only ( , cancel[q]) has come in, a signal is sent back \non cancel[q].  .. If s gets a signal on con.rm[p] (reaching state s (p)), it signals on p to continue. \nIf s gets a signal on cancel[p] (reaching state .. s), it reboots with fresh names for the points in \nits domain, so that those points can begin another round. Below we formalize this interpretation of states \nas (recursively de.ned) processes. For convenience, we let the interpreted states carry some auxiliary \nstate variables in [...]; these state variables represent names that are created at run time. The state \nvariables carried by any state are unique to that state. Thus, they do not convey any new, distinguishing \ninformation about that state. For simplicity, we leave states of the form a uninterpreted, and consider \nthem inactive. We de.ne a as shorthand for i[c] if a is of the form c, and o[c] if a is of the form c. \nPrograms in the source language are now compiled to pro\u00adcesses in the p-calculus. Suppose that the set \nof channels in a pro\u00adgram .k.1..mSk is C. We compile this program to the process (.c.C i[c], o[c]) (.c.C \n8c | .k.1..m Sk), where 8 >>>< >>>: a if S = a We interpret states of our machine as p-calculus processes \nthat run -. - . pi ) ai), at points, channels, and synchronizers. These processes reduce by if S = select( \n(.s, S communication to simulate transitions in the abstract state machine. In this setting: ( s | .i.1..n(pi \n. ai)[s, a i]) i . 1..n, and . -pi are fresh names s,  Interpretation of states as processes States \nof a point (p . c)[s, i[c]](. candidate[p]) i[c](candidate[p]). candidate[p](decision[p]). .p[decision[p], \ns, c] (q . c)[s, o[c]](. candidate[q]) o[c](candidate[q]). candidate[q](decision[q]). .q[decision[q], \ns, c] .p[decision[p], s, a]s(p, decision[p]).p(). a States of a channel [c] 8c[i[c], o]i[c](candidate[p]). \no[c](candidate[q]). ((.decision[p], decision[q]) candidate[p](decision[p]). candidate[q](decision[q]). \n.c (p, q)[decision[p] , decision[q]|8c [i[c] , o[c]]) ] .c(p, q)[decision[p] , decision[q]](decision[p](con.rm[p], \ncancel[p]). (decision[q](con.rm[q], cancel[q]). con.rm[p](). con.rm[q](). 0 | decision[q](). cancel[p](). \n0) | decision[p](). (decision[q](con.rm[q] , cancel[q]). cancel[q](). 0 | decision[q](). 0)) States of \na synchronizer s s(p, decision[p]). (.s(p)[decision[p]] | s) s s(p, decision[p]). (\u00d7(p)[decision[p]] \n| s) .s(p)[decision[p]](. con.rm[p] , cancel[p]) decision[p](con.rm[p] , cancel[p]). .. (con.rm[p](). \ns (p) .. | cancel[p](). s) \u00d7s(p)[decision[p]]decision[p](). 0 .. s (p) p(). 0 .. - . s (.s, pi )( s | \n.i.1..n(pi . ai)[s, a i]) . where dom(s)= {- pi }, i . 1..n, and .i . 1..n. s(pi)= ai Let . be a partial \nfunction from processes to states that, for any state s, maps its interpretation as a process back to \ns. For any process p such that . p is de.ned, we de.ne its denotation 'p' to be '. p'; the denotation \nof any other process is unde.ned. We then prove the following theorem (Chaudhuri 2009), closely following \nthe proof of Theorem 3.1. THEOREM 4.1 (Correctness of the p-calculus implementation). Let C be the set \nof channels in a program .k.1..mSk. Then [c][c] .k.1..mSk (.c.C i, o) (.c.C 8c | .k.1..m Sk) where \n is the largest relation such that P p iff (Invariant) p -. * p ' for some p ' such that 'P' = 'p ''; \n(Safety) if p -. p ' for some p ', then P -. * P ' for some P ' such that P ' p ' ; p '' (Progress) \nif P -. , then p -.+ and P -.P for some p ' and P ' such that P ' p ' ; (Fairness) if P -.P ' for some \nP ', then p0 -. ... -. pn for some p0,...,pn such that pn = p, P pi for all 0 = i<n, p '' and p0 -.+ \nfor some p ' such that P p ' . 5. A CML library in Concurrent Haskell We now proceed to code a full CML-style \nlibrary for events in a fragment of Concurrent Haskell with .rst-order message pass\u00ading (Peyton-Jones \net al. 1996). This fragment is close to the p\u00adcalculus, so we can simply lift our implementation of Section \n4.1. Going further, we remove the restrictions on the source language: a program can be any well-typed \nHaskell program. We implement not only receive, transmit, choose, and sync, but also new, guard, wrap, \nand wrapabort. Finally, we exploit Haskell s type system to show how events can be typed under the standard \nIO monad (Gordon 1994; Peyton-Jones and Wadler 1993). Before we proceed, let us brie.y review Concurrent \nHaskell s concurrency primitives. (The reader may wish to refer (Peyton-Jones et al. 1996) for details.) \nThese primitives support concurrent I/O computations, such as forking threads and communicating on mvars \nwhich are synchronized mutable variables, similar to p\u00adcalculus channels (see below). Note that MVar \nand IO are polymorphic type constructors in Concurrent Haskell, as follows: The type MVar tau is given \nto a communication cell that car\u00adries values of type tau.  The type IO tau is given to a computation \nthat yields results of type tau, with possible side effects via communication. We rely on the following \nsemantics of MVar cells.  A cell can carry at most one value at a time, that is, it is either empty \nor full.  The function newEmptyMVar :: IO (MVar tau) returns a fresh cell that is empty.  The function \ntakeMVar :: MVar tau -> IO tau is used to read from a cell; takeMVar m blocks if the cell m is empty, \nelse gets the content of m (thereby emptying it).  The function putMVar :: MVar tau -> tau -> IO () \nis used to write to a cell; putMVar m M blocks if the cell m is full, else puts the term M in m (thereby \n.lling it).  Further, we rely on the following semantics of IO computations; see (Peyton-Jones and Wadler \n1993) for details. The function forkIO :: IO () -> IO () is used to spawn a concurrent computation; forkIO \nf forks a thread that runs the computation f.  The function return :: tau -> IO tau is used to inject \na value into a computation.  Computations can be sequentially composed by piping . We use Haskell s \nconvenient do {...} notation for this purpose, instead of applying the underlying piping function  (>>=) \n:: IO tau -> (tau -> IO tau ) -> IO tau Thus, e.g., we write do {x <-takeMVar m; putMVar m x} instead \nof takeMVar m >>= \\x -> putMVar m x. Our library provides the following CML-style functions for pro\u00adgramming \nwith events in Concurrent Haskell.1 (Observe the differ\u00adences between ML and Haskell types for these \nfunctions. Since Haskell is purely functional, we must embed types for computa\u00adtions, with possible side-effects \nvia communication, within the IO monad. Further, since evaluation in Haskell is lazy, we can discard \n.-abstractions that simply delay eager evaluation.) new :: IO (channel tau) receive :: channel tau -> \nevent tau transmit :: channel tau -> tau -> event () guard :: IO (event tau) -> event tau wrap :: event \ntau -> (tau -> IO tau ) -> event tau choose :: [event tau] -> event tau wrapabort :: IO () -> event tau \n-> event tau sync :: event tau -> IO tau In this section, we focus on events that are built without wrapabort; \nthe full implementation appears in Section 6. 5.1 Type de.nitions We begin by de.ning the types of cells \non which messages are exchanged in our protocol (recall the discussion in Section 4.1).2 These cells \nare of the form i and o (on which points initially send messages to channels), candidate (on which channels \nre\u00adply back to points), s (on which points forward messages to syn\u00adchronizers), decision (on which synchronizers \ninform channels), confirm and cancel (on which channels reply back to synchro\u00adnizers), and p (on which \nsynchronizers .nally signal to points). type In = MVar Candidate type Out = MVar Candidate type Candidate \n= MVar Decision type Synchronizer = MVar (Point, Decision) type Decision = MVar (Maybe (Confirm, Cancel)) \ntype Confirm = MVar () type Cancel = MVar () type Point = MVar () Below, we use the following typings \nfor the various cells used in our protocol: i :: In, o :: Out, candidate :: Candidate, s :: Synchronizer, \ndecision :: Decision, confirm :: Confirm, cancel :: Cancel, and p :: Point. We now show code run by points, \nchannels, and synchronizers in our protocol. This code may be viewed as a typed version of the p-calculus \ncode in Section 4.1. 1 Instead of wrapabort, some implementations of CML provide the com\u00adbinator withnack. \nTheir expressive powers are exactly the same (Reppy 1999). Providing withnack is easier with an implementation \nstrategy that relies on negative acknowledgments. Since our implementation strategy does not rely on \nnegative acknowledgments, we stick with wrapabort. 2 In Haskell, the type Maybe tau is given to a value \nthat is either Nothing, or of the form Just v where v is of type tau. 5.2 Protocol code for points The \nprotocol code run by points abstracts on a cell s for the associ\u00adated synchronizer, and a name p for \nthe point itself. Depending on whether the point is for input or output, the code further abstracts on \nan input cell i or output cell o, and an associated action alpha. AtPointI :: Synchronizer -> Point -> \nIn -> IO tau -> IO tau AtPointI s p i alpha = do { candidate <-newEmptyMVar; putMVar i candidate; decision \n<-takeMVar candidate; putMVar s (p,decision); takeMVar p; alpha } AtPointO :: Synchronizer -> Point -> \nOut -> IO () -> IO () AtPointO s p o alpha = do { candidate <-newEmptyMVar; putMVar o candidate; decision \n<-takeMVar candidate; putMVar s (p,decision); takeMVar p; alpha } We instantiate the function AtPointI \nin the code for receive, and the function AtPointO in the code for transmit. These as\u00adsociate appropriate \npoint principals to any events constructed with receive and transmit. 5.3 Protocol code for channels \nThe protocol code run by channels abstracts on an input cell i and an output cell o for the channel. \nAtChan :: In -> Out -> IO () AtChanio= do{ candidate_i <-takeMVar i; candidate_o <-takeMVar o; forkIO \n(AtChan i o); decision_i <-newEmptyMVar; decision_o <-newEmptyMVar; putMVar candidate_i decision_i; putMVar \ncandidate_o decision_o; x_i <-takeMVar decision_i; x_o <-takeMVar decision_o; case (x_i,x_o) of (Nothing, \nNothing) -> return () (Just(_,cancel_i), Nothing) -> putMVar cancel_i () (Nothing, Just(_,cancel_o)) \n-> putMVar cancel_o () (Just(confirm_i,_), Just(confirm_o,_)) -> do { putMVar confirm_i (); putMVar \nconfirm_o () } } We instantiate this function in the code for new. This associates an appropriate channel \nprincipal to any channel created with new.  5.4 Protocol code for synchronizers The protocol code run \nby synchronizers abstracts on a cell s for that synchronizer and some rebooting code reboot, provided \nlater.  (We encode a loop with the function fix :: (tau -> tau) -> tau; the term fix f reduces to f \n(fix f).) AtSync :: Synchronizer -> IO () -> IO () AtSync s reboot = do { (p,decision) <-takeMVar s; \nforkIO (fix (\\iter -> do { (p ,decision ) <-takeMVar s; putMVar decision Nothing; iter } )); confirm \n<-newEmptyMVar; cancel <-newEmptyMVar; putMVar decision (Just (confirm,cancel)); forkIO (do { takeMVar \nconfirm; putMVar p () } ); takeMVar cancel; reboot } We instantiate this function in the code for sync. \nThis associates an appropriate synchronizer principal to any application of sync.  5.5 Translation of \ntypes Next, we translate types for channels and events. The Haskell types for ML channel and event values \nare: type channel tau = (In, Out, MVar tau) type event tau = Synchronizer -> IO tau An ML channel is \na Haskell MVar tagged with a pair of input and output cells. An ML event is a Haskell IO function that \nabstracts on a synchronizer cell.  5.6 Translation of functions We now translate functions for programming \nwith events. We begin by encoding the ML function for creating channels. new :: IO (channel tau) new \n=do{ i <-newEmptyMVar; o <-newEmptyMVar; forkIO (AtChan i o); m <-newEmptyMVar; return (i,o,m) } The \nterm new spawns an instance of AtChan with a fresh pair of input and output cells, and returns that pair \nalong with a fresh MVar cell that carries messages for the channel. Next, we encode the ML combinators \nfor building communi\u00adcation events. Recall that a Haskell event is an IO function that abstracts on the \ncell of its synchronizer. receive :: channel tau -> event tau receive (i,o,m) = \\s -> do { p <-newEmptyMVar; \nAtPointI s p i (takeMVar m) } transmit :: channel tau -> tau -> event () transmit (i,o,m) M = \\s -> do \n{ p <-newEmptyMVar; AtPointO s p o (putMVar m M) } The term receive c s runs an instance of AtPointI \nwith the synchronizer s, a fresh name for the point, the input cell for channel c, and an action that \ninputs on c.  The term transmit c M s is symmetric; it runs an instance of AtPointO with the synchronizer \ns, a fresh name for the point, the output cell for channel c, and an action that outputs term M on c. \n Next, we encode the ML combinators for specifying pre-and post-synchronization actions. guard :: IO \n(event tau) -> event tau guardf =\\s ->do{ v <-f; vs } wrap :: event tau -> (tau -> IO tau ) -> event \ntau wrapvf=\\s-> do{ x <-vs; fx } The term guard f s runs the computation f and passes the synchronizer \ns to the event returned by the computation.  The term wrap v f s passes the synchronizer s to the event \nv and pipes the returned value to function f.  Next, we encode the ML combinator for choosing among \na list of events. (We encode recursion over a list with the function foldM :: (tau -> tau -> IO tau ) \n-> tau -> [tau] -> IO tau . The term foldM f x [] reduces to return x, and the term foldM f x [v,V] reduces \nto do {x <-f x v; foldMf xV}.) choose :: [event tau] -> event tau chooseV=\\s-> do{ temp <-newEmptyMVar; \nfoldM (\\_ -> \\v -> forkIO (do { x <-v s; putMVar temp x })) () V; takeMVar temp } The term choose V \ns spawns a thread for each event v in V, passing the synchronizer s to v; any value returned by one of \nthese threads is collected in a fresh cell temp and returned. Finally, we encode the ML function for \nevent synchronization. sync :: event tau -> IO tau sync v = do { temp <-newEmptyMVar; forkIO (fix (\\iter \n-> do { s <-newEmptyMVar; forkIO (AtSync s iter); x<-vs; putMVar temp x } ) ); takeMVar temp } The term \nsync v recursively spawns an instance of AtSync with a fresh synchronizer s and passes s to the event \nv; any value returned by one of these instances is collected in a fresh cell temp and returned.  6. \nImplementation of wrapabort The implementation of the previous section does not account for wrapabort. \nWe now show how wrapabort can be handled by enriching the type for events. Recall that abort actions \nare spawned only at events that do not enclose the commit point. Therefore, in an encoding of wrapabort, \nit makes sense to name events with the sets of points they enclose. Note that such names may not be static. \nIn particular, for an event built with guard, we need to run the guard functions to compute the set of \npoints that such an event encloses. Thus, we do not name events at compile time. Instead, we introduce \nevents as principals in our protocol; each event is named in situ by com\u00adputing the list of points it \nencloses at run time. This list is carried on a fresh cell name :: Name for that event. type Name = MVar \n[Point] Further, each synchronizer carries a fresh cell abort :: Abort on which it accepts wrapabort \nfunctions from events, tagged with the list of points they enclose. type Abort = MVar ([Point], IO ()) \nThe protocol code run by points and channels remains the same. We only add a handler for wrapabort functions \nto the protocol code run by synchronizers. Accordingly, that code now abstracts on an abort cell. AtSync \n:: Synchronizer -> Abort -> IO () -> IO () AtSync s abort X = do { ...; forkIO (do { ...; fix (\\iter \n-> do { (P,f) <-takeMVar abort; forkIO iter; if (elem p P) then return () else f }) } ); ... } Now, after \nsignaling the commit point p to continue, the syn\u00adchronizer continues to accept abort code f on abort; \nsuch code is spawned only if the list of points P, enclosed by the event that sends that code, does not \ninclude p. The enriched Haskell type for event values is as follows. type event tau = Synchronizer -> \nName -> Abort -> IO tau Now, an ML event is a Haskell IO function that abstracts on a synchronizer, an \nabort cell, and a name cell that carries the list of points the event encloses. The Haskell function \nnew does not change. We highlight minor changes in the remaining translations. We begin with the functions \nreceive and transmit. An event built with either function is named by a singleton containing the name \nof the enclosed point. receive (i,o,m) = \\s -> \\name -> \\abort -> do { ...; forkIO (putMVar name [p]); \n... } transmit (i,o,m) M = \\s -> \\name -> \\abort -> do { ...; forkIO (putMVar name [p]); ... } In the \nfunction choose, a fresh name cell is passed to each event in the list of choices; the names of those \nevents are concate\u00adnated to name the choose event. choose V = \\s -> \\name -> \\abort -> do { ...; P <\u00ad \nfoldM (\\P -> \\v -> do { name <-newEmptyMVar; forkIO (do { x <-v s name abort; ... } ); P <-takeMVar \nname ; putMVar name P ; return (P ++ P) } )[] V; forkIO (putMVar name P); ... } We now encode the ML \ncombinator for specifying abort actions. wrapabort :: IO () -> event tau -> event tau wrapabort f v = \n\\s -> \\name -> \\abort -> do { forkIO (do { P <-takeMVar name; putMVar name P; putMVar abort (P,f) } \n); v s name abort } The term wrapabort f v s name abort spawns a thread that reads the list of enclosed \nevents P on the cell name and sends the function f along with P on the cell abort; the syn\u00adchronizer \ns is passed to the event v along with name and abort. The functions guard and wrap remain similar. guard \nf = \\s -> \\name -> \\abort -> do { v <-f; v s name abort } wrapvf=\\s-> \\name-> \\abort->do { x <-v s name \nabort; fx } Finally, in the function sync, a fresh abort cell is now passed to AtSync, and a fresh name \ncell is created for the event to be synchronized. sync v = do { ...; forkIO (fix (\\iter -> do { ...; \nname <-newEmptyMVar; abort <-newEmptyMVar; forkIO (AtSync s abort iter); x <-v s name abort; ... } )); \n... }  7. Implementation of communication guards Beyond the standard primitives, some implementations \nof CML further consider primitives for guarded communication. In par\u00adticular, Russell (2001) implements \nsuch primitives in Concurrent Haskell, but his implementation strategy is fairly specialized for example, \nit requires a notion of guarded events (see Section 9 for a discussion on this issue). We show that in \ncontrast, our implemen\u00adtation strategy can accommodate such primitives with little effort. Speci.cally, \nwe wish to support the following receive combi\u00adnator, that can carry a communication guard. receive :: \nchannel tau -> (tau -> Bool) -> event tau Intuitively, (receive c cond) synchronizes with (transmit c \nM) only if cond M is true. In our implementation, we make minor adjustments to the types of cells on \nwhich messages are exchanged between points and channels. type In tau = MVar (Candidate, tau -> Bool) \ntype Out tau = MVar (Candidate, tau) type Candidate = MVar (Maybe Decision) Next, we adjust the protocol \ncode run by points and channels. Input and output points bound to actions on c now send their conditions \nand messages to c. A pair of points is matched only if the message sent by one satis.es the condition \nsent by the other. AtChan :: In tau -> Out tau -> IO () AtChan io=do { (candidate_i,cond) <-takeMVar \ni; (candidate_o,M) <-takeMVar o; ...; if (cond M) then do { ...; putMVar candidate_i (Just decision_i); \nputMVar candidate_o (Just decision_o); ... } else do { putMVar candidate_i Nothing; putMVar candidate_i \nNothing } } AtPointI :: Synchronizer -> Point -> In tau -> (tau -> Bool) -> IO tau -> IO tau AtPointI \ns p i cond alpha = do { ...; putMVar i (candidate,cond); x <-takeMVar candidate; case x of Nothing -> \nAtPointI s p i cond alpha Just decision -> do { putMVar s (p,decision); ... } } AtPointO :: Synchronizer \n-> Point -> Out tau -> tau-> IO()-> IO() AtPointOspoMalpha= do{ ...; putMVar o (candidate,M); x <-takeMVar \ncandidate; case x of Nothing -> AtPointO s p o M alpha Just decision -> do { putMVar s (p,decision); \n... } } Finally, we make minor adjustments to the type constructor channel, and the functions receive \nand transmit. type channel tau = (In tau, Out tau, MVar tau) receive (i,o,m) cond = \\s -> \\name -> \\abort \n-> do { ...; AtPointI s p i cond (takeMVar m) } transmit (i,o,m) M = \\s -> \\name -> \\abort -> do { ...; \nAtPointO s p o M (putMVar m M) } 8. Evaluation Our implementation is derived from a formal model, constructed \nfor the purpose of proof (see Theorem 4.1). Not surprisingly, to simplify reasoning about the correctness \nof our code, we overlook several possible optimizations. For example, we heavily rely on lazy evaluation \nand garbage collection in the underlying language for reasonable performance of our code. It is plausible \nthat this per\u00adformance can be improved with explicit management. We also rely on fair scheduling in the \nunderlying language to prevent starvation. Nevertheless, preliminary experiments indicate that our code \nis already quite ef.cient. In particular, we compare the performance of our library against OCaml s Event \nmodule (Leroy et al. 2008). The implementation of this module is directly based on Reppy s original design \nof CML (Reppy 1999). Furthermore, it supports wrapabort, unlike recent versions of CML that favor an \nalterna\u00adtive primitive, withnack, which we do not support (see footnote 1, p.7). Finally, most other \nimplementations of CML-style prim\u00aditives do not re.ect the standard semantics (Reppy 1999), which makes \ncomparisons with them meaningless. Indeed, some of our benchmarks rely on the symmetry of choose see, \ne.g., the swap channel abstraction implemented below; such benchmarks cannot work correctly on a previous \nimplementation of events in Haskell (Russell 2001).3 For our experiments, we use several small benchmark \nprograms that rely heavily on higher-order concurrency. We describe these benchmarks below; their code \nis available online (Chaudhuri 2009). These benchmarks are duplicated in Haskell and OCaml to the extent \npossible. Furthermore, to minimize noise due to inherent differences in the implementations of these \nlanguages, we avoid the use of extraneous constructs in these benchmarks. Still, we cannot avoid the \nuse of threads, and thus our results may be skewed by differences in the implementations of threads in \nthese languages. We compile these benchmarks using ghc 6.8.1 and ocamlc 3.10.2 (using the -vmthread option \nin the latter). All these benchmarks run faster using our library than using OCaml s Event module. Our \nbenchmarks are variations of the following programs. Extended example Recall the example of Section 3.5. \nThis is a simple concurrent program that involves nondeterministic com\u00admunication; either there is communication \non channels x and z, or there is communication on channel y. To observe this nondeterminism, we add guard, \nwrap, and wrapabort func\u00adtions to each communication event, which print messages such 3 Nevertheless, \nwe did consider comparing Russell s implementation with ours on other benchmarks, but failed to compile \nhis implementation with recent versions of ghc; we also failed to .nd his contact information online. \n as \"Trying\", \"Succeeded\", and \"Failed\" for that event at run time. Both the Haskell and the ML versions \nof the program exhibit this nondeterminism in our runs. Primes sieve This program uses the Sieve of Eratosthenes \n(Wikipedia 2009) to print all prime numbers up to some n = 2. We implement two versions of this program: \n(I) uses choose, (II) does not. (I) In this version, we create a prime channel and a not prime channel \nfor each i . 2..n, for a total of 2 * (n - 1) channels. Next, we spawn a thread for each i . 2..n, that \nselects between two events: one receiving on the prime channel for i and printing i, the other receiving \non the not prime channel for i and looping. Now, for each multiple j = n of each i . 2..n, we send on \nthe not prime channel for j. Finally, we spawn a thread for each i . 2..n, sending on the prime channel \nfor i. (II) In this version, we create a prime/not prime channel for each i . 2..n, for a total of n \n- 1 channels. Next, we spawn a thread for each i . 2..n, receiving a message on the prime/not prime channel \nfor i, and printing i if the message is true or looping if the message is false. Now, for each multiple \nj = n of each i . 2..n, we send false on the prime/not prime channel for j. Finally, we spawn a thread \nfor each i . 2..n, sending true on the prime/not prime channel for i.  Swap channels This program implements \nand uses a swap channel abstraction, as described in (Reppy 1994). Intuitively, if x is a swap channel, \nand we run the program forkIO (do {y <-sync (swap x M); ...}); do {y <-sync (swap x M ); ...} then M \nis substituted for y and M is substituted for y in the continuation code (elided by ...s). type swapChannel \ntau = channel (tau, channel tau) swap :: swapChannel tau -> tau -> event tau swap ch msgOut = guard (do \n{ inCh <-new; choose [ wrap (receive ch) (\\x -> let (msgIn, outCh) = x in do { sync (transmit outCh \nmsgOut); return msgIn } ), wrap (transmit ch (msgOut, inCh)) (\\_ -> sync (receive inCh)) ] }) Communication \nover a swap channel is already highly nonde\u00adterministic, since one of the ends must choose to send its \nmes\u00adsage .rst (and accept the message from the other end later), while the other end must make exactly \nthe opposite choice. We add further nondeterminism by spawning multiple pairs of swap on the same swap \nchannel. Buffered channels This program implements and uses a buffered channel abstraction, as described \nin (Reppy 1992). Intuitively, a buffered channel maintains a queue of messages, and chooses between receiving \na message and adding it to the queue, or removing a message from the queue and sending it. Our library \nperforms signi.cantly better for all except one of these benchmarks for the swap channels benchmark, \nthe differ\u00ad ence is only marginal. Note that in this case, our protocol possi\u00adbly wastes some rounds \nby matching points that have the same synchronizer (and eventually canceling these matches, since such \npoints can never be selected together). An optimization that elimi\u00adnates such matches altogether should \nimprove the performance of our implementation. Beyond total running times, it should also be interesting \nto com\u00adpare performance relative to each CML-style primitive, to pinpoint other possible sources of inef.ciency. \nWe defer a more detailed investigation of these issues, as well as a more robust account of implementation \ndifferences between the underlying languages (es\u00adpecially those of threads), to future work. All the \ncode that appears in this paper is available online at: http://code.haskell.org/cml/ Additional resources \non this project are available at (Chaudhuri 2009; Chaudhuri and Franksen 2009). 9. Related work We are \nnot the .rst to implement CML-style concurrency prim\u00aditives in another language. In particular, Russell \n(2001) presents an implementation of events in Concurrent Haskell. The imple\u00admentation provides guarded \nchannels, which .lter communication based on conditions on message values (as in Section 7). Unfortu\u00adnately, \nthe implementation requires a rather complex Haskell type for event values. In particular, a value of \ntype event tau needs to carry a higher-order function that manipulates a continuation of type IOtau->IO \n(). Further, a critical weakness of Russell s implementation is that the choose combinator is asymmetric. \nAs observed in (Reppy and Xiao 2008), this restriction is necessary for the correctness of that implementation. \nIn contrast, we implement a (more expressive) symmetric choose combinator, following the standard CML \nsemantics. Finally, we should point out that Rus\u00adsell s CML library is more than 1300 lines of Haskell \ncode, while ours is less than 150. Yet, guarded communication as proposed by Russell is already implemented \nin our setting, as shown in Sec\u00adtion 7. In the end, we believe that this difference in complexity is \ndue to the clean design of our synchronization protocol. Independently of our work, Reppy and Xiao (2008) \nrecently pursue a parallel implementation of a subset of CML, with a dis\u00adtributed protocol for synchronization. \nAs in (Reppy 1999), this im\u00adplementation builds on ML machinery such as continuations, and further relies \non a compare-and-swap instruction. Unfortunately, their choose combinator cannot select among transmit \nevents, that is, their subset of CML cannot express selective communica\u00adtion with transmit events. It \nis not clear whether their implemen\u00adtation can be extended to account for the full power of choose. Orthogonally, \nDonnelly and Fluet (2006) introduce transac\u00adtional events and implement them over the software transactional \nmemory (STM) module in Concurrent Haskell. More recently, Ef.nger-Dean et al. (2008) implement transactional \nevents in ML. Combining all-or-nothing transactions with CML-style concur\u00adrency primitives is attractive, \nsince it recovers a monad. Unfortu\u00adnately, implementing transactional events requires solving NP-hard \nproblems (Donnelly and Fluet 2006), and these problems seem to interfere even with their implementation \nof the core CML-style concurrency primitives. In contrast, our implementation of those primitives remains \nrather lightweight. Other related implementations of events include those of Flatt and Findler (2004) \nin Scheme and of Demaine (1998) in Java. Flatt and Findler provide support for kill-safe abstractions, \nextending the semantics of some of the CML-style primitives. On the other hand, Demaine focuses on ef.ciency \nby exploiting communication patterns that involve either single receivers or single transmitters. It \nis unclear whether Demaine s implementation of non-deterministic communication can accommodate event \ncombinators.  Distributed protocols for implementing selective communica\u00adtion date back to the 1980s. \nThe protocols of Buckley and Silber\u00adschatz (1983) and Bagrodia (1986) seem to be among the earliest in \nthis line of work. Unfortunately, those protocols are prone to dead\u00adlock. Bornat (1986) proposes a protocol \nthat is deadlock-free as\u00adsuming communication between single receivers and single trans\u00admitters. Finally, \nKnabe (1992) presents the .rst deadlock-free pro\u00adtocol to implement selective communication for arbitrary \nchannel communication. Knabe s protocol appears to be the closest to ours. Channels act as locations \nof control, and messages are exchanged between communication points and channels to negotiate synchro\u00adnization. \nHowever, Knabe assumes a global ordering on processes and maintains queues for matching communication \npoints; we do not require either of these facilities in our protocol. Furthermore, as in (Demaine 1998), \nit is unclear whether the protocol can accom\u00admodate event combinators. Finally, our work should not be \nconfused with Sangiorgi s trans\u00adlation of the higher-order p-calculus (HOp) to the p-calculus (San\u00adgiorgi \n1993). While HOp allows processes to be passed as values, it does not immediately support higher-order \nconcurrency. For in\u00adstance, processes cannot be modularly composed in HOp. On the other hand, it may \nbe possible to show alternate encodings of the process-passing primitives of HOp in p-like languages, \nvia an in\u00adtermediate encoding with CML-style primitives. 10. Conclusion In this paper, we show how to \nimplement higher-order concurrency in the p-calculus, and thereby, how to encode CML s concurrency primitives \nin Concurrent Haskell, a language with .rst-order mes\u00adsage passing. We appear to be the .rst to implement \nthe standard CML semantics for event combinators in this setting. An interesting consequence of our work \nis that implementing selective communication ` a la CML on distributed machines is re\u00adduced to implementing \n.rst-order message passing on such ma\u00adchines. This clari.es a doubt raised in (Peyton-Jones et al. 1996). \nAt the heart of our implementation is a new, deadlock-free pro\u00adtocol that is run among communication \npoints, channels, and syn\u00adchronization applications. This protocol seems to be robust enough to allow \nimplementations of sophisticated synchronization primi\u00adtives, even beyond those of CML. Acknowledgments \nThanks to Cormac Flanagan for suggesting this project for his Spring 2007 Concurrent Programming course \nat UC Santa Cruz. Thanks to Mart\u00b4in Abadi, Jeff Foster, and several anonymous referees of Haskell 07 \nand ICFP 09 for their comments on this paper. Finally, thanks to Ben Franksen for maintaining the source \npackage for this library at HackageDB. This work was supported in part by NSF under grants CCR-0208800 \nand CCF\u00ad0524078, and by DARPA under grant ODOD.HR00110810073. References R. Bagrodia. A distributed algorithm \nto implement the general\u00adized alternative command of CSP. In ICDCS 86: International Conference on Distributed \nComputing Systems, pages 422 427. IEEE, 1986. R. Bornat. A protocol for generalized Occam. Software Practice \nand Experience, 16(9):783 799, 1986. ISSN 0038-0644. G. N. Buckley and A. Silberschatz. An effective \nimplementation for the generalized input-output construct of CSP. ACM Trans\u00adactions on Programming Languages \nand Systems, 5(2):223 235, 1983. ISSN 0164-0925. A. Chaudhuri. A Concurrent ML library in Concurrent \nHaskell, 2009. Links to proofs and experiments at http://www.cs. umd.edu/~avik/projects/cmllch/. Avik \nChaudhuri and Benjamin Franksen. Hackagedb cml pack\u00adage, 2009. Available at http://hackage.haskell.org/ \ncgi-bin/hackage-scripts/package/cml. E. D. Demaine. Protocols for non-deterministic communication over \nsynchronous channels. In IPPS/SPDP 98: Symposium on Parallel and Distributed Processing, pages 24 30. \nIEEE, 1998. K. Donnelly and M. Fluet. Transactional events. In ICFP 06: International Conference on Functional \nProgramming, pages 124 135. ACM, 2006. L. Ef.nger-Dean, M. Kehrt, and D. Grossman. Transactional events \nfor ML. In ICFP 08: International Conference on Functional Programming, pages 103 114. ACM, 2008. M. \nFlatt and R. B. Findler. Kill-safe synchronization abstractions. In PLDI 04: Programming Language Design \nand Implementa\u00adtion, pages 47 58. ACM, 2004. ISBN 1-58113-807-5. A. D. Gordon. Functional programming \nand Input/Output. Cam\u00adbridge University, 1994. ISBN 0-521-47103-6. C. A. R. Hoare. Communicating sequential \nprocesses. Communi\u00adcations of the ACM, 21(8):666 677, 1978. F. Knabe. A distributed protocol for channel-based \ncommunica\u00adtion with choice. In PARLE 92: Parallel Architectures and Lan\u00adguages, Europe, pages 947 948. \nSpringer, 1992. ISBN 3-540\u00ad55599-4. X. Leroy, D. Doligez, J. Garrigue, D. R\u00b4and J. Vouil\u00ad emy, lon. The \nObjective Caml system documentation: Event mod\u00adule, 2008. Available at http://caml.inria.fr/pub/docs/ \nmanual-ocaml/libref/Event.html. R. Milner, J. Parrow, and D. Walker. A calculus of mobile pro\u00adcesses, \nparts I and II. Information and Computation, 100(1): 1 77, 1992. S. L. Peyton-Jones and P. Wadler. Imperative \nfunctional program\u00adming. In POPL 93: Principles of Programming Languages, pages 71 84. ACM, 1993. S. \nL. Peyton-Jones, A. D. Gordon, and S. Finne. Concurrent Haskell. In POPL 96: Principles of Programming \nLanguages, pages 295 308. ACM, 1996. J. H. Reppy. Concurrent programming in ML. Cambridge Univer\u00adsity, \n1999. ISBN 0-521-48089-2. J. H. Reppy. Higher-order concurrency. PhD thesis, Cornell University, 1992. \nTechnical Report 92-1852. J. H. Reppy. First-class synchronous operations. In TPPP 94: Theory and Practice \nof Parallel Programming. Springer, 1994. J. H. Reppy and Y. Xiao. Towards a parallel implementation of \nConcurrent ML. In DAMP 08: Declarative Aspects of Multicore Programming. ACM, 2008. G. Russell. Events \nin Haskell, and how to implement them. In ICFP 01: International Conference on Functional Program\u00adming, \npages 157 168. ACM, 2001. ISBN 1-58113-415-0. D. Sangiorgi. From pi-calculus to higher-order pi-calculus, \nand back. In TAPSOFT 93: Theory and Practice of Software Devel\u00adopment, pages 151 166. Springer, 1993. \nWikipedia. Sieve of Eratosthenes, 2009. See http://en. wikipedia.org/wiki/Sieve_of_Eratosthenes.   \n \n\t\t\t", "proc_id": "1596550", "abstract": "<p>In Concurrent ML, synchronization abstractions can be defined and passed as values, much like functions in ML. This mechanism admits a powerful, modular style of concurrent programming, called <i>higher-order concurrent programming</i>. Unfortunately, it is not clear whether this style of programming is possible in languages such as Concurrent Haskell, that support only first-order message passing. Indeed, the implementation of synchronization abstractions in Concurrent ML relies on fairly low-level, language-specific details. In this paper we show, constructively, that synchronization abstractions can be supported in a language that supports only first-order message passing. Specifically, we implement a library that makes Concurrent ML-style programming possible in Concurrent Haskell. We begin with a core, formal implementation of synchronization abstractions in the &#960;-calculus. Then, we extend this implementation to encode all of Concurrent ML's concurrency primitives (and more!) in Concurrent Haskell. Our implementation is surprisingly efficient, even without possible optimizations. In several small, informal experiments, our library seems to outperform OCaml's standard library of Concurrent ML-style primitives. At the heart of our implementation is a new distributed synchronization protocol that we prove correct. Unlike several previous translations of synchronization abstractions in concurrent languages, we remain faithful to the standard semantics for Concurrent ML's concurrency primitives. For example, we retain the symmetry of choose, which can express selective communication. As a corollary, we establish that implementing selective communication on distributed machines is no harder than implementing first-order message passing on such machines.</p>", "authors": [{"name": "Avik Chaudhuri", "author_profile_id": "81309507612", "affiliation": "University of Maryland, College Park, College Park, MD, USA", "person_id": "P1613943", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1596550.1596589", "year": "2009", "article_id": "1596589", "conference": "ICFP", "title": "A concurrent ML library in concurrent Haskell", "url": "http://dl.acm.org/citation.cfm?id=1596589"}