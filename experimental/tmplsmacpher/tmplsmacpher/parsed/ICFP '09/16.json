{"article_publication_date": "08-31-2009", "fulltext": "\n Experience Report: Haskell in the Real World Writing a Commercial Application in a Lazy Functional Language \nCurt J. Sampson Starling Software, Tokyo, Japan cjs@starling-software.com Abstract I describe the initial \nattempt of experienced business software developers with minimal functional programming background to \nwrite a non-trivial, business-critical application entirely in Haskell. Some parts of the application \ndomain are well suited to a mathematically-oriented language; others are more typically done in languages \nsuch as C++. I discuss the advantages and dif.culties of Haskell in these circumstances, with a particular \nfocus on issues that commercial developers .nd important but that may receive less attention from the \nacademic community. I conclude that, while academic implementations of advanced programming languages \narguably may lag somewhat behind imple\u00admentations of commercial languages in certain ways important to \nbusinesses, this appears relatively easy to .x, and that the other ad\u00advantages that they offer make them \na good, albeit long-term, invest\u00adment for companies where effective IT implementation can offer a crucial \nadvantage to success. Categories and Subject Descriptors D.1.1 [Programming Tech\u00adniques]: Applicative \n(Functional) Programming; D.2.3 [General]: Coding Tools and Techniques; D.3.2 [Programming Languages]: \nLanguage Classi.cations Applicative (functional) Languages General Terms Experimentation, Human Factors, \nLanguages, Performance Keywords functional programming, Haskell, commercial pro\u00adgramming, .nancial systems \n1. Introduction 1.1 Background and Beginnings We1 are reasonably smart but not exceptional programmers \nwith several decades of professional experience amongst us. Our main working languages were C in the \n1990s, Java in the very late 1990s and the .rst part of the 2000s, and Ruby(7) after that. Other developers \non the team have experience in similar languages (such 1 Despite this paper having one author, I am reporting \non the combined experiences of a multi-person developer and customer team. Thus, I use we throughout, \nexcept when relating distinctly personal experiences. Permission to make digital or hard copies of all \nor part of this work for personal or classroom use is granted without fee provided that copies are not \nmade or distributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. ICFP 09, August 31 September 2, 2009, Edinburgh, Scotland, UK. \nCopyright c &#38;#169; 2009 ACM 978-1-60558-332-7/09/08. . . $5.00 as C# and Python). When we embarked \non this project, we had no signi.cant functional programming experience. In the mid-2000s a fellow programmer \nextolled to me the virtues of Scheme and, following up on this, I found that people such as Paul Graham \nwere also making convincing arguments (in essays such as Beating the Averages 2(3)) that functional programming \nwould increase productivity and reduce errors. Working through The Little Schemer(1) I was impressed \nby the concision and adapt\u00adability of what was admittedly toy code. Upon founding a software development \ncompany soon after this, my co-founder and I agreed that pursuing functional programming was likely to \nbe bene.cial, and thus we should learn a functional language and develop a soft\u00adware project in it. There \nthings sat for about two years, until an opportunity arose. In the spring of 2008 we were approached \nby a potential client to write an .nancial application. He had previous experience working on a similar \napplication in Java, and approached us with the idea of doing something similar.  1.2 Selling the Client \nWe felt that the client s application domain could bene.t from using a more powerful language, and we \nsaw this as an opportunity to explore functional programming. Convincing the client to do this took some \nwork, especially since we had no real experience with functional languages at that point. We took a two-pronged \napproach to selling functional programming for this job. First, we explained that we had signi.cant programming \nexperi\u00adence in more traditional languages, especially Java. We pointed out that this made us familiar \nwith the limitations of these languages: so familiar, in fact, that in the case of Java we had already \nmoved away from the language to escape those very limitations. We explained that, for us, switching languages \nevery few years was a normal tech\u00adnology upgrade (much as any other industry will incorporate new advancements \nin the state of the art) and we had previous experi\u00adence with working through these sorts of changes. \nSecond, we argued that we had been for some time looking to move on to a new language, had done signi.cant \nresearch in that direction, and had already identi.ed many of the speci.c things needed to make the change \nsuccessful and pro.table. As well as general promises of better productivity and fewer bugs, we pointed \nto speci.c features we wanted in a new language for which the client himself was looking. In particular, \nhe had expressed a preference for a system where he would have the ability to examine and modify the \n.nancial algorithms himself; we explained and demonstrated that the languages we were looking at offered \nmuch 2 Particularly fascinating to us was, In business, there is nothing more valuable than a technical \nadvantage your competitors don t understand. We are still considering the importance of ourselves understanding \nthe technical advantages we ve gained.  more concision and a more mathematical notation than Java, and \nshowed how this would enable him to more easily participate in the programming process. 2. Selection \nof Language and Tools 2.1 Desired Language Features The world of functional programming offers a wide, \neven bewilder\u00ading variety of features and technologies, spread across many differ\u00adent languages. In some \ncases a language has has strong support for an imperative or object-oriented coding style, with merely \nthe pos\u00adsibility of doing functional programming (such as JavaScript); in other cases one must abandon \nwholesale one s non-functional style and adopt a completely different one. As we had already extensive \nexperience with Java and Ruby and had become unhappy with the level of expressive power that either offered, \nwe chose to pursue what we felt were more advanced language features, at the cost of an increased learning \ncurve and greater risk. Functional language features of particular interest to us were a sophisticated \ntype system (Hindley-Milner is the obvious exam\u00adple here), and advanced, modular structures for control \n.ow. More generally, we were looking for minimal syntax, concise yet expres\u00adsive code, powerful abstractions, \nand facilities that would allow us easily to use parallelism. The concise yet expressive code requirement \nwas particularly important to us, both because we ve found that reading and un\u00adderstanding existing code \nis a substantial part of the programming process, and because we wanted to be able to develop code that, \nat least for the parts in his domain, our client could read and perhaps modify. 2.2 Considerations of \nCommercial Software Developers As commercial software developers, there were also several other factors \nin.uencing our selection of tools. A primary criterion is reliability: bugs in the toolchain or run\u00adtime \nsystem can derail a project irrecoverably, and may not appear until well into development. Good support \ncan help to mitigate these problems, but that still provides no guarantee that any partic\u00adular issue \ncan be solved in a timely manner. Generally, widespread use on a particular platform provides the best \nhope that most serious problems will have already been encountered and debugged. For developers with \nsuf.cient technical knowledge, having the source for the system can also provide the ability to debug \nproblems in more detail than the systems supporters are perhaps willing to do, increasing the chances \nof .nding a solution, even if the solution is implemented by someone else. The compiler or interpreter \nis only one part of a developer s environment; just as important are the methods of code storage (.at \n.les or otherwise), source code control systems, editors, build tools (such as make), testing frameworks, \npro.lers, and other tools, many of these often home-grown. These represent a substantial investment in \nboth learning and development time, and are usually important to the success of a project. Thus, being \nable to use a new language implementation with existing and familiar tools is a huge bene.t. If the implementation \nis very nearly a drop-in replacement for a language implementation already in use (as with GHC or Objective \nCaml for GCC in typical Unix development projects, or F# in .NET environments), much of the previously-built \ndevelopment environment can be reused. If, on the other hand, everything needs to be replaced (as with \nsome Smalltalk environments, or Lisp machines), this imposes a tremendously increased burden. For many \ncommercial developers, integration with other code running on the platform (such as C libraries) and \ncomplete access to the platform facilities themselves (system calls and so on) is important. At times \nbeing able to do things such as set operating\u00adsystem-speci.c socket options for individual network connections \ncan make the difference between an application working well or poorly. In some cases, applications need \nto be able to control memory layout and alignment in order to marshal speci.c data structures used by \nthe system or external libraries. In our case performance was an important consideration. The application \nreads and parses market data messages that arrive on a network connection and responds with orders. The \nresponse must occur within a few dozen milliseconds for the system to work reasonably well, and the faster \nthe response, the greater the chance the system s algorithms will be able to make pro.table trades. Finally, \nthough this is not true of all commercial developers, we prefer to use a free, open source implementation \nof a language, rather than commercial tools. There are two reasons for this beyond the purchase cost. \nFirst, we ve found that commercial support for proprietary software is not signi.cantly better, and is \noften worse, than community support for open source software under active development. Second, the access \nto source code and ability to rebuild it can be extremely helpful when it comes to debugging problems \n(the burden of which usually falls on the customer). Third, clients often feel more comfortable with \nopen source when using niche products (as most functional language platforms are) as it ensures that \nthey can have continued access to the tools needed to build their system, as well as the system itself. \n 2.3 Languages Examined With the above criteria in mind, we looked at some of the more pop\u00adular free \nfunctional language implementations available. Our abil\u00adity to compare these was limited: we simply didn \nt know enough about functional languages in general, and had no extensive experi\u00adence with any functional \nlanguage implementation. Thus, we were forced to rely on what we could learn from textbooks and infor\u00admation \non the web. (Blog entries documenting interesting features of the various languages were particularly \nin.uential.) Especially, we had no way of judging some of the more esoteric and complex language features \nas we simply didn t understand them. This no doubt skewed our evaluation process, but we saw no reasonable \nway of dealing with this other than spending several months building a non-trivial test application in \neach of several different languages. This point should be kept in mind by language promoters: the information \nyou need to communicate to convince non-functional-programmers to try a language is of a substantially \ndifferent sort than what sways those who are already familiar with at least one functional programming \nlanguage. We looked most seriously at the Glasgow Haskell Compiler(2), Objective Caml(6), and Scala(8). \n(We considered looking at LISP and Scheme implementations, but these seemed to be both lack\u00ading in certain \nlanguage features and we had (what are in retrospect perhaps undeserved) concerns about the available \nfree implemen\u00adtations.) All three of these language implementations we examined share some features in \ncommon: expressive static type systems;  concise code, and syntax suited to building embedded domain\u00adspeci.c \nlanguages without macros;  compilers known to be fairly reliable and produce reasonably fast code; \n tools that .t with our current Unix development systems;  the ability to interface with code compiled \nfrom other lan\u00adguages; and  an open source implementation under active development.   2.3.1 The Glasgow \nHaskell Compiler Haskell was the language that most appealed to us; it seemed to have the most advanced \nset of features of any language out there. Especially, being able easily to guarantee the purity of parts \nof the code promised to make testing easier, which is an important point for us as we rely heavily on \nautomated testing. The other signi.cantly different language feature of Haskell, lazy evaluation, had \nmild appeal for potential performance bene\u00ad.ts, but we really had no idea what the true rami.cations \nof lazy evaluation were. The extensive use of monads was interesting to us, but, as with lazy evaluation, \nwe didn t know enough about it to have any sort of informed opinion. We simply believed what we d read \nthat monads were a good thing. There were things about Haskell we found easier to understand. Type classes \nwe found relatively simple, and they seemed to offer more .exibility than the standard object-oriented \ninheritance-based approach. Two things we could understand very well about Haskell were that it was relatively \npopular, with a strong community, and that the Foreign Function Interface offered us an escape into other \nlanguages should we encounter any overwhelming problems, per\u00adformance or otherwise. The books available \nfor learning Haskell, though at times mys\u00adtifying, seemed to provide the promise of wonderful things. \nThe Haskell School of Expression(4), in particular, we found fascinat\u00ading. There are several implementations \nof Haskell available; we chose the Glasgow Haskell Compiler(2) as that is widely recog\u00adnized as the most \nmature Haskell compiler available. 2.3.2 Objective Caml To us, Objective Caml(6) appeared to be a more \nconservative al\u00adternative to GHC, offering many of GHC s and Haskell s features, but without being so \nradical as to introduce enforced purity or lazy evaluation. Advantages particular to the implementation \nwere that it was well known, mature, reputedly produced very fast code, and we knew it to be used for \nlarge amounts of production code by other companies in the .nancial community. OCaml did have several \nthings that put us off. The language syntax didn t seem nearly as clean as Haskell, which was partially \ninherent (such as double-semicolons to end declarations) and partly due to the inability to overload \nfunctions (due to the lack of type classes). As well, the books we had available were just not as good. \nAt that time we d just .nished reading a fairly recent book on OCaml that, unlike the Haskell books we \nd read, did not show us any impressive new programming techniques but instead appeared to treat the language \nin a very imperative fashion. 2.3.3 Scala We considered Scala(8) mainly because we thought we might \nhave to use Java libraries. This turned out not to be the case, and as we otherwise preferred to run \nnative code, we didn t investigate the language very deeply.  2.4 Selection of Haskell After examining \nthe three options above, we chose the Glasgow Haskell Compiler, version 6.8, as our development platform. \nWe felt that GHC offered the following advantages: Haskell appeared to be a more powerful and more interesting \nlanguage than Objective Caml.  The Haskell community offered good support and was (and still is) growing. \nThere were several good books available  with more due to appear, blog entries describing interesting \nuses of Haskell and various techniques were plentiful, and the #haskell channel on IRC was particularly \nresponsive to ques\u00adtions. The compiler, while perhaps at the time not as good as the Objective Caml compiler, \nappeared to be under more active development. 3. Successes and Advantages 3.1 Concise and Readable Code \nAfter a week or two of adjustment, we found Haskell syntax to be remarkably clean and expressive. An \ninitial test was to work with the client to code some of the mathematical algorithms used by the trading \nsystem; many were straightforward translations of the mathematical equations into similar Haskell syntax. \nThis obvious advantage of Haskell proved a particularly good selling point to the client early on in \nthe process, reinforcing the idea that the client, though not a programmer, would be able easily to understand \nsome of the more important domain-related aspects of his application. Another early area of work was \nthe parser for the market data feed. Initially we used Parsec for this, and were quite impressed by how \nexpressive Haskell and a combinator-based approach can be. However, we soon decided to experiment with \nwriting our own parser, for two reasons. First, Parsec did not allow us to keep cer\u00adtain items of state \nthat we felt we needed for error-checking and re\u00adcovery purposes. Second, the version of Parsec that \nwe were using at the time used String rather than ByteString, and some brief pro.ling experiments showed \nthat the performance advantages of ByteString were considerable.3 As it turned out, after some study \n(the Functional Parsers chapter of Hutton s Programming in Haskell(5) was particularly helpful here), \nwe found that parsers in Haskell are particularly easy to write. Learning about control structures beyond \nsimple recursion, par\u00adticularly monadic ones, took considerably more time, but also proved fertile ground \nfor .nding ways to improve our code s clarity and concision. After a year or so we are extremely happy \nwith our improved ability (over object-oriented languages, and well beyond just parsers) to build combinators, \nmanage state, and deal with complex control .ow through monadic code. To summarize, though the learning \neffort for the various tech\u00adniques available to us ranged from low (for mathematical formu\u00adlae) to moderately \nhigh (monadic control structures), there seemed to be almost no area in which Haskell code was not more \nclear and considerably more concise than doing the equivalent in Ruby or, particularly, Java.  3.2 The \nType System and Unit Tests It s not unusual, when writing in languages with little compile-time type-checking \nsuch as Ruby, for about a third of our code to be unit tests. We had anticipated that we might write \nfewer unit tests in Haskell, but the extent to which the type system reduced the need for unit tests \nsurprised us. As a comparison, in reasonably complex Ruby application of similar line count (though rather \nless 3 ByteString is, I think, from an academic point of view a rather trivial optimization of some fairly \nordinary routines. But from our point of view it was a huge win, and we are eternally grateful to Don \nStewart and Duncan Coutts for writing this library. This is worth considering when designing a new system: \nyou need not immediately supply ef.cient things for basic commercial needs, but if you can give others \nthe ability (through mecha\u00adnisms such as the OverloadedStrings language extension) to easily bring those \nin later, you open up more possibilities for success in the commercial arena. Haskell has not been bad \nin this respect, but there are many times I ve wished for improvements such as making String a typeclass. \n functionality), we have about 5200 lines of production code and just over 1500 lines of unit tests. \nIn contrast, in the version of the trading system as of early 2009, we had over 5600 lines of production \ncode and less than 500 lines of unit tests. The functional tests show similar differences. Further, in \nmost areas where we used the type system to show program correctness, we came out with much more con.dence \nthat the program was indeed correct than if we had used testing. Unlike tests, a good type system and \ncompiler will often force the developer to deal with cases that might have been forgotten in testing, \nboth during the initial development and especially when later modifying the code. On one last note, the \ndifference in the amount of test code we used was far larger between Ruby and Haskell than between Ruby \nand Java, though Java, as with Haskell, also offers static type checking. We attribute this to a large \ndifference in expressive power between the type systems of Haskell and Java.  3.3 Speed With the exception \nof space leak problems (see next section), speed was never an issue for us. The application is sensitive \nto how fast it can do the calculations to build a pricing model for the current market, but the code \ngenerated by GHC 6.8.3 turned out to be sig\u00adni.cantly faster than the client s previous Java application. \nWe ve to this point seen no signi.cant difference between the current sys\u00adtem s performance and what \nwe believe we could achieve in C or C++. We use multi-core systems, and make extensive use of multi\u00adthreaded \ncode for both concurrency and parallelism. This has worked well for us. However, when used for parallelism, \nwith lazy evaluation one has to be careful about in which thread computa\u00adtions are really occurring: \ni.e., that one is sending a result, and not an unevaluated thunk, to another thread. This has brought \nup prob\u00adlems and used solutions similar in style to the space leak issues. We have yet to make extensive \nuse of explicitly parallel com\u00adputation based on the par function. However, it seems clear that if and \nwhen we move in to this area, implementation will be con\u00adsiderably simpler than when using a thread-based \nmodel. Haskell s purity by default model helps greatly here. 3.4 Portability One pleasant surprise was \nthat, once we d made a few modi.ca\u00adtions to our build system, building and running our application un\u00adder \nMicrosoft Windows was no trouble at all. While not an initial requirement, this was a nice facility to \nhave as it saved our client setting up a separate Unix machine to run the simulator for him\u00adself. Eventually, \nwe ended up being able to write in Haskell a small amount of Windows code we d initially planned to write \nin C++, which saved us considerable time and effort (see below).  3.5 The Foreign Function Interface \nOne part of our application involved interfacing with the Microsoft Windows DDE facility in order to \ntransfer hundreds of data values to Excel to be displayed and updated several times per second. We had \noriginally planned to write this this in C++, but in view of the portability we d discovered, we decided \nsee how much of this we could do in Haskell. This involved a non-trivial interface with a Microsoft C \nlibrary. In addition to simple calls into C code, we had to deal with low\u00adlevel data structures involving \nmachine words that combined bit .elds and integers of non-standard bit sizes, special memory alloca\u00adtion \nand deallocation schemes speci.c to the library, and callbacks from the library back into our code. This \nmight well have been the biggest and most pleasant sur\u00adprise we encountered: GHC s excellent Foreign \nFunction Interface allowed us to code this entirely in Haskell without having to write a single line \nof C. Further, we were able to make extensive use of the C header .les supplied for the library, resorting \nonly minimally to error-prone supplying of type information by hand. Being able to use the Haskell type \nchecker to write C-like code was particularly enjoyable. Type-checking memory allocations and the like \nis a well known and impressive technique in the Hindley-Milner type system community, but it wasn t until \nwe d used it ourselves in anger that we realized that programming this close to the machine could be \nquite a relaxing thing. This was a signi.cant change from any other language we ve used, and from what \nwe believe we would have needed to do in, say, Objective Caml. Every other foreign interface we ve seen \nwould have required us to write at least some code in C and would have provided us with signi.cantly \nless type safety. 4. Problems and Disadvantages No development environment or set of tools is without \nits problems, and GHC turned out to be no exception. However, while the nature of the problems we ran \ninto was different from other systems, the general number and level of dif.culty of the problems was \nnot signi.cantly different from any other system, especially taking into account how different the language \nis. The largest problems for us were issues with learning the lan\u00adguage itself, dealing with lazy evaluation, \nand the performance is\u00adsue of space leaks. 4.1 Language Issues Haskell s syntax is quite simple and consistent, \nand this is great advantage. However syntax is a relatively small part of learning a language, even in \nlanguages where it is signi.cantly more complex. To use a language well, and take good advantage of it, \none must also learn its structures, idioms and style. Haskell is well known as a language that s very \ndifferent from others in this regard, and even after many months of programming in it, we still don t \nfeel we ve progressed particularly far in this direction, aside from the use of monads for handling state \nand control .ow. (For example, we use applicative style in very limited ways, and as yet make no use \nof Control.Applicative.) We found that to use many common Haskell structures, such as functors, monoids \nand monads, we needed to think in signi.\u00adcantly different ways than we did in other languages, even other \nfunctional languages such as Scheme. Further, the level of abstract thought required to understand these \nstructures (in great part due to their generality) is noticeably higher than in other languages, and \nin our experience not in the background of many typical commercial programmers. This is not an insurmountable \nproblem, given appro\u00adpriate learning materials: we built our .rst monadic parser from scratch after only \na few days of study. But learning these things, especially on one s own, can be pretty rough going. This \nis mitigated to some degree by being able to fall back easily to more commonly known structures from \nother languages. We spent a couple of days as an extended interview working with a programmer unfamiliar \nwith Haskell but with extensive experience in LISP, and while the code we produced made little use of \ntypical Haskell idioms, it was certainly clear and workable.  4.2 Refactoring One of our reasons for \ndoing extensive automated testing is to support refactoring, a technique which we use extensively. As \ncompared to Ruby, refactoring in the small, that is, within a single module or a small change crossing \na few modules, was more dif.cult. The main issue was that where, with a language such as Ruby, one can \nchange a small handful of functions and leave the rest broken while testing the change, in Haskell one \nmust .x every function in the module before even being able to compile. There are certain techniques \nto help work around this, such as copying the module and removing the code not currently of interest \n(which is tedious), or minimizing the use of type declarations (which doesn t always help, and brings \nits own problems), but we didn t .nd these to be very satisfactory.  We feel that a good refactoring \ntool could signi.cantly ease this problem, but there seems nothing like this in common use in the Haskell \ncommunity at the moment. That said, the major refactorings we do on a regular basis ( ma\u00adjor ones being \nrestructuring that crosses many modules and moves signi.cant amounts of code between them) were less \naffected by this problem, and did not seem to take signi.cantly longer than im\u00adplementing the same sort \nof change in any other language. As well, being able to do more work with the type checker and less with \ntest\u00ading increased our con.dence that our refactorings had not broken parts of the system.  4.3 Pro.ling \nTools, and De.ciencies Thereof Due especially to lazy evaluation (of which more is mentioned later), \nunderstanding the runtime behaviour of one s application is rather important when working in Haskell. \nGHC s pro.ling tools are good certainly as good as anything in Java but unfortunately not good enough \nthat we don t have some signi.cant complaints. First, they are rather subtle: it takes time and some \ncareful ex\u00adamination of the documentation and the output to learn to use them well (though this is very \noften true of any pro.ling tool). We have found that, as of mid-2009, there are no good tutorials available \nfor this; frustrating and time-consuming experimentation has been the rule. Better descriptions of the \noutput of the summarization tools (such as hp2ps, which generates a graph of heap usage from raw pro.le \ndata) may not be the only solution to this: providing more documentation of the raw pro.ling data itself \nmight not only en\u00adable developers to build their own pro.le analysis tools, but provide more insight \nas to just what s going on inside the runtime system. One of the major problems, directly connected to \nlazy evalua\u00adtion, is that where the work happens is quite a slippery concept. One module may de.ne and \napply all of the functions for the work to be done, but in actuality leave only a few thunks in the heap: \nthe real work of the computation occurs in some other place where the data are used. If several other \nmodules use the results of, say, a parser, which module or function actually ends up doing the com\u00adputational \nwork of the parsing (evaluating the thunks) can change from run to run of the program. This problem is \nonly exacerbated by doing work in multiple threads. When one hopes to spread the work more or less evenly \namongst all the threads in the application, inter-thread communica\u00adtion mechanisms that as happily transfer \nthunks as they do data are more of a liability than an asset. It s quite possible to have a .nal consumer \nof work actually doing all of the work that one wanted to have done in several other threads running \non different cores. Na\u00a8ive approaches to .xing the problem can easily back.re. At one point we made some \nof our data structures members of the NFData class from Control.Parallel.Strategies and used the rnf \nfunction at appropriate points to ensure that evaluation had been fully forced. This turned out to do \nmore harm than good, caus\u00ading the program s CPU usage shoot through the roof. While we didn t investigate \nthis in detail, we believe that the time spent re\u00adpeatedly traversing large data structures, even when \nalready mostly evaluated, was the culprit. (This would certainly play havoc with the CPU s cache.) We \nwould particularly like to have a way at runtime to monitor the CPU usage of each individual Haskell \nthread within our appli\u00adcation no matter which operating system threads it might run on over time. This \nwould allow us to monitor the distribution of work to see where more strictness needs to be applied in \norder to make the best use of our multi-core machines. This needs to be available for non-pro.led builds \nso that we can monitor how the application runs in the real environment, as well as when it s specially \nbuilt for pro.ling. We feel that this would be a great step forward in ful.ll\u00ading pure functional programming \ns promise of much easier use of multi-core CPUs than conventional languages. A last note about a particular \nproblem for us with GHC: pro\u00ad.ling builds must run using the non-parallel runtime. For an ap\u00adplication \nthat requires more than a full core even for builds not slowed down by pro.ling, this can at times be \nentirely useless: the problems that show up in the pro.le output are due entirely to the lack of CPU \npower available to run the application within its nor\u00admal time constraints. We are contemplating modifying \nour stock exchange simulator and trading system to run at a small fraction of real-time speed in order \nto see if this might provide more realistic pro.ling results.  4.4 Lazy Evaluation and Space Leaks Lazy \nevaluation turned out to have good and bad points in the end. However, it caused, and continues to cause, \nus pain in two particular areas. The .rst is that, when distributing computation amongst mul\u00adtiple co-operating \nthreads, it s important to ensure that data struc\u00adtures passed between threads are evaluated to head \nnormal form when necessary before being used by the receiving thread. Initially we found .guring out \nhow to do this to be dif.cult; even determin\u00ading when this is necessary can be a challenge in itself. \n(One clue is a large amount of heap use connected with communications queues between threads, as the \nsending thread overruns the receiver s abil\u00adity to evaluate the thunks it s receiving.) A pro.ler that \ndid not slow down the program and that could help with this would be highly welcome. The second is the \ndreaded space leak, when unevaluated com\u00adputations retain earlier versions of data in the heap and cause \nit to grow to enormous proportions, slowing and eventually halting the program as it consumes all memory \nand swap space. This proved to be a particular problem in our application, which is essentially sev\u00aderal \nloops (processing messages and doing calculations) executed millions of times over the course of a many-hour \nrun. The pro.ler that comes with GHC is of more help here, but we have yet to work out a truly satisfactory \nway of dealing with this. A particularly common cause of space leaks for us was when using data structures \n(such as lists and Data.Map) that have many small changes over time. As one inserts and removes data, \nold, no\u00adlonger-referenced versions of the data structure are kept around to enable the evaluation of \nthunks that will create the new version of the data structure. As mentioned previously, na\u00a8ive approaches \nto the problem can back.re, and better approaches were not always obvious. For Data.Map and similar structures, \nwe resorted to do\u00ading an immediate lookup and forced evaluation of data just inserted; this seemed to \n.x the problem while avoiding the massive perfor\u00admance penalty of doing an rnf on the root of the data \nstructure. Another issue relates to interactive programs. Certain lazy id\u00adioms (such as using the getContents \nfunction) can effectively deadlock such programs, rendering them unresponsive. This is not immediately \nobvious to programmers new to lazy evaluation, and caused us early on to spend some .guring out what \nwas going on. Knowing what we know now, we are still hesitant about the value of lazy evaluation; every \ntime we feel we ve .nally got a good handle on it, another problem seems to crop up. We feel that the \nsubject needs, at the very least, a good book that would both cover all of the issues well and help the \nreader develop a good intuition for the behaviour of lazily-evaluated systems.  5. Conclusions Our experience \nwith GHC has shown that, while it has its quirks and issues, these are no worse than those of other commonly-used \nprogramming language implementations when used for commer\u00adcial software development. However, some of \nthe issues are differ\u00adent enough that one should be prepared to spend a little more time learning how \nto deal with them than when moving between more similar systems. We found signi.cant advantages to using \nHaskell, and it s clear to us that there is much more expressive power available of which we ve yet to \nmake use. Learning to take advantage of Haskell takes a fair amount of work, but bene.ts are seen fairly \nquickly and continuously as one improves. We were lucky with our choice of Haskell and GHC and, in light \nof our experience, we would make the same choice again. However, we note that, given our lack of experience \nwith other functional languages and platforms, we cannot really say whether or not it is signi.cantly \nbetter than, say, OCaml or Scala. It is our opinion that, overall, our switch to a functional lan\u00adguage \nin a commercial environment has been successful, and we are convinced we will continue to see further \nbene.t over the long term. References [1] Friedman, Daniel P., and Felleisen, Matthias, The Little Schemer, \nFourth Edition. MIT Press, 1996. ISBN-13: 978-0-262-56099-3. [2] The Glasgow Haskell Compiler. http://www.haskell.org/ghc/ \n[3] Graham, Paul, Beating the Averages , from Hackers and Painters O Reilly, 2004, ISBN-13: 978-0-596-00662-4. \nAlso available from http://paulgraham.com/avg.html [4] Hudak, Paul, The Haskell School of Expression: \nLearning Functional Programming Through Multimedia. Cambridge University Press, 2000. ISBN-13: 978-0-521-64408-2. \n[5] Hutton, Graham, Programming in Haskell. Cambridge University Press, 2007. ISBN-13: 978-0-521-69269-4. \n [6] Objective Caml. http://caml.inria.fr/ocaml/index.en.html [7] Ruby Programming Language. http://www.ruby-lang.org/en/ \n[8] The Scala Programming Language. http://www.scala-lang.org/ [9] Duncan Coutts, Don Stewart and Roman \nLeshchinskiy, Rewriting Haskell Strings. http://www.cse.unsw.edu.au/ dons/papers/CSL06.html     \n\t\t\t", "proc_id": "1596550", "abstract": "<p>I describe the initial attempt of experienced business software developers with minimal functional programming background to write a non-trivial, business-critical application entirely in Haskell. Some parts of the application domain are well suited to a mathematically-oriented language; others are more typically done in languages such as C++.</p> <p>I discuss the advantages and difficulties of Haskell in these circumstances, with a particular focus on issues that commercial developers find important but that may receive less attention from the academic community.</p> <p>I conclude that, while academic implementations of \"advanced\" programming languages arguably may lag somewhat behind implementations of commercial languages in certain ways important to businesses, this appears relatively easy to fix, and that the other advantages that they offer make them a good, albeit long-term, investment for companies where effective IT implementation can offer a crucial advantage to success.</p>", "authors": [{"name": "Curt J. Sampson", "author_profile_id": "81442613820", "affiliation": "Starling Software, Tokyo, Japan", "person_id": "P1614021", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1596550.1596578", "year": "2009", "article_id": "1596578", "conference": "ICFP", "title": "Experience report: Haskell in the 'real world': writing a commercial application in a lazy functional lanuage", "url": "http://dl.acm.org/citation.cfm?id=1596578"}