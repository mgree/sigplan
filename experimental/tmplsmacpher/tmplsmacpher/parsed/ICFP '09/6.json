{"article_publication_date": "08-31-2009", "fulltext": "\n Runtime Support for Multicore Haskell Simon Marlow Simon Peyton Jones Satnam Singh Microsoft Research \nMicrosoft Research Microsoft Research Cambridge, U.K. Cambridge, U.K. Cambridge, U.K. simonmar@microsoft.com \nsimonpj@microsoft.com satnams@microsoft.com Abstract Purely functional programs should run well on parallel \nhardware because of the absence of side effects, but it has proved hard to realise this potential in \npractice. Plenty of papers describe promis\u00ading ideas, but vastly fewer describe real implementations \nwith good wall-clock performance. We describe just such an implementation, and quantitatively explore \nsome of the complex design tradeoffs that make such implementations hard to build. Our measurements are \nnecessarily detailed and speci.c, but they are reproducible, and we believe that they offer some general \ninsights. Categories and Subject Descriptors D.3.2 [Programming Lan\u00adguages]: Language Classi.cations \nApplicative (functional) lan\u00adguages; D.3.2 [Programming Languages]: Language Classi.c\u00adations Concurrent, \ndistributed and parallel languages; D.3.3 [Programming Languages]: Language Constructs and Features Concurrent \nprogramming structures; D.3.4 [Programming Lan\u00adguages]: Processors Runtime-environments General Terms \nLanguages, Performance 1. Introduction At least in theory, Haskell has a head start in the race to .nd \nan effective way to program parallel hardware. Purity-by-default means that there should be a wealth \nof inherent parallelism in Haskell code, and the ubiquitous lazy evaluation model means that, in a sense, \nfutures are built-in. How can we turn these bene.ts into real speedups on commod\u00adity hardware? This paper \ndocuments our experiences with building and optimising a parallel runtime for Haskell. Our runtime sup\u00adports \nthree models of parallelism: explicit thread-based concur\u00adrency (Peyton Jones et al. 1996), semi-explicit \ndeterministic par\u00adallelism (Trinder et al. 1998), and data-parallelism (Peyton Jones et al. 2009). In \nthis paper, however, we focus entirely on semi\u00adexplicit parallelism. Completely implicit parallelism \nis still a distant goal; one re\u00adcent attempt at this in the context of Haskell can be found in Harris \nand Singh (2007). The semi-explicit GpH programming model, in contrast, has been shown to be remarkably \neffective (Loidl et al. 1999, 2003). The semantics of the program remains completely de\u00adterministic, \nand the programmer is not required to identify threads, Permission to make digital or hard copies of \nall or part of this work for personal or classroom use is granted without fee provided that copies are \nnot made or distributed for pro.t or commercial advantage and that copies bear this notice and the full \ncitation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to \nlists, requires prior speci.c permission and/or a fee. ICFP 09, August 31 September 2, 2009, Edinburgh, \nScotland, UK. Copyright c &#38;#169; 2009 ACM 978-1-60558-332-7/09/08. . . $5.00. communication, or synchronisation. \nThey merely annotate sub\u00adcomputations that might be evaluated in parallel, leaving the choice of whether \nto actually do so to the runtime system. These so-called sparks are created and scheduled dynamically, \nand their grain size varies widely. Our goal is that programmers should be able to take exist\u00ading Haskell \nprograms, and with a little high-level knowledge of how the program should parallelise, make some small \nmodi.ca\u00adtions to the program using existing well-known techniques, and thereby achieve decent speedup \non today s parallel hardware. How\u00adever, when we started benchmarking some existing Parallel Haskell programs, \nwe found that many programs which at .rst glance ap\u00adpeared to be completely reasonable-looking parallel \nprograms, in fact failed to achieve signi.cant speedup when run with our imple\u00admentation on parallel \nhardware. This led us to a critical study of our (reasonably mature) base\u00adline implementation of semi-explicit \nparallelism in GHC 6.10. In this paper we report the results of that study, with the following contributions: \n We give a complete description of GHC s parallel runtime, starting with an overview in Section 4, and \nampli.ed in the rest of the paper. A major constraint is that we do barely compro\u00admise the (excellent) \nexecution speed of sequential code.  We discuss several major sets of design choices, relating to spark \ndistribution, scheduling, and memory management (Sec\u00adtions 5 and 7); parallel garbage collection (Section \n6); the im\u00adplementation of mutual exclusion on thunks (Section 8); and load balancing and thread migration \n(Section 9). In each case we give quantitative measurements for a number of optimisa\u00adtions that we made \nover our baseline GHC 6.10 implementa\u00adtion.  While we focus mainly on the implementation, our work has \nhad some impact on the programming model: we identify the need for pseq as well as seq (Section 2.1), \nand we isolated a sign.cant dif.culty in the strategies approach to writing parallel programs (Section \n7).  On the way we developed a useful new pro.ling and tracing tool (Section 10.1).  Overall, our results \nare encouraging (Figure 1). The optimisations we describe improve the absolute runtime and scalability \nof all benchmarks, sometimes dramatically so. Before our work, some programs would speed up on a parallel \nmachine, but others slowed down. Afterwards, using 7 cores of an 8-core machine yielded speedups in the \nrange 3x to 6.6x, which is not bad for a modest investment of programmer effort. Some of the improvements \nwere described in earlier work (Berthold et al. 2008), along with preliminary measurements, as part of \na comparison between shared-heap and distributed-heap parallel execution models. In this paper we extend \nboth the range of measurements and the range of improvements, while focussing exclusively on shared-heap \nexecution.  All our results are, or will be, repeatable using a released ver\u00adsion of the widely-used \nGHC compiler. Our results do not require special builds of the compiler or libraries: identical results \nwill be obtainable using a standard binary distribution of GHC. At the time of writing, most of our developments \nhave been made public in the GHC source code repository, and we expect to include the remain\u00ading changes \nin the forthcoming 6.12.1 release of GHC, scheduled for the autumn of 2009. The sources to our benchmark \nprograms are available in the public nofib source repository. 2. Background: programming model The basic \nprogramming model is known as Glasgow Parallel Haskell, or GpH (Trinder et al. 1998), and consists of \ntwo com\u00adbinators: par ::a->b->b pseq::a-> b-> b The semantics of par a b is simply the value of b, whereas \nthe semantics of pseq is given by pseq ab = ., if a= . = b, otherwise Informally, par stores its .rst \nargument as a spark in the spark pool, and then continues by evaluating its second argument. The intention \nis that idle processors can .nd (probably) useful work in the spark pool. Typically the .rst argument \nto par will be an expression that is shared by another part of the program, or will be an expression \nthat refers to other such shared expressions. 2.1 The need for pseq The pseq combinator is used for sequencing; \ninformally, it eval\u00aduates its .rst argument to weak-head normal form, and then eval\u00aduates its second \nargument, returning the value of its second argu\u00adment. Consider this de.nition of parMap: parMap f [] \n= [] parMap f (x:xs) = y par (ys pseq y:ys) wherey =fx ys = parMap f xs The intention here is to spark \nthe evaluation of fx, and then evaluate parMap f xs, before returning the new list y:ys. The programmer \nis hoping to express an ordering of the evaluation: .rst spark y, then evaluate ys. The obvious question \nis this: why not use Haskell s built-in seq operator instead of pseq? The only guarantee made by seq \nis that it is strict in both arguments; that is, seq a . = . and seq . a = .. But this semantic property \nmakes no operational guaran\u00adtee about order of evaluation. An implementation could impose this operational \nguarantee on seq, but that turns out to limit the optimi\u00adsations that can be applied when the programmer \nonly cares about the semantic behaviour. Instead, we provide both pseq and seq (with and without an order-of-evaluation \nguarantee), to allow the programmer to say what she wants while leaving the compiler with as much scope \nfor optimisation as possible. To our knowledge this is the .rst time that this small but im\u00adportant point \nhas been mentioned in print. The pseq operator .rst appeared in GHC 6.8.1.  2.2 Strategies In Algorithms \n+ Strategies = Parallelism (Trinder et al. 1998), Trinder et al explain how to use strategies to modularise \nthe con\u00adstruction of parallel programs. In brief, the idea is as follows. A Speedup on 4 cores Speedup \non 7 cores Program Before After Program Before After gray 2.19 2.50 gray 2.61 2.77 mandel 2.94 3.51 mandel \n4.50 4.96 matmult 2.56 3.37 matmult 4.07 5.04 par.b 3.73 3.89 par.b 5.94 6.67 partree 0.74 1.99 partree \n0.68 3.18 prsa 3.28 3.56 prsa 5.22 5.23 ray 0.81 2.11 ray 0.82 3.48 sumeuler 3.74 3.85 sumeuler 6.32 \n6.42 Figure 1. Speedup results strategy is a function that may evaluate (parts of) its argument and create \nsparks, but has no interesting results: type Done = () done = () type Strategy a = a -> Done Strategies \ncompose nicely; that is, we can build complex strate\u00adgies out of simpler ones: rwhnf :: Strategy a rwhnf \nx = x pseq done parList :: Strategy a -> Strategy [a] parList strat [] = done parList strat (x:xs) = \nstrat x par parList strat xs Finally, we can combine a data structure with a strategy for evaluating \nit in parallel: using :: a -> Strategy a -> a usingx s= sx pseq x Here is how we might use the combinators \nto evaluate all the element of a (lazy) input list in parallel, and then add them up: psum :: [Int] -> \nInt psum xs = sum (xs using parList rwhnf) 3. Making parallel programs run faster We now turn our attention \nfrom the programming model to the implementation. Our baseline is GHC 6.10.1, a mature Haskell compiler. \nIts performance on sequential code is very good, so the overheads of parallelism are not concealed by \nsloppy sequential execution. It has supported parallel execution for several years, but while parallel \nperformance is sometimes good, it is sometimes surprisingly bad. The trouble is that it is hard to know \nwhy it is bad, because performance is determined by the interaction of four systems the compiler itself, \nthe GHC runtime system, the operating system, and the physical hardware each of which is individually \nextremely complex. The rest of this paper reports on our experience of improving both the absolute performance \nand its consistency. To whet your appetite, Figure 1 summarises the cumulative improvement of the work \nwe present, for 4 and 7 cores1. Each table has 2 columns: 1 Why did we use only 7 of the 8 cores on our \ntest system? In fact we did perform the measurements for all 8 cores, but found that the results were \nfar less consistent than the 7 core results, and in some cases perfor\u00admance degraded signi.cantly. On \ncloser inspection the OS appeared to be descheduling one or more of our threads, leading to long pauses \nwhen the threads needed to synchronise. This effect is discussed in more detail in Section 10.1.  Figure \n2. Speedup results Before: speedup achieved by parallel execution using GHC 6.10.1, compared to the \nsame program compiled sequentially with 6.10.1, with the parallel GC turned off. In GHC 6.10.1 the parallel \nGC tended to make things worse rather than better, so this column re.ects the best settings for GHC 6.10.1. \n After: our best speedup results, using the PARGC3 con.gura\u00adtion (Section 6).  The improvements are \nsubstantial, especially for the most disap\u00adpointing programs which actually ran slower when parallelism \nwas enabled in 6.10.1. Section 10 gives more details about the experi\u00admental setup and the benchmark \nprograms. Figure 2 shows the scaling results for each benchmark program after our cumulative improvements, \nrelative to the performance of the sequential version. By sequential we mean that the single\u00adthreaded \nversion of the runtime system was used, in which par is a no-op, and there are no synchronisation overheads. \n4. Background: the GHC runtime By way of background, we describe in this section how GHC runs Haskell \nprograms in parallel. In the sections that follow we present various measurements to show the effectiveness \nof certain aspects of our implementation design. Each of our measurements compare two con.gurations of \nGHC. Many of our improvements are cumulative and it proved dif.cult to untangle the source-code dependencies \nfrom each other in order to be able to make each measurement against a .xed baseline, so in each case \nwe will clearly state what the baseline is. 4.1 The basic setup The GHC runtime system supports millions \nof lightweight threads by multiplexing them onto a handful of operating system threads, roughly one for \neach physical CPU. This overall scheme is well\u00adestablished, but it is easier to sketch than to implement! \nEach Haskell thread runs on a .nite-sized stack, which is allo\u00adcated in the heap. The state of a thread, \ntogether with its stack, is kept in a heap-allocated thread state object (TSO). The size of a TSO is \naround 15 words plus the stack, and constitutes the whole state of a Haskell thread. A stack may grow \nby copying the TSO into a larger area, and may subsequently shrink again. Haskell threads are executed \nby a set of operating system threads, which we call worker threads. We maintain roughly one worker thread \nper physical CPU, but exactly which worker thread may vary from moment to moment, as we explain in Section \n4.2. Since the worker thread may change, we maintain exactly one Haskell Execution Context (HEC) for \neach CPU2. The HEC is a data structure that contains all the data that an OS worker thread requires in \norder to execute Haskell threads. In particular, a HEC contains An Ownership Field, protected by a lock, \nthat records which worker thread is currently animating the capability (zero if none is). We explain \nin Section 4.2 why we do not use the simpler device of a lock to protect the entire HEC data structure. \n A Message Queue, containing requests from other HECs. For example, messages saying Please wake up thread \nT arrive here.  A Run Queue of threads ready to run.  2 In the source code of the runtime system, a \nHEC is called a Capability . The HEC terminology comes from the lightweight concurrency primitives work \n(Li et al. 2007b). Others call the same abstraction a virtual processor (Fluet et al. 2008a).  An Allocation \nArea (Section 6). There is a single heap, shared among all the HECs, but each HEC allocates into its \nown local allocation area.  GC Remembered Sets (Section 6.2).  A Spark Pool. Each invocation of par \nab adds the thunk a to the (current HEC s) Spark Pool; this thunk is called a spark .  A Worker Pool \nof spare worker threads, and a Foreign Outcall Pool of TSOs that are engaged in foreign calls (see Section \n4.2).  In addition there is a global Black Hole Pool, a set of threads that are blocked on black holes \n(see Section 8). An active HEC services work using the following priority scheme. Items lower down the \nlist are only performed if there are no higher-priority items available. 1. Service a message on the \nMessage Queue. 2. Run a thread on the Run Queue; we use a simple round-robin scheduling order. 3. If \nany spark pool is non-empty, create a spark thread and start running it (see Section 5.3). 4. Poll the \nBlack Hole Pool to see if any thread has become runnable; if so, run it.  All the state that a HEC needs \nfor ordinary execution of Haskell threads is local to the HEC, so under normal execution a HEC proceeds \nwithout requiring any synchronisation, locks, or atomic instructions. Synchronisation is only needed \nwhen: Load balancing is needed (Section 9).  Garbage collection is required (Section 6).  Blocking \non black holes (Section 8.1).  Performing an operation on an MVar, or an STM transaction.  Unblocking \na thread on another HEC.  Throwing an exception to a thread on another HEC, or a blocked thread.  Allocating \nlarge or immovable memory objects; since these operations are relatively rare, we allocate such objects \nfrom single global pool.  Making a (safe) foreign call (Section 4.2).   4.2 Foreign calls Suppose \nthat a Haskell thread makes a foreign call to a C procedure that blocks, such as getChar. We do not want \nthe entire HEC to seize up so, before making the call, the worker thread relinquishes ownership of the \nHEC, leaving the Haskell thread in a tidy state. The thread is then placed in the Foreign Outcall Pool \nso that the garbage collector can .nd it. We maintain a Worker Pool of worker threads for each HEC, each \neager to become the worker that animates the HEC. When one worker relinquishes ownership, it triggers \na condition variable that wakes up another worker from the Worker Pool. If the latter is empty, a new \nworker is spawned. What happens when the original worker thread W completes its call to getChar and wants \nto return? To return, it must re-acquire ownership of the HEC, so it must somehow displace any worker \nthread X that currently owns the HEC. To do so, it adds a message to the HEC s Message Queue. When X \nsees this message, it signals W, and returns itself to the worker pool. Worker thread W wakes up, and \ntakes ownership of the HEC. This approach is slightly better than directly giving W ownership of the \nHEC, because W might be slow to respond, and the HEC does not remain locked for the duration of the handover. \n4 cores 7 cores Program . Time (%) Program . Time (%) gray +6.5 gray -2.1 mandel -3.4 mandel -4.9 matmult \n-2.1 matmult +1.6 par.b +3.5 par.b -1.2 partree -1.2 partree -3.7 prsa -4.7 prsa -6.7 ray -35.4 ray -66.1 \nsumeuler +0.0 sumeuler +1.4 Geom. Mean -5.5 Geom. Mean -14.4 Figure 3. The effect of adding work-stealing \nqueues vs. GHC 6.10.1 This also explains why we don t simply have a mutex protecting the HEC, which all \nthe spare worker threads are blocked on. That approach would afford us less control in the sense that \nwe often want to hand the HEC to a particular worker thread, and a simple mutex would not allow us to \ndo that. Foreign calls are not the focus of this paper, but more details can be found in Marlow et al. \n(2004). 5. Faster sparks We now discuss the .rst set of improvements, which relate to the handling of \nsparks. 5.1 Sharing sparks GHC 6.10.1 has a private Spark Pool for each HEC, but it uses a push model \nfor sharing sparks, as follows. In between running Haskell threads, each HEC checks whether its spark \npool has more than one spark. If so, it checks whether any other HECs are idle (a cheap operation that \nrequires no atomic instructions); if it .nds an idle HEC it gives one or more sparks to it, by temporarily \nacquiring ownership of the remote HEC and inserting the sparks in its pool. To make spark distribution \ncheaper and more asynchronous we re-implemented each HEC s Spark Pool as a bounded work\u00adstealing queue \n(Arora et al. 1998; Chase and Lev 2005). A work\u00adstealing queue is a lock-free data structure with some \nattractive properties: the owner of the queue can push and pop from one end without synchronisation, \nmeanwhile other threads can steal from the other end of the queue incurring only a single atomic instruction. \nWhen the queue is almost empty, popping also incurs an atomic instruction to avoid a race between the \npopping thread and a stealing thread. When a spark is pushed onto an already full queue, we have a choice \nbetween discarding the new spark or discarding one of the older sparks. Our current implementation discards \nthe newer spark; we do not investigate this choice further in this paper. Figure 3 shows the effect of \nadding work-stealing queues to our baseline GHC 6.10.1. As we can see from the results, work-stealing \nfor sparks is almost always bene.cial, and increasingly so as we add more cores. It is of particular \nbene.t to ray, where the task granularity is very small.  5.2 Choosing a spark to run Because we use \na work-stealing queue for our spark pools, stealing threads must always take the oldest spark in the \npool. However, the HEC owning the spark pool has a choice between two policies: it can take the youngest \nspark from the pool (LIFO), or it can take the oldest spark (FIFO). Taking the oldest spark requires \nan atomic instruction, but taking the youngest spark does not. Figure 4 shows the effect of changing \nthe default (FIFO) to LIFO. In most of our benchmarks this results in worse performance, because the \nolder sparks tend to be larger .  4 cores 7 cores Program . Time (%) Program . Time (%) gray +10.1 gray \n+5.3 mandel +4.5 mandel +6.2 matmult -0.4 matmult -1.2 par.b +0.2 par.b +1.0 partree -7.3 partree -1.6 \nprsa -0.3 prsa -2.3 ray +10.0 ray +18.7 sumeuler +0.3 sumeuler +17.9 Geom. Mean +2.0 Geom. Mean +5.2 \nFigure 4. Using LIFO rather than FIFO for local sparks 4 cores 7 cores Program . Time (%) Program . Time \n(%) gray -0.7 gray +2.3 mandel -1.5 mandel +1.3 matmult -8.2 matmult -11.3 par.b +0.6 par.b +6.4 partree \n-0.6 partree +0.3 prsa -1.0 prsa -1.0 ray -31.2 ray -24.3 sumeuler +0.2 sumeuler -0.3 Geom. Mean -5.9 \nGeom. Mean -3.8 Figure 5. The effect of batching sparks  5.3 Batching sparks To run a spark a, a HEC \nsimply evaluates the thunk a to head normal form. To do so, it needs a Thread State Object. It makes \nno sense to create a fresh TSO for every spark, and discard it when the evaluation is complete for the \ngarbage collector to recover. Instead, when a HEC has no work to do, it checks whether there are any \nsparks, either in the HEC s local spark pool or in any other HEC s spark pool (check the non-empty status \nof a spark pool does not require a lock). If there are sparks available, then the HEC creates a spark \nthread, which is a perfectly ordinary thread except that it runs the following steps in a loop: 1. If \nthe local Run Queue or Message Queue is non-empty, exit. 2. Remove a spark from the local spark pool, \nor if that is empty, steal a spark from another HEC s pool. 3. If there were no sparks to steal, exit. \n 4. Evaluate the spark to weak-head-normal form.  where exit means that the spark thread exits and performs \nno further work; its TSO will be recovered by a subsequent GC. A spark thread will therefore evaluate \nsparks to WHNF one after another until it can .nd no more sparks, or until there is other work to do, \nat which point it exits. This is a particularly simple strategy and works well: the cost of creating \nthe spark thread is amortized over multiple sparks, and the spark thread gets out of the way quickly \nif any other work arrives. If a spark blocks on a black hole, since the spark thread is just an ordinary \nthread it will block in the usual way, and the scheduler will create another spark thread to continue \nrunning the available sparks. We don t have to worry unduly about having too many spark threads, because \na spark thread will always exit when there are other threads around. This reasoning does rely on sparks \nnot being too large, however: many large sparks becoming blocked could lead to a large number of running \nspark threads. Figure 5 compares the effect of using the spark-batching ap\u00adproach described above to \nthe approach taken in GHC 6.10.1, which was to create a new thread for each activated spark. Our baseline \nfor these measurements is GHC 6.10.1 plus work-stealing\u00adqueues (Section 5.1). Batching sparks is particularly \nbene.cial to two of our benchmarks, matmult and ray, while it is a slight pes\u00adsimisation for par.b on \n7 cores. For ray the rationale is clear: there are lots of tiny sparks, so reducing the overhead for \nspark execution has a signi.cant effect. For par.b we believe that the reduction in performance shown \nhere is because the program is actually be\u00ading more effective at exploiting parallelism, which leads \nto reduced performance due to lack of locality (Section 6); as we shall see later, this performance loss \nis recovered by proper use of parallel GC. 6. Garbage collection The shared heap is divided into .xed-size \n(4kbyte) blocks, each with a block descriptor that speci.es which generation it belongs to, along with \nother per-block information. A HEC s Allocation Area simply consists of a list of such blocks. When any \nHEC s allocation area is exhausted, a garbage col\u00adlection must be performed. GHC 6.10.1 offers a parallel \ngarbage collector (see Marlow et al. (2008)), but GC only takes place when all HECs stop together, and \nagree to garbage collect. We aim to keep this synchronisation overhead to a minimum by ensuring that \nwe can stop a HEC quickly (Section 6.3). In future work we plan to relax the stop-the-world requirement \nand adopt some form of CPU-independent GC (Section 12.1). When a GC is required, we have the option of \neither Performing a single-threaded GC. In this case, the HEC that initiated the GC waits for all the \nother HECs to cease execution, performs the GC, and then releases the other HECs.  Performing a parallel \nGC. In this case, the initiating HEC sends a signal to the other HECs, which causes them to become GC \nthreads and await the start of the GC. Once they have all responded, the initiating HEC performs the \nGC initialisation and releases the other GC threads to perform GC. When the GC termination condition \nis reached, each GC thread waits at the GC exit barrier. The initiating HEC performs any post-GC tasks \n(such as starting .nalizers), and then releases the GC threads from the barrier to continue running Haskell \ncode.  In a single-threaded program, it is often better to use single\u00adthreaded GC for the quick young-generation \ncollections, because the cost of starting up and shutting down the GC threads can outweigh the bene.ts \nof doing GC in parallel. 6.1 Avoiding synchronisation in parallel copying GC Parallel copying GC normally \nrequires each GC thread to use an atomic instruction to synchronise when copying an object, so that objects \nare not accidentally duplicated. The cost of these atomic in\u00adstructions is high: roughly 30% of GC time \n(Marlow et al. 2008). However, as we suggested in that paper, it is possible to relax the synchronisation \nrequirement where immutable objects are con\u00adcerned. The only adverse effect from making multiple copies \nof an immutable object is a little wasted space, and we know from mea\u00adsurements that the rate of actual \ncollisions is very low typically less than 100 collisions per second of GC time so the amount of wasted \nspace is likely to be minuscule. Our parallel GC therefore adopts this policy, and avoids syn\u00adchronising \naccess to immutable objects. Figure 6 compares the two policies: the baseline is our current system in \nwhich we only lock mutable objects, compared to a modi.ed version in which we lock every object during \nGC. As the results show, our optimisation of only locking mutable objects has a signi.cant bene.t on \noverall performance: without it, performance drops by over 7%. The effect  7 cores Program . Time (%) \ngray +18.7 mandel +9.4 matmult +4.5 par.b -0.5 partree +17.3 prsa +2.5 ray +4.8 sumeuler +5.8 Geom. Mean \n+7.6 Figure 6. Effect of locking all closures in the parallel GC is the most marked in benchmarks that \ndo the most GC: gray, but is negligible in those that do very little GC: par.b.  6.2 Remembered Sets \nRemembered sets are used in generational GC to track pointers from older generations into younger ones, \nso that when collecting the younger generation we can quickly .nd all the pointers into that generation. \nWhenever a pointer into a younger generation is created, an entry must be added to the remembered set. \nThere are many choices for the representation of the remem\u00adbered set and the form of its associated write \nbarrier (Blackburn and Hosking 2004). In GHC, because mutation is rare, we opted for a relatively expensive \nwrite barrier in exchange for an accurate remembered set. Our remembered set representation is a sequential \nstore buffer, which contains the addresses of objects in the old gen\u00aderation that contain pointers into \nthe new generation. Each mutable object is marked to indicate whether it is dirty or not; dirty objects \nare in the remembered set. The write barrier adds a new entry to the remembered set if and only if the \nobject being mutated is in the old generation and is not already marked dirty. In our parallel runtime, \neach HEC has its own remembered set. The reasons for this are twofold: Even though appending to the \nremembered set is not a common operation, it is common enough that the effect of including any synchronisation \nwould be noticeable. Hence, we must be able to add new entries to the remembered set without atomic instructions. \n Objects that have been mutated by the current CPU are likely to be in its cache, so it is desirable \nto visit these objects by the garbage collector on the same CPU. This is particularly important in the \ncase of threads: the stack of a thread, and hence its TSO, is itself a mutable object. When a thread \nexecutes, the stack will accumulate pointers to new objects, and so if the TSO resides in an old generation \nit must be added to the remembered set. Having HEC-local remembered sets helps to ensure that the garbage \ncollector traverses a thread on the same CPU that was running the thread.  One alternative choice for \nthe remembered set is the card table. Card tables have the advantage that they can be updated by multiple \nthreads without synchronisation, but they compromise on accuracy. More importantly for us, however, would \nbe the loss of locality from using a single card table instead of per-HEC sequential store buffers. We \ndo not have individual measurements for the bene.t of using HEC-local remembered sets, but believe that \nit is essential for good performance of parallel programs. In GHC 6.10.1 remembered sets were partially \nlocalised: they were local during execution, but shared during GC. We subsequentially modi.ed these partially \nHEC-local remembered sets to be fully localised. 4 cores 7 cores Program . Time (%) Program . Time (%) \ngray -3.9 gray -9.7 mandel -1.4 mandel -2.1 matmult -0.5 matmult +0.0 par.b -0.5 par.b +0.4 partree +5.0 \npartree +0.3 prsa +2.3 prsa -0.1 ray +3.4 ray -2.8 sumeuler -0.3 sumeuler -1.7 Geom. Mean +0.5 Geom. \nMean -2.0 Figure 7. Using the heap-limit for context switching 6.3 Pre-emption and garbage collection \nSince garbage collection is relatively frequent, and requires all HECs to halt, it is important that \nthey all do so promptly. One way to do this would be to use time-based pre-emption; however that would \nessentially mandate the use of conservative GC, which we consider an unacceptable compromise. Hence in \norder to GC, we require that all HECs voluntarily yield at a safe point, leaving the system in a state \nwhere all the heap roots can be identi.ed. The standard way to indicate to a running thread that it should \nyield immediately is to set its heap-limit register to zero, thus causing the thread to return to the \nHEC scheduler when it next tries to allocate. On a register-poor machine, we keep the heap-limit register \nin memory, in a block of registers pointed to by a single real machine register. In this case, it is \neasy for one HEC to set another HEC s heap limit to zero, simply by overwriting the appropriate memory \nlocation. On a register-rich machine we can keep the heap limit in a real machine register, but it is \nthen a good deal more dif.cult for one HEC to zap another HEC s heap limit, since it is part of the register \nset of a running operating-system thread. We therefore explored two alternatives for register-rich architectures: \n Keep the heap limit in memory. This slows the heap-exhaustion check, but releases an extra register \nfor argument passing.  Keep the heap limit in a register, and implement pre-emption by setting a separate \nmemory-resident .ag. The .ag is checked whenever the thread s current allocation block runs out, since \nit would be too expensive to insert another check at every heap\u00adcheck point. This approach is cheap and \neasy, but pre-emption is much less prompt: a thread can allocate up to 4k of data before noticing that \na context-switch is required.  Figure 7 measures the bene.t of using the heap-limit register to signal \na context-switch, versus checking a .ag after each 4k of allocation. We see a slight drop in performance \nat 4 cores, changing to an increase in performance at 7 cores. This technique clearly becomes more important \nas the number of cores and the amount of garbage collection increases: benchmarks like gray that do a \nlot of GC bene.t the most.  6.4 Parallel GC and locality When we initially developed the parallel GC, \nour goal was to im\u00adprove GC performance, so we focused most of our effort on us\u00ading parallelism to accelerate \ngarbage collection for single-threaded programs (Marlow et al. 2008). In this case the key goal is achiev\u00ading \ngood load-balancing, that is, making sure that all of the GC threads have work to do. However, there \nis another factor working in the opposite direc\u00adtion: locality. For parallel programs, when GC begins \neach CPU already has a lot of data in its cache; in a sequential program only one CPU does. It would \nobviously make sense for each CPU to  . Time (%) Program PARGC1 PARGC2 PARGC3 gray -3.9 +52.5 -11.9 \nmandel +3.6 +19.0 -9.9 matmult -14.6 -3.9 -8.1 par.b -0.5 -1.9 -6.4 partree +5.0 -60.7 -65.8 prsa -0.4 \n+1.2 -4.4 ray +2.0 -2.6 -18.0 sumeuler +0.3 -0.5 -1.4 Min -14.6 -60.7 -65.8 Max +5.0 +52.5 -1.4 Geometric \nMean -1.2 -5.1 -19.3 Figure 8. The effectiveness of parallel GC (4 cores) . Time (%) Program PARGC1 \nPARGC2 PARGC3 gray +2.8 +106.2 -3.0 mandel +8.9 +57.5 -5.2 matmult -19.2 +14.1 -14.1 par.b +0.9 -1.5 \n-7.6 partree -2.1 -70.3 -79.8 prsa +3.2 +16.7 +8.2 ray -0.8 +6.2 -5.2 sumeuler +5.1 +0.0 -2.8 Min -19.2 \n-70.3 -79.8 Max +8.9 +106.2 +8.2 Geometric Mean -0.5 +3.7 -21.3 Figure 9. The effectiveness of parallel \nGC (7 cores) garbage-collect its own data, so far as possible, rather than to allow GC to redistribute \nit. Each HEC starts by tracing its own root set, starting from the HEC s private data (Section 4.1). \nHowever, our original parallel GC design used global work queues for load-balancing (Marlow et al. 2008). \nThis is a poor choice for locality, because the link between the CPU that copies the data and the CPU \nthat scans it for roots is lost. To tackle this, we modi.ed our parallel GC design to use work-stealing \nqueues. The bene.ts of this are threefold: 1. Contention is reduced. 2. Locality is improved: a CPU \nwill take work from its own queue in preference to stealing. Local work is likely to be in the CPU s \ncache, because it consists of objects that this CPU recently copied. 3. We can easily disable load-balancing \nentirely, by opting not to steal any work from other CPUs. This trades parallelism in the GC for locality. \n The use of work-stealing queues for load-balancing in parallel GC is a well-known technique (Flood \net al. 2001), however what has not been studied before is the trade-off between whether to do load-balancing \nat all or not for parallel programs. We will measure our benchmarks in three con.gurations: The baseline \nis our best system, with parallel GC turned off.  PARGC1: using parallel GC in the old generation only, \nwith load-balancing.  PARGC2: using parallel GC in both young and old generations, with load-balancing. \n PARGC3: using parallel GC in both young and old generations, without load-balancing. PARGC1 and PARGC2 \nuse work-stealing for load-balancing, PARGC3 uses no load-balancing. In terms of locality, PARGC2 will \nimprove locality signi.cantly by traversing most of the data reachable by parallel threads on the same \nCPU as the thread is executing. PARGC3 will improve locality further by not moving data from one CPU \ns cache to another in an attempt to balance the work of GC. Figures 8 and 9 present the results, for \n4 cores and 7 cores respectively. There are several aspects to these .gures that are striking: partree \ndelivers an 80% improvement with PARGC3 on 7 cores, with most of the bene.t coming with PARGC2. Clearly \nlocality is vitally important in this benchmark.  gray and mandel degrade signi.cantly with PARGC2, \nrecover\u00ading with PARGC3. Load-balancing appears to be having a sig\u00adni.cant negative effect on performance \nhere. These are bench\u00admarks that don t achieve full speedup, so it is likely that when a GC happens, \nidle CPUs are stealing data from the busy CPUs, harming locality more than would be the case if all the \nCPUs were busy.  PARGC3 is almost always better than the other con.gurations.  There are of course \nother possible con.gurations. For instance, parallel GC in the old generation only without load balancing, \nor parallel GC in both generations but with load-balancing only in the old generation. We have performed \ninformal measurements on these and other con.guarations and found that on average they performed less \nwell than PARGC3, although for individual bench\u00admarks it is occasionally the case that a different con.guration \nis a better choice. Future versions of GHC will use PARGC3 by default for paral\u00adlel execution, although \nit will be possible to override the default and select any combination of parallel/sequenctial GC for \neach genera\u00adtion with and without load-balancing. 7. The space behaviour of par The spark pool should \nideally contain only useful work, and we might hope that the garbage collector would assist the scheduler \nby removing useless sparks from the spark pool. One sure-.re way to do so is to remove any .zzled sparks.A \nspark has .zzled if the thunk to which it refers it has already been evaluated, so that running the spark \nwould terminate immediately. Indeed, we expect most sparks to .zzle. The par operation creates opportunities \nfor parallel evaluation but, if the machine is busy, few of these opportunities are taken up. For example, \nconsider x par (y pseq (x+y)) This sparks the thunk x (adding it to the spark pool), evaluates y, and \nthen adds x and y. The addition operation forces both its arguments, so if the sparked thunk x has not \nbeen taken up by some other processor, the addition will evaluate it. In that case, the spark has .zzled. \nClearly a .zzled spark is useless, and the garbage collector can (and does in GHC 6.10) discard them, \nbut which other sparks should the garbage collector retain? Two policies immediately spring to mind, \nthat we shall call ROOT and WEAK: ROOT: Treat (non-.zzled) sparks as roots for the garbage col\u00adlector. \nThat is, retain all such sparks and the graph they point to.  Total MUT GC Total Fizzled time(s) time(s) \ntime(s) sparks Sparks Strat. 10.7 5.1 5.7 1000000 0 No Strat. 6.4 5.2 1.2 1000000 999895 Figure 10. Comparison \nof ray using strategies vs. no strategies WEAK: Only retain (non-.zzled) sparks that are reachable from \nthe roots of the program. The problem is, neither of these policies is satisfactory. WEAK seems attractive, \nbecause it lets us discard sparks that are no longer required by the program. However, the WEAK policy \nis completely incompatible with strategies. Consider the parList strategy: parList :: Strategy a -> Strategy \n[a] parList strat [] = done parList strat (x:xs) = strat x par parList strat xs Each spark generated \nby parList is a thunk for the expression strat x ; this thunk is not shared, since it is created uniquely \nfor the purposes of creating the spark, and hence can never .z\u00adzle. Hence, the WEAK policy will discard \nall sparks created by parList, which is obviously undesirable. So, what about the ROOT policy? This is \nthe policy that is used in existing implementations of GpH, including GUM (Trinder et al. 1996) and GHC. \nHowever, it leads to the converse problem: too many sparks are retained, leading to space leaks. Consider \nthe expression sum (parList rnf (map expensive [1..100000])) With the ROOT policy we will retain all \nof the sparks created by parList, and hence lose no parallelism. But if there are not enough processors \nto evaluate all of the sparks, they will never be garbage collected, even after the sum is complete! \nThey remain in the spark pool, retaining the list elements that they point to. This can lead to serious \nspace leaks3. To quantify the effect, we compared two versions of the ray benchmark. The .rst version \nuses parBuffer from the standard strategies library, applied to the rwhnf strategy, while the second \nuses a modi.ed version of parBuffer which avoids the space leak (we will explain how the modi.ed version \nworks in Section 7.2). We ran both versions of the program on a single CPU, to illustrate the degenerate \ncase of having too few CPUs to use the available par\u00adallelism. Figure 10 gives the results; MUT is the \namount of muta\u00adtor time (execution time excluding garbage collection), GC is the time spent garbage collecting. \nWe can see that with the strategies version, no sparks .zzle, and the GC time suffers considerably as \na result4. Implementations using the ROOT policy have been around for quite a long time, and yet the \nproblem has only recently come to light. Why is this? We are not sure, but postulate that the applica\u00adtions \nthat have been used to benchmark these systems do not suffer unduly from the space leaks, perhaps because \nthe amount of extra space retained is small, and there is little or no speculation involved. If there \nare enough CPUs to use all the parallelism, then no space leaks are observed; the problem comes when \nwe want to write a single program that works well when run both sequentially and in parallel. Are there \nany other policies that we should consider? Perhaps we might try to develop a policy along the lines \nof discard sparks 3 In fact, this space leak was reported to the GHC team as a bug, http: //hackage.haskell.org/trac/ghc/ticket/2185. \n4 Why don t all the sparks .zzle in the second version? In fact the runtime does manage to execute a \nfew sparks while it is waiting for IO to happen. that share no graph with the main program . This is \nclearly an im\u00adprovement on the ROOT policy because it lets us discard sparks that share nothing with \nthe main program. However, it is quite dif.cult to establish whether there is any sharing between the \nspark and the main program, since this entails establishing a reaches property, where each closure in \nthe graph is marked if it can reach certain other closures (namely the main program). This is exactly \nthe op\u00adposite of the property that a garbage collector normally establishes, namely is reachable from \n, and is therefore at odds with the way the garbage collector normally works. It requires a completely \nnew traversal, perhaps by reversing all the pointers in the graph. Even if we could implement this strategy, \nit does not completely solve the problem. A spark may share data with the main program, but that is not \nenough: it has to share unevaluated data, and that unevaluated data must be part of what the spark will \nevaluate. Moreover, perhaps we still want to discard sparks that are retaining a lot of unshared data, \nbut still refer to a small amount of shared data, on the grounds that the cost of the space leak outweighs \nthe bene.ts of any possible parallelism. 7.1 Improving space behaviour of sparks One way to improve the \nspace behaviour of sparks is to use the WEAK policy for garbage collection. This guarantees, by construc\u00adtion, \nthat the spark pool does not leak any space whatsoever. How\u00adever, this choice would affect the programming \nmodel. In partic\u00adular we can no longer use the strategies abstraction as it stands, because every strategy \ncombinator involves sparking unique, un\u00adshared thunks, which WEAK will discard immediately. It is for \nthis reason that GHC still uses the ROOT policy: if we were to switch to WEAK, then existing code using \nStrategies would lose parallelism. We can continue to write parallel programs without space leaks under \nthe ROOT policy, as long as we observe the rule that all sparks must be eventually evaluated. Then we \ncan be sure that any unused sparks will .zzle, and in this case there is no difference between ROOT and \nWEAK. The following section describes how to write programs in this way. Nonetheless, we believe that \nin the long term the implemen\u00adtation should use the WEAK policy. The WEAK policy has one distinct advantage \nover ROOT, namely that it is possible to write programs that use speculative parallelism without incurring \nspace leaks. A speculative spark is by its nature one that may or may not be eventually evaluated, and \nin order to ensure that such sparks are eventually garbage collected if they turn out not to be required, \nwe need to use WEAK.  7.2 Avoiding space leaks with ROOT We can still use strategy-like combinators \nwith ROOT, but they are no longer compositional. In the case of parList, if we simply want to evaluate \neach element to weak-head-normal-form, we use a specialised version of parList: parListWHNF :: Strategy \n[a] parListWHNF [] = done parListWHNF (x:xs) = x par parListWHNF xs Now, as long as the list we pass \nto parListWHNF is also evaluated by the main program, the sparks will all be garbage collected as usual. \nThe rule of thumb is to always put a variable on the left of par. Reducing the granularity with parListChunk \nis a common technique. The idea is for each spark to evaluate a .xed-size chunk of the list, rather than \na single element. To do this without incurring a space leak means that the sparked list chunks must be \nconcate\u00adnated into a new list, and returned to the caller:  parListChunkWHNF :: Int -> [a] -> [a] parListChunkWHNF \nn = concat . ( using parListWHNF) . map ( using seqList) . chunk n where chunk :: Int -> [a] -> [[a]] \nsplits a list into chunks of length n. A combinator that we .nd ourselves using often is parBuffer, which \nbehaves like parList except that it does not traverse the whole list eagerly; it sparks a .xed number \nof elements initially, and then sparks subsequent elements as the list is consumed. This formulation \nworks particularly well with programs that produce output as a lazy list, since it allows us to retain \nthe constant-space property of the program while taking advantage of parallelism. The disadvantage is \nthat we have to pick a buffer size, and the best choice of buffer size might well depend on how many \nCPUs we have available. Our modi.ed version of parBuffer that avoids space leaks is parBufferWHNF: parBufferWHNF \n:: Int -> [a] -> [a] parBufferWHNF n xs = return xs (start n xs) where return (x:xs) (y:ys) = y par (x \n: return xs ys) return xs [] = xs start !n [] = [] start 0 ys = ys start !n (y:ys) = y par start (n-1) \nys  7.3 Compositional strategies revisited We can recover a more compositional approach to strategies \nby changing their type. The existing Strategy type is de.ned thus: type Strategy a = a -> Done Suppose \nthat instead we de.ne Strategy as a projection, like this: type Strategy a = a -> a then a Strategy can \ndo some evaluation and sparking, and re\u00adturn a new a. In order to use this new kind of Strategy effectively, \nwe need a new version of the par combinator: spark :: Strategy a -> a -> (a -> b) -> b spark strataf \n=x par fx where x = strat a pseq a The spark combinator takes a strategy strat, a value a, and a continuation \nf. It creates a spark to evaluate strat a, and then passes a new object to the continuation with the \nsame value as a. When evaluated, this new object will cause the spark to .zzle and be discarded. Now \nwe can recover compositional parList and seqList combinators: parList :: Strategy a -> Strategy [a] parList \nstrat xs = foldr f [] xs where f x xs = spark strat x $ \\x -> xs pseq x:xs seqList :: Strategy a -> Strategy \n[a] seqList strat xs = foldr seq ys ys where ys = map strat xs and indeed this works quite nicely. Note \nthat parList requires linear stack space; it is also possible to write a version that only requires linear \nheap space, but that requires two traversals of the list. Here is parListChunk in the new style: parListChunk \n:: Int -> Strategy a -> Strategy [a] parListChunk n strat xs = ys pseq concat ys where ys = parList (seqList \nstrat) $ chunk n xs 4 cores 7 cores Program . Time (%) Program . Time (%) gray -2.1 gray +7.5 mandel \n-1.0 mandel +1.8 matmult -7.8 matmult +4.4 par.b -1.0 par.b -8.6 partree +5.2 partree +9.5 prsa -0.2 \nprsa -0.1 ray +1.5 ray +0.2 sumeuler -1.2 sumeuler +1.7 Geom. Mean -0.9 Geom. Mean +1.9 Figure 11. The \neffect of eager black-holing 8. Thunks and black holes Suppose that two Haskell threads, A and B, begin \nevaluation of a thunk t simultaneously. Semantically, it is acceptable for them both to evaluate t, since \nthey will get the same answer (Harris et al. 2005); but operationally it is better to avoid this duplicated \nwork. The obvious way to do so is to lock every thunk when starting evaluation, but that is expensive; \nmeasurements in the earlier cited work demonstrate an increase in execution time of around 50%. So we \nconsidered several variants that trade a reduced overhead against the risk of duplicated work: EagerBH: \nimmediately on entry, thread A overwrites t with a black hole. If thread B sees a black hole, it blocks \nuntil A per\u00adforms the update (Section 8.1). The window of vulnerability , in which a second thread might \nstart a duplicate evaluation, is now just a few instructions wide. The cost compared to sequen\u00adtial execution \nis an extra memory store on every thunk entry. RtsBH: enlists the runtime system, using the scheme described \nin Harris et al. (2005). The idea is to walk a thread s stack whenever it returns to the scheduler, and \nclaim each of the thunks under evaluation using an atomic instruction. If a thread is found to be evaluating \na thunk already claimed by another thread, then we suspend the current execution and put the thread to \nsleep until the evaluation is complete. Since every thread will return to the scheduler at regular intervals \n(say, to do garbage collection), this ensures that we cannot continue to evaluate the same thunk in multiple \nthreads inde.nitely. The overhead is much less than locking every thunk because most thunks are entered, \nevaluated, and updated during a single scheduler time\u00adslice. PostCheck: As Harris et al. (2005) points \nout, if two threads both succeed in completing the evaluation of the same thunk, and its value itself \ncontains more thunks, there is a danger that an unbounded amount of work can be duplicated. The PostCheck \nstrategy adds a test just before the update to check whether the thunk has already been updated by another \nthread. This test does not use an atomic instruction, but reduces the chance of further duplicate work \ntaking place. In our earlier work (Harris et al. 2005) we measured of the over\u00adheads of locking every \nthunk, but said nothing about the overheads or work-duplication of the other strategies. GHC 6.10.1 implements \nRtsBH by default. Figure 11 shows the additional effect of EagerBH on our benchmark programs, for 4 and \n7 cores. As you might expect, the effect is minor, because RtsBH catches almost all the cases that EagerBH \ndoes, except for very short-lived thunks which do not matter much anyhow. Fig\u00adure 12 shows how many times \nRtsBH catches a duplicate com\u00adputation in progress, both with and without adding EagerBH. As we can see, \nwithout EagerBH there are occasionally a substantial number of duplicate evaluations (eg. in ray), but \nEagerBH reduces RtsBH + Program RtsBH EagerBH  gray 00 mandel 3 0 matmult 5 5 par.b 70 1 partree 3 3 \nprsa 45 0 ray 1261 0 sumeuler 0 0 Figure 12. The number of duplicate computations caught that number \nto almost zero. In ray, although we managed to elim\u00adinate a large number of duplicate evaluations using \nEagerBH, the effect on overall execution time was negligible: this program cre\u00adates 106 tiny sparks, \nso 1200 duplicate evaluations has little impact. In fact, with the .ne granularity in this benchmark, \nit may be that the cost of suspending the duplicate evaluation and blocking the thread outweighs the \ncost of just duplicating the computation. To date we have not measured the effect of PostCheck. We expect \nit to have no effect on these benchmarks, especially in combination with EagerBH. However, we have experienced \nthe effect of unbounded duplicate work in other programs; one good example where it can occur is in this \nversion of parMap: parMap :: (a -> b) -> [a] -> [b] parMap f [] = [] parMap f (x:xs) = fx par (pmxs par \n(fx:pmxs)) where fx=f x pmxs = parMap f xs This function sparks both the head and the tail of the list, \ninstead of traversing the whole list sparking each element as in the usual parMap. The duplication problem \noccurs if two threads evaluate the pmxs thunk: then the tail of the list is duplicated, possibly resulting \nin a large number of useless sparks being created. 8.1 Blocking on a black hole When a thread A tries \nto evaluate a black hole, it must block until the thread currently evaluating the black hole (thread \nB) completes the evaluation, and overwrites the thunk with (an indirection to) its value. In earlier \nimplementations (before 6.6) we arranged that thread B would attach its TSO to the thunk, so that thread \nA could re-awaken B when it performed the update. But that requires expensive synchronisation on every \nupdate, in case the thunk by now has a sleeping thread attached to it. Since thunk updates are very common, \nbut collisions (in which a sleeping thread attaches itself to a thunk) are very rare, GHC 6.10 instead \noptimises for the common case. Instead of attaching itself to the thunk, the blocked thread B simply \npolls the thunk, waiting for the update. Since a thunk can only be updated once, an update can therefore \nbe performed without any synchronisation whatsoever, provided that writes are not re-ordered. Our earlier \nwork (Harris et al. 2005) discusses these synchronisation issues in much more detail. GHC 6.10 maintains \na single, global Black Hole Pool, which the HECs poll when they are otherwise idle, and at least once \nper GC. We have considered two alternative designs: (a) privatising the Black Hole Pool to each HEC; \nand (b) using the thread scheduler directly, by making the blocked thread sleep and retry the evalua\u00adtion \nwhen it reawakens. We have not yet measured these alterna\u00adtives but, since contention is rare (Figure \n12), they will probably only differ in extreme cases. 4 cores 7 cores Program . Time (%) Program . Time \n(%) gray +2.9 gray +2.9 mandel +4.2 mandel +1.2 matmult +42.1 matmult -0.0 par.b -0.3 par.b +7.5 partree \n+3.3 partree +7.0 prsa -0.5 prsa -1.5 ray -5.4 ray -14.4 sumeuler -0.2 sumeuler +1.2 Geom. Mean +5.0 \nGeom. Mean +0.3 Figure 13. Disabling thread migration 9. Load balancing and migration In this section \nwe discuss design choices concerning which HEC should run which Haskell threads. 9.1 Sharing runnable \nthreads In the current implementation, while we (now) use work-stealing for sparks, we use work-pushing \nfor threads. That is, when a HEC detects that it has more than one thread in its Run Queue and there \nare other idle HECs, it distributes some of the local threads to the other HECs. The reason for this \ndesign is mostly historical; we could without much dif.culty represent the Run Queue using a work-stealing \nqueue and thereby use work-stealing for the load\u00adbalancing of threads. We measured the effect that automatic \nthread migration has on the performance of our parallel benchmarks. Figure 13 shows the effect of disabling \nautomatic thread migration, against a baseline of the PARGC3 con.guration (Section 6.4). Since these \nare par\u00adallel, rather than concurrent, programs, the only way that multiple threads can exist on the \nRun Queue of a single CPU is when a thread becomes temporarily blocked (on a blackhole, Section 8.1), \nand then later becomes runnable again. As we can see from the re\u00adsults, often allowing migration makes \nno difference. Occasionally it is essential: for example matmult on 4 cores. And occasionally, as in \nray, allowing migration leads to worse performance, possibly due to lost locality. Whether to allow migration \nor not is a runtime .ag, so the programmer can experiment with both settings to .nd the best one. 9.2 \nMigrating on wakeup A blocked thread can be woken up for various reasons: if it is blocked on a black \nhole, it is woken up when some HEC notices that the black hole has now been evaluated (Section 8.1); \nif it is blocked on an empty MVar, then it can be unblocked when another thread performs a putMVar operation \non that MVar. When a thread is woken up, if it was previously running on another HEC, we have a choice: \nit can be placed on the Run Queue of the current HEC (hence migrating it), or we could arrange to awaken \nit on the HEC it was previously running on. In fact, we could wake it up on any HEC, but typically these \ntwo options are the most pro.table. Moving the thread to the current HEC might be advantageous if the \nthread is involved in a series of communications with another thread on this HEC: context-switching between \ntwo threads on the same HEC is particularly cheap. However, locality might also be important: the thread \nmight be referencing data that is in the cache of the other HEC. In GHC we take locality seriously, so \nour default is not to mi\u00adgrate awoken threads to the current CPU. For parallel programs, it is never \nworthwhile to change this setting, at least with the current implementation of black holes, since it \nis essentially random which HEC awakens a blocked thread. If we were to change the imple\u00admentation of \nblack holes such that a thread can tell when an update should wake a blocked thread (perhaps by using \na hash table to map the address of black holes to blocked threads), then there might be some bene.t in \nmigrating the blocked thread to the CPU on which the value it was waiting for resides.  10. Benchmarks \nand experimental setup Our test system consists of 2 quad-core Intel Xeon(R) E5320 pro\u00adcessors at 1.6GHz. \nEach pair of cores shares 4MB of L2 cache, and there is 16GB of system memory. The system was running \nFedora 9. Although the OS was running in 64-bit mode, we used 32-bit binaries for our measurements (programs \ncompiled for 64-bit tend to place more stress on the memory system and garbage collector resulting in \nless parallelism). In all cases we ran the programs .ve times and took the average wall-clock execution \ntime. Our benchmarks consist of a selection of small-to-medium\u00adsized Parallel Haskell programs: par.b: \nthe ubiquitous parallel .bonacci function, included here as a sanity test to ensure that our implementation \nis able to parallelise micro-benchmarks. The parallelism is divide-and\u00adconquer-style, using explicit \npar and pseq.  sumeuler: the sum of the value of Euler s function applied to each integer up to a given \nbound. This is a map/reduce style problem: applications of the Euler function can be performed in parallel, \nand the results must be summed (Trinder et al. 2002). The parallelism is expressed using parListChunk \nfrom the strategies library.  matmult: A naive matrix-multiply algorithm. The matrix is represented \nas a [[Int]]. The parallelism is expressed using parListChunk.  ray: A ray-tracer benchmark5. The parallelism \nis expressed using parBuffer, and is quite .ne-grained (each pixel to be rendered is a separate spark). \n gray: Another ray-tracing benchmark, this time taken from an entry6 in the ICFP 00 programming contest. \nOnly the rendering part of the program has been parallelised, using a parBuffer as above. According to \ntime pro.ling, the program only spends about 50% of its time in the renderer, so we expect this to limit \nthe parallelism we can achieve. The parallelism is expressed using a single parBuffer in the renderer. \n prsa: A parallel RSA message encoder, encoding a 500KB message. Parallelism is again expressed using \nparBuffer.  partree: A parallel map and fold over a tree. The program orig\u00adinates in the GUM benchmark \nsuite, and in fact appears to be badly written: it is quadratic in the size of the tree. Neverthe\u00adless, \nit does appear to run in parallel, so we used the program unmodi.ed for the purposes of benchmarking. \n mandel: this is a mandelbrot-set program originating in the nofib benchmark suite (Partain 1992). It \ngenerates a lazy list of pixel data (for a 1024x1024 scene), in a similar way to the ray tracer, and \nit was parallelised in the same way with the addition of parBuffer. The difference in this case is that \nthe parallelism is more coarse-grained: each scan-line of the result is a separate spark.  5 This program \nhas a long history. According to comments in the source code, it was taken from Paul Kelly s book, adapted \nby Greg Michaelson for SML, converted to (parallel) Haskell by Kevin Hammond . 6 from the Galois team \n These programs are all small, are mostly easy to parallelise, and are not highly optimised, so the results \nwe report here should be interpreted as suggestive rather than conclusive. Nevertheless, our goal has \nnot been to optimise the programs, but rather to optimise the implementation to make existing programs \nparallelise better. Furthermore, smaller benchmarks have their uses: Small benchmarks show up in high \nrelief interesting differences in the behaviour of our runtime and execution model. These differences \nwould be less marked had we used only large pro\u00adgrams.  We know that most of these programs should parallelise \nwell, so any lack of parallelism is more likely to be as a result of choices made in the language implementation \nthan in the program itself. Indeed, the lack of linear speedup in Figure 1 shows that we still have plenty \nof room for improvement.  10.1 Pro.ling To help understand the behaviour of our benchmark programs we \ndeveloped a graphical viewer called ThreadScope for event infor\u00admation generated by the runtime system. \nThe viewer is modeled after circuit waveform viewers with a pro.le drawn with time on the x-axis and \nHEC number on the y-axis. In each HEC s timeline, the bar is coloured solid black when the HEC is running \nHaskell code with the thread ID inside the box, and gray when it is garbage collecting. Events, such \nas threads blocking or being woken up, are indicated by labels annotating the timeline when the view \nis zoomed enough. This visualisation of the execution is immensely useful for be\u00ading able to quickly \nidentify problem areas. For example, when we ran our benchmarks on all 8 cores of our 8-core machine, \nwe ex\u00adperienced inexplicable drops in performance. Figure 14 shows one problem area as seen in the pro.le \nviewer. In the middle of the pic\u00adture there is a long period where one HEC has initiated a GC, and is \nwaiting for the other HECs to stop. The initiating/waiting HEC has a white bar, the HECs that have already \nstopped and are ready to GC are shown in gray. One HEC is running (black with thread ID 164) during this \nperiod, indicating that it is apparently running Haskell code and has not responded to the call for a \nGC. In fact, the OS thread running this HEC has been descheduled by the OS, so does not respond for a \nrelatively long period. The same pattern re\u00adpeats many times during the execution, having a signi.cant \nimpact on the overall runtime. This experience does illustrate that our runtime is particularly sensitive \nto problems such as this due to the relatively high fre\u00adquency of full synchronisations needed for GC, \nand that tackling independent GC (Section 12.1) should be a high priority.  11. Related work The design \nspace for language features to support implict paral\u00adlelism and the underlying run-time system is very \nlarge. Here we identify just a few systems that make different design decisions and trade-offs from the \nGHC run-time system. Like GHC the Manticore (Fluet et al. 2008b,a) system also sup\u00adports implicit and \nexplicit .ne-grained parallelism which in turn has been in.uenced by previous work on data parallel languages \nlike NESL (Blelloch et al. 1994) and Nepal/DPH (Chakravarty et al. 2001). Unlike NESL or Nepal/DPH, GHC \nalso implements support for explicit concurrency as does Manticore. Many of the underlying implementation \nchoices made for GHC and Manti\u00adcore are interchangeable e.g. Manticore uses a partially shared heap whereas \nGHC uses a totally shared heap. Manticore however presents quite a different programming model based \non parallel data structures (e.g. tuples and arrays) which provide a fork-join pattern of computation \nas well as a parallel case expression which can introduce non-determinism. Neither GHC nor Manticore \nsup\u00adport implicit parallelism without the need for user annotations which has been implemented in other \nfunctional languages like Id (Nikhl 1991), pH (Nikhl and Arvind 2001) and Sisal (Gaudiot et al. 1997). \nSTING (Jagannathan and Philbin 1992) is a system that supports multiple parallel language constructs \nfor a dialect of SCHEME through three layers of process abstraction as well as special sup\u00adport for specifying \nscheudling policies. We also intend to modify GHC s infrasturcture to allow different scheduling policies \nto be composed together in a .exible manner (Li et al. 2007a). GpH (Trinder et al. 1998) extended Haskell98 \nto introduce the parallel (par) and sequential (seq) coordination primitives and provides strategies \nfor controlling the evaluation order. Unlike semi-implicit parallelism annotations in Haskell which identify \nop\u00adportunities for parallelsim, in Eden (Loogen et al. 2005) one ex\u00adplicitly creates processes which \nare always executed concurrently. GUM (Trinder et al. 1996) targets distributed systems and is based \non message passing. In our work we pro.led the execution time of Haskell threads and garbage collection. \nHowever, we will also need to perform space pro.ling and the work on the MLton project on semantic space \npro.ling (Spoonhower et al. 2008) represents an interesting approach for a strict language. Erlang (Armstrong \net al. 1996) provides isolated threads which communicate through a mailbox mechanism with pattern match\u00ading \nused to select messages of interest. These design decisions have a subtantial effect on the design of \nthe run-time system. Eden (Loogen et al. 2005) provides special annontations to control parallel evaluation \nof processes. Cilk (Blumofe et al. 2001) is an imperative programming lan\u00adguage based on C which also \nsupports .ne grain parallelism in a fork-join manner by spawning off parallel invocations of proce\u00addures. \nLike GHC Cilk also performs work-stealing for load balanc\u00ading. The spawn feature of Cilk, expressions \nbound with pcase in Manticore and sparks in GHC can all be considered to be instances of futures. 12. \nConclusion and future work While we have achieved some signi.cant improvements in parallel ef.ciency, \nour work clearly has some way to go; several bench\u00admarks do not speed up as much as we might hope. Our \nfocus in the future will therefore continue to be on using pro.ling tools to iden\u00adtify problem areas, \nand using those results to direct our attention to appropriate areas of the runtime system and execution \nmodel. The work on implicit parallelization described in Harris and Singh (2007) may bene.t from the \nrecent changes to the GHC run-time and we are considering re-running the benchmarks to measure any improvements. \nIn particular we expect benchmarks that perform a lot of garbage collection to bene.t from the parallel \ngarbage collector. It is clear from our investigation of the programming model in Section 7 that we should \nchange the GC policy for sparks from ROOT to WEAK, but we must also revisit the Strategies abstraction \nand develop a new library that works effectively under WEAK. 12.1 Independent GC Stop-the-world GC will \ninevitably become more of a bottleneck as the number of cores increases. There are known techniques for \ndoing CPU-independent GC (Doligez and Leroy 1993), and these techniques are used in systems such as Manticore \n(Fluet et al. 2008a). We fully intend to pursue CPU-independent GC in the future. However this is unlikely \nto be an easy transition. CPU-independent GC replaces direct sharing by physical separation and explicit \ncom\u00admunication. This leads to trade-offs; it isn t a straightforward win. More speci.cally, CPU-independent \nGC requires a local-heap in\u00advariant, namely that there are no pointers between local heaps, or from the \nglobal heap into any local heap. Ensuring and maintain\u00ading this invariant introduces new costs and complexities \ninto the runtime execution model. On the other hand, as the number of cores in modern CPUs increases, \nthe illusion of shared memory begins to break down. We are already experiencing severe penalties for \nlosing locality (Section 6.4), and it is likely that these will only get worse in the future. Hence, \nmoving to more explicitly-separate heap regions is a more honest re.ection of the underlying memory architecture, \nand is likely to allow the implementation to make intelligent decisions about data locality. Acknowledgments \nWe wish to thank Jost Berthold, who helped us identify some of the bottlenecks in the parallel runtime \nimplementation, and built the .rst implementation of work-stealing queues for spark distribution during \nan internship at Microsoft Research in the summer of 2008. We also wish to thank Phil Trinder and Tim \nHarris for their helpful comments on an earlier draft of this paper. References J. R. Armstrong, R. Virding, \nC. Wikstr\u00a8om, and M. Williams. Con\u00adcurrent programming in ERLANG (2nd ed.). Prentice Hall Inter\u00adnational \n(UK) Ltd., Hertfordshire, UK, 1996. Nimar S. Arora, Robert D. Blumofe, and C. Greg Plaxton. Thread scheduling \nfor multiprogrammed multiprocessors. In In Pro\u00adceedings of the Tenth Annual ACM Symposium on Parallel \nAl\u00adgorithms and Architectures (SPAA), Puerto Vallarta, pages 119 129, 1998. J. Berthold, S. Marlow, A. \nAl Zain, and K. Hammond. Compar\u00ading and optimising parallel Haskell implementations on multi\u00adcore. In \nIFL 08: International Symposium on Implementation and Application of Functional Languages (Draft Proceedings), \nHat.eld, UK, 2008. Stephen M. Blackburn and Antony L. Hosking. Barriers: friend or foe? In ISMM 04: Proceedings \nof the 4th international symposium on Memory management, pages 143 151, New York, NY, USA, 2004. ACM. \nISBN 1-58113-945-4. doi: http://doi. acm.org/10.1145/1029873.1029891. G. E. Blelloch, S. Chatterjee, \nJ. C. Hardwick, J. Sipelstein, and M Zagha. Implementation of a portable nested data-parallel language. \nJDPC, 21(1):4 14, 1994.  R. D. Blumofe, C. F. Joerg, B. C. Kuszmaul, and C. E. Lieserson. Cilk: an ef.cient \nmultithreaded runtime system. SIGNPLAN Not., 30(8):207 216, 2001. M. T. Chakravarty, G. Keller, R. Leshinskiy, \nand W. Pfannenstiel. Nepal -Nested Data Parallelism in Haskell. LNCS, 2150, Aug 2001. David Chase and \nYossi Lev. Dynamic circular work-stealing deque. In SPAA 05: Proceedings of the seventeenth annual ACM \nsym\u00adposium on Parallelism in algorithms and architectures, pages 21 28, New York, NY, USA, 2005. ACM. \nISBN 1-58113-986\u00ad 1. doi: http://doi.acm.org/10.1145/1073970.1073974. Damien Doligez and Xavier Leroy. \nA concurrent, generational garbage collector for a multithreaded implementation of ml. In POPL 93: Proceedings \nof the 20th ACM SIGPLAN-SIGACT symposium on Principles of programming languages, pages 113 123, New York, \nNY, USA, 1993. ACM. ISBN 0-89791\u00ad560-7. doi: http://doi.acm.org/10.1145/158511.158611. Christine Flood, \nDave Detlefs, Nir Shavit, and Catherine Zhang. Parallel garbage collection for shared memory multiprocessors. \nIn Usenix Java Virtual Machine Research and Technology Sym\u00adposium (JVM 01), Monterey, CA, 2001. URL citeseer.ist. \npsu.edu/flood01parallel.html. Matthew Fluet, Mike Rainey, and John Reppy. A scheduling frame\u00adwork for \ngeneral-purpose parallel languages. SIGPLAN Not., 43 (9):241 252, 2008a. ISSN 0362-1340. doi: http://doi.acm.org/ \n10.1145/1411203.1411239. Matthew Fluet, Mike Rainey, John Reppy, and Adam Shaw. Implicitly-threaded Parallelism \nin Manticore. International Conference on Functional Programming, pages 119 130, 2008b. J. L. Gaudiot, \nT. DeBoni, J. Feo, W. Bohm, W. Najjar, and P. Miller. The Sisal model of functional programming and its \nimplementa\u00adtion. In As 97, pages 112 123, Los Altimos, CA, March 1997. IEEE Computer Society Press. Tim \nHarris and Satnam Singh. Feedback directed implicit paral\u00adlelism. In ICFP 07: Proceedings of the 12th \nACM SIGPLAN in\u00adternational conference on Functional programming, pages 251 264, New York, NY, USA, 2007. \nACM. ISBN 978-1-59593\u00ad815-2. doi: http://doi.acm.org/10.1145/1291151.1291192. Tim Harris, Simon Marlow, \nand Simon Peyton Jones. Haskell on a shared-memory multiprocessor. In Haskell 05: Proceedings of the \n2005 ACM SIGPLAN workshop on Haskell, pages 49 61. ACM Press, September 2005. ISBN 1-59593-071-X. doi: \nhttp://doi.acm.org/10.1145/1088348.1088354. URL http:// www.haskell.org/~simonmar/papers/multiproc.pdf. \nS. Jagannathan and J. Philbin. A customizable substrate for con\u00adcurrent languages. In ACM Conference \non Programming Lan\u00adguages Design and Implementation (PLDI 92), pages 55 81. ACM Press, June 1992. P. \nLi, Simon Marlow, Simon Peyton Jones, and A. Tolmach. Lightweight concurrency primitives for GHC. In \nHaskell 07: Proceedings of the 2007 ACM SIGPLAN workshop on Haskell, pages 107 118. ACM Press, September \n2007a. Peng Li, Simon Marlow, Simon Peyton Jones, and Andrew Tol\u00admach. Lightweight concurrency primitives \nfor GHC. Haskell 07: Proceedings of the ACM SIGPLAN workshop on Haskell workshop, June 2007b. URL http://www.haskell.org/ \n~simonmar/papers/conc-substrate.pdf. H-W. Loidl, P.W. Trinder, K. Hammond, S.B. Junaidu, R.G. Mor\u00adgan, \nand S.L. Peyton Jones. Engineering Parallel Symbolic Pro\u00ad grams in GPH. Concurrency Practice and Experience, \n11: 701 752, 1999. URL http://www.cee.hw.ac.uk/\\~{}dsg/ gph/papers/ps/cpe.ps.gz. H.-W. Loidl, F. Rubio, \nN. Scaife, K. Hammond, S. Horiguchi, U. Klusik, R. Loogen, G. J. Michaelson, R. Pe na, S. Priebe,\u00b4 A \nJ. Reb\u00b4on, and P. W. Trinder. Comparing parallel functional languages: Programming and performance. Higher \nOrder Sym\u00adbol. Comput., 16(3):203 251, 2003. ISSN 1388-3690. doi: http://dx.doi.org/10.1023/A:1025641323400. \nRita Loogen, Yolanda Ortega-Mall\u00b4en, and Ricardo Pena. Parallel Functional Programming in Eden. Journal \nof Functional Pro\u00adgramming, 15(3):431 475, 2005. Simon Marlow, Simon Peyton Jones, and Wolfgang Thaller. \nEx\u00adtending the haskell foreign function interface with concur\u00adrency. In Proceedings of the ACM SIGPLAN \nworkshop on Haskell, pages 57 68, Snowbird, Utah, USA, Septem\u00adber 2004. URL http://www.haskell.org/~simonmar/ \npapers/conc-ffi.pdf. Simon Marlow, Tim Harris, Roshan P. James, and Simon Peyton Jones. Parallel generational-copying \ngarbage collection with a block-structured heap. In ISMM 08: Proceedings of the 7th international symposium \non Memory management. ACM, June 2008. URL http://www.haskell.org/~simonmar/ papers/parallel-gc.pdf. R. \nS. Nikhl. ID language reference manual. Laboratory for Computer Science, MIT, Jul 1991. R. S. Nikhl and \nArvind. Implicit Parallel Programming in pH. Morgan Kaufmann Publishers, San Francisco, CA, 2001. WD \nPartain. The nofib benchmark suite of Haskell programs. In Functional Programming, Glasgow 1992, Workshops \nin Com\u00adputing, pages 195 202. Springer Verlag, 1992. S. Peyton Jones, A. Gordon, and S. Finne. Concurrent \nHaskell. In Proc. of POPL 96, pages 295 308. ACM Press, 1996. Simon Peyton Jones, Roman Leshchinskiy, \nGabriele Keller, and Manuel M. T. Chakravarty. Harnessing the multicores: Nested data parallelism in \nHaskell. In FSTTCS 2009: IARCS Annual Conference on Foundations of Software Technology and Theo\u00adretical \nComputer Science, 2009. Daniel Spoonhower, Guy E. Blelloch, Robert Harper, and Phillip B. Gibbons. Space \npro.ling for parallel functional programs. In ICFP 08: Proceedings of the 12th ACM SIGPLAN international \nconference on Functional programming, pages 253 264, New York, NY, USA, 2008. ACM. P. W. Trinder, H.-W. \nLoidl, and R. F. Pointon. Parallel and distributed Haskells. J. Funct. Program., 12(5):469 510, 2002. \nISSN 0956-7968. doi: http://dx.doi.org/10.1017/ S0956796802004343. PW Trinder, K Hammond, JS Mattson, \nAS Partridge, and SL Pey\u00adton Jones. GUM: a portable parallel implementation of haskell. In ACM Conference \non Programming Languages Design and Implementation (PLDI 96). ACM Press, Philadelphia, May 1996. PW Trinder, \nK Hammond, H-W Loidl, and SL Peyton Jones. Algo\u00adrithm + strategy = parallelism. Journal of Functional \nProgram\u00adming, 8:23 60, January 1998.      \n\t\t\t", "proc_id": "1596550", "abstract": "<p>Purely functional programs should run well on parallel hardware because of the absence of side effects, but it has proved hard to realise this potential in practice. Plenty of papers describe promising ideas, but vastly fewer describe real implementations with good wall-clock performance. We describe just such an implementation, and quantitatively explore some of the complex design tradeoffs that make such implementations hard to build. Our measurements are necessarily detailed and specific, but they are reproducible, and we believe that they offer some general insights.</p>", "authors": [{"name": "Simon Marlow", "author_profile_id": "81100515135", "affiliation": "Microsoft Research Ltd., Cambridge, United Kingdom", "person_id": "P1613989", "email_address": "", "orcid_id": ""}, {"name": "Simon Peyton Jones", "author_profile_id": "81100271851", "affiliation": "Microsoft Research Ltd., Cambridge, United Kingdom", "person_id": "P1613990", "email_address": "", "orcid_id": ""}, {"name": "Satnam Singh", "author_profile_id": "81100280060", "affiliation": "Microsoft Research Ltd., Cambridge, United Kingdom", "person_id": "P1613991", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1596550.1596563", "year": "2009", "article_id": "1596563", "conference": "ICFP", "title": "Runtime support for multicore Haskell", "url": "http://dl.acm.org/citation.cfm?id=1596563"}