{"article_publication_date": "08-31-2009", "fulltext": "\n Partial Memoization of Concurrency and Communication Lukasz Ziarek KC Sivaramakrishnan Suresh Jagannathan \nDepartment of Computer Science Purdue University {lziarek,chandras,suresh}@cs.purdue.edu Abstract Memoizationisa \nwell-known optimization technique usedto elim\u00adinate redundant calls for pure functions. If a call to \na function f with argument v yields result r,a subsequent call to f with v can be immediately reduced \nto r without the need to re-evaluate f s body. Understanding memoization in the presence of concurrencyand \ncommunicationis signi.cantly more challenging.Forexample,if f communicates with other threads, it is \nnot suf.cient to simply record its input/output behavior; we must also track inter-thread de\u00adpendencies \ninduced by these communication actions. Subsequent calls to f can be elided only if we can identify an \ninterleaving of actions from these call-sites that lead to states in which these de\u00adpendencies are satis.ed. \nSimilarissues ariseif f spawns additional threads. In this paper, we consider the memoization problem \nfor a higher-order concurrent language whose threads may communicate through synchronous message-based \ncommunication.Toavoid the need to perform unbounded state space search that may be neces\u00adsary to determine \nif all communication dependencies manifest in an earlier call can be satis.ed in a later one, we introduce \na weaker notion of memoization called partial memoization that gives im\u00adplementations the freedom to \navoid performing some part, if not all, of a previously memoized call. To validate the effectiveness \nof our ideas, we consider the bene\u00ad.ts of memoization for reducing the overhead of recomputation for \nstreaming, server-based, and transactional applicationsexecuted on a multi-core machine. We show that \non a variety of workloads, memoization can lead to substantial performance improvements without incurring \nhigh memory costs. Categories and Subject Descriptors D.3.3[LanguageConstructs andFeatures]: Concurrent \nprogramming structures; D.1.3[Con\u00adcurrent Programming]; D.3.1[Formal De.nitions and Theory]: Semantics \nGeneral Terms Design, Experimentation, Languages, Perfor\u00admance, Theory, Algorithms Keywords Concurrent \nProgramming,Partial Memoization, Soft\u00adwareTransactions, Concurrent ML, Multicore Systems. Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page.To copyotherwise, to republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. ICFP 09, August 31 September \n2, 2009, Edinburgh, Scotland, UK. Copyright c &#38;#169; 2009ACM 978-1-60558-332-7/09/08... $5.00. 1. \nIntroduction Eliminating redundant computation is an important optimization supported by manylanguage \nimplementations. One important in\u00adstance of this optimization class is memoization (Liu and Teitel\u00adbaum \n1995;PughandTeitelbaum 1989; Acaretal. 2003),a well\u00adknown dynamic technique that can be utilized to avoid \nperforming a function application by recording the arguments and results of previous calls.Ifa callis \nsuppliedanargumentthathas been pre\u00adviously cached, the execution of the function body can be elided, \nwith the corresponding result immediately returned instead. When functions perform effectful computations, \nleveraging memoization becomes signi.cantly more challenging. Two calls to a function f that performs \nsome stateful computation need not generate the same result if the contents of the state f uses to pro\u00adduce \nits result are different at the two call-sites. Concurrency and communication introduce similar complica\u00adtions. \nIf a thread calls a function f that communicates with func\u00adtions invoked in other threads, then memo \ninformation recorded with f must include the outcome of these actions. If f is subse\u00adquently applied \nwith a previously seen argument, and its commu\u00adnication actions at this call-site are the same as its \neffects at the original application, re-evaluation of the pure computation in f s body canbeavoided. \nBecauseof thread interleavings, synchroniza\u00adtion, and non-determinism introducedby scheduling choices, \nmak\u00ading such decisions is non-trivial. Nonetheless, we believe memoization can be an important com\u00adponent \nin a concurrent programming language runtime. Our be\u00adlief is enforced by the widespread emergence of \nmulti-core plat\u00adforms, and renewed interest in streaming (Gordon et al. 2006), speculative (Pickett and \nVerbrugge 2005) and transactional (Har\u00adris and Fraser 2003; Adl-Tabatabai et al. 2006) abstractions to \nprogram these architectures.For instance, optimistic concurrency abstractions rely on ef.cient control \nand state restoration mech\u00adanisms. When a speculation fails because a previously available computation \nresource becomes unavailable, or when a transaction aborts due to a serializability violation and must \nbe retried (Harris etal. 2005), theireffects mustbe undone.Failure representswasted work, both in terms \nof the operations performed whose effects must now be erased, and in terms of overheads incurred to implement \nstate restoration; these overheads include logging costs, read and write barriers, contention management, \netc.Onewayto reducethis overheadistoavoid subsequent re-executionof those function calls previouslyexecutedbythefailed \ncomputation whose results are un\u00adchanged. Thekeyissueis understanding when utilizing memoized informationissafe,giventhe \npossibilityof concurrency, communi\u00adcation, and synchronization among threads. In this paper, we consider \nthe memoization problem for a higher-order concurrent language in which threads communicate through synchronous \nmessage-passing primitives (e.g. Concurrent ML (Reppy1999)).Asynchronizationevent acknowledges theex\u00adistenceofanexternal \naction performedby another thread willingto sendorreceivedata.Ifsuchevents occur withinafunction f whose \napplications are memoized, thenavoiding re-executionatacall-site c is only possible if these actions \nare guaranteed to succeed at c. In other words, using memo information requires discovery of inter\u00adleavings \nthat satisfy the communication constraints imposed by a previous call. If we can identify a global state \nin which these con\u00adstraints are satis.ed, the call to c can be avoided; if there exists no such state, \nthen the call must be performed. Because .nding such a state can be expensive (it may require an unbounded \nstate space search),we consideraweaker notionof memoization:byrecording the context in which a memoization \nconstraint was generated, im\u00adplementations canalways chooseto simply resumeexecutionofthe function at \nthe program point associated with the constraint using the saved context. In other words, rather than \nrequiring global exe\u00adcutionto reachastatein whichall constraintsinamemoized appli\u00adcation are satis.ed, \npartial memoization gives implementations the freedom to discharge some fraction of these constraints, \nperforming the rest of the application as normal. Although our description and formalization is developed \nin the context of message-based com\u00admunication, the applicability of our solution naturally extends to \nshared-memory communication as well given the simple encoding of the latter in terms of the former (Reppy1999). \n Wheneveraconstraintbuilt during memoizationis dischargedon a subsequent application, there is a side-effect \non the global state that occurs. For example, consider a communication constraint associated with a memoized \nversion of a function f that expects a thread T to receive data d on channel c.To use this information \nat a subsequent call, we must identify the existence of T, and having done so, must propagate dalong \nc for T to consume. Thus, whenever a constraint is satis.ed, an effect that re.ects the action represented \nby that constraint is performed. We consider the set of constraintsbuilt during memoization as forming \nan ordered log, with each entry in the log representing a condition that must be satis.ed to utilize \nthe memoized version, and an effect that must be performed if the condition holds. The point of memoization \nfor our purposesisthustoavoid performingthepure computationsthat execute between these effectful operations. \n 1.1 Contributions Besides providing a formal characterization of these ideas, we also present performance \nevaluation of two parallel benchmarks. We consider the effect of memoization on improving performance \nof multi-threaded CML applications executing on a multicore archi\u00adtecture. Our results indicate that \nmemoization can leadto substan\u00adtial runtime performance improvement over a non-memoized ver\u00adsion of the \nsame program, with only modest increases in memory overhead (15% on average). To the best of our knowledge, \nthis is the .rst attempt to formalize a memoization strategy for effectful higher-order concurrent lan\u00adguages,andtoprovidean \nempiricalevaluationofits impactonim\u00adproving wall-clock performance for multi-threaded workloads. The \npaper is organized as follows. The programming model is de.ned in Section 2. Motivation for the problem \nis given in Section3. The formalizationof our approachisgivenin Sections4 and Section 5. A detailed description \nof our implementation and results aregivenin Sections6and7.Wediscusspreviousworkand provide conclusions \nin Section 8. 2. Programming Model Our programming modelisa simple synchronous message-passing dialect \nof ML similar to CML. Threads communicate using dynam\u00adically created channels through which theyproduce \nand consume values. Since communication is synchronous, a thread wishing to communicate on a channel \nthat has no ready recipient must block until oneexists,andall communicationon channelsis ordered.Our \nformal treatment does not consider references,but there are no ad\u00additional complications that ensue in \norder to handle them; our im\u00adplementation supports all of Standard ML. In this context, deciding whether \na function application can be avoided based on previously recorded memo information depends upon the \nvalue of its arguments, its communication actions, chan\u00adnels it creates, threads it spawns, and the return \nvalue it produces. Thus, the memoized result of a call to a function f can be used at a subsequent call \nif (a) the argument given matches the argument previously supplied;(b) recipients forvalues sentby f \non channels in an earlier memoized call are still available on those channels; (c) values consumed by \nf on channels in an earlier call are again ready to be sent to other threads; (d) channels created in \nan earlier call have the same actions performed on them, and (e) threads cre\u00adated by f can be spawned \nwith the same arguments supplied in the memoized version. Ordering constraints on all sends and receives \nperformed by the procedure must also be enforced. A successful application of a memoized call yields \na new state in which the ef\u00adfects captured within the constraint log have been performed; thus, the values \nsent by f are received by waiting recipients, senders on channels from which f expects to receive values \npropagate these values on those channels, and channels and threads that f is ex\u00adpected to create are \ncreated. To avoid making a call, a send action performed within the applied function, for example, will \nneed to be paired with a receive operationexecutedby some other thread. Unfortunately, there may be no \nthread currently scheduled that is waiting to receive on this channel. Consider an application that calls \na memoized function f which (a) creates a thread T that receives a value on channel c, and (b) sends \na value on c computed through values received on other channels that is then consumed by T.To safely \nuse the memoized return value for f nonetheless still requires that T be instantiated, and that communication \nevents executed in the .rst call can still be satis.ed (e.g., the values f previously read on other channels \nare stillavailable on those channels). Ensuring these actions can succeed may involve an exhaustive exploration \nof the execution statespacetoderiveaschedulethatallowsusto consider thecallinthe contextofaglobal stateinwhichthese \nconditionsare satis.ed. Becausesuchanexplorationmaybe infeasiblein practice, our formulation also supports \npartial memoization. Rather than requiring global execution to reach a state in which all constraints \nin a memoized application are satis.ed, partial memoization gives implementations the freedom to discharge \nsomefraction of these constraints, performing the rest of the application as normal. 3. Motivation As \na motivating example, we consider how memoization can be pro.tably utilized in a concurrent message-passing \nred-black tree implementation. The data structure supports concurrent insertion, deletion, and traversal \noperations. Anodeinthetreeisatuple containingthenode scolor,aninteger value, and links to its left and \nright children. Associated with every node is a thread that reads from an input channel, and outputs \nthe node s data on an output channel, effectively encoding a server. Accessing and modifying a tree node \ns data is thus accomplished throughacommunication protocol with the node sinput and output channels. \nAbstractly, a read corresponds to a receive operation (recv )fromanode soutput channel,andawritetoasend(send \n) on a node s input channel.  Whenanodeis initially created,we instantiatetwonewchannels (channel() \n), and spawn a server. The function server , the concrete representation of a node, chooses between accepting \na communication on the incoming channel cIn (corresponding to a write operation) and sending a communication \non the outgoing channel cOut (correspondingtoaread operation), synchronizeson the communication, and \nthen updates the server througharecursive call. datatype rbtree = N of {cOut: node chan, cIn: node chan}and \nnode = Node of {color: color, value: int, left:rbtree, right:rbtree}| EmptyNode fun node(c:color, v:int, \nl:rbtree, r:rbtree):rbtree = let val (cOut, cIn) = (channel(), channel()) val node = Node{color=c,value=v,left=l,right=r}fun \nserver(node) = sync( choose([wrap(sendEvt(cOut, node), (fn x => server(node)) ), wrap(recvEvt(cIn), \n(fn x => server(x)))])) in spawn(fn () => server(node)); N{cOut=cOut, cIn=cIn}) end For example, the \nprocedure contains queries the tree to deter\u00admine if a node containing its integer argument is present. \nIt takes as input the number being searched, and the root of the tree from which to begin the traversal. \nThe recvT operation reads the value from each node, and based on this value check navigates the tree: \nfun recvT(N{cOut, cIn}:rbtree) = recv(cOut) fun contains (n:int, tree:rbtree):bool = let fun check(n, \ntree) = (case recvT(tree) of Node {color,value,left,right} => (case Int.compare (value, n) of EQUAL \n=> true | GREATER => check (n,left) | LESSER => check (n,right)) | EmptyNode => false) in check(n, tree) \nend Memoization can be leveraged to avoid redundant traversals of the tree. Consider the red/black tree \nshown in Fig. 1. Triangles represent abstracted portions of the tree, red nodes are unbolded, and black \nnodes are marked as bold circles. Suppose we memoize the call to contains which .nds the node containing \nthe value 79. Whenever a subsequent call to contains attempts to .nd the node containing 79, the traversal \ncan directly use the result of the previous memoized call if both the structure of the tree along the \npath to the node, and the values of the nodes on the path remainthe same.Bothofthese propertiesare re.ectedinthevalues \ntransmitted by node processes. The path is depicted by shading relevant nodes in gray in Fig. 1. More \nconcretely, we can avoid recomputing the traversal if com\u00admunication with node processes remains unchanged. \nInformally, memo information associated witha function f can be leveraged to avoid subsequent applications \nof f if communication actions per\u00adformed by the memoized call can be satis.ed in these later appli-cations.Thus,to \nsuccessfullyleverage memo informationforacall to contains with input 79, we would need to ensure a subsequent \ncall of the function with the same argument would be able to re\u00adceive the sequence of node values: (red, \n48), (black, 76), (red, 85), and(black,79)inthatorderduringatraversal.InFig.1threadT1 can take advantage \nof memoization, while thread T2 subsequently recolors the node containing 85. Figure 1. Memoization \ncanbe usedtoavoid redundant tree traver\u00adsals. In this example, two threads traverse a red-black tree. \nEach node in the tree is represented as a process that outputs the current value of the node, and inputs \nnew values. The shaded path illus\u00adtrates memoization potential. Figure 2. Even if the constraints stored \nin a memoized call can\u00adnot be fully satis.ed at a subsequent call, it may be possible to discharge some \nfraction of these constraints, retaining some of the optimization bene.ts memoization affords. Because \nof the actions of T2, subsequent calls to contains with argument 79 cannot avail of the information recorded \nduring evaluation of the initial memoized call. As a consequence of T2 s update,a subsequenttraversalbyT1would \nobserveachangetothe tree. Note however, that even though the immediate parent of 79 has changed in color, \nthe path leading up to node 85 has not (see Fig.2).Byleveraging partial memoizationonthe earlier memoized \ncall to contains , a traversal attempting to locate node 79 can avoid traversing this pre.x, if not the \nentire path. Notice that if thenodewithvalue85was later recolored,and assuming structural equality of \nnodes is used to determine memoization feasibility, full memoization would once again be possible. Asthisexample \nsuggests,akeyrequirementforeffectivelyutiliz\u00ading memoized function applications is the ability to track \ncommu\u00adnication(and othereffectful) actions performedbyprevious instan\u00adtiations. Provided that the global \nstatewould permit these same ac\u00adtions(orsome subset thereof)to succeedifafunctionis re-executed with \nthe same inputs, memoization can be employed to avoid re\u00adcomputing applications, or to reduce the amount \nof the application thatisexecuted.We notethat althoughtheexample presented dealt with memoization of \na function that operates over base integer val\u00adues, our solution detailed in the following sections considers \nmem\u00adoization in the context of any value that can appear on a channel, including closures, vectors, and \ndatatype instances.  4. Syntax and Semantics Our semanticsis de.nedin termsofa core call-by-value functional \nlanguage with threading and communication primitives. Commu\u00adnication between threads is achieved using \nsynchronous channels. For perspicuity,we .rst presentasimple multi-threaded language with synchronous \nchannel based communication.We thenextend this core language with memoization primitives, and subsequently \nconsider re.nements of this semantics to support partial memo\u00adization. Although the core language has \nno support for selective communication, extending it to support choice is straightforward. Memoization \nwould simply record the result of the choice and re\u00adplay would only be possible if the recorded choice \nwas satis.able. In the following, we write a to denoteasequenceof zeroor more elements, \u00df.a to denote \nsequence concatenation, and 0/ to denote an empty sequence. Metavariables x and y range over variables, \nt ranges over thread identi.ers, l ranges over channels, v ranges over values, and a,\u00df denote tags that \nlabel individual actions in a program sexecution.We use Pto range over program states, Efor evaluation \ncontexts, and e for expressions. Our communication model is a message-passing system with synchronous \nsend and receive operations. We do not impose a strict ordering of communications on channels; communication \nac\u00adtions on the same channel by different threads are paired non\u00addeterministically.To model asynchronous \nsends, we simply spawn a thread to perform the send. Spawning an expression (that evalu\u00adatestoa thunk) \ncreatesanew threadin whichthe applicationofthe thunk is performed. 4.1 Language The syntax and semantics \nof the language are given in Fig. 3. Ex\u00adpressions are either variables, locations that represent channels, \n.\u00adabstractions, function applications, thread creation operations, or communication actions that send \nand receive messages on chan\u00adnels.We do not consider references in this core language as they can be \nmodeled in terms of operations on channels (Reppy1999). A thread context t[E[e]] denotes an expression \ne available for execution by a thread with identi.er t within context E. Evalua\u00adtionis speci.ed viaa \nrelation( -.)that mapsa program state(P) to another program state. Evaluation rules are applied up to \ncom\u00admutativity of parallel composition(I).Anevaluationstepis marked withatagthat indicatesthe action \nperformedbythatstep.As short\u00ad hand, we write P a -. P' to represent the sequence of actions a that transforms \nPto P'. Application (rule A PP)substitutes the argument value for free occurrences of the parameter in \nthe body of the abstraction, and channel creation (rule C HANNEL)results in the creation of a new location \nthat acts as a container for message transmission and reception. A spawn action (rule SPAWN), given an \nexpression e that evaluates to a thunk changes the global state to include a new threadin which the thunkis \napplied.Acommunicationevent (rule COMM)synchronouslypairsasender attemptingto transmitavalue alongaspeci.c \nchannelinonethreadwithareceiverwaitingonthe same channel in another thread.  4.2 Partial Memoization \nThe core language presented above provides nofacilities for mem\u00adoization of the functions it executes. \nTo support memoization, we must record, in addition to argument and return values, syn\u00adchronous communication \nactions, thread spawns, channel creation etc. as part of the memoized state. These actions de.ne a log \nof constraints(C)that must be satis.ed at subsequent applications of a memoized function, and whose associated \neffects must be discharged if the constraint is satis.ed.To record constraints, we augment our semantics \nto include a memo store(s), a map that given a function identi.er and an argument value, returns the \nset of constraints and result value that was previously recorded for a call to that function with that \nargument. If the set of constraints re\u00adturned by the memo store is satis.ed in the current state (and \ntheir effects performed), then the return value can be used and the ap\u00adplication elided. The memo store \ncontains only one function/value pair for simplicityof the presentation.We can envisionextending the \nmemo store to contain multiple memoized versions of a func\u00adtion based on its arguments or constraints. \nWe utilize two thread contexts t[e] and tC[e], the former to indicate that evaluation of terms should \nbe captured within the memo store, and the latter to indicate that previously recorded constraints should \nbe discharged. We elaborate on their use below. The de.nition of the language augmented with memoization \nsup\u00adportisgivenin Fig.4.We now de.neevaluation usinga new re\u00adlation( =.)de.ned over two global con.gurations. \nIn one case, a con.guration consistsofa program state(P)anda memo store(s). This con.gurationis used \nwhenevaluation does notleverage mem\u00adoized information. The second con.guration is de.ned in terms of \na thread id and constraint sequence pair((t,C)), a program state (P), anda memo store(s); transitions \nuse this con.guation when discharging constraints recorded from a previous memoized appli\u00adcation. In \naddition, a thread state is augmented to hold an additional structure. The memo state(.)records the function \nidenti.er(d), the argument(v)supplied to the call, the context(E)in which the callis performed, and the \nsequenceof constraints(C)that arebuilt during the evaluation of the application being memoized. Constraintsbuilt \nduringa memoized function application de.ne actions that must be satis.ed at subsequent call-sites in \norder to avoid complete re-evaluationofthe functionbody.Fora communi\u00adcation action,aconstraint records \nthe location being operated upon, thevalue sent or received, the action performed(R for receiveand S \nfor send), and the continuation immediately prior to the action be\u00ading performed;the application resumesevaluationfromthispointif \nthe corresponding constraint couldnotbe discharged.Fora spawn operation, the constraint records the action(Sp), \nthe expression be\u00ading spawned, and the continuation immediately prior to the spawn. For a channel creation \noperation(Ch), the constraint records the lo\u00adcation of the channel and the continuation immediately prior \nto the creation operation. Returns are also modeled as constraints(Rt,v) where v is the return value \nof the application being memoized. Consider an application of function f to value v that has been memoized. \nSince subsequent calls to f with v may not be able to discharge all constraints, we need to record the \nprogram points for all communication actions within f that represent potential resump\u00adtionpointsfromwhichnormalevaluationofthe \nfunctionbodypro\u00adceeds; these continuations are recorded as part of any constraint that can fail 1 (communication \nactions, and return constraints as describedbelow).But,sincethecalling contextsattheseothercall\u00adsites \nare different from the original, we must be careful to not in\u00adclude them within saved continuations recorded \nwithin these con\u00adstraints. Thus, the contexts recorded as part of the saved constraint during memoization \nonly de.ne the continuation of the action up to the return pointof the function; the memo state(.)stores \nthe eval\u00aduation context representing the caller s continuation. This context is restored once the application \ncompletes (see ruleRET). 1We also record continuations on non-failing constraints; while not strictly \nnecessary, doing so simpli.es our correctness proofs.  SYNTAX: PROGRAM STATES: P ::= PIP | t[e] P . \nProcess e . Exp ::= x | y | v | e(e) | spawn(e) t . Tid | mkCh() | send(e,e) | recv(e) x,y . Var v . \nVal ::= unit | .x.e | ll . Channel a,\u00df . Tag = {App,Ch,Spn,Com} EVALUATION CONTEXTS: E::=[] | E(e) | \nv(E) | spawn(E) | send(E,e) | send(l,E) | recv(E)  (APP) (CHANNEL) l fresh App PIt[E[.x.e (v)]] -. \nPIt[E[e[v/x]]] Ch PIt[E[mkCh()]] -. PIt[E[l]] (SPAWN) (COMM) t ' fresh P= P'It[E[send(l, v)]]It '[E'[recv(l)]] \nSpnCom PIt[E[spawn(.x.e)]] -. PIt[E[unit]]It '[e[unit/x]] P -. P'It[E[unit]]It '[E'[v]] Figure 3. Aconcurrent \nlanguage with synchronous communication. If function f calls function g ,then actions performedby g must \nbe satis.able in any call that attempts to leverage the memoized version of f . Consider the following \nprogram fragment: let fun f(...) = ... let fun g(...) = ... send(c,v) ... in ... end in ... g(...) ... \nend If we encounter an application of f after it has been memoized, then g s communication action (i.e., \nthe send of v on c )must be satis.able at the point of the application to avoid performing the call.We \ntherefore associatea call stack of constraints(.)with each thread that de.nes the constraints seen thusfar, \nrequiring the constraints computed for an inner application to be satis.able for any memoization of an \nouter one. The propagation of constraints to the memo states of all active calls is given by the operation \n. shown in Fig. 4. Channels created within a memoized function must be recorded in the constraint sequence \nfor that function (ruleCHANNEL). Con\u00adsider a function that creates a channel and subsequently initiates \ncommunicationonthat channel.Ifacalltothis functionwas mem\u00adoized, later applications that attempt to avail \nof memo information must still ensure that the generativeeffect of creating the channel is not omitted. \nFunction evaluation now associates a label with func\u00adtionevaluationthatisusedtoindexthe memo store(ruleFUN). \nIf a new thread is spawned within a memoized application, a spawn constraint is added to the memo state, \nand a new global state is created that starts memoization of the actions performed by the newly spawned \nthread (rule SPAWN). A communication action performed by two functions currently being memoized are also \nappropriately recorded in the corresponding memo state of the threads that are executing these functions \n(rule C OMM). When a memoized application completes, its constraints are recorded in the memo store (ruleRET). \nWhen a function f is applied to argument v, and there exists no previous invocation of f to v recorded \nin the memo store, the func\u00adtion s effects are tracked and recorded (rule APP). Until an appli\u00adcation \nof a function being memoized is complete, the constraints induced by its evaluation are not immediately \nadded to the memo store. Instead, they are maintained as part of the memo state(.) associated with the \nthread in which the application occurs. The most interesting rule is the one that deals with determining \nhow much of an application of a memoized function can be elided (ruleMEMO APP).If an applicationof function \nf with argument v has been recorded in the memo store, then the application can be potentiallyavoided;if \nnot, itsevaluationis memoizedby ruleA PP. Ifamemoizedcallisapplied,wemustexaminethesetof associated constraintsthatcanbe \ndischarged.Todoso,weemployan auxiliary relation I de.ned in Fig. 5. Abstractly, I checks the global state \nto determine which communication, channel creation, and spawn creation constraints (the possible effectful \nactions in our language) canbe satis.ed,and returnsasetoffailed constraints, representing those actions \nthat could not be satis.ed. The thread context(tC[e]) is used to signal the utilization of memo information. \nThe failed constraints are added to the original thread context. RuleMEMO APPyieldsanewglobal con.guration \nwhose thread id and constraint sequence((t,C))corresponds to the constraints satis.able in the current \nglobal state (de.ned as C'')for threadt as de.ned by I. These constraints, when discharged, will leave \nthe thread performing the memoized call in a new state in which the evaluation of the call is the expression \nassociated with the .rst failed constraint returned by I. As we describe below in Sec 4.3, there is always \nat least one such constraint, namely Rt , the return constraintthat holdsthe returnvalueofthe memoized \ncall.Wealso introduce a rule to allow the completion of memo information use (rule END MEMO). The rule \ninstalls the continuation of the .rst currently unsatis.ed constraint; no further constraints are subse\u00adquentlyexamined.In \nthis formulation, the otherfailed constraints are simply discarded; we present an extension of this semantics \nin Section. 4.6 that make use of them.  4.3 Constraint Matching The constraintsbuilt asa resultofevaluating \nthese rules are dis\u00adchargedby the rules shownin Fig.6. Each rulein Fig.6is de.ned with respect to a thread \nid and constraint sequence. Thus, at any given pointin itsexecution,a threadis eitherbuildingup memo \nconstraints (i.e., the thread is of the form t[e])within an applica\u00adtion for subsequent calls to utilize, \nor attempting to discharge these constraints (i.e., the thread is of the form tC[e])for applications \nindexed in the memo store. The function I leveragestheevaluation rules de.nedinFig.6by examining the \nglobal state and determining which constraints can be discharged, except for the return constraint. I \ntakes a constraint set(C)and a program state(P)and returns a sequence of unmatch\u00adable constraints(C').Send \nand receiveconstraints are matched with threads blocked in the global program state on the opposite commu\u00adnication \naction.Onceathreadhasbeen matchedwithaconstraintit is no longeracandidate for future communication since \nits commu\u00adnication action is consumed by the constraint. This guarantees that  SYNTAX: P v . Val E. \nContext ::= ::= PIP | (.,t[e]) | (., tC [e]) unit | .d x.e | l CONSTRAINT ADDITION: .' = {(d,v,E,C.C)|(d, \nv, E,C) . .}.,C .' (CHANNEL) ., ((Ch, l),E[mkCh()]) .' l fresh Ch PI(., t[E[mkCh()]]),s =. PI(.' ,t[E[l]]),s \n(SPAWN) t ' fresh .,((Sp,.d x.e(unit)),E[spawn(.d x.e)]) .' tk = (.' ,t[E[unit]]) ts = (0/, t '[.d x.e(unit)]) \n PROGRAM STATES: d . MemoId c . FailableConstraint=({R,S}\u00d7 Loc \u00d7 Val)+ Rt C . Constraint =(FailableConstraint\u00d7 \nExp)+ ((Sp \u00d7 Exp) \u00d7 Exp) + ((Ch \u00d7 Location) \u00d7 Exp) s . MemoStore = MemoId \u00d7 Val. Constraint* . . MemoState \n= MemoId \u00d7 Val\u00d7 Context \u00d7 Constraint* a, \u00df . Tag = {Ch,Spn, Com, Fun, App,Ret, MCom, MCh,MSp, MemS,MemE, \nMemR, MemP} (FUN) d fresh PI(.,t[E[.x.e]]),s Fun =. PI(.,t[E[.d x.e]]),s (COMM) P= P'I(.,t[E[send(l,v)]])I(.' \n,t '[E'[recv(l)]]) .'' .''' .,((S, l, v), E[send(l,v)]) .' ,((R, l, v), E'[recv(l)]) ts = (.'' tr = \n(.''' ,t[E[unit]]) , t '[E'[v]]) PI(.,t[E[spawn(.d x.e)]]),s Spn=. PItkIts, s P, s Com =. P' ItsItr,s \n(RET) (APP) . = (d,v,E,C) (d, v) . Dom(s) . = (d, v, E, 0/) Ret PI(.,t[E[.d x.e (v)]]), s App PI(..., \nt[v ']),s =. =. PI(...,t[e[v/x]]),s PI(., t[E[v ']]),s[(d,v) . C.(Rt,v ')] (MEMO APP) (END MEMO) (d,v) \n. Dom(s) s(d, v)= C = C'' ') I(C,P)= C' C.C' C=(c,e PI(.,t[E[.d x.e (v)]]),s MemS (t,0/),PI(.,tC.C[E[.d \nx.e (v)]]), s MemE =. (t,C''),PI(.,t c ' [E[.d x.e (v)]]),s =. PI(.,t[E[e ']]),s Figure 4. Aconcurrent \nlanguage supporting memoization of synchronous communication and dynamic thread creation. the candidate \nfunction will communicate at most once with each threadinthe global state. Althougha threadmayinfactbeable \nto communicate more than once with the candidate function, deter\u00admining this requires arbitrary look \nahead and is infeasible in prac\u00adtice.We discuss the implicationsof this restrictionin Section 4.5. Thus,aspawn \nconstraint(ruleMSPAWN)isalways satis.ed,and leads to the creation of a new thread of control. Observe \nthat the application evaluated by the new thread is now a candidate for memoization if the thunk was \npreviously applied and its result is recorded in the memo store. Achannel constraint of the form ((Ch,l), \nE[e])(rule MCH)cre\u00adates a new channel location l ', and replaces all occurrences of l found in the remaining \nconstraint sequence for this thread with l ';the channel location may be embedded within send and receive \nconstraints, either as the target of the operation, or as the argument value being sent or received. \nThus, dischargingachannel constraint ensures that the effect of creating a new channel performed within \nan earlier memoized call is preserved on subsequent applications. The renaming operation ensures that \nlater send and receive con\u00adstraints refer to the new channel location. Both channel creation and thread \ncreation actions neverfail theymodify the global state with a new thread and channel, respectfully, \nbut impose no pre\u00adconditions on the state in order for these actions to succeed. There are two communication \nconstraint matching rules(MCom =. ), bothof whichmay indeedfail.Ifthe current constraintexpectsto receivevalue \nv on channel l ,and thereexistsathread ableto send v on l ,evaluation proceedstoa statein whichthe communication \nsucceeds (the receiving thread now evaluates in a context in which the receiptofthevaluehas occurred),andthe \nconstraintis removed fromthesetof constraintsthatneedtobe matched(ruleMRECV). Note also that the sender \nrecords thefact thatacommunication with a matching receive took place in the thread s memo state, and \nthe receiver does likewise. Anymemoization of the sender must con\u00adsider the receive action that synchronized \nwith the send, and the application in which the memoized call is being examined must record the successful \ndischarge of the receive action. In this way, the semantics permits consideration of multiple nested \nmemoiza\u00adtion actions. If the current constraintexpectstosendavalue v on channel l , and there exists \na thread waiting on l , the constraint is also sat\u00adis.ed (ruleMSEND).A send operation can match with \nany wait\u00ading receive action on that channel. The semantics of synchronous communication allows us the \nfreedom to consider pairings of sends with receives other than the one it communicated with in the orig\u00adinal \nmemoized execution. This is because a receive action places no restriction on either the value it reads, \nor the speci.c sender that provides the value. If there is no matching receiver, the constraint fails. \nObserve that there is no evaluation rule for the Rt constraint that can consume it. This constraint contains \nthe return value of the memoized function(seeruleRET).Ifallother constraintshave  I(((S,l,v),e).C,PI(.,t[E[recv(l)]]))= \nI(C, P) I(((R,l,v),e).C,PI(.,t[E[send(l,v)]]))= I(C, P) I((Rt,v), P) =(Rt,v) I(((Ch, l), e).C,P)= I(C, \nP) I(((Sp, e '),e).C,P)= I(C, P) I(C,P)= C, otherwise Figure 5. The function I yields the set of constraints \nCwhich are not satis.able in program state P. let val (c1,c2) = (mkCh(),mkCh()) fun f () = (send(c1,v1); \n...; recv(c2)) fun g () = (recv(c1); ...; recv(c2); g()) fun h () = (...; send(c2,v2); send(c2,v3); h()); \nfun i () = (recv(c2); i()) in spawn(g); spawn(h); spawn(i); f(); ...; send(c2, v4); ...; f() end Figure \n7. Determining if an application can fully leverage memo informationmay requireexaminingan arbitrary \nnumberof possible thread interleavings. Figure 8. The communication pattern of the code in Fig 7. Circles \nrepresent operations on channels. Gray circles are sends and white circles are receives. Double arrows \nrepresent communications that are captured as constraints during memoization. been satis.ed, it is this \nreturn value that replaces the call in the current context (see the consequentof ruleM EMO APP).  4.4 \nExample The program fragment shown in Fig. 7 applies functions f, g, h and i. The calls to g, h, and \ni are evaluated within separate threads of control, while the applications of f takes place in the original \nthread. These different threads communicate with one other over shared channels c1 and c2. The communication \npattern is depicted in Fig. 8. Separate threads of control are shown as rectangles. Com\u00admunication actions \nare represented as circles; gray for send actions and white for receives. The channel on which the communication \naction takes place is annotated on the circle and the value which is sent on the arrow. Double arrows \nrepresent communication actions for which constraints are generated. To determine whether the second \ncall to f can be elided we must examine the constraints that would be added to the thread state of the \nthreads in which these functions are applied. First, spawn constraintswouldbeaddedtothemainthreadforthethreads \nexecuting g, h, and i. Second, a send constraint followed by a receive constraint, modeling the exchange \nof values v1 and either let fun f() = (send(c,1); send(c,2)) fun g() = (recv(c);recv(c)) in spawn(g); \nf(); ...; spawn(g); f() end Figure 9. The second application of f can only be partially mem\u00adoized up \nto the second send since only the .rst receive made by g is blocked in the global state. v2 or v3 on \nchannels c1 and c2 would be included as well. For the sake of discussion, assume that the send of v2 \nby h was consumed by g and the send of v3 was paired with the receive in f when f() was originally executed. \nConsider the memoizability constraintsbuilt during the .rst call to f . The send constraint on f s application \ncan be satis.ed by matching it with the corresponding receive constraint associated with the application \nof g ;observe g() loops forever, consuming values on channels c1 and c2 . The receive constraint associated \nwith f can be discharged if g receives the .rst send by h , and f receives the second.A schedule that \norders theexecutionof f and g in this way, and additionally pairs i with a send operation on c2 in the \nlet -body would allow the second call to f to fully leverage the memo information recorded in the .rst. \nDoing so would enable eliding the pure computation in f (abstracted by ...) in its de.nition, performing \nonly the effects de.ned by the communication actions (i.e., the send of v1 on c1 , and the receipt of \nv3 on c2 ).  4.5 Issues As thisexample illustrates, utilizing memo information completely may require \nforcing a schedule that pairs communication actions in a speci.c way, making a solution that requires \nall constraints to be satis.ed infeasible in practice. Hence, rule MEMO APP allows evaluation to continue \nwithin an application that has already been memoized once a constraint cannot be matched. As a result, \nif during the second call to f , i indeed received v3 from h , the constraint associated with the recv \noperation in f would not be satis.ed, and the rules would obligate the call to block on the recv , waiting \nfor h or the main body to send a value on c2 . Nonetheless, the semantics as currently de.ned does have \nlimi\u00adtations.Forexample, the function I does not examine future ac\u00adtions of threads and thus can only \nmatch a constraint with a thread if that thread is able to match the constraint in the current state. \nHence, the rules do not allow leveraging memoization information for function callsinvolvedina producer/consumer \nrelation. Con\u00adsider the simple example given in Fig. 9. The second application of f can take advantage \nof memoized information only for the .rst send on channel c. This is because the global state in which \ncon\u00adstraints are checked only has the .rst recv made by g blocked on the channel. The second recv only \noccurs if the .rst is successfully paired witha corresponding send. Althoughin this simpleexample the \nsecond recv is guaranteed to occur, consider if g contained a branch which determined if g consumed a \nsecond value from the channel c. In general, constraints can only be matched against the current communication \naction of a thread. Secondly, exploiting memoization may lead to starvation since subsequent applications \nof the memoized call will be matched based on the constraints supplied by the initial call. Consider \nthe example shown in Fig. 10. If the initial application of f pairs with the send performedby g, subsequent \ncalls to f that use this memo\u00adized version will also pair with g since h produces different values. This \nleads to starvation of h. Although this behavior is certainly le\u00adgal,onemight reasonablyexpectaschedulerto \ninterleavethesends of g and h.  (MCH) (MSPAWN) C= ((Ch,l), ) l ' fresh C'' = .''), ) .' C[l ' /l] .,C \nC= ((Sp,et ' fresh .,C C ' [E[.d x.e (v)]]), s MCh (t,C.C), PI(., tC' [E[.d x.e (v)]]), s MSp '[e (t,C.C),PI(.,t=. \n=. (t,C), PI(.' ,tC ' [E[.d x.e (v)]])I(0/,t ']), s (t,C''),PI(.' , tC ' [E[.d x.e (v)]]),s (MRECV) (MSEND) \nC= ((R, l, v), ) C= ((S,l,v), ) ' ' ts = (.,t[E[send(l, v)]]) tr = (.' ,t [E'[.d x.e (v)]]) ts = (.' \n,t [E[.d x.e (v)]]) tr = (.,t[E'[recv(l)]]) C ' C ' .''' .'' .''' .'' .' ,C ., ((S,l,v),E[send(l, v)]) \n.' ,C ., ((R,l,v),E'[recv(l)]) ts ' = (.'' ,t[E[unit]]) tr ' = (.''' ,t ' [E'[.d x.e (v)]]) ts ' = (.''' \n,t ' [E[.d x.e (v)]]) tr ' = (.'' ,t[E'[v]]) C ' C ' '' '' ,C.C), PItsItr,s MCom ,C.C),PItsItr, s MCom \n(t =. (t ,C), PIts 'Itr ' ,s (t =. (t ,C),PIts 'Itr ' , s Figure 6. Constraint matching is de.ned by \nfour rules. Communication constraints are matched with threads performing the opposite communication \naction of the constraint. let fun f() = recv(c) fun g() = send(c,1);g() fun h() = send(c,2);h() in spawn(g); \nspawn(h); f(); ...; f() end Figure 10. Memoization of the function f can lead to the starva\u00adtion of either \nof g or h depending on which value the original ap\u00adplication of f consumed from channel c.  4.6 Schedule \nAware Partial Memoization To address the limitations in the previous section, we de.ne two new symmetric \nrules to pause and resume memoization (see Fig. 11). Pausing memoization (rule PAUSE MEMO)is similar \nto the rule END MEMO in Fig. 4 except the failed constraints are not discarded and the thread context \nis not given an expression to evaluate. Instead the thread retains its log of currently unsat\u00adis.ed constraints \nwhich prevents its further evaluation. This ef\u00adfectively pauses the evaluation of this thread but allows \nregular threads to continue normal evaluation. Notice we only pause a thread utilizing memoinformation \nonceit has correctly discharged its constraints.We could envision an alternative de.nition which pauses \nnon-deterministically on anyconstraint and moves the non\u00addischarged constraints back to the thread context \nwhich holds un\u00adsatis.ed constraints.Forthesakeof simplicityweoptedfor greedy semantics whichfavors the \nutilizationof memoization. We can resume the paused thread, enabling it to discharge other constraintsusingtheruleR \nESUMEMEMO,whichbegins constraint discharge anew for a paused thread. Thus, if a thread context has a \nset of constraints that were not previously satis.ed and evaluation is not utilizing memo information, \nwe can once again apply our I function. Note that the use of memo information can be ended at any time \n(rule END MEMO can be applied instead of PAUSE MEMO).We can, therefore, changea threadina paused state \ninto a bona .de thread by .rst applying RESUME MEMO. If I does not indicate we can discharge anyadditional \nconstraints, we simply apply the ruleEND MEMO. We also extend our evaluation rules to allow constraints \nto be matched against other constraints (rule MC OM). This is accom\u00adplished by matching constraints between \ntwo paused threads. Of course, it is possible that two threads, both of which were paused on a constraint \nthat was not satis.able may nonetheless satisfy one another. This happens when one thread is paused on \na send con\u00adstraint and another on a receive constraint both of which match on the channel and value. \nIn this case, the constraints on both sender and receiver can be safely discharged. This allows calls \nwhich at\u00adtempt to use previously memoized constraints to match against con\u00adstraints extant in other calls \nthat also attempt to exploit memoized state. 5. Soundness We can relate the states produced by memoized \nevaluation to the states constructedby the non-memoizing evaluator.Todo so, we .rst de.ne a transformation \nfunction T that transforms process states (and terms) de.ned under memo evaluation to process states \n(and terms) de.ned under non-memoized evaluation (see Fig. 12). Since memo evaluation stores evaluation \ncontexts in . they must be extracted and restored. This is done in the opposite order that they were \npushed onto the stack . since the top represents the most recent call. Functions currently in the process \nof utilizing memo information must be resumed from the expression captured in the .rst non-discharged \nconstraint. Similarly threads which are currently paused must also be resumed. Our safety theorem ensures \nmemoization does not yield states which could not be realized under non-memoized evaluation: Theorem[Safety] \nIf MemS.\u00df1.MemE.\u00df2 PI(.,t[E[.d x.e (v)]]), s =. P 'I(.' , t[E[v ']]),s' then there exists a1,...,an .{App,Ch, \nSpn, Com} s.t. a1an T (PI(.,t[E[.d x.e (v)]])) -. ... -. T (P 'I(.' ,t[E[v ']])) o The proof 2 is by \ninduction on the length of \u00df1.MemE.\u00df2. Each of the elements comprising \u00df1.MemE.\u00df2 corresponds to an ac\u00adtion \nnecessary to discharge previously recorded memoization con\u00adstraints.We can show thatevery \u00df step taken \nunder memoization corresponds to some number of pure steps, and zero or one side\u00adeffecting steps under \nnon-memoized evaluation: zero steps for re\u00adturns and memo actions (e.g.MEMS,MEME,MEMP,andMEMR), and one \nstep for core evaluation, and effectful actions (e.g., MCH, MSPAWN,MRECV,MSEND,andMCOM).Sinceafunction \nwhich 2 see http://www.cs.purdue.edu/homes/lziarek/memoproof.pdf for full details.  (PAUSE MEMO) (t,0/),P,s \nMemP =. P,s (RESUME MEMO) (MCOM) C= ((S,l,v), ) C' = ((R,l,v), ) ' ts = (.,tC.C [.d x.e (v)]) tr = (.' \n,t [.d1x.e ' (v ')]) C ' .C ' .'' .' ,C' .''' .,C I(C,P)= C' C= C'' .C' ts ' = (.'' , tC [.d x.e (v)]) \ntr ' = (.''' , t ' [.d1x.e ' (v ')]) C ' PI(., tC[E[.d x.e (v)]]),s MemR PItsItr, s MCom =. (t,C''),PI(.,tC' \n[E[.d x.e (v)]]), s =. PIts 'Itr ' ,s Figure 11. ScheduleAwarePartial Memoization. T ((t,C), PI(., tC' \n[E[.d x.e (v)]]))= T (PI(.,tC.C' [E[.d x.e (v)]])) T ((P1IP2)) = T (P1)IT (P2) T ((.,t[e]))= t[T (En[... \nE1[e]])] .i =(di,vi,Ei,Ci) . . T ((.,t( ,e ').C[e]))= t[T (En[... E1[e ']])] .i =(di,vi, Ei,Ci) . . T \n(.d x.e)= .x.e T (e1(e2)) = T (e1)(T (e2)) T (spawn(e)) = spawn(T (e)) T (send(e1 ,e2 )) = send(T (e1 \n),T (e2 )) T (recv(e)) = recv(T (e)) e otherwise Figure 12. T de.nes an erasure property on program states. \nThe .rst four rules remove memo information and restore evaluation contexts. is utilizing memoized information \ndoes not execute pure code (rule APPunder -.),itmay correspondtoa numberofAPPtransitions under -.. 6. \nImplementation Our implementationis incorporated within Multi-MLton,anexten\u00adsion of MLton (MLton), a \nwhole-program optimizing compiler for Standard ML, that provides support for parallel thread execution. \nThe main extensions to Multi-MLton to support partial memoiza\u00adtion involve insertion of read and write \nbarriers to track accesses and updates of references, barriers to monitor function arguments and return \nvalues, and hooks to the Concurrent ML library to mon\u00aditor channel based communication. 6.1 Multi-MLton \nTo support parallel execution, we modi.ed the MLton compiler to support parallel threads.A POSIX pthreadexecutes \non each pro\u00adcessor. Each pthread manages a lightweight Multi-MLton thread queue. Each pthread switches \nbetween lightweight MLton threads on its queue when it is preempted. Pthreads are spawned and man\u00adaged \nby Multi-MLton s runtime. Currently, our implementation does not support migration of Multi-MLton threads \nto different thread queues. The underlyinggarbage collector also supports parallel alloca\u00adtion. Associated \nwith every processor is a private memory region used by threads it executes; allocation within this region \nrequires no global synchronization. These regions are dynamic and grow\u00adable. All pthreads must synchronize \nwhen garbage collection is triggered. Data shared by threads found on different processors are copied \nto a shared memory region that requires synchronization to access. 6.2 Parallel CML and hooks Our parallel \nimplementation of CML is based on Reppy s parallel modelof CML (Reppyand Xiao 2008).We utilizelowlevel \nlocks implemented with compare and swap to provide guarded access to channels. Whenever a thread wishes \nto perform an action on a channel, it must .rst acquire the lock associated with the channel. Since a \ngiven thread may only utilize one channel at a time, there is no possibility of deadlock. The underlying \nCML library was also modi.ed to make mem\u00adoization ef.cient. Thebulk of the changes were hooks to monitor \nchannel communication and spawns, additional channel queues to support constraint matching on synchronous \noperations, and to log successful communication (including selectivecommunication and complex composed \nevents). The constraint matching engine required a modi.cation to the channel structure. Each channel \nis augmented with two additional queues to hold send and receive constraints. When a constraint is being \ntested for satis.ability, the opposite queue is .rst checked (e.g.asend constraintwouldcheckthereceiveconstraintqueue).If \nno match is found, the regular queues are checked for satis.ability. If the constraint cannot be satis.ed \nimmediately it is added to the appropriate queue.  6.3 Supporting Memoization AnySML function can be \nannotated as a candidate for memoiza\u00adtion.For such annotated functions, its arguments and returnvalues \nat different call-sites, the communication it performs, and infor\u00admation about the threads it spawns \nare recorded in a memo table. Memoization information is logged through hooks to the CML run\u00adtime and \nstored by the underlying client code. In addition, to sup\u00adport partial memoization, the continuations \nof logged communica\u00adtion events are also saved.  Our memoization implementation extended CML channels \nto be aware of memoization constraints. Each channel structure con\u00adtained a queue of constraints waiting \nto be solved on the channel. Because it will not be readily apparent if a memoized version of a CML function \ncan be utilized at a call site, we delay a function ap\u00adplication to see if its constraints can be matched; \nthese constraints must be satis.ed in the order in which theywere generated. Constraint matching can \ncertainlyfail ona receive constraint.A receive constraint obligates a receive operation to read a speci.c \nvalue from a channel. Since channel communication is blocking, a receive constraint that is being matched \ncan choose from all values whose senders are currently blocked on the channel. This does not violatethe \nsemanticsofCML sincethevalues blockedonachannel cannot be dependent on one another; in other words, a \nschedule must exist where the matched communication occurs prior to the .rst value blocked on the channel. \nUnlikeareceiveconstraint,asend constraintcanonlyfailifthere are (a) no matching receive constraints on \nthe sending channel that expect the value being sent, or (b) no receive operations on that same channel.ACML \nreceive operation(not receive constraint) is ambivalenttothevalueit removesfromachannel;thus,anyreceive \non a matching channel will satisfy a send constraint. If no receives or sends are enqueued onaconstraint \nstarget chan\u00adnel, a memoized execution of the function will block. Therefore, failure to fully discharge \nconstraints by stalling memoization on a presumed unsatis.able constraint does not compromise global \nprogress. This observationis criticaltokeeping memoizationover\u00adheads low. Our memoization technique relies \non ef.cient equality tests, and approximate equality on reals and functions; the latter is modeled as \nclosure equality. Memoization data is discarded duringgarbage collection. This prevents unnecessary build \nup of memoization meta-data during execution. As a heuristic, we also enforce an upper bound for the \namount of memo data stored for each function, and the space that each memo entry can take.Afunction that \ngeneratesa setof constraints whose size exceeds the memo entry space bound is not memoized.For each memoized \nfunction, we storea listof memo meta-data. When the length of the list reaches the upper limitbut new \nmemo data is acquired upon an application of the function to previously unseen arguments, one entry from \nthe list is removed at random.  6.4 Benchmarks We examined three benchmarks to measure the effectiveness \nof partial memoization in a parallel setting. The .rst benchmark is a streaming algorithm for approximating \na k-clustering of points on a geometric plane. The second is a port of the STMBench7 bench\u00admark (Guerraoui \net al. 2007). STMBench7 utilizes channel based communication instead of shared memory and bears resemblance \nto the red-black tree program presented in Section 3. The third is Swerve (Ziarek et al. 2006), a highly-tuned \nwebserver written in CML. Similar to most streaming algorithms (Matthew Mccutchen and Khuller 2008), \na k-clustering application de.nes a number of worker threads connectedina pipelinefashion. Eachworker \nmain\u00adtains a cluster of points that sit on a geometric plane. A stream generator creates a randomized \ndata stream of points. A point is passed to the .rst worker in the pipeline. The worker computes a convex \nhull of its cluster to determine if a smaller cluster could be constructed from the newly received point. \nIf the new point results in a smaller cluster, the outlier point from the original cluster is passedtothenextworker \nthread.Ontheotherhand,ifthereceived point does not alter the con.guration of the cluster, it is passed \non to the next worker thread. The result de.nes an approximation of n clusters (where n is the number \nof workers) of size k(points that compose the cluster). STMBench7 (Guerraouietal. 2007),isacomprehensive, \ntunable multi-threaded benchmark designed to compare different STM im\u00adplementations and designs. Based \non the well-known 007 database benchmark (Careyet al. 1993), STMBench7 simulates data storage and access \npatterns of CAD/CAM applications that operate over complex geometric structures. At its core, STMBench7 \nbuilds a tree of assemblies whose leaves contain bags of components;these components have a highly connected \ngraph of atomic parts and de\u00adsign documents. Indices allow components, parts, and documents tobe accessed \nvia their properties and IDs.Traversalsof this graph can begin from the assembly root or anyindex and \nsometimes ma\u00adnipulate multiple pieces of data. STMBench7 was originally written in Java. Our port, besides \ntranslating the assembly tree to use a CML-based server abstrac\u00adtion(as discussedin Section3),alsoinvolvedbuildinganSTMim\u00adplementation \nto support atomic sections, loosely based on the tech\u00adniques described in (Saha et al. 2006; Adl-Tabatabai \net al. 2006). All nodes in the complexassembly structure and atomic parts graph are represented as servers \nwith one receiving channel and handles to all other adjacent nodes. Handles to other nodes are simply \nthe channels themselves. Each server thread waits for a message to be received, performs the requested \ncomputation, and then asyn\u00adchronously sends the subsequent part of the traversal to the next node.Atransactioncanthusbe \nimplementedasa seriesof commu\u00adnications with various server nodes. Swerve is a web server written entirely \nin CML. It consists of a collection of modules that communicate using CML s message\u00adpassing operations. \nThere are three critical modules of interest: (a) a listener that processes incoming requests;(b)a .le \nprocessor that handles access to the underlying .le system; and, (c) a timeout manager that regulates \nthe amount of time allocated to serve a request. There is a dedicated listener for every distinct client, \nand each request received by a listener is delegated to a server thread responsible for managing that \nrequest. Requested .les are broken into chunks and packaged as messages sent back to the client. 7. Results \nOur benchmarks were executed on a 16-way AMD Opteron 865 server with 8 processors, each containing two \nsymmetric cores, and 32 GB of total memory, with each CPU having its own lo\u00adcal memory of 4 GB. Access \nto non-local memory is mediated by a hyper-transport layer that coordinates memory requests be\u00adtween \nprocessors.Tomeasuretheeffectivenessof our memoization technique,weexecutedtwo con.gurations(one memoized,andthe \nother non-memoized) of our k-clustering algorithm, STMBench7, and Swerve, and measured overheads and \nperformance by averag\u00ading results over ten executions. Non-memoized executions utilized a clean version \nof Multi-MLton without memoization hooks and barriers. The k-clustering algorithm utilizes memoization \nto avoid redun\u00addant computations based on previously witnessed points as well as redundant computations \nof clusters.For STMBench7 the non\u00admemoized con.guration uses our STM implementation without any memoization \nwhere as the memoized con.guration implements partial memoization of aborted transactions. Swerve uses \nmemo\u00adization to avoid reading and processing previously requested .les from disk. For k-clustering, we \ncomputed 16 clusters of size 60 out of a stream of 10K randomly generated points. This resulted in the \ncreationof16workers threads, one stream generating thread,anda sink thread which aggregates the computation \nresults. STMBench7 was executed on a graph in which there were approximately 280k complex assemblies \nand 140k assemblies whose bags referenced one of 100 components; by default, each component contained \na parts graph of 100 nodes. STMBench7 creates a number of threads proportional to the number of nodes \nin the underlying data structure; this is roughly 400K threadsfor our con.guration. Our experiments on \nSwerve were executed using the default server con.guration and wereexercised using HTTPerf,awell known \ntool for measuring webserver performance.  The benchmarks represent three very different programming \nmodels pipeline stream-based parallelism (k-clustering), dy\u00adnamically established communication links \n(Swerve), and soft\u00adware transactions (STM-Bench7),andleveragedifferentexecutions models k-clusteringmakes \nuseof long-livedworker-threads while STMBench7 utilizes manylightweight server threads. Swerve uti\u00adlizes \nboth lightweight server threads in conjunction with long-lived worker threads. Each run of the benchmarks \nhave execution times that range between1and3minutes. Fork-clustering we varied the number of repeated \npoints gener\u00adated by the stream. Con.gurations in which there is a high degree of repeated pointsoffer \nthe best performancegain (see Fig. 13(b)). Forexample,aninputinwhich50%oftheinputpointsare repeated yields \nroughly 50% performancegain. However, we also observe roughly 17% performanceimprovementeven when all \npoints are randomized. This is because the cluster sconvexhull shrinks as the points which comprise the \ncluster become geometrically compact. Thus, as the convex hull of the cluster shrinks, the likelihood \nof a random point being contained within the convex hull of the cluster is reduced. Memoization can take \nadvantage of this phenomenon by avoiding recomputation of the convex hull as soon as it can be determined \nthat the input point resides outside the current cluster. Although we do not envision workloads that \nhave high degrees of repeatability,memoization nonetheless leadstoa30% performance gain on a workload \nin which only 10% of the inputs are repeated. In STMbench7, the utility of memoization is closely correlated \nto the number and frequencyof aborted transactions. Our tests varied the read-only/read-write ratio (see \nFig. 13(a)) within transactions. Only transactions that modify values can cause aborts. Thus, an execution \nwhere all transactions are read-only cannot be acceler\u00adated,but one in which transactions can frequently \nabort (because of con.icts due to concurrent writes) offer potentialopportunities for memoization. Thus, \nthe cost to support memoization is seen when there are 100% read-only transactions; in this case, the \nover\u00adhead is roughly 11%. These overheads arise because of the cost to capture memo information (storing \narguments, continuations, etc) and the cost associated with trying to utilize the memo information (discharging \nconstraints). Notice that as the number of transactions which perform modi.\u00adcations to the underlying \ndata-structure increases so do memoiza\u00adtiongains.Forexample, when the percentageof read-only transac\u00adtions \nis 60%, we see an 18% improvement in runtime performance compared to a non-memoizing implementation for \nSTMBench7. We expected to see roughly a linear trend correlated to the in\u00adcrease in transactions which \nperform an update. However, we no\u00adticed that performance degrades about 10% from a read/write ratio of \n20 to a read/write ratio of zero. This phenomenon occurs be\u00adcause memoized transactions are more likely \nto complete on their .rst try when there are fewer modi.cations to the structure. Since a non-memoized \ntransaction requires longer to complete, it has a greater chance of aborting when there are frequent \nupdates. This difference becomes muted as the number of changes to the data structure increase. In Swerve, \nwe observe an increase in performance correlated to thesizeofthe.lebeing requestedbyHTTPerf(seeFig.13(c));per\u00adformancegainsarecappedatroughly80%for.lesizes \ngreaterthan 8MB.For each requested .le, webuild constraints corresponding to the .le chunks read from \ndisk. As long as no errors are encoun\u00adtered, the Swerve .le processor sends the .le chunks to be pro\u00adcessed \ninto HTTP packets by another module. After each chunk has been read from disk the .le processor polls \nother modules for timeouts and other error conditions. If an error is encountered, sub\u00adsequent .le processing \nis stopped and the request is terminated. Partial memoization is particularly well suited for Swerve \ns .le processing semantics because control is reverted to the error han\u00addling mechanism precisely at \nthe point an error is detected. This correspondstoafailed constraint. Forall benchmarks, memoryoverheads \nare proportionalto cache sizesandaveragedroughly15%for cachesofsizeeight.Thecache size de.nes the number \nof different memoized calls for a function maintined; thus a cache size of eight means that every memoized \nfunction records effects for eight different arguments. 8. Related Work Memoization, or function caching \n(Liu andTeitelbaum 1995; Pugh 1988; Heydon et al. 2000; Swadi et al. 2006), is a well under\u00adstood method \nto reduce the overheads of redundant function exe\u00adcution. Memoization of functions in a concurrent setting \nis signif\u00adicantly more dif.cult and usually highly constrained (Pickett and Verbrugge 2005). We are unaware \nof any existing techniques or implementations that apply memoization to the problem of reduc\u00ading transaction \noverheads in languages that support selective com\u00admunication and dynamic thread creation. Our approach \nalso bears resemblance to the procedure summary construction for concurrent programs (Qadeer et al. 2004). \nHowever, these approaches tend to bebasedonastatic analysis(e.g.,thecited referenceleveragespro\u00adcedure \nsummaries to improvethe ef.ciencyof model checking) and thus are obligated to make use of memoization \ngreedily. Because our motivation is quite different, our approach can consider lazy al\u00adternatives, ones \nthat leverage synchronization points to stall memo information use, resulting in potentially improved \nruntime bene.t. Recently software transactions (Harris and Fraser 2003; Saha etal.2006)haveemergedasanewmethodtosafelycontrol \nconcur\u00adrent execution. There has also been work on applying these tech\u00adniquestoafunctional programmingsetting(Harrisetal.2005;Rin\u00adgenburgand \nGrossman 2005). These proposals usually rely on an optimistic concurrency control model that checks for \nserializabil\u00adity violations prior to committing the transaction, aborting when a violation is detected. \nOur benchmark results suggest that partial memoization can help reduce the overheads of aborting optimistic \ntransactions. Self adjusting mechanisms (Acar et al. 2008; Ley-Wild et al. 2008) leverage memoization \nalong with change propagation to au\u00adtomatically alter a program s execution to a change of inputs given \nan existing execution run. Memoization is used to identify parts of the program which have not changed \nfrom the previous execu\u00adtion while change propagation is harnessed to install changed val\u00adues where memoization \ncannot be applied. There has also been re\u00adcent work on using change propagation in a parallel setting \n(Ham\u00admer et al. 2007). The programming model assumes fork/join par\u00adallelism, and is therefore not suitable \nfor the kinds of contexts we consider.Webelieveour memoization techniqueis synergistic with current self-adjusting \ntechniques and can be leveraged along with self-adjusting computation to create self-adjusting programs \nwhich utilize message passing.  (a) (b) (c) Our technique also shares some similarity with transactional \nevents (DonnellyandFluet2006;Ef.nger-Deanetal.2008).Trans\u00adactional events explore a state space of possible \ncommunications .nding matching communications to ensure atomicity of a col\u00adlection of actions. To do \nso transactional events require arbitrary lookahead in evaluation to determine if a complex composed \nevent can commit.Partial memoization alsoexplores potential matching communication actions to satisfy \nmemoization constraints. How\u00adever, partial memoization avoids the need for arbitrary lookahead failure \nto discharge memoization constraints simply causes execu\u00adtion to proceed as normal. Acknowledgements \nThanks to Matthew Fluet and Dan Spoonhower for their help in the design and development of Multi-MLton. \nThis work is supported by the National Science Foundation under grants CCF-0701832 and CCF-0811631, and \nan Intel graduate fellowship. References Umut A. Acar,Guy E. Blelloch, and Robert Harper. SelectiveMemoization. \nIn POPL, pages 14 25, 2003. Umut A. Acar, Amal Ahmed, and Matthias Blume. Imperative Self-Adjusting Computation. \nIn POPL, pages 309 322, 2008. Ali-Reza Adl-Tabatabai, BrianT. Lewis,Vijay Menon, Brian R. Murphy, Bratin \nSaha, andTatiana Shpeisman. Compiler and Runtime Support for Ef.cient SoftwareTransactional Memory. In \nPLDI, pages 26 37, 2006. Michael J. Carey, David J. DeWitt, and Jeffrey F. Naughton. The 007 benchmark. \nSIGMOD Record, 22(2):12 21, 1993. Kevin Donnelly and Matthew Fluet. Transactional Events. InICFP, pages \n124 135, 2006. Laura Ef.nger-Dean, Matthew Kehrt, and Dan Grossman. Transactional events for ml. In ICFP \n08, pages 103 114, 2008. ISBN 978-1-59593\u00ad919-7. Michael I. Gordon,William Thies, and Saman Amarasinghe. \nExploiting Coarse-Grained Task, Data, and Pipeline Parallelism in Stream Pro\u00adgrams. In ASPLOS-XII, pages \n151 162, 2006. Rachid Guerraoui, Michal Kapalka, and JanVitek. STMBench7:ABench\u00admark for SoftwareTransactional \nMemory. In EuroSys, pages 315 324, 2007. Matthew Hammer, Umut A. Acar, Mohan Rajagopalan, and Anwar Ghu\u00adloum.AProposal \nforParallel Self-Adjusting Computation. In Workshop on Declarative Aspects of Multicore Programming, \n2007. Tim Harris andKeir Fraser. Language support for lightweight transactions. In OOPSLA, pages 388 \n402, 2003. Tim Harris, Simon Marlow, Simon Peyton-Jones, and Maurice Herlihy. Composable MemoryTransactions. \nIn Proceedingsof theACM Confer\u00adenceon PrinciplesandPracticeofParallelProgramming, pages 48 60, 2005. \nAllan Heydon, Roy Levin, and Yuan Yu. Caching function calls using precise dependencies. In PLDI, pages \n311 320, 2000. Ruy Ley-Wild, MatthewFluet, and Umut A. Acar. Compiling self-adjusting programs with continuations. \nIn ICFP, pages 321 334, 2008. Yanhong A. Liu and Tim Teitelbaum. Caching Intermediate Results for Program \nImprovement. In PEPM, pages 190 201, 1995. Richard Matthew Mccutchen and Samir Khuller. Streaming algorithms \nfor k-center clustering with outliers and with anonymity. In APPROX 08/ RANDOM 08, pages 165 178, 2008. \nMLton. http://www.mlton.org. Christopher J. F. Pickett and Clark Verbrugge. Software Thread Level SpeculationfortheJavaLanguageandVirtual \nMachineEnvironment.In Proceedingsofthe InternationalWorkshoponLanguagesand Compilers forParallel Computing, \n2005. W. Pugh and T. Teitelbaum. Incremental Computation via Function Caching. In POPL, pages 315 328, \n1989. William Pugh. An Improved Replacement Strategy for Function Caching. In LFP, pages 269 276, 1988. \nShaz Qadeer, Sriram K. Rajamani, and Jakob Rehof. Summarizing proce\u00addures in concurrent programs. In \nPOPL, pages 245 255, 2004. JohnReppyandYingqiXiao.TowardsaParallel ImplementationofCon\u00adcurrent ML. In \nDAMP 2008, January 2008. John H. Reppy. Concurrent Programming in ML. Cambridge University Press, 1999. \nMichaelF. Ringenburgand Dan Grossman. AtomCaml: First-Class Atom\u00adicity via Rollback. In Proceedings of \ntheACM SIGPLAN International Conference on Functional Programming, pages 92 104, 2005. Bratin Saha, Ali-Reza \nAdl-Tabatabai, Richard L. Hudson, Chi Cao Minh, and Benjamin Hertzberg. McRT-STM: a High-Performance \nSoftware Transactional Memory system for a Multi-Core Runtime. In PPoPP, pages 187 197, 2006. KedarSwadi,WalidTaha,Oleg \nKiselyov,and EmirPasalic. A Monadic Approach for Avoiding Code Duplication When Staging Memoized Functions. \nIn PEPM, pages 160 169, 2006. Lukasz Ziarek, Philip Schatz,and SureshJagannathan. Stabilizers:AMod\u00adular \nCheckpointing Abstraction for Concurrent Functional Programs. In ACM International Conference on Functional \nProgramming,pages 136 147, 2006.   \n\t\t\t", "proc_id": "1596550", "abstract": "<p>Memoization is a well-known optimization technique used to eliminate redundant calls for pure functions. If a call to a function f with argument v yields result r, a subsequent call to f with v can be immediately reduced to r without the need to re-evaluate f's body. Understanding memoization in the presence of concurrency and communication is significantly more challenging. For example, if f communicates with other threads, it is not sufficient to simply record its input/output behavior; we must also track inter-thread dependencies induced by these communication actions. Subsequent calls to f can be elided only if we can identify an interleaving of actions from these call-sites that lead to states in which these dependencies are satisfied. Similar issues arise if f spawns additional threads. In this paper, we consider the memoization problem for a higher-order concurrent language whose threads may communicate through synchronous message-based communication. To avoid the need to perform unbounded state space search that may be necessary to determine if <i>all</i> communication dependencies manifest in an earlier call can be satisfied in a later one, we introduce a weaker notion of memoization called <i>partial memoization</i> that gives implementations the freedom to avoid performing some part, if not all, of a previously memoized call. To validate the effectiveness of our ideas, we consider the benefits of memoization for reducing the overhead of recomputation for streaming, server-based, and transactional applications executed on a multi-core machine. We show that on a variety of workloads, memoization can lead to substantial performance improvements without incurring high memory costs.</p>", "authors": [{"name": "Lukasz Ziarek", "author_profile_id": "81318492573", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P1614017", "email_address": "", "orcid_id": ""}, {"name": "KC Sivaramakrishnan", "author_profile_id": "81442594976", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P1614018", "email_address": "", "orcid_id": ""}, {"name": "Suresh Jagannathan", "author_profile_id": "81100208907", "affiliation": "Purdue University, West Lafayette, IN, USA", "person_id": "P1614019", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1596550.1596575", "year": "2009", "article_id": "1596575", "conference": "ICFP", "title": "Partial memoization of concurrency and communication", "url": "http://dl.acm.org/citation.cfm?id=1596575"}