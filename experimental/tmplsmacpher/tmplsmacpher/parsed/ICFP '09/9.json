{"article_publication_date": "08-31-2009", "fulltext": "\n Biorthogonality, Step-Indexing and Compiler Correctness Nick Benton Chung-Kil Hur Microsoft Research \nUniversity of Cambridge nick@microsoft.com ckh25@cam.ac.uk Abstract We de.ne logical relations between \nthe denotational semantics of a simply typed functional language with recursion and the opera\u00adtional \nbehaviour of low-level programs in a variant SECD machine. The relations, which are de.ned using biorthogonality \nand step\u00adindexing, capture what it means for a piece of low-level code to implement a mathematical, domain-theoretic \nfunction and are used to prove correctness of a simple compiler. The results have been formalized in \nthe Coq proof assistant. Categories and Subject Descriptors F.3.1 [Logics and Mean\u00adings of Programs]: \nSpecifying and Verifying and Reasoning about Programs Mechanical veri.cation, Speci.cation techniques; \nF.3.2 [Logics and Meanings of Programs]: Semantics of Program\u00adming Languages Denotational semantics, \nOperational seman\u00adtics; F.3.3 [Logics and Meanings of Programs]: Studies of Pro\u00adgram Constructs Type \nstructure,Functional constructs; D.3.4 [Programming Languages]: Processors Compilers; D.2.4 [Soft\u00adware \nEngineering]: Software / Program Veri.cation Correctness proofs, Formal methods General Terms Languages, \ntheory, veri.cation Keywords Compiler veri.cation, denotational semantics, biorthog\u00adonality, step-indexing, \nproof assistants 1. Introduction Proofs of compiler correctness have been studied for over forty years \n(McCarthy and Painter 1967; Dave 2003) and have recently been the subject of renewed attention, .rstly \nbecause of increased interest in security and certi.cation in a networked world and sec\u00adondly because \nof advances in veri.cation technology, both theoret\u00adical (e.g. separation logic, step-indexed logical \nrelations) and prac\u00adtical (e.g. developments in model checking and improvements in interactive proof \nassistants). There are many notions of correctness or safety that one might wish to establish of a compiler. \nFor applying language-based tech\u00adniques in operating systems design, as in proof-carrying code, one is \nprimarily interesting in broad properties such as type-safety, memory-safety or resource-boundedness. \nAlthough these terms are widely used, they are subject to a range of interpretations. For ex\u00adample, type-safety \nsometimes refers to a simple syntactic notion ( is the generated code typable using these rules? ) and \nsometimes Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. ICFP \n09, August 31 September 2, 2009, Edinburgh, Scotland, UK. Copyright c &#38;#169; 2009 ACM 978-1-60558-332-7/09/08. \n. . $5.00 to a deeper semantic one ( does the observable behaviour of the code satisfy this desirable \nproperty? ). In previous work (Benton 2006; Benton and Zarfaty 2007; Benton and Tabareau 2009), we have \nlooked at establishing type-safety in the latter, more semantic, sense. Our key notion is that a high-level \ntype translates to a low\u00adlevel speci.cation that should be satis.ed by any code compiled from a source \nlanguage phrase of that type. These speci.cations are inherently relational, in the usual style of PER \nsemantics, capturing the meaning of a type A as a predicate on low-level heaps, values or code fragments \ntogether with a notion of A-equality thereon. These relations express what it means for a source-level \nabstractions (e.g. functions of type A . B) to be respected by low-level code (e.g. taking A-equal arguments \nto B-equal results ). A crucial property of our low-level speci.cations is that they are de.ned in terms \nof the behaviour of low-level programs; making no reference to any intensional details of the code produced \nby a particular compiler or the grammar of the source language. Of course, the speci.cations do involve \nlow-level details of data representations and calling con\u00adventions these are part of the interface to \ncompiled code but up to that, code from any source that behaves suf.ciently like code generated by the \ncompiler should meet the speci.cation, and this should be independently veri.able. Ideally, one might \nwish to establish the sense in which a compi\u00adlation scheme is fully abstract, meaning that the compiled \nversions of two source phrases of some type are in the low-level relation interpreting that type iff \nthe original source phrases are contextu\u00adally equivalent. If low-level speci.cations are used for checking \nlinking between the results of compiling open programs and code from elsewhere1 and full abstraction \ndoes not hold, then source level abstractions become leaky : reasoning about equivalence or encapsulation \nat the source level does not generally translate to the target, which can lead to unsound program transformations \nin optimizing compilers and to security vulnerabilities (Abadi 1998; Kennedy 2006). Ahmed and Blume (2008) \nalso argue that fully abstract translation should be the goal, and prove full abstraction for (source \nto source) typed closure conversion for a polymorphic lambda calculus with recursive and existential \ntypes. Later on we will say something about why we believe that full full abstraction may not, in practice, \nbe quite the right goal, but we certainly do want suf.ciently abstract compilation, i.e. the preservation \nof the reasoning principles that we actually use in an optimizing compiler or in proving security properties. \nThe low-level relations of our previous work are not, however, really even suf.ciently abstract, having \nroughly comparable power to a denotational semantics in continuation-passing style (CPS). This is a very \nstrong and useful constraint on the behaviour of machine-code programs, but does not suf.ce to prove \nall the equa\u00ad 1 This is obviously important for foreign function interfaces and multilan\u00adguage interoperability, \nbut can be an issue even for separate compilation using the same compiler. It also covers the simpler \nand ubiquitous case of handcrafted implementations of standard library routines.  tions we might like \nbetween low-level programs: even something as simple as the commutativity of addition does not hold for \narbi\u00adtrary integer computations (just as it doesn t in a lambda calculus with control), even though it \ndoes in our pure source language. To understand how to re.ne our low-level relations further, it is natural \nto look at logical relations between low-level code and elements of the domains arising in a standard, \ndirect-style, denotational model of our language, which is what we ll do here. Given such a typed relation \nbetween high-level semantics and low-level programs, a low-level notion of typed equivalence can be generated \nby consid\u00adering pairs of low-level programs that are related to some common high-level denotational value. \nThe relations we de.ne will establish a full functional correct\u00adness theorem for a simple compiler, not \nmerely a semantic type safety theorem. Just as for type-safety, there are several approaches to formulating \nsuch correctness theorems in the literature. A com\u00admon one, used for example by Leroy (2006), is to de.ne \nan oper\u00ad ational semantics for both high-and low-level languages and then establish a simulation (or \nbisimulation) result between source pro\u00adgrams and their compiled versions, allowing one to conclude that \nif a closed high-level program terminates with a particular observ\u00adable result, then its compiled version \nterminates with the same re\u00adsult, and often the converse too (Hardin et al. 1998; Leroy and Grall 2009). \nThe limitation of these simulation-based theorems is that they are not as compositional (modular) or \nextensional (be\u00adhavioural) as we would like, in that they only talk about the be\u00adhaviour of compiled \ncode in contexts that come from the same com\u00adpiler, and usually specify a fairly close correspondence \nbetween the (non-observable) intermediate states of the source and target. We would rather have maximally \npermissive speci.cations that capture the full range of pieces of low-level code that, up to observations, \nbehave like, or realize, a particular piece of high-level program. A slogan here is that the ends justify \nthe means : we wish to allow low-level programs that extensionally get the right answers, whilst intensionally \nusing any means necessary. The main results here will involve logical relations between a cpo-based denotational \nsemantics of a standard CBV lambda calculus with recursion and programs in an extended SECD-like target, \nchosen to be suf.ciently low-level to be interesting, yet simple enough that the important ideas are \nnot lost in detail. The relations will involve both biorthogonality and step-indexing, and in the next \nsection we brie.y discuss these constructions in general terms, before turning to the particular use \nwe make of them. The results in the paper have been formalized and proved in the Coq proof assistant, \nbuilding on a formalization of domain theory and denotational semantics that we describe elsewhere (Benton \net al. 2009). The scripts are available from the authors webpages. 2. Orthogonality and Step-Indexing \n2.1 Biorthogonality Biorthogonality is a powerful and rather general idea that has been widely used in \ndiffent kinds of semantics in recent years, beginning with the work of Pitts and Stark (1998) and of \nKrivine (1994). One way of understanding the basic idea is as a way of contextualizing properties of \nparts of a system, making them compositional and behavioural. In the unary case, we start with some set \nof systems S (e.g. con.gurations of some machine, lambda terms, denotations of programs, processes) and \nsome predicate O.S, which we call an observation, over these systems (e.g. those that execute without \nerror, those that terminate, those that diverge). Then there is some way of combining, or plugging, program \nfragments p .P in whose properties we are interested (bits of machine code, terms, denotations of terms, \nprocesses) with complementary contexts c . C (frame stacks, terms with holes, continuations, other processes) \nto yield complete systems. The plugging -.- : P\u00d7C . S might be effected by appending bits of program, \nsubstituting terms, applying continuations to arguments or composing processes in parallel. In any such \nsituation, there is contravariant map (\u00b7). : P(P) . P(C) given by P . = {c .C|.p . P, p . c . O} and \na homonymous one in the other direction, (\u00b7). : P(C) . P(P) C. = {p .P|.c . C, p . c . O} yielding a \ncontravariant Galois connection, so that, amongst many other things, (\u00b7).. is a closure operator (in.ationary \nand idempo\u00adtent) on P(P), and that any set of the form C. is (\u00b7)..-closed.2 The binary version of this \nconstruction starts with a binary rela\u00adtion (e.g. an equivalence relation) on S and proceeds in the obvious \nway. For compiler correctness, we want the interpretations of source\u00adlevel types or, indeed, source-level \nvalues, to be compositional and extensional: properties of low-level program fragments that we can check \nindependently and that make statements about the observable behavior of the complete con.gurations that \narise when we plug the fragments into appropriate contexts. This set of appropriate contexts can be thought \nof as a set of tests: a low-level fragment is in the interpretation of a source type, or correctly represents \na source value, just when it passes all these tests. Thus these low-level interpretations will naturally \nbe (\u00b7)..-closed sets. For a simply typed source language, we can de.ne these sets by induction on types, \neither positively, starting with an over-intensional set and then taking its (\u00b7)..-closure, or negatively, \nby .rst giving an inductive de.nition of a set of contexts and then taking its orthogonal. See Vouillon \nand Melli`es (2004) for more on the use of biorthogonality in giving semantics to types. 2.2 Step-Indexing \nLogical predicates and relations can be used with many different styles of semantics. When dealing with \nlanguages with recursion and working denotationally, however, we often need extra admis\u00adsibility properties, \nsuch as closure under limits of .-chains. Op\u00aderational logical relations for languages with recursion \ngenerally need to satisfy some analogue of admissibility. One such that has often been used with operational \nsemantics based on lambda terms (Plotkin 1977; Pitts and Stark 1998) considers replacing the recur\u00ad sion \nconstruct rec fx = M (or .xpoint combinator) with a family of .nite approximations: recn fx = M for n \n. N, unfolds the recursive function n times in M and thereafter diverges. Appel and McAllester (2001) \nintroduced step-indexed logical relations, which have since been re.ned and succesfully applied by various \nauthors to operational reasoning problems for both high and low level languages, many of which involve \nchallenging language fea\u00adtures (Ahmed 2006; Appel et al. 2007; Benton and Tabareau 2009; Ahmed et al. \n2009). Step-indexing works with small-step opera\u00ad tional semantics and N-indexed sets of values, with \n(n, v) . P (or v . Pn) meaning value v has property P for n steps of reduction . An interesting feature \nof step-indexing is that one usually works di\u00adrectly with this family of approximants; the limits that \none feels are being approximated (like {v |.n, v . Pn}) do not play much of a r ole.  2.3 On Using Both \nAmongst the useful properties of the operational (\u00b7)..-closure op\u00aderation used by Pitts and Stark (1998) \nis that it yields admissible 2 Pitts and Stark, and some other non-Gallic authors, tend to write (\u00b7)T \nrather than (\u00b7). .  relations (in the recn sense). The same is true of its natural deno\u00adtational analogue \n(Abadi 2000). Our earlier work on low-level in\u00adterpretations of high-level types (Benton and Tabareau \n2009) used both step-indexing and orthogonality, but there was some question as to whether the step-indexing \nwas really necessary. Maybe our closed sets are automatically already appropriately admissible , just \nby construction, and there is no need to add extra explicit in\u00addexing? Slightly to our surprise, it turns \nout that there are good reasons to use both (\u00b7)..-closure and step-indexing, for which we now try to \ngive some intuition. The aim is to carve out interpretations of high-level types and values as well-behaved \nsubsets of low-level, untyped programs. The essence of these interpretations generally only depends upon \nthese well-behaved subsets: we ll (roughly) interpret a function type A . B as the set of programs that \nwhen combined with a good argument of type A and a good continuation expecting some\u00adthing of type B, \nyield good behaviour. So exactly what range of impure, potentially type-abstraction violating, operations \nare avail\u00adable in the untyped language (the range of any means necessary above) does not seem to affect \nthe de.nitions or key results. Pro\u00adgrams that use low-level features in improper ways will simply not \nbe in the interpretations of high-level entities, and nothing more needs to be said. For a simply-typed \ntotal language without re\u00adcursion, this intuition is quite correct: a Krivine-style realizability interpretation \nis essentially unaffected by adding extra operations to the untyped target. Even though orthogonality \nintroduces quan\u00adti.cation over bigger sets of contexts, nothing relies explicitly on properties of the \nuntyped language as a whole. In the presence of recursion, the situation changes. The fact that Pitts \nand Stark s (\u00b7)..-closed relations are admissible depends on a compactness of evaluation result, sometimes \ncalled an unwind\u00ading theorem , saying that any complete program p terminates iff there is some n such \nthat for all m = n, p with all the recs re\u00adplaced by recms terminates, which is clearly a global property \nof all untyped programs. In the denotational case, attention is already restricted to operations that \ncan be modelled by continuous func\u00adtions, i.e. ones that behave well with respect to approximation, in \nthe chosen domains. But realistic targets often support operations that can violate these global properties. \nExamples of such egre\u00adgiously non-functional operations include the re.ection features of some high-level \nlanguages (such as Java or Ct) or, more inter\u00adestingly, the ability of machine code programs to switch \non code pointers or read executable machine instructions.3 We have found that the presence of such seriously \nnon-functional operations does not just make the proofs harder, but can actually make the natu\u00adral theorems \nfalse. Appendix A shows how the addition of equal\u00ad ity testing on lambda terms to an untyped lambda calculus \nbreaks a standard syntactic interpretation of simple types as sets of un\u00adtyped terms in the presence \nof term-level recursion in the source. Fortunately, as we will see, step-indexing sidesteps this prob\u00adlem. \nIn place of appealing to a global property that holds of all untyped programs, we build a notion of approximation, \nand the re\u00adquirement to behave well with respect to it, directly into the de.ni\u00adtion of our logical relations. \nIn fact, we will also do something very similar on the denotational side, closing up explicitly under \nlimits of chains. 3 Real implementations, even of functional languages, can make non-trivial use of such \nfeatures. For example, interpreting machine instructions that would normally be executed in order to \nadvance to a safe-point for inter\u00adruption, building various runtime maps keyed on return addresses, doing \nemulation, JIT-compilation or SFI. 3. Source Language Our high-level language PCFv is a conventional \nsimply-typed, call\u00adby-value functional language with types built from integers and booleans by products \nand function spaces, with type contexts G de.ned in the usual way: t := Int | Bool | t . t' | t \u00d7 t' \nG := x1 : t1 , ... , xn : tn We separate syntactic values (canonical forms), ranged over by V , from \ngeneral expressions, ranged over by M and restrict the syntax to ANF, with explicit sequencing of evaluation \nby let and explicit inclusion of values into expressions by [\u00b7]. The typing rules for values and for \nexpressions are shown in Figure 1. Note that there are really two forms of judgement, but we refrain \nfrom distinguishing them syntactically. The symbol * stands for an arbitrary integer-valued binary operation \non integers, whilst > is a representative boolean-valued one. PCFv has the obvious CBV operational semantics, \nwhich we elide here, and a conventional, and computationally adequate, de\u00adnotational semantics in the \ncategory of .-cpos and continuous maps, which we now brie.y summarize to .x notation. Types and environments \nare interpreted as cpos: def [Int] = N def [Bool] = B 'def ' = [t . t[t] . [t ]]. def [x1 : t1,...,xn \n: tn] = [t1] \u00d7\u00b7 \u00b7\u00b7\u00d7 [tn] where . is the cpo of continuous functions and \u00d7 is the Cartesian product cpo. \nTyping judgements for values and expressions are then interpreted as continuous maps: : [G f V : t][G] \n. [t] : [G f M : t][G] . [t]. de.ned by induction. So, for example [G f Fix fx = M : A . B] . = \u00b5df ..dx \n. [A].[G,f : A . B, x : A f M : B] (., df ,dx) We write [\u00b7]: D . D. for the unit of the lift monad. We \nelide the full details of the denotational semantics as they are essentially the same as those found \nin any textbook, such as that of Winskel (1993). The details can also be found, along with discussion \nof the Coq formalization of the semantics, in Benton et al. (2009). 4. Target Language and Compilation \n 4.1 An SECD Machine The low-level target is a variant of an SECD virtual machine (Landin 1964). We have \nchosen such a machine rather than a lower\u00adlevel assembly language, such as that of our previous work, \nso as to keep the formal development less cluttered with detail. But we are emphatically not interested \nin the SECD machine as some\u00adthing that is inherently for executing programs in a language like our source. \nWe have included an equality test instruction, Eq, that works on arbitrary values, including closures, \nso a counterexample to a naive semantics of types like that in Appendix A can be con\u00ad structed. Furthermore, \nthe logical relations we present have been carefully constructed with applicability to lower-level machines \nin mind.  Values: [TV AR][T BOOL](b . B)[TINT ](n . N) G,x : t f x : t G f b : Bool G f n : Int G,f \n: t . t ' ,x : t f M : t ' G f Vi : ti (i =1, 2) [TFIX] [TP ] G f Fix fx = M : t . t ' G f(V1,V2) : t1 \n\u00d7 t2 Expressions: G f V : t G f M : t G,x : t f N : t ' [TV AL][T LET ] G f [V ]: t G f let x = M in \nN : t ' G f V1 : t . t ' G f V2 : t G f V : Bool G f M1 : t G f M2 : t [T AP P ] [TIF ] G f V1 V2 : t \n' G f if V then M1 else M2 : t G f V1 : Int G f V2 : Int G f V1 : Int G f V2 : Int [T OP ] [T GT ] G \nf V1 *V2 : Int G f V1 >V2 : Bool G f V : t1 \u00d7 t2 [T F ST, T SND] G f pi(V ): ti (i =1, 2) Figure 1. \nTyping rules for PCFv The inductive type Instruction, ranged over by i, is de.ned by i := Swap | Dup \n| PushV n | Op * | PushC c | PushRC c | App | Ret | Sel (c1,c2) | Join | MkPair | Fst | Snd | Eq where \nc ranges over Code, the set of lists of instructions, n ranges over integers, and * over binary operations \non integers. The set Value of runtime values, ranged over by v is de.ned by v := n | CL (e, c) | RCL \n(e, c) | PR (v1,v2) where e ranges over Env, de.ned to be list Value. So a Value is either an integer \nliteral, a closure containing an environment and some code, a recursive closure, or a pair values. We \nalso de.ne Stack = list Value Dump = list (Code \u00d7 Env \u00d7 Stack) CESD = Code \u00d7 Env \u00d7 Stack \u00d7 Dump CESD \nis the set of con.gurations of our virtual machine. A con.guration (c, e, s, d) comprises the code to \nbe executed, c, the current environment, e, an evaluation stack s and a call stack, or dump, d. The deterministic \none-step transition relation . between con\u00ad.gurations is de.ned in Figure 2. There are many con.gurations \nwith no successor, such as those in which the next instruction is Swap but the the stack depth is less \nthan two; we say such con.gu\u00adrations are stuck or terminated. (So there is no a priori distinction between \nnormal and abnormal termination.) We write cesd .k to mean that the con.guration cesd takes at least \nk steps without getting stuck, and say it diverges, written cesd .. if it can always take a step: def \ncesd .. .. (.k, cesd .k) Conversely, we say cesd terminates, and write cesd . * , if it does not diverge: \ndef cesd . *.. \u00ac(cesd ..).  4.2 Compiling PCFv to SECD The compiler comprises two mutually-inductive \nfunctions mapping (typed) PCFv values and expressions into Code. We overload .\u00b7 for both of these functions, \nwhose de.nitions are shown in Figure 3.. 5. Logical Relations In this section we de.ne logical relations \nbetween components of the low-level SECD machine and elements of semantic domains, with the intention \nof capturing just when a piece of low-level code realizes a semantic object. In fact, there will be two \nrelations, one de.ning when a low-level component approximates a domain ele\u00adment, and one saying when \na domain element approximates a low\u00adlevel component. These roughly correspond to the soundness and adequacy \ntheorems that one normally proves to show a correspon\u00addence between an operational and denotational semantics, \nbut are rather more complex. Following the general pattern of biorthogonality sketched above, we work \nwith (predicates on) substructures of complete con.gura\u00adtions. On the SECD machine side, complete con.gurations \nare ele\u00adments of CESD, whilst the substructures will be elements of Value and of Comp, which is de.ned \nto be Code \u00d7 Stack. If v : Value then we de.ne vc: Comp to be ([], [v]), the computation compris\u00ading \nan empty instruction sequence and a singleton stack with v on. Similarly, if c : Code then cc : Comp \nis (c, []). The basic plugging operation on the low-level side is \u00b7 \u00b7, taking an element of Comp, the \ncomputation under test, and an element of CESD, thought of as a context, and combining them to yield \na con.guration in CESD: '''' '' ' (c, s) .,e ,s ,d ) = (c ++ c,e ,s ++ s,d ' ) (c We also have an operation \n\u00b7 . \u00b7 that appends an element of Env onto the environment component of a con.guration: ''' ''' e. (c,e \n,s ,d ' ) = (c,e ++ e,s ,d ' ) 5.1 Approximating Denotational By Operational The logical relation expressing \nwhat it means for low-level compu\u00adtations to approximate denotational values works with step-indexed \nentities. We write iValue for N\u00d7 Value, iComp for N\u00d7 Comp and  (Swap :: c, e, v1 :: v2 :: s, d) .(c, \ne, v2 :: v1 :: s, d)(Dup :: c, e, v :: s, d) .(c, e, v :: v :: s, d)(PushV n :: c, [v1,...,vk], s, d) \n.(c, [v1,...,vk],vn :: s, d)(PushN n :: c, e, s, d) .(c, e, n :: s, d)(PushC bod :: c, e, s, d) .(c, \ne, CL (e, bod) :: s, d)(PushRC bod :: c, e, s, d) .(c, e, RCL (e, bod) :: s, d)(App :: c, e, v :: CL \n(e ' , bod) :: s, d) .(bod,v :: e ' , [], (c, e, s) :: d) ' '' (App :: c, e, v :: RCL (e, bod) :: s, \nd) .(bod,v :: RCL (e, bod) :: e, [], (c, e, s) :: d) (Op * :: c, e, n2 :: n1 :: s, d) .(c, e, n1 *n2 \n:: s, d) ''' ''' (Ret :: c, e, v :: s, (c ,e ,s ) :: d) .(c,e ,v :: s,d) (Sel (c1, c2) :: c, e, v :: \ns, d) . (c1, e, s, (c, [], []) :: d) (if v = 0) (Sel (c1, c2) :: c, e, 0 :: s, d) . (c2, e, s, (c, [], \n[]) :: d) ' ' ' ' (Join :: c, e, s, (c , e , s ) :: d) . (c , e, s, d) (MkPair :: c, e, v1 :: v2 :: \ns, d) .(c, e, PR (v2,v1) :: s, d) (Fst :: c, e, PR (v1,v2) :: s, d) .(c, e, v1 :: s, d) (Snd :: c, e, \nPR (v1,v2) :: s, d) .(c, e, v2 :: s, d) (Eq :: c, e, v1 :: v2 :: s, d) .(c, e, 1 :: s, d) (if v1 = v2) \n(Eq :: c, e, v1 :: v2 :: s, d) .(c, e, 0 :: s, d) (if v1= v2) Figure 2. Operational Semantics of SECD \nMachine Values: x1 : t1,...,xn : tn f xi : ti =[PushV i] G f true : Bool =[PushN 1] G f false : Bool \n=[PushN 0] G f n : Int =[PushN n] G f(V1,V2) : t1 \u00d7 t2 =G f V1 : t1 ++ G f V2 : t2 ++[MkPair] ' '' G \nf Fix fx = M : t . t =[PushRC (G,f : t . t ,x : t f M : t ++[Ret])] Expressions: G f [V ]: t =G f V \n: t G f let x = M in N : t ' =[PushC (G,x : t f N : t ' ++[Ret])]++ G f M : t ++[App] G f V1 V2 : t ' \n=G f V1 : t . t ' ++ G f V2 : t ++[App] G f if V then M1 else M2 : t =G f V : Bool ++[Sel (( G f M1 : \nt ++[Join]), (G f M2 : t ++[Join])) G f V1 *V2 : Int =G f V1 : Int ++ G f V2 : Int ++[Op *] G f V1 >V2 \n: Bool =G f V1 : Int ++ G f V2 : Int ++[Op (.(n1,n2).n1 >n2 . 1 | 0)] Figure 3. Compiler for PCFv iCESD \nfor N\u00d7CESD and de.ne an Env-parameterized observation O e over pairs of indexed computations from iComp \nand indexed contexts from iCESD: def cesd)) .min(i,j) O e (i, comp)(j, cesd) .. (comp (e So, given an \nenvironment e, O e holds of an indexed computation and an indexed context just when the con.guration \nthat results from appending the environment e and the code and stack from the computation onto the corresponding \ncomponents of the context steps for at least the minimum of the indices of the iComp and the iCESD. We \nalso de.ne an observation on pairs of indexed values and indexed contexts by lifting values to computations: \ndef O e (i, v)(j, cesd) .. O e (i, cv)(j, cesd) Now we follow the general pattern of orthogonality, but \nwith some small twists. We actually have a collection of observations, indexed by environments, made \nover step-indexed components of con.gu\u00adrations. And each observation gives rise to two, closely related, \nGa\u00adlois connections: one between predicates on (indexed) values and predicates on (indexed) contexts, \nand the other between those on (indexed) computations and (indexed) contexts. So there are four contravariant \nmaps associated with each e : Env. Our de.nitions use these two of them: .e (\u00b7) : P(iValue) . P(iCESD) \n.e (P ) = {jcesd | .iv . P, O e iv jcesd} .e (\u00b7) : P(iCESD) . P(iComp) .e (Q) = {icomp | .jcesd . Q, \nO e icomp jcesd} To explain the notation: down arrows translate positive predicates (over values and \ncomputations) into negative ones (over contexts), whilst up arrows go the other way. We use single arrows \nfor the operations relating to values and double arrows for those relating to computations.  Now we \ncan start relating the low-level machine to the high\u00adlevel semantics. If D is a cpo and RDi . Value\u00d7D \nis a N-indexed relation between machine values and elements of D then de.ne the indexed relation [RD]n \n. Value \u00d7 D. by [RD]n = {(v, dv) |.d . D, [d]= dv . (v, d) . RDn} If, furthermore, S is a cpo and RSi \n. Env \u00d7 S an indexed relation between machine environments and elements of S, then we de.ne an indexed \nrelation < (RS . RD.)i . Comp \u00d7 (S . D.) = {(comp, df) |.k = i, .(e, de) . RSk, (k, comp) ..e (.e ({(j, \nv) | (v, dfde) . [RD]j }))} which one should see as the relational action of the lift monad, relative \nto a relation on environments. The de.nition looks rather complex, but the broad shape of the de.nition \nis logical : relating machine computations to denotational continuous maps just when RS-related environments \nyield [RD]-related results. Then there is a little extra complication caused by threading the step indices \naround, but this is also of a standard form: computations are in the relation at i when they take k-related \narguments to k-related results for all k = i. Finally, we use biorthogonality to close up on the right \nhand side of the arrow; rather than making an intensional direct-style de.nition that the computation \nyields a value v that is related to the denotational application df de , we take the set of all such \nrelated results, .ip it across to an orthogonal set of contexts with .e (\u00b7) and then take that back to \na set of computations with .e (\u00b7). A special case of indexed relations between machine environ\u00adments \nand denotational values is that for the empty environment. We de.ne Ii . Env \u00d7 1, where 1 is the one-point \ncpo, by Ii = {([], *)}. We can now de.ne the real indexed logical relation of approximation between machine \nvalues and domain elements St i . Value \u00d7 [t] where t is a type and i is a natural number index like \nthis: SInt i = {(n, n) | n . N} SBool i = {(0, false)}.{(n +1, true) | n . N} St\u00d7t' = {(PR (v1,v2), (dv1, \ndv2)) | (v1, dv1) . Sti . i (v2, dv2) . Sti ' } Sti .t' = {(f, df) |.k = i, .(v, dv) . Stk, < (([App], \n[v, f]),.* :1.df dv) . (I . (St' ).)k} This says that machine integers approximate the corresponding \ndenotational ones, the machine zero approximates the denotational false value, and all non-zero machine \nintegers approximate the denotational true value, re.ecting the way in which the low\u00adlevel conditional \nbranch instruction works and the way in which we compile source-level booleans. Pair values on the machine \napproximate denotational pairs pointwise. As usual, the interesting case is that for functions. The de.nition \nsays that a machine value f and a semantic function df are related at type t . t ' if whenever v is related \nto dv at type t, the computation whose code part is a single application instruction and whose stack \npart is the list [v, f] is related to the constantly (df dv) function, of type 1 . [t ' ]., by the monadic \nlifting of the approximation relation at type t ' . Having de.ned the relation for values, we lift it \nto environments in the usual pointwise fashion. If G is x1 : t1,...,xn : tn then SG . Env \u00d7 [G] is given \nby i SG i = {([v1,...,vn], (d1,...,dn)) |.l, (vl,dl) . Sitl } For computations in context, we de.ne SGi \n,t . Comp \u00d7 ([G] . ) using the monadic lifting again [t]. SG,t . (St i =(SG <).)i These relations are \nantimonotonic in the step indices and monotone in the domain-theoretic order (we also switch to in.x \nnotation for relations at this point): Lemma 1. 1. If v Sti d, d . d ' and j = i then v Stj d ' . 2. \nIf e SG ., . . . ' j . '  i and j = i then e SG . '' 3. If comp SGi ,t f, f . f and j = i then comp \nSGj ,t f . The non-indexed versions of the approximation relations are then given by universally quantifying \nover the indices. def v .t d .. .i, v Sit d def e .G . .. .i, e SG i . def comp .G,t df .. .i, comp SGi \n,t df Note that the relation on computations extends that on values: v .[],t Lemma 2. If v .t d then \nc(.* :1.[d]).  5.2 Approximating Operational By Denotational Our second logical relation captures what \nit means for a denota\u00adtional value to be less than or equal to a machine computation. This way around \nwe will again use biorthogonality, but this time with respect to the observation of termination. This \nis intuively rea\u00adsonable, as showing that the operational behaviour of a program is at least as de.ned \nas some domain element will generally in\u00advolve showing that reductions terminate. We will not use opera\u00adtional \nstep-indexing to de.ne the relation this way around, but an explicit admissible closure operation will \nplay a similar r ole. For e . Env, comp . Comp and cesd . CESD our termination observation is de.ned \nby def T e comp cesd .. (comp (e cesd)) . * which we again lift to values v . Value: def T ev cesd .. \nT e cv cesd and again the observations generate two e-parameterized Galois connections, one between predicates \non values and predicates on contexts, and the other between predicates on computations and predicates \non contexts. Once more we use two of the four maps: .e(\u00b7): P(Value) . P(CESD) .e (P )= {cesd |.v . P, \nT ev cesd} .e(\u00b7): P(CESD) . P(Comp) .e(Q)= {comp |.cesd . Q, T e comp cesd} to de.ne a relational action \nfor the lift monad. If S and D are cpos, RS . Env \u00d7 S and RD . Value \u00d7 D, then de.ne . (RS . RD.) . Comp \n\u00d7 (S . D.) = {(comp, df) |.(e, de) . RS, .d, [d]=(df de) =. comp ..e(.e({v | (v, d) . RD}))} which follows \na similar pattern to our earlier de.nition, in using biorthogonality on the right hand side of the arrow: \nstarting with all values related to d, .ipping that over to the set of contexts that terminate when plugged \ninto any of those values and then coming back to the set of all computations that terminate when plugged \ninto any of those contexts.  Now we can de.ne the second logical relation apply a transitive closure \noperation to get a low-level notion of equivalence ~: tt by induction on the type t: G f comp1 ~ comp2 \n: t def .. G f comp1comp2 : t . Value \u00d7 [t] ~+ So now we have a notion of what it means for a piece \nof tInt = SECD machine code to be in the semantic interpretation of a source language type, and what \nit means for two pieces of code to be {(n, n) | n . N} tBool = {(0, false)}.{(n +1, true) | n . N} ' \nequal when considered at that type. Clearly, we are going to use tt\u00d7t = {(PR (v1,v2), (dv1, dv2)) | (v1, \ndv1) . tt . this to say something about the correctness of our compilation ' (v2, dv2) . tt } scheme, \nbut note that the details of just what code the compiler produces have not really shown up at all in \nthe de.nitions of our ' tt.t = {(f, df) |.(v, dv) . tt , logical relations: it is only the interfaces \nto compiled code the ' . (tt This relation also lifts pointwise to environments. For G= x1 : . way in \nwhich integers, booleans and pairs are encoded and the way in which function values are tested via application \n that are mentioned in the logical relation. In fact, equivalence classes of the (([App], [v, f]),.* \n:1.df dv) . (I ).)} t1,...,xn : tn de.ne tG . Env \u00d7 [G] by = {([v1,...,vn], (dv1, . . . , dvn)) |.l, \n(vl, dvl) . ttl } and then for computations in context, tG,t ~ relations can be seen as de.ning a perfectly \ngood compositional tG denotational semantics for the source language in their own right. A feature of \nthis semantics is that there are no statements about what (non-observable) intermediate con.gurations \nshould look like. For example, we never say that when a function is entered ) is . ([G] . [t]. given \nby tG,t =(tG . (tt The t relations are all down-closed on the denotational side: Lemma 3. 1. If v tt \nd and d ' d then v tt d ' 2. If e tG . and . ' . then e tG . ' 3. If comp tG,t f and f ' f then comp \ntG,t f '  However, for a .xed v, it turns out that {d | v tt d} is not always closed under taking limits \nof chains (see Appendix B for .amoredetailedexplanation),whichwouldpreventourcompiler correctnessproofgoingthroughinthecaseofrecursion.Wesolve \nthisproblembytakingtheScott-closure.Theclosedsubsetsof arethosethatarebothdown-closedandclosedunderDacpo \nlimitsof -chains.Theclosure ofasubset is..Clos(P ) PD thesmallestclosedsubsetof containing.Sowede.neDP \nttt... .{|}dvdv Clos(dd)vv GGt... .{|}dede Clos(dd)ee GG,t,tt... .{|}df df Clos(dd)comp comp def def \ndef Again, the relation for computations extends that for values: v .[],t Lemma 4. If v .t d then c(.* \n:1.[d]).  5.3 Realizability and Equivalence Having de.ned the two logical relations, we can clearly \nput them together to de.ne relations expressing that a machine value, envi\u00adronment or computation realizes \na domain element: t deft v |= d .. vd . v .t d G defG e |= de .. e de . e .G de G,t defG,t comp |= df \n.. comp df . comp .G,t df. and these relations naturally induce typed relations between ma\u00adchine components. \nWe just give the case for computations ~ comp2 : t ).) G f comp1 with a call stack that looks like x \n, then eventually one reaches a con.guration that looks like y . At the end of the day, all we ever talk \nabout is termination and divergence of complete con.gu\u00adrations, which we need to connect with the intended \nbehaviour of closed programs of ground type; this we do by considering a range of possible external test \ncontexts, playing the role of top-level con\u00adtinuations. If c . Code then we say c diverges unconditionally \nif .cesd, (c cesd) .. The following says that if a piece of code realizes the denotational bottom value \nat any type, then it diverges unconditionally: Lemma 5 (Adequacy for bottom). For any c . Code and type \nt, if c |=[],t (.* :1..[tl) then c diverges unconditionally. For ground type observations, we say a computation \ncomp converges to a particular integer value n if plugging it into an arbitrary context equiterminates \nwith plugging n into that context: .cesd,nccesd . * =. comp cesd . * . nccesd .. =. comp cesd .. . And \nwe can then show that if a piece of code realizes a non-bottom element [n] of [Int]. in the empty environment, \nthen it converges to n: Lemma 6 (Ground termination adequacy). For any c . Code, if c |=[],t (.* :1.[n]) \nthen c converges to n. Adequacy also holds for observation at the boolean type, with a de.nition of convergence \nto a value b that quanti.es over those test contexts cesd that terminate or diverge uniformly for all \nmachine values representing b. We .nally show the compositionality of our realizability seman\u00adtics. Lemma \n7 (Compositionality for application). For any cf, cx . Code and df . [G] . ([t] . [t ' ].)., dx . [G] \n. [t]., if G,t.t G,t cf |= ' df . cx |= dx then def .. G,t df . comp2 |= G,t df ), comp1 |=  .df . \n([G] . [t]. and we overload this notation to apply to simple pieces of code too: G f c1~ c2 : t def ~ \ncc2 : t .. G f cc1 ' cf ++ cx ++[App] |= G,t . de : [G]. (df de) Y (dx de) where Y denotes the lifted \n(Kleisli) application.  ~ relations are transitive (though we have no concrete counterexample), so we \nThere seems no general reason to expect that the  6. Applications In this section, we illustrate the \nkind of results one can establish using the logical relations of the previous section. 6.1 Compiler Correctness \nOur motivating application was establishing the functional correct\u00adness of the compiler that we presented \nearlier. Theorem 1. 1. For all G,V ,t, if G f V : t then G f V : t G,t [ [G f V : t] ] 2. For all G,M,t, \nif G f M : t then G,t [G f M : t]  G f M : t The two parts are proved simultaneously by induction on \ntyping derivations, as in most logical relations proofs. In the case for recursive functions, there is \na nested induction over the step indices. Theorem 2. 1. For all G,V ,t, if G f V : t then G f V : t G,t \n[ [G f V : t] ] 2. For all G,M,t, if G f M : t then G,t [G f M : t]  G f M : t This is another simultaneous \ninduction on typing derivations. This time, the proof for recursive functions involves showing that each \nof the domain elements in the chain whose limit is the denota\u00adtion is in the relation and then concluding \nthat the .xpoint is in the relation by admissibility. Corollary 1. 1. For all G,V ,t, if G f V : t then \nG f V : t |= G,t [ [G f V : t] ] 2. For all G,M,t, if G f M : t then G f M : t |= G,t [G f M : t]  \nSo the compiled code of a well-typed term always realizes the denotational semantics of that term. A \nconsequence is that compiled code for whole programs has the correct operational behaviour according \nto the denotational semantics: Corollary 2. For any M with [] f M : Int, If [[] f M : Int] = . then \n[] f M : Int diverges uncondi\u00adtionally.  If [[] f M : Int] =[n] then [] f M : Int converges to n.  \nwhich follows by the adequacy lemmas above. And of course, by composing with the result that the denotational \nsemantics is ade\u00adquate with respect to the operational semantics of the source lan\u00adguage, one obtains \nanother corollary, that the operational semantics of complete source programs agrees with that of their \ncompiled ver\u00adsions. It is this last corollary that is normally thought of as compiler correctness, but \nit is Corollary 1 that is really interesting, as it is that which allows us to reason about the combination \nof compiled code with code from elsewhere.  6.2 Low-level Equational Reasoning In this section we give \nsome simple examples of typed equivalences one can prove on low-level code. We .rst de.ne some macros \nfor composing SECD programs. If c, cf, cx . Code then de.ne LAMBDA(c)=[PushC (c ++[Ret])] APP(cf, cx)= \ncf ++ cx ++[App] 6.2.1 Example: Commutativity of addition De.ne the following source term plussrc =(Fix \nfx =[Fix gy = x + y]) and its compiled code pluscode(G)= G f plussrc : Int . Int . Int . Now we can show \nthe following: Lemma 8. For any G, for any c1,c2 . Code, and dc1, dc2 . [G] . [Int]., if G,Int G,Int \nc1 |= dc1 and c2 |= dc2 then G f APP(APP(pluscode G,c1),c2) ~ APP(APP(pluscode G,c2),c1): Int In other \nwords, for any code fragments c1,c2 that are in the in\u00adterpretation of the source language type Int, \nmanually composing those fragments with the code produced by the compiler for the cur\u00adried addition function \nin either order yields equivalent behaviour of type Int. 6.2.2 Example: First projection De.ne the source \nterm projfstsrc =(Fix fx =[Fix gy = x]) and the compiled code projfstcode(G , t, t ' )= G f projfstsrc \n: t . t ' . t then Lemma 9. For any G, t,t, for any c1,c2 . Code and dc1 . [G] . [t]. and dc2 . [G] . \n[t ' ]., if G,t G,t c1 |= dc1 and c2 |= ' dc2 and furthermore .de . [G], .dv . [t ' ] (dc2 de)=[dv] which \nsays that the code c2 realizes some total denotational com\u00adputation of type t ' in context G, then G \nf APP(APP(projfstcode(G, t, t ' ),c1),c2) ~ c1 : t. This says that the compiled version of projfstsrc \nbehaves like the .rst projection, provided that the second argument does not diverge.  6.2.3 Example: \nOptimizing iteration Our last example is slightly more involved, and makes interesting use of the non-functional \nequality test in the target language. We start by compiling the identity function on integers idsrc = \nFix id x = x idcode(G)= G f idsrc : Int . Int and then de.ne a higher-order function appnsrc that takes \na func\u00adtion f from integers to integers, an iteration count n and an integer v, and returns f applied \nn times to v. We present the de.nition in ML-like syntax rather than our ANF language to aid readability: \nappnsrc = fun f=> letrec apf n = fun v=> ifn> 0 then f (apf (n-1) v) else v  and we let appncode(G) \n= G f appnsrc :(Int . Int) . Int . Int . Int Now we de.ne a handcrafted optimized version, appnoptcode(G) \nin the SECD language, which would, if one could write it, corre\u00adspond to ML-like source code looking \nsomething like this: funf=>funn=> funv=> if f =a idcode(G) then v else appnsrc fnv The optimized code \nchecks to see if it has been passed the literal closure corresponding to the identity function, and if \nso simply returns v without doing any iteration. We are then able to show that for any G, G f appnoptcode(G) \n~ appncode(G) :(Int . Int) . Int . Int showing that the optimized version, which could not be written \nin the source language, is equivalent to the unoptimized original. 7. Discussion We have given a realizability \nrelation between the domains used to model a simply-typed functional language with recursion and the \nlow level code of an SECD machine with non-functional features. This relation was used to establish a \nsemantic compiler correctness result and to justify typed equational reasoning on handcrafted low-level \ncode. The relations make novel use of biorthogonality and step-indexing, and the work sheds interesting \nnew light on the interaction of these two useful constructions. As we said in the introduction, there \nare many other compiler correctness proofs in the literature, but they tend not to be so compositional \nor semantic in character as the present one. The classic work on the VLISP veri.ed Scheme compiler by \nGuttman et al. (1995) is very close in character, being based on relating a denotational semantics to, \nultimately, real machine code. The untyped PreScheme language treated in that work is signi.cantly more \nrealistic than the toy language of the present work, though the proofs were not mechanized. The denotational \nsemantics used there was in CPS and the main emphasis was on the behaviour of complete programs. Chlipala \n(2007) has also used Coq to formalize a correctness relation between a high-level functional language \nand low-level code, though in that case the source language is total and so can be given an elementary \nsemantics in Sets. For ML-like languages (pure or impure), contextual equivalence at higher types is \nhighly complex, depending subtly on exactly what primitives one allows. This is why, as we said in the \nintroduction, we feel that fully abstract compilation might not be quite the right thing to aim for. \nFor example, Longley (1999) shows that there are extensionally pure (and even useful) functionals that \nare only de.neable in the presence of impure features, such as references. Adding such functionals is \nnot conservative they re.ne contextual equivalence at order four and above in pure CBV languages yet \nit seems clear that they will be implementable in many low-level machines. Complicating one s speci.cations \nto rule out these ex\u00adotic programs in pursuit of full abstraction is not obviously worth\u00adwhile: it seems \nimplausible that any compiler transformations or information .ow policies will be sensitive to the difference. \nAs an\u00adother example, the presence of strong re.ective facilities at the low\u00adlevel, such as being able \nto read machine code instructions, might well make parallel-or de.neable; this would obviously break \nfull abstraction with respect to the source language, but we might well wish to allow it.4 The case against \nfull abstraction becomes stronger when one considers that one of our aims is to facilitate semantically \ntype safe linking of code produced from different programming languages. There is a fair degree of wiggle \nroom in deciding just how strong the semantic assumptions and guarantees should be across these interfaces. \nThey should be strong enough to support sound and useful reasoning from the point of view of one language, \nbut not insist on what one might almost think of as accidental properties of one language that might \nbe hard to interpret or ensure from the point of view of others. The Coq formalization of these results \nwas pleasantly straight\u00adforward. Formalizing the SECD machine, compiler, logical rela\u00adtions and examples, \nincluding the compiler correctness theorem, took a little over 4000 lines, not including the library \nfor domain theory and the semantics of the source language. The extra burden of mechanized proof seems \nfairly reasonable in this case, and the results, both about the general theory and the examples, are \nsuf.\u00adciently delicate that our con.dence in purely paper proofs would be less than complete. There are \nmany obvious avenues for future work, including the treatment of richer source languages and type systems, \nand of lower-level target languages. We intend particularly to look at source languages with references \nand polymorphism and at a target machine like the idealized assembly language of our previous work. We \nwould also like to give low-level speci.cations that are more independent of the source language -the \ncurrent work doesn t mention source language terms, but does still talk about particular cpos. We would \nlike to express essentially the same constraints in a more machine-oriented relational Hoare logic which \nmight be more language neutral and better suited for independent veri.cation. It would also be interesting \nto look at type systems that are more explicitly aimed at ensuring secure information .ow. Acknowledgments \nThanks to Andrew Kennedy, Neel Krishnaswami, Nicolas Tabareau and Carsten Varming for many useful discussions. \nExtra thanks to Andrew and Carsten for their work on the Coq formalization of the source language and \nits denotational semantics. References M. Abadi. TT-closed relations and admissibility. Mathematical \nStructures in Computer Science, 10(3), 2000. M. Abadi. Protection in programming-language translations. \nIn 25th International Colloquium on Automata, Languages and Programming (ICALP), volume 1443 of Lecture \nNotes in Computer Science, 1998. A. Ahmed. Step-indexed syntactic logical relations for recursive and \nquan\u00adti.ed types. In 15th European Symposium on Programming (ESOP), volume 3924 of Lecture Notes in Computer \nScience, 2006. A. Ahmed and M. Blume. Typed closure conversion preserves observa\u00adtional equivalence. \nIn 13th ACM SIGPLAN International Conference on Functional Programming (ICFP), 2008. A. Ahmed, D. Dreyer, \nand A. Rossberg. State-dependent representation in\u00addependence. In 36th ACM SIGPLAN-SIGACT Symposium on \nPrinciples of Programming Languages (POPL), 2009. A. Appel and D. McAllester. An indexed model of recursive \ntypes for foundational proof-carrying code. ACM Transactions on Programming Languages and Systems (TOPLAS), \n23(5), 2001. A.W. Appel, P.-A. Melli`es, C.D. Richards, and J. Vouillon. A Very Modal Model of a Modern, \nMajor, General Type System. 34th ACM SIGPLAN\u00ad 4 Discussing the sense in which these two candidate full \nabstraction\u00adbreaking operations are incompatible would take us too far a.eld, however.  SIGACT Symposium \non Principles of Programming Languages (POPL), 2007. N. Benton. Abstracting allocation: The new new thing. \nIn 20th Interna\u00adtional Workshop on Computer Science Logic (CSL), volume 4207 of LNCS, 2006. N. Benton \nand N. Tabareau. Compiling functional types to relational speci.cations for low level imperative code. \nIn 4th ACM SIGPLAN Workshop on Types in Language Design and Implementation (TLDI), 2009. N. Benton and \nU. Zarfaty. Formalizing and verifying semantic type sound\u00adness of a simple compiler. In 9th ACM SIGPLAN \nInternational Sympo\u00adsium on Principles and Practice of Declarative Programming (PPDP), 2007. N. Benton, \nA. Kennedy, and C. Varming. Some domain theory and deno\u00adtational semantics in Coq. In 22nd International \nConference on Theo\u00adrem Proving in Higher Order Logics (TPHOLs), volume 5674 of Lecture Notes in Computer \nScience, 2009. A. Chlipala. A certi.ed type-preserving compiler from lambda calculus to assembly language. \nIn ACM SIGPLAN 2007 Conference on Program\u00adming Language Design and Implementation (PLDI), 2007. M. Dave. \nCompiler veri.cation: a bibliography. ACM SIGSOFT Software Engineering Notes, 28(6), 2003. J. Guttman, \nJ. Ramsdell, and M. Wand. VLISP: A veri.ed implementation of scheme. Lisp and Symbolic Computation, 8(1/2), \n1995. T. Hardin, L. Maranget, and B. Pagano. Functional runtime systems within the lambda-sigma calculus. \nJournal of Functional Programming, 8, 1998. A. Kennedy. Securing the .NET programming model. Theoretical \nCom\u00adputer Science, 364(3), 2006. J. L. Krivine. Classical logic, storage operators and second-order lambda \ncalculus. Annals of Pure and Applied Logic, 1994. P. Landin. The mechanical evaluation of expressions. \nThe Computer Journal, 6(4), 1964. X. Leroy. Formal certi.cation of a compiler back-end, or: programming \na compiler with a proof assistant. In 33rd ACM SIGPLAN-SIGACT Symposium on Principles of Programming \nLanguages (POPL), 2006. X. Leroy and H. Grall. Coinductive big-step operational semantics. Infor\u00admation \nand Computation, 207(2), 2009. J. Longley. When is a functional program not a functional program? In \n4th ACM SIGPLAN International Conference on Functional Programming (ICFP), 1999. J. McCarthy and J. Painter. \nCorrectness of a Compiler for Arithmetic Expressions. Proceedings Symposium in Applied Mathematics, 19:33 \n41, 1967. A. M. Pitts and I. D. B. Stark. Operational reasoning for functions with local state. In Higher \nOrder Operational Techniques in Semantics. CUP, 1998. G. D. Plotkin. LCF considered as a programming \nlanguage. Theoretical Computer Science, 5, 1977. J. Vouillon and P.-A. Melli`es. Semantic types: A fresh \nlook at the ideal model for types. In 31st ACM Symposium on Principles of Programming Languages (POPL), \n2004. G. Winskel. The Formal Semantics of Programming Languages. MIT Press, 1993. A. The Problem With \nRealizing Recursion This appendix gives a concrete example of how de.ning seman\u00adtics for functional types \nas sets of untyped programs can run into problems in the case that the source language includes recursion \nand the untyped target includes non-functional operations. By non\u00adfunctional operations, we particularly \nmean operations that exam\u00adine the actual syntax or code of functional terms. The example we take here \nis a syntactic equality test: an operation that takes two ar\u00adguments and returns true if the two (possibly \nfunctional) terms are syntactically equal and otherwise returns false. De.nition of VULE. As an example \nof a target language, we consider the call-by-value untyped lambda calculus with a syntactic equality \ntest (VULE). We give the formal de.nition and operational semantics of VULE below. We .x a countable \nset V of variables. The set of values Val and the set of terms Term with variables in V are mutually \ninductively de.ned by the following rule: Val := x | .x. t Term := v | ts | u =a v | ERROR where x . \nV, u, v . Val, t, s . Term. As usual, we assume that the appplication is left-associative. FVar(t) for \nany term t denotes the set of free variables in t, and t [x . s] the capture-avoiding substitution of \nthe term s for the variable x in the term t. We also de.ne some syntactic sugar for boolean operation \nand recursion (implemented via a CBV .xed point combinator). TRUE . .x. .y. x y FALSE . .x. .y. y x if \nt then s1 else s2 . t (.x. s1)(.x. s2) x . FVar(s1) . FVar(s2) rec t . .x. (.y. t (.z. y y z)) (.y. \nt (.z. y y z)) x x, y . FVar(t),z = y The small-step call by value operational semantics of VULE is \ngiven as follows: t t ' =. ts t ' s s s ' =. vs vs ' (.x. t) v t [x . v] u a v =. u =a v TRUE u a v =. \nu =a v FALSE ERROR v ERROR where x . V, u, v . Val, t, t ' , s, s ' . Term and where u a v means that \nu is alpha-equivalent to v. For convenience, we de.ne ++ the multi-step relation by setting t t ' iff \nthe term t reaches t ' in one or more steps. Although we have used lambda-encodings of booleans, condi\u00adtionals \nand recursion, so as to be both concrete and minimal, the argument we shall make does not depend on these \ndetails. In fact we will only rely on some elementary properties of our encodings. The following hold \nfor any v . Val and t, s1,s2 . Term: (Prop 1) TRUE, FALSE and rec t are values; + (Prop 2) if TRUE then \ns1 else s2 s1; + (Prop 3) if FALSE then s1 else s2 s2; + (Prop 4) (rec t) v t (rec t) v. The problem. \nWe now de.ne the problem. Let Type be the set of simple types de.ned by the rule T . Type := Bool | T \n. T. As usual, the type constructor . is right-associative. Now we con\u00adsider what happens if we try to \ngive a semantics to these types as sets of closed VULE terms that admit recursively-de.ned func\u00adtions. \nWhatever the details, we would expect the following, very minimal, conditions to be satis.ed by the interpretation \nof types  [-] : Type . P(Term): (Asm 1) TRUE, FALSE . [Bool]. (Asm 2) Given a value u . [A1 . ... . \nAn . B] and values v1 . [A1] ... vn . [An], the application of u to the vis does not go wrong: uv1 ... \nvn + / ERROR. (Asm 3) For a value u, if uv + v for all values v . [A], then u . [A . A]. We now show \nthat, rather distressingly, for any semantics [-]satisfying the above conditions, the .xed point combinator \nY de\u00ad.ned by Y .f. rec f is not in the set [((Bool . Bool) . Bool . Bool) . Bool . Bool]. Let the value \nF be de.ned by F .g. .f. if f =a (rec g) then ERROR else f and observe the following facts about the \nbehaviour of F : (Obs 1): For any value v . Val, (rec F ) v + F (rec F ) v +ERROR if v a rec rec F v \notherwise (Obs 2): (rec rec F ) TRUE + (rec F )(rec rec F ) TRUE + ERROR TRUE ERROR (Obs 3): Y (rec F \n) TRUE (rec rec F ) TRUE + ERROR From these observations, we can conclude that (Con 1): rec rec F . [Bool \n. Bool] because TRUE . [Bool](Asm 1), (rec rec F ) TRUE + ERROR (Obs 2), and well\u00adtyped applications \ndon t go wrong (Asm 2). (Con 2): rec F . [(Bool . Bool) . Bool . Bool] because rec F behaves as the identity \non all values except rec rec F (Obs 1), and since rec rec F is not in [Bool . Bool](Con 1), rec F must \nbehave as the identity on all values that are in [Bool . Bool], we get the conclusion by (Asm 3). (Con \n3): Now we see that Y . [((Bool . Bool) . Bool . Bool) . Bool . Bool] because Y (rec F ) TRUE + ERROR \n(Obs 3) rec F . [(Bool . Bool) . Bool . Bool] (Con 2) TRUE . [Bool] (Asm 1) and well-typed applications \ndon t go wrong (Asm 2). This is not at all what we wanted! One would expect (Con 2) to be false, since \nF is clearly highly suspicious, and then the blameless Y could have the expected type. Our analysis of \nthe problem is that (Asm 3) is the only one of our assumptions that could be modi.ed in order to get \nthe expected result. In short, just testing with good arguments is actually insuf.cient grounds for concluding \nthat a function is good: we need some extra tests on partially good values, which is just what step-indexing \nwill supply. B. On the non-closure of tt The reason {d | v tt d} is not always closed under taking limits \nof chains is essentially that (\u00b7).. does not preserve meets. In particular TT T fT fTT Pi =(Pi ).Pi \nii i with the last inclusion following from contravariance and the strict inclusion fT Pi T .Pi . ii \nNow let (dfi) be a chain of elements of [t1 . t2] with Uidfi = df, such that for all i and for all dv \n. [t1], dfi dv : .. Then we claim that in general, there is a strict inclusion {f |.i, f tt1.t2 dfi}.{f \n| f tt1.t2 df} Expanding the de.nitions, f being in the right-hand set above means exactly that for all \nv, dv such that v tt1 dv, . tt2 (([App], [v, f]),.*. df dv) . (I .) . which expands to .d, d =[df dv] \n. ([App], [v, f]) ..(.({w | w tt2 d})) or, equivalently (because of the non-. assumption), ([App], [v, \nf]) ..(.({w |.d, d =[df dv] . w tt2 d})) which is to say, ([App], [v, f]) is in .(.({wc|.d, d =[df dv] \n. w tt2 d})). (1) Using similar reasoning, one can deduce that f being in the left\u00adhand side of our inclusion \nis equivalent to saying that for all v, dv such that v tt1 dv, the computation ([App], [v, f]) is in \n .(.({wc|.d, d =[dfi dv] . w tt2 d})). (2) i But there is in general a strict inclusion between the set \nin (1) and that in (2). By down-closure, wc|.d, d =[df dv] . w tt2 d} { .{wc|.d, d =[dfi dv] . w tt2 \nd} i so .(.( wc|.d, d =[df dv] . w tt2 d})) { ..(.( {wc|.d, d =[dfi dv] . w tt2 d})) { i ..(.({wc|.d, \nd =[dfi dv] . w tt2 d})) i by the general property of biorthogonals above. To address this issue, we \ncan either add in just enough limits or close up under all limits. The .rst version of our de.nitions \nlooked like def v t dv .. .(di),dv di ..i, v tt di i which adds limits of arbitrary chains to (and down-closes) \nthe right\u00adhand side of the relation. The resulting relation may still not be closed under limits of chains, \nbut actually works perfectly well for our purposes, as it does contain limits of all chains arising in \nthe semantics of functions in our language. (And we even proved all the theorems in Coq using this de.nition. \n. . ) Nevertheless, it seems mathematically more natural to work with Scott-closed sets, so we have now \nadopted the de.nition using Clos(\u00b7) shown in the main text.    \n\t\t\t", "proc_id": "1596550", "abstract": "<p>We define logical relations between the denotational semantics of a simply typed functional language with recursion and the operational behaviour of low-level programs in a variant SECD machine. The relations, which are defined using biorthogonality and stepindexing, capture what it means for a piece of low-level code to implement a mathematical, domain-theoretic function and are used to prove correctness of a simple compiler. The results have been formalized in the Coq proof assistant.</p>", "authors": [{"name": "Nick Benton", "author_profile_id": "81100165244", "affiliation": "Microsoft Research, Cambridge, United Kingdom", "person_id": "P1614000", "email_address": "", "orcid_id": ""}, {"name": "Chung-Kil Hur", "author_profile_id": "81436594089", "affiliation": "University of Cambridge, Cambridge, United Kingdom", "person_id": "P1614001", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1596550.1596567", "year": "2009", "article_id": "1596567", "conference": "ICFP", "title": "Biorthogonality, step-indexing and compiler correctness", "url": "http://dl.acm.org/citation.cfm?id=1596567"}