{"article_publication_date": "08-31-2009", "fulltext": "\n Finding Race Conditions in Erlang with QuickCheck and PULSE Koen Claessen Michal Palka John Hughes \nHans Svensson Ulf Wiger Nicholas Smallbone Thomas Arts Erlang Training and Consulting Chalmers University \nof Technology, Chalmers University of Technology and ulf.wiger@erlang-consulting.com Gothenburg, Sweden \nQuviq AB koen@chalmers.se rjmh@chalmers.se michal.palka@chalmers.se hans.svensson@ituniv.se nicsma@chalmers.se \nthomas.arts@ituniv.se Abstract We address the problem of testing and debugging concurrent, dis\u00adtributed \nErlang applications. In concurrent programs, race condi\u00adtions are a common class of bugs and are very \nhard to .nd in prac\u00adtice. Traditional unit testing is normally unable to help .nding all race conditions, \nbecause their occurrence depends so much on tim\u00ading. Therefore, race conditions are often found during \nsystem test\u00ading, where due to the vast amount of code under test, it is often hard to diagnose the error \nresulting from race conditions. We present three tools (QuickCheck, PULSE, and a visualizer) that in \ncombi\u00adnation can be used to test and debug concurrent programs in unit testing with a much better possibility \nof detecting race conditions. We evaluate our method on an industrial concurrent case study and illustrate \nhow we .nd and analyze the race conditions. Categories and Subject Descriptors D.2.5 [Testing and Debug\u00adging]: \nDistributed debugging General Terms Veri.cation Keywords QuickCheck, Race Conditions, Erlang 1. Introduction \nConcurrent programming is notoriously dif.cult, because the non\u00addeterministic interleaving of events \nin concurrent processes can lead software to work most of the time, but fail in rare and hard\u00adto-reproduce \ncircumstances when an unfortunate order of events occurs. Such failures are called race conditions. In \nparticular, con\u00adcurrent software may work perfectly well during unit testing, when individual modules \n(or software units ) are tested in isolation, but fail later on during system testing. Even if unit tests \ncover all as\u00adpects of the units, we still can detect concurrency errors when all components of a software \nsystem are tested together. Timing de\u00adlays caused by other components lead to new, previously untested, \nschedules of actions performed by the individual units. In the worst case, bugs may not appear until \nthe system is put under heavy load in production. Errors discovered in these late stages are far more \nexpensive to diagnose and correct, than errors found during unit testing. Another cause of concurrency \nerrors showing up at a late stage is when well-tested software is ported from a single-core to Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. ICFP 09, August \n31 September 2, 2009, Edinburgh, Scotland, UK. Copyright c &#38;#169; 2009 ACM 978-1-60558-332-7/09/08. \n. . $5.00 a multi-core processor. In that case, one would really bene.t from a hierarchical approach \nto testing legacy code in order to simplify debugging of faults encountered. The Erlang programming language \n(Armstrong 2007) is de\u00adsigned to simplify concurrent programming. Erlang processes do not share memory, \nand Erlang data structures are immutable, so the kind of data races which plague imperative programs, \nin which concurrent processes race to read and write the same memory lo\u00adcation, simply cannot occur. \nHowever, this does not mean that Er\u00adlang programs are immune to race conditions. For example, the order \nin which messages are delivered to a process may be non\u00addeterministic, and an unexpected order may lead \nto failure. Like\u00adwise, Erlang processes can share data, even if they do not share memory the .le store \nis one good example of shared mutable data, but there are also shared data-structures managed by the \nEr\u00adlang virtual machine, which processes can race to read and write. Industrial experience is that the \nlate discovery of race conditions is a real problem for Erlang developers too (Cronqvist 2004). More\u00adover, \nthese race conditions are often caused by design errors, which are particularly expensive to repair. \nIf these race conditions could be found during unit testing instead, then this would de.nitely re\u00adduce \nthe cost of software development. In this paper, we describe tools we have developed for .nding race \nconditions in Erlang code during unit testing. Our approach is based on property-based testing using \nQuickCheck (Claessen and Hughes 2000), in a commercial version for Erlang developed by Quviq AB (Hughes \n2007; Arts et al. 2006). Its salient features are described in section 3. We develop a suitable property \nfor testing parallel code, and a method for generating parallel test cases, in section 4. To test a wide \nvariety of schedules, we developed a randomizing scheduler for Erlang called PULSE, which we explain \nin section 5. PULSE records a trace during each test, but interpreting the traces is dif.cult, so we \ndeveloped a trace visualizer which is described in section 6. We evaluate our tools by applying them \nto an industrial case study, which is introduced in section 2, then used as a running example throughout \nthe paper. This code was already known to contain bugs (thanks to earlier experiments with QuickCheck \nin 2005), but we were previously unable to diagnose the problems. Using the tools described here, we \nwere able to .nd and .x two race conditions, and identify a fundamental .aw in the API. 2. Introducing \nour case study: the process registry We begin by introducing the industrial case that we apply our tools \nand techniques to. In Erlang, each process has a unique, dynamically-assigned identi.er ( pid ), and \nto send a message to a process, one must know its pid. To enable processes to discover the pids of central \nservices, such as error logging, Erlang provides a process registry a kind of local name server which \nassociates static names with pids. The Erlang VM provides operations to reg\u00adister a pid with a name, \nto look up the pid associated with a name, and to unregister a name, removing any association with that \nname from the registry. The registry holds only live processes; when reg\u00adistered processes crash, then \nthey are automatically unregistered. The registry is heavily used to provide access to system services: \na newly started Erlang node already contains 13 registered processes.  However, the built-in process \nregistry imposes several, some\u00adtimes unwelcome, limitations: registered names are restricted to be atoms, \nthe same process cannot be registered with multiple names, and there is no ef.cient way to search the \nregistry (other than by name lookup). This motivated Ulf Wiger (who was working for Ericsson at the time) \nto develop an extended process registry in Erlang, which could be modi.ed and extended much more easily \nthan the one in the virtual machine. Wiger s process registry soft\u00adware has been in use in Ericsson products \nfor several years (Wiger 2007). In our case study we consider an earlier prototype of this soft\u00adware, \ncalled proc_reg, incorporating an optimization that proved not to work. The API supported is just: reg(Name,Pid) \nto register a pid, where(Name) to look up a pid, unreg(Name) to remove a registration, and send(Name,Msg) \nto send a message to a regis\u00adtered process. Like the production code, proc_reg stores the as\u00adsociation \nbetween names and pids in Erlang Term Storage ( ETS tables ) hash tables, managed by the virtual machine, \nthat hold a set of tuples and support tuple-lookup using the .rst component as a key (cf. Armstrong 2007, \nchap 15). It also creates a moni\u00adtor for each registered process, whose effect is to send proc_reg a \nDOWN message if the registered process crashes, so it can be removed from the registry. Two ETS table \nentries are created for each registration: a forward entry that maps names to pids, and a reverse entry \nthat maps registered pids to the monitor reference. The monitor reference is needed to turn off monitoring \nagain, if the process should later be unregistered. Also like the production code, proc_reg is implemented \nas a server process using Erlang s generic server library (cf. Armstrong 2007, chap 16). This library \nprovides a robust way to build client\u00adserver systems, in which clients make synchronous calls to the \nserver by sending a call message, and awaiting a matching reply1. Each operation reg, where, unreg and \nsend is supported by a different call message. The operations are actually executed by the server, one \nat a time, and so no race conditions can arise. At least, this is the theory. In practice there is a \nsmall cost to the generic server approach: each request sends two messages and requires two context switches, \nand although these are cheap in Er\u00adlang, they are not free, and turn out to be a bottleneck in system \nstart-up times, for example. The prototype proc_reg attempts to optimize this, by moving the creation \nof the .rst forward ETS table entry into the clients. If this succeeds (because there is no previous \nentry with that name), then clients just make an asyn\u00adchronous call to the server (a so-called cast message, \nwith no re\u00adply) to inform it that it should complete the registration later. This avoids a context switch, \nand reduces two messages to one. If there is already a registered process with the same name, then the \nreg operation fails (with an exception) unless, of course, the process is dead. In this case, the process \nwill soon be removed from the reg\u00adistry by the server; clients ask the server to audit the dead process \nto hurry this along, then complete their registration as before. 1 Unique identi.ers are generated for \neach call, and returned in the reply, so that no message confusion can occur. This prototype was one \nof the .rst pieces of software to be tested using QuickCheck at Ericsson. At the time, in late 2005, \nit was believed to work, and indeed was accompanied by quite an extensive suite of unit tests including \ncases designed speci.cally to test for race conditions. We used QuickCheck to generate and run random \nsequences of API calls in two concurrent processes, and instrumented the proc_reg code with calls to \nyield() (which cedes control to the scheduler) to cause .ne-grain interleaving of concurrent operations. \nBy so doing, we could show that proc_reg was incorrect, since our tests failed. But the failing test \ncases we found were large, complex, and very hard to understand, and we were unable to use them to diagnose \nthe problem. As a result, this version of proc_reg was abandoned, and development of the production version \ncontinued without the optimization. While we were pleased that QuickCheck was able to reveal bugs in \nproc_reg, we were unsatis.ed that it could not help us to .nd them. Moreover, the QuickCheck property \nwe used to test it was hard-to-de.ne and ad hoc and not easily reusable to test any other software. This \npaper is the story of how we addressed these problems and returned to apply our new methods successfully \nto the example that defeated us before. 3. An Overview of Quviq QuickCheck QuickCheck (Claessen and Hughes \n2000) is a tool that tests univer\u00adsally quanti.ed properties, instead of single test cases. QuickCheck \ngenerates random test cases from each property, tests whether the property is true in that case, and \nreports cases for which the prop\u00aderty fails. Recent versions also shrink failing test cases automat\u00adically, \nby searching for similar, but smaller test cases that also fail. The result of shrinking is a minimal \n2 failing case, which often makes the root cause of the problem very easy to .nd. Quviq QuickCheck is \na commercial version that includes sup\u00adport for model-based testing using a state machine model (Hughes \n2007). This means that it has standard support for generating se\u00adquences of API calls using this state \nmachine model. It has been used to test a wide variety of industrial software, such as Ericsson s Media \nProxy (Arts et al. 2006) among others. State machine models are tested using an additional library, eqc_statem, \nwhich invokes call-backs supplied by the user to generate and test random, well\u00adformed sequences of calls \nto the software under test. We illustrate eqc_statem by giving fragments of a (sequential) speci.cation \nof proc_reg. Let us begin with an example of a generated test case (a se\u00adquence of API calls). [{set,{var,1},{call,proc_reg_eqc,spawn,[]}}, \n{set,{var,2},{call,proc_reg,where,[c]}}, {set,{var,3},{call,proc_reg_eqc,spawn,[]}}, {set,{var,4},{call,proc_reg_eqc,kill,[{var,1}]}}, \n{set,{var,5},{call,proc_reg,where,[d]}}, {set,{var,6},{call,proc_reg_eqc,reg,[a,{var,1}]}}, {set,{var,7},{call,proc_reg_eqc,spawn,[]}}] \neqc_statem test cases are lists of symbolic commands represented by Erlang terms, each of which binds \na symbolic variable (such as {var,1}) to the result of a function call, where {call,M,F,Args} represents \na call of function F in module M with arguments Args3. Note that previously bound variables can be used \nin later calls. Test cases for proc_reg in particular randomly spawn processes (to use as test data), \nkill them (to simulate crashes at random times), or pass them to proc_reg operations. Here proc_reg_eqc \nis the module containing the speci.cation of proc_reg, in which we de.ne local 2 In the sense that it \ncannot shrink to a failing test with the shrinking algorithm used. 3 In Erlang, variables start with \nan uppercase character, whereas atoms (constants) start with a lowercase character.  versions of reg \nand unreg which just call proc_reg and catch any exceptions. This allows us to write properties that \ntest whether an exception is raised correctly or not. (An uncaught exception in a test is interpreted \nas a failure of the entire test). We model the state of a test case as a list of processes spawned, processes \nkilled, and the {Name,Pid} pairs currently in the reg\u00adistry. We normally encapsulate the state in a record: \n-record(state,{pids=[],regs=[],killed=[]}). eqc_statem generates random calls using the call-back function \ncommand that we supply as part of the state machine model, with the test case state as its argument: \ncommand(S) -> oneof( [{call,?MODULE,spawn,[]}] ++ [{call,?MODULE,kill, [elements(S#state.pids)]} || S#state.pids/=[]] \n++ [{call,?MODULE,reg,[name(),elements(S#state.pids)]} || S#state.pids/=[]] ++ [{call,?MODULE,unreg,[name()]}] \n++ [{call,proc_reg,where,[name()]}]). name() -> elements([a,b,c,d]). The function oneof is a QuickCheck \ngenerator that randomly uses one element from a list of generators; in this case, the list of candidates \nto choose from depends on the test case state. ([X||P] is a degenerate list comprehension, that evaluates \nto the empty list if P is false, and [X] if P is true so reg and kill can be generated only if there \nare pids available to pass to them.) We decided not to include send in test cases, because its implementation \nis quite trivial. The macro ?MODULE expands to the name of the module that it appears in, proc_reg_eqc \nin this case. The next_state function speci.es how each call is supposed to change the state: next_state(S,V,{call,_,spawn,_}) \n-> S#state{pids=[V|S#state.pids]}; next_state(S,V,{call,_,kill,[Pid]}) -> S#state{killed=[Pid|S#state.killed], \nregs=[{Name,P} || {Name,P} <-S#state.regs, Pid /= P]}; next_state(S,_V,{call,_,reg,[Name,Pid]}) -> case \nregister_ok(S,Name,Pid) andalso not lists:member(Pid,S#state.killed) of true -> S#state{regs=[{Name,Pid}|S#state.regs]}; \nfalse -> S end; next_state(S,_V,{call,_,unreg,[Name]}) -> S#state{regs=lists:keydelete(Name,1,S#state.regs)}; \nnext_state(S,_V,{call,_,where,[_]}) -> S. register_ok(S,Name,Pid) -> not lists:keymember(Name,1,S#state.regs). \nNote that the new state can depend on the result of the call (the second argument V), as in the .rst \nclause above. Note also that killing a process removes it from the registry (in the model), and that \nregistering a dead process, or a name that is already registered (see register_ok), should not change \nthe registry state. We do allow the same pid to be registered with several names, however. When running \ntests, eqc_statem checks the postcondition of each call, speci.ed via another call-back that is given \nthe state be\u00adfore the call, and the actual result returned, as arguments. Since we catch exceptions in \neach call, which converts them into val\u00adues of the form { EXIT ,Reason}, our proc_reg postconditions \ncan test that exceptions are raised under precisely the right circum\u00adstances: postcondition(S,{call,_,reg,[Name,Pid]},Res) \n-> case Res of true -> register_ok(S,Name,Pid); { EXIT ,_} -> not register_ok(S,Name,Pid) end; postcondition(S,{call,_,unreg,[Name]},Res) \n-> case Res of true -> unregister_ok(S,Name); { EXIT ,_} -> not unregister_ok(S,Name) end; postcondition(S,{call,_,where,[Name]},Res) \n-> lists:member({Name,Res},S#state.regs); postcondition(_S,{call,_,_,_},_Res) -> true. unregister_ok(S,Name) \n-> lists:keymember(Name,1,S#state.regs). Note that reg(Name,Pid) and unreg(Name) are required to re\u00adturn \nexceptions if Name is already used/not used respectively, but that reg always returns true if Pid is \ndead, even though no reg\u00adistration is performed! This may perhaps seem a surprising design decision, \nbut it is consistent. As a comparison, the built-in process registry sometimes returns true and sometimes \nraises an excep\u00adtion when registering dead processes. This is due to the fact that a context switch is \nrequired to clean up. State machine models can also specify a precondition for each call, which restricts \ntest cases to those in which all preconditions hold. In this example, we could have used preconditions \nto exclude test cases that we expect to raise exceptions but we prefer to allow any test case, and check \nthat exceptions are raised correctly, so we de.ne all preconditions to be true. With these four call-backs, \nplus another call-back specifying the initial state, our speci.cation is almost complete. It only remains \nto de.ne the top-level property which generates and runs tests: prop_proc_reg() -> ?FORALL(Cmds,commands(?MODULE), \nbegin {ok,ETSTabs} = proc_reg_tabs:start_link(), {ok,Server} = proc_reg:start_link(), {H,S,Res} = run_commands(?MODULE,Cmds), \ncleanup(ETSTabs,Server), Res == ok end). Here ?FORALL binds Cmds to a random list of commands generated \nby commands, then we initialize the registry, run the commands, clean up, and check that the result of \nthe run (Res) was a success. Here commands and run_commands are provided by eqc_statem, and take the \ncurrent module name as an argument in order to .nd the right call-backs. The other components of run_commands \nresult, H and S, record information about the test run, and are of interest primarily when a test fails. \nThis is not the case here: sequential testing of proc_reg does not fail. 4. Parallel Testing with QuickCheck \n 4.1 A Parallel Correctness Criterion In order to test for race conditions, we need to generate test \ncases that are executed in parallel, and we also need a speci.cation of the correct parallel behavior. \nWe have chosen, in this paper, to use a speci.cation that just says that the API operations we are testing \nshould behave atomically. How can we tell from test results whether or not each operation behaved atomically \n? Following Lamport (1979) and Herlihy and Wing (1987), we consider a test to have passed if the observed \nresults are the same as some possible sequential execution of the operations in the test that is, a possible \ninterleaving of the parallel processes in the test.  Of course, testing for atomic behavior is just \na special case, and in general we may need to test other properties of concurrent code too but we believe \nthat this is a very important special case. Indeed, Herlihy and Wing claim that their notion of linearizability \nfocuses exclusively on a subset of concurrent computations that we believe to be the most interesting \nand useful ; we agree. In particular, atomicity is of great interest for the process registry. One great \nadvantage of this approach is that we can reuse the same speci.cation of the sequential behavior of an \nAPI, to test its behavior when invocations take place in parallel. We need only .nd the right linearization \nof the API calls in the test, and then use the sequential speci.cation to determine whether or not the \ntest has passed. We have implemented this idea in a new QuickCheck mod\u00adule, eqc_par_statem, which takes \nthe same state-machine speci.\u00adcations as eqc_statem, but tests the API in parallel instead. While state \nmachine speci.cations require some investment to produce in real situations, this means that we can test \nfor race conditions with no further investment in developing a parallel speci.cation. It also means that, \nas the code under test evolves, we can switch freely to\u00adand-fro between sequential testing to ensure \nthe basic behavior still works, and race condition testing using eqc_par_statem. The dif.culty with this \napproach is that, when we run a test, then there is no way to observe the sequence in which the API operations \ntake effect. (For example, a server is under no obligation to service requests in the order in which \nthey are made, so observing this or\u00adder would tell us nothing.) In general, the only way to tell whether \nthere is a possible sequentialization of a test case which can explain the observed test results, is \nto enumerate all possible sequentializa\u00adtions. This is prohibitively expensive unless care is taken when \ntest cases are generated.  4.2 Generating Parallel Test Cases Our .rst approach to parallel test case \ngeneration was to use the standard Quviq QuickCheck library eqc_statem to generate se\u00adquential test cases, \nthen execute all the calls in the test case in parallel, constrained only by the data dependencies between \nthem (which arise from symbolic variables, bound in one command, be\u00ading used in a later one). This generates \na great deal of parallelism, but sadly also an enormous number of possible serializations in the worst \ncase in which there are no data dependencies, a sequence of n commands generates n! possible serializations. \nIt is not prac\u00adtically feasible to implement a test oracle for parallel tests of this sort. Instead, \nwe decided to generate parallel test cases of a more re\u00adstricted form. They consist of an initial sequential \npre.x, to put the system under test into a random state, followed by exactly two sequences of calls which \nare performed in parallel. Thus the possible serializations consist of the initial pre.x, followed by \nan interleaving of the two parallel sequences. (Lu et al. (2008) gives clear evidence that it is possible \nto discover a large frac\u00adtion of the concurrency related bugs by using only two parallel threads/processes.) \nWe generate parallel test cases by parallelizing a suf.x of an eqc_statem test case, separating it into \ntwo lists of commands of roughly equal length, with no mutual data de\u00adpendencies, which are non-interfering \naccording to the sequential speci.cation. By non-interference, we mean that all command pre\u00adconditions \nare satis.ed in any interleaving of the two lists, which is necessary to prevent tests from failing because \na precondition was unsatis.ed not an interesting failure. We avoid parallelizing too long a suf.x (longer \nthan 16 commands), to keep the num\u00adber of possible interleavings feasible to enumerate (about 10,000 \nin the worst case). Finally, we run tests by .rst running the pre.x, then spawning two processes to run \nthe two command-lists in par\u00adallel, and collecting their results, which will be non-deterministic depending \non the actual parallel scheduling of events. We decide whether a test has passed, by attempting to construct \na sequentialization of the test case which explains the results ob\u00adserved. We begin with the sequential \npre.x of the test case, and use the next_state function of the eqc_statem model to com\u00adpute the test \ncase state after this pre.x is completed. Then we try to extend the sequential pre.x, one command at \na time, by choosing the .rst command from one of the parallel branches, and moving it into the pre.x. \nThis is allowed only if the postcondition speci\u00ad.ed in the eqc_statem model accepts the actual result \nreturned by the command when we ran the test. If so, we use the next_state function to compute the state \nafter this command, and continue. If the .rst commands of both branches ful.lled their postconditions, \nthen we cannot yet determine which command took effect .rst, and we must explore both possibilities further. \nIf we succeed in moving all commands from the parallel branches into the sequential pre\u00ad.x, such that \nall postconditions are satis.ed, then we have found a possible sequentialization of the test case explaining \nthe results we observed. If our search fails, then there is no such sequence, and the test failed. This \nis a greedy algorithm: as soon as a postcondition fails, then we can discard all potential sequentializations \nwith the fail\u00ading command as the next one in the sequence. This happens often enough to make the search \nreasonably fast in practice. As a further optimization, we memoize the search function on the remaining \nparallel branches and the test case state. This is useful, for exam\u00adple, when searching for a sequentialization \nof [A, B] and [C, D], if both [A, C] and [C, A] are possible pre.xes, and they lead to the same test \nstate for then we need only try to sequentialize [B] and [D] once. We memoize the non-interference test \nin a similar way, and these optimizations give an appreciable, though not dramatic, speed-up in our experiments \nof about 20%. With these optimiza\u00adtions, generating and running parallel tests is acceptably fast. 4.3 \nShrinking Parallel Test Cases When a test fails, QuickCheck attempts to shrink the failing test by searching \nfor a similar, but smaller test case which also fails. QuickCheck can often report minimal failing examples, \nwhich is a great help in fault diagnosis. eqc_statem already has built-in shrinking methods, of which \nthe most important tries to delete un\u00adnecessary commands from the test case, and eqc_par_statem in\u00adherits \nthese methods. But we also implement an additional shrink\u00ading method for parallel test cases: if it is \npossible to move a com\u00admand from one of the parallel suf.xes into the sequential pre.x, then we do so. \nThus the minimal test cases we .nd are minimally parallel we know that the parallel branches in the failing \ntests reported really do race, because everything that can be made se\u00adquential, is sequential. This also \nassists fault diagnosis.  4.4 Testing proc reg for Race Conditions To test the process registry using \neqc_par_statem, it is only nec\u00adessary to modify the property in Section 2 to use eqc_par_statem rather \nthan eqc_statem to generate and run test cases. prop_proc_reg_parallel() -> ?FORALL(Cmds,eqc_par_statem:commands(?MODULE), \nbegin {ok,ETSTabs} = proc_reg_tabs:start_link(), {ok,Server} = proc_reg:start_link(), {H,{A,B},Res} = \neqc_par_statem:run_commands(?MODULE,Cmds), cleanup(ETSTabs,Server), Res == ok end).  The type returned \nby run_commands is slightly different (A and B are lists of the calls made in each parallel branch, paired \nwith the results returned), but otherwise no change to the property is needed. When this property is \ntested on a single-core processor, all tests pass. However, as soon as it is tested on a dual-core, tests \nbegin to fail. Interestingly, just running on two cores gives us enough .ne\u00adgrain interleaving of concurrent \nprocesses to demonstrate the pres\u00adence of race conditions, something we had to achieve by instru\u00admenting \nthe code with calls to yield() to control the scheduler when we .rst tested this code in 2005. However, \njust as in 2005, the reported failing test cases are large, and do not shrink to small examples. This \nmakes the race condition very hard indeed to diag\u00adnose. The problem is that the test outcome is not determined \nsolely by the test case: depending on the actual interleaving of mem\u00adory operations on the dual core, \nthe same test may sometimes pass and sometimes fail. This is devastating for QuickCheck s shrinking, \nwhich works by repeatedly replacing the failed test case by a smaller one which still fails. If the smaller \ntest hap\u00adpens to succeed by sheer chance, as a result of non-deterministic execution then the shrinking \nprocess stops. This leads Quick-Check to report failed tests which are far from minimal. Our solution \nto this is almost embarrassing in its simplicity: instead of running each test only once, we run it many \ntimes, and consider a test case to have passed only if it passes every time we run it. We express this \nconcisely using a new form of QuickCheck property, ?ALWAYS(N,Prop), which passes if Prop passes N times \nin a row4. Now, provided the race condition we are looking for is reasonably likely to be provoked by \ntest cases in which it is present, then ?ALWAYS(10,...) is very likely to provoke it and so tests are \nunlikely to succeed by chance during the shrinking process. This dramatically improves the effectiveness \nof shrinking, even for quite small values of N. While we do not always obtain minimal failing tests with \nthis approach, we .nd we can usually obtain a minimal example by running QuickCheck a few times. When \ntesting the proc_reg property above, we .nd the follow\u00ading simple counterexample: {[{set,{var,5},{call,proc_reg_eqc,spawn,[]}}, \n{set,{var,9},{call,proc_reg_eqc,kill,[{var,5}]}}, {set,{var,15},{call,proc_reg_eqc,reg,[a,{var,5}]}}], \n{[{set,{var,19},{call,proc_reg_eqc,reg,[a,{var,5}]}}], [{set,{var,18},{call,proc_reg_eqc,reg,[a,{var,5}]}}]}} \nThis test case .rst creates and kills a process, then tries to regis\u00adter it (which should have no effect, \nbecause it is already dead), and .nally tries to register it again twice, in parallel. Printing the diag\u00adnostic \noutput from run_commands, we see: Sequential: [{{state,[],[],[]},<0.5576.2>}, {{state,[<0.5576.2>],[],[]},ok}, \n{{state,[<0.5576.2>],[],[<0.5576.2>]},true}] Parallel: {[{{call,proc_reg_eqc,reg,[a,<0.5576.2>]}, { EXIT \n,{badarg,[{proc_reg,reg,2},...]}}}], [{{call,proc_reg_eqc,reg,[a,<0.5576.2>]},true}]} Res: no_possible_interleaving \n(where the ellipses replace an uninteresting stack trace). The values displayed under Parallel: are the \nresults A and B from the two parallel branches they reveal that one of the parallel calls to reg raised \nan exception, even though trying to register a dead process should always just return true! How this \nhappened, though, is still quite mysterious but will be explained in the following sections. 4 In fact \nwe need only repeat tests during shrinking. 5. PULSE: A User-level Scheduler At this point, we have found \na simple test case that fails, but we do not know why it failed we need to debug it. A natural next step \nwould be to turn on Erlang s tracing features and rerun the test. But when the bug is caused by a race \ncondition, then turning on tracing is likely to change the timing properties of the code, and thus interfere \nwith the test failure! Even simply repeating the test may lead to a different result, because of the \nnon-determinism inherent in running on a multi-core. This is devastating for debugging. What we need \nis to be able to repeat the test as many times as we like, with deterministic results, and to observe \nwhat happens during the test, so that we can analyze how the race condition was provoked. With this in \nmind, we have implemented a new Erlang module that can control the execution of designated Erlang processes \nand records a trace of all relevant events. Our module can be thought of as a user-level scheduler, sitting \non top of the normal Erlang scheduler. Its aim is to take control over all sources of non-determinism \nin Erlang programs, and instead take those scheduling decisions randomly. This means that we can repeat \na test using exactly the same schedule by supplying the same random number seed: this makes tests repeatable. \nWe have named the module PULSE, short for ProTest User-Level Scheduler for Erlang. The Erlang virtual \nmachine (VM) runs processes for relatively long time-slices, in order to minimize the time spent on context \nswitching but as a result, it is very unlikely to provoke race con\u00additions in small tests. It is possible \nto tune the VM to perform more context switches, but even then the scheduling decisions are en\u00adtierly \ndeterministic. This is one reason why tricky concurrency bugs are rarely found during unit testing; it \nis not until later stages of a project, when many components are tested together, that the stan\u00addard \nscheduler begins to preempt processes and trigger race condi\u00adtions. In the worst case, bugs don t appear \nuntil the system is put under heavy load in production! In these later stages, such errors are expensive \nto debug. One other advantage (apart from repeata\u00adbility) of PULSE is that it generates much more .ne-grain \ninterleav\u00ading than the built-in scheduler in the Erlang virtual machine (VM), because it randomly chooses \nthe next process to run at each point. Therefore, we can provoke race conditions even in very small tests. \nErlang s scheduler is built into its virtual machine and we did not want to modify the virtual machine \nitself. Not only would this be dif.cult it is a low-level, fairly large and complex C program but we \nwould need to repeat the modi.cations every time a new version of the virtual machine was released. We \nde\u00adcided, therefore, to implement PULSE in Erlang, as a user-level scheduler, and to instrument the code \nof the processes that it con\u00adtrols so that they cooperate with it. As a consequence, PULSE can even be \nused in conjunction with legacy or customized versions of the Erlang VM (which are used in some products). \nThe user level scheduler also allows us to restrict our debugging effort to a few processes, whereas \nwe are guaranteed that the rest of the processes are executed normally. 5.1 Overall Design The central \nidea behind developing PULSE was to provide absolute control over the order of relevant events. The .rst \nnatural question that arises is: What are the relevant events? We de.ne a side-effect to be any interaction \nof a process with its environment. Of particu\u00adlar interest in Erlang is the way processes interact by \nmessage pass\u00ading, which is asynchronous. Message channels, containing mes\u00adsages that have been sent but \nnot yet delivered, are thus part of the environment and explicitly modelled as such in PULSE. It makes \nsense to separate side-effects into two kinds: outward side-effects, that in.uence only the environment \n(such as sending a message over a channel, which does not block and cannot fail, or printing a message), \nand inward side-effects, that allow the environment to in.uence the behavior of the process (such as \nreceiving a message from a channel, or asking for the system time).  We do not want to take control \nover purely functional code, or side-effecting code that only in.uences processes locally. PULSE takes \ncontrol over some basic features of the Erlang RTS (such as spawning processes, message sending, linking, \netc.), but it knows very little about standard library functions it would be too much work to deal with \neach of these separately! Therefore, the user of PULSE can specify which library functions should be \ndealt with as (inward) side-effecting functions, and PULSE has a generic way of dealing with these (see \nsubsection 5.3). A process is only under the control of PULSE if its code has been properly instrumented. \nAll other processes run as normal. In instrumentation, occurrences of side-effecting actions are replaced \nby indirections that communicate with PULSE instead. In particular, outward side-effects (such as sending \na message to another process) are replaced by simply sending a message to PULSE with the details of the \nside-effect, and inward side-effects (such as receiving a mes\u00adsage) are replaced by sending a request \nto PULSE for performing that side-effect, and subsequently waiting for permission. To ease the instrumentation \nprocess, we provide an automatic instrumenter, described in subsection 5.4.  5.2 Inner Workings The \nPULSE scheduler controls its processes by allowing only one of them to run at a time. It employs a cooperative \nscheduling method: At each decision point, PULSE randomly picks one of its waiting processes to proceed, \nand wakes it up. The process may now perform a number of outward side-effects, which are all recorded \nand taken care of by PULSE, until the process wants to perform an inward side-effect. At this point, \nthe process is put back into the set of waiting processes, and a new decision point is reached. The (multi-node) \nErlang semantics (Svensson and Fredlund 2007) provides only one guarantee for message delivery order: \nthat messages between a pair of processes arrive in the same order as they were sent. So as to adhere \nto this, PULSE s state also maintains a message queue between each pair of processes. When process P \nperforms an outward side-effect by sending a message M to the process Q, then M is added to the queue \n(P, Q). When PULSE wants to wake up a waiting process Q, it does so by randomly picking a non-empty queue \n(P I,Q) with Q as its destination, and delivering the .rst message in that queue to Q. Special care needs \nto be taken for the Erlang construct receive ... after n -> ... end, which allows a receiving process \nto only wait for an incoming message for n milliseconds before continuing, but the details of this are \nbeyond the scope of this paper. As an additional bene.t, this design allows PULSE to detect deadlocks \nwhen it sees that all processes are blocked, and there exist no message queues with the blocked processes \nas destination. As a clari.cation, the message queues maintained by PULSE for each pair of processes \nshould not be confused with the internal mailbox that each process in Erlang has. In our model, sending \na message M from P to Q goes in four steps: (1) P asynchronously sends off M , (2) M is on its way to \nQ, (3) M is delivered to Q s mailbox, (4) Q performs a receive statement and M is selected and removed \nfrom the mailbox. The only two events in this process that we consider side-effects are (1) P sending \nof M, and (3) delivering M to Q s mailbox. In what order a process decides to process the messages in \nits mailbox is not considered a side-effect, because no interaction with the environment takes place. \n 5.3 External Side-effects In addition to sending and receiving messages between themselves, the processes \nunder test can also interact with uninstrumented code. PULSE needs to be able to control the order in \nwhich those inter\u00adactions take place. Since we are not interested in controlling the order in which pure \nfunctions are called we allow the programmer to specify which external functions have side-effects. Each \ncall of a side-effecting function is then instrumented with code that yields before performing the real \ncall and PULSE is free to run another process at that point. Side-effecting functions are treated as \natomic which is also an important feature that aids in testing systems built of multiple components. \nOnce we establish that a component contains no race conditions we can remove the instrumentation from \nit and mark its operations as atomic side-effects. We will then be able to test other components that \nuse it and each operation marked as side\u00adeffecting will show up as a single event in a trace. Therefore, \nit is possible to test a component for race conditions independently of the components that it relies \non.  5.4 Instrumentation The program under test has to cooperate with PULSE, and the rel\u00adevant processes \nshould use PULSE s API to send and receive mes\u00adsages, spawn processes, etc., instead of Erlang s built-in \nfunction\u00adality. Manually altering an Erlang program so that it does this is te\u00addious and error-prone, \nso we developed an instrumenting compiler that does this automatically. The instrumenter is used in exactly \nthe same way as the normal compiler, which makes it easy to switch between PULSE and the normal Erlang \nscheduler. It s possible to instrument and load a module at runtime by typing in a single com\u00admand at \nthe Erlang shell. Let us show the instrumentation of the four most important constructs: sending a message, \nyielding, spawning a process, and receiving a message. 5.4.1 Sending If a process wants to send a message, \nthe instrumenter will redirect this as a request to the PULSE scheduler. Thus, Pid ! Msg is replaced \nby scheduler ! {send, Pid, Msg}, Msg The result value of sending a message is always the message that \nwas sent. Since we want the instrumented send to yield the same result value as the original one, we \nadd the second line. 5.4.2 Yielding A process yields when it wants to give up control to the scheduler. \nYields are also introduced just before each user-speci.ed side\u00adeffecting external function. After instrumentation, \na yielding process will instead give up control to PULSE. This is done by telling it that the process \nyields, and waiting for permission to continue. Thus, yield() is replaced by scheduler ! yield, receive \n{scheduler, go} -> ok end In other words, the process noti.es PULSE and then waits for the message go \nfrom the scheduler before it continues. All control messages sent by PULSE to the controlled processes \nare tagged with {scheduler, _} in order to avoid mixing them up with real messages.  5.4.3 Spawning \nA process P spawning a process Q is considered an outward side\u00adeffect for P , and thus P does not have \nto block. However, PULSE must be informed of the existence of the new process Q, and Q needs to be brought \nunder its control. The spawned process Q must therefore wait for PULSE to allow it to run. Thus, spawn(Fun) \nis replaced by  Pid = spawn(fun() -> receive {scheduler, go} -> Fun() end end), scheduler ! {spawned, \nPid}, Pid In other words, the process spawns an altered process that waits for the message go from the \nscheduler before it does anything. The scheduler is then informed of the existence of the spawned process, \nand we continue.  5.4.4 Receiving Receiving in Erlang works by pattern matching on the messages in the \nprocess mailbox. When a process is ready to receive a new message, it will have to ask PULSE for permission. \nHowever, it is possible that an appropriate message already exists in its mailbox, and receiving this \nmessage would not be a side-effect. Therefore, an instrumented process will .rst check if it is possible \nto receive a message with the desired pattern, and proceed if this is possible. If not, it will tell \nthe scheduler that it expects a new message in its mailbox, and blocks. When woken up again on the delivery \nof a new message, this whole process is repeated if necessary. We need a helper function that implements \nthis checking\u00adwaiting-checking loop. It is called receiving: receiving(Receiver) -> Receiver(fun() -> \nscheduler ! block, receive {scheduler, go} -> receiving(Receiver) end end). receiving gets a receiver \nfunction as an argument. A receiver function is a function that checks if a certain message is in its \nmailbox, and if not, executes its argument function. The function receiving turns this into a loop that \nonly terminates once PULSE has delivered the right message. When the receiver function fails, PULSE is \nnoti.ed by the block message, and the process waits for permission to try again. Code of the form receive \nPat -> Exp end is then replaced by receiving(fun (Failed) -> receive Pat -> Exp after 0 -> Failed() end \nend) In the above, we use the standard Erlang idiom (receive ... after 0 -> ... end) for checking if \na message of a certain type exists. It is easy to see how receive statements with more than one pattern \ncan be adapted to work with the above scheme.  5.5 Testing proc reg with PULSE To test the proc reg \nmodule using both QuickCheck and PULSE, we need to make a few modi.cations to the QuickCheck property \nin Section 4.4. prop_proc_reg_scheduled() -> ?FORALL(Cmds,eqc_par_statem:commands(?MODULE), ?ALWAYS(10,?FORALL(Seed,seed(), \nbegin SRes = scheduler:start([{seed,Seed}], fun() -> {ok,ETSTabs} = proc_reg_tabs:start_link(), {ok,Server} \n= proc_reg:start_link(), eqc_par_statem:run_commands(?MODULE,Cmds), cleanup(ETSTabs,Server), end), {H,AB,Res} \n= scheduler:get_result(SRes), Res == ok end))). PULSE uses a random seed, generated by seed(). It also \ntakes a function as an argument, so we create a lambda-function which ini\u00adtializes and runs the tests. \nThe result of running the scheduler is a list of things, thus we need to call scheduler:get result to \nretrieve the actual result from run commands. We should also re\u00admember to instrument rather than compile \nall the involved modules. Note that we still use ?ALWAYS in order to run the same test data with different \nrandom seeds, which helps the shrinking process in .nding smaller failing test cases that would otherwise \nbe less likely to fail. When testing this modi.ed property, we .nd the following counterexample, which \nis in fact simpler than the one we found in Section 4.4: {[{set,{var,9},{call,proc_reg_eqc,spawn,[]}}, \n{set,{var,10},{call,proc_reg_eqc,kill,[{var,9}]}}], {[{set,{var,15},{call,proc_reg_eqc,reg,[c,{var,9}]}}], \n[{set,{var,12},{call,proc_reg_eqc,reg,[c,{var,9}]}}]}} When prompted, PULSE provides quite a lot of information \nabout the test case run and the scheduling decisions taken. Below we show an example of such information. \nHowever, it is still not easy to explain the counterexample; in the next section we present a method \nthat makes it easier to understand the scheduler output. -> < start_link.Pid1 > calls scheduler:process_flag \n[priority,high] returning normal. -> < start_link.Pid1 > sends {call,{attach,<0.31626.0>}, <0.31626.0>,#Ref<0.0.0.13087>} \nto < start_link.Pid >. -> < start_link.Pid1 > blocks. *** unblocking < start_link.Pid > by delivering \n{call,{attach,<0.31626.0>}, <0.31626.0>, #Ref<0.0.0.13087>} sent by < start_link.Pid1 >. ... 6. Visualizing \nTraces PULSE records a complete trace of the interesting events during test execution, but these traces \nare long, and tedious to understand. To help us interpret them, we have, utilizing the popular GraphViz \npackage (Gansner and North 1999), built a trace visualizer that draws the trace as a graph. For example, \nFigure 1 shows the graph drawn for one possible trace of the following program: procA() -> PidB = spawn(fun \nprocB/0), PidB ! a, process_flag(trap_exit, true), link(PidB), receive { EXIT ,_,Why} -> Why end. procB() \n-> receive a -> exit(kill) end.  Figure 1. A simple trace visualization. The function procA starts \nby spawning a process, and subsequently sends it a message a. Later, procA links to the process it spawned, \nwhich means that it will get noti.ed when that process dies. The default behavior of a process when such \na noti.cation happens is to also die (in this way, one can build hierarchies of processes). Setting the \nprocess .ag trap exit to true changes this behaviour, and the noti.cation is delivered as a regular message \nof the form {EXIT,_,_} instead. In the .gure, each process is drawn as a sequence of state tran\u00adsitions, \nfrom a start state drawn as a triangle, to a .nal state drawn as an inverted triangle, all enclosed in \na box and assigned a unique color. (Since the printed version of the diagrams may lack these colors, \nwe reference diagram details by location and not by color. However, the diagrams are even more clear \nin color.) The diagram shows the two processes, procA (called root) which is shown to the left (in red), \nand procB (called procA.PidB, a name automat\u00adically derived by PULSE from the point at which it was spawned) \nshown to the right (in blue). Message delivery is shown by gray ar\u00adrows, as is the return of a result \nby the root process. As explained in the previous section, processes make transitions when receiving \na message5, or when calling a function that the instrumenter knows has a side-effect. From the .gure, \nwe can see that the root process spawned PidB and sent the message a to it, but before the message was \ndelivered then the root process managed to set its trap_exit process .ag, and linked to PidB. PidB then \nreceived its message, and killed itself, terminating with reason kill. A message was sent back to root, \nwhich then returned the exit reason as its result. Figure 2 shows an alternative trace, in which PidB \ndies before root creates a link to it, which generates an exit message with a different exit reason. \nThe existence of these two different traces indicates a race condition when using spawn and link separately \n(which is the reason for the existence of an atomic spawn_link function in Erlang). The diagrams help \nus to understand traces by gathering together all the events that affect one process into one box; in \nthe original traces, these events may be scattered throughout the entire trace. But notice that the diagrams \nalso abstract away from irrelevant information speci.cally, the order in which messages are deliv\u00ad 5 \nIf messages are consumed from a process mailbox out-of-order, then we show the delivery of a message \nto the mailbox, and its later consumption, as separate transitions. Figure 2. An alternative possible \nexecution. Figure 3. A race between two side-effects. ered to different processes, which is insigni.cant \nin Erlang. This abstraction is one strong reason why the diagrams are easier to un\u00adderstand than the \ntraces they are generated from. However, we do need to know the order in which calls to functions with \nside-effects occur, even if they are made in different processes. To make this order visible, we add \ndotted black arrows to our diagrams, from one side-effecting call to the next. Figure 3 illustrates one \npossible execution of this program, in which two processes race to write to the same .le: write_race() \n-> Pid = spawn(fun() -> file:write_file(\"a.txt\",\"a\") end), file:write_file(\"a.txt\",\"b\"). In this diagram, \nwe can see that the write_file in the root pro\u00adcess preceded the one in the spawned process write_race.Pid. \nIf we draw these arrows between every side-effect and its suc\u00adcessor, then our diagrams rapidly become \nvery cluttered. However,  it is only necessary to indicate the sequencing of side-effects ex\u00adplicitly \nif their sequence is not already determined. For each pair of successive side-effect transitions, we \nthus compute Lamport s happens before relation (Lamport 1978) between them, and if this already implies \nthat the .rst precedes the second, then we draw no arrow in the diagram. Interestingly, in our examples \nthen this eliminates the majority of such arrows, and those that remain tend to surround possible race \nconditions where the message pass\u00ading (synchronization) does not enforce a particular order of side\u00adeffects. \nThus black dotted arrows are often a sign of trouble. 6.1 Analyzing the proc reg race conditions Interestingly, \nas we saw in Section 5.5, when we instrumented proc_reg and tested it using PULSE and QuickCheck, we \nobtained a different even simpler minimal failing test case, than the one we had previously discovered \nusing QuickCheck with the built-in Erlang scheduler. Since we need to use PULSE in order to obtain a \ntrace to analyze, then we must .x this bug .rst, and see whether that also .xes the .rst problem we discovered. \nThe failing test we .nd using PULSE is this one: {[{set,{var,9},{call,proc_reg_eqc,spawn,[]}}, {set,{var,10},{call,proc_reg_eqc,kill,[{var,9}]}}], \n{[{set,{var,15},{call,proc_reg_eqc,reg,[c,{var,9}]}}], [{set,{var,12},{call,proc_reg_eqc,reg,[c,{var,9}]}}]}} \nIn this test case, we simply create a dead process (by spawning a process and then immediately killing \nit), and try to register it twice in parallel, and as it happens the .rst call to reg raises an exception. \nThe diagram we generate is too large to include in full, but in Figure 4 we reproduce the part showing \nthe problem. In this diagram fragment, the processes are, from left to right, the proc_reg server, the \nsecond parallel fork (BPid), and the .rst parallel fork (APid). We can see that BPid .rst inserted its \nargu\u00adment into the ETS table, recording that the name c is now taken, then sent an asynchronous message \nto the server ({cast,{..}}) to inform it of the new entry. Thereafter APid tried to insert an ETS entry \nwith the same name but failed. After discovering that the process being registered is actually dead, \nAPid sent a message to the server asking it to audit its entry ({call,{..},_,_}) that is, clean up the \ntable by deleting the entry for a dead process. But this message was delivered before the message from \nBPid! As a result, the server could not .nd the dead process in its table, and failed to delete the entry \ncreated by BPid, leading APid s second attempt to create an ETS entry to fail also which is not expected \nto happen. When BPid s message is .nally received and processed by the server, it is already too late. \nThe problem arises because, while the clients create forward ETS entries linking the registered name \nto a pid, it is the server which creates a reverse entry linking the pid to its monitoring reference \n(created by the server). It is this reverse entry that is used by the server when asked to remove a dead \nprocess from its tables. We corrected the bug by letting clients (atomically) insert two ETS entries \ninto the same table: the usual forward entry, and a dummy reverse entry (lacking a monitoring reference) \nthat is later overwritten by the server. This dummy reverse entry enables the server to .nd and delete \nboth entries in the test case above, thus solving the problem. In fact, the current Erlang virtual machine \nhappens to deliver messages to local mailboxes instantaneously, which means that one message cannot actually \novertake another message sent earlier the cause of the problem in this case. This is why this minimal \nfailing test was not discovered when we ran tests on a multi-core, using the built-in scheduler. However, \nthis behavior is not guar\u00adanteed by the language de.nition, and indeed, messages between nodes in a distributed \nsystem can overtake each other in this way. It is expected that future versions of the virtual machine \nmay allow message overtaking even on a single many-core processor; thus we consider it an advantage that \nour scheduler allows this behavior, and can provoke race conditions that it causes. It should be noted \nthat exactly the same scenario can be trig\u00adgered in an alternative way (without parallel processes and \nmulti\u00adcore!); namely if the BPid above is preempted between its call to ets:insert new and sending the \ncast-message. However, the likelihood for this is almost negligible, since the Erlang sched\u00aduler prefers \nrunning processes for relatively long time-slices. Us\u00ading PULSE does not help triggering the scenario \nin this way either. PULSE is not in control at any point between ets:insert new and sending the cast-message, \nmeaning that only the Erlang scheduler controls the execution. Therefore, the only feasible way to repeat\u00adedly \ntrigger this faulty scenario is by delaying the cast-message by using PULSE (or a similar tool).  6.2 \nA second race condition in proc reg Having corrected the bug in proc reg we repeated the QuickCheck test. \nThe property still fails, with the same minimal failing case that we .rst discovered (which is not so \nsurprising since the problem that we .xed in the previous section cannot actually occur with today s \nVM). However, we were now able to reproduce the failure with PULSE, as well as the built-in scheduler. \nAs a result, we could now analyze and debug the race condition. The failing case is: {[{set,{var,4},{call,proc_reg_eqc,spawn,[]}}, \n{set,{var,7},{call,proc_reg_eqc,kill,[{var,4}]}}, {set,{var,12},{call,proc_reg_eqc,reg,[b,{var,4}]}}], \n{[{set,{var,18},{call,proc_reg_eqc,reg,[b,{var,4}]}}], [{set,{var,21},{call,proc_reg_eqc,reg,[b,{var,4}]}}]}} \nIn this test case we also create a dead process, but we try to register it once in the sequential pre.x, \nbefore trying to register it twice in parallel. Once again, one of the calls to reg in the parallel branches \nraised an exception. Turning again to the generated diagram, which is not included in the paper for space \nreasons, we observed that both parallel branches (APid and BPid) fail to insert b into the ETS table. \nThey fail since the name b was already registered in the sequential part of the test case, and the server \nhas not yet processed the DOWN message generated by the monitor. Both processes then call where(b) to \nsee if b is really registered, which returns undefined since the process is dead. Both APid and BPid \nthen request an audit by the server, to clean out the dead process. After the audit, both processes assume \nthat it is now ok to register b, there is a race condition between the two processes, and one of the \nregistrations fails. Since this is not expected, an exception is raised. (Note that if b were alive then \nthis would be a perfectly valid race condition, where one of the two processes successfully registers \nthe name and the other fails, but the speci.cation says that the registration should always return true \nfor dead processes). This far into our analysis of the error it became clear that it is an altogether \nrather unwise idea ever to insert a dead process into the process registry. To .x the error we added \na simple check (is_process_alive(Pid)) before inserting into the registry. The effect of this change \non the performance turned out to be negligible, because is_process_alive is very ef.cient for local processes. \nAfter this change the module passed 20 000 tests, and we were satis.ed. 7. Discussion and Related Work \nActually, the .x just described does not really remove all pos\u00adsible race conditions. Since the diagrams \nmade us understand the algorithm much better, we can spot another possible race condition: If APid and \nBPid try to register the same pid at the same time, and that process dies just after APid and BPid have \nchecked that it is alive, then the same problem we have just .xed, will arise. The rea\u00ad  Figure 4. \nA problem caused by message overtaking. son that our tests succeeded even so, is that a test must contain \nthree K parallel branches to provoke the race condition in its new form two processes making simultaneous \nattempts to register, and a third process to kill the pid concerned at the right moment. Because our \nparallel test cases only run two concurrent branches, then they can never provoke this behavior. The \nbest way to .x the last race condition problem in proc_reg would seem to be to simplify its API, by restricting \nreg so that a process may only register itself. This, at a stroke, eliminates the risk of two processes \ntrying to register the same process at the same time, and guarantees that we can never try to register \na dead process. This simpli.cation was actually made in the production version of the code. Parallelism \nin test cases We could, of course, generate test cases with three, four, or even more concurrent branches, \nto test for this kind of race condition too. The problem is, as we explained in section 4.2, that the \nnumber of possible interleavings grows extremely fast with the number of parallel branches. The number \nof interleavings of K sequences of length N are as presented in Figure 5. The practical consequence is \nthat, if we allow more parallel branches in test cases, then we must restrict the length of each branch \ncorrespondingly. The bold entries in the table show the last feasible entry in each column with three \nparallel branches, we would need to restrict each branch to just three commands; with 2 3 4 5 1 2 6 24 \n120 2 6 90 2520 113400 3 20 1680 369600 108 3 \u00d7 1011 N 4 70 34650 6 \u00d7 107 1010 6 \u00d7 1014 5 252 756756 \n... ... 1010 1017 8 \u00d7 1024 8 12870  Figure 5. Possible interleavings of parallel branches four branches, \nwe could only allow two; with .ve or more branches, we could allow only one command per branch. This \nis in itself a restriction that will make some race conditions impossible to detect. Moreover, with more \nparallel branches, there will be even more possible schedules for PULSE to explore, so race conditions \ndepending on a precise schedule will be correspondingly harder to .nd. There is thus an engineering trade-off \nto be made here: allowing greater parallelism in test cases may in theory allow more race con\u00additions \nto be discovered, but in practice may reduce the probability of .nding a bug with each test, while at \nthe same time increasing the cost of each test. We decided to prioritize longer sequences over more parallelism \nin the test case, and so we chose K =2. How\u00adever, in the future we plan to experiment with letting QuickCheck \nrandomly choose K and N from the set of feasible combinations. To be clear, note that K only refers to \nthe parallelism in the test case, that is, the number of processes that make calls to the API. The system \nunder test may have hundreds of processes running, many of them controlled by PULSE, independently of \nK.  The problem of detecting race conditions is well studied and can be divided in runtime detection, \nalso referred to as dynamic detection, and analyzing the source code, so called static detec\u00adtion. Most \nresults refer to race conditions in which two threads or processes write to shared memory (data race \ncondition), which in Erlang cannot happen. For us, a race condition appears if there are two schedules \nof occurring side effects (sending a message, writing to a .le, trapping exits, linking to a process, \netc) such that in one schedule our model of the system is violated and in the other sched\u00adule it is not. \nOf course, writing to a shared ETS table and writing in shared memory is related, but in our example \nit is allowed that two processes call ETS insert in parallel. By the atomicity of in\u00adsert, one will succeed, \nthe other will fail. Thus, there is a valid race condition that we do not want to detect, since it does \nnot lead to a failure. Even in this slightly different setting, known results on race conditions still \nindicate that we are dealing with a hard problem. For example, Netzer and Miller (1990) show for a number \nof rela\u00adtions on traces of events that ordering these events on could have been a valid execution is \nan NP-hard problem (for a shared mem\u00adory model). Klein et al. (2003) show that statically detecting race \nconditions is NP-complete if more than one semaphore is used. Thus, restricting eqc par statem to execute \nonly two pro\u00adcesses in parallel is a pragmatic choice. Three processes may be feasible, but real scalability \nis not in sight. This pragmatic choice is also supported by recent studies (Lu et al. 2008), where it \nis concluded that: Almost all (96%) of the examined concurrency bugs are guaranteed to manifest if certain \npartial order between 2 threads is enforced. Hierarchical approach Note that our tools support a hierarchical \napproach to testing larger systems. We test proc_reg under the assumption that the underly\u00ading ets operations \nare atomic; PULSE does not attempt to (indeed, cannot) interleave the executions of single ETS operations, \nwhich are implemented by C code in the virtual machine. Once we have established that the proc_reg operations \nbehave atomically, then we can make the same assumption about them when testing code that makes use of \nthem. When testing for race conditions in mod\u00adules that use proc_reg, then we need not, and do not want \nto, test for race conditions in proc_reg itself. As a result, the PULSE schedules remain short, and the \nsimple random scheduling that we use suf.ces to .nd schedules that cause failures. Model Checking One \ncould argue that the optimal solution to .nding race conditions problem would be to use a model checker \nto explore all possible interleavings. The usual objections are nevertheless valid, and the rapidly growing \nstate space for concurrent systems makes model checking totally infeasible, even with a model checker \noptimized for Erlang programs, such as McErlang (Fredlund and Svensson 2007). Further it is not obvious \nwhat would be the property to model check, since the atomicity violations that we search for can not \nbe directly translated into an LTL model checking property. Input non-determinism PULSE provides deterministic \nscheduling. However, in order for tests to be repeatable we also need the external functions to behave \nconsistently across repeated runs. While marking them as side\u00adeffects will ensure that they are only \ncalled serially, PULSE does nothing to guarantee that functions called in the same sequence will return \nthe same values in different runs. The user still has to make sure that the state of the system is reset \nproperly before each run. Note that the same arguments apply to QuickCheck testing; it is crucial for \nshrinking and re-testing that input is deterministic and thus it works well to combine QuickCheck and \nPULSE. False positives In contrast to many race .nding methods, that try to spot common patterns leading \nto concurrency bugs, our approach does not pro\u00adduce false positives and not even does it show races that \nresult in correct execution of the program. This is because we employ property-based testing and classify \ntest cases based on whether the results satisfy correctness properties and report a bug only when a property \nis violated. Related tools Park and Sen (2008) study atomicity in Java. Their approach is sim\u00adilar to \nours in that they use a random scheduler both for repeatabil\u00adity and increased probability of .nding \natomicity violations. How\u00adever, since Java communication is done with shared objects and locks, the analysis \nis rather different. It is quite surprising that our simple randomized scheduler and even just running \ntests on a multi-core coupled with repetition of tests to reduce non-determinism, should work so well \nfor us. After all, this can only work if the probability of provoking the race condition in each test \nthat contains one is reasonably high. In contrast, race conditions are often regarded as very hard to \nprovoke because they occur so rarely. For example, Sen used very carefully constructed schedules to provoke \nrace conditions in Java programs (Sen 2008) so how can we reasonably expect to .nd them just by running \nthe same test a few times on a multi-core? We believe two factors make our simple approach to race detec\u00adtion \nfeasible. Firstly, Erlang is not Java. While there is shared data in Erlang programs, there is much \nless of it than in a concurrent Java program. Thus there are many fewer potential race conditions, and \na simpler approach suf.ces to .nd them.  Secondly, we are searching for race conditions during unit \ntest\u00ading, where each test runs for a short time using only a relatively small amount of code. During \nsuch short runs, there is a fair chance of provoking race conditions with any schedule. Finding race \nconditions during whole-program testing is a much harder problem.  Chess, developed by Musuvathi et \nal. (2008), is a system that shares many similarities with PULSE. Its main component is a scheduler capable \nof running the program deterministically and re\u00adplaying schedules. The key difference between Chess and \nPULSE is that the former attempts to do an exhaustive search and enumer\u00adate all the possible schedules \ninstead of randomly probing them. Several interesting techniques are employed, including prioritizing \nschedules that are more likely to trigger bugs, making sure that only fair schedules are enumerated and \navoiding exercising schedules that differ insigni.cantly from already visited ones. Visualization Visualization \nis a common technique used to aid understanding software. Information is extracted statically from source \ncode or dynamically from execution and displayed in graphical form. Of many software visualization tools \na number are related to our work. Topol et al. (1995) developed a tool that visualizes executions of \nparallel programs and shows, among other things, a trace of messages sent between processes indicating \nthe happened-before relationships. Work of Jerding et al. (1997) is able to show dynamic call-graphs \nof object-oriented programs and interaction patterns between their components. Arts and Fredlund (2002) \ndescribe a tool that visualizes traces of Erlang programs in form of abstract state transition diagrams. \nArtho et al. (2007) develop a notation that extends UML diagrams to also show traces of concurrent executions \nof threads, Maoz et al. (2007) create event sequence charts that can express which events must happen \nin all possible scenarios.  8. Conclusions Concurrent code is hard to debug and therefore hard to get \ncorrect. In this paper we present an extension to QuickCheck, a user level scheduler for Erlang (PULSE), \nand a tool for visualizing concurrent executions that together help in debugging concurrent programs. \nThe tools allow us to .nd concurrency errors on a module test\u00ading level, whereas industrial experience \nis that most of them slip through to system level testing, because the standard scheduler is deterministic, \nbut behaves differently in different timing contexts. We contributed eqc par statem, an extension of \nthe state ma\u00adchine library for QuickCheck that enables parallel execution of a sequence of commands. \nWe generate a sequential pre.x to bring the system into a certain state and continue with parallel execution \nof a suf.x of independent commands. As a result we can provoke concurrency errors and at the same time \nget good shrinking behav\u00adior from the test cases. We contributed with PULSE, a user level scheduler that \nenables scheduling of any concurrent Erlang program in such a way that an execution can be repeated deterministically. \nBy randomly choosing different schedules, we are able to explore more execution paths than without such \na scheduler. In combination with QuickCheck we get in addition an even better shrinking behavior, because \nof the repeatability of test cases. We contributed with a graph visualization method and tool that enabled \nus to analyze concurrency faults more easily than when we had to stare at the produced traces. The visualization \ntool depends on the output produced by PULSE, but the use of computing the happens before relation to \nsimplify the graph is a general principle. We evaluated the tools on a real industrial case study and \nwe detected two race conditions. The .rst one by only using eqc par statem; the fault had been noticed \nbefore, but now we did not need to instrument the code under test with yield() com\u00admands. The .rst and \nsecond race condition could easily be pro\u00advoked by using PULSE. The traces recorded by PULSE were visual\u00adized \nand helped us in clearly identifying the sources of the two race conditions. By analyzing the graphs \nwe could even identify a third possible race condition, which we could provoke if we allowed three instead \nof two parallel processes in eqc par statem. Our contributions help Erlang software developers to get \ntheir concurrent code right and enables them to ship technologically more advanced solutions. Products \nthat otherwise might have re\u00admained a prototype, because they were neither fully understood nor tested \nenough, can now make it into production. The tool PULSE and the visualization tool are available under \nthe Simpli.ed BSD License and have a commercially supported version as part of Quviq QuickCheck. Acknowledgments \nThis research was sponsored by EU FP7 Collaborative project ProTest, grant number 215868. References \nJoe Armstrong. Programming Erlang: Software for a Concurrent World. Pragmatic Bookshelf, July 2007. Cyrille \nArtho, Klaus Havelund, and Shinichi Honiden. Visualization of concurrent program executions. In COMPSAC \n07: Proc. of the 31st Annual International Computer Software and Applications Conference, pages 541 546, \nWashington, DC, USA, 2007. IEEE Computer Society. Thomas Arts and Lars-\u00b0Ake Fredlund. Trace analysis \nof Erlang programs. SIGPLAN Notices, 37(12):18 24, 2002. Thomas Arts, John Hughes, Joakim Johansson, \nand Ulf Wiger. Testing Telecoms Software with Quviq QuickCheck. In ERLANG 06: Proc. of the 2006 ACM SIGPLAN \nworkshop on Erlang. ACM, 2006. Koen Claessen and John Hughes. QuickCheck: a lightweight tool for random \ntesting of Haskell programs. In ICFP 00: Proc. of the .fth ACM SIGPLAN international conference on Functional \nprogramming, pages 268 279, New York, NY, USA, 2000. ACM. Mats Cronqvist. Troubleshooting a large Erlang \nsystem. In ERLANG 04: Proc. of the 2004 ACM SIGPLAN workshop on Erlang, pages 11 15, New York, NY, USA, \n2004. ACM. Lars-\u00b0McErlang: a model checker for Ake Fredlund and Hans Svensson. a distributed functional \nprogramming language. SIGPLAN Not., 42(9): 125 136, 2007. Emden R. Gansner and Stephen C. North. An open \ngraph visualization system and its applications. Software -Practice and Experience, 30: 1203 1233, 1999. \nM. P. Herlihy and J. M. Wing. Axioms for concurrent objects. In POPL 87: Proc. of the 14th ACM SIGACT-SIGPLAN \nsymposium on Principles of Prog. Lang., pages 13 26, New York, NY, USA, 1987. ACM. John Hughes. QuickCheck \nTesting for Fun and Pro.t. In 9th Int. Symp. on Practical Aspects of Declarative Languages. Springer, \n2007. Dean F. Jerding, John T. Stasko, and Thomas Ball. Visualizing interactions in program executions. \nIn In Proc. of the 19th International Conference on Software Engineering, pages 360 370, 1997. Klein, \nLu, and Netzer. Detecting race conditions in parallel programs that use semaphores. Algorithmica, 35:321 \n345, 2003. Leslie Lamport. How to make a multiprocessor computer that correctly executes multiprocess \nprograms. IEEE Transactions on Computers, 28 (9):690 691, 1979. Leslie Lamport. Time, clocks, and the \nordering of events in a distributed system. Commun. ACM, 21(7):558 565, 1978. Shan Lu, Soyeon Park, Eunsoo \nSeo, and Yuanyuan Zhou. Learning from mistakes: a comprehensive study on real world concurrency bug charac\u00adteristics. \nSIGARCH Comput. Archit. News, 36(1):329 339, 2008. Shahar Maoz, Asaf Kleinbort, and David Harel. Towards \ntrace visualization and exploration for reactive systems. In VLHCC 07: Proc. of the IEEE Symposium on \nVisual Languages and Human-Centric Computing, pages 153 156, Washington, DC, USA, 2007. IEEE Computer \nSociety. Madanlal Musuvathi, Shaz Qadeer, Thomas Ball, G\u00b4erard Basler, Pira\u00admanayagam Arumuga Nainar, \nand Iulian Neamtiu. Finding and repro\u00adducing heisenbugs in concurrent programs. In OSDI, pages 267 280, \n2008. Robert H. B. Netzer and Barton P. Miller. On the complexity of event ordering for shared-memory \nparallel program executions. In In Proc. of the 1990 Int. Conf. on Parallel Processing, pages 93 97, \n1990. Chang-Seo Park and Koushik Sen. Randomized active atomicity violation detection in concurrent programs. \nIn SIGSOFT 08/FSE-16: Proc. of the 16th ACM SIGSOFT International Symposium on Foundations of soft\u00adware \nengineering, pages 135 145, New York, NY, USA, 2008. ACM. Koushik Sen. Race directed random testing of \nconcurrent programs. SIG-PLAN Not., 43(6):11 21, 2008. H. Svensson and L.-\u00b0 A. Fredlund. A more accurate \nsemantics for distributed Erlang. In Erlang 07: Proc. of the 2007 SIGPLAN Erlang Workshop, pages 43 54, \nNew York, NY, USA, 2007. ACM. B. Topol, J.T. Stasko, and V. Sunderam. Integrating visualization support \ninto distributed computing systems. Proc. of the 15th Int. Conf. on: Distributed Computing Systems, pages \n19 26, May-Jun 1995. Ulf T. Wiger. Extended process registry for Erlang. In ERLANG 07: Proc. of the 2007 \nSIGPLAN workshop on ERLANG Workshop, pages 1 10, New York, NY, USA, 2007. ACM.   \n\t\t\t", "proc_id": "1596550", "abstract": "<p>We address the problem of testing and debugging concurrent, distributed Erlang applications. In concurrent programs, race conditions are a common class of bugs and are very hard to find in practice. Traditional unit testing is normally unable to help finding all race conditions, because their occurrence depends so much on timing. Therefore, race conditions are often found during system testing, where due to the vast amount of code under test, it is often hard to diagnose the error resulting from race conditions. We present three tools (QuickCheck, PULSE, and a visualizer) that in combination can be used to test and debug concurrent programs in unit testing with a much better possibility of detecting race conditions. We evaluate our method on an industrial concurrent case study and illustrate how we find and analyze the race conditions.</p>", "authors": [{"name": "Koen Claessen", "author_profile_id": "81100206977", "affiliation": "Chalmers University of Technology, Gothenburg, Sweden", "person_id": "P1614010", "email_address": "", "orcid_id": ""}, {"name": "Michal Palka", "author_profile_id": "81442614205", "affiliation": "Chalmers University of Technology, Gothenburg, Sweden", "person_id": "P1614011", "email_address": "", "orcid_id": ""}, {"name": "Nicholas Smallbone", "author_profile_id": "81442618913", "affiliation": "Chalmers University of Technonlogy, Gothenburg, Sweden", "person_id": "P1614012", "email_address": "", "orcid_id": ""}, {"name": "John Hughes", "author_profile_id": "81100166325", "affiliation": "Quviq AB, Gothenburg, Sweden", "person_id": "P1614013", "email_address": "", "orcid_id": ""}, {"name": "Hans Svensson", "author_profile_id": "81337494107", "affiliation": "Chalmers University of Technology and Quviq AB, Gothenburg, Sweden", "person_id": "P1614014", "email_address": "", "orcid_id": ""}, {"name": "Thomas Arts", "author_profile_id": "81100631897", "affiliation": "Chalmers University of Technology and Quviq AB, Gothenburg, Sweden", "person_id": "P1614015", "email_address": "", "orcid_id": ""}, {"name": "Ulf Wiger", "author_profile_id": "81100019436", "affiliation": "Erlang Training and Consulting, London, Gt Britain", "person_id": "P1614016", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1596550.1596574", "year": "2009", "article_id": "1596574", "conference": "ICFP", "title": "Finding race conditions in Erlang with QuickCheck and PULSE", "url": "http://dl.acm.org/citation.cfm?id=1596574"}