{"article_publication_date": "08-31-2009", "fulltext": "\n Causal Commutative Arrows and Their Optimization Hai Liu Eric Cheng Paul Hudak Department of Computer \nScience Yale University {hai.liu,eric.cheng,paul.hudak}@yale.edu Abstract Arrows are a popular form \nof abstract computation. Being more general than monads, they are more broadly applicable, and in par\u00adticular \nare a good abstraction for signal processing and data.ow computations. Most notably, arrows form the \nbasis for a domain speci.c language called Yampa, which has been used in a variety of concrete applications, \nincluding animation, robotics, sound syn\u00adthesis, control systems, and graphical user interfaces. Our \nprimary interest is in better understanding the class of ab\u00adstract computations captured by Yampa. Unfortunately, \narrows are not concrete enough to do this with precision. To remedy this situa\u00adtion we introduce the \nconcept of commutative arrows that capture a kind of non-interference property of concurrent computations. \nWe also add an init operator, and identify a crucial law that captures the causal nature of arrow effects. \nWe call the resulting computational model causal commutative arrows. To study this class of computations \nin more detail, we de.ne an extension to the simply typed lambda calculus called causal commutative arrows \n(CCA), and study its properties. Our key con\u00adtribution is the identi.cation of a normal form for CCA \ncalled causal commutative normal form (CCNF). By de.ning a normal\u00adization procedure we have developed \nan optimization strategy that yields dramatic improvements in performance over conventional implementations \nof arrows. We have implemented this technique in Haskell, and conducted benchmarks that validate the \neffectiveness of our approach. When combined with stream fusion, the overall methodology can result in \nspeed-ups of greater than two orders of magnitude. Categories and Subject Descriptors D.3.3 [Programming \nLan\u00adguages]: Language Constructs and Features General Terms Languages, Performance, Theory Keywords Functional \nProgramming, Arrows, Functional Reac\u00adtive Programming, Data.ow Language, Stream Processing, Pro\u00adgram \nOptimization Permission to make digital or hard copies of all or part of this work for personal or classroom \nuse is granted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. ICFP \n09, August 31 September 2, 2009, Edinburgh, Scotland, UK. Copyright c &#38;#169; 2009 ACM 978-1-60558-332-7/09/08. \n. . $5.00 1. Introduction Consider the following recursive mathematical de.nition of the exponential \nfunction: Z t e(t)=1+ e(t)dt 0 In Yampa [35, 21], a domain-speci.c language embedded in Haskell [36], \nwe can write this using arrow syntax [32] as follows: exp = proc () . do rec let e = 1 + i i . integral \n-. e returnA -. e  Even for those not familiar with arrow syntax or Haskell, the close correspondence \nbetween the mathematics and the Yampa program should be clear. As in most high-level language designs, \nthis is the primary motivation for developing a language such as Yampa: reducing the gap between program \nand speci.cation. Yampa has been used in a variety of applications, including robotics [21, 34, 33], \nsound synthesis [15, 6], animation [35, 21], video games [11, 7], bio-chemical processes [22], control \nsystems [31], and graphical user interfaces [10, 9]. There are several reasons that we prefer a language \ndesign based on arrows over, for example, an approach such as that used in Fran [13]. First, arrows are \nmore modular they convey information about input as well as output, whereas Fran s inputs are implicit \nand global. Second, the use of arrows eliminates a subtle but devastating form of space leak,as described \nin [27]. Third, arrows introduce a meta-level of compu\u00adtation that aids in reasoning about program correctness, \ntransfor\u00admation, and optimization. But in fact, conventional arrows (or to borrow a phrase from [26], \nclassic arrows ) are not strong enough to capture the family of computations that we are interested in \n more laws are needed to constrain the computation space. Unfortunately, more constrained forms of computation \n such as monads [29] and applicative func\u00adtors [28] are not general enough. In addition, there are not \nenough operators. In particular, we .nd the need for an abstract initializa\u00adtion operator and its associated \nlaws. In this paper we give a precise abstract characterization of a class of arrow computations that \nwe call causal commutative arrows, or just CCA for short. More precisely, the contributions in this paper \ncan be summarized as follows: 1. We de.ne a notion of commutative arrow by extending the conventional \nset of arrow laws to include a commutativity law. 2. We de.ne an ArrowInit type class with an init operator \nand an associated law that captures the essence of causal computation. 3. We de.ne a small language \ncalled CCA, an extension of the simply typed lambda calculus, in which the above ideas are manifest. \nFor this language we establish:  (a) a normal form,and (b) a normalization procedure.  We achieve \nthis result using only CCA laws, without referring to any concrete semantics or implementation. 4. We \nde.ne an optimization technique for causal commutative ar\u00adrows that yields substantial improvements in \nperformance over previous attempts to optimize arrow combinators and arrow syntax. 5. Finally, we show \nhow to combine our ideas with those of stream fusion to yield speed-ups that can exceed two orders of \nmagni\u00adtude.  We begin the presentation with a brief overview of arrows in Section 2. The knowledgeable \nreader may prefer to skip directly to Section 3, where we give the de.nition and laws for CCA. In Section \n4 we de.ne an extension of the simply-typed lambda calculus that captures CCA, and show in Section 5 \nthat any CCA program can be transformed into a uniform representation that we call Causal Commutative \nNormal Form (CCNF). We show that the normalization procedure is sound, based on equational reasoning \nusing only the CCA laws. In Section 6 we discuss further optimizations, and in Section 7 we present benchmarks \nshowing the effectiveness of our approach. We conclude in Section 8 with a discussion of related work. \n 2. An Introduction to Arrows Arrows [23] are a generalization of monads that relax the stringent linearity \nimposed by monads, while retaining a disciplined style of composition. Arrows have enjoyed a wide range \nof applications, often as a domain-speci.c embedded language (DSEL [19, 20]), including the many Yampa \napplications cited earlier, as well as parsers and printers [25], parallel computing [18], and so on. \nAr\u00adrows also have a theoretical foundation in category theory, where they are strongly related to (but \nnot precisely the same as) Freyd categories [2, 37]. 2.1 Conventional Arrows Like monads, arrows capture \na certain class of abstract compu\u00adtations, and offer a way to structure programs. In Haskell this is \nachieved through the Arrow type class: class Arrow a where arr :: (b . c) . abc (>>>) ::abc . acd . abd \nfirst::ab c . a (b,d) (c,d) The combinator arr lifts a function from b to c to a pure arrow computation \nfrom b to c, namely abc where a is the arrow type. The output of a pure arrow entirely depends on the \ninput (it is analogous to return in the Monad class). >>> composes two arrow computations by connecting \nthe output of the .rst to the input of the second (and is analogous to bind ((>>=))in the Monad class). \nBut in addition to composing arrows linearly, it is desirable to compose them in parallel i.e. to allow \nbranching and merging of inputs and outputs. There are several ways to do this, but by simply de.ning \nthe first combinator in the Arrow class, all other combinators can be de.ned. first converts an arrow \ncomputation taking one input and one result, into an arrow computation taking two inputs and two results. \nThe original arrow is applied to the .rst part of the input, and the result becomes the .rst part of \nthe output. The second part of the input is fed directly to the second part of the output. Other combinators \ncan be de.ned using these three primitives. For example, the dual of first can be de.ned as: arr :: Arrow \na . (b . c) . abc (>>>) :: Arrow a . abc . acd . abd first :: Arrow a . abc . a(b, d)(c, d) (***) :: \nArrow a . abc . ab c . a (b, b ) (c, c ) loop :: Arrow a . a (b,d) (c,d) . abc (a) arr f (b) f >>> g \n (c) first f (d) f *** g (e) loop f   Figure 1. Commonly Used Arrow Combinators second :: (Arrow a) \n. abc . a (d,b) (d,c) second f = arr swap >>> first f >>> arr swap where swap (a, b) = (b, a) Parallel \ncomposition can be de.ned as a sequence of .rst and second: (***) :: (Arrow a) . abc . ab c . a (b, b \n) (c, c ) f***g = first f >>> second g A mere implementation of the arrow combinators, of course, does \nnot make it an arrow the implementation must additionally satisfy a set of arrow laws, which are shown \nin Figure 2. 2.2 Looping Arrows To model recursion, we can introduce a loop combinator [32]. The exponential \nexample given in the introduction requires recursion, as do many applications in signal processing, for \nexample. In Haskell this combinator is captured in the ArrowLoop class: class Arrow a . ArrowLoop a where \nloop :: a (b,d) (c,d) . abc  A valid instance of this class should satisfy the additional laws shown \nin Figure 3. This class and its associated laws are related to the trace operator in [40, 17], which \nwas generalized to arrows in [32]. We .nd that arrows are best viewed pictorially, especially for applications \nsuch as signal processing, where domain experts com\u00admonly draw signal .ow diagrams. Figure 1 shows some \nof the basic combinators in this manner, including loop. 2.3 Arrow Syntax Recall the Yampa de.nition \nof the exponential function given ear\u00adlier: exp = proc () . do rec let e = 1 + i i . integral -. e returnA \n-. e  This program is written using arrow syntax, introduced by Paterson [32] and adopted by GHC (the \npredominant Haskell implementa\u00adtion) because it ameliorates the cumbersome nature of writing in left \nidentity arr id \u00bb f = f right identity f \u00bb arr id = f associativity (f \u00bb g) \u00bb h = f \u00bb (g \u00bb h) composition \narr (g \u00b7 f)= arr f \u00bb arr g extension .rst (arr f)= arr(f \u00d7 id) functor .rst (f \u00bb g)= .rst f \u00bb .rst g \nexchange .rst f \u00bb arr (id \u00d7 g)= arr (id \u00d7 g) \u00bb .rst f unit .rst f \u00bb arr fst = arr fst \u00bb f association \n.rst (.rstf) \u00bb arr assoc = arr assoc \u00bb .rstf where assoc ((a, b),c)=(a, (b, c)) Figure 2. Conventional \nArrow Laws left tightening loop (.rst h \u00bb f)= h \u00bb loop f right tightening loop (f \u00bb .rst h)= loop f \u00bb \nh sliding loop (f \u00bb arr (id \u00d7 k)) = loop (arr (id \u00d7 k) \u00bb f) vanishing loop (loop f)= loop (arr assoc \n-1 \u00bb f \u00bb arr assoc) superposing second (loop f)= loop (arr assoc \u00bb second f \u00bb arr assoc -1) extension \nloop (arr f)= arr(trace f) where trace f b = let (c, d)= f (b, d) in c Figure 3. Arrow Loop Laws commutativity \n.rst f \u00bb second g = second g \u00bb .rst f product init i*** init j = init (i, j) Figure 4. Causal Commutative \nArrow Laws the point-free style demanded by arrows. The above program is equivalent to the following \nsugar-free program: exp = fixA (integral >>> arr (+1)) where fixA f = loop (second f >>> arr (. (_, y) \n. (y, y))) Although more cumbersome, we will use this program style in the remainder of the paper, in \norder to avoid having to explain the meaning of arrow syntax in more detail.  3. Causal Commutative \nArrows In this section we introduce two key extensions to conventional ar\u00adrows, and demonstrate their \nuse by implementing a stream trans\u00adformer in Haskell. First, as mentioned in the introduction, the set \nof arrow and ar\u00adrow loop laws is not strong enough to capture stream computations. In particular, the \ncommutativity law shown in Figure 4 establishes a non-interference property for concurrent computations \n effects are still allowed, but this law guarantees that concurrent effects cannot interfere with each \nother. We say that an arrow is commutative if it satis.es the conventional laws as well as this critical \nadditional law. Yampa is in fact based on commutative arrows. Second, we note that Yampa has a primitive \noperator called iPre that is used to inject a delay into a computation; indeed it is the primary effect \nimposed by the Yampa arrow [35, 21]. Similar operators, often called delay, also appear in data.ow program\u00adming \n[43], stream processing [39, 41], and synchronous languages [4, 8]. In all cases, the operator introduces \nstateful computation into an otherwise stateless setting. In an effort to make this operation more abstract, \nwe rename it init and capture it in the following type class: class ArrowLoop a . ArrowInit a where init \n:: b . abb Intuitively, the argument to init is the initial output; subsequent output is a copy of the \ninput to the arrow. It captures the essence newtype SF a b = SF {unSF ::a . (b, SF ab)} instance Arrow \nSF where arr f = SF h where h x = (fx,SF h) first f = SF (h f) where h f (x, z) = let (y, f ) = unSF \nfx in ((y, z), SF (h f )) f >>> g = SF (h fg) wherehfg x = let (y, f ) = unSF fx (z, g ) = unSF gy in \n(z, SF (h f g )) instance ArrowLoop SF where loop f = SF (h f) where hf x = let ((y, z), f ) = unSF f \n(x, z) in (y, SF (h f )) instance ArrowInit SF where init i = SF (h i) where hi x = (i, SF(hx)) runSF \n::SF a b . [a] . [b] runSF f = gf where g f (x:xs) = let (y, f ) = unSF f x in y: gf xs Figure 5. Causal \nStream Transformer of causal computations, namely that the current output depends only on the current \nas well as previous inputs. Besides causality, we make no other assumptions about the nature of these \nvalues: they may or may not vary with time, and the increment of change may be .nite or in.nitesimally \nsmall. More importantly, a valid instance of the ArrowInit class must satisfy the product law shown in \nFigure 4. This law states that two inits paired together are equivalent to one init of a pair. Here we \nuse the *** operator instead of its expanded de.nition first ... >>> second ... to imply that the product \nlaw assumes commutativity. We will see in a later section that init and the product law are critical \nto our normalization and optimization strategies. But init Constants Syntax Typing Rules Variables Primitive \nTypes Types Expressions Environment V t a, \u00df, . E G ::= ::= ::= ::= ::= x | y | z | ... 1 | Int | Bool \n| ... t | a \u00d7 \u00df | a . \u00df | a r \u00df V | (E1,E2) | fst E | snd E | .x : a.E | E1 E2 | () | ... x1 : a1,..., \nxn : an (UNIT) G f () : 1 (VAR) (x : a) . G G f x : a (ABS) G,x : a f E : \u00df G f .x : a.E : a . \u00df (APP) \nG f E1 : a . \u00df G f E2 : a G f E1 E2 : \u00df (PAIR) G f E1 : a G f E2 : \u00df G f (E1,E2): a \u00d7 \u00df (FST) G f E : \na \u00d7 \u00df G f fst E : a (SND) G f E : a \u00d7 \u00df G f snd E : \u00df arra,\u00df :(a . \u00df) . (a r \u00df) loopa,\u00df,. :(a \u00d7 . r \n\u00df \u00d7 .) . (a r \u00df) \u00bba,\u00df,. :(a r \u00df) . (\u00df r .) . (a r .) inita : a . (a r a) .rsta,\u00df,. :(a r \u00df) . (a \u00d7 . \nr \u00df \u00d7 .) De.nitions assoc :(a \u00d7 \u00df) \u00d7 . . a \u00d7 (\u00df \u00d7 .) id : a . a assoc = .z.(fst (fst z), (snd (fst z), \nsnd z)) id = .x.x assoc -1 : a \u00d7 (\u00df \u00d7 .) . (a \u00d7 \u00df) \u00d7 . (\u00b7): (\u00df . .) . (a . \u00df) . (a . .) assoc -1 = .z.((fst \nz, fst (snd z)), snd (snd z)) (\u00b7)= .f..g..x.f(gx) juggle :(a \u00d7 \u00df) \u00d7 . . (a \u00d7 .) \u00d7 \u00df (\u00d7): (a . \u00df) . (. \n. .) . (a \u00d7 . . \u00df \u00d7 .) juggle = assoc -1 \u00b7 (id \u00d7 swap) \u00b7 assoc (\u00d7): .f..g..z.(f (fst z),g (snd z)) transpose \n:(a \u00d7 \u00df) \u00d7 (. \u00d7 .) . (a \u00d7 .) \u00d7 (\u00df \u00d7 .) dup : a . a \u00d7 a transpose = assoc \u00b7 (juggle \u00d7 id) \u00b7 assoc -1 dup \n= .x.(x, x) shu.e : a \u00d7 ((\u00df \u00d7 d) \u00d7 (. \u00d7 .)) . (a \u00d7 (\u00df \u00d7 .)) \u00d7 (d \u00d7 .) swap : a \u00d7 \u00df . \u00df \u00d7 a shu.e = assoc \n-1 \u00b7 (id \u00d7 transpose) swap = .z.(snd z, fst z) shu.e-1 :(a \u00d7 (\u00df \u00d7 .)) \u00d7 (d \u00d7 .) . a \u00d7 ((\u00df \u00d7 d) \u00d7 (. \u00d7 \n.)) second :(a r \u00df) . (. \u00d7 a r . \u00d7 \u00df) shu.e-1 =(id \u00d7 transpose) \u00b7 assoc second = .f.arr swap \u00bb .rst f \n\u00bb arr swap Figure 6. CCA: a language of Causal Commutative Arrows is also important in allowing us to \nde.ne operators that were pre\u00adviously taken as domain-speci.c primitives. In particular, consider the \nintegral operator used in the exponentiation examples. With init, we can de.ne integral using the Euler \nintegration method and a .xed global step dt as follows: integral :: ArrowInit a . a Double Double integral \n= loop (arr (. (v, i) . i + dt * v) >>> init 0 >>> arr (.i . (i, i))) To complete the picture, we give \nan instance (i.e. an implemen\u00adtation) of CCA that captures a causal stream transformer,as shown in Figure \n5, where: SF ab is an arrow representing functions (transformers) from streams of type a to streams \nof type b. It is essentially a recur\u00adsively de.ned data type consisting of a function with its con\u00adtinuation, \na concept closely related to a form of .nite state au\u00adtomaton called a Mealy Machine [14]. Yampa enjoys \na similar implementation, and the same data type was called Auto in [32].  SF is declared an instance \nof type classes Arrow, ArrowLoop and ArrowInit. For example, exp can be instantiated as type exp :: SF \n() Double. These instances obey all of the arrow laws, including the two additional laws that we introduced. \n runSF :: SF ab ->[a] -> [b] converts an SF arrow into a stream transformer that maps an input stream \nof type  [a] to an output stream of type [b]. As a demonstration, we can sample the exponential function \nat a .xed time interval by running the exp arrow over an uniform input stream inp: dt = 0.01 :: Double \ninp = () : inp :: [()] *Main> runSF exp inp [1.0,1.01,1.0201,1.030301,1.04060401,1.0510100501, ... \nWe must stress that the SF type is but one instance of a causal commutative arrow, and alternative implementations \nsuch as the synchronous circuit type SeqMap in [32] and the stream function type (incidentally also called) \nSF in [24] also qualify as valid instances. The abstract properties such as normal forms that we develop \nin the next section are applicable to any of these instances, and thus are more broadly applicable than \noptimization techniques based on a speci.c semantic model, such as the one considered in [5]. 4. A Language \nof Causal Commutative Arrows To study the properties of CCA more rigorously, we .rst introduce a language \nof CCA terms in Figure 6. which is an extension of the simply-typed lambda calculus with a few primitives \ntypes, tuples, and arrows. Note that: Although the syntax requires that we write type annotations for \nvariables in lambda abstraction, we often omit them and instead give the type of an entire expression. \n (a) Reorder parallel pure and stateful (b) Reorder sequential pure and stateful (c) Change sequential \n(d) Move sequential (e) Move parallel composition to parallel composition into loop composition into \nloop (f) Fuse nested loops Figure 7. Arrow Transformations In previous examples we used the Haskell \ntype Arrow a =>a bc to represent an arrow type a mapping from type b to type c. However, CCA does not \nhave type classes, and thus we write a r \u00df instead.  Each arrow constant represents a family of constant \narrow func\u00adtions indexed by types. We ll omit the type subscripts when they are obvious from context. \n The .gure also de.nes a set of commonly used auxiliary func\u00adtions. Besides satisfying the usual beta \nlaw for lambda expressions, ar\u00adrows in CCA also satisfy the nine conventional arrow laws (Figure 2), \nthe six arrow loop laws (Figure 3), and the two causal commu\u00adtative arrow laws (Figure 4). Due to the \nexistence of immediate feedback in loops, CCA is able to make use of general recursion that is not allowed \nin the simply typed lambda calculus. To see why immediate feedback is necessary, we can look back at \nthe fixA function used to de.ne the combinator version of exp. We rewrite it using CCA syntax below: \n.xA :(a r a) . (\u00df r a) .xA = .f.loop(second f \u00bb arr(.x.(snd x, snd x))) It computes a .xed point of an \narrow at the value level, and contains no init in its de.nition. We consider the ability to model general \nrecursion a strength of our work that is often lacking in other stream or data.ow programming languages. \n(a) Original (b) Reorganized  Figure 8. Diagrams for exp Figure 9. Diagram for loopB  5. Normalization \nof CCA In most implementations, programs written using arrows carry a runtime overhead, primarily due \nto the extra tupling forced onto functions arguments and return values. There have been several attempts \n[30, 24] to optimize arrow-based programs using arrow laws, but the results have not been entirely satisfactory. \nAlthough conventional arrow and arrow loop laws offer ways to combine pure arrows or collapse nested \nloops, they are not powerful enough to deal with effectful arrows, such as the init combinator. 5.1 Intuition \nOur new optimization strategy is based on the following rather striking observation: any CCA program \ncan be transformed into a single loop containing one pure arrow and one initial state value. More precisely, \nany CCA program can be normalized into either the form arr f or: loop(arr f \u00bb second (second (init i))) \n where f is a pure function and i is an initial state. Note that all other arrow combinators, and therefore \nall of the overheads associated with them (tupling, etc.) are completely eliminated.Not surprisingly, \nthe resulting improvement in performance is rather dramatic, as we will see later. We treat the loop \ncombinator not just as a way to provide feedback from output to input, but also as a way to reorganize \na complex composition of arrows. To see how this works, it is helpful to visualize a few examples, as \nshown in Figure 7, and explained below. This should help explain the intuition behind our normalization \nprocess, which is treated formally in the next section. The diagrams in Figure 7 can be explained as \nfollows: (a) Re-order parallel pure and stateful arrows. Figure 7(a) shows the exchange law for arrows, \nwhich is a special case of the commutativity law, and useful for re-ordering pure and stateful arrows. \n (b) Re-order sequential pure and stateful arrows. Figure 7(b) shows how the immediate feedback of the \nloop combinator  loop loop f . loopB () (arr assoc -1 \u00bb .rst f \u00bb arr assoc) init init i . loopB i (arr \n(swap \u00b7 juggle \u00b7 swap)) composition arr f \u00bb arr g . arr (g \u00b7 f) extension .rst (arr f) . arr (f \u00d7 id) \nleft tightening h \u00bb loopB if . loopB i (.rst h \u00bb f) right tightening loopB if \u00bb arr g . loopB i (f \u00bb \n.rst (arr g)) vanishing loopB i (loopB jf) . loopB (i, j)(arr shu.e \u00bb f \u00bb arr shu.e-1 ) superposing .rst \n(loopB if) . loopB i (arr juggle \u00bb .rst f \u00bb arr juggle) Figure 10. One Step Reduction for CCA (NORM) \n.(i, f) s.t. e = arr f or e = loopB i (arr f) e . e ''''' e1 . ee2 . ee1 \u00bb e2 . ee . e (SEQ) 12 ' e1 \n\u00bb e2 . e '' f . f' .rst f' . ee . einit i . ee . e (FIRST) ' (INIT) ' .rst f . einit i . e '' loop f \n. ee . ef . f' loopB if' . ee . e (LOOP) ' (LOOPB) ' loop f . eloopB if . e Figure 11. Normalization \nProcedure for CCA helps to re-order arrows. This follows from the de.nition of second , and the tightening \nand sliding laws for loops. (c) Change sequential composition to parallel. Figure 7(c) shows that in \naddition to the sequential re-ordering we can use the product law to fuse two stateful computations into \none.  (d) Move sequential composition into loop. Figure 7(d) shows the left-tightening law for loops. \nBecause the .rst arrow can also be a loop, we are able to combine sequential compositions of two loops \ninto a nested one. (e) Move parallel composition into loop. Figure 7(e) shows a vari\u00adant of the superposing \nlaw for loops using .rst instead of second . Since we know that parallel composition can be de\u00adcomposed \ninto .rst and second, and if each of them can be transformed into a loop, they will eventually be combined \ninto a nested loop as shown. (f) Fuse nested loops. Figure 7(f) shows an extension of the van\u00adishing \nlaw for loops to handle stateful computations. Its proof requires the commutative law and product law \nto switch the po\u00adsition of two stateful arrows and join them together.  As a concrete example, Figure \n8(a) is a diagram of the original exp example given earlier. In Figure 8(b) we have unfolded the de.nition \nof integral and applied the optimization strategy. The result is a single loop, where all pure functions \ncan be combined together to minimize arrow implementation overheads.  5.2 Algorithm In this section \nwe give a formal de.nition of the normalization procedure. First we de.ne a combinator called loopB that \ncan be viewed as syntactic sugar for handling both immediate and delayed feedback: loopB : . . (a \u00d7 (. \n\u00d7 .) r \u00df \u00d7 (. \u00d7 .)) . (a r \u00df) loopB = .i..f.loop (f \u00bb second (second (init i))) A pictorial view of loopB \nis given in Figure 9. The second argu\u00adment to loopB is an arrow mapping from an input of type a to output \n\u00df, while looping over a pair . \u00d7 .. The value of type . is initialized before looping back, and is often \nregarded as an internal state. The value of type . is immediately fed back and often used for general \nrecursions at the value level. We de.ne a single step reduction . as a set of rules in Fig\u00adure 10, and \na normalization procedure in Figure 11. The normal\u00adization relation . can be seen as a big step reduction \nfollowing an innermost strategy, and is indeed a function. Note that some of the reduction rules resemble \nthe arrow laws of the same name. However, there are some subtle but important differences: First, unlike \nthe laws, reduction is directed. Second, the rules are extended to handle loopB instead of loop. Finally, \nthey are adjusted to avoid overlaps. Theorem 5.1 (CCNF) For all f e : a r \u00df, there exists a normal form \nenorm , called the Causal Commutative Normal Form, which is either of the form arr f,or loopB i (arr \nf) for some i and f, such that f enorm : a r \u00df, and e . enorm . In unsugared form, the second form is \nequivalent to: loop(arr f \u00bb second (second (init i))) Proof: Follows directly from Lemmas 5.1 and 5.2. \n0 Note that we only consider closed terms with empty type en\u00advironments in Theorem 5.1, otherwise we \nwould have to include lambda normal forms as part of CCNF. For example, x : a r \u00df f x : a r \u00df would qualify \nas a valid CCNF since x is of an ar\u00adrow type, and there is no further reduction possible. Although this \naddition may be needed in real implementations, it would unnec\u00adessarily complicate the discussion, so \nwe disallow open terms for simplicity. Lemma 5.1 (Soundness) The reduction rules given in Figure 10 ' \n are both type and semantics preserving, i.e., if e . e' then e = eis syntactically derivable from the \nset of CCA laws. Proof: By equational reasoning using arrow laws. The loop and init rules follow from \nthe de.nition of loopB; composition and extension are directly based on the arrow laws with the same \nname; left and right tightening and superposing rules follow the de.nition of loopB, the commutativity \nlaw and the arrow loop laws with the same name. The proof of the vanishing rule is more involved, and \nis given in Appendix B. 0 Note that the set of reduction rules is sound but not complete, because the \nloop combinator can introduce general recursion at the value level. Lemma 5.2 (Termination) The normalization \nprocedure for CCA given in Figure 11 terminates for all well-typed arrow expressions f e : a r \u00df. Proof: \nBy structural induction over all possible combinations of well-typed arrow terms. See Appendix A for \ndetails. 0  6. Further Optimization We have implemented the normalization procedure of CCA in Haskell. \nIn fact the normalization of an arrow term does not have to stop at CCNF, because pure functions in the \nlanguage are of simply typed lambda calculus, which is strongly normalizing. Extra care was taken to \npreserve sharing of lambda terms, to eliminate redundant variables, and so on. In the remainder of this \nsection we describe a simple sequence of other optimizations that ultimately leads to a single imperative \nloop that can be implemented extremely ef.ciently. Optimized Loop In addition to loopB, for optimization \npurposes we introduce another looping combinator, loopD, for loops with only delayed feedback. For comparison, \nthe Haskell de.nitions of both are given below: loopB :: ArrowInit a . e . a (b, (d, e)) (c, (d, e)) \n. abc loopD :: ArrowInit a . e . a (b, e) (c, e) . abc loopB i f = loop (f >>> second (second (init i))) \nloopD i f = loop (f >>> second (init i)) The reason to introduce loopD is that many applications of CCA \nre\u00adsult in an arrow in which all loops only have delayed feedback. For example, after removing redundant \nvariables, normalizing lambdas, and eliminating common sub-expressions, the CCNF for exp is: exp = loopB \n0 (arr (. (x, (z, y)) . let i = y + 1 in (i, (z, y + dt * i)))) Clearly the variable z here is redundant, \nand it can be removed by changing loopB to loopD: exp = loopD 0 (arr (. (x, y) . let i = y + 1in(i,y \n+ dt * i))) The above function corresponds nicely with the diagram shown in Figure 8(b). We call this \nresult optimized CCNF. Inlining Implementation In fact loopD can be made even more ef.cient if we expose \nthe underlying arrow implementation. For example, using the SF data type shown in Figure 5, loopD can \nbe de.ned as: loopD i f = SF(g i f) wheregif x = let ((y, i ), f ) = unSF f (x, i) in(y,SF(gi f )) Also, \nif we examine the use of loopD in optimized CCNF, we notice that the arrow it takes is always a pure \narrow, and hence we can drop the arrow and use the pure function instead. Furthermore, if our interest \nis just in computing from an input stream to an output, we can drop the intermediate SF data structure \naltogether, thus yielding: runCCNF :: e . ((b, e) . (c, e)) . [b] . [c] runCCNF i f = gi where g i (x:xs) \n= let (y, i ) = f(x, i) in y: gi xs runCCNF essentially converts an optimized CCNF term directly into \na stream transformer. In doing so, we have successfully trans\u00adformed away all arrow instances, including \nthe data structure used to implement them! The result is of course no longer abstract, and is closely \ntied to the low-level representation of streams. Combining CCA With Stream Fusion We can perform even \nmore aggressive optimizations on CCNF by borrowing the stream representation and optimization techniques \nintroduced by Coutts et al. [12]. First, we de.ne a datatype to encapsulate a stream as a product of \na stepper function and an initial state: data Stream a = forall s. Stream (s . Stepas)s data Step a s \n= Yield a s Here a is the element type and s is an existentially quanti.ed state type. For our purposes, \nwe have simpli.ed the return type of the original stepper function in [12]. Our stepper function essentially \nconsumes a state and yields an element in the stream paired with a new state. The key to effective fusion \nis that all stream producers must be non-recursive. In other words, a recursively de.ned stream such \nas exp should be written in terms of non-recursive stepper functions, with recursion deferred until the \nstream is unfolded. Programs written in this style can then be fused by the compiler into a tail\u00adrecursive \nloop, at which point tail-call eliminations and various unboxing optimizations can be easily applied. \nThis is where CCA and our normalization procedure .t together so nicely. We can take advantage of the \narrow syntax to write recursive code, and rely on the arrow translator to express it non\u00adrecursively \nusing the loop combinator. We then normalize it into CCNF, and rewrite it in terms of streams. The last \nstep is surprisingly straightforward. We introduce yet another loop combinator loopS that closely resembles \nloopD: loopS :: t . ((a, t) . (b, t)) . Stream a . Stream b loopS z f (Stream next0 s0) = Stream next \n(z, s0) where next (i, s) = case next0 s of Yield x s . Yield y (z , s ) where (y, z ) = f(x, i) Intuitively, \nloopS is the unlifted version of loopD. The initial state of the output stream consists of the initial \nfeedback value z and the state of the input stream. As the resulting stream gets unfolded, it supplies \nf with an input tuple and carries the output along with the next state of the input stream. In general, \nwe can rewrite terms of the form loopD i (arr f) into loopS if for some i and f. To illustrate this, \nlet us revisit the exp example. We take the op\u00adtimized CCNF exp and rewrite it in terms of loopS as \nexpOpt: expOpt :: Stream Double expOpt sr = loopS 0 (. (x, y) . let i = y + 1 in (i, y + dt * i)) (constS \n()) constS :: a . Stream a constS c = Stream next () where next _ = Yield c ()  Since the resulting \nstream producer ignores any input, we de.ne constS to supply a stream of unit values. This does not negatively \nimpact performance, as the compiler is able to remove the dummy values eventually. To extract elements \nfrom a stream, we can write a tail-recursive function to unfold it. For example, the function nth extracts \nthe nth element from a stream: nth::Int . Stream a . a nth n (Stream next0 s0) = go n s0 where go ns \n= case next0 s of Yield x s . if n == 0 then x else go (n-1) s e2 :: Double e2 = nth 2 expOpt --1.0201 \n We can de.ne unfolding functions other than nth in a similar manner. With the necessary optimization \noptions turned on, GHC fuses nth and expOpt into a tail-recursive loop. The code below shows the equivalent \nintermediate representation extracted from GHC af\u00adter optimization. It uses only strict and unboxed types \n(Int# and Double#). go :: Int# . Double# . Double# gony = case n of __DEFAULT . go (n -1) (y + dt * (y \n+ 1.0)) 0 . y + 1.0 e2 :: Double e2 = D# (go 2 0.0) In summary, employing stream fusion, the GHC compiler \ncan turn any CCNF into a tight imperative loop that is free of all cons cell and closure allocations. \nThis results in a dramatic speedup for CCA programs and eliminates the need for heap allocation and garbage \ncollection. In the next section we quantify this claim via benchmarks.  7. Benchmarks We ran a set of \nbenchmarks to measure the performance of several programs written in arrow syntax, but compiled and optimized \nin different ways. For each program, we: 1. Compiled with GHC, which has a built-in translator for arrow \nsyntax. 2. Translated using Paterson s arrowp pre-processor to arrow combinators, and then compiled \nwith GHC. 3. Normalized into CCNF combinators, and compiled with GHC. 4. Normalized into CCNF combinators, \nrewritten in terms of streams, and compiled with GHC using stream fusion.  The .ve benchmarks we used \nare: the exponential function given earlier, a sine wave with .xed frequency using Goertzel s method, \na sine wave with variable frequency, 50 s sci-. sound synthesis program taken from [15], and a robot \nsimulator taken from [21]. The programs were compiled and run on an Intel Core 2 Duo machine with GHC \nversion 6.10.1, using the C backend code generator and -O2 optimization. We measured the CPU time used \nto run a program through 106 samples. The results are shown in Figure 12, where the numbers represent \nnormalized speedup ratios, and we also include the lines of code (LOC) for the source program. The results \nshow dramatic performance improvements using normalized arrows. We note that: 1. Based on the same arrow \nimplementation, the performance gain of CCNF over the .rst two approaches is entirely due to program \ntransformations at the source level. This means that the runtime overhead of arrows is signi.cant, and \ncannot be neglected for real applications. 2. The stream representation of CCNF produces high-performance \ncode that is completely free of dynamic memory allocation and  Name (LOC) 1. GHC 2. arrowp 3. CCNF \n4. Stream exp (4) 1.0 2.4 13.9 190.9 sine (6) 1.0 2.66 12.0 284.0 oscSine (4) 1.0 1.75 4.1 13.0 50 s \nsci-. (5) 1.0 1.28 10.2 19.2 robotSim (8) 1.0 1.48 8.9 36.8 Figure 12. Performance Ratio (greater is \nbetter) intermediate data structures, and can be orders of magnitude faster than its arrow-based predecessors. \n3. GHC s arrow syntax translator does not do as well as Paterson s original translator for the sample \nprograms we chose, though both are signi.cantly outperformed by our normalization tech\u00adniques. 8. Discussion \nOur key contribution is the discovery of a normal form for core Yampa, or CCA, programs: any CCA program \ncan be transformed into a single loop with just one pure (and strongly normalizing) function and a set \nof initial states. This discovery is new and original, and has practical implications in implementing \nnot just Yampa, but a broader class of synchronous data.ow languages and stream computations because \nthis property is entirely based on axiomatic laws, not any particular semantic model. We discuss such \nrelevance and related topics to our approach below. 8.1 Alternative Formalisms Apart from arrows, other \nformalisms such as monads, comon\u00adads and applicative functors have been used to model compu\u00adtations over \ndata streams [3, 42, 28]. Central to many of these approaches are the representation of streams and computations \nabout them. However, notably missing are the connections between stream computation and the related laws. \nFor example, Uustalu s work [42] concluded that comonad is a suitable model for data.ow computation, \nbut it lacks any discussion of whether the comonadic laws are of any relevance. In contrast, it is the \nvery idea of making sense out of arrow and arrow loop laws that motivated our work. We argue that arrows \nare a suitable abstract model for stream computation not only because we can implement stream functions \nas arrows, but also because abstract properties like the arrow laws help to bring more insights to our \ntarget application domain. Besides having to satisfy respective laws for these formalisms, each abstraction \nhas to introduce domain speci.c operators, other\u00adwise it would be too general to be useful. With respect \nto causal streams, many have introduced init (also known as delay)as a primitive to enable stateful computation, \nbut few seem to have made the connection of its properties to program optimizations. Notably the product \nlaw we introduced for CCA relates to a bisimilarity property of co-algebraic streams, i.e., the product \nof two initialized streams are bisimilar to one initialized stream of product. 8.2 Co-algebraic streams \nThe co-algebraic property of streams is well known, and most rel\u00adevant to our work is Caspi and Pouzet \ns representation of stream and stream functions in a functional language setting [5], which also uses \na primitive similar to the trace operator (and hence the ar\u00adrow loop combinator) to model recursion. \nTheir compilation tech\u00adnique, however, lacks a systematic approach to optimize nested re\u00adcursions. We \nconsider our technique more effective and more ab\u00adstract. Most synchronous languages, including the \none introduced in [5], are able to compile stream programs into a form called sin\u00adgle loop code by performing \na causality analysis to break the feed\u00adback loop of recursively de.ned values. Many efforts have been \nmade to generate ef.cient single loop code [16, 1], but to our best knowledge there has not been a strong \nresult like normal forms. Our discovery of CCNF is original, and the optimization by normaliza\u00adtion approach \nis both systematic and deterministic. Together with stream fusion, we produce a result that is not just \na single loop, but a highly optimized one. Also relevant is Rutten s work on high-order functional stream \nderivatives [38]. We believe that arrows are a more general abstrac\u00adtion than functional stream derivatives, \nbecause the latter still ex\u00adposes the structure of a stream. Moreover, arrows give rise to a high-level \nlanguage with richer algebraic properties than the 2-adic calculus considered in [38].  8.3 Expressiveness \nIt is known that operationally a Mealy machine is able to represent all causal stream functions [38], \nwhile the CCA language de.ned in Figure 6 represents only a subset. For example, the switch com\u00adbinator \nintroduced in Yampa [21] is able to dynamically replace a running arrow with a new one depending on an \ninput event, and hence to switch the system behavior completely. With CCA, there is no way to change \nthe compositional structure of the arrow pro\u00adgram itself at run time. For another example, many data.ow \nand stream programming languages also provide conditionals, such as if-then-else, as part of the language \n[43, 4]. To enable condi\u00adtionals at the arrow level, we need to further extend CCA to be an instance \nof the ArrowChoice class. Both are worthy extensions to consider for future work. It should also be noted \nthat the local state introduced by init is one of the minimal side effects one can introduce to arrow \npro\u00adgrams. The commutativity law for CCA ensures that the effect of one arrow cannot interfere with another \nwhen composed together, and it is no longer satis.able when such ordering becomes impor\u00adtant, e.g., when \narrows are used to model parsers and printers [25]. On the other hand, because the language for CCA remains \nhighly abstract, it could be applicable to domains other than FRP or data.ow. We ll leave such .ndings \nto future work.  8.4 Stream fusion Stream fusion can help fuse zips, left folds, and nested lists into \nef.cient loops. But on its own, it does not optimize recursively and lazily de.ned streams effectively. \nConsider a stream generating the Fibonacci sequence. It is one of the simplest classic examples that \ncharacterizes stateful stream computation. One way of writing it in Haskell is to exploit laziness and \nzip the stream with itself: fibs :: [Int] fibs = 0:1:zipWith (+) fibs (tail fibs) While the code is \nconcise and elegant, such programming style relies too much on the de.nition of an inductively de.ned \nstruc\u00adture. The explicit sharing of the stream fibs in the de.nition is a blessing and a curse. On one \nhand, it runs in linear time and con\u00adstant space. On the other hand, the presence of the stream struc\u00adture \ngets in the way of optimization. None of the current fusion or deforestation techniques are able to effectively \neliminate cons cell allocations in this example. Real-world stream programs are usu\u00adally much more complex \nand involve more feedback, and the time spent in allocating intermediate structure and by the garbage \ncol\u00adlector could degrade performance signi.cantly. We can certainly write a stream in stepper style that \ngenerates the Fibonacci sequence: fib_stream :: Stream Int fib_stream = Stream next (0, 1) where next \n(a, b) = Yield r (b, r) where r = a + b f1 :: Int f1 = nth 5 fib_stream --13  Stream fusion will fuse \nnth and fib stream to produce an ef.\u00adcient loop. For a comparison, with our technique the arrow version \nof the Fibonacci sequence shown below compiles to the same ef.\u00adcient loop as f1 above, and yet retains \nthe bene.t of being abstract and concise. fibA = proc _ . do rec let r = d2 + d1 d1 . init 0 -. d2 d2 \n. init 1 -. r returnA -. r We must stress that writing stepper functions is not always as easy as in \ntrivial examples like fib and exp. Most non-trivial stream programs that we are concerned with contain \nmany recur\u00adsive parts, and expressing them in terms of combinators in a non\u00adrecursive way can get unwieldy. \nMoreover, this kind of coding style exposes a lot of operational details which are arguably unnecessary \nfor representing the underlying algorithm. In contrast, arrow syn\u00adtax relieves the burden of coding in \ncombinator form and allows recursion via the rec keyword. It also completely hides the actual implementation \nof the underlying stream structure and is therefore more abstract. The strength of CCA is the ability \nto normalize any causal and recursive stream function. Combining both fusion and our nor\u00admalization algorithm, \nany CCA program can be reliably and pre\u00addictably optimized into an ef.cient machine-friendly loop. The \npro\u00adcess can be fully automated, allowing programmers to program at an abstract level while getting performance \ncompetitive to pro\u00adgrams written in low-level imperative languages. Acknowledgements This research was \nsupported in part by NSF grants CCF-0811665 and CNS-0720682, and a grant from Mi\u00adcrosoft Research.  \n References [1] Pascalin Amagbegnon, Loc Besnard, and Paul Le Guernic. Im\u00adplementation of the data-.ow \nsynchronous language signal. In In Conference on Programming Language Design and Implementation, pages \n163 173. ACM Press, 1995. [2] Robert Atkey. What is a categorical model of arrows? In Mathematically \nStructured Functional Programming, 2008. [3] Per Bjesse, Koen Claessen, Mary Sheeran, and Satnam Singh. \nLava: Hardware design in haskell. In ICFP, pages 174 184, 1998. [4] P. Caspi, N. Halbwachs, D. Pilaud, \nand J.A. Plaice. Lustre: A declarative language for programming synchronous systems. In 14th ACM Symp. \non Principles of Programming Languages, January 1987. [5] Paul Caspi and Marc Pouzet. A Co-iterative \nCharacterization of Synchronous Stream Functions. In Coalgebraic Methods in Computer Science (CMCS 98), \nElectronic Notes in Theoretical Computer Science, March 1998. Extended version available as a VERIMAG \ntech. report no. 97 07 at www.lri.fr/~pouzet. [6] Eric Cheng and Paul Hudak. Look ma, no arrows a functional \nreactive real-time sound synthesis framework. Technical Report YALEU/DCS/RR-1405, Yale University, May \n2008. [7] Mun Hon Cheong. Functional programming and 3d games, November 2005. also see www.haskell.org/haskellwiki/Frag. \n [8] Jean-Louis Colac\u00b8o, Alain Girault, Gr\u00b4egoire Hamon, and Marc Pouzet. Towards a higher-order synchronous \ndata-.ow language. In EMSOFT 04: Proceedings of the 4th ACM international conference on Embedded software, \npages 230 239, New York, NY, USA, 2004. ACM. [9] Antony Courtney. Modelling User Interfaces in a Functional \nLanguage. PhD thesis, Department of Computer Science, Yale University, May 2004. [10] Antony Courtney \nand Conal Elliott. Genuinely functional user interfaces. In Proc. of the 2001 Haskell Workshop, September \n2001. [11] Antony Courtney, Henrik Nilsson, and John Peterson. The Yampa arcade. In Proceedings of the \n2003 ACM SIGPLAN Haskell Workshop (Haskell 03), pages 7 18, Uppsala, Sweden, August 2003. ACM Press. \n[12] Duncan Coutts, Roman Leshchinskiy, and Don Stewart. Stream fusion: From lists to streams to nothing \nat all. In Proceedings of the ACM SIGPLAN International Conference on Functional Programming, ICFP 2007, \nApril 2007. [13] Conal Elliott and Paul Hudak. Functional reactive animation. In International Conference \non Functional Programming, pages 263 273, June 1997. [14] G. H. Mealy. A method for synthesizing sequential \ncircuits. Bell System Technical Journal, 34(5):1045 1079, 1955. [15] George Giorgidze and Henrik Nilsson. \nSwitched-on yampa. In Paul Hudak and David Scott Warren, editors, Practical Aspects of Declarative Languages, \n10th International Symposium, PADL 2008, San Francisco, CA, USA, January 7-8, 2008, volume 4902 of Lecture \nNotes in Computer Science, pages 282 298. Springer, 2008. [16] N. Halbwachs, P. Raymond, and C. Ratel. \nGenerating ef.cient code from data-.ow programs. In J. Maluszy\u00b4nski and M. Wirsing, editors, Proceedings \nof the Third International Symposium on Programming Language Implementation and Logic Programming, number \n528, pages 1 13207 218. Springer Verlag, 1991. [17] Masahito Hasegawa. Recursion from cyclic sharing: \ntraced monoidal categories and models of cyclic lambda calculi. pages 196 213. Springer Verlag, 1997. \n[18] L. Huang, P. Hudak, and J. Peterson. Hporter: Using arrows to compose parallel processes. In Proc. \nPractical Aspects of Declarative Languages, pages 275 289. Springer Verlag LNCS 4354, January 2007. [19] \nP. Hudak. Building domain speci.c embedded languages. ACM Computing Surveys, 28A:(electronic), December \n1996. [20] Paul Hudak. Modular domain speci.c languages and tools. In Proceedings of Fifth International \nConference on Software Reuse, pages 134 142. IEEE Computer Society, June 1998. [21] Paul Hudak, Antony \nCourtney, Henrik Nilsson, and John Peterson. Arrows, robots, and functional reactive programming. In \nSummer School on Advanced Functional Programming 2002, Oxford Uni\u00adversity, volume 2638 of Lecture Notes \nin Computer Science, pages 159 187. Springer-Verlag, 2003. [22] Paul Hudak, Paul Liu, Michael Stern, \nand Ashish Agarwal. Yampa meets the worm. Technical Report YALEU/DCS/RR-1408, Yale University, July 2008. \n[23] John Hughes. Generalising monads to arrows. Science of Computer Programming, 37:67 111, May 2000. \n[24] John Hughes. Programming with arrows. In Advanced Functional Programming, pages 73 129, 2004. [25] \nPatrik Jansson and Johan Jeuring. Polytypic compact printing and parsing. In ESOP, pages 273 287, 1999. \n[26] Sam Lindley, Philip Wadler, and Jeremy Yallop. The arrow calculus (functional pearl). Draft, 2008. \n[27] Hai Liu and Paul Hudak. Plugging a space leak with an arrow. Electronic Notes in Theoretical Computer \nScience, 193:29 45, nov 2007. [28] Conor McBride and Ross Paterson. Applicative programming with effects. \nJ. Funct. Program., 18(1):1 13, 2008. [29] Eugenio Moggi. Notions of computation and monads. Inf. Comput., \n93(1):55 92, 1991. [30] Henrik Nilsson. Dynamic optimization for functional reactive programming using \ngeneralized algebraic data types. In ICFP, pages 54 65, 2005. [31] Clemens Oertel. RatTracker: A Functional-Reactive \nApproach to Flexible Control of Behavioural Conditioning Experiments.PhD thesis, Wilhelm-Schickard-Institute \nfor Computer Science at the University of T\u00a8ubingen, May 2006. [32] Ross Paterson. A new notation for \narrows. In ICFP 01: International Conference on Functional Programming, pages 229 240, Firenze, Italy, \n2001. [33] John Peterson, Gregory Hager, and Paul Hudak. A language for declarative robotic programming. \nIn International Conference on Robotics and Automation, 1999. [34] John Peterson, Paul Hudak, and Conal \nElliott. Lambda in motion: Controlling robots with Haskell. In First International Workshop on Practical \nAspects of Declarative Languages. SIGPLAN, Jan 1999. [35] John Peterson, Zhanyong Wan, Paul Hudak, and \nHenrik Nilsson. Yale FRP User s Manual. Department of Computer Science, Yale University, January 2001. \nAvailable at http://www.haskell.org/ frp/manual.html. [36] Simon Peyton Jones et al. The Haskell 98 language \nand libraries: The revised report. Journal of Functional Programming, 13(1):0 255, Jan 2003. http://www.haskell.org/de.nition/. \n[37] John Power and Hayo Thielecke. Closed freyd-and kappa-categories. In ICALP, pages 625 634, 1999. \n[38] Jan J. M. M. Rutten. Algebraic speci.cation and coalgebraic synthesis of mealy automata. Electr. \nNotes Theor. Comput. Sci, 160:305 319, 2006. [39] Robert Stephens. A survey of stream processing. Acta \nInformatica, 34(7):491 541, 1997. [40] Ross Howard Street, A. Joyal, and D. Verity. Traced monoidal cat\u00adegories. \nMathematical Proceedings of the Cambridge Philosophical Society, 119(3):425 446, 1996. [41] William Thies, \nMichal Karczmarek, and Saman P. Amarasinghe. Streamit: A language for streaming applications. In CC 02: \nProceedings of the 11th International Conference on Compiler Construction, pages 179 196, London, UK, \n2002. Springer-Verlag. [42] Tarmo Uustalu and Varmo Vene. The essence of data.ow program\u00adming. In Zolt\u00b4an \nHorv\u00b4ath, editor, CEFP, volume 4164 of Lecture Notes in Computer Science, pages 135 167. Springer, 2005. \n[43] William W. Wadge and Edward A. Ashcroft. LUCID, the data.ow programming language. Academic Press \nProfessional, Inc., San Diego, CA, USA, 1985. A. Proof for the termination lemma Proof: We will show \nthat the there always exists a enorm for well formed arrow expression f e : a r \u00df, and the normalization \nprocedure always terminates. This is done by structural induction over all possible arrow terms, and \nany closed expression e that s not already in arrow terms shall be .rst beta reduced. 1. e = arr f It \nalready satis.es the termination condition.  2. e = .rst f By induction hypothesis, f . arr f ' ,or \nf . loopB i (arr f '' ),  where f ' and f '' are pure functions. In the .rst case by extension rule \n.rst f . arr(f ' \u00d7 id) and terminates; In the second case .rst f * '' ..rst(loopB i(arr f )) superposing \n.loopB i (arr juggle \u00bb arr f '' \u00bb arr juggle) composition .loopB i (arr (juggle \u00b7 f '' juggle)) and terminates. \n3. e = f \u00bb g By induction hypothesis, f . arr f ' or f . loopB i (arr f '' ), and g . arr g ' or g . \nloopB i (arr g '' ).So there are 4 combinations, and in all cases they terminate. 1) f \u00bb g *' ' .arr \nf \u00bb arr g composition .arr(g ' \u00b7 f ' ) 2) f \u00bb g *' '' .arr f \u00bb loopB i (arr g ) left tightening .loopB \ni (.rst (arr f ' ) \u00bb arr g '' ) extension .loopB i (arr (f ' \u00d7 id) \u00bb arr g '' ) composition .loopB i \n(arr (g '' \u00b7 (f ' \u00d7 id))) 3) f \u00bb g * ''' .loopB i (arr f ) \u00bb arr g right tightening .loopB i (arr f '' \n\u00bb .rst(arr g ' )) extension .loopB i (arr f '' \u00bb arr(g ' \u00d7 id)) composition .loopB i (arr ((g ' \u00d7 id) \n\u00b7 f '' )) 4) f \u00bb g * '' '' .loopB i (arr f ) \u00bb loopB i (arr g ) left tightening '' '' ) .loopB j (.rst(loopB \ni (arr f )) \u00bb arr g superposing .loopB j (loopB i (arr juggle \u00bb arr f '' \u00bb arr juggle) \u00bb arr g '' ) composition \n. '' \u00b7 juggle)) * loopB j (loopB i (arr (juggle \u00b7 f \u00bb arr g '' ) right tightening .loopB j (loopB i \n(arr (juggle \u00b7 f '' \u00b7 juggle) \u00bb .rst(arr g '' ))) extension .loopB j (loopB i (arr (juggle \u00b7 f '' \u00b7 juggle) \n\u00bb arr (g '' \u00d7 id))) composition .loopB j (loopB i (arr ((g '' \u00d7 id) \u00b7 juggle \u00b7f '' \u00b7 juggle))) vanishing \n.loopB (j, i)(arr shu.e \u00bb '' '' arr ((g \u00d7 id) \u00b7 juggle \u00b7 f \u00b7 juggle) \u00bb arr shu.e-1 ) composition * '' \n.loopB (j, i)(arr (shu.e-1 \u00b7 (g \u00d7 id)\u00b7 juggle \u00b7 f '' \u00b7 juggle \u00b7 shu.e)) 4. e = loop f By induction hypothesis, \nf . arr f ' or f . loopB i (arr f '' ). In the .rst case loop f . ' ) * loop (arr f loop .loopB ()(arr \nassoc -1 \u00bb arr f ' \u00bb arr assoc) composition .loopB ()(arr (assoc \u00b7 f ' \u00b7 assoc -1 )) and terminates. \nIn the second case loop f * '' .loop (loopB i (arr f )) loop -1 '' .loopB ()(arr assoc \u00bb loopB i (arr \nf ) \u00bb arr assoc) left and right tightening *-1 '' .loopB ()(loopB i (.rst(arr assoc ) \u00bb arr f \u00bb .rst(arr \nassoc))) extension and composition * .loopB ()(loopB i (arr ((assoc \u00d7 id)\u00b7 '' -1 f \u00b7 (assoc \u00d7 id)))) \nvanishing .loopB ((),i)(arr shu.e \u00bb arr ((assoc \u00d7 id)\u00b7 '' -1 f \u00b7 (assoc \u00d7 id)) \u00bb arr shu.e-1 ) composition \n.loopB ((),i)(arr(shu.e-1 \u00b7 (assoc \u00d7 id) \u00b7 f '' \u00b7 (assoc -1 \u00d7 id) \u00b7 shu.e)) and terminates. 5. e = init \ni By init rule, init i . loopB i (arr (swap \u00b7 juggle \u00b7 swap)) and terminates. B. Proof for the vanishing \nrule of loopB Proof: We will show that loopB i (loopB j f ) = loopB (i, j)(arr shu.e \u00bb f \u00bb arr shu.e-1 \n) by equational reasoning. loopB i (loopB j f ) de.nition of loopB = loop (loopB jf \u00bb second (second \n(init i))) de.nition of loopB = loop (loop (f \u00bb second (second (init j))) \u00bb second (second (init i))) \n right tightening of loop = loop (loop (f \u00bb second (second (init j)) \u00bb first(second (second (init i))))) \n commutativity = loop (loop (f \u00bb first(second (second (init i))) \u00bb second (second (init j)))) vanishing \nof loop = loop (arr assoc -1 \u00bb f \u00bb .rst (second (second (init i))) \u00bb second (second (init j)) \u00bb arr \nassoc) Lemma B.1 = loop (arr assoc -1 \u00bb f \u00bb arr shu.e-1 \u00bb second (second (init (i, j))) \u00bb arr shu.e \n\u00bb arr assoc)  -1 ) shuf.e-1 \u00b7 shuf.e = id arr((swap \u00d7 id) \u00b7 assoc -1 \u00b7 (swap \u00d7 id) \u00b7 assoc = loop (arr \n(shu.e-1 \u00b7 assoc -1 ) \u00bb arr shu.e \u00bb f \u00bb composition and normalization arr shu.e-1 \u00bb second (second (init \n(i, j))) \u00bb = arr(.((a, (c, d)), (b, e)).(d, (e, ((c, b),a)))) \u00bb arr shu.e \u00bb arr assoc) .rst(init i) \u00bb \nshuf.e-1 \u00b7 assoc-1 =id \u00d7 transpose arr(.(d, (e, ((c, b),a))).((a, (c, d)), (b, e))) = loop (arr (id \u00d7 \ntranspose) \u00bb arr shu.e \u00bb f \u00bb and from lhs: arr shu.e-1 \u00bb second (second (init (i, j))) \u00bb arr shu.e \u00bb \narr assoc) sliding = loop (arr shu.e \u00bb f \u00bb arr shu.e-1 \u00bb second (second (init (i, j))) \u00bb arr shu.e \u00bb \narr assoc \u00bb arr (id \u00d7 transpose)) shuf.e-1 =(id \u00d7 transpose) \u00b7 assoc = loop (arr shu.e \u00bb f \u00bb arr shu.e-1 \n\u00bb second (second (init (i, j))) \u00bb arr shu.e \u00bb arr shu.e-1 ) shuf.e \u00b7 shuf.e-1 =id = loop (arr shu.e \n\u00bb f \u00bb arr shu.e-1 \u00bb second (second (init (i, j)))) de.nition of loopB = loopB (i, j)(arr shu.e \u00bb f \u00bb \narr shu.e-1 ) Lemma B.1 .rst (second (second (init i))) \u00bb second (second (init j)) = arr shu.e-1 \u00bb second \n(second (init(i, j))) \u00bb arr shu.e Proof: We .rst show .rst (second (second (init i))) = arr shu.e-1 \n\u00bb second (second (.rst(init i))) \u00bb arr shu.e This can be done by equational reasoning from both sides. \nFrom lhs: .rst (second (second (init i))) de.nition of second = .rst(arr swap \u00bb .rst(arr swap \u00bb .rst(init \ni) \u00bb arr swap) \u00bb arr swap) functor and extension = arr(swap \u00d7 id) \u00bb .rst(.rst(arr swap \u00bb .rst(init i) \n\u00bb arr swap)) \u00bb arr(swap \u00d7 id) association = arr(swap \u00d7 id) \u00bb arr assoc \u00bb .rst(arr swap \u00bb .rst(init i) \n\u00bb arr swap) \u00bb arr assoc -1 \u00bb arr(swap \u00d7 id) functor and extension = arr(assoc \u00b7 (swap \u00d7 id)) \u00bb arr(swap \n\u00d7 id) \u00bb .rst(.rst(init i)) \u00bb -1 ) arr(swap \u00d7 id) \u00bb arr((swap \u00d7 id) \u00b7 assoc association = arr((swap \u00d7 \nid) \u00b7 assoc \u00b7 (swap \u00d7 id)) \u00bb arr assoc \u00bb .rst(init i) \u00bb arr assoc -1 \u00bb arr((swap \u00d7 id) \u00b7 assoc -1 \u00b7 (swap \n\u00d7 id)) composition = arr(assoc \u00b7 (swap \u00d7 id) \u00b7 assoc \u00b7 (swap \u00d7 id)) \u00bb .rst(init i) \u00bb -1 -1 arr((swap \n\u00d7 id) \u00b7 assoc \u00b7 (swap \u00d7 id) \u00b7 assoc ) Lemma B.2 = arr(assoc \u00b7 (swap \u00d7 id) \u00b7 assoc \u00b7 (swap \u00d7 id)) \u00bb -1 \n-1 arr(id \u00d7 (swap \u00b7 assoc \u00b7 transpose \u00b7 assoc )) \u00bb .rst(init i) \u00bb arr(id \u00d7 (assoc \u00b7 transpose \u00b7 assoc \n\u00b7 swap)) \u00bb arr shu.e-1 \u00bb second (second (.rst(init i))) \u00bb arr shu.e de.nition of second = arr shu.e-1 \n\u00bb arr swap \u00bb .rst(arr swap \u00bb .rst(.rst(init i)) \u00bb arr swap) \u00bb arr swap \u00bb arr shu.e functor and extension \n = arr(swap \u00b7 shu.e-1 ) \u00bb arr(swap \u00d7 id) \u00bb .rst(.rst(.rst(init i))) \u00bb arr(swap \u00d7 id) \u00bb arr(shu.e \u00b7 swap) \n association = arr((swap \u00d7 id) \u00b7 swap \u00b7 shu.e-1 ) \u00bb arr assoc \u00bb arr assoc \u00bb .rst(init i) \u00bb arr assoc \n-1 \u00bb arr assoc -1 \u00bb arr(shu.e \u00b7 swap \u00b7 (swap \u00d7 id)) composition = arr(assoc \u00b7 assoc \u00b7 (swap \u00d7 id) \u00b7 \nswap \u00b7 shu.e-1 ) \u00bb .rst(init i) \u00bb -1 ) arr(shu.e \u00b7 swap \u00b7 (swap \u00d7 id) \u00b7 assoc -1 \u00b7 assoc normalization \n = arr(.((a, (c, d)), (b, e)).(d, (e, ((c, b),a)))) \u00bb .rst(init i) \u00bb arr(.(d, (e, ((c, b),a))).((a, (c, \nd)), (b, e))) Hence lhs = rhs. Using similar technique, we can also prove (details omitted to save space) \nsecond (second (init j)) = arr shu.e-1 \u00bb second (second (second (init j))) \u00bb arr shu.e Therefore we have \n.rst(second (second (init i))) \u00bb second (second (init j)) substitution = arr shu.e-1 \u00bb second (second \n(.rst(init i))) \u00bb arr shu.e \u00bb arr shu.e-1 \u00bb second (second (second (init i))) \u00bb arr shu.e shuf.e \u00b7 shuf.e-1 \n=id = arr shu.e-1 \u00bb second (second (.rst(init i))) \u00bb second (second (second (init i))) \u00bb arr shu.e functor \nand product = arr shu.e-1 \u00bb second (second (init(i, j))) \u00bb arr shu.e 0 -1 -1 Lemma B.2 .g,g ,g \u00b7 g \n= id, we have .rst f = arr (id \u00d7 g) \u00bb .rstf \u00bb arr(id \u00d7 g -1) Proof: arr (id \u00d7 g) \u00bb .rstf \u00bb arr(id \u00d7 \ng -1) exchange = .rstf \u00bb arr (id \u00d7 g) \u00bb arr(id \u00d7 g -1) composition = .rstf \u00bb arr ((id \u00d7 g -1) \u00b7 (id \n\u00d7 g)) normalization = .rstf \u00bb arr id right identity = .rstf 0   \n\t\t\t", "proc_id": "1596550", "abstract": "<p>Arrows are a popular form of abstract computation. Being more general than monads, they are more broadly applicable, and in particular are a good abstraction for signal processing and dataflow computations. Most notably, arrows form the basis for a domain specific language called <i>Yampa</i>, which has been used in a variety of concrete applications, including animation, robotics, sound synthesis, control systems, and graphical user interfaces. Our primary interest is in better understanding the class of abstract computations captured by Yampa. Unfortunately, arrows are not concrete enough to do this with precision. To remedy this situation we introduce the concept of <i>commutative arrows</i> that capture a kind of non-interference property of concurrent computations. We also add an <i>init</i> operator, and identify a crucial law that captures the causal nature of arrow effects. We call the resulting computational model <i>causal commutative arrows</i>. To study this class of computations in more detail, we define an extension to the simply typed lambda calculus called <i>causal commutative arrows</i> (CCA), and study its properties. Our key contribution is the identification of a normal form for CCA called <i>causal commutative normal form</i> (CCNF). By defining a normalization procedure we have developed an optimization strategy that yields dramatic improvements in performance over conventional implementations of arrows. We have implemented this technique in Haskell, and conducted benchmarks that validate the effectiveness of our approach. When combined with stream fusion, the overall methodology can result in speed-ups of greater than two orders of magnitude.</p>", "authors": [{"name": "Hai Liu", "author_profile_id": "81537400056", "affiliation": "Yale University, New Haven, CT, USA", "person_id": "P1613980", "email_address": "", "orcid_id": ""}, {"name": "Eric Cheng", "author_profile_id": "81407594224", "affiliation": "Yale University, New Haven, CT, USA", "person_id": "P1613981", "email_address": "", "orcid_id": ""}, {"name": "Paul Hudak", "author_profile_id": "81100539650", "affiliation": "Yale University, New Haven, CT, USA", "person_id": "P1613982", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1596550.1596559", "year": "2009", "article_id": "1596559", "conference": "ICFP", "title": "Causal commutative arrows and their optimization", "url": "http://dl.acm.org/citation.cfm?id=1596559"}