{"article_publication_date": "08-31-2009", "fulltext": "\n Parallel Concurrent ML John Reppy Claudio V. Russo Yingqi Xiao University of Chicago Microsoft Research \nUniversity of Chicago jhr@cs.uchicago.edu crusso@microsoft.com xiaoyq@cs.uchicago.edu Abstract Concurrent \nML (CML) is a high-level message-passing language that supports the construction of .rst-class synchronous \nabstrac\u00adtions called events. This mechanism has proven quite effective over the years and has been incorporated \nin a number of other languages. While CML provides a concurrent programming model, its imple\u00admentation \nhas always been limited to uniprocessors. This limitation is exploited in the implementation of the synchronization \nprotocol that underlies the event mechanism, but with the advent of cheap parallel processing on the \ndesktop (and laptop), it is time for Paral\u00adlel CML. Parallel implementations of CML-like primitives for \nJava and Haskell exist, but build on high-level synchronization constructs that are unlikely to perform \nwell. This paper presents a novel, par\u00adallel implementation of CML that exploits a purpose-built opti\u00admistic \nconcurrency protocol designed for both correctness and per\u00adformance on shared-memory multiprocessors. \nThis work extends and completes an earlier protocol that supported just a strict subset of CML with synchronization \non input, but not output events. Our main contributions are a model-checked reference implementation \nof the protocol and two concrete implementations. This paper fo\u00adcuses on Manticore s functional, continuation-based \nimplementa\u00adtion but brie.y discusses an independent, thread-based implemen\u00adtation written in C# and running \non Microsoft s stock, parallel run\u00adtime. Although very different in detail, both derive from the same \ndesign. Experimental evaluation of the Manticore implementation reveals good performance, dispite the \nextra overhead of multipro\u00adcessor synchronization. Categories and Subject Descriptors D.3.0 [Programming \nLan\u00adguages]: General; D.3.2 [Programming Languages]: Language Classi.cations Applicative (functional) \nlanguages; Concurrent, distributed, and parallel languages; D.3.3 [Programming Lan\u00adguages]: Language \nConstructs and Features Concurrent program\u00adming structures General Terms Languages, Performance Keywords \nconcurrency, parallelism, message passing 1. Introduction Concurrent ML (CML) [Rep91, Rep99] is a statically-typed \nhigher-order concurrent language that is embedded in Standard Permission to make digital or hard copies \nof all or part of this work for personal or classroom use is granted without fee provided that copies \nare not made or distributed for pro.t or commercial advantage and that copies bear this notice and the \nfull citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. ICFP 09, August 31 September 2, 2009, Edinburgh, \nScotland, UK. Copyright c &#38;#169; 2009 ACM 978-1-60558-332-7/09/08. . . $5.00. ML [MTHM97]. CML extends \nSML with synchronous message passing over typed channels and a powerful abstraction mecha\u00adnism, called \n.rst-class synchronous operations, for building syn\u00adchronization and communication abstractions. This \nmechanism allows programmers to encapsulate complicated communication and synchronization protocols as \n.rst-class abstractions, which en\u00adcourages a modular style of programming where the actual un\u00adderlying \nchannels used to communicate with a given thread are hidden behind data and type abstraction. CML has \nbeen used suc\u00adcessfully in a number of systems, including a multithreaded GUI toolkit [GR93], a distributed \ntuple-space implementation [Rep99], and a system for implementing partitioned applications in a dis\u00adtributed \nsetting [YYS+01]. The design of CML has inspired many implementations of CML-style concurrency primitives \nin other languages. These include other implementations of SML [MLt], other dialects of ML [Ler00], other \nfunctional languages, such as HASKELL [Rus01], SCHEME [FF04], and other high-level lan\u00adguages, such as \nJAVA [Dem97]. One major limitation of the CML implementation is that it is single-threaded and cannot \ntake advantage of multicore or multi\u00adprocessor systems.1 With the advent of the multicore and manycore \nera, this limitation must be addressed. In a previous workshop pa\u00adper, we described a partial solution \nto this problem; namely a pro\u00adtocol for implementing a subset of CML, called Asymmetric CML (ACML), that \nsupports input operations, but not output, in choice contexts [RX08]. This paper builds on that previous \nresult by pre\u00adsenting an optimistic-concurrency protocol for CML synchroniza\u00adtion that supports both \ninput and output operations in choices. In addition to describing this protocol, this paper makes several \naddi\u00adtional contributions beyond the previous work. We present a reference implementation of the protocol \nwritten in SML extended with .rst-class continuations.  To check the correctness of the protocol, we \nhave used stateless model-checking techniques to test the reference code.  We have two different parallel \nimplementations of this proto\u00adcol: one in the Manticore system and one written in C#. While the implementations \nare very different in their details e.g., the Manticore implementation relies heavily on .rst-class \ncon\u00adtinuations, which do not exist in C# both implementations were derived from the reference implementation. \n We describe various messy, but necessary, aspects of the imple\u00admentation.  We present an empirical \nevaluation of the Manticore implemen\u00adtation, which shows that it provides acceptable performance (about \n2.5\u00d7 slower than the single-threaded implementation).  1 In fact, almost all of the existing implementations \nof events have this limi\u00adtation. The only exceptions are presumably the Haskell and Java implemen\u00adtations, \nwhich are both built on top of concurrency substrates that support multiprocessing. The remainder of \nthis paper is organized as follows. In the next section, we give highlights of the CML design. We then \ndescribe the single-threaded implementation of CML that is part of the SM-L/NJ system in Section 3. This \ndiscussion leads to Section 4, which highlights a number of the challenges that face a parallel imple\u00admentation \nof CML. Section 5 presents our main result, which is our optimistic-concurrency protocol for CML synchronization. \nWe have three implementations of this protocol. In Section 6, we de\u00adscribe our reference implementation, \nwhich is written in SML us\u00ading .rst-class continuations. We have model checked this imple\u00admentation, \nwhich we discuss in Section 7. There are various imple\u00admentation details that we omitted from the reference \nimplementa\u00adtion, but which are important for a real implementation. We discuss these in Section 8. We \nthen give an overview of our two parallel implementations of the protocol: one in the Manticore system \nand one in C#. We present performance data for both implementations in Section 10 and then discuss related \nwork in Section 11. 2. A CML overview Concurrent ML is a higher-order concurrent language that is em\u00adbedded \ninto Standard ML [Rep91, Rep99]. It supports a rich set of concurrency mechanisms, but for purposes of \nthis paper we focus on the core mechanism of communication over synchronous chan\u00adnels. The interface \nto these operations is val spawn : (unit -> unit) -> unit type a chan val channel : unit -> a chan val \nrecv : a chan -> a val send : ( a chan * a) -> unit The spawn operation creates new threads, the channel \nfunction creates new channels, and the send and recv operations are used for message passing. Because \nchannels are synchronous, both the send and recv operations are blocking. 2.1 First-class synchronization \nThe most notable feature of CML is its support for .rst-class syn\u00adchronous operations. This mechanism \nwas motivated by two obser\u00advations about message-passing programs [Rep88, Rep91, Rep99]: 1. Most inter-thread \ninteractions involve two or more messages (e.g., client-server interactions typically require a request, \nreply, and acknowledgment messages). 2. Threads need to manage simultaneous communications with multiple \npartners (e.g., communicating with multiple servers or including the possibility of a timeout in a communication). \n For example, consider the situation where a client is interacting with two servers. Since the time \nthat a server needs to .ll a request is indeterminate, the client attempts both transactions in parallel \nand then commits to whichever one completes .rst. Figure 1 illus\u00adtrates this interaction for the case \nwhere the .rst server responds .rst. The client-side code for this interaction might look like that in \nFigure 2. In this code, we allocate fresh reply channels and con\u00addition variables2 for each server and \ninclude these with the request message. The client then waits on getting a reply from one or the other \nserver. Once it gets a reply, it signals a negative acknowl\u00adedgement to the other server to cancel its \nrequest and then applies the appropriate action function to the reply message. Notice how the interactions \nfor the two servers are intertwined. This property 2 By condition variable, we mean a write-once unit-valued \nsynchronization variable. Waiting on the variable blocks the calling thread until it is signaled by some \nother thread. Once a variable has been signaled, waiting on it no longer blocks. Figure 1. A possible \ninteraction between a client and two servers let val replCh1 = channel() and nack1 = cvar() val replCh2 \n= channel() and nack2 = cvar() in send (reqCh1, (req1, replCh1, nack1)); send (reqCh2, (req2, replCh2, \nnack2)); select [ (replCh1, fn repl1 => ( signal nack2; action1 repl1)), (replCh2, fn repl2 => ( signal \nnack1; action2 repl2)) ] end Figure 2. Implementing interaction with two servers type a event val recvEvt \n: a chan -> a event val sendEvt : ( a chan * a) -> unit event val never : a event val always : a -> a \nevent val choose : ( a event * a event) -> a event val wrap : a event * ( a -> b) -> b event val guard \n: (unit -> a event) -> a event val withNack : (unit event -> a event) -> a event val sync : a event -> \na Figure 3. CML s event API makes the code harder to read and maintain. Furthermore, adding a third \nor fourth server would greatly increase the code s complexity. The standard Computer Science solution \nfor this kind of prob\u00adlem is to create an abstraction mechanism. CML follows this ap\u00adproach by making \nsynchronous operations .rst class. These val\u00adues are called event values and are used to support more \ncompli\u00adcated interactions between threads in a modular fashion. Figure 3 gives the signature for this \nmechanism. Base events constructed by sendEvt and recvEvt describe simple communications on channels. \nThere are also two special base-events: never, which is never enabled and always, which is always enabled \nfor syn\u00ad datatype server = S of (req * repl chan * unit event) chan fun rpcEvt (S ch, req) = withNack \n( fn nack => let val replCh = channel() in send (ch, (req, replCh, nack)); recvEvt replCh end) Figure \n4. The implementation of rpcEvt chronization. These events can be combined into more complicated event \nvalues using the event combinators: Event wrappers (wrap) for post-synchronization actions.  Event \ngenerators (combinators guard and withNack) for pre-synchronization actions and cancellation (withNack). \n Choice (choose) for managing multiple communications. In CML, this combinator takes a list of events \nas its argument, but we restrict it to be a binary operator here. Choice of a list of events can be constructed \nusing choose as a cons operator and never as nil.  To use an event value for synchronization, we apply \nthe sync operator to it. Event values are pure values similar to function values. When the sync operation \nis applied to an event value, a dynamic instance of the event is created, which we call a synchronization \nevent.A single event value can be synchronized on many times, but each time involves a unique synchronization \nevent. Returning to our client-server example, we can now isolate the client-side of the protocol behind \nan event-valued abstraction. type server type req = ... type repl = ... val rpcEvt : (server * req) -> \nrepl event With this interface, the client-side code becomes much cleaner sync (choose ( wrap (rpcEvt \n(server1, req1), action1), wrap (rpcEvt (server2, req2), action2) )) The implementation of the rpcEvt \nfunction is also straightfor\u00adward and is given in Figure 4. Most importantly, the details of the client-server \nprotocol are now hidden behind an abstraction, which improves the code s readability, modularity, and \nmaintainability. 3. The single-threaded implementation Our parallel protocol has a similar high-level \nstructure and event\u00adrepresentation as the original single-threaded implementation of CML [Rep99]. In \nthis section, we review these aspects of the single-threaded design to set the stage for the next section. \n 3.1 Event representation An event value is represented as a binary tree, where the leaves are wrapped \nbase-event values and the interior nodes are choice operators.3 This canonical representation of events \nrelies on the 3 Strictly speaking, the CML implementation represents events as a two\u00adlevel tree, where \nthe root is a list of base-events, but we are treating choice as a binary operator in this paper. following \nequivalences for pushing wrapper functions to the leaves: wrap(wrap(ev,g),f)= wrap(ev,f . g) wrap(choose(ev1, \nev2),f1)= choose(wrap(ev1,f), wrap(ev2,f)) Figure 5 illustrates the mapping from a nesting of wrap and \nchoose combinators to its canonical representation. 3.2 Synchronization The heart of the implementation \nis the protocol for synchronization on a choice of events. The implementation of this protocol is split \nbetween the sync operator and the base-event constructors (e.g., sendEvt and recvEvt). As described above, \nthe base events are the leaves of the event representation. Each base event is a record of three functions: \npollFn, which tests to see if the base-event is enabled (e.g., there is a message waiting); doFn, which \nis used to synchronize on an enabled event; and blockFn, which is used to block the calling thread on \nthe base event. In the single-threaded implementation of CML, we rely heavily on the fact that sync is \nexecuted as an atomic operation. The single-threaded protocol is as follows: 1. Poll the base events \nin the choice to see if any of them are enabled. This phase is called the polling phase. 2. If one or \nmore base events are enabled, pick one and synchro\u00adnize on it using its doFn. This phase is called the \ncommit phase. 3. If no base events are enabled we execute the blocking phase, which has the following \nsteps: (a) Enqueue a continuation for the calling thread on each of the base events using its blockFn. \n (b) Switch to some other thread. (c) Eventually, some other thread will complete the synchro\u00adnization. \n   Because the implementation of sync is atomic, the single-threaded implementation does not have to \nworry about the state of a base event changing between when we poll it and when we invoke the doFn or \nblockFn on it. 4. Issues There are a number of challenges that must be met in the design of a protocol \nfor CML synchronization. One issue that implementations must address is that a given event may involve \nmultiple occurrences of the same channel. For example, the following code nondeterministicly tags the \nmessage received from ch with either 1 or 2: sync (choose ( wrap (recvEvt ch, fn x => (1, x)), wrap (recvEvt \nch, fn y => (2, y)) )) A na\u00a8ive implementation might lock all of the channels involved in a synchronization, \nwhich would result in deadlock, unless reentrant locks were used. One must also avoid deadlock when multiple \nthreads are simul\u00adtaneously attempting communication on the same channel. For ex\u00adample, if thread P is \nexecuting sync (choose (recvEvt ch1, recvEvt ch2)) at the same time that thread Q is executing sync \n(choose (recvEvt ch2, recvEvt ch1)) we have a potential deadlock if the implementation of sync at\u00adtempts \nto hold a lock on both channels simultaneously (i.e., where  Figure 5. The canonical-event transformation \ntype a evt val choose : ( a evt * a evt) -> a evt val wrap : a evt * ( a -> b) -> b evt val sync : a \nevt -> a type a chan val recvEvt : a chan -> a evt val sendEvt : ( a chan * a) -> unit evt Figure 6. \nPrimitive CML operations P holds the lock on ch1 and attempts to lock ch2, while Q holds the lock on \nch2 and attempts to lock ch1). Another problem is that a thread can both offer to send and receive on \nthe same channel at the same time as in this example: sync (choose ( wrap (recvEvt ch, fn x => SOME x), \nwrap (sendEvt ch, fn () => NONE) )) In this case, it is important that the implementation not allow these \ntwo communications to match.4 Lastly, the implementation of the withNack combinator re\u00adquires fairly \ntricky bookkeeping. Fortunately, it is possible to im\u00adplement the full set of CML combinators on top \nof a much smaller kernel of operations, which we call primitive CML. While imple\u00admenting primitive CML \non a multiprocessor is challenging, it is signi.cantly simpler than a monolithic implementation. The \nsigna\u00adture of this subset is given in Figure 6. 5 To support full CML, we use an approach that was suggested \nby Matthew Fluet [DF06]. His idea is to move the bookkeeping used to track negative acknowledgments out \nof the implementation of sync and into guards and wrappers. In this implementation, negative ac\u00adknowledgments \nare signaled using the condition variables (cvars) described earlier. Since we must create these variables \nat synchro\u00adnization time, we represent events as suspended computations (or thunks). The event type has \nthe following de.nition: datatype a event = E of (cvar list * (cvar list * a thunk) evt) thunk where \nthe thunk type is type a thunk = unit -> a. The outermost thunk is a suspension used to delay the evaluation \n4 This problem was not an issue for the asymmetric protocol described in our previous work [RX08]. 5 \nThis subset is equivalent to the original version of .rst-class synchronous operations that appeared \nin the PML language [Rep88].  of guards until synchronization time. When evaluated, it produces a list \nof cvars and a primitive event. The cvars are used to signal the negative acknowledgments for the event. \nThe primitive event, when synchronized, will yield a list of those cvars that need to be signaled and \na thunk that is the suspended wrapper action for the event. More details of this implementation can be \nfound in our previous paper [RX08]. 5. An optimistic protocol for CML In this section, we present our \nmain result, which is our protocol for CML synchronization on shared-memory multiprocessors. Our approach \nto avoiding the pitfalls described above is to use an optimistic protocol that does not hold a lock on \nmore than one channel at a time and avoids locking whenever possible. The basic protocol has a similar \nstructure to the sequential one described above, but it must deal with the fact that the state of a base \nevent can change during the protocol. This fact means that the commit phase may fail and that the blocking \nphase may commit. As before, the synchronization protocol is split between the sync operator and the \nbase events. The sync operator executes the following algorithm: 1. The protocol starts with the polling \nphase, which is done in a lock-free way. 2. If one or more base events are enabled, pick one and attempt \nto synchronize on it using its doFn. This attempt may fail because of changes in the base-event state \nsince the polling was done. We repeat until either we successfully commit to an event or we run out of \nenabled events. 3. If there are no enabled base events (or all attempts to synchro\u00adnize failed), we \nenqueue a continuation for the calling thread on each of the base events using its blockFn. When block\u00ading \nthe thread on a particular base event, we may discover that synchronization is now possible, in which \ncase we attempt to synchronize immediately.  This design is guided by the goal of minimizing synchronization \noverhead and maximizing concurrency. The implementations of the doFn and blockFn for a partic\u00adular base-event \nconstructor depend on the details of the underly\u00ading communication object, but we can describe the synchroniza\u00adtion \nlogic of these operations as state diagrams that abstract away the details of the implementation. For \neach dynamic instance of a synchronization, we create an event-state variable that we use to track the \nstate of the protocol. This variable has one of three states:  Figure 7. Allowed event-state-variable \ntransitions Figure 8. State diagram for the doFn protocol WAITING this is the initial state and signi.es \nthat the event is available for synchronization. CLAIMED this value signi.es that the owner of the event \nis attempting to complete a synchronization. SYNCHED this value signi.es that the event has been synchro\u00adnized. \nThe state variable is supplied to the blockFn during the blocking phase and is stored in the waiting \nqueues, etc. of the communica\u00adtion objects. Figure 7 shows the state transitions that are allowed for \nan event-state variable. This diagram illustrates an important property of state variables: a variable \nmay change from WAITING to SYNCHED at any time (once it is made visible to other threads), but a CLAIMED \nvariable is only changed by its owner. An important property of the commit phase is that the event state \nhas not yet been published to other threads, so it cannot change asynchronously. This fact means that \nthe doFn part of the protocol is fairly simple, as is shown in Figure 8. We use W, C, and S to represent \nthe event state values in this diagram; states are represented as ovals, actions as rectangles, and atomic \ncompare\u00adand-swap (CAS) tests as diamonds. The outgoing edges from a CAS are labelled with the cell s \ntested value. The .rst step is to attempt to get a match from the communication object. We expect that \nsuch an object exists, because of the polling results, but it might have been consumed before the doFn \nwas called. Assuming that it is present, however, and that it has state variable b, we attempt to synchronize \non the potential match. We then attempt to change its Figure 9. State diagram for the blockFn protocol \n state from WAITING to SYNCHED using a CAS instruction. There are three possibilities: b = WAITING: in \nthis case, the CAS will have changed b to SYNCHED and the doFn has successfully committed the syn\u00ad chronization. \n b = CLAIMED: in this case, the owner is trying to synchronize on some other base event that is associated \nwith b, so we spin until either it succeeds or fails. b = SYNCHED: in this case, the event is already \nsynchronized, so we try to get another event. The state diagram for the blockFns is more complicated \nbe\u00adcause the state variable for the event may already be enqueued on some other communication object. \nFor example, consider the case where thread A executes the synchronization sync (choose (recvEvt ch1, \nrecvEvt ch2)) Assuming that A calls the blockFn for ch1 .rst, then some other thread may be attempting \nto send A a message on ch1 while A is attempting to receive a message on ch2. Figure 9 gives the state \ndiagram for a thread with event-state variable a, attempting to match a communication being offered by \na thread with event\u00adstate variable b. As with the doFn diagram, we start by attempting to get an item \nfrom the communication object. Given such an item, with state variable b, we attempt to set our own state \nvariable a to CLAIMED to prevent other threads from synchronizing on our event. We use a CAS operation \nto do so and there are two possible situations: a = WAITING: in this case, the CAS will have changed \na to CLAIMED and we continue with the protocol. a = SYNCHED: in this case, A s event has already been \nsynchro\u00adnized and we can schedule some other thread to run, but before type a queue val queue : unit \n-> a queue val isEmpty : a queue -> bool val enqueue : ( a queue * a) -> unit val dequeue : a queue -> \na option val dequeueMatch : ( a queue * ( a -> bool)) -> a option val undequeue : ( a * a queue) -> unit \nFigure 10. Speci.cation of queue operations doing so, we need to put b back into the communication object \ns queue. Once we have successfully set a to CLAIMED, we know that its value will not be changed by another \nthread. At this point, we attempt to change b from WAITING to SYNCHED as we did in the doFn diagram. \nThere are three possibilities: b = WAITING: in this case, the CAS will have changed b to SYNCHED, so \nwe set a to SYNCHED to mark that we have successfully committed the synchronization. b = CLAIMED: in \nthis case, the owner is trying to synchronize on some other base event that is associated with b, so \nwe reset a to WAITING and spin try to match b again. b = SYNCHED: in this case, the event is already \nsynchronized, so we reset a to WAITING and try to get another event. This protocol is somewhat similar \nto a two-variable STM transac\u00adtion, except that we do not need a read log, since we never reset b s value \nand we always reset a to WAITING when rolling back. 6. A reference implementation To make the protocol \nmore concrete, we present key excerpts from our reference implementation in this section. 6.1 Preliminaries \nWe present our reference implementation using SML syntax with a few extensions. To streamline the presentation, \nwe elide several aspects that an actual implementation must address, such as thread IDs and processor \naf.nity, but we discuss these in Section 8. 6.1.1 Queues Our implementation uses queues to track pending \nmessages and waiting threads in channels. We omit the implementation details here, but give the interface \nto the queue operations that we use in Figure 10. Most of these operations are standard and have the \nexpected semantics, but the last two are less common. The dequeueMatch function dequeues the .rst element \nof the queue that satis.es the given predicate and the undequeue operation pushes an item onto the front \nof the queue.  6.1.2 Threads and thread scheduling As in the uniprocessor implementation of CML, we \nuse .rst-class continuations to implement threads and thread-scheduling. The continuation operations \nhave the following speci.cation: type a cont val callcc : ( a cont -> a) -> a val throw : a cont -> a \n-> b We represent the state of a suspended thread as a continuation: type thread = unit cont The interface \nto the scheduling system is two atomic operations: val enqueueRdy : thread -> unit val dispatch : unit \n-> a The .rst enqueues a ready thread in the scheduling queue and the second transfers control to the \nnext ready thread in the queue. 6.1.3 Low-level synchronization Our implementation also relies on the \natomic compare-and-swap instruction. We also assume the existence of spin locks. These low\u00adlevel operations \nhave the following interface: val CAS : ( a ref * a * a) -> a type spin_lock val spinLock : spin_lock \n-> unit val spinUnlock : spin_lock -> unit  6.2 The representation of events We start with the representation \nof events and event-states: datatype event_status = WAITING | CLAIMED | SYNCHED type event_state = event_status \nref datatype a evt = BEVT of { pollFn : unit -> bool, doFn : a cont -> unit, blockFn : (event_state * \na cont) -> unit } | CHOOSE of a evt * a evt In our reference implementation we use .rst-class continuations \nto represent thread state. Notice that both the doFn and blockFn functions take a continuation argument. \nThis continuation is the resume continuation for when the event is synchronized on. 6.3 Implementing \nsync The sync operation is given in Figure 11 and directly follows the logic described in the previous \nsection. It starts with a polling phase, then attempts to commit on any enabled events, and, failing \nthat, blocks the thread on the base events. The main omitted detail is that it passes its return continuation \nas an argument to the doFn and blockFn calls. Note that we also allocate a new event-state variable that \nis passed into the blockFn calls. It is worth noting that we implement the sync operation as a single \npass of invoking the blockFn for each base event. The problem with this approach is that it implements \na biased choice that always favors the left alternative over the right. Although we do not describe it \nhere, the structure that we use allows us to support priorities and/or fairness mechanisms for choice \n(see Chapter 10 of [Rep99] for more discussion). 6.4 Implementing wrap The implementation of wrap, given \nin Figure 12, is not directly in\u00advolved in the synchronization protocol, but it is responsible for maintaining \nthe canonical representation of event values. The wrap function pushes its action argument f to the leaves \nof the event, where it composes f with the base event s doFn and blockFn functions. This composition \nrequires some horrible con\u00adtinuation hacking to implement. 6.5 Implementing sendEvt To illustrate how \nthe synchronization protocol works in a concrete example, we examine the reference code for the sendEvt \nevent base-event constructor (the recvEvt function follows the same synchronization pattern). This operation \nworks on the following representation of channels: fun sync ev = callcc (fn resumeK => let (* optimistically \npoll the base events *) fun poll (BEVT{pollFn, doFn, ...}, enabled) = if pollFn() then doFn::enabled \nelse enabled | poll (CHOOSE(ev1, ev2), enabled) = poll(ev2, poll(ev1, enabled)) (* attempt an enabled \ncommunication *) fun doEvt [] = blockThd() | doEvt (doFn::r) = ( doFn resumeK; (* if we get here, that \nmeans that the *) (* attempt failed, so try the next one *) doEvt r) (* record the calling thread s continuation \n*) and blockThd () = let val flg = ref WAITING fun block (BEVT{blockFn, ...}) = blockFn (flg, resumeK) \n| block (CHOOSE(ev1, ev2)) = ( block ev1; block ev2) in block ev; dispatch () end in doEvt (poll (ev, \n[])) end) Figure 11. The reference implementation of sync fun wrap (BEVT{pollFn, doFn, blockFn}, f) \n= BEVT{ pollFn = pollFn, doFn = fn k => callcc (fn retK => throw k (f (callcc (fn k => (doFn k ; throw \nretK ()))))), blockFn = fn (flg, k) => callcc (fn retK => throw k (f (callcc (fn k => (blockFn(flg, k \n); throw retK ()))))) } | wrap (CHOOSE(ev1, ev2), f) = CHOOSE(wrap(ev1, f), wrap(ev2, f)) Figure 12. \nThe reference implementation of wrap datatype a chan = Ch of { lock : spin_lock, sendq : (event_state \n* a * unit cont) queue, recvq : (event_state * a cont) queue } Each channel has a pair of queues: one \nfor waiting senders and one for waiting receivers. It also has a spin lock that we use to protect the \nqueues. It is important to note that we only lock one channel at a time, which avoids the problem of \ndeadlock. The high-level structure of the sendEvt function is fun sendEvt (Ch{lock, sendq, recvq, ...}, \nmsg) = let fun pollFn () = not(isEmpty recvq) fun doFn k = ... fun blockFn (myFlg : event_state, k) = \n... in BEVT{ pollFn = pollFn, doFn = doFn, blockFn = blockFn} end fun doFn k = let fun tryLp () = (case \ndequeue recvq of NONE => spinUnlock lock | SOME(flg, recvK) => let fun matchLp () = ( case CAS (flg, \nWAITING, SYNCHED) of WAITING => ( spinUnlock lock; enqueueRdy k; throw recvK msg) | CLAIMED => matchLp \n() | _ => tryLp () (* end case *)) in if (deref flag <> SYNCHED) then matchLp () else tryLp () end (* \nend case *)) in spinLock lock; tryLp () end Figure 13. The sendEvt doFn code It de.nes the three base-event \nfunctions for the operation and makes an event value out of them. Note that the polling func\u00adtion just \ntests to see if the queue of waiting receivers is not empty. There is no point in locking this operation, \nsince the state may change before the doFn is invoked. The bulk of the sendEvt implementation is in the \ndoFn and blockFn functions, which are given in Figures 13 and 14 respec\u00adtively. The doFn implementation \nconsists of a single loop (tryLp) that corresponds to the cycle in Figure 8. If the doFn is success\u00adful \nin matching a receive operation, it enqueues the sender in the ready queue and throws the message to \nthe receiver s resumption continuation. The blockFn code also follows the corresponding state diagram \nclosely. It consists of two nested loops. The outer loop (tryLp) corresponds to the left-hand-side cycle \nin Figure 9, while the inner loop (matchLp) corresponds to the right-hand-side cycle. 6.6 Asymmetric \noperations In addition to synchronous message passing, CML provides a number of other communication primitives. \nThese primitives have the property that they involve only one active thread at a time (as is the case \nfor asymmetric-CML), which simpli.es synchronization. In Figure 15, we give the reference implementation \nfor the cvar type and waitEvt event constructor. In this case, the doFn is trivial, since once a cvar \nhas been signaled its state does not change. The blockFn is also much simplier, because there is only \none event-state variable involved. 7. Verifying the protocol Designing and implementing a correct protocol, \nsuch as the one described in this paper, is very hard. To increase our con.dence in the protocol design, \nwe have used stateless model checking to verify the reference implementation. Our approach is based on \nthe ideas of the CHESS model checker [MQ07], but we built our own tool tailored to our problem. We used \nthis tool to guide the design of the protocol; in the process, we uncovered several bugs and missteps \nin the design that we were able to correct. Our approach to model checking was to implement a virtual \nma\u00adchine in SML that supported a scheduling infrastructure and mem\u00adory cells with both atomic and non-atomic \noperations. The imple\u00ad fun blockFn (myFlg : event_state, k) = let datatype cvar = CV of { fun notMe \n(flg , _, _) = not(same(myFlg, flg )) lock : spin_lock, fun tryLp () = (case dequeueMatch (recvq, notMe) \nstate : bool ref, of SOME(flg , recvK) => let waiting : (event_state * thread) list ref (* a receiver \nblocked since we polled *) } (case CAS (flg , WAITING, SYNCHED) of WAITING => ( (* we got it! *) spinUnlock \nlock; myFlg := SYNCHED; enqueueRdy k; throw recvK msg) | CLAIMED => ( myFlg := WAITING; matchLp ()) | \nSYNCHED => ( myFlg := WAITING; tryLp ()) (* end case *)) | sts => ( undequeue ((flg , recvK), recvq); \nspinUnlock lock; dispatch ()) (* end case *)) in if (!flg <> SYNCHED) then matchLp () else tryLp () end \n| NONE => ( enqueue (sendq, (myFlg, msg, k)); spinUnlock lock) (* end case *)) in spinLock lock; tryLp \n() end Figure 14. The sendEvt blockFn code mentation of the virtual machine operations are allowed to \ninject preemptions into the computation. We used SML/NJ s .rst-class continuations to implement a roll-back \nfacility that allowed both the preempted and non-preempted execution paths to be explored. To keep the \nnumber of paths explored to a tractable number, we bound the number of preemptions to 3 on any given \ntrace.6 Our reference implementation was then coded as a functor over the virtual machine API. On top \nof this we wrote a number of test cases that we ran through the checker. These tests required explor\u00ading \nanywhere from 20,000 to over one million distinct execution traces. Our experience with this tool was \nvery positive. Using this tool exposed both a bug in the basic design of our protocol and a couple of \nfailures to handle various corner cases. We strongly recommend such automated testing approaches to developers \nof concurrent language implementations. Perhaps the best proof of its usefulness is that when we ported \nthe reference implementation to the Manticore runtime system, it worked out of the box. 6 Experience \nshows that bounding the preemptive context switches is an effective way to reduce the state space, while \nstill uncovering many concur\u00adrency bugs [MQ07]. fun matchLp () = ( fun waitEvt (CV{lock, state, waiting}) \n= let case CAS(myFlg, WAITING, CLAIMED) fun pollFn () = !state of WAITING => ( fun doFn k = throw k () \n (* try to claim the matching event *) fun blockFn (flg : waitK) = event_state, spinLock lock; if !state \n then ( spinUnlock lock; case CAS(flg, WAITING, SYNCHED) of WAITING => throw waitK ()) | _ => dispatch \n() (* end case *)) else let val wl = !waiting in waiting := (flg, waitK) :: wl; spinUnlock lock end) \nin BEVT{ pollFn = pollFn, doFn = doFn, blockFn = blockFn} end Figure 15. The waitEvt event constructor \n 8. The messy bits To keep the sample code clean and uncluttered, we have omitted several implementation \ndetails that we discuss in this section. 8.1 Locally-atomic operations This implementation uses spin-lock-style \nsynchronization at the lowest level. One problem with spin locks is that if a lock-holder is preempted \nand a thread on another processor attempts to access the lock, the second thread will spin until the \n.rst thread is rescheduled and releases the lock. To avoid this problem, the Manticore runtime provides \na lightweight mechanism to mask local preemptions. We run sync as a locally-atomic operation, which has \ntwo bene.ts. One is that threads do not get preempted when holding a spin lock. The second is that certain \nscheduling structures, such as the per\u00adprocessor thread queue, are only by the owning processor and, \nthus, can be accessed without locking. 8.2 Thread af.nity In the above implementation, we assume a single, \nglobal, schedul\u00ading queue for threads that are ready to run. In the Manticore runtime system, however, \nthere is a separate thread queue for each proces\u00adsor. If a thread on processor P blocks on sending a \nmessage and then a thread on processor Q wakes it up by receiving the message, we want the sender to \nbe rescheduled on P s queue. To this end, we include the thread s host processor in the blocking information. \n 8.3 Avoiding space leaks Another issue that the implementation must deal with is removing the dead elements \nfrom channel waiting queues. While setting the event-state .ag to SYNCHED marks a queue item as dead, \nit does not get removed from the waiting queue. Consider the following loop: fun lp () = sync (choose \n( wrap (recvEvt ch1, fn x => ...), wrap (recvEvt ch2, fn x => ...)))  If there is a regular stream of \nmessages on channel ch1, but never a sender on channel ch2, the waiting-sender queue for channel ch2 \nwill grow longer and longer. To .x this problem, we need to remove dead items from the waiting queues \non insert. Since scanning a queue for dead items is a potentially expensive operation, we want to scan \nonly occasionally. To achieve this goal, we add two counters to the representation of a waiting queue. \nThe .rst keeps track of the number of elements in the queue and the second de.nes a threshold for scanning. \nWhen inserting an item, if the number of items in the queue exceeds the threshold, then we scan the queue \nto remove dead items. We then reset the threshold to max(n+k1,k2 *n), where n is the number of remaining \nitems, and k1 and k2 are tuning parameters.7 For actively used channels with few senders and receivers, \nthe threshold is never exceeded and we avoid scanning. For actively used channels that have large numbers \nof senders and receivers, the threshold will grow to accommodate the larger number of waiting threads \nand will subsequently not be exceeded. But for channels, like ch2 above, that have many dead items, the \nthreshold will stay low (equal to k1) and the queues will not grow without bound. One should note that \nthere is still the possibility that large data objects can be retained past their lifetime by being inserted \ninto a queue that is only rarely used (and doesn t exceed its threshold). We could address this issue \nby making the garbage collector aware of the structure of queue items, so that the data-pointer of a \ndead item could be nulli.ed, but we do not believe that this problem is likely and worth the extra implementation \ncomplexity.  8.4 Reducing bus traf.c In the reference implementation, we often spin on tight loops per\u00adforming \nCAS instructions. In practice, such loops perform badly, because of the bus traf.c created by the CAS \noperation. It is gener\u00adally recommended to spin on non-atomic operations (e.g., loads and conditionals) \nuntil it appears that the CAS will succeed [HS08]. 9. Parallel implementations of the protocol In Section \n6, we presented a reference implementation of the CML synchronization protocol described in Section 5. \nWe have translated this reference implementation into two parallel implementations. One is a continuation-based \nimplementation as part of the Manti\u00adcore system [FFR+07]. Although very different in detail, both de\u00adrive \nfrom the same design. In this section, we describe some spe\u00adci.c aspects of these translations. We report \non the performance of the Manticore and C# implementations in Section 10. 9.1 A continuation-based implementation \nThe Manticore implementation is written in a low-level functional language that serves as one of the \nintermediate representations of our compiler. This language can be viewed as a stripped-down ver\u00adsion \nof ML with a few extensions. Speci.cally, it supports .rst\u00adclass continuations via a continuation binder \nand it provides ac\u00adcess to mutable memory objects8 and operations (including CAS). While the actual code \nis more verbose, the translation from the ref\u00aderence implementation was direct. The Manticore runtime \nsystem is designed to emphasize sepa\u00adration between processors [FRR08]. While this design helps with \nscalability, it does impose certain burdens on the implementation of the CML primitives. One aspect is \nthat each processor has its own local scheduling queue, which other processors are not al\u00adlowed to access. \nThus, to schedule a thread on a remote proces\u00adsor requires pushing it on a concurrent stack that each \nprocessor 7 We currently set k1 = 10 and k2 =1.5. 8 Manticore s surface language does not have mutable \nstorage. maintains (called the landing pad) and then waiting until the re\u00admote processor notices it \nand schedules it. The effect of this design is that message passing and remote thread creation have increased \nlatency (cf. Section 10). 9.2 A thread-based implementation Although we described our CML implementation \nelegantly using .rst-class continuations, their use is by no means essential. Any continuations are used \nat most once and can readily be replaced by calls to threading primitives. To demonstrate this claim, \nwe implemented a version of Parallel CML in C# [TG2] running on Microsoft s Common Language Runtime [CLR]. \nThe CLR does not support .rst-class continuations but can make use of parallel hardware. The framework \nlibraries provide access to low-level synchronization primitives such as CAS, spin waiting and volatile \nreads and writes of machine words. This is in addition to the expected higher-level synchronization constructs \nsuch as CLR monitors that ultimately map to OS resources. The CLR thus provides a useful test-bed for \nour algorithms. CML s event constructors have a natural and unsurprising trans\u00adlation to C# classes deriving \nfrom an abstract base class of events. The main challenge in translating our CML reference implemen\u00adtation \nlies in eliminating uses of callcc. However, since CML only uses a value of type a cont to denote a suspended \ncom\u00adputation waiting to be thrown some value, we can represent these continuations as values of the following \nabstract class: internal abstract class Cont<T> { internal void Throw(T res) { Throw(() => res); } internal \nabstract void Throw(Thunk<T> res); } Here, Thunk<T> is the type of a .rst-class method -a delegate -with \nno argument and return type T. In general, the thrown value res will be a delayed computation of type \nThunk<T> to accommodate the composition of post-synchronization functions using wrap -these must be lazily \ncomposed then executed on the receiving end of a synchronization. Now we can capture a waiting thread \nusing a concrete subclass of Cont<T>: internal class SyncCont<T> : Cont<T> { private Thunk<T> res; private \nbool Thrown; internal override void Throw(Thunk<T> res) { lock (this){ this.res = res; Thrown = true; \nMonitor.Pulse(this); } } internal virtual T Wait() { lock (this){ while (!Thrown) Monitor.Wait(this); \n} return res(); } } In order to suspend itself, a thread allocates a new SyncCont<T> value, k, does \nsome work, and eventually calls k.Wait() to receive the result res() of this or another thread s intervening \nor future call to k.Throw(res): k is essentially a condition variable carrying a suspended computation. \nFor example, consider the callcc-based SML implementa\u00adtion of sync in Figure 11. Note that the current \ncontinuation resumeK, that encloses the entire body of sync ev, is just to return to the caller of sync. \nThe call to doFn will either trans\u00adfer control to the outer resumeK continuation once, when suc\u00adcessful, \nor return if it fails. Similarly, the blockFn may com\u00adplete synchronization, transferring control to \nresumeK, or return; in which case the call to sync ev blocks by entering the sched\u00aduler to dispatch another \nthread. Finally, the scheduler ensures that at most one thread will continue with resumeK. This is our \nC# implementation of method Sync: public abstract class Evt<T> { internal abstract List<BEVT<T>> Poll(List<BEVT<T>> \nenabled);  internal abstract bool Block(Evt_State state, Cont<T> resumeK); public T Sync() { List<BEVT<T>> \nenabled = Poll(null); Tt= default(T); while (enabled != null){ if (enabled.head.DoFn(ref t)) return t; \nenabled = enabled.tail; } var resumeK = new SyncCont<T>(); Block(new Evt_State(), resumeK); return resumeK.Wait(); \n} } The DoFn(ref t) method call cannot directly transfer control when it succeeds -unlike the CML doFn \nresumeK; application in Figure 11. Instead, DoFn returns true to indicate a successful commit, or false \nto indicate commit-failure. As a side-effect, it also updates the location t with any T-result that its \ncaller should return. If the commit phase succeeds, the code simply returns the value of t and skips \nthe blocking phase. Otherwise, it allocates a new SyncCont<T> instance, resumeK, queues resumeK on all \nthe base events and exits with a call to resumeK.Wait(), blocking unless the Block call managed to commit. \nNotice that, unlike the CML code for sync, the C# code delays creating a resumeK continuation until the \ncommit phase is known to have failed, avoiding the additional heap-allocation, synchronization and potential \ncontext switch inherent in a more direct translation of the callcc-based code. In the message-passing \nbenchmark of Section 10.4, this optimization improves performance by at least 10% over the literal translation \nof the reference implementation. Since CLR threads are expensive operating system threads, it is useful \nto avoid the overhead of blocking by using asynchronous calls when possible. To this end, we extended \nthe CML event sig\u00adnature with an additional Async operation that, instead of blocking on the return of \na value, immediately queues a callback that takes a value, to be invoked as a CLR task on completion \nof the event.9 Enabling this requires a new class of continuations whose Throw method queues a CLR task \nbut that has no Wait method: internal class AsyncCont<T> : Cont<T> { private Action<T> k; internal AsyncCont(Action<T> \nk) { this.k = k; } internal override void Throw(Thunk<T> res) { P.QueueTask(() => k(res())); } } The \ncode for method Async(k) takes a continuation action k and follows the same logic as Sync: public void \nAsync(Action<T> k) { List<BEVT<T>> enabled = Poll(null); Tt= default(T); while (enabled != null){ if \n(enabled.head.DoFn(ref t)) { QueueTask(() => k(t)); return; } enabled = enabled.tail; 9 A full implementation \nwould also need to take a failure callback and properly plumb exceptions in the body of method Async(k). \nSpawn benchmark System Threads/sec. Ratio CML 2,628,000 1.00 Manticore (1P) 1,235,000 0.47 Manticore \n(2P) 330,300 0.13 Ping-pong benchmark System Messages/sec. Ratio CML 1,608,000 1.00 Manticore (1P) 697,800 \n0.43 Manticore (2P) 271,400 0.17 Ping-pong benchmark Figure 16. Micro-benchmark results  } var resumeK \n= new AsyncCont<T>(k); Block(new Evt_State(), resumeK); } Here, Action<T> is the type of a .rst-class \nmethod expecting a T argument that returns void. Instead of returning t or blocking on resumeK.Wait(), \nas in the code for Sync(), Async(k) immediately returns control, having either queued () => k(t) as a \nnew asynchronous task or saved k for a future synchronization through a successful call to Block(...,resumeK): \nThe Async method makes it possible to use C# iterators to provide a form of light-weight, user-mode threading. \nAlthough somewhat awkward, iterators let one write non-blocking tasks in a sequential style by yield-ing \ncontrol to a dispatcher that advances the iterator through its states [CS05]. In particular, by yielding \nCML events, and having the dispatcher queue an action to resume the iteration asynchronously on completion \nof each event, we can arrange to multiplex a large number of lightweight tasks over a much smaller set \nof CLR worker threads. 10. Performance This section presents some preliminary benchmark results for our \ntwo implementations. To test the Manticore implementation of the protocol, we compare the results against \nthe CML implementation, which is distributed as part of SML/NJ (Version 110.69). These tests were run \non a system with four 2GHz dual-core AMD Opteron 870 processors and 8Gb of RAM. The system is running \nDebian Linux (kernel version 2.6.18-6-amd64). Each benchmark was run ten times; we report the average \nwall-clock time.    10.1 Micro-benchmarks Our .rst two experiments measure the cost of basic concurrency \noperations: namely, thread creation and message passing. Spawn This program repeatedly spawns a trivial \nthread and then waits for it to terminate. In the two-processor case, the parent thread runs on one machine \nand creates children on the other. Ping-pong This program involves two threads that bounce mes\u00ad sages \nback and forth. In the two-processor case, each thread runs on its own processor. For Manticore, we \nmeasured two versions of these programs: one that runs on a single processor and one that runs on two \nprocessors. Note that these benchmarks do not exhibit parallelism; the two\u00adprocessor version is designed \nto measure the extra overhead of working across processors (see Section 9.1). The results for these experiments \nare given in Figure 16. For each experiment, we report the measured rate and the ratio between the measured \nrate and the CML version (a higher ratio is better). As can be seen from these numbers, the cost of scheduling \nthreads on remote processors is signi.cantly higher.  10.2 Parallel ping-pong While the above programs \ndo not exhibit parallelism, it is possible to run multiple copies of them in parallel, which is predictor \nof aggragate performance across a large collection of independently communicating threads. We ran eight \ncopies (i.e., 16 threads) of the ping-pong benchmark simultaneously. For the multiprocessor version, \neach thread of a communicating pair was assigned to a different processor. System Messages/sec. Ratio \n(vs. CML) (vs. 1P) CML 1,576,000 1.00 Manticore (1P) 724,000 0.46 1.00 Manticore (2P) 412,000 0.26 0.57 \nManticore (4P) 734,000 0.47 1.01 Manticore (8P) 1,000,000 0.63 1.38 As expected, this benchmark demonstrates \nthat we will get speedups on parallel hardware when computations are independent. It is worth noting \nthat if we had assigned pairs of communicating threads to the same processor (instead of different ones), \nwe would expect even better results, since we would not be paying the inter\u00adprocessor communication overhead. \n 10.3 Primes The Primes benchmark computes the .rst 2000 prime numbers using the Sieve of Erastothenes \nalgorithm. The computation is structured as a pipeline of .lter threads as each new prime is found, a \nnew .lter thread is added to the end of pipeline. We ran both single and multiprocessor versions of the \nprogram; the .lters were assigned in a round-robin fashion. We report the time and speedup relative to \nthe CML version in the following table: System Time (sec.) Speedup (vs. CML) (vs. 1P) CML 1.34 1.00 Manticore \n(1P) 3.08 0.43 1.00 Manticore (2P) 3.37 0.40 0.91 Manticore (4P) 1.61 0.83 1.91 Manticore (8P) 0.92 1.45 \n3.35 Even though the computation per message is quite low in this program, we see a speed up on multiple \nprocessors.  10.4 C# Performance We also measured the performance of the C# implementation on a system \nwith two 2.33MHz quad-core Intel Xeon E5345 processors and 4GB of memory, running 32-bit Vista Enterprise \nSP1 and CLR 4.0 Beta 1. Each benchmark was run ten times allowing the OS to schedule on 1 to 8 cores; \nwe report the average wall-clock time. Since we have no uniprocessor implementation (such as CML) to \ncompare with, we resort to taking the single-processor runs as our baseline. Our .rst C# benchmark is \nthe parallel ping-pong program from above. The implementations use proper threads synchronising us\u00ading \nblocking calls to Sync. The mapping of threads to processors was left to the OS scheduler. # Procs Messages/sec. \nRatio 1 37,100 1.00 2 68,400 1.84 4 75,000 2.02 8 84,700 2.28  As before, the benchmark demonstrates \nthat we will get speedups on parallel hardware when computations are independent. Our second C# benchmark \nis an asynchronous, task-based im\u00adplementation of the primes benchmark from above. Note that the synchronous \nversion that uses one CLR thread per prime .lter ex\u00adhausts system resources after around 1000 threads \n(as expected), but the task based implementation, written using C# iterators yield\u00ading Evt<Unit> values, \nscales better, handling both larger inputs and bene.ting from more processors. # Procs Time (sec.) Speedup \n1 6.68 1.00 2 4.70 1.42 4 3.07 2.17 8 2.49 2.68   10.5 Summary The results presented in this section \ndemonstrate that the extra over\u00adhead required to support parallel execution (i.e., atomic memory operations \nand more complicated protocols) does not prevent ac\u00adceptable performance. As we would expect, the single-threaded \nim\u00adplementation of CML is much faster than the parallel implemen\u00adtations (e.g., about 2.5 times faster \nthan the Manticore 1P imple\u00admentation). Since the performance of most real applications is not dominated \nby communication costs, we expect that the bene.ts of parallelism will easily outweigh the extra costs \nof the parallel im\u00adplementation. We also expect that improvements in the Manticore compiler, as well \nas optimization techniques for message-passing programs [RX07], will reduce the performance gap between \nthe single-threaded and multi-threaded implementations. These experiments also demonstrate that there \nis a signi.cant cost in communicating across multiple processors in the Manti\u00adcore system. Scheduling \nthreads for the same processor will re\u00adduce message-passing costs. On the other hand, when the two com\u00admunicating \nthreads can compute in parallel, there is an advantage to having them on separate processors. Thus, we \nneed scheduling policies that keep threads on the same processor when they are not concurrent, but distribute \nthem when they are. There is some ex\u00adisting research on this problem by Vella [Vel98] and more recently \nRitson [Rit08] that we may be able to incorporate in the Manticore runtime. 11. Related work Various \nauthors have described implementations of choice proto\u00adcols using message passing as the underlying mechanism \n[BS83, Bor86, Kna92, Dem98]. While these protocols could, in principle, be mapped to a shared-memory \nimplementation, we believe that our approach is both simpler and more ef.cient. Russell described a monadic \nimplementation of CML-style events on top of Concurrent Haskell [Rus01]. His implementa\u00adtion uses Concurrent \nHaskell s M-vars for concurrency control and he uses an ordered two-phase locking scheme to commit to \ncom\u00admunications. A key difference in his implementation is that choice is biased to the left, which means \nthat he can commit immediately to an enabled event during the polling phase. This feature greatly simpli.es \nhis implementation, since it does not have to handle changes in event status between the polling phase \nand the commit phase. Russell s implementation did not support multiprocessors (because Concurrent Haskell \ndid not support them at the time), but presumably would work on a parallel implementation of Concur\u00adrent \nHaskell. Donnelly and Fluet have implemented a version of events that support transactions on top of \nHaskell s STM mecha\u00adnism [DF06]. Their mechanism is quite powerful and, thus, their implementation is \nquite complicated. This paper builds on our previous protocol for asymmetric CML. In addition to generalizing \nthe protocol to handle output guards, this paper provides a more complete story, including veri.\u00adcation, \nmultiple parallel implementations, and performance results. In earlier work, we reported on specialized \nimplementations of CML s channel operations that can be used when program analysis determines that it \nis safe [RX07]. Those specialized implementa\u00adtions .t into our framework and can be regarded as complementary. \n12. Conclusion We have described what we believe to be the .rst ef.cient par\u00adallel implementation of \nCML that supports fully symmetric input and output events. We found the application of state-less model \nchecking to be a valuable tool during the development of the pro\u00adtocol, both uncovering bugs and increasing \nour con.dence in the .nal design of a reasonably intricate and novel synchronization protocol. Our dual \nparallel implementations, both the continuation passing for Manticore and the thread-based implementation \nin C#, demonstrate that the underlying protocols have wider applicability than just Manticore. We evaluated \nthe performance of the contin\u00aduation based implementation and found it within a factor of 2.5 of the \nsingle-threaded implementation. More signi.cantly, the parallel implementation will allows speedups on \nparallel hardware. Inter\u00adesting future work would be to further evaluate the performance of the C# implementation \nand to use Microsoft s CHESS framework to model-check its code. Acknowledgments The extension of the \nasymmetric protocol [RX08] to the symmet\u00adric case was done while the .rst author was a Visiting Researcher \nat Microsoft Research Cambridge. The machine used for the bench\u00admarks was supported by NSF award 0454136. \nThis research was also supported, in part, by NSF award 0811389. Mike Rainey pro\u00advided help with .tting \nthe implementation into the Manticore run\u00adtime infrastructure. References [Bor86] Bornat, R. A protocol \nfor generalized occam. SP&#38;E, 16(9), September 1986, pp. 783 799. [BS83] Buckley, G. N. and A. Silberschatz. \nAn effective implementa\u00ad tion for the generalized input-output construct of CSP. ACM TOPLAS, 5(2), April \n1983, pp. 223 235. [CLR] The .NET Common Language Runtime. See http: //msdn.microsoft.com/en-gb/netframework/. \n[CS05] Chrysanthakopoulos, G. and S. Singh. An asynchronous mes\u00adsaging library for C#. In Synchronization \nand Concurrency in Object-Oriented Languages (SCOOL), OOPSLA 2005 Work\u00adshop. UR Research, October 2005. \n[Dem97] Demaine, E. D. Higher-order concurrency in Java. In WoTUG20, April 1997, pp. 34 47. Available \nfrom http: //theory.csail.mit.edu/ edemaine/papers/ WoTUG20/. [Dem98] Demaine, E. D. Protocols for non-deterministic \ncommu\u00ad nication over synchronous channels. In IPPS/SPDP 98, March 1998, pp. 24 30. Available from http://theory. \ncsail.mit.edu/ edemaine/papers/IPPS98/. [DF06] Donnelly, K. and M. Fluet. Transactional events. In ICFP \n06, Portland, Oregon, USA, 2006. ACM, pp. 124 135. [FF04] Flatt, M. and R. B. Findler. Kill-safe synchronization \nabstractions. In PLDI 04, June 2004, pp. 47 58. [FFR+07] Fluet, M., N. Ford, M. Rainey, J. Reppy, A. \nShaw, and Y. Xiao. Status report: The Manticore project. In ML 07. ACM, October 2007, pp. 15 24. [FRR08] \nFluet, M., M. Rainey, and J. Reppy. A scheduling framework for general-purpose parallel languages. In \nICFP 08, Victoria, BC, Candada, September 2008. ACM, pp. 241 252. [GR93] Gansner, E. R. and J. H. Reppy. \nA Multi-threaded Higher\u00adorder User Interface Toolkit, vol. 1 of Software Trends, pp. 61 80. John Wiley \n&#38; Sons, 1993. [HS08] Herlihy, M. and N. Shavit. The Art of Multiprocessor Programming. Morgan Kaufmann \nPublishers, New York, NY, 2008. [Kna92] Knabe, F. A distributed protocol for channel-based communi\u00adcation \nwith choice. Technical Report ECRC-92-16, European Computer-industry Research Center, October 1992. [Ler00] \nLeroy, X. The Objective Caml System (release 3.00), April 2000. Available from http://caml.inria.fr. \n[MLt] MLton. Concurrent ML. Available at http://mlton. org/ConcurrentML. [MQ07] Musuvathi, M. and S. \nQadeer. Iterative context bounding for systematic testing of multithreaded programs. In PLDI 07, San \nDiego, CA, June 2007. ACM, pp. 446 455. [MTHM97] Milner, R., M. Tofte, R. Harper, and D. MacQueen. The \nDe.nition of Standard ML (Revised). The MIT Press, Cambridge, MA, 1997. [Rep88] Reppy, J. H. Synchronous \noperations as .rst-class values. In PLDI 88, June 1988, pp. 250 259. [Rep91] Reppy, J. H. CML: A higher-order \nconcurrent language. In PLDI 91. ACM, June 1991, pp. 293 305. [Rep99] Reppy, J. H. Concurrent Programming \nin ML. Cambridge University Press, Cambridge, England, 1999. [Rit08] Ritson, C. Multicore scheduling \nfor lightweight communi\u00adcating processes. Talk at the Workshop on Language and Runtime Support for Concurrent \nSystems, October 2008. Slides available from http://www.mm-net.org.uk/ workshop171008/mmw07-slides. [Rus01] \nRussell, G. Events in Haskell, and how to implement them. In ICFP 01, September 2001, pp. 157 168. [RX07] \nReppy, J. and Y. Xiao. Specialization of CML message\u00adpassing primitives. In POPL 07. ACM, January 2007, \npp. 315 326. [RX08] Reppy, J. and Y. Xiao. Toward a parallel implementation of Concurrent ML. In DAMP \n08. ACM, January 2008. [TG2] TG2, E. T. C# language speci.cation. See http:// www.ecma-international.org/publications/ \nstandards/Ecma-334.htm. [Vel98] Vella, K. Seamless parallel computing on heterogeneous networks of multiprocessor \nworkstations. Ph.D. dissertation, University of Kent at Canterbury, December 1998. [YYS+01] Young, C., \nL. YN, T. Szymanski, J. Reppy, R. Pike, G. Narlikar, S. Mullender, and E. Grosse. Protium, an infrastructure \nfor partitioned applications. In HotOS-X, January 2001, pp. 41 46.  \n\t\t\t", "proc_id": "1596550", "abstract": "<p>Concurrent ML (CML) is a high-level message-passing language that supports the construction of first-class synchronous abstractions called events. This mechanism has proven quite effective over the years and has been incorporated in a number of other languages. While CML provides a concurrent programming model, its implementation has always been limited to uniprocessors. This limitation is exploited in the implementation of the synchronization protocol that underlies the event mechanism, but with the advent of cheap parallel processing on the desktop (and laptop), it is time for Parallel CML.</p> <p>Parallel implementations of CML-like primitives for Java and Haskell exist, but build on high-level synchronization constructs that are unlikely to perform well. This paper presents a novel, parallel implementation of CML that exploits a purpose-built optimistic concurrency protocol designed for both correctness and performance on shared-memory multiprocessors. This work extends and completes an earlier protocol that supported just a strict subset of CML with synchronization on input, but not output events. Our main contributions are a model-checked reference implementation of the protocol and two concrete implementations. This paper focuses on Manticore's functional, continuation-based implementation but briefly discusses an independent, thread-based implementation written in C# and running on Microsoft's stock, parallel runtime. Although very different in detail, both derive from the same design. Experimental evaluation of the Manticore implementation reveals good performance, dispite the extra overhead of multiprocessor synchronization.</p>", "authors": [{"name": "John Reppy", "author_profile_id": "81100590527", "affiliation": "University of Chicago, Chicago, IL, USA", "person_id": "P1613940", "email_address": "", "orcid_id": ""}, {"name": "Claudio V. Russo", "author_profile_id": "81100638789", "affiliation": "Microsoft Research, Cambridge, United Kingdom", "person_id": "P1613941", "email_address": "", "orcid_id": ""}, {"name": "Yingqi Xiao", "author_profile_id": "81322510236", "affiliation": "University of Chicago, Chicago, IL, USA", "person_id": "P1613942", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1596550.1596588", "year": "2009", "article_id": "1596588", "conference": "ICFP", "title": "Parallel concurrent ML", "url": "http://dl.acm.org/citation.cfm?id=1596588"}