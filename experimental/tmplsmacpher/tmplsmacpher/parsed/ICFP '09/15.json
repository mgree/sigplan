{"article_publication_date": "08-31-2009", "fulltext": "\n Free Theorems Involving Type Constructor Classes Functional Pearl Janis Voigtl\u00a8ander Institut f\u00a8ur \nTheoretische Informatik Technische Universit\u00a8at Dresden 01062 Dresden, Germany janis.voigtlaender@acm.org \nAbstract Free theorems are a charm, allowing the derivation of useful state\u00adments about programs from \ntheir (polymorphic) types alone. We show how to reap such theorems not only from polymorphism over ordinary \ntypes, but also from polymorphism over type constructors restricted by class constraints. Our prime application \narea is that of monads, which form the probably most popular type constructor class of Haskell. To demonstrate \nthe broader scope, we also deal with a transparent way of introducing difference lists into a pro\u00adgram, \nendowed with a neat and general correctness proof. Categories and Subject Descriptors F.3.1 [Logics and \nMean\u00adings of Programs]: Specifying and Verifying and Reasoning about Programs Invariants; D.1.1 [Programming \nTechniques]: Ap\u00adplicative (Functional) Programming; D.3.3 [Programming Lan\u00adguages]: Language Constructs \nand Features Polymorphism General Terms Languages, Veri.cation Keywords relational parametricity 1. Introduction \nOne of the strengths of functional languages like Haskell is an ex\u00adpressive type system. And yet, some \nof the bene.ts this strength should hold for reasoning about programs seem not to be re\u00adalised to full \nextent. For example, Haskell uses monads (Moggi 1991) to structure programs by separating concerns (Wadler \n1992; Liang et al. 1995) and to safely mingle pure and impure compu\u00ad tations (Peyton Jones and Wadler \n1993; Launchbury and Peyton Jones 1995). A lot of code can be kept independent of a concrete choice of \nmonad. This observation pertains to functions from the Prelude (Haskell s standard library) like sequence \n:: Monad \u00b5 . [\u00b5a] . \u00b5 [a] , but also to many user-de.ned functions. Such abstraction is cer\u00adtainly a \nboon for modularity of programs. But also for reasoning? Permission to make digital or hard copies of \nall or part of this work for personal or classroom use is granted without fee provided that copies are \nnot made or distributed for pro.t or commercial advantage and that copies bear this notice and the full \ncitation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute to \nlists, requires prior speci.c permission and/or a fee. ICFP 09, August 31 September 2, 2009, Edinburgh, \nScotland, UK. Copyright c &#38;#169; 2009 ACM 978-1-60558-332-7/09/08. . . $5.00 Let us consider a more \nspeci.c example, say functions of the type Monad \u00b5 . [\u00b5 Int] . \u00b5 Int. Here are some:1 f1 = head f2 ms \n= sequence ms >>= return . sum f3 = f2 . reverse f4 [] = return 0 f4 (m : ms)= do i . m let l = length \nms if i>l then return (i + l) else f4 (drop i ms) As we see, there is quite a variety of such functions. \nThere can be simple selection of one of the monadic computations from the input list (as in f1), there \ncan be sequencing of these monadic computa\u00adtions (in any order) and some action on the encapsulated values \n(as in f2 and f3), and the behaviour, in particular the choice which of the computations from the input \nlist are actually performed, can even depend on the encapsulated values themselves (as in f4, made a \nbit arti.cial here). Further possibilities are that some of the monadic computations from the input list \nare performed repeatedly, and so forth. But still, all these functions also have something in common. \nThey can only combine whatever monadic computations, and associated effects, they encounter in their \ninput lists, but they cannot introduce new effects of any concrete monad, not even of the one they are \nactually operating on in a particular application in\u00adstance. This limitation is determined by the function \ntype. For if an f were, on and of its own, to cause any additional effect to happen, be it by writing \nto the output, by introducing additional branching in the nondeterminism monad, or whatever, then it \nwould immedi\u00adately fail to get the above type parametric over \u00b5. In a language like Haskell, should not \nwe be able to pro.t from this kind of abstrac\u00adtion for reasoning purposes? If so, what kind of insights \ncan we hope for? One thing to ex\u00adpect is that in the special case when the concrete computations in an \ninput list passed to an f :: Monad \u00b5 . [\u00b5 Int] . \u00b5 Int cor\u00adrespond to pure values (e.g., are values of \ntype IO Int that do not perform any actual input or output), then the same should hold of f s result \nfor that input list. This statement is quite intuitive from the above observation about f being unable \nto cause new effects on 1 The functions head, sum, reverse, length, and drop are all from the Prelude. \nTheir general types and explanation can be found via Hoogle (http://haskell.org/hoogle). The notation \n. is for function composi\u00adtion, while >>= and do are two different syntaxes for performing compu\u00adtations \nin a monad one after the other. Finally, return embeds pure values into a monad. its own. But what about \nmore interesting statements, for example the preservation of certain invariants? Say we pass to f a list \nof stateful computations and we happen to know that they do depend on, but do not alter (a certain part \nof) the state. Is this property pre\u00adserved throughout the evaluation of a given f? Or say the effect \nen\u00adcapsulated in f s input list is nondeterminism but we would like to simplify the program by restricting \nthe computation to a determin\u00adistically chosen representative from each nondeterministic mani\u00adfold. Under \nwhat conditions, and for which kind of representative\u00adselection functions, is this simpli.cation safe \nand does not lead to problems like a collapse of an erstwhile nonempty manifold to an empty one from \nwhich no representative can be chosen at all? One could go and study these questions for particular functions \nlike the f1 to f4 given further above. But instead we would like to answer them for any function of type \nMonad \u00b5 . [\u00b5 Int] . \u00b5 Int in general, without consulting particular function de.nitions. And we would \nnot like to restrict to the two or three scenarios depicted in the previous paragraph. Rather, we want \nto explore more abstract settings of which statements like the ones in question above can be seen, and \ndealt with, as particular instances. And, of course, we pre\u00adfer a generic methodology that applies equally \nwell to other types than the speci.c one of f considered so far in this introduction. These aims are \nnot arbitrary or far-fetched. Precedent has been set with the theorems obtained for free by Wadler (1989) \nfrom rela\u00ad tional parametricity (Reynolds 1983). Derivation of such free the\u00ad orems, too, is a methodology \nthat applies not only to a single type, works independently of particular function de.nitions, and applies \nto a diverse range of scenarios: from simple algebraic laws to pow\u00aderful program transformations (Gill \net al. 1993), to meta-theorems about whole classes of algorithms (Voigtl\u00a8ander 2008b), to speci.c applications \nin software engineering and databases (Voigtl\u00a8ander 2009). Unsurprisingly then, we do build on Reynolds \nand Wadler s work. Of course, the framework that is usually considered when free theorems are derived \nneeds to be extended to deal with types like Monad \u00b5 . ... . But the ideas needed to do so are there \nfor the taking. Indeed, both relational parametricity extended for poly\u00admorphism over type constructors \nrather than over ordinary types only, as well as relational parametricity extended to take class con\u00adstraints \ninto account, are in the folklore. However, these two strands of possible extension have not been combined \nbefore, and not been used as we do. Since we are mostly interested in demonstrating the prospects gained \nfrom that combination, we refrain here from developing the folklore into a full-.edged formal apparatus \nthat would stand to blur the intuitive ideas. This is not an overly theoret\u00adical paper. Also on purpose, \nwe do not consider Haskell intricacies, like those studied by Johann and Voigtl\u00a8ander (2004) and Stenger \nand Voigtl\u00a8ander (2009), that do affect relational parametricity but in a way orthogonal to what is of \ninterest here. Instead, we stay with Reynolds and Wadler s simple model (but consider the extension to \ngeneral recursion in Appendix C). For the sake of accessibility, we also stay close to Wadler s notation. \n2. Free Theorems, in Full Beauty So what is the deal with free theorems? Why should it be possible to \nderive statements about a function s behaviour from its type alone? Maybe it is best to start with a \nconcrete example. Consider the type signature f :: [a] . [a] . What does it tell us about the function \nf? For sure that it takes lists as input and produces lists as output. But we also see that f is polymorphic, \ndue to the type variable a, and so must work for lists over arbitrary element types. How, then, can elements \nfor the output list come into existence? The answer is that the output list can only ever contain elements \nfrom the input list. For the function, not knowing the element type of the lists it operates over, cannot \npossibly make up new elements of any concrete type to put into the output, such as 42 or True, or even \nid, because then f would immediately fail to have the general type [a] . [a].2 So for any input list \nl (over any element type) the output list fl consists solely of elements from l. But how can f decide \nwhich elements from l to propagate to the output list, and in which order and multiplicity? The answer \nis that such decisions can only be made based on the input list l. For in a pure functional language \nf has no access to any global state or other context based on which to decide. It cannot, for example, \nconsult the user in any way about what to do. And the means by which to make decisions based on l are \nlimited as well. In particular, decisions cannot possibly depend on any speci.cs of the elements of l. \nFor the function is ignorant of the element type, and so is prevented from analysing list elements in \nany way (be it by pattern\u00admatching, comparison operations, or whatever). In fact, the only means for \nf to drive its decision-making is to inspect the length of l, because that is the only element-independent \ninformation content of a list. So for any pair of lists l and l' of same length (but possibly over different \nelement types) the lists fl and fl' are formed by making the same position-wise selections of elements \nfrom l and l', respectively. Now consider the following standard Haskell function: map :: (a . \u00df) . \n[a] . [\u00df] map g [] =[] map g (a : as)=(ga):(map g as) Clearly, map g for any g preserves the lengths \nof lists. So if l' = map gl, then fl and fl' are of the same length and con\u00adtain, at each position, position-wise \nexactly corresponding elements from l and l', respectively. Since, moreover, any two position-wise corresponding \nelements, one from l and one from l' = map gl, are related by the latter being the g-image of the former, \nwe have that at each position fl' contains the g-image of the element at the same position in fl. So \nfor any list l and (type-appropriate) function g, we have f (map gl)= map g (fl). Note that during the \nreasoning leading up to that statement we did not (need to) consider the actual de.nition of f at all. \nThe methodology of deriving free theorems `a la Wadler (1989) is a way to obtain statements of this .avour \nfor arbitrary function types, and in a more disciplined (and provably sound) manner than the mere handwaving \nperformed above. The key to doing so is to interpret types as relations. For exam\u00adple, given the type \nsignature f :: [a] . [a], we take the type and replace every quanti.cation over type variables, including \nimplicit quanti.cation (note that the type [a] . [a], by Haskell convention, really means .a. [a] . [a]), \nby quanti.cation over relation vari\u00adables: .R. [R] . [R]. Then, there is a systematic way of reading \nsuch expressions over relations as relations themselves. In particu\u00adlar, base types like Int are read \nas identity relations,  for relations R and S, we have  R.S = {(f, g) |.(a, b) .R. (f a,g b) . S} , \nand 2 The situation is more complicated in the presence of general recursion. For further discussion, \nsee Appendix C. for types t and t ' with at most one free variable, say a, and a function F on relations \nsuch that every relation R between closed types t1 and t2, denoted R : t1 . t2, is mapped to a relation \nFR : t [t1/a] . t ' [t2/a], we have .R. FR = {(u, v) |.t1,t2, R : t1 . t2. (ut1 ,vt2 ) . F R} (Here, \nut1 :: t[t1/a] is the instantiation of u :: .a. t to the type t1, and similarly for vt2 . In what follows, \nwe will always leave type instantiation implicit.) Also, every .xed type constructor is read as an appropriate \ncon\u00adstruction on relations. For example, the list type constructor maps every relation R : t1 . t2 to \nthe relation [R]:[t1] . [t2] de.ned by (the least .xpoint of) [R]= {([ ], [ ])}.{(a : as,b : bs) | (a, \nb) .R, (as, bs) . [R]} , the Maybe type constructor maps every relation R : t1 . t2 to the relation Maybe \nR : Maybe t1 . Maybe t2 de.ned by Maybe R = {(Nothing, Nothing)}. {(Just a, Just b) | (a, b) . R} , and \nsimilarly for other user-de.nable types. The key insight of relational parametricity `a la Reynolds (1983) \nnow is that any expression over relations that can be built as above, by interpreting a closed type, \ndenotes the identity relation on that type. For the above example, this insight means that any f :: .a. \n[a] . [a] satis.es (f, f) . .R. [R] . [R], which by unfolding some of the above de.nitions is equivalent \nto having for every t1, t2, R : t1 . t2, l :: [t1], and l ' :: [t2] that (l, l ' ) . [R] implies (f l,f \nl ' ) . [R], or, specialised to the function level (R . g, and thus [R] . map g), for every g :: t1 . \nt2 and l :: [t1] that f (map gl)= map g (fl). This proof .\u00adnally provides the formal counterpart to the \nintuitive reasoning earlier in this section. And the development is algorithmic enough that it can be \nperformed automatically. Indeed, an online free the\u00adorems generator (B\u00a8 ohme 2007) is accessible at our \nhomepage (http://linux.tcs.inf.tu-dresden.de/~voigt/ft/). 3. The Extension to Type Constructor Classes \nWe now want to deal with two new aspects: with quanti.cation over type constructor variables (rather \nthan just over type variables) and with class constraints (Wadler and Blott 1989). For both aspects, \nthe required extensions to the interpretation of types as relations appear to be folklore, but have seldom \nbeen spelled out and have not been put to use before as we do in this paper. Regarding quanti.cation \nover type constructor variables, the necessary adaptation is as follows. Just as free type variables \nare in\u00adterpreted as relations between arbitrarily chosen closed types (and then quanti.ed over via relation \nvariables), free type constructor variables are interpreted as functions on such relations tied to ar\u00adbitrarily \nchosen type constructors. Formally, let .1 and .2 be type constructors (of kind *.*). A relational action \nfor them, denoted F : .1 . .2, is a function F on relations between closed types such that every R : \nt1 . t2 (for arbitrary t1 and t2) is mapped to an FR : .1 t1 . .2 t2. For example, the function F that \nmaps every R : t1 . t2 to FR = {(Nothing, [ ])}. {(Just a, b : bs) | (a, b) .R, bs :: [t2]} is a relational \naction F : Maybe . []. The relational interpreta\u00adtion of a type quantifying over a type constructor variable \nis now performed in an analogous way as explained for quanti.cation over type (and then, relation) variables \nabove. In different formulations and detail, the same basic idea is mentioned or used by Fegaras and \nSheard (1996), Ku.can (1997), Takeuti (2001), and Vytiniotis and Weirich (2009). Regarding class constraints, \nWadler (1989, Section 3.4) directs the way by explaining how to treat the type class Eq in the context \nof deriving free theorems. The idea is to simply restrict the relations chosen as interpretation for \ntype variables that are subject to a class constraint. Clearly, only relations between types that are \ninstances of the class under consideration are allowed. Further restrictions are obtained from the respective \nclass declaration. Namely, the restrictions must precisely ensure that every class method (seen as a \nnew constant in the language) is related to itself by the relational interpretation of its type. This \nrelatedness then guarantees that the overall result (i.e., that the relational interpretation of every \nclosed type is an identity relation) stays intact (Mitchell and Meyer 1985). The same approach immediately \napplies to type constructor classes as well. Consider, for example, the Monad class declaration: class \nMonad \u00b5 where return :: a . \u00b5a (>>=) :: \u00b5a . (a . \u00b5\u00df) . \u00b5\u00df Since the type of return is .\u00b5. Monad \u00b5 . \n(.a. a . \u00b5a), we expect that (return, return) . .F. Monad F. (.R. R. FR), and similarly for >>=. The \nconstraint Monad F on a relational action is now de.ned in precisely such a way that both conditions \nwill be ful.lled. De.nition 1. Let .1 and .2 be type constructors that are instances of Monad and let \nF : .1 . .2 be a relational action. If (return.1 , return.2 ) . .R. R.FR and  ((>>=.1 ), (>>=.2 )) \n. .R. .S. FR. ((R.FS) . FS),  then F is called a Monad-action.3 (While we have decided to gen\u00aderally \nleave type instantiation implicit, we explicitly retain instan\u00adtiation of type constructors in what follows, \nexcept for some exam\u00adples.) For example, given the following standard Monad instance de.ni\u00adtions: instance \nMonad Maybe where return a = Just a Nothing >>= k = Nothing Just a >>= k = ka instance Monad [] where \nreturn a =[a] as >>= k = concat (map k as) the relational action F : Maybe . [] given above is not a \nMonad\u00adaction, because it is not the case that ((>>= Maybe), (>>= [])) . .R. .S. FR. ((R.FS) .FS). To \nsee this, consider R = S = idInt , m1 = Just 1 , m2 = [1, 2] , k1 = .i . if i> 1 then Just i else Nothing \n, and k2 = .i . reverse [2..i] .  Clearly, (m1,m2) .F idInt and (k1,k2) . idInt .F idInt, but (m1 >>>.F \nidInt. On = Maybe k1,m2 >= [] k2)=(Nothing, [2]) /the other hand, the relational action F ' : Maybe . \n[] with F ' R = {(Nothing, [ ])} .{(Just a, [b]) | (a, b) . R} is a Monad-action. 3 It is worth noting \nthat dictionary translation (Wadler and Blott 1989) would be an alternative way of motivating this de.nition. \n We are now ready to derive free theorems involving (poly\u00admorphism over) type constructor classes. For \nexample, functions f :: Monad \u00b5 . [\u00b5 Int] . \u00b5 Int as considered in the introduc\u00adtion will necessarily \nalways satisfy (f, f) . .F. Monad F. [F idInt] .F idInt, i.e., for every choice of type constructors \n.1 and .2 that are instances of Monad, and every Monad-action F : .1 . .2, we have (f.1 ,f.2 ) . [F idInt] \n.F idInt. In the next section we prove several theorems by instantiating the F here, and provide plenty \nof examples of interesting results obtained for concrete monads. An important role will be played by \na notion connecting differ\u00adent Monad instances on a functional, rather than relational, level. De.nition \n2. Let .1 and .2 be instances of Monad and let h :: .1 a . .2 a. If h . return.1 = return.2 and  for \nevery choice of closed types t and t ' , m :: .1 t , and k :: t . .1 t ' ,  h (m >>= .1 k)= hm >>= .2 \nh . k , then h is called a Monad-morphism. The two notions of Monad-action and Monad-morphism are strongly \nrelated, in that Monad-actions are closed under point\u00adwise composition with Monad-morphisms or the inverses \nthereof, depending on whether the composition is from the left or from the right (Filinski and St\u00f8vring \n2007, Proposition 3.7(2)). 4. One Application Field: Reasoning about Monadic Programs For most of this \nsection, we focus on functions f :: Monad \u00b5 . [\u00b5 Int] . \u00b5 Int. However, it should be emphasised that \nresults of the same spirit can be systematically obtained for other func\u00adtion types involving quanti.cation \nover Monad-restricted type con\u00adstructor variables just as well. And note that the presence of the concrete \ntype Int in the function signature makes any results we ob\u00adtain for such f more, rather than less, interesting. \nFor clearly there are strictly, and considerably, fewer functions of type Monad \u00b5 . [\u00b5a] . \u00b5a than there \nare of type Monad \u00b5 . [\u00b5 Int] . \u00b5 Int 4, so proving a statement for all functions of the latter type \ndemon\u00adstrates much more power than proving the same statement for all functions of the former type only. \nIn other words, telling f what type of values are encapsulated in its monadic inputs and output entails \nmore possible behaviours of f that our reasoning principle has to keep under control. Also, it is not \nthe case that using Int in most examples in this section means that we might as well have monomorphised \nthe monad interface as follows: class IntMonad \u00b5 where return :: Int . \u00b5 (>>=) :: \u00b5 . (Int . \u00b5) . \u00b5 and \nthus are actually just proving results about a less interesting type IntMonad \u00b5 . [\u00b5] . \u00b5 without any \nhigher-orderedness (viz., quantifying only over a type variable rather than over a type constructor variable). \nThis impression would be a misconception, as we do indeed prove results for functions critically depending \non the use of higher-order polymorphism. That the type under consideration is Monad \u00b5 . [\u00b5 Int] . \u00b5 Int \ndoes by no way mean that monadic encapsulation is restricted to only integer values 4 After all, any \nfunction (de.nition) of the type polymorphic over a can also be given the more speci.c type, whereas \nof the functions f1 to f4 given in the introduction as examples for functions of the latter type only \nf1 can be given the former type as well. inside functions of that type. Just consider the function f2 \nfrom the introduction. During that function s computation, the monadic bind operation (>>=) is used to \ncombine a \u00b5-encapsulated integer list (viz., sequence ms :: \u00b5 [Int]) with a function to a \u00b5-encapsulated \nsingle integer (viz., return . sum :: [Int] . \u00b5 Int). Clearly, the same or similarly modular code could \nnot have been written at type f2 :: IntMonad \u00b5 . [\u00b5] . \u00b5, because there is no way to provide a function \nlike sequence for the IntMonad class (or any single monomorphised class), not even when we are content \nwith making sequence less .exible by .xing the a in its current type to be Int. So again, proving results \nabout all functions of type f :: Monad \u00b5 . [\u00b5 Int] . \u00b5 Int covers more ground than might at .rst appear \nto be the case. Having rationalised our choice of example function type, let us now get some actual work \ndone. As a .nal preparation, we need to mention three laws that Monad instances . are often expected \nto satisfy:5 return. a >>=. k = ka (1) m >>=. return. = m (2) (m >>=. k) >>=. q = m >>=. (.a . ka >>=. \nq) (3) Since Haskell does not enforce these laws, and it is easy to de.ne Monad instances violating \nthem, we will explicitly keep track of where the laws are required in our statements and proofs. 4.1 \nPurity Preservation As mentioned in the introduction, one .rst intuitive statement we naturally expect \nto hold of any f :: Monad \u00b5 . [\u00b5 Int] . \u00b5 Int is that when all the monadic values supplied to f in the \ninput list are actually pure (not associated with any proper monadic effect), then f s result value, \nthough of some monadic type, should also be pure. After all, f itself, being polymorphic over \u00b5, cannot \nintroduce effects from any speci.c monad. This statement is expected to hold no matter what monad the \ninput values live in. For example, if the input list consists of computations in the list monad, de.ned \nin the previous section and modelling nondeterminism, but all the concretely passed values actually correspond \nto deterministic computations, then we expect that f s result value also corresponds to a deterministic \ncomputation. Similarly, if the input list consists of IO computations, but we only pass ones that happen \nto have no side-effect at all, then f s result, though living in the IO monad, should also be side-effect-free. \nTo capture the notion of purity independently of any concrete monad, we use the convention that the pure \ncomputations in any monad are those that may be the result of a call to return. Note that this does not \nmean that the values in the input list must syntactically be return-calls. Rather, each of them only \nneeds to be semantically equivalent to some such call. The desired statement is now formalised as follows. \nIt is proved in Appendix A, and is a corollary of Theorem 3 (to be given later). Theorem 1. Let f :: \nMonad \u00b5 . [\u00b5 Int] . \u00b5 Int, let . be an instance of Monad satisfying law (1), and let l :: [. Int]. If \nevery element in l is a return.-image, then so is f. l. We can now reason for speci.c monads as follows. \nExample 1. Let l :: [[Int]], i.e., l :: [. Int] for . = []. We might be interested in establishing that \nwhen every element 5 Indeed, only a Monad instance satisfying these laws constitutes a monad in the \nmathematical sense of the word. in l is (evaluated to) a singleton list, then the result of applying \nany f :: Monad \u00b5 . [\u00b5 Int] . \u00b5 Int to l will be a singleton list as well. While this propagation is easy \nto see for f1, f2, and f3 from the introduction, it is maybe not so immediately obvious for the f4 given \nthere. However, Theorem 1 tells us without any further effort that the statement in question does indeed \nhold for f4, and for any other f of the same type. Likewise, we obtain the statement about side-effect-free \ncomputa\u00adtions in the IO monad envisaged above. All we rely on then is that the IO monad, like the list \nmonad, satis.es monad law (1).  4.2 Safe Value Extraction A second general statement we are interested \nin is to deal with the case that the monadic computations provided as input are not necessarily pure, \nbut we have a way of discarding the monadic layer and recovering underlying values. Somewhat in the spirit \nof unsafePerformIO :: IO a . a, but for other monads and hope\u00adfully safe. Then, if we are interested \nonly in a thus projected result value of f, can we show that it only depends on likewise projected input \nvalues, i.e., that we can discard any effects from the monadic computations in f s input list when we \nare not interested in the effectful part of the output computation? Clearly, it would be too much to \nexpect this reduction to work for arbitrary projections , or even arbitrary monads. Rather, we need to \ndevise appropriate re\u00adstrictions and prove that they suf.ce. The formal statement is as fol\u00adlows. Theorem \n2. Let f :: Monad \u00b5 . [\u00b5 Int] . \u00b5 Int, let . be an instance of Monad, and let p :: .a . a. If p . return. \n= id and  for every choice of closed types t and t ' , m :: .t, and k :: t . .t ' ,  p (m >>=. k)= \np (k (pm)) , then p . f. gives the same result for any two lists of same length whose corresponding elements \nhave the same p\u00adimages, i.e., p . f. can be factored as g . (map p) for some suitable g :: [Int] . Int. \n6 The theorem is proved in Appendix B. Also, it is a corollary of Theorem 4. Note that no monad laws \nat all are needed in Theorem 2 and its proof. The same will be true for the other theorems we are going \nto provide, except for Theorem 5. But .rst, we consider several example applications of Theorem 2. Example \n2. Consider the well-known writer, or logging, monad (specialised here to the String monoid): newtype \nWriter a = Writer (a, String) instance Monad Writer where return a = Writer (a, ) Writer (a, s) >>= \nk = ''' ' Writer (case ka of Writer (a ,s ) . (a ,s + s )) 6 In fact, this g is explicitly given as \nfollows: unId . fId . (map Id), using the type constructor Id and its Monad instance de.nition from Appendix \nA. Assume we are interested in applying an f :: Monad \u00b5 . [\u00b5 Int] . \u00b5 Int to an l :: [Writer Int], yielding \na monadic result of type Writer Int. Assume further that for some partic\u00adular purpose during reasoning \nabout the overall program, we are only interested in the actual integer value encapsulated in that result, \nas extracted by the following function: p :: Writer a . a p (Writer (a, s)) = a Intuition suggests that \nthen the value of p (fl) should not depend on any logging activity of elements in l. That is, if l were \nreplaced by another l ' :: [Writer Int] encapsulating the same integer values, but potentially attached \nwith different logging information, then p (fl ' ) should give exactly the same value. Since the given \np ful.ls the required conditions, Theorem 2 con.rms this intuition. It should also be instructive here \nto consider a negative example. Example 3. Recall the list monad de.ned in Section 3. It is tempting \nto use head :: [a] . a as an extraction function and expect that for every f :: Monad \u00b5 . [\u00b5 Int] . \u00b5 \nInt, we can factor head . f as g . (map head) for some suitable g :: [Int] . Int. But actually this factorisation \nfails in a subtle way. Consider, for example, the (for the sake of simplicity, arti.cial) function f5 \n:: Monad \u00b5 . [\u00b5 Int] . \u00b5 Int f5 [] = return 0 f5 (m : ms)= do i . m f5 (if i> 0 then ms else tail ms) \nThen for l = [[1], [ ]] and l ' = [[1, 0], [ ]], both of type [[Int]], we have map head l = map head \nl ', but head (f5 l)= head (f5 ). In fact, the left-hand side of this inequation l ' leads to an head \nof empty list -error, whereas the right\u00adhand side delivers the value 0. Clearly, this means that the \nsupposed g cannot exist for f5 and head. An explanation for the observed failure is provided by the conditions \nimposed on p in Theorem 2. It is simply not true that for every m and k, head (m >>= k)= head (k (head \nm)). More concretely, the failure for f5 observed above arises from this equation being violated for \nm = [1, 0] and k = .i . if i> 0 then [] else [0]. Since the previous (counter-)example is a bit peculiar \nin its reliance on runtime errors, let us consider a related setting without empty lists, an example \nalso serving to further emphasise the predictive power of the conditions on p in Theorem 2. Example 4. \nAssume, just for the scope of this example, that the type constructor [] yields (the types of) nonempty \nlists only. Clearly, it becomes an instance of Monad by just the same de.nition as given in Section 3. \nThere are now several choices for a never failing extraction function p :: [a] . a. For example, p could \nbe head, could be last, or could be the function that always returns the element in the middle position \nof its input list (and, say, the left one of the two middle elements in the case of a list of even length). \nBut which of these candidates are good in the sense of providing, for every f :: Monad \u00b5 . [\u00b5 Int] . \n\u00b5 Int, a factorisation of p . f into g . (map p) ?  The answer is provided by the two conditions on \np in Theorem 2, which specialised to the (nonempty) list monad require that for every a, p [a]= a, and \n for every choice of closed types t and t ' , m :: [t ], and k :: t . [t ' ], p (concat (map km)) = \np (k (pm)).  From these conditions it is easy to see that now p = head is good (in contrast to the situation \nin Example 3), and so is p = last, while the proposed middle extractor is not. It does not ful.l the \nsecond condition above, roughly because k does not necessarily map all its inputs to equally long lists. \n(A concrete counterexample f6, of appropriate type, can easily be produced from this observation.)  \n4.3 Monad Subspacing Next, we would like to tackle reasoning not about the complete absence of (`a la \nTheorem 2), a la Theorem 1), or disregard for (`monadic effects, but about .ner nuances. Often, we know \ncer\u00adtain computations to realise only some of the potential effects to which they would be entitled according \nto the monad they live in. If, for example, the effect under consideration is nondetermin\u00adism ` a la \nthe standard list monad, then we might know of some computations in that monad that they realise only \nnone-or-one\u00adnondeterminism, i.e., never produce more than one answer, but may produce none at all. Or \nwe might know that they realise only non\u00adfailing-nondeterminism, i.e., always produce at least one answer, \nbut may produce more than one. Then, we might want to argue that the respective nature of nondeterminism \nis preserved when com\u00adbining such computations using, say, a function f :: Monad \u00b5 . [\u00b5 Int] . \u00b5 Int. \nThis preservation would mean that applying any such f to any list of empty-or-singleton lists always \ngives an empty\u00ador-singleton list as result, and that applying any such f to any list of nonempty lists \nonly gives a nonempty list as result for sure. Or, in the case of an exception monad (Either String), \nwe might want to establish that an application of f cannot possibly lead to any ex\u00adceptional value (error \ndescription string) other than those already present somewhere in its input list. Such invariants can \noften be captured by identifying a certain subspace of the monadic type in question that forms itself \na monad, or, indeed, by embedding another, smaller , monad into the one of interest. Formal counter\u00adparts \nof the intuition behind the previous sentence and the vague phrases occurring therein can be found in \nDe.nition 2 and the fol\u00ad lowing theorem, as well as in the subsequent examples. Theorem 3. Let f :: Monad \n\u00b5 . [\u00b5 Int] . \u00b5 Int, let h :: .1 a . .2 a be a Monad-morphism, and let l :: [.2 Int]. If every element \nin l is an h-image, then so is f.2 l. Proof. We prove that for every l ' :: [.1 Int], f.2 (map hl ' )= \nh (f.1 l ' ) . (4) To do so, we .rst show that F : .2 . .1 with FR =(.2 R); h-1 , where ; is (forward) \nrelation composition and gives the -1 inverse of a function graph, is a Monad-action. Indeed, (return.2 \n, return.1 ) . .R. R . FR, since for ev\u00adery R and (a, b) .R, (return.2 a, h (return.1 b)) = (return.2 \na, return.2 b) . .2 R by (return.2 , return.2 ) . .R. R. .2 R (which holds due to return.2 :: .a. a . \n.2 a), and  ((>>=.2 ), (>>=.1 )) . .R. .S. FR. ((R.FS) . FS), since for every R, S, (m1,m2) . (.2 R); \nh-1, and (k1,k2) .R. ((.2 S); h-1),  (m1 >>= .2 k1,h (m2 >>= .1 k2)) = (m1 >>= .2 k1,hm2 >>= .2 h . \nk2) . .2 S by ((>>=.2 ), (>>=.2 )) . .R. .S..2 R. ((R. .2 S) . .2 S), (m1,h m2) . .2 R, and (k1,h . k2) \n. R. .2 S. Hence, (f.2 ,f.1 ) . [F idInt] .F idInt. Given that we have F idInt =(.2 idInt); h-1 = h-1, \nthis implies the claim. (Note that .2 idInt is the relational interpretation of the closed type .2 Int, \nand thus itself denotes id.2 Int.) Using Theorem 3, we can indeed prove the statements mentioned for \nthe list and exception monads above. Here, for diversion, we instead prove some results about more stateful \ncomputations. Example 5. Consider the well-known reader monad: newtype Reader .a = Reader (. . a) instance \nMonad (Reader .) where return a = Reader (.r . a) Reader g >>= k = Reader (.r . case k (gr) of Reader \ng ' . g ' r) Assume we are given a list of computations in a Reader monad, but it happens that all present \ncomputations depend only on a certain part of the environment type. For example, for some closed types \nt1 and t2, l :: [Reader (t1,t2) Int], and for every element Reader g in l, g (x, y) never depends on \ny. We come to expect that the same kind of independence should then hold for the result of applying any \nf :: Monad \u00b5 . [\u00b5 Int] . \u00b5 Int to l. And indeed it does hold by Theorem 3 with the following Monad-morphism: \nh :: Reader t1 a . Reader (t1,t2) a h (Reader g)= Reader (g . fst) It is also possible to connect more \ndifferent monads, even involving the IO monad. Example 6. Let l :: [IO Int] and assume that the only \nside\u00adeffects that elements in l have consist of writing strings to the output. We would like to use Theorem \n3 to argue that the same is then true for the result of applying any f :: Monad \u00b5 . [\u00b5 Int] . \u00b5 Int to \nl. To this end, we need to somehow capture the concept of writing (potentially empty) strings to the \noutput as only side-effect of an IO computation via an embedding from another monad. Quite naturally, \nwe reuse the Writer monad from Example 2. The embedding function is as follows: h :: Writer a . IO a \nh (Writer (a, s)) = putStr s >> return a What is left to do is to show that h is a Monad-morphism. But \nthis property follows from putStr = return (), putStr (s + s ' )= putStr s >> putStr s ', and monad \nlaws (1) and (3) for the IO monad.  Similarly to the above, it would also be possible to show that when \nthe IO computations in l do only read from the input (via, possibly repeated, calls to getChar), then \nthe same is true of fl. Instead of exercising this through, we turn to general state transformers. Example \n7. Consider the well-known state monad: newtype State sa = State (s . (a, s)) instance Monad (State s) \nwhere return a = State (.s . (a, s)) State g >>= k = State (.s . let (a, s ' )= gs in ' '' case ka of \nState g . gs ) Intuitively, this monad extends the reader monad by not only allowing a computation to \ndepend on an input state, but also to transform the state to be passed to a subsequent computation. A \nnatural question now is whether being a speci.c state trans\u00adformer that actually corresponds to a read-only \ncomputation is an invariant that is preserved when computations are com\u00adbined. That is, given some closed \ntype t and l :: [State t Int] such that for every element State g in l, snd . g = id, is it the case \nthat for every f :: Monad \u00b5 . [\u00b5 Int] . \u00b5 Int, also fl is of the form State g for some g with snd . g \n= id ? The positive answer is provided by Theorem 3 with the following Monad-morphism: h :: Reader ta \n. State ta h (Reader g)= State (.s . (g s,s)) Similarly to the above, we can show preservation of the \ninvariant that a computation transforms the state in the background , while the primary result value \nis independent of the input state. That is, if for every element State g in l, there exists an i :: Int \nwith fst . g = const i, then the same applies to fl. It should also be possible to transfer the above \nkind of reasoning to the ST monad (Launchbury and Peyton Jones 1995).  4.4 Effect Abstraction As a .nal \nstatement about our pet type, Monad \u00b5 . [\u00b5 Int] . \u00b5 Int, we would like to show that we can abstract from \nsome aspects of the effectful computations in the input list if we are interested in the effects of the \n.nal result only up to the same abstraction. For conveying between the full effect space and its abstraction, \nwe again use Monad-morphisms. Theorem 4. Let f :: Monad \u00b5 . [\u00b5 Int] . \u00b5 Int and let h :: .1 a . .2 a \nbe a Monad-morphism. Then h . f.1 gives the same result for any two lists of same length whose corresponding \nelements have the same h-images. Proof. Let l1,l2 :: [.1 Int] be such that map hl1 = map hl2. Then h \n(f.1 l1)= h (f.1 l2) by statement (4) from the proof of Theorem 3. Example 8. Consider the well-known \nexception monad: instance Monad (Either String) where return a = Right a Left err >>= k = Left err Right \na >>= k = ka We would like to argue that if we are only interested in whether the result of f for some \ninput list over the type Either String Int is an exceptional value or not (and which ordinary value is \nencapsulated in the latter case), but do not care what the concrete error description string is in the \nfor\u00admer case, then the answer is independent of the concrete error description strings potentially appearing \nin the input list. For\u00admally, let l1,l2 :: [Either String Int] be of same length, and let corresponding \nelements either be both tagged with Left (but not necessarily containing the same strings) or be iden\u00adtical \nRight-tagged values. Then for every f :: Monad \u00b5 . [\u00b5 Int] . \u00b5 Int, fl1 and fl2 either are both tagged \nwith Left or are identical Right-tagged values. This statement holds by Theorem 4 with the following \nMonad-morphism: h :: Either String a . Maybe a h (Left err)= Nothing h (Right a)= Just a  4.5 A More \nPolymorphic Example Just to reinforce that our approach is not speci.c to our pet type alone, we end \nthis section by giving a theorem obtained for another type, the one of sequence from the introduction, \nalso showing that mixed quanti.cation over both type constructor variables and ordinary type variables \ncan very well be handled. The theorem s statement involves the following function: fmap :: Monad \u00b5 . \n(a . \u00df) . \u00b5a . \u00b5\u00df fmap gm = m >>= return . g Theorem 5. Let f :: Monad \u00b5 . [\u00b5a] . \u00b5 [a] and let h :: \n.1 a . .2 a be a Monad-morphism. If .2 satis.es law (2), then for every choice of closed types t1 and \nt2 and g :: t1 . t2, f.2 . map (fmap.2 g) . map h = fmap.2 (map g) . h . f.1 . 7 Intuitively, this theorem \nmeans that any f of type Monad \u00b5 . [\u00b5a] . \u00b5 [a] commutes with, both, transformations on the monad structure \nand transformations on the element level. The occurrences of map and fmap are solely there to bring those \ntransformations h and g into the proper positions with respect to the different nestings of the type \nconstructors \u00b5 and [] on the input and output sides of f. Note that by setting either g or h to id, we \nobtain the specialised versions f.2 . map h = h . f.1 7 For the curious reader: the proof derives this \nstatement from (f.2 ,f.1 ) . [F g-1] .F [g-1] for the same Monad-action F : .2 . .1 as used in the proof \nof Theorem 3. and f. . map (fmap. g)= fmap. (map g) . f. . (5) Further specialising the latter by choosing \nthe identity monad for ., we would also essentially recover the free theorem derived for f :: [a] . [a] \nin Section 2. 5. Another Application: Difference Lists, Transparently It is a well-known problem that \ncomputations over lists sometimes suffer from a quadratic runtime blow-up due to left-associatively nested \nappends. For example, this is so for .attening a tree of type data Tree a = Leaf a | Node (Tree a)(Tree \na) using the following function: .atten :: Tree a . [a] .atten (Leaf a) =[a] .atten (Node t1 t2)= .atten \nt1 + .atten t2 An equally well-known solution is to switch to an alternative rep\u00adresentation of lists \nas functions, by abstraction over the list end, often called difference lists. In the formulation of \nHughes (1986), but encapsulated as an explicitly new data type: newtype DList a = DL {unDL :: [a] . [a]} \nrep :: [a] . DList a rep l = DL (l ++) abs :: DList a . [a] abs (DL f)= f [] emptyR :: DList a emptyR \n= DL id consR :: a . DList a . DList a consR a (DL f)= DL ((a :) . f) appendR :: DList a . DList a \n. DList a appendR (DL f)(DL g)= DL (f . g) Then, .attening a tree into a list in the new representation \ncan be done using the following function: .atten ' :: Tree a . DList a .atten ' (Leaf a)= consR a emptyR \n ' '' .atten (Node t1 t2)= appendR (.atten t1)(.atten t2) and a more ef.cient variant of the original \nfunction, with its original type, can be recovered as follows: .atten :: Tree a . [a] .atten = abs . \n.atten ' There are two problems with this approach. One is correctness. How do we know that the new .atten \nis equivalent to the original one? We could try to argue by distributing abs over the de.nition of .atten \n', using abs emptyR = [], abs (consR a as)= a : abs as, and abs (appendR as bs)= abs as + abs bs . (6) \nBut actually the last equation does not hold in general. The rea\u00adson is that there are as :: DList t \nthat are not in the image of rep. Consider, for example, as = DL reverse. Then neither is as = rep l \nfor any l, nor does (6) hold for every bs. Any argu\u00adment by distributing abs would thus have to rely \non the implicit assumption that a certain discipline has been exercised when going from the original \n.atten to .atten ' by replacing [], (:), and (++) by emptyR, consR, and appendR (and/or applying rep \nto explicit lists). But this implicit assumption is not immediately in reach for formal grasp. So it \nwould be nice to be able to provide a single, conclusive correctness statement for transformations like \nthe one above. One way to do so was presented by Voigtl\u00a8 ander (2002), but it requires a certain restructuring \nof code that can hamper composi\u00ad tionality and .exibility by introducing abstraction at .xed program \npoints (via lambda-abstraction and so-called vanish-combinators). This also brings us to the second problem \nwith the simple approach above. When, and how, should we switch between the original and the alternative \nrepresentations of lists during program construction? If we .rst write the original version of .atten \nand only later, after observing a quadratic runtime overhead, switch manually to the .atten '-version, \nthen this rewriting is quite cumbersome, in par\u00adticular when it has to be done repeatedly for different \nfunctions. Of course, we could decide to always use emptyR, consR, and appendR from the beginning, to \nbe on the safe side. But actually this strategy is not so safe, ef.ciency-wise, because the representa\u00adtion \nof lists by functions carries its own (constant-factor) overhead. If a function does not use appends \nin a harmful way, then we do not want to pay this price. Hence, using the alternative presentation in \na particular situation should be a conscious decision, not a de\u00adfault. And assume that later on we change \nthe behaviour of .atten, say, to explore only a single path through the input tree, so that no appends \nat all arise. Certainly, we do not want to have to go and manually switch back to the, now suf.cient, \noriginal list represen\u00adtation. The cure to our woes here is almost obvious, and has often been applied \nin similar situations: simply use overloading. Speci.cally, we can declare a type constructor class as \nfollows: class ListLike d where empty :: da cons :: a . da . da append :: da . da . da and code .atten \nin the following form: .atten :: Tree a . (.d. ListLike d . da) .atten (Leaf a)= cons a empty .atten \n(Node t1 t2)= append (.atten t1)(.atten t2) Then, with the obvious instance de.nitions instance ListLike \n[] where empty = [] cons = (:) append = (++) and instance ListLike DList where empty = emptyR cons = \nconsR append = appendR we can use the single version of .atten above both to produce ordinary lists \nand to produce difference lists. The choice between the two will be made automatically by the type checker, \ndepending on the context in which a call to .atten occurs. For example, in last (.atten t) (7) the ordinary \nlist representation will be used, due to the input type of last. Actually, (7) will compile (under GHC, \nat least) to ex\u00adactly the same code as last (.atten t) for the original de.nition of .atten from the \nvery beginning of this section. Any overhead related to the type class abstraction is simply eliminated \nby a stan\u00addard optimisation. In particular, this means that where the original representation of lists \nwould have perfectly suf.ced, programming against the abstract interface provided by the ListLike class \ndoes no harm either. On the other hand, (7) of course still suffers from the same quadratic runtime blow-up \nas with the original de.nition of .atten. But now we can switch to the better behaved difference list \nrepresentation without touching the code of .atten at all, by simply using last (abs (.atten t)) . (8) \nHere the (input) type of abs determines .atten to use emptyR, consR, and appendR, leading to linear runtime. \nCan we now also answer the correctness question more satis\u00adfactorily? Given the forms of (7) and (8), \nit is tempting to simply conjecture that abs t = t for any t. But this conjecture cannot be quite right, \nas abs has different input and output types. Also, we have already observed that some t of abs s input \ntype are prob\u00adlematic by not corresponding to any actual list. The coup now is to only consider t that \nonly use the ListLike interface, rather than any speci.c operations related to DList as such. That is, \nwe will indeed prove that for every closed type t and t :: ListLike d . dt, abs tDList = t[] . Since \nthe polymorphism over d in the type of t is so important, we follow Voigtl\u00a8ander (2008a) and make it \nan explicit requirement in a function that we will use instead of abs for switching from the original \nto the alternative representation of lists: improve :: (.d. ListLike d . da) . [a] improve t = abs t \nNow, when we observe the problematic runtime overhead in (7), we can replace it by last (improve (.atten \nt)) . That this replacement does not change the semantics of the program is established by the following \ntheorem, which provides the sought\u00adafter general correctness statement. Theorem 6. Let t :: ListLike \nd . dt for some closed type t . Then improve t = t[] . Proof. We prove unDL tDList =(t[] ++) , (9) which \nby the de.nitions of improve and abs, and by t[] + [] = t[], implies the claim. To do so, we .rst show \nthat F : DList . [] with FR = unDL ; ([R] . [R]) ; (++)-1 is a ListLike-action, where the latter concept \nis de.ned as any relational action F : .1 . .2 for type constructors .1 and .2 that are instances of \nListLike such that (empty.1 , empty.2 ) . .R. FR,  (cons.1 , cons.2 ) . .R. R. (FR.FR), and  (append.1 \n, append.2 ) . .R. FR. (FR.FR).  Indeed, (emptyR, [ ]) . .R. FR, since for every R and (l1,l2) . [R], \n(unDL emptyR l1, [] + l2)=(l1,l2) . [R],  (consR, (:)) . .R. R. (FR.FR), since for every R, (a, b) .R, \n(f, bs) . ([R] . [R]) ; (++)-1, and (l1,l2) . [R],  (unDL (consR a (DL f)) l1, (b : bs)+ l2)= (a : fl1,b \n: bs + l2) . [R] by (a, b) .R and (fl1, bs + l2) . [R] (which holds due to (f, (bs ++)) . [R] . [R] and \n(l1,l2) . [R]), and (appendR, (++)) . .R. FR. (FR.FR), since for every R, (f, as) . ([R] . [R]) ; (++)-1 \n, (g, bs) . ([R] . [R]) ; (++)-1, and (l1,l2) . [R], (unDL (appendR (DL f)(DL g)) l1, (as + bs)+ l2)= \n(f (gl1), as + (bs + l2)) . [R] by (f, (as ++)) . [R] . [R], (g, (bs ++)) . [R] . [R], and (l1,l2) . \n[R]. Hence, (tDList,t[]) .F idt . Given that we have F idt = unDL ; ([idt ] . [idt ]) ; (++)-1 = unDL \n; (++)-1, this im\u00adplies (9). Note that the ListLike-action F : DList . [] used in the above proof is \nthe same as FR =(DList R); rep -1 , given that DList R = unDL ; ([R] . [R]) ; DL. This connection suggests \nthe following more general theorem, which can actually be proved much like above. Theorem 7. Let t :: \nListLike d . dt for some closed type t , let .1 and .2 be instances of ListLike, and let h :: .1 a . \n.2 a. If h empty.1 = empty.2 ,  for every closed type t , a :: t, and as :: .1 t , h (cons.1 a as)= \ncons.2 a (h as), and  for every closed type t and as, bs :: .1 t , h (append.1 as bs)= append.2 (h as)(h \nbs),  then ht.1 = t.2 . Theorem 6 is a special case of this theorem by setting .1 = [], .2 = DList, \nand h = rep, and observing that rep [] = emptyR,  for every closed type t, a :: t, and as :: [t], rep \n(a : as)= consR a (rep as), for every closed type t and as, bs :: [t ], rep (as + bs)= appendR (rep \nas)(rep bs), and  abs . rep = id,  all of which hold by easy calculations. One key observation here \nis that the third of the above observations does actually hold, in contrast to its faulty dual (6) considered \nearlier in this section. Of course, free theorems can now also be derived for other types than those \nconsidered in Theorems 6 and 7. For example, for every closed type t, f :: ListLike d . dt . dt , and \nh as in Theorem 7, we get that: f.2 . h = h . f.1 . 6. Discussion and Related Work Of course, statements \nlike that of Theorem 7 are not an entirely new revelation. That statement can be read as a typical fusion \nlaw for compatible morphisms between algebras over the signature de\u00adscribed by the ListLike class declaration. \n(For a given t, consider ListLike d . dt as the corresponding initial algebra, .1 t and .2 t as two further \nalgebras, and the operation \u00b7.i of instantiating a t :: ListLike d . dt to a t.i :: .i t as initial algebra \nmor\u00adphism, or catamorphism. Then the conditions on h in Theorem 7 make it an algebra morphism and the \ntheorem s conclusion, also expressible as h .\u00b7.1 = \u00b7 .2 , is just that of the standard cata\u00admorphism \nfusion law.) But being able to derive such statements directly from the types in the language, based \non its built-in ab\u00adstraction facilities, immediately as well for more complicated types (like ListLike \nd . dt . dt instead of ListLike d . dt ), and all this without going through category-theoretic hoops, \nis new and unique to our approach. There has been quite some interest recently in enhancing the state \nof the art in reasoning about monadic programs. Filinski and St\u00f8vring (2007) study induction principles \nfor effectful data types. These principles are used for reasoning about functions on data types involving \nspeci.c monadic effects (rather than about func\u00adtions that are parametric over some monad), and based \non the func\u00adtions de.ning equations (rather than based on their types only), and thus are orthogonal \nto our free theorems. But for their ex\u00adample applications to formal models of backtracking, Filinski \nand St\u00f8vring also use a form of relational reasoning very close to the one appearing in our invocation \nof relational parametricity. In par\u00adticular, our De.nition 1 corresponds to their De.nition 3.3. They \nalso use monad morphisms (not to be confused with their monad\u00adalgebra morphisms, or rigid functions, \nplaying the key role in their induction principles). The scope of their relational reasoning is dif\u00adferent, \nthough. They use it for establishing the observational equiv\u00adalence of different implementations of the \nsame monadic effect. This is, of course, one of the classical uses of relational parametric\u00adity: representation \nindependence in different realizations of an ab\u00adstract data type. But it is only one possible use, and \nour treatment of full polymorphism opens the door to other uses also in connection with monadic programs. \nRather than only relating different, but se\u00admantically equivalent, implementations of the same monadic \neffect (as hard-wired into Filinski and St\u00f8vring s De.nition 3.5), we ac\u00adtually connect monads embodying \ndifferent effects. These connec\u00adtions lead to applications not previously in reach, such as our rea\u00adsoning \nabout preservation of invariants. It is worth pointing out that Filinski (2007) does use monad morphisms \nfor subeffecting , but only for the discussion of hierarchies inside each one of two com\u00adpeting implementations \nof the same set of monadic effects; the rela\u00adtional reasoning (via Monad-actions and so forth) is then \northogo\u00adnal to these hierarchies and again can only lead to statements about observational equivalence \nof the two realizations overall, rather than to more nuanced statements about programs in one of them \nas such. The reason again, as with Filinski and St\u00f8vring (2007), is that no full polymorphism is considered, \nbut only parametrisa\u00adtion over same-effect-monads on top-level. Interestingly, though, the key step in \nall our proofs in Section 4, namely .nding a suitable Monad-action, can be streamlined in the spirit \nof Proposition 3.7 of Filinski and St\u00f8vring (2007) or Lemmas 45, 46 of Filinski (2007). It seems fair \nto mention that the formal accounts of Filinski and St\u00f8vring are very complex, but that this is necessarily \nso because they deal with general recursion at both term and type level, while we have completely dodged \nsuch issues. Treating general recursion in a semantic framework typically involves a good deal of domain \ntheory such as considered by Birkedal et al. (2007). We only pro\u00ad vide a very brief sketch of what interactions \nwe expect between general recursion and our developments from the previous sections in Appendix C. Swierstra \n(2008) proposes to code against modularly assembled free monads, where the assembling takes place by \nbuilding coprod\u00aducts of signature functors corresponding to the term languages of free monads. The associated \ntype signatures are able to convey some of the information captured by our approach. For example, a monadic \ntype Term PutStr Int can be used to describe com\u00adputations whose only possible side-effect is that of \nwriting strings to the output. Passing a list of values of that type to a function f :: Monad \u00b5 . [\u00b5 \nInt] . \u00b5 Int clearly results in a value of type Term PutStr Int as well. Thus, if it is guaranteed (note \nthe proof obligation) that execution of such a term value, on a kind of virtual machine (Swierstra and \nAltenkirch 2007) or in the actual IO monad, does indeed have no other side effect than potential out\u00adput, \nthen one gets a statement in the spirit of our Example 6. On the other hand, statements like the one \nin our Example 8 (also, say, reformulated for exceptions in the IO monad) are not in reach with that \napproach alone. Moreover, Swierstra s approach to subeffect\u00ading depends very much on syntax, essentially \non term language inclusion along with proof obligations on the execution functions from terms to some \nsemantic space. This dependence prevents di\u00adrectly obtaining statements roughly analogous to our Examples \n5 and 7 using his approach. Also, depending on syntactic inclusion is a very strong restriction indeed. \nFor example, putStr is seman\u00adtically equivalent to return (), and thus without visible side-effect. \nBut nevertheless, any computation syntactically containing a call to putStr would of necessity be assigned \na type in a monad Term g with g containing (with respect to Swierstra s functor-level rela\u00adtion :-:) \nthe functor PutStr, even when that call s argument would eventually evaluate to the empty string. Thus, \nsuch a computation would be banned from the input list in a statement like the one we give below Example \n6. It is not so with our more semantical approach. Dealing more speci.cally with concrete monads is \nthe topic of recent works by Hutton and Fulger (2008), using point-free equational reasoning, and by \nNanevski et al. (2008), employing an axiomatic extension of dependent type theory. On the tool side, \nwe already mentioned the free theorems gener\u00adator at http://linux.tcs.inf.tu-dresden.de/~voigt/ft/. It \ndeals gracefully with ordinary type classes (in the of.ine, shell\u00adbased version even with user-de.ned \nones), but has not yet been extended for type constructor classes. There is also another free theorems \ngenerator, written by Andrew Bromage, running in Lambdabot (http://haskell.org/haskellwiki/Lambdabot). \nIt does not know about type or type constructor classes, but deals with type constructors by treating \nthem as .xed functors. Thus, it can, for example, derive the statement (5) for functions f. :: [.a] . \n. [a], but not more general and more interesting statements like those given in Theorem 5 and earlier, \nconnecting different Monad instances, concerning the beyond-functor aspects of monads, or our results \nabout ListLike. Acknowledgments I would like to thank the anonymous reviewers of more than one version \nof this paper who have helped to improve it through their criticism and suggestions. Also, I would like \nto thank Helmut Seidl, who inspired me to consider free theorems involving type construc\u00adtor classes \nin the .rst place by asking a challenging question re\u00adgarding the power of type(-only)-based reasoning \nabout monadic programs during a train trip through Munich quite some time ago. (The answer to his question \nis essentially Example 7.) References L. Birkedal, R.E. M\u00f8gelberg, and R.L. Petersen. Domain-theoretical \nmod\u00adels of parametric polymorphism. Theoretical Computer Science, 388 (1 3):152 172, 2007. S. B\u00a8ohme. \nFree theorems for sublanguages of Haskell. Master s thesis, Technische Universit\u00a8at Dresden, 2007. N.A. \nDanielsson, R.J.M. Hughes, P. Jansson, and J. Gibbons. Fast and loose reasoning is morally correct. In \nPrinciples of Programming Languages, Proceedings, pages 206 217. ACM Press, 2006. L. Fegaras and T. \nSheard. Revisiting catamorphisms over datatypes with embedded functions (or, Programs from outer space). \nIn Principles of Programming Languages, Proceedings, pages 284 294. ACM Press, 1996. A. Filinski. On \nthe relations between monadic semantics. Theoretical Computer Science, 375(1 3):41 75, 2007. A. Filinski \nand K. St\u00f8vring. Inductive reasoning about effectful data types. In International Conference on Functional \nProgramming, Proceedings, pages 97 110. ACM Press, 2007. A. Gill, J. Launchbury, and S.L. Peyton Jones. \nA short cut to deforesta\u00adtion. In Functional Programming Languages and Computer Architec\u00adture, Proceedings, \npages 223 232. ACM Press, 1993. R.J.M. Hughes. A novel representation of lists and its application to \nthe function reverse . Information Processing Letters, 22(3):141 144, 1986. G. Hutton and D. Fulger. \nReasoning about effects: Seeing the wood through the trees. In Trends in Functional Programming, Draft \nProceedings, 2008. P. Johann and J. Voigtl\u00a8ander. Free theorems in the presence of seq. In Prin\u00adciples \nof Programming Languages, Proceedings, pages 99 110. ACM Press, 2004. J. Ku.can. Metatheorems about \nConvertibility in Typed Lambda Calculi: Applications to CPS Transform and Free Theorems . PhD thesis, \nMassachusetts Institute of Technology, 1997. J. Launchbury and S.L. Peyton Jones. State in Haskell. \nLisp and Symbolic Computation, 8(4):293 341, 1995. S. Liang, P. Hudak, and M.P. Jones. Monad transformers \nand modular interpreters. In Principles of Programming Languages, Proceedings, pages 333 343. ACM Press, \n1995.  J.C. Mitchell and A.R. Meyer. Second-order logical relations (Extended abstract). In Logic of \nPrograms, Proceedings, volume 193 of LNCS, pages 225 236. Springer-Verlag, 1985. E. Moggi. Notions of \ncomputation and monads. Information and Compu\u00adtation, 93(1):55 92, 1991. A. Nanevski, G. Morrisett, \nA. Shinnar, P. Govereau, and L. Birkedal. Ynot: Dependent types for imperative programs. In International \nConference on Functional Programming, Proceedings, pages 229 240. ACM Press, 2008.  S.L. Peyton Jones \nand P. Wadler. Imperative functional programming. In Principles of Programming Languages, Proceedings, \npages 71 84. ACM Press, 1993. J.C. Reynolds. Types, abstraction and parametric polymorphism. In Infor\u00admation \nProcessing, Proceedings, pages 513 523. Elsevier, 1983. F. Stenger and J. Voigtl\u00a8ander. Parametricity \nfor Haskell with imprecise error semantics. In Typed Lambda Calculi and Applications, Proceedings, volume \n5608 of LNCS, pages 294 308. Springer-Verlag, 2009.  W. Swierstra. Data types `a la carte. Journal of \nFunctional Programming, 18(4):423 436, 2008. W. Swierstra and T. Altenkirch. Beauty in the beast A functional \nsemantics for the awkward squad. In Haskell Workshop, Proceedings, pages 25 36. ACM Press, 2007. I. Takeuti. \nThe theory of parametricity in lambda cube. Manuscript, 2001. J. Voigtl\u00a8ander. Concatenate, reverse and \nmap vanish for free. In Inter\u00adnational Conference on Functional Programming, Proceedings, pages 14 25. \nACM Press, 2002. J. Voigtl\u00a8ander. Asymptotic improvement of computations over free monads. In Mathematics \nof Program Construction, Proceedings, volume 5133 of LNCS, pages 388 403. Springer-Verlag, 2008a. J. \nVoigtl\u00a8ander. Much ado about two: A pearl on parallel pre.x computation. In Principles of Programming \nLanguages, Proceedings, pages 29 35. ACM Press, 2008b. J. Voigtl\u00a8ander. Bidirectionalization for free! \nIn Principles of Programming Languages, Proceedings, pages 165 176. ACM Press, 2009. D. Vytiniotis and \nS. Weirich. Type-safe cast does no harm: Syntactic parametricity for F. and beyond. Manuscript, 2009. \n P. Wadler. Theorems for free! In Functional Programming Languages and Computer Architecture, Proceedings, \npages 347 359. ACM Press, 1989. P. Wadler. The essence of functional programming (Invited talk). In Principles \nof Programming Languages, Proceedings, pages 1 14. ACM Press, 1992. P. Wadler and S. Blott. How to make \nad-hoc polymorphism less ad hoc. In Principles of Programming Languages, Proceedings, pages 60 76. ACM \nPress, 1989. A. Proof of Theorem 1 We prove that for every l ' :: [Int], f. (map return. l ' )= return. \n(unId (fId (map Id l ' ))) , where newtype Id a = Id {unId :: a} instance Monad Id where return a = \nId a Id a >>= k = ka To do so, we .rst show that F : . . Id with -1 FR = return . ; R ; Id , where \n; is (forward) relation composition and -1 gives the inverse of a function graph, is a Monad-action. \nIndeed, (return., returnId) . .R. R.FR, since for every R and (a, b) .R, (return. a, returnId b)=(return. \na, Id b) . -1 return. ; R ; Id, and ((>>= .), (>>= Id)) . .R. .S. FR. ((R.FS) . FS), since for every \nR, S, (a, b) .R, and (k1,k2) .R. FS, (return. a >>=. k1, Id b >>=Id k2)=(k1 a, k2 b) . FS. (Note the \nuse of monad law (1) for ..) Hence, by what we derived towards the end of Section 3, (f.,fId) . [F idInt] \n.F idInt. Given that we have F idInt = return-1 ; Id = . (return. . unId)-1, this implies the claim. \nB. Proof of Theorem 2 We prove that for every l :: [. Int], p (f. l)= unId (fId (map (Id . p) l)) , \nwhere the type constructor Id and its Monad instance de.nition are as in the proof of Theorem 1. To do \nso, we .rst show that F : . . Id with FR = p ; R ; Id is a Monad-action. Indeed, (return., returnId) \n. .R. R.FR, since for every R and (a, b) .R, (return. a, b) . p ; R by p (return. a)= a, and  ((>>=.), \n(>>=Id)) . .R. .S. FR. ((R.FS) . FS), since for every R, S, (m, b) . p ; R, and (k1,k2) . R.FS, (m >>=. \nk1, Id b >>= Id k2) . p ; S ; Id by p (m >>=. k1)= p (k1 (pm)) and (k1 (pm),k2 b) . p ; S ; Id (which \nholds due to (k1,k2) .R.FS and (p m,b) .R).  Hence, (f.,fId) . [F idInt] .F idInt. Given that we have \nF idInt = p ; Id = Id . p = p ; unId-1, this implies the claim. C. Free Theorems, the Ugly Truth Free \ntheorems as described in Section 2 are beautiful. And very nice. Almost too good to be true. And actually \nthey are not. At least not unrestricted and in a setting more closely resembling a modern functional \nlanguage than the plain polymorphic lambda-calculus for which relational parametricity was originally \nconceived. In par\u00adticular, problems are caused by general recursion with its poten\u00adtial for nontermination. \nWe have purposefully ignored this issue throughout the main body of the paper, so as to be able to explain \nour ideas and new abstractions in the most basic surrounding. In a sense, our reasoning has been up to \n. , or fast and loose, but morally correct (Danielsson et al. 2006). We leave a full formal treatment \nof free theorems involving type constructor classes in the presence of partiality as a challenge for \nfuture work, but use this appendix to outline some re.nements that are expected to play a central role \nin such a formalisation. So what is the problem with potential nontermination? Let us .rst discuss this \nquestion based on the simple example f :: [a] . [a] from Section 2. There, we argued that the output \nlist of any such f can only ever contain elements from the input list. But this claim is not true anymore \nnow, because f might just as well choose, for some element position of its output list, to start an arbitrary \nlooping computation. That is, while f certainly (and still) cannot possibly make up new elements of any \nconcrete type to put into the output, such as 42 or True, it may very well put . there, even while not \nknowing the element type of the lists it operates over, because . does exist at every type. So the erstwhile \nclaim that for any input list l the output list fl consists solely of elements from l has to be re.ned \nas follows. For any input list l the (potentially partial or in.nite) output list fl consists solely \nof elements from l and/or .. The decisions about which elements from l to propagate to the output list, \nin which order and multiplicity, and where to put . can again only be made based on the input list l, \nand only by inspecting its length (or running into an unde.ned tail or an in.nite list). So for any pair \nof lists l and l ' of same length (re.ning this notion to take partial and in.nite lists into account) \nthe lists fl and fl ' are formed by making the same position-wise selections of elements from l and l \n', respectively, and by inserting . at the same positions, if any. For any l ' = map gl, we then still \nhave that fl and fl ' are of the same length and contain position-wise exactly corresponding elements \nfrom l and l ' = map gl, at those positions where f takes over elements from its input rather than inserting \n.. For those positions where f does insert ., which will then happen equally for fl and fl ', we may \nonly argue that the element in fl ' contains the g-image of the corresponding element in fl if indeed \n. is the g-image of ., that is, if g is a strict function. So for any list l and, importantly, strict \nfunction g, we have f (map gl)= map g (fl). The formal counterpart to the extra care exercised above \nregarding potential occurrences of . is the provision of Wadler (1989, Sec\u00ad tion 7) that only strict \nand continuous relations should be allowed as interpretations for types. In particular, when interpreting \nquanti.cation over type vari\u00adables by quanti.cation over relation variables, those quanti.ed re\u00adlations \nare required to contain the pair (., .), also signi.ed via \u00b7 the added \u00b7 in the new notation R : t1 . \nt2. With straight\u00adforward changes to the required constructions on relations, such \u00b7 as explicitly including \nthe pair (., .) in [R]:[t1] . [t2] and \u00b7 Maybe R : Maybe t1 . Maybe t2, and replacing the least by the \ngreatest .xpoint in the de.nition of [R], we get a treatment of free theorems that is sound even for \na language including general recursion, and thus nontermination. For the extension to the setting with \ntype constructor classes (cf. Section 3), we will need to mandate that any relational action, \u00b7 now \ndenoted F : .1 . .2, must preserve strictness, i.e., map \u00b7\u00b7 R : t1 . t2 to FR : .1 t1 . .2 t2. Apart \nfrom that, De.nition 1, for example, is expected to remain unchanged (except that R and S will now range \nover strict relations, of course). Under these assumptions, we can investigate the impact of the presence \nof general recursion on the results seen in the main body of this paper. Consider Theorem 1, for example. \nIn order to have \u00b7 F : . . Id in its proof, we need to change the de.nition of FR as follows: -1 FR \n= {(., .)}. (return . ; R ; Id) . For this relational action to be a Monad-action, we would need the \nadditional condition that . >>=. k1 = k1 . for any choice of k1. Then, (f.,fId) . [F idInt] .F idInt \nwould allow to derive the following variant, valid in the presence of general recursion and .. Theorem \n1 . Let f :: Monad \u00b5 . [\u00b5 Int] . \u00b5 Int, let . be an instance of Monad satisfying law (1) and . >>= . \nk = k . for every (type-appropriate) k, and let l :: [. Int]. If every element in l is a return.-image \nor ., then so is f. l. Note that the Reader monad, for example, satis.es the conditions for applying \nthe thus adapted theorem. Similar repairs are conceivable for the other statements we have derived, or \none might want to derive. Just as another sample, we expect Example 7 to change as follows. Example 7 \n. Let f :: Monad \u00b5 . [\u00b5 Int] . \u00b5 Int, let t be a closed type, and let l :: [State t Int]. If for every \nelement State g in l, the property P (g) de.ned as P (g) := .s. snd (gs)= s . snd (gs)= . holds, then \nalso fl is of the form State g for some g with P (g). Note that even if we had kept the stronger precondition \nthat snd . g = id for every element State g in l, it would be im\u00adpossible to prove snd . g = id instead \nof the weaker P (g) for fl = State g. Just consider the case that f invokes an immedi\u00adately looping computation, \ni.e., fl = . = State ..8 The g = . here satis.es P (g), but not snd . g = id. 8 The equality . = State \n. holds by the semantics of newtype in Haskell.  \n\t\t\t", "proc_id": "1596550", "abstract": "<p>Free theorems are a charm, allowing the derivation of useful statements about programs from their (polymorphic) types alone. We show how to reap such theorems not only from polymorphism over ordinary types, but also from polymorphism over type constructors restricted by class constraints. Our prime application area is that of monads, which form the probably most popular type constructor class of Haskell. To demonstrate the broader scope, we also deal with a transparent way of introducing difference lists into a program, endowed with a neat and general correctness proof.</p>", "authors": [{"name": "Janis Voigtl&#228;nder", "author_profile_id": "81100011863", "affiliation": "Technische Universit&#228;t Dresden, 01062 Dresden, Germany", "person_id": "P1614020", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1596550.1596577", "year": "2009", "article_id": "1596577", "conference": "ICFP", "title": "Free theorems involving type constructor classes: functional pearl", "url": "http://dl.acm.org/citation.cfm?id=1596577"}