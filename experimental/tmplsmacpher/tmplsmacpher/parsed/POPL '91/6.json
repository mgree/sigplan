{"article_publication_date": "01-03-1991", "fulltext": "\n Dependence Flow Graphs: An Algebraic Approach to Program Dependencies Keshav Pingali Micah Beck Richard \nJohnson Mayan Moudgill Paul Stodghill Department of Computer Science, Cornei! University, Ithaca, NY \n14853  Abstract The topic of intermediate languages for optimizing and parallelizing compilers has received \nmuch attention lately. In this paper; we argue that any good repre\u00ad sentation of a program must have \ntwo crucial prop erties: first, it must be a data structure that can be rapidly traversed to determine \ndependence information, and second this representation must be a program in its own right, with a parallel, \nlocal model of execu\u00ad tion. In this paper, we illustrate the importance of these points by examining \nalgorithms for a standard optimization global constant propagation. We dis\u00ad cuss the problems in working \nwith current representa\u00ad tions. Then, we propose a novel representation called the dependence flow graph \nwhich has each of the proper\u00ad ties mentioned above. We show that this representation leads to a simple \nalgorithm, based on abstract interpre\u00ad tation, for solving the constant propagation problem. Our algorithm \nis simpler than, and as efficient as, the best known algorithms for this problem. An interesting feature \nof our representation is that it naturally incor\u00ad porates the best aspects of many other representations, \nincluding continuation-passing style, data and program dependence graphs, static single assignment form \nand dataflow program graphs. I This ~e~earch w.= supported by an NSF Presidential Yowg Investigator award \n(NSF grant #CCR 8958543), and by grants from the Digitaf Equipment Corporation, IBM, and Hewlett-Packard \nCorporation. 2 Comments regarding thk paper should be directed to pingali@cs.cOrnell .edu. Permission \nto copy without fee all or part of this material is granted provided that the copies are not made or \ndistributed for direct commercial advantage, the ACM copyright notice and the title of the publication \nand its date appear, and notice is given that copying is by permission of the Association for Computing \nMachinery. To copy other\u00adwise, or to republish, requires a fee and/or specific permission. 1 Introduction \nThe growing complexity of optimizing and parallelizing compilers has re-focused the attention of the \nprogram\u00adming languages community on the design of interme\u00addiate program representations. Some well-known \nrep\u00adresentations are: control flow graphs [ASU86], clef-use chains [AS U86], data dependence graphs [KUC78], \npro\u00adgram dependence graphs and webs [FOW87, BM090], program representation graphs [CF89], static sin\u00adgle \nassignment form [CFR+89], continuation-passing style [Ste78], and program graphs [Ack84]. The choice \nof program representation has a profound effect on the de\u00adsign, asymptotic complexity, and implementation \nof op\u00adtimizing and parallelizing transformations. As an anal\u00adogy, consider Hindu numerals3, which are \nmore conve\u00adnient than Roman numerals for performing arithmetic operations, while representing the same \ninformation. In this paper, we argue that a good intermediate represen\u00adtation should have the following \nproperties: It should be executable. That is, it should be a language with a well-defined, compositional \noper\u00adational semantics. This allows abstract interpre\u00adtation to be employed when designing algorithms, \nwhich facilitates systematic algorithm development and proof of correctness [CC77, CC79]. It should be \npossible to view the representation as a data structure that can be traversed efficiently for data dependence \ninformation, as required by many compiler transformations [Kuc78]. Loops should be represented explicitly. \nSome rep\u00adresentations replace loops with tail-recursive pro\u00adcedures [AA89, Eka90]. In our experience, \nthis transformation is not desirable since many im\u00adportant loop transformations, such as loop inter\u00adchange, \nhave no natural analog in the context of tail-recursive procedures. The storage model should include \nan updatable, imperative store. The operational semantics of an 3Because of an unfortunate instance \nof alksing, these are fcnown as Arabic numerals in the West. imperative language is phrased naturally \nin terms of an updatable store. While it is possible to treat the store functionally (as is done in denotational \nse\u00admantics), such treatments are quite clumsy in deal\u00ading with data structures, especially arrays [ANP89]. \n. The represent at ion should be compact. A new pro\u00adgram representation whose size is asymptotically \nbigger than that of well-accepted representations (such as clef-use chains) is unlikely to gain accep\u00adtance. \nIn this paper, we illustrate the importance of these issues by examining a particular optimization global \nconstant propagation. This optimization is performed by all optimizing compilers and is representative \nof scalar optimizations such as partial redundancy elim\u00adination and strength reduction. We discuss the \ndraw\u00adbacks of the representations cited above, and demon\u00adstrate how a representation that meets our criteria \nleads to a simple, elegant algorithm based on abstract inter\u00adpretation. This algorithm is as efficient \nas the best algorithms that use the other forms, and has an ele\u00adgant proof of correctness. Our representation, \ncalled the dependence jlow graph, is based on a generalization of the dataflow model of computation, \ncalled dependence\u00addriven execution. Interestingly enough, many fea\u00adtures of previously proposed intermediate \nrepresenta\u00adtions arise naturally in the context of dependence flow graphs. This paper makes the following \ncontributions: For compiler writers, we propose a novel interme\u00addiate program representation that has \ndemonstra\u00adble advantages over existing representations. This intermediate representation can be used \nfor both functional and imperative languages.  For designers of optimization algorithms, we demonstrate \nhow the simple and powerful tech\u00adnique of abstract interpretation can be used with\u00adout loss of efficiency \non dependence flow graphs. At present, this technique is not widely used be\u00adcause abstract interpretation \nof control flow graphs is inefficient.  o We describe a novel algorithm for global constant propagation. \nThis algorithm is simpler than, and as efficient as, the most powerful algorithms to date, such as the \none due to Wegman and Zadeck [WZ84]. . For dataflow researchers, we demonstrate a way of implementing \nimperative languages like FORTRAN on dataflow machines. To date, these machines ex\u00adecute only functional \nlanguages. The availability of FORTRAN will make these machines acceptable to a much wider group of users. \nWe propose a provocative view of the future of the dataflow model of computation [Den74]. Conven\u00adtionally, \nthe dataflow model is viewed as a way of organizing parallel architectures for executing func\u00adtional \nlanguage programs, but these ideas have not had a major impact on mainline architectures. Our results \nsuggest that the dataflow model of computa\u00adtion may yet find its destiny as a way of organizing information \n in imperative language (FORTRAN) compilers! In Section 2, we illustrate the drawbacks of previ\u00adously \nproposed representations by describing how con\u00adstant propagation is performed on these representations. \nIn Section 3, we present dependence flow graphs along with a formal, Plotkin-style operational semantics. \nIn Section 4, we present our algorithm for constant prop\u00adagation on dependence flow graphs, and we indicate \na proof of correctness. Finally, in Section 5 we discuss ongoing work.  2 Constant Propagation In this \nsection, we examine the problem of global con\u00adstant propagation, a standard analysis performed by optimizing \ncompilers. We define a particularly ambi\u00adtious class of constants, the possible-paths constants, which \nis discovered by an algorithm due to Wegman and Zadeck [WZ84]. We then consider a number of in\u00adtermediate \nforms most commonly used for optimization in imperative language compilers. Finally, we show how previous \nconstant propagation algorithms have been af\u00adfected by the shortcomings of these underlying repre\u00adsent \nat ions. 2.1 Problem Description A definition of a variable x is a statement that assigns (or may assign) \nto x. A use of x is an occurrence of x in a statement that reads (or may read) the value of x. We say \nthat a definition of x reaches a use of x if execution of the definition may be followed by execution \nof the use without intervening execution of any other definition of x, As is standard, this definition \nassumes that conditional branches may go either way [ASU86]. If the right hand side of a definition of \nx is a con\u00adstant c, we can sometimes substitute c for a use of x without changing the meaning of the \nprogram. For ex\u00adample, in Figure l(a), the first use of z can be replaced by 1 and the second by 2. This \nis a simple example of constant propagation. If all of the variables on the right hand side of a definition \nare replaced by constants, then we can evaluate the expression and replace it by a con\u00adstant. Recursively, \nthis opens up fresh opportunities for constant propagation. For example, in Figure l(a), the right hand \nsides of the two definitions of x can be p := true if (p) then if (p) then {Z:= 1; X:= Z+2} {X:=l} else \nelse {Z:= 2; X:= Z+1} {x:=2} y:=x y:=x (a) all-paths (b) possible-paths Figure 1: Examples of runtime \nconstants simplified to the constant 3. The use of x in the last statement is reached by two definitions \nof x, However, since the right hand sides of both definitions are the same constant, we can replace the \nuse of x by 3 without changing the meaning of the program. This motivates the following definition. An \nall-paths constant is either: a constant expression c, or . an expression e over some set of variables \n{Vi, V2,... Vm} such that for each vi, the right hand side of every definition of vi that reaches e is \nan all-paths constant Ci. The class of all-paths constants takes no account of constants in conditionals. \nHowever, if the predicate of a conditional can be determined to be constant, then we can ignore the effect \nof definitions on the side that is never executed. If we modify the definition of all-paths constants \nto exclude such definitions, the result is the class of possible-paths constants [WZ84]. In Figure l(b), \nthe use of x in the last statement is a possible-paths constant with value 1. Note that this use is not \nan all\u00adpaths constant. A variety of algorithms for constant propagation have been proposed in the literature \n[ASU86, Ki173, RL77, WZ84]. Some of these algorithms are more powerful than others for example, only \nthe algorithm of Weg\u00adman and Zadeck [WZ84] finds possible-paths constants in a single pass. Repeated \napplication of the less pow\u00aderful algorithms, combined with dead code elimination, will find all possible-paths \nconstants. However, repeated rounds of program transformation and analysis are ex\u00adpensive, and so we \nseek to discover as many constants as possible in a single pass through the program. As we will see, \nthe choice of program representation plays a critical role in this task, It is standard to express constant \npropagation algo\u00adrithms in the framework due to Kildall [Ki173]. We de\u00adfine a lattice Lat shown in Figure \nZ, consisting of all the constant values and two distinguished values T and 1. The special constant $ \nis used only in dependence flow graphs, and plays no part in the algorithms described in this section. \nUses of variables are assigned values from (cannot be determined to be constant) T true false O -1 1 \n-2 2... $ (not ye;considered) Figure 2: Lat Lattice for constant propagation Lat during constant propagation. \nInitially, every use of every variable is mapped to 1, meaning that we have no information yet about \nthe values that it is assigned at runtime. A use is mapped to T when the algorithm cannot determine that \nthe use is a constant (e.g. if the use is reached by two definitions whose right hand sides are 3 and \n4.)4 At the end of constant propagation, the interpretation of the lattice value assigned to a use of \na variable x is as follows: J-: This use was never examined during constant propagation; it is dead code. \nc: This use of x has the value c in all executions. T: This use of x may have different values in dif\u00adferent \nexecutions. To permit evaluation of right hand sides of definitions in the abstract interpretation, \nit is convenient to extend the usual arithmetic and boolean operators so that they can take 1. and T \nas arguments. For example, the op erator VI + V2 is interpreted as follows: T ifvl=Torvz=T Pius(vl, V2) \n= c1 + C2 if V1= c1 and vz = cz 1 otherwise {  2.2 Control Flow Graphs Figure 3(b) shows the controi \njlow graph [ASU86] for a small imperative program. Nodes are either assignment statements or conditional \nexpressions that affect flow of control, and edges represent possible transfer of control between nodes. \nAn assignment node has a single succes\u00adsor while a conditional node has two successors repre\u00adsenting \nthe possible branching of control. In our figures, we follow the convention that the true branch of a \ncondi\u00adtional is always left-most. Algorithms for constructing 4pJote that the sense of T and 1 in the \nlattice are reversed with respect to the lattice used by previous researchers [K1173, RL77, WZ84]. These \nresearchers viewed constant propagation as an all-paths data flow problem; such problems are traditionally \nformulated so that the desired solution is the greatest fixed point of a set of equations. In our framework, \nwe will use abstract inter\u00adpretation to find constants, and it is more convenient to formulate the desired \nsolution as the least fixed point of a set of equations. START T x:-1 T X:=l y:-2 y;=2 9 if (x~l) n \nthen y:= 3 A Y ... ... y,-3 X:=2 RI ,,,Y , ~ G1  (a) Source Program (b) Control flow Graph x:-1 flow \n7 x~l output flow / y:-3 p anti ? flow G1 El (c) Data Dependence Graph Figure3: A Small Program and \nits Representations the control flow graph representation ofa program are well-known [AS U86]. Control \nflow graphs have a sim\u00adple sequential semantics based on transforming a global imperative store. A simple \nalgorithm based on abstract interpretation finds possible-paths constants in the control flow graph. \nAt each node, we maintain a vector of values from Lat. These vectors have an entry for each variable, \nand intu\u00aditively, they summarize the possible values of variables before the statement is executed. Initially, \nevery entry at every node is set to 1 except at the START node where the entries are set to T. These \nvalues are up\u00addated monotonically (in the lattice-theoretic sense) as the algorithm proceeds. The algorithm \nmaintains a worklist of nodes to be processed; initially, this worklist contains all of the nodes immediately \nfollowing the START node. Nodes are dequeued from the worklist and processed as fol\u00adlows. Let iV be a \nnode from the worklist, and let i Vin be the vector at the input of N. If the node is an assignment statement, \n(say x := e), then the expression e is evaluated, using the values of variables in Nin, and a new vector \nNOUt is created that is identical to Nin except at x where it has the value just computed for e. The \nvector NOUt must be propagated to the successor node S of the assignment statement. Let Sin be the vector \nat the successor node. This vector is updated to the join of its value and NOUt.5 If this changes the \nvalue of S~n, then S is added to the worklist. o If the node is a conditional branch, then the pred\u00adicate \nis evaluated. If the value of the predicate is T, then the vector Nin is propagated to both suc\u00adcessors \nof the conditional branch. If the value of the predicate is true or false, then Nin is propa\u00adgated only \nalong the corresponding side of the con\u00additional branch. If the value of the input vector changes at \na successor, then the successor is added to the worklist. We leave it to the reader to verify that this \nalgorithm will find all of the constants in Figure 1. Unfortunately, the asymptotic complexity of this \nalgorithm is poor. If we let V be the number of program variables and N the number of statements, then \nthe algorithm requires O(NV) space and 0(NV2) time. Although the abstract interpretation algorithm on \ncontrol flow graphs is sim\u00adple, it is not used in practice because of its high cost. The inefficiency \narises because lattice values must be propagated along control flow paths from definitions of variables \nto their uses. What is needed is a sparse representation that links definitions to the uses they reach, \nso that we can propagate the values of individual variables to places where they are needed, rather than \npropagating the values of all variables to all program locations. Def-use chains and their generalization, \ndata dependence graphs, provide such a representation. 2.3 Data Dependence Graphs Def-use chains are \ngraphs that have the same nodes as control flow graphs, but the edges connect each de fini\u00adt ion of a \nvariable to all uses reached by that definition. For compilers that perform wholesale reorganization \nof programs, a generalization of def-use chains called the data dependence graph [KUC78] is commonly \nused. The data dependence graph for our example is shown in Fig\u00adure 3(c). Edges in the graph represent \ndependencies that are classified as jlozu (clef-use), anti (use-clef), or output (def-def) dependence. \n Note that the data dependence graph is not an ex\u00adecutable representation and does not incorporate in\u00adformation \nabout flow of control. For example, in Fig\u00adure 3(c), execution of the definition y := 3 is not related \nto the predicate x &#38; 1 in any way. Negating the pred\u00adicate will change the value of y that is read, \nbut this 6 We cannot simply store Noti~ at Sin because S may have other predecessors. If a node has \ntwo predecessors, z := 2 and z := 3, then the entry for z at its input should be T. does not change the \ndependence edges that sequence operations on y. The constant propagation algorithm based on clef-use \nchains is similar to the one described earlier for con\u00adtrol flow graphs, except that we propagate lattice \nval\u00adues from definitions to uses along clef-use edges, rather than along control flow paths. At each \nnode, we keep a vector containing values only for those variables that are used by the node, A worklist \nis kept of nodes to be processed. Initially, every definition whose right hand side is a constant c is \nplaced on this worklist. To process a definition of the form x := e, the expres\u00adsion e is evaluated, \nand its value is propagated along clef-use edges originating at this node to nodes that use x. Conditional \nnodes do not need any processing, since there are no clef-use chains originating at these nodes. The \ncomplexity of this algorithm is linear in the size of the clef-use chains of the program, If we let E \n~ 2N be the number of edges in the control flow graph, then a naive representation of def\u00aduse chains \ncan be O(E2V ) in size. However, Reif and Lewis have shown that a factored form of clef-use chains can \nbe represented in size O(EV) [RL77]. This yields an algorithm which is a factor of V faster than the \none which uses the control flow graph. Although this algorithm will find all-paths constants as in Figure \n1(a), it will not find possible-paths con\u00adstants as in Figure 1(b). In relying on data dependence information \nto direct the flow of values, we have lost important connections between data dependence and flow of \ncontrol. In our control flow algorithm, values were propagated down only one side of a conditional with \na constant predicate. This was possible because the control flow path from the definition to the use \npassed through the conditional. Def-use edges do not flow through conditionals, but reach directly from \ndefi\u00adnitions to uses. Thus, our new algorithm does not find all possible-paths constants, and we conclude \nthat def\u00aduse information needs to be augmented with control flow information in some way.  2.4 Control \nFlow Graphs with Data De\u00adpendence Graphs Most optimizing compilers generate a control flow graph as a \nfirst step towards computing the data dependence graph. This suggests the development of hybrid al\u00adgorithms \nthat use both data structures. The constant propagation algorithm described next is adapted from that \nof Wegman and Zadeck [WZ84], To find possible-paths constants while still obtaining the efficiency of \nclef-use chains, Wegman and Zadeck re\u00adfer back to the control flow graph. To keep propagation of values \nfrom bypassing conditionals, a boolean eze\u00ad cut able flag is added to each statement, and is initially \nset to false, except for START. The executable flag in\u00adX;=2 x:= 2 p := true p := true if(p) then {x:= \n1} if(p) then {x:= 1} else {x:=x} Yx. . y:=x (a) one-sided (b) dummy assignment Figure 4: Transforming \na one-sided conditional dicates that the statement maybe executed, i.e.,that i{ has not been determined \nto be dead. Lattice values flow along clef-use chains as before; in addition, information about which \nnodes may execute flows through the control flow graph. These two flows are not independent since intermediate \nresults of con\u00adstant propagation may be used to determine that one side of a conditional is never executed. \nConversely, a definition does not participate in constant propagation until it is determined that it \nmay be executed. Two worklists keep track of these two flows: the jlow work\u00adlist and the def worklist. \nThe flow worklist is used to propagate the executable flag through the control flow graph. If a non-conditional \nnode N may be executed, then its successors may be executed. Once the predicate of a conditional has \nbeen assigned a lattice value, the executable flag can be prop\u00adagated down one or both sides aa appropriate. \nThe def worklist is used to propagate lattice values along clef-use edges w in the previous algorithm. \nNodes are placed on a worklist only if their executable flag is truq conditionals are placed on the flow \nworklist and definitions are placed on the def worklist. Initially, the flow worklist contains only START \nand the def worklist is empty. Although this algorithm finds the possible-paths con\u00adstants in Figure \n1, it fails to discover the one-sided possible-paths constant in Figure 4(a) since the assign\u00adment x \n:= 2 reaches the use of x and is not dead. Weg\u00adman and Zadeck suggest transforming every one-sided conditional \nby inserting a dummy assignment of the form x := x on the else branch. In the transformed program, as \nin Figure 4(b), only one value is propa\u00adgated through the conditional if the predicate is con\u00adstant. \nWhen performed on the transformed program, the Wegman-Zadeck algorithm does find the possible\u00adpaths constants. \nThe problem of maintaining two data structures to represent the program s execution semantics and its \nde\u00adpendencies is addressed in part by the program depen\u00addence graph. This graph consists of the data \ndependence graph augmented with control dependence arcs. A more elegant constant propagation algorithm \nbased on the one described in this section can be developed using the program dependence graph. However, \nprogram depen\u00advar x, y: integer pendence flow graphs are a synthesis of ideas from data v, vl: integer \nb: boolean dl, d4, d5 : output d2, d4, d6 : flow x d7, d8, dlO: flow x:-1 y:.=2 d3, d9: anti T ci2 d4 \nload x dl =START () d3 T d2 =store (x,l,dl) v E% witch v,d3 =Ioad (x,d2)05 dfi b =equal (vtl) :-3 d4 \n=store (y,2,dl) d5,d6 =switch (b,d4) ~ ?Td7 d7 =store (y,3,d5) merge d8 = merge (d7,d6) 9 d8 vl,d9 =Ioad \n(y,d8) x:-2 load y d10 =store (x,2,d3) Y d10 Rd9 VI Figure 5: Dependence Flow Graph for a Small Program \n dence graphs inherit many of the problems of the data dependence graph; for example, for constant propaga\u00adtion, \nwe must still perform the program transformation shown in Figure 4. Moreover, they do not have a simple, \nlocal execution semantics [cF89]. 2.5 Summary The control flow graph allows us to formulate a sim\u00adple \nalgorithm, based on abstract interpretation, that finds possible-paths constants without the need for \npro\u00adgram transformations. However, its asymptotic com\u00adplexity is poor. Algorithms that use the various \ndepen\u00addence graphs are more complex, and none of them find possible-paths constants without some program \ntrans\u00adformation. However, the asymptotic complexity of these algorithms is a factor of V better than \nthe algorithm that use control flow graphs. An ideal program representation for constant prop\u00adagation \nwould have a local execution semantics from which an abstract interpretation can be easily derived. It \nwould also be a sparse representation of program de\u00adpendencies, in order to yield an efficient algorithm. \nLike a Necker cube, this representation will permit two points of view it can be viewed as a data structure \nthat can be traversed efficiently for dependence information, but it can also be viewed as a precisely \ndefined language with a local operational semantics. The dependence flow graph is just such a representation. \n 3 Dependence Flow Graphs Figure 5 shows the dependence flow graph for the im\u00adperative language program \nconsidered in Figure 3. De\u00addependence graphs and the dataflow model of compu\u00adtation. As in the data dependence \ngraph, the depen\u00addence flow graph can be viewed as a data structure in which arcs represent dependencies \nbetween opera\u00adtions. It is easy to verify that for every dependence arc in the data dependence graph \n(Figure 3), there is a corresponding path in the dependence flow graph (Fig\u00adure 5). However, unlike data \ndependence graphs, de\u00adpendence flow graphs are executable, and the execu\u00adtion semantics, called dependence-driven \nexecution, is a generalization of the data-driven execution semantics of dataflow graphs. In dataflow \ngraphs, nodes represent functional operators that communicate with each other by exchanging value-carrying \ntokens along arcs in the graph. These arcs can be viewed as flow dependencies since they connect a node \nproducing a value, such as an integer or boolean, to nodes that consume this value; we will call such \narcs functional dependencies in our presentation. In Figure 5, v, vI, and b are functional dependencies. \nWe extend the dataflow model by adding an impera\u00adtive (updatable) global store and two operations called \nload and store which manipulate it. As one would ex\u00adpect, the load operator reads the contents of a storage \nlocation and outputs the value as a token. The store op\u00aderator is the inverse of the load operator it \nreceives a value on a token and stores it into a memory loca\u00adtion. To sequence these operations, we introduce \na new kind of arc called an imperative dependence. For exam\u00adple, in Figure 5, d2 and d3 are imperative \ndependencies that sequence operations on location x, corresponding to arcs in the data dependence graph. \nTo preserve the local, token-pushing semantics of dataflow graphs, we make load and store operators produce \na special token, $, when they have completed. These tokens flow down imperative dependence arcs to enable \noperators at the destinations of those arcs. For example, when the x := 1 operator executes, it produces \na token carrying $ on line d2. This is said to satisfy the dependence d2, thereby enabling the load x \noperator for execution. When the load x operator executes, it produces tokens carrying $ on line d3 and \nthe value 1 on line v, In this way, oper\u00adations on a given memory location are sequenced, but operations \non different locations can execute in parallel. Imperative dependencies are further classified aa flow, \nanti and output as in data dependence graphs. We classify d2 as a flow dependence and d3 as an anti\u00addependence. \nNote that dependence d4 is both a flow and an output dependence, since logically it corresponds to both \nof the dependence arcs coming out of the definition y := 2 in the data dependence graph. Dependence arcs \nthat sequence operations on location y are intercepted by switch and merge operators, which implement \nflow of control as discussed below. These operators serve to combine control information with data dependencies, \nwhich is exactly what is missing from the represent~ tions discussed in Section 2. To understand dependence \nflow graphs, it is useful to execute the graph depicted in Figure 5 by pushing to\u00adkens. Execution begins \nwhen the START operator sends a token carrying $ to the store operations x := 1 and y := 2. Depending \non whether the token received on arc b is true or false, the switch operator outputs the token it receives \non d4 onto either arc d5 or d6. In our example, the switch routes the token to d5, and the def\u00adinition \ny := 3 is executed. The merge operator receives a token on either one (but not both) of its inputs, and \nsimply outputs this token. The reader can verify that a token carrying the value 3 will be generated \non arc v1. In a forthcoming paper, we will describe how depen\u00addence flow graphs are constructed, starting \nfrom the control-flow graph of a program. This construction can handle unstructured control-flow. Some \npreliminary ideas are presented in an earlier paper [B P90]. From an analysis of the construction, we \nshow two facts. Dependence flow graphs constructed by our algo\u00adrit hm sat isfy Bernstein s conditions: \nthat is, a store operator can never be enabled for execution simul\u00adtaneously with another store or load \noperator on the same storage location [Ber66].  The dependence flow graph of a program whose control \nflow graph has E edges and V variables has size O(EV).  Although token-pushing provides useful intuition, \nwe adopt a different style of operational semantics in the formal development. Arcs in the dependence \nflow graph can be viewed aa names that represent a set of single assignment registers or temporaries. \nProducing a token carrying a value on an arc is similar to storing that value in the corresponding register. \nExplicit load and store operators to transfer values between the global store and a set of registers/temporaries \nhave been used in the PL.8 compiler [AH82] and many Scheme compilers [Ste78]. We develop this point of \nview in the rest of this section. 3.1 Acyclic Dependence Flow Graphs: Formal Semantics From a formal \nperspective, a dependence flow graph is a set of declarations followed by a set of definitions, Dec\u00adlarations \nintroduce names for locations in the store and for dependencies, which can be viewed as names for a set \nof single assignment registers or temporaries. The body of the dependence flow graph is a set of definitions. \nA definition is an equation with a left hand side consisting of one or more dependencies, and a right \nhand side con\u00adsisting of the application of an operator to dependencies, locations, and constants. The \noperators and their arity are shown in the left column of Figures 6. A definition is said to be a source \nfor dependencies named on the left hand side of the equation and a sink for dependen\u00adcies named on the \nright hand side. A dependence has exactly one source but can have many sinks. We now give a Plotkin-style, \nformal operational se\u00admantics for dependence flow graphs [P108 1]. Rather than rewrite programs, as is \ncommon in this style of semantics, we will define a state transition semantics in which we rewrite configurations. \nInformally, a con\u00adfiguration represents the state of the computation and a transition represents a step \nin the computation. In our system, a configuration is a pair consisting of an environment and a store. \nTo define them, we need the following sets. V = Bool U Int U {$} k the set over which we com\u00adpute. The \nmetavariables b and c stand for elements of v.  Loc = {Lo, LI, ...} is an infinite set of global store \nlocations. The metavariable z stands for an ele\u00adment of Lot.  Dep = {do, dl, . ..} is an infinite set \nof dependencies. The metavariables d, v, p, and tstand for elements of Dep.  The environment keeps \ntrack of the state of dependen\u00adcies in the program and the store keeps track of the state of locations \nused by the program. The environment is a mapping from program dependencies to the set V. For technical \nreasons, a dependence will be added to the en\u00advironment only when it is satisfied; therefore, the initial \nenvironment is empty. The environment grows mono\u00adtonically during execution, in the sense that as compu\u00adtation \nprogresses, dependencies are only added to and never deleted from the environment; in addition, the value \nbound to a functional dependence in the environ\u00adment never changes. Similarly, the store is a mapping \nfrom the set of locations used in the program to the set V; however, locations can be updated arbitrarily. \nThe store, like the environment, is initially empty and a location is added to the store the first time \na value is stored to it. Definition 1 1. An environment p : D + V is a jinite function its domain D \nC Dep is finite. 2. A dependence d is said to be defined or satisfied in p if d is in the domain of \np. Otherwise, it is said to be undefined in p. The notation p[v I+ c] represents an environment identical \nto p ezcept for dependence v which is mapped to c. 3. A store 0: L + V is a jinite junction its domain \nL c Loc is jinite. Just as for dependencies, we can  d undef d d=starto: (p,u)~ (p[dw$],a) tl, tzdef \nd A t undef d t=op(tl, t2): (p,a) + (p[tOp(p[tl],a) l-+p[t,])], i$I , tF = switch (p, t) : p~] = true \nA t def d A t2 , tF undef d (P, 4 + (P[~T * P[t]], u) tT, tF= switch (P, t) : p~] = false A t def d \nA tT, tFundef d (P, U) -+ (P[tF -P[t]], U) tl def d A t,tzundef d (p, u)+ (p[t = p[t,]], a) tz def d \nA t,tlundef d t = merge (tl, tz): (p, a)+ (p[t -p[t2]], a) v, d~tit = load (z, d;~) : d~~, a[z] def d \nA v, d.ut undef d (P, U)+ (P[~ ++ uIzI,LmtI-+$],U) d~~~ = store (z, V, (tin) : v, din def d A d~~t undef \nd (P, ~)+ (P[&#38;wt + $1,4Z -PM) Figure 6: Transition Rules for Acyclic DFGs talk about a location x \nbeing defined or undefined in a store u. A configuration is a pair (p, a) consisting of an environment \nand a store. The initial configuration has empty environment and store. Figure 6 shows the transition \nrules for acyclic depen\u00addence flow graphs. The left column consists of defini\u00adtions and the right column \nshows a precondition above the line and a transition below the line. If the defini\u00adtion in the left column \nis present in the dependence flow graph and the precondition on top of the line is satis\u00adfied, then the \ntransition shown below the line can be performed. As an example, consider a definition of the form t \n= add (tl, t2). We would expect this operator to exe\u00adcute when tland tzare defined. Once this operator \nhas executed, we want to disable this transition. Therefore, we perform the transition only if t is undefined. \nThe load and store operators are the only operators that access the store. The load operator checks that \nthe contents of location z are defined; this will catch an attempt to read from an uninitialized location. \nThe switch and merge operators implement flow of control. Depending on the boolean, value p, the switch \noperator satisfies either de\u00adpendence tT or tF. The dependence at the output of the merge is satisfied \nwhen either of the dependencies tin. I def d A t.I.l undef d t = IOOP (tin, tbctck) : (P, a) + (p[t.1.l \n++ p[tin.~]]j ~) i! = IOOP (tin, ~back) t tb..k .I. j def d A t.1.j+ 1 undef d (p, a) e (p[t.~.j+l * \np[tback.~.j]], ~) i!, tback = until (~, fin) : p~.I.j] = false A ti~.I.j def d A &#38;Ck.I.j undef d \n(P, 4 + (P[tback.I.j * P[tiJ.j]], ~) t, ~back = until (j), tin) : p~.I.j] = true A ti~.1.j def d A t.I \nundef d (Pj ~) + (P[t.~ + p[tifi.~.j]], u) Figure 7: Transition Rules for Loop Operators at its input \nis satisfied. Notice that the rules check that at most one input dependence is satisfied.  3.2 Cyclic \ndependence flow graphs As far as the input/output behavior of programs goes, loops can be replaced by \ntail-recursion. However, many loop optimizations, such as loop interchange, have no natural analog in \nthe context of tail-recursion, so we felt it was important to model loops directly using cyclic dependence \nflow graphs. For this, we need two new operators called loop and until which are used at loop entrance \nand loop exit respectively. In addition, the transition rules for the operators discussed in Section \n3.1 must be altered slightly. Consider the definition t= add (t1,tz)occurring in\u00adside a loop. trepresents \na different dependence in each iteration; to model this we index tby the iteration num\u00adber, so that t.irepresents \nthe dependence t in the ith loop iteration.6 t.1 is the dependence in the first iter\u00adation. This scheme \nextends naturally for nested loops so that for a two-dimensional loop, t.i.jrepresents this dependence \nin iteration i of the outer loop and iteration j of the inner loop. It is sometimes convenient to write \nthis as t.I where I is a (two dimensional) index vector i.j. To reflect this intuition, the definition \nof environ\u00adments is modified: Definition 2 Let Seq be the set of finite sequences of positive integers, \nincluding the empty sequence. An en\u00advironment p : DS + V is a finite function its domain DS c Dep x \nSeq is finite. To avoid introducing more notation, we will let the term dependence stand for both an \nidentifier (arc) in the dependence flow graph and its dynamic instance in This device is like scalar \nexpansion PW86]. 74 START dl ++ x := 1; dm loop x := X+l until (x > 10) dw VI b dull (a) Source Program \n(b) Dependence Flow Graph Figure 8: Dependence Flow Graph of a Simple Loop various iterations, relying \non context to make the dis\u00adtinction clear. For any index vector I, the add operator can execute aa soon \nas its operands are available, i.e. as soon as tl.I and t2.1 are defined. Therefore, the rule for the \nadd becomes: t = add (tl,tz): t,.1., tn.1def d A t.I undef d (P, u)-+ (PILI -(P[tl.q + P[h.q)], u) The \ntransition rules for the other operators shown in Figure 6 are extended in a similar manner. We now discuss \nthe semantics of the loop and exit operators. Figure 8 shows a simple loop and its depen\u00addence flow graph. \nIn the first iteration, the statement x := x+1 reads the value of x assigned by the statement x := 1 \noutside the loop. Therefore, in the dependence flow graph, we must have a dependence from the assignm\u00adent \nstatement outside the loop to the use within the loop. In subsequent iterations, the statement x := x+1 \nreads the value of x assigned in the previous iteration. Therefore, in the dependence flow graph, we \nmust have a dependence from the assignment to x in the ith itera\u00adtion to the use of x in the (i + l)~h \niteration. The loop operator (see Figure 7) accomplishes this transfer of de\u00adpendence from outside the \nloop into the first iteration and from one iteration to the next. The until operm tor determines if another \niteration of the loop should be executed. In the definition = until (p, tin), if p t, tb..Jjis false \nthen another iteration is to be performed, and the dependence t~.~k is satisfied. Otherwise, the loop \nis to be exited; if this is the I.j iteration of the loop, the dependence t.1outside the loop is satisfied. \n 3.3 Discussion The transition system for dependence flow graphs, as described above is deterministic \nin the sense that it haa the one-step Church-Rosser property. The proof, which we have omitted for lack \nof space, rests on the fact that dependence flow graphs, by construction, satisfy Bern\u00adstein s conditions \n[Ber66]. We refer the interested reader to a companion technical report [PBJ+90]. We can exploit the \none-step Church-Rosser prop\u00aderty to define a simple interpreter for dependence flow graphs. The interpreter \nmaintains an environment and a store, and keeps a worklist of definitions that may be ready for execution. \nThe initial environment and store are empty, and the worklist is initialized to {sTART}. While the worklist \nis not empty, the interpreter de\u00adqueues a definition, checks the precondition and per\u00adforms the transition \nif the precondition is satisfied. All definitions that are sinks for dependencies sourced by the definition \njust executed are then enqueued onto the worklist. The major difference between our representation and \nconventional dependence graphs is that dependencies, for us, are part of the computational model, and \nare manipulated by an algebra of operators. Note that load and store, switch and merge, and loop and \nuntil are, in an algebraic sense, inverses of each other. Data dependen\u00adcies are combined with control, \nand in the next section, we demonstrate how this facilitates the development of optimization algorithms. \n 4 Constant Propagation on De\u00adpendence Flow Graphs In this section, we demonstrate how global constant \npropagation can be performed on dependence flow graphs using a simple algorithm based on abstract in\u00adterpretation. \nWe show that for any program, we can write down a set of equations whose solution corre\u00adsponds to the \npossible-paths constants for that program. Next, we show that this solution can be computed ef\u00adficiently, \nthereby developing an algorithm that haa the same asymptotic complexity as the algorithm due to Wegman \nand Zadeck [WZ84]. This algorithm can be proved correct by an induction on the length of the com\u00adputation. \n4.1 Equational Characterization of Constants Figure 9 shows how to write down a set of semantic equations \nfrom a dependence flow graph representation of a program. The equations are obtained by replac\u00ading the \noperator in each definition with a function that denotes the abstract interpretation of the operator \nin Lat. We let DenOp stand for the interpretation of an Syntactic Equation Semantic Equation d = START \n() d =$ v = Op (VI, ?M) v = DenOp(vl, v2) w, Vj = switch (b, v) v~, vf = DenSw(b, v) v = merge (VI, v2) \nv =V1UV2 u,d = load (z, din) V, d = din, din d= store (Z, V,din ) d = if din = 1 then J_else v v= ioop \n( Uin7 ~bk) v = ?lin U llbh v, Vbk = until (b, Vin) V, VM = Vin, Vin Figure 9: Abstract Interpretation \nof Operators arithmetic or logical operator op in the domain Lat, as in Section s:constants. The function \nDenSw stands for the interpretation of switch and is defined as follows: c, c ifb=T c,L if b= true DenSw(b, \nc) = ~ ~ if b= false L;J_ ifb=l { This is similar to the standard interpretation of switch, except that \nit deals with the case when b is T if the value of the predicate cannot be determined during constant \npropagation, then the input value c is propagated to both sides of the switch. Therefore, in the abstract \ninterpretation, both inputs of a merge can be defined. The output of a merge operator is constant only \nif both inputs are the same constant or if one side of the merge is never executed, and the other side \nis con\u00adstant. In the equations, this is stated compactly using the least upper bound operator on Lat. \nIn the standard semantics, the global store was used to communicate values between the load and store \noper\u00adators. As is the case in all the algorithms discussed in Section 2, the global store plays no role \nin our constant propagation algorithm. Instead, we take advantage of the fact that there is a flow dependence \npath in the graph from a store to every load dependent on it. Lat\u00adtice values are propagated along these \npaths. The store operator propagates the value of its input to its output, provided that din is not J- \nthat is, if it is possible that this operator may be executed. Therefore, the load operator need only \npropagate the value of input din to its outputs. For any dependence flow graph, we can write down a set \nof semantic equations over Lat in which the func\u00adtions on the right hand side are monotonic and contin\u00aduous. \nIt is a well-known result that such a system of equations has a least solution [Man8 1]. Figure 10 shows \nthese values for the program of Figure 5. The possible\u00adpaths constants can be read off from the least \nsolution of the semantic equations as follows. Let C : D + Lat be the least solution. If t is a functional \ndependence, then, as in Section 2, C[t] = J-means that the operator that defines t will never be executed, \nC[t] = c means that if tis ever assigned a value, it is assigned the value c, and C[t]= T means that \nthe value of t cannot be determined to be constant. The values assigned to im\u00adperative dependencies must \nbe interpreted a little dif\u00adferently. Consider dependence d2 in Figure 10. This dependence is given the \nvalue 1 by the constant propa\u00adgation algorithm, but it is given the value $ in the stan\u00addard interpretation \nwhich uses the global store, rather than dependencies, to transmit values between a store operation and \ncorresponding load operations. If t is an imperative dependence, then C[t] = 1 means that the operator \nthat defines t will never be executed, but if C[t] is any other value, we conclude that this operator \nmay be executed that is, tmay get the value $ in the standard interpretation. To compute. the least \nsolution of the equations effi\u00adciently, we run the dependence flow graph interpreter defined in Section \n3.3, using the abstract, rather than the standard, interpretation of the operators. This ab\u00adstract interpreter \nmaintains only an environment, since the store plays no role in constant propagation. In this environment, \nwe keep only a single value associated with each dependence, rather than an indexed family of values, \nbecause a dependence is constant only if it is constant in all iterations. Since there are only a finite \nnumber of dependencies, we start with an initial envi\u00adronment that maps all dependencies to -L. The work\u00adlist \nof definitions ready for processing is initialized to {START}. While the worklist is non-empty, we remove \na definition from the worklist and update the environ\u00adment using the abstract interpretation rules shown \nin Figure 9. This update to the environment consists of binding new values to the dependencies whose \nsource is the definition being interpreted. If the value bound to a dependence changes, we add every \ndefinition that is a sink for that dependence to the worklist. Since there can be O(EV) edges in the \ndependence flow graph and the value propagated along each edge can change at most twice, the complexity \nof this constant propagation algorithm on dependence flow graphs is O(EV). This is the same asymptotic \ncomplex\u00ad ity as that of the algorithm due to Wegman and Zadeck. This algorithm can be proved correct \nby a simple in\u00ad duction on the length of the execution sequence. We omit the proof for lack of space \nand refer the interested reader to the technical report [PBJ+ 90].  5 Conclusions An interesting aspect \nof dependence flow graphs is that they exhibit many features of previously proposed rep\u00ad resentations. \nThese connections will be discussed in full in another paper here, we summarize them. Connec\u00ad tions \nwith data and program dependence graphs have already been discussed in Section 2.  FE-1 Figure 10: \nResult of Constant Propagation in Figure 5 Continuation passing style (CPS) is an executable representation \nthat was proposed by Steele as a suitable intermediate language for compiling Scheme [Ste78]. Since then, \nit has been used in a number of other com\u00adpilers such as the Standard ML compiler [AM87]. In\u00adformally, \nthe continuation of an operator is a represen\u00adtation of the effect of the rest of the program after the \noperator is executed. The execution model of CPS is se\u00adquential. Arcs in dependence flow graphs can be \nviewed as continuations in a parallel execution model. This generalization is crucial since it lets us \nrepresent depen\u00addence information directly in the representation. How\u00adever, in CPS, continuations are \nfirst-class citizens in the sense that they can be passed into and out of functions, Static single assignment \n(SSA) is a compact represen\u00adtation of clef-use chains that uses so-called #-functions to combine clef-use \narcs [CFR+89]. #-functions are simi\u00adlar to our merge nodes and we get the same compactness advantage \nin our representation, In addition, unaliased variables are renamed so that each variable is assigned \nby just one statement. The result of this renaming is to assign a unique name to each dependence, which \nis convenient for program transformation. In our repre\u00adsentation, the unique naming of dependencies is \nfunda\u00admental and not a variant used for optimizations. Updat\u00adable storage locations can be eliminated \nin a dependence flow graph through a simple program transformation in essence, imperative dependencies \nare converted into functional dependencies [BP90]. In translating the dataflow language VAL into static \ndataflow graphs, Ackerman defined an intermediate rep\u00adresentation called VAL program graphs [Ack84]. \nThis representation has become popular in the dataflow world; with minor modifications, it has been used \nby Traub to translate the dataflow language Id into dy\u00adnamic dataflow graphs [Tra86]. However, this represen\u00adtation \nsupports only functional languages, and cannot handle imperative updates or arbitrary flow of control \nas dependence flow graphs can. At HP Labs, Rau and Schlansker are investigating an intermediate form \ncalled PIF (parallel intermedi\u00adate form) for compiling FORTRAN to VLIW machines. Ekanadham at IBM has \nan intermediate form called Ku\u00addos which is used to translate FORTRAN and functional languages into code \nfor the Empire hybrid dataflow ma\u00adchine [Eka90]. Suhler at IBM, and Ballance, Maccabe and Ottenstein \nat Los Alamos, have been working on implementing FORTRAN on dataflow machines. In an earlier paper [B \nP89], we solved this problem completely, and pointed out the advantages of basing intermediate languages \non the dependence-driven execution model. While our suggestion has been taken to heart by these researchers \n[BM090], it is too early to tell if there will be a convergence of these ongoing efforts. Future Research \nThe ideas presented in this paper form the basis of the Typhoon parallelizing compiler project at Cornell \nUni\u00adversity. We have implemented prototype front-ends that translate programs in FORTRAN and in the dataflow \nlan\u00adguage Id into an intermediate language called Pidgin. Pidgin is a textual form of the dependence \nflow graph data structure, on which our optimizer is built, The optimizing portion of the compiler is \na source-to-source transformer of Pidgin programs. Much of our future work will focus on loop transfor\u00admations \nand scheduling and code generation for specific architectures. We have extended the definition of de\u00adpendence \nflow graphs so that we can represent the de\u00adpendence information needed to implement loop trans\u00adformations. \nPreliminary work on the scheduling of de\u00adpendence flow graphs has focused on generating code for pipelined \nRISC architectures such m SPARC; fu\u00adture work will target dataflow architectures, NUMA m~ chines and \nVLIW architectures. Acknowledgements We are indebted to Radha Jagadeesan for catching sev\u00aderal errors \nin the proofs. It is a pleasure to acknowledge the support and encouragement of Bob Rau and Mike Schlansker \nof the Advanced Architectures Group at HP. We have benefited from discussions with them and with Corky \nCartwright at Rice and Ekanadham at IBM. The ~ersistent auestionim? of Arvind and Nikhil at M.I.T. has \ndone much to s~arpen our presentation. Charles DeVane, Richard Huff, Wei Li, and Anne Rogers have provided \nuseful comments. 7Personal communication. References [CFR+89] Ron Cytron, Jeanne Ferrante, Barry K. \nRosen, [AA89] Zena Ariola and Arvind. PTAC: A parallel inter\u00admediate language. In Proceedings of the \nFunc\u00adtional Programming Languages and Computer Architecture, London, September 1989. [Ack84] W. B. Ackerman. \nEfficient implementation ofap\u00adplicative languages. Technical Report TR-323, M.I.T. Laboratory for Computer \nScience, April 1984. [AH82] M. Auslander and M. Hopkins. An overview of the PL.8 compiler. Proceedings \nof the 1982 SIGPLA N Symposium on Compiler Construc\u00adtion, 17(6):22-31, June 1982. [AM87] A. W. Appel \nand D. B. MacQueen. A Standard ML compiler. Lecture Notes In Computer Sci\u00adence, September 1987. [ANP89] \nArvind, R. Nikhil, and K. Pingali. I-structures: Data structures for parallel computing. ACM Transactions \non Programming Languages and Systems, 11, October 1989. [ASU86] A. V. Aho, R. Sethi, and J. ers: Principles, \nTechniques, Wesley, 1986. D. Unman. and Tools. Compil-Addison\u00ad [Ber66] A. J. Bernstein. Analysis of programs \nprocessing. IEEE Transactions on 1(5):757 762, October 1966. for parallel Computers, [BM090] Robert A. \nBa.llance, Arthur B. Maccabe, and Karl J. Ottenstein. The Program Dependence Web: A representation supporting \ncontrol-, data-, and demand-driven interpret ation of im\u00adperative languages. Proceedings of the 1990 \nSIG-PLAN Conference on Programming Language Design and Implementation, 25(6):257-271, June 1990. [BP89] \nM. Beck dataflow. University, and K. Pingali. Technical Report October 1989. From control TR89-105O, \nflow to Cornell [BP90] Micah Beck and Keshav Pingali. From control fiow to dataflow. In Proceedings of \nthe 1990 International Conference on Parallel Processing, August 1990. [CC77] P. Cousout and R. Cousout. \nAbstract Interpre\u00adtation: A unified lattice model for static analysis of programs by construction of \napproximations of fixpoints. Proceedings of the ~ th ACM Sympo. sium on Principles of Programming Languages, \nJanuary 1977. [CC79] P. Cousout and R. Cousout. Systematic design of program analysis frameworks. Proceedings \nof the 6th ACM Symposium on Principles of Program\u00adming Languages, pages 269 282, January 1979. [CF89] \nR. Cartwright and M. Felleisen. The seman\u00adtics of program dependence. Proceedings of the 1989 SIGPLAN \nConference on Programming Language Design and Implementation, 25(6), June 1989. Mark N. Wegman, and \nF. Kenneth Zadeck. An efficient method of computing static single as\u00ad signment form. In Proceedings of \nthe 16th ACM Symposium on Principles of Programming Lan\u00ad guages, pages 25 35, January 1989. [Den74] \nJ. B. Dennis. First version of a data flow pro\u00adcedure language. In Proceedings of the Colloque sur la \nProgrammation, Vol. 19, Lecture Notes in Computer Science, pages 362-376, 1974. [Eka90] K. Ekanadham. \nKudos. IBM Yorktown Heights, 1990. [FOW87] J. Ferrante, K. J. Ottenstein, and J. D. Warren. The program \ndependency graph and its uses in optimization. ACM Transactions on Program\u00adming Languages and Systems, \n9(3):319 349, June 1987. [Ki173] G. A. Kildall. A unified approach to global pro\u00adgram optimization. In \nProceedings of the 1st ACM Symposium on Principles of Programming Languages, pages 194-206, October 1973. \n[Kuc78] D. J. Kuck. The Structure of Computers and Computations, volume 1. John Wiley and Sons, New York, \n1978. [Man81] Z. Manna. Mathematical Theory of Computa\u00adtion. McGraw-Hti Publishing Company, 1981. [PBJ+ \n90] Keshav Pingali, Micah Beck, Richard Johnson, Mayan Moudgill, and Paul Stodghill. Depen\u00addence Flow \nGraphs: An algebraic approach to program dependencies. Technical Report TR 90\u00ad1152, Cornell University, \n1990. [P1081] Gordon D. Plotkln. A structural approach to operationrd semantics. Technical Report DAIMI \nFN-19, Aarhus Universityj 1981. [PW86] D. Padua and M. Wolfe. Advanced compiler op\u00adtimization for supercomputers. \nCommunications of the ACM, pages 1184 1201, December 1986. [RL77] John H. Reif and H. R. Lewis. Symbolic \neval\u00aduation and the global value graph. In Proceed\u00adings of the Idth ACM Symposium on Principles of Programming \nLanguages, pages 104 1 18, Jan\u00aduary 1977. [Ste78] G. Steele. RABBIT: A compiler for SCHEME. Technical \nReport AI memo 474, M.I.T. Labora\u00adtory for Artificial Intelligence, May 1978. [Tra86] K. R. Traub. A \ncompiler for the MIT tagged\u00adtoken dataflow architecture. Technicrd report, M.I.T. Laboratory for Computer \nScience, Cam\u00adbridge, Massachusetts, August 1986. [WZ84] M. N. Wegman and F. K. Zadeck. Constant propagation \nwith conditional branches. In Pro\u00adceedings of the Ilth ACM Sympo9ium on Princi\u00adples of Programming Languages, \npages 291 299, 1984. \n\t\t\t", "proc_id": "99583", "abstract": "", "authors": [{"name": "Keshav Pingali", "author_profile_id": "81100554731", "affiliation": "Department of Computer Science, Cornell University, Ithaca, NY", "person_id": "PP39048331", "email_address": "", "orcid_id": ""}, {"name": "Micah Beck", "author_profile_id": "81341488208", "affiliation": "Department of Computer Science, Cornell University, Ithaca, NY", "person_id": "PP42052637", "email_address": "", "orcid_id": ""}, {"name": "Richard Johnson", "author_profile_id": "81406600453", "affiliation": "Department of Computer Science, Cornell University, Ithaca, NY", "person_id": "PP31081507", "email_address": "", "orcid_id": ""}, {"name": "Mayan Moudgill", "author_profile_id": "81100284784", "affiliation": "Department of Computer Science, Cornell University, Ithaca, NY", "person_id": "P195381", "email_address": "", "orcid_id": ""}, {"name": "Paul Stodghill", "author_profile_id": "81100219663", "affiliation": "Department of Computer Science, Cornell University, Ithaca, NY", "person_id": "P222278", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/99583.99595", "year": "1991", "article_id": "99595", "conference": "POPL", "title": "Dependence flow graphs: an algebraic approach to program dependencies", "url": "http://dl.acm.org/citation.cfm?id=99595"}