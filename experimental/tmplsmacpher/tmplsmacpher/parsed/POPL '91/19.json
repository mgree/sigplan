{"article_publication_date": "01-03-1991", "fulltext": "\n Communication with Directed Logic Variables Alon Kleinman Yael Moscowitz Amir Pnueli Ehud Shapiro Department \nof Applied Mathematics and Computer Science The Weizmann Institute of Science Rehovot 76100, Israel \nExtended Abstract Abstract 1 Introduction Logic variables as communication channels A directed logic \nvariable is a two-port communication Logic variables, as employed in logic programming, have channel \nthat can transmit at most one message, which the following properties: may contain embedded ports. The \nability to send both input and output ports in messages allows directed logic 1. A variable can be assigned \nonly oncel. variables to implement other communication primitives 2. The assigned value is a term (structured \npiece of such as streams and remote procedure calls; to real-data), possibly containing variables. In \nparticular, ize dynamic process migration; and to support sophis\u00ad 3. The value can be another variable, \nresulting in ticated mail and window system protocols. Hence di\u00ad identifying the two variables. rected \nlogic variables can be useful as an implementa-As a result, the value of a variable can be determined \nin\u00adtion layer for concurrent languages and as an extension crementally, and be only partially defined \nat each state to sequential procedural languages. of a computation. Another property of logic variables \nis that there is an infinite supply of them; new variablesWe provide an abstract, programming language \ninde\u00adcan be allocated on the fly as needed. pendent, operational semantics for communication with directed \nlogic variables and develop effective distributed In concurrent logic programs logic variables are used \nimplementations of it. Our implementation algorithms as (multi-port) communication channels. Processes \nensure that, in spite of arbitrary port migration, if a sharing a logic variable may communicate by one \nas\u00admessage is sent on a channel s output port and the signing it and the others reading its value. A \nprocess channel s input port eventually stops migrating then sharing a variable with others can synchronize \nby block\u00ad ing until the variable is assigned by another process. the message will eventually reach it. \nIn addition, our algorithms are efficient and robust: They dynamically When used as communication channels, \nthe properties shorten the logical routing paths caused by port mi\u00ad of logic variables can be restated: \ngration, collect unused routing table entries, and are 1. A channel can transmit at most one message. \nresilient to a certain class of network faults. 2. The message is a structured piece of data, possibly \ncontaining channels. In particular, We prove that the algorithms implement the oper\u00adational semantics \nusing the refinement mapping proof 3. The message can be another channel, resulting in technique and \ntemporal logic inference rules for weak identifying the two channels. and strong fairness, demonstrating \nthe effectiveness of The ability to allocate new channels on the fly and combining these two met hods. \nto send channels in a message gives rise to a rich set of communication protocols. As two basic examples, \nwe consider point-to-point stream communication and remote procedure calls. Stream communication via \na shared logic variable is realized by the sender incremen\u00adtally assigning the variable to a list, and \nthe receiver reading this list. If the variable Xs is initially shared, then to send the message m the \nsender assigns to Xs Permission to copy without fee all or part of this material is granted provided \nthat the copies are not made or distributed for direct 1Usually logic programs do not specify assignments \nexplicitly. commercial advantage, the ACM copyright notice and the title of the Rather, they specify \nequality constraints on terms. The assign\u00adpublication and its date appear, and notice is given that copying \nis by ments are generated by the unification algorithm while solving permission of the Association for \nComputing Machinery. To copyotfrer-these constraints. wise , or to republish, requires a fee arrd/or \nspecific permission. @ 1990 ACM 089791419-8/90/0012/0221 $1.50 221 a pair2 [ml Xs ] containing the message \nm and a vari\u00adable Xs . Xs can be used similarly to send the next message, and so on ad infinitum. The \nreceiver blocks until Xs is assigned to a pair [X IXS ], reads the first message from X, and blocks on \nXs for receiving the next message. A fundamental operation on streams is forwarding: A process with input \nstream Xs and output stream Ys can cause all messages arriving on Xs to be transparently forwarded to \nYs by simply assigning Xs to Ys. A remote procedure call (RPC) is a convenient mech\u00adanism for client-server \ncommunication [22]. Syntacti\u00adcally, it resembles an ordinary procedure call, except that arguments must \nbe passed by value. Semantically, executing an RPC results in the client sending a message containing \nthe name of the called procedure, its argu\u00adments, and a return address to the server, and blocking until \na reply is received. The server receives the request, executes it, and sends the reply back to the client, \nusing the return address. (Complications arise if the server, the client, or the network may fail; we \ndo not address the former two here.) A remote procedure call can be implemented using logic variables \nas follows. The client sends a message rpc(P, Reply) to the server, with P be\u00ading the procedure name \nand its arguments and Reply a new logic variable, and blocks until Reply is assigned. The server performs \nthe RPC upon receipt of the mes\u00adsage and replies by assigning Reply. It is the task of the implementation \nof logic variables to ensure that the reply (i.e., the value of the Rep/y variable) is indeed sent from \nthe server back to the client. One extension to the basic RPC mechanism that can be easily realized using \nlogic variables is stream\u00ad ing remote procedure calls [17]; in this protocol the client may issue several \nRPC )S without blocking on their replies. Another is transparent forwarding. A server can transparently \nforward a remote procedure call to another server by simply sending it the RPC message; no further processing \nis required on behalf of the for\u00ad warder. Furthermore, the server may decide to forward all future requests \nby connecting its input stream to a stream served by some other server, as explained above. The implementation \nalgorithms described in this paper ensure that the reply to a forwarded RPC is routed from its ultimate \nserver directly to its original client and, in case the entire stream is forwarded, that RPC S will eventually \nbe sent directly from clients to the new server. Remote procedure calls are sent from the client to the \nserver as ordinary messages on a stream. A stream merger process, plugged in front of the server, can \nallow several clients to share the server. A client c1 wishing to share its stream to the server with \nan\u00ad other client C2 can fork its stream by assigning it the value merge(Xsl, Xs2), keeping one stream \nvariable to 2[X l/Ys] is the standard notation for a list ( cons cell) whose head (car) is X and tail \n(cdr) is Xs. itself and sending the other stream variable to C2. This message is trapped and served by \nthe stream merger. No client can access a server unless explicitly given a stream to it by someone that \nalready has access to the server, This property of logic variables naturally gives rise to capability \nand security mechanisms, as discussed in [20, 13]. These two examples show that rather different mod\u00adels \nof communication can be naturally realized by shared logic variables. We would like to point out another \nas\u00adpect of communication with logic variables: Since a mes\u00adsage and its recursively embedded channels \nare the value of a variable, the history of communication on a shared variable can be viewed as a cooperative, \nincremental, effort to construct its value. The variable s value can be inspected in the future; this \nis useful for monitoring, logging, and debugging. Coordinating multiple writers The single assignment \nproperty of logic variables, com\u00adbined with the ability to share a variable among many processors, immediately \nraises the question of how to coordinate the writing on shared variables. Concurrent logic languages \ndiffer in their answer to this question. Languages with atomic unification such as Flat Concur\u00adrent Prolog \n[19], FCP(l, 1) [23], Atomic Herbrand [24], and FCP(: ,?) [16], require that the assignments associ\u00adated \nwith a unification be done atomically, if and only if the unification attempt succeeds. Languages with \neven\u00adtual unification, such as Flat GHC [33], Oc [11], and Flat PARLOG [5], and languages with eventual \nassign\u00adment, such as an earlier version of PARLOG [4] and Strand [6], require assignments neither to \nbe atomic nor consistent. If an inconsistency arises it is eventually de\u00adtected, failing the entire computation \nand invalidating its result. Loosening the consistency requirement results in greater flexibility in \nparallel and distributed imple\u00admentations of these languages. However, the result is that syntactically \ncorrect programs may cause incon\u00adsistent assignments to the same variable to be made, resulting in different \nprocessors observing inconsistent values of the same variable. This is not a problem for algorithmic \nprogramming since the consistency of the final result can be verified, and an inconsistency can be viewed \nas a witness to an error in the program that needs to be corrected. However, it is not known how to contain \nthe effect of such inconsistencies, i.e., to protect good programs from being exposed to the inconsistencies \ncaused by bad programs they share logic variables with. Hence it seems that languages with eventual unification \nor eventual assignment are not ad\u00ad equate for programming distributed applications in an open computational \nenvironment. In comparison, lan\u00ad guages with atomic unification cannot make inconsistent assignments. \nHowever, the distributed implementation of their basic operation atomic unification is rather complicated \n[15]. Directed logic variables An alternative approach to communication with logic variables, which \neschews the atomiclnonatomic unifica\u00adtion dichotomy, was proposed by Hirata, in his language Doc (Directed \nOc) [10]3, and was further elaborated by Saraswat et al [25] in the language Janus. The ap\u00adproach associates \ncapabilities with logic variables, as in Concurrent Prolog [26], but, instead of using atomic unification, \nit ensures that at most one writer exists for any variable at any state of the computation. Hence inconsistent \nassignments cannot be made, and no coor\u00addination of writing is necessary. The approach employs directed \nlogic variables, which are the subject of this paper. Intuitively, a directed logic variable is a logic \nvariable (channel) with two types of occurrences (ports): A read-only variable [26] (also called input \nport, receiver [10], asker [25]) and a writable variable (also called output port, sender, teller). A \nwritable occurrence can be used for assigning the vari\u00adable (i. e., sending a message) and a read-only \noccurrence can be used only for reading the variable s value (i.e., receiving the message). A directed \nlogic variable has at most one input port and one output port. Following the above definition, communication \nwith directed logic variables must preserve the Single- Reader Single-Writer (SRS W) Invariant: No writable \nor read-only variable occurs in more than one processor in any state of the computation. We require a \nsingle occurrence of each writable vari\u00ad able to ensure consistency without locking. We must extend \nthe requirement to read-only variables because of the ability to send variables in messages. To under\u00ad \nstand why, assume we require only single writers, and consider the following. Processor F l has an output \nport of a channel X and two other processors P2, P3 have its input port. PI assigns the value f(Y) to \nX, where Y is an output port. Both P2 and P3 read this value, causing this output port to occur both \nin P2 and in PB, violating the single writer requirement. For concurrent logic programs the SRSW invariant \n can be easily enforced by a local syntactic restric\u00ad tion [10], which does not exclude most concurrent \nlogic programming techniques [25]. Although useful in con\u00ad current logic programming, we believe that \ndirected logic variables can be of benefit in other contexts aa well. Specifically, we believe that they \ncould be a use\u00ad ful extension to conventional procedural languages; that they can provide a useful layer \nin the implementation of other concurrent programming languages; and that they can implement services \ncommonly associated with the upper layers of network and operating systems, such as sophisticated mail \nand window systems. 3See [28] for a more accessible reference. Sending ports in messages The key property \nthat separates (directed) logic vari\u00adables from other models of communication is the abil\u00adity to send \nboth input and output ports in messages. Abstract models of concurrency such aa dataflow net\u00adworks [2] \nand CSP [12] do not allow ports to be sent in messages. Object-oriented models such as Actors [9] and \nPOOL [3] allow object names, i.e. only output ports, to be sent in messages, but the (implicit) input \nports associated with objects are usually static. A simi\u00adlar behavior is manifested in operating systems: \nUnix s socket names [34] and Amoeba s capabilities [21] can also be sent in messages. However, we are \nnot aware of models of concurrency, other than concurrent logic pro\u00adgramming, that allow sending both \ninput and output ports in messages. The utility of this extra capability becomes apparent when dynamic \nmigration of processes is required. Con\u00adsider the following distributed programming problem: Define in \nthe language Lm a distributed program, with a fixed mapping of processes to processors, that sup\u00ad ports \ndynamic process migration in the language Lo. (The purpose of migrating Lo processes is not impor\u00ad tant \nat present.) If Lm = LO = L, the problem becomes: Implement in L a statically mapped meta-interpreter \nthat supports dynamic process migration; examples of such concurrent logic meta-interpreters can be found \nin [28, 31, 32]. Any solution to the problem requires that Lm can encode LO processes in messages and \nimplement local creation of such processes. A particularly simple SOIU\u00ad tion can be obtained if, in addition, \nthe following two (loosely stated) conditions are met: 1. Channels of Lm can implement naturally chan\u00adnels \nof Lo. 2. Both input and output ports can be sent in L~ messages.  If these two conditions are met, \nthen migrating an Lo process p can be realized by sending an L~ message consisting of p s description \nand its ports to the destina\u00ad tion processor. Upon receipt, the destination processor decodes the description \nof the process and activates it locally with the ports received in the message. If only output ports \ncan be sent in Lm messages, a more complicated implementation is required. A mi\u00ad grating process p can \nleave a forwarder process on the processor from which it migrates, take an output port to the forwarder \nwith it, and use it, upon arrival at the destination processor, to send the forwarder a new out\u00ad put \nport with which the forwarder can send messages to p. This protocol, although correct, causes communi\u00ad \ncation to a migrating process to go via its old locations forever. Take Lm to be directed logic variables. \nDirected logic variables can naturally implement dataflow streams and RPC client/server channels, as \nshown above, as well as CSP-like communication [28] and Actor mailboxes [14]. They can also be sent \nin messages. Hence they satisfy the two conditions stated above, with LO taken to be one of these models. \nThus, directed logic variables can be used to implement process migrations in these models. If logic \nvariables are used to implement Lo process mi\u00adgration, then the more sophisticated algorithms in this \npaper ensure that if two LO processes stop migrating then eventually all communication between them will \nbe direct. Data structures with recursively embedded commu\u00adnication ports are useful on their own right, \nnot just as a tool for implementing dynamic process migration. In the data-structure view, a port is \njust a pointer to a (possibly not available yet) recursive data-structure. This view is employed in the \nfollowing applications of ports in messages. Consider a mail system with the following incremen\u00adtal reply \nprotocol: A mail message is sent with a re\u00adply variable Reply embedded in it, where Reply? is kept by \nthe sender. Upon receipt by the mail server at destination (and perhaps also when crossing net\u00adwork boundaries), \nReply is assigned to the structure [received(By,At) lReply ?], and the message is forwarded with the \nnew variable Reply substituted for Reply. If the address is incorrect, Repl~ is assigned the appro\u00adpriate \nerror message. Otherwise, when the message is read by the user, Repl~ is automatically assigned the value \n[read( At)/ Repifl?] by the user s mailer. If the user wishes to reply to that message with the text \nm, he can either compose a new mail message and send it, as usual, or assign to Replf the value reply(m, \nRepl~), keep\u00ading Reply ? to itself. In the latter case the original sender can easily associate the reply \nm with the origi\u00ad nal message, and can continue the dialogue, if needed, using Repl~ ). Note that messages \nexchanged on the reply variables are routed by the underlying directed logic variable protocol and not \nby the (presumably more costly) mail routing protocol. A similar concept can be applied to windows. One \ncan easily envision the utility of a window system in which windows are first-class objects, with explicit \ncommunication ports. Windows in such systems can be viewed as data-strictures with communication ports. \nThus, a window can be shared between users, can be sent from one user to another in a message and can \nbe recursively embedded in another window. Directed logic variables can provide the foundation for such \na window system. Implementing directed logic variables The difficulty of implementing directed logic \nvariables should now be apparent. Since ports can be sent in messages, the input port and the output \nport of a chan\u00adnel can migrate to arbitrary locations before commu\u00adnication takes place on the channel. \nIt is the task of the implementation of directed logic variables to ensure that, in spite of arbitrary \nport migration, if a message is sent on a channel s output port and the channel s in\u00adput port eventually \nstops migrating then the message will eventually reach it. Our algorithms realize this cor\u00adrectness requirement \nand have additional efficiency and robustness properties. Contributions The contributions of this paper \nare in: e Identifying the general applicability of directed logic variables, which transcends programming \nlan\u00adguage boundaries. b Providing an abstract, language independent, oper\u00adational semantics for communication \nwith directed logic variables. e Developing effective distributed implementations of directed logic variables \nfor reliable and faulty net\u00adworks, and proving them correct with respect to the operational semantics. \nDemonstrating the effectiveness of combining the refinement mapping proof technique [1] with tem\u00adporal \nlogic inference rules for weak and strong fair\u00adness [18] for proving the correctness of complicated distributed \nimplementation algorithms. Rest of paper In the next section we present an abstract operational semantics, \ncalled DVOS, for processors that communi\u00adcate with directed logic variables. These processors can form \na distributed implementation of a concurrent logic program, or simply be arbitrary procedural programs \nthat employ directed logic variables for communication. The following sections develop several implementations \nof DVO~ and study their properties. We first study im\u00adplementations that assume reliable end-to-end commu\u00adnication! \nDV~ (a naive implementation), DVI (an im\u00adproved implementation that dynamically shortens log\u00adical communication \npaths) and DV~c (an implementa\u00adtion which, in addition, collects redundant routing table entries). We \nthen describe an implementation, DV~C, that can operate on a faulty network, provided that a message \nthat is sent infinitely often from one processor to another eventually does not get lost. The correctness \nof each implementation is proved by showing a refine\u00ad ment mapping [1] from it to DVO~. We prove that \nthe refinement mappings preserve fairness using temporal logic inference rules based on well-founded \nrankings [18]. (The refinement mapping proof technique and the infer\u00adence rules are given in the appendix.) \nDue to space limitations we only sketch the imple\u00admentations and their proofs. All the implementations \nwe describe were defined by formal transition systems and all the theorems we cite were given detailed \nproofs. Some of these details will be given in the full version of this paper. 2 Operational semantics \nTerminology We assume two infinite sets of variables, V! and VT, re\u00adferred to as writable variables and \nread-only variables, respectively, and a one-to-one function ? : v! + V?. For each writable variable \nX c V! (also denoted X! to avoid ambiguity), X? (which is the result of applying the function ? to X) \nis called the corresponding read\u00ad ordy variable of X. V ~f v! U VT. We assume a given vocabulary from \nwhich terms over V are built. An assignment has the form X := T where X ~ V! and T is a term. A set of \nassignments 0 is consistent if (X := T) c 0 and (X := T ) c 0 implies that T=T . For any syntactic object \nS, Writers(S) denotes the set of writable variables in S, Readers(S) denotes the set of read-only variables \nin S, and Vars(S) =j Readers(S) U Writers(S). A local transition system We model processors communicating \nwith directed logic variables in an abstract way. The state of such a pro\u00adcessor is a triple S = (Z, \nV, p), where Z is an (unin\u00adterpreted) internal state, V is a set of variables (ports) owned by the processor \nand p is a consistent set of the assignments it has received. A processor can perform local transitions \n(z, V, p)~ (Z ,V , p), in which it may change its internal state from T to Z , and may do one of the \nfollowing: 1. Allocate a new channel X and add its two ports to V, i.e., V = VU {X!, X?}. 2. Send a \nmessage X := T where Vars(X := T) ~ V  and V = V Vars(X := T). If a message is sent the transition is \ncalled non-silent, and 1 = (X := T), otherwise it is called silent, and 1 = r. Note that when a processor \nsends a message X :=T itrelinquishes its ownership of all the ports appearing in the message, including \nX!. A global transition system, DVO, The transitions of a processor depend not only on Z but also on \nV and p. A processor executing within an en\u00advironment may receive assignments. The receipt of an assignment \nincreases p and updates V, thus it may en\u00adable processor transitions that were hitherto disabled. The \noperational semantics models a set of communicat\u00ading processors as follows: If a processor sends X := \nT, then the processor owning X? may receive it. Upon receipt, the assignment is added to the p component \nof the processor and its V component is updated by adding Vars(T) and deleting X?. A DVo~ states has \nthe form (Sl II . . . II Sn; u), where each Si = (Zi, Vi, pi ) is the local state of processor i and \no is a consistent set of assignments. o represents the entire set of assignments sent. Initial states \nare states in which u= Oand pi = 0,for all i. DVo~ has the following transitions: c Locali: (sill . ..sll \nSfl.u)l(si llsS(l...S( . ..[ls~. u), where Si &#38; S; is a silent local transition. Sendi(X := T): (Sill...s[p..[p \nn;a) \u00ad(S,1] . . .S: . . .//~j;u\\X := T}), where S i + Si is a non-silent local transi\u00adtion. Receivei(X \n:= T) : (S1/l... S[l Sn;a)n; a) \u00ad(Sill...SllS*;~),;~), where (X := T) E a, X? E Vi, andS(= (Zi, (U -{X?}) \nU Vars(T), pi U {X := T}). In the following, we employ the standard weak fairness requirement: If a transition \nis enabled then eventually either it becomes disabled or it is taken. As we will show, this requirement \nimplies that if X := T is sent then it will eventually be received, provided that X? eventually stops \nmigrating. We take DVO, to be our specification. In the fol\u00adlowing we employ the refinement mapping framework \nof Abadi and Lamport [1] (see Appendix A) to prove the correctness of the implementations with respect \nto this specification. In these proofs we take a to be the observable part of the state. We note that \nDVO~ is an instance of the Asynchronous Concurrency model [7], where u is taken to be the shared store \nand < is the partial order on the domain.  3 A naive implementation, DV~ In the specification DVO,, \nglobal knowledge is assumed: A message (X := T ) can be received by the owner of X? immediately after \nit is sent although the location of this owner cannot be derived from the local state of the sending \nprocessor. In this section we present a simple implementation of DVOS, called DV~, that does not as\u00ad \nsume global knowledge. It employs a routing table in each processor in order to transmit the value of \nX! to the processor in which X? resides. The implementation is naive in the sense that once a logical \npath is estab\u00adlished between X! and X?, all subsequent messages on subchannels of X! may follow the same \nlogical path, in\u00adstead of going directly. For example, consider a stream transducer, residing on processor \nP2, that transforms a stream produced on PI to some other stream read on P3. If the transducer decides \nto connect its input stream to its output stream and halt, then the naive algorithm will still route \nall stream messages from PI to P3 via P2. As another example, if an application process mi\u00adgrates from \none processor to another, then the naive algorithm will route all messages to that process via its original \nlocation. States and Transitions A DV~ state s has the form (Pl II . . . II Pm), where Pi = (Si, RTi, \nIi) is the state of processor i and S i is defined as in DVO,. RTi is the processor s routing ta\u00adble, \nconsisting of pairs (X?, j) of a read-only variable and a processor identifier. Ii is the processor s \ninput buffer, consisting of messages sent to it but not yet received. Messages transmitted bet ween processors \nof DV. have the form (X := T )~, where m is the index of the originator of the message. In the following \nwe do not distinguish between a set of messages and a set of assignments. We say that a transition associated \nwith processor i receives the message M if currently M E Ii, and the transition removes the message from \nIi, We say that the transition sends the message M to processor j if it places the message in the input \nbuffer Ij. With this terminology we define the transitions of DV.: Locali: Performs a local transformation \non Si (as in Local~). e Sendi(X := T): Changes Si as in Send~s(X := T) and sends the message (X := T)i \nto itself. e Receivei(X := T): On receiving the message (X := T)m where X? c ~, the transition updates \nthe local state Si in a way similar to Receive~s(X := T). It also updates the routing ta\u00adble RTi to contain \nan entry (Y?, m) for each Y! E Writers(T) and deletes the routing table entries of Readers(T). Forwardi(X \n:= T): On receiving (X := T)m where X? @ Va and (X?, j) c RTi, this transition sends the message (X := \nT)~ to processor j. It also updates the routing table so that Readers(T) point to j. Define T(S) to be \nthe set of assignments sent but not yet received, ~(s) ~f Ui Ii, and p(s) to be the set of assignments \nreceived, p(s) =f Ui pi. The observable component of a state s is defined to be the set of the assignments \nsent, namely: u(s) +f ~(s) U p(s). Correctness Fora DV~state s = (( SI, RTI, II) [1 . . . [1 (Sri, RTn, \nIn)) define f(s) ~f (Sl [1 . . . [1 Sn; o-(s)). Theorem 1 (Correctness of DVN ) f is a refine\u00ad men tmapping \nfrom DV~ to DVO~. Proof outline: It is easy to see that: (RI) observable are preserved by f; (R2) initial \nstates are mapped to initial states; and (R3) ~ maps Local?, Send~(X := T), Receive~(X := T) to Local~s, \nSend~s(X := T), Receive~(X := T), respectively, and F orward~(X := T) to stuttering. It remains to show \nthat: (R4) ~ maps fair DV. computations to fair DVO, computations. It is easy to verify that the fairness \nrequirement is pre\u00adserved by f with respect to the transitions Send~s and LocaJ~s. To show that the fairness \nrequirement is pre\u00adserved for the transition Receive~s, we first prove the tree property, defined below. \nWe say that processor i virtually owns a port X in a state sif X EVi; or (X := T) c Ii; or for some (X \n:= T) E Ii, X c Readers(T); or for some j, (X := T)i c lj and X c Writers(T). By the properties of DV~, \nthe virtual owner of X is weI1-defined for every variable X and state s s.t. X occurs in s and (X := \nT) @ p(s). With every variable X and state s, we associate a di\u00adrected graph whose nodes are processors. \nThe graph has an edge (i, j) iff (X?, j) c RT; and i is not the virtual owner of X?. Such an edge implies \nthat processor i cur\u00adrently believes processor j to be the owner of X?. We include in the graph only \nthe nodes corresponding to the processor virtually owning X? and any other processor having a routing \ntable entry for X?. Proposition 1 (Tree property) For each variable X occurring in a DV. computation \nstate s s. t. (X := T) @ p(s), the graph of X in s forms an in-tree whose root is the virtual owner of \nX? and one of its nodes is the virtual owner of X!. From the tree property it follows that the routing \ntable entries for X? form a path from the virtual owner of X! to the virtual owner of X?. We call it \nthe path from X! to X? in s. Using this property we prove: Proposition 2 (Eventuality) Let S1, 52, . \n. . be a (weakly) fair DV. computation. If (X := T) E ~(si) then for some j > i either (X := T) E p(sj) \nor X? 6 VU? S(7T (Sj )). The proposition states that if an assignment X := T is on its way to processor \nk which is the current owner of X?, then eventually either the message will reach processor k and be \napplied there locally, or processor k will relinquish the right on port X? by sending it as part of a \nterm T in a message (X := T )k to another processor. The proof uses the W-RESP rule (given in Ap\u00ad pendix \nB), with p being the precondition of the proposi\u00adtion and q its goal. The assertion p is p A (X? G Ui \n~). The ranking function 6 is the length of the path from X! to X?. Note that the path can become longer \nonly by the current owner of X? relinquishing its owner\u00adship, which immediately achieves q. The helpful \npredi\u00adcate h identifies the transitions Forwardj (X := T) and Receivej (X := T) to be helpful when enabled. \nWe show that premises W1 W4 of W-RESP are satisfied, and since the computation is weakly fair, conclude \nthat the proposition is valid. This completes the proof of Theo\u00adrem 1.  4 An improved implementation, \n DVl u where Active = Activei (Y). The improved implementation, DVI, employs the notion of subports. \nLet X be an output port, Y some port, and 0 a set of assignments. Y is a subport of X in L9if (X := T) \nG 0 and either Y E Vars(T) or, for some Z? G Vars(T), Y is a subport of Z in 0. DVI dynamically shortens \nthe logical paths from writ\u00ad ers to readers. It employs acknowledgement messages, which eventually cause \na sequence of messages, sent on a sequence of recursively embedded output ports, to be direct, provided \nthe respective input and output sub\u00ad ports eventually stop migrating. Messages transmitted in the DVI \nsystem are either assignments of the form (X := T)m, or acknowledgements of the form ackj(X) that acknowledge \nthe receipt of a message (X := T)m by j. States and Transitions A DV, state for processor i is given \nby (Si, RTi, Ii, ~i), where Oi is an output buffer, Sj, RTi and Ii are defined as in DV~. The output \nbuffer Oi contains pairs of the form ((X := T)i, Active) where (X := T)i is a message sent by processor \ni for which an ack message has not been received yet, and Active is the set of active input ports in \nT. An active input port of the assignment (X := T)i in Oi is an input port in T that has not been received \nby proce~sor i since the message (X := T)i was sent. The key difference between DV, and DV. is that in \nDVI, for each (X := T)i, routing table entries of Readers(T) are updated only by the sender, both when \nplacing a message in Oi and when receiving an acknowl\u00adedgment for it. When placing (X := T)i in Oi, where \n(X?, j) c RU, the routing table entries of input sub\u00adports of X! are updated to point to j, as a first \napprox\u00adimation. When receiving ackk(X), we wish to update these input subports to point to k, which normally \nis a better approximation than j. However, since our buffers are not FIFO, it is possible that some input \nsubport Z? of X! was already received in some message and, possibly, already forwarded to another processor \nm. In such a case the routing table contains (Z?, m), and we would like this entry not to be damaged \nby the handling of Uckk (X), lest a permanently incorrect routing table may result, For this we maintain \nthe Active sets associ\u00adated with every message in the output buffer and define Active~ (X) as follows. \nActivea(X) = Active U Active ((X := T)i , Active) c Oi 0 ((X := T)i, Active) f Oi { YT~ACt~Ve Upon placing \n(X := T)i in Oi, or receiving ackk(X) from the ultimate receiver k, only routing table entries of Activei \n(X) are updated. It is important to notice that a message is sent on a subport immediately, without blocking \nuntil acknowl\u00adedgements for messages on its super-ports are received. As a result, DV, routing table \nentries can be temporar\u00adily incorrect, and they do not always form a path from the virtual owner of X! \nto the virtual owner of X?. (The definition of virtual ownership in DV,, which is different from DV., \ncan be found in the section about the cor\u00adrectness of DVI.) All we ensure is that if X! and X? stop migrating \nthen eventually the routing table entries will form such a path. The temporary inconsistencies may cause \nmessages to get lost in black holes . Therefore, messages in the output buffer are repeatedly retransmit\u00adted \nin DV,, until an acknowledgement is received. DVI has the following transitions: Locali: Similar to Local?. \nSendi(X := T): Changes Si as in Sendy and places the pair ((X := T)i, Readers(T)) in the out\u00adput buffer \nOi. In addition, if (X?, j) E RTi then routing table entries of Activei (X) are updated to point to j. \n(This recursive updating is the key to path shortening without delay of subsequent Send transitions.) \nReceivea(X := 7 ): On receiving (X := T)m, where X? c Vi, this transition updates the local state Si \nin a way similar to Receive~(X := T). It also sends acki(X) to processor m and removes Readers(T) from \nall the Active sets in Oi. Forwardi(X := T): On receiving (X := T)~, where X? @ Vi and (X?, j) E RTi, \nthis transition sends the message (X := T)m to processor j. Discard;(X := T): On receiving (X := T)~, \nwhere X? @ Vi and RTi contains no information about X?, this transition simply discards the mes\u00adsage. \nTransmiti(X := T): If the message (X := T)i is currently in Oi and (X?, j) ~ RTi, this transition sends \nthe message (X := T)i to processor j. Aclcnovvledgedi( X): On receiving the message ackj (X), this transition \nlocates and removes a pair of the form ((X := T)i, Active) from Oi and up\u00addates the table RTi to contain \nan entry (Y?, j) for each Y? c Active. Correctness For a DV1 state s define r(s) to be the set of assign\u00adments \n(X := T) that were sent but not yet received, i.e., T(S) = Ui Oi Ui pi. In DV1, we define the virtual \nowner ofX tobeiifeither X E~ orX EOuti nr(s). Similar to DVN, with every DV, state s and variable X \nwe associate a variable graph. (i, j) is an edge in that graph if i is not the virtual owner of X? and \neither (Y?, j) E Rll where X? is not contained in any Ac\u00ad tive set in Oi or ((Y := T)i, Active) E Oi \nwhere X? E Readers(T) nActive and ack(j, Y) c Bi. Such an edge implies that there is a reliable information \nin the local state of i to believe that j is the owner of X?. With these new definitions and the same \ndefinitions for p, u and f as in DV~, we state: Theorem 2 (Correctness of DV,) f is a refine\u00admen t mapping \nfrom DV, to DVO~. Proof outline: It is easy to see that (RI), (R2), and (R3) are satisfied (with the \nnew transitions Transmit, Discard, and Acknowledged being mapped to stutter\u00ad ing). It remains to show \nthat: (R4) f maps fair DVI computations to fair DVO~ computations. The proof follows the same structure \nas in Theorem 1, with the following differences. The tree property is preserved although the path on \nthe graph may be (temporarily) different from the path derived from the routing tables. However, after \ntaking a finite number of Acknowledged transitions, routing table entries form the path from the virtual \nowner of X! to the virtual owner of X?. This added complexity is reflected in the definition of 6 and \nh, used in the proof of the eventuality property (Proposition 2) for DV,. We define 6 = j+rn-k, where \nj is the length of the path from X! to X?, m is the number of Acknowledged transitions that are required \nto correct the path, and k = Oif m >0, else k is the maximal 1~ O such that (X := T)v E Iv, and WI is \nthe lth processor on the path from X! to X?. The helpful transitions are: Receive, (X := 7 ) when enabled, \nForwardk(X := T) when m = O and k is defined as above, ~ansmitu (X := T) when k = O and m = O, and Acknowledged, \nwhich corrects an entry on the path from X! to X?. Effectiveness We say that a DVI state s is quiescent \nif m(s) = o. A cleanup transition is a Sendi (X := Y?) transition, where Y? is a new port, also called \na cleanup variable, Let c = S1, S2, . . be a DV, computation. p(c) is the set of all assignment received \nin c. For a set of assignments 6, define frontier(@) to be the set of ports which are subports of ports \nin 0, but have no subports in 0, We show that by employing a certain protocol in which every processor \nsends a cleanup message on every writable variable it owns, paths become direct. Proposition 3 (Cleanup) \nLet c = SI, sz, . . . be a DVI computation and Si a state beyond which all Send transitions are clean \nup transitions, whose set of cleanup variables are Y, Assume that every Y? ~ Y occurs at most once in \np(c), and that frontier(p(c)) = Y. Then there is a j z i such that sj is a quiescent state and for every \nvariable X in sj the length of the path from X to X? is at most one. The second property we show demonstrates \nthe dy\u00adnamic path shortening capability of DVI. It shows that if a sequence of messages is sent on recursively \nembed\u00added output ports, where these ports and their corre\u00adsponding input ports do not migrate, then eventually \nthese messages will be direct. Letc=sm, s~+l, ..., Sn be a DV, sub-computation. Y is a subport of X at \nc with respect to the processor i if either Y is a subport of X in Oi at Sm or there exists Z? which \nis asubport of X in Oi at s~ and Y is a subport of Z at the sub-computation s~+l, . . . , Sn with respect \nto the processor i. Proposition 4 (Path Shortening) Let c=s~, sz, . . . be a DV, computation and A = \n{(X~ := ~i) [ i > 1} a subset of assignments in p(c) sent by pr~ cessor p such that X;+l ? G Readers \nand both Xi! and Xi? occur exactly once in p(c). There exist k, m such that Xm = Tm cA and for every \n1> kand Xj such that Xj := Tj @ A and Xj is a subport of Xm at sk, ..., SI with respect to p, the routing \ntable entry for Xj? in RTP points to the uJtimate receiver of Xj := Tj. The k and m in the proposition \nare chosen to satisfy that sk is a state in which an acknowledgement for Xm is received (where Xm := \nTm c A). It can be shown that such k and m exist (under the proposition conditions) due to the properties \nof DV, and since only fair DV, computations are considered.  5 Garbage Collection, DVGC In the previous \nimplementations redundant entries were left in the routing tables and no attempt to reclaim these entries \nwaa made. In the full paper we describe DV~C, an extension of DV1 that reclaims redundant entries us\u00ad \ning a distributed garbage collection scheme based on reference counting. DV~C is similar to DV,. A component \nis added to each processor state for managing the reference counts. Some of DV, transitions are extended \nin DV~c in order to update the reference counts, New transitions are de\u00ad fined in DV.C that remove a \nrouting table entry which is found to be redundant and propagate a decrement of the reference count along \nthe variable graph. The correctness proof of DVeC is similar to the cor\u00ad rectness proof of DV,. We first \nshow that the tree prop\u00ad erty is preserved by DV~c computation states (the mod\u00ad ifications in DVGC relative \nto DVI do not affect the tree property). In order to show that the eventuality prop\u00ad erty holds in DV~C \ncomputations we extend the proof of the W-RESP rule in DVI by observing that the mod\u00ad ifications of the \nDVI transitions do not affect the proof and the new transitions of DVGC satisfy the require\u00ad ments for \nnon-helpful transitions, We finally show that f (as defined for DV,) is a refinement mapping where the \nnewly added transitions are mapped to stuttering. This proof also follows the corresponding proof in \nDV,. The effectiveness of the garbage-collection mechanism is demonstrated by the following proposition: \nProposition 5 (Garbage Collection) Let SI, SZ, . . . be a DVOC computation and Si a state beyond which \nno Send transition is taken, Then there is a j > i such that sj is a quiescent state and: (1) the graph \nof every variable X ! occurring in sj for which (X := T) @ p(sj) contains exactly the path from X! to \nX?; (2) the graph of every variable X! for which (X := T) E p(sj) is empty. In DV.C, Proposition 3 can \nbe strengthened to con\u00ad clude that for every variable X in sj for which (X := T) @ p(sj ), the graph \nof X contains at most one edge, which is the path from X! to X?.  6 Communicating over a Faulty Network, \nDV~c In this section we consider faulty end-to-end communi\u00adcation networks which are strongly fair If \na message M is sent infinitely often from processor i to processor j then it is eventually received by \nprocessor j. This is an important model to consider since it approximates the widely available datagram \nservice [30]. Although DVI assumes a reliable end-to-end commu\u00adnication network, it contains several \ncomponents that enable a relatively easy extension of it to faulty net\u00adworks: retransmission; unique \nidentifiers for messages; and no FIFO assumptions. In DV1, assignments are retransmitted, but not ac\u00adknowledgements, \nsince the destination of the latter is always correct and hence, in a reliable network, they are guaranteed \nto arrive. The only algorithmic modification required of DV,, in order to operate over a non-reliable \ncommunication network, is that an ackj (X) message be retransmitted to m every time (X := T )m is received, \nand that superfluous ack messages be discarded. The rest of the changes employed in DV.C relative to \nDV, are in order to model communication via a faulty network. A global network component is incorporated \nin DV~C states. Transitions that send a message place it in the network, rather than in the receiver \ns input buffer. Additional transitions are Accept, which accepts a message from the network, and Fault, \nwhich removes a message from the network. We use the standard weak fairness requirement and, in addition, \nrequire that Ac\u00adcept be strongly fair: If a message M from processor i to processor j is intermittently \nin the network then eventually it either ceases to be in the network or it is accepted by j. In the full \npaper we prove that f, as defined for DVl, is a refinement mapping from DVPC to DVO~. The proof follows \nthe same structure as the proof of Theorem 2, except that the proof of the eventuality property is much \nmore complicated and it uses the RESP rule for strong fairness, instead of the W-RESP rule.  7 Conclusions \nand Future Work One aspect of logic variables remained unexploited in the above discussion: Since a message \nand its recur\u00adsively embedded ports are the value of a variable, the history of communication on a directed \nlogic variable can be viewed as a cooperative, incremental, effort to construct its value. With this \nview in mind, the op\u00aderational semantics DVOS can be made more abstract: View the store u as a conjunction \nof equality constraints over the initial variables, and allow a processor i own\u00ading X? to receive any \nassignment X := T for which the equality X = T is implied by a U ~i (with new variables in T existentially \nquantified). This operational semantics prevents the receiver from observing the granularity in which \nthe variable s value is produced, and hence, gives the implementa\u00adtion greater freedom in determining \nmessage granular\u00adity. In particular, the sequence of assignments (Xi := [MilXi+l?])~=l could be implemented \nby one message Xl := [Ml, . . . . Mn lXn+l?], reducing message handling overhead. Conversely, if a large \nlist is assigned in one step, the implementation can break it into sev\u00aderal smaller lists of a size appropriate \nfor the under\u00adlying network protocol and the receiver s buffer size. The alternative in both cases is \nto do packet assembly\u00addisassembly in the underlying network protocol, which is highly undesirable. We \nhave not discussed above the possibility of circular assignments: a set of assignments {Xi := Ti }$=1 \nfor which Xi+l ? E Readers(Ti ) and Xl? E Readers(T. ). Since no processor can be the owner of such an \nXi?, no processor can perform a Receive transition on such a message. In DVO, these messages simply remain \nin a. In DV. they are routed in cycles forever, and in DVI, for each assignment Xi := Ti, an infinite \nnumber of messages are created, each of which may travel in the network forever. To be robust, these \nalgorithms have to be upgraded to detect and discard circular assignments, Although in theory there is \nan infinite supply of logic variables, in practice they need to be recycled. Standard approaches to recycling \npacket identifiers [30], combined with a protocol that sends a cleanup message for every active writable \nvariable every so often, may be applica\u00adble to recycling logic variables. The two extensions to DV, \nDV.C and DV,C could be integrated, to form an implementation that performs garbage collection over a \nfaulty network. All that is required is that garbage-collection messages will safely arrive at destination. \nTo avoid an additional ac\u00adknowledgement protocol, these messages can piggyback on the other two types \nof messages employed. Piggy\u00adbacking on acknowledgements is relatively simple be\u00adcause they are always \nsent to a known destination (and are proved to finally reach it). In contrast, the fi\u00adnal destination \nof an assignment is not known in ad\u00advance. Therefore garbage-collection messages should include their \ndestinations when piggybacked on ass\u00adignments. The receiving processor should handle a garbage-collection \nmessage (piggybacked on an assign\u00adment) only if it is the destination of that message. Fi\u00adnally, upon \nreceiving an acknowledgement, the receiving processor (the sender of the assignment) identifies the accepted \ngarbage-collection messages according to the sender of the acknowledgement. Note that it is impor\u00adtant \nto employ the piggybacking on the two types of DVrC messages since the two message types are gener\u00adall \ny sent to different locations. We have considered network faults but not proces\u00adsor faults. Methods \nof optimistic recovery [29], al\u00adthough designed for a different model of message pass\u00ading, may be applicable \nto directed logic variables as well. Furthermore, since directed logic variables are single\u00adassignment, \nthe transformation described in [8] is appli\u00adcable, and hence some of the recovery, replication, and \nsnapshot techniques described there could be applied. References [1] Abadi, M,, and Lamport, L., The \nExistence of Re\u00adfinement Mappings, Proc. 3rd Annual Symposium on Logic in Computer Science, pp. 165-175, \nIEEE, 1988. [2] Ackerman, W. B., Dataflow Languages, IEEE Com\u00adputer, 15(2), pp. 15-25, 1982. [3] America, \nP., de Bakker, J., Kok, J. M., and Rut\u00adten, J., Operational Semantics of a Parallel Object-Oriented Language, \nProc. 13th ACM Symposium on Principles of Programming Languages, pp. 194\u00ad208, 1986. [4] Clark, K. L., \nand Gregory, S., PARLOG: A Parallel Logic Programming Language, Research Report, DOC 8315, Dept. of Computing, \nImperial College, 1983. [5] Foster, I., and Taylor, S., Flat PARLOG: A Basis for Comparison, Int. J. \nof ParalleJ Programming, 16(2), pp. 87-125, 1987. [6] Foster. I., and Taylor, S., Strand: A Practical \nPar\u00adallel Programming Language, Proc. North Ameri\u00adcan Conference on Logic Programming, edited by Lusk, \nE., L., and Overbeek, R., A., MIT Press, pp. 497-512, 1989. [7] Gaifman, H., Maher. M., and Shapiro. \nE., Full Ab\u00adstraction for Asynchronous Concurrency, in prepa\u00adration. [8] Gaifman H., Maher, M., J., and \nShapiro, E., Replay, Recovery, Replication, and Snapshots of Nondeterministic Concurrent Programsj submit\u00adted, \n1990. [9] Hewitt, C., A Universal, Modular Actor Formalism for Artificial Intelligence, Proc. International \nJoint Conference on Artificial Intelligence, 1973. [10] Hirata, M., Programming Language Doc and its \nSelf-description, or, X = X is Considered Harmful, Proc. 3rd Conference of Japan Society of Software \nScience and Technology, pp. 69-72, 1986. [11] Hirata, M., Parallel List Processing Language Oc and its \nSelf-description, Computer Software 4(3), pp. 41-64, 1987 (in Japanese). [12] Hoare, C. A. R., Communicating \nSequential Pro\u00adcesses, Prentice Hall, 1985. [13] Kahn, K., M., and Kornfeld, W., A., Money as a Concurrent \nLogic Program, Proc. North American Conference on Logic Programming, edited by Lusk, E., L., and Overbeek, \nR., A., MIT Press, pp. 513\u00ad534, 1989. [14] Kahn, K., and Saraswat, V. A., Actors as a Special Case of \nConcurrent Constraint (Logic) Program\u00adming, Xerox Technical Report, 1990. [15] Kleinman, A,, Moses, M,, \nand Shapiro, E., A Dis\u00adtributed Variable Server for Atomic Unification, Proc. 9th ACM SIGACT-SIGOPS Symposium \non Principles of Distributed Computing, pp. 59-74, 1990. [16] Kliger, S., Yardeni, E., and Shapiro, E., \nThe Lan\u00adguages FCP(:) and FCP(:,?), J. New Generation Computing, 7, pp. 89-107, 1990. [17] Liskov, B., \nand Shrira, L., Promises: Linguis\u00ad tic Support for Efficient Asynchronous Procedure Calls in Distributed \nSystems, Proc. ACM SIG-PLAN Conference on Programming Languages De\u00adsign and Implementation, 1988. [18] \nManna, Z., and Pnueli, A., Completing the Tempo\u00adral Picture, 16th lCALP, LNCS 372, pp. 534558, 1989. \n[19] Mierowsky, C., Taylor, S., Shapiro, E., Levy, J., and Safra, S., The Design and Implementation of \nFlat Concurrent Prolog, Technical Report CS85\u00ad 09, Weizmann Institute of Science, 1985. [20] Miller, \nD., Borbow, E., and Levy, J., Logical Se\u00adcrets, pp. 140-161, Chapter 24, Vol. 2 in [27]. [21] [22] [23] \n[24] [25] [26] [27] [28] [29] [30] [31] [32] [33] [34] Mullender, S. J., van Rossum, C., Tanenbaum, A. \nS., van Renesse, R., and van Staveren, H., Amoeba: A Distributed Operating System for the 1990s, D3EE \nComputer, 23(5), pp. 44-53, 1990. Nelson, B. J., Remote Procedure Call, Ph.D. The\u00adsis, Rep CMU-CS-81-1 \n19, Dept. of Computer Sci\u00adence, Carnegie-Mellon Univ., 1981. Saraswat, V. A., Partial Correctness Semantics \nfor 5 tb Conference on Foundations CP[J, I,&#38;], Proc, of Software Technology and Theoretical Computer \nScience, LNCS 206, pp. 347-368, 1985. Saraswat, V. A., Concurrent Constraint Program\u00ad ming Languages, \nPh. D. Thesis, Carnegie Mellon Univ., 1989. Saraswat, V. A,, Kahn, K., and Levy, J., Janus: A step towards \nDistributed Constraint Programming, To appear in Proc. North American Conference on Logic Programming, \nMIT Press, 1990. Shapiro, E., A Subset of Concurrent Prolog, Tech\u00adnical Report CS83-06, Weizmann Institute \nof Sci\u00adence, 1983. Shapiro, E. (Editor), Concurrent Prolog: Collected Papers, Vol. 1 &#38; 2, MIT Press, \n1987. Shapiro, E., The Family of Concurrent Logic Pro\u00ad gramming Languages, ACM Computing Surveys 21:3, \npp. 412-510, 1989. Strom, R. E., and Yemini, S., Optimistic Recov\u00adery in Distributed Systems, ACM Transactions \non Computer Systems, Vol. 3, No. 3, pp. 204-226, 1985. Tanenbaum, A. S., Computer Networks, Prantice \nHall, 1988. Taylor, S., Av-Ron. E., and Shapiro, E., A Layered Method for Process and Code Mapping, J. \nNew Generation Computing 5(2), 1987. Also Chapter 22, Vol. 2 in [27]. Taylor, S., Parallel Logic Programming \nTech\u00adniques, Prentice Hall, 1989. Ueda, K., and Furukawa, K., Transformation Rules for GHC Programs, \nProc. International Conference on FiI%h Generation Computer Science, pp. 582\u00ad591, 1988. Unix Reference \nManual Reference Guide, Univ. of California, 1984.  A The Refinement Mapping Proof Technique In this \nappendix we briefly describe the refinement map\u00adping proof technique, which is formally given in [1]. \n To show that a specification S1 implements a specifi\u00adcation S2, we have to show that there exists a \nmapping function f from S1 states to S2 states, satisfying the following conditions:  (Rl) Observable \nare preserved by ~. (R2) ~ maps S~ initial states to Sz initial states. (R3) ~ maps SI transitions to \n(possibly stuttering) S2 transitions. (R4) j maps computations allowed by S1 into computa\u00adtions allowed \nby S2. B Well-Founded Rules for Re\u00adsponse Properties In this appendix we present two general rules \nfor prov\u00ading the validity of the response formula p + Oq over computations of a given transition system \nP. For state formulae p and q, the temporal formula p + Oq is de\u00adfined to hold over a computation so, \nSI, ..., if every i >0 such that si satisfies p is followed by a j ~ i, such that sj satisfies q, or \nmore succinctly: Every p is followed by a q. A transition system (program) P consists of states X, transitions \nT, and the following two sets of fairness requirements: W ~ T A set of weakly fair transitions.  \nS ~ T A set of strongly fair transitions.  The constraints imposed by the two classes of fairness requirements \non computations c = so, SI, s2, ..., are: . Weak Fairness: For each transition T E W, it is not the case \nthat r is enabled at all but finitely many positions of c (i.e., is continuously enabled from a certain \nposition on), but taken at only finitely many positions of c. Strong Fairness: For each transition ~ \nc S, it is not the case that r is enabled at infinitely many positions of c, but taken at only finitely \nmany po\u00adsitions of c. For an assertion p and a state s such that p holds on s, we say that s is a pstate. \nWe define the verification condition of the transition r with respect to the assertions p and q, denoted \nby {P}~{~}, to be the condition ensuring that every r\u00adsuccessor of a pstate is a q-state. The two rules \nwe present refer to assertions p, q and p, and to: A helpfulness predicate h(r), identifying states in \nwhich the transition T is helpful in the sense that taking ~ will get us closer to the realization of \nq. W-RESP WI. p+(qv@) W2. for every ? E T {~ Ah(;) A(6=T)} 7\u00ad {Q v @A(r> 6)) V [wAh(?)A(r=c$))} for \neach r E W w3. {p Ah(T) A(6=T-)} T {q v (Q A(T-> 6))} W4. (p A h(T)) + (q V Enabled(r)) p*oq Figure 1: \nThe W-RESP Rule A well-founded structure (A, > ), consisting of a set d and a binary relation >, such \nthat there does not exist an infinite sequence ao > al > a2, . . . . A ranking function 6 : E H A, mapping \neach state s into a well-founded value 6(s) c A, to which we refer as the rank of the state. The intended \nmean\u00ad ing of the ranking function is that it measures the distance of the current state from the closest \nstate that satisfies the goal q. The first rule relies only on weakly fair transitions as the helpful \nones. In the rule we denote: It is given in Figure 1. A more general form of the rule, called RESP, relies \nas helpful transitions on both the weakly fair and the strongly fair transitions. Without loss of generality, \nwe can assume that S ~ W. For a strongly fair transition T, we can replace premise W4, requiring that \nr be en\u00ad abled now, by a weaker version requiring that either q is established or r becomes eventually \nenabled.  \n\t\t\t", "proc_id": "99583", "abstract": "", "authors": [{"name": "Alon Kleinman", "author_profile_id": "81100603758", "affiliation": "Department of Applied Mathematics and Computer Science, The Weizmann Institute of Science, Rehovot 76100, Israel", "person_id": "P15627", "email_address": "", "orcid_id": ""}, {"name": "Yael Moscowitz", "author_profile_id": "81100245930", "affiliation": "Department of Applied Mathematics and Computer Science, The Weizmann Institute of Science, Rehovot 76100, Israel", "person_id": "P304388", "email_address": "", "orcid_id": ""}, {"name": "Amir Pnueli", "author_profile_id": "81100648459", "affiliation": "Department of Applied Mathematics and Computer Science, The Weizmann Institute of Science, Rehovot 76100, Israel", "person_id": "PP15038449", "email_address": "", "orcid_id": ""}, {"name": "Ehud Sharpio", "author_profile_id": "81332526865", "affiliation": "Department of Applied Mathematics and Computer Science, The Weizmann Institute of Science, Rehovot 76100, Israel", "person_id": "P75896", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/99583.99615", "year": "1991", "article_id": "99615", "conference": "POPL", "title": "Communication with directed logic variables", "url": "http://dl.acm.org/citation.cfm?id=99615"}