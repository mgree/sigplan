{"article_publication_date": "01-03-1991", "fulltext": "\n Optimal Derivations in Weak Lambda-calculi and in Orthogonal Terms Rewriting Syst ems. Luc Maranget \n* Abstract We introduce the new framework of Labeled Terms Rewriting Systems (T~RS), a general framework \nto ex\u00adpress sharing in Term Rewriting Systems (TRS). For Orthogonal TZRS, an important subclass of T~ \nRS, we characterize optimal derivations. This result is applied to weak A-calculi, showing the optimality \nof the tazy strategy, that is, the call-by-name with sharing strat\u00adegy. The result is also valid in the \npresence of &#38;rules, as in PCF. Orthogonal T~RS is also useful as a calculus for proving syntactic \nproperties of functional languages. 1 Compilation of the ~-calculus Most compilers for functional languages \ntranslate their source language into some enriched Acalculus [17], and then, compile this intermediate \nlanguage to a low-level language, such as mutually recursive supercombinators, as in LML [2, 10], or \ncategorical combinators, as in CAML [4]. These low-level languages define differ\u00adent forms of weak /3-reduction. \nWe now describe two of these low-level languages, supercombinators and ex\u00ad plicit substitutions, and \ngradually introduce our exten\u00ad sions to the TRS formalism, INRIA Rocquencourt ; this work was partially \nfunded by DRET under grant N08780814. Permission to copy without fee all or part of this material is \ngranted provided that the copies are not made or distributed for direct commercial advantage, the ACM \ncopyright notice and the title of the publication and its date appear, and notice is given that copying \nis by permission of tbe Association for Computing Machinery. To copy other\u00adwise, or to republish, requires \na fee and/or specific permission. @ 1990 ACM 089791-419-8/90/0012/0255 $1.50 1.1 Compilation to supercombinators \nConsider the following A-expression : The compilation into supercombinators yields a set of combinator \ndefinitions and an expression to be com\u00adputed : II X=X 12X=X Kxy=.z Ffz=f(Gfz) Gfz=fx F (K 11) IZ L/ \n Note how the variable f, free in (Jz.f z), is turned into a locally bound variable which is supplied \nwhile calling the supercombinat ors F and G. This process is known as A lifting [2]. These expressions \nare graphs, to be reduced by the method of lazy graph reduction [2, 17]. There is an implicit binary \napplication node, written @ in the following. As an example, the expression F (K 11) 12 is equivalent \nto @(@(F, @(I<, 11)), 12), or graphically : @ Substituting (+ for = in the supercombinators definitions \nyields the rules {llX + x, . . .}, of the general form ai + pi , to produce a classical Term Rewriting \nSystem. The elementary one-step reduction relation, written M -+ M , is induced by the following axioms \nand inference rules : 255 (Red) O_(CX~) -+ c@) M-+M (APP1) @(M, N) + @(M , N) N+JJ (APP2) @(M, N) --i \n@(M, N ) In the rules above, a stands for any substitution, that is, a function mapping terms to terms, \nsuch that, a(~(kfl , Ivfz, . . .Mn)) = f(m(M1), a(M2), . .c(Mn)). This system is left-linear (no variable \nappears twice in the left-hand sides of the rules) and has no critical pairs (the left-hand sides of \nthe rules do not overlap with a subterm of another left-hand side). It is an orthogonal system in the \nsense of [9]. 1.2 Compilation to weak J-calculus with explicit substitution There is no simple definition \nof weak reduction in the traditional A-calculus. If reduction under the A s is not allowed (i.e. if the \n(f) rule is eliminated), the Church- Rosser property of the A-calculus no longer holds : MJ.y ((AZ.%) \n(k.%)) Although some deterministic reduction strategies in the A-calculus may be qualified as weak \nstrategies , as long as they prohibit reduction under the A s, there is no straightforward way to define \nand study such strate\u00adgies. A general approach is to map A-terms onto a new formal system. In this new \nframework, it is natural to define a reduction resembling fl-reduction with no re\u00adduction under the A \ns , and which possesses the Church-Rosser property, inducing a kind of weak-calculus over A-terms. This \ncan be done with supercombinators, as introduced in the previous section, or with a fixed set of combinators, \nas in [7], where combinat ory logic and weak combinatory equality are proposed as a basis for defining \nweak A-equality . These solutions are a bit cent rived, since the y involve a non-trivial translation \nprocess from the A-calculus into a (somewhat) different framework, A-1ifting in the case of supercombinators, \nand the so called abstraction algorithm in the case of combinatory logic. A simpler approach is compilation \nto a restricted ver\u00adsion of the b calculus, one of the many A-calculi with explicit substitution as introduced \nin [1, 6]. These cal\u00adculi are similar, and very close to the original ~-calculus. Terms are defined by \n: ~-t erms M::=nl MNl~MIM[s]l.X substitutions s::= M.s]idl~ where n is any number greater or equal than \n1 and X, x are free variables representing unknown terms and substitutions. The substitution id is the \nidentity sub\u00adstitution, We use the De Bruijn s notation, with vari\u00adables names replaced by indices n \nreferring to the n~ surrounding A binder. As an example, the A-terms k.% and k,~y,~ become Al and AA2 \nin De Bruijn s notation, This notation is well-suited to formal sys\u00adtems, eliminating the a-conversion \nstep needed for the A-calculus with named variables. The compilation process is just a translation of \nthe original J-term into De Bruijn notation, and the addi\u00adtion of the identity environment : The reduction \nrules are a subset of the general rules of the full Au-calculus : (WeakBeta) al = (~M)[s] N + M[N.s] \n= ,&#38; (App) @z = (M N)[s] ---+ M[s] N[s] = ,&#38; (FVar) a3 = IIM.s] + M = ~3 (RVar) cw = n+l[M,s] \n+ n[s] = ~4 The rule (RVar) stands for an infinity of rules (RVar2), (RVar3),. . . The elementary elementary \nreduction rela\u00adtion is the relation M + M on terms induced by : (AppSubst2) $+$ M[s] + M[s ] (Appl) M+Mt \nMN+M N (APP2) N--+N MN-+ MN (Consl) M--+Jf M.s + Mf.s (Consz) Jf,s $+$ -+ M.s{  Again there are some \nsyntactic shortcuts. This re\u00adduction system is called the weak k-calculus, it is fully characterized \nby the four reduction rules {.. . cu + /3~ . . .}, and by a domain function demo, which maps function \nsymbols to allowed directions of reduction. Here we have, dorn(@) = {1, 2}, dom([]) = {2}, dom(.) = {1, \n2} and dor-n(~) = 0. In other words, it is not possible to compute under A-abstractions or in the non-environment \npart of a closure, hence the name (weak b-calculus. These restrictions allow great sim\u00adplifications to \nthe full Am calculus which has 24 reduc\u00adtion rules. We recall, without further development, that the \nh calculus implements the }-calculus. See [1, 6] for detailed explanations and [14] for examples of de\u00ad \nscriptions of abstract machines implementing strategies of weak reduction in the framework of the weak \nAu\u00ad calculus< In the rest of this paper, we shall investigate the struc\u00ad ture of the derivations in \nconditional TRS. That is, TRS wit h a restricting domain function. We now use the example of the weak \nX_r-calculus to recall some basic notions on terms and TRS. See [9] for a complete intro\u00ad duction to \nthese notions. Each subterm in a term is characterized by its occw-\u00adrence which is either ~ for the whole \nterm or a sequence of integers nl .nz. . .nm representing the access path to it. For instance, in &#38;f \n= (~l)[s] 2[s], the subterms at occurrences 1.1.1 and 2.2 are 1 and s, or &#38;f/1.l.l = 1 and kl/2.2 \n= .s . Let u, v and w be occurrences. If u = VW, the occurrence v is a prefix of u or above u (v ~ u). \nIf w# e, thenv isaproperprejixofu (v < u). If neither u is a prefix of v nor v a prefix of u then u and \nv are said to be disjoint (u I v). If u and v are disjoint, then there exist a maximum common prefix \nw, with u= w.nu. u{, v= w.nv. v and nu # nw. If n~ < nv then u is to the lefl of v. The iefimost-outermost \norder\u00ading on occurrences <10 is defined by : u <10 v, iff the occurrence u is above v or to the left \nof v. The leftmost\u00adoutermost ordering is a total order relation on the set of the valid occurrences of \na given term. Occurrences gives a compact way to express reduc\u00adtions. In the proof tree of A: M -+ N \nthere is one and only one use of the (Red) axiom, the occurrence u of the subterm a(a) in M involved \nis sufficient to characterize the reduction A. This is written A: M ~ N. The sub\u00adterm a(a) is the redez \ncontracted by A, whereas a is the name contracted by A. Occurrences are followed across reductions by \nthe notion of residuals. As an example, in the reduction : A : (M)[s] 2[s] 3 1[2[s].s] the only residual \nof 1.1.1 is 1 and the only residual of 2.2 is 2.1.2. The notation u\\A stands for the set of the residuals \nof an occurrence u by a reduction A. The con\u00adtracted occurrences (i.e. the non-variable ones inside the \nname Q contracted) have no residual. In the exam\u00adple 6\\A = l\\A = l.l\\A = 1.2\\A = 0. There may be more \nthan one residual, in B: (M N)[s] + M[s] N[S], we have 2\\B = {1.2, 2.2}. There may be none of them, even \nif the original occurrence is not contracted, as in C: IIM.s] + M, where 2.2\\C = 0. Some occurrences \nin the result term are not residuals of any initial occur\u00adrence, they are created occurrences. For instance, \nthe reduction A creates the occurrences c and 2 in the final term 1[2[s].s]. In the weak Ar calculus, \nin contrast with conventional TRS, reduction is not allowed everywhere. A distinction between allowed \nand forbidden occurrences can be made. A subterm at occurrence u = nl .nz. . .n~ is allowed, iff, for \nevery i, the integer n~+l is in the domain of the function symbol at occurrence nl .nz. . .n~. For instance, \nin the term (( MM)[s] N)[-t], the allowed occurrences are e, and 2, all the other occurrences are forbidden, \nbecause they are prefixed by 1 (1 @ dorn([])). Therefore, the critical pair between rules (App) and (WeakBeta), \n(Mf)[s][t] IV[t] -((M14)[S] N)[t] + (M[N.s])[t] does not exist, since only the reduction +-is possible. \nHere, a redex in a term M is any subterm of the form a(a) at an aliowed occurrence. In the weak kr-calculus, \nredexes never overlap. More precisely, let al (al) and Lrz(az ) be two redexes in a term M at allowed \noccur\u00adrences U1 and U2. Then we never have U2 = U1 VI where VI is the occurrence of a subterm inside \nal, and al/vl is not a variable, except for the trivial case U1 = uz, al = crz. As in the case of standard \nTRS, it is sufficient to check the non-overlapping condition on the left-hand sides of the reduction \nrules only. In this sense, the weak h calculus has no critical pairs. As, in addition, it is left linear, \nwe call it an Orthogonal conditional TRS.  2 Sharing The rules with non linear right-hand sides introduce \nsharing. For instance, in the case of the supercombi\u00adnators, the reduction of the expression F (K 11 \n)Iz at occurrence e gives (K 11) (G (K 11) 12). An ordinary term cannot express the sharing of the two \ninstances of (1-11). To capture this sharing, we need to reduce graphs, instead of terms, as shown in \nfigure 1, in an informal representation. The Term Rewriting formalism is unable to express the leftmost-outermost \nwith sharing strategy, or lazy strategy of reduction of supercombinators, as it as im\u00adplemented in the \nG-machine [2, 10]. It can only express the leftmost-outermost or (call-by-name strategy. To express subterm \nsharing we introduce hzbeis in terms. Each subterm ~(kfl, Alz, . . .Mn ) is associated with a label i. \nThis is written as ~Z (lkfl, &#38;f2, , . .&#38;fn) or T(jz(A fl, M2, . . .Mn )) = 1, where r is the \ntop label func\u00adtion. A label is the address of a sub term. Sharing is Figure 1: Reduction on graphs \n expressed by equality of labels : a labeled representation of the right graph above is @~ (@~2(@ , I?), \n@r5(@~ (G~7, @~ (1{~3, 1?)), I))). Not all labeled terms represent graphs : the term @Z(1T, @~(l~, z)) \nor (l~(l~(x))~)~ cannot be associ\u00adated to a graph. We define which terms are graphs : Definition 2.1 \n(DAG) Let M be a labeled term in T . Then, M is a Directed Acyclic Graph ( DAG), ifffor all Ml and M2 \nsubterms of M, which are not variables, ~(M1) = ~(M2) implies Ml = M2. As Ml and MZ are labeled terms, \nthis means that, not only the structures of Ml and M2 are equal, but also ever y address (label) of the \ncorresponding subterms in Ml and Ivfz. This definition of DAGs prohibits cycles. The interaction of labels \nwith allowed and forbidden occurrences is delicate. It is important not to let re\u00adductions change redex-like \nterms (i.e. terms of the form a(a)) from non-redex state to redex state or vice-versa. Otherwise, there \ncould be a situation where some shared subterm is or is not a redex, depending on the instance you are \nconsidering. Even if reduction is started from a term with no sharing. Two conditions are sufficient \nto avoid this problem (see the definition of the residual redexes in section 3.1, as well as lemma 4,3 \nfor more details). (I) The residuals of allowed occurrences are allowed occurrences.  (II) The residuals \nof forbidden occurrences are forbid\u00adden occurrences.  It is important to notice that these condition \nare eas\u00adily decidable, given the domain function and the reduc\u00adtion rules. The set V(M) of the variables \nof a term M may be divided into the set V.(M) of allowed vari\u00adables and the set V~ (M) of forbidden variables, \nwith respect to their occurrences. For any term M, we have V.(M) U Vj (M) = V(M). If M is the left-hand \nside of a reduction rule ai + &#38;, then V. (CYi) n Vf (CYi) = 0, by the linearity of cq. Then, the \ntwo conditions (1) Va(fli) G v.(%) (2) ~f(Pi) G vf(~i)  on the reduction rules a~ + Pi, are sufficient \nto enforce (I) and (II). For instance, for rule (RVar) in the weak h-calculus : (1) V~(a[s]) = {s} Q \nv~(n+l[M.d) = {M, S} (2) Vf(n[s]) = Vf(n-1-l[Ii4.s]) = @  Whereas for rule (WeakBeta) : (1) Va((AM)[s] \nN) = VG(M[N.S]) = {N, S} (2) Vf((AM)[s] N)= V~(M[N.s]) = {M}  3 Labeled conditional reduction In this \nsection, we fully define conditional reduction on labeled terms (T~RS). The labels we introduce are more \nthan just addresses in a graph, they are designed in or\u00adder to capture the history of the subterm that \nthey dec\u00adorate. Further in this paper, we shall prove that these sophisticated labeled terms may be abstracted \ninto sim\u00adpler labeled terms that indeed represent acyclic graphs. Let V = {z, y,... } be the set of variables. \nLet 7. be the set of function symbols of arity n and F = U{F~ \\ n z O} the set of all function symbols. \nIn the case of the weak Aa calculus, FI = {A} and Fz = {@, [], .}. Let {a, b, . ..} be the atomic labehfrom \na set of symbols A. The sets 71 of labeled terms, g of elementary labels and L of labeis are defined \nby the fol\u00adlowing grammar : TZ: M ::= z I CZ I fZ(Ml, M2, . ..Mn) E: e ::=a I <lvf, a> L: i ::= eIl.e \n Labels can be omited on variables. The operation . on labels is the concatenation. It is associative. \nIt is convenient to extend the set of labels with the empty label ~. Unlabeled variables are then written \nx , and we have c. I = l.e = i. The top label function r: Tz --+ ZIJ{C} is then defined by : r(d) =i \nr(f~(Ml, Mz, . ..Mn)) = 1  The dot operator is extended to C u {c} x 7 ; [.zm = ~l.m ~.fm(~I,Mz> ..J%) \n= frm(MI, ~2, ..J&#38;) The terms that contribute to the creation of a subterm are introduced into its \nlabel with the (catch) operator, <M, a>. This operator is also extented to (atomically labeled) terms \nas a diffusion operation : <~, ~a> = ~c~,a> <M, p(. ... Ni)>=f<~==>( =>(. ... <Mini >,...) Substitutions \non labeled terms are similar to substitu\u00ad tions on standard terms, with the addition that they also operate \non labeled variables : C(Z1) = Z.rJ(z) a(f~(M1 , Mz, . . .kfn)) = ft(a(~l), f7(~2), . . .@(JL)) Reduction \nrules are labeled versions of the rules {.. ., % --+ t%, . ..} of an unlabeled system, with two important \nrestrictions. (1)Each pi is labeled, including variables, with atomic labels, giving Bi. (2) Each rule \nCYi + pi is replaced by a set of labeled rules, one for every constrained labeling of ~i. A constrained \nlabeling of ai is a labeling for which the top label of ai is an elementary label and the vari\u00adables \nin ai are not labeled. For every constrained labeling Az of ai, the new rule is Ai + <Ai, Bi >. Consider, \nfor instance, the symbol sets 71 = {1}, ~0 = {A} and V = {8}, the unlabeled rules 1(z) + z and A + I(A). \nThen, corresponding valid sets of la\u00adbeled rules may be written as J (a) + ~z~ (z)~ a> and A + I< A > \n>(AKA z), where e stands for any ele\u00admentary label, whereas a and b are fixed atomic labels. Valid reduction \nrules for the labeled weak b-calculus are represented in figure 2. Instead of considering only one-step \nreduction, we opt for a more general notion of reduction, the parallel re\u00adduction relation, as introduced \nby Tait and Martin-Lof. Parallel reduction is defined by figure 3. The inference rules (red) and (ret) \nrepresent collections of rules, one for each labeled reduction rule a + ~, and one for each non-nullary \nfunction symbol j. In the case of the weak k-calculus, the parallel reduction relation is defined by \nfigure 4. We now give the precise definition of a redex in the context of labeled conditional reduction. \nproof tree includes the application of a (red) rule, at occurrence u. In the following, we study Orthogonal \nTiRS only, i.e. the systems defined by a set of labeled reduction rules {.. ., cu ~ pi. ..} and a domain \nfunction, such that : (1)A variable does not appear twice in the CYi s (left\u00adlinearity). (2) Redexes \ndo not overlap in terms (no critical pairs). (3) For any reduction rule, V.(@i ) ~ V. (CYi) and   \n Vj(pi) g Vf (a;). A fixed point rule may be added to the weak Au\u00adcalculus : (Fix) (pM)[s] + M[(pM)[s].s] \n It is then straightforward to add reduction rules for con\u00adst ants to the weak k-calculus, such as integers \nand booleans, getting exactly the PCF calculus [18, 8], Pro\u00advided that, the conditions (l), (2) and (3) \nabove, still hold for the extended system. A parallel reduction A: M + N is fully characterized by the \nset U of the occurrences of the redexes that it contracts. This is written, A: M ~ N. If U is empty, \nthen A is the empty reduction 0. From now on, parallel reduction are simply called reductions. The derivation \nrelation +* is the reflexive and transitive closure of +. The length 1A] of a derivation A is the number \nof + steps in it. The null derivation O is the derivation of length zero. It is not to be confused with \nthe empty reduction 0, since ]01 = 1. The set D(M) is the set of all the derivations starting from term \nM. If A: M + N and B : N +* P, then AB is the derivation &#38;f +* N --+* P. For instance, consider the \nOrthogonal labeled system defined by the single rule 1 (z) + ~i~ (c)) b> and by the domain function dom(l) \n= {1}. Then, all the deriva\u00adtions starting from the term la (la(a)) are represented below : Ia(la(r)) \n{.} {1} f,1} /\\ Definition 3.1 (Redex) Let h be a term, -Let R be a subterm of M, at occurrence u. \nThe subierm R is a {,} {,} redex, iff the occurrence u is ailowed and R is in the \\ / form lo(a), where \na is the ?efl-hand side of a reduction ~< I (x), b>.<1 (x), b> rule, v is a substitution, and i is a \n(possibly empty) labei. The lejl-hand side a is the name of the redex R. Remark 3.1 Let A : M + N be \na reduci!ion in an Orthogonal PRS. Let u be an occurrence in N. We have That is, a subterm in M, at occurrence \nu is a redex, -two cases. Zither u is a residual of some occurrence v iff there exists a reduction starting \nfrom M, of which in M, and Figure 2: Labeled reduction rules for the weak As-calculus (id) &#38;f -+ \nkf (reef ) kfij + Nij if y(lfl, ~redl CT(W)--+ p(z) if z E V.(a) /.a(a!) Figure 3: Definition T(N/u) \n= <CYl, al >.. . <cr., a.> .T(M/v) Or u is creaied by the contraction by A, and ~(N/u)= <al, al> . . \n..<cxm. In both cases, ihe cu s are the trac-ted by A, and the ai s are corresponding ~~ s. And, in \nthe atomic label in the right-hand rule a + D. 3.1 The parallel moves The parallel moves lemma is the \nof Orthogonal TRS, as studied Orthogonal Tt RS, the key point not overlap in terms. We give a confluence \nof parallel reductions, of some redez i.u(a) a<a. a>, a>. names atomic second side ~ of redezes con\u00ad \nlabels from the case, a is some of the reduction lemma fundamental property in [9]. It also holds for \nbeing that redexes do full proof of the strong as this proof illustrates some important points in our \nlabeling scheme. First, two technical lemmas. Lemma 3.2 Let M be a labeled term. Let n and p be two substitutions \nover labeled terms. We have : Proofl Easy induction over Ikf[. 0 Lemma 3.3 Let A : M + N. For any label \n1, there exists a reduction B: l.M --+ i.N. Proofi The only subtle case is (red). In this case there \nexist a label m, two substitutions a and p, satisfying the of ij < dorn(f) Mij = Nij if ij @ dom(f) M2,... \na(z) + i,p(p) parallel premises that M a or P. written inference AL&#38;) -+ ~l(Nl, N2, . ..Nn) =p(z) \nifz GVj(cv) reduction of rule (red) and a reduction rule a + ~, such = m,a(cx) and N = m.p(@. Let 7 be \neither By the associativity of . , l.(m,a(-y)) may be (J.m) o(y). And 1.M + i.N follows, by a (red) rule \nproving (1.m) a(a) + (1.m).p(~). Proposition 3.4 (Strong confluence) _Lei M be a labeled term. Let A: \nM -+ N and B : M --+ P. There exists two reductions C and D such that C: N + Q and D: P+Q. Proof By induction \non the proof trees of reductions A and B. The non-trivial cases are : (ret w ret) Direct use of the \ninduction hypothesis.  (red +-+ ret) Let A and B be of the form :  A.: O(xa) + p(Z=) C(%j) = p(zj) \n(red) A: i.o(a) + i.p(@ (KC) B:M+P By the non-overlapping condition, we have P = l.p(cz) with VXa G Va(cx) \nB. : a(za) + p(ra) and Vzj c Vj(cr), U(zf) = p(zj). By induction hy\u00adpothesis, for any variable z. in \nV.(o), there exists a term Q. such that p(a. ) + Q. + p(z.). Define the substitution ~ by : @(za) = Q. \nif $. ~ Va(cr) +(aj) = O(zf) if z, e Vj(a!) *(X) = x otherwise  (IdT,,m,) M -+ M (IdSubst) S -+ S Ml \n-+ N1 Afz --+ Nz ~l--+Nl s2--+t2 s~ --+ tz (recApp) (recCOns) (Ml M2)i -+ (Nl N,)z (Ml O(N) -+ p(N) 0(s) \n+ p(s) Lr(ll !f) = p(kf) (redBeta) i.o(q) + i.p(pl) O(M) + p(fw) a(s) --+ p(s) (redFVar) i.a(cql) + Z.p(ps) \nFigure 4: Parallel reductions Let Q be the term i.v(~). The inference rule (red) may be applied directly \nto prove P = i.p(rx) + Q = i.4(P). Moreover, since Va(@) G v.(a) and Vj(@ g Vj (a), lemma 3.2 does apply \nand p(j?) -+ 4(8). Finally, N = i. J@) ~ Q = i.o(e) comes from lemma 3.3. (red e red) By the non-overlapping \ncondition, the reduction rules involved in A and B are, in fact, the same rule a ~ ,f3 :  A.: a(za) \n+ p(za) a(%j) = p(izj) (red) A : i.a(a) + i.p(~) ~red) B.: a(%a) + $0(2.) a(zf ) = $2(Z j ) 1?: 1.(7(0!) \n+ i.p(p) The proof is then similar to the one above. 0 Let A: M z N and B:M Z P. Then, thanks to the \nnon-overlapping condition and because the residuals of an allowed occurrence are allowed occurrences, \nall the subterms rooted at occurrences in V\\A are redexes. They are residual redexes. And the reduction \nB\\A : N LA Q is the residual reduction of B by A. We use A U B to denote the derivation A(B\\A). Lemma \n3.5 (The parallel moves lemma) LetM he a labeled term. Let A and B be two reductions starting from M \n: (1) Bo-th A U B and B l-l A lead to the same term. (2,) For any occurrence u in M, u\\(AuB) = u\\(BuA). \nProof By a careful relabeling of M one may track the redexes contracted by A and B, as any other set \n(recSubst) .s2) --+ (N1.t,){ (lW,[s,]) + (M,[t2]) ~redApp) a(s) + p(s) cr(kf) = P(M) CT(N) = P(N) 1.c7(a~) \n+ 4J.f@2) C7(fkf) + p(kf) cr(s) --+ p(s) (redRVar) i.a(aq) + i.p(pq) for the weak Ao-calculus of occurrences \nin M. The result then follows from the strong confluence of parallel reductions. 0 Derivations satisfying \n(1) and (2) are said to be equiv\u00adalent. This is written A U B s B U A, a formula ex\u00adpressing the parallel \nmoves lemma in a compact way. A key consequence of the parallel moves lemma is the extension of the notion \nof residual to composite deriva tions. As shown by figure 5, the parallel moves lemma for derivations \nA and B of any lengths n and m comes from the parallel moves lemma for the elementary re\u00adduction steps. \nMore precisely, the residual derivations are defined inductively : (1) o\\B =o (2) (AlA2)\\B = (Al\\B)(A,\\(B\\Al)) \n (3) A\\O =A (4) A\\(BIB,) = (A\\Bl)\\B2  The inclusion relation (~) on derivations is defined by : A ~ \nB, iff there exists a derivation C, such that AC s B. The residual derivations gives a simple deci\u00adsion \nprocedure for the inclusion (L) and the equivalence (~). For any two derivations A and B starting from \nthe same term : A~BsA\\B=@lAl A-BsB~AGB We shall not go further into the development of the properties \nof derivations. The study in [9] is applicable to Orthogonal TZRS, since all results follow from the \nparallel moves lemma. We mention two useful results. (1) The relation --+ * is Church-Rosser, therefore \nnor\u00admal forms, when they exist, are unique. (2) <D(M)I, -, G, U > is an upper semi-lattice.  Al B1 \nA B A B ,. . .< / .. ,/ . B. /@\\ ,/ . n . /., , ,/ . /\\ A;1 / . / . ,, A \\ ,2 poy / . B/A S , A/B \nm A Figure 5: The parallel moves lemma for derivations Proposition (1) is a classical result. Proposition \n(2) fol\u00adlows from algebraic manipulations and inductions over definitions.   4 Complete derivations \nThe complete derivations are well behaved parallel derivations. They mimic the derivations on graphs, \nby contracting at once all the instances of a shared redex. 4.1 Redex family Recall that a redex is in \nthe form i.o(a) where 1 is a label, u is a substitution and a is the name of the redex. Definition 4.1 \n(Redex family) Let M be a labeled term. A redex family F in lf is any set of redexes with the same name. \nA complete redex family ~ in ilf is the set of all redexes with the same name. The intuition is that \nthe name of a redex fully char\u00adacterizes it. Let A: M + N be a derivation. If a redex S in N is a residual \nof a redex R in M, then R and S have the same name. Furthermore, the labels inside the name of a created \nredex tells us about its history : Definition 4.2 (The contribution relation) -M a and -y be two redex \nnames. The redez name a directly contributes to ~, written Q < y; ifl there is a label 1 in -y such that \n: 1 = 11.<a, a>.lz where 11, i2 are possibly empty labels, and a is an atomic label. The contribution \nrelation <+ is the transitive closure of < . The is equal or contributing relation <* is the refiexive \nand transitive closure of < . In fact, in the definition of direct contribution, ~ could be any term \nM. We thereby define the direct contribu\u00adtion relation a < A4 for any redex name a and any term M. The \nrelation <+ and <* are also extended to the case where 7 is any term Ivf. For any redex name a and anyterm \nM : The following lemma explicits how the labels capture the creators of a new redex. Lemma 4.3 Let A: \nM --+ N be a reduction. Let a be the name of a redex R in N. Either, there exists a redex S in M, contracted \nby A, of name 7 directly contributing to a, or R is a residual of a redex of name cx in M. Proof: Consider \na redex R = ~.~(~), of name a, at an allowed occurrence u in N. Suppose that there is no redex name y \nin M such that y < a. Notice that 7(R) = i.e where e = r(a), The occurrence u can\u00adnot be a created occurrence. \nIf it was the case, then e would be equal to <y, a > where y is a redex name in M. Therefore the occurrence \nu in N is a residual of an allowed occurrence v in M. Now, what remains to be proved is that M/v is a \nredex of name a, of which R is a residual. That is to say, for any non variable occurrence w % e in a, \nthe occurrence vw is a valid occurrence in M, the occurrence uw in N is a residual occurrence of vw and \n~(M/vw) = ~(N/uw). For the same reason as the occurrence u, the occurrence uw cannot be a created occurrence. \nTherefore uw is a residual of some occur\u00adrence VW , below v, in M. And, always for the same reason, ~(M/uw \n) = ~(N/uw). Therefore, there are no redexes to be contracted in M, between occurrences and VW , and \nw = wt. So, there is a redex of name a at occurrence v in M, of which R is a residual. 0 The direct extension \nto derivations of this lemma will prove useful. Lemma 4.4 Lei A: M + N be a derivation. Let a be the \nname of a redez R in N. Either, there ezists a redex S of name y in M, such that y contributes to a, \nand a residual of S is contracted by some step in A. Or, R is a residual by A of a redez of name a in \nM.  4.2 Complete derivations Definition 4.5 (Complete reduction) Let A be a reduction, contracting the \nset of redex occurrences U. Let {Fl, Fz, . . . .Fn} be the partition by families of the set U. The completion \n~ of A is the reduction contract\u00ad ing the set of redez occurrences ~ = %1 U ~ U ., . ~. BB l\\ C\\(~\\B) \nc I c\\(B\\B) \\ + Figure 6: Completion of derivation BC Note that A ~ ~, since U ~ ~. The completion \nof derivations is defined inductively : A derivation is complete, iff it contracts only complete families \nat each step (~= A). Complete derivations are related to ordinary derivations. Proposition 4.6 Let A \nbe a derivation. Then, A ~ ~. Proofi To get an idea, see figure 6. The actual proof uses the definition \nA z B &#38; X7, AC -B. 0 Complete derivations are closed under the concatena\u00adtion operation. Lemma 4.7 \nLet ~: M -+* N and ~: N +* P betwo complete derivations. The derivation A B is complete. Conversely, \nif a derivation AB is complete then A and B are also complete. Definition 4.8 (Consistency) A term M \nis consis\u00adtent, ifl, for any redez in M, of name CY,the name cx does not contribute to M. This technical \ncondition expresses the fact that a redex family should not coexist in the same term, with sub\u00adterms \nwhich it could have helped to generate. A very useful consequence of consistency is that residual fami\u00adlies \nare characterized by their names : Lemma 4.9 Let M be a consistent term. Let A: M +* N. Let ~ be a complete \nredex family in M, of name a. The set of residual o..urrencea ~\\A is eza.ily ihe complete redez family \nof name Q in N : ~\\A = {u with N/u redex of name a in N}. Proof Only the inclusion ~ is not a direct \nconsequence of the definitions. Let R be a redex, of name a, at occurrence u in N. Suppose R is not a \nresidual of a redex in M. Then, by lemma 4.4, there exist a redex name y in M, contributing to a. This \ncontradicts the consistency of M. 0 Remark 4.1 The complete derivations starting from a consistent term \nM well behave : the names of the fami\u00adlies contracted are ail different one from another. More precisely, \nlet ~ be a complete redez family of name a in . M. Let ~ = Al AZ A3 be a complete derivation starting \nfrom M, such that AZ is a complete reduction contract\u00ading a family of name a. Then, the family of name \na, contracted by AZ is ~\\A1. And no step in the deriva\u00adtion A3 contracts a family of name a. In such \ncase, by abuse of language, we say that ~ contracts ~. Complete derivations are compatible with the consis\u00adtency \ncondition. Lemma 4.10 (Preservation~f consistency) Let M be a consistent term. Let A : M + N be a com\u00adplete \nderivation. The term N is consistent. Proof It is sufficient to prove the lemma for 1~1 = 1. Let a be \nthe name of a redex R in N. Suppose that a contributes to N, then there exists an occurrence u in N and \na label 1 = r(N/u) = 11.<y, a> .lz, with a <* y. The redex R maybe a residual of a redex in M, or not. \nThe occurrence u may be a residual of an occurrence or not. Therefore, there are four cases : (1) The \nredex R is a residual. Then a is the name of a complete family in M, and this family is not contracted \nby ~ complete. (i) The occurrence u is the residual by ~ of some occurrence v in M, and i = <al, al \n> . . . <czn, an > .r(M/v). The names ai for i E [1. . .n] are the names of redexes contracted by ~, \ntherefore a cannot be equal to any of the ai s. Furthermore a <+ ai for some iE [l.. .n] violates the \nconsistency of M, since the labels in the ai s are labels in M. There\u00adfore ~(M/v) must in the form ml \n.<~, a > .mz, with a <* y, which, again, contradicts the consistency of M. (ii) The occurrence u is \ncreated by the contraction of a redex of name 6. Then, by a proof similar to(i) above, a<+ N~a<*6~a < \nM.  (2) The redex R is not a residual. Then, by lemma 4.3, there exists a family in M, of name q, contracted \nby ~, such that q< a.  (i) Wehave uEv\\~ and i= <al, al > ,.. <an, an > .~(A!f/v). If a <* cu for some \ni E [1. . .n], then we have q <+ ai, which vi\u00adolates the consistency of M. Moreover, a <* ~(lf/v) implies \n6 <+ r(N/v) and contradicts the consistency of hf. (ii) The occurrence u is created by the contraction \nof a redex of name 6. Then, we have Q <+ M~LI<*6~V<+6~q<+M. 0 The result of lemma 4.9 on families is \nalso a result on reductions. It extends to composite derivations : Lemma 4.11 Let M be a consistent term. \nLet ~ : M + N be a complete derivation and B : M + P a derivation. The derivation ~\\B is complete. Proof \nThe proof is by induction on III, thanks to the preservation of consistency lemma (4.10). 0 Let 5(M) \nbe the set of the complete derivations start\u00ading from a consistent term M. Lemmas 4.7 and 4.11 im\u00adplies \nthat <~(Lf)/, a, ~, u> is a sublattice of D(Lf), with the same lub function.  4.3 Cost of a derivation \nDefinition 4.12 (Cost of a reduction) Let A be a reduction contracting the set of redex occurrences U. \nThe cost C(A) of A is the number of diflerent redez fam\u00adilies in U. For a composite derivation, C(AB) \n= C(A)+ C(B). In\u00adtuitively, we assume that the cost of the reduction of a given family is one. From an \nimplementation point of view, this means that we have to represent a complete family by a shared data \nstructure. The cost measure is relative to this data structure, and it will be of practical interest \nonly if this data structure is simple enough. The following results on costs are intuitive and rela\u00adtively \neasy, but they deserve some attention, since they illustrate the relevance of our cost measure. Lemma \n4.13 (Cost of residual) Let A : M --+ N and B : M -+* P be two derivations, starting from the same term. \nWe have : C(A\\B) s C(A). Proofi By induction on IA!. If IA I = 1, then consider the (may be not com \nplete) families contracted by A : Y1, 52 . . F.. The reduction A\\B contracts the non-empty fam\u00adilies \namong Fl\\B, F2\\B, . ..Fa\\l?. The result fol\u00adlows.  Now, if A is a composite derivation A1A2, then C(A\\B) \n= C(A1\\.B) + C(A2\\(B\\A1)). The re\u00ad  sult follows by direct application of the induction hypothesis. \n0 Proposition 4.14 (Cost of completion) Let A be any derivation. We have : C(z) < C(A). Proofi By induction \non the definition of comple\u00ad tion. Let A = BC. Then by definition, ,.,C(~) = C(z) + C(C \\(~\\13)). The \ninduction hypothesis gives C(x) S C(B) +C(C\\(~\\B)). Therefore, by Lemma 4.13 C(z) S C(B)+ C(c).n Together \nwith A ~ ~, this proves that contracting a complete family whenever one of its members is con\u00adtracted \ngoes (possibly) further at a (possibly) lower cost. This result is an important step toward the search \nof an optimal derivation} as it enables to restrict ourselves to the sublat tice of complete derivations. \n 5 The implementation of com\u00adplete derivations In this section, it is shown that complete reductions \nmay be implemented as reductions on directed acyclic graphs. The idea of representing DAGs as terms labeled \nby their location in a graph structure was introduced in sect ion 2. But the labeling scheme of section \n3 does not fit this purpose. The labels retain too much of the history of a subterm. For instance, the \nresiduals of a redex may very well have different top labels, although they sould be shared in a graph \nimplementation. Therefore, graphs are r~presented by a restricted la beling scheme. This new scheme is \nbuild with the same set A of atomic labels as T . We defined the set of pre-DAGs, Td as : P : pre-DAGs \nill ::= z [ fi(Ml, M2,. , .Mn) Ld : locations I ::= a I i.a There is an homomorphism H, from the labeled \nterms in T~ to the pre-DAGs in Td : H(d) =z H(fi(..., M,,...))= fh[ l(oo., H(Mi),...) { h(a) =a h(l.e) \n= h(e) h(<cr, a>) = h(-r(cr)).a { Let M be a labeled term in T~. The term lY(Af) in Td is the pre-DA \nG representation of M. The simple label &#38;(M) = h(r(M)) is the location of M. In the definition of \nh, the last clause may be rewritten as h(<a, a>) = &#38;(a) .a. Pre-DAGs are DAGs, if and only if they \nmeet the (intuitive) definition 2.1. The DAG condition on labeled terms is different, It is weaker than \nthe intuitive DAG definition and stronger than the H(M) is a DAG in Td condition. Definition 5.1 (DAG \nin labeled terms) Let J4 be a labeled term. The term M is a DAG, if, for any two subterms of M, that \nis Ml = f: e (M:, M;, . . .M~, ) and Mz = f$e (M~, M;, . . .M~,), we have : fl =fz &#38;(Ml) = &#38;(Mz) \na el = e2 Vi E [1. .m..,] M: = M: { This definition makes sense in terms of implementation. Indeed, \nif M is a DAG in T1, then its representation H(M) is a DAG in Td. Furthermore, let RI and R2 be two redexes \nin M, of names al and a2. Then, RI and RZ are in the same family al = az , iff they have the same location \n&#38;( RI) = &#38;( R2) . This last point is important. Intuitively, the reduc\u00adtions on DAGs are the \nreductions, that are complete relatively to the locations of the redexes. Definition 5.2 (DAG-reductions) \nLet M ~ T~. .Lei A : M % M. The reduction A is a DA G-reduction, ifl, for any two redex occurrences U1 \nand U2, in M : U1 E U and &#38;( M/ul) = &#38;( M/uz) implies uz G U. From the previous remark, the DAG-reductions \nstarting from a DAG are ezactly the complete reductions. Also notice that, if all the labels in M E T~ \nare atomic, then M and H(M) are syntactically equal, and then, the complex DAG definition is equivalent \nto the intuit ive definition. A general TIRS E is said to be a DAG Rewriting System (DRS), iff the right-hand-sides \nof the reduction rules in Z (the pi s) are DAGs. This restriction makes sense, since non-DAG right-hand \nsides may be expected to introduce non-DAG parts in the result of a reduction, even if reduction is started \nfrom a DAG. Moreover, this condition is easily checked, by only looking at the atomic labels in the pi \ns. All the systems seen so far are DRS. Lemma 5.3 (Preservation of DAGs) In a DRS, let M be a consistent \nDAG in T . Let ~ : M + N be complete derivation. Then N is a DAG. Proofi Omited, by lack of space. 0 \nThis result enables to identify the DAG-derivations and the complete derivations and closes the study \nof the implementation of complete derivations. Complete-call-by-need deriva\u00adt ions In this section, \nwe introduce a new notion of call-by\u00adneed derivation among the complete derivations. It is based on a \nstraightforward criterion on labels. Unfortu\u00adnately, there does not seem to be a simple counterpart of \nthis notion of call-by-need in unlabeled systems. 6.1 Properties of complete-call-by-need derivations \nAs the redex families are identified by their names, a family contributes to a complete derivation ~ \nwhen its name is present in the labels of the final term of ~ : Definition 6.1 (Contributing family) \nLei! M be a term. Le-1 ~: M +* N. .Let ~ be a complete redex family of name a in M. The family ~ is said \n-to con\u00adtribute to ~, iff its name a contributes to N. The following syntactical lemmas deal with the \norigin of contributing names. Lemma 6.2 Le-t A: M + N be a reduction. Let Q be a redex name contributing \nto N. Then, either a con\u00adtributes to M, or&#38; is the name of a redex in M, which is con-tracted by \nA. Proof Easy from remark 3.1. 0 . The transitive closure of this lemma easily follows from lemma 4.4: \nLemma 6.3 Let A : M --+ N be a derivation. Let a be a redex name contributing to N. Then, either a contributes \nto M, or there exists ~ redex R, of name -y in M, such that Y contributes to Q or Y is equal to a, and \na residual of R is contracted by some step in A. Some contributing families are more important than others, \nin the sense that their names cannot be deleted : Definition 6.4 (Needed family) Let M be a term and \n~ be a complete family of name a in M. Let ~ : M --+* N be a complete derivation. The family ~ is needed \nby the derivation ~, or ~ c N(Z), i~, for all complete derivations ~: N --+* P> the name a con\u00adtributes \nto P. In other words, a family is needed by a complete deriva\u00adtion ~, when it contributes to all the \ncomplete deriva\u00adtions greater that ~. Two basic properties of needed families are consequences of the \ndefinition : A complete-call-by-need derivation (CCBN) is a com\u00adplete derivation contracting only families \nneeded by the rest of itself, at each step. Definition 6.5 (CCBN) A complete derivation ~ : M --+* N \nis complete-call-by-need (CCBN~ iff, eiiher ~ is the nui! derivation, or~ = B C, where C is complete\u00adcall-by-need, \nand ~ is a complete reduction such that : ~: M uF% F P and Yl, Fz, . ..% in N(X).  For instance, consider \nthe terms formed with the nullar y symbol A, the binary symbol K, and the variables z and y. Also consider \nthe rules {Ae -+ AzA , a>, Ke(z, ~) ~ ZZKe( ~yJI b>} and the domain function dorn(K) = {1, 2}. Then, \nany derivation con\u00adtracting the Ad redex (always at occurrence 2), is not CCBN, whereas any derivation \nnot contracting it is. {1, 2} ]<~(A<A ,G>,A<A ,a>) Kb(Ac, Ad) A< K (x,Y), a>.<Ac, a> . ~. It is easy \nto check, from ~ ~ ~ a ~(~) ~ Af(@, that CCBN derivations derivations are closed by con\u00ad catenation . \nThe converse AI A2 CCBN a Z CCBN is not so obvious. It requires an important technical lemma, which we \nexpress in a very general form : Lemma 6.6 Let M be a consistent term, ~: M + N and B: M + P be two complete \nderivations. Let ~ be a complete redex family in M, of name Q, needed by ~. Then, either ~ is contracted \nby ~ and ~ is needed by ~, or F is not contracted by ~ and ~\\~ is not empty. Proof Suppose that there \nexists a complete deriva\u00adtion ~ : P - Q, such that a does not contribute to Q. The Church-Rosser property \nis true for complete derivations (end of section 4.2). Therefore, there ex\u00adist two complete derivations \nclosing the diamond dia\u00adgram :_~: Q ~ R and ~: N --+* R (figure 7). Since 7 E M(A), the name a does contribute \nto R. Then, by lemma 6.2, applied to the derivation ~, there must be a redex family of name ~ <* a in \nQ. Furthermore by lemma 4.4, applied to ~ and ~, there exists a redex of name 6<* ~ in P, and aredex \nof name q<* 6in M. By transitivity, q <* a. Therefore, by the consistency ofM, weget a=q,and then, a=b. \n0 Lemma 6.7 Let ~ : M + N be a complete-call-by\u00adneed derivation, where M is a consistent term. Then, \nfor any decomposition ~ = Al AZ, both derivations Al and ~ are complete-call-by-need. Proofi The result \nfor the prefix derivation ~ requires a full proof by induction on 1~1. e Case IZI = 1. By lemma 6.6, \napplied to ~ and ~, any redex family needed by ~ and contracted by ~ is also needed by ~. * M P z E* \nZ* 1 Q 1 N *5 * / \\ ~R Figure 7: Proof of lemma 6.6 e Case Al = B1 CI. By induction hypothesis, the derivation \nK is CCBN. Moreover, the derivation (71 AZ is CCBN as a suflix of ~. Therefor-by induction hypothesis, \nc is CCBN. Then Al =  B1 Cl is CCBN, since CCBN are closed under concatenation. 0 Complete-call-by-need \nderivations are also closed by residuals : Lemma 6.8 Let M be a consistent term, ~: M --+* N be a complete-tail-by-need \nderivation, and ~: M +* Q be a complete deriva~ion. Then, the derivation ~\\~ : P + Q Z S complete-call-by-need. \nProofi By induction on 1~[. Assume ]~1 = 1. By lemma 6.6, any complete fam\u00adily ~ contracted by ~ (and \ntherefore needed by A . CCBN), is needed by ~JA\\B), since ~ cannot have . any residuals by B(A\\B). Therefore \nif 7\\B M non\u00ad . empty then it is needed by A\\B, and the reduction ~\\~ contracts needed families. If ~ \n= Al AZ, then A\\B = (Al\\ B)(A2\\(B\\Al)). The induction hypothesis directly applies to AI and ~, which \nare CCBN by lemma-6.7.0 Intuitively, a CCBN derivation never contracts use\u00adless families. So, the CCBN \nderivations should be less expensive than other complete derivations, Proposition 6.9 Let ~: M +* N be \na complete-call\u00adby-need derivation, and ~ : M +* Q be a complete derivation, where M is a consistent \nterm. Then, the inequality C(A u B) ~ C(B u A) holds. Proofl By induction on 1~1. * If IZI = 1, then \nlet Xl, Yz, . . .Yn be the complete redex families contracted by A. Let al, az, . . ,an be their names. \nBy lemma 6.6~heflerivation BuA must contract the families F1, F2, ..25. sup\u00adpose, for instance, that \nthe derivation B contracts the families of names al, CY2. . .a~  with m ~ n, . whereas the reduction \nA\\B contracts the families AO=A of names a~+l, a~+2 . . .an (by remark 4.1, such a (M, = partition is \npossible). The cost of % is at least m, therefore there exists an integer k such that . C(z) = m + k. \nWhereas the cost of A\\B M ex\u00adactly n m. Hence, C~~ u~) = n+ k. Now consider the derivation A LI ~. The \ncost of ~ is exactly n. The derivation B\\A contract families with the same names as the families contracted \nby ~. But it does certainly not contract the families of names al, a2, . . .am, since these families \nare al\u00adready contracted by ~ (remark 4.1, again). Hence . C(B\\A) s k, and C(~U~) s n +k.  If ~ = Al \nA2. By the inductive definition of\\ : C(X uB) = C(A1 U ~) + C(AZ u (~\\Al)) C(~\\AJ C(B u~) = C(B U ~) \n+ C((B\\Al) LI ~) C(B\\AJ The result follows, since both Al and A2 are CCBN.D This important result demonstrates \nthat it is alway more efficient to start a complete derivation with a call\u00adby-need step than with any \nother step. 6.2 Existence of complete-call-by-need The CCBN derivations have nice properties, but their \ndefinition is not very constructive. In this section, for any complete derivation ~, it is shown that \nthere exists a CCBN derivation ~, such that : (1) =~x (2)~CCBNand~~~ a ~z~ The derivation ~ is a maximal \nCCBN derivation in\u00adcluded in ~. It will be constructed by induction over ~. A restricted version of the \nfinite development theorem will ensure the termination of this construction : Lemma 6.10 (Termination) \nLet ~ : M + N be a complete derivation. Let ~ be a complete redex family in M, contracted by ~. Let ~: \nM + Q be the complete reduction, starting from M, contracting the family ~. Then, C(A\\F) < C(z) Proo&#38; \nThis is a strict case of lemma 4.13. 0 Let M be consistent term. Let ~: M ~ lV be a com\u00ad plete deriva~ion, \nWe now ~nstruct a maximal CCBN derivation B, included in A. This is done by the fol\u00adlowing indu@ive process \n@gure 8). If there is no family needed by A in A4, take B = 0. Otherwise, there exists I 0 I Ml 0 I I \nM2 ! i Am with Af(A~) = 0 I i M. N   A\\B Figure 8: Maximal CCBN included in ~ a complete redex family \n~ in Af(~). Let ~: A4 ~ M be the complete reduction starting from M and contracting the family ~. Notice \nthat ~ ~ ~, since ~ contracts the family ~. Take for ~ the concatenation of ~ and ~, a maximal CCBN derivation \nincluded in A\\ J . By induc\u00ad tion hypothesis, ~ is CCBN. Whereas, by lemma 6.6, ~ is also CCBN. Therefore, \nF B is CCBN. By lemma 6.10 this process terminates, since the pos\u00ad itive integer C(A\\B) strictly decreases \nat each step. In fact, the construction stops when N(A\\B) = 0, which may occur before C(A\\B) = O. By \nconstruction, we get ~ ~ ~ (since at each step in the process, F\\A = @). It remains to be proved that \n~ is indeed maximal. Let ~ be any CCBN derivation included in A. The proposi\u00adtion ~ ~ ~ ~ ~\\~ ~ ~\\~ is \ntrue in the general case and easy to show from the definitions. There fo~e, here, we get C\\~ ~ A\\B, and \nN(c\\B) L N@\\B) = 0. . But, C \\B is CCBN, since ~ is CCBN. Therefore ~\\~, which can only contract families \nin N(c\\B), is empty, and ~ P~. The construction of a maximal CCBN derivation in\u00adcluded in a derivation \nA, is now specialized to the case where A is a normalizing derivation (i.e. A leads to normal form). \nLemma 6.11 Let A: M -+ N be a non-empty de? iv&#38;\u00adtion. Then, there exists a redex of name a in M, \nsuch that, either a is the name of a redez in N, or a con\u00adtributes to N and a residuai of R iS contracted \nby A. Proof From definitions and lemma 4.4. 0 Proof: Since ~ is non-empty and there is no redex in Corollary \n6.12 Let ~ : M + N be a non-empty normalizing complete derivation. Then, N(z) is not empty. N, there \nmust be a redex family in M, contributing to N. Furthermore, there is no non-empty derivation starting \nfrom the normal form N. So, any redex family contributing to N is needed by ~. 0 This means that, for \nany formalizable term, there exists a normalizing CCBN derivation. To prove this, we construct a suitable \nderivation as follows : (1)Since M has a normal form N, there is a complete normalizing derivation ~ \n(In the general case, take the Gross derivation, which, at each step, contracts all the available redex \nfamilies). (2) Construct a maximal CCBN included in ~. This yields the CCBN derivation B. By construction, \n~\\~ is normalizing and N(B\\Z) = 0. Hence, by corollary 6.12, B\\A is empty and B is normalizing. In fact, \nthis construction does not require the full knowledge of a normalizing derivation. What is really needed, \nat each step, is a family in Mi which contributes to the normal form. In some particular cases, it is \npos\u00adsible to find such a family only by looking at M~. Definition 6.13 (Left systems) A ~RS is said to \nbe a left system, i~, for any redez name a, and any allowed variable occurrence u in a, the subterms \nofa located tat occurrences after u in the lefimost-outermosi! ordering are all variables. This notion \nwas defined in [9, 16] for regular TRS, we ex\u00adtend it to TZRS. It is easy to check that both supercom\u00adbinators \nand the weak As-calculus are left systems. This definition has the following important consequence : \nLemma 6.14 ln a lefl system, let M be a term. Let A : M + N be a derivation. Let R be the leftmost\u00ad outermost \nredez in M. Let a be the name of R. Let S be the leftmost-outermost redex in N (if it exists), and let \nu be its occurrence. Then, either S is the only residual of R by A, or Q contributes to the name of S, \nor Q contributes to N, in a label at an occurrence before u in the leftmost-ontemnost ovdeving. Proof \nTedious case analysis. 0 In a left system, let M be a consistent, formaliz\u00adable term of normal form N. \nThen, from the previ\u00adous lemma, when M is not already in normal form, the family of the leftmost-outermost \nredex in M al\u00adways contributes to N. Therefore, the construction of a normalizing CCBN derivation may \nyield the complete leftmost-outermost derivation. It suffice to select, at each step, the family of the \nleftmost-outermost redex, which is known to be needed by any complete normal\u00ad izing derivation. In the \ngeneral case of (unconditional) Orthogonal TRS, the completions of the standard or outside-in derivations \nas defined by Huet and L6vy in [9], are also CCBN derivations. Fully exposing this result would go beyond \nthe scope of this paper. To summarize, we are able to construct a normaliz\u00ading CCBN in all cases, and \nin many practical cases, we showed well known standard derivations to be CCBN. 7 Optimal normalizing \nderiva\u00adt ions A result of optimality for reductions leading to normal form follows directly from the \npreceding sections. Theorem 7.1 Let M be a formalizable consistent term and N its normal form. Let ~: \nM --+* N be a complete call-by-need derivation from M to N. The cost of ~ is lower than the cost of any \nother normalizing derivation starting from M, that is, ~ is optimal. Proofl Let B be any normalizing \nderivation B. Then, because B ~ ~, its completion ~ is also normalizing. Since there 1s no redex left \nin the normal form N, we have A\\B = OIAI and B\\A = 01~1 (i.e. ~ and ~ are eq~valent). Hence : C(Z u ~) \n= c(~), and C(B u ~) = C(B). So, by lemmas 4.14 and 6.9, we get : C(B) z C(B) ~ C(A). 0 This proves the \nfollowing intuitive result : a deriva\u00adtion contracting only redexes that have to be contracted, while \navoiding duplication, is optimal for a cost measure taking sharing into account. Our notion of sharing \nis sensible, since it corresponds exactly to ordinary acyclic graphs, as used by actual graph reducers, \nTherefore, for the supercombinators as well as for the weak As-calculus, the leftmost-outermost strategy \nwith sharing is an optimal one. In the case of the weak A a\u00adcalculus, the substitution steps are taken \ninto account, making our cost measure close to implementation cost,  Conclusion The new notions of conditional \nTerm Rewriting Systems and labeled Term Rewriting Systems have been intro\u00adduced, their addition to TRS \nyields the more general T?RS framework. This new class of reduction systems deserves a full study in \nitself. For the Orthogonal TZRS, a large significant subclass of T~ RS, there is an optimal derivation \nleading to normal form. Furthermore, this optimal derivation may be implemented with ordinary graphs. \nThis paper extends the optimality result al\u00adready known for the recursive program schemes [3, 19] and \nthe graphical SK combinators [12] to Orthogonal T2RS, a clear, convenient and more abstract formalism. \nIn all cases, the optimal normalizing derivation may be effectively computed from any normalizing deriva\u00adtion. \nIn many practical cases, the optimal derivation may be computed from the starting term only. In par\u00adticular, \nwe prove the leftmost-outermost strategy with sharing to be optimal in two distinct forms of weak J\u00adcalculi, \nformalizing a result widely known as a folk s theorem . The exact proof is complicated by the neces\u00adsity \nof extending the TRS formalism in two new direc\u00adtions. As demonstrated in [5], finding optimal deriva\u00adtions \nin the full ~-calculus is a much harder problem, and the effective strategies proposed by [11, 13] along \nthe theoretical work of [15] are more complicated than simple lazy strategy. Acknowledgments I thank \nJean-Jacques L6vy for many helpful discussions. also thank John Bradley Chen and Xavier Leroy for their \neditorial work. References [1] M. Abadi, L. Cardelli, P.-L. Curien, J.-J. L6vy, Explicit substitutions \n. POPL 90. [2] L. Augustsson, Compiling Lazy Functional Lan\u00adguages Part II . PhD thesis, Chalmers University \nof Technology, Sweden, 1987. [3] G. Berry, J.-J. L6vy, Minimal and Optimal Com\u00adputations of Recursive \nPrograms . JACM Vol. 26, No 1, 1979. [4] G. Cousineau, P.-L. Curien, M. Mauny, The Cat\u00adegorical Abstract \nMachine . Functional Program\u00adming Languages an Computer Architecture, 1985, LCNS. [5] J. Field, On Laziness \nand Optimality in Lambda Interpreters : Tools for Specification and Analysis . POPL 90. [6] T. Hardin, \nJ.-J. L6vy, A confluent Calculus of Substitution . France-Japan Artificial Intelligence and Computer \nScience Symposium, Izu, 1989. [7] J .R. Hindley, J .P. Seldin, Introduction to Com\u00ad binators and A-calculus \n. London Mathematical Society, Students Texts 1, Cambridge University Press, 1986. [8] B.T. Howard, \nJ .C. Mitchell, Operational and Ax\u00adiomatic Semantics of PCF . L&#38; FP 90. [9] G. Huet, J-J. L6vy, Call \nby Need Computations in Non-Ambiguous Linear Term Rewriting Sys\u00adtems . INRIA, technical report 359, 1979. \n[10] T. Johnsson, Compiling Lazy Functional Lan\u00adguages . PhD thesis, Chalmers University of Tech\u00adnology, \nSweden, 1987. [11] V. K. Kathail, Optimal Interpreters for Lambda\u00adcalculus Based Functional Languages \n. Communi\u00adcation at the SEMAGRAPH workshop, Paris, April 1990. [12] J .R. Kennaway, An Outline of Some \nResults of Staples on Optimal Reduction Orders in Replace\u00adments Systems . Internal Report CSA/19/1984. \n[13] J. Lamping, An Algorithm for Optimal Lambda Calculus Reduction . POPL 90. [14] X. Leroy, The Zinc \nexperiment : an economi\u00adcal implementation of the ML Language . INRIA, technical report 117, 1989. [15] \nJ.-J. L6vy, Reductions correctes et optimales clans le ~-calcul . Th2se de Doctorat d Etat, University \nParis VII, 1978. [16] M. O Donnell, Computing in Systems Described by Equations . LNCS No 58, Eds G. \nGoos and J. Hartmanis, Springer, 1977. [17] S. L. Peyton Jones, The Implementation of Func\u00adtional Programming \nLanguages . Prentice-Hall, 1987. [18] G.D. Plotkin, LCF Considered as a Programming Language . Theoretical \nComputer Science, 5:225\u00ad255, 1977. [19] J. Vuillemin, Syntaxe, S4mantique et axiomatique d un language \nde programmation simple . Th&#38;e de doctorat d &#38;at, Universit6 Paris VI, 1974. \n\t\t\t", "proc_id": "99583", "abstract": "", "authors": [{"name": "Luc Maranget", "author_profile_id": "81100574739", "affiliation": "INRIA Rocquencourt", "person_id": "PP37038467", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/99583.99618", "year": "1991", "article_id": "99618", "conference": "POPL", "title": "Optimal derivations in weak lambda-calculi and in orthogonal term rewriting systems", "url": "http://dl.acm.org/citation.cfm?id=99618"}