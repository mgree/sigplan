{"article_publication_date": "01-03-1991", "fulltext": "\n Parallel Programming With Coordination Structures Steven Lucco Oliver Sharpt Computer Science Division, \n571 Evans Hall UC Berkeley, IBerkeley CA, 94720 Abstract All parallel programs have a communication \npat\u00ad tern that characterizes the exchange of information Parallel programs display two fundamentally \ndifferent and synchronization among the sub-computations of kinds of execution behavior: synchronous \nand asyn\u00ad the program. A communication pattern has some con\u00ad chronous. Some methodologies, such as distributed \nnectivity and may possess the property of uniqueness. data structures, are best suited to the construction \nThe connectivity determines which sub-computations of asynchronous programs. In this paper, we propose \ncan communicate. If a communication pattern has a methodology for synchronous parallel programming uniqueness, \nthen any given pair of sub-computations based on the notion of a coordination structure, a di\u00ad can communicate \nonly once (and only in one direc\u00ad rect representation of the multidimensional dataflow tion). patterns \ncommon to synchronous programs. We intro- Parallel programs can exhibit two fundamentally dif\u00ad duce Delirium, \na language in which one can concisely ferent kinds of communication pattern: synchronous express many \nuseful coordination structures. and asynchronous. We define synchronous programs to be those whose communication \npatterns have both 1 Introduction uniqueness and way to understand deterministic synchronous connectivity. \nprograms is Another in terms We are proposing a new methodology for writing par\u00ad of an execution graph. \nAn execution graph is simply a allel programs based on the idea of coordination struc\u00ad dataflow graph \n[1] where each node is a stateless sub\u00ad tures. Coordination structures are a direct representa\u00ad computation. \nEach unique pair of sub-computations in tion of the multi-dimensional data flow patterns com\u00ad the communication \npattern is an arc in the program s mon to a large class of parallel programs. We will begin execution \ngraph (see figure 1 for an example). by defining coordination as a basis that class structures for parallel \nof programs. and showing programming, After how we introducing to use them will describe If all execution \ninstances marized by an execution chronous. Summarization of a program can be sum\u00adgraph, that program \nis syn\u00adof asynchronous programs Delirium, a coordination language which cise, declarative expression \nof coordination supports con\u00adstructures. requires graph a (see more figure general model based 1). Like \nexecution on a graphs, message a mes\u00ad sage graph shows which sub-computations exchange *Supported in \npart by an IBM Fellowship. Email address: information during the computation. The differences luccot?f \nire .Berkeley. EDU t supported in part by a Hertz Fellowship and by the between these two types of graphs \nare: Lawrence Llvermore oli.ver@karakonrm. Nationaf Berkeley. Laboratory. EDU Email address: 1. Exactly \none data item travels along each arc of an execution graph; multiple items may travel along arcs in a \nmessage graph. I possible communication patterns of the program for a given Permission to copy without \nfee all or part of this matertial is granted input provided that tfte copies are not made or distributed \nfor direct commercial advantage, the ACM copyright notice and the title of the publication and its date \nappear, and notice is given that the copying is by permission of the Association for Computing Machinery. \nTo copy other\u00ad wise, or to republish, requires a fee and/or specific permission. 01990 ACM 089791-419-8190/001210197 \n$1.50 197 The thesis of this paper, however, is that the best @ .finctionhvacatkm @ =in&#38;pe.&#38;ntpm.ss \nmethodologies for writing synchronous programs are irreconcilably different from methodologies for writ\u00ading \nasynchronous programs. We propose a method\u00adology y, coordination structures, which leads to concise, \no Execution Graph Meswge Graph (fork-join) (Di.ing PhiiOsOphers) Figure 1: Execution vs. Message Graphs \n 2. A node in an execution graph corresponds to a state-less function; anode inamessage graph cor\u00adresponds \nto a process with local state. A simple example of a problem with only asyn\u00adchronous solutions is the \nDining Philosophers. A more interesting example is the Delirium runtime system [22]. Fast fourier transform \n[27], and the ray tracer discussed below, are good examples of problems with useful synchronous solutions. \n Having identified a program as synchronous or asyn\u00adchronous, the programmer must choose a notation for \nexpressing it. One purpose of the Delirium project is to investigate whether a single notation can support \nconcise and efficient solutions for both types of prob\u00adlems. A variety of languages, including SR [4], \nSloop [18], and Linda [13], can express asynchronous solutions to problems which also have good synchronous \nsolu\u00adtions. Using such languages, one can construct proto\u00adcols [2, 7] or distributed data structures \n[10] which re\u00adstrict the interactions between cooperating processes. These methodologies simplify the \nconstruction of cor\u00adrect asynchronous programs. However, the do not eliminate non-deterministic communication \npatterns, which are hard to debug, and hard to implement effi\u00adciently [21, 20, 17]. For problems with \ngood synchronous solutions, an alternative is to use a functional language. Such languages support deterministic \ncommunication pat\u00adterns but are unable to directly express asynchronous programs3. One might argue that \na language which can not express asynchronous programs is too restric\u00adtive. 2Distributed data structures \nare a class of protocols. 3 Given a stream of random numbers, a progmmm er could write a simulator that \nsupported a truly asynchronous model. However, we are only interested in concise, efficient solutions. \nefficient synchronous programs. We show how to im\u00adplement this methodology using a declarative, deter\u00administic \nfunctional language. By declarative, we mean that the connectivity of the program s communication pattern \nis clear from the program text. In contrast, languages based on distributed data structures elabo\u00adrate \ntheir communication patterns procedurally, and thus the pattern that emerges is dependent on the lan\u00adguage \ns underlying evaluation model. The remainder of this paper has the following orga\u00adnization. Sect ion \n2 describes coordination structures, showing their usefulness as a methodology for writing synchronous \nprograms. Section 3 introduces the idea of a coordination language and argues for the linguistic decoupling \nof coordination from computation. Section 4 discusses Delirium, a coordination language that sup\u00adports \nthe construction of concise, efficient synchronous programs. Section 5 presents the Delirium implemen\u00adtation \nof a medium-sized application. Section 6 dis\u00adcusses related work. 2 Coordination Structures We are proposing \na methodology for implementing synchronous parallel programs that is based on co\u00adordination structures. \nA coordination structure is a (structured) collection of coordination items. Coordi\u00adnation items can \nbe understood as individual ordering dependencies within a program. Imagine the dataflow graph for the \nfollowing expression: let x = f(<exprl>) in g(x) In a normal strict functional language, x is a name \nthat corresponds to the result of evaluating the appli\u00adcation off to <exprl>. A different way to understand \nx, however, is that it expresses an ordering dependency. To evaluate the application of g, one must first \nhave evaluated the application of f. Think of x as a pipe that connects an application of f to an application \nof g, through which data can flow. Each such pipe is a coordination item. A parallel computation can \nbe expressed as a multi\u00adstage pipeline of coordination structures. At each stage in the pipeline, a function \nis applied to the data flowing through each member in the collection of data pipes. Following a function \napplication, the order of items (data pipes) within the coordination structure may be namic programming \nsee appendix), algorithms based Coordination Item on communication over trees (including the Delirium \ncompiler [30] ), and algorithms based on convolutions Merge Operator _ over grids (such as Laplace s \nequation and successive over-relaxation [27]). A significant proportion of nu\u00ad 4444  il n-- Figure \n2: Coordination Structure for Mergesort permuted to create the appropriate data organization for the \nnext stage of the pipeline. For example, a useful primitive for many parallel al\u00adgorithms is binary reduction. \nA binary reduction takes N data items and applies some associative binary op\u00aderation to successive pairs, \nyielding a group of N/2 results, The same operation is performed repeatedly until there is only one value \nleft. Many algorithms are based on binary reduction, including, for exam\u00adple, merge sort, The pipeline \nfor merge sort is shown in figure 2 and consists of log(N) applications of the merge operator. If the \noriginal N values flow into the pipeline as a vector of pipes, the first step is to divide the pipes \ninto N/2 pairs. Next, the program applies an instance of the merge operator to each of these pairs in \nparallel. One pipe flows out of each merge operator, so the cross section of the pipeline has been reduced \nto N/2 coordination items. The grouping and function application are done repeatedly, until at the end \nonly a single pipe flows out of the pipeline and it contains the sorted list. It is important to note \nthat the coordination struc\u00ad ture of the application as a whole looks like a tree, even though the data \nthat moves through the pipes is probably organized into an entirely different data structure (like a \nlist). We believe that an algorithm is much clearer if the coordination structure is linguis\u00adtically \ndecoupled from the underlying data structure. A binary reduction always looks like a tree, regardless \nof whether the structure being operated on is a set, a list, a tree, or an array. Section 5 describes \nan application with a complex coordination structure and shows a Delirium realiza\u00adtion of this structure. \nMany classes of algorithms have good synchronous solutions which can be ex\u00adpressed as coordination structures. \nA few examples are: wavefront algorithms (including many types of dy\u00admerical scientific programs fall \ninto one of these cate\u00adgories [3]. Because coordination structures directly support data parallel operations, \nthey encourage a program\u00adming style that incorporates techniques found in SIMD programs. However, SIMD \narchitectures impose syn\u00adchronization requirements which narrow their applica\u00adtion domain. As illustrated \nby the application case study in [21], the class of data parallel problems with useful synchronous solutions \nis significantly larger than the class of problems with SIMD solutions [14], 3 A Case for Coordination \nLan\u00adguages Many parallel language proposals assume that people write parallel programs from scratch. \nPractical ex\u00adperience [22] and common sense both contradict this assumption. When parallelizing an application, \na programmer of\u00adten begins with a working version in a sequential lan\u00adguage. Typically, the programmer \ns environment in\u00adcludes useful debugging, profiling, and optimization (or vectorization) tools for this \nsequential language. In this situation, people don t rewrite their code, they restructure it. They decompose \nthe program into sub\u00adcomputations that could run in parallel, and then they write new code to coordinate \nthe sub-computations, Usually the coordination code relies on low level syn\u00adchronization primitives, \nsuch as locks or message pass\u00ading, because no higher level of abstraction is available. We believe that \ncoordination among sequential sub\u00adcomputations should be the basic function performed by a parallel programming \nenvironment. A coordha\u00adtion language performs only this function, giving the programmer the opportunity \nto express computation in the most convenient notation available. 4 Delirium Existing coordination languages, \nsuch as Linda and Sloop, are embeddecJ they consist of a set of asyn\u00adchronous coordination primitives \nwhich are accessed through statements scattered throughout a host lan\u00adguage program. Delirium is the \nfirst example of an embedding coor\u00addination language [22]. We call it an embedding lan\u00adguage because \na Delirium program specifies a frame\u00adwork for accomplishing a task in parallel; sequen\u00adtial sub-computations \ncalled operators are embedded within that framework. To guarantee that a Delirium program will execute \ndeterministically, one need only ensure that operators do not maintain state across in\u00advocations. We \nbelieve that embedding coordination languages such as Delirium offer significant advantages for the expression \nof parallelism. One can express all the glue necessary to coordinate a mid-sized application on a single \npage of Delirium. This organizing princi\u00adple makes parallelization easier. Instead of scattering coordination \nthroughout a program, creating a set of ill-defined sub-computations, a Delirium programmer precisely \ndefines sequential operators and embeds these operators within a coordination framework. Each of these \noperators is callable directly from Delirium with the same syntax as a function invoca\u00adtion. Operators \ncan be written in any language, in\u00adcluding traditional imperative languages like C or For\u00adtran. This \nallows the programmer to take advantage of existing tools, libraries, and coding strategies. The Delirium \nenvironment currently runs on the Se\u00adquent Symmetry, Cray-2, Cray Y-MP, and the BBN Butterfly TC2000. \nWe have developed an efficient run time system for executing Delirium which generally adds less than \nthree percent overhead to the running time of an application. On the Butterfly, the runtime system uses \nthe Tarmac distributed shared memory toolkit to manage caching of data passed between op\u00aderators [19]. \nThe environment includes an optimizing compiler (written in Delirium) [30], various tools for analyz\u00ading \nand improving execution speed, and a visualization tool for coordination structures. These tools support \nthe incremental parallelization of an existing program. The program can be decomposed into a set of oper\u00adators; \nwhen an operator is too expensive to execute sequentially, it is further decomposed into component pieces \nwhich can be computed simultaneously. 4.1 Basic Delirium At heart, Delirium is a straight-forward functional \nlanguage which supports first class functions, recur\u00adsion, iteration, let-bindings, and conditional expres\u00adsions. \nAll functions are evaluated strictly. There are no computation primitives in the language; all real work \nis accomplished within operators that are defined in a different language. Here is a sample Delirium \nprogram which solves the eight queens problem, expressing backtracking di\u00adrectly: maino let board = \nempt y.board ( ) in show_ solut ions (do_it (board, 1)) do.it (board, column) let hi = try(board, column, \n1) h2 = try(board, column,2) h3 = try(board, column, 3) h4 = try(board, column,4) h5 = try(board, column,5) \nh6 = try(board, column,6) h? = try(board, column,?) h8 = try(board, column,8) in merge (hi, h2, h3, h4, \nh5, h6, h7, h8) try(board, column, row) let new.board = add_queen(board, column, row) in if is-valid \n(new_board) then if is_equal(column,8) then new_board else do_it(new_board, incr(column)) else O The \ncode uses the operators is-equal, is_ valid, add-queen, show.-solut ions, and empty_ board. Because each \nof these does not involve much computation, theoverhead for expressing all the back\u00adtracking inparallel \nissignificant4. Asimple solution to reducing overhead is to express only two levels of the recursion \nin Delirium, calling a recursive C operator to descend the rest of the search tree. The modified version \nredefines try to be: try(board, column, row) let new_board = add-queen(board, column jrow) in if is-valid_ \nop(new_board) then if is_equal(column,2) then do_it_op(new_board, incr(column) ) else do_i.t(new_board, \nincr(column)) else O This version, run on eight Sequent processors, is seven times faster than a sequential \nC program,  4.2 Support for Coordination Struc\u00ad tures This section describes how one can build coordination \nstructures using Delirium transforms. Each applica\u00ad tionofa transform creates a structure which connects \ntwo successive stages inapipelined computation. Names in Delirium refer to either integers, functions, \ntransforms or n-dimensional arrays of untyped values. 40n twelve processors, this version took three \nseconds versus five seconds for the sequential version. 200 Atransformis a rule which replaces a set \nof input ar\u00ad rays by a result arra~ the result array represents some permutation (with possible copying) \nof the values in the input arrays. Transforms have two parts, a re\u00ad shape statement and a set of wiring \nrules. The wiring rules describe how the transform will permute its input arrays. The reshape statement \ndetermines the shape of the transform s result. It also requires that the shapes of the transform s input \narrays conform to an input pat tern. Application of a transform to an array that does not match the transform \ns input pattern results in a runtime error. The following grammar describes the syntax of transforms: \ntransform ::= heading reshape-stint [wiring-rules] (jiI1-constant] heading ::= identifier ( identifier-list \n) identifier-list ::= identifier L, y ideniijier-!isi I identifier reshape-stint ::= result-shape C<- \ninput-pattern input-pattern ::= subscript-expr result-shape ::= subscript -expr wiring-rules ::= with \nwiring-rule-list wiring-rule-list ::= wiring-rule wiring-rule-list ~ wiring-rule wiring-rule ::= lsubscript-expr \nC=p rsubscript-expr jill-constant ::= fill number lsubscript-expr ::= identifier [ simple-expr-list ] \nY simple -ezpr-list ::= simp!e-expr C, 7 simple -expr-list ~ simple-expr simple -expr ::= number ~ identifier \nrsubscript-ezpr ::= subscript-ezpr ~ number subscript-ezpr :z= identifier 1? index-expr-list 1 indez-expr-list \n::= indez-expr C, p index-ezpr-list ~ indez-expr index-expr ::= expr ezpr ::= any Delirium expression \n(should yield an integer at runtime) The operator = in a wiring rule is read depends on . Each wiring \nrule specifies how a section of the transform s result depends on its input5. For example, the transform \nshown below groups into pairs adjacent elements of a one-dimensional input array: adj scent (P) C [n, \n2] <-P [n] with C[i, j]=P[i+j] 5For wiring rules in which the left and right subscript expres\u00adsions \nrefer to different arrays, = is semantically equivalent to an assignment. At present, this is the only \nkind of rule permitted in a transform. The reshape statement, C [n, 2] <-P [n], contains the input pattern \nP [n]. This pattern matches any one\u00addimensional array, binding the name P to that array. The reshape \nstatement declares that the result of the transform will be a two-dimensional array, C. The single wiring \nrule of this transform maps suc\u00adcessive (overlapping) pairs of adjacent elements from P into the corresponding \ncolumns of C. In general, the left-hand side (lhs) of the wiring rule specifies some elements of the \ntransform s result. Array subscripts appearing on the lhs must be either index variables or constants. \nAn index variable is an identifier which represents the entire range (with zero origin) of index values \nalong a particular dimension of the result array. Index variables must be unique within a wiring rule \nand consistent within a transform s set of wiring rules. To be consistent, a given index variable must \nalways refer to the same dimension of the result. The right-hand side (rhs) of a wiring rule must be \neither a constant or a selection of some elements from one of the transform s input arrays. Array subscripts \nappearing on the rhs can contain arbitrary arithmetic expressions. Index variables bound on the lhs of \na wiring rule can appear in its rhs. If an rhs subscript expression does not specify a valid index of \nthe input array, the value of the entire rhs expression is deter\u00admined by the fill constant of the transform. \nWhen defining transforms containing such rhs expressions, the programmer must explicitly specify a fill \nconstant. The semantics of a set of wiring rules are captured by the following algorithm: for each wiring \nrule for each value in the range of each index variable compute the lhs expression if the indicated element \nof the result haa already been specified then report error else assign the value of the rhs expression \nto this element To understand how transforms work, imagine the dataflow graph of a computation to be \na set of ribbon cables. These cables have a male and a female plug, both n-dimensional, and wires which \nconnect each in\u00adput of the female plug to one or more outputs of the male plug. The male plugs correspond \nto Delirium ar\u00adrays. Female plugs correspond to the input patterns of transforms; they determine what \nshapes of input ar\u00adrays can plug into the transform. The wiring rules of the transform constitute a schematic \nfor creating a new male plug. A computation is just a series of transformations, applied to the program \ns input wires . For complete\u00adness, we should generalize the ribbon cable analogy so that, at any point, \none can bifurcate a cable such that it has two male ends (since a program may copy an array). The only \nthing that remains is to specify how actual work gets done. To do this, we introduce the primitive map, \nwhich applies a function to groups of elements in Delirium arrays. Depending on how it is applied, map \ngroups array elements in different ways; this flexibility is necessary to support functions with multiple \narray arguments and multiple return values. The simplest way to use map is to apply a single argument, \nsingle return value function to one array. The result is an array of the same shape as the input array, \nwhere each element in the new array is computed by applying the function to the corresponding element \nof the original one. If map applies a function that returns r results, the dimensionality of the output \narray is correspondingly increased. An n x m array, for example, would be transformed into an n x m x \nr array. Element (8, 10,2) is the third return value of the function when it is applied to element (8, \n10) of the input array. There are two ways to use map with multiple argu\u00adment functions. The first is \nto apply an n argument function to n arguments. All the arguments must con\u00adform, meaning that they must \neither be arrays with the same shape, or scalars, which are automatically replicated (and coerced into \nthe array type). For ex\u00adample, if a four argument single return value function is applied to two m x \nq arrays and two scalars, the result would be an m x q array where each element is computed by applying \nthe function to the two scalars and the two corresponding array elements. The other way to handle multiple \ninputs is to get them all from a single dependency array. In this case, the programmer applies an n-element \nfunction to an array whose lowest-numbered dimension is n. If the function has r return values, the map \noperation will change the size of the array s lowest-numbered dimen\u00ad sion from n to r. For example, if \na three argument, two return value function is applied to a 20 x 3 depen\u00ad dency array, the result array \nwill be of shape 20 x 2. The two elements in row ten of the result array are the two values returned \nby the function when it is applied to the three elements in row ten of the original array. To summarize, \nmap can apply a function to either one or n dependency arrays. In the first case, the input arity of \nthe function must match the lowest-numbered dimension of the input array. In the second case, the input \narit y must be n. The lowest-numbered dimen\u00adsion of the output array will be the output arity of the \nfunction. Note that the reshaping effect of map can be de\u00ad scribed as a transformation. The full semantics \nof map cannot be expressed in a transform, however, be\u00ad cause transforms can only build coordination \nstruc\u00adtures. They are not able to apply functions to data flowing through a coordination structure. With \nthis re\u00adstriction, Delirium linguistically enforces a decoupling between coordination and computation. \nNormal trans\u00adforms create patterns of dataflow. The map transform applies a function across a pattern. \nTo demonstrate how map is used in practice, here is Delirium code that applies a function of two arguments \nto each pair of adjacent elements in a vector. The first step is to transform the vector into a two-dimensional \narray where each two-element column contains adja\u00adcent elements from the vector. Once the intermediate \narray is constructed, a simple map operation performs the computation: adj scent (group, P) C [n, groupl \n<-P [n] with C[i, j]= P[i+j] f(a, b) op(a, b) f -on-adj scent (vector) map(f, adjacent (2, vector) ) \nThis example generalizes the definition of adjacent to group by an arbitrary number (rather than just \nby pairs). In the example, vector is a one-dimensional ar\u00adray, f and f -on-adj scent are functions, and \nadj scent is a transform. The identifier op refers to a functional operator defined in a computational \nlanguage (see the above description of basic Delirium). Note that adjacent makes use of an integer argument \ngroup. If a program passes an array in this argument position, it will generate a runtime type error. \n 4.3 Current Research 4.3.1 Expression of Coordination Structures While the current transform mechanism \nis sufficient for the specification of any coordination structure, it is oriented toward building these \nstructures one pipeline stage at a time. Coordination structures built this way are easy to understand \nand manipulate because they can be represented as relationships among n\u00ad dimensional arrays. To represent \na general coordina\u00ad tion structure, we need a more flexible data structure, such as a directed graph. \nSome coordination struc\u00ad tures, such as wavefront computations, can be em\u00ad bedded in arrays. For such \nstructures, it is useful in practice to support both views. Transforms expecting arrays should be able \nto take array-embedded coor\u00ad dination structures as arguments. In other contexts, programmers need to \nbe able to express a direct mod\u00adification to the array-embedded graph, We have developed multi-stage \ntransforms which provide this flexibility. Multi-stage transforms also in\u00adclude a notion of recurrence, \nRecurrence relations are resolved into coordination structures that can be ma\u00adnipulated explicitly or \nimplicitly (as array-embedded graphs) by transforms. A future report will describe both the semantics \nand compilation of multi-stage transforms. Most current Delirium applications use a different strategy \nfor expressing recurrences. Prior to the devel\u00adopment of multi-stage transforms, we provided a recur\u00adrence \nnotation similar to Crystal [11] which program\u00admers could use as an alternative to writing recurrences \nas iterations over transforms. Crystal is a system that converts computations expressed as recurrence \nequa\u00adtions into a systolic architecture, At an intermediate step in this conversion, the Crystal compiler \nderives a set of linear combinations that express the commu\u00adnication inherent in the given recurrence, \nThere is a straightforward mapping from such sets to Delirium transforms.  4.3.2 Discovery of Coordination \nStructures Delirium is a notation for the expression of coordina\u00adtion structures. However, it does not \ndirectly support the discovery of coordination structures inherent in ex\u00adisting code. We are developing \na workbench which supports this activity through analysis of sequential G and Fortran programs. The analysis \ntool can also as\u00adsist in decomposing a program into sets of Delirium operators.  5 A Delirium Example \nThis section presents a case study of an application which can be parallelized using a synchronous commu\u00ad \nnication pattern. The study demonstrates that Delir\u00ad ium programs can be both concise and efficient. \nThe UNIGRAFIX ray tracer is a 10,000 line C pro\u00adgram developed at UC Berkeley [23]. A parallel version \nof this ray tracer has been implemented in C on the Sequent Symmetry [9], using the notion of replicated \nworker processes [32]. The pattern of data dependen\u00adcies in this program is complex for two reasons, \nFirst, the program breaks scan lines into subsections with a fixed number of pixels. Second, the program \nuses an antialiasing strategy involving averaging across multi\u00adple scan lines. These two properties combine \nto create a computation with a three-dimensional data depen\u00addency graph. The C language solution to this \nproblem is based on distributed data structures. Specifically, the C pro- Init Grid Figure 3: First Stage \nof Ray Tracing Computation gram creates a set of queues, one for each averaging operation. Each worker \nprocess polls one of the work queues, waiting for a complete set of data to accumu\u00adlate on the front \nof the queue. This solution requires 500 lines of C (with references to locking primitives). It completely \nobfuscates the underlying communica\u00adtion pattern of the algorithm. Even in a language like Linda, one \nhas no choice but to express the commu\u00adnication pattern of this algorithm indirectly through a distributed \ndata structure.6 The coordination structure of the ray tracing task as a whole is a five stage pipeline, \nwhere the stages are init, grid, super-sample, filter, and output. The Delirium strategy for constructing \nthis pipeline is to create a transform to represent each stage. The first stage is init, which generates \na database describing the image being rendered. A reference to the database is handed to each of n x \nm instances of the grid function. The first rerouting stage of the pipeline takes the single pipe emerging \nfrom the init function and branches it into an n x m array of pipes all containing the database reference \n(see figure 3). The super-sampling stage computes a value for each two by two block of grid values. Figure \n4 is a two\u00addimensional representation of its three-dimensional co\u00adordination structure, using cubes to \nrepresent the grid functions and pyramids for super-sample. The filter stage computes a value for each \nthree by three block of super-sample values. All the resulting values are passed to the output stage \nwhere they are pro\u00adcessed linearly. Here is a Delirium realization of the ray tracer s co\u00adordination \nstructure: 6 The Linda ~oordin~tion code required 165 ~nes. Grid ( Super-Sample Grid Super-Sampl Grid \nFigure 4: Second Stage of Ray Tracing Computation replicate (n, m,value) C [n ,111]<-value with C [n \n,ml =value maino let database=ini.t ( scene-database-file ) grid-points=  map(grid,replicate(lJSCAliLIIIES, \nlJPIXELS,database) ) samples= map(super-sample,adjacent(2,gri.d-points) ) image=map(fi.lter ,adjacent(3, \nsamples)) in output(image) This example uses the transform adjacent de\u00adscribed above. It also introduces \na new transform, replicate, whose purpose is to change a scalar value into a two-dimensional array. The \nfunction init re\u00adturns a reference to a complex scene database; how\u00adever, replicate treats the reference \nto this datastruc\u00adture ss a scalar, since interpretation of the database takes place only within the \ncomputational code. The example takes advantage of aspecial property of one-dimensional transforms. Ifa \none-dimensional transform is applied to a multi-dimensional array, the transform is generalized to apply \nalong all dimensions of the array. To compute the result array in this case, the transform is applied \nto successive dimensions of its k-dimensional input array, yieldingk result arrays. The result arrays \nare then concatenated along their lowest-numbered dimension, yielding the single array that is the final \nresult of the transform. 350 . .. ..... .. . ... ...... . .. .. .. ... . ...... ... . ... ..... . . t \n\\ 300 50 o 2 4 6 8101214 Number of Processors Figure5: Performance of Ray TkwerPrograms Inthe absenceof \nthis property, one could define the two-dimensional adjacent transform as follows: flatten(P) C[n*ml<-P[n,ml \nwith C[i]=P[i/n,i mod n] adjacent4D(group ,P) C[n,m,groip,group] <-PCn,m] with C[i, j,k,l]=P[i+k, j+l] \nadjacent (group, P) f latten(adjacent4D (group ,P) ) Figure 5 compares the performance of this Delirium \nprogram with programs written in C and Linda for the same application. The Delirium program for the ray \ntracer runs 14% faster than the version in which the coordination is expressed in C and 22% faster than \nan optimized Linda version. One strategy that allows the Delirium run time system to implement transforms \nef\u00adficiently is called summarization. Due to their regular structure, transforms can often be subdivided \ninto sec\u00adtions of optimal grain size. Figure 6 shows the effect of different subdivisions on the ray \ntracer s execution time. o51O152O253O 35 Pieces Per Proceesor Figure 6: Effect of Partition Size  6 \nRelated Work 6.1 Functional Languages One can achieve the organizing effects of coordination structures \nusing higher-order functions. For example, one could write a function that groups pairs of adjacent elements \nin a vector: AP. Af. construct array A with range i = O to /ength(P) 1 such that A[i] = f(P[i], P[i \n+ 1]) To realize the same degree of parallelism as the equivalent Delirium transform, this higher-order \nfunc\u00adtion must be implemented in a functional language which has lazy aggregate construction and strict \nfunc\u00adtion applications. By lazy aggregate construction, we mean that an array can be used before all \nits elements are computed. Combining this property with strict function application, a functional language \ncould r~ alize the multi-dimensional, pipelined communication pattern created by Delirium transforms. \n6.1.1 Strict Functional Languages Most dataflow languages, including VAL [24] and SISAL [25], are evaluated \nstrictly. These languages have strict aggregate construction, and so they can not implement Delirium \ntransforms. The language Par-ALFL [15], though not a dataflow language, also has strict arrays; however, \none could implement coordina\u00adtion structures inefficiently in ParALFL using its lazy lists. 6.1.2 Lazy \nFunctional Languages Some dataflow languages, such as Id [5, 26], have lazy aggregate construction as \nwell as lazy function applica\u00adtion. To realize the parallelism of Delirium transforms, a coordination \nstructure built in Id would require strict evaluation of function applications within lazy arrays. This \nunintuitive evaluation strategy could be arranged by a compiler that was designed to recognize coordina\u00adtion \nstructures as a stylized idiom. In contrast, Delir\u00adium adds the transform mechanism to an otherwise strict \nlanguage; using this mechanism, programmers directly express the desired evaluation behavior. 6.1.3 \nArray Comprehensions Anderson and Hudak discuss the construction of lazy arrays within a strict context \nusing a functional syn\u00adtax called array comprehensions [3]. One can use such array comprehensions to \nimplement coordination structures. However, array comprehensions can con\u00adveniently express only those \ncoordination structures which can be embedded in arrays. Like multi-stage transforms, array comprehensions \ncan express recurrences. We will provide a detailed comparison between array comprehensions and multi\u00adstage \ntransforms in a future report. We have found that the complexity of compiling the Delirium recur\u00adrence \nnotation into iterations over transforms is similar to the complexity of compiling Haskell [16] array \ncom\u00adprehensions for sequential machines.  6.2 Asynchronous Coordination As was mentioned above, protocols \n[2, 7] and dis\u00adtributed data structures [10] are useful in asynchronous languages because they place \nrestrictions on the gen\u00aderality of communication patterns. Another approach suggested by Guy Steele [31] \nlimits the types of opera\u00adtions that cooperating processes can perform on shared data. For synchronous \nprograms, Delirium s coordina\u00adtion structures are a better methodology than these asynchronous programming \ntechniques. Coordination structures have two main advantages: they are deter\u00administic and they express \nthe program s communica\u00adtion pattern declaratively, making it easier to deduce from examination of the \nprogram text. We have com\u00adpleted a study [21] which demonstrates that for syn\u00adchronous applications, \nprograms written in terms of co\u00adordination structures are often more concise and more efficient than \nprograms written in terms of distributed data structures. 6.3 Aggregate primitives C [n/group, groupI \n<-P [n] 6.3.1 Early Aggregate Languages Delirium transformations have been heavily influenced by APL \n[12], which introduced the idea of a pipeline of functional transformations that modify an aggre\u00adgate \nstructure. This idea was significantly elaborated by FP [6], which provides a rich set of functional \noper\u00adators for creating new transformations. APL does not have first class functions, and so can t express \ncoordi\u00adnation structures. FP s functional operators are simi\u00ad lar to Delirium transforms, but operate \non a data ob\u00adject that must model memory as well as coordination, and are thus difficult to implement \nefficiently. Wa\u00ad t er s series expressions [33] also create pipelined com\u00adputations; however, like FP \nand APL, they provide a fixed set of operators for modifying dataflow through the pipeline. In contrast, \nDelirium transforms are a general mechanism for creating dataflow modification operators. The appendix \nlists some of the FP, series, and APL functional operators, implemented as Delir\u00ad ium transforms. 6.3.2 \nSIMD Languages For many applications, data parallel operations are the most likely source of massive \nparallelism. Concise and efficient data parallel programs have been written for SIMD multiprocessors. \nThe class of data parallel prob\u00ad lems with useful synchronous solutions is significantly larger than \nthe class of problems with SIMD solutions [14]. This is because SIMD architectures impose syn\u00ad chronization \nrequirements which limit their apphca\u00ad tion domain. Languages intended for SIh4D program\u00ad ming, such \nas C* [29], incorporate these architectural limitations into their semantics. C* programs compiled for \nMIMD machines expe\u00ad rience performance degradation for two reasons [28]. First, the compiler must insert \nbarriers to synchronize after each conditional branch within a data parallel operation (and to synchronize \nafter the whole opera\u00ad tion). Second, if data parallel operations on an array refer to other values within \nthat array, the compiler must pre-copy the array to ensure data consistency. C* includes as primitives \nuseful data parallel op\u00ad erations such as scan [8]. In Delirium, one can use transforms to specify the \ndataflow pattern for this and many other such operations. For example, the Delir\u00ad ium transform for general \nreduction is: # combine sets up for function # call on grouped elements combine (group, P) with C [i, \nj] =P [i*group+j] # this function performs a reduction # the repeated assignment does not # imply serialization \nof the loops reduce(f ,group, P) while (shapeof (P) [01 >1) P=map(f, combine (group, P) )   7 Conclusion \nWe have presented a framework which classifies the communication patterns of parallel programs into two \ntypes: synchronous and asynchronous. Asynchronous programs have inherently non-deterministic behavior. \nThe challenge with these programs is to discipline the communication among a set of independent processes \nso that this non-determinism is manageable. With synchronous programs, communication is determinis\u00adtic, \nbut often complex and multi-dimensional. The two kinds of parallel programs map naturally onto differ\u00adent \nimplementation methodologies. Distributed data structures are well-suited to the expression of asyn\u00adchronous \nprograms. Synchronous programs can be concisely and efficiently expressed in terms of coordi\u00ad nation \nstructures. We have proposed a language mech\u00ad anism which supports the creation and manipulation of such \nstructures. We have used this mechanism to con\u00ad struct concise and efficient implementations for several \napplications.  8 Acknowledgments We would like to thank Guy Steele for his extensive comments on earlier \ndrafts of this paper, Marti Hearst for her help in improving the presentation, and David Culler for his \ninsights into functional languages. References [1] W. Ackerman. Data Flow Languages . Com\u00adputer, 15(2), \nFebruary lg82\u00ad [2] D. Anderson. Automated Protocol Implementa\u00adtion with RTAG . IEEE Transactions on Soft\u00adware \nEngineering, 14(3):291 300, March 1988. [3] S. Anderson and P. Hudak. Compilation of Haskell Array Comprehensions \nfor Scientific Com\u00adputing . In Proceedings of the ACM SIGPLAN [4] [5] [6] [7] [8] [9] [10] [11] [12] \n[13] [14] [15] 90 Conference on Programming Language De\u00adsign and Implementation, pages 137 149, White \nPlains, NY, June 1990. ~~The Distributed programming Lanaguage SR Mechanisms, Design and Imple\u00admentation \n. Sofiware Practice and Experience, 12(8):719-753, 1982. G. Andrews. Arvind and K. P. Gostelow. An Asynchronous \nProgramming Language and Computing Ma\u00adchine . Technical Report TR1 14a, Dept. of In\u00adformation and Computer \nScience, University of California, Irvine, December 1978. J. Backus. Can Programming Be Liberated From \nthe Von Neumann Style? A Functional Style and Its Algebra of Programs . Communications of the ACM, 21(8), \nAugust 1978. A. Birrell and B. Nelson. Implementing Remote Procedure Calls . Transactions on Computer \nSys\u00adtems, 2(1):39-59, February 1984. G. Blelloch. Scans as Primitive Parallel Oper\u00adations . IEEE Transactions \non Computers, C\u00ad38(11):1526-1538, November 1989. B. Boothe. Multiprocessor Strategies for Ray Tracing \n. Master s thesis, University of Califor\u00adnia/Berkeley, 1989. Tech Report 534. N. Carriero, D. Gelernter, \nand J. Leichter. Dis\u00adtributed Data Structures in Linda . In Proceed\u00adings of the ACM Symposium on Principles \nof Pro\u00adgramming Languages, January 1986. M. Chen. A Parallel Language and Its Compi\u00adlation to Multiprocessor \nMachines or VLSI . In Proceedings of -lhe Thirteenth ACM Symposium on the Principles of Programming Languages, \npages 131-139, 1986. A. Falkoff and K. Iverson. The Design of APL . In E. Horowitz, editor, Programming \nLanguages: A Grand Tour, pages 240 250. Computer Science Press, Inc., 1987. D. Gelernter. Parallel Programming \nin Linda . In Proceedings of the International Conference on Parallel Processing, pages 255-263, August \n1985. D. Hillis and G. L. Steele Jr. Data Paral\u00adlel Algorithms . Communications of the ACM, 29(12):1170-1183, \n1986. P. Hudak and L. Smith. Para-functional Pro\u00adgramming; A Pcwadi5m for Programming Multi\u00adprocessor \nSystems . In Proceedings of the Thir\u00ad teenth ACM Symposium on -the Principles of Pro\u00adgramming Languages, \npages 243 254, 1986. [16] P. Hudak and P. Wadler. Report on the Pro\u00adgramming Language Haskell . Technical \nRe\u00adport YALU/DCS/RR666, Yale University, De\u00adpartment of Computer Science, November 1988. [17] S, Lucco, \nA Heuristic Linda Kernel for Hy\u00adpercube Multiprocessors . In Proceedings of the Second Conference on \nHypercube Multiprocessors, September 1986. [18] S. Lucco. Parallel Programming in a Virtual Ob\u00adject Space \n. In Conference on Object-Oriented Programming Systems, Languages, and Applica\u00adtions, October 1987. [19] \nS. Lucco and D. Anderson. Tarmac: A Language System Substrate Based on Mobile Memory . In International \nConference on Distributed Comput\u00ading Systems, 1990. [20] S, Lucco and K. Nichols. A Performance Analy\u00adsis \nof Three Parallel Programming Methodologies in the Context of MOS Timing Simulation . In Digest of Papers: \nIEEE Compcon, pages 205-210, 1987. [21] S. Lucco and O. Sharp. A Comparison of Two Strategies for Data \nParallel Programming on MIMD Architectures (submitted for publication) . [22] S. Lucco and O. Sharp. \nDelirium: An Embed\u00adding Coordination Language . In Proceedings of Supercomputing 90, November 1990. [23] \nD. Marsh. UgRay-An Efficient Ray-Tracing Renderer for UniGrafix . Master s thesis, Uni\u00adversity of California/Berkeley, \nMay 1987. [24] J. R. McGraw. The VAL Language: Descrip\u00adtion and Analysis . ACM Transactions on on Programming \nLanguages and Systems, 4(1):44-82, January 1982. [25] J. R. McGraw. SISAL: Streams and Iteration in a \nSingle Assignment Language . Technical report, Lawrence-Livermore National Laboratory, March 1985. [26] \nR. S. NikhiL ID Reference Manual, version 88.0 . Technical Report 284, MIT Laboratory for Com\u00adputer Science, \n1988. [27] W. H. Press, B, P. Flannery, S. A. Teukolsky, and W. T. Vetterling. Numerical Recipes in C: \nThe Art of Scientific Computation. Cambridge Uni\u00advcmit y Press, 19?38. [29] J. Rose and G. L. Steele \nJr. C*: An Extended C Language for Data Parallel Programming . In Proceedings of the Second International \nConfer\u00adence on Supercomputing, May 1987. [30] 0. Sharp. Pythiti An Parallel Compiler for Delirium . Master \ns thesis, University of Califor\u00adnia/Berkeley, 1990. [31] G. L. Steele Jr. Making Asynchronous Paral\u00adlelism \nSafe for the World . In Proceedings of the Seventeenth ACM Symposium on the Principles of Programming \nLanguages, pages 218 231, January 1990. [32] M. Sullivan and D. Anderson. Marionette: a Sys\u00adtem for Parallel \nDistributed Programming using a Master/Slave Model . In International Confer\u00adence on Distributed Computing \nSystems, 1989. [33] R. Waters. Appendix A: Series . In G. L. Steele Jr,, editor, Common LISP: The Language, \n.2nd edition. Digital Press, 1990.  A Appendix Some APL, series, and FP functional operators imple\u00admented \nas Delirium transforms: # concatenates two arrays # along lowest-numbered dimension catenate (P, Q) \nC [m+nl <-P [m] , Q [n] # removes first S1 x s2 elements from P drop(P, sl, s2) C[n-si ,m-s2]<-P [n, \nm] with C[i, j]= P[i+sl, j+s21 # APL outer product (function # supplied separately by map) outer-product \n(P,Q) P[n],Q[m]->C[n,m,2] with C[i,j,O]=P[i] [28] M. Quinn, P. Hatcher, and K. Jourdenais. Com\u00adpiling \nC* Programs for a Hypercube Multicom\u00adputer . In Proceedings of the A CM/SIGPLAN Parallel Programming \nExperience Symposium, July 1988.  C[i, j,l]=Q[j] # reshape for inner product set-up-ip(P, Q) P[m, n] \n,Q[n, q]-> C[m, with C[i, j,k, O]= P[i, k] c[i, j,k, i]= Q[k, j] q,n,2] # # perform definition inner \nof product reduce (see in text) inner-product reduce (plus-f (plus-fn, times-fn, n,2, map(times-fn, a \n,b) set-up-ip(a, b))) Recipe for dynamic programming: # first N stages of 2D dynanic programming dptop(P) \nP [n] ->C [n, 2] with C[i, jl=P[i-j+ll # second N stages dpbottom(P) adjacent(2,P)  \n\t\t\t", "proc_id": "99583", "abstract": "", "authors": [{"name": "Steven Lucco", "author_profile_id": "81100431977", "affiliation": "Computer Science Division, 571 Evans Hall, UC Berkeley, Berkeley CA", "person_id": "PP42051841", "email_address": "", "orcid_id": ""}, {"name": "Oliver Sharp", "author_profile_id": "81100266702", "affiliation": "Computer Science Division, 571 Evans Hall, UC Berkeley, Berkeley CA", "person_id": "PP31096097", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/99583.99612", "year": "1991", "article_id": "99612", "conference": "POPL", "title": "Parallel programming with coordination structures", "url": "http://dl.acm.org/citation.cfm?id=99612"}