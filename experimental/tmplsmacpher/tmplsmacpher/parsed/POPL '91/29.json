{"article_publication_date": "01-03-1991", "fulltext": "\n Semantic foundations of concurrent constraint programming Vijay A. Saraswat, Xerox PARC Martin Rinard, \nStanford University Prakash Panangaclen, McGill University (Preliminary Report) Abstract Concurrent constraint \nprogramming [Sar89 ,SR90] is a sim\u00adple and powerful model of concurrent computation based on the notions \nof store-as-constraint and process as information transducer. The store-as-valuation conception of von \nNeu\u00admann computing is replaced by the notion that the store is a constraint (a finite representation \nof a possibly infinite set of valuations) which provides partial information about the possible values \nthat variables can take. Instead of reading and writing the values of variables, processes may now ask \n(check if a constraint is entailed by the store) and tell (augment the store with a new constraint). \nThis is a very general paradigm which subsumes (among others) nonde\u00adterminate data-flow and the (concurrent) \n(constraint) logic programming languages. This paper develops the basic ideas involved in giving a coherent \nsemantic account of these languages. Our first con\u00adtribution is to give a simple and general formulation \nof the notion that a constraint system is a system of partial infor\u00admation (a la the information systems \nof Scott). Parameter passing and hiding is handled by borrowing ideas from the cylindric algebras of \nHenkin, Monk and Tarski to introduce diagonal elements and cylindrification operations (which mimic the \nprojection of information induced by existential quantifiers). The se;ond contribution is to introduce \nthe notion of determinate concurrent constraint programming languages. The combinators treated are ask, \ntell, parallel composition, hiding and recursion. We present a simple model for this language based on \nthe specification-oriented methodology of [OH86]. The crucial insight is to focus on observing the resting \npoints of a process those stores in which the pro\u00adcess quiesces without producing more information. It \nturns out that for the determinate language, the set of resting points of a process completely characterizes \nits behavior on all inputs, since each process can be identified with a closure operator over the underlying \nconstraint system. Very nat\u00adural definitions of parallel composition, communication and hiding are given. \nFor example, the parallel composition of two agents can be characterized by just the intersection of \nthe sets of constraints associated with them. We also give a complete axiomatization of equality in this \nmodel, present Permission to copy without fee all or part of this matertial is granted provided that \nthe copies are not made or distributed for direct commercial advantage, the ACM copyright notice and \nthe title of the publication and its date appear, and notice is given that the copying is by permission \nof the Association for Computing Machinery. To copy other\u00ad wise, or to republish, requires a fee aud/or \nspecific permission. @ 1990 ACM 089791-419-8/90/0012/0333 $1.50 a simple operational semantics (which \ndispenses with the explicit notions of renaming that plague logic programming semantics), and show that \nthe model is fully abstract with respect to this semantics. The third contribution of this paper is to \nextend these modelling ideas to the nondeterminate language (that is, the language including bounded, \ndependent choice). In this context it is no longer sufficient to record only the set of resting points \nof a process we must also record the path taken by the process (that is, the sequence of ask/tell inter\u00adactions \nwit h the environment) to reach each resting point. Because of the nature of constraint-based communication, \nit turns out to be very convenient to model such paths as certain kinds of closure operators, namely, \nbounded trace operators. We extend the operational semantics to the non\u00addeterminate case and show that \nthe operational semantics is fully consistent with the model, in that two programs denote the same object \nin the model iff there is no context which distinguishes them operationally. This is the first simple \nmodel for the cc languages (and ipso facto, concurrent logic programming languages) which handles recursion, \nis compositional with respect to all the combinators in the language, can be used for proving liveness \nproperties of programs, and is fully abstract with respect to the obvious notion of observation. 1 Introduction \nThe aim of our enterprise is simple to develop the semantic foundations of a new paradigm for concurrent \ncomputing [Sar89,SR90]. The basic paradigm. The crucial concept underlying this paradigm is to replace \nthe notion of store-as-valuation be\u00adhind imperative programming languages with the uotion of store-as-constraint. \nBy a constraint we mean a (possibly in\u00adfinite) subset of the space of all possible valuations in the \nvariables of interest. For the store to be a constraint rather than a valuation means that at any stage \nof the computation one may have only partial information about the possible values that the variables \ncan take. We take as fundamental the possibility that the state of the computation may only be able to \nprovide partial information about the variables of interest. This paradigm shift renders the usual notions \nof of (im\u00adperative\\ Lwrite>> and read incoherent. For example, there ., may be no single, finitely-describable \nvalue left to return as the result of a read operation on a variable. Similarly, an assign operation \nis only capable of prescribing a fully formed, concrete value for a variable and the new value may have \nnothing to do with the previous value. This runs into difficulties with the notion that the store specifies \nsome con\u00adstraints that must always be obeyed by the given variables. Instead, [Sar89] proposes the replacement \nof read with the notion of ask and write with the notion of tell. An ask operation takes a constraint \n(say, c) and uses it to probe the structure of the store. It succeeds if the store con\u00adt ains enouzh \ninformation to entail c. Tell takes a constraint and conjoins it to the constraints already in place \nin the store. That is, the set of valuations describing the resultant store is the intersection of the \nset of valuations describing the original store and those describing the additional con\u00adstraint. Thus, \nas computation progresses, more and more information is accumulated in the store a basic ste~. does not \nchange the value of a variable but rules out certain values that were possible before; the store is monotonically \nrefined. The idea of monotonic update is centraJ to the theo\u00adretical treatment of I-structures in Id \nNouveau [JPP89]. I\u00adstructures were introduced in order to have some of the ben\u00adefits of in-place update \nwithout introducing the problems of interference. It is interesting that the concurrent constraint paradigm \ncan be seen as arising as a purification of logic pro\u00adgramming [Sar89], an enhancement to functional \nprogram\u00adming and as a generalization of imperative programming. From the viewpoint of dataflow programming, \nthe concur\u00adrent constraint paradigm is aJso a generalization in that the flow of information between \ntwo processes is bidirectional. It might relate to the goal of a more symmetrical theory of computation \nadvocated by Girard [Gir87)Gir89]. Central to our notion of constraint system isa theory of Dartial information \nand entailment between Dartial informa\u00ad . tlon. Such a theory exists in the form of Scott s treatment \nof information systems [SC082]. In our case it is natural to imagine two concurrent processes imposing \ninconsistent constraints on the store. Thus, we need to represent the possibility y of inconsistent information. \nThe app~oach of this paper makes the possibilities for concurrency quite apparent. Instead of a single \nagent inter\u00adacting with the store via ask and tell operations, any num\u00adber of agents can simultaneously \ninteract with the store in such a fashion. Synchronization is achieved via a blocking ask an agent blocks \nif the store is not strong enough to entail the constraint it wishes to check; it remains blocked until \nsuch time as (if ever) some other concurrently execut\u00ading agents add enough information to the store \nfor it to be strong enough to entail the query. Note, in particular, that even though the paradigm is \nbased on the notion of a shared store, ideas such as read/write locks do not have to be in\u00adtroduced for \nsynchronization. The basic reason is that only a benign form of change -accumulation of information is \nallowed in the system.1 If desired, indeterminacy can be introduced by allowing an agent to block on \nmultiple dis\u00adtinct constraints simultaneously, and specify distinct actions which must be invoked if \nthe corresponding ask condition is ] Which of course, is not to say that systems with changable state \nare not describable in the cc paradigm State change can be repre. sented without compromising the basic \nparadigm by adapting stan. dard techniques from logic and functional programming, Namely, assignable \nvariables>> are embedded in the local state of a recursive a.gent=the agent changes the value of the \nvariable by merely recur\u00adring with a new value for one of its arguments. In some languages in the cc \nframework (e.g., Janus[SKL90]) there is considerable hope that such mechanisms for representing state \nchange can be competitive in performance with traditional assignment-based techniques. satisfied by the \nstore. Thus. in this view, an agent ( computing station ) is thought of as an information transducer. \nAn agent can ei\u00adther add a constraint to the store (tell), suspend until there is enough information \nin the store to entail a given con\u00adstraint, or decompose into a number of other agents run\u00adning in parallel, \npossibly with hidden interconnections, and communicating and synchronizing via the store. Of course, \nas is standard, computations of unbounded length can be achieved by using recursion. Computational significance \nof the paradigm. While these ideas are extremely simple, they conceal a powerful pro\u00adgramming paradigm \nwhich derives its strength from the ver\u00adsatility of its communication mechanism. It is not possible in \na short introduction to describe the many interesting com\u00admunication schemes possible here. We shall \njust attempt to indicate the basic ideas and refer the reader to more detailed treatments such as [Sar89]. \nThe essentiaJ idea is that variables serve as communica\u00adtion channels bet ween multiple concurrently \nexecuting agents. The computational framework does not insist that there be an a priori partitioning \nof the agents into <producers and consumers of information on the variables (as happens, for example, \nin function application, where information flows from an argument to the function body, or in CSP/CCS\u00adstyle \nor actor-style languages). In fact, the same agent may simultaneously add or check constraints on the \nsame vari\u00adables. Also, the computational framework allows for the under\u00adlying network transport protocol \nto perform (potentially arbitrarily sophisticated ) inferences on the messages en\u00adtrusted to them these, \nof course, are the deductions sanc\u00adtioned by the entailment relation of the constraint system. This aJlows \neach agent to state the constraints as it sees them and frees up the programmer from having to put in \nagents for explicitly collecting these messages and drawing the appropriate inferences ( reformatting \nthe data ). This allows for very high-level languages in which a lot of com\u00adputation can be expressed \nmerely by posting constraints. Even in a constraint system (Herbrand) over a domain as mundane as that \nof finite trees, such a communication scheme leads to many interesting idioms such as incomplete messages, \nshort-circuits, difference-lists, messages to the future etc. [Sar89] the techniques that have colloquially \nbeen referred to as stemming from the power of the logical variable .2 For instance, it is possible to \nembed a commu\u00adnication channel (variable) in a message as a [first-class object, in exactly the same \nway as data. This makes for an elegant form of dynamic reconfigurability, of a kind which is difficult \nto achieve in a simple way within frameworks such as those of CCS and CSP. The practicality of this framework \nis attested to by the fact that several implemented cc lan\u00adguages are now available, including at least \none commercial implementation [FT89]. This paper focusses on the use of constraints for commu\u00adnication \nand control in concurrent constraint programming languages. It is worth pointing out that the concurrent \ncon\u00adstraint programming framework is, however, concerned with z~he ~]nd of con~traint.baaed communication \nschemes we are de\u00ad scribing here are the essence of computation in the logic programming style. Indeed, \nthe origins of the cc framework are in concurrent logic programming and in the notion of constraint logic \nprograming intro. duced by [JL87,Mah8~. See [Sar89] for further details. much more than just concurrency \nit takes the first step to\u00ad wards a general architecture for computing with constraints that is dependent \nonly on the form of constraint systems, not on their particular details (following [J L87]). As such, \nit provides one coherent attempt to articulate the vision of constraint programming manifest in the work \nof Suther\u00ad land, Sussman, Steele, Borning and others. In particular, the cc framework is also concerned \nwith many other com\u00adbinators for introducing and controlling logic-programming style search non determinism \n. 3 One of the goals of our research is defining a general class of constraint systems to which the concurrent \nconstraint paradigm applies. A beginning has been made in the present paper. The presence of constraint \nsystems in computational and engineering problems is very widespread. For example, several applications \nto AI are usefully viewed in terms of constraints. A general enough definition would allow one to define \nconstraint systems as over data types as diverse as finite trees, streams, intervals of rational numbers, \nvari\u00adous types of functions spaces and data types derived from knowledge rerpesentation applications. \nIndeed the design of constraint systems of use in computational systems is lim\u00adited only by our imaginations. \nIt is not difficult to consider any data-structure of interest in computer science enu\u00admerate types, \nrecords, bags, arrays, hash-tables, graphs, bi\u00adnary search trees and devise constraint systems of interest \nover them which can usefully and interestingly be embedded within a cc language. Generality. The cc framework \nis parametrized by an arbi\u00adtrary constraint system. This schematic treatment brings with it all the usual \nadvantages: results need to be proven only once and are immediately applicable to all members of the \nclass. In particular the models we develop in this paper are in fact a class of models, for a large class \nof programming languages. We place very few restrictions on the nature of a con\u00ad straint svstem: we demand \nonlv that there be some svstem of partial information, some notion of what it means for various pieces \nof partial information to be true about the same objects (the notion of co rw%tency), and what it means \nfor certain Dieces of information to alwavs hold. given that . some other pieces of information must \nalso hold (the notion of entailment). Relationship to other theories of concurrence. Recentlv .. there \nhave been radical new ideas about concurrency. Two of particular note are the so called Chemical Abstract \nMa\u00adchine [B B90], due to Boudol and Berry, and mobile pro\u00adcesses, due to Milner and his co-workers [MP \nW89]. In both of these approaches the key new ingredient is that processes can alter their interactions \nand are, in efffect, mobile. In our approach the interactions between processes is dynamic too in the \nsense that there is no predetermined set of agents that a given agent is limited to interact with. The \nrelationships need, however, to be understood carefully. It would be par\u00adticularly interesting to understand \nhow a lambda abstrac\u00adtion mechanism could be incorporated into the concurrent constraint paradigm. Understanding \nthe relationships with 3Throughout this paper when we taIk of the cc languages we shall mean the cc languages \nwith Ask and Tell as the only primitive con~traint.relatad apepations Other cc.herent ~md very useful \n@rni\u00adtives can be (and have been, in [Saz-89]) defined, but they are outside the scope of this paper. \nthe work on mobile processes or Boudol s gamma calculus would be very helpful, as it is known that the \nlatter can encode the lazy lambda calculus [Mi190 ,JP90]. 1.1 Contributions of this paper The central \ntask of this paper is to develop the semantic foundations of the programming paradigm discussed above. \nTowards this end, we formalize the basic notion of a con\u00adst raint system, and present operational and \ndenot ational semantics for the determinate and nondeterminate cc lan\u00ad guages. The next several paragraphs \ndiscuss each of these contributions in detail. Constraint systems. We formalize the notion of constraint \nsystem, generalizing and simplifying previous treatments in [Sar89,JL87,SR90]. The basic insight is to \ntreat constraint systems as systems of partial information, in the style of Dana Scott s development \nof information systems, together with operators to express hiding of information. From a programming \npoint of view such operations allow the in\u00adtroduction of parameter-passing in constraint programming \nlanguages, and the introduction of local variables within procedures. Philosophy of modeling. The models \ndeveloped in this pa\u00adper are based on the philosophy of modeling discussed in [OH86,Hoa90]. In the next \nfew paragraphs we summarize the basic ideas and ontological commitments made in this style. Crucial to \nthis philosophy is the identification of the no\u00adtion of observation of a process. A process is then equated \nwith the set of observations that can be made of it. The set of processes is identified by presenting \na collection of natu\u00adrally motivated closure conditions on sets of observations. It is important that \nthe observations of a process include all ways in which the process can go wrong , that is, fail to meet \nobligations imposed on it by the environment. Once a process- goes wrong , further detailed modeling \nof the process is irrelevant, since the emphasis of this approach is on guaranteeing that as long as \nthe environment honors its obligations to the process, the process cannot go wrong . This ontological \ncommitment is usually captured in the slo\u00adgan divergence is catastrophic , or, {anything can happen once \nthe process goes wrong. One way in which the process can go wrong is by en\u00ad gaging in (internal chatter \n, that is, in an infinite sequence of internal actions (possibly continually producing output in the \nmeantime), without asking the environment for some in\u00adput (and thereby giving the environment the ability \nto con\u00adtrol its execution by denying it that input). Within the cc framework, another way in which a \nprocess can go wrong is if it causes the shared store to become inconsistent for in such a state the \nprocess is again completely uncontrol\u00adlable from the environment. No action that the environ\u00adment can \ntake can then influence the course of execution of the process, since the inconsistent store can answer \na ng ask request. Hence the process is free to follow any branch that it could potentially follow, without \nmore input from the environment. In particular, if the process recursively calls another process, then \nit can engage in an infinite execution sequence because every ask operation guarding a recursive procedure \ncall will be answered successfully. This suggests that in the semantic model the process that can produce \nfalse should not be distinguished form the pro\u00adcess that can engage in an infinite execution sequence; \nboth such processes should be treated as catastrophic . This is indeed the approach followed in the main \nbody of the paper. However, other alternative treatments of failure and diver\u00adgence are possible, and \nin Section 3 we shall indicate some of the possibilities and how they can be treated. Determinate constraint \nprogramming. Sixteen years ago, Kahn gave a theory of determinate data-flow [Kah74]. To\u00adday this theory \n(with some extensions, e.g. to concrete data structures) remains the dominant theory of determi\u00adnate \nconcurrent computation. While simple, elegant and widely applicable, the theory s emphasis on computing \nwith directional point-to-point communication channels imposes some expressibility restrictions: channels \ncarry only fully formed values from the underlying data domain; networks are not dynamically reconfigurable \nin that connections can\u00adnot themselves be passed as data on communication chan\u00adnels; and each channel \nhas only one reader and one writer. Because of these restrictions, Kahn s theory cannot model such useful \nprogramming idioms as incomplete messages, difference lists, recursive doubling, multiple readers/writers \nof a communication channel, dynamic reconfiguration, etc. Although these techniques were originally developed \nin the context of nondeterminate concurrent logic programming, they are aJl both inherently determinate \nand usefully ex\u00adpressible in determinate contexts. This paper presents a simple theory of determinate \ncom\u00adputation which can model all of the above idioms and more: our theory preserves the essential features \nof Kahn s the\u00adory while substantially extending its domain of applicability. From the contraint programming \npoint of view it is useful and interesting to focus on the determinate subset because a mathematically \nmuch simpler treatment can be given, with\u00adout sacrificing any essential novelty the major semantic and \ncomputational ideas underlying the cc paradigm can already be illustrated in the determinate case. We \npresent a simple algebraic syntax for determinate concurrent constraint programs. We also present an \nop\u00aderational semantics based on a labelled transition system in which a configuration is just an agent \nand a label is a pair of constraints (the store in which the transition is ini\u00adtiated and the store in \nwhich it terminates). The transition system is able to completely avoid using substitutions and variable-renamings \n, thereby considerably simplifying the treatment. Past theoretical treatments of (constraint) logic programming \nhave had to use various ad hoc techniques for dealhw with this rn-oblem. Th~ denotatio~ of a process \nis taken to be a set of con\u00adstraints (the resting points of the process) satisfying cer\u00adtain properties. \nVarious combinators are defined on such processes, corresponding to ask and tell operations, to run\u00adning \ntwo agents in parallel, and to creating a new ~loca.1~> communication channel. We show that the denotational \nse\u00admantics is fully abstract with respect to he operational se\u00admantics, We also give a sound and complete \naxiomatization of equality for finite programs, so that finite program equiv. alence can be established \npurely by equational reasoning. We also develop a slightly different model in which limits of (fair) \ninfinite execution sequences are observed. Models for nondeterminate cc languages. The denotation of \na nondetermin ate Drocess. is somewhat more comdex than in the determinate case: rather than storing \njust the resting points of a process, we must also associate with each resting point the path followed \nby the process in reaching that point. Such a path is also called a failure. The denotation of a rxocess \nis the set of all its failures. The set of all rxo\u00adcesse~ is identified by presenting some naturally \nmotiv~ted closure conditions on sets of failures, following [Jos90], (For example, one of the major conditions \nensures that processes are finitely nondetermin ate. ) The resulting notion of a pro\u00adcess is intuitive \nand easy to understand. Combinators cor\u00adresponding to ask, tell, nondeterminate (dependent) choice, parallel \ncomposition and hiding are defined. A simple op\u00aderational semantics is given, and a full correspondence \nwith the denot ational semantics is established. In particular, we believe that this treatment gives \na completely satisfactory account of the semantics of concurrent logic programming languages. A central \nissue in our semantic treatment is the repre\u00adsentation of a failure. A failure could be represented as \na sequence of ask/tell annotated constraints. Such a strategy is followed in earlier work on related \nlanguages, for example in [Sar85, Lev88, GL90], [GCLS88], [GMS89] etc. However, ask-tell seouences are \nfar too concrete in this settirw. . Thev store too much information which must then be abstracted from \n(usually via complex closure conditions) since they en\u00adcode the precise path followed by a process to \narrive at a resting point. Furthermore, the definition of various combi\u00adnators becomes rather cumbersome \nbecause of the need to propagate information down the trace. Instead, we chose to remesent observations \nvia various kinds of closure oDera\u00ad . tors. To capture various operational notions of interest, we introduce \nthe concept of trace operators, and bounded clo\u00adsure operators, and present some portions of their theory \nrelevant to this paper. We SJSO establish a very simple relationship between the nondeterminate and determinate \nsemantics, showing that the two semantics are essentially identical for determinate programs over finit \nary constraint systems. In summary, we present a simple model for the cc lan\u00adguages that is fully able \nto account for nondeterminism and divergence, thus permitting the use of this model in reason\u00ading about \nliveness properties of programs. The model is also shown to be fully consistent with the intuitive operational \nsemantics of cc programs.  1.2 Related work The basic concepts of concurrent constraint programming \nwere introduced in [Sar88, Sar89]. Subsequently, we developed an operational semantics and a bisimulation \nsemantics in [SR90]. This line of research extends and subsumes (for all intents and purposes) earlier \nwork on the semantics of concurrent logic programming languages, The power of the logical variable has \nbeen amply rec\u00adognized in recent years, and numerous authors too numer\u00adous for us to survey their work \nin this extended abstract have investigated the combination of functional and logic 4The semantics presented \nin this paper does not account for the language Concurrent Prolog studied in [G CLS88]; we do not, however, \nview this as a defect in our approach/ Indeed, researchers working with Concurrent Prolog have moved \nto the Ask-and-Tell cc framework [KYSK88,GMS89]. programming languages [Lin85,DL86,ANP89, JPP89]. How\u00adever, \nno account of the-basic paradigm comparable with Kahn s original account in simplicity and generality \nhas been forthcoming. Perhaps the most noteworthy in this context is the work of [J PP89], which discusses \na subset of the functional pro\u00adgramming language Id Noveau with logical arrays [ANP89]. Their semantic \ntreatment also identifies a program as a clo\u00adsure operator. However, their treatment is specirdized to \nthe particular kinds of arrays they were dealing with; our work is parametrized by a very general notion \nof constraint system. Further, they do not explicitly discuss the hiding and the Ask operations; though \ntheir Tell operation does implicit asks. We also present an axiornatization for the model. Their operational \nmodel includes an explicit discus\u00adsion of how the constraints are resolved and remesented. . we achieve \nconsiderable simplicity by abstracting away the details of the constraint resolution process. The characterization \nof concurrency via intersection of sets of fixed points has also been elaborated by Hoare in a recent \npaper [Hoa89], in a setting related to Unity [CM88]. Our paper develops that viewpoint, and presents \na char\u00adacterization of other combinators as well, in the same style. We believe that the concurrent constraint \nprogramming lan\u00adguages are the natural setting for the development of a the\u00adory of computing with closure \noperators. The semantics of nondeterminate concurrent constraint programming languages is becoming an \nactive area [SR90, GMS89,GL90,dBP90 a]. However none of this work explores the very simple characterizations \npossible for the determi\u00adnant e languages, deahng instead with the representation of mocesses as various \nsorts of trees or sets of i/o annotated sequences of constraints. The notion of det&#38;minacy was studied \nin the set-up of logic programming languages in [Mah87], but no characterizations of the kind we provide \nhere were given. [GMS89] does not consider most of the combinators we treat here, besides having a complicated \ntreatment of ask/tell sequences. Neither does it treat recursion. In work to appear, deBoer and Palamidessi \n[dBP90a, dBP90b] propose a model which is similar to ours in many respects. In particular, they have \nalso recognized that it is sufficient to take seauences of ask/tell actions in order to get a model for \nthe ~c languages. However, their treatment ignores recursion. Much of the sophistication of the present \nmodel lies in the way finite nondeterminate axioms need to be introduced in order to correctly model \ndivergence. Their model in [d BP90b] is not compositional with respect to the choice operator. Our development \nfollows closely Mark Josephs treat\u00ad ment of receptive processes in [Jos90]. A receptive process can \nalways accept any input from the environment, but may produce different (or no) output depending on the \ninput. Josephs gives a set of axioms for reactive processes that turn out to be more or less what is \nneeded in order to develop a model for the cc languages as well. The primary differences lie in the nature \nof communication, and in the combina\u00ad tors treated. Josephs theory treats communication in the usual \nCCS/CSP style as the exchange of uninterpreted to\u00ad kens with the environment. The constraint-based nature \nof the cc languages imposes additional structure which must be considered. However, as this paper demonstrates, \nit is pos\u00ad sible to adapt his basic model to the cc setup without major reworking, which should confirm \nthe .tssential robustness of his conceptualization of asynchronous systems. 2 Constraint Systems Our \npresentation here is simplified and generalized from the present ation in [SR90]. More details may be \nfound in [SPRng]. What do we have when we have a constraint system? First, of course, there must be a \nuocabuhary of assertions that can be made about how things can be each assertion will be a syntactically \ndenotable object in the programming language. Postulate then a set D of tokens, each giving us partial \ninformation about certain states of affairs. At any finite state of the computation, the program will \nhave de\u00adposited some finite set u of such tokens with the embedded constraint-solver and may demand to \nknow whether some other token is entailed by u. Postulate then a compact en\u00adtailment relation l-~ pD \nx D (pD is the set of finite subsets of D), which records the inter-dependencies between t okens. The \nintention is to have a set of tokens v entail a token P just in case for every state of affairs for which \nwe can assert every token in v, we can also assert P. This leads us to:5 Definition 2.1 A simple constraint \nsystem is a structure (D, }), where D is a non-empty (countable) set of tokens or (primitive) constraints \nand l-~ pD x D is an entaihnent relation satisfying (where pD is the set of finite subsets of D: Cl u \nEP whenever P ~ u, and, C2ul-Qwhenever ul-Pforall PC v,andv+Q. Extend 1-to be a relation on pD xpD by: \nu E v iff u + P forevery PEv. Define u%vifu E vand vi-u. 0 Of course, in any implementable language, \nF must be decidable and as efficient as the intended class of users of the language demand. Compactness \nof the entailment relation ensures that one has a semi-decidable entailment relation. If a token is entailed, \nit is entailed by a finite set and hence if entailment holds it can be checked in finite time. If the \nstore does not entail the constraint it may not be possible for the constraint solver to say this at \nany finite stage of the computation. Such a treatment of systems of partial information is, of course, \nwell-known, and underlies Dana Scot t s informa\u00adtion systems approach to domain theory [SC082]. A simple \nconstraint system is just an information system with the consistency structure removed, since it is natural \nin our set\u00adting to conceive of the possibility that the execution of a program can give rise to an inconsistent \nstate of affairs. Following standard lines, states of affairs (at least those representable in the system) \ncan be identified with the set of all those tokens that hold in them. Definition 2.2 The elements of \na constraint system (D, l-) are those subsets c of D such that P 6 c whenever u ~f c (i.e. u is a finite \nsubset of c) and u t-P. The set of all such elements is denoted by ID I. For every u ~ f D define z c \nID I to betheset {Pe Dlu l-P}. II 5 In reality, the systems underlying most concrete concurrent con\u00adstraint \nprogramming languages have slightly more structure to them, namely they are ask-and-tell constraint systems \n[Sar89]. The addi\u00adtional structure arises because it is possible to state at the level of a constraint \nsystem that the imposition of certain constraints can be de\u00adlayed until such time as some associated \nconstraint is entailed by the store ( implicit Ask-restriction ). However, this additional structure \nis not crucial, and can be easily handled by extending the techniques presented in this paper. As is \nwell known, (ID], Q) is a complete algebraic lattice, the compactness of 1-gives us algebraicit y of \nID], with least element true = {P I 0 1-P}, greatest element D (which we will mnemonically denote false), \nglbs (denoted by n) given by intersection and lubs (denoted by U) given by the closure of the union. \nThe lub of chains is, however, just the union of the members in the chain. The finite elements of ID \nI are just the elements generated by finite subsets of D; the set of such elements will be denoted ID \n10. We use a, b,c, d and e to stand for elements of ID I; c z d means c 1-d. Two common notations that \nwe use when referring to the elements of ID I are Tc= {dlc< d} and J c= {did < c}, The alert reader will \nhave noticed that the constraint system need not generate a jinitcarye algebraic lattice since, in general, \nScott information systems do not generate fini\u00adtary domains. Indeed many common constraint systems are \nnot finitary even when the data type that they are defined over is finitary. For the interpretation of \ndeterminate con\u00adcurrent constraint programs we do not need the constraint system to be finitary but we \ndo for the nondeterminate case. If we drop the requirement that the entailment relation is compact we \nwill generate, in general, lattices that are not algebraic. We always need the entailment relation to \nbe compact since we do not know, as yet, whether these ideas can be extended to nonalgebraic constraint \nsystems. In what follows, by a finite constraint we shall mean a finite set of tokens. We also take the \nliberty of confusing a finite constraint u with z ~ I.DIo. Example 2.1 Generating constraint systems. \nFor any first-order vocabulary L, and countably infinite set of variables var, take D to be an arbitrary \nsubset of open (L, var)-formulas, and t-to be the entailment relation with respect to some class A of \nL-structures. That is, {PI, . . . . P.} > Q iff for every structure M ~ A, an M-valuation realizes Q \nwhenever it realizes each of PI, . . . . P~. Such a (D, t-) is a simple constraint system. II Example \n2.2 The Kahn constraint system. More concretely, let us define the Kahn constraint system D(B) = (D, \n}n) underlying data-flow languages [Kah74], for B = (1?, I-B) so some underlying constraint system on \na domain of data elements, E. Let L be the vocabulary consisting of the predicate symbols = /2, c/1 and \nthe func\u00adtion symbols f/1, r/1, a/2, A/O. Postulate an infinite set (X, Y C)Var of variables. Let the \nset of tokens D consist of atomic (Z, Var) formulas. Let A consist of the single struc\u00adture with domain \nof interepretation BW the set of (possibly infinite) sequences over l?, (including the empty sequence \nA) and interpretations for the symbols in .C given by: = is the equality predicate, e c is the predicate \nthat is true of all sequences except A. b f is the function which maps A to A, and every other sequence \ns to the unit length sequence whose first el\u00adement is the first element of s, * r is the function which \nmaps A to A, and every other sequence s to the sequence obtained from s by drop\u00adping its first element, \nThis means that a finite element dominates only finitely many elements. t a is the function which returns \nits second argument if its first argument is A; otherwise it returns the se\u00adquence consisting of the \nfirst element of its first argu\u00adment followed by the elements of the second argument. Now, we can define \nb_V by: {C1,..., Cn}EDC* AED(CICn+C)n+C) thus completing the definition of the constraint system D = \n(D, I-~). Note that in this constraint system the set of elements are not finitary. The constraint X \n= Y, which is finite, entails infinitely many constraints of the form f(r (X)) = ~(r (Y)). Since the \nconstraint system has a compact en\u00ad tailment relation we will have algebraicity. In the lattice generated \nby the entailment closed sets of tokens the set consisting of the entailment closure of {X = Y} will \ncon\u00adtain all the tokens of the form f (T*(X)) = f (r-n(Y)); the set consisting of all the latter, however, \nwill not contain X = Y. It is possible to define a variant system that is finitary. The data type of \nstreams is, of course, finitary. D Example 2.3 The Herbrand constraint system. We describe this example \nquickly. There is an ordinary first\u00adorder language L with equality. The tokens of the constraint system \nare the atomic propositions. Entailment can vary de\u00adpending on the intended use of the predicate symbols \nbut it must include the usual entailment relations that one expects from equality. Thus, for example, \nf(x, Y) = f(A, g(~, C)) must entail X = A and Y = g(13, C). If equality is the only predicate symbol \nthen the constraint system is finitary. With other predicates present the finitariness of the lattice \nwill depend on the entailment relation. 0 Example 2.4 [Rational intervals. The underlying tokens are \nof the form X c [z, y] where z and y are rational numbers and the notation [z, y] means the closed interval \nbetween z and y. We assume that every such membership assertion is a primitive token. The entailment \nrelation is the one derived from the ob\u00ad vious interpretation of the tokens. Thus, X c [xl, g]] R X \nc [ZZ, Y2] if and only if [xl, VI] ~ [ZZ, yz]. Whether this yields a compact entailment relation is a \nslightly del\u00ad icate issue. If we assume the usual definition of intersec\u00ad tion and unions of infinite \nfamilies of intervals, we will def\u00ad initely not have a compact entailment relation. For exam\u00ad ple, since \nfim>O [0,1 + I/n] = [0, 1], we would have {X E [0, 1 + I/n]ln > O} E (X c [0, I]) but no finite subset \nof {X 6 [0,1 + l/n]ln > O} would entail X 6 [0, I]. We may take the definition u t-X E [o, y] to mean \nthat u must be a finite collection of intervals. In this case the entailment relation is, by definition, \ncompact and the lattice generated is algebraic. It will, however, appear slightly peculiar with respect \nto one s normal expectations about intervals. The join of a family of assertions involving membership \nof X in a nested family of intervals will not yield an assertion about membership in the intersection \nof the family. Instead there will be a new element of ]Dl that sits below the intersec\u00ad tion. Thus, for \nexample, the join Un>o X C [0,1 + I/n] will not be X E [0, 1] but rather a new element that sits below \nX c [0, 1]. Clearly the lattice is not fiuitary but the entailment relation is compact and the lattice \nis indeed al\u00adgebraic. It is worth noting that this example shows that we can model determinate computations \nover domains that are not incretnenta17. In fact we have an order-dense subse~ We can work with such \na lattice when we model the determinate language but, as yet, we cannot model the nondeterminate languages \nover such constraint systems. It is known that the closure operators over a lattice with an order-dense \nsubset cannot form an algebraic complete partial orderg. Thus, the extension of these ideas to higher-order \nprogramming wilf be challenging. 0 Hiding in constraint systems. Any reasonable program\u00ad ming language \nsupports modularity by providing a notion of hiding the internal structure of an agent from its cent \next. We support this hiding with a family of hiding operators on the underlying constraint system; these \noperators capture the notion of projecting away information. In this we use the axiom atization of cylindric \nalgebra[HMT71]. In future research we plan to give a more principled account of hiding and of the choice \nof axioms using notions from categorical logic. Definition 2.3 A cylincbic constraint system is a structure \n(D, l-, Var, {=x ] X c Var}) such that: (D, t-) is a simple constraint system,  Var is an infinite \nset of indeterminate or variables,  For each variable X ~ Var, ax ; pD -pD is an operation satisfying: \n El ut-3xtt EZ u 1-v implies 3XU R 3XV E3 ~x(UuqXV) x 3xu U3X0, E4 3x3yu % 3y~xU 0 For every variable \nX, 3X is extended to be a continuous function from IDI 4 IDI in the obvious way: 3xc = {P I3xu 1-P, for \nsome u ~f c} Example 2.5 Let the token set consist of some subclass of (L, Var) formulas closed under \nexistential quantification of finite conjunctions. Each operator 3X is then interpreted by the function \nwhich maps each finite set {PI, . . . . P*} of tokens to the set of tokens {3X. P1 A A Pn}. It is eaay \nto see that the four conditions above will be satisfied. 1 Diagonal elements. For all useful programming \nlanguages in this framework, it is necessary to consider procedures with parameter passing. In usual \nlogic programming languages, parameter passing is supported by using substitutions. We use a trick due \nto Tarski and his colleagues. For the class of constraint systems discussed above, this trick can be \nil\u00adlustrated by providing, as tokens, the diagonal formulas X = Y, for X, Y c Var. Now, the formula #[Y/X] \nis nothing else but the formula 3X.(X = Y) A ~. More generally, for an 7A prime interval is ~ pair of \nfinite elements such that there is no finite element properly bet ween them, An incremental domain is \none in which between every two related finite elements there is a finite sequence of prime intervals \nthat interpolates between them, aThis means that between any two elements there is ~ another distinct \nelement. We are indebted to Michael Huth for pointing this out. arbitrary constraint system, we can axiomatize \nthe required properties oft he diagonal elements, following [H MT71]. We demand that the token set D \ncent ain, for every pair of vari\u00adables X, Y G Var, the token dx y satisfying the properties: D1. 0h dxx \nD2. if X # Y, {dxY} % 3z{dxY, dYz} D3. {dxy} U3X(ZLU {(ix~}) t_ u The defect of thw axiomatization is \nthat it appears out oft hln air . In particular, categorical logic, as expounded by Lambek and Scott \n[LS86] haa a thorough analysis of vari\u00adables and also of variable-free calculi and the relationship between \nthem. If we cast the notion of constraint system in categorical terms, we would be able to use the vast \nbody of results about categorical logic [LS86] in the course of our de\u00advelopment of the concept of constraint \nsystem. The axioms for hiding would emerge from fundamental principles, Inves\u00adtigations into catgoricaf \nlogic suggest that the logic implicit in our treatment is a form of coherent logic [MR97] 10. 3 The determinate \nlanguage . . . the most important observations are those which can be made only indirectly by a more \nor less eJaborat e experiment . . . a successful choice of the right kind of indirect observation can \nprovide a remarkably coherent and general explanation of a wide range of diverse phenomena. C.A.R. Hoare \n(1990) Our basic semantic insight is that the crucial observation to make of a process is the set of \nits resting points. A process is an information processing device that interacts with its environment \nvia a shared constraint representing the known common information. A resting point of a process is a \ncon\u00adstraint c such that if the process is initiated in c, it will eventually halt wit bout producing \nany more information. l] While the basic idea is the same, the semantics for the nondeterminate language \nis significantly more complex than the semantics for the determinate language. The reason is simple. \nFor the determinate language, it turns out to be sufficient to store with a process just the set of its \nresting points, as we now discuss. To be determinate, the process must always produce the same output \nconstraint when given the same input con\u00adst raiut. We can therefore identify a process with a function: \nThis function maps each input c to false if the process when initiated in c engages in an infinite execution \nsequence, and to d if the process ultimately quiesces having upgraded the store to d. 12 It turns out \nthat there is suficied information 10we are indebted to Robert Seely and Phil ScOtt fOr this observation. \n11 Strjct,y Speakjng, this js not true. In the cOlltinUOUs c105ure 0P\u00ad erator semantics we consider later \nin this section, a resting point of a process is a constraint c such that if the process were to be initiated \nin c, it would not be able to ouptut any new information without receiving some more information from \nthe outside. The point is that the process may engage in an infinite execution sequence in c, as long \nm it produces no new information. 12 Thus we shall confuse the process that on input c produces false \nand halts with the process that on input c engages in an infinite execution sequence. Recall from the \nintroduction that this is in tune with the specification-oriented approach to semantics, in the resting \npoints of the process to uniquely determine its associated function. This property highlights the semantic \nsimplicity of the Ask-and-Tell communication and synchro\u00adnization mechanism. Let f be the operator on \nID 10 corresponding to a given process. Now, the only way in which this process can affect the store \nis by adding more information to it. Therefore, ~ must be extensive: Vc.c < f(c) (1) Second, the store \nis accessible to the process as well as its environment: therefore if on input c a process produces output \nd and halts, it must be the case that d is a resting point, that is, on input d the process cannot progress \nfurther (because if it could, then it wouldn t have stopped in d). That is, ~ must be idempotent: Vc.f(f-(c)) \n= f(c) (2) Finally, consider what happens to the output of such a function when the information content \nof the input is in\u00adcreased. If the invocation of the function corresponds to the imposition of a constraint, \nthen as information in the input is increased, information in the output should not decrease. Such a \nfunction is called monotone (3) An operator over a partial order that is extensive, idem\u00adpotent and monotone \nis called a closure operator (or, more classical, a consequence operator, [Tar56]). Closure opera\u00adtors \nare extensively studied in [GKK+ 80] and enjoy several beautiful properties which we shall exploit in \nthe following. Within computer science, continuous closure operators have been used in [SC076] to characterize \ndata-types. We list here some basic properties. The most fundamen\u00adtal property of a closure operator \nj over a lattice E is that it can be represented by its range f(E) (which is the same as its set of fixed \npoints). ~ can be recovered from ~(E) by mapping each input to the least element in f(E) above it. This \nis easy to see: by extensiveness, ~(c) is above c; by idempotence, ~(c) is a fixed-point, of $, and by \nmonotonic\u00adity, it is the least such fixed-point. This representation is so convenient that in the following, \nwe shall often confuse a closure operator with its range, writing c c ~ to mean ~(c) = c. In fact, a \nsubset of E is the range of a closure operator on E iff it is closed under glbs of arbitrary subsets \n(whenever such glbs exist in the lattice). Thus, the range of every closure operator is non-empty, since \nit must contain false = f10. Partial order. For a closure operator ~ over ID 10, let df be the divergences \nof ~, that is, those inputs in which the process diverges. As discussed above, the divergences are exactly \nthose constraints which are mapped to false by f, that is, j l (false). The partial order on determinate \npro\u00adcesses of interest to us will be based partly on the processes divergences. The intention is that \na process f can be im\u00adproved to a process g if the divergences of g are contained in those of ~ and at \nevery point in the convergence of f, f and g take on identical values. In terms of fixed points, this \nyields: Definition 3.1 The divergence order on closure operators over Illlo is given by: II The bottom \nelement of this partial order is {false} which diverges everywhere. It is not hard to see that this order \nis complete, with limits of chains given by unions of the set of fixed-points. 3.1 Process Algebra In \nthis section we develop a simple language, the determi\u00adnate cc language, for expressing the behavior \nof concurrent determinate constraint processes. We consider agents con\u00adstructed from tells of finite \nconstraints, asks of finite con\u00adstraints, hiding operators, parallel composition and proce\u00addure calls. \nThroughout this section we shall assume some fixed cylindrical constraint system (with diagonal elements) \nD. As usual, IDI denotes this constraint system s set of elements, while IDIo denotes its set of finite \nelements. We also define a quartic transition relation +~ Env x(IDIO xIll\\o) xAxA which will be used \nto define the operational semantics of the programming language. (Here Env is the set of all par\u00adtial \nfunctions from procedure names to (syntactic) agents. ) Rather than write (p, (c, d), A, 1?) <---+ we \nshall write p ~ A 3) B (omitting the p H if it is not relevant) and take that to mean that when initiated \nin store c, agent A can, in one uninterruptible step, upgrade the store to d, and subse\u00adquently behave \nlike B. In the usual SOS style, this relation will be described by specifying a set of axioms, and tak\u00ading \nthe relation to be the smallest relation satisfying those axioms. The syntax and semantics of the determinate \nlanguage are given in Table 1. We discuss these semantic definitions in this section. For purposes of \nexposition we assume that procedures take exactly one variable as a parameter and that no program calls \nan undefined procedure. We also system\u00adatically confuse the syntactic object consisting of a finite set \nof tokens from D with the semantic object consisting of this set s closure under 1-. Tells. The process \nc augments its input with the finite con\u00adstraint c. Thus it behaves as the operator At, c U x, which \nin terms of fixed points, is just: c={dclDlold~c} The operational behavior of c is described by the transi\u00adtion \naxiom: ~ (d~d) true (c # true) (4) corresponding to adding the information in c to the shared constraint \nin a single step. 13 Asks. Let c be a constraint, and f a process. The process c ~ f wtits untfi the \nstore contains at le~t ss m~ch infor\u00admation as c. It then behaves like f. Such a process can be described \nby the function Az.zf z > c thenf(z) else c. In terms of its range: c-+f={dclDlold~c+def} lsThroughOut \nthe ~e~t of this paper, depending on cOntext, we shall let c stand for either a syntactic object consisting \nof a finite set of tokens, the constraint obtained by taking the closure of that set under F, the (semantic) \nprocess that imposes that constraint on the store, or the (syntactic) agent that imposes that constraint \non the store, Syntax. P::=D.A D::=. I p(x) :: A I D.D A::=c]c -+ AI AA A13XA [P(X) Semantic Equations. \nA(c). = {d 6 IDIo Id ~ c} A(c+A)e ={d EIDlo\\d~c+d EA(A)e} A(A A~)e = {~ c ID]O Id GA(A)e A d c A(l?)e} \nA(3XA). = {d c IDIo I 3. E A(A)e.3xd = 3XC} A(p(X))e = %(d~~ U e(p)) D(c)e = e D(p(X) :: A.D) = D(D)e[p \nw 3x(d.x U A(A)e)] P(D.A) = A(A)( X D(D)) Above, c ranges over basic constraints, that is, Iinite sets \nof to\u00adkens. P is the syntactic class of programs, D is the syntactic class of sequences of procedure \ndeclarations, and A is the syn\u00adtactic class of agents. a is some variable in the underlying con\u00adstraint \nsystem which is not allowed to occur in user programs. (It is used as a dummy variable during parameter \npassing. ) e maps procedure names to processes, providing an environment for in\u00adterpreting procedure \ncalls. We use the notation c U j to stand for ~ldcj}. Table 1: Denotational semantics for the Ask-and-Tell \nDe\u00adterminate cc languages The ask operation is monotone and continuous in its process argument. It satisfies \nthe laws: (Ll) c~d=c~(c Ad) (.L2) c ~ true= true (L3) c+d+A=(c IJd)+A (L4) true+ A = A The rule for c \n-+ A is: c+ A(fl)A if ~>c (5) Parallel composition. Consider the parallel composition of two processes \nf and g. Suppose on input c, f runs first, producing ~(c). Because it is idempotent, j will be unable \nto produce any further information. However, g may now run, producing some more information, and enabling \nadditional information production from ~. The system will quiesce exactly when both f and g quiesce. \nTherefore, the set of fixed points of f A g is exactly the intersection of the set of fixed points of \nf with the set of fixed points of g: fAg=fng It is straightforward to verify that this operation is \nwell\u00addefined, and monotone and continuous in both its argu\u00adments. While the argument given above is quite \nsimple and elegant, 14 it hides issues of substantial complexity. The ba\u00ad sic property being exploited \nhere is the restartability of a determinate process. Suppose an agent A is initiated in a 14 And ~hou,d \nbe con~r=ted with most definitions of concurrency for other computational models which have to fall \nback on some sort of interleaving of basic actions. store c, and produces a constraint d before quiescing, \nleav\u00ading a residual agent B to be executed. To find out its subsequent behavior (e.g., to find out what \noutput it would produce on a store e ~ d), it is not necessary to maintain any explicit represent ation \nof B in the denotation of A. Rather, the effect of B on input e z d can be obtained simply by running \nthe original program A on e! Indeed this is the ba\u00adsic reason why it is possible to model a determinate \nprocess accurately by just the set of its resting points. As we shall see in the next section, this restartability \nproperty is not true for nondeterminate processes. Indeed, we cannot take the denotation of a process \nto be a function (nor even a relation) from finite stores to finite stores; rather it becomes necessary \nto also preserve information about the path (that is, the sequence of ssk/tell interactions with the \nenvironment) followed by the process in reaching a resting point. From this definition, several laws \nfollow immediately. Parallel composition is commutative, associative, and has an identity element. (L5) \nAA B= BAA (L6) AA(BAC)=(AAB)AC (L7) A A true= A Telling two constraints in parallel is equivalent to \ntelling the conjunction. Prefixing distributes through parallel com\u00adposition. (L8) cAd=(c Ud) (L9) c+(AAB) \n=(c+A)A(c+B) (L1O) (a-+ b) A(c+d)=(a-+b) ifc>a, b~d (~11) (a+ b) A(c+d)= (a+ b) A(c Ub+d) ifc>a (L12) \n(a~b)A(c ~d)=(a~b)A(c-+d Ub) ifd>a The transition rule for A A B reflects the fact that A and B never \ncommunicate synchronously in A A B. Instead, all communication takes place asynchronously with information \nadded by one agent stored in the shared constraint for the other agent to use. A (~ A, (6)AA B(Q) A,AB \nBAA(~) BAA, Projection. Suppose given a process f. We wish to define the behavior of 3X f, which, intuitively, \nmust hide all inter\u00adactions on X from its environment. Consider the behavior of 3Xf on input c. c may \nconstrain X; however this X is the external X which the process f must not see. Hence, to obtain the \nbehavior of 3Xf on c, we should observe the behavior of f on 3X c. However, f (= Xc) may constrain X, \nand this X is the internal X. Therefore, the result seen by the environment must be c u 3X f (3Xc). This \nleads us to define: 3Xf = {CG1~10 I~d e f.&#38;C = 3Xd} These hiding operators enjoy several interesting \nproper\u00adties. For example, we can show that they are dual closure operators (i.e., kernel operators), \nand also cylindrification operators on the class of denotations of determinate pro\u00adgrams. In order to \ndefine the transition relation for 3XA, we ex\u00adtend the transition relation to agents of the form 3X(d, \nA), where d is an internal store holding information about X which is hidden outside 3X(d, A). The transition \naxiom for 31XA yields an agent with an internal store: A ( ~gd) ~ (7)~xA (c,c~xd) ~x(d, 1?) This axiom \nreflects the fact that all information about X in c is hidden from 3XA, and ail information about X that \nA produces is hidden from the environment. Note that B may need the produced information about X to progress; \nthis information is stored with B in the constraint d. The axiom for agents with an internal store is \nstraightfor\u00adward. The information from the external store is combined with information in the internal \nstore, and any new con\u00adstraint generated in the transition is retained in the internal store: A (du~,d \n) ~ (8) 3X(d, A) c~xd ) 3X(d , 1?) In order to canonicalize agents with this operator, we need the following \nlaw: (Ezl) 3XC = 3XC In order to get a complete equational axiomatization for finite agents containing \nsubagents of the form 3XB, we need the constraint system to be expressive enough. Specifically, we require: \n(Cl) For all c 6 IDIo and X ~ Var, there exists d E \\D\\O (written Vxc) such that for all d ~ IDIo, d \nz d iff IXd ~ C. (C2) For all c, c E \\.D]O and X c Var, there exists a d c IDI. (written +x (c)c )) such \nthat for all d c [Dl, cU3Xd ~c iff3Xc U3Xd ~d. Now we can state the remaining laws needed to obtain a \ncomplete axiom atization. (Ez2) (Ez3) (Ez4) 3XC A 3XA:eI +x (c, ci) + di Recursion. Recursion is handled \nin the usual way, by tak. ing limits of the denotations of all syntactic approximants, since the underlying \ndomain is a cpo and all the combinators are continuous in their process arguments. Operationally, procedure \ncalls are handled by looklng up the procedure in the environment p. The co~responding ax. iom is: p k \nP(X) *) 3cr(dax, ~(p)) (9) Example 3.1 (Append) To illustrate these combinators, consider the append \nprocedure in the determinate cc lan\u00adguage, using the Kahn constraint system: append(Inl, In2, Out) :: \nInl = h+ Out=In2 A c(Inl) + 3X (Out = a(f (Ini), X) A -append(r(InI), In2, X)). Thk procedure waits until \nthe environment either equates X to A or puts at least one data item onto the communica\u00adtion channel \nX. It then executes the appropriate branch of the body. Note that because the ask conditions in the two \nbranches are mutually exclusive, no call will ever execute the entire body of the procedure. This procedure \ntherefore uses the A operator (which ostensibly represents parallel execu\u00adtion) as a determinate choice \noperator. This is a common idiom in determinate concurrent constraint programs. Completeness of axiomatization. \nCompleteness of axiom\u00adatization is proven via the following normal form . Definition 3.2 An agent A is \nin normal form iff A = true or A = A,cJc~ -+ d; and A satkfies the following properties: (Pi) c:< d: \n(P2) i# j implies Ci# Cj (P3) c;<., implies di < Cj (P4) ci s dj implies di < dj o Lemma 3.1 Any agent \nA containing no constructs of the form 3XB can be converted to normal form using equations (U) -(L12), \nLemma 3.2 For ang agent A = AICICI + d; in normal form, P(c.A)(ci) = d;. We use this lemma when proving \nthe following complete\u00adness theorem: Theorem 3.3 P(c.A) = P(c. B) ifl A and B have the same normal form. \nThus the laws (Ll) . . . (L12) are both sound and com\u00adplete for finite agents built using tells, asks \nand parallel composition. In addition, we also have: Theorem 3.4 Laws (Ll) (L12) and (Ez1)-(Ex4) are \nsound and complete for all finite agents. Operational semantics. In order to extract an environment from \nD.A h which to run A, we define: ?qe)p = p 7Z(p(X) :: A.D)p = 7L(D)LI~ * (3XdaX A A)] A computation in \nthis transition system is a sequence of transitions in which the environment is constrained to pro\u00adduce \nnothing. Hence the final constraint of each transition should match the initial constraint of the succeding \ntram sition. The following definition formalizes the notion of a computation starting from a finite constraint \nc: Definition 3.3 A c-transition sequence s for a program D.A is a possibly infinite sequence (c~, Ai)i \nof pairs of agents and stores such that co = c and A. = A and for all i, 7Z(D)po 1-Ai c;~+ ) Ai+l. Here \np. is the partial map from procedure names to (syntactic) agents whose domain is empty. Such a transition \nsequence is said to be terminal if it is finite, of length n ~ 1 and An_ I k stuck in c~ 1 (that is, \nthere is no constraint d and agent B such that (..=4 ~(D)po * A.-I B). In such a case, c~ 1 is also called \nthe final store. 1 One can prove a number of operational results in a fairly straightforward way. Lemma \n3.5 (Operational monotonicity.) 1. Al fi A2 as a possible transition and c < c then Al - ) AZ. This \nis essentially an operational rnonotonicity property. Definition 3.4 Suppose that an agent A in a store \nc has two transitions enabled, i.e. it could do either one of A u) Al and A n) A2. We say that these \ntransitions commute if Al ~ ) As, and Az ~uc ) Aa are both possible. 0 The following lemma is almost \nimmediate and character\u00adizes a key property of determinate agents. Lemma 3.6 If an agent has more than \none transition pos\u00adsible in a given store they will commute. The following theorem can be proved by appealing \nto commut ativit y. Theorem 3.7 (Confluence) For any constraint c and de\u00adterminate program D.A, if D.A \nhas a terminal c-transition sequence with final store d, then D.A has no infinite c\u00adtransition sequence. \nFurther, all terminal c-transition se\u00adquences have the same final store. This theorem allows us to define \nan observation function on programs mapping IDIO to ID]O by: O(P)(C) = d if P has a terminal c-transition \nsequence with final store d, and O(P)(C) = false otherwise, Theorem 3.8 The @nction O(P) is a closure \noperator. The only nontrivial part of this proof is showing idem\u00adpotence; it is done by induction on \nthe length of reduction sequences and use of Lemma 3.5. We can now connect the operational semantics \nwith the denotational semantics. Theorem 3,9 (Strong adequacy) O(P) = P(P) Therefore, two programs P \nand Q are observationally equal (O(P) = O(Q)) iff their denotations are equal ( P(P) = P(Q)). Thus the \ndenotations of programs contain enough information to distinguish programs that are operationally different. \nProof sketch: One can show that a single re\u00adduction step preserves the denotational semantics. Then we \nshow that the sets of fixed points of the two closure opera\u00adtors are the same. In order to do this we \nuse a structural induction and a fixed-point induction for the recursive case. The proofs are not trivial \nbut they are not particularly novel either. The full paper will contain a more thorough discus\u00adsion. \nIt remains to show that the denotations of two programs are identified if, from the viewpoint of the \noperational se\u00ad mantics, they behave identically in all contezts. Definition 3.5 A context C[*] is a \nprogram D.AIo] whose agent A contains a placeholder (denoted by c). We put a program D .A into this context \nby taking the union of the definitions (renaming procedures where necessary to avoid name clashes) and \nreplacing the placeholder in A with A , yielding C[D .A ] = D u D .A[A ]. u Theorem 3.10 (Full abstraction) \nP(P) = P(Q) if/ for all contexts C[*], Obs(CIP]) = Obs(CIQ]). The theorem is easy to prove given that \nwe have strong adequacy and a compositional definition of the denotational semantics. 3.2 Alternate \nsemantic treatments The first semantics is based on the notion that it is appropri\u00adate to confuse the \nprocess that takes some input c to false and halts, with the process that diverges on input c. How\u00adever, \nseveral other coherent alternative notions for handling divergence can be modelled with minor variations \non the above theme. In this section we show briefly how to gen\u00aderate a model which distinguishes between \nfalse and div, and also how to generate a model which associates with each input the limit of fair execution \nsequences of the program on that input. In each case we sketch the major idea and leave a full development \nas an exercise for the reader. Distinguishing div from false. Suppose for each input to a process we \nobserve whether or not the process diverges, and if it does not, we observe the resultant store. Thus, \nthe denotation f of an agent A will be a partial function from [D1O to ID IO. What sort of function? \nobserve that if a determinate cc process engages in an infinite execution se\u00adquence in c, then it must \nalso engage in an infinite execution sequence in a store d > c. Therefore the domain of f will be downward-closed. \nHowever, as before, on this domain f will be a closure operator. This motivates the definition: Definition \n3.6 A partial closure operator on a lattice E is a closure operator on a downward-closed subset of E. \n0 As before, the range of a partial closure operator contains enough information to recover the function. \nIn particular, the domain of the function is just the downward closure of the range of the function. \nIn fact, the set of fixed points of a partial closure operator can be characterized quite simply as follows: \nFor any lattice E, a set S G E is the set of fixed points of a partial closure operator on E iff S is \nclosed under glbs of arbitrary non-emptg subsets. Thus, the added gener\u00adality arises merely from the \nfact that false is not required to be a fixed point of a partial closure operator! Note that the (range \nof the) partial closure operator cor\u00adresponding to div, the program that diverges on every input, is \njust 0, since the domain of the function is the empty set. On the other hand, the (range of the) partial \nclosure opera\u00adtor corresponding to false is {false}. Thus this semantics distinguishes between these \ntwo programs. As before, partial closure operators can be partially or\u00ad dered by the divergence ordering: \nf<guf~gcfudf where d~, the set of inputs on which ~ is undefined is just, the complement in IDIo of \nthe domain of f (i.e., lDlo\\ 1 f). Rather surprisingly, the definition of the combinators re\u00admains unchanged, \neven though the fmeaning (operational interpretation) of the denotation has changed: c*={d G{ Dlold>c} \nc+ A={d61Dlold>c+dc A} A,AAz={dEIDIoldeA,Ad~Az} 3XA = {d, G IDIo I~C ~ A.qxc = ~xd}  Each of these definitions \nyields a partial closure operator when its process arguments are partial closure operators. Each defines \na function that is monotone and continuous in its process arguments. Connections with the operational \nsemantics can be es\u00adtablished in a manner analogous to the connections estab\u00adlished above. A semantics \nbased on observing limits. The above seman\u00adtics treats a divergent computation as catastrophic it is \ntreated as the computation that causes the store to become inconsistent. As discussed earlier, it is \npossible to develop a different semantics, one in which limits of fair execution se\u00ad quences are observed. \nFor example, such a semantics would associate the cc/Kahn process: ones(X) :: ~Y X = a(l.h, Y) A ones(Y). \nwith the closure operator that maps true to the (limit) constraint that forces X to be the infinite seauence \nof 1s. . whereas the previous semantics would associate this pro\u00adgram with the partial closure operator \nthat diverges in true. First we need to define the notion of fair execution se\u00adquence. At any stage of \nthe computation there may be sev\u00aderal enabled transitions, each of which reduces one of the agent s subagents. \nNote that if a subagent can be reduced at a given stage of the computation, it can be reduced at every \nsuccessive stage of the computation until a transition is taken that actually carries out the reduction. \nWe say that a c-transition sequence s is jair if it eventually reduces every subagent that can be reduced \nat some stage of s. This is a common notion that one needs in defining the operational semantics of concurrent \nsvstems. In such a semantics, \\he denotation of a process asso\u00adciates with each input the limit of the \nsequence of store on any fair execution sequence of the process. Hence, the denotation is taken to be \nan operator over ID I (instead of over IDIO). As above, the denotation must be a closure oDerator but \nin addition. it seems reasonable to demand that no process can decide to produce some output after it \nhas examined an infinite amount of input. That is, we demand that ~ be continuous: for every directed \nS Q D: f(us) = Uf(s) (lo) In terms of fixed points, it is not hard to see that if S is the set of fixed \npoints of a closure operator f, then ~ is continuous iff S is closed under lubs of directed subsets. \nThe partial order on processes is now the extensional one: ~ ~ g iff $ ~ g. The bottom element in the \npartial order is Id (that is, ID[) (thus limits of chains are given by intersection) and the top element \nis {false], the operator which maps every element to false. Even more surprisingly, the definition of \ncombinators re\u00admains unchanged from the previous section, modulo the fact that fixed points must now \nbe taken from IDI instead of just lDlo: c+={dcIDlldzc} c-+A={dcIDlld~c+dcA} AIAAz={delDlldeAIAdeAz} 3XA \n= {d c IDI I~C c A.3xc = ~xd} Each of these combinators is seen to be we~-defined (they yield continuous \nclosure operators when their process argu\u00adments are continuous closure operators), and monotone and continuous \nin their process arguments. The following result follows from the commutativity prop\u00aderties of transitions. \nTheorem 3.11 If S1 and sz are both fair c-transition se\u00adquences for A, then uCons(sI) = UCons(sz), where \nCons(s) yields the set of constraints from s. This theorem allows us to define an observation function \non programs mapping IDIo to IDI by: (MS(P)(C) = I-Icons(s) for s any fair c-transition sequence for \nP. Relationship with the denotational semantics. This dis\u00adcussion is quite brief as it is quite similar \nto the previous discussion. The new issues one must to deal with are that transition sequences have to \nbe fair and the semantic do\u00admain has an entirely different order. Also, the operational semantic function \nis defined on the entire domain generated by the constraint system rather than just the finite elements. \nThe relevant theorems are as follows. Theorem 3.12 O(P), the continuous extension of Ohs(P) is a closwe \noperator on IDI. Theorem 3.13 O(P) = P(P). Theorem 3.14 (Full abstraction) P(P) = P(Q) ifl for all contezts \nC[e], Obs(CIP]) = Obs(CIQ]). 4 The nondeterminate language Let us now consider the determinate cc language \nin the previous section, together with (bounded) nondeterminate choice. Syntactically, admit as an agent \nexpressions of the form CI~AICIC2~A211. ..O cn+A. for finite constraints c~ and agents Ai, n z 1. Intuitively, \nin any store d, such an agent can in one uninterruptible step reduce to A, without affecting the store \nprovided that the ith branch is open , that is, d z ci. If no branch is open, the agent remains stuck, \nand if more than one branch is open, then any one can be chosen. Thus the axiom for dependent choice \nsatisfied by the -relation is: With this construct admitted into the language, the de\u00adnotation of an \nagent can no longer be a function from ]DIo to IDIo. Neither can it be just a relation in IDIo x ]Dlo, \nsince parallel composition will not be definable. Instead we model a process as a set of ~aihmes, which \nrecord the in\u00adteractions that a process engages in with the environment before reaching a state ( resting \npoint ) in which it cannot progress without intervention by the environment. This sim\u00adple idea turns \nout to be adequate to give us a denotational semantics which is fully abstract with respect to a notion \nof observation that includes observation of divergence and the final (quiescent ) stores of an execution \nsequence. The rest of this section is devoted to giving an expo\u00adsition of this model. Because of the \nnature of constraint\u00adbased communication, it turns out to be very convenient to model failures as certain \nkinds of closure operators, namely, bounded trace operators. In the next subsection we treat some of \nthe basic ideas underlying bounded trace operators, before turning to a presentation of the model. 4.1 \nThe basic model Trace operators. In general (provided that that the un\u00adderlying constraint system is \nexpressive enough, see Sec\u00adtion .3), a finite closure operator can be represented as the parallel composition \nof a finite set of finite sequences of asks and tells, where a sequence al !bl * . . . an !b~ * (called \na trace) is thought of as representing the closure op erator a,~(b, A(az+b,... (am+ b~)...).]5 A trace \noperator over a finitary lattice E is, intuitively, a closure operator that can be represented by a single \n(possi\u00adbly infinite) ask/tell sequence. The characterizing property of a trace operator ~ is that if \nS ~ E is a set of elements none of which are fixed points of ~, then neither is nS (provided that it \nexists): Definition 4.1 A trace operator over a finitary lattice E is a closure operator f over E such \nthat for any S ~ E, if S is disjoint from ~, then ilS @ ~ (whenever flS is defined). Let T(E) be the \nset of all trace operators over E. D Intuitively this definition can be justified as follows. Let d be \nan arbitrary element in S, and suppose that t is a trace. Then, if d is not a fixed point oft, itshould \nbe possible for t to execute some prefix of its actions, including at least one tell action involving \na constraint stronger than d, before quiescing. Similarly for any other e c S. Let s be the smallest \nprefix executed by tin eor d. dmewill be > all the asks in s, so t will be able to execute all of s, \ninclu~ing a tell involvirw a constraint strorwer than d n e. The chara~teristic condition o; a trace \noperator can be stated much more elegantly as follows. For j a closure op\u00ad erator over a lattice E, define \n~-], the inverse of f to be the set of elements (E\\ ~) u { TE}. Lemma 4.1 A closure operator f : E + \nE is a trace oper\u00ad ator iff f-l is a closure operator. If f is a trace operator, then so is f 1. f-l \nis. s~~ to be the inverse of f because it is the weak\u00adest g satmfymg f A g = f n g = {T E}. Intuitively, \n~-1 is exactly the sequence of asks and tells that unzips .f: it asks exactly what f tells and tells \nexactly what ~ asks. Consequently, on any input to j A f 1, both the sequences can be traversed completely, \nyielding the final answer T E. Thus trace operators can be thought of as invertible closure operators. \nConversely, it is possible to show that each trace opera\u00adtor can be represented canonically as a sequence \nof ask/tell actions: 15RecaII that for c c E and / a closure operator on e, c + f is the closure operator \non E with fixed points {d G E I d ~ c * d C -f}. Lemma 4.2 Every trace operator f : E --+ E can be rep\u00adresented \nby an alternating, strictly increasing sequence of ask/tell actions. The basic idea behind the construction \nof the canonical sequence is quite simple. Let ~ be a trace operator and g = f-l. Then the canonical \ntrace corresponding to ~ is just: f(tr-ue) * g(f(tr-ue))!f(g( f(true))) * . . The following lemma is \nnot difficult to show: Lemma 4.3 Let D = (D, k, Var, {3x1X ~ Var}) be a cylin\u00addric constraint system. \nFor every c c ]Dlo, Y 6 Jar and f c T(IDIo), C, c -+ ~,c A j, 3~f ~ ?_(/DIo), where c is the closure \noperator {d E IDIo I d ~ c}. Thus trace operators are closed under almost all the op\u00aderations of interest \nto us except, naturally enough, arbi\u00adtrary parallel composition. Bounded trace operators. The failures \nof a process record a resting point of the process, together with information about how to get there. \nSo it would seem as if a failure should be represented as a pair (f, c) where c E ~ is the resting point, \nand f is a trace operator describing the set of ask/tell interactions needed to reach c. Note however, \nthat the only information of interest in f is its behavior on j c. But if c E f, then fn J c is also \na trace operator but on the sub-lattice 1 c. Therefore, a bounded trace operator (or bto, for short) \non a finitary lattice E is defined to be a trace operator on J c, for some c ~ E, This makes bounded \ntrace operators a special kind of partial trace operators specifically, those whose range contains a \nmaximal element. (Partial trace operators are just the partial closure operators of Section 3 that are \nin addition traces.) Let bT(E) = uceET(l C) denote the set of all such operators. Just like any other \n(partial) trace operator, a bto .f is also representable by its range, and its domain of definition is \njust J ~, where j (read max P) is the greatest fixed point of ~. Various operations defined on closure \noperators are applicable to btos, with obvious adjustments. Thus, for any constraint c, we shall take \nthe bto corresponding to the imposition of c to be just the bto (whose set of fixed points are) {c}. \nSimilarly, for a constraint c and trace operator f, withj~c, the btoc+f is just the bto{d<~[d~c+ d ~ \nf}. However, some additional operations are also of interest over btos. We next discuss operations that \nreflect the opera\u00ad tional notion of extending a sequence of aak/tell interactions with more ask/tell \nactions. Let f be a (finite) bto, with canonical sequence of ask/tell actions s, and let c ~ ~ be any \nconstraint. Define f.ck (read: f output extended by c ) to be the bto corresponding to the sequence of \nactions s.c*. It is not hard to find a dkect represent ation of f. C* in terms of f and c: 16 f.c*={d \n<cldfli E.f, dZj}U{c} In the following, we shall assume that the expression f .c* is well-defined even \nif c ~ j, and take it to stand for f in such cases. Note that f.c + .d* = f.d+, if c s d. 16The eXPre~8ion \nd n j ~ f should be taken to stand for d n ~ is contained in ~, provided that it exists . Similarly, \nwe can define the notion of input extending a bto ~ with a constraint c by: As above, note that ~.c!. \nd! = ~.d!, if c < d. Given the definitions of input-and output-extensions, it is not hard to see that \nfor any sequence of ask/tell actions s=e~e~. . . en, the corresponding closure operator is just (.. . \n(({true} .e~).ez) . . .).e~ (where we have abused notation by writing $.e for the expression ~.c! in \ncase e ~ c! and for the expression ~.c* in case e = c*). Let us write ~ L g for the case in which ~ can \nbe thought of as a prefix of g, that is, g can be thought of as extending the sequence of interactions \nwith the environment engaged in by ~. How can this partial order be expressed directly in terms of (the \nset of fixed-points of) ~ and g? Clearly, none of the additional interactions in g can cause g to take \non a different value from ~ at all points in f s domain (J ~), except possibly at ~.17 Therefore, we \ncan define: As can be verified from the definition, ~ is a partial order. Finally, one more partial order \nwill be of interest in what follows. We say that ~ asks more than g (and write f ~ g) if the resting \npoint of both ~ and g are identical, but f records more contributions from the environment than g. This \nhappens just in cas~ ij = ~ and Vz c1 ij.~(z) s g(z), that is, just in case j = j and ~z g.  4.2 The \nmodel Let the set of all observations, OtM(l DIO), be the set of finite, bounded trace operators on ID \n10. A process will be a subset of Obs satisfying certain conditions which we now motivate, following \n[Jos90] closely. At any stage of the computation, a process will have engaged in some ask/tell interactions \nwith the store. Subse\u00adquently, it may produce some output and then quiesce (per\u00adhaps to be activated \non more input at a subsequent stage) or it may engage in an infinite sequence of actions, with\u00adout requiring \nany input from the environment to progress (perhaps producing more and more output as it progresses). \nWe will model a process as divergent if it can quiesce in infinitely many ways (or output forever) or \nif it causes the 18 (Thus we are reqUiring that store to become inconsistent. processes be finitely nondetermininate.) \nIf F is the set of failures of such a process, its divergences can then be defined as: dF = {-f I {c \nI f.c* G J?} is infinite} u{f \\ f. false* E F} Each of these situations is considered undesirable, and \nwe are not concerned about detailed modelling of the process 17For an example of = closure operator g \nwhich extends f but takes on a different value at f-than j, consider the closure operators ob. tained \nfrom the sequences a!&#38;k and a!b * c*, for a < b < c, 18&#38; di~cus~~d in the introduction, it is \nquite reasonable in this set-up to regard the process that produces the inconsistent store as divergent. \nIt is possible to give a minor variation of the current treat\u00adment which distinguishes the process that \ndiverges from the process that tells false, but this is outside the scope of this paper. once it has \nbecome divergent. Thus such a process is treated as chaotic , aa being able to exhibit any behavior whatso\u00adever. \nFurther, we require that the set of possible behaviors of a process cent sin aii its possible behaviors, \nespecially its diverging ones. Thus the first condition we impose on a process F is: where for any S~ \nOhs, eS isthe set {s ~ tItES} of extensions of S. Note that ~.c* E dF implies ~ c dF. Thus the last action \nin a sequence of ask/tell interactions constituting a minimal divergence must be an ask action. In other \nwords, a divergence characterizes those inputs from the environment that are undesirable, that can cause \nthe process to break. From the definition, it should be clear that a! distributes through finite unions \nand arbitrary intersections of arbitrary sets of observations. Also, the divergences of a process can \nbe characterized rather nicely: Lemma 4.4 fEdFuvg2f.g~F-f.false* EF The next few conditions are best \nmotivated by consid\u00adering the traces of a process. A trace of a process is just a sequence of aak/tell \ninteractions that the process many engage in (without necessarily reaching a quiescent state), But the \ntraces of a process can be recovered in a simple way from its failures: they are just the observations \nwhich can be ontput-extended to obtain a failure: Clearly, F G tF and tdistributes through arbitrary \nunions of sets of observations. We require that if a process exhibits a trace, then it should be possible \nfor it to exhilit a prefix of the trace as well-this is inherent in the very idea of a trace: We also \nrequire that every process should have some be\u00adhaviors, hence a non-empty set of failures. Given Condi\u00adtion \n13, this is equivalent to stating that the empty bto true! = true* = {true} be a trace of every process: \n{true} E F (14) Since cc processes are asynchronous, the environment can never be prevented from adding \nconstraints to the store. Therefore, it should be possible to extend every sequence of interactions that \na process may have with its environment with an input action: (the receptiveness condition): fGtF*f.c!~tF \n(15) It is not hard to show that for any chain of processes F1 s Fz. . . . t n,>l F~ = ni21tF,. We req~lre \none final condition on processes. If a process can engage in a sequence of actions recorded by a bto \nf before quiescing, then it can engage in the same sequence of actions even if at some or all stages \nthe environment were to supply more input than the minimum required by the process to engage in ~. Thus \nwe require that the failures of a process be closed under the ask more relationship: gSfeF+g~F (16) \nIn essence, this condition represents the monotonic na\u00adture of the basic ask and tell actions. Now we \nare ready to define: Definition 4.2 A (nondeterminate) process is a subset of OZW(IDIO) satisfying Conditions \n12 16. Let NProc be the set of all such subsets. 0 The following lemma establishes that the convergence \nof a process already contain enough information to generate its divergences. (The converse is not true. \n) For F a process, define iF, the input extensions of F to be the set {~.c! I f ~ F}, and cF, the convergence \nof F to be the set F \\ dF. Lemma 4,5 dF = e((icF U {{true}}) \\ tcF) Essentially, any input extension \nof a convergent trace of a process must have an output extension that is a failure of the process (Condition \n15); if this output extension is not a convergence, it must be a divergence and so must its extensions. \nConversely, a divergence of a process must have a prefix which input-extends a convergence and is not \nitself a convergent trace; otherwise the process is chaotic, and every bto is a divergence. Partial order \non processes. Usually, processes in specification\u00adorient ed semantics are ordered by the so-called no \nndeterm in\u00adisrn ordering: FLG~FzG which corresponds to the intuition that a process is better than another \nif it is more deterministic. The completely undefined process is the chaotic process, which can exhibit \nall possible behaviors: as more and more information about a process is generated, more and more behaviors \nget ruled out . However, in many senses, this ordering is more liberal than desired, as discussed by \nRoscoe in [Ros88]. For exam\u00adple, one way in which a process G can improve a process F is by dropping \nsome convergent behavior of F, This sort of capability is not manifested by any cc combinator (or, indeed, \nany CSP combinator), and we find it more conve\u00adnient to adopt instead the divergence ordering proposed \nby Roscoe. In this ordering G is better than F iff it diverges at fewer places than F, and the convergent \nbehaviors of F are preserved in G. More precisely, the partial order is: F<GwcF~cG~F It is easy to see \nfrom the definition that F < G implies F E G. Furthermore, the least element in the partial order ~ OIM, \nand limits of increasing chains are given by intersection. In fact, ifFl <F2 <,,. is an increasing chain \nwith lub F = ni21Fi, we have CF = Ui21cFi and dF = na21dFj. Theorem 4.6 (NProc, <) is a complete partial \norder. Syntax. P::=D.A D::=, I P(X) :: A I D.D A::=c IA AA13XAIP(X)IC1+A1 CI... lJCj+Aj Auxiliary Definitions. \ndF = {f I {c I j.ti ~ F} is infinite} U{~ I ~.fals% ~ F} tF = {f I~C.j.ti c F} FllG={fng cOh. li=ti, \njEF, g6G} X-F= {g G Obs I g <Jdn @X( f.false!)),3xd = 3xi,f c F} 3x(dxYUF) = {{ax(dxyuc) I c~ f} I f \n~ F} Semantic Equations. .4(c)e= {jlfn~c~j,c<j} u{g~flfn tc~f, cuj= false} A(DJcJ(cJ + Aj))e= {f6A(Aj)elf=cj+f,f?cj,~e~} \n u{jdlvjc.l. d~cj} A(A A l?). = A(A)e11A(J3)e U ed(L4(A)e[ltA(B)e) A(3XA)e = (X d(A)e) u {g < f I f c \n4x A(A)e)} A(p(X))e = 3~(dex U e(~)) S(e)e = e fi(p(X) :: A.D) = C(D)e[p w ~x(dax uA(A)e)] P(D.A) = .4(A)( \n&#38; S(D)) Above, c ranges over basic constraints, that is, finite sets of to\u00adkens. P is the syntactic \nclass of programs, D is the syntactic class of sequences of procedure de&#38;rations, and A is the syn\u00adtactic \nclass of agents. a is some variable in the underlying con\u00adstraint system which is not allowed to occur \nin user programs. (It is used as a dummy variable during parameter passing. ) e maps procedure names \nto processes, providing an environment for in\u00adterpreting procedure calls. We use the notation c U j to \nstand for {cudld~j}. Table 2: Denotational semantics for the Ask-and-Tell non\u00addeterminate cc languages \n 4.3 Process algebra As can be calculated, the divergences and traces of F are: In this section, we define \nvarious processes in and combina\u00adtors on NProc, including div, the process that immediately diverges, \nthe tell (of finite constraints), parallel composi\u00adtion, nondeterminate choice and hiding combinators. \nThe syntax of the programming language is given in Table 2, where the semantic definitions, to be discussed \nbelow, are also summarized. As before, we also simultaneously define the operational semantics of the \nlanguage and assume that procedures take exactly one variable as a parameter and that no program calls \nan undefined procedure. Chaos. The chaotic process can do anything whatsoever. div= Obs Clearly, d(div) \n= t(div) = Ohs. Operationally, such an agent is always willing to progress in any store. This progress \naffects neither thestore nor the agent s subsequent behavior: div (~) div (17) e Tells. Consider a process \nwhich immediately terminates, after augmenting the store with some constraint c E E. Let us call such \na process c. The resting points of such a process are clearly all stores e z c. To reach this resting \npoint, the process can at most add cto the store. It is not hard to see that a bto ~ satisfies the condition \nthat for all inputs z in its domain, ~(x) s xu ciff ~m TcG ~. When does such a process diverge? It must \ndiverge iff it can engage in some sequence of interactions with the store (in which its output is bounded \nby c), after which it reaches a state in which if it were to output c, it would reach false: C= {flfn~c~f,f>c} \nUe{~[~llTc~$,~Uc=false} It is easy to work through the definitions and establish that ri(c)=e{~l ~ntc~~, \n~l-lc=false} t(c)={~l~ntc~$}ud(c) and that c (as defined above) is a process. The relevant axiom for \nthe transition relation for these agents is the same as in the determinate case (Axiom 4). Dependent \nchoice. Consider the process F = o ~c~(cj + I j). It has two kinds of resting points: first, the resting \npoints d that arise because for no j s J is d > Cj, and secondly, the resting points of each F3 which \nare stronger than the corresponding C3. Furthermore, the btos generating the first kind of resting point \nare simple: they are of the form d! =J. d, since no output is produced by the process before it quiesces \nin d. On the other hand, the path followed by F in reaching a resting point of Fj stronger than Cj is \nthe path that F3 would have followed given that the environment is willing to supply at least C3, that \nis paths f E Fj such that ~ = Cj + ~. This leads us to the definition: CljeJ(cj~Fj)= {f~Fjlf =c, +f, \nc,< f,j~J} u{~d lVj 6 l.d~cj} d(uje~(cj + F,)) = {~cdFjli=c]+f,cj<;,~cJ} t( H je.r(cj + Fj)) = {fctFil \nf=cj+f, cjS$, jEJ} u{Jd Ivj G 7.dz Cj} The combinator is monotone and continuous in each of its process \narguments. Two special cases of this operator are worth singling out. In case the index-set is singleton, \ndependent choice is not a form of choice at all and reduces to just the ask-combinator. That is, c --+ \nA is just dependent choice in which only one conditional is given. In terms of denotations, we get: c-+ \nF={ f~Flf=c+f, c<f} u{ldld~c} Note that for such agents, the transition Axiom II reduces to just Axiom \n5 (Section 3). Similarly, blind unconditional choice can also be expressed. Consider the binary combinator \nn defined such that F n G can behave either like F or like G. The decision can be made arbitrarily, even \nat compile-time. Thus the failures of F n G should be precisely the failures of F or the faihres of G. \nAs can easily be checked, F uG= true -+ F otrue ~ G as well. Therefore, FnG can be defined as true * \nF u true - G. Clearlv. blind choice is idempotent, associative and com\u00ad .. mutative and has div as a \nzero element. Operationally, an agent built from blind choice satisfies the axioms: (18) Parallel composition. \nWhat are the resting points of FAG ? Clearly, if c is a resting point of F and o~ G, then it is a resting \npoint of F A G. The path followed to this resting point by F A G can be any parallel composition of the \npaths followed by F and by G. Therefore, each failure in the set F IIG is going to be a failure of F \nA G, where FllG={fng cObslf=ij, fcF, geG} But what are the divergences of F A G? The divergences of \nF A G arise not only from the divergences of F and of G, but also from the possibility that the two agents \nmay engage in an infinite sequence of interactions with each an\u00adswering the others asks, without demanding \ninput from the environment at any stage. There will be no bto in Fll G cor\u00adresponding to such mutual \nfeed-back because there is no common resting-point on this execution branch. Capturing these possibilities \nfor a cc language built over an arbitrary constraint system seems rather subtle. A sim\u00adple formulation \nis possible, however, for finitary constraint systems, that is, constraint systems in which a finite \nelement dominates only finitely many finite elements. In this case we can show: Lemma 4.7 If D is a \njinitary constraint system, then for every F E NProc(D), dF = dtF. This suggests that to determine the \ndivergences of F A G, it is sufficient to determine the divergences of the traces of F A G. But this \nis easy: a trace of F A G is just a trace of F running in parallel with a trace of G, and hence the divergent \ntraces are just ed(tFl[tG). We thus get: F A G = FIIG U ed(t(F)l[t(G)) From these we can caculate: d(F \nA G) = ed(tFlltG) t(F A G) = (tFlltG) U ed(tFlltG) Proving continuity of this operator requires some \ncare. The basic issue is to show that ed(tFlltG) is continuous in its arguments. The operational transition \nrule for agents built with A is the same aa in the deterministic case (Axiom 6). Projection. d is a resting \npoint of %YF iff when F is ini\u00adtiated in (=xd), itreaches a resting point e such that the only new information \nin e (over d) is on X; that is, such that (3xe) = (Sxd). Therefore d is a resting point of 3XF iff there \nis an j ~ F such that 3x~ = 3Xd. The route taken by (3XF) to reach d from true must be the route prescribed \nby 3Xg, where g is obtained from ~ by extending it to be a closure operator over ID IO, restricted to \nJ.d. Thus define: X-F= {g c Obs I g Sldn (3X(~.false!)),3xd = 3x~,~ c F} Now the failures of 3XF are: \n3XF=X FU {g< flfeed(X-F)} The divergences and traces for this process can be shown to be: d(3XF) = {g \n< f I f~ed(X-F)) t@XF)=X tFu{g ~~1~< d(X F)) This operator is monotone and continuous in its argument. \nThe transition relation for 3XA is the same as in the de\u00adterminate case; the transition relation must \ntherefore be ex\u00adtended to agents with an internal store. Recursion. Recursion is handled in the usual \nway, by tak\u00ading limits of the denotations of all syntactic approximants, since the underlying domain \nof processes is a cpo and all the combinators are continuous in their process arguments. The diagonal \nelements are used to effect parameter passing (see Table 2). Operationally, procedure calls are handled \nas in the determinate case. 4.4 Operational Semantics The operational semantics associates with every \nprogram and every initial store the set of all possible outputs ob\u00adtainable from the store, with the \ncaveat that if the process diverges or produces false, then every output is deemed ob\u00adservable. Here \nwe use the notation P has a (c, d)-sequence to mean P has a terminal c-transition sequence with final \nstore d. Definition 4.3 For any program P define Relationship with the denotational semantics. We make \nthe connection between the operational semantics and the denotational semantics via the following theorems. \nThe proofs are omitted in this version. Theorem 4.8 (Adequacy) O(P) = {d I {d} P(P)} That is, the results \nobtained by executing a program are identical to the resting points of the program obtained from the \nstore true. Note that thk is a weaker correspon\u00addence than in the determinate caae, when the operational \nsemantics was identical to the denotational semantics. The following fulI abstraction proof uses the \nnotion of context previously defined for determinate programs. Theorem 4.9 (Full abstraction) P(P) = \nP(Q) @ for a/J contexts C[e], O(CIP]) = O(CIQ]). If the denotations of two programs P and Q are differ\u00adent, \nthen there will be a <-maximal bto ~ in one and not, in the other. It can be shown that the sequence \ns corre\u00adsponding to such an ~ can be expressed in the language, which implies that the sequence s-l corresponding \nto $-1 can also be expressed in the language. But then the finite bto s l A is a context which distinguishes \nthe two pro\u00adgrams. Let F = P(P), G = P(Q). Assume without loss of generality that ,f c F and ~ # G. There \nare two cases: f EdF and f EcF. Iff EdF then s-l AF will diverge, and O(s l A F) = IDIO. f @ G implies \nS-l A G will not diverge, and therefore false @ O(s-l A G). If ~ = cF, then ~ < O(s-l AF). If ~ 6 O(s- \nAG) then ~ must be a convergence of G, which violates the assumption. 4.5 Relationship between the nondeterminate \nand deter\u00adminate semantics We have only one set of transition rules for the determinate combinators, \nand the same notion of observation for the determinate and nondeterminate semantics. Therefore, the operational \nsemantics for the determinate language and the determinate subset of the nondeterminate language are \nthe same. Because both the determinate (AD ) and nondeter\u00adminate (A.N) denot ational semantics are fully \nabstract with respect to the corresponding operational semantics, there should be some relationship between \nAjv (P) and AD (P). Consider the two determinate agent-equivalence classes CD and CN induced by AD and \nAN, respectively. Because the nondeterminate language has more contexts with which to tell apart agents \nthan the determinate language, CD should be a coarsening of CN, and we should therefore be able to re\u00adcover \na determinate program s determinate denotation from its nondeterminate denotation. Definition 4.4 An \nelement F E NProc is determinate if 1.f ECF1F cNProc and c> ~implies f.ck @tF 2. f 6cF, gEtF and j = \n~implies gccF.  Let DNProc be the subset of determinate processes of NProc. n Henceforth when we say \ndeterminate, we mean that we O(P) = Obs if P has an infinite true-sequence have an agent in NProc that \nsatisfies the determina.y condi-Obs if P has a (true, ~atse)-sequence tion and not an element of the \nsyntactic class of determinate { {d I P has a (true, d)-sequence }otherwise processes. We, of course, \nwould like the denotation of agents D built from the determinate combinators to be determinate: Theorem \n4.10 AN(A) is determinate if A is constructed using the determinate combinators. We are now ready to \ndefine DN, which associates with F c DNProc a corresponding element in DProc, the domain used for the \ndenotational semantics of determinate agents in Section 3. Definition 4.5 DN(F) = {~ I f c cF} U {false}, \nfor F S DNProc. 1 Theorem 4.11 DN(AN(A)) = AD(A) for all agents A constructed from the determinate operators. \nWe prove this theorem by first showing that DN(AN(A)) is a closure operator. We then show by structural \ninduction that the theorem holds for finite agents built using the deter\u00adminate combinators. We prove \nthe theorem for recursively defined agents by showing that DN is monotone and con\u00adtinuous. 5 Conclusion \nand Future work This paper presents a comprehensive treatment of the specifi\u00adcation-oriented approach \nto the semantics of programs writ\u00adten in concurrent constraint programming languages. This treatment \nincludes programs built using recursion. By for\u00admalizing a general notion of constraint system, we cleanly \nseparate the semantics of the programming language com\u00adbinators from the semantics of the underlying \ndata domain. This separation allows us to uniformly address the semantics of a wide variety of concurrent \nconstraint programming lan\u00ad guages with a single generaJ framework. These languages include, among others, \nthe concurrent logic programming languages and Kahn data-flow net works. Our work brings into sharp focus \nthe seamntic complex\u00adity caused by having nondeterminacy in the cc languages. The determinate semantics \nneed only record the stores at which a process quiesces there is no need to maintain any intermediate \ne process state information. The nondetermi\u00adnate semantics, on the other hand, must record both the stores \nin which a process may quiesce, and, for each such store, the possibIe computation paths to that store. \nIt is in\u00adteresting that finitariness plays a key role in the determinate semantics but not in the nondet \nerminat e semantics. We make the connection between the determinate se\u00admantics and the nondeterminate \nsemantics by defining an operator that extracts the determinate denotation of a pro\u00adgram built with the \ndeterminate combinat,ors from its non\u00addeterminate denotation. We also present an equational ax\u00adiomatization \nthat is complete for finite programs built with the determinate combinators. This paper also presents \na single transition system for both the determinate and nondeterminate languages. This transition system \nuses diagonal elements and local stores to eliminate messy variable renaming operations. There are many \ndirections for future research. These include foundational concerns, such as are addressed here, implementation \nissues and applications. We intend to pur\u00adsue all these issues in the coming months. In this section \nwe only mention the semantic issues. We have by no means exhausted the range of interesting combinators \nthat are available in the determinate cc lan\u00adguages. For example, the glb operator on agents is also \navailable, and provides a sort of determinant e disjunction . Some of these operators will be treated \nin the complete ver\u00ad sion of this paper. A useful line of investigation is to try to characterize all \nsensible combinators that one may use. Here general results from category theory may help. There are \na variety of different semantics correspond\u00ading to different notions of observations. We would like to \ndevelop a semantics for the indeterminate case that is not based on viewing divergence as chaos. This \nwould be like Plotkin s powerdomain treatment of indeterminate impera\u00adtive languages [P1076]. In subsequent \nwork we plan to de\u00advelop proof systems for safety and liveness properties of cc programs based on these \nmodels. In a related paper we are developing the closely related safety model and an axioma\u00adtization \nof equaJit y for it. We also believe that it is possible to develop a theory of higher-order determinate \ncc programming languages. There are interesting connections to be made with other theories of higher-order \nconcurrent processes [BB90, JP90, Mi190] and alSO with classical linear logic. It appears that concurrent \nconstraint languages may be related to the proof nets in\u00adtroduced by Girard in his discussion of the \nproof theory of linear logic. If this connection were successful it would ex\u00adhibit concurrent constraint \nprograms as arising from linear logic via a Curry-Howard isomorphism. Acknowledgements This research \nwas supported in part by DARPA contract NOO014-87-K-0828, NSF grant CCR\u00ad8818979 to Cornell University \nand an NSERC grant to McGill University. We gratefully acknowledge discussions with Seif Haridi, Tony \nHoare, Radha Jagadeesan, Mark Josephs, Ken Kahn, John Lamping, Keshav Pingali and Gordon Plotkin. The \ndebt our treatment owes to Mark s development of re\u00adceptive processes should be clear to anyone who has \nread his paper. None of them should be held responsible for any remaining errors. References [ANP89] \nArvind, Rlshiyur Nikhil, and Keshav K. Pin\u00adgali. I-structuers: dat~structures for parallel computing. \nACM Transactions on Principles of Programming Languages, 11(4):598 632, Octo\u00adber 1989. [BB90] G. Boudol \nand G. Berry. The chemical abstract machine. In Proceedings of the 17th Annual ACM Symposium on Principles \nof Programming Languages, pages 81-94. ACM, 1990. [CM88] Mani Chandy and Jay Misra. Parallel Program \nDesign A foundation. Addison Wesley, 1988. [dBP90a] F, S. de Boer and C. Palamidessi. A fully ab\u00adstract \nmodel for concurrent constraint logic lan\u00adguages. In Proceedings of CONCUR 90, 1990. [dBP90b] F. S. de \nBoer and C. Palamidessi. A fully ab\u00adstract model for concurrent constraint program\u00adming. June 4 1990. \n[DL86] Doug DeGroot and Gary Lindstrom, editors. Logic Programming: Functions, Relations and Equations. \nPrentice Hall, 1986. [FT89] Ian Foster and Steve Taylor. Strand: New con\u00adcepts in parallel programming. \nPrentice Hall, 1989. [GCLS88] Rob Gerth, Mike and Ehud Shapiro. semantics for Flat 88, 1988. Codish, \nYossi Liechtenstein, A fully abstract denotational Concurrent Prolog. In LICS [Gir87] J.-Y. Girard. Linear \nlogic. Science, 50:1-102, 1987. Theoreticcd Computer [Gir89] J.-Y. Girard. Proofs and Types, volume 7 \nof Cambridge tracts in Theoretical C ornputer Sci\u00adence. Cambridge University Press, 1989. Trans\u00adlated \nand with appendices by Y. Lafont and P. Taylor. [GKK+80] G. Gierz, K. H. Hoffman, K. Keimel, J. D. Law50n, \nM. Mislove, and D. S. Scott, editors. A COnl\u00adpendiwn of continuous lattices. Springer-Verlag Berlin Heidelberg \nNew York, 1980. [GL90] M. Gabbrielli and G. Levi. Unfolding and fix\u00adpoint semantics of concurrent constraint \nlogic programs, Technical report, University of Piss, 1990. [GMS89] Haim Gaifman, Michael J. Maher, and \nEhud Shapiro. Reactive behavior semantics for concur\u00adrent constraint logic programs. In North A mer-i\u00adcan \nLogic Programming Conference. MIT Press, October 1989. [HMT71] Leon Henkin, Tarski. Cylindric land Publishing \nJ. Donald Algebras Company, Monk, (Part I). 1971. and Alfred North Hol\u00ad [Hoa89] C.A. R. Hoare. A currency. \nOxford theory PRG, of conjunction May 1989. and con\u00ad [Hoa90] C.A.R. ings Of Hoare. Let s make models. \nCONCUR 90, August 1990. In Proceed\u00ad [.JL87] Joxan Jaffar and Jean-Louis Lassez. Con\u00adstraint logic programming. \nIn Proceedings o~ the SIGA CT-SIGPLAN Symposium on Princi. pies of Programming Languages, pages 111-119, \nACM, January 1987. [Jos90] Mark B. Josephs. Receptive process the\u00adory. Technical report, Programming \nResearch Group, Oxford University, July 1990. [JP90] R. Jagadeesan and P. Panangaden. A domain\u00adtheoretic \nmodel of a higher-order process cal-CUIUS. In M. S. Paterson, editor, The Seven\u00adteenth International \nColloquium On Automata Languages And Programming, pages 181-194. Springer-Verlag, 1990. Lecture Notes \nIn Com\u00adputer Science 443. [JPP89] R. Jagadeesan, P. Panangaden, and K. Pingali. A fully abstract semantics \nfor a functional lan\u00adguage with logic variables. In Proceedings Of IEEE Symposium on Logic in Computer \nScience, pages 294 303, 1989. [Kah74] G. Kahn. The semantics of a simple language for parallel programming. \nIn J.L. Rosenfeld, editor, Proceedings of IFIP Congress 7J, pages 471\u00ad475., August 1974. [KYSK88] [Lev88] \n[Lin85] [LS86] [Mah87] [Mi190] [MPW89] [MR97] [OH86] [P1076] [ROS88] [Sar85] [Sar88] [Sar89] [SC076] \nS. Kliger, E. Yardeni, E. Shapiro, and K, Kahn, The language fcp(:,?). In Conference on Fifth Generation \nComputer Systems, December 1988. Giorgio Levi. Models, unfolding rules and fix\u00adpoint semantics. In Proceedings \nof the Fifth In\u00adternational Conference and Symposium on Logic Programming, Seattle, pages 1649-1665, \nAugust 1988. Gary Lindstrom. Functional programming and the logical variable. In Proceedings of the Twelfth \nACM Symposium on Principles of Programming Languages, pages 266-280, January 1985. J. Lambek and P. Scott. \nAn introduction to higher-order categorical logic, volume 7 of Stud\u00adies in Advanced Mathematics. Cambridge \nUni\u00adversity Press, 1986. Michael Maher. Logic semantics for a class of committed-choice programs. In \n4 th In tern a\u00adtional Conference on Logic Programming. MIT Press, May 1987, R. Milner. Functions as processes. \nIn M. S. Paterson, editor, The Seventeenth Interna\u00adtional Colloquium On Automata Languages And Programming, \npages 167-180. Springer-Verlag, 1990. Lecture Notes In Computer Science 443. R. Milner, J. G. Parrow, \nand D. J. Walker. A cal\u00adculus for mobile processes. LFCS Report EC S\u00adLFCS-89-85, University of Edinburgh, \n1989. M. Makkai and G. Reyes. First order cutegoricai logic, volume 611 of Lecture Notes in Mathemat\u00adics. \nSpringer-Verlag, 197. E.-R. Olderog and C.A.R. Hoare. Specification\u00ad oriented semantics for communicating \nprocesses. Acts Informatica, 23:9-66, 1986. G.D. Plotkin. A powerdomain construction. SIAM J. of Computing, \n5(3):452-487, Septem\u00adber 1976. A. W. R.oscoe. An alternative order for the fail\u00adures model. Technical \nReport Technical Mono\u00adgraph PRG-67, Programming Research Group, Oxford University, July 1988.  Vijay \nA. Saraswat. Partial correctness seman\u00adtics for cp($,l, &#38;), In Proceedings of the FSTTCS Conference, \nnumber 206, pages 347\u00ad 368. Springer-Verlag, December 1985. ViJay A. Saraswat. A somewhat logical formula\u00adtion \nof CLP synchronization primitives. In Pro\u00adceedings of LP 88. MIT Press, August 1988. Vijay A. Saraswat. \nConcurrent Constraint Pro\u00adgramming Languages. PhD thesis, Carnegie-Mellon University, January 1989. To \nappear, Doctoral Dissertation Award and Logic Pro\u00ad gramming Series, MIT Press, 1990. Dana S. Scott. Data \ntypes as lattices. SIAM, 5(3):522-587, 1976, [SC082] Dana S. Scott. Domains for denotational seman\u00ad \ntics. In Proceedings of ICALP, 1982. [SKL90] Vijay A. Saraswat, Ken Kahn, and Jacob Levy. Jan us: A step \ntowards distributed constraint programming. In Proceedings of the North American Conference on Logic \nProgramming, October 1990. [SPRng] Vijay A. Saraswat, Prakash Panangaden, and Martin Rinard. What is \na constraint? Tech\u00adnical report, Xerox PARC, forthcoming. [SR90] Vijay A. Sara.swat and Martin Rinard. \nCon\u00adcurrent constraint programming. In Proceedings of Seventeenth ACM Symposium on Principles of Programming \nLanguages, San Fransisco, Jan\u00aduary 1990. [Tar56] A. Tarski. Logics, semantics and meta\u00admathematics. Oxford \nUniversity Press, 1956. Translated by J.H. Woodger.  \n\t\t\t", "proc_id": "99583", "abstract": "", "authors": [{"name": "Vijay A. Saraswat", "author_profile_id": "81100152268", "affiliation": "Xerox PARC", "person_id": "P291132", "email_address": "", "orcid_id": ""}, {"name": "Martin Rinard", "author_profile_id": "81100087275", "affiliation": "Stanford University", "person_id": "P192534", "email_address": "", "orcid_id": ""}, {"name": "Prakash Panangaden", "author_profile_id": "81100010719", "affiliation": "McGill University", "person_id": "PP39023096", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/99583.99627", "year": "1991", "article_id": "99627", "conference": "POPL", "title": "The semantic foundations of concurrent constraint programming", "url": "http://dl.acm.org/citation.cfm?id=99627"}