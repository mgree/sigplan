{"article_publication_date": "01-03-1991", "fulltext": "\n Program Optimization and Parallelization Using Idioms Shlomit S. Pintert Ron Y. Pinter * Dept. of Electrical \nEngineering IBM Scientific Center Technion Israel Institute of Technology Technion city Haifa 32000, \nIsrael BITnet: SHLOMIT@TECHUNIX Abstract Programs in languages such as FORTRAN, Pascal, and C, were \ndesigned and written for a sequential machine model. Several methods to vectorize such programs and recover \nother forms of parallelism that apply to more advanced machine architectures have been developed during \nthe last decade. We propose and demonstrate a more powerful translation technique for making such programs \nrun efficiently on parallel machines which of\u00adfer facilities such as parallel prefix operations as well \nas parallel and vector capabilities. This technique, which is global in nature and involves a modification \nof the traditional definition of the program dependence graph (PDG), is based on the extraction of parallelizable \npro\u00adgram structures ( idioms ) from the given (sequential) program. The benefits of our technique extend \nbeyond the abovementioned architectures, and can be viewed as a general program optimization method, \napplicable in many other situations. We show a few examples in which our method indeed outperforms existing \nanalysis techniques. 1 Introduction Many of the classical compiler optimization techniques [2] comprise \nthe application of local transformations *Work performed in part while on sabbatical leave at the Dept. \nof Computer Science, Yale University. t Work supported in part by NSF grant number DCR-8405478. ~Work \nsupported in part by ONR grant number NOOO1489-J\u00ad1906. Permission to copy without fee all or part of \nthis matertial is granted provided that the copies are not made or distributed for direct commercial \nadvantage, the ACM copyright notice and the title of the publication and its date appear, and notice \nis given that the copying is by permission of the Association for Computing Machinery. To wpy other\u00adwise, \nor to republish, requires a fee and/or specific permission. @ 1990 ACM 089791-419-81901001210079 $1.50 \nHaifa 32000, Israel BITnet: PINTER~ISRAEARN to the (intermediate) code, replacing sub-optimal frag\u00adments \nwith better ones. One hopes, as is often the case, that repeating this process (until a fixed-point is \nfound or some other criterion is met) will result in an overall better program. To a large extent, attempts \nto vector\u00adize and otherwise parallelize sequential code [27] are also based on the detection of local \nrelationships be\u00adtween data items and the control structures that enclose them. These approaches, however, \nare local in nature and do not recognize the structure of the computation that is being carried out. \nA computation structure sometimes spans a fragment of the program which may include some irrelevant de\u00adtails \n(that might obscure the picture both to the hu\u00adman eye and to automatic parallelism detection algo\u00adrithms). \nSuch idioms [22] may include inner-product calculations and recurrence equations in numeric code, data \nstructure traversal in a symbolic context, selective processing of vector elements, and patterns of updating \nshared values in a distributed application. Recognizing them is far beyond the potential of the classical, \nlocal methods. Once recognized, such structures can be replaced lock, stock, and barrel by a new piece \nof code that has been highly optimized for the task at hand. This pertains to sequential target machines, \nbut is most appealing in view of the potential gains in parallel computing. For example, a loop computing \nthe convolution of two vec\u00adtors, which uses array references, can be replaced by an equivalent fragment \nusing pointers which was tailored by an expert assembler programmer. The same code can be replaced by \na call to a BLAS [12, 18] routine that does the same job on the appropriate target ma\u00adchine. Better yet, \nif the machine at hand for example, TMC S CM-2 [26] supports summary operators, such as reduction which \nworks in time O(log n) using a par\u00adallel prefix implementation [8, 17] rather than O(n) on a sequential \nmachine (where n is the length of the vectors involved), the gains could be even more dramatic. In this \npaper we propose a method for extracting par\u00adallelizable idioms from scientific programs. We cast our \ntechniques in terms of the construction and analysis of the computation graph, which is a modified extension \nof the program dependence graph (PDG) [13]. PDG based analysis methods seem to be most flexible and amenable \nfor extensions. Forming this new type of graph as is necessary for the required analyses from a source \nprogram and using it for optimization transformations involves three major steps: When necessary, loops \nare unrolled so as to reach normal form~, whose precise definition and algo\u00adrithms for its formation \nare an additional contri\u00adbution of this paper. Normalization is carried out only when standard dependence \nanalysis techniques are not helpful (thereby providing a proper exten\u00adsion to dependence analysis theory). \nThe normal\u00adization process includes a reference range an a[ysis stage which reduces the number of loop \ncarried de\u00adpendencies by observing that different references to an array may not interfere during the \nexecution of the loop. We construct the computation graph for the basic block constituting each innermost \nloop. Certain conditionals (which are classified as data filters ) are handled gracefully, whereas others \nwe disallow. The graph of each loop is replicated three times, for the initial, middle , and final iteration, \nand together with an additional control node we obtain the computation graph of the whole loop. This \npr~ cess is repeated as we go up the nesting structure as far aa possible (until we hit a problematic \nbranching structure). Once the graph is built, we apply optimization trans\u00adformations to it using appropriate \nalgorithms that we provide for this purpose. Using this method, we were able to extract struc\u00adtures that \nother methods fail to recognize, as reported in the literature, thereby providing a larger potential \nfor speed-up. We believe that our techniques can also be fitted to other program analysis frameworks, \nsuch as symbolic evaluation and plan analysis. Its power can be demonstrated by a small example, taken \nfrom [5], who gave up on trying to parallelize the following loop (which indeed cannot be vectorized): \nDO 100 1=2, N-1 C(I)=A(I)+B(I) B(I+I)=C(I-l)*A(I)  100 CONTINUE Known methods, and even a human observer, \nmay not realize that this loop hides two independent recurrences In the sense of [21], n&#38; that of \n[6]. that can be implemented [16] in time O(log n) (n being the value of N) rather than O(n). This kind \nof transfor\u00admation can, however, be effected using our computation graph. Another advantage of our approach \nis that it handles data and control uniformly. Our method encapsulates the effect of inner loops so that \nidioms spanning deep nesting can be identified. Moreover, by incorporating data filters that abstract \ncertain forms of conditionals, idioms for the selective manipulation of vector elements (which are useful \non vector and SIMD machines) can be extracted. Finally, due to the careful definition the framework and \nby the appropriate selection of transfor\u00ad mation rules, our method can uniformly serve a wide variety \nof target architectures, ranging from vector to distributed memory machines. We first review other work \non dependency analysis and evaluate its limit ations for purposes of idiom extrac\u00adtion. Then, in Section \n3, we provide a formal presen\u00adtation of our technique, including the definition of loop normalization, \nthe construction of computation graphs, and the necessary transformations algorithms. In Sec\u00adtion 4 we \nexemplify our techniques, stressing the advan\u00adtages over existing methods. Section 5 tries to put our \nwork in perspective, discusses applications other than parallelism and proposes further research. 2 Existing \nParallelization Methods Three major program analysis methods for purposes of optimization and parallelization \nare most common: con\u00adstructing and then using the program dependency graph (PDG) or a similar structure, \nsetting up and then solv\u00ading a system of linear equations that reflect the rela\u00adtionship between array \nreferences, and (partial) sym\u00adbolic evaluation. In this section we briefly review each method, reason \nwhy in itself none can be used to serve our purposes, and explain why we have decided to ex\u00adtend the \nPDG approach. Much of the traditional program analysis work culmi\u00adnated in the definition and use of \nthe PDG or its con\u00adstituents, the control and data dependence graphs. Such a graph defines the flow of \ncontrol in the program as well as the dependence between the variables being used in the program s statements. \nAnalyzing the graph allows us to detect loop invariant assignments for purposes of vectorization, classify \nthe type of other dependencies as loop carried and internal ones, and when applied to pro\u00adgrams in single \nstatic assignment (SSA) form [11] it can be quite effective. The analysis methods allowed by this framework, \nhowever, are very much tied to the original structure of the program. Moreover, the focus is on following \nde\u00adpendencies as they are reflected at the syntactic level (variable names and statements) rather than \ntracking data flow (among values) using deeper semantic analy\u00adsis. Thus, this approach is not quite strong \nenough to allow us to extract patterns of data modification that are not readily apparent from the source \nprogram. For example, the PDG of the loop shown in Section 1 contains two nodes on a directed cycle, \nimplying a cir\u00adcular dependency. This representation is not accurate since the two reductions hidden \nin this computation do not depend on each other. Thus, the PDG is more con\u00ad servative than our graph \nwhich will expose the potential for parallelization in this loop. Another approach is that of capturing \nthe data depen\u00addence between array references by means of dependence vectors [10, 15, 27]. Then methods \nfrom linear alge\u00adbra can be brought to bear to extract wavefronts of the computation. The problem with \nthis framework is that only certain types of dependencies are modeled, again there is no way to talk \nabout specific operations, and in general it is hard to extend. Finally, various symbolic methods have \nbeen proposed [14, 20, 24], mostly for plan analysis of programs. These methods all follow the structure \nof the program rigor\u00ad ously, and even though the program is transformed into some normal form up-front \nand all transformations pre\u00adserve this property, still these methods are highly sensi\u00adtive to noise in \nthe source program. More severely, the reasoning about array references (which are the main\u00adstay of scientific \ncode) is quite limited for purposes of finding reduction operations on arrays. The PDG based methods \nseem to be most flexible and amenable for extensions. They have served as the basis for successful parallelization \nsystems, e.g. PTRAN [4]. Recently these methods have also been shown to be se\u00admantically sound [9], removing \na potential draw-back. Moreover, this approach offers the most extensive sup\u00adport for program analysis \nthat is required for enabling the usage of our techniques, namely it allows applica\u00adtion of classical \noptimizations, vectorization, and more. Thus, we have chosen to extend the PDG based frame\u00adwork and provide \nalgorithms that can be used on the new structure. Partial success in recovering reduction operators is \nreported in the work on Parafrase [19, 23] (which also does a very good job in vectorization and parallelizing \nloops). The restructuring methods that were used are in the spirit of the PDG based work, but the techniques \nfor detecting reduction operators are different from ours and somewhat ad hoc. Our method, we believe, \nis more systematic and general. 3 Computation Graphs and Algorithms for Idiom Extraction In this section \nwe propose a new graph theoretic model to represent the data flow of and the dependencies be\u00adtween values \nin a program, namely the computation graph. To allow effective usage oft his model, programs need to \nbe preprocessed, and most importantly loops must be normalized; this technique will be presented in \nthe beginning of this section. Then we define the computation graph formally and provide an algorithmic \nframework that uses this new abstraction in order to analyze the structure of programs and identify compu\u00adtations \nthat can be parallelized or otherwise optimized. We conclude this section by stating some properties \nof computation graphs and by outlining the correctness of our transformations. To guide the reader through \nthis section we use the sample program of Figure 1, which is written in FOR-TRAN. This example is merely \nmeant to illustrate the definitions and algorithms, not to show off the advan\u00adtages of our approach over \nother work; this will be done in Section 4. DO 10 1=1,N T = 1+4 c assume M is even DO 10 J=3, M A(I, \nJ) = A(I, J) + T* A(I, J-2) 10 CONTINUE Figure 1: A sample FORTRAN program. 3.1 Preprocessing and Loop \nNormalization Before our analysis can be applied to a given program we must transform it so as to allow \nthe computation graph to effectively capture the program s structure for purposes of idiom extraction. \nSome of these prepro\u00adcessing transformations are straightforward and well known optimizations, as we \nshall point out in passing, but others require some new techniques that we shall explain. At this stage \nwe also assume that programs con\u00adtain only IF statements that serve as data jilters, as in IF(A(I) .EQ. \nO) B(I)=I. Methods for recognizing con\u00additionals as data filters (so they can be transformed into appropriate \ntarget code) will be discussed in Sec\u00adtion 3.2. Some of the other conditionals can be pulled out of loops \nor converted to filters using standard tech\u00adniques, but those that cannot delineate the scope of our \ntechniques. Finally, we assume that all array references depend linearly on loop variables. The preprocessing \ntransformations are then as fol\u00adlows : e Standard basic block optimizations (such as dead\u00adcode and dead-store \nelimination, common subex\u00adpression detection, etc.) are performed per [2], Sec\u00adtion 9.4, so that the \nresulting set of assignments to values is clean . This implies that the expression DAG that is obtained \nrepresents only the depen\u00addencies between variables that are live upon entry to each block and those \nthat are live at the exit, thus reflecting the substitution of temporaries in expressions that use them \nlocally and the like. We further assume that the dependency between val\u00adues is simple , meaning that \neach defining expres\u00adsion is of some predefine form. Typical restrictions could be that it contains at \nmost two values and one operator or that it be a linear form; which restric\u00adtion is imposed depends on \nthe type of recognition algorithm we want to apply later (in Section 3.3). o Some PDG based analysis \n(on both structure and data), such as the recognition of 100PS wherever possible, must be performed. \nThe possibility to vec\u00adtorize and apply some other parallelizing transfor\u00admations (such as scalar expansion) \ncan be detected with our own techniques, as we shall see. Loops are normalized with respect to their \nindex, as defined below. Munshi and Simons [21] have ob\u00adserved that by sufficient unrolling all loop \ncarried dependencies can be made to occur only between consecutive iterations. The exact conditions stating \nhow much unrolling is required and the technique for transforming a loop to normal form have never been \nspelled out, however, thus in what follows we provide the necessary details. Furthermore, we refine \nthe conditions so that only true depen\u00addencies are taken into account; in other words, some apparent \ndependencies which seem like loop carried ones but actually are not are indeed not considered. Normalization \ncomprises three steps: reference range analysis, determination of loop carried dependencies (among values \nin each array), and the actual unrolling. The purpose of the first step is to save unrolling, nev\u00adertheless \nif the necessary information is hard to derive it can be omitted without affecting the correctness of \nthe remainder of the process. We next define the above terms and then explain how to normalize loops. \nDefinition 1 Let A be a one-dimensional array that is referenced (used or dejined) in a loop. For each \nreference of A in the loop its reference range is an interval of the integers [1, u], where 1 is the \nsmallest value the indez expression can obtain during the loop s execution and u is the largest such \nvalue. Intuitively, a reference range describes the minor of the array which is affected by the references \nthroughout the loop. The above definition can be easily extended to multi-dimensional arrays by using \nCartesian products of intervals. Note that all the values that are involved in computing the range (such \nas 1, u, and the stride) must be known at compile time (and, of course, be loop invariant); this is less \nof a problem than seems at first since even though bounds are often symbolic (in terms of program variables), \nthey can still be used effectively for purposes of normalization. In order to identify true loop carried \ndependencies, we need to find out how the ranges interact. For each array we form the transitive closure \nof its ranges as in\u00adtervals with respect to intersection; since intersection (between intervals) is reflexive \nand symmetric, we thus obtain equivalence classes of ranges. We can safely con\u00adclude that two array references \ndo not interact directly throughout the execution of a loop if the corresponding ranges do not belong \nto the same class. For example, there are two reference ranges for A of Figure 1: [1, n] x [3, m] and \n[1, n] x [1, m 2] where n and m are the values of the variables N and M, respectively. These two ranges \nare disjoint only when m <4, which is rather restrict ive (and cannot always be deduced), but often this \nanalysis results in significant advantages, as we shall see later. Next we must define loop carried dependencies \nbe\u00adtween reference ranges, rather than statements (as is customary, e.g. [5, 27]]. This is key for the \nproper defi\u00adnition and usage of computation graphs, as we shall see later. Definition 2 A reference range \nD is loop carried de\u00adpendent on a reference range C ifD uses C and the value of C was set in an iteration \npreceding the use in D. Next, we look at the transitive closure relation of the data dependencies and \nloop carried dependencies between variable instances and reference ranges in the loop s body. For each \narray in the loop we define the cross range dependencies of the array to be all those dependencies in \nthe relation that are among the array s references whose ranges are not in the same class. In the following \nexample A has two references ranges, [2, m] and [m+ 2, 2m], where m is the value of M. The loop carried \ndependency between these reference ranges is a cross range dependency since the ranges are in dif\u00adferent \nclasses. DO20 1=2, M B(I) = 2*A(I) 20 A(I+M) = B(I-1) Conceptually one can view the array as if it were \npar\u00adtitioned into many (pairwise disjoint) arrays, one for each class. Now we are ready to say when a \nloop is in normal form: Definition 3 A loop is in normal form if al! of its loop carried dependencies \nthat are not cross range dependen\u00adties are only between two consecutive iterations. Note that if standard \ntechniques such as the GCD and Banerjee tests [7] reveal no loop carried dependen\u00adcies then the loop \nis also in normal form. Any given loop can be normalized as follows. First we build the equiv\u00adalence \nclasses of the array reference range intersection relation, It suffices to perform this process for each \nar\u00adray separately since all the transit ive effects (e.g. data dependencies that occur through assignments \nto inter\u00admediate variables, even if they are entries in other ar\u00adrays) are known due to basic block optimizations \nthat have been performed. Next we find all value based loop carried dependencies that are not cross \nrange dependencies. Finally, let the span of the original loop be the largest integer k such that some \nvalue set in iteration i depends directly on a value set in iteration i k (again, this does not take \ninto account cross range dependencies). Then, to normalize a loop with span k > 1, the loop s body is \ncopied (un\u00adrolled) k 1 times and the loop s index is adjusted so it starts at 1 and is incremented appropriately \nat each iteration. Loops are normalized from the most deeply nested outwards. In the example of Figure \n1, if rn > 4 (where m de\u00adnotes the value of M) then the inner loop needs to be normalized by unrolling \nit once, whereas the outer loop is already in normal form (it is vacuously so, since there are no loop \ncarried dependencies). We also assume that the temporary scalar T has been expanded, thus obtain\u00ading \nthe program in Figure 2. DO10 1=1,N T(I) = 1+4 c assume M is even DO 10 J=l, M-2,2 A(I, J+2) = A(I, \nJ+2) + T(I)* A(I, J) A(I, J+3) = A(I, J+3) + T(I)* A(I, J+l) 10 CONTINUE Figure 2: The program of Figure \n1 in normal form. Normalization is the first step in exposing all the de\u00adpendencies in a loop. At the \ncurrent stage the scope 2We assume there are no EQUIVALENCE declarations. of our work includes only \nloops for which the span is constant. Note that the loop can still include array ref\u00ad erences which use \nits bound in some cases. For example the span of the following loop is 2: DO 10 1=2, N-1 A(N-1) = A(N-1) \n+ A(N+2-1) 10 CONTINUE  Also, we cannot handle indirect array references, such as A (X (I ) ), without \nemploying partial evaluation tech\u00adniques. Indirect referencing can generate access patterns which are \nirregular and data dependent; since current target machines cannot take advantage of such patterns anyway, \nwe believe that this limitation is not too severe. An immediate result of normalization is that implicit \nloop carried dependencies become explicit. This obser\u00advation is, later on, being used in the computation \ngraph to explicitly reveal all the possible interaction patterns between iterations of the loop and its \nsurrounding con\u00adtext. Thus, we can match graph patterns in order to enable the transformations. Finally, \nnote that a similar notion to normalization appears in [3] where the com\u00adputation of a loop is carried \non until a steady state is reached. 3.2 The Computation Graph Given a program satisfying the above assumptions, \nwe define its computation graph in two stages: first we han\u00addle basic blocks and data filters, and then \nwe show how to represent normalized loops. For a basic block, a node v represents the instance of an \nassignment statement to a variable or a variable s initial value, denoted var(v). Each node is uniquely \nidentified by the statement num\u00adber (special numbers are generated for nodes represent\u00ading initial values). \nThere are three types of edges drawn between nodes, representing the three types of dependencies between \nvalues [27]: data, anti, and output dependencies. For example, we draw a data dependency edge from a \nnode u to node v if the value defined in u is used in v; similarly, for the other dependencies we draw \nappropriate edges. If not otherwise noted, when referring to edges (in the text and in figures) we mean \ndata dependence edges. We label each node v by the function performed on the arguments to obtain the \nvalue assigned to var(v). The function is represented in terms of a numbering on the data dependency \nedges entering the node (i. e. the arguments) which is used to disambiguate the defining express lon3, \nas w done in expression trees and DAGs. 3When the function is commutative, such as addition, this is \nnot necessary. Figure 3: The computation graph of the basic block Indeed, for basic blocks the computation \ngraph (with\u00adout the anti and output dependency edges) is similar to a data flow graph, but later on, \nwhen data filters and loops are involved, the definitions diverge. Note, how\u00adever, that computation graphs \nwill remain acyclic by construction. Using the array references verbatim as atomic vari\u00adables, the basic \nblock constituting the body of the inner loop of Figure 2 gives rise to the computation graph shown in \nFigure 3. Since we shall be looking for poten\u00adtial reduction and scan operations, which apply to lin\u00adear \nforms, we allow the functions at nodes to be ternary multiply-add combinations. Note also that the nodes \ndenoting the initial values of the variables use 1 as their function label (which could be replaced when \nthe graph is embedded in a larger context, as we shall see). Next we define data filters and show how \nthey are integrated into the computation graph. Definition 4 A data filter is a conditional expression \n which is not affected by any loop carried dependency  controlling an assignment.  A data filter is \nrepresented in the graph by a spe\u00adcial jilter node, denoted by a double circle. The im\u00admediate predecessors \nof this node are the arguments of the Boolean expression constituting the predicate of the conditional, \nthe previous definition of the variable to which the assignment is being made, and the values in\u00advolved \nin the expression on the right hand side of the assignment. The outgoing edge from the filter node goes \nto the node corresponding to the assignment that is be\u00ading controlled. The filter node is labelled by \na condi\u00adtional expression whose then value is the right hand side of the original assignment statement \nand the else value is the previous (old) value of the variable to which the assignment is being made. \nThis expression is written in term of the incoming edges, as Figure 4 demonstrates: in statement 20, \nthe assignment to S is controlled by a predicate on the value of A (I ), hence the corresponding Figure \n4: A program with a data filter and the compu\u00adtation graph of its loop s body, constituting the inner \nloop of the program in Figure 2. computation graph (of the loop s body) contains a filter node. s = 0.0 \nDO 20 1=1, N 20 IF (A(I) .NE.0) S = S + B(I) Next we define computation graphs for normalized loops. \nSuch graphs are obtained by replicating the DAGs representing the enclosed body4 as follows. We generate \nthree copies of the graph representing the body: one represents the initial iteration, one a typ\u00adical \nmiddle iteration, and one stands for the final itera\u00adtion. Each node is uniquely identified by the statement \nnumber and the unrolled copy it belongs to (initial, mid\u00addle, or final). Furthermore, array references \nt hat depend on the loop variable are changed by substituting the ini\u00adtial loop boundary, the name of \nthe variable, and the final boundary in the index expressions respectively. If the loop control variable \nis used in the computations within its body we treat it like any other variable with the proper initial \nvalue (node), and we add a statement (node) for incrementing its value. 4At the innermost level these \nare basic blocks, but as we go up the structure these are general computation graphs. Figure 5: The computation \ngraph of the inner loop of the program in Figure 2. Figure6: The computation graph of the program of \nFigure4. For every loop carried data dependency we identify Figure 6 shows the computation graph of the \nprogram the appropriate vertex in the initial copy with the one of Figure 4. in the middle copy unless \nthere exists an anti or output In order to cover the cases in which the loop body is loop carried dependence \nbetween them. The same ap\u00adexecuted fewer than three times, we splice each depen\u00adplies to the middle and \nfinal copies. The graph is linked dency edge that goes into the loop by adding a qiin-node to its surroundings \nby (data) dependency edges where on it. From this #in-node we also draw an extra edge necessary (i. e. \nfrom initializations outside the loop and leading to the proper place in an unrol[ed subgraph rep\u00adto \nsubsequent uses). resenting the computations in which the loop is unrolled or is rolled less than three \ntimes. Such a node contains The main contribution of the triple unfolding of loops is that when combined \nwith normalization the compu\u00adthe proper @-function (per [11]) which navigates the con\u00adtrol flow for every \nexecution. Similarly, we splice everytation graph represents explicitly all possible dependen\u00addependency \nedge leaving the loop, adding a q$OUt-node,cies. The computation graph spans a signature of the whole \nstructure, and we call it the summary form of the and draw an edge from the proper place of the unrolled \nsubgraph to the new node (if that dependency appearsloop. Once inner loops are transformed into their \nsum\u00admary form, they are treated as part of the body of the in the unrolled graph). In the rest of the \npaper we omit enclosing loops and they may in turn be replicated the unrolled part of the graph since \nwe are interested in parallelizing loops that must be iterated more thansimilarly to yield another summary \nform. twice. Figure 5 shows the computation graph of the inner loop of the example in Figure 2. The graph \ncomprises three copies of the graph from Figure 3 with the appro-3.3 Algorithms priate edges added. The \ngraph for the whole program would consist of three copies of what is shown in Fig-Having constructed \nthe computation graph, the task of ure 5, with appropriate annotation of the nodes. finding computational \nidioms in the program amounts 1 1 (init ) var (init ) 1 J-redwe (f ) f (m d) var (mid var  z f 33GJ--@ti \nFigure 7: Matching and replacement rule for reduction. to recognizing certain patterns in the graph. \nThese similar to the conventional application of optimizw patterns comprise graph structures such as \npaths or tion transformations k la [2], is just one alterna\u00adother particular subgraphs, depending on \nthe idiom, and tive. One could assign costs that reflect the merits some additional information pertaining \nto the labeling of transformations and find a minimum cost cover of the nodes. The idiom recognition \nalgorithm consti-of the whole graph at each stage, and then iterate. tutes both a technique for identifying \nthe graph patterns in the given graph as well as the conditions for when We next elaborate on each of \nthe above items, filling they apply, i.e. checking whether the context in which in the necessary details. \nFirst we discuss graph rewrit\u00adthey are found is one where the idiom can indeed be ing rules. Since we \nare trying to summarize information, used. these will be mostly reduction rules, i.e. shrinking sub\u00ad \ngraphs into smaller ones. More importantly, there are Overall, the optimization procedure consists of \nthe three characteristics that must be matched besides the following algorithmic ingredients: skeletal \ngraph structure: the operators (functions), the array references, and context, i.e. relationship to the \nMatching and replacement of individual patterns enclosing structure. can be achieved by using graph grammars \nto de- The first two items can be handled by looking at the scribe the rewrite rules. While rewriting, \nwe also labels of the vertices. The third involves finding a par\u00ad transform the labels (including the \noperators, of ticular subgraph that can be replaced by a new structure course), thus generating the \ntarget idioms. We and making sure it does not interfere with the rest of the make sure no side effects \nare lost by denoting for\u00ad computation. For example, to identify a reduction op\u00adbidden entries and exits \nper [25]. eration on an array, we need to find a path of nodes all having the same associative operator \nlabel (e. g. multi\u00ad We need to provide a list of idioms and the graph plication, addition, or an appropriate \nlinear combination rewriting rules that replace them. These include thereof ) and using consecutive \n(i.e. initial, middle, and structures such as reduction, scan, recurrence equa\u00adfinal) entries in the \narray to update the same summary tions, transposition, reflection, and FFT butterflies. variable; we \nalso need to ascertain that no intervening Compositions thereof, such as inner product, con\u00adcomputation \nis going on (writing to or reading from the volution, and other permutations, can be generated summary \nvariable). as a preprocessing stage. Notice that data filters can be part of a basic rule. Both the annotations \nof the nodes as well as the guards against intervening computations are part of the At the top level, \nwe (repeatedly) match patterns graph grammar productions defining the replacement from the given list \naccording to a predetermined ap-rules. Figure 7 and Figure 8 provide two such rules to plication schedule \nuntil no more changes are appli-make this notion clear. Here we assume that vectoriza\u00adcable (or some \nother termination condition is met). tion transformations, including scalar expansion, have This means \nthat we need to establish a precedence occurred, so we use their results when looking for pat\u00adrelation \namong rules that will govern the order in terns; the vectorization transformations themselves can which \nthey are applied. This greedy tactic, which is be applied first by using a different tool or can be part \nJ\u00ad(ini/+A, Q => Figure 8: Matching and replacement rule for scan. J\u00ad (init Q => 1 jilter\u00ad (WIIX reduce \n(f) 0 1 var @nal % Figure 9: A matching and replacement rule for a filtered reduction. of our rules \nbase (they can be formulated similarly with essarily follows the loop structure of the given program. \nappropriate expanding rewrite rules). On the contrary, the computation graph as constructed allows the \ndetection of structures that might otherwise To accommodate data filters, additional rules are nec\u00adbe \nobscured, as can be seen in Section 4. essary. For example, the rule of Figure 9 takes care of a filtered \nreduction. Notice the similarity to the rule of The result of applying the rule of Figure 8 to the Figure \n8 if we splice out the filter node and redirect the graph of Figure 5 is shown in Figure 10. edges; the \nscan rule of Figure 8 can be similarly used to If we had started with a graph for the entire program \n derive a rule for a filtered scan. of Figure 27 not just the inner loop, applying a vec\u00adtorization rule \nto the result would have generated the Once such rules are applied, the computation graph contains new \ntypes of nodes, namely summary nodes following program: representing idioms. These nodes can, of course, \nappear themselves as candidates for replacement (on the left DOALL 10 1=1, N hand side of a rule), thereby \nenabling further optimiza-T(I) = 1+4 tion. Obviously, we would apply the rules for lower level SCAN(A(I, \n*), l, M-1,2, T(I) , *ll,t~+tt) optimizations first and only then use the others, but one SCAN(A(I, *),2, \nH,2, T(I), t1*11,11+~!) should not get the impression that this procedure nec-10 CONTINUE Figure 10: \nThe comput at ion graph resulting from applying (We use an arbitrary template for SCAN which includes \nall the necessary information, including array bounds, strides inside vectors or minors, and the operations \nto be performed in reverse polish notation.) In the case of Figure 4, the resulting program would be \nT(l:N) = 0.0 WHERE(A(I:N).NE.0) T(l:N)=B(l:N) S=REDUCE(T, l,N,l, + ) where T is a newly defined array. \n(This somewhat awkward phrasing of the resultant program is due to the way dialects such as FORTRAN 8X \nare defined). In general, the order in which rules are applied and the termination condition depend on \nthe rule set. If the rules are Church-Rosser then this is immaterial, but often they are competing (in \nthe sense that the applica\u00adtion of one would outrule the consequent application of the other) or are \ncontradictory (creating potential oscil\u00adlation). This issue is beyond the scope of this paper and we \ndefer its discussion to general studies on properties of rewriting systems. 3.4 Correctness In this \nsection we argue that the computation graph faithfully represents the computation expressed by the program. \nA computation of a program is a partially ordered set of events that occur during the program s execution. \nThe events of the computation are the exe\u00adcution of assignment statements and the evaluation of predicates \nin data filters, and the ordering is defined by the execution order. We define the computation relation \nto be the irreflexive transitive closure of the program s computation. The collection of all such relations \nof a program, with respect to the input domain, represents all the computations of the program. Definition \n5 Given a computation graph of a program, toe define a subgraph of the computation graph to be a the \ntransformations of Figure 8 to the graph in Figure 5. representative of a computation of the program \nif the following four conditions hold: (i) it contains the source (root) nodes corresponding to the variabies \ninitialized by the computation, (ii) for every node which is not a ~\u00adnode it contains all the edges and \nnodes reachable from the node via non g$-nodes, (iii) for every ~-node reached only one exiting edge \nis in the subgraph, and (iv) the ir\u00adreflexive transitive closure relation of the subgraph (afier the \nomission of q$-nodes) is a subset of the computation relation. To establish the correctness of this representation, \nit is not difficult to prove the following. Lemma 1 Every terminating computation of the pro\u00ad gram has \na representative in the program s computation graph and every representative subgraph is consistent with \nthe ordering of some computation. Next we claim that the representatives are detailed enough in order \nto carry out our transformations. For non-looping computations, the computation graph de\u00adfines the same \nordering relation as the computation of the program. Since the major area of optimization is array access \nin loops, we must justify the construction of the computation graph for loops. There is one key lemma \n(presented here without proof, which is straight\u00adforward), as follows: Lemma 2 Unfolding three iterations \nof a normalized loop is enough to revea! its data dependence structure. We finally define the correctness \nconditions for trans\u00adformations in our framework. Definition 6 A transformation is correct if for any \ngiven program every terminating computation of the transformed program has a representative in the com\u00adputation \ngraph of the original program, and each rep\u00ad resentative subgraph of the computation graph is consis\u00adtent \nwith the ordering of some computation in the trans\u00adformed program. Figure 11: The computation graph \nof the program appearing in the introduction. 4 Examples 1]; since only the first two ranges are not \ndisjoint, the corresponding references are the only ones taken into To exemplify the advantages of our \nidiom recovery account when calculating the span, which hence is 1. method, we present here three cases \nin which other Furthermore, we observe that the computation of F(I) methods cannot speed the computation \nup but ours can can be performed in a separate loop, as can be deduced (if combined properly with standard \nmethods). We do using the well known technique of loop distribution . not include the complete derivation \nin each case, but we The loop computing F can be easily vectorized, so all outline the major steps and \npoint out the key items. we are left with is a loop containing a single statement The first example is \none of the loops used in a which computes WK(I+ 1 ). If we draw the computation Romberg integration routine. \nThe original code (as re\u00ad graph for this loop and allow operators to be linear com\u00ad leased by the Computer \nSciences Corporation of Hamp\u00ad binat ions with a coefficient (F(I) in this case), we can ton, VA) looks \nas follows immediately recognize the scan operation that is taking place, and produce DO 40 N=2, M+1 \nKN2 = K+ N-2 WK1(K+M:K+2*M-l )=WK(K:K+M-1) KNM2 = KN2+M SCAN(WK, K+ M, K+2*M-l,l, WK1, - *F, * , + ) \nKNM1 = KNM2+ 1 F = 1.0 1 (4. O**(N-1) -1.0) The second example is the program appearing in the TEMPI \n= WK(KNM2) -WK(KN2) introduction (taken from [5], who gave up on trying wK (KNMI ) = wK (KNM2) + F*TEMP \nI to parallelize it). Notice that the loop is already in 40 CONTINUE normal form. Once the loop is unfolded \nthree times, as required, we detect two independent chains in the but after substitution of temporaries, \nscalar expan\u00ad computation graph, one including the computation of sion of F, and renaming of loop bounds, \nwhich are all the even5 entries in B and C, and the other computing straightforward t ransformat ions, \nwe obtain the odd entries. If the data is arranged properly, the whole computation can be performed &#38;ing \nthe m_ethod DO 40 I= K+M, K+2*M-I for computing recurrence equations presented in [16], F(I)= 1.0 / (4.0* \n*(1-K-M+I) -1.0) taking time O(log n) (n being the value of N) rather WK(I+I)=WK(I)+F( I)*(WK(I)-WK(I-M)) \nthan O(n). The computation graph can be seen in Fig\u00ad 40 CONTINUE ure 11. Notice that it is completely \ndifferent from the PDG which consists of two nodes on a directed cycle. Notice that The reference no \nnormalization range analysis is for necessary WK yields in this case. three ranges: 5We use even and \nodd here just to distinguish the chains; the recognition procedure does not need to know or prove thk \nfact [k+m+l, k+2rn], [k+m, k+2rn-l], and[k, k+rn at all. 89 (a) Figure 12: Transforming an Finally, \nwe parallelize a program computing the inner product of two vectors, which is a commonly occurring computation. \nConsider P=o DO 100 1=1, N P= P+ X( I)*Y(I) 100 CONTINUE Traditional analysis of the program (in preparation \nfor vectorization) replaces the loop body by T(I)= X( I)*Y(I) P= P+T(I)  Figure 12 shows the computation \ngraphs of this trans\u00adformed basic block and that of the relevant part of the loop. The vectorizable portion \nof the loop can be trans\u00adformed into a statement of the form T=X*Y, and what is left can be matched by \nthe left hand side of the rule of Figure 7. Applying the rule produces the graph corre\u00adsponding to the \nstatement P= REDUCE (T, 1, N, 1, , + ). The recognition of an inner product using our tech\u00adniques will \nnot be disturbed by enclosing contexts such as a matrix multiplication program. The framed part of the \nprogram in Figure 13 produces the same reduced graph that appears replicated after treating the two out\u00adermost \nloops. 5 Discussion, Other Applications, and Future Work We have proposed a new program analysis methods \nfor purposes of optimization and parallelization. We offer at least as much functionality as each of \nthe other meth\u00adods we know of plus the ability to recognize idioms more *+ T(m d) P W inner product \nto a reduction DO 100 1=1, N DO 100 J=I, N C(I, J)=O DO 100 K=l, N C(I, J)= C(I, J)+ A(I, K)* B(K, J) \n 100 CONTINUE Figure 13: The portion of a matrix multiplication that is recognized by the transformation \nof Figure 12. globally. In summary, when comparing our method to the other most common methods, we can \nsay the fol\u00adlowing: Constructing and then using the program depen\u00addency graph (PDG) or a similar structure \nfocuses on following dependencies as they are reflected at the syntactic level alone. Thus, the PDG is \nmore conservative than our computation graph which ex\u00adposes further potential for parallelization. 0 \nCapturing the data dependence between array ref\u00aderences by means of dependence vectors [10, 15, 27] and \nthen solving a system of linear equations to ex\u00adtract the wavefronts of the computation is limited to \nfunctional (side-effect free) sets of expressions. By the  Various symbolic and partial evaluation meth\u00adods \nhave been proposed-[14, 20, 24], mostly for plan analysis of programs. These methods all follow the structure \nof the program rigorously, and even though the program is transformed into some normal form up-front \nand all transforma\u00adtions preserve this property, still these methods are  highly sensitive to noise \nin the source program. More severely, the reasoning about array references (which are the mainstay of \nscientific code) is quite limited for purposes of finding reduction operations [3] A. Aiken and A. Nicolau. \nOptimal loop paralleliza\u00adon arrays. tion. In SIGPLAN 88 Conference on Programming Language Design and \nImplementation, pages 308- There wae partial success in recovering reduction 317. ACM, 1988. operators \n(using PDG-like analysis) in the work on Parafrase [19, 23], The techniques employed, how\u00ad [4] F. E. \nAllen, M. Burke, P. Charles, R. Cytron, and ever, are somewhat ad hoc compared to ours. J. Ferrante. \nAn overview of the PTRAN analy\u00adsis system for multiprocessing. Journal of Para!iei Our primary objective \nis to cater for machines with and Da stm buted Computing, 5(5):617 640, October effective support of \ndata parallelism. Our techniques, 1988. however, are not predicated upon any particular hard\u00adware, but \ncan rather be targeted to an abstract ar-[5] R. Allen, D. Callahan, and K. Kennedy. Automatic chitecture \nor to language constructs that reflect such decomposition of scientific programs for parallel features. \nExamples of such higher level formalisms are execution. In Fourteenth Annual Symposium on APL functional \nand idioms, the BLAS package (which Principles of Programming Languages, pages 63 has many efficient \nimplement ations on a variety of ma-76. ACM, January 1987. chines), vector-matrix primitives aa suggested \nin [1], and [6] R. Allen and K. Kennedy. Automatic translation oflanguages such as Crystal, C* and *lisp \nwhich all sup-FORTRAN programs to vector form. ACM Trans. port reductions and scans. on Programming \nLanguages and Systems, 9(4):491 All in all, we feel that this paper makes both method\u00ad542, October 1987. \n ological and algorithmic contributions that should be further investigated. We believe that in addition \nto [7] U. Banerjee. Data dependence in ordinary pro\u00adthe constructs mentioned above, many other can be \nex-grams. Master s thesis, University of Illinois at pressed as patterns and be identified as idioms. \nSuch Urbana-Champaign, October 1976. constructs need to be formulated in terms of compu\u00ad [8] G. E. Blelloch. \nScans as primitive parallel opera\u00adtation graph patterns, and further additional work tions. IEEE Trans. \non Computers, C-38(ll):1526\u00ad on algorithmic methods other than repeated applica-November tion of the \nrules (such as fixed-point computations) is 1539, 1989. also necessary. Finally, we are currently implementing \n[9] R. Cartwright and M. Felleisen. The semantics of the techniques mentioned here as part of an existing \nprogram dependence. In SIGPLAN 89 Conference parallelization package and plan to obtain experimental \n on Programming Language Design and Implemen\u00adresults that would indicate how prevalent each of the tation, \npages 13-27. ACM, 1989. transformations is. Acknowledgements. A substantial amount of the re-[10] M. \nC. Chen. A parallel language and its compila\u00adsearch reported in this paper was performed while the tion \nto multiprocessor machines or VLSI. In Thir\u00adauthors were on sabbatical leave, visiting the Dept. of teenth \nAnnual Symposium on Principles of Pro-Computer Science, Yale University, New Haven, CT. gramming Languages, \npages 131 139. ACM, Jan-The work of Shlomit Pinter was supported in part by uary 1986. NSF grant number \nDCR-8405478, and that of Ron Pin\u00ad [11] R. Cytron, J. Ferrante, B. K. Rosen, M. N. Weg\u00ad ter by ONR grant \nnumber NOOO 14-89-J-1906. man, and F. K. Zadeck. An efficient method of com- The authors would like to \nthank Ron Cytron and puting static single assignment form. In Sixteenth Liron Mizrahi for helpful discussions, \nas well as Jeanne Annual Symposium on Principles of Programming Ferrante and David Bernstein for comments \non an ear-Languages, pages 25-35. ACM, January 1989. lier draft of this paper. [12] J. J. Dongarra, J. \nDu Croz, S. Hammarling, and R. J. Hanson. An extended set of FORTRAN basic References linear algebra \nsubprograms. ACM Trans. on Math\u00adematical Sofiware, 14(1):1 17, March 1988. [1] A. Agrawal, G. E. Blelloch, \nR. L. Krawitz, and [13] J, Ferrante, K. J. Ottenstein, and J. D. Warren. program dependence and use \nC. A. Phillips. Four vector-matrix primitives. In Symposium on Parallel Algorithms and Architec-The \ngraph its in opti\u00adtures, pages 292 302. ACM, June 1989. mization. A CM Trans. on Programming Languages \nand Systems, 9(3):319 349, July 1987. [2] A. V. Aho, R. Sethi, and J. D. Unman. CompiL ers Principles, \nTechniques, and Tools. Addison-[14] P. Jouvelot and B. Dehbonei. A unified semantic Wesley, Reading, \nMA, 1986. approach for the vectorization and parallelization of generalized reductions. In International \nCon\u00ad ference on Supercomputing, pages 186-194. ACM, June 1989. [15] R. M. Karp, R. E. Miller, and S. \nWinograd. The or\u00adganization of computations for uniform recurrence equations. Journal of the Association \nfor Comput\u00ading Machinery, 14(3):563 590, July 1967. [16] P.M. Kogge and H. S. Stone. A parallel algorithm \nfor the efficient solution of a general CISSS of re\u00adcurrence equations. IEEE Trans. on Computers, C-22(8):786 \n793, August 1973. [17] R. E. Ladner and M. J. Fischer. Parallel prefix computation. Journal of the Association \nfor Com\u00adputing Machinery, 27(4):831 838, October 1980. [18] C. L. Lawson, R. J. Hanson, D. R. Kincaid, \nand F. T. Krogh. Basic linear algebra subprograms for Fortran usage. ACM Trans. on Mathematical Sofi\u00adware, \n5(3):308 323, September 1979.  [19] G. Lee, C. P. Kruskal, and D. J. Kuck. An empirical study of automatic \nrestructuring of nonnumerical programs for parallel processors. IEEE Trans. on Computers, C-34( 1O):927-933, \nOctober 1985. [20] S. I. Letovsky. Plan Analysis of Programs. PhD thesis, Dept. of Computer Science, \nYale University, December 1988. Available as YALEU/CSD/RR 662. [21] A. Munshi and B. Simons. Scheduling \nsequential loops on parallel processors. Technical Report RJ 5546, IBM Almaden Research Center, 1987. \n[22] A. J, Perlis and S. Rugaber. Programming with idioms in APL. In APL 7 9, pages 232 235. ACM, June \n1979. APL Quote Quad, Vol. 9, No. 4. [23] C. D. Polychronopoulos, D. J. Kuck, and D. A. Padua. Execution \nof parallel loops on parallel pro\u00adcessor systems, In International Conference on Para//el Processing, \npages 519-527. IEEE, 1986, [24] C. Rich and R. C. Waters. The programmer s apprentice: a research overview. \nComputer, 21(11):10-25, November 1988. [25] M. Rosendahl and K. P. Mankwald. Analy\u00adsis of programs by \nreduction of their structure, In V. Claus, H. Ehrig, and G. Rozenberg, edi\u00adtors, Graph-Grammars and Their \nApplications to Computer Science and Bio/ogy, pages 409417. Springer-Verlag, 1979. Lecture Notes in Computer \nScience, Vol. 73. [26] Thinking Machines Corporation, Technical Report HA87-4. Connection Machine Model \nCM-2 Tech\u00adnical Summary, April 1987. [27] M. J. Wolfe. Optimizing computers. PhD thesis, University of \nIllinois at ber 1982. Available as Super-compilers for Super-Dept. of Computer Science, Urbana-Champaign, \nOcto- TR UIUCDCS-R-82-1105.  \n\t\t\t", "proc_id": "99583", "abstract": "", "authors": [{"name": "Shlomit S. Pinter", "author_profile_id": "81100157492", "affiliation": "IBM Scientific Center, Technion city, Haifa 32000, Israel", "person_id": "PP39077323", "email_address": "", "orcid_id": ""}, {"name": "Ron Y. Pinter", "author_profile_id": "81100157491", "affiliation": "", "person_id": "PP39081934", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/99583.99597", "year": "1991", "article_id": "99597", "conference": "POPL", "title": "Program optimization and parallelization using idioms", "url": "http://dl.acm.org/citation.cfm?id=99597"}