{"article_publication_date": "01-03-1991", "fulltext": "\n Incremental Computation via Partial Evaluation R.S. Sundaresh* Paul Hudaki Yale University Department \nof Computer Science Box 2158 Yale Station New Haven, CT 06520 {sundaresh ,hudak}@cs. yale. edu 1 Introduction \nevaluation. Besides providing a precise definition of the term increment al program , this framework \noffers: It is a common occurrence in a programming environment to apply a software tool to a series of \nsimilar inputs. Examples A methodology to generate an incremental program include compilers, interpreters, \ntext formatters, etc., whose from its non-incremental counterpart plus a specifica\u00ad inputs are usually \nincrementally modifed text files. Thus tion of a partition of the input domain. (This reduces programming \nenvironment researchers have recognized the the designer s primary task to determining the parti\u00ad importance \nof building incremental versions of these tools tion of the input domain, which controls the granular\u00ad \ni.e. ones which can efficiently update the result of a com\u00ad ityy of the increment alit y as well as overall \nefficiency.) putation when the input changes only slightly. An algebraic basis for reasoning about the \ncorrect- However, despite the preponderance of work on incre\u00ad ness of the incremental programs thus generated. \n(The mental algorithms and programs, formal and general treat\u00ad framework relies partially on the notion \nof a Brouwe\u00ad ments of the problem are rare. Historically the approach rian atgebra.) has been to hand-craft \nincremental algorithms for many im\u00ad portant problems, and as a result common elements of the A comparative \nbasis for better understanding existing designs are often obscured. Indeed, looking at various ex\u00ad incremental \nalgorithms. In particular, we have re-cast tant incremental algorithms, one might be led to believe that \nmany existing incremental programs into our frame\u00ad there is no common element at all! There seems to \nbe some work. consensus that incremental algorithms are hard to derive, debug and maintain [Pug88, YS89, \nFT90], and as we at\u00adtempt to create incremental programs for larger tasks, this problem will only get \nworse. To overcome the overhead of incremental interpreta\u00adtion, a method to generate compiled incremental \nprograms using Futarnw-a projections is described. Thus there is an increasing need for a fiamezuork \nfor in\u00ad cremental computation which will help us understand exist\u00ad 2 Incremental Computation and Partial \nEvaluation ing incremental algorithms and facilitate (if not automate) the construction of new ones. \nWe have developed such a To understand our framework one must first have a good framework based on the \nrecently popular notion of partial understanding of partial evaluation, and thus we begin with * Supported \nin part by an IBM graduate fellowship and DARPA grant NOO014-W-K-0573, some basic definitions (for a \ngood survey of the field see [JSS89]). t Supported in part by DARPA grant NOO014-W!-K-0573. 2.1 Partial \nEvaluation Partial evaluation is a program transformation technique for specializing a function with \nrespect to some known (i.e. static ) part of its input. The result is called a residual function, o and \nhas the property that when applied to the remaining part of the input, will yield the desired result. \nFollowing Launchbury [Lau88], we give a precise definition of partial evaluation using projections. Permission \nto copy without fee all or part of this mztertial is granted Definition 2.1 A projection on a domain \nD is a continu\u00ad provided that the copies are not made or distributed for direct ous mapping p : D -+ \nD such that: commercial advantage, the ACM copyright notice and the title of the publication and its \ndate appear, and notice is given that the copying is by permission of the Association for Computing Machinery. \nTo copy other\u00ad wise, or to republish, requires a fee and/or specific permission. @ 1990 ACM 089791-419-8/90/0012/0001 \n$1.50 1 p G ID (no information addition) ab f Result > p o p = p (idempotence) dc 03 Note that ID (the \nidentity function) is the greatest pro\u00ad jection and ABSENT (the constant function with value 1) the \nleast (under the standard information ordering on func\u00ad tions). Definition 2.z If p and q are projections \nand p u q = ID, then q is a complement of p. Note that by the above definition the complement of a pro\u00adjection \nmay not be unique (for example, ID is a complement of euer~ projection). We will tighten this definition \nin Sec\u00ad tion 3 to achieve uniqueness by choosing the least of these projections. Indeed, a major goal \nof that section is to define domains of projections where such a construction always ex\u00adists. We write \np to denote the unique (to be defined later) complement of p. Definition 2.3 A partial evaluator P&#38; \nis a function which takes representations of a function f, a projection p, and a value a, and produces \na representation of the residual func\u00adtion, fP~, defined as follows: such that where apply takes the \nrepresentation of a function and its argument and produces a representation of the result.] The idea \nhere is to use projections to capture the known parts of the input. When there is no ambiguity we use \nrP to denote PS f p a. Given this notation, note that rr~ = apply f a. Although the partial evaluator \nreally takes representa\u00adtions of its arguments and not actual values, hereafter we will treat it as taking \nvalues as arguments (primarily to avoid having to propagate apply everywhere). On the other hand, the \nalgorithm for combining residual functions in Section 5 depends crucially on manipulating the representations. \n2.2 Incremental Computation Returning now to the problem of incremental computation, we can summarize \nthe situation as in Figure 1, Here the function f (which may be a compiler, text formatter, etc.) is \nbeing applied to a structured argument to give the result. If only part of the argument changes, such \nas part b, we would like to compute the new result without having to redo the entire computation; in \nother words, we would like to avoid having j reprocess parts a, c, and d. ] This can be seen extensionally \nas a restatement of Kleene s S: theorem from recursive function theory. { 1a b  f = Result 6)dc Figure \n1: Incremental computation Now, here s the connection to partial evaluation, and the basis of our framework: \nThe partitioning of the input domain can be described using a set of projections as defined in the previous \nsection; let s call them pa, P6, p., and pd for the example in Figure 1. If we then compute the residual \nfunctions rp., rp,, rP=, and rP,, we have essentially cached those portions of the computation that depend \nonly on parts a, b, c, and d of the input, respectively. Recalling that rlD = apply f a, all we need \nnow to com\u00adpute the final result is a (presumably efficient) way to con\u00adstruct r~D from the set of residual \nfunctions for now, let s assume that such a technique exists. If part of the in\u00adput were to change, \nsay b changes to b , then all we have to do is replace rp~ with rp~, ; computation of rlD then takes \nplace with this new residual function in place. An alternative way to describe this process is as follows: \nAt the point when b changes to b , suppose we had by some means already computed rp~ then all we need \nto do to compute the new result is to apply rp, to b . We can thus view the problem as an attempt to \nfind (at least a conservative approximation to) ~P-~by combining existing residual functions. We can \ndefine all this more formally as follows: Definition 2.4 A partition P of a domain D is a set of projections \n{pi} on D such that U{pi} = ID. Definition 2.5 An incremental program specification is a pair (f, P) \nwhere f : D +. E is the function to be incremen\u00adtalized and P is a partition of D. We now describe an \nincremental interpreter, denoted Z, which captures the methodology described earlier. Z has functionality: \nZ:(f, P) + a. + (&#38;, &#38;,...) + (be, bl,...) (j , P) is the incremental program specification, \nand a. is the initial argument. The 6s are functions capturing small changes to the input, and the k \nare the successive output results. Algorithm 1: * Setup: Compute rPi = P&#38; f pi a for each pi in the \npartition P.  Reestablish: If a changes to a , recompute all rPi for which Pi a# pi a .  Combine: The \nnew result rID is obtained from {rp, } using appropriate combining operations.  The main purpose of \nT is to maintain the invariant: rpi = PL$ f pi ~ for all pi E P, and in so doing satisfies the following \ncorrectness criterion: bi = .f al where ai = 6i_1 al_l The above forms the basis for our approach. But \nwe have so far made many assumptions that require fleshing out. In particular: 1. What is the basis for \nchoosing a good partition of the input domain? (If it is too coarse, even small changes will trigger \nmassive recomputation; if too fine, the stored residual functions will each capture very little computation \nand excessive work will be done in the combhing phase.) 2. How do we combine residual functions to get \nlarger ones? (Does the construction even e~5t? If so, is it unique? Can it be done efficiently?) 3. \nHow does one determine which residual functions need to be recomputed? (Le., how does one determine the \nset of projections that see the changes to the input?)  These and other technical questions are answered \nin the next 3 sections. For examples of the application of our frame\u00adwork, the reader may wish to jump \nto Section 6 and return to the technical sections after having developed more intu\u00adition for the methodology. \nProjection Algebras The most critical aspect of our methodology is the ability to combine residual functions \n correctly and efficiently. We first deal with correctness, which requires the construction of domains \nin which suitable combining operators are well\u00addefined. We begin with a set of domains and domain formers \nthat are adequate in capturing most of the domains found in conventional programming languages. t ::= \n1, Nat,... base domains [ ~i type parameter t,+t, separated sum I t, x t~ non-strict product I t domain \npr,.recursive The structure described above can be easily generahzed to more than one type parameter. \nIn what follows we as\u00adsume that standard domains such as lists, pairs, and natural numbers have been \npre-defined, and we use conventional no\u00adtation (1, 2, NiJ, Cons, etc. ) when referring to elements of \nthe domains. For example, polymorphic lists and pairs can be defined by: 3.1 Projection Domains Consider \na domain of projections under the standard infor\u00admation ordering of functions this domain is not closed \nwith respect to greatest lower bound. This is because pl n P2 = AZ. (pI Z) 11 (P2 Z) is not necessarily \nidempotent, and therefore may not be a projection. A simple example should convince the reader of this: \nExample 3.1 Consider projections on the domain of pairs of natural numbers. Let pl and pQ be defined \nas follows: pl (z,y) = if z=2then (l, y) else (z,y) P2 (z, v) = if x = J-then (z,l) else (z, Y) Let p \nbe Az.(plz) n (PQZ). It is easy to verify that p (2, 3)= (1, 3) which is not the same a. p o p (2,3) \n= (1, 1). Thus p is not idempotent, and is therefore not a projection. This problem arises because the \ndomain of all projections is too large. We are interested (for purposes of partial eval\u00aduation) in projections \nwhich only depend on the structure of the object they are manipulating and not on the ualues of its components. \nLaunchbury [Lau88] describes a smaller (finite) domain of projections, but his domain does not serve \nour purpose because it does not contain useful projections such ss the following one on the domain of \nlists: p Nil = Nil p (Cons x x3) = Cons 1 X5 Here is a first attempt at constructing a domain of pro\u00adjections \nsmall enough to possess properties of interest to us yet large enough to contain examples such as the \nabove. Definition 3.1 A polymorphic projection on a domain F(r) is a collection of instances fA : F(A) \n-+ F(A), such that for any strict function a : A + B, the diagram in Figure commutes; i.e. fB o mapF(cs) \n= mapF(a) o fA. By mapF we mean the appropriate map function for the datatype F. Example 3.2 Define two \nprojections over the domain of pairs: p (-L, b) = (1,1), p (a, b) = (a, b); LEFT (z, y) = (x, 1). p is \nnot pol~morphic , but LEFT is. Unfortunately, the domain of polymorphic projections does not exactly \ncapture the intuition of depending only on the structure of the input. Consider g: g Nil = Nil, g (z \n: 1)=1: J_, g z = z. g is a polymorphic projection but still depends on the values of subcomponents of \nits input (in this case 1 as the tail). To restrict away projections like g, we define a domain of projections \nas follows: fA only projections defined on them are ID and ABSENT). A Assume that dl ~ dl and d2 ~ \nd2 are commutative dc- F(A) =-F(A) mains, pl, p3 c dl ~ dl, p2, p4 C d2 ~ d2. (Pl xp2)o(p3 XP4) = (plop3)x(p20p4) \n= (P30Pl)x(P40 mapF (CL) P2) = (P3 xP4)o(P1mapF (CZ) is commutative. x@). Thus dl X d2 ~ dl X d2 II \n F(B) >F(B) f B Figure 2: Polymorphic projections ID [\\ LEF RIGHT V ASET Figure 3: Projections on the \ndomain of pairs P (d) = { ~Dd, AB.$ENTd } (d is a base domain) P (r) = { IDr, ABSENTT } P (dl + d2) \n= {PI +P2 IPI C P(d~), P2 E P(d2) } P (dl Xd2) = {PI x P2 IP] CP(dl), P2 P(d2) } P (pT.T(r)) = P(T(ur.T(~))) \nU { ABSENTP,,~(,) 1 The subscript to ID or ABSENT refers to the domain of definition. + and x are defined \nfor functions as follows: (~ x g)(a) ~)= (fa,g~) and (f+ 9)1= -L, (f + g)(ird a) = inl(~ a), (~ + g)(in~ \nb) = inr(g b). Note the case of the recursive domain where the ABSENT projection can be invoked on any \ntail of the list. We use D ~ D to denote the domain defined above, under the standard information ordering. \nNote that any element of D ~ D is guaranteed to be polymorphic. Example 3.3 Consider projections on the \ndomain of pairs, Pair(rl, TZ) ~ Pair(rl, TZ) is shown in Figure 5 , where LEFT(x, y) = (x,1) and RIGHT(x, \ny) = (1, y). 3.2 Properties of D - D Definition 3.2 A commutative domain is one whose ele\u00adrnent9 commute \nwrt function composition. Lemma 3.1 D ~ D is a commutative domain. Proofl The proof is by induction on \nthe structure of do\u00admains. The base domains are commutative (because the (PI +p2)o(p3 +P4) = (PI op3)+(p20p4) \n= (P30Pl )+(P40 P2) = (P3 +P4)o(P1 +PZ). Thus dl +dz ~ d] +d2 is commutative. In the caae of the recursive \ndomain, if one of the pro\u00adjections chosen is ABSENT commutativity holds since p o q is ABSENT if either \np or q is ABSENT. If both of the projections are in ?J(T(r)), then whether or not the projections commute \ndepend on whether the elements of P(pr.T(r)) chosen in place of r commute. This follows from the fact \nthat since T is constructed from + and x, T(7) com\u00admutes whenever r does. Repeating the earlier argument, \nif ABSENT is chosen for either of the projections, we are done. By repeating this argument, as long aa \nABSENT is chosen at some level of recursion, the projections will com\u00admute. The other case is when ABSENT \nis never chosen. This represents the single projection pp. T(p). In this caae since both the projections \nhave to be the same, they com\u00admute. Cl We prove the following properties (Lemmas 3.2 through 3.4) for \nanY p, g from a commutative projection domain. Lemma 3.2 poq is a projection, if p and q are members \nof a commutative domain. Proof No information addition: p E ID and q ~ ID ~ poq~ID. Idempotence: (poq)o(p \noq)=po(qop)oq= po(poq) oq=(pop) o(qoq)=poq. 0 Lemma 3.3 In a commutative projection domain, the great. \nest lower bound (glb) of p and q exists and is p o q. Proofi Note that since p Q ID and q~ ID, poq L \npand poq~q.Let rbeany projection such that r~pand r~q. Then by montonicity, r o r ~ p o q and by the \ndefinition of projection, r ~ p o q. 0 Lemma 3.4 In a commutative projection domain, the least upper \nbound (lub) exists. Proof7 (By contradiction.) Suppose that there exists p and q such that p u q does \nnot exist. Since ID is the top element, this implies that there exist two incomparable upper bounds \n 11 and fZ, and that there is no upper bound less than both 11 and 12. (This is because there are no \ninfinite descending chains in D ~ D. ) But since glbs exist, 11 012 is an upper bound of p and q which \nis (by definition of glb) less than both 11 and 12. Contradiction. 0 Commutative domains with the additional \nproperty of distributivity are of special interest. Definition 3.3 A domain is said to be distributive \niff for all elernentsp, g, r of the domain, pu(gflr) = (pUg)t_i(pUr) andpn(q ur)=(pnq)u(pn~). Lemma 3.5 \nD ~ D is a distributive domain. Proofi The proof is by induction on the domain structure, We detail the \nproof of pLI(qnr) = (pLIq)n(p u T); the proof of p n (q 1-lr) = (p n g) LI (p n r) is similar. The distributivity \nproperties of the base domains are easy to verify. Assume that dl ~ dl and d2 ~ d2 are distributive, \nand pl, gl, rl 6 dl and P2, g2, r2 c d2. In what follows we write (~, g) for fxg. (P1jP2) U((91,9Wrl,r2)) \n= (Pi, P2) u (910r1,920r2) = (Pl U (91 Orl), P2 U(92 OrZ)) = ((Pl LJql) n(pl u ~1), (P2 Uq2) ri(p2 ur2)) \n= (PI U91, P2 uq2)n (P1u T1, P2 UT2) = ((Pi, PZ) U(91,92)) n((Pl, P2)U(rl, r2)). Thus dl x d2 @ dl x \nd2 is distributive. The proof for dl + d2 works similarly. In the csse of the recursive domain construction, \nfirst note that if any of p, q or r is chosen as ABSENT, the distributivity property holds. If none of \nthem are ABSENT, then each of them must be in P(Z (T)) where r is in P(pT.T(r)). Now by the above argu\u00adments, \nwe know that T(r) commutes whenever r does. Thus if any of the TS are chosen as ABSENT then we are done. \nWe can repeat this argument to show that as long sa ABSENT is chosen at some level of re\u00adcursion, commutativity \nholds, The other case (when ABSENT is never chosen) represents the single pro\u00adjection ,ap.T(p). In this \ncase since the three projections are the same, distributivity holds. Lemma 3.6 For a commutative distributive \ndomain, there exists a least T such that p ~ q U r. Proofi clearly there is always at least one T which \nsat\u00adisfies the definition (take T = lD ). Assume we have two incomparable elements T1 and T2 such that \np ~ q U T1 and p ~ q U rz. Also assume that there is no element smaller than both rl and r2 satisfying \nthe difference condition. Then p L q U T1 and p ~ q U r2, implies (from definition of glb) P Q (qU rl) \nn (qUrQ) and by distributivity: p G q u (rl nrQ). But rl n r2 is less than rl and r2 and it satisfys \nthe differ\u00adence equation. Contradiction. (In case there are an infinite number of r; satisfying the above \ncondition, the existence of the infinite glb needs to be shown. The infinite glb exists because we know \nthat the domain has no infinite decreasing chains (this can be seen from the definition of D ~ D) and \nrl, rl n T2, rl n r2 n n,... is a decreasing chain. It is also easy to see that lub distributes over \nthe infinite glb.) 0 For domains which are distributive in addition to being com\u00admutative, we define \nthe difference operation as follows: Definition 3.4 If p and q are elements of a commutative distributive \ndomain, the difference of p and q (written p q) is the least r such that p ~ qU r. Lemma 3.6 ensures \nthat the difference is uniquely and well defined. This leads us, aa promised, to a unique definition \nof complement: Definition 3.5 The complement P of an element p in a commutative distributive domain is \nID p. 3.3 Algebraic Properties of D ~ D The properties that we have thus far defined were motivated \nby our application to incremental computation. Interest\u00adingly, they form what is known as a Brouwerian \nalgebra. Definition 3.6 A Brouwerian algebra is an algebra (L, U, n, ~, T) where (L, U, i7) is a lattice \nwith greatest e/e\u00adment T, L is closed under ~, and a~b ~ c ifla ~ b Uc. A Brouwerian algebra can be seen \nas a generalization of a Boolean algebra where the following equation need not hold: ID (ID p)=p. Theorem \n3.1 ( l? s V, U, 17, , ID) is a Brouwerian alge\u00ad bm. Proof: Follows directly from lemmas 3.3 through \n3.6. 0 Knowing that we are dealing with a Brouwerian algebra al\u00adlows us to use known properties of such \nalgebras for rea\u00adsoning about our projection domains. For example, when specifying an incremental program, \nthe partition we are in\u00ad terested in may only be a subset of D z D, and thus we may need to extend the \ndomain to make it Brouwerian; it is a known theorem that such completions always exist. The following \ntheorem partially addresses this problem, quoted without proof from [MT46]: Theorem 3.2 If (L, U,n, , \nT) is a Brouwerian algebra, and M is a finite sub<et of L containing n elemfnts, then there exists a \nsubset L of L and an operation with the following properties: (L , u, n,- , T) is a Brouwerian algebra. \n L contains at most 22 elements.  M is a subset of L .  cIfx, Yandz-yarein C ,thenx y=x y. One possible \nL is the set of all elements of C which are expressible as US and m of elements of M. Example 3.4 Consider \nprojections on the domain of lists of pairs. D = List(Pair(a, b)). A finite subdomain of D ~ D is shown \nin Figure 4. LEFT, RIGHT and ABSENT are pro\u00adjections on pairg. Here L = D = D and C and M are the subdomain \nshown in the figure. However, if ID were not one of the projections, then the algebra would not be Brouwerian. \nPff(pnq) a=(Ptfpa)n(P&#38;fga) Ptf(p g) f3=(Ptfpa)-(Ptfga) 1  /lD\\ MAP(LEFT) Corollary 4.1 U, n and \n on the domain R are the lub, glb and difference operations, respectively. .>ABZ(R GHT) We quote the \nfollowing theorem from [MT46]: t Theorem 4.2 Any homomorphic image of a Brouwerian algebra is a Brouwerian \nalgebra. ABSkNT Since the domain of residuals is a homomorphic image of Figure 4: Projections on the \ndomain of lists of pairs the projection domain, we can state that: Corollary 4.2 (%, U, Fl, , TID) is \na Brouwerian algebra. An interesting and much more extensive application of Brouwerian algebras, the \nmodelling of program integration, may be found in [Rep90]. 5 Algorithm for Least Upper Bound of Residual \nFunctions Residual Function Algebras We now return to partial evaluation. We demonstrate that th~ d~main \nof projections in the last section induces an iso\u00admorphic domain of residual functions. This will make \npre\u00adcise the notion of combining residual functions which was alluded to earlier. Definition 4.1 The \ndomain 72. of residual functions for a function f, its argument a, and a commutative, distributive domain \nof projections P is defined as R = {r I r = PE f pa, p G P }, The ordering relation on R is defined as \nfollows: rl&#38;r2iflp ~qwhererl=PE fpaandr2=PEfq afor some f, p, q and a. We define U, n and for the \ndomain R as follows: def rptirq = Tpuq, de f rp~rq = rPnqand def Tp rq = Tp q It is easy to verify that \nthe ordering on residuals is a partial order. This ordering is intimately related to the standard information \nordering. If the type of f is A -B, the type of a t esidwd function for a fixed argument a is A + B where \nA is the subdomain of A of elements ~ a. The monotonicity of the PE implies: Theorem 4.1 Given a commutative, \ndistributive projection domain P and its corresponding domain of res{dual func\u00ad$ions 72., Ap. P&#38;f \np a is a homomorphism from (P, U, n, , ID) ~0 ( )?, U,n, -, TID). Proofi Clearly PE preserves the identities \nof U, n and . It follows dir~ctly from definition 4.2 that PS satisfies: PEf(p Ug)a=(PE fpa)U(P~fqa) \nThe last section described three binary operations on resid\u00aduaJ functions: U, n and , but did not describe \nalgorithms for them. If we had an efficient algorithm for , the whole problem of incremental computation \nwould be solved! But this is a difficult operation to compute since it involves backing up of computation. \nOur methods can be seen as trying to approximate by using the other two opera\u00adtions. In this paper (and \nin all applications that we have investigated to date) we only use u, and thus in this section we develop \nan efficient algorithm for it. Since we know that PE is a homomorphism from the domain of projections \nto the domain of residual functions, the following equality holds: (PSfpa) Ll(PZfq a)= PEf(p Uq)a While \nthis gives us a simple method to compute the lub, it is obviously inefficient, since it ignores the work \nalready done in computing rp and r~. A good algorithm will avoid redoing any reductions already done \nto compute rP and rq. Indeed if our incremental interpreter is to achieve good performance, this is essential. \nIn what follows, we assume that the partial evaluator is implemented using binding time analysis. This \ntechnique has been shown to be crucial in achieving self\u00adapplication of partial evaluators [JSS89]. Binding \ntime analysis. The algorithm to compute the lub uses binding time information of the two residual functions \nin the form of action trees [CD90]. Binding time analy\u00adsis computes the binding time (static or dynamic) \nof each expression in the source program given the binding times of the argument. For purposes of partial \nevaluation it is common to determine the binding time information prior to actual specialization. The \nsource program (a ~-term) is represented as a tree with three kinds of nodes: application, abstraction \nand variable. Given a source program and a de\u00adscription of the binding time of its input (in the form \nof a projection), the result of binding time analysis is an action tree, isomorphic to the source program, \nwith the following nodes: Reduce: An action tree which says process the chil\u00addren of the syntax tree \naccording to the action subtrees rooted at this node and then Reduce the node.  Rebuild: An action tree \nwhich says process the chil\u00addren of the syntax tree according to-th~ action subtrees rooted at this node \nand then Rebuild the node.  The specialize simply processes each node of the source program by executing \nthe corresponding action. Algorithm LU B. We describe the algorithm in the context of the J-calculus; \nit can easily be adapted to languages with more syntactic sngar, Aleo, we assume that the fnnctions have \nfirst been a-converted to avoid any name clashes. A residual function is described by a triple (r, a, \ne) where T is the A-term representing the residual function, a is the action tree which produced the \nresidual function and e is an associated environment which is initially empty. The environment is meant \nto map variables to pairs of the form (t, a), where t is a A-term and a is the associated action tree. \nAlgorithm LUB( (TI, al, el) , (rz, a2, ez) ) Apply rewrite rules in Figure 5 to (rl, al, el ) U (T2, \nLZ2, ez) until all u symbols are removed from the term.  Reduce nodes all of whose children are reduced. \nThis is needed to perform reductions not performed by either of the two residual functions but made possible \nby their combination.  To reduce the number of rules, symmetric cases have been omitted from Figure \n5. The rules can be understood if we realize that the motivation is to avoid doing any reduc\u00adtion which \nhas already been done in any one of the other residual functions. For example in rule 1, an application \nhas been reduced in one residual function but has been left residual in another. The rule reduces this \nto taking the U of the bodies of the abstractions, but also updates the en\u00advironment of the second residual \nfunction so as not to lose reductions performed in the argument. The other rules are similar. There is \na final post-processing step to be executed, whose purpose should be clear from the following example: \ng (x, y) = if (x == O) then f (x,y) else f (x,y) -5 f (X,y) = x*x -y*y Partial evaluating g with the \nprojection LEFT and argu\u00adment (0,2) gives:z gl (X, y) =o-y*y Partial evaluating g with the projection \nRIGHT and the same argument (0,2) gives: 2Note that since residual functions have thesame type asthefunc\u00adtion \nbeing partially evaluated, gl still takes a pair ss argument. It simply ignores the left element of the \npair, A similar comment applies to g2. 1. (rl, Reduce(all)(alz), el)u ((k.tz~)(tzz),e,) Rebuild, =+ \n(rl, all, e] )U ( t~~, rz~~, e~[z/( t~z,azz)]) 2. (rl,Reduce(a11)(a12 ),el[r/(tl, al)])U (z, Rebuildoo, \nez[z/(tz,az)]) ~(rI, al, el)U (i?2, a2, e2) 3. ( rI, RedMall)(a12), e, ) u (u, Reduce(azl)(a,, ),ez) \n* (n, all, el )U ( T2, azI, ez ) (If the node reduced was an application) 4. (TI, Reduce(all)(a~, ),e~[z/(t,, \na,)])U (n, Reduce(az~)(azz), ez[z/( t2, a2 )1) +(rl, al, el)U (nz, az, ez)(Ifthe nodereduced wasavariablex) \n 5. ((k.tll)tlz,z), Rebuild(a~~)(a~ el)U ((h.tzi)tzz,ez) Rebuild(az~)(azz), *Az. ((tIl, all, el )U(t21, \na21, e2)) ((~12, a12, el)U(~22, a22, e2)) 6. (x, Rebuildoo, el[x/(tl, al)]) U ( x, Rebuildoo, ez[m/(tz,az)] \n) =+ (tl,al,el) U (t2, a2, e2) 7. (z, Rebuildo[), el)U (z, Rebuild(j(j, ez)==+z (ifneither elnorez have \nbindings for x).  Figure5: Rewrite rules for algorithm LUB g2 (x*y) =if (x == O) then x*x -4else x*x \n- The result ofusing theabove rewrite rules is: g12 (X, y) =o-4 Clearly this can be further reduced; \ni.e., there may be reductions in the least upper bound which are neither in gl nor g2. These can be performed \nthrough another phase of partial evaluation. Before we go on to examine properties of algorithm LUB, \nnote that as long as there is still a U in the term, one of the rules will apply. Termination of the \nfirst phase of the algorithm is not difficult to verify: each rule reduces the size of the A-term under \nconsideration, until the base case is reached. The second phase may not terminate when the source program \ncontains non-terminating computations, but this is common among partial evaluators. First we prove the \ncorrectness of algorithm LUB. To do so let us first exam\u00adine some properties of action trees. The domain \nof actions is a two point domain with Rebuild ~ Reduce. We now define an ordering on action trees. Note \nthat since action trees are isomorphic to the source program, they are isomor\u00adphic to one another. Thus \nif atl and at2 are two isomorphic action trees for the same program, there is an obvious one\u00adto-one \nonto mapping from the nodes of atl to the nodes of at2. Definition 5.1 An action tree ai?l is ~ atz ifl \neuery action in atl is ~ its corresponding action in at2. It is not difi\u00adcuitto see that least upper \nbounds exist under this ordering. An enabling transformation on an action tree is the rep/ace\u00adraent a \nRebuild action node whose children are all Reduce action nodes by a Reduce action node. The closure o,f \nan action tree is the result of repeatedly applying enabling trans\u00ad formations to it until no opportunities \nto do so rtmain. We denote the closure oft by C(t). How is the action tree corresponding to rpUq related \nto rP and r~ ? We use the notation atP to refer to the action tree corresponding to rP. Lemma 5.1 atPug \n= C(atP u atq) Proof: Any expression marked static in either rP or r~ must be marked static in atPuq. \nIn addition if this makes all the children of a Rebuild node Reduce, then the Rebuild node must also \nbe made into Reduce. 0 Lemma 5.2 Algorithm LUB outputs a lambda term whose action tree is C(atp u atq) \nwhere rp and rq are the inputs to the aigorithrn. Proofl By case analysis of the rewrite rules of the \nalgorithm, it is not difficult to see that the result of the first phase has the following action tree: \natp u atq. This is because each rule chooses Reduce over Rebuild. The post-processing stage simply applies \nas many enabling transformations as possible, I.e. it computes the closure of the action tree. 0 Theorem \n5.1 Algorithm LUB correctly computes the ieast upper bound. Proofi From the last two lemmas we note that \nthe output of algorithm LUB and atPuq have the same action tree. Since both are specializations of the \nsame function, they have to be equal. I Theorem 5.2 Algorithm LUB does not re-perform any re\u00adduction \nalready performed in the computation of either Tp or Tq . Proot? During the application of the rewrite \nrules, no reduc\u00adtions are done (all of them are done in the post-processing phase). At the end of this \nphase, the term has the action tree atP U atq. This means that all reductions in rP and 7* are already \nincorporated, 0 Applications In this section we consider two well known problems data flow analysis \nand attribute grammar evaluation for which we have recast, and implemented, existing algorithms us\u00ading \nour framework. Other problems for which we have con\u00adstructed incremental programs include strictness \nanalysis, Hindley-Milner type inference and solving simple systems of constraints [Sun91]. \\Lo L1 Y L3I \nFigure 6: Example flow graph 6.1 Incremental Data Flow Analysis As an example of compiler data flow \nanalysis, consider the problem of determining the set of reaching definitions at every program point. \nA definition of a variable is said to reach a program point if there exists a path from the def\u00adinition \nto the program point which does not pass through a redefinition of the same variable. For example, consider \nthe flow graph in Figure 6 (taken from [MR90]). Each of the circles denotes a basic block, where a label \nx = means that x is assigned a value in that block. Arcs are labelled with sets Li of definitions which \nreach that arc. For exam\u00adple, the set LO = {(z, cl), (y, e2)} means that the definition of z at program \npoint e1 and y at e2 reach the arc labelled LO. We can then write the following equations ( ? refers \nto a wildcard): Lo = {(z, el), (v, d)} L1 = L3UL3 L, = L, {(x,?)} U {(z, B)} L, = (L1 U~2) {(y,?)} \nU {(y, C)} The solution to these set equations is defined by a least fixpoint construction in the obvious \nmanner, yielding: Lo = {(t, el), (y)a)} L] = {(x, el), (y, e.2), (x, B), (y, c)} L, = {(u, ea), (Y, C \n), (X, B)} L3 = {(z, el), (z, B), (V, C)} Recall that an incremental algorithm specification con\u00adsists \nof a non-incremental program plus a partition of the input domain. Therefore we first need to describe \na non\u00adincremental algorithm. (In this and subsequent examples Haskell [H We90] syntax is used to describe \nthe algorithms.) We assume the input to the algorithm to be a list eqns of strongly connected components \nof the set of data flow equations in topological order (which can be produced by a standard dependency \nanalysis). The overall solution is defined by: df a eqns = foldl fix null env eqns where fix (defined \nbelow) is a function which takes an ini\u00adtial environment mapping arc labels to sets, and a set of mutually \nrecursive equations, and computes the leaat fix\u00adpoint of the equations using the initial environment \nas the first approximation. The fixpoint is found by computing the lesat upper bound of the ascending \nKleene chain; since the domain is finite, termination is guaranteed. fix env eqns = if env == env then \nenv else fix env$ eqns where erm$ = eval eqns env eval is a function which recomputes the identifiers \ndefined by the equations eqns using the old environment env to produce anew environment env . The next \nstepisto describe apartition ofthe input. The partition we use is quite simple: p=[pri i<-[1..]] where \npr 1 [1 =bot pr 1(X:XS) =X :bot pri [1 =bot pr i (X:XS) = bot : pr (i-1) xs Note thatpr i,theith projection \nin the partition, discards all information but the ith element of its input list (bet is the unknown \nmarker forthe partial evaluator). Also note that although the partition itself is infinite (the notation \n[1. .1 denotes the infinite list of positive integers), for a finite list only finitely many of these \nprojections are needed; i.e., there are only a finite number of residual functions to store. The incremental \nalgorithm specification is thus (df a, p). Example 6.1 We use the flowgraph in Figure 6 to detail the \nworking of the algorithm. Assume that it represents the second connected component in the input list. \nDuring the Setup phase, the computation of theresidual function cor\u00ad reponding to projection pr 2 unrolls \nthe fixpoint iteration for that component to yield: (this issanitized Haskell code representing the actual \noutput of our partial evaluator) dfa-pr2 (eqnl : ( -: reet_eqns)) = foldl fix env2 rest-eqns rrhere env2 \n= eval [ (Ll, (rmi.on,LO, [(x,B),(y,C)])), (L2,(union,(diff,L0, [(x,?)]), [(x, Ft), (y, Ol)), (f-.3,(union,(diff,LO, \n[(y,?)]), [(x, It), (y,c)l)) 1 env 1 envl ~ fix null-env eqnl Note that dfa_.pr2 ignores the second \nelement of its input (the notation .meansdon t care). A .imilar computatiorr has been carried out for \neach element of the partition (a connected component). If any of the components change, then the Reestablish \nphase computes the residuatfor the affected members of the partition. During the third phase (Combine), \nthe result is obtained by computing the lub of all the residual functions, which effectively propagates \nin\u00adformation intopological order between the components. For example, the predecessor (in topological \norder) of the com\u00adponent above will suppJy the value of Lo, which will enable this component to suppl~its \ntopological successor the value of L3, and so on. It is easy to see that there is no fixpoint iteration \nin this stage. Note that partitioning into strongly connected compo\u00adnents plays an important role in \nenabling fixpoint iteration to proceed to completion even when only part of the input is known. This \nis essentially the intuition behind the algo\u00adrithm in [MR90]. How can webe sure that thw algorithm posseses \nthe same time complexity aa the one in [MR90]? The algorithm in [MR90] has two main steps (called steps \nand 6in thepaper).3 The first step corresponds torecom\u00adputing residual functions of projections which \nsee changes in the input. The second corresponds to the computation of the result via alub construction. \nDuring the Setup and Reestablish phases the partial evaluator haa enough information to unroll the function \nfix fora given strongly connected component. Lemma 6.1 The cost of the Reestablish phase is propor\u00adtional \nto the size of the affected strongly connected compo\u00adnent. Proofi Thecost ofcarrying fixpoint iteration \n(for this prob\u00adlem) to completion is well known to be 0(13 x V) where B is the size of the component \nand Vis the number of variables of interest. Forafixed number ofvariables thecost is O(B). u The next \nstep simply combines residual functions using the U operation. No fixpoint iteration needs to be performed. \nThe only taak left to do is to transmit information among the various components. The cost of this is \neasy to see: Lemma 6.2 The Combine phase does no fizpoint itera\u00ad tion, but only propagates information \nin topological order. The cost of this step is proportionalto the condensed flow graph. Proof: Follows \nfrom the fact that theinformation is prop\u00adagated in topological order with a constant amount of work \ndone at each component. 0  6.2 Incremental Attribute Evaluation The use of attribute grammars [?] to \ndescribe language specifications is well known. These specifications have been used aa the starting point \nfor the generation of language based programming environments [Rep84]. Programs in this model are represented \naa attributed trees, i.e. syntax trees sThe first four steps of the algorithm in [MR901 are implicitly \npresent in our algorithm because of the input DataFlowAnalysis ex\u00adpects, namely strongly connected components \nof the flow graph in topological order. N-+SL { L.scale o N.val ij S.neg then L.val else L.val } s \n++ { S.neg false } s+-{ S.neg tTue } L -+B { B.scale L.scale L.val B.val } LO -+ LIB { LI scale LO.scale \n+ 1 B.scale Lo.scale Lo.val L1.val + B.val } B-+0 { B.val 0} ~B.sc51e }  B+l { B.val Figure 7: Attribute \ngrammar for signed binary numerals N (o) s (1) L (2)  1 / k L (3) B (4) I B (7) I 1 Figure 8: Parse \ntree for -110 with attributes carrying semantic values. The goal of incre\u00admental attribute evaluation \nis to efficiently produce a well attributed tree after each editing operation. In what fol\u00adlows, editing \nis modelled by subtree replacement, and we only consider raon-circtdar attribute grammars. As usual let \nus first describe a non-incremental algo\u00adrithm, which we do by means of an example. Consider the attribute \ngrammar iu Figure 7, which describes the syntax for signed binary numerals, as well as the semantics: \nthe decimal value that the numeral denotes. For example, the parse tree for -110 is shown in Figure 8 \n(we will refer to this example later). Given an attribute grammar and a parse tree, the non\u00adincremental \nalgorithm for attribute evaluation is as follows: e Generate the equations described by the parse tree. \n(For example, Figure 9 shows the equations for the parse tree in Figure 8; note the subscripting of the \nattributes by the node numbers.) Generate a dependency graph, where an attribute ai valo ifneglthen \nva12elseva12 scalez o negl true valz vala + va14 scale3 scale2 + 1 scaled valh vala scales scaiee valG \nvals va17 scale? scales ~scale, valr Figure 9: Attribute equations for the example parse tree depends \non aj iff the right hand side for the equation defining ai contains aj. (See Figure 10.) Q Perform a \ntopological sort of the dependency graph, to get a safe order of evaluation of the attributes, and then \nperform the evaluation.4 To generate an incremental algorithm, we need only spec\u00adify a partition of \nthe input domain (in this case a parse tree). As a first attempt at such a partition, suppose pi is the \npro\u00adjection reflecting the fact that only the descendants of node z are known. Then ~ is the projection \nwhere every node ew cept thedescendants of i are known. A candidate partition is thus: P = {pi} u{fi} \n Recall that the Setup phase computes the residual func\u00adtions corresponding to each projection in the \npartition. Us\u00ading the above partition for Figure 8, let us see what r= looks like. The partial evaluator \nhas knowledge of all nodes except 7. This means that it can construct the modified dependency graph shown \nin Figure 11, where the dotted portion is unknown. It is not difficult to see that the partial evaluation \nof a topological sort on this graph will result in the evaluation of Scaiez, scares, Scaiel, sca]es,, \nscaietj, va(4, va/6, and negl, even though the subtree rooted at 5 is not known. The Reestablish and \nCombine phases work as follows. If we are given a new subtree at (say) node 5, we compute and TP5 to \nobtain the answer. But TP5 and take the lub of TIF now the Reestablish phase can be unacceptably expensive, \nChanging a subtree rooted at node 5 causes residual func\u00adtions far away from node 5 to be altered. This \nmeans that every time a subt ree is changed, unacceptably many residual functions will have to be recomputed. \nThis problem leads us to seek a new partition. Con\u00adsulting the literature, we find iu [RTD83] an incremental \n4 Since we are considering only noracircuiur attribute grammars, the graph is guaranteed to be acyclic. \nalgorithm forthe same problem which reduces the amount of work done while updating the stored information. \nWe  / 1 2\\ scaleh scales / 1 scales scales + W&#38;3 IVa,, A--\u00ad 4 scaler+ va17+ a vale / ~eg~ \\ valz \n~ valo Figure 10: Dependency graph calez / \\ scalee scales / td scaleb 4r-------I I scale? I I I +[ \nI I va17 I i I b-----\u00ad -, --- I I I I I vala \\ / valz / 5 Figure 11: Partial dependency graph can use \nexactly the same method, described below in our framework. The key idea is to make use of a restricted \nediting model, in which cursor position is maintained: The cursor is at auy given moment at one node \nin the parse tree, and can be moved in one of two ways: to the parent node, or to one of the children. \n The only edit operation permitted is subtree replace\u00adment.  Thus at any given moment the nodes can \nbe partitioned into three disjoint sets: The first is the singleton set R consisting of the cursor position \nr. The second is the set S of nodes on the path from the root to the cursor position, including the root. \nThe third is the set T of remaining nodes, which includes nodes below the cursor. Using this knowledge, \nthe new input partition is as follows: P={Pi,~liGR}U{PiliET}U{~l iGS} The Setup phase is similar to that \ndescribed for the previous partition, only simpler. The Reestablish phase, however, is more interesting. \nConsider a node a with three children b, c and d. There can now be three kinds of changes. Move To Parent. \nThe set T gets a new member (the old cursor position); but since the old R had both TP and v Corwuted, \nthere is no work to do. The set S loses a member, so again their is no work to do. The set R gets a new \nmember for which we need to compute rP. Moving from child b (say) to parent a, the operation needed to \nReestablish the invariant is: Tpa=Tpb ~ Tpc ~Tpd Move to Child. By similar reasoning, the operation \nto be performed for a move from parent a to child c is:  Replace Subtree. Here we simply need to recompute \nTp for the current cursor position. By virtue of the partition, nothing else needs to be changed (cf. \nthe previous partition).  When a subtree is replaced at node p, the new result is obtained by computing \nTp u Tp. The first observation we make concerns the computation of Tp: any attribute which does not depend \non projection p of the tree gets a value during the computation of Tp. How much work is done when a subtree \nis replaced? Theorem 6.1 The work done b~ the Reestablish phase after a subtree replacement is proportional \nto the number of attributes potentially affected by the change. Proof During the Reestablish stage, \nthe work done is in computing those attributes in the modified subtree which do not depend on the rest \nof the tree. During the Com\u00adbine phase the work done is in computing those attributes in the modified \nsubtree which depend on the rest of the tree. Thus the total work done is exactly in recomputing those \nattributes which are potentially affected by the sub\u00adtree modification. 0 Compare this to the algorithm \nin [RTD83] which achieves a better time complexity, namely that an attribute will be re-evaluated only \nif the values of any of the attributes it depends upon change. The algorithm we outlined will re\u00adevaluate \nall attributes which depend on the changed subtree. During cursor movement, the algorithm in [RTD83] \nachieves unit cost per move. The algorithm we have outlined takes time proportional to the number of \nattributes whose values are resolved as a result of the u operation. We are investi\u00adgating ways to improve \nour solution to match the efficiency of [RTD83]. Brief Comparison With Other Approaches Readers familiar \nwith the literature of partial evaluation will recall Lombardi and Raphael s pioneering paper [LR64], \nwhich coined the term partial evaluation in the context of doing incremental computation. However, their \nnotion of incremental computation was to monotonically add infor\u00admation about the input to a function, \nand to do as much computation as possible at each step. This is achieved by computing a series of residual \nfunctions each of which is the result of partially evaluating the previous one with the ad\u00additional input. \nOur definition of incremental computation (and what is generally understood by the term today) is more \ngeneral thau this in that changes to the input need not always be in the form of adding information \ninformation can change non-monotonicail y. Thus our work can be seen as a generalization of the work \nwhich originally introduced partial evaluation. The goals of all the following approaches are very similar \nto ours, but -they differ in methodology. INC. Yellin and Strom [YS89] describe a restricted func\u00adtional \nlanguage for incremental computation. The main data structure in the language is a bag. Programs written \nin the language make use of certain combining forms which are guaranteed to have efficient incremental \nperformance. The emphasis in this work has been to design efficient incremen\u00ad tal algorithms for the \nvarious operations in the language. A main difference is that the target language for INC are objects \ncalled circuits. Our target language is the same as the one in which the non-incremental program is described. \nCaching. Pugh [Pug88] tackles the problem of caching re\u00adsults of function calls to achieve incrementality. \nThe scheme depends crucially on clever run-time support. In addition, programs have to be written using \nwhat Pugh calls stable de\u00adcompositions of data structures to obtain good performance. Incremental A-Calcultrs \nReduction. In [FT90] Field and Teitelbaum construct an incremental evaluator for the J\u00adcalculns. It keeps \ntrack of exactly which reductions did not depend on the part of the A-term which changed. For this purpose \nthe parts of the term which can change have to be marked in advance. In contrast with all these approaches, \nour approach allows the possibility of generating a com\u00ad piled incremental program via partial evaluation \nof the in\u00ad cremented interpreter itself. 8 Discussion Compiled Incremental Programs. The use of the \nincremen\u00adtal interpreter Z gives rise to inefficiency due to a level of in\u00adterpretation. Inspired by \nthe Futamura Projections [Fut71], we can achieve a process of incrementalization as follows:5 (we use \nthe notation [f] to denote the function correspond\u00ad ing to the program ~, i.e. the semantic function \nof the lan\u00adguage) [l f] ~ (f, P)= fin. Running ~inc is more efficient than using the incremental interpreter, \nand has the advantage that it can be used inde\u00adpendently of the incremental interpreter. Using the second \nFutamura projection, we can self apply the partial evalua\u00adtor to generate an incrementalizen a program \nwhich con\u00adverts incremented program specifications into increment al programs. Binding Time Analysis. \nBinding time analysis is essential for good performance of the incremental programs we have described. \nSince the partition is known before the actual data elements are available, binding time analysis can \nbe carried out off-line. This means that the self-application described above can avoid the overhead \nof interpretation. Binding time analysis is usually achieved by an abstract in\u00adterpretation of the program. \nTo ensure termination, finite domains are usually used. But as we have seen our domain of projections \nis infinite (although the number of projections used in any given session is finite). One way to overcome \nthis problem for implementation purposes is to place a fixed up\u00adper limit on the size of the input data. \nWe are investigating other solutions. Maintaining the Partition. We have not discussed the cost of maintaining \nthe input partition. In some cases (the at\u00adtribute grammar example) the cost of maintaining the parti\u00adtion \nis not very high. In the case of the data flow analysis we 5 In ~ha~ follo~~ w se the more COmmOn nOtatiOn \nfOr partial val\u00aduation: a partial evaluator takes a function and some of its arguments to produce a residual \nfunction. must employ a method to maintain the strongly connected components of the flow graph, Restrictions \non the use of Residual Functions. A draw\u00ad back of our framework is that residual functions cannot be \nfreely used in the following sense. If two data flow graphs share a strongly connected component, it \nshould be possible to use the residual function from one incremental session in the other session. But \nthis may not be possible since the two strongly connected components, although they are the same, may \noccupy different positions in the list of strongly connected components. We are investigating ways of \nover\u00ad coming such restrictions. Conclusions We have presented a framework for constructing incremen\u00adtal \nprograms from their non-incremental counterparts, We have used the notion of partial evaluation to provide \na basis for our framework. This is particularly appropriate since binding-time analysis (an important \nphase of partial eval\u00aduation) is concerned with analysing dependencies in a pro\u00adgram based on the known/unkown \nsignature of the input. This approach offers a degree of automation in the construc\u00adtion of incremental \nprograms. The framework presented in this paper (including the examples) has actually been im\u00ad plemented. \nFor details of the implementation, the reader is referred to [Sun90]. Acknowledgements. Thanks to Charles \nConsel, Olivier Danvy, John Hughes, G. RamaJkgam and Tom Reps for their de\u00adtailed comments and suggestions. \nThanks also to Juan C. Guzmi$n for help with 14TEX. References [CD90] C. Consel and O. Danvy, From interpreting \nto compiling binding times, In Proceedings o.f the %-l European Symposium on Programming, Lecture Notes \nin Computer Science, Vol. -&#38;2. Springer- Verlag, May 1990. [FT90] J. Field and T. Teitelbaum. Incremental \nreduc\u00adtion in the lambda calculus. In Proceedings of the 1990 ACM Conference on Lisp and Functional Programming, \nJune 1990. [Fut71] Y. Futamura. Partial evaluation of computation process-am approach to a compiler-compiler. \nSys\u00adtems, computers, Contro/s, 2(5), 1971, [HWe90] P. Hudak and P. Wadler (editors). Report on the programming \nlanguage Haskell. Technical Report YALEU/DCS/RR-777, YaJe University, Depart\u00ad ment of Computer Science, \nApril 1990. [JSS89] N. D. Jones, P. Sestoft, and H. S@ndergaard. Mix: A self-applicable partial evaluator \nfor experiments in compiler generation. Lisp and Symbolic Com\u00adputation, 2(l), 1989. [Knu68] D, Knuth, \nSemantics of context-free languages. Math Systems Theory, 2(2):127 145, February 1968. [Lau88] J. Launchbury. \nProjections for specialisation. In A. P. Ershov D. BjOrner and N. D. Jones, edi\u00adtors, Partial Evaluation \nand Mixed Computation. North-Holland, 1988. [LR64] L. A. Lombardi and B. Raphael. Lisp as the language \nfor an incremental computer. In The Programming Language LISP: Its Operation and Applications, pages \n204-219. Information Interna\u00adtional Inc., The MIT Press, 1964. MR90] T. J. Marlowe and B. G. Ryder. An \nefficient hy\u00adbrid algorithm for incremental data flow analysis. In Conference Record of the Seventeenth \nAnnual ACM Symposium on Principles of Programming Languages, 1990. MT46] J. C. C. McKinsey and A. Tarski. \nOn closed ele\u00adments in closure algebras. A nnais of Mathematics, 47(l), January 1946. [Pug88] W. W. Pugh, \nJr. Incremental Computation and the Incremental Evaluation of Functional Pro\u00adgrams. PhD thesis, Cornell \nUniversity, August 1988. [Rep84] T. W. Reps. Generating Language-Based Environ\u00adments. The MIT Press, \n1984. [Rep90] T. Reps. Algebraic properties of program integra\u00adtion. In Proceedings of the 3rd European \nSympos\u00adium on Programming, Lecture Notes in Computer Science, Vol. 43.2. Springer-Verlag, May 1990. [RTD83] \nT. Reps, T. Teitelbaum, and A. Demers. Incre\u00admental context dependent analysis for language\u00adbased editors. \nACM Transactions on Programming Languages and Systems, 5(3):449-477, July 1983. [Sun90] R. S. Sundaresh. \nImplementing incremental com\u00adputation via partial evaluation. Technical Re\u00adport YALEU/DCS/RR828, Yale \nUniversity, De\u00adpartment of Computer Science, November 1990. [Sun91] R, S. Sundaresh. Incremental Computation \nand Partial Evaluation. PhD thesis, Yale University, (Forthcoming) 1991. [YS89] D. Yellin and R. Strom. \nINC: A language for incre\u00admental computation. Technical report, IBM, RC 14375 (#64375) 1989.  \n\t\t\t", "proc_id": "99583", "abstract": "", "authors": [{"name": "R. S. Sundaresh", "author_profile_id": "81332530381", "affiliation": "Yale University, Department of Computer Science, Box 2158 Yale Station, New Haven, CT", "person_id": "P236212", "email_address": "", "orcid_id": ""}, {"name": "Paul Hudak", "author_profile_id": "81100539650", "affiliation": "Yale University, Department of Computer Science, Box 2158 Yale Station, New Haven, CT", "person_id": "PP40028396", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/99583.99587", "year": "1991", "article_id": "99587", "conference": "POPL", "title": "A theory of incremental computation and its application", "url": "http://dl.acm.org/citation.cfm?id=99587"}