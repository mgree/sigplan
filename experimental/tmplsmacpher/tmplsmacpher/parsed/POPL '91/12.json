{"article_publication_date": "01-03-1991", "fulltext": "\n Look Ma, No Hashing, And No Arrays Neither Jiazhen Cai 1 and Robert Paige 2 New York University/ Courant \nInstitute New York, NY 10012 and University of Wisconsin Madison, WI 53706 ABSTRACT It is generally assumed \nthat hashing is essential to many algorithms related to efficient compilation; e.g., symbol table formation \nand maintenance, grammar manipulation, basic block optimization, and global optimization. This paper \nquestions this assumption, and initiates development of an efficient alternative compiler methodology \nwithout hashing or sorting. Underlying this methodology are several generic algorithmic tools, among \nwhich special importance is given to Multiset Discrimination, which partitions a multiset into blocks \nof duplicate elements. We show how multiset discrimination, together with other tools, can be tailored \nto rid compilation of hashing without loss in asymptotic per\u00adformance. Because of the simplicity of these \ntools, our results maybe of practical as well as theoretical interest. The various applications presented \nculminate with a new algorithm to solve iterated strength reduction folded with useless code elimination \nthat runs in worst case asymptotic time and auxiliary space linear in the maximum text length of the \ninitial and optimized programs. 1. Introduction. with linear search time and a hash table. They also \npro\u00adpose these two data structures for methods to turn an expression tree into a dag and the more general \nbasic An important practical and theoretical question in Computer Science is whether there are algorithms \nwhose block optimization of value numbering. Hashing is worst case performance can match the expected \nperfor\u00adinvolved in preprocessing for global optimization to per\u00ad mance of solutions that utilize hashing. \nIn the context of form constant propagation [22], global redundant code this broader question, we initiate \nan investigation of elimination [4], and code motion [8]. The best methods efficient compilation without \nhashing and, consequently, of strength reduction [3,6] rely on hashed temporaries to raise some doubts \nabout the prevailing view that hashing obtain efficient implementations. (e.g., universal hashing [5]) \nis essential to the various aspects of compilation from symbol table management There are several reasons \nwhy hashing is used in [2] to reduction in strength by hashed temporaries [6]. these applications. Hashing \nhas O(1) expected time per\u00adformance and Iincar auxiliary space. The method of Aho, Sethi, and Unman [2] \npresent only two data Universal Hashing, due to Carter and Wegman [5], is structures for storing symbol \ntables -a linear linked list especially desirable, since the expected O(1) time is independent of the \ninput distribution. Universal hashing is well suited to applications such as compilation, where the hash \ntables do not persist beyond a single compilation run. In the applications mentioned above hashing leads \nto 1.Theresearchofthisauthorwaspartially supportedbyNational ScienceFoundationgrantCCR-9002428. 2. The \nresearch of this author was partiatly supported by Office of Navat Research Grant No. NCKX314-90-J-1890. \nPermission to copy without fee all or part of this material is granted provided that the copies are not \nmade or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication \nand its date appear, and notice is given that copying is by permission of the Association for Computing \nMachinery. To copy otherwise, or to republish, requires a fee and/or specific permission. @ 1990 ACM \n089791-419-8/90/0012/0143 $1.50 143 simple on-line algorithms supporting immediate storage/access. Consequently, \nvarious phases of compi\u00adlation can be carried out incrementally with few passes and with good space utilization. \nHowever, liberal use of hashing incurs certain costs. Even discounting the costs of collisions and rehashing, \nthe calculation of a single hash operation, say ((ax + b) mod N) mod m for input x and constants a, b \nbelonging to {O, .... N), is much greater that the cost of an array or pointer access. Mairson proved \nthat for any minimal class of universal hash functions there exists a bad input set on which every hash \nfunction will not per\u00adform much better than binary search [13]. The slow speed of SETL, observed in the \nSETL implemented ADA-ED compiler, has been attributed to an overuse of hashing. And a hash table implementation \ninvolving an array twice the size of the data set is another cost. Arrays lack the benefits offered by \nlinked lists -namely, easy dynamic allocation, dynamic maintenance, and easy integration with other data \nstructures. Fhmlly, although on-line algorithms are vital to incremental compilation, batch processing \nmay otherwise suffice. In this paper we show that all of the hashed imple\u00admentations of applications \nmentioned above can be replaced by algorithms with matching or superior worst case performance. This \nis achieved by using several sim\u00ad ple algorithmic tools (that exclude sorting), the most important of \nwhich is multiset discrimination; i.e., finding all duplicate values in a multiset. Multiset discrimination \nis discussed for various types of elements including pointers, stings, numeric constants, subtrees, and \ndags. In this paper it is adapted to solve the follow\u00ading probIems. (i) Array or list-based Symbol tables \ncan be formed during lexical scanning with unit-time curser or pointer storagelaccess. (ii) Many grammar \ntransformations can be imple\u00admented efficiently using multiset string discrimina\u00adtion. In this paper \nwe exhibit a new linear time left factoring transformation based on a generalization of Homer s rule \nto mttltisets of strings. Simpler forms of this heuristic transformation were previ\u00adously studied by \nSteams [19] and others (see also [2, 12]) to turn non-LL context free grammars into LL grammars.  (iii) \nAn expression tree-to-dag transformation is imple\u00admented without hashing in a simpler way than before \nand in linear time and space. The numerous applications include one in which any linear pat\u00adtern matching \nalgorithm (e.g., [10]) can be turned into an efficient nonlinear matching algorithm, where each equality \ncheck takes unit-time. (iv) A new hash-free basic block optimization by value numbering [2, 7] is given, \nwhich leads to a faster solution to the program equivalence problem used in integration by Yang, Horwitz, \nand Reps [23,24]. (v) Although the main parts of algorithms for global constant propagation [22], global \ncommon subex\u00adpression detection [4], and code motion [8] do not use hashing, the preprocessing portions \nfor each of these algorithms do. Such hashing can be elim\u00adinated without penrdty by efficient construction \nand maintenance of the symbol table. (vi) We regard the strength reduction transformation presented \nby Cocke and Kennedy [6] to be the most practical reduction in strength algorithm pub\u00adlished in the literature. \nAlthough the transforma\u00adtion due to Allen, Cocke, and Kennedy [3] is more powerful (since it can reduce \nmultivariate pro\u00adducts) and analyzes control flow more deeply, this algorithm can degrade performance \nby introducing far too many sums in order to remove nests of pro\u00adducts (as was shown in [14]).  We solve \nthree progressively more complex ver\u00adsions of the Cocke and Kennedy transformation without hashing and \nwith superior worst case time and space than the expected performance in previ\u00adous hash-based solutions[6]. \nIn particular, an algo\u00adrithm is presented to solve iterated strength reduc\u00adtion folded with useless code \nelimination in worst case asymptotic time and auxiliary space linear in the maximum text length of the \ninitial and optim\u00adized programs, 2. Partial Tool Kit for Algorithm Design Without Hashing There are many \nsimple combinatorial problems for which hashing seems like the natural, perhaps only, way to obtain an \nefficient solution. These include such basic computations as: (i) set union, difference, and intersection; \n (ii) multiset sting discrimination; i.e., finding all duplicates in a multiset of stings;  (iii) computing \n( (x,y]: [x,y] e S x T ) Although hashing may seem like a panacea, it does incur costs, and we should \nnot overlook the many contexts in which the preceding computations can be solved by an efficient hash-free \napproach. In [15] a different more general discussion of prin\u00adciples underlying hash-free algorithms \nfor simple set operations is presented. Below we discuss a few sharper techniques with a focus on multiset \ndiscrimination. Unless otherwise stated, throughout this paper we will assume that sets and multisets \nare implemented as linked lists. 2.1. Multiset Discrimination of Pointers We use the following notation \nfor pointer manipu\u00adlation. If variable x contains a pointer to variable y, expression x ~ retrieves the \nvalue stored in y, and ?X is a pointer to the value stored in x. Consider a multiset M of pointers to \nelements in a set S. For each element (i.e., symbolic address) x in M, we want to compute the set j (x \nT) of pointers to all ele\u00adments in M with the same value as x, (Note here that x ? is the value of the \nelement in S that x points to.) Assume that f (x ~) is initially nil. Multiset M can be partitioned into \nblocks of duplicates using the following simple pro\u00adcedure F:={] --F will be the set underlying M (for \nx e M) --linear search through M iff (x~) =nil then F:= F u {x) end if f (XT) :=f (Xt) u {-TX] end 2.2. \nMultiset Discrimination of Strings Solving multiset discrimination of strings is slightly more complicated. \nLet M be a multiset of n variable-length strings over a k-symbol alphabet Z = {1, .... k). Assume for \nconvenience that each string ends with a sentinel symbol O. Starting with an initial partition P containing \nonly one block M, we can solve this problem by repeatedly splitting blocks of P until all the duplicates \nare found. For each strings we implement a currt?nt position where the current symbol for s is stored. \nInitially the current symbol for each string is the symbol in its first position. A block is a set of \npointe~ to strings in M. Once we know that a block contains all duplicate strings we say that the block \nis jinishe~ otherwise, we say the block is unfinished. Partition P contains two parts -a set of unfinished \nblocks initially containing the single block M, and a set of finished blocks initially empty. A partition \nrefinement of P can be implemented using a primitive operation split(B) that replaces block B by new \nblocks, each containing pointem to stings with the same current symbol. The technique implements a variant \nof multiset discrimination of pointers that makes use of an array of k + 1 buckets, where the i-th bucket \ncontains the new block with current symbol i. Any new block found in the O bucket or containing only \none pointer to a string is finished; otherwise it is unfinished. During execution of split(B), the current \nposition is incremented in each string belonging to a new unfinished block. It is easy to implement sp/it(B) \nin time O ( IB I) and space O ( IB I + k). We can also implement split(B) exclusively with lists and \nlist processing. For each symbol i = O, 1, .... k form a special list called the i-list with no elements, \nStrings are represented as lists of pointers to i-lists. Buckets can then be formed using these i-lists \ninstead of arrays, and multiset discrimination can be solved for pointers as in the preceding subsection. \nThe following algorithm makes use of split(B) to solve multiset discrimination of strings: 1. Form the \ninitial partition P = (M). 2. Repeat Step 3 until all of the blocks in P are finished. 3. Scan the \nset of unfinished blocks, and replace  each such block B by split (B). The algorithm runs in O (m ) \ntime and O (n +/c) space, where m is the total length of the prefixes needed to dis\u00adtinguish the strings \nin M. Both the theoretical time bounds and the simplicity of the implementation make it superior to lexicographic \nsorting for solving multiset discrimination. Previously, the lexicographic sorting algorithm found in \nAho, Hopcroft, and Unman s book [1] was used to solve congruence closure [9] and also tree isomor\u00adphism \n[1]. Both these problems can be solved more sim\u00adply by our solutions to multiset discrimination of strings. \nTheir sorting algorithm has the theoretical disadvantage of an ~(m) complexity in time and auxiliary \nspace, where m is the totat length of all strings in M. Their algorithm also has the practical disadvantage \nof a com\u00adplex multi-pass implementation. The array-based version of multiset discrimination of strings \nwas used earlier by Paige and Tarjan to obtain improved solutions to Iexicogmphic sorting [16] and DFA \nminimization for one symbol alphabets [17]. The implementation that uses pure list processing without \narrays has the advantage of easier memory management, Both implementations are simple, and involve one \npass through the prefixes of the strings. Consequently, our proposed applications may be practical. \n2.3. Multiset Discrimination of Numeric Constants Multiset discrimination of numeric constants can be \nsolved by treating these constants as strings over a k\u00adbit alphabet for arbitrary k. By treating character \nstrings as bit strings, we can also vary k to obtain space/time tradeoffs in solving string discrimination. \n3. Applications 3.1. Symbol Tables Multiset discrimination of strings can be used directly to implement \na two-pass lexical scanner. First the string is scanned to produce tokens and initial pointers to symbol \ntable entries. The symbol table is a multiset of lexemes (implemented as a doubly linked list), and an \nadditional pass is needed to remove redun\u00addant entries, and redirect pointers (the lexical values) to \ndistinct entries in the modified table. The performance is linear time and space in the length of the \ninput string. Consequently, parsing and semantic analysis can proeced without hashing, since these processes \ncan store and access the symbol table using pointers.  3.2. Fast Left Factoring Left factoring is a \ncontext free grammar transfor\u00admation that replaces productions of the form A -+ ~j3i i = 1,...n byproductions \nA -+ a CandC+ pi i= 1,.... n, where C is a new nonterminal added to the grammar. It was studied by Stearns \n[19] and others [2, 12] as a tool for turning non-LL grammars into LL grammars. They did not describe \noptimal forms of factoring or algo\u00adrithmic details. By making an analogy with Homer s rule for polynomials, \nwe can define a left factoring transformation so that the new grammm has a minimal number of new productions \nand cannot be further fac\u00adtored. The algorithm proceeds by repeatedly partitioning the set of grammar \nproductions starting with an initial partition P in which every set of productions with the same left-hand-side \nnonterminal forms a block. The data structure for P makes use of a set M of pointers to all the right-hand-sides \nof productions in the grammar. This set can be obtained by multiset discrimination of all the right-hand-side \nstrings. Each block is represented by a subset B Q M, a nonterminal symbol A, and an interval [i, ]3, \nwhere the substring from the i-th to j-th symbol of every string in B must be the same. Initially, for \neach nonterminal A, P has a block containing the set of pointers to all right-hand-sides rhs such that \nA -+ rhs is a production, A is the grammar symbol for this block, and [1,0] is the interval, which represents \nthe empty string k. In presenting the algorithm below we use the nota\u00adtion sij to denote the substring \nof string s from the i-th to the j-th symbol; si denotes the subsrnng of s from the i-th symbol to the \nlast symbol of s. After initialization the algorithm computes the new grammar Gas described below: G:={) \n(while P#()) remove block [B, [i,j~, A] from P --find a left factor for strings in block B starting --from \nthe ith position -. J:=j+l split (B) (case 1: B was not refined into more than one block by split(B)) \n_\u00ad--part of a nonempty left factor is found for strings in B if sentinel O is reached then every string \nin B is identically the left factor rhSi,j_l G := G U (A+ rhsi,,j_l } else rhsi.,j is part of the left \nfactor yet to be found add [B, [i,jl, A] to P end if end case 1 (case 2: B was refined into more than \none block by split (B)) -. complete left factor is found for strings in B if i =j then .\u00ad--left factor \nis the empty string, and B must bean --initial block C:= A --C represents nonterminal A else nonempty \nleft factor is ~hij_l create new nonterminal C G:= G u {A+ rh~i,,j.l C) end if (for each new blockD \nthat results from split(B)) .\u00ad.-nothing more to factor .. ifitisa Oblockthen G:= Gu{C-+k) --D contains \njust a single string, which is the trivial --factor rhj,, elseif ID I = 1 then G :=G u (C +rh~j,) -. \ntry to find the left factor for strings in D .\u00adelseif ID I >1, then add [D,~,j~ ,Cl to P end if end for \nend case 2 end while THEOREM 1. The preceding algorithm is correct and runs in linear time in the sum \nof the grammar sym\u00adbols contained in all the input productions. 3.3. Multiset Discrimination of Trees \nand Applica\u00adtions Suppose we have a forest of syntax trees produced by syntactic analysis. Suppose also \nthat the nodes of the syntax tree contain pointers to symbol !able entries for function symbols, constants, \nand variables. There are various applications in which we want to find duplicate subtrees. Multiset subtree \ndiscrimination can be solved in a new way without hashing by combining multiset string discrimination \nwith muh.iset pointer discrimina\u00adtion. Let T be a forest of n nodes. We identify each sub\u00adtree rooted \nin node j by a string of length 1 + the number of children of j and with symbols ranging over the alpha\u00adbet \n(1, .... n). First we solve multiset pointer discrimina\u00adtion on the symbol table pointers in all the \nnodes of T, Next, we assign successive integers, called local numbers, starting with 1 to the distinct \npointers of T. The local number at each node j will be the initial symbol of the string that identifies \nj. To obtain the remaining symbols of the subtree identifier, we exploit the idea that subtrees at different \nheights must be distinct. This allows us m solve multiset subtree discrimination separately for all nodes \nof the same height bottom-up starting from the leaves to the tree height d. That is, (1) Solve multiset \nstring discrimination for the leaves, and identify each distinct local number with new numbers, called \nvalue numbers, with successive values starting with 1. (2) For height i =2,3, .... d, repeat steps 3 \nand 4: (3) Identify each node j at height i with a string formed from the local number of j followed \nby the value numbers of the children of j. (4) Solve multiset string discrimination on the strings described \nin step 3. This solves the multiset sub\u00adtree discrimination problem at height i. Then iden\u00adtify each \ndistinct subtree at height i with new suc\u00adcessive value numbers, starting from the last value number \nassigned to a subtree at height i-1.  The preceding algorithm requires O(n) time and space and is a \ngreat deal simpler than the previous best algorithm based on lexicographic sorting. It can be used to \nobtain new hash-free solutions to many applications including tree-to-dag compression, turning an arbitrary \nlinear tree pattern matching algorithm [10] into a non\u00adlinear matching algorithm [18], deciding structural \nequivalence of type denotations [2], and many more. 3.4. Multiset Dag Discrimination and Acyclic Coar\u00adsest \nPartitioning The solution to multiset tree discrimination extends without modification to solve multiset \ndiscrimination for dags with m edges and n nodes in time O (m) and space O(n). Recall that this space \nbound improves the O(m) space bound that could be obtained to solve this problem using Aho, HopCroft, \nand Unman s lexicographic sorting algorithm [1]. We show how multiset dag discrimination can be used \nto obtain an improved solution to acyclic instances of the many-function coarsest partition prob\u00adlem. \nThe many-function coarsest partition problem, used by Hopcroft to model the problem of DFA minimi\u00adzation \n[11], has applications in program optimization and program integration. It can be formulated as follows. \nGiven a directed multi-graph (V, E ~, .... Ek) (where V is the set of vertices, and E ~, .... E~ are \nsets of edges), and an initial partition P = (VI, .... V=) of V, find a coarsest refinement P of P such \nthat for each block C in P and each i = 1, ,.,, k, there exists a block CO in P such that the image set \nEi[C] G CO, where Ei[C] = {y: [x, Y l~Ei and XG C). Here we assume that for each i = 1, .... k, the outdegree \nof each vertex v E V in (V, Ei) is at most 1. An algorithm was given in[l 1] that solves this problem \nin time @(k I V I log I V I ) and space @(k Ill) in the worst case, which is true even when the graph \n(V, E ~ v ... u Ek) is acyclic. However, when the graph (V, E 1 v .,. u Ek) is acyclic, we can solve \nthe problem in time and space O (k IV I) using a solution to multiset discrim\u00adination for dags. THEOREM \n2. If (V, E ~ u ... u Ek) is acyclic, then the many function coarset partition problem can be solved \nby Hopcroft s algorithm in time @(k IV Ilog IV I) and space @(k ~V ~) in the worst case, and by multiset \ndag discrimination in time O(k I V I ) and space O(ltl). 3.5. The Sequence Congruence Problem The sequence \ncongruence problem [23, 24] arises in the context of program integration. It asks how to par\u00adtition program \ncomponents into classes whose members have equivalent execution behaviors. The algorithm presented in \n[23, 24] solves this problem in two phases: the program components are first partitioned w.r.t the flow \ndependence graph, and then refined w.r.t. the con\u00adtrol graph. Hoperoft s coarsest partition algorithm \nis used in both phases, giving the O (m Ilogm 1 + m 210grn2) time complexity, where m ~ and m ~ are the \nsizes of the flow dependence graph and control graph respectively. Since their control graph is essentially \nacyclic, the linear time multiset dag discrimination method can be used for the second phase to improve \ntheir time bound to O(mllogml +m2). 3.6. Value Numbering Without Hashing Value numbering is a standard \noptimization tech\u00adnique of determining equalities within basic blocks to avoid redundant computations[2, \n7]. Although the tech\u00adnique is mostly implemented with hashing, multiset discrimination can be used to \nobtain an efficient imple\u00admentation without hashing. Consider a basic block B consisting of a sequence \nof assignment statements s 1, ,.., Sk, each of the form lhs := rhs, where lhs is a variable, and rhs \nis either a con\u00ad stant, a variable, or an expression of the form x op y. Here, op is some binary operator, \nand x, y can be con\u00adstants or variables. Assume that B is lexically scanned, and that variables and constants \nare represented by pointers to a symbol table as described previously. We want to assign an integer (i.e. \na value number ) to each occurrence of an expression in the statements of B so that if two occurrences \nof expressions have the same value number, then they must have the same run-time value. We compute value \nnumbers in three steps as follow~ 1. Construct an initial dag representation D = (V, E ~, E2) of B, where \nthe vertices of D represent the values of subexpressions. Each vertex has a label representing an approximate \ninitial value. Leaves are associated with variables and constants, and are labeled by pointers to symbol \ntable entries. Each internal vertex is associated with a right-hand-side rhs of the form x op y, and \nis labeled op. We construct D by scanning the statements in B in order frOms 1 tO sk. During this scan, \nthe vertex nod? (v), representing the current dag vertex for variable or con\u00ad stant v, is accessed by \npointer from the symbol table entry for v, Initially node(v) is nil. When scanning statement lhs := rhs, \nfor each right-hand-side argument x in which node (x) is nil, assign a new vertex to node(x) labeled \nby a pointer to the symbol table entry for x, If rhs is a variable or con\u00adstant y, then rhs is represented \nby vertex v=node (y), Otherwise, if rhs is of the form x op y, then create a new vertex v labeled op \nand having two children nook (x) and node (y), representing the most recent value of x and y respectively. \nThe edge [v, node (x)] belongs to E 1, and the edge [v, node (y)] belongs to E2. Finally, assign v to \nnode (lhs), so that lhs and rhs are represented by the same vertex. Two different vertices of D may have \nthe same label and children during this step, but they will be merged in the final step. 2. Propagate \nconstants in D bottom up. If a vertex w in D is labeled op and the labels for its two children point \nto constants c 1 and C2, then label w with a pointer to the computed value of c I op c z. Then delete \nthe edges leaving w from E ~ and E ~. 3. Compute the value numbers. We first partition V based on the \nlabels of its elements. Call the resulting partition P ~. We solve the coarsest partition problem with \nthe input (V, E ~, E2) and P o by multiset dag discrimination to get a final partition P = { B ~, .... \nBt ) of V. Then for i = 1, .... t, we assign integer i to each ele\u00adment of Bi as its value number. Each \nof the abo~e steps can be done in lines time without hashing. Note that greater accuracy can be achieved \nby using explicit constant labels (instead of pointers to constants), folding step 3 with step 2, and \nper\u00adforming multiset discrimination of all the constant labels before performing step 3. 4. Reduction \nin Strength The final three examples use the preceding tech\u00adniques to obtain new solutions to strength \nreduction with worst case performance asymptotically better than the expected performance of the previous \nbest algorithms. Ironically, the efficiency obtained seems to stem from using batch techniques to implement \nstrength reduction, which itself uses incremental techniques to improve pro\u00adgram performance. 4.1. Basic \nStrength Reduction First we consider a new hash-free algorithm that implements Cocke and Kennedy s strength \nreduction transformation [6]. The algorithm runs in worst case time/space linear in the length of the \nfinal program text, which, as we shall show, can be as much as two orders of magnitude better than their \nhash-based algorithm. Like their algorithm we are careful not to compute the poten\u00adtially costly data \nflow relation, Coeke and Kennedy s transformation is concerned with replacing hidden costs of linear \npolynomials involved in the array access formula used in program\u00adming languages like Fortmn or Algol. \nAs was suggested by Allen, Cocke, and Kennedy [3], the earlier transfor\u00admation [6] can be improved by \nsharper analysis of con\u00adtrol flow and taking safety of code motion into account. However, such improvement \nis orthogonal to the solution presented here. The strength reduction transformation of [6] may be defined \nas follows. Let L be a strongly conneeted region of code. We assume that this code consists of assignments \nto simple variables of the form z :=op (x,y) or z :=op (x) and conditional branches with boolean valued \nvariables as predicates. We assume implicit assignment to certain designated input variables, and implicit \noutput variables that are printecl whenever they are assigned a new value. All concern for control flow \nis simplified by taking a most conservative position that L forms a cliquq i.e., that every two statements \nin L can be executed one after the other. If c is either a region constant variable of L or a constant, \nand if t is a variable that is defined in L, then product ixc is reducible if all definitions to i occurring \nin L are among the following forms: i:= j, i:= -j, i:= j + k, i:= j -k, i:= -j+ k, or i:= -j -k, where \nirneach such form jxc and kxc must also be reducible. For each reducible product ixc occurring in L, \nstrength reduction transforms Las follows: (i) Replace each occurrence of ixc in L by a new vari\u00adable \ntiC uniquely associated with text expression iXc. (ii) If variable i is live on entry to L, then introduce \n assignment tic := ixc in a unique entry block (a detail we add to their transformation for correct\u00adness), \nwhich must be entered before entering L. (iii) Within L and just prior to each definition to i of the \nforms either i :=* j or i :=* j * k, insert the code tic :=* jxc or ti. :=* jxc f kxc respectively. (iv) \nIf any of the products introduced in step (iii) has been previously elimimted by either code motion or \nstrength reduction, replace it by its associated temporary variable. Remove all other products introduced \nin step (iii) by either code motion or recursive application of strength reduction as appropriate. Like \nCocke and Kennedy we assume that strength reduction is performed after redundant code elimination, constant \npropagation, and code motion. Given a strongly connected program region L as input, our solution shares \nthe first four steps of the Cocke and Kennedy algorithm; i.e., (i) Compute the set RC of region constant \nvariables of L and a set Defs (v) of all definitions in L to each variable v defined in L. (ii) Compute \nthe set IV of induction variables; that is, the set of all variables x with definitions occurring in \nL such that any product xxc would be reducible. This procedure was also described by Cocke and Schwartz \n[7].  (iii) Find the set Canals of all reducible products XXC actually appearing in L, and the asswiated \nplaces where they occur. (iv) For each induction variable x, compute the set Afct (x) = {x) u (y: y is \na variable or constant on the right-hand-side of any assignment to x in L). The preceding steps can be \nperformed in worst case time and space linear in the program text. If Afct is regarded as a binary relation \nand Afct* represents its transitive closure, then the following fact immediately follows from Cocke and \nKennedy s paper. LEMMA 1. The set of all expressions removed from L by strength reduction is defined \nby Rm = {jxc: ixc G Canals, j E Afct (i)}. Calculation of Rm is central to the implementation of strength \nreduction, and it is important to observe three sources of redundancy in computing this set naively. \n (i) when ixc and jxc belong to Canals and Afct (i) n A@* (j) is nonempty; (ii) when ixc ~ and jxcz \nbelong to Cartds, c ~ G Afct* (j), and Cz E Afct* (i);  (iii) when two different products of constants \nevaluate to the same constant Because only the first source of redundancy can lead to an asymptotic blowup \nin time and space, we avoid it dur\u00ading the calculation of Rm. Because the other two sources of redundancy \nonly contribute constant factors in com\u00adplexity, we avoid them during a postpass cleanup. Our approach \ncombines multiset discrimination with data structuring techniques. It is at this point that our solution \ndiffers horn Cocke and Kennedy. They go on to compute the transi\u00adtive closure Afct* in time @(n 3+m) \nusing, say, Warshall s algorithm[21] (see also[l]), where n is the number of variables and constants \ncontained in Afct, and m is the number of assignments to induction variables. They also use a greedy \nstrategy committed to hashing each product removed by strength reduction. In contrast, we compute the \nstrong component decomposition of Afct inverse (i.e., we consider decomposition of a graph with directed \nedge i + j iff i = Afc( (j) ) in ~(m) time and space using Tarjan s algorithm [20]. The dag structure \nScd of this decomposition is used to efficiently compute Rm in time O(final text length). The algorithm \nrests on the following obvious face LEMMA 2. Let Cs = {c: ixc e Cards). For each c e Cs let Crops(c) \nbe the set of strong components con\u00adtaining some variable i for which ixc G Canals. If c is any region \nconstant variable or constant, then the set of all expressions jxc removed by strength reduction is defined \nby Rm (c) = {jxc: j belongs to a component of Scd from which there is a path in Scd to any component \nof Crops (c)J. The remaining steps of the algorithm are given just below: (v) Compute the set CS using \nmuh.iset pointer discrimi\u00adnation. At the same time, for each constant c = Cs, form a set of pointers \nto strong components Crops (c) as described in Lemma 2, and mark vari\u00adables v within these components \nsuch that vxc belongs to Canals. (vi) Initialize an empty multiset Mrc of subtrees and an empty multiset \nMc of numeric constants. For each constant c E Cs repeat steps (vii) and (viii)  (vii) Compute the set \nScdC = {v :vxceRm (c)) using a depth-first-search through dag Scd in the reverse direction of its edges \nand starting tiom components belonging to Crops(c). Observe that for each strong component of Scd, if \nit has no edges leading in, then its entries are constants or Egion constant variables; otherwise, its \nentries are induction vari\u00adables. Link each induction variable v= ScdG to a new symbol table entry containing \nunique identifier tvc,and insert assignment t,C := vxc on entry to L if v is live on entry to L. If v \nis marked, indicating that vxc e Canals, then replace each occurrence of vxc in L by a pointer to the \nsymbol table entry for t,C. Link each region constant vari\u00adable v~ Scdc to a new entry in A4rc containing \nsub\u00adtree VXC. For each constant c = ScdC, if c is a region constant variable, then link c to a new entry \nin Mrc containing subtree CXC ; otherwise, link c to a new entry in Mc containing the computed value \nof Cxc . (viii) For each induction variable v in ScdC and each assignment to v in Defs (v), introduce \nupdate code to tvcaccording to the definition of the strength reduction transformation described earlier. \nReplace products that are introduced within this update code by references to the symbol table, ikfrc, \nor Mc as is indicated by the links in Scdc. (ix) Use multiset subtree discrimination and constant discrimination \nto find duplicate region constant expressions and constants in Mrc and Mc, and aug\u00adment the symbol table \nwith new variables for each distinct item in i vfrc and Mc. At the same time readjust pointers inside \nL to the symbol table, and insert an assignment t,,., := c 1xc z on entry to L for each product c ~xc \n~ e Mrc. THEOREM 3. The preceding algorithm is correct and has worst case time and space O(length of \nthe jinal program text). 4.2. Strength Reduction With Cleanup Cocke and Kennedy noted that after strength \nreduction is applied, it is necessary to apply global cleanup transformations such as useless code elimination \n(i.e., elimination of statements not contributing to the output) and variable subsumption (i.e., eliminating \nuse\u00adless copy operations). In this section we show how to fold useless code elimination together with \nstrength reduction. Our hash-free solution runs in worst case time and space linear in the sum of the \nlengths of the initial and final program texts. As before we assume that L is a strongly connected region \nof code, and Defs (v) is a set of all definitions in L to each variable v defined in L. Instead of computing \nCanals directly, we compute the set 1%-odsof all products appearing in L and the places where they occur. \nAlso, the set W of induction variables is not computed explicitly, but is detected implicitly in a simpler \nway. By a spoiler we mean any variable v for which Defs (v) contains a definition not amongst the forms \nv := * j or v := i j * k. We compute the set Spoilers of all such variables. Finally, we generalize relation \nAfcf so that Afct (x) is defined for each variable x (and not just induction variables) that is assigned \nwithin L. Let Afcti denote Afc[ inverse. As before we compute the strong component decomposition dag \nScd of Afcti. Recall that those single node components of Scd with no edges leading in contain only constants \nand region constant variables. Also, any product xxcc Prods is reducible (i.e., belongs to Canals) iff \nthere is no path in Scd from a spoiler to x, If we mark all strong com\u00adponents containing spoilers, and \nmark all other com\u00adponents reachable from these marked components, then the unmarked portion of Scd corresponds \nprecisely to the data structure at the heart of the strength reduction algo\u00adrithm in the preceding subsection. \nRecall that the induc\u00adtion variables are all those variables contained in unmarked strong components \nwith edges leading in. Consequently, we can proceed to solve strength reduction starting with step (v) \nof the previous algorithm. We now have an alternative linear time strength reduc\u00adtion algorithm, where \nthe first four steps of Cocke and Kennedy s solution are simplified. This new algorithm can also be extended \nto support efficient analysis for use\u00ad less code. Consider how the new strong component dag ScdUW of \nthe program loop L after strength reduction differs from the initial dag ScdOld, LEMMA 3. i. The subdag \nof Scd induced by unmarked strong components and the subdag of Scd induced by markxd strong components \nare both invariant with respect to strength reduction. ii. The only new components in ScdWWare ones containing \nonly new tem\u00adporaries; the only edges incident to these components are between them andfrom them to marked \ncomponents. iii. Edges only go from unmarked to marked com\u00ad ponents, and these can only be deleted by \nstrength reduc\u00adtion. Proof Strength Reduction alters loop L in the fol\u00adlowing ways: i Assignments are \nintroduced within L to modify compiler-generated temporaries t=. The right\u00adhand-side of any such assignment \nmust contain only compiler-generated temporaries. Hence, these assignments carmot create new edges from \nScd.ld to any strong components in Scdw contain\u00ading compiler-generated variables. ii An assignment Z:=XXC \nappearing in L can be replaced by assignment z :=tK. In this case, vari\u00adable z must be a spoiler that \nbelongs to a marked component Scdz~ ScdOld, and x must appear in an unmarked component Scdx~ ScdO1d. \nMoreover, there must be an edge from Scdx to Scdz. After replacement, there would be an edge from the \nstrong component in ScdW containing t=toScdz. If the edge count between Scdx and Scdg after replacement \nbecomes zero, indicating no assign\u00adments in L from a right-hand-side variable in Scdx to a variable in \nScdz, then this edge is deleted in Scd-. Let inputs be the set of input variables, outputs be the set \nof output variables, and controls be the set of predicate variables of control statements. We will assume \nthat these variables are all useful, and that the strong components of Scd containing them, which we \ncall the critical set crit, are also useful. The useful com\u00adponents include crit and all strong components \nof Scd that can reach the components in crit. If we assume that all statements in L are initially useful, \nthen after strength reduction is applied to L once, only induction variables, region constants, and constants \ncan become useless. Temporaries generated by strength reduction must all be useful. Consequently, only \nthe replacement of products by temporaries can create use\u00adless code. And all statements that undergo \nsuch replace\u00adment will be useful in the end. Hence, we can modify steps (vii)-(ix) of the algo\u00adrithm \nin the previous subsection to facilitate useless code elimination as follows. In step (vii), for each \nassignment z :=VXC replaced by assignment z :=tvc,decrement the edge count from Scd, to Scdz. If the \nedge count reaches zero, then delete the edge from Scdv to Scdz. This is implemented using a pointer \nlinking a record for assign\u00adment z :=VXC into the adjacency list for Std. Also, add edges from t,.to \nz in Afcti. In step (viii) introduce a new edge in Afcti for each assignment to a temporary introduced, \nIn step (ix) multiset discrimination will determine the new vertices corresponding to new tem\u00adporaries \nin Afcti. Add a final step (x) in which the useful components of Scd are computed. Within L all assign\u00ad \n ments to variables not in useful components can be removed. By the preceding discussion we have THEOREM \n3, The preceding algorithm is correct and has worst case time and space O(length of the initial plus \nfinal program text). 4.3. Iterated Strength Reduction Cocke and Kennedy noted that after strength reduction \nis applied, the new compiler generated vari\u00adables tvcand other variables can become new induction variables, \nand new products defined in terms of these variables can be removed by further applications of strength \nreduction[6]. In this section we show how iterated strength reduction folded with useless code elim\u00adination \ncan be solved in worst case time and space linear in the maximum length of the initial and final program \ntexts, Note, first of all, that iterated strength reduction terminates, because each iteration except \nthe last must eliminate at least one product in the original strongly connected region L. In order to \nachieve the promised linear time complexity, we must be careful to generate only temporaries that are \nnot useless. We will also exploit the observation that the portion of graph Afcti involving only temporaries \ngenerated by strength reduc\u00adtion is an exact copy of the core part of Afcti invoIving original program \nvariables only. Consequently, a key idea underlying the new algorithm will be to avoid alter\u00ading the \noriginal graph and its strong component decom\u00adposition Std. By attaching labels to the components of \nScd, we will be able to interpret these components as being formed from either original program variables \nor from temporaries generated in some ith application of strength reduction. Following Cocke and Schwartz[7], \nwe say that a temporary tvc, . . . ~, is available in program region L if, whenever it is referenced \nduring execution of L, it stores the value of vxc ~x...xcj. BY default, [V = v, and [V is said to be \navailable in L if v is not useless. The main task of the algorithm is to determine all the tempormies \nthat need to be kept available in L in order to eliminate all candidate products. It is then straightforward \nto generate the code to keep them available, First consider the preprocessing. For iterated strength \nreduction we relax our definition of spoiler to be any variable z in which there is an assignment to \nz in L thatisnotamongst theforms v:=fj,v:=tjik,or v :=jxc, where jxc is a candidate product, Compute \nScd as in the preceding subsection. We say that a component of Scd is clean if it has no spoilers; it \nis reducible if all its ancestors in Scd are clean. The set Canals consists of all those products xxc \noccurring in L, such that x belongs to a reducible component. It is straightforward to compute the spoilers, \nthe reducible components, and Canals in a single topological search through Scd in the direction of its \nedges. For each com\u00adponent C, if it has no spoilers and if its predecessors are all reducible, then it \nis reducible; otherwise it is not redu\u00adcible. The search continues until no more reducible com\u00adponents \ncan be detected. Next consider analysis for all temporaries needed to remove the products from Cards. \nWe have the fol\u00adlowing observations: LEMMA 4. Let C be a component in Scd, v G C, ands = c ~ . . . Cjfor \nj>(). (When j=t), thens is the empty string, denoted by 1.) 1. If v, e C, then temporary tv~ is kept \navailable if ty,. is kpt available. 2. Ifs # A, then tv~ is kept available if either of the following \nconditions hold: i. there exists a successor C ~ G Scd of C and an assignment u := dl x c ~ in L such \nthat u e C ~, dl e C, and tw=... .j is kept available; or ii. there em sts a successor C ~ e Scd of C \nand an assign\u00ad ment u := dl op d2 in L such that op is not multiplica\u00ad tion, u e C ~, dl G C, and tW \nis kept available. Define labelc (C) to be the set of stingss such that for all v G C, t,. is kept available. \nTo determine all tem\u00adporaries to be kept available, we only need to compute the set Zabe/c (C) for all \nC G Scd, For this purpose, we also associate a set of strings labele with each edge e = [C ~, C2] in \nScd as follows: for each assignment v:= dl x dz of a candidate product in L such that v G C2 and dl E \nC ~, then dz is in labele (e); if e is associated with any other kind of assignment, then 1 is in hzbele \n(e). According to Lemma 4, we can compute the map\u00adping labelc as following: 1. For each nonreducible \ncomponent C, set labelc (C)= ( 1]. 2. Let ready be the set of minimal reducible com\u00ad ponents; 3. While \nready is not empty, select an arbitrary component C and delete it from ready; then do steps 4, 5, 6; \n4. Let succ (C) be the set of successors of C in Std. Set labelc (C)= ~G$;C(c) ( XIIX X C labele ([C, \nCi]), S G 1 labelc (Ci) ]. 5. If C contains output variables or control vari\u00adables, add k to labelc (C). \n 6. Mark C processed. Put the predecessors of C whose successors are all processed into ready.  In order \nto keep temporaries available, following actions are added to step 4: 4 . Just before each assignment \nu := dl op d2 in L such that u e the code tm:=fall. C, insert op td,~. Although multiset string discrimination \ncould be used in computing the union in step 4, the Q(lsl) worst case cost contributed by each strings \nin lablec (C) is too slow. More efficient is to modify the preceding algorithm to generate all stings \nof a given length before applying multiset discrimination. We will compute kzbelc by gen\u00aderating these \nstrings in order of ascending length. Initially, labelc (C) is empty for all reducible com\u00adponents C. \nThen for length i = O, 1,,,., k, we generate the strings of length i in labelc (C) for all reducible \ncom\u00adponents C. When i =0 useful program variables are detected, We assume no new strings are generated \nin round k. After each round i, i = 1, .... k l, we assign a unique identifier for each distinct string \nof length i. Thus, in round i+l, each newly generated string C1C2  ci+l can be represented by a pair \n[c 1,n 1], where n 1 is the name for C2 .0 ci+l. Consequently, in order to determine distinct strings \ngenerated in each ith round i> 1 multiset string discrimination is only meded for strings ;f length 2. \nIt is easy to compute strings of length i+l from strings of length i using the following two rules: 1. \nIf lablec (C) contains a strings (of length i, and if for some predecessor Ci of C, labele([Ci, C]) contains \nC, then CIISis a string of length i+l that must be contained in Iabelc (Ci); 2. If lablec (C) contains \na sting s of length i+l, and if for some predecessor Ci of C, kbele([Ci, C]) con\u00adtains 1, thens must \nalso be contained in Iabelc (Ci).  The above representation of strings can also be used to initialize \nconstants and temporaries. If s = cl  c~ is a new string in labelc (C) for some k> 1 and C e Scd, and \nif nl is the name of c2 os c~, then we use t. to store the value of c 1x...xc~, and insert an assignment \nt$ := c 1 x L, at the end of the initialization block. Once all the constants are initialized, we insert \nan assignment t~~:= v x t, at the end of the initialization block for each temporary tv~. After round \n1, when all temporaries for strings of length 1 are determined, replace all occurrences of candi\u00addate \nproducts in L by their associated temporaries. Because the algorithm processes components Scd in the \nreverse direction of edges, only useful variables and tem\u00adporaries are introduced to the final program. \nConse\u00adquently, we have THEOREM 4. The iterated strength reduction prob\u00adlem with useless code elimination \ncan be solved in time and au.n liary space linear in the maximum length of the initial and final program \ntexts. 4.4. Extensions Two possible approaches that exploit commutative and associative laws of products \nmay reduce the number of strings, and therefore temporaries, generated in the preceding strength reduction \nalgorithms. One approach is to use a weak form of the Paige/Tarjan lexicographic sorting algonthm[16] \nto generate strings of constants in some arbitrarily chosen order. Another more effective, but less efficient, \napproach, would be to actually compute the product of constants identifying each temporary, and to use \nmtdtiset constant discrimination. We are currently investigating these ideas as well as extensions that \nimplement a more powerful transfor\u00admation integrating strength reduction of sums, products, quotients, \nexponentiations, and multivariate expressions. Such extensions would allow different kinds of spoilers \nfor different arguments of candidate expressions. Development of simpler hash-based algorithms is another \npromising direction. 5. Conclusion We have suggested hash-free methods for solving various aspects of \noptimizing compilation. These methods have been based in large part on efficient algo\u00adrithms for solving \nmultiset discrimination for different datatypes. Multiset discrimination of ordered flow graphs, unordered \ntrees and dags, and unordered graphs with respect to given depth-fist-search spanning trees are straightforward. \nAn empirical investigation compar\u00ading our hash-free alternatives with their conventional hash-based counterparts \nwould be worthwhile future work. 14. 6. Acknowledgements We are grateful to Alan Siegel, whose independent \ninvestigation of lexicographic sorting and great interest 15. in its applications to algorithm design \nprovided motiva\u00adtion for our work. We thank Bob Tarjan for describing a list based data structure known \nto Knuth for implement\u00ad 16. ing fast string matching, which is related to our list based implementation \nof multiset discrimination. We also thank Ralph Wachter, whose workshop on randomized 17. algorithms \nbrought to our attention questions about ran\u00addomized versus deterministic algorithms, which, we felt, \n18. raised similar questions about hash-based versus hash\u00adfree algorithms, and led to the current paper. \n19. References 1. Aho, A., Hopcroft, J., and Ulhnan, 1., Design and 20. Analysis of Computer Algorithms, \nAddison-Wesley, 1974.  2. Aho, A., Sethi, R. and Unman, J., Compilers, Addison\u00ad21. Wesley, 1986. 3. \nAllen, F. E., Cocke, J., and Kemedy, K., ( Reduction of 22. Operator Strength, in Program Flow Analysis, \ned. Muchrick, S. and Jones, N., pp. 79-101, Prentice Hall, 1981. 23. 4. Alpem, B., Wegman, N., and Zadeck, \nK., Detecting Equality of Variables in programs, in Proc. 15th ACM POPL, JarL 1988. 5. Carter, J. and \nWegman, M., Universal Classes of Hash  24. Functions, .KXS, vol. 18, no. 2, pp. 143-154, 1979. 6. Cocke, \nJ. and Kennedy, K., An Algorithm for Reduc\u00adtion of Operator Strength, CACM, vol. 20, no. 11, pp. 850-856, \nNov., 1977. 7. Cocke, J. and Schwartz, J. T., Programming Languages and Their Compilers, Lecture Notes, \nCIMS, New York University, 1969. 8. Cytron, R., Lowry, A., and Zadeck, K., S Code Motion of Control \nStructures in High-level Languages, IBM Research Center/Yorktown Heights, 1985. 9. Downey, P., Sethi, \nR., and Tarjan, R., Variations on the Common Subexpression Problem, JACM, vol. 27, no. 4, pp. 758-771, \nOct., 1980. 10. Hoffmann, C. and O Donnell, J., Pattern Matching in Trees, JACM, vol. 29, no. 1, pp. \n68-95, Jan, 1982.  11. Hoperof; J., An n log n Algorithm for Minimizing States in a Finite Automaton, \nin Theory of Machines and Computations, ed. Kohavi and Paz, pp. 189-196, Academic Press, New York 1971. \n 12. Lewis, F., Rosencmntz, D., and Stearns, R., Compiler Design Theory, Addison-Wesley , 1976,  13. \nMairson, H., The Program Complexity of Searching a Table, in 24th IEEE FOCS, pp. 40-47, Nov., 1983. \n  Paige, R., Symbolic Finite Differencing -Part I, in Proc. ESOP 90, ed. N. Jones, Lecture Notes in \nCom\u00adputer Science, vol. 432, Springer-Verlag, 1990. Paige, R., Real-time Simulation of a Set Machine \non a RAM, in ICCI 89, ed. W. Koczkodaj, Computing and Information, Vol II, pp. 69-73, 1989. Paige, R \nand Tarjw R., Three Efficient Algorithms Based on Partition Refinement, SIAM Journal on Com\u00adputing, vol. \n16, no. 6, Dee., 1987. Paige, R., Tarjan, R., and Bonic, R., A Linear Time Solution to the Single Function \nCoarsest Partition Prob\u00ad lem, TCS, vol. 40, no. 1, pp. 67-84, Sep, 1985. PelegtiLlopart, E., Rewrite \nSystems, Patrern Matching, and Code Generation, U. of CA -Berkeley, 1987. Ph.D. Dissertation Stearns, \nR., Deterministic top-down parsing, in Proc. 5th Princeton Conf on Information Sciences and Sys\u00ad tems, \npp. 182-188, 1971. Tarjan, R., Depth first search and linear graph algo\u00ad rithms, SIAM J. Cornput, vol. \n1, no. 2, pp. 146-160, 1972. Warshall, S., A Theorem on Boolean matrices, JACM, vol. 9, no. 1, pp. 11-12, \n1962. Wegman, M. N. and Zadeck, F. K,, Constant Propaga\u00ad tion with Conditional Branches, in Proc. 12th \nACM POPL, Jan, 1985. Yang, W., Horwitz, S., and Reps, T., Detecting pro\u00adgram components with equivalent \nbehaviors, TR-840, Computer Sciences Dept., Univ. of Wisconsim Madison, WI, April 1989. Yang, w., A new \nalgorithm for semantics-based pro\u00adgram integration, Ph.D. Dissertation, TR 962, Com\u00adputer Sciences Dept., \nUniv. of Wisconsin, Madison, Wf, August 1990.  \n\t\t\t", "proc_id": "99583", "abstract": "", "authors": [{"name": "Jiazhen Cai", "author_profile_id": "81100431654", "affiliation": "New York University/Courant Institute, New York, NY and University of Wisconsin, Madison, WI", "person_id": "P140012", "email_address": "", "orcid_id": ""}, {"name": "Robert A. Paige", "author_profile_id": "81100242795", "affiliation": "New York University/Courant Institute, New York, NY and University of Wisconsin, Madison, WI", "person_id": "P246036", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/99583.99605", "year": "1991", "article_id": "99605", "conference": "POPL", "title": "Look ma, no hashing, and no arrays neither", "url": "http://dl.acm.org/citation.cfm?id=99605"}