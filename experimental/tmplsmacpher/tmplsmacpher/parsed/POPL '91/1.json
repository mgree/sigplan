{"article_publication_date": "01-03-1991", "fulltext": "\n Static and Dynamic Semantics Processing Charles Consel Olivier Danvy Department of Computer Science \nDepartment of Computing and Information Sciences Yale University * Kansas State University t (consel@cs.yale.edu) \n(danvy@cis.ksu.edu) Abstract This paper presents a step forward in the use of partial eval\u00aduation for \ninterpreting and compiling programs, as well as for automatically generating a compiler from denotationrd \ndefinitions of programming languages. We determine the static and dynamic semantics of a pro\u00adgramming \nlanguage, reduce the expressions representing the static semantics, and generate object code by instantiating \nthe expressions representing the dynamic semantics. By pro\u00adcessing the static semantics of the language, \nprograms get compiled. By processing the static semantics of the par\u00adtial evaluator, compilers are generated. \nThe correctness of a compiler is guaranteed by the correctness of both the ex\u00adecutable specification \nand our partial evaluator. The results reported in this paper improve on previous work in the domain \nof compiler generation [16, 30], and solves several open problems in the domain of partial eval\u00aduation \n[15]. In essence: Our compilation goes beyond a mere syntax-to\u00adsemantics mapping since the static semantics \ngets pro\u00adcessed at compile time by partial evaluation. It is re\u00adally a syntax-to-dynamic-semantics mapping. \nQ Because our partial evaluator is self-applicable, a com\u00adpiler is actually generated. Our partial evaluator \nhandles non-flat binding time domains. Our source programs are enriched with an open-ended set of algebraic \noperators. This experiment parallels the one reported by Monteny\u00adohl and Wand in [24]: starting with \nthe same denot ational semantics of an Algol subset, we obtain the same good re\u00adsults, bnt completely \nautomatically and using the original semantics only, instead of writing several others, which re\u00adquires \nproving their congruence. We are able to compile strongly typed, Algol-like programs and to process their \nstatic semantics at compile time (scope resolution, storage calculation, and type checking), as well \nax to generate the corresponding compiler completely automatically. P.O. Box 2158, New Haven, CT 06520, \nUSA. This research was supported by Darpa under grant NOOO14-88-k-0573. Manhattan, KS 66506, USA Permission \nto copy without fee all or part of this material is granted provided that the copies are not made or \ndistributed for direct cotntnercial advantage, the ACM copyright notice and the title of the publication \nand its date appear, and notice is given that copying is by permission of the Association for Computing \nMachinery. To copy other\u00ad wise . or to republish, requires a fee and/or specific permission. @ 1990 ACM \n089791-419-8/90/0012/0014 $1.50 The object code is reasonably efficient. It has been found to be more \nthan a hundred times faster than the interpreted source program, whereas it runs about two times slower \nthan the out put of a regular, production-qualit y compiler. The compiIer is well-structured and efficient. \nThe static seman\u00adtics stiU gets processed at compile time. Self-application pays off: compiling using \nthe compiler is correspondingly much faster than compiling by partially evaluating the in\u00ad terpreter \nwith respect to the source program. Our compiler is still slower than a produ ction\u00ad quality one, but \nwe derived it automatically. Keywords Compiler generation, semantics, partial evaluation, self-application, \nbinding times, Algol, Scheme Introduction Existing semantics-directed compiler generators essen\u00adtially \namount to a syntax-to-semantics mapping [16, 35, 30]. They map the representation of programs as abstract \nsyntax trees into the representation of their meaning as lambda\u00adexpressions. Lacking a static/dynamic \ndistinction in the semantic specification, it is not clear how to simplify the resulting lambda-expressions \n[26]. For this reason compile time actions are likely to be performed at run time, thereby impeding the \nperformances of the whole system. This spe\u00adcific problem gets solved by distinguishing static and dy\u00adnamic \nsemantics soundly in semantic specifications [28]. As a static semantics processor, a self-applicable \npartial evaluator is an obvious choice. It ensures the static seman\u00adtics of a program to be processed \nat compile time. Viewing the set of valuation functions as a definitional interpreter [32], compiling \na program is achieved by specializing its in\u00adterpreter. Further, generating a compiler is achieved by \nspe\u00adcializing the partial evaluator with respect to the interpreter [1]. Partial evaluation provides \na great deal of flexibility. Ex\u00adecut able specifications can be experimented with. Compil\u00ading can be \nexperimented with by specializing the same exe\u00adcut able specification. A stand-alone compiler can be \ngener\u00ad ated by self-application with respect to the same executable specification. However, the partial \nevaluator needs to be sufficiently powerful. We have solved a series of open problems in partial evac\u00aduation \n[15]. Our partial evaluator specifllzes higher order Scheme programs [31] with an open-ended set of algebraic \noperators and non-flat binding time domains (i. e., partially static structures). Also, it can specialize \nitself and thus gen\u00aderate compilers automatically. Its source language has the block (letrec ( [evPrograml \n(lambda (s) {n int 5; r int 1;} (1OOP2 (intUpdat.e 1 S (intupdate 01 s))) )1 { while n > 0 [100p2 (lsmbda \n(s) do (if (gtInt (f et chInt 1 s) O) r:=n *r; (let ( [s1 (intUpdate O (mulInt (f etchInt 1 S) (f etchInt \nO S)) s)1 ) n:= n-l; (1OOP2 (intUpdate 1 (subInt (f etchInt 1 sI) 1) s1) ) ) od; } (initCcOnt s)))]) \nend evProgrsml) Figure 1: Source and object code of the factorial program same expressive power as the \nmeta-language of denotational recursive and in direct style. All the locations have been semantics. This \nmakes it possible to match-the requirements computed at compile time (variables r and n at locations \nO that have been found necessary in semantics-directed com-and 1, respectively). There is no type-checking \nat run time: piler generation [30]. all the injection tags have disappeared andall the operators The \npresent paper illustrates this step forward with are properly typed. the compilation of Algol-like programs \nand the automatic Essentially we obtain a front-end compiler that maps derivation of a stand-alone compiler \nfrom an executable syntax to a lambda-expression representing the dynamic se\u00adspecification of this language. \nmantics. Representing this lambda-expression with assem- Our experiment parallels theone reportedly Monteny-bly \nlanguage instructions is out of the scope of this paper, ohl and Wand in [24], where a compiler is derived \nby hand, but is naturally achieved by program transformation [21]. which necessitates (1) to introduce \nthree denotational spec-Let us evaluate our approach by comparing interpreted ifications and prove their \ncongruence to make it possible and compiled code, and then compiled code with object pro\u00adto process the \nstatic semantics and (2) to introduce combi-grams generated by a production quality compiler: na(ors \nand use the compiling algori~hrn of [38] to generate \u00adobject code. Running the compiled code can be \nmore than a hun-In contrast, self-applicable partird evaluation offers a uni-dred times faster than interpreting \nthe source code. fied framework for semantics-directed compiler generation. Compiling an Algol program \nusing the stand-alone The static and dynamic semantics are determined by ana-compiler is correspondingly \nfaster than compiling by lyzing the binding times [18] of the executable specification. specializing \nthe definitional interpreter of Algol with Compile time and run time comblnators are extracted au-respect \nto the Algol program. tomatically, based on the binding time information [9]. The Running a compiled \nprogram (using the T system [22]) compiling algorithm is provided by the partial evaluator. is only two \ntimes slower than running the output of a Two reasons motivate Montenyohl and Wand s subset Pascal or \na C compiler. of Algol. It is small enomzh for its comdete description to fit inva paper, and yet si~nificant \nenou~h to highlight the The rest of this paper is organized as follows. Section 1 effectiveness of partial \nevaluation. It is precisely the same describes the background of this work and compares its re\u00ad w in \n[24] and thus our treatment can be compared directly. sults with related works. Section 2 presents an \noverview of We have processed a superset of this language that includes the source language specification. \nSection 3 describes how a procedures, bnt though the results are comparable they fall specification is \nanalyzed and its static and dynamic seman\u00ad out of the scope of this paper. tics are separated. Section \n4 describes how the binding time Compiling includes redncing the expressions represent\u00ad information is \nprocessed. Section 5 describes the generation ing the static semantics and therefore goes beyond a mere \nof a compiler. Finally this work is put into perspective, syntax-semantics mapping. Our compiler is \nstand-alone and therefore optimizes the compilation process. The whole static semantics of Algol is processed \nat compile time: syn-1 Background and Related Works tax analysis, scope resolution, storage calculation, \nand type checlrhw. Section 1.1 addresses the implementation of formal seman- We ~ompile Algol programs \ninto low level, tail-recursive tics. Section 1.2 covers partial evaluation. Section 1.3 draws Scheme \ncode, with explicit store and properly typed oper-a synthesis. ators. Figure 1 displays the source and \ncompiled code of the factorial program. The Algol source program is written 1.1 implementation of formal \nsemantics with a while loop and an accumulator. The target program This section addresses semantics-directed \ninterpretation, is a specialized version of the interpreter with respect to compilation, and compiler \ngeneration. the source program. It is written in Scheme because the interpreter is written in Scheme. \nIt computes the factorial of 5 because the source program computes the factorial of Semantics-directed \ninterpretation 5. The main procedure is passed a store and updates it Stoy asserted it firmly: denotational \nsemantics specifications during the computation. The while loop has been mapped may have the format of \na program, but programs they are to a tail-recursiye procedure iterating on the store. All the not they \nare mathematical objects [37, pages 179-181]. continuations of the original continuation semantics except \nThis issue is getting blurred progressively M programming the initial one have dkiappeared: the target \nprogram is tail\u00adlanguages are getting closer to the meta-languages of for\u00admal semantics. Today, transliterating \nformal specifications into executable programs is a matter of routine. For exam\u00adple, denotational semantics \ndefinitions can be mapped into executable specifications by transliterating their valuation functions \ninto functional programs that act as definitional interpreters [32] and can be experimented with. Next \nstep aims at getting rid of their interpretive over\u00adhead. This is achieved by considering them as data \nobjects [14]. Semantics-directed compilation A definitional interpreter serves two distinct purposes \nbut they are interleaved. On the one hand programs need to be syntactically analyzed. On the other hand \nthe opera\u00adtions they specify need to be performed. Compiling a pro\u00adgram based on its formal semantics \namounts to analyzing its syntax and to producing a representation of the operations the program specifies. \nIn other terms, compiling a program amounts to dividing the representation of its formal mean\u00adi~g into \ncompile time and run time forms, and to reducing the former while emitting a representation of the latter \nas object code [26]. It is only natural to compile programs with a program [16]. Next section describes \nhowtoderive such a compiler from the semantics of a programming language. Semantics-directed compiler \ngeneration A naive semantics-based compiler is organized in several phases: one mapping source programs \nto a representation of their meaning; another simplifying this representation; and a last one mapping \nthis representation to object code [30]. This strategy requires to separate compile time and run time \nforms within the meaning of each source program. This separation can be foreseen in the semantics of \nthe programming language instead of in the semantics of each source program. Then programs can be mapped \ndirectly to their run time meaning. This contrasts with simplifying expressions that need to be generated \nfirst. The separation is achieved by analyzing the binding times of expressions in therepresentation \nof the semantic specification [18,27]. Intuitively, this separation makes sense. For example, the first \ncompiler derived from an interpreter using Peter Landin s Applicative Expressions was based on the results \nof an implicit binding time analysis [25]. Today the TML approach is based on an explicit binding time \nanalysis [28]. State of the art From the point of view of partial evaluation, which in essence processes \nstatic semantics [35], existing semautics\u00addirected compiler generators suffer from the same problem of \nnot processing the static semantics for fear of looping [26]. The d-by-need strategy of Paulson s compiler \ngener\u00adator [29] often delays compile time computations until run time, which clearly is unsatisfactory. \nThe toolbox approach illustrated by SPS [39] stresses the problem of mere com\u00adpositions of tools: they \nmake compiler compilers without actual speedups, What is expected from a compiler is that it processes \nthe static semantics of a language. Yet as sound as it is and despite its binding time analysis, PSI \n[28] does not unfold static fixpoints and has no partially static data. In contrast, the micro and macro \nsemantics of MESS [23] characterize the static and the dynamic semantics of a lan\u00adguage and ensure the \nstatic semantics to be processed at compile time, even though distinguishing between micro and macro \nsemantics relies on the initiative of the user, who also needs to separate static and dynamic representations \ninto completely static and completely dynamic structures. More recently, Wand and Wang int reduce a log \nrecording static information about earlier static reductions [40]. The corre\u00adsponding change in the semantics \n(type information located in the log instead of the run time store) can be propagated directly using \npartial evaluation. Semantics-directed compiler generation systems share the same goal, and ultimately \nuse the same methods. The trends, as analyzed in Uwe Pleban s POPL 87 tutorial [30], are to use semantic \nalgebras instead of A-terms; to improve the software engineering of systems; to manage a tradeoff between \ngenerality and efficiency; and to choose existing functional languages as specification languages. Since \nthis tutorial, the open problem of including specifications of flow analyses and optimization transformations \nhas been solved [28]. On the other hand, the very format of denotational se\u00admantics may be criticized \n[33]. This is not our point here. We want to illustrate progress in partial evaluation with the classical \nexample of compiling and compiler generation. 1.2 Partial evaluation A thorough overview of partial evaluation \ncan be found in [1, 12]. This section addresses its extensional aspects, including self-application, \nand compilhg and compiler generation by partial evaluation. Extensional aspects . Let P be a dyadic program \ncomputing a binary function p and let PE be a partial evaluator computing Kleene s spe\u00adcialization function \nS]. A version of P specialized with re\u00adspect to its first argument can be derived as follows: s; (P,<v,.>) \n(1) The first argument of S; is the source program P. The sec\u00adond argument of S; represents the arguments \nof P. The first argument of P is known to be v. The second argument of P is unknown. Let P be the speciaUzed \nversion of P with respect to v and let p be the function computed by P.. By definition of partial evaluation, \napplying p to the two input values v and w yields the same result as applying p. to w: p (U, w) s p (w) \n(2) Partial evaluation differs from currying because it is a program transformation and we cannot confuse \na program with the function this program computes. Using a processor and representing it as the function \nrun , (1) and (2) become run PE <P, <v,.>> (3) and run P<v,w> z run PU<w> (4) The essential motivation \nfor using PE is to optimize the run time of a program if part of its input is known before\u00adhand or somehow \nis invariant. Partial evrduation can be optimized aa follows. Suppose program Pneeds to be specialized \nrepeatedly with respect to several values v. In this caae, program PE will be run repeatedly with a invariant \nfirst argument (P) and a variable second argument (<u,.>). Kleene s S: the\u00adorem tells us that we can \nbuild a version PEP of PE spe\u00adcialized with respect to Pand run PEP instead. run PE <PE,<P,.> > = PEP \n(5) based on the identity run PE <P, <v,.>> G run PEP <v,-> (6) PEP is a specialize dedicated to P. Whereas \nPE can specialize any program with respect to part of its input, PEP can only specialize P with respect \nto part of its input. PEP haa been obtained by self-application. Similarly, because, in (5), P is the \nvariable and PE k invariant, we can derive a generator of dedicated specializes by specializing the partial \nevaluator with respect to itself. State of the art Before Mix Due to its generality, partial evaluation \nhsa been largely investigated. Unfortunately partial evaluators very quickly were running into complexity \nbarriers and were loop-prone [1, 12]. Mix The MIX project aimed at realizing a self-applicable partial \nevaluator bssed on simplicity and reasonable au\u00adtomatism [19, 20]. MIX handles first-order Lisp-t ype \nrecur\u00adsive equations with a fixed set of symbolic operators. As one of the assets of the MIX project, \nbinding time analysis has been discovered to be an essential component for realistic self-application \n[4]. Since Mix On the side of functional programming, the ac\u00adtivity has been concentrated on strenghtening \nself-applicable specializes. Building on top of them [5, 6, 3], the barrier of higher-orderness has been \ntorn down in 1989 [17, 2, 7]. Further progress has led us to the results reported here. [7] presents \na. unified treatment of non-flat binding time do\u00admains, i.e., of partially static structures in function \nspaces, products and co-products. [9] enhances the actual treatment of the static and dynamic semantics. \nCompiling by partial evaluation Let INT be an interpreter for the language L. INT k written in B and \nPE is a partial evaluator for B programs. Interpret\u00ad ing a L-program P on some input 1 is achieved by \nrunning INT as follows run INT <P, I> Because P may be run on several input values, we can use partial \nevaluation to build a specialized version of the inter\u00adpreter tailored to 1+ run PE <IN T,< P,.>> = INTP \nunder the definitional condition run INT < P,I> z run INTP <I> As can be observed, P k a source program \nwritten in L and INTP k a residual program written in B. In other terms, P has been compiled from L to \nB. Specirdizing an interpreter with respect to a program is a particular application of partial evaluation. \nIt is known as the first Futamura projection [13]. Compiler generation by partial evaluation Correspondingly, \nwe can build a specialize dedicated to INT: run PE < PE, <INT,->> = PEIm under the definitional condition \nrun PE < INT, < P,-> > G run PEm-<P, < P,->> PEIrrr has the functionality of a compiler since it maps \na source program into object code. Finally, the generator of dedicated specializes PEPE haa the functionality \nof a com\u00adpiler generator, in the particular case of interpreters. These two applications of partial evaluation \nare known as the second and third Fut amura projections [13]. 1.3 Synthesis Higher order programming \nconstructs match the expressive power needed for in semantics-directed compiler generation (intensional \nreason). Partial evaluation captures semantics\u00ad directed compiler generation from interpretive specifications \n(extensional reason). Therefore it makes sense to use the new generation of partial evaluators to solve \nproblems like the turning of non-trivial interpreters into realistic compil\u00ad ers. To this end, we use \nthe self-applicable partial evaluator Schism [5, 6, 7]. 2 Representation of Formal Definitions As for \nsemantics-directed compiler generation systems, the expressiveness of the input language in Schism mainly \ndeter\u00admines the class of languages that can be described. This sec\u00adtion presents an overview of this \nspecification language and how closely it matches the meta-language of denotational se\u00admantics. We illustrates \nit with excerpts of the specification of Montenyohl and Wand s Algol subset [24]. 2.1 The specification \nlanguage Schism specializes programs written in a pure dialect of Scheme [31]. This specification language \nhas the same ex\u00adpressive power aa the usual meta-language of denotational semantics [37, 35]. In particular, \nit is higher order, which makes it possible to handle e.g., continuation semantics. Ac\u00adtually since Montenyohl \nand Wand s is a continuation se\u00admantics, our transliteration is evaluation-order independent [32] and \nthus is not tailored to run only in Scheme. Further, we can specify data abstractions by defining data \ntypes algebraically, based on the following rationale. A source program presumably includes primitive \nopera\u00adtions that are defined in the underlying machine (here, the T system). In a language specification, \nprimitive operations are used to abstract certain low level semantic details, e.g., (de fineType Program \nblock) (clef ineType Declaration ident expr) (Program) ::= (Block) (Block) ::= block { (DeclList) } { \n(StmtList) } end (DeclList) ::= empty I (Declaration) ; (DeclList ) (clef ineType Statement (Declaration) \n::= (Ident) (Expr) (Block declList stmtList) (StmtList) ::= empty I (Stint) ; (StmtList) (Ukile expr \nstmtList) (Stint) ::= (Block) I (Ident) := (Expr) (If expr s.tmtl stmt2) I while (Expr) do (StmtList) \nod (Assign ident expr) ) I if (Expr) then (Stint) else (Stint) (Expr) ::= (Constant) I (ldent)  (de \nfineType Expr I (Expr} (AritBinop) (Expr) I (Expr) (RelBinop) (Expr)  (Int value) (Real value) (Bool \nvalue) (Constant) ::= int (Int) I real (Real) I bool (Bool) (Identifier ident ) (AritBinop) ::=+ l 1/1* \n(AritBinop op exprl expr2) (RelBinop) ::=< 1>1= (RelBinop op exprl expr2) ) Figure 2: The abstract syntax \nand its concrete specification (sample). the actual represent ation of the store. As a result, differ-giving \nexamples of information yielded by this analysis when ent implementation strategies can be explored within \nthe applied to the Algol definition. same semantic framework. Furthermore, properties of the language \ndefinition can allow efficient implementations of 3.1 Binding time analysis primitives in the underlying \nmachine. For an outstanding example, primitive operations modeling a store algebra can Our approach consists \nin automatically analyzing the se\u00adbe implemented imperatively given a denotational specifica-mantic definition \nto split it into two parts: the static se\u00adtion which is single-threaded in its store argument [34]. mantics \n(the usual compile time actions) and the dynamic semantics. This phase is a binding time analysis [4, \n7, 18, 27]. The binding time analysis of Schism processes higher 2.2 Specification of the Algol subset \n order functional programs. For each function, this phase This section presents fragments of the Algol \ndefinition. It yields a binding time signature that describes the binding is essentially a direct transliteration \nof Montenyohl and time properties of function parameters and the binding time Wand s continuation semantics. \nThe rest of the definition is properties of results. The analysis determines the binding displayed in \nappendix A. time descriptions of structured data made of both static and We first illustrate the data \ntype facility by defining the dynamic parts (or recursively of partially static data), As a abstract \nsyntax. Together with the concrete syntax, this consequence, the usual restriction in denotational semantics \ndefinition is displayed in figure 2. for separating static and dynamic elements of a specification One \ncould include the definition of the store operations can be relaxed without loss of staticness. This \ncapability is in the specification. Every compiled program would then illustrated below with a disjoint \nsum defining the domain of be dedicated to a fixed implementation. This implement a-expressible values. \nIn the representation of the elements of tion would be functional since our specification language is \nthis domain, and as determined by the binding time analy\u00adpurely functional. Alternatively, one can abstract \nthe im-sis, the injection tag is static and the value is dynamic. plementation of the store from the \nlanguage definition by declaring the store operations to be primitive. Like most 3.2 Static and dynamic \nsemantics of the Algoi subset other imperative sequential languages, this definition of Al\u00adgol is single-threaded \nin its store argument. Therefore, aS This section presents examples of binding time descriptions outlined \nin section 2.1, we can implement the store impera-yielded by the binding time analysis for the definition \nof the tively and thus improve the run time of compiled programs. Algol subset. Let us annotate the domains \nand the type As another illustration of our metalanguage, the function constructors as follows: they \nare overlined or underlined de\u00adevaluating stat ements is displayed in figure 3. Its type is pending on \nhow they account for static or dynamic computa\u00adpresented in figure 4. tions, respectively. Further, constructors \nare accented with Notice that the valuation functions are uncurried, for the a tilde to denote partially \nstatic data. For simplicity, we sake of readabfit y in Scheme. have not annotated continuations, though \nthey are a good example of partially static, higher order values. Figure 4 displays semantics domains \nof our language def\u00ad 3 Separating the Static and the Dynamic Semantics inition and their binding time \nproperties. By analogy with the traditional approach, let us explain these results by con-Traditionally, \nto derive a compiler from a semantic defini\u00ad sidering the binding time analysis as a theorem prover: \nfor tion, one first has to determine its static semantics. Then, each synt attic construct, a set of \ninference rules is defined to the soundness of the static semantics is proved manually. infer the static \nproperties; as starting axioms, the program This process is generally agreed to be very difficult and \ner\u00ad is static (available at compile time) and the store is dynamic ror prone [20, 30]. (not available \nuntil run time). This section first presents an analysis that determines the static and the dynamic semantics \nof a language specifi- Property 1 Every valuation function is static in its pro\u00ad cation automatically. \nThen, we illustrate the approach by gram argument. (define (evStmt stint r k s) (caseType stint [(Block \ndeclList stmtList) (evBlock declList stmtList r k s)] [(While expr stmtList) ((f ix (lambda (loop) (lsmbda \n(a) (evExpr expr r (lambda (v) (caseType v [(Int -) (terminate error7 k s)1 [(Real ) (terminate error8 \nk a)] [(Boo1 b) (if b (evStmtList stmtI.ist r 100P S) (k s))1)) s)))) s)1 [(If expr etrntl stmt2) (evExpr \nexpr r (lambda (v) (caseType v [(8001 b) (if b (evStrnt strntl r k s) (evStmt stmt2 r k s))1 [else (terminate \n~errorl k s)])) s)] [(Assign ident expr) (evExpr expr r (lambda (v) (caseType (locIdent ident r) [( IntLoc \n1) (caseType v [(Int n) (let ([s1 (intupdate 1 n s)]) (k s1))] [(Real -) (terminate Jerror2 k s)] [(Boo1 \n-) (terminate error3 k s)1 )1 [( RealLoc 1) (caseType v [(Int n) (let ( [s1 (realupdate 1 (coerceToResl \nn) s)1) (k al))] [(Real n) (let ([s1 (realUpdate 1 n s)1) @ s1))I [(Boo1 -) (terminate error4 k s)1 )1 \n[@OOILOC 1) (caseType v [(Int -) (terminate ~error5 k s)] [Oteal ) (terminate error6 k s)] [@ool b) \n(let ([s1 (boolUpdate 1 b s)]) (k SI))I)I)) s)))) (define (evBlock declList stmtList r k s) (let ( [(list \nr s) (makeDecl declList r s)]) (evStmtList stmtLiat r k s))) (define (evStmtList stmtList r k s) (if \n(empty? strntList ) (k S) (evStrnt (head stmtList) r (lambda (a) (evStmtList (tail stmtList) r k s)) \ns))) Figure 3: Valuation functions for the statements. Initially the program is static. Because the semantic \ndefi-All the identifiers of the program are static since they are nition respects the denot ational \nassumption [35], i.e., it is part of the program (by property 1). Because storage cal\u00adcompositional, \nthe meaning of a sentence is solely defined culation only depends on identifiers, this operation is com\u00adin \nterms of its proper subparts. The originrd motivation for pletely static (cf. function locIdent in figure \n4), and therefore the denotational assumption was to enable structural induc-will be performed at compile \ntime. 1 tion over abstract syntax trees. Presently, compositionality implies st aticness: no abstract \nsyntax tree is ever built. Fur-Property 3 In the representation of an expressible ualue, thermore, because \nthe arguments of the valuation functions the injection tag is static. are used consist ently, i.e., a \nvariable is uniquely bound to elements of the same domain, no dynamic arguments can Let us consider \nthe domain Evalue, defined as a disjoint interfere with a program argument. Therefore, the static-sum \nof basic values, As such, this sum is used to perform ness of a program argument is guaranteed in every \nvaluation type checking. Operationally, this domain is implemented function. D as a cartesian product. \nIts elements hold the injection tag and the actual value, The injection tag is static because it is induced \nby the program text (static by property 1) orProperty 1 is at the basis of compiling by partial evrduation. \nthe location type (static by property 2). As a remarkableIt has numerous consequences. Consequence,all \nthe operations depending on the injection tag, i.e., type checking, will be performed at compile time. \n Property 2 Storage calculation and location types are com\u00adpletely static. I-J u c Evalue = @T~lT~i \n[C-= IntLoc + ReaILoc + BOOILOC r c Environment = FYeeLoc x Mappings k E Ccont = Store T Store e6Econt \n= E~ue T _ evPrOgrsm evBlock : : = Block 77 Store -Store  x Env x Ccont ~ m ~ h makeDecl : ~~%~@vEnv \n x ti evStmtLlst : 7GZZZTZZYCc0nt T h?e evStmt :  Stint x Env x Ccont ~ &#38; T b evExpr : Expr \n x Enu x Econt ~ Store T &#38; locIdent : Ident x Env -+ Location For simplicity, we have left out \nlifting domains, etc. that account for errors. Figure 4: Annotated semantic domains. Once static properties \nof language definitions have beeu determined automatically, we can perform static and dy\u00adnamic processing, \nIn addition to the present application, static properties are useful both from a language design and \nfrom an implementation point of view: they give precise and safe bases to reason about this language. \n4 Processing the Static and Dynamic Information The static and dynamic properties of a language definition \ndetermine what to process at compile time and at run time. This section focuses on how the information \nprovided by the binding time analysis is processed to determine compile time actions. Section 4.2 presents \nthe treatment of binding time information in Schism. Section 4.3 describes the result of processing the \nbinding time information about Algol. 4.1 Principles Classical, MIX-type partial evaluators specialize \nsource pro\u00adgrams based on interpreting the binding time information. In Schism, this information is compiled \ninto elementary spe\u00adcialization actions (i. e., basic program transformations) to perform for each source \nexpression during specialization. This simplifies and improves the specialization phase con\u00adsiderably, \nas int reduced and discussed in [9]. Indeed, the binding time information of a given expression does \nnot have to be interpreted repeatedly to determine which elementary action to perform., 4.2 Processing \nthe binding time information in Schism The main partial evaluation actions denote the following treatments. \nReduction Red the outermost syntactic construct can be reduced. For example, a conditional expres\u00adsion \nwhose test yields a value statically reduces to its consequent or its alternative. Standard evaluation \n EV because the expression only manipulates available data it can be reduced to a value statically. Ev \nrefines Red.  Rebuilding Relx the outermost syntactic construct has to be ~ebuilt, but sub-components \nneed to be par\u00adtially evaluated. For example, a primitive operation that does not yield a partially static \ndata has to be reconstructed if one of its arguments does not yield a value statically.  o Reproduction \nof the expression verbatim Id no data are available. For example, a primitive operation that does not \nyield a partially static data and whose arguments are reproduced verbatim can be reproduced verbatim. \nId refines Reb. These actions are defined for each syntactic construct of the input language. Those described \nabove capture the usual program transformations for partial evaluation of first order functional programs \nas described in [6, 36]. This set of actions is extended in [8] to handle higher order functions and \nstructured data. Actions can be exploited further for extracting two sets of comblnators representing \nthe purely static and purely dynamic semantics of a language definition [9]. An Ev\u00adcombinator is extracted \nfrom an expression solely annotated with flu, and an l&#38;combinator is extracted from an expres\u00adsion \nsolely annotated with Id. These combinators actually establish an instruction set to specialize a source \nprogram with the Ewcomblnators; and an instruction set to execute a specialized program with the Id-combinators. \nAt this stage, we have reduced speciahzation to execut\u00ading the partial evaluation actions. The specialize \nis imple\u00admented as a simple processor for these actions. 4.3 Processing the static and dynamic information \nabout Algol In the case of Algol, processing the specialization actions amounts to compiling Algol programs \nsince static, compile time operations get executed and a representation of the dynamic, run time meaning \nof programs gets constructed. The .Lh-combinators define an instruction set for compiling and the ld-combhators \ndefine the instruction set of a run time machine for compiled programs. In the present specification, \nan Ewcombinator will perform compile time storage calculation and some Id\u00adcomblnat ors will perform run \ntime storage management. For example, looking up in the environment ends up in the in\u00adstruction set of \nthe compiler because it has been established to be a static operation. In the core compiler, the case \ndispatch on an abstract syntax node has been established to be static and therefore reduces to one of \nits consequent branches statically. Sym\u00ad metrically, accessing the store has been established to be a \ndynamic operation, but its offset can be determined stati\u00ad cally.  4.4 Synthesis Separating the static \nand dynamic semantics of partial eval\u00aduation appears to be particularly convenient for compiling programs. \nIt has also been found to be crucial for generat\u00ading compilers by self-application, not only for generating \na compiler and but also for running a generated compiler [20]. With respect to simplicity, orthogonality, \nand efficiency, our treatment as given in this section improves earlier results. For example, extracting \ncombinators usually reduces the size of source programs sharply, yielding smaller programs that are faster \nto specialize and, correspondingly, smaller and faster compilers [9], This observation has been confirmed \nagain in the present Algol experiment. The object code obtained by compiling an Algol pro\u00adgram represents \nthe same dynamic semantics as the one in [24] since we have identified and processed the same static \nsemantics. Its correctness depends only on the correctness of the denotational specification of the language \nand the correctness of the partial evaluation technique. 5 Generating a Compiler Generating a compiler \nis achieved by specializing the partial evaluator with respect to the executable specification: run PE \n<PE, <IN T,.>> = PEIATT This is realized straightforwardly because our partial evalu\u00adator is self-applicable, \nWhat we specialize is the processor for partial evaluation actions presented in section 4. We spe\u00adcirdize \nit with respect to the preprocessed executable speci\u00adfication, encoded with partial evaluation actions, \nAs listed below, deriving a compiler follows the standard steps of spe\u00adcialization. Representing the \nsemantics of partial evaluation This step is trivial, since we already have the program PE. As in section \n2, we also need to represent the semantics of the programming language as a definitional interpreter. \nSeparating the static and dynamic semantics Next step consists in determining the static and dynamic \nsemantics of partial evaluation. This is achieved by analyz\u00ading the binding times of the partial evaluator, \nknowing it to be specialized with respect to its program argument. As in section 3, we also need to analyze \nthe binding times of the definitional interpreter. Processing the static and dynamic information Next \nstep amounts to processing the information accumu\u00adlated by the binding time analysis to determine the \nstatic actions, i.e., the actions to be executed at compiler gener\u00adation time. As in section 4, we rdso \nneed to process the binding time information about the definitional interpreter. Given the actions to \nexecute in the partial evaluator and the actions to execute in the interpreter, executing the for\u00admer \non the latter generates a specialize dedicated to the interpreter. In essence, all the static semantics \nof the spe\u00adcialize is processed at compiler generation time, yielding a residual program dedicated to \nprocessing the static se\u00admantics of the language and emitting a representation of its dynamic semantics \n in other terms: a stand-alone com\u00adpiler whose instruction set is defined by the Ewcomblnators of the \nlanguage definition. Generating a compiler generator Based on the equation run PE <PE, <PE,-> > = PEPE \n and following the steps above based the knowledge that the inner specializes will be passed their program \nargument [4], a generator of dedicated specializes can be built. Its in\u00adstruction set is defined by the \nEwcombinators of the partial evaluator. Using this compiler generator requires one to rep\u00adresent and \nanalyze the binding times, etc. of a programming language specification as described above. Synthesis \n Beyond its theoretical interest and its conceptual elegance, self-application pays off: compiling using \nthe compiler is considerably fiwter than compiling by specirdizing the ex\u00adecut able specification. On \nthe other hand, it is still much slower than a conventional, production quality com\u00adpiler, which is not \nproduced automatically out of an inter\u00adpreter. This waa expectable because our syntax-to-dynamic\u00adsemantics \nmapping has to be composed with a Scheme com\u00adpiler. The object code obtained with our Algol compiler \nrepre\u00adsents the same dynamic semantics as the one in [24]. Its cor\u00adrectness depends only on the correctness \nof the denotational specification and the correctness of the partial evaluator. Conclusion and Issues \nThe general mechanism of compilation and compiler gen\u00aderation reducing expressions representing the \nstatic se\u00admantics and emit ting residual expressions representing the dvnamic semantics was . extensionallv \nin the carAured . def\u00adi~ition of a partial evaluator: partially evaluating an inter\u00adpreter with respect \nto a program amounts to compiling th~ program. Previous experiences in semantics-directed com\u00adpiler generation \nestablished the need for modularity, for rd\u00adgebras, for handling higher order constructs, etc. in source \nspecifications. This paper illustrates the intensional use of partial evalu\u00adation where source rmocrams \nare s~ecified modularlv and al\u00ad .. gebraically with non-flat binding t~me domains. We consider an executable \nspecification of a block-structured, strongly typed language. We automatically derive a compiler where \nall the static semantics scope resolution, storage calcu\u00adlation, type checking, etc. is reduced at \ncompile time. The compiler inherits the structure of the partial evaluator, and the object programs inherit \nthe structure of the inter\u00adpreter. All our systems run in Scheme: partial evaluators, interpreters, compilers, \nand object programs (though they are parameterized by the semantic algebras of the specialize and of \nthe interpreter). This work was possible due to a series of breakthroughs starting with separating the \nstatic and dynamic semantics of partial evaluation and including: bkding time analysis for non-flat binding \ntime domains [7] and combinator extraction [9]. Present applications address tackling pattern matching \n [111 and Prolog [101: environment-wise, sJI the potential substitutions are computed at compile time; \ncontrol wise, failure continuations are discovered to be bound statically, enabling backtracking to be \ndetermined entirely at compile time. We are currently working on the problem of restruc-[10] turing source \nprograms automatically to ensure them to be specialized better. Future works will include developing \na better programming environment; better extensional crite\u00ad [11]ria for the quality of a source specification \nand their im\u00adplementation; parameterizing post-optimizers; and the au\u00adtomatic generation of congruence \nrelations and correctness proofs, both from the binding time analysis and from the [12]actual specialization. \nAcknowledgements To Neil D. Jones for hls scientific insight as to how to tame self-application. To Mitchell \nWand and Margaret Mont eny-[13] ohl for providing the Algol example and for their encourage\u00adments. Thanks \nare also due to Karoline Malmkjam, David Schmidt, Austin Melton, Franqois Bodin, Pierre Jouvelot, [14]Siau \nCheng Khoo, Paul Hudak, and Andrzej Filinski for their thoughtful comments. Finally we would like to \nthank the referees. [15] References [1] D. Bj@ner, A. P. Ershov, and N. D. Jones, editors. [16] Partial \nEvaluation and Mixed Computation. North-Holland, 1988. [2] A. Bondorf. Automatic autoprojection of higher \norder [17] recursive equations. In N. D. Jones, editor, ESOP 90, $d European Symposium on Programming, \nvolume 432 of Lecture Notes in Computer Science, pages 70 87. Springer-Verlag, 1990. [18] [3] A. Bondorf \nand O. Danvy. Automatic autoprojection of recursive equations with global variables and abstract data \ntypes. Diku Research Report 90/04, University of Copenhagen, Copenhagen, Denmark, 1990. To appear  [19]in \nScience of Computer Programming. [4] A. Bondorf, N. D. Jones, T. Mogensen, and P. Ses\u00ad toft. Binding \ntime analysis and the taming of self\u00adappl.ication. Diku report, University of Copenhagen, Copenhagen, \nDenmark, 1988.  [5] C. Consel. New insights into partial evaluation: the [20] Schism experiment. In \nH. Ganzinger, editor, ESOP 88, 2nd European Symposium on Programming, volume 300 of Lecture Notes in \nComputer Science, pages 236 246. Springer-Verlag, 1988.  [21] [6] C. Consel. Analyse de Programmed, \nEvaluation Par\u00adtielle et Generation de Compiiateur9. PhD thesis, Uni\u00adversitc$ de Paris VI, Paris, France, \n1989.  [22] [7] C. Consel. Binding time analysis for higher order untyped functional languages. In ACM \nConference on Lisp and Functional Programming, pages 264 272, 1990.  [23] [8] C. Consel. The Schism \nManual. Yale University, New Haven, Connecticut, USA, 1990. Version 1.0.  [24] [9] C. Consel and O. \nDanvy. From interpreting to compil\u00ading binding times. In N. D. Jones, editor, ESOP 90, .5 European Symposium \non Programming, volume 432 of Lecture Notes in Computer Science, pages 88 105. [25] Springer-Verlag, \n1990. C. Consel and S. C. Khoo. Semantics-directed genera\u00adtion of a Prolog compiler. Research Report \n781, Yale University, New Haven, Connecticut, USA, 1990. 0. Danvy. Semantics-directed compilation of \nnon-linear patterns. Technical Report 303, Indiana University, Bloomington, Indiana, USA, 1990. A. P. \nErshov, D. Bj@rner, Y. Futamura, K. Furukawa, A. Haraldsson, and W. L. Scherlis, editors. Seiected Papers \nfrom the Workshop on Partial Evaluation and Mixed Computation, volume 6 of 2,3. OHMSHA. LTD. and Springer-Verlag, \n1988. Y. Futamura. Partial evaluation of computation pro\u00adcess an approach to a compiler-compiler. S@ \nems, Computers, Controls 2, 5, pages 45-50, 1971. H. Ganzinger and N. D. Jones, editors. Programs as \nData Objects, volume 217 of Lecture Notes in Computer Science. Springer-Verlag, 1985. N. D. Jones. Challenging \nproblems in partial evaluation and mixed computation. In [12], pages 1 14. N. D. Jones, editor. Semantics-Directed \nCompiler Gen\u00aderation, volume 94 of Lecture Notes in Computer Sci\u00adence. Springer-Verlag, 1980. N. D. Jones, \nC. K. Gomard, A. Bondorf, O. Danvy, and T. Mogensen. A self-applicable partial evaluator for the lambda \ncalculus. In IEEE International Conference on Computer Languages, pages 49-58, 1990. N. D. Jones and \nS. S. Muchnick. Some thoughts towards the design of an ideal language. In ACM Conference on Principles \nof Programming Languages, pages 77 94, 1976. N. D. Jones, P. Sestoft, and H. S@ndergaard. An exper\u00adiment \nin partial evaluation: the generation of a com\u00adpiler generator. In J.-P. Jouannaud, editor, Rewrit\u00ading \nTechniques and Applications, Dijon, France, VOL ume 202 of Lecture Notes in Computer Science, pages 124-140. \nSpringer-Verlag, 1985. N. D. Jones, P. Sestoft, and H. S@ndergaard. Mix: a self-applicable partial evaluator \nfor experiments in compiler generation. LISP and Symbolic Computation n, 2(1):9-50, 1989. R. Kesley and \nP. Hudak. Realistic compilation by pro\u00adgram transformation. In ACM Symposium on Princi\u00adples of Programming \nLanguages, pages 281 292, 1989. D. A. Kranz, R. Kesley, J. A. Rees, P. Hudak, J. Philbin, and N. I. Adams. \nOrbIt: an optimizing com\u00adpiler for Scheme. SIGPLA N Notices, ACM Symposium on Compiler Construction, \n21(7):219 233, 1986. P. Lee. Realistic Compiler Generation. MIT Press, 1989. M. Montenyohl and M. Wand. \nCorrect flow analysis in continuation semantics. In ACM Symposium on Prin\u00adciples of Programming Languages, \npages 204 218, 1988. F. L. Morris. The next 700 formal language descrip\u00adtions. Unpublished paper, 1970. \n [26] [27] [28] [29] [30] [31] [32] [33] [34] [35] [36] [37] [38] [39] [40] P. Mosses. S1S -Semantics \nImplementation System, reference manual and user guide. University of Aarhus, Aarhus, Denmark, 1979. \nVersion 1.0. H. R. Nielson and F. Nielson. Automatic binding time analysis for a typed A-calculus. In \nACM Symposium on Principles of Programming Languages, pages 98-106, 1988. H. R. Nielson and F, Nielson, \nThe TM L-approach to compiler-compilers. Technical Report 88-47, TechnicaJ University of Denmark, Lyngby, \nDenmark, 1988. L. Paulson. A semantics-directed compiler generator. In ACM Symposium on Principles of \nProgramming Lan\u00adguages, pages 224-233, 1982. U. Pleban. Semantics-directed compiler generation. In ACM \nSymposium on Principles of Programming Lan\u00adguages,1987. TutoriaJ. J. Rees and W. Clinger. Revised3 report \non the algorithmic language Scheme. SIGPLAN Notices, 21(12):37-79, 1986. J. Reynolds. Definitional interpreters \nfor higher order programming languages. In ACM Nationa/ Conference, pages 717-740, 1971, J. Reynolds. \nThe essence of Algol. In Van Vliet, editor, International Symposium on Algorithmic Lan\u00adguages, pages \n345-372. North-Holland, 1981. D. A. Schmidt. Detecting global variables in denot a\u00adtional specification. \nACM Transactions on Prograrn\u00adrning Languages and Systems, 7(2):299 310, 1985. D. A. Schmidt. Denotational \nSemontics: a Methodol\u00adogy for Language Development. Allyn and Bacon, Inc., 1986. P. Sestoft. The structure \nof a self-applicable partial evaluator. In [14], pages 236-256. J. Stoy. Denotationcd Semantics: the \nScott-Strachey approach to programming languages theory. MIT Press, 1977. M. Wand. Deriving target code \nas a representation of continuation semantics. ACM Transactions on Pro\u00adgramming Languages and Systems, \n4(3):496-517, 1982. M. Wand. A semantic protot yping system. SIGPLA N Notices, ACM Symposium on Compiler \nConstruction, 19(6):213 221, 1984. M. Wand and Z. Wang. Conditional lambda-theories and the verification \nof static properties of programs. In IEEE Symposium on Logic in Computer Science, pages 321 332, 1990. \n A The Algol Definition (continued) Application of operators (define (applyAritBinop Op type) (case type \n[(Int) (case 0p [(+) addInt] [(-) snbInt] [(*) mullntl [(/) divIntl)] [(Real) (case 0p [(+) addReal] \n [(-) subReal] [(*) mulReal] [(/) divReal] )1)) (clef in. (applyRelBinop op type) (case type [(Int) (case \n0p [(=) eqIntl [(>) gtInt] [(<) ltInt])] [(Real) (case 0p [(=) eqReal] [(>) gtlteal] [(<) ltReal] )] \n[( Boo1) (case op [(=) eqBool] )] )) Scope resolution (define (locIdent ident r) (let ( [(Environment \n-mappings) r]) (f et chLoc ident mappings))) Valuation function for a program (defime (evProgram program \ns) (let ( [(Program block) program]) (caseType block [(Block declList stmtList) (avBlock declList stmtList \n(Environment O initMappings) initCcOnt a)]))) Valuation function for the expressions define (evExpr \nexpr r c s) (caseType expr [(Int value) (c (Int value) )1 [(Real value) (c (Real value))] [(Boo1 value) \n(c (Bool value) )1 [(Identifier ident) (caseType (locIdent ident r) [( IntLoc 1) (c (Int (fetchInt 1 \ns)))] [( RealLoc 1) (c (Real (fetchResl 1 s)))] [( BooILoc 1) (c (Bool (fetchBool 1 s)))])) [( AritBinop \nOP exprl expr2) (evExpr exprl r (lambda (vI) (evExpr expr2 r (lambda (v2) (caseType vi [ (Int valuel \n) (caseType .2 [(Int value2) (c (Int ( (applyAritBinop op Int) valuel value2) ) )1 [(Real valne2) (c \n(Real ( (applyAritBinop op Real) (coerceToReal valuel) value2) ) )1 [(Boo1 value2) (terminate error9 \nc s)))1 [(Real valuel) (caseType V2 [(Int value2) (C (Int ( (applyAritBinop Op Int) valuei value2) ) \n)1 [(Real value2) (c (Real ( (aPPlvAritBinoP OP Real) (coerceToReal valuel) value2) ) )1 [(Boo1 -) (te-siiate \nmrkb c s)1)1 [(Boo1 -) (terminate errOrll c s)1)) s)) s)1 [( RelBinOp op exprl expr2) (evExpr exprl r \n(lambda (vi) ( evExpr expr2 r (lambda (v2) (caseType vi [(Int valnel) (caseType V2 [(Int value2) (c (Boo1 \n( (apply RelBinop OP Int) valuel value2) ) )1 [(Real value2) (c (Bool ( (applyRelBinop OP Real) (coerceToReal \nvaluel) value2) ) ): [(Boo1 value2) (terminate error12 c s)] )1 [(Real valuel) (caseType V2 [(Int value2) \n(C (Boo1 ( (applyRelBinop OP Int ) valuel value2) ) )1 [(Real value2) (c (Bool ( (applyRelBinop op Real) \n(coerceToReal valuel) value2) ) ): [(Boo1 -) (terminate error13 c s)1 )1 [(Boo1 -) (terminate error14 \nc s)])) s)) s)])) Location and Store algebras Domain m E Mappings Domain s c Store Operations Operations \ninitMappings : Mappings initStore Store. addMapping : Ident x Location x Mappings d Mappings int Update \n: _ IntLoc X ~ Y Store + Store fetchLoc : Ident x Mappings + Location reai Update : ReaMoc ~ ~ ? Store \n-A Store BoolUpdate : BOOILOC F ~ x Store -+ Store _ fetchht : htLoc x Store -+ Int fetchlteal : RealLoc \n~ Store + Real fetchBool : BOOJLOC ~ Store + Boo]   \n\t\t\t", "proc_id": "99583", "abstract": "", "authors": [{"name": "Charles Consel", "author_profile_id": "81100552270", "affiliation": "Department of Computer Science, Yale University, P.O. Box 2158, New Haven, CT", "person_id": "PP39048247", "email_address": "", "orcid_id": ""}, {"name": "Olivier Danvy", "author_profile_id": "81100394275", "affiliation": "Department of Computing and Information Sciences, Kansas State University, Manhattan, KS", "person_id": "PP15031217", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/99583.99588", "year": "1991", "article_id": "99588", "conference": "POPL", "title": "Static and dynamic semantics processing", "url": "http://dl.acm.org/citation.cfm?id=99588"}