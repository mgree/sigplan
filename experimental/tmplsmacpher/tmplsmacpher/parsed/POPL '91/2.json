{"article_publication_date": "01-03-1991", "fulltext": "\n How to Make Destructive Updates Less Destructive Martin Odersky * Abstract We present a safe embedding \nof mutable data structures in functional languages. With safety we mean that con\u00adfluence and (in some \nsense) referential transparency are maintained. We develop a static criterion based on ab\u00adstract interpretation \nwhich checks that any side-effect which a function may exert via a destructive update re\u00admains invisible. \nThe technique opens up the possibility of designing safe and efficient wide-spectrum languages which \ncombine functional and imperative language con\u00adstructs. 1 Introduction We are interested in the design \nof programming no\u00adtations which augment a functional core with imper\u00adative statements and mutable data \nstructures such as arrays. These wide-spectrum ianguages [14] promise to bridge the gap between abstract \nand declarative specifications/prototypes and efficient implementations. There are advantages to having \none notation instead of two for both programming activities: We can derive the program from the specification \nby local transformations (which can be proved correct in advance). Since we do not switch notations in \nthe process, every intermediate step is a well defined program. Moreover, the trans\u00adformation process \ncan stop se soon as the result is effi\u00adcient enough. If the strong points of both programming concepts \nare to be retained, however, their combination must be safe. Safety means that the usual semantics of \nfunctional and imperative programs should be pre\u00adserved. This is the semantics of predicate transformers \n*IBM T.J. Watson Research Center, P.O. Box 704, Yorktown Heights, NY 10598. e-mail: odersky@ibm.com \nPermission to copy without fee all or part of this material is granted provided that the copies are not \nmade or dkributed for direct commercial advantage, the ACM copyright notice and the title of the publication \nand its date appear, and notice is given that copying is by permission of the Association for Computing \nMachinery. To copy other\u00adwise, or to republish, requires a fee and/or specific permission. @ 1990 ACM \n089791419-8/90/0012/0025 $1.50 in the imperative part and lambda-calculus reduction in the functional \npart, As the semantics carry over to the combinations, so should the main theorems. In partic\u00adular, even \nin the presence of assignment, the functional part should remain confluent and referentially transpar\u00adent. \nCombining functional expressions with statements means that, first, statements such as assignments can \ncontain function applications and, second, that func\u00adtion bodies can contain statements. To obtain safety, \nwe must control all side-effects caused by statements in a function body. An obvious first step is to \nallow only read access to variables which are declared outside the scope of a function. This is not enough, \nhowever. A more complex problem, which we attack in this pa\u00adper, is presented by arguments to a function \nwhich are modified in the function s statement part. Consider the following implementation of function \nswap: swap aij= x:=a.i; a.i:=a.j; a.j:=x ; a. To prevent a side-effect in swap, we either would have \nto implement array updates non-destructively, or we would have to pass array parameters by value. Both \nchoices have a disastrous effect on efficiency. One could opti\u00admize array updates by using reference \ncounts [7] or trail\u00aders [1]. However, these optimizations impose a sizable run-time overhead themselves, \nand it is difficult for a programmer to convince theirself of their effectiveness. We propose instead \nto choose the efficient implemen\u00adtation for both parameter passing and array updating. Function side-effects \nare then possible but we will insist that all such effects remain invisible. This means that we have \nto impose some restrictions on the contexts in which a side-effecting function may be used. For exam\u00adple, \nf (swap a i j) (a,i) is illegal, since the value of a.i depends on the evaluation order of f s arguments. \nA legal application, which avoids the race condition would be: x:=a.i; f(swapaij)x A context is clearly \nlegal if the old value of an updated array a is never again accessed after the update took place. We \nsay in this case: a is single-threaded [15]. To check single-threadedness, we develop a static well\u00ad \nformedness criterion for expressions. The criterion guar\u00ad antees that: . the evaluation of a well-formed \nexpression is con\u00adfluent, and o if a well-formed expression is transformed in a se\u00adquence of substitutions \nof equals into another well\u00adformed expression, then both expressions denote the same value. This is the \ncase even if the result of some intermediate step is not well-formed. The criterion is decidable using \nan abstract interpreta\u00adtion of the source program. Indeed, it can be efficiently computed by a compiler. \nIn general, our effect analysis does not need any user-declared effect specifications, as effects can \nbe reconstructed from the program text. Related Work Existing designs of wide-spectrum languages have \nnot yet addressed the problem of maintaining confluence in the presence of destructive array updates. \nEither loss of confluence is accepted [8,18], or array updates are pre\u00adsumed to be implemented non-destructively \n[11, 13, 14]. An alternative way to handle the problem of muta\u00adtions in functional languages relies on \nstatic optimiza\u00adtion techniques in the compiler. Data structures are then always conceptually immutable, \nbut the optimizer might transform copying updates into in-place updates whenever it is recognized that \nthe transformation is semantics-preserving. Some published techniques for update optimization are [1,4,5,6]. \nOptimization techniques have the advantage that they do not change the context-rules of the compiled \nlanguage, Even if optimization fails, the program is still legal. The downside is that a programmer must \nhave a very good understanding of the optimizer s capabili\u00adties in order to know which program can be \noptimized and which cannot. Such knowledge is sometimes cru\u00adcial since unrecognized opportunities for \noptimization can add a linear factor to resource consumption. An example is the quicksort function whose \nspace require\u00adments can be O (log n) or O ( n log n), depending on whether update optimization is successful \nor not [10]. All published optimization techniques are global op\u00adtimization and therefore do not coexist \nwell with sepa\u00adrate compilation. Moreover, due to their costs they are applicable only for small programs. \nAdaptation of any one of these techniques for program-checking purposes would encounter difficulties \ndue to the large (notational and computational) overhead. Besides optimizations, several program checking \ntech\u00adniques have been developed for the update problem, Gifford and Lucassen present in [12] a notational \nframe\u00adwork which captures information about function side\u00adeffects. Their treatment is based on heaps \ninstead of variables and they do not give an effect reconstruction algorithm. Wadler [16] developed linear \ndata types, which admit destructive updates but require every vari\u00adable to be used exactly once. Again, \ntype information has to be given explicitly, no reconstruction algorithm is presently known. A more recent \napproach [17], also presented by Wadler, represents an array as an abstract data type on which only one \noperation, init, is defined. init creates a fresh array, applies a state transformer (an argument function) \nto it, and subsequently discards the array while returning a computed result (which is never an array). \nThis interesting approach has the ad\u00advantage that the single-reference property is established by design. \nHowever, arrays can be manipulated only in a restricted manner. They cannot be passed to func\u00adtions and \nstate transformers can operate only on a single array at a time. The problem we tried to solve in this \npaper is very similar to the one addressed by work of Hudak and Guzm6n [9]. They present a type-system \nto capture single-threadedness, whereas our method is based on ab\u00adstract interpretation. We simplify \ntheir approach in that we are able to drop without loss of precision their dis\u00adtinction between single-and \nmultiple-threaded accesses. We extend their results with better approximations to the sharing behavior \nof a program. We model sharing configurations by an inclusion hierarchy over the stor\u00adage which is reachable \nfrom bound variables. Sharing can be quite compIex, as in the case where an array contains a list whose \nelements are again arrays. Our analysis is able to detect in that case that modifica\u00adtions to the root \narray do not change the values of the list elements, whereas modifications to a list element do change \nthe values of all variables referring to it. Such accuracy is definitely needed for the analysis of lan\u00adguages \nwith a fully orthogonal type system in which mutable and immutable types can be freely combined. The \nmain restriction with respect to the work of Hudak and Guzm5n is that we consider here only first-order \nlanguages. The reason for this restriction is that the issues of automatic effect reconstruction in the \npresence of higher-order functions are not yet fully resolved (this holds for our approach as well as \ntheirs). Automatic effect reconstruction faces the problem that, in contrast to types in the Hindley-Milner \nsystem [3], a function does not always have a most general effect. The rest of this paper is organized \nas follows: Section 2 presents a small language with both imperative and functional components on which \nthe subsequent treat\u00adment is based. Section 3 introduces abstract domains used in the analysis. Section \n4 presents the abstract interpretation function. Section 5 discusses human\u00adreadable representations. \nSection 6 concludes.  2 An Example Language To base the following discussion on a concrete found a\u00adtion, \nwe introduce a small single assignment language with the following syntax: e ::= lx e where {f {r} = \ne} I I f{e} if el then e2 eise e3 I r:=e~;e~. The first four clauses define a purely functional first\u00adorder \nlanguage with local definitions, bound variables, function applications, and a conditional expression. \nThe only statement in the language is the assignment, whose syntax is given in the fifth clause. Variables \nmay not be assigned to more than once. Our method can be ex\u00adtended to more general functional/imperative \ncombina\u00adtions by mapping them into the above language. The transformation is similar in spirit to a translation \ninto single assignment form [2]. We assume lazy evaluation in general, except for the evaluation of sequential \ncomposition. The expression z := el ; ez is evaluated by first reducing el to normal form and then reducing \nez. If reduction terminates, the value of this construct is equal to e2 where x = el. We say that ; is \nhyperstrict in its left argument. Structured data types in our language are lists or ar\u00adrays. Lists are \nrepresentatives of more general algebraic data types found in most functional languages. They are constructed \nwith the operator cons and taken apart with the selectors hd and t!.Arrays have dynamic size; their indices \nare integers. Arrays are created and ma\u00adnipulated with the following operators: vec I array of (references \nto) list I s elements, updaix array with (a reference to) element x at index position i which is otherwise \nequal to a, a.i the i th component of a. Our type system is fully orthogonal. The component type of \nan array may be any type. The elements of a list may also assume any type, including an array type. \n3 The Abstract Domains Since the implementation of updates is intended to be destructive, we have to \nascertain that the old value of an array is never accessed after an update. A static tech\u00adnique for doing \nso is presented in this section and the next. We will present an abstract interpretation which yields \nfor every expression a description of its effect on all variables of a program, The effect on a single \nvariable shall be captured in an abstract use. Some requirements for this Use domain can be derived from \nthe following examples, (1) (7). Assume that f is a function which accesses both of its elements without \nmodifying them. Nothing is known about the order of accesses in f, and hence, because of lazy evaluation, \nabout the order of argument reduction. (1) faa safe (2) fa(upda ix) unsafe (3) g ccwhere gab= fa(updb \nix) unsafe (4) b:=a; fb(upda ix) unsafe (5) b := c.(a.i) ;f b (upd aj x) safe (6) b:=a.i; fb(updajx) \nsafe (7) b := a.i ;f b (upd a.i j x) unsafe  (1) is clearly safe since nothing is updated. (2) is \nun\u00adsafe, since the second argument might be evaluated be\u00adfore the first one, thereby producing a visible \nside-effect. Cases (1) and (2) can be distinguished by having differ\u00adent abstract uses rd for read and \nwr for write accesses. Line (3) shows that effect information has to be propa\u00adgated beyond function boundaries. \nOur abstract inter\u00adpretation will map every function to an effect signature, which maps argument liabilities \nto a result liability. Since (:=) means reference assignment, (4) is essen\u00adtially the same situation \nas (2), i.e. unsafe. The similar looking case (5) however, is safe. Because of the hyper\u00adstrict evaluation \nof ; the read access of value a occurs before a is updated. To distinguish cases (4) and (5) we need \nto keep information about variable aliases. This is done by having a value id for aliasing reads in the \nUse domain as well as rd (rd guarantees absence of aliasing). Line (6) presents some difficulties, In \nthe assignment b := a.i, should a be abstracted to rd or to id? rd seems to be too optimistic, since \nb does contain a reference to a part of a. Indeed, abstracting a to rd would lead to acceptance of the \n(unsafe) case (7). If we abstract a to id, however, we get the same analysis as in case (4), leading \nto a rejection of an expression which is perfectly safe. (6) is safe, because an update of an array does \nleave (the storage occupied by) its elements unchanged. Hence, the modification of a by evaluation of \nf s second argument does not interfere with the first argument, b, which is bound to a .i. This is true \nin our standard semantics which assumes uniform access by reference. It would also be true if array elements \nwere uniformly updated and retrieved by value. It would not be true for an implementation in which array \nelements are assigned by value but retrieved by reference, but then, this is not the semantics we consider \nhere. The example shows that we have to distinguish be\u00adtween variables and their elements in the abstract \ndo\u00admain. This is done by mapping every variable to a pair of uses U1. U2, one for the variable itself \nand an\u00adother one for its elements. The value of every expres\u00adsion is also split into the value itself \nand its elements. Hence, there are three combinations of sharing: An en\u00adtity (which stands for either \na variable or the elements of one) might be shared by the expression s value, or it might be shared by \nthe value s elements, or it might be shared by both. The cases correspond to the abstract uses id, el, \nand sh. To summarize: The effect on an entity c (either a bound variable or the elements of one) of evaluating \nan expression e is abstracted to: . 1, if z is not accessed, rd, if x is read, but does not form part \nof e s normal form,  id, if z is a potential alias of e s value,  el, if z is a potential element of \ne s value,  sh, if z is potentially an alias or an element of e s value.  wr, if z is potentially updated \n(overwritten) during evaluation of e (in this case, the old value of ~ may of course be neither an alias \nnor an element of the result),  T, in case of conflicting accesses, where confluence might be lost. \n  The partial ordering of Use is depicted in Figure 1. The total effect of an expression is abstracted \nto a liability, a mapping from the program s bound variables to pairs of uses. Lia = BVar -+ Use x Use \n A pair of uses U1 and U2 will be written U1. U2 Given that the liability of expression e in variable \nz is UI .u2, U1 represents the effect of e on m itself while U2 repre\u00adsents the effect on z s elements. \nIn the product domain, all pairs z .T and T.y are merged into the single error element T. T I wr I I \nFigure 1: The Use domain Remark: The term liability (instead of effect ) em\u00adphasizes the point that \nevery non-bottom use in the li\u00adability of an expression restricts the contexts in which this expression \nmay occur. Example: The liabilities of some simple expressions are: a.i aw rd. sh, i++ rd. rd updaix \n: a++ wr.el, i+ rd.rd, xR el.el.  4 Computation of Liabilities This section presents our abstract interpretation \nfor ef\u00adfect analysis. First, we need some operators on uses and liabilities. The liability of a compound \nexpression is computed from the liabilities of its constituents us\u00ad ing the combinators U (least upper \nbound), II (parallel composition) and ;; (sequential composition). The lat\u00ad ter two are defined by: \n= T, ifx=wr Vy=wr = Xug, otherwise XIIY T;;y =T ~;; T=T l;; ?J= ?J x ;;1 = Wr ifx=wr rd, ifx+wr x = \nT, afx=wr ;; Y = Y, afx+wr All operators defined in this section are assumed to be extended pointwise \nto the liability domain where appro\u00adpriate. A function f xl ... Xn is abstracted to an effect signa\u00adture \nof the form: Ml,... n/@ai@ai II (lb xl) : il II ... II (16 %n) : /n. That is, the effect of a function \napplication is the parallel composition of the function s effect on every parameter (lb stands for the \nliability of its body), and its effect ~~~ObaJon non-local variables. This amounts to regarding a function \nas a black box , in which nothing is known about execution paths. The expression x : 1 represents the \nresult of applying a function with an effect x on its formal parameter to an argument expression whose \nliability is 1. : is defined by: T : X.y =T U1.U2:T =T U1. U2 : X.y = (74,.242 : Z).(U,.U2 : y) 1.J-:x \n=1 U1. U2:X = x, if x E {L, rd, wr} UI.U2:ad = UI ul,u2:ei = U2 U1.U2 :sh = U1u q. If a function accesses \nlocal variables only, we can com\u00adbine its type and effect signature. We write argument uses before argument \ntypes. For example, the signatures of the predefine functions are: (+) :: rd.rd Int + rd.rd Int + Int \n(and likewise for , *, ...) cons :: ei. el CY+ ei. el [a] + [a] hd :: rd.sh[a] -a (and likewise for tl) \nvec :: rd. el [CY] -+ Array ~ rd. sh Array Q+ rd. rd Int -+ a() upd :: wr. el Array a + rd. rd Int + \nel. el a -+ Array a. The sequential composition x := el; e2 is computed by evaluating first el and then \ne2. Accordingly, the liability of the whole expression is the sequential composition of the liability \n/l of el and the liability 12 of e2. Inside e2, variable x is associated with a liability which reflects \nthe normal form of el , and which is computed by a composition of the function norm with !1. norm u = \n1, if uc{rd, wr} = u, otherwise Note that norm is not monotonic. We will therefore have to give proof \nthat the abstract interpretation as a whole is well-defined. Figure 2 details our effect anal\u00adysis. There \nare two abstract interpretation functions: LD, which abstracts sets of function definitions to func\u00adtion \nenvironments and L, which abstracts expressions to liabilities. The functions take two environments as \nparameters, one for bound variables and one for func\u00adtions. We assume that all bound variable names and \nall function names in a program are distinct from each other. Notation: The list abstraction ~ x ] x \n+-~s] stands for f applied to all elements z of list XS. Reduction of a list with a binary operator such \nas II or U is expressed by prefixing the list with the operator. All operators which we use in this cent \next are associative and have an identity element. Reduction of an empty list yields the identity of the \nreducing operator. The function .zzp takes two lists and combines corresponding elements in a list of \npairs. Mappings from variable names V to some domain D are expressed as follows: The expression x ~ d \ndenotes the function which maps variable x c V to d E D and which maps all other variables in V to ~D. \n1 V+.D stands for the function which maps all variables in V to ~D. Other mappings can be constructed \nfrom these with the operators U (pointwise upper bound) and o (function composition). f I v denotes the \nrestriction of function f to sub-domain V. A program is well-formed if its liability maps no bound variable \nto T: wf [e] = dx CBVar :~ [e] initfe ~~E~. x # T. The initial bound variable environment maps all vari\u00adables \nto 1. The initial function environment has been presented above. Syntactic Domains x E BVar (bound variables) \nf E FVar (functions) e E Exp (expressions) Semantic Domains Use = {_L, r-d, id, ei, sh, wr, T} Lia = \nBVar d Use x Use (liabilities) BEnv = BVar ~ Lia (bound variable environments) FEnv = FVar --+ Lia* + \nLia (function environments) Interpretation Functions LD :: Dci* -FEnv --+ BEnv * FEnv L :: Exp ~ FEnv \n-+ BEnv ~ Lia LD [dcls] fe be = fe w hererec fe = U[LF dId -dcis] ufe LF ~ zs = b] = f ++~ys. II [LB \nz : y I(z, y) +---Zip Z.S ~s] II LBl~Va.\\z, where LB = L[b] fe be be = U[T w (T H id.ei) Ic -ZS]ube \nys is a list of fresh variables, s.t. length ys = length xs L [e where dck-] fe be = L [e] (LD [dcls] \nfe be) be L[x]fe be = be [z] L ~ es] fe be = (fe ~]) [L [e] fe be[e -es] L [T := el ;e2] fe be = L [cl] \nfe be ;; L[e2] fe be where be = (z P-+ ((z H id.ei) u norm o (L [cl] fe be))) u be L [if el then e2 eise \ne3] fe be =L [,1]f, be ;; (L [,2] f, be U L[,3] f, be) Figure 2: Effect Analysis 30 The non-monotonic \nbehavior of norm might raise some doubts whether our abstract interpretation func\u00adtion is well-defined. \nIndeed, LF fails to be monotonic. This can be seen by considering the expression E ~ x:=fy; cons (upd \nx OO) (upd x OO). If the function environment ~e associates j with A1.(id, el : i) (that is, ~ is typed \nid. el TI + T2), we have: On the other hand, assuming LF ~] = J1.(wr.el : /), we get: Hence, here iS \nan example where $e < je but L [E] fe be > L [E] fe be. Fortunately, the non\u00admonotonic behavior of LF \nis restricted to error cases, i.e. situations where both result values contain T. LF can be shown to \nbe monotonic in a setting where all error states of domain Lia are merged into a single element. This \nsuggests that we should work in a lifted liability domain Lia whose elements are equivalence classes \nof liabilities. The equivalence classes are given by: i = T=, zj3~:lx=T = {1}, otherwise. All functions \non liabilities are lifted to Lia in the ob\u00advious way. The lifting is well-defined, as shown by the following \nreasoning: T= is the only equivalence class in Lia which corresponds to more than one element of Lz a. \nSince all operators on abstract uses preserve T u,., we have that all operators on liabilities map error \nlia\u00adbilities to error liabilities. Hence, every lifted function on Lia maps Tm to a unique value, namely \nitself. Theorem 1 (Monotonicit y) In the lifted domain Lia, for all e G Ezp, be c BEnv, fe, fe e FEnv: \nfe <fe =) LF [e] fe be< LF [e] fe be The proof of Theorem 1 is given in appendix A. From Theorem 1, and \nand the fact that all domains are finite, Theorem 2 follows. Theorem 2 (Computability) wf is well-defined \nand computable. Example: We compute the liability of function scatter, which updates array a with elements \nof array b. The update indices come from list u and are translated by table c. scatter xsabc= ifxs = \n[] then a else scatter (tl XS) (upd a (c.(hd XS)) (b.(hd XS))) bc According to our rule for function \ndefinition, the liabil\u00adity l~catter of scatter is: The liability lb of the body of scatter is: lb = xsH \nrd.rd ;; ( (a H id.ei) u l,catter (XS ++ rd.sh) (a H wr. ei, b ++ rd.el, c +.+ rd. rd, xs H rd. rd) (b \n-+ id.ei) (c -+ id.ei) ) Substituting the definition of l,ca~~er and rearranging gives: lb = (xs ++ rd.rd \n;; ((lb XS) : rd.sh II (lb a) : rd.rd) a t-+ id. el u (lb a) :wr. e[ b+-+(lb a):rd. elII(lb b):id. el \nct-+ (lba):rd. rd[[(lb c):id. el ) The least fixpoint of this equation is: ES++ rd.rd, aN wr.el, bw rd.el, \ncH rd.rd. Translated into prose, we have that array a is written, that the elements of array b are elements \nof the result, and that xs and c are read. This describes precisely our intuitive understanding of the \nfunction s effect.  Representation In this section, we discuss the human-readable repre\u00adsentation \nof liabilities and effect signatures. We would expect that liabilities are in general computed automat\u00adically, \nbut there are also situations where their repre\u00adsentation in human-readable form is required. First, \nif single-threadedness is violated, the precise cause of the error has to be pinpointed by the compiler. \nCommu\u00adnication in the other direction is needed if we adopt a module system like Haskell s [11]. The \ncompilation rules of Haskell are such that the clients of a module may be submitted to compilation before \nthe module itself is completed; the only requirement is that the module s interface be defined. In the \nabsence of an implemen\u00adtation, the programmer has to define the types and li\u00adabilities of all exported \nentities. Finally, because our technique is a checking algorithm (as opposed to an op\u00adtimization), programmers \nhave to anticipate the results of the checker, that is they have to reason about liabil\u00adities. We should \ntherefore look for a notation which is as concise and natural as possible. The UsePair domain which we \nhave employed in the last two sections leaves something to be desired in this respect. It is rather large, \ncomprising 62 + 1 states. Some of these states can never be reached. For example, the element-part of \na use-pair can never be id, since the element-part stands for elements of differing nesting depths. Generally, \nthe use-pair notation is better suited for automatic effect analysis than for human reasoning. We have \nfound it useful to employ another represen\u00adtation, in which all use-pairs of interest are given names \nof their own. We thus have to write just one identifier instead of-two but pay the price of a larger \nUse domain. Table 1 lists the interesting use-pairs together with their descriptive names and example \nexpressions. Every row describes a use value and contains an example in which the variable z is abstracted \nto this use. copy is used if an expression s result contains the ele\u00adments of a variable, but not the \nvariable itself. select is used if a selection yields an element of a variable. alias and include are \nused for variables which are identical to the result or an element of it. share stands for any combination \nof select, alias, and in Ade. Of the various write values, only write itself is likely to occur often \nin liabilities. It describes the effect of an update on a variable. writeread and writeshare stand for \ncases where an updated variable is subsequently discarded; only its elements are read or shared by the \nresult. deepwrite is used for variables whose elements are modified. Figure 3 presents some examples \nof effect signatures of predefine and user-defined functions. Name Pair Example J_ J-.-L read rd. rd \nlength x cop y rd. el vec x select rd. sh xl include el, ei updaix alias id. el x share sh. sh if p then \nx else tail x write read wr. rd length (upd x i y) writ e wr. el updxiy writeshare wr. sh (upd x i y).j \ndeepwrite wr. wr upd (x.j) i y -r T.T Table 1: Interesting use-pairs upd :: write Array cr _ read Int \n-+ include cr + Array a Vec :: copy [a] -+ Array CY cons :: include m -+ inciude [~] -[a] hd :: seiect \n[cr] + a swap :: write Array a d read Int + read Int + Array a scatter :: read [a] -write Array a -copy \nArray a * read Array Q + Array a histogram :: read Int -+ copy [(lnt, a)] + Array a histogram n xs = \naccum (vec[O [ i -[0,. n l]]) xs accum ~~ write Array Int -+ copy [(lnt, cv)] + Array Int accumaD = \na accum a (Cons (i, x) XS) = y:=a.i; updai(x+y) Figure 3: Example effect signatures  Discussion We have \npresented a framework which allows us to rea\u00ad son about side-effects in a language which combines functional \nexpressions with destructive array updates. The framework gives us a criterion for programs with\u00ad out \nobservable side-effects. The two main simplifications of the abstract seman\u00ad tics relative to the standard \nsemantics are: functions are black boxes, outside of which nothing is known about execution paths, and \n all elements of a structured value are treated the same.  One might think of relaxing the second \nsimplification by maintaining more information about structure elements, for example by distinguishing \nelements according to in\u00adclusion level. This might be useful for analyzing updates of structure elements. \nOur current analysis just marks as written the containing variable and all its elements. However, considerably \nmore knowledge is needed for an\u00adalyzing these deep updates . If we update a structure element we usually \nwant to re-link the updated datum into the enclosing structure. In our example language, this would be \nachieved by something like upd a i (upd (a.i)j x) The expression is safe only if all elements of a are \ndis\u00adtinct (which could be detected by an extension of our analysis along the lines of Wadler s linear \ndata types [16]), and the two z indices agree. Since the indices might be arbitrary expressions, a useful \nanalysis must be able to deduce equality of expressions, at least in some restricted cases. Also desirable \nwould be an extension of effect analysis to higher order functions. One problem in this area is that \nfunctional arguments do not always have a most general effect signature. Consider the function declara\u00adtion \nFfga=cons (updaix)(f(g a)) This function is confluent only if a is not passed through ~ o g. That is, \neither ~ or g can have an effect signature id. el rl --+ T2, but not both of them. This shows that effect \nsignatures do not possess the princi\u00adpal type property. Therefore, the best information an effect reconstruction \nalgorithm could produce is a set of admissible signatures. A more pragmatic immediate fix to the problem \nis also possible: One could require the effect signatures of function valued parameters to be declared. \nIf no dec\u00adlaration is present, unrestricted sharing but no writing could be assumed as a default, in \norder to stay compati\u00adble with the purely functional case. Our existing analy\u00adsis could then check conformance \nwith the programmer\u00ad supplied declarations.  Acknowledgements This work was inspired by a talk of Paul \nHudak in which he presented a preliminary version of [9]. It benefitted greatly from subsequent discussions \nwith Paul Hudak and Juan-Carlos Guzmiin. A Monotonicity of the Abstract Interpretation Functions The \nappendix contains a proof of Theorem 1, which states that function LF of Figure 2 is monotonic in its \nfunction environment argument. From this theorem the computability of our abstract interpretation function \nL follows. The main complication in the proof arises from the non-monotonic behavior of function norm. \nThis pre\u00advents a simple structural induction over the form of ex\u00adpressions and mandates an analysis of \nthe context in which norm appears. Theorem 1 (Monotonicity) In the lifted domain Lia, for all e e Exp, \nbe E BEnv, fe, fe E FEnv: fe <fe + LF [e] fe be< LF [e] fe be Proof: We first introduce a simplification. \nRather than regarding a liability as a mapping from variables to use\u00adpairs, we split every variable into \ntwo parts, each of which is mapped to a single use. A variable z is split into Zid and Xei, and, given \nthat the liability i of x is U1 .U2, we define 1 ztd = U1 and ~ Xel = U2. This is clearly only a notational \nmodification. The proof proceeds by structural induction on the form of expressions. With the exception \nof norm, all operators which construct liabilities are monotonic. The only nontrivial case is hence the \nsequential composition, which looks in our simplified model as follows: L[z=el;ez]fe be = L [el] fe be \n;; L [e2] fe be where be = (x H ((z,, H id) U(~e~H e/)U norm (L [el] fe be))) U be. To give names to \nthe parts of these equations, we define: h = ~~.(Z~d H id) U (ze~ = e/) U norm /, = A1. L [e2] fe ((z \nI-+ 1) U be), ; = N.l;; (goh)l. We have then: L[t=el ;e2]fe be = f(L[el]fe be) In th e following, we \nwill mostly reason in the original liability domain Lia but will need sometimes also the lifted domain \nLia. To distinguish both cases, we will overline operators in Lia. Our task is to show that ~ is monotonic \nin its liability argument. i.e: The only non-trivial case in equation (2) is given by U1 E {id, el}, \nU2 = wr. This is the only case where function norm shows non-monotonic behavior, namely norm U1 = U1 \nand norm U2 = 1. We have to show that the decrease in norm is cancelled out in the whole of ~, i.e. that \nthe following holds: To show (l), wetakea closer look at the possible forms of ~, g, and h. Given fixed \nenvironments je and be, all these functions are L-Mappings, i.e. they satisfy Defi\u00adnition 1. Definition \n1 An L-Mapping is a function @ : Lia + Lia which is of one of the forms (z) (v). A case analysis is \nused to show Case 1: ~ (.z H id) # Tm. In this case, ~ (z ~ id) = f (z We have to show that (3). w id)). \nLet y c BVar. (z) (22) (222) (iv) (v) @ @ @ @ @ = = = = = id K 1 (u :) 00, 0, (j)a?z id;; (01 0(02 u \nnorm o@3)) If y # z, strengthened we get: this to follows = . from Lemma On the other 1(b), hand, with \nif y < = z, Here, u E Use, 1 ~ Lia, @l and 02 andoisone ofu, n,;;. Two useful properties of L-Mappings \nLemma 1 For all L-Mappings 0, xl, Z2 c BVar, are L-Mappings, are: Ul, U2 E Use: < < = f(g=@~ (case assumption) \nwr (properties of ;; ) ((g = ~r) ;; (90 k) (definition of f) (Y+ W~)) Y f(Y++w~)Y Case 2: ~ (z w id) \n= Tm. To show in this case: ~(zwwr)=Tm (5) The proofs of properties (a) and tions over the form of L-Mappings \nThey are left out here since they difficulties. (b) are simple induc\u00adgiven in Definition 1. present no \ntechnical From the case assumption follows that there exists Y c BVar such that f (z H id) y= Tu,e. If \ny # z, Lemma l(b) yields f (z H wr) y = T u.,, which implies (5). To prove (5) in case y = z, we need \ninequality (6): Proof of Theorem (a), we can rewrite 1 continued: Because f and ~ as follows: of property \n(goh)(y~wr)y=l+ (goh)(y~id)y < (goh)(ywid)z,d. (6) The proof definition of of (6) the proceeds L-Mapping \nby structural g. induction on the Hence, f is monotononic if we can show that: Case 2.2 (i): g = id 34 \n (id Oh) (y +-+ id) y = (definition of h) ((za~ I+ id) U(z,, ~ ei) U (y ~ id)) y = (@-reduction) id = \n(/?-abstraction) ((z,d ~ id) u (zej w et) !-l (y H id)) z,~ = (definition of h) (id oh) (g w id) ~id \n Case 2.2 (ii): g= K V (K/ oh)(y~id)y = (~-reduc.tion) 1 y = (~-abstraction) (K 1 0h)(y= Wr) y = (premise \nof (6)) 1 < (~ / oh) (y ~ id) ~,d  Case 2.2 (iii): g = (M :) O@ (( a :)O@) (y w id) y = (/3-reduction) \n u:(~(yt+id)y) < (induction hypothesis, monotonicity of : ) u :(0 (y ~ id) Z,d) = (~-abstraction) ((u \n:) oQ) (y + id) Z,d  The proofs of cases (iv) and (v) are like the proof of case (iii) simple induct \nion steps; we leave them out for brevity. This concludes our proof of proposition (6). But (6) implies \n(5) as shown by the following reasoning: j(y~id)y=T (definition of ~) (( Y=id);; (90~)(Y~id))Y=T (properties \nof ;: ) (goh)(y~id)y=T (6) (goh)(?JHw? )g#J_v (90h)(y=id)%d=T ~ (properties of ;; , Lemma l(b)) (( Y~w~);; \n(9 h)(y~w ))y=TV (goh)(y~w~)~zd=T a (definition of ~) j(YE+wr)Y=T V .f(y HWr )Zz~=T s (definition of \n~) 7(g~wr)=T= This concludes our proof of (3). The proof of (4) differs only in that occurrences of \nZ,d are replaced by Xel. (3) and (4) together imply (2) and with it the main theorem.  References [1] \nA. Bless. Update analysis and the efficient implemen\u00adtation of functional aggregates. In Proc. ~th Conf. \non Functional Programming and Computer Architecture, August 1989. [2] R. Cytron, J. Ferrante, B.K. Rosen, \nM.N. Wegman, and F.K. Zadeck. An efficient method for computing static single assignment form. In Proc. \n16th ACM Symp\u00adosium on Principles of Programming Languages, Jan\u00aduary 1989. [3] L. Damas and R. Milner. \nPrincipal type schemes for fnnctionsJ programs. In Proc. %h ACM Symposium on Principles of Programming \nLanguages, pp. 207 212, January 1982. [4] A. Deutsch. On determining lifetime and aliasing of dynamically \nallocated data in higher-order functional specifications. In Proc. 17 th ACM Symposium on Prin\u00adciples \nof Programming Languages, January 1990. [5] M. Draghicescu and S. Purushothaman. A composi\u00adtional analysis \nof evaluation order and its application. In Proc. ACM Conference on LISP and Functional Pro\u00adgramming, \nJune 1990. [6] W.L. Harrison. The interprocedural analysis and au\u00adtomatic parallelization of Scheme programs. \nLisp and Symbolic Computation 2(3/4), October 1989. [7] K. Gharachorloo, V. Sarkar aud J.L. Hennessy. \nA sim\u00adple and efficient implementation approach for single as\u00adsignment languages. In Proc. ACM Conference \non LISP and Functional Programming, 1988. [8] M. Gordon, R. Milner, L. Morris, M. Newey, and C. Wadsworth. \nA metalanguage for interactive proof in LCF. In Pr-oc. 5th ACM Symposium on Principles of Programming \nLanguages, 1978. [9] J.C. Guzmin and P. Hudak. Single-threaded polymor\u00adphic lambda calculus. In Proc. \n5th IEEE Symposium on Logic in Computer Science, June 1990. [10] P. Hudak. A semantic model of reference \ncounting and its abstraction. In: S. Abramsky and C. Hankln (eds): Abstract interpretation of declarative \nlanguages, Ellis Horwood Ltd., 1987. [II] P. Hudakand P. Wadler(editors) .Reportonthefunc\u00adtional programming \nlanguage Haskell. Technical Re\u00adport YALEU/DCS/RR666, Yale University, Depart\u00adment of Computer Science, \nNovember 1988. [12] J.M. Lucassen and D.K. Gifford, Polymorphic effect systems. In Proc. 15th ACM Symposium \non Principles of Programming Languages, January 1988. [13] J. McGraw et al. SISAL: streams and iterators \nin a single assignment language, language reference manual. Technical Report M-146, LLNL, March 1985. \n[14] The Munich project CIP, volume 1: The wide spectrum language CIP/L. Springer Verlag LNCS 183, 1985. \n[15] D.A. Schmidt. Denotational semantics as a program\u00adming language. Internal report CSR-1OO, Computer \nScience Department, University of Edinburgh, 1982. [16] P. Wadler. Linear types can change the world! \nProc. IFIP T(Y2 Working Conference on Programming Con\u00adcepts and Methods, April 1990. [17] P. Wadler. \nComprehending monads. In Proc. ACM Conference on LISP and Functional Programming, June 1990. [18] J.H. \nWilliams and E. L. Wimmers. Sacrificing simplicity for convenience: Where do you draw the line?. In Proc. \n15th ACM Symposium on Principles of Programming Languages, January 1988. \n\t\t\t", "proc_id": "99583", "abstract": "", "authors": [{"name": "Martin Odersky", "author_profile_id": "81100056476", "affiliation": "IBM T.J. Watson Research Center, P.O. Box 704, Yorktown Heights, NY", "person_id": "PP14030830", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/99583.99590", "year": "1991", "article_id": "99590", "conference": "POPL", "title": "How to make destructive updates less destructive", "url": "http://dl.acm.org/citation.cfm?id=99590"}