{"article_publication_date": "01-01-2002", "fulltext": "\n An Efficient Profile-Analysis Framework for Data-Layout Optimizations Shai Rubin Rastislav Bod\u00edk Trishul \nChilimbi Computer Sciences Dept. Computer Sciences Dept. Microsoft Research University of Wisconsin-Madison \nUniversity of Wisconsin-Madison One Microsoft Way Madison, WI 53705 Madison, WI 53705 Redmond, WA 98502 \nshai@cs.wisc.edu bodik@cs.wisc.edu trishulc@microsoft.com ABSTRACT Data-layout optimizations rearrange \nfields within objects, objects within objects, and objects within the heap, with the goal of increasing \nspatial locality. While the importance of data-layout optimizations has been growing, their deployment \nhas been limited, partly because they lack a unifying framework. We propose a parameterizable framework \nfor data-layout optimization of general\u00adpurpose applications. Acknowledging that finding an optimal layout \nis not only NP-hard, but also poorly approximable, our framework finds a good layout by searching the \nspace of possible layouts, with the help of profile feedback. The search process iteratively proto\u00adtypes \ncandidate data layouts, evaluating them by simulating the program on a representative trace of memory \naccesses. To make the search process practical, we develop space-reduction heuristics and optimize the \nexpensive simulation via memoization. Equipped with this iterative approach, we can synthesize layouts \nthat outperform existing non-iterative heuristics, tune application-specific memory allocators, as well \nas compose multiple data-layout optimizations. 1. INTRODUCTION The goal of memory optimizations is to \nimprove the effective\u00adness of the memory hierarchy [18]. The memory hierarchy, typi\u00adcally composed of \ncaches, virtual memory, and the translation\u00adlookaside buffer (TLB), reduces the memory access time by \nexploit\u00ading the execution s locality of reference. The opportunity for the optimizer is to help the memory \nhierarchy by enhancing the pro\u00adgram s inherent locality, either temporal or spatial, or both. To alter \nthe temporal locality, one must modify the actual algo\u00adrithm of the program, which has proved possible \nfor (stylized) sci\u00adentific code, where transformations such as loop tiling and loop interchange can significantly \nincrease both temporal and spatial locality [4, 14, 26]. However, when the code is too complex to be \ntransformed a situation common in programs that manipulate pointer-based data structures one must resort \nto transforming the layout of data structures, improving spatial locality. Many data-lay\u00adout optimizations \nhave been proposed [3, 4, 7, 8, 9, 11, 15, 16, 21, Permission to make digital or hard copies of all or \npart of this work for personal or classroom use is granted without fee provided that copies are not made \nor distributed for profit or commercial advantage and that copies bear this notice and the full citation \non the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires \nprior specific permission and/or a fee. POPL '02, Jan. 16-18, 2002 Portland, OR USA &#38;#169; 2002 ACM \nISBN 1-58113-450-9/02/01...$5.00 23, 24], with their specific goals ranging from reordering structure \nfields [7] to object inlining [11]. Data-layout optimizations synthesize a layout with good spatial locality \ngenerally by (i) attempting to place contemporaneously accessed memory locations in physical proximity \n(i.e., in the same cache block or main-memory page), while (ii) ensuring that fre\u00adquently accessed memory \ncells do not evict each other from caches. It turns out that these goals make the problem of finding \na good layout not only intractable but also poorly approximable [20]. The key practical implication of \nthis hardness result is that it may be dif\u00adficult to develop data-layout heuristics that are both robust \nand effective (i.e., able to optimize a broad spectrum of programs con\u00adsistently well). The hardness \nof the data-layout problem is also reflected in the lack of tools that static program analysis offers \nto a data-layout opti\u00admizer. First, there appear to be no static models for predicting the dynamic memory \nbehavior of general-purpose programs that are both accurate and scalable (as was shown possible for scientific \ncode [5]), although initial successes have been achieved for small C programs [1, 13]. Second, while \nsignificant progress has been made in deducing shapes of pointer-based data structures [22], in order \nto create a good layout, one is likely to have to understand also the temporal nature of accesses to \nthese shapes. This problem appears beyond current static analyzers (although a combination of [1] and \n[22] may produce the desired analysis power). The insufficiency of static analysis information has been \nrecog\u00adnized by existing data-layout optimizations, which are all either pro\u00adfile-guided or exploit programmer-supplied \napplication knowledge [3, 7, 8, 9, 11, 15, 16, 21, 23, 24]. Although many of these tech\u00adniques manage \nto sidestep the problem of analyzing the program s memory behavior by observing it at run-time, they \nare still funda\u00admentally limited by the hard problem of synthesizing a good layout for the observed behavior \n[20]. Typically based on greedy profile\u00adguided heuristics, these techniques provide no guarantees of \neffec\u00adtiveness and robustness. Indeed, our experiment show that some\u00adtimes their optimization is far \nfrom optimal (not effective), and that some programs are actually impaired (not robust). In this paper, \nwe propose a framework that enables effective and robust data-layout optimizations. Our framework finds \na good data layout by searching the space of possible layouts, using profile feed\u00adback to guide the search \nprocess. A naive approach to profile\u00adguided search may transform the program to produce the candidate \ndata layout, then recompile and rerun it. Clearly, such a search cycle is uninformative and too slow \n(or cumbersome) to be practical. Indeed, to avoid this tedious cycle our framework evaluates a candidate \nlayout by simulating its fault rate (e.g., cache-miss rate or page-fault rate) on a representative trace \nof memory accesses. Sim\u00adulation is more informative than rerunning since it allows us not only to measure \nthe resulting miss rates of the candidate data lay\u00adouts, but also to identify the objects that are responsible \nfor the poor memory performance. We use this information to narrow the layout search space by (a) determining \nwhich levels of memory hierarchy need to be optimized, (b) selecting optimizations (and thus layouts) \nthat may influence the affected levels of memory hierarchy, and finally by (c) excluding infrequently \naccessed objects from consid\u00aderation. Trace-based fault-rate simulation is not only more informative \nbut it is also faster than editing, recompiling and rerunning the pro\u00adgram. To simulate the program without \nre-compilation, we pre\u00adtend the program was optimized by remapping the original addresses of data objects \nto reflect the candidate data layout. To make theiterativesearchevenmorepractical, wespeed up thesim\u00adulation \nby compressing the trace as a context-free grammar, which in turn allows us to develop algorithms for \nmemoized simulation. This paper makes the following contributions: We present a framework for data-layout \noptimization of gen\u00aderal-purpose programs that permits composing multiple optimiza\u00adtions. Unifying existing \nprofile-based data-layout optimizations, the framework operates by first heuristically selecting a space \nof profit\u00adable layouts, and then iteratively searching for a good layout, using profile-guided feedback. \n(Section 3).  We develop techniques for efficiently evaluating a candidate layout optimization using \na trace of memory references. These techniques, based on memoization, miss-based trace compaction, and \non-demand simulation, make the framework s iterative data layout search efficient enough to be practical \n(Section 4).  Using the framework, we re-implement two existing data-lay\u00adout optimizations: Field Reordering \nand Custom Memory Alloca\u00adtion. The experimental results show that our iterative-search versions outperform \nthe existing single-pass heuristic optimizations (Section 5).   2. RELATED WORK In this section we \ndescribe previous work on profile-driven data\u00adlayout optimization. First, we discuss general concepts \nthat apply to all optimizations. Next, we illustrate the optimization steps com\u00admon to all, and finally, \nwe summarize existing optimizations and highlight their unique features. 2.1 Common Optimization Concepts \nData-layout optimization aims to improve a program s memory performance. Usually, memory performance \nis characterized in terms of the fault rates a program incurs on various memory resources. Memory resources \nare hardware or software elements that are part of the memory sub-system: all levels of the processor \ncache, the translation lookaside buffer (TLB), and the virtual-mem\u00adory page table [18, 19]. The fault \nrate is the fraction of references not found in the cache/TLB/page table. To reduce fault rate, data-layout \noptimizations attempt to find a good data layout for a program s data objects. Data objects are the optimization \ns building blocks . For example, structure fields rep\u00adresent the data objects for Field Reordering and \nClass Splitting opti\u00admizations [7], while structure instances are data objects for Linearization and \nCustom Memory Allocation [8, 21, 23]. A data layout is a placement of data objects in memory, namely \nit is a map from data objects to their memory locations. A good memory lay\u00adout is one that improves a \nprogram s memory performance. Finding an optimal memory layout is intractable [20]. Indeed, existing \ndata-layout optimizations use heuristic methods to produce a good memory layout. The heuristics are usually \nbased on two simple objectives. First, optimizations such as Class Splitting [7], Coloring [8], and Custom \nMemory Allocation [23] attempt to pre\u00advent having highly referenced data being evicted from the cache \nby infrequently accessed data. Second, optimizations such as Field Reordering [7, 15, 24], Linearization \n[8, 9, 16, 21], Global Variable Positioning [3], and Custom Memory Allocation try to increase the implicit \nprefetch that cache blocks provide, by packing contempo\u00adraneously accessed data in the same cache block. \nMost optimizations target only specific types of data objects. For example, Global Variable Positioning \ntargets global variables, Field Reordering targets structures that are larger than the cache block size, \nand Custom Memory Allocation targets heap objects that are smaller than the virtual-memory page size. \nMost optimizations use profile information to guide the heuristic layout techniques. A profile is a map \nfrom a set of data objects to a set of attributes. For example, the Field Pair-wise Affinity profile \nmaps a pair of fields to the number of pre-defined intervals in which they are both referenced. A field \nreordering heuristic can then use this pair-wise profile to place fields according to their temporal \naffinity [7, 15]. 2.1.1 Common Optimizations Steps The first optimization step is to identify the memory \nresources that limit program performance, which we call bottlenecks. This step is usually done manually, \nusing profiling tools such as Intel Vtune [25] that measures a program s memory performance (e.g., its \ncache miss rate). The next step is to select, out of many available optimizations, one that can potentially \nimprove the bottleneck s performance. Selecting a good optimization requires a deep understanding of \nthe program memory behavior, and is typically done by a programmer familiar with the program. Then, the \nselected optimizer is invoked and the automatic phase of the opti\u00admization begins. The optimizer first \nidentifies the data objects it intends to target. For example, the target objects can be the structure \nfields that should be reordered, the classes that should be split, or the global variables that should \nbe repositioned in memory. Next, it chooses a heuristic method to construct a new data layout for the \ntarget objects. The choice of heuristic is often tied to the profile informa\u00adtion used to drive the optimization. \nFor example, the Fields Access Frequency profile can be used to identify the splitting point between \nhighly referenced and rarely accessed member fields of a class. Finally, the program must be modified \nto produce the new data layout. This step is often done manually. The program is re-written to generate \nthe new layout, then re-compiled and re-executed to measure the performance benefit. In most cases, the \ninitial benefit is small and the optimization must be tuned further. This tuning often requires using \na different profile and/or a different heuristic to pro\u00adduce the optimized layout. In the worst, though \nnot uncommon, case the new layout may not yield the expected results despite best tuning efforts, and \nthe entire process must be repeated with a differ\u00adent optimization. Table 1. data-layout optimizations \nMemory Resources and Data Objects Targeted Profile Information Used Heuristic Objective and Methods Field \nReordering [7, 15, 24]. Increases cache block utilization. Targetsstructuresthathave more than two fields \nand are larger than the cache block size. Two profiles are commonly used: 1. Fields Pair-wise Affinity: \ngcount. Maps pairs of fields to their temporal affinity. The fields tem\u00adporal affinity is the number \nof inter\u00advals in which they were both f1\u00d7f2 Objective: place fields that are concurrently accessed in \nthe same cache block. The heuristic uses the profile to build the affinity graph of fields: nodes are \nfields, arcs represent the affinity between them. Based on graph clustering algorithms [12], the optimization \nclusters fields with high temporal affinity into the same cache block. Class Splitting [7, 24]. Increases \nObjective: Avoid bringing rarely accessed data into the cache. The cache utilization. Targets data objects \nreferenced, for a given length of a time heuristic uses the Frequency profile to split classes into hot \nand whose collection of hot fields are interval. cold parts. Hence, when accessing the hot part, the \ncold part is smaller than the cache block size. 2. Fields Access Frequency: f1gcount. Maps a single structure \nfield to the number of its references. not brought into the cache, leaving more space for other hot data. \nGlobal Variable Positioning [3]. Reduces cache conflicts among glo\u00adbal variables. Targets globals. Similar \nto the Field Reordering optimization. Coloring [8]. Reduces cache con\u00adflicts. Usually targets dynamically \nallocated objects. The two profiles mentioned above, but at an object granularity. Alternatively, an \nimplicit profile based on knowledge of data structure topology. Objective: to prevent frequently referenced \ndata being evicted from the cache by rarely accessed data. Using the Frequency profile, or programmer \nknowledge, the heuristic distributes the objects among the cache sets, so that frequently accessed objects \ndo not conflict with rarely accessed objects. Linearization, Clustering [7,8,11, Objective: to increase \nthe locality of reference among structure nodes 16, 21]. Increases cache-block utili\u00ad (e.g., link list \nnodes). The heuristic usually exploits the topology of zation. Targets recursive data struc\u00ad data structure \nnodes to re-position them in memory. For example, the tures with elements smaller than the optimization \nclusters two linked list nodes that are linked via the cache block size. next pointer into the same \ncache block. Custom Memory Allocation [23]. Heap Object Lifetime profile: xgl. Objective: to increase \nheap object reference locality by allocating Increases virtual-memory utilization. Where x is a heap \nobject, l is an attribute describing its lifetime behav\u00adior. Previous work used four attributes: (i) \nhighly referenced, (ii) rarely refer\u00adenced, (iii) short lived, and (iv) other. objects with the same \nlifetime behavior within the same memory region. Two heuristic methods are used. First, the programmer \nheu\u00adristically defines the correct lifetime attributes to use (e.g., one for hot and one for cold objects). \nSecond, the optimizer heuristically partitions the objects into memory regions, according to their lifetime \nbehavior (see Section 5.1 for more details).  2.2 Existing Data-Layout Optimizations To the best of \nour knowledge, all previous work on (implicit) profile-driven data-layout optimization [3, 7, 8, 11, \n15, 16, 21, 23, 24] follows the process described above. Three features distinguish the optimizations: \nthe memory resources targeted, the heuristic method used to construct a new layout, and the profile information \nused to guide the heuristics. Table 1 summarizes these features for existing data-layout optimizations. \nHowever, prior work does not directly address several key issues such as systematic identification of \nbottlenecks, optimization selec\u00adtion, profile and heuristic selection, and, perhaps most importantly, \ntuning the optimization to the program being optimized. Our data\u00adlayout optimization framework, described \nin the next sections, uses profile feedback and an iterative search process to address these issues. \n 3. NEW DATA-LAYOUT OPTIMIZATION PROCESS We now present the overall structure of our data-layout optimi\u00adzation \nprocess. First, we give a general description of the process, then describe each step in detail. 3.1 \nProcess Overview Figure 1 shows the overall structure of the process. The goal of the process is to find \na data layout that improves the program s memory performance. Since efficiently computing even an approx\u00adimation \nof an optimal layout is intractable [20], our process is based on a search in the space of data layouts. \nWe define this space, infor\u00admally, to be all possible layouts that can result from applying any data-layout \noptimization in any order. Because the layout space is too large and an exhaustive search is usually \nnot practical, we pro\u00adpose two techniques to increase the search feasibility: analyses to narrow the \nsearch space, and the possibility to use a hill climbing to guide the search into more promising areas. \nThe goal of the first three steps is to narrow the search space. In the Bottleneck Identification step \nthe process identifies the memory resources (e.g., cache, TLB) that limit the program memory perfor\u00admance. \nThen, for each resource bottleneck the process finds the data objects that mostly influence its memory \nbehavior by perform\u00ading the Data-Objects Analysis step. Last, it selects optimizations that affect the \nmemory behavior of these critical objects (Select Optimizations step). By selecting these optimizations, \nthe process effectively narrows the search space only to layouts that have the potential to improve the \nbottleneck s memory behavior. The Build Profiles step computes profile information for the selected optimizations. \nNext, a search to find better layouts begins (Apply Optimizations and Evaluation steps): Starting with \nthe origi\u00adnal program layout, the process may evaluate a large number of candidate layouts, incrementally \nimproving the best layout it has foundsofar.The Apply Optimizations step provides the next candi\u00addate \ndata layout, by applying (possibly simultaneously) the optimi\u00adzations chosen in the Select Optimization \nstep. The Evaluate step evaluates the candidate layout and provides feedback that can guide a hill-climbing \nsearch. Bottleneck Identification Table 2. Optimization Predicates. Return true iff the object Narrowing \nthe search space is a suitable target for the optimization Data-Objects Analysis  Hill climbing search \n Figure 1. Overall Process Structure  3.2 Process Steps We now describe each process step in detail. \nAll steps use the data-objects trace which is the sequence of data objects (and their addresses) referenced \nby the program during execution. Each can\u00addidate optimization must provide the following inputs: the \nmemory resources the optimization targets, a Build Profile function that builds the profile(s) the optimization \nrequires, an Optimization Predicate which specifies the data objects the optimization targets, and a \nNext Layout function that iterates over different layouts the optimization may produce. The formal definitions \nof these inputs, together with other definitions, are found in Table 3; the formal pro\u00adcess is described \nin Table 4. 1. Bottleneck Identification. This step identifies the memory resources that limit program \nperformance. The process defines a memory resource as a bottleneck if itsmissrateishigher thana pre-defined \nthreshold (e.g., L1 miss rate higher than 2%). To cal\u00adculate the miss rate for a specific memory resource, \nthe process uses the Simulate function (see Table 4) that simulates the resource memory behavior on the \ndata-objects trace. 2. Data-Objects Analysis. For each bottleneck, the Data-Objects Analysis identifies \ncritical data objects. Critical objects are those objects that are responsible for most of the faults. \nFinding the critical objects helps narrow the search space to layouts that affect the placement of these \nobjects.  The Bottleneck Critical Objects set contains two type of objects: (i) The faulting objects \nthat frequently miss in the given resource. These objects are critical since changing their layout can \nreduce the resource s memory faults. (ii) The hot objects are the fre\u00adquently referenced data objects. \nThese objects are critical because introducing too many new memory faults in them will degrade performance. \nTo find the bottleneck critical objects, the process uses the Bot\u00adtleneck Critical Objects function (BCO \nin Table 4). This function traverses the data-objects trace and uses two parameters, k and j, to identify \nthe frequently faulting and frequently referenced objects. Ideally, the function should return the smallest \nset of objects that covers at least k% of the misses and j% of the refer\u00adences. Since finding the smallest \nset is difficult, the process finds a larger set that has the desired coverage. The BCO function builds \nthis set by adding the frequently referenced and frequently missed objects incrementally, until the set \nsatisfies the coverage require\u00adments. Our empirical results (Section 5.2) show that setting k=j=80% usually \nyields a small critical set (typically 10% of the total number of data objects) that covers both 90% \nof the refer\u00adences and 90% of the misses. Field Reordering [7, 15, 24]. (i) Structure size is larger \nthan the cache block size, and (ii) the structure has more than two fields. Class Splitting [7, 24]. \n(i) Some structure fields are accessed more frequently than others, and (ii) the set of frequently accessed \nstructure fields is smaller than the cache block size. Global Variable Positioning [3]. A global variable \n(data object) that is smaller than the cache block. Linearization, Clustering [8, 9, 11, 21]. a node \nof a recursive data\u00adstructure (e.g., a node of a link list) that is smaller than the cache block size. \nColoring [8]. All data objects that incur cache conflicts. Custom memory allocation [23]. A dynamically \nallocated data object that is smaller than the page size. 3. Select Optimization. The goal of the Select \nOptimization step is to narrow the search to those optimizations (and their resulting lay\u00adouts) that \ncan potentially improve the bottleneck s memory perfor\u00admance. Since critical data objects affect bottleneck \nperformance, the process looks for optimizations that target these objects. To this end, the process \nbuilds, for each optimization, the Optimization Target Objects set: the set of critical objects that \nare also targets for the given optimization. Optimizations with a non-empty Target Objects set have the \npotential to improve program performance. To quantify the optimization potential, the process calculates \nthe Target Objects set coverage, which is the fraction of the trace accounted for by references to objects \nin the set. Larger coverage implies greater optimization potential. To build the Target Objects set, \nthe process uses the Optimiza\u00adtion Target Objects function (OTO function in Table 4). For a given bottleneck \nand optimization, this function places Bottleneck Critical Objects that satisfy the optimization predicate \ninto the Optimization Target Objects set. The optimization predicate deter\u00admines if a data object is \nlikely to benefit from the optimization. For example, field reordering benefits structures that are larger \nthan the cache block size and have more than two fields. Table 2 lists the optimization predicates the \nprocess uses, which were selected based on previous work [3, 7, 8, 11, 15, 16, 21, 23, 24], and our experience \nwith these data-layout optimizations. From this point on, the search focuses only on optimizations with \nthe potential to improve program performance. 4. Build Profile. For each optimization selected, the process \nuses the Build Profile function (see Table 3) to build the profile(s) needed for that optimization. The \nprofiles are needed for the next step (Apply Optimization). At this point the process begins its search \nprocess. 5. Apply Optimizations. In this step the process produces a new candidate data layout for the \nprogram data objects. It uses the Lay\u00adout Selector function (LS in Table 4) to iterate over the space \nof possible layouts. Each time the function is called, it returns a valid data layout, or a null value. \nWhen the function returns a null, the process terminates. When the function returns a data layout, the \nprocess continues to the next step where the layout is evaluated and feedback is provided to guide the \nsearch (if desirable).  Table 3. Process Definitions 1. A data object x is an elementary piece of data, \nsuch as an object, a global variable of a basic type, a field in an instance of a class type, or an array. \nA trace T is a sequence of references to data objects. O is the set of all data objects in the trace \nT.The coverage of SOis the sub-trace length induced by S . objects, divided by the length of T. 2. Let \nM be the set of all memory addresses (locations) available for the program. A data layout is a map DL: \nOM . . 3. Memory Resource is a pair: r=(SimulationFunction, Threshold) where  Simulation_Function: Trace.double \n. A function that returns the fault rate (e.g., cache miss rate on T)of r on trace T.  Threshold is \na fault rate above which a resource is defined as a bottleneck. For a resource r, the Resource Critical \nObjects Set, O.O , contains data objects that are responsible for most of the misses in resource r. R \nis the set of  r all memory resources. 2O 2Profile attributes  4. A Profile is a map P: . .The profile \nattributes used are optimization specific. 5. Optimization is a tuple opt = (resources, ep, bp, nl). \n  resources.R is a set of memory resources the optimization targets to improve.  ep is the Optimization \nPredicate: ep: TO.bool which holds if the optimization targets the given data objects. (The trace, T \nis an input since some   \u00d7 r predicates need to calculate the miss rate). .Ois the Optimization Target \nObjects Set for resource r. ={ xO| ep(T,x)}. . OOPTr OOPTr rr 2Profile bp is the Build Profile function: \nbp: TO. . It returns a set of possible profiles needed to drive the optimization. \u00d7 OPT r 2Profile nl \nis the Next Layout function: nl: \u00d7T\u00d72O\u00d7misc.DL . The function is used to iterate over possible layouts \nof the optimization. It returns the next layout from the set of possible layouts for this optimization. \nmisc. is miscellaneous information needed to create a legal layout (e.g., order of allo\u00adcations of heap \nobjects). Table 4. The Framework data-layout optimization Process Process input: (i) Optimizations: a \nset of available optimizations (ii) Resources: a set of memory resources to optimize. (iii) T: the program \nmemory reference trace. (iv) restrictions: information needed to build a legal data layout (e.g., heap \nobjects allocation order). (v) k, j: coverage percentages for the BTO function. The defaults are k=j=80% \nand were determined by the experimental results (see Section 5.2). Step Procedural Description Functions \nUsed Initialization double benefit=0, best_benefit=0; pr: an empty vector; // optimization profiles CS: \na map from resources to Critical Sets. TS: a map from <optimization,resource> to Target Sets SO: an empty \nset; // selected optimizations Simulate(trace T, Resource r) MRR The Simulate function returns the miss \nrate of resource r on trace T. BCO(trace T, resource r, double k, double j) CSr The Bottleneck Critical \nObjects function returns the critical data\u00adobjects set, , for the resource r. CSr covers more than k% \nof T, and more than j% of memory faults incurred at resource r. OTO(trace T, Optimization o, critical \nset CSr) The Optimization Target Objects (OTO) function calculates the Optimi\u00adzation Target Objects set. \nUsing the optimization predicate, ep the func\u00adtion filters out only the critical objects that are targeted \nby the optimization. LS(trace T, Optimization[] v, Profile[] p, Target Objects T, double benefit, restrictions \nm) DL The Layout Selector function uses the Next Layout function of the selected optimizations to return \nthe next data layout the process should evaluate. It can use the benefit variable, which indicates the \nperfor\u00admance benefit of the last layout returned, to guide its decision about which data layout to return. \nEvaluate(trace T, data-layout DL) benefit Tnew is a trace defined by assigning to each data object, x, \nits address: DL(x). The function returns the performance benefit of the layout Tnew. The benefit is defined \nas . Wr is the weight of the resource r: the number of machine cycles it takes to access r. CSr.O OOPTr \nWr Simulate T r,( ) Simulate Tnew,r( ) ( ) r.Resources . Analysis Phases Bottleneck Identification. for \neach do { if (Simulate(T,r)>r.threshold CS[r] = BCO(T, r,k, j); } for each do { for each resource do \n{ TS[o,r]=OTO(T, o, CS[o.r]); if (| TS[o,r] | > 0 ) do pr = pr + o.bp(T, TS[o,r]); SO = SO + o; }}} while \n((DL= LS(T, SO, pr, TS, benefit, restrictions) != NULL) do { benefit = Evaluate(T,DL); if (benefit > \nbest_benefit) do { best_benefit = benefit; BestDL = DL; }} return BestDL;//the selected layout r.Resources \no.Optimizations ro.resources. Data-Objects Analysis. Optimization Phases Select Optimization. Build Profile. \nApply Optimi\u00adzations Evaluation The goal of the LS function is to guide the search towards more promising \nlayouts. Unfortunately, selecting good layouts is a difficult task, especially when applying several \noptimization simultaneously. For example, it is not known what is the best order in which to apply Field \nReordering and Class Splitting together with a Custom Memory Allocation. Ideally, the LS function should \nsynthesize a layout by combining several optimizations. However, we considered only one (described next) \nsimple approach to pro\u00adduce new layouts. The Layout Selector presented in the paper applies each optimi\u00adzation \nseparately, starting with the optimization whose target set has the largest coverage. The LS function \nuses the optimization s Next Layout function (see Table 3), to return a new layout that has not yet been \nevaluated. If feasible, the optimization s Next Layout function can simply iterate over all possible \nlayouts, as we did suc\u00adcessfully for the Field Reordering optimization in Section 5.5.1. If such an approach \nis too time consuming, the Next Layout function can use heuristics to limit the search, as we did for \nthe Custom Memory Allocation optimization in Section 5.5.2. 6. Evaluate Optimization. To evaluate the \noptimization, the pro\u00adcess uses the Evaluate function (see Table 4) that returns the per\u00adformance benefit \n(i.e., the difference in memory resource fault rates) of the data layout obtained in the Apply Optimizations \nstep. Previous work on data-layout optimization [3, 7, 23] evaluates a layout using a tedious, manual, \ncycle of editing the program, re\u00adcompiling it, and re-executing it. By contrast, in the process imple\u00admentation \npresented in Section 4, the Evaluate step uses the trace to perform the evaluation in an extremely efficient, \nautomatic way: The Evaluate function simulates the data-objects trace, but, instead of using the original \nobject addresses, it uses the new addresses from the data layout being evaluated. This efficiency is \ncrucial to making the iterative search feasible.  4. EFFICIENT FRAMEWORK FOR DATA-LAYOUT OPTIMIZATION \nThe previous section described a process for profile-based data\u00adlayout optimization at a fairly abstract \nlevel. This section presents a concrete instantiation of such a process and describes how it can be efficiently \nimplemented in a general framework for data-layout optimizations. The section begins with the description \nof the frame\u00adwork components, continues with algorithms needed for efficient process implementation, \nand ends with quantitative measurements of the framework efficiency. The general structure of the frame\u00adwork \nis presentedinFigure2. 4.1 Major Framework Components As described in Section 3, all process phases require \nprofiling information; in most cases the data-objects trace. In this section we describe this trace in \nmore detail, along with how the framework compacts it to facilitate process efficiency. Data- Objects \nTrace SEQUITUR  Simulation  Memoization  On Demand Cache Simulation  TLB-Miss Rate Page Faults \nCache MissRate Figure 2. Framework for data-layout optimization 4.1.1 Data-Objects Trace The data-objects \ntrace is an extension of the program data-refer\u00ad ence trace. Each entry in the trace contains not only \nthe address that was referenced, but also a unique identifier (a symbolic name) of the referenced data \nobject1. For example, assume a program refer\u00adences a global variable c, and two dynamically allocated \ninstances, a and b, of the same structure type foo,where foo has two fields: x and y. Assume that the \nprogram references these objects in the fol\u00adlowing order <a.x, b.y, b.x, c, a.x>. Then, the data-objects \ntrace is the following sequence of pairs: <(A1,a.x), (A2,b.y), (A3,b.x), (A4,c), (A1,a.x)>where A1, A2, \nA3,and A4 are the addresses of the sym\u00adbolic names a.x, b.y, b.x,and c, respectively. The data-objects \ntrace contains more information than the data\u00adreference trace. For example, in the reference trace one \ncannot always distinguish between instances of dynamically allocated objects (or stack variables) since \ntwo objects can share the same address at two different points in the program execution. However, in \nthe data-objects trace one can always distinguish between such cases. This unique feature of the data-objects \ntrace enables one of the framework s novel capabilities; it enables the framework to evalu\u00adate the future \neffects of a candidate data layout without program re\u00adexecution. As described in Table 3, a data layout \nis a map from data objects to memory locations. Hence, to evaluate the memory per\u00adformance of a candidate \ndata layout, the Evaluate function in Table 4 uses the data layout to assign to each data object its \nnew memory location, then, it simulates the trace with the new addresses to measure the new memory behavior. \nEvaluating a new layout without re-execution is not the only feature that enables an efficient search \nprocess. To increase effi\u00adciency further, the framework uses additional two methods. First, to enable \nfast trace traversal, it compresses the trace, so that the trace resides entirely in memory rather than \non disk. Second, to enable fast memory simulation, the framework uses novel simulation tech\u00adniques that \nexploit the internal structure of the compressed trace. The following sections describe the compressed \ntrace representa\u00adtion and our efficient simulation techniques. 1. Unique object id s can be generated \nby techniques presented in [3, 23]. 4.1.2 Compact Trace Representation For most programs, the data-objects \ntrace is very large, com\u00adposed of about 100M memory accesses, or about a few GBytes of data [6]. Since \nthe trace is used in all process phases, it is important to compactly represent it. Compact representation \nenables the framework to place the trace in physical memory so it can be easily analyzed (for example \nby the Simulate and the Evaluation func\u00adtions). Our compression scheme is based on Nevill-Manning's SEQUI-TUR \nalgorithm, which represents the trace as a context-free gram\u00admar that derives a single string the original \ntrace [6]. In the SEQUITUR representation, the grammar terminals represent the program data objects, \nand non-terminals represent sub-traces of the original trace. Figure 3b illustrates how SEQUITUR compresses \nthe trace in Figure 3a. The compression is possible because sub\u00adtraces 24, 25, and 2525 are repeated. \nThese sub-traces are repre\u00adsented with non-terminals A, B, and C, respectively. The SEQUI-TUR context-free \ngrammar is internally represented as a directed acyclic graph (DAG), as in Figure 3c. Internal nodes \nare non-termi\u00adnals and leaf nodes are the terminals. Outgoing edges are ordered and connect a non-terminal \nwith the symbols of its grammar pro\u00adduction. The grammar's start symbol is the root of this DAG. Given \nthe DAG representation, which is called Whole Program Streams (WPS), the original trace can be regenerated \nby traversing the DAG in depth-first order, visiting children of a node left to right, and repeatedly \nvisiting each shared sub-DAG. Chilimbi describes this process in more detail [6].  4.2 Efficient Algorithms \nfor Profile Analysis and Optimization Evaluation Compact trace representation is the first method the \nframework uses to achieve the efficiency that facilitates the iterative data-lay\u00adout search. The iterative \nprocess requires efficient methods to implement the Build Profile, Evaluate, and the Bottleneck-Identifi\u00adcation \nfunctions (see Table 4). This section describes three tech\u00adniques to obtain this vital efficiency. 4.2.1 \nMemoization-Based Profile Analysis Our memoization technique exploits the SEQUITUR grammar structure \nto increase the computation efficiency of profile analysis. To illustrate the memoization technique, \nassume we want to calcu\u00ad late the length of the trace represented by the grammar in Figure 3. In a brute-force \ngrammar traversal, each edge XYis traversed . paths(X)1 times (e.g., the edge A.2 is traversed 3 times). \nHow\u00adever, if we memoized the length of a rule after we traverse its sub\u00addag (e.g., the memoization value \nof the rule (sub-dag) A.24 is 2), then during the memoized-length computation each edge is tra\u00adversed \nonly once. Although the memoization speed-up depends on SEQUITUR representation s compression ratio (which \nis trace dependent), empirical results (Section 4.3) show that the benefits from memoization in terms \nof the computation time can be an order of magnitude. For a more realistic example, we describe the memoization \nval\u00adues needed for memoizing cache simulation of a fully-associative cache with a Least Recently Used \n(LRU) replacement policy. Figure 3. SEQUITUR Compression Scheme Assume a cache with K blocks. For each \ngrammar rule we per\u00adform cache simulation on the string defined by the rule, starting simulation with \nan empty cache. During simulation of the rule, we will compute the LRU state which iscomposedof 3 summaryval\u00adues: \n1. The CompulsoryList. This list keeps the first K compulsory misses defined as the first access to a \ncache block that occur during simulation. 2. The LRUcontext which is the context of the LRU queue at \nthe end of the simulation. The LRU queue is a list of cache blocks ordered by accessed time: the most \nrecent access at the head, the oldest access at the tail of the queue. 3. The misses variable, which \ngives the total number of cache misses that occurred during the simulation.  To understand why these \nthree values are sufficient to memoize the total number of cache misses in the trace, let us consider \na sim\u00adple example. Assume that T is a data-reference trace that is divided to two consecutive sub-traces \nT1,and T2. Assume that we have already computed (using cache simulation starting with an empty cache) \nthe above summary values for these two sub-traces. Since we simulated T2 starting with an empty cache, \nsome of the cache misses we counted would not have been occurred if we started sim\u00ad ulation with the \nLRU queue state at the end of the simulation of T1. We call these cache misses false misses. To obtain \nthe number of false misses, we compare T2.CompulsoryList and T1.LRUcontext. Each reference that is found \nin both of these lists is a false miss. Now let us consider how to obtain the final LRU context of T \nby composing the summary values of T1 and T2. Since the cache policy is LRU, all references in T2 are \nnewer than the references in T1. Hence, to obtain T.LRUcontext, we keep the first K entries in the concatenation \nof [T2.LRUcontext]and [T1.LRUcontext \\ T2.LRU\u00ad context]2. 2. T1.LRUcontext \\ T2.LRUcontext means T1.LRUcontext \nafter removing 1. The total number of paths from the start non-terminal S to X. elements that appear \nin T2.LRUcontext. // K is the number of blocks in the cache // input: LRU state after simulation of two \nconsecutive sub-traces T1, T2, starting with an empty cache. // output: LRU state after simulation of \nT1 followed by T2. 1. Compose(LRU T1, LRU T2) { 2. LRU ANSWER;  3. ANSWER.misses=T1.misses+T2.misses-T1.LRUcontext \nnT2.CompulsoryList 4. ANSWER.LRUcontext=head(concat(T2.LRUcontext,[T1.LRUcontext\\T2.LRUcontext]),K); \n 5. ANSWER.CompulsoryList= head(concat( T1.CompulsoryList ,{T2.CompulsoryList\\{T1.LRUcontext .T1.CompulsoryList \n}} ,K) 6. return ANSWER 7. }  Figure 4. Memoization Algorithm for a Fully-Associative LRU Cache with \nK Blocks Figure 4 presents the algorithm for producing the summary value of a trace T when T is divided \nto two consecutive sub-traces. To obtain the cache simulation result for a trace that is split into more \nthan two sub-traces (e.g., the WPS representation), we com\u00adpute summary values for all rules traversing \nthe grammar structure in a bottom-up manner and recursively composing the memoized\u00adrule values. In practice \nthe memoization algorithm (Figure 4) has two disad\u00advantages: (i) It is not suitable for simulation of \nlarge LRU caches since such caches have a large number of blocks (e.g., 512 blocks for a typical L1 cache), \nwhich causes the LRUcontext to become quite long. Long lists increase the overhead of memoization (espe\u00adcially \nthe set operations in lines 3, 4 and 5 in Figure 4); (ii) It is dif\u00adficult to use the same algorithm \nwhen we simulate a D-way set\u00ad associative cache1 since we need to keep recently used lists for every \ncache-set. To overcome these overhead problems, the framework simulates large caches using a more compact \ntrace representation, the Whole Program Misses (WPM) described in the next section. In addition, we describe \nan on-demand cache simulation technique (Section 4.2.3) that enables yet more efficient simulation for \nmea\u00adsurement of the effect of small data layout changes on the cache miss rate. 4.2.2 Miss-Based Compaction \nAlthough the grammar already significantly compresses the trace (about 10-fold), the resulting grammar \ncan be compressed fur\u00adther. In [6], Chilimbi describes several ways to further compress the SEQUITUR \ngrammar. Here we extend this work and present a new compression scheme, the Whole Program Misses (WPM) \nrepresen\u00adtation. From our framework s perspective, the WPM representation offers an advantage over the \nWPS representation and previous work: It is a more compact trace representation and enables more efficient \ncache simulation in cases where memoization is less effec\u00adtive. It preserves the number of cache misses \nin the program and consequently, the accuracy of cache simulation. The WPM repre\u00adsentation retains the \nability to remap all program data objects to new addresses, so it is also useful for evaluating of the \nimpact of an optimization. In addition, the WPM representation is easy to build 1. A D-way set associative \ncache is a cache that is divided into sets, each set containing D lines. Each memory block can reside \nin only one cache set. The replacement policy inside each set is LRU. directly from the trace or from \nthe WPS. The WPM contains only memory references that may cause a memory fault. In other words, the representation \nomits references that can never suffer any memory fault. For example, consider the reference sequence \nabba , where a and b are two different data objects in the original trace. Regardless of the cache (or \nTLB or page table) configuration, the second reference to b never causes any cache miss. Hence, this \nreference can be omitted from the trace (and also from its SEQUITUR representation) without affecting \nthe total number of misses. Furthermore, for a specific cache configura\u00adtion, it is easy to determine \nthat the second reference to a will always hit in the cache (even if we re-map a and b to different addresses). \nFor example, since we have only one unique access between the two consecutive references to a, the second \nreference to a will never suffer a cache miss if the cache is at least 2-way set\u00adassociative, each set \nhas at least two entries, and each set is man\u00adaged using the LRU policy. To retain the original reference \nbehav\u00adior, we never omit the first reference to a data-object. This maintains the ability of the framework \nto perform accurate cache simulation even if the addresses of all symbols are changed. To build the WPM, \nwe traverse the trace and keep a list of the last K data-objects seen (K depends on the particular cache \nconfigu\u00adration, see below). When processing a new reference, we compare it to the references in the list \nto see if it never causes a cache miss, and if so, we ignore the reference. Although this technique depends \non our cache configuration, a single WPM can be used for a several cache configurations. For example, \na WPM built for a fully-associa\u00ad tive LRU cache with 4 entries can be used with any fully-associa\u00adtive \nLRU-cache with more than 4 entries, or in any cache that is N\u00adway set associative cache where N=4.  \n4.2.3 On-Demand Cache Simulation The goals of the on-demand cache simulation are: (i) Calculate the number \nof cache misses that result from small changes to the original memory layout (e.g, changing the locations \nof a few global variables); (ii) Perform such simulation faster than whole-trace re\u00adsimulation. Such \nfast simulation capability is especially useful for optimizations such as global variables positioning \n[3], where the optimization needs to determine the effect of alternative layouts on the cache miss rate. \nWe describe such an algorithm for a D-way set\u00adassociative cache with K sets. On demand set-associative \ncache simulation is based on the observation that it can be accomplished by simulating each cache set \nseparately1, and later summing up the number of misses and the tion technique improve the efficiency \nof TLB simulation, we perform the following experiments. First, we simulate a TLB with number of references \nto obtain the overall cache misses. More for\u00ad mally, let missi denote the number of cache misses to the \nith cache set, then the total number of cache misses can be computed by k Misses =.missi . Hence re-mapping \na single data object from set m i=1 to set n only affects the number of misses and the number of refer\u00adences \nof those sets. Let us denote the new number of misses in sets n and m as missNewn and missNewm, then \nthe new number of misses is calculated by: k .. MissesNew =..missesi.+missNewn+missNew m.imn. ., Since \nthe total number of references in the new layout is the same as in the original layout the new miss rate \nis easily obtained from the last equation. The above discussion implies that for small data layout changes \nwe need to simulate only two or a small number of cache sets. Unfortunately, to simulate two sets we \nneed to traverse the whole trace (or the equivalent representative grammar) to reveal where these two \nsets are referenced, so it would appear that the benefits from this approach are minimal. However, given \nthe hierarchical structure of the SEQUITUR grammar, we can use memoization to avoid the linear trace \ntraversal. After the first traversal of a sub\u00adtrace (a rule in the grammar, see Section 4.1.2) we record \na single boolean value indicating whether set m or set n are referenced in this sub-trace. If m or n \nare not referenced in the sub-trace, subse\u00adquent traversals of this sub-dag are unnecessary.  4.3 Evaluation \nof Framework Efficiency This section presents empirical results for evaluating the frame\u00adwork s efficiency \nbased on the three methods discussed (i.e., memoization, miss-based compaction, and on-demand cache simu\u00adlation). \nTo evaluate how the WPM representation and the memoiza\u00ad 1. In such a cache, each address can be mapped \nto a single, unique set. 32 entries and LRU replacement policy on the two grammar types, using a complete \nlinear grammar traversal. Second, we apply the simulation again, this time using the memoization algorithm \n(Figure 4) to avoid the linear traversal. In a different experiment we evaluate the effectiveness of \nthe on\u00addemand simulation method using a 4-way set-associative 64Kb cache with 512 sets (i.e., each cache-block \nis 32 bytes). First, we perform a full cache simulation of the two trace representation (i.e., WPS and \nWPM). Second, for the same cache we performed cache simulation of only two sets. Table 5 presents the \nresults from these two experiments (all times are average of three runs), and shows that these techniques \ncan improve simulation efficiency by over an orderofmagnitude. The results in Table 5 serve as a summary \nfor this section. We present algorithms that form the basis of the framework s effi\u00adciency. The memoization, \ncompaction,and on-demand cache simu\u00adlation algorithms are used to efficiently implement the functions \nrequired by the process (Table 4). They are especially useful for the implementation of the Simulate \nfunction and Evaluation functions. Table 5 shows that incorporating these methods into the framework \nenables it to evaluate the future optimization effect, in most cases, within seconds. This efficiency \nenables the iteration that is neces\u00adsary for our data-layout optimization process. Indeed, as presented \nin Section 5.5, this efficient iterative process enables us to develop new types of iterative search-based \noptimizations.  5. FRAMEWORK APPLICATION TO OPTIMIZE PROGRAM DATA LAYOUT This section describes the \nprocess of using our data-layout opti\u00admization framework to apply two optimizations: Field Reordering \nand Custom Memory Allocation. First, we define the goals of the two optimizations and then detail how \nthe optimizations interact with the framework at each step. Table 5. Framework Efficiency (all times \nmeasured on 500-Mhz 20164-Alpha machine) Benchmark Number of heap references in tracea TLB simulation \nusing complete grammar traversal (sec.) TLB simulation using memoization (sec.) Cache Simulation times \n(sec.) On demand cache simulation (simulating two cache sets) Whole Program Streams Whole Program Misses \nWhole Program Streams Whole Program Misses Whole Program Streams Whole Program Misses espresso 13,604,108 \n5.7 1.2 2.6 1 4.74 1.11 0.74 boxsim 36,157,141 25.8 12.3 1.4 1.4 17.69 8.9 0.51 twolf 41,354,395 22.7 \n12.5 1.4 1.3 17.3 10.82 0.45 perl 38,171,324 25.8 11.2 2.6 1.3 18.87 8.63 0.74 gs 82,872,046 60.7 22.3 \n11.0 5.0 40.64 16.1 3.44 lp_solve 28,050,510 19.2 7.1 1.2 0.5 14.39 5.6 0.42 Average speed up 1 2.62b \n11.1b 18.2b 1 2.22c 18.0c a. Since our main interest lies in data-layout optimizations for heap-intensive \nprograms, our grammars contain only heap references. How\u00ad ever, grammars for all program references can \nbe built easily. b. Speed up over TLB simulation using complete grammar traversal when using Whole Program \nStreams. c. Speed-up over full cache simulation using Whole Program Streams. 5.1 Optimization Description \nThe goal of Field Reordering is to reduce the number of cache misses by reordering the fields of a structure \ntype [7, 15, 24]. The cache miss rate is reduced by increasing cache block utilization, which is achieved \nby grouping fields with high temporal affinity in the same cache block. Existing Field Reordering heuristics \nuse the Pair-wise Affinity profile (see Table 1) to identify fields with high temporal affinity [7, 15]. \nStarting with the hottest pair, the heuristic incrementally builds a layout by appending a field that \nmaximizes the temporal affinity with the fields already in the layout. First proposedbySeidl andZorn[23], \nthe Custom Memory Allo\u00adcation (CMA) optimization aims to improve virtual memory perfor\u00admance (i.e., page \nfaults, TLB faults, memory consumption) by increasing memory page utilization. To achieve this goal, \nCMA attempts to place heap objects with high temporal affinity in the same memory page. CMA decomposes \nthe optimization task into two sub-problems: (i) finding a layout for heap objects that improves virtual \nmemory performance, and (ii) defining an allocation policy that enforces this layout at runtime. An example \nof an allocation policy might be to cluster all objects that were allocated in a procedure foo in a sep\u00adarate \nmemory region. Typically, the allocation policy is based on predictors [2], which are runtime attributes \nassociated with each allocated object (e.g., the call-stack context at object allocation time). Seidl \nand Zorn solved the two sub-problems separately [23]. First, using a heap-object lifetime behavior, they \ndefined a good layout based on four pre-defined memory regions corresponding to four object lifetime \nbehaviors: the highly referenced objects region, the rarely referenced objects region, the short-lived \nobjects region, and the other objects region. Then, they proposed a set of run-time predictors (i.e., \nthe object size and allocation call stack context) that could enforce an approximation of their desired \nlayout. 5.1.1 Optimization Instantiation Table 6 presents the parameters for the two optimizations that \npermit them to be instantiated in our framework. For both optimiza\u00adtions, the Next Layout functions encapsulate \nthe main difference between previous work and our implementation of these optimiza\u00adtions. The Field Reordering \nNext Layout function drives an exhaus\u00adtive search in the data layout space to find a good layout for \neach structure (as described in Section 5.5.1). The CMA Next Layout function drives a hill climbing search \nto find a good allocation pol\u00adicy (as described in Section 5.5.2). Our search based approaches inherently \ndiffer from the heuristic approaches used in the past. Since heuristics are based on certain assumptions \nto construct a new layout (e.g., reordering fields according to their temporal affinity improves cache \nbehavior), if an assumption does not hold for a par\u00adticular program this may result in a layout that \nactually degrades performance (as in the case of Field Reordering for perl in Table 10). In contrast, \nour search will never find a layout that low\u00aders performance since our framework commits to a data layout \nonly if it is proven better through simulation on the whole program trace than the original layout. . \nTable 6. Optimization Parameters Field Reordering Custom Memory Allocation Optimized L1 cache: Virtual \nMemory resource r=(CS, 0.02). CS is a Cache Simulator function that returns the miss rate of r on the \ntrace T. r=(WSS, 10). WSS is a Working Set Simulator function that returns the average working set size \nof the trace T. Optimiza\u00adtion Predi\u00adcate see Table 2 Build Field Frequency Profile: Heap Objects Profile: \nProfile , maps fields objects.real .objects.attribute set function to their access frequencies. maps \nobjects to runtime attribute (see Section 5.5.2). Next Exhaustive search for a Recursively partition \nthe Layout candidate layout. In case objects into memory function this search is too costly, perform \nan exhaustive search only using fre\u00adquently accessed fields (see Section 5.5.1). regions (see Section \n5.5.2). Another difference from previous work arises in the way we measure the performance benefit of \nCMA. Workstations today have large physical memories (e.g., at least 512MB), that accommodate the data \nset of most programs. Hence, most programs suffer only a few compulsory page faults (i.e., faults resulting \nwhen a page is first referenced) when running in isolation on a machine. Thus, we measure the benefit \nof CMA using a program s Average Working Set size the average number of virtual memory pages the program \nuses [10] rather than its page fault rate. Improving this perfor\u00admance metric allows more applications \nto concurrently run on a machine without paging to disk. As discussed in Section 3, the framework requires \na threshold for each memory resource to identify bottlenecks. Selecting the correct threshold is a difficult \ntask that requires experience and tuning. For a 16KB directed mapped cache, miss rates lower than 2% \nappear normal behavior for well tuned programs (see page 391 in [18]). For the virtual memory system, \nwe use our own esti\u00admation that a program using less than 10 pages (each page is 8Kb) is not limited \nby its virtual memory performance.  5.2 Bottleneck Analysis The process starts with Bottleneck Identification \nwhich finds the memory resources that limit the program s memory performance (see Section 3). Table 7 \npresents the cache miss rates and the work\u00ading set sizes for our six benchmarks. Using the resource thresholds \npresented in Table 6, all programs except espresso suffer from L1 cache and virtual memory bottlenecks. \nTable 7. Memory Bottlenecksa Table 8. Critical Objectsa Bench\u00admark Number and per\u00adcentage of heap references. \nCache miss rateb Working set size (pages)c espresso 13,604,108 (65%) 0.67% 8.27 boxsim 36,157,141 (36%) \n7.70% 60.04 twolf 41,354,394 (44%) 9.31% 17.35 perl 38,171,324 (37%) 3.21% 25.90 gs 82,072,046 (45%) \n8.22% 128.30 lp_solve 28,050,510 (58%) 13.80% 26.30 a. A bold number indicates that this resource is \na bottleneck for that benchmark. b. 16KB direct-mapped cache. c. Average number of pages touched every \n100,000 heap references. Thesizeofeachpageis 8Kb.  We compute these metrics, and all further results \nin this section, only for heap references for the following reasons. First, in general purpose programs \n(and also in our benchmarks as the percentage of heap references demonstrates), heap objects have a large \ninflu\u00adence on overall memory performance [3]. Hence, improving the memory behavior of these objects is \nimportant for good overall memory performance. Second, both optimizations mainly target heap objects \nstructure fields or structure instances [7, 23] by clustering them into the same memory unit a cache \nblock or a memory page to increase utilization. Since other objects, such as global or stack variables \ncannot be clustered together with heap objects, their presence in the trace does not affect the decision \nof which objects to cluster together. As mentioned in Table 4, the process continues with Data-Objects \nAnalysis, which builds the critical set for each bottleneck. Since the bottleneck memory behavior is \nmostly influenced by the critical objects, the process uses them, in the next step, to narrow the search \nonly to those optimization that target these objects. Our L1-cache critical set covers at least 80% of \nboth the cache misses and data references (i.e., k=j=80% in function BCO in Table 4). It turns out that \nthe virtual memory critical set is almost identical to the L1-cache critical set, since both sets contain \nobjects that cover at least 80% of data references. Table 8 presents critical set sizes and their coverage \nrates for all benchmarks. For completeness, we show the results even for cases where the framework does \nnot actually perform the calculation for a given program (espresso in Table 8); these results are in \nstrike\u00adthrough text. Table 8 suggests an analogy between code optimiza\u00adtion and data-layout optimization. \nSimilar to the commonly used rule-of-thumb that 10% of the code is executed 90% of the time, Table 8 \nsuggests that 10% of data-objects cover 90% of all memory references and (almost) 90% of cache misses. \nIndeed, some data\u00adlayout optimizations such as Field Reordering and Class Splitting do optimize only \nthe highly referenced objects [7]. However, whether other optimizations such as CMA can derive almost \nall their benefit from placing only critical objects requires further investigation. Bench\u00admark Total \n# of objects Critical set Sizeb Reference coverage Cache-miss coverage espresso 131,326 9.6% 86% 85% \nboxsim 99,881 5.2% 96% 90% twolf 16,287 10.3% 90% 96% perl 108,552 11.7% 91% 85% gs 368,205 14% 88% 85% \nlp_solve 19,928 13.7% 84% 84% Average 10.70% 89.1 87.5% a. We present data for all benchmarks even if \nno bottlenecks were iden\u00ad tified. Suchdataismarkedwithstrike-throughtext. b. Percentage of all data objects. \n 5.3 Select Optimizations To narrow the search, the process selects optimizations that pro\u00adduce layouts \nwith the potential to influence bottleneck behavior. To achieve this, the process computes the Optimization \nTarget Objects set for each candidate optimization. The set is computed by apply\u00ading the optimization \npredicate (presented in Table 2) to the critical objects, retaining only those that qualify as optimization \ntargets. Table 9 shows the target sets coverage rates for our two optimi\u00adzations. The coverage rate, \nwhich is the fraction of the trace cov\u00adered by the set, measures the potential of the optimization to \naffect the program memory behavior; the higher the rate, the higher the potential (see Section 3 for \na more formal definition of these con\u00adcepts). The process eliminates optimizations whose target sets \nare empty1; these optimizations have very little potential to improve the bottleneck performance. In \nour case, Field Reordering is eliminated for espresso, lp_solve,and ghostscript. Table 9. Target Objects \nSet Coverage Rate Coverage Rates Selected Optimizations Field Reorder\u00ading CMA Field Reorder\u00ading CMA espresso \n0% 71% no yes boxsim 27% 18% yes yes twolf 42% 45% yes yes perl 22% 33% yes yes gs 0% 2.5% no yes lp_solve \n0% 11% no yes  5.4 Build Profile The Build Profile step builds the profile information needed to drive \nthe optimizations. As we will shortly discuss in Section 5.5, our Field Reordering optimization is based \non an exhaustive search 1. The framework can be easily changed so that it eliminates optimizations with \ntarget sets below a preset threshold. in the space of field layouts. However, when the exhaustive search \nis too time consuming, the optimization uses the Field Frequency profile (see Table 6) to narrow the \nsearch space. First, the optimiza\u00adtion groups fields into pairs according to their frequency, and then \niterates over all possible field-pair permutations. To build an allocation policy, which maps sets of \nheap object attributes to memory regions, the CMA requires a profile of all the heap objects allocated \nduring the sample execution with each object annotated with the values of its runtime attributes. For \nthis purpose, we used an annotated allocation trace. The allocation trace is the history of all program \nallocations and de-allocations. Each entry in the trace corresponds to one physical dynamically allocated \ndata object, and each entry is annotated with the values of seven runtime attributes: the last three \nprocedures on the stack, the sizes of the last three allocated objects, and the current allocated object \nsize. The values of these attributes are recorded at the entry point of the allo\u00adcation routine (e.g., \nC malloc). 5.5 Iterative Profile-Feedback Search To find a better memory layout, the process continues \nwith an iterative search process. Starting with the optimization that has the highest potential (i.e., \nthe highest coverage rate for the optimization target set), our framework applies a separate hill climbing \nsearch for each selected optimization; each search process starts with the resulting layout of the optimization \napplied previously. In this section, we describe the search for each of our example optimizations. The \nsearch combines two process steps: The Apply Optimizations step, which uses the Next Layout function \nto produce a candidate data layout, and the Evaluate step, which provides the necessary profile-feedback \nby measuring the performance benefit of the candidate data layout. 5.5.1 Field Reordering Exhaustive \nSearch Since finding the optimal structure layout is intractable [20], our Field Reordering Next Layout \nfunction iterates over all possible layouts (i.e., over field permutations) for each targeted structure \ntype. As specified in Table 2, Field Reordering targets data struc\u00adtures with more than two fields that \nare larger than the cache block size. Starting with the most frequently referenced structure, we iter\u00adate \nover all field permutations within each target structure, commit\u00adting the best layout produced. This \nexhaustive search strategy finds almost optimal layouts for each structure. The layout is almost optimal \nsince it depends on the order in which we iterate over the structures themselves. If the number of the \nstructure fields is too large and iteration over all possible structure layouts becomes too time consuming \nin our current framework implementation if the number of layouts exceed one thousand we narrow the exhaustive \nsearch using the field pairs technique. Since the cache miss rate is mostly influ\u00adencedbythe distribution \nof fields among cache blocks rather than the order of fields inside a cache block, our goal is to quickly \nexplore the set of fields that should reside in the same cache block rather than their order within the \nblock. Consequently, we first order the fields according to their access frequency. Then, we group the \nfields into pairs according to this order (such that the most fre\u00adquently accessed field is paired with \nthe second most frequently accessed, the third with the forth, and so on). Last, we iterate over all \nthe layouts resulting from all pair permutations. For example, a structure with 12 fields that originally \nrequires evaluation of 12! layouts, now requires the evaluation of only 6! layouts. Indeed, our results \nshow that the difference between full iteration and the field pairs approach is negligible (less that \n0.1% reduction in miss rate). 5.5.2 CMA Hill Climbing Search As mentioned, Seidl and Zorn s CMA solves \none sub-problem at a time: First they find a good memory layout using a predefined number of memory regions, \nand then they build, using runtime pre\u00addictors (attributes), an allocation policy to enforce it at runtime. \nThis decomposition has a drawback: the desired layout may not be enforceable by the set of predictors, \nso the actual layout enforced at runtime may not yield the expected performance benefit. We tackle this \ndifficulty by combining the two problems into one classifica\u00adtion problem: given a set of attributes, \nfind an allocation policy that classifies the objects into different memory regions such that the resulting \nmemory layout has good memory behavior. Solving the this classification problem yields an enforceable \nlayout that should improve performance. We adopt a simple approach to solving the classification prob\u00adlem: \nwe synthesize an allocation policy by building a decision tree from a given set of attributes [17]. Our \nCMA Next Layout function synthesizes an allocation policy by incrementally partitioning the heap objects \ninto different regions. Starting with all objects in a sin\u00adgle memory region, the function works as follows. \nGiven a memory layout with n regions, it produces all possible layouts with n+1 regions. To obtain a \nlayout with n+1 regions the function divides a region into two sub-regions by using a single attribute \nto distinguish between the objects in the original region. After evaluating all pos\u00adsible layouts with \nn+1 regions, the function selects the layout with the highest performance benefit. If there is no additional \nbenefit from this partitioning, the function returns null. 5.5.3 Evaluation Step The evaluation step \nis similar for both optimizations. The frame\u00adwork pretends that the optimization was applied by substituting \nthe address of each data object in the data-object trace with its new address from the candidate data \nlayout. After the addresses are re\u00admapped, the framework uses the Simulate function (see Table 4) to \nevaluate the new trace.  5.6 Optimizations Results This section presents the results of our two example \noptimiza\u00adtions. We also discuss the cost and effectiveness of the iterative approach. 5.6.1 Field Reordering \nResults We have compared our iterative approach with other existing methods for Fields Reordering. First, \nwe ordered fields according to their frequency profile, starting with the fields with the highest fre\u00adquencies \n[24]. Second, we ordered the fields according to their pair\u00adwise affinity profile, as discussed in Section \n5.1 [7]. Last, we iter\u00adated over the fields layout space, exhaustively exploring many pos\u00adsible layouts, \nas described in Section 5.5.1. Table 10 presents the results for the three version of the Field Reordering \noptimization. The frequency-based approach and the affinity approach reduced the cache miss rate by 19.3% \nand 16% respectively, while the exhaustive search reduced the miss rate by 25.6%. The exhaustive search \nis consistently better than both the frequency and affinity profiles. Table 10. Field Reordering Resultsa \n Original miss rate Reduction in cache miss rate Iteration Affinity Profile Frequency Profile boxsim \n7.70% 18% 10% 7% twolf 9.31% 42% 39% 38% perl 3.21% 17% -1% 13% Average reduction in miss rate 25.6% \n16% 19.3% lp_solve,gs, espresso Less than 0.01% reduction in cache miss rate a. The results were obtained \nusing the train input the input that is used to build the grammar and drive the search. Results for \nthe test input are not available yet because some of the layouts require considerable code modifications \nto unfold nested structures into the containing structure. However, previous work [3, 7] indi\u00adcates that \nthere is high correlation between the train and test inputs. When using the field pair heuristic (see \nSection 5.5.1), the search was completed in less than an hour. Although this may seem costly, both the \nFrequency and Affinity approaches require a time consuming effort, (i.e., instrumenting the program, \nbuilding the profile, etc.). Furthermore, Field Reordering is usually applied rarely, towards the end \nof the application development cycle.  5.6.2 CMA Results We have measured the working set size of our \nbenchmarks both with the train input, which was used to learn the allocation policy, and a test input, \nwhich was used to measure how well the alloca\u00adtion policy behaves on an unfamiliar input. Table 11 presents \nthe results for these two inputs. On average, our CMA reduces the working set size on test inputs by \n6.1%.We compared the iterative approach to a static approach in which the allocation policy uses a predefined \nnumber of memory regions. We built a Custom Memory Allocation that uses two regions: the hot region for \nfrequently accessed heap objects, and the cold region for rarely accessed objects. Using this allocator, \nonly perl and box\u00adsim showed a reduction in the working set size; and we obtained these results only \nafter we manually tuned the definition of hot objects for each program. We believe that because the iterative \npro\u00adcess adapts the number of regions to the program under consider\u00adation, it can build a better CMA \nthan the static approach. The allocation policy learning method requires more layouts to be evaluated \nthan the Field Reordering case (i.e., thousands vs. hun\u00addreds), and the learning is completed in a few \nhours. As in the case of Field Reordering, we do not consider this too costly. Previous work on CMA requires \nalmost the same effort [23], and CMA is usually applied once at the end of the development process. Table \n11 presents another interesting observation. Although the virtual memory was not identified as a bottleneck \nfor espresso,it shows considerable reduction in the working set size. Note that, eliminating a memory \nresource as a bottleneck does not imply that its memory behavior can not be improved; it just questions \nthe necessity of doing so. In fact, Table 9 shows that the CMA target set of espresso covers 71% of the \ntrace, indicating the high potential of the optimization to reduce the working set size. Table 11. Custom \nmemory allocation -results Benchmar ks Input Original Working Set Sizea New Working Set size Improv \nement Number of Regions Usedb espresso mlp4 8.27 7.97 3.65% 2 largest 19.81 16.51 16.6% boxsim N 2 T \n250 60.04 57.22 4.7% 8 N4 T 500 71.16 61.52 13.5% twolf spec 2000 test 17.35 14.14 18.5% 5 spec2000 ref \n114.7 112.8 1.6% perl recurse 25.9 24.4 6.0% 5 scrabble 35.4 30.5 7.9% gs intro 128.3 120.7 5.9% 10 report \n121.4 115.8 4.6% lp_solve etamacro 26.3 23.8 9.4% 6 fit1p 52.6 50.8 3.2% Average reduction in working \nset size (measured only on test input without espresso). 6.1% a. Average number of pages touched every \n100,000 references. b. The number of memory regions used is also the number of sets in the partition \nthat the decision tree found.   6. CONCLUSIONS We present a generalized process for profile-driven \ndata-layout optimization. Unlike prior work on data-layout optimization, the process is based on a profile-guided \niterative search in the data lay\u00adout space. Profile-feedback is used both to narrow the data layout search \nspace and to select among many candidate layouts. The pro\u00adcess unifies all existing data-layout optimizations \nand permits com\u00adposing multiple layout optimizations. In addition, it enables selection, application, \nand tuning of the more profitable optimiza\u00adtions for the program under consideration. We implement the \npro\u00adcess in an efficient framework. The framework efficiency stems from new techniques for fast memory \nsimulation: memoization\u00adbased profile analysis, miss-based profile compaction, and on\u00addemand cache simulation. \nThese techniques, together with the abil\u00adity to evaluate a data layout without program re-execution, \ndrive the iterative search. Using the framework, we instantiate two example optimizations: Field Reordering \nand Custom Memory Allocation. Our Field Reordering technique, based on an exhaustive search, outperforms \nexisting non-iterative techniques and reduces the cache miss rate by 25%. Our Custom Memory Allocation \nis based on a hill climbing search that classifies heap objects according to their runtime attributes \nrather than a pre-defined partition of program data objects. Without our new data-layout optimization \nframework and its efficient implementation, development of this allocator would not have been possible. \n ACKNOWLEDGMENTS We thank Brian Fields, Denis Gopan, Pacia Harper, and Martin Hirzel, as well as the \nanonymous referees for their comments on early drafts of the paper. Andy Edwards patiently answered our \nx86 questions. This work was supported in part by the National Science Foundation with grants CCR-0093275 \nand CCR-0103670, the Uni\u00adversity of Wisconsin Graduate School, and donations from IBM and Microsoft Research. \nShai Rubin was supported in part by a Fullbright Scholarship. The work was started when Shai Rubin was \nan intern at Microsoft Research. References [1] ALT,M., FERDINAND,C., MARTIN,F., AND WILHELM,R. Cache \nbehavior prediction by abstract interpretation. Lecture Notes in Computer Science 1145 (1996), 52 66. \n[2] BARRETT,D. A., AND ZORN, B. G. Using lifetime predictors to improve memory allocation performance. \nIn Proceedings of PLDI 93, Conference on Programming Languages Design and Implementation (Albuquerque, \nNM, June 1993), pp. 187 196. [3] CALDER,B., CHANDRA,K., JOHN,S., AND AUSTIN, T. Cache\u00adconscious data \nplacement. In Proceedings of ASPLOS 98, Conference on Architectural Support for Programming Lan\u00adguages \nand Operating Systems (San Jose CA, 1998). [4] CARR,S., MCKINLEY,K. S., AND TSENG, C.-W. Compiler optimizations \nfor improving data locality. In Proceedings of ASPLOS 94, Conference on Architectural Support for Pro\u00adgramming \nLanguages and Operating Systems (Boston, MA, Oct. 1994), pp. 252 262. [5] CHATTERJEE,S., PARKER,E., HANLON,P. \nJ., AND LEBECK, A. R. Exact analysis of the cache behavior of nested loops. In Proceedings of PLDI 01, \nConference on Programming Lan\u00adguages Design and Implementation (Snowbird, UT, June 2001), pp. 286 297. \n[6] CHILIMBI, T. M. Efficient representations and abstractions for quantifying and exploiting data reference \nlocality. In Proceed\u00adings of PLDI 01, Conference on Programming Languages Design and Implementation (Snowbird, \nUT, June 2001), pp. 191 202. [7] CHILIMBI,T. M., DAVIDSON,B., AND LARUS, J. R. Cache\u00adconscious structure \ndefinition. In Proceedings of PLDI 99, Conference on Programming Languages Design and Imple\u00admentation \n(Atlanta, GA, May 1999), pp. 13 24. [8] CHILIMBI,T. M., HILL,M. D., AND LARUS, J. R. Cache-con\u00adscious \nstructure layout. In Proceedings of PLDI 99, Confer\u00adence on Programming Languages Design and Implementation \n(Atlanta, GA, May 1999), pp. 1 12. [9] CHILIMBI,T. M., AND LARUS, J. R. Using generational gar\u00adbage collection \nto implement cache-conscious data placement. In Proceedings of the International Symposium on Memory \nManagement (ISMM-98), vol. 34, 3 of ACM SIGPLAN Notices, pp. 37 48. [10] DENNING,P. J., AND SCHWARTZ, \nS. C. Properties of the work\u00ading set model. Communications of the ACM 15,3(Mar.1972), 191 198. [11] DOLBY,J., \nAND CHIEN, A. An automatic object inlining opti\u00admization and its evaluation. In Proceedings of PLDI 00, \nCon\u00adference on Programming Languages Design and Implementation (Vancuver, Canada, June 2000), pp. 345 \n357. [12] DUTT, S. New faster kernighan-lin-type graph-partitioning algorithms. In Proceedings of the \nIEEE/ACM International Conference on Computer-Aided Design (Santa Clara, CA, Nov. 1993), M. Lightner, \nEd., IEEE Computer Society Press, pp. 370 377. [13] FERDINAND,C., AND WILHELM, R. Fast and efficient \ncache behavior prediction. [14] GANNON,D., JALBY,W., AND GALLIVAN,K.Strategies for cache and local memory \nmanagement by global programming transformation. Journal of Parallel and Distributed Comput\u00ading 5, 5 \n(Oct. 1988), 587 616. [15] KISTLER,T., AND FRANZ, M. Automated data-member layout of heap objects to \nimprove memory-hierarchy performance. ACM Transactions on Programming Languages and Systems 22, 3 (2000), \n490 505. [16] LAMARCA,A., AND LADNER, R. E. The influence of caches on the performance of heaps. ACM \nJournal of Experimental Algorithms 1 (1996), 4. [17] MITCHELL,T. M. Machine learning. McGraw Hill, New \nYork, US, 1996. [18] PATTERSON,D.A., AND HENNESSY,J.L. Computer Architec\u00adture: A Quantitative Approach. \nMorgan Kaufmann, Inc., San Mateo, CA, 1990. [19] PATTERSON,D. A., AND HENNESSY,J. L. Computer Organiza\u00adtion \nand Design: Hardware and Software Interface. Morgan Kaufmann Publishers, San Mateo, California, June \n1993. [20] PETRANK,E., AND RAWITZ, D. The hardness of cache con\u00adscious data placement. In Proceedings \nof POPL 02, Confer\u00adence on Principles of Programming Languages (Portland, OR, Jan. 2002). [21] RUBIN,S., \nBERNSTEIN,D., AND RODEH, M. Virtual cache line: A new technique to improve cache exploitation for recursive \ndata structures. Lecture Notes in Computer Science 1575 (1999), 259 273. [22] SAGIV,M., REPS,T., AND \nWILHELM, R. Parametric shape anal\u00adysis via 3-valued logic. In Proceedings of POPL 99 Confer\u00adence on Principles \nof Programming Languages (San Antonio, TX, Jan. 1999), pp. 105 118. [23] SEIDL,M. L., AND ZORN, B. G. \nSegregating heap objects by reference behavior and lifetime. In Proceedings of ASP\u00adLOS 98, Conference \non Architectural Support for Program\u00adming Languages and Operating Systems (San Jose, CA, Oct. 1998), \npp. 12 23. [24] TRUONG,D. N., BODIN,F., AND SEZNEC, A. Improving cache behavior of dynamically allocated \ndata structures. In Proceed\u00adings of PACT 98, Conference on Parallel Architectures and Compilation Techniques \n(Paris, France, Oct. 1998), pp. 322 329. [25] VAN DER WAL, R. Programmer s toolchest: Source-code pro\u00adfilers \nfor Win32. Dr. Dobb s Journal of Software Tools 23,3 (Mar. 1998), 78, 80, 82 88. [26] WOLF,M. E., AND \nLAM, M. S. A data locality optimizing algorithm. In Proceedings of PLDI 91, Conference on Pro\u00adgramming \nLanguages Design and Implementation (Toronto, Canada, June 1991), pp. 30 44.  \n\t\t\t", "proc_id": "503272", "abstract": "Data-layout optimizations rearrange fields within objects, objects within objects, and objects within the heap, with the goal of increasing spatial locality. While the importance of data-layout optimizations has been growing, their deployment has been limited, partly because they lack a unifying framework. We propose a parameterizable framework for data-layout optimization of general-purpose applications. Acknowledging that finding an optimal layout is not only NP-hard, but also poorly approximable, our framework finds a good layout by searching the space of possible layouts, with the help of profile feedback. The search process iteratively prototypes candidate data layouts, evaluating them by \"simulating\" the program on a representative trace of memory accesses. To make the search process practical, we develop space-reduction heuristics and optimize the expensive simulation via memoization. Equipped with this iterative approach, we can synthesize layouts that outperform existing non-iterative heuristics, tune application-specific memory allocators, as well as compose multiple data-layout optimizations.", "authors": [{"name": "Shai Rubin", "author_profile_id": "81100267810", "affiliation": "University of Wisconsin-Madison, Madison, WI", "person_id": "PP31034782", "email_address": "", "orcid_id": ""}, {"name": "Rastislav Bod&#237;k", "author_profile_id": "81100033082", "affiliation": "University of Wisconsin-Madison, Madison, WI", "person_id": "P239460", "email_address": "", "orcid_id": ""}, {"name": "Trishul Chilimbi", "author_profile_id": "81100578606", "affiliation": "Microsoft Research, Redmond, WA", "person_id": "PP14199837", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/503272.503287", "year": "2002", "article_id": "503287", "conference": "POPL", "title": "An efficient profile-analysis framework for data-layout optimizations", "url": "http://dl.acm.org/citation.cfm?id=503287"}