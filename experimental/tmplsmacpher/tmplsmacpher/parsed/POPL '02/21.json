{"article_publication_date": "01-01-2002", "fulltext": "\n Scalable Formal Design Methods for Asynchronous VLSI Rajit Manohar Computer Systems Laboratory Cornell \nUniversity http://vlsi.cornell.edu/~rajit/ ABSTRACT This lecture will provide an overview of the .eld \nof asyn\u00adchronous VLSI, and show how formal methods have played a critical role in the design of complex \nasynchronous systems. In particular, I will talk about program transformations and their application \nto asynchronous VLSI, as well as describe a simple language that I developed to describe these circuits \nand aid in their validation. Introduction It was intended that when Newspeak had been adopted once and \nfor all and Oldspeak forgotten, a heretical thought ... should be literally unthinkable, at least so \nfar as thought is dependent on words. ... Newspeak was designed not to ex\u00adtend but to diminish the range \nof thought ... George Orwell, 1984 The design of any modern VLSI system is a daunting task. The chip \ncomplexity in terms of the number of devices has been quadrupling every three years for the past three \ndecades. All the devices on a chip operate concurrently; therefore, the design of VLSI systems can be \nthought of as the design of highly concurrent computations. Clocked systems use a periodic global clock \nsignal to im\u00adplement barrier synchronization. These barriers are used to organize the concurrency on \na chip. In traditional clocked design, actions that modify variables must complete be\u00adfore a predetermined \nbarrier after which the variable can be read. There is no advantage obtained if some actions complete \nearly unless we can reduce the gap between succes\u00adsive barriers which requires that all actions must \ncomplete early, since the barriers are global and periodic. In asynchronous (clockless) systems, synchronization \nis im\u00adplemented by using message-passing primitives or local bar\u00adriers between concurrent components \nthat communicate with each other. Therefore, if some action .nishes early, it is pos- Permission to make \ndigital or hard copies of all or part of this work for personal or classroom use is granted without fee \nprovided that copies are not made or distributed for pro.t or commercial advantage and that copies bear \nthis notice and the full citation on the .rst page. To copy otherwise, to republish, to post on servers \nor to redistribute to lists, requires prior speci.c permission and/or a fee. POPL 02 Jan. 16 18, 2002 \nPortland, OR USA .2002 ACM ISBN 1-58113-450-9/02/01 ... $5.00 sible for the next action to begin without \nwaiting for a global barrier synchronization. The number of possible executions of an asynchronous com\u00adputation \ncan be much larger than the corresponding clocked system with the same functionality, since the absence \nof global barriers increases the number of possible interleavings of actions. This makes the problem \nof design veri.cation more challenging. However, the absence of a global syn\u00adchronization constraint \nmakes asynchronous circuits more amenable to a formal top-down design approach, thereby eliminating the \nneed for logical veri.cation because the cir\u00adcuits are guaranteed to be correct by construction. While \nformal top-down design can be used for clocked sys\u00adtems, the resulting circuits are less e.cient than \nthose pro\u00adduced by experienced designers. On the other hand, asyn\u00adchronous circuits produced with this \nformal approach are as e.cient (and sometimes even more e.cient) than those produced by experienced designers. \nThis is partly because the absence of the clock makes hand design of such circuits challenging. Thus, \nformal synthesis becomes the best way to explore the design space and manage design complexity. To date, \nformal methods have not proven realistic for build\u00ading large-scale devices and so practicing engineers \nshun them. Thus, our focus has been on a practical, scalable method that allows us to design extremely \ncomplex asynchronous computations using transformations whose correctness can be guaranteed by using \nlocal information. To achieve this, we are forced to restrict the class of circuits that we use. I will \ndiscuss some of the constraints we have introduced that we believe give us a scalable methodology without \nsacri.cing the e.ciency of the .nal implementation.  Program Transformations We describe asynchronous \ncomputations using CHP (Com\u00admunicating Hardware Processes), a language derived from Hoare s CSP written \nusing Dijkstra s guarded command no\u00adtation. At the this level of abstraction, we treat circuits as programs \n[6]. The introduction of hardware features that improve the .nal design (either in terms of energy requirements, \nperformance, or other metrics) is treated as an exercise in program trans\u00adformation. The original description \nis transformed into a di.erent description in a way that guarantees that the ob\u00adservable features of \nthe two computations are identical [7]. Once we are satis.ed with the concurrent description, it can \nbe transformed into an asynchronous circuit implementation by a mechanical procedure that only uses local \ninformation for each concurrent process. A convincing demonstration of this design methodology was provided \nby the .rst asynchronous microprocessor ever de\u00adsigned [8]. This project used program transformations \nto synthesize asynchronous circuits from a high-level speci.\u00adcation. While the methodology used for this \nproject was sound, several parts of the design required an analysis of the complete processor to guarantee \nthat the transforma\u00adtions applied were indeed correct. An important realization after this project was \nthat avoiding shared variables across concurrent components (except those introduced in an ex\u00adtremely \ndisciplined way) was important to avoid synchro\u00adnization issues that would dominate the design. If a \ntransformation depends on the syntax of the program, then it is very easy to check whether a particular \napplication of the transformation is valid, or to automate the applica\u00adtion of the transformation [1]. \nIdeally, one should be able to apply transformations to a concurrent component by exam\u00adining the component \nitself, not the entire program. There\u00adfore, we preclude the use of shared variables across concur\u00adrent \ncomponents except in the rarest of circumstances [5] to keep the design modular. The goal of the MiniMIPS \nproject was the design of a high\u00adperformance asynchronous processor with an industry-standard instruction \nset [9]. During this project, we realized that pipelining issues would dominate. In particular, perfor\u00admance \noptimization of asynchronous circuits requires that the number of bu.er stages along various paths is \ncarefully chosen to optimize throughput. Since the actual through\u00adput of various components was not known \nuntil very late in the design, we decided to adopt a design style that allowed us to introduce bu.er \nstages at arbitrary points in the com\u00admunication graph to optimize throughput. This required a global \ndesign constraint for every process to guarantee that introducing these bu.er stages would not a.ect \nthe correct\u00adness of the design. We call the resulting design slack elastic, because we can adjust the \nsynchronization slack on commu\u00adnication links without a.ecting correctness [4]. Slack elasticity allows \nus to introduce pipelining in an asyn\u00adchronous computation without a.ecting correctness. In fact, we \ncan obtain even more sophisticated program transforma\u00adtions where we examine the data.ow graph of a program \nand use projection to break up a sequential computation into concurrent parts [3].  Language Design \nand Tools We designed a very simple language (named CAST) to de\u00adscribe asynchronous circuits. While the \nlanguage itself is simple, it was su.ciently expressive for us to be able to de\u00adscribe a two million \ntransistor asynchronous processor that was correct on .rst silicon. The language has two primitive constructs: \ncreation of an instance of a type, and connection between two types. In ad\u00addition, it has meta language \nconstructs that support compo\u00adnent de.nitions, conditional instances, and loop constructs to permit the \ncreation of regular structures. A connection between two circuit nodes corresponds to the two nodes be\u00ading \nconnected by a wire. Because connections in the lan\u00adguage correspond to two components being permanently \naliased, CAST is essentially a linking language. In addition to supporting the description of circuit \ncompo\u00adnents, the language also contains primitive support for spec\u00adifying simple invariants. The presence \nof these invariants permits other tools to perform their checks more e.ciently. In some cases the invariants \nare necessary for a particular circuit implementation to be correct. Exploiting the design hierarchy \nis critical in the implementa\u00adtion of any tool. One of the recurring themes we encountered while we were \ndeveloping design tools is that the designer has a tremendous amount of information that is not ex\u00adpressed \nin the circuit description. Expressing some of this information in the circuit description not only allows \nthe tools to check more properties of the system, but allows them to do so more e.ciently [2]. I will \nconclude by discussing some open problems in the design of tools and languages for asynchronous systems. \n References [1] S. M. Burns and A. J. Martin. Syntax-directed translation of concurrent programs into \nself-timed circuits. In Proceedings of the Fifth MIT Conference on Advanced Research in VLSI, pages 35 \n50, 1988. [2] R. Manohar. A case for asynchronous computer architecture. In ISCA Workshop on Complexity-E.ective \nDesign, June 2001. [3] R. Manohar, T.-K. Lee, and A. J. Martin. Projection: A synthesis technique for \nconcurrent systems. In Proc. Fifth International Symposium on Asynchronous Circuits and Systems, pages \n125 134, April 1999. [4] R. Manohar and A. J. Martin. Slack elasticity in concurrent computing. In Proc. \nFourth International Conference on the Mathematics of Program Construction LNCS 1422, pages 272 285, \nJune 1998. [5] R. Manohar and A. J. Martin. Pipelined mutual exclusion and the design of an asynchronous \nmicroprocessor. Technical Report CSL-TR-2001-1017, Cornell University, 2001. [6] A. J. Martin. Compiling \ncommunicating processes into delay-insensitive VLSI circuits. Distributed Computing, 1(4):226 234, 1986. \n[7] A. J. Martin. Formal program transformations for VLSI circuit synthesis. In E. W. Dijkstra, editor, \nFormal Development of Programs and Proofs, UT Year of Programming Series, pages 59 80, 1989. [8] A. J. \nMartin, S. M. Burns, T. K. Lee, D. Borkovic, and P. J. Hazewindus. The design of an asynchronous microprocessor. \nIn Proc. Decennial Caltech Conference on VLSI, pages 351 373, 1989. [9] A. J. Martin, A. Lines, R. Manohar, \nM. Nystrom, P. Penzes, R. Southworth, U. Cummings, and T.-K. Lee. The design of an asynchronous MIPS \nR3000. In Proceedings of the 17th Conference on Advanced Research in VLSI, pages 164 181, 1997.  \n\t\t\t", "proc_id": "503272", "abstract": "This lecture will provide an overview of the field of asynchronous VLSI, and show how formal methods have played a critical role in the design of complex asynchronous systems. In particular, I will talk about program transformations and their application to asynchronous VLSI, as well as describe a simple language that I developed to describe these circuits and aid in their validation.", "authors": [{"name": "Rajit Manohar", "author_profile_id": "81100613995", "affiliation": "Cornell University", "person_id": "P238113", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/503272.503295", "year": "2002", "article_id": "503295", "conference": "POPL", "title": "Scalable formal design methods for asynchronous VLSI", "url": "http://dl.acm.org/citation.cfm?id=503295"}