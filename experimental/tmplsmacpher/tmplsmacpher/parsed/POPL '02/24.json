{"article_publication_date": "01-01-2002", "fulltext": "\n Composing Data.ow Analyses and Transformations Sorin Lerner David Grove Craig Chambers Univ. of Washington \nIBM T.J. Watson Research Center Univ. of Washington lerns@cs.washington.edu groved@us.ibm.com chambers@cs.washington.edu \n Abstract Data.ow analyses can have mutually bene.cial interactions. Previous e.orts to exploit these \ninteractions have either (1) iteratively performed each individual analysis until no further improvements \nare discovered or (2) developed super\u00adanalyses that manually combine conceptually separate anal\u00adyses. \nWe have devised a new approach that allows anal\u00adyses to be de.ned independently while still enabling \nthem to be combined automatically and pro.tably. Our approach avoids the loss of precision associated \nwith iterating indi\u00advidual analyses and the implementation di.culties of man\u00adually writing a super-analysis. \nThe key to our approach is a novel method of implicit communication between the individual components \nof a super-analysis based on graph transformations. In this paper, we precisely de.ne our ap\u00adproach; \nwe demonstrate that it is sound and it terminates; .nally we give experimental results showing that in \npractice (1) our framework produces results at least as precise as iter\u00adating the individual analyses \nwhile compiling at least 5 times faster, and (2) our framework achieves the same precision as a manually \nwritten super-analysis while incurring a compile\u00adtime overhead of less than 20%.  1. INTRODUCTION Data.ow \nanalyses can interact in mutually bene.cial ways, with the solution to one analysis providing information \nthat improves the solution of another, and vice versa. A classic example is constant propagation and \nunreachable code elim\u00adination: performing constant propagation and folding may replace branch predicates \nwith constant boolean values, en\u00adabling more code to be identi.ed as unreachable; conversely, eliminating \nunreachable code can remove non-constant as\u00adsignments to variables thus improving the precision of con\u00adstant \npropagation. Many other combinations of data.ow analyses exhibit similar mutually bene.cial interactions. \nThe possibility of mutually bene.cial interactions between analyses is one source of the ubiquitous phase \nordering prob\u00adlem in optimizing compiler design. If two or more analyses Permission to make digital or \nhard copies of all or part of this work for personal or classroom use is granted without fee provided \nthat copies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. POPL 02, Jan. 16-18, 2002 Portland, OR USA \n&#38;#169; 2002 ACM ISBN 1-58113-450-9/02/01...$5.00 are mutually bene.cial, then any ordering of the \nanalyses in which each is run only once may yield sub-optimal re\u00adsults. The most common partial solution \nused today is to selectively repeat analyses in carefully tuned sequences that strive to enable most \nof the mutually bene.cial in\u00adteractions without performing too much useless work. At high optimization \nlevels, some compilers even iteratively ap\u00adply a sequence of analyses until none of the analysis results \nchange. Unfortunately, in the presence of loops, even this iterative application of analyses can yield \nsolutions that are strictly worse than a combined super-analysis that simulta\u00adneously performs all the \nanalyses. When analyzing a loop, optimistic initial assumptions must be made simultaneously for all mutually \nbene.cial analyses to reach the best solu\u00adtion; performing the analyses separately in e.ect makes pes\u00adsimistic \nassumptions about the solutions of all other analy\u00adses, from which it is not possible to recover simply \nby iter\u00adating the separate analyses. By solving all problems simultaneously, super-analyses avoid the \nphase ordering problem (there is only one phase). However, all previous de.nitions of super-analyses \nhave re\u00adquired designing and implementing special versions of the analyses with explicit code to exploit \nthe bene.cial interac\u00adtions. For example, each of Wegman and Zadeck s condi\u00adtional constant propagation \nalgorithms [29, 30] is a special\u00adpurpose monolithic super-analysis that simultaneously per\u00adforms constant \npropagation and unreachable code elimina\u00adtion. Click and Cooper [9] provide a lattice-theoretic ex\u00adplanation \nof conditional constant propagation with special .ow functions de.ned over the composed domain. Pioli \nand Hind [25] developed a monolithic analysis that com\u00adbines constant propagation and pointer analysis \nusing spe\u00adcial combined .ow functions. Chambers and Ungar manu\u00adally combined class analysis, splitting, \nand inlining [7]. In all these cases, the analyses had to be combined manually in order for them to interact \nin mutually bene.cial ways. In fact, Cousot and Cousot discussed product domains in ab\u00adstract interpretation, \nand proved that special .ow functions need to be used in order for the combination to produce results \nbetter than the analyses performed separately [11]. This is unfortunate, because it seems to demonstrate \nthat it is not possible to simultaneously write data.ow analy\u00adses in a modular, reusable, replaceable \nfashion and achieve the best solutions for analyses that have mutually bene.cial interactions. We present \nan approach for de.ning data.ow analyses in a modular way, while also allowing analyses to be automat\u00adically \ncombined and interact in mutually bene.cial ways. This is achieved by changing the way that optimizations \nare speci.ed. Traditionally, an optimization is de.ned in two separate parts: (1) an analysis which produces \ndata.ow in\u00adformation and (2) rules for transforming the program repre\u00adsentation once the analysis has \nbeen solved. Merging these two speci.cations into one allows our framework to auto\u00admatically process \nanalyses in novel ways, and in particular it allows our framework to combine modular analyses prof\u00aditably. \nThe two speci.cations are combined by extending the de.nition of .ow functions: whereas traditional .ow \nfunc\u00adtions only return data.ow values, our .ow functions can also return a sub-graph with which to replace \nthe current state\u00adment. Such replacement graphs are used by our framework in two ways: Replacement graphs \nare used to compute the data.ow information at the program point after the current state\u00adment. This is \nachieved by recursively analyzing the replacement graph in place of the original statement: the input \nedges of the replacement graph are initial\u00adized with the data.ow values .owing into the original statement, \nand then iterative data.ow analysis is per\u00adformed on the replacement graph. When this recursive analysis \nreaches a .xed point, the values on the output edges of the replacement graph are propagated to the program \npoint after the original statement. Once this is done, the replacement graph can be thrown away; the \noriginal statement remains unchanged. If the original statement is analyzed again (with more conservative \ninputs), the .ow function can choose another (more conservative) graph transformation, or no transforma\u00adtion \nat all. Thus, during analysis, graph replacements are used as a convenient way of specifying what might \notherwise be a complicated .ow function.  Replacement graphs indicate what .nal transforma\u00adtions the \nanalysis wants to do. Once a sound .xed point has been reached, the last set of transformations selected \nduring analysis can be applied, yielding an op\u00adtimized program.  Our new method for specifying optimizations \nimposes no extra design e.ort, since the writer of an analysis needs to specify graph transformations \nanyway to perform changes to the intermediate representation. However, by changing the way in which optimizations \nare speci.ed, our framework can automatically process analyses in novel ways: Because .ow functions \nare allowed to return graph re\u00adplacements, our framework can automatically simulate transformations during \nanalysis, and therefore reach a better solution. For example, using simple and straight\u00adforward .ow functions \nde.ned in our framework, an analysis writer can achieve the same e.ect as Wegman and Zadeck s more complicated \nconditional constant propagation algorithms [29, 30] (as will be shown in section 2).  Our way of de.ning \n.ow functions allows modular anal\u00adyses to be automatically combined and achieve mutu\u00adally bene.cial interactions. \nWhen one component anal\u00adysis of a super-analysis selects a transformation, our framework recursively \nanalyzes the replacement graph using the super-analysis, not just the component anal\u00adysis that selected \nthe transformation. As a result, all other analyses immediately see the transformation, and can bene.t \nfrom it. This key insight allows analyses  that are composed in our framework to implicitly com\u00admunicate \nthrough graph transformations. This method of communication through graph trans\u00adformations is natural \nbecause it is in fact the way that analyses communicate when they are run in sequence: one analysis makes \nchanges to the program represen\u00adtation that later analyses observe. However, analyses that are automatically \ncomposed into a super-analysis cannot interact in this way if the transformations are only considered \nafter the super-analysis .nishes. Our method of automatically simulating transformations during analysis \nallows individual components of a super\u00adanalysis to communicate in the same natural and mod\u00adular way \nthat sequentially executed analyses do. The key contributions of this paper can therefore be sum\u00admarized \nas follows: We introduce a new technique for implicit communi\u00adcation between component analyses based \non graph transformations. This allows analyses to be written modularly, while still getting mutually \nbene.cial re\u00adsults when they are composed. Section 2 outlines the key ideas of our approach by showing \nhow several op\u00adtimizations can be de.ned in our framework.  Using abstract interpretation [10], we formalize \nthe use of graph transformations as a method of communica\u00adtion between analyses that are combined together. \nIn particular, we show in sections 3 through 6 that our combined super-analysis terminates, is sound \nif the in\u00addividual analyses are sound, and under a certain mono\u00adtonicity condition is guaranteed to produce \nno worse results than running arbitrarily iterated sequences of the individual analyses.  We have implemented \nour framework in the Vortex compiler [13] and more recently in the Whirlwind com\u00adpiler. Section 7 provides \nexperimental results showing that our framework can combine modular analyses au\u00adtomatically and pro.tably \nwith low compile-time over\u00adhead.  2. OVERVIEW OF OUR APPROACH This section highlights the key ideas \nof our approach by sketching how several optimizations can be de.ned in our framework and explaining \nhow they work on a few examples. We .rst show how a single analysis and its transformations can be combined \ninto an integrated analysis, and then we show how several integrated analyses can be automatically combined \ninto a single super-analysis. 2.1 Integrating Analysis and Transformation Imagine that we have a compiler \nthat uses a simple control .ow graph (CFG) intermediate representation. Flow func\u00adtions usually take \nas input the analysis information com\u00adputed at the program point before the statement being an\u00adalyzed \nand return the information to propagate to the pro\u00adgram point after the statement. The key novelty in \nour framework is that .ow functions also have the option of re\u00adturning a (possibly empty) sub-CFG with \nwhich to replace the current statement. To express this in our examples, we assume that .ow functions \nreturn something akin to an ML datatype with two constructors: the PROPAGATE con\u00adstructor, which speci.es \nsome data.ow value to propagate, and the REPLACE constructor, which speci.es a replace\u00adment graph for \nthe current statement. As an initial example, we de.ne a constant propagation and folding pass by giving \na few of the key .ow functions. The information propagated by this analysis consists of maps, denoted \nby O, that associate variables in the program text to either a constant, the symbol . (which indicates \nnon\u00adconstant), or the symbol . (which indicates no informa\u00adtion).1 The .rst .ow function for constant \npropagation handles statements that assign some constant k to some variable x: Fconst-prop[x := k]O = \nPROPAGATE (O[x .. k]) This .ow function says that to analyze statements of this form, the analysis should \nsimply propagate an updated ver\u00adsion of its input map that associates x with k (but is other\u00adwise unchanged). \nNext, consider the .ow function for binary arithmetic operations: Fconst-prop[x := y op z]O = let t := \nO[y] op O[z] in . if constant(t) then REPLACE ([x := t]) else PROPAGATE (O[x .. t]) This .ow function \n.rst computes the new abstract value t for x using the . op operator, which is the standard extension \nof op to . and .: . if a = .V b = . { a opb =. if \u00ac(a = .V b = .) 1 (a = .V b = .) . {a op b otherwise \n. If t is a constant, then the .ow function performs constant folding by replacing the current statement \n(x := yop z) with a sub-CFG containing a single statement (x := t) that assigns x the value computed \nby the constant folded op\u00aderation. If t is not constant (either . or .) then the .ow function simply \npropagates its input map after updating the binding for x. Finally, we de.ne a .ow function for conditional \nbranches: Fconst-prop[if b then goto L1 else goto L2 end]O = if constant(O[b]) then if O[b]= true then \nREPLACE ([goto L1 ]) else REPLACE ([goto L2 ]) else PROPAGATE (O) This .ow function either optimizes \na constant conditional branch by replacing it with a direct jump to the appropriate target, or simply \npropagates its input map unchanged to both targets if the branch condition is not constant. In all cases, \nwhen a REPLACE action is selected, the re\u00adplacement graph is analyzed, and the result of the recursive \nThroughout the paper we use the abstract interpretation convention that . represents no behaviors of \nthe program and T represents all possible behaviors. Thus, opposite to the data.ow analysis literature, \n. is the most optimistic information, and T is the most conservative information. analysis is propagated \nto the program point after the re\u00adplaced statement. For example, when Fconst-prop returns REPLACE ([x \n:= t]), the x := t statement is automati\u00adcally analyzed in place of the original statement, yielding \na map that associates x with the constant t. Although return\u00ading PROPAGATE (O[x . t]) in this case would \nproduce the same data.ow value, it would not specify the constant folding transformation. As another \nexample, if a constant branch is replaced with a direct jump, the framework au\u00adtomatically analyzes the \ndirect jump in place of the condi\u00adtional, and in doing so simulates the removal of the unreach\u00adable branch.2 \nWhile in the middle of an optimistic iterative analysis of a statement in a loop, the REPLACE action \nis only simulated, with the replacement graph recursively ana\u00adlyzed but otherwise unused. Only after \nthe analysis reaches a sound .xed point is the original CFG modi.ed destruc\u00adtively. Consider applying \nthe integrated constant propagation and folding optimization to the following simple program:3 x := 10; \nwhile (...) { if(x== 10){ DoSomething(); } else { DoSomethingElse(); x:= x+1; } } y := x; As it enters \nthe while loop, the analysis makes the op\u00adtimistic assumption that x contains the constant 10. As a result, \nthe .ow function for the if statement chooses to re\u00adplace the conditional by a jump to the true branch, \nimplicitly deleting the false branch as dead code. However, this trans\u00adformation is not actually applied \nyet, since further iteration might invalidate the inputs to the .ow function. Instead, the transformation \nis only simulated, producing the information that x holds the value 10 at the end of the while loop. \nThis matches the optimistic assumption made when entering the loop, and therefore a .xed point is reached. \nThe most re\u00adcently selected transformations can now be applied, which results in the following optimized \ncode: x := 10; while (...) { DoSomething(); } y := 10; The conditional constant propagation algorithms \nof Weg\u00adman and Zadeck [29, 30] would produce the same optimized code as above. However, their algorithm \nmanually simulates the e.ects of an optimization during analysis, whereas our framework can do this work \nautomatically. Now consider what happens when the integrated analysis is applied to a very similar program: \n2 Unreachable code elimination can either be built into the framework, as in Vortex, or done by a modular \npass composed with all other analyses, as in Whirlwind. In this section, we assume the framework provides \nunreachable code elimination because it makes the examples easier to follow. 3 For clarity, we use structured \nconstructs such as if and while instead of gotos. However, the underlying .ow functions are still evaluated \nover the nodes of the CFG. x := 10; while (...) { if (x==10){ DoSomething(); x := x -1; } else { DoSomethingElse(); \nx := x + 1; } } y := x; Again, as it enters the while loop, the analysis makes the optimistic assumption \nthat x contains the constant 10. It also simulates the replacement of the if statement with its true \nsub-statement. However, because of the decrement of x, the analysis now associates x with 9 at the end \nof the loop. When the join operations are applied at the loop head to determine whether or not a .xed \npoint has been reached, the framework discovers that its initial assumption was unsound, and it is forced \nto re-analyze the loop with the more conservative assumption that x is . (10 . 9= .). The second time \nthrough the loop, the if statement does not choose to do a transformation because x is not constant, \nand thus in the end no optimizations are performed. 2.2 Combining Multiple Integrated Analyses and Transformations \nThe next example illustrates how our approach allows multiple modular analyses to communicate through \ngraph replacements when they are automatically combined into a super-analysis. As a result of the communication \nthrough graph replacements, the composed super-analysis is able to exploit mutually bene.cial interactions \neven though the in\u00addividual analyses were written separately. We .rst de.ne two more optimizations, class \nanalysis and inlining. For class analysis (which maps each variable to the set of classes of which values \nin the variable might be instances), we pro\u00advide .ow functions for new statements, message send state\u00adments \n(virtual function calls), and instance-of tests. These .ow functions use two helper functions: subclasses, \nwhich returns the subclasses of a given class, and method lookup, which returns the function that results \nfrom doing a method lookup on a given class and a message id. Fclass-analysis[x := new C]O = PROPAGATE \n(O[x .. {C}]) Fclass-analysis[x := send y.ID(z1 ,..., zn )]O = let methods = . method lookup(c, ID) in \nc..[y] if methods = {F } then REPLACE ([x := F(y, z1 ,..., zn )]) else PROPAGATE (O[x .. .]) Fclass-analysis[x \n:= y instanceof C]O = if O[y] . subclasses(C) then REPLACE ([x := true]) else if O[y] . subclasses(C)= \n. then REPLACE ([x := false]) else PROPAGATE (O[x .. {Bool}]) The key .ow function for the inlining optimization \nphase is shown below, where should inline is an inlining heuristic that determines if a particular function \nshould be inlined, and subst formals is used to substitute the formals and the result in the body of \nthe inlined function:4 Finlining [x := F(y1 ,..., yn )]O = if should inline(F ) then let G = body(F ) \nin let G. = subst formals(G,x, y1,...,yn) in REPLACE ([G.]) else PROPAGATE (O) Now imagine that our framework \nis used to automatically combine these three modularly de.ned analyses (constant propagation, class analysis, \nand inlining) into a single super\u00adanalysis. The information propagated by the super-analysis is the tuple \nof the information propagated by the individual analyses, and the .ow function for a particular statement \nis a combination of the .ow functions of the individual analy\u00adses. The combined .ow function performs \neach of the indi\u00advidual .ow functions, accumulating the individual analysis information to propagate. \nIf any individual analysis selects a transformation action, then that transformation action is selected \nby the composed .ow function, causing the whole super-analysis to be applied to the replacement graph, \nin lieu of the original statement. On the other hand, if all individual analyses select propagation actions, \nthen the overall action of the composed .ow function is propagation of the tuple of the individual analysis \ninformations. The separate indi\u00advidual analyses interact through transformations: when one analysis selects \na transformation action, all the other analy\u00adses are applied to the replacement graph, thereby bene.tting \nfrom the simpli.cations of the program representation even if they cannot independently justify the optimization. \nConsider applying this super-analysis to the following sam\u00adple program, where C and D are unrelated subclasses \nof the A class: decl x:A; x := new C; while (...) { S1: decl b: Bool; b := x instanceof C; S2: if(b){ \nx := send x.foo(); } else { x := new D; } S3: } class A { method foo():A { return new A; } }; class C \nextends A { method foo():A { return self; } }; class D extends A { }; The composed analysis function \nof the .rst assignment statement selects a propagation action, as all the individ\u00adual analysis functions \nselect propagation actions. On the .rst pass through the while loop, optimistic iterative anal\u00adysis will \ncompute at label S1 the 3-tuple of information 4 Note that the inlining optimization is a pure transformation, \nand all of its .ow functions ignore the input data.ow value. ([x .. .], [x .. {C}], .).5 The composed \nanalysis of the instanceof statement will select the transformation action replacing the computation \nwith b := true, since class anal\u00adysis elects to fold the instanceof test. However, the control .ow graph \nis not modi.ed, since the information on entry to the .ow function is only tentative and may be invalidated \nby later iterative approximation. Instead, the replacement graph is analyzed recursively, yielding a \ncombined propaga\u00adtion action that yields the tuple ([x .. .,b .. true], [x .. {C},b .. {Bool}], .) at \nlabel S2. Analysis proceeds to the if statement, where the constant propagation .ow func\u00adtion selects \na transformation action replacing the conditional branch with a direct jump to the true sub-statement \n(im\u00adplicitly deleting the false sub-statement as dead code). As a result, analysis now proceeds to the \ntrue sub-statement, where the .ow function for class analysis selects a transfor\u00admation action replacing \nthe message send with a direct pro\u00adcedure call to the C::foo procedure. This replacement sub\u00adstatement \nis analyzed, at which time the call is replaced with inlined code, yielding the statement x := x. Recursive \nanal\u00adysis of this statement doesn t spawn any additional transfor\u00admation actions, .nally propagating \ndata.ow information to label S3 of ([x .. .,b .. true], [x .. {C},b .. {Bool}], .). After dropping the \nbindings for out of scope variables (b), it\u00aderative analysis detects that a .xed point has been reached, \nat which point the most recently selected transformations are applied, yielding the following optimized \ncode (a later dead-assignment elimination phase could clean up this code further): decl x:A; x := new \nC; while (...) { S1: decl b: Bool; b := true; S2: x:=x; S3: } This optimized version is sound; it has \nthe same behavior as the original code. But no single optimization phase alone, nor arbitrarily iterated \nsequences of separate optimization phases, could have produced this code. Class analysis is the only \noptimization that can fold the instanceof test, but it requires constant propagation to fold the if statement \nand thereby delete the other assignment to x, and it requires inlining to expose the implementation of \nthe foo method to the (intraprocedural) class analysis. If the analyses were run separately, no optimizations \nat all could be performed. 2.3 Uses in Practice Our framework has been implemented in the Vortex com\u00adpiler, \nwhich uses a standard CFG representation, and more recently in the Whirlwind compiler, which uses a data.ow \ngraph (DFG) representation augmented with control-edges. Both implementations support forward and backward \ndata\u00ad.ow analyses, although the analyses that are composed into a super-analysis must all have the same \ndirectionality. The Vortex framework has been used to de.ne a number of inter\u00adesting analyses and optimizations, \nincluding constant propa\u00adgation and folding, symbolic assertion propagation and fold\u00ading, copy propagation, \ncommon sub-expression elimination 5 Recall that inlining is a pure transformation, and does not propagate \nany meaningful data.ow information. We therefore arbitrarily choose T as the third element of the tuple \nto denote the inlining data.ow information. (CSE), must-point-to analysis, redundant load and store elimination, \ndead assignment elimination, dead store elim\u00adination, class analysis, splitting [7], and inlining. However, \nsome optimizations over the CFG, such as loop-invariant code motion and instruction scheduling, do not \ncurrently bene.t from the special features of our framework because their optimizations cannot be expressed \nas local graph trans\u00adformations. We are currently looking at ways of relaxing the locality of graph replacements, \nas is explained in an accom\u00adpanying technical report [22]. Nevertheless, even with the local graph replacement \nrestriction, compiler writers are no worse o. using our framework for implementing such anal\u00adyses than \nthey would be using any other extant data.ow analysis framework: our framework supports writing a pure \nanalysis pass (i.e., one that makes no transformations) that can be followed by a separate transformation \npass, and in fact this is how loop-invariant code motion has been imple\u00admented using our framework in \nthe Vortex compiler.  3. PRELIMINARIES Now that we have outlined the key ideas of our approach, we proceed \nto the formalization. In this section we de.ne basic notation and the abstract intermediate representation \nthat we assume throughout the rest of the paper. Section 4 reviews the well-know de.nition of a single \nanalysis followed by transformations, and serves as a foundation for the for\u00admalization of the novel \nparts of our framework in sections 5 and 6. 3.1 Notation If A is a set, then A= is the set .i>0 Ai , \nwhere Ak = {(a1 ,...,ak)|ai . A}. We denote the ith projection of a tuple x =(x1,...,xk ) by x[i] . xi. \nGiven a function f : - . A . B, we extend f to work over tuples by de.ning f : . A= . B= as -f ((x1,...,xk \n)) . (f (x1),...,f (xk )). We also extend f to work over maps by de.ning f :(O . A) . (O . B) as f (m) \n. Ao.f (m(o)). We extend a binary relation R < 2D\u00d7D over D to tuples .-- . by de.ning the R relation \nby: R ((x1 ,...,xk), (y1,...,yk)) i. R(x1,y1) 1 ... 1 R(xk,yk). Finally, we extend a binary relation \nR < 2D\u00d7D to maps by de.ning the R relation as: R(m1,m2) i. for all elements o in the domain of both m1 \n and m2, it is the case that R(m1(o),m2(o)). To make the equations clearer, we drop the tilde and arrow \nannotations on binary relations when they are clear from context. 3.2 Intermediate Representation We \nassume that programs are represented by directed multigraphs with nodes representing computations that \npro\u00adduce values on their output edges on the basis of the values consumed from their input edges. The \nexact type of nodes, edges, values, and the relative sparseness/denseness of a par\u00adticular program representation \nare orthogonal to the main ideas of this paper. Therefore we suppress them by using an abstract intermediate \nrepresentation (IR) in which compu\u00adtations are represented by graphs. For example, if the com\u00adpiler uses \na CFG representation, then nodes are program statements, and edges are control-.ow edges. If instead \nthe compiler uses a DFG representation, then nodes are primi\u00adtive computations, and edges are data.ow \nedges. A graph in our IR is a tuple g =(N, E, In, Out, InEdges, OutEdges) where N < Nodes is a set of \nnodes (with N odes being a prede.ned in.nite set), E < Edges is a set of edges (with Edges being a prede.ned \nin.nite set), In : N . E= speci.es the input edges for a node, Out : N . E= spec\u00adi.es the output edges \nfor a node, InEdges . E= speci.es the input edges of the graph, and OutEdges . E= speci.es the output \nedges of the graph. Each node n in N repre\u00adsents a primitive computation mapping input edges In(n) to \noutput edges Out(n), while a graph analogously repre\u00adsents a computation from input edges InEdges to \noutput edges OutEdges. When necessary, we use subscripts to ex\u00adtract the components of a graph. For example, \nif g is a graph, then its nodes are Ng , its edges are Eg , and so on. Note that our de.nition of the \nintermediate representation uses ordered tuples to represent input and output edges, because unordered \nsets would not be su.cient to capture some useful representations. For example, the two control\u00ad.ow output \nedges of a branch node in a CFG are ordered: one leads to the true computation, and the other leads to \nthe false computation. Similarly, the two data.ow inputs to the minus node in a DFG are ordered.  4. \nA SINGLE ANALYSIS FOLLOWED BY TRANSFORMATIONS This section reviews the well-known lattice-theoretic for\u00admulation \nof data.ow analysis frameworks using abstract in\u00adterpretation [10]. It shows how we use this formulation \nto de.ne analyses and transformations over the abstract IR de\u00ad.ned in the previous section, and provides \nthe foundation for describing our approach in sections 5 and 6. 4.1 De.nition An analysis is a tuple \nA =(D, ., n, ., ., ., s, F ) where (D, ., n, ., ., .) is a complete lattice, s : Dc . D is the abstraction \nfunction, and F : Node \u00d7 D= . D= is the .ow function for nodes. The elements of D, the domain of the \nanalysis, are data.ow facts about edges in the IR (which would correspond to program points in a CFG \nrepresen\u00adtation). The .ow function F provides the interpretation of nodes: given a node and a tuple of \ninput data.ow values, one per incoming edge to the node, F produces a tuple of out\u00adput data.ow values, \none per outgoing edge from the node. Dc is the domain of a distinguished analysis, the concrete analysis \nC =(Dc, .c, nc, .c, .c, .c, id, Fc), which speci.es the concrete semantics of the program. For example, \none can de.ne C over a CFG representation using a collecting semantics, with the elements of Dc being \nsets of concrete stores. Alternatively, for a DFG representation that com\u00adputes over integer values, \nthe elements of Dc could be sets of integers. C is .xed throughout the paper, and we assume that Fc and \ns are continuous. The solution of an analysis A over a domain D is provided by the function SA : Graph \n\u00d7 D= . (Edges . D). Given a graph g and a tuple of abstract values for the input edges of g, SA returns \nthe .nal abstract value for each edge in g. This is done by initializing all edges in g to bottom, and \nthen applying the .ow functions of A until a .xed point is reached. A detailed de.nition of SA can be \nfound in appendix A.6 An Analysis followed by Transformations, or an AT\u00ad 6 Although the concrete solution \nfunction SC is usually not com\u00adputable, the mathematical de.nition of SC is still perfectly valid. Our \nframework does not evaluate SC ; we only use SC to formalize the soundness of analyses. analysis for \nshort, is a pair (A,R) where A =(D, ., n, ., ., ., s, F ) is an analysis, and R : Node\u00d7D= . Graph.{.} \nis a local replacement function. The local replacement func\u00adtion R speci.es how a node should be transformed \nafter the analysis has been solved. Given a node n and a tuple of elements of D representing the .nal \ndata.ow analysis solu\u00adtion for the input edges of n, R either returns a graph with which to replace n, \nor . to indicate that no transformation should be applied to this node. To be syntactically valid, a \nreplacement graph must have the same number of input and output edges as the node it replaces, and its \nnodes and edges must be unique (so that splicing a replacement graph into the enclosing graph does not \ncause con.icts). We denote by RFD the set of all replacement functions over the domain D, or in other \nwords RFD = Node \u00d7 D= . Graph .{.}. After analysis completes, the intermediate representation is transformed \nin a separate pass by a transformation func\u00adtion T : RFD \u00d7 Graph \u00d7 (Edges . D) . Graph. Given a replacement \nfunction R, a graph g, and the .nal data.ow analysis solution, T replaces each node in g with the graph \nreturned by R for that node, thus producing a new graph. A detailed de.nition of T can be found in appendix \nB. The e.ect of an analysis followed by transformations is therefore summarized as follows: given an \nanalysis A over domain D, a replacement function R, an initial graph g, and abstract values t . D= for \nthe input edges of g, the .nal graph that (A,R) produces is T (R, g,SA(g, t)). 4.2 Soundness We want \nthe graph produced by (A,R) to have the same concrete semantics as the original graph. This if formalized \nin the following de.nition of soundness of (A,R): Def 1. Let (A,R) be an AT-analysis with A =(Da, ., \nn, ., ., ., s, Fa). Let (g, tc,ta) . Graph \u00d7 Dc = \u00d7 Da = such that . -s (tc) . ta and let r = T (R, g,SA(g, \nta)). We say that (A,R) is sound i.: -----. -----. SC(r, tc)(OutEdgesr )= SC(g, tc)(OutEdgesg ) We de.ne \nhere two conditions that together are su.cient to show that an AT-analysis is sound. First, the analysis \nA in (A,R) must be locally sound according to the following de.nition: Def 2. We say that an analysis \nA =(Da, ., n, ., ., ., s, Fa) is locally sound i. it satis.es the following local sound\u00adness property: \n\u00d8(n, cs, ds) . Node \u00d7 Dc = \u00d7 Da= . -. . s (cs) . ds .-s (Fc(n, cs)) . Fa(n, ds) (1) If A is locally \nsound, then it is possible to show that A is sound, meaning that its solution correctly approximates \nthe solution of the concrete analysis C. This is formalized by the following de.nition and theorem, the \nlatter of which is proved in an accompanying technical report [22]. Def 3. We say that an analysis A \n=(Da, ., n, ., ., ., s, Fa) is sound i.: \u00d8(g, tc,ta) . Graph \u00d7 Dc = \u00d7 Da= . . -s (tc) . ta s(SC(g, tc)) \n. SA(g, ta) . Theorem 1. If an analysis A is locally sound then A is sound. Property (1) is su.cient \nfor proving Theorem 1. Moreover it is weaker than the local consistency property of Cousot and Cousot \n(property 6.5 in [10]), which is: \u00d8(n, cs, ds) . Node \u00d7 Dc = \u00d7 Da= . -.- . s (Fc(n, cs)) Fa(n, s (cs)) \nIndeed, the above property and the monotonicity of Fa im\u00adply property (1). We use the weaker condition \n(1) because in this way our formalization of soundness does not depend on the monotonicity of Fa. As \nshown in sections 5 and 6, the .ow function Fa is usually generated by our framework and reasoning about \nits monotonicity requires additional e.ort on the part of the analysis writer. By decoupling our sound\u00adness \nresult from the monotonicity of Fa, we can guarantee soundness even if Fa has not been shown to be monotonic.7 \nSecond, R must produce graph replacements that are semantics-preserving. This is formalized by requiring \nthat the replacement function R be locally sound according to the following de.nition: Def 4. We say \nthat a replacement function R in (A,R) is locally sound i. it satis.es the following local soundness \nproperty, where A =(Da, ., n,, ., ., s,Fa): \u00d8(n, ds,g) . Node \u00d7 Da = \u00d7 Graph. R(n, ds)= g . = -(2) . \n[\u00d8cs . Dc .s (cs) ds . -----. Fc(n, cs)= SC(g, cs)(OutEdgesg )] Property (2) requires that if R decides \nto replace a node n with a graph g on the basis of some analysis result ds, then for all possible input \ntuples of concrete values consistent with ds, it must be the case that n and g compute exactly the same \noutput tuple of concrete values. It is not required that n and g produce the same output for all possible \ninputs, just those consistent with ds. For example, if A determines that some input edge e to n will \nalways have a value between 1 and 100 then n and g are not required to produce the same output for any \ninput in which e is assigned a value outside of this range. We say that (A,R) is locally sound i. both \nA and R are locally sound. If(A,R) is locally sound, then it is possible to show that (A,R) is sound \naccording to de.nition 1, which means that the .nal graph produced by (A,R) has the same concrete behavior \nas the original graph. This is stated in the following theorem, which is proved in a technical report \n[22]. Theorem 2. If an AT-analysis (A,R) is locally sound, then (A,R) is sound. 4.3 Termination If the \nlattice has .nite height, then the termination of an analysis A =(D, ., n,, ., ., s, F ) is guaranteed \nfrom within SA, even if F is not monotonic: as iteration proceeds, SA forces the data.ow values to monotonically \nincrease by joining the next solution with the current solution at each step. If the lattice has in.nite \nheight, then the .ow function for loop header nodes can include widening operators [10] to guarantee \ntermination. We chose to enforce termination from within SA, instead of requiring F to be monotonic, \nfor the same reason we chose 7 Termination in the face of a non-monotonic .ow function is discussed in \nsection 4.3. the weaker soundness condition (1): .ow functions are gen\u00aderated by our framework, and proving \nthat they are mono\u00adtonic requires additional e.ort on the part of the analysis writer. By having termination \nand soundness be decoupled from the monotonicity of F , we allow analysis designers the option of not \nproving that F is monotonic. The drawback of not having F be monotonic is that the .xed point computed \nby SA is not necessarily a least .xed point anymore. As a result, the solution returned by SA is not \nguaranteed to be the most precise one.  5. INTEGRATING ANALYSIS AND TRANSFORMATION Now that we have \nde.ned a single analysis followed by some transformations, we proceed to formalizing how our framework \nintegrates an analysis with its transformations. 5.1 De.nition An Integrated Analysis is a tuple IA =(D, \n., n,, ., ., s, FR) where (D, ., n,, ., .) is a complete lattice, s : Dc . D is the abstraction function, \nand FR : Node \u00d7 D= . D= . Graph is a .ow-replacement function. The .ow\u00adreplacement function FR takes \na node and a tuple of input abstract values, one per incoming edge to the node, and re\u00adturns either a \ntuple of output abstract values, one per out\u00adgoing edge from the node, or a graph with which to replace \nthe node. An integrated analysis is an analysis which has been com\u00adbined with its transformations. The \n.ow replacement func\u00adtion can now return graph transformations that are taken into account during the \n.xed point computation, and used after the .xed point has been reached to make permanent transformations \nto the graph. The .ow functions de.ned in section 2 were in fact .ow-replacement functions. The PROPAGATE \ndatatype constructor corresponds to FR returning an element of D=, whereas the REPLACE con\u00adstructor corresponds \nto FR returning an element of Graph. The meaning of an integrated analysis is de.ned in terms of an associated \nAT-analysis, for which the behavior has already been de.ned in section 4.1. Given an integrated analysis \nIA =(D, ., n,, ., ., s,FR), we de.ne the asso\u00adciated AT-analysis ATIA as (A,R), with A =(D, ., n,, ., \n., s, F ), where F and R are derived from FR as follows: FR(n, ds) if FR(n, ds) . D= F (n, ds)= SolveSubGraphF \n(FR(n, ds), ds) otherwise ------. SolveSubGraphF (g, ds)= SA(g, ds)(OutEdgesg ) . if FR(n, ds) . D= \nR(n, ds)= SolveSubGraphR(FR(n, ds), ds) otherwise SolveSubGraphR(g, ds)= T (R, g, SA(g, ds)) The de.nition \nof F above shows how transformations are taken into account while the analysis is running. If FR returns \na tuple of data.ow values, then that tuple is imme\u00addiately returned. If, on the other hand, FR chooses \nto do a transformation, the replacement graph is recursively ana\u00adlyzed and the data.ow values computed \nfor the output edges of the graph are returned. The next time the same node gets analyzed, FR can choose \nanother graph transformation, or possibly no transformation at all. Transformations are only committed \nafter the analysis has reached a .nal sound so\u00adlution, as speci.ed by the de.nition of R. If at the .nal \ndata.ow solution, FR returns a tuple of data.ow values, then R returns ., indicating that the analysis \nhas chosen not to do a transformation. If, on the other hand, FR chooses a replacement graph, then R \nreturns this replacement graph after transformations have been applied to it recursively. Al\u00adthough the \nde.nition of R above reanalyzes recursive graph replacements, an e.cient implementation, such as the \nones in Vortex and Whirlwind, can cache the solution of the last replacement graph computed by FR for \neach node, so that the transformation pass need not recompute them. 5.2 Soundness An integrated analysis \nIA is sound if the associated AT\u00adanalysis ATIA is sound. We de.ne here conditions that are su.cient to \nshow that ATIA is sound, and therefore that IA is sound. Intuitively, we want the .ow-replacement function \nFR to satisfy condition (1) when it returns a tuple of data.ow values, and condition (2) when it returns \na re\u00adplacement graph. Formally, this amounts to having IA be locally sound according to the following \nde.nition: Def 5. We say that an integrated analysis IA =(D, ., n, , ., ., s, FR) is locally sound i. \nit satis.es the following two local soundness properties: \u00d8(n, cs, ds) . N ode \u00d7 D= c \u00d7 D= . F R(n, ds) \n. D= . (3) [-.s (cs) ds . -.s (Fc(n, cs)) F R(n, ds)] \u00d8(n, ds, g) . N ode \u00d7 D= \u00d7 Graph. F R(n, ds) = \ng . [\u00d8cs . D= c . -.s (cs) ds . (4) -----. Fc(n, cs)= SC(g, cs)(OutEdgesg )] Note that the .rst property \nis the same as (1) with Fa re\u00adplaced by FR, except for the additional antecedent FR(n, ds) . D=, and \nthe second property is the same as (2), with R replaced by FR. Theorem 3. If an integrated analysis IA \nis locally sound, then the associated AT-analysis ATIA is sound, and there\u00adfore IA is sound. Proving \nTheorem 3 involves showing that if FR satis.es properties (3) and (4), then F and R as de.ned in section \n5.1 satisfy properties (1) and (2) respectively. A proof is given in an accompanying technical report \n[22]. 5.3 Termination As in the case of an AT-analysis, the function SA forces the solution to monotonically \nincrease as iteration proceeds, even if the .ow function F is not monotonic. If the designer of the analysis \nputs in the e.ort to prove that F is mono\u00adtonic, then SA computes the least .xed point. Otherwise, the \nresult computed by SA is not necessarily a least .xed point, but it is nevertheless sound as long as \nproperties (3) and (4) hold. However, having the solution monotonically increase is no longer su.cient \nto ensure termination: it is now possible for the .ow functions to choose graph replacements that cause \nin.nite recursion of nested graph analysis. For example, an inlining optimization could choose to inline \na recursive func\u00adtion inde.nitely. To ensure termination, we require that the user s graph replacements \ndo not trigger such endless recur\u00adsive transformations. Graph replacements either obviously simplify \nthe program (such as deleting a node or replacing a complex node with several simpler ones), and thus \ncannot cause unbounded recursive graph replacements, or there are standard ways of avoiding endless recursive \ngraph transfor\u00admations (for instance, by marking selected nodes in the re\u00adplacement graph as non-replaceable). \nOur framework does not enforce an arbitrary .xed bound on recursive graph re\u00adplacements. Instead, we \nfeel that individual data.ow analy\u00adses will have their own most appropriate solution, which can be explicitly \nimplemented in the .ow-replacement function. This non-termination issue with our framework is already \npresent in any system that iteratively applies analyses and transformations. Such systems have either \nimposed some .xed bound on the number of iterations, or, as we do, re\u00adquire the analyses to avoid endless \ntransformations.  6. COMBINING MULTIPLE ANALYSES In this section, we de.ne how our framework automati\u00adcally \ncombines several modular analyses, while still allowing mutually bene.cial interactions. 6.1 De.nition \nThe Composition of k Integrated Analyses, or a Composed Analysis for short, is a tuple CA =(IA1 , IA2,..., \nIAk), where each IAi is an integrated analysis (Di, .i, ni, i, .i, .i,si,FRi). Here again, we de.ne the \nmeaning of a composed analysis in terms of an associated AT-analysis. Given a composed analysis CA =(IA1, \nIA2 ,..., IAk), we de.ne the associ\u00adated AT-analysis ATCA as (A,R), where A =(D, ., n,, ., ., s, F ). \nWe .rst de.ne the lattice (D, ., n,, ., .) of the composed analysis, then we de.ne the composed abstrac\u00adtion \nfunction s, the composed .ow function F and .nally the composed replacement function R. Composed lattice. \nThe lattice (D, ., n,, ., .) of the composed analysis is the product of the individual lattices, namely: \nDD1 \u00d7 D2 \u00d7 ... \u00d7 Dk . is de.ned by (a1 ,...,ak) . (b1,...,bk)(a1 .1 b1,...,ak .k bk) n is de.ned similarly \nto .  is de.ned by (a1 ,...,ak)(b1,...,bk ) a1 b1 1 ... 1 akk bk  . (.1, .2 ,..., .k) and . (.1, .2 \n,..., .k) Composed abstraction function. The abstraction func\u00adtion s : Dc . D is de.ned by s(c)(s1(c),...,sk(c)). \nComposed .ow function. Before de.ning F , we must .rst introduce two helper functions, c2s and s2c. The \n.rst function, c2s (which stands for composed to single ), is used to extract the data.ow values of an \nindividual analysis from the data.ow values of the composed analysis. Given an integer i, and an n-tuple \nof k-tuples, c2s returns an n-tuple whose elements are the ith entries of each k-tuple. Formally: c2s(i, \n(x1 ,...,xn)) (x1[i],...,xn[i]) For example, if ds . D= is a tuple of input values to a node in the composed \nanalysis, then c2s(i, ds) is the tuple of input values to that node for the ith component analysis. The \nsecond function, s2c (which stands for single to com\u00adposed ) has the exact opposite role as c2s: it combines \nthe data.ow values of individual analyses to form the data.ow values of the composed analysis. Formally, \nit is de.ned by s2c(x1 ,...,xk) ((x1[1],...,xk[1]),..., (x1[n],...,xk[n])) where each xi is an n-tuple. \nFor example, if x1,...,xk are n-tuples, each one being the result of a single analysis for the n output \nedges of a given node, then s2c(x1,...,xk) is the output tuple of the composed analysis for that node. \nAlso, note that c2s(i, s2c(x1 ,...,xk)) = xi. We are now ready to give the de.nition of F : F (n, ds)= \ns2c(res1, . . . , resk) where for each i . [1..k]: resi = lresi ni .i c2s(i, SolveSubGraphF (g, ds)) \ng.gsi fresi if fresi . Di = lresi = c2s(i, SolveSubGraphF (f resi, ds)) otherwise fresi = FRi(n, c2s(i, \nds)) gsi = Graph . . {fresj } j.[1..k].j8 =i and ------. SolveSubGraphF (g, ds)= SA(g, ds)(OutEdgesg \n) The above de.nition looks daunting but it is in fact quite simple. To compute the result resi of the \nith analysis, the composed .ow function .rst determines what the ith analy\u00adsis would do in isolation \nby evaluating FRi and storing the result in fresi. The next step is to determine the data.ow value lresi \nthat results from the selected action. lresi is ei\u00adther f resi if fresi is a tuple of data.ow values, \nor the result of a recursive analysis if fresi is a replacement graph. Fi\u00adnally, the expression for resi \ntakes into account not only the action of the ith analysis, through the lresi term, but also the actions \nof other analyses, through the second term. This second term of resi computes the result of the ith analysis \non all graph replacements (gsi) selected by other analyses. Because graph replacements are required to \nbe sound with respect to the concrete semantics, they are sound to apply for any analysis, not just the \none that selected them. This means that the result produced by the ith analysis on any graph replacement \nis sound given the current data.ow ap\u00adproximation. Doing a meet of these recursive results, each of which \nis sound, provides the most optimistic inference that can be soundly drawn. This last term in the de.nition \nof resi is important for two reasons. First, it allows analyses to communicate im\u00adplicitly through graph \nreplacements. If one analysis makes a transformation, then the chosen graph replacement will immediately \nbe seen by other analyses. Second, it ensures the precision result which we cover in section 6.4. Because \nall potential graph replacements are recursively analyzed, and the returned value is the most optimistic \ninference that can be drawn from these recursive results, we are guaran\u00adteed to get results which are \nat least as good as any inter\u00adleaving of the individual analyses. Although analyzing all potential graph \nreplacements is theoretically required to en\u00adsure this precision result, an implementation could choose \nto recursively analyze only a subset of the potential graph replacements. The Vortex and Whirlwind implementations \nin fact only analyze those graph replacements selected by the PICK function de.ned below. Composed replacement \nfunction. The de.nition of R relies on a cost function PICK :2Graph . Graph .{.} to select which graph \nreplacement to apply if more than one analysis selects a transformation. Although the composed .ow function \nrecursively analyses all graph replacements, only one of these graphs can actually be applied once the \nanalysis has reached .xed point. The PICK function is used to make this decision: given a set of graphs, \nPICK selects at most one of them to apply, which means that if PICK(gs)= g, then either g = ., or g . \ngs. R can now be de.ned as follows: . if PICK(gs)= . R(n, ds)= SolveSubGraphR(PICK(gs), ds) otherwise \nwhere gs = Graph . .{FRj (n, c2s(j, ds))} j.[1..k] SolveSubGraphR(g, ds)= T (R, g, SA(g, ds)) This de.nition \nof R is very similar to the one for an inte\u00adgrated analysis from section 5.1, except that here the PICK \nfunction selects which graph to apply from the set (gs) of all potential replacement graphs. If PICK \nselects no transfor\u00admation, then R does the same. If, however, PICK chooses a replacement graph, then \nthis replacement graph is returned after transformations have been applied to it recursively. 6.2 Soundness \nA composed analysis CA is sound if the associated AT\u00adanalysis ATCA is sound. We say that a composed analy\u00adsis \nCA =(IA1, IA2 ,..., IAk) is locally sound if each in\u00adtegrated analysis IAi is locally sound (according \nto de.ni\u00adtion 5). Theorem 4. If a composed analysis CA is locally sound, then the associated AT-analysis \nATCA is sound, and there\u00adfore CA is sound. Theorem 4 says that if each integrated analysis has been shown \nto be sound (by showing that each one is locally sound), then the composed analysis is sound. Proving \nThe\u00adorem 4 involves showing that if each FRi satis.es properties (3) and (4), then F and R as de.ned \nin section 6.1 satisfy properties (1) and (2) respectively. A proof is given in an accompanying technical \nreport [22]. 6.3 Termination Termination is handled in a similar way to the case of integrated analyses \nfrom section 5.3. The only di.erence is that the analysis designer must now show that the com\u00adposed analysis \ndoes not cause endless recursive graph re\u00adplacements. Even if each integrated analysis by itself does \nnot cause in.nite recursive analysis, the interaction between two analyses can. For example, two analyses \ncan oscillate back and forth, the .rst one optimizing a statement that the second one reverts back to \nthe original form. However, as long as the lattice has .nite height, our framework does guarantee that \nnon-termination will never be caused by in\u00ad.nite traversal of the lattice. 6.4 Precision of Composed \nAnalyses The soundness result from section 6.2 guarantees that the information computed by the composed \nanalysis correctly approximates the actual behavior of the program. It does not however say anything \nabout the precision of the com\u00adputed information. After all, if the composed .ow function always returned \n., it would still be sound (and in fact mono\u00adtonic). In this section we show that in addition to being \nsound, the composed analysis is at least as precise as any iterated sequence of the individual analyses. \nConsider running a set of analyses in sequence without re\u00adpeating any analysis. This sequence generates \nfor each edge in the original graph one data.ow value per analysis.8 The composition of these analyses \nalso computes one data.ow value per analysis per edge, except that the method for com\u00adputing the data.ow \nvalues is di.erent. Our precision result states that if the composed .ow function is monotonic, then \nfor any edge, the data.ow values computed by the composed analysis are at least as precise (in a lattice \ntheoretic sense) as the data.ow values computed by the analyses running in sequence. This guarantees \nthat the composition cannot do worse than running the analyses in sequence. In practice, however, the \ncomposition often does better, and an example of this was shown in section 2.2. Once the precision result \nfor analyses without iteration is proved, it is easy to gen\u00aderalize it for arbitrarily iterated sequences \nof analyses. We refer the reader to an accompanying technical report [22] for a formal statement and \na proof of the precision theorem. In order to guarantee the precision result, the analysis writer must \nshow that the composed .ow function is mono\u00adtonic. Even if each integrated analysis in the composition \nis monotonic in isolation, interaction through graph replace\u00adments can lead to a non-monotonic composed \n.ow func\u00adtion. In particular, the graph replacement that one analy\u00adsis chooses may produce non-monotonic \nresults for another analysis. One can prove that the composed .ow function is monotonic by establishing \na partial order on all the possi\u00adble replacement graphs for a given node, and showing that smaller inputs \nto an integrated analysis produce smaller sub-graphs, and that smaller sub-graphs lead to smaller computed \nvalues when the combined analysis is recursively solved. This is usually not di.cult because for any \none given node, there are only a few types of replacement graphs. For example, in the case of virtual \nfunction calls, there is only one replacement graph (the one that changes the virtual call to a static \ncall), and in the case of assignment statements, there are only a handful of replacement graphs (such \nas the empty sub-graph, the sub-graph generated by constant fold\u00ading, and the sub-graph generated by \nCSE).  7. EXPERIMENTAL RESULTS In this section we provide experimental results showing that our approach \nfor communication between analyses is useful in practice. We have collected performance numbers for the \nVortex compiler [13] using several Cecil [5] bench\u00admarks. The individual analyses under consideration \nare: class analysis [7], splitting [7], inlining, constant propagation 8 Edges are never removed by the \ntransformation function T . When a node is replaced by an empty subgraph, the adjacent edges are disconnected \nfrom the node, but remain in the graph. As a result edges in the original graph are guaranteed to exist, \neven by the time the last analysis runs, although they may be completely disconnected by then. benchmark \n(num lines9) monolithic comp\u00adposed modular\u00aditerated modular\u00adonce queens (50) 1.00 1.02 1.25 13.14 1.00 \n1.17 6.17 0.84 life (80) 1.00 1.00 1.09 7.39 1.00 1.17 5.72 0.83 msort (110) 1.00 0.99 1.01 6.28 1.00 \n1.11 6.04 0.20 .t (150) 1.00 1.00 0.98 3.00 1.00 1.00 6.06 0.71 richards (400) 1.00 1.00 1.07 13.66 1.00 \n1.18 6.52 1.03 deltablue (650) 1.00 1.03 0.94 12.89 1.00 1.20 6.54 0.66 instr-sched (2,400) 1.00 1.00 \n1.01 3.78 1.00 1.18 5.80 1.02 typechecker (20,000) 1.00 1.01 1.01 5.30 1.00 1.18 6.55 0.94 new-tc (23,500) \n1.00 1.05 1.03 4.57 1.00 1.17 6.09 1.16 compiler (50,000) 1.00 1.02 1.00 4.05 1.00 1.15 7.46 1.22 Figure \n1: Performance numbers for the Vortex compiler. For each benchmark, the .rst row of numbers shows the \nruntime of the generated code, and the second row shows compile-time, all normalized to the monolithic \ncon.gura\u00adtion. and folding, common sub-expression elimination, removal of redundant loads and stores, \nand symbolic assertion propa\u00adgation. We used four di.erent con.gurations of the compiler: The monolithic \ncon.guration uses a manually writ\u00adten monolithic analysis that incorporates all the op\u00adtimizations of \nthe individual analyses. This analysis was written before our framework was implemented, and it acts \nas the gold standard against which other con.gurations are measured.  The composed con.guration automatically \ncomposes the analyses using our framework.  The modular-iterated con.guration runs the analyses in sequence, \niterating until no more transformations are performed.  The modular-once con.guration runs the analyses \nin sequence once.  Figure 1 shows for each benchmark the runtime of the gen\u00aderated code (the .rst row \nof numbers for the benchmark), and the runtime of the compiler (the second row of num\u00adbers for the benchmark), \nall normalized to the monolithic con.guration. Due to run-to-run variations, di.erences of a few percent \nare not signi.cant. Smaller numbers are better since they indicate faster runtime. The important facts \nto note are the following: The modular-once con.guration generates code that runs 3-13 times slower than \nthe monolithic con.gu\u00adration. This indicates that the analyses exhibit non\u00ad 9 The number of lines of \ncode is approximate and does not include 11,000 lines of library code that gets compiled with the benchmarks. \ntrivial mutually bene.cial interactions in these bench\u00admarks. The key interaction here is between inlining \nand class analysis. Merging these two analyses leads to more precise class analysis information, and \nthis is crucial for optimizing a pure object-oriented language like Cecil, since it allows inlining of \nmessage sends in critical loops. The iterated con.guration generates code that runs nearly as fast as \nthe monolithic version, but slows down compile-time by at least a factor of 5.  The composed con.guration \n(which uses our frame\u00adwork) generates code that runs as fast as the mono\u00adlithic version, while incurring \na compile-time cost of less than 20%. This shows that our technique for com\u00admunication through graph \ntransformations can cap\u00adture (with low compile-time overhead) the cases needed in practice to exploit \nmutually bene.cial interactions.  8. EXTENSIONS TO THE BASE FRAMEWORK This section describes an extension \nto the base framework. Other extensions, including the interprocedural aspect of our framework, are described \nin two technical reports [6, 22]. In addition to supporting communication via graph trans\u00adformations, \nour framework also supports communication via what we call snooping. Snooping allows the .ow function \nof one analysis to look at the data.ow values being produced by other analyses running in parallel with \nthe .rst. Snoop\u00ading is used in our framework to allow analyses that make no transformations (which we \ncall pure analyses) to commu\u00adnicate information to other analyses. Pointer analysis, for example, can \nbe framed as a pure analysis, on which other optimizations can snoop. Snooping does not however make \ncommunication through graph replacements less useful: al\u00adthough snooping is used to communicate from \npure analy\u00adses to other analyses, graph transformations are still used to communicate in the other direction, \nfrom other analyses to pure analyses. For instance, pointer analysis can produce better results when \nit is composed with other analyses such as constant propagation and inlining, because it will be ex\u00adposed \nto the simplifying graph transformations of the other analyses. Snooping violates the strict modularity \nof individual anal\u00adyses presented so far, because the snooping analysis is aware of the possibility of \nbeing combined with other analyses, and knows how to interpret the information they are com\u00adputing. However, \nthe snooping analysis need not always be combined with the analyses on which it snoops, because de\u00adfault \nimplementations of any missing analyses that simply set all snooped-on edges to . can be provided automatically \nby the framework, causing the snooping analysis to behave conservatively. The ability to reuse the snooping \nanalysis in other analysis combinations is not hindered. 9. RELATED WORK A number of analysis frameworks \nhave been developed for making intra-and interprocedural analyses easier to write and reason about, including \nSharlit [27], SPARE [28], FIAT [17], McCAT [19], System-Z [34], PAG [2], the k-tuple data.ow analysis \nframework [23], and Dwyer and Clarke s system [15]. However, none of these systems address in\u00adtegrating \ntransformations with analyses, nor automatically combining analyses pro.tably. Nelson and Oppen [24] \ndescribe how under certain condi\u00adtions satis.ability programs for several theories can be com\u00adbined into \na satis.ability program for the combined theory. Click and Cooper [9] de.ne formally the circumstances \nin which two data.ow analyses should be integrated to reach better .xed points than repeated sequences \nof the two anal\u00adyses run separately. Cousot and Cousot [11] also point out that such interactions can \narise. However, in all these cases, the composition needs to be done manually by de.ning spe\u00adcial .ow \nfunctions over the combined data.ow information. Whit.eld and So.a [32, 33] have developed a framework \nfor examining the interactions between di.erent optimiza\u00adtions. By analyzing the pre-and post-conditions \nof opti\u00admizations, their framework can determine if one optimiza\u00adtion helps or hinders another optimization. \nThis informa\u00adtion can then be used to select an order in which to run the optimizations. However, they \ndo not provide a method for exploiting mutually bene.cial transformations: when cyclic interactions are \nfound between optimizations, a linear or\u00adder is still chosen, based on experimental results or on the \nperceived importance of the optimizations. Assmann [3, 4] has developed a technique for uniformly specifying \nanalyses and transformations using graph rewrite rules which trigger based on pattern matching. An analy\u00adsis \nis de.ned by rewrite rules that add edges to the graph, thus creating a relation which encodes the analysis \nresults. Transformations are then speci.ed using rewrite rules that trigger on patterns which can include \nedges added by the analysis, thus allowing transformations based on the analy\u00adsis results. Assmann s \nwork, however, is not motivated by the phase ordering problem. In fact, he argues that his sys\u00adtem works \nbetter when the analyses are written individually and run in sequence, instead of having a large graph \nrewrite system that composes multiple analyses, because it becomes hard to reason about the termination \nof such large rewrite systems. His formalization of graph rewrite systems is also mainly concerned with \ntermination, and he does not provide soundness or precision results, as we do. Finally, Assmann s framework \ncannot handle arbitrary abstract interpretations, whereas our framework can. On the other hand, his formu\u00adlation \ndoes allow a richer set of transformations, because edges and nodes can be arbitrarily replaced. Clients \nof our framework would simply sequence analyses and transforma\u00adtions if non-local graph replacements \nare needed, as in all other frameworks, including Assmann s. There is also a large body of literature \non advanced and e.cient program representations [16, 12, 8, 14, 21, 31, 1, 18, 20, 26]. The de.nition \nof our framework is independent of the speci.c program representation used, and thus our work should \nbe applicable to a wide range of graph-based intermediate representations. In fact, our current Whirl\u00adwind \nimplementation works over both control .ow graphs and data.ow graphs. 10. CONCLUSION We have presented \na framework that allows modular anal\u00adyses to be automatically composed and achieve mutually bene.cial \ninteractions through graph transformations. We have shown that the composed analysis terminates, is sound \nif the individual analyses are sound, and under a certain monotonicity condition is guaranteed to produce \nno worse results than running arbitrarily iterated sequences of the in\u00addividual analyses (but often produces \nbetter results). Our framework has been implemented and used success\u00adfully in the Vortex compiler, and \nmore recently in the Whirl\u00adwind compiler. Our approach allowed us to regain modular\u00adity while still maintaining \nthe bene.ts of mutually bene.\u00adcial interactions. Manually simulating transformations while the analysis \nis running is tedious and error-prone. Man\u00adually composing analyses pro.tably is even harder. Using our \nframework, we were able to design, debug, and reason about analyses separately, while combining them \npro.tably with little additional e.ort than the design of the individual parts. Acknowledgments This \nresearch is supported in part by an NSF grant (number CCR-9503741), an NSF Young Investigator Award (number \nCCR-9457767), and gifts from Sun Microsystems, IBM, Xe\u00adrox PARC, Object Technology International, Edison \nDesign Group, and Pure Software. We would like to thank Jef\u00adfrey Dean for his work on the implementation \nof the Vortex data.ow analysis engine, and Tapan Parikh for his work on an early formalization of our \nframework. We would also like to thank Manuvir Das, Vinod Grover, Todd Millstein and the anonymous reviewers \nfor their useful suggestions on how to improve the paper. 11. REFERENCES [1] A. Aiken and E. Wimmers. \nSolving systems of set con\u00adstraints. In Proceedings of the 7 th IEEE Symposium on Logic in Computer Science, \npages 329 340, Santa Cruz, CA, June 1992. [2] Martin Alt and Florian Martin. Generation of e.cient inter\u00adprocedural \nanalyzers with PAG. In Proceedings of the Second International Static Analysis Symposium, LNCS 983, pages \n33 50, Glasgow, Scotland, September 1995. Springer-Verlag. [3] Uwe Assmann. How to uniformly specify \nprogram analysis and transformations with graph rewrite systems. In Proceed\u00adings of the CC 96. 6 th International \nConference on Com\u00adpiler Construction, pages 121 135. Springer-Verlag, April 1996. [4] Uwe Assmann. Graph \nrewrite systems for program optimiza\u00adtion. ACM Transactions on Programming Languages and Systems, 22(4):583 \n637, 2000. [5] Craig Chambers. The Cecil language: Speci.cation and ra\u00adtionale. Technical Report UW-CSE-93-03-05, \nDepartment of Computer Science and Engineering. University of Washing\u00adton, March 1993. Revised, March \n1997. [6] Craig Chambers, Je.rey Dean, and David Grove. Frame\u00adworks for intra-and interprocedural data.ow \nanalysis. Tech\u00adnical Report UW-CSE-96-11-02, University of Washington, November 1996. [7] Craig Chambers \nand David Ungar. Iterative type analysis and extended message splitting: Optimizing dynamically\u00adtyped \nobject-oriented programs. In Proceedings of the ACM SIGPLAN 90 Conference on Programming Language De\u00adsign \nand Implementation, pages 150 164, June 1990. [8] Jong-Deok Choi, Ron Cytron, and Jeanne Ferrante. Auto\u00admatic \nconstruction of sparse data .ow evaluation graphs. In Conference Record of the Eighteenth Annual ACM \nSympo\u00adsium on Principles of Programming Languages, pages 55 66, January 1991. [9] Cli. Click and Keith \nD. Cooper. Combining analyses, com\u00adbining optimizations. ACM Transactions on Programming Languages and \nSystems, 17(2):181 196, March 1995. [10] Patrick Cousot and Radhia Cousot. Abstract interpretation: A \nuni.ed lattice model for static analysis of programs by construction or approximation of .xpoints. In \nConference Record of the Fourth ACM Symposium on Principles of Pro\u00adgramming Languages, pages 238 252, \nJanuary 1977. [11] Patrick Cousot and Radhia Cousot. Systematic design of program analysis frameworks. \nIn Conference Record of the Sixth Annual ACM Symposium on Principles of Program\u00adming Languages, pages \n269 282, January 1979. [12] Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth \nZadeck. An e.cient method of computing static single assignment form. In Conference Record of the Sixteenth \nAnnual ACM Symposium on Princi\u00adples of Programming Languages, pages 25 35, January 1989. [13] J. Dean, \nG. DeFouw, D. Grove, V. Litvinov, and C. Cham\u00adbers. Vortex: An optimizing compiler for object-oriented \nlan\u00adguages. In OOPSLA 96 Conference Proceedings, pages 83 100, October 1996. [14] Dhananjay M. Dhamdhere, \nBarry K. Rosen, and F. Ken\u00adneth Zadeck. How to analyze large programs e.ciently and informatively. SIGPLAN \nNotices, 27(7):212 223, July 1992. Proceedings of the ACM SIGPLAN 92 Conference on Pro\u00adgramming Language \nDesign and Implementation. [15] Matthew B. Dwyer and Lori A. Clarke. A .exible architec\u00adture for building \ndata .ow analyzers. In 17th International Conference on Software Engineering, pages 554 564, Berlin, \nGermany, March 1998. [16] Jeanne Ferrante, Karl J. Ottenstein, and Joe D. Warren. The program dependence \ngraph and its use in optimiza\u00adtion. ACM Transactions on Programming Languages and Systems, 9(3):319 349, \nJuly 1987. [17] M.W. Hall, J.M. Mellor-Crummey, A. Carle, and R. Ro\u00addriguez. Fiat: A framework for interprocedural \nanalysis and transformation. In The Sixth Anunual Workshop on Parallel Languages and Compilers, August \n1993. [18] Nevin Heintze. Set-based analysis of ml programs. In Pro\u00adceedings of the 1994 ACM Conference \non Lisp and Func\u00adtional Programming, pages 306 317, Orlando, FL, June 1994. [19] Laurie J. Hendren, Maryam \nEmami, Rakesh Ghiya, and Clark Verbrugge. A practical context-sensitive interproce\u00addural analysis framework \nfor c compilers. Technical Report ACAPS Technical Memo 72, McGill University School of Computer Science, \nJuly 1993. [20] Suresh Jagannathan and Stephen Weeks. A uni.ed treat\u00adment of .ow analysis in higher-order \nlanguages. In Confer\u00adence Record of the 22nd ACM SIGPLAN-SIGACT Sympo\u00adsium on Principles of Programming \nLanguages, pages 393 407, January 1995. [21] Richard Johnson and Keshav Pingali. Dependence-based program \nanalysis. In Proceedings of the ACM SIGPLAN 93 Conference on Programming Language Design and Imple\u00admentation, \npages 78 89, June 1993. [22] Sorin Lerner, David Grove, and Craig Chambers. Compos\u00ading data.ow analyses \nand transformations. Technical Re\u00adport UW-CSE-01-11-01, University of Washington, Novem\u00adber 2001. [23] \nStephen P. Masticola, Thomas J. Marlowe, and Barbara G. Ryder. Lattice frameworks for multisource and \nbidirectional data .ow problems. ACM Transactions on Programming Languages and Systems, 17(5):777 803, \nSeptember 1995. [24] Greg Nelson and Derek C. Oppen. A simpli.er based on e.cient decision algorithms. \nIn Conference Record of the Fifth Annual ACM Symposium on Principles of Program\u00adming Languages, pages \n141 150, January 1978. [25] Anthony Pioli and Michael Hind. Combining interprocedu\u00adral pointer analysis \nand conditional constant propagation. Technical Report 21532, IBM T.J. Watson Center, 1999. [26] Vugranam \nC. Sreedhar, Guang R. Gao, and Yong fong Lee. A new framework for exhaustive and incremental data .ow \nanalysis using DJ graphs. In Proceedings of the ACM SIG-PLAN 96 Conference on Programming Language Design \nand Implementation, pages 278 290, May 1996. [27] Steven W. K. Tjiang and John L. Hennessy. Sharlit \na tool for building optimizers. SIGPLAN Notices, 27(7):82 93, July 1992. Conference on Programming Language \nDesign and Implementation. [28] G. A. Venkatesh and Charles N. Fischer. SPARE: A devel\u00adopment environment \nfor program analysis algorithms. IEEE Transactions on Software Engineering, 18(4):304 318, April 1992. \n[29] Mark N. Wegman and F. Kenneth Zadeck. Constant prop\u00adagation with conditional branches. ACM Transactions \non Programming Languages and Systems, 13(2):181 210, April 1991. [30] Mark N. Wegman and Frank Kenneth \nZadeck. Con\u00adstant propagation with conditional branches. In Conference Record of the Twelfth Annual ACM \nSymposium on Prin\u00adciples of Programming Languages, pages 291 299, January 1985. [31] Daniel Weise, Roger \nF. Crew, Michael Ernst, and Bjarne Steensgaard. Value dependence graphs: Representation without taxation. \nIn Conference Record of the 21st ACM SIGPLAN-SIGACT Symposium on Principles of Program\u00adming Languages, \npages 297 310, January 1994. [32] Debbie Whit.eld and Mary Lou So.a. An approach to order\u00ading optimizing \ntransformations. In Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Program\u00adming \n(2nd PPOPP 90), SIGPLAN Notices, pages 137 146, March 1990. [33] Deborah L. Whit.eld and Mary Lou So.a. \nAn approach for exploring code improving transformations. ACM Transac\u00adtions on Programming Languages \nand Systems, 19(6):1053 1084, November 1997. [34] Kwangkeun Yi and Williams Ludwell Harrison III. Auto\u00admatic \ngeneration and management of interprocedural pro\u00adgram analyses. In Conference Record of the Twentieth \nACM SIGPLAN-SIGACT Symposium on Principles of Program\u00adming Languages, pages 246 259, January 1993. APPENDIX \n A. DEFINITION OF THE SOLUTION FUNC-TION S Given an analysis A =(D, ., n,, ., .,s, F ), a graph g and \na tuple of data.ow values t . D= for the input edges of g, SA(g, t) is de.ned as follows. First, we de.ne \nthe interpretation function Int : Eg \u00d7 (Eg . D) . D as in Cousot and Cousot [10]: given an edge e and \nthe current data.ow solution m, Int computes the data.ow value for e at the next iteration. Int is de.ned \nas: t[k] if nk.e = InEdgesg [k]Int(e, m)= - . F (n, m(Ing (n)))[k] where e = Outg (n)[k] The global \n.ow function FG :(Eg . D) . (Eg . D) takes a map representing the current data.ow solution, and computes \nthe data.ow solution at the next iteration. FG is de.ned as: FG(m)= Ae.Int(e, m) The global ascending \n.ow function F GA is the same as FG, except that it joins the result of the next iteration with the current \nsolution before returning. This ensures that the solution monotonically increases as iteration proceeds, \neven if F is not monotonic. F GA is de.ned as: F GA(m)= FG(m) . m Finally, the result of SA is a .xed \npoint of F GA (the least .xed point if F is monotonic): SA(g, t)= . F GAn(. ) n=0 where . Ae.., F GA0 \n= Ax.x and F GAk = F GA . F GAk-1 for k> 0. B. DEFINITION OF THE TRANSFORMATION FUNCTION T Given a \nreplacement function R, a graph g and some anal\u00adysis results m, T (R, g, m) is de.ned as follows. First, \nwe introduce the update function Update : Graph \u00d7 Node \u00d7 Graph . Graph, which is used to replace a single \nnode in a graph. Given an original graph old, a node n and a replace\u00adment graph repl for this node, Update \nreturns the result of replacing the node n with repl in old. Update is de.ned as follows: Update(old, \nnode, repl)=(Nnew,Enew , Innew, Outnew , InEdgesnew, OutEdgesnew ) where Nnew =(Nold -{n}) . Nrepl Enew \n=(Eold . Erepl)- (Elmts(InEdgesrepl) . Elmts(OutEdgesrepl)) with Elmts(tuple)= {d|ni.tuple[i]= d} InEdgesnew \n= InEdgesold OutEdgesnew = OutEdgesold Inold(s) if s . Nold -{n} Innew (s)= ----. ReplIn(Inrepl(s)) \nif s . Nrepl Outold(s) if s . Nold -{n} Outnew (s)= -----. ReplOut(Outrepl(s)) if s . Nrepl and Inold(n)[k] \nif nk.e = InEdgesrepl[k]ReplIn(e)= e otherwise Outold(n)[k] if nk.e = OutEdgesrepl[k]ReplOut(e)= e otherwise \nWe now de.ne U pdate., a simple extension to Update that works correctly if the replacement graph is \n.:  g if r = . Update.(g, n,r)= Update(g, n, r) otherwise The graph returned by T (R, g,m) is then simply \nthe it\u00aderated application of Update. on all the nodes of g. Thus, T (R, g,m) is de.ned by: T (R, g, m)= \nIT (R, g, m,Ng ) where IT (which stands for IteratedT ) is: IT (R, gnew , m, N -{n}) if nn . N IT (R, \ng,m, N )= g if N = . with - . gnew = Update.(g, n, R(n, m(Ing (n))))  \n\t\t\t", "proc_id": "503272", "abstract": "Dataflow analyses can have mutually beneficial interactions. Previous efforts to exploit these interactions have either (1) iteratively performed each individual analysis until no further improvements are discovered or (2) developed \"super-analyses\" that manually combine conceptually separate analyses. We have devised a new approach that allows analyses to be defined independently while still enabling them to be combined automatically and profitably. Our approach avoids the loss of precision associated with iterating individual analyses and the implementation difficulties of manually writing a super-analysis. The key to our approach is a novel method of implicit communication between the individual components of a super-analysis based on graph transformations. In this paper, we precisely define our approach; we demonstrate that it is sound and it terminates; finally we give experimental results showing that in practice (1) our framework produces results at least as precise as iterating the individual analyses while compiling at least 5 times faster, and (2) our framework achieves the same precision as a manually written super-analysis while incurring a compile-time overhead of less than 20%.", "authors": [{"name": "Sorin Lerner", "author_profile_id": "81100399150", "affiliation": "Univ. of Washington", "person_id": "PP43119616", "email_address": "", "orcid_id": ""}, {"name": "David Grove", "author_profile_id": "81100575938", "affiliation": "IBM T.J. Watson Research Center", "person_id": "PP39049261", "email_address": "", "orcid_id": ""}, {"name": "Craig Chambers", "author_profile_id": "81100528252", "affiliation": "Univ. of Washington", "person_id": "PP39047060", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/503272.503298", "year": "2002", "article_id": "503298", "conference": "POPL", "title": "Composing dataflow analyses and transformations", "url": "http://dl.acm.org/citation.cfm?id=503298"}