{"article_publication_date": "01-01-2002", "fulltext": "\n Proving Correctness of Compiler Optimizations by Temporal Logic . David Lacey Neil D. Jones David.Lacey@ \nneil@diku.dk comlab.ox.ac.uk University of Copenhagen University of Oxford  ABSTRACT Many classical \ncompiler optimizations can be elegantly ex\u00adpressed using rewrite rules of form: I =. I * if \u00df, where \nI,I * are intermediate language instructions and \u00df is a property expressed in a temporal logic suitable \nfor describing program data .ow. Its reading: If the current program a contains an instruction of form \nI at some control point p, and if .ow condition \u00df is satis.ed at p, then replace I by I * . The purpose \nof this paper is to show how such transforma\u00adtions may be proven correct. Our methodology is illustrated \nby three familiar optimizations, dead code elimination, con\u00adstant folding and code motion. The meaning \nof correctness is that for any program a, if Rewrite(a, a * , p,I =. I * if \u00df) then [[a]] =[[a * ]], \ni.e. a and a * have exactly the same seman\u00adtics. 1. INTRODUCTION This paper shows that temporal logic \ncan be used to validate some classical compiler optimizations in a very strong sense. First, typical \noptimizing transformations are shown to be simply and elegantly expressible as conditional rewrite rules \non imperative programs, where the conditions are formulas in a suitable temporal logic. In this paper \nthe temporal logic is an extension of CTL with free variables. The .rst trans\u00adformation example expresses \ndead code elimination, the sec\u00adond expresses constant folding and the third expresses loop invariant \nhoisting. The .rst involves computational futures, the second, computational pasts and the third, involves \nboth the computational future and past. Second, the optimizing transformations are proven to be fully \nsemantics preserving : in each case, if a is a program .This research was partially supported by the \nDanish Nat\u00adural Science Research Council (PLT project), the EEC (Daedalus project) and Microsoft Research. \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. POPL \n02, Jan. 16-18, 2002 Portland, OR USA = 2002 ACM ISBN c1-58113-450-9/02/01...$5.00 Eric Van Wyk Carl \nChristian Frederiksen Eric.Van.Wyk@ xeno@diku.dk comlab.ox.ac.uk University of Copenhagen University \nof Oxford and a * is the result of transforming it, a induction relation is established between the \ncomputations of a and a * . A conse\u00adquence is that if a has a terminating computation with .nal answer \nv, then a * also has a terminating computation with the same .nal answer; and vice versa. 1.1 Compiler \noptimizing transformations A great many program transformations are done by opti\u00admizing compilers; an \nexhaustive catalog may be found in [20]. These have been a great success pragmatically, so it is important \nthat there be no serious doubt of their correct\u00adness: that transformed programs are always semantically \nequivalent to those from which they were derived. Proof of transformation correctness must, by its very \nnature, be a semantics-based endeavor. 1.2 Semantics-based program manipulation Much has happened in \nthis .eld since the path-breaking 1977 Cousot paper [5] and 1980 conference [11]. The .eld of abstract \ninterpretation [1, 5, 6, 13, 23, 25] arose as a mainly European, theory-based counterpart to the well\u00addeveloped \nmore pragmatic North American approach to pro\u00adgram analysis [2, 10, 21]. The goal of semantics-based \npro\u00ad gram manipulation ([12] and the PEPM conference series) is to place program analysis and transformation \non a solid foundation in the semantics of programming languages, mak\u00ading it possible to prove that analyses \nare sound and that transformations do not change program behaviors. This approach succeeded well in placing \non solid semantic foundations some program analyses used by optimizing com\u00adpilers, notable examples being \nsign analysis, constant prop\u00adagation, and strictness analysis. An embarrassing fact must be admitted, \nthough: Rather less success was achieved by the semantics-based approach toward the goal of validating \ncorrectness of program transformations, in particular, data\u00ad.ow analysis-based optimizations as used \nin actual compil\u00aders. One root of this problem: Semantic frameworks such as de\u00adnotational and operational \nsemantics describe program ex\u00adecution in precise mathematical or operational terms; but representation \nof data dependencies along computational fu\u00adtures and pasts is rather awkward, even when continuation \nsemantics is used. Worse, such dependency information lies at the heart of the most widely used compiler \noptimizing transformations. 1.3 Semantics-based transformation correct\u00adness Transformation correctness \nis somewhat complex to estab\u00adlish, as it involves proving a soundness relation among three actors : the \ncondition that enables applying the transfor\u00admation and the semantics of the subject program both before \nand after transformation. Denotational and operational se\u00admantics (e.g., [33]) typically present many \nexample proofs of equivalences between program fragments. However most of these are small (excepting \nthe monumental and indigestible [19]), and their purpose is mainly to illustrate proof method\u00ad ology \nand subtle questions involving Scott domains or pro\u00adgram contexts, rather than to support applications. \nA problem is that denotational and operational methods seem ill-suited to validating transformations \nthat involve a program s computational future or computational past. Even more di.cult are transformations \nthat change pro\u00adgram control .ow, notable examples being code motion and strength reduction. Few formal \nproofs have been made of correctness of such transformations. Two works relating the semantics-based \napproaches to transformation correctness: Nielson s thesis [24] has an unpublished chapter proving correctness \nof con\u00adstant folding perhaps omitted from the journal paper [23], because of the complexity of its development \nand proof. Havelund s thesis [9] carefully explores semantic aspects of transformations from a Pascal-like \nmini-language into typ\u00adical stack-based imperative intermediate code, but correct\u00adness proofs were out \nof its scope (and would have been im\u00adpractically complex in a denotational framework, witness [19]). \n Another approach to verifying the correctness of compiler optimizations is presented by Kozen and Patron \n[16]. Us\u00ading an extension of Kleene algebra, Kleene algebra with tests (KAT), an extensive collection \nof instances of pro\u00adgram transformations are proven correct, i.e. a concrete optimization is proven correct \ngiven a concrete source pro\u00adgram and the transformed program. Programs are repre\u00adsented as algebraic \nterms in KAT and it is shown that the original and transformed program are equal under the al\u00adgebraic \nlaws of KAT. In many instances these KAT terms are not ground, that is, they contain variables, and thus \nthe reasoning could be applied to general program trans\u00adformations. One has to note, however, that the \npaper sets out with a di.erent perspective on program transformation. The paper is geared towards establishing \na framework where one can formally reason about program manipulations spec\u00adi.ed as KAT equalities that \nimply semantic equivalence. Al\u00adthough the results in the paper are not directly applicable to compilers \nsince no automatic method is given for apply\u00ading the optimizations described, they are however applicable \nto proof carrying code (PCC) [22] and e.cient code certi.\u00ad cation (ECC) [15], as Kozen and Patron state. \nIn ECC, an optimized program contains a proof verifying that the trans\u00adformations applied to it were \nsound, in which case proofs about speci.c instances are exactly what is needed. In contrast, the present \npaper aims to formalize a framework for describing and formally proving classical compiler opti\u00admizations. \nWe claim that these speci.cations (once proven correct) can be directly and automatically utilized in \nopti\u00admizing compliers. It is also worth noting that Kozen and Patron study only well-structured programs, \ni.e. those with while loops but no goto statements, whereas we do not make this restriction. Some transformation \ncorrectness proofs have been made for functional languages, especially the work by Wand and col\u00adleagues, \nfor example [31], using logical relations. These methods are mathematically rather more sophisticated \nthan those of this paper, which seem more appropriate for tradi\u00adtional intermediate-code optimizations. \n 1.4 Model checking and program analysis This situation has improved with the advent of model check\u00ading \napproaches to program analysis [4, 27, 29, 30, 32, 28]. Work by Ste.en and Schmidt [29, 30] showed that \ntemporal logic is well-suited to describing data dependencies and other program properties exploited \nin classical compiler optimiza\u00adtions. In particular, work by Knoop, Ste.en and R\u00a8uthing [14] showed that \nnew insights could be gained from using temporal logic, enabling new and stronger code motion al\u00adgorithms, \nnow part of several commercial compilers. More relevant to this paper: The code motion transforma\u00adtions \ncould be proven correct. 1.5 Model checking and program transforma\u00adtion In this paper we give a formalism \n(essentially it is a subset of [18]) for succinctly expressing program transformations, making use of \ntemporal logic; and use this formalism to prove the universal correctness (semantics preservation for \nall programs) of the three optimizing transformations: dead code elimination, constant folding and loop \ninvariant hoist\u00ading. The thrust of the work is not just to prove these three transformations correct, \nthough, but rather to establish a framework within which a wide spectrum of classical com\u00adpiler optimizations \ncan be validated. More instances of this paper s approach may be found in [7]. Many optimizing transformations \ncan be elegantly expressed using rewrite rules of form: I =. I * if \u00df, where I,I * are intermediate language \ninstructions and \u00df is a property ex\u00adpressed in a temporal logic suitable for describing program data \n.ow. Its reading [18]: If the current program a con\u00adtains an instruction of form I at some control point \np, and if .ow condition \u00df is satis.ed at p, then replace I by I * . The purpose of this paper is to show \nhow such transforma\u00adtions may be proven correct. The meaning of correctness is that for any program a, \nif Rewrite(a, a * , p,I =. I * if \u00df) then [[a]] =[[a * ]], i.e. a and a * have exactly the same semantics. \n  2. PROGRAMS AND TRANSFORMATIONS In this section we provide fundamental de.nitions used in our representations \nof programs, analyses, transformations, 0: read N; \u00ae 0 1: Five:=5 \u00ae 1 2: Sum:=0 \u00ae 2 3: C:=Five \u00ae 3 \n 4: Sum := Sum + C * N 4\u00ae 5: N := N-1 5\u00ae 6: if N goto 4 else 7 6\u00ae 7: write Sum  7\u00ae Figure 2.1: Example \nprogram and control .ow.  and correctness proofs. In section 2.1 we introduce a simple imperative programming \nlanguage that we use to demon\u00adstrate program transformations and their proofs of correct\u00adness. In section \n2.2 we describe the control .ow graph rep\u00ad resentation of programs that serves as the model over which \nthe temporal logic formulas in the rewrite rules are checked. The temporal logic CTL with free variables \nis presented in section 2.3. The rewriting rules are de.ned in section 2.4 and section 2.5 provides the \nspeci.cations for the dead code elimination and constant folding transformations. 2.1 A simple programming \nlanguage De.nition 1 A program a has form: a = read X; I1 ; I2; ... Im-1; write Y where I1,...Im-1 are \ninstructions, labeled by the program labels of a in Nodes\" = {0, 1, 2,...,m}. Further, let in\u00adstruction \nI0 be the initial read X, and instruction Im be the concluding write Y . The read and write instructions \nmust, and can only, appear respectively at the beginning and end of a program. The syntax of all other \ninstructions in a is given by the following grammar: Inst 3 I ::= skip | X := E | if X goto n else n \n* Expr 3 E ::= X | O E...E Op 3 O ::= various unspeci.ed operators o, each with arity(o) = 0 Var 3 X \n::= X | Y | Z | ... Label 3 n, n * ::= 1 | 2 . . . | m Semantics are as expected and are formally de.ned \nbelow in Section 3. Figure 2.1 contains an example program. For readability it has explicit instruction \nlabels, and operators are written in in.x position. In order to provide a simple framework for proving \ncorrect\u00adness this language has no exceptions or procedures. We expect the technique can be extended to \ninclude such fea\u00adtures and maintain its fundamental nature, but this is future work. 2.2 Modeling program \ncontrol .ow In order to reason about the program with a view to trans\u00adform it, we look at the control \n.ow graph of the program. This is a type of transition system. We introduce this con\u00adcept here and will \nfurther use it to describe the semantics of a program: De.nition 2 A transition system is a pair T =(S, \n*), where S is aset and *. S \u00d7 S. The elements of S are referred to as states or nodes. A path is a maximal \nsequence of nodes (.nite1 or in.nite) n0 * n1 * ... such that Vi = 0,ni * ni+1 .A backwards path is a \npath over the inverse of * (written as ) and *0 *0 *0 written as n0 n1 ... or n0 . n1 . .... Closely \nrelated to transition systems are models (as used in model checking). These are transition systems where \neach state is labeled with certain information: De.nition 3 A model is a triple M =(S, *,L) where (S, \n*) is a transition system, and labeling function L : S * 2P labels each state in S with a set of propositions \nin P. The control .ow transition system is a system where states are program points and transitions are \nbetween pairs of pro\u00adgram points that could follow each other in the execution. De.nition 4 The control \n.ow transition system for a is Tcf (a)=(Nodes\" , * cf ) where the (total) relation * cf is de.ned by \nn1 * cf n2 if and only if (In1 E{X:=E, skip, read X}. n2 = n1 + 1) . (In1 = if X goto n else n * . (n2 \n= n . n2 = n * )) . (In1 = write Y . n2 = n1)  We set up a control .ow model by labeling the states \nof the system (program points in this case) with propositions of interest. These will include the instruction \nat that program point plus information on which variables are de.ned or used at that point. Figure 2.2 \nshows an example model in which node 2, whose instruction Sum :=0 is labeled by the propo\u00adsitions node(2), \nstmt(Sum := 0), def (Sum), and conlit(0). De.nition 5 The control .ow model for program a is de\u00ad.ned \nas Mcf (a)=(Nodes\" , * cf ,L) where (Nodes\" , * cf ) are as in De.nition 4, and L(n) is de.ned as follows \nfor n E Nodes\" : 1A .nite path n0 * n1 * ... * nm is maximal if Vn, \u00ac(nm * n). That is, nm has no successors. \nS = {0, 1, 2, 3, 4, 5, 6, 7} * = {0 * 1, 1 * 2, 2 * 3, 3 * 4, 4 * 5, 5 * 6, 6 * 7, 6 * 4, 7 * 7} L(0) \n.{node(0), stmt(read N), def (N)} L(1) .{node(1), stmt(Five := 5), def (Five), conlit(5)} L(2) .{node(2), \nstmt(Sum := 0), def (Sum), conlit(0)} L(3) .{node(3), stmt(C := Five), def (C), use(Five)} L(4) .{node(4), \nstmt(Sum := Sum + C . I), def (Sum), use(Sum), use(C), use(I)} L(5) .{node(5), stmt(N := N - 1), def \n(N), use(N), conlit(1)} L(6) .{node(6), stmt(if N goto 4 else 7), use(N)} L(7) .{node(7), stmt(write \nSum), use(Sum)} Figure 2.2: Control .ow model for the example program. L(n)= {stmt(In) | 0 . n . m} \n.{node(N) | n = N} .{def (X) | In has form X:=E or read X} .{use(X) | In form: Y:=E with X in E, or In \n= if X goto p else p * } .{use(Y) | n = m and In = write Y} .{conlit(O) | O is a constant in In (operator \nwith arity(O) = 0)} .{trans(E) | E is an expression in a and In is not of form: X:=E * or read X with \nX in vars(E) } The predicates stmt(I), def(X), use(X), conlit(O), trans(E) are the building blocks for \nthe conditions that specify when optimizing transformations can be safely applied. These conditions are \nspeci.ed as CTL-FV formulas. De.nition 6 An expression E is transparent at a program point if none of \nthe variables in the expression are de.ned at that point (i.e. assigned by := or read). 2.3 CTL with \nfree variables The temporal logic CTL-FV used in specifying transforma\u00adtion conditions is in two respects \na generalization of CTL [3]. First, as is common, the temporal path quanti.ers E and A are extended to \nalso quantify over backwards paths in the .- . - obvious way. Our notation for this: E and A . These \npaths may be .nite and our quanti.cations over paths are thus over in.nite and maximal .nite paths. That \nis, we consider a branching notion of past which may be either .nite, as in CTLbp [17], or in.nte, as \nin P OT L [26, 34]. A branch\u00ading past is more appropriate here than the linear past in PCTL. [8] which \ncan also be used to augement branching time logics with past time operators. Second, propositions are \ngeneralized to predicates over free variables. (A traditional atomic proposition is simply a predicate \nwith no arguments.) For example, the formula stmt(x := e) where stmt E Pr, has free variables x and e \nranging over program variables and expressions, respec\u00adtively. These free variables will henceforth be \ncalled CTL\u00advariables to avoid confusion with variables or program points appearing in the program being \ntransformed or analyzed. The e.ect of model checking will be to bind CTL-variables to program points \nor bits of program syntax, e.g., dead vari\u00adables or available expressions. CTL-FV formulas are either \nstate or path formulas gener\u00adated by the grammar with non-terminals {\u00df, .}, terminals true, false, pr \nE Pr and free variables x1,...,xn, start sym\u00adbol \u00df and the productions: \u00df ::= true \u00df ::= E. . ::= X\u00df \n\u00df ::= false \u00df ::= A. . ::= \u00dfU\u00df . - \u00df ::= pr(x1 ,...,xn) \u00df ::= E. . ::= \u00dfW \u00df . - \u00df ::= \u00df . \u00df\u00df ::= A. \u00df \n::= \u00ac\u00df Operational interpretation: A model checker will not simply .nd which nodes in a model satisfy \na (state) for\u00admula, but will instead .nd the instantiation substitutions that satisfy the formula. Mathematically, \nwe model this by extending the satisfaction relation n |= \u00df to include a substi\u00adtution . binding its \nfree variables. The extended satisfaction relation n |= a \u00df will hold for any . such that n |= .(\u00df). \nThis relation is de.ned in Figure 2.3. All is as usual, except for . and the interpretation of W on maximal \n.nite paths. The job of the model checker is thus, given \u00df, to return the set of all n and . such that \nn |= a \u00df. For the example program in Figure 2.1 and formula def (x).use(x), the model checker returns \nthe following set of instantiation substitutions. (For brevity, CTL-variable n is bound to the program \npoint in the substitutions.) {.1,.2} = {[n .* 4,x .* Sum], [n .* 5,x .* N]} Of particular interest when \nanalysing the control .ow model is the universal weak until operator (AW ). This is due to the following \nlemma: Lemma 1 If n0 |= A(\u00df1 W\u00df2) then for any maximal .nite path n0 * ... * nN , there exists an j such \nthat nj |= \u00df2 and V0.i<j ni |= \u00df1. A similar result holds for backwards paths. Proof Omitted. We use \nthis lemma in the correctness proofs since the max\u00adimal .nite paths are exactly the set of terminating \nprogram traces of the execution transition system of De.nition 10.  2.4 Rewriting De.nition 7 A rewrite \nrule has the form I =. I * if \u00df, where I,I * are instructions built from the program and CTL vari\u00adables, \nand \u00df is a CTL-FV temporal logic formula. By de.\u00adnition Rewrite(a, a * , n,I =. I * if \u00df) is true if \nand only if for some substitution ., the following hold: State Formulas: n |= a true i. true n |= a false \ni. false n |= a pr(x1,...,xn) i. pr(.x1, . . . , .xn) E L(n) n |= a \u00ac\u00df i. not n |= a \u00df n |= a \u00df1 . \u00df2 \ni. n |= a \u00df1 and n |= a \u00df2 n |= a E. i. .path p = n * n1 * n2 ...,[p |= a .] n |= a A. i. Vpaths p = \nn * n1 * n2 ...,[p |= a .] . - n |= a E. i. .path p = n *0 n1 *0 n2 ...,[p |= a .] . - n |= a A. i. Vpaths \np = n *0 n1 *0 n2 ...,[p |= a .] Path Formulas: (below * * is * or *0) * * * * p |= a X\u00df i. p = n0 n1 \n... and n1 |= a \u00df p |= a \u00df1 U\u00df2 i. p = n0 * * n1 * * ... and .i = 0[ni |= a \u00df2 .Vj[0 . j<i implies nj \n|= a \u00df1]] * * (.k = 0[nk |= a \u00df2 and Vi, 0 . i<k =. ni |= a \u00df1 )] or (Vk = 0[nk |= a \u00df1 and nk+1 exists]) \np |= a \u00df1 W\u00df2 i. p = n0 * * n1 ... and Figure 2.3: CTL-FV satisfaction relation n |= a stmt(I) . \u00df a \n= read X; I1; ...In;...Im-1; write Y, where In = .(I), and a * = read X; I1; ....(I * ); ...Im-1; write \nY Sometimes we may want to alter the program at more than one point. In this case we specify several \nrewrites and side conditions at once. For example, to transform two nodes the form of the rewrite would \nbe: n : I1 =. I1 * m : I2 =. I2 *  if n |= \u00df1 m |= \u00df2 The operational interpretation of this is that \nwe .nd a sub\u00adstitution . that satis.es both of n |= a stmt(I1) . \u00df1 and m |= a stmt(I2 ) . \u00df2 and then \nuse this substitution to alter the program in the two relevant places. 2.5 Sample transformations Following \nare versions of three classical optimizations (sim\u00adpli.ed in comparison to compiler practice, to make \nit easier to follow the techniques used in the proofs). For convenience we express code removal as replacement \nof an instruction by skip, and code motion as simultaneous replacement of an instruction I and skip by \n(respectively) skip and instruction I. We assume the compiler will remove useless occurrences of skip. \nWhile most programmers do not write code that contains dead code or opportunities for constant folding \nit often re\u00adsults from other transformations, especially automated ones. Dead Code Elimination: Dead \ncode elimination removes assignment statements that assign a value that is never used. In our model, \nthe rewrite replaces the assignment with the skip instruction: x := e =. skip The side condition on the \nrewrite must specify that the value assigned is never referenced again. This is exactly the kind of condition \nthat temporal logic can specify. The rewrite rule with its side condition is thus written x := e =. skip \nif AX \u00acE( true U use(x)). Since we do not care whether x is used at the current node, we skip past it \nwith the AX operator. Constant Folding: Constant folding is a transformation that replaces a variable \nreference with a constant value: x := y =. x := c. One method of implementing constant folding for a \nvariable Y is to check whether all possible assignments to Y assign it the same constant value. To check \nthis condition we use the past temporal operators, specifying the complete transfor\u00admation as follows:2 \n x := y =. x := c if . - A (\u00acdef (y) W stmt(y := c) . conlit(c)) Code motion/loop invariant hoisting: \nA restricted ver\u00adsion of a code motion transformation (CM) that covers the loop invariant hoisting transformation \nis de.ned as p : skip =. x:=e q : x:=e =. skip  if p |= A(\u00acuse(x) W node(q)) q |= \u00acuse(x) . . - A \n((\u00acdef (x) . node(q)) . trans(e) W node(p)) This transformation involves two (di.erent) statements in \nthe subject program. The transformation moves an assign\u00adment at label q to label p provided that two \nconditions are met: 1. The assigned variable x is dead after p and remain so until q is reached. If this \nrequirement holds, then introducing the assignment x:=e at label p will not change the semantics of the \nprogram. 2. The second requirement (in combination with the .rst rewrite rule) states that the expression \ne should be available at q after the transformation.  2The conlit is introduced so that the modelchecker \nwill not match c with a non-constant expression. This transformation could also be obtained by applying \ntwo transformations: One that inserts the statement x:=e pro\u00advided that x is dead between p and q, followed \nby the elimi\u00adnation of available expressions transformation. With the two transformations one would need \nsome mechanism of control\u00adling where to insert which assignments. By formulating the transformation as \na single transformation, the two labels p and q are explicitly linked. Since all paths from p may not \nevelually reach q, it is possi\u00adble to move assignments to labels such that e is still available in q \nand x is dead in all paths not leading to q, which would still be a semantics preserving transformation. \n1 : skip; 2 : if . . . then 3 else 6; 3 : x := a + b; 4 : y := y - 1; 5 : if y then 3 else 6; 6 : x := \n0; In this program is is possible to lift the statement x:=a+b from label 3 to label 1. In general the \ntransformation by it\u00adself could slow down the computation, since there is no need to compute the expression \nif the expression is not needed. Note that we use the weak until (W ), this is so that the transformation \nis not disabled by cycles in the control .ow graph that do not a.ect the correctness of the transforma\u00adtion. \n 2.6 Computational aspects We discuss computational aspects only brie.y; more can be found in [18] and \nrelated papers. Model checking with respect to I =. I * if \u00df yields a set of pairs {(p1,.1),..., (pk \n,.k )} satisfying \u00df. Consequence: {p1,...,pk} is the set of all places where this rule can be applied. \nFor instance, all places where dead code elimination can be done are found by a single model check. The \ntime to model check n |= p for transition system T is a low-degree polynomial, near linear for many transition \nsystems, and |T |2 \u00b7|\u00df| in the worst case. Of course, in the case of model checking CTL-FV formulas times \ncould be higher, since |T | depends on the size of labelling function * 2AP L : Nodes\" as in De.nition \n5. For each node n, L(n) can be found in time proportional at most to the size of the instruction In, \nwith one exception: propositions trans(E), which can take time proportional to the size of a. For greater \ne.ciency these can be treated specially, maintaining a single global table for the transparency relation. \nExperience from [18] and related work indicates that their algorithm for model checking CTL-FV is not \ntoo expensive in practice, i.e. that the free variables do not impose an unreasonable time cost.  3. \nPROGRAM SEMANTICS In this section we de.ne the semantics of the simple pro\u00adgramming language introduced \nin De.nition 1. In Section 5 we use this semantics to show for a program a and its trans\u00adformed version \na * that their semantics are the same. That is, [[a]] = [[a * ]]. De.nition 8 (Semantic framework) We \nassume the following have been .xed in advance, and apply to all pro\u00adgrams: A set Value of values (not \nspeci.ed here), containing a designated element true.  A .xed interpretation of every n-ary operator \nsymbol  o as a function [[o]] : Valuen * Value. Note that [[o]] E Value if n = 0. De.nition 9 (Expression \nevaluation) A store is a func\u00adtion p E Store = Var . Value. Expression evaluation [[Expr]] : Store * \nValue is de.ned by: [[X]]p = p(X) [[oE1 ... En]]p = [[o]]([[E1]]p, . . . , [[En]]p) De.ne p\\X to be the \nstore function p restricted to its original domain minus X. Further, p[X .* v] is the same as p except \nthat it maps X to v. De.nition 10 (Semantics) At any point in its compu\u00adtation, the program will be in \na state of the form s = (p, p) E State = Nodes\" \u00d7 Store. The Initial state for input v E Value is In(v) \n= (0,p) where p(X)= v and p(Z)= true for all other variables appearing in program a. The state transition \nrelation *. State \u00d7 State is de.ned by: 1. If Ip = skip or Ip =(read X) then (p, p) * (p +1,p). 2. If \nIp =(X := E) then (p, p) * (p +1,p[X .* [[E]]p]). 3. If Ip =(if X goto p * else p ** ) and p(X)= true \nthen (p, p) * (p * ,p). 4. If Ip =(if X goto p * else p ** ) and p(X)= true then (p, p) * (p ** ,p). \n 5. (m, p) * (m, p) for any store p.  Note that the read X has no e.ect on the store p since the initial \nvalue v of X is set in the initial state. The operational semantics of a program is given the form of \na transition system: the execution transition system Trun . De.nition 11 The execution transition system \nfor program a and input v E Value is by de.nition Trun (a, v)=(Nodes\" \u00d7 Store, *) where s1 * s2 is as \nin De.nition 10. De.nition 12 The semantic function is the partial func\u00adtion [[ ]]: Value . Value de.ned \nby: [[a]](v)= p(Y) if there exists a .nite sequence s0 * s1 * ... * st =(m, p) In order to reason about \nthe computational history of pro\u00adgram executions we also introduce the notion of compu\u00adtational pre.x, \nand a corresponding transition system Tpfx (both de.ned below). The control .ow model de.ned above in \nDe.nition 5 is an abstraction of each of these, see Lemma 3. These are used in the correctness proofs \nto relate a program s semantics to its control .ow model. 4. A METHOD FOR SHOWING SEMAN-TIC EQUIVALENCE \nFor all rewrite rules I =. I * if \u00df we need to show that Rewrite(a, a * , p,I =. I * if \u00df) implies [[a]] \n= [[a * ]], i.e. for all input v, In(v) *. (m, p) is a terminating computation for program a if and only \nif In(v) *. (m * ,p * ) is a terminating computation for program a * with p(Y)= p * (Y). The problem \nnow is how to link the temporal property \u00df, which concerns futures and pasts , to the transformation \nI =. I * . For this it is not su.cient to regard states one at a time . - due to operators, such as AU \nand AU, giving access to information computed earlier or later. Our solution is to enrich the semantics \nand its transition system by considering computation pre.xes of form: C = a, v . s0 * ... * st. Now suppose \np |= \u00df has been model checked. The resulting substitutions (see Lemma 3 below) also describe the com\u00adputation \npre.x C. Conclusion: The results of the model check, contain information about the state sequence in \nC, thus relating past and present states. What about futures? Our choice is to build a transition system \nTpfx (a, v) so C * C1 ETpfx (a, v) if and only if C1 is identical to C, but with one additional state: \nC1 = a, v . s0 * ... * st * st+1. Now reasoning that involves futures can be done by ordinary induction: \nassuming CRC * , show C * C1 implies C * * C1 * for a C1 * with C1RC1* , and C * * C1 * for C1 with C1RC1 \n* . De.nition 13 For a program a and initial value v E V alue, a computation pre.x is an sequence (.nite \nor in.nite) a, v . s0 * s1 * s2 * ... such that s0 = In(v) and si * si+1 for i =0, 1, 2,... De.nition \n14 The computation pre.x transition system for program a and input v E Value is by de.nition Tpfx (a, \nv)=(C, *) where C is the set of all .nite computation pre.xes, and C1 * C2 if and only if C1 = a, v . \ns0 * s1 * ... * st, C2 = a, v . s0 * s1 * ... * st * st+1. where st * st+1 is the state transition relation \nfrom De.ni\u00adtion 10. Note that we use the same symbol, *, to represent both the transition relation for \nthe execution transition sys\u00adtem Trun and the computation pre.x transition system Tpfx but that the relations \ncan be distinguished by their context. Goal: Show that if C, C * are computation pre.xes of a, a * on \nsame input v then siRs * j for every corresponding pair of states in C, C * where R is a relation on \nstates that expresses correct simulation .3 Consider two programs, a and a * such that: a = read X; \nI1; I2; ...Im-1; write Y and * *** a = read X; I1; I2; ...Im. -1; write Y. The aim is to show that a \nand a * are semantically equivalent, [[a]] = [[a * ]]. That is, for any value v either both [[a]](v) \nand [[a * ]](v) are not de.ned or for any computation pre.x a, v . In(v) * (p1,p1) * ... * (m, p) there \nexists a computation pre.x for the transformed pro\u00adgram * ** ** a ,v . In(v) * (p1,p1) * ... * (m ,p \n) p * such that p(Y)= (Y), and conversely. We can naturally prove this result by induction on the length \nof the pre.xes. In practice the induction hypothesis needs to be strength\u00adened for the proof to work. \nThe general form of the strength\u00adened hypothesis is that a relation R holds between com\u00adputation pre.xes \nof the original program and computation pre.xes of the transformed program. Noting that the transitions \nbetween pre.xes are determinis\u00adtic (since the language is), we can see that proving the step case of \ninduction is achieved by proving that the relation between two pre.xes is preserved by any one step in \nthe transition system. The following lemma details the work that needs to be done to show semantic equivalence. \nLemma 2 (Program Equivalence/Induction) Programs a and a * are semantically equivalent: [ a] = [ a * \n] if there exists a relation R, such that for all values v: 1. (Base Case) R holds between the initial \ncomputation pre.xes i.e. ((a, v . In(v)), (a * ,v . In(v))) ER 2. (Step Case) If C1RC1* , C1 * C2 and \nC1 * * C2 * then  C2RC2* . 3This is actually closer to bi-simulation. 3. (Equivalence) If CRC * and \nC = a, v . s0 * s1 ... * (pt,p) and C * = a * ,v . s * 0 * s * 1 ... * (pt. ,p * ) then (i) pt = m .. \npt* . = m and (ii) pt = p * t. = m =. pt(Y)= pt* . (Y)  So proofs of equivalence are split into these \nthree steps. Un\u00adsurprising, it is the step case of induction that is the hardest to prove. If a .ow condition \nin a rewrite rule holds then it states a fact about the control .ow graph. This relation between this \ngraph and the computational pre.x system is captured in the following lemma that follows from the de.nitions \nof the two systems: Lemma 3 Suppose C = a, v . s0 * s1 * ... * st is a computation pre.x in Tpfx (a, \nv), where si =(pi,pi). For any 0 . j . t: 1. pj * cf pj+1 * cf ... * cf pt is a path in Tcf (a) 2. pj \n* cf pj-1 * cf ... * cf p0 is a maximal backwards path in Tcf (a)  Here we can see that the .ow graph \nis an abstraction of the semantics of the program. We know that if we are not at the program point speci.ed \nin the rewrite then the instruction in the original program and the transformed program will coincide: \nLemma 4 Suppose a and a * are programs that are related by Rewrite(a, a * , p, R), then p = q . Iq = \nIq * We also know that if we are not at the transformed point in the program then the original program \nand the new program will behave identically: Lemma 5 Suppose a and a * are programs that only di.er by \nhaving a di.erent instruction at program point p. Let s1 =(p1,p1) with p1 = p, then for any s2: Trun \n(a, v) . s1 * s2 .Trun (a * ,v) . s1 * s2 5. THE THREE EXAMPLES The following lemma states that if an \nexpression E does not contain the variable X and two stores p and p * di.er at most in their value of \nX then the evaluation of E is the same under both stores. Lemma 6 For any program variable X, expression \nE, and store p, if X E/vars(E) and p \\ X = p * \\ X then [[E]]p = [[E]]p * . Given this lemma, we can \nsee that if a variable is not used in an instruction and two stores di.er only by that variable then \nthe program will behave in the same way: Lemma 7 Suppose we have program points p, p2, p2 * and stores \np1, p1* , p2 , p2 * such that (p1,p) * (p2,p2), (p1* ,p) * (p2* ,p 2* ) and p1 \\ X = p1 * \\ X. If p |= \n\u00acuse(X) then p2 \\ X = p2 * \\ X and p2 = p * 2 Proof If Ip = skip then trivially p2 \\ X = p1 \\ X = p1 \n* \\ X = p * * 2 \\ X and p2 = p +1= p2. If Ip =(Z := E) then p |= \u00acuse(X) implies that X E/vars(E). So \nby Lemma 6: [[E]]p1 = [[E]]p1* . So p2 \\ X = p2 * \\ X as required. Trivially p2 = p +1= p * 2. If Ip \n=(if Z goto p * else p ** ) then p |= \u00acuse(X) implies that X = Z. So p1(Z)=(p1 \\ X)(Z)=(p1 * \\ X)(Z)= \np1* (Z), therefore p2 * = p2. The statement does not a.ect the stores so trivially p2 \\ X = p2 * \\ X. \n. We also note the following lemma that states that if we have a series of instructions that do not de.ne \na variable then the value of the store with respect to that variable does not change. Lemma 8 Consider \na state sequence (p0,p0) ... (pt,pt) in Trun (a, v) such that (pi,pi) * (pi+1,pi+1) for 0 . i<t. If the \ninstruction at each pi does not de.ne a variable X then p0 (X)= pt(X). Lemma 9 Consider a state sequence \n(p0,p0) ... (pt,pt) in Trun (a, v) such that (pi,pi) * (pi+1,pi+1) for 0 . i<t. If expression E is transparent \nat each pi, then [[E]]pi = [[E]]pt. 5.1 Dead Code Elimination The dead code elimination rewrite rule \ndescribed earlier was: x := e =. skip if AX \u00acE[ true U use(x)]. Following De.nition 7 of rewriting, for \nthis rewrite to ap\u00ad ply the model checker must .nd a particular program point p and a substitution that \nmaps x to a particular program variable X and e to a particular expression E. In this case we need to \nprove that an original program a and transformed program a * are equivalent. Below, we assume that Rewrite(a, \na * ,p, x:=e =. skip if AX\u00acE[ true U use(x)]) holds. De.nition 15 Consider C ETpfx (a, v) and C * ETpfx \n(a * ,v) such that: C = a, v . s0 * s1 * ... * st , C * a * ** * = ,v . s0 * s1 * ... * sr in which Vi[0 \n. i . t =. si =(pi,pi)] and Vi[0 . i . r =. s * i =(pi,p i* )]. Then CRC * if and only if t = r and for \nany i, 0 . i . t: 1. pi = pi * 2. [Vj,j <i =. p = pj ]=. pi = pi * and 3. [.j,j <i . p = pj ]=. pi\\X \n= pi* \\X  Base Case: Note that CRC for any C EC, i.e. R is re.exive. In particular it will hold for \npre.xes of length 1. Step Case: Suppose C1 RC1* . The language is deterministic so C1 * C2 and C1 * * \nC2 * for exactly one C2 and C2* . We need to show that C2 RC2* . By De.nition 15: C1 = a, v . (p0,p0) \n* ... * (pt,pt) C1 * = a * ,v . (p0,p0* ) * ... * (pt,pt* ) Let Ip be the instruction in program a at \np and Ip * be the instruction in program a * at p. Suppose p = pi for 0 . i . t. Then by De.nition 15 \neach pi = pi* , so C1 = C1* , implying C2 = C2 * (by Lemma 5) and so C2RC2* . Alternatively, suppose \np = pt. Then Ip =(X := E) and Ip * = skip. So pt+1 = pt[X .* [ E] pt] and pt* +1 \\ X = pt * \\ X = pt \n\\ X. Therefore, pt+1 \\ X = pt* +1 \\ X and C2RC2* . Finally, consider p = pt and p = pk for some k<t. \nBy De.nition 15, pt \\ X = pt * \\ X. Thus, by Lemma 4, Ipt = Ip* t . Now by Lemma 3, sequence pk * cf \npk+1 * cf ... * cf pt is a path in .ow chart Tcf and by condition p |= AX\u00acE( true U use(X)) we can conclude \nthat pi |= \u00acuse(X) for all i with k<i . t. By Lemma 7 this implies pt+1 \\X = pt* +1 \\X and pt+1 = pt* \n+1. So again we have C2RC2* . Equivalence: By R, the program points of computation pre.xes of a and a \n* are the same and thus a terminates if and only if a * terminates. We then need to show that the variable \nY written by a is the same as Y * written by a * . By the side condition of the rewrite we know that \nY = X. So given that both programs terminate at program point pn with their pre.xes related by R, we \nknow that at least pn \\ X = pn * \\ X. Therefore, pn(Y)= pn* (Y) and by Lemma 2 we can conclude that [ \na] = [ a * ] . 5.2 Constant Folding The constant folding rule is: x := v =. x := c if . - A (\u00acdef(v) \nW stmt(v := c) . conlit(c)) Following De.nition 7, for this rewrite to apply, the model checker must \n.nd a particular program point p and a sub\u00adstitution that maps x to a particular program variable X, \nv to a particular program variable V and c to a particular constant C. In this case we need to prove \nthat an original program a and transformed program a * are equivalent. In this case the relation R is \nthe identity relation. That is, we wish to prove that for any length n, a computation pre.x of a of length \nn is equal to a computation pre.x of a * with the same length. Base case: (p0,p0)=(p0* ,p0* ) since vars(a)= \nvars(a * ). Step case: Suppose R (equality) holds between relations C1 and C1 * where: C1 = C1 * = a, \nv . (p0,p0) * ... * (pt,pt) Also suppose that C1 * C2 (by the semantics of a) and C1 * * C2 * (by the \nsemantics of a * ). We wish to prove that C2 = C2* . The proof is split depending on whether p = pt. \nSuppose pt = p. Now by Lemma 5 it follows that C2 = C2* . Suppose pt = p then Ipt =(X := V) and Ip* t \n=(X := C). We know from the side condition and Lemma 3 that the path pt . ... . p0 is a maximal .nite \nbackwards path in the .ow graph. The side condition states that . - pt |= a A (\u00acdef(v) W stmt(v := c) \n. conlit(c)) for a substitution . that maps v to V and c to C. By Lemma 1 we know that there exists i \nsuch that i . t and Ipi =(V := C) and we know pj does not de.ne V for all i<j<t. Thus pi+1(V)= C and \nby Lemma 8: pt(V)= pi+1(V)= C. So the instruction X:= V and X := C will set X to the same value. Therefore \nC2 will be the same as C2* . Equivalence: Since R is the identity, the program points of computation \npre.xes of a and a * are the same, and thus a terminates if and only if a * terminates. Clearly if two \nterminating pre.xes are equal they will have the same value in their .nal stores. So [[a]] = [[a * ]] \nby Lemma 2.  5.3 Code motion/loop invariant hoisting The code motion/loop invariant hoisting rule is: \np : skip =. x:=e q : x:=e =. skip  if p |= A(\u00acuse(x) W node(q)) q |= \u00acuse(x) . . - A ((\u00acdef (x) . \nnode(q)) . trans(e) W node(p)) Following De.nition 7, for this rewrite to apply, the model checker must \n.nd particular program points p and q and a substitution that maps x to a particular program variable \nX and e to a particular program expression E. In this case we need to prove that an original program \na and transformed program a * are equivalent. De.nition 16 Suppose C ETpfx (a, v) and C * ETpfx (a * \n,v) for some v such that C = a, v . (p0,p1) * ... * (pt,pt) C * a ** * = ,v . (p0,p 1* ) * ... * (ptf \n,p t* f ) We then de.ne the R relation on computation pre.xes as: CRC * if and only if t = t * , pi = \np * i for all 0 . i . t and one of the following cases holds: 1. pt = pt * .Vi [0 . i<t . pi E{p, q}] \n=/ 2. pt = pt * ..i [0 . i<t . pi = q . pi = pi * . Vj (i<j<t =pj E{p, q})] . / 3. .i [0 . i<t . pi \n= p . (pt \\ X = pt * \\ X) . (pi \\ X = pi * \\ X) .Vj (i<j<t . pj E{p, q})] =/ The notation CRkC * will \nbe used to indicate that CRC * holds by case k, as de.ned above. Base case: (p0,p0 )=(p * 0,p 0* ) since \nvars(a)= vars(a * ). Also, case 1 of the relation holds trivially. Step case: Suppose C * C2 and C * \n* C2* . Assuming that CRC * we need to show that C2RC2* . The proof is split up into cases depending \non the label pt and each of these is further split up, depending on the case for which CRC * holds. (A) \nSuppose pt /= Ip* t E{p, q}, then Ipt . (i) Suppose CRkC * where k = 1 or 2. By assumption pt = pt* , \nimplying (pt+1 ,pt+1 )=(pt* +1 ,p t* +1 ) by Lemma 5. Thus C2Rk C2 * holds. (ii) Otherwise let 0 . i \n. t be given such that CR3C * holds. The side condition for p must be satis.ed at pi:  pi |= A(\u00acuse(X) \nW node(q)). Since the control .ow model describes all possible computa\u00adtion pre.xes (Lemma 3), the same \nmust hold for the compu\u00ad tation pre.x C. By assumption (A) and (ii) we know that Vj (i<j . t =. pj = \nq), so pj |= \u00acuse(X) must be satis.ed for all i<j . t. In particular this holds for j = t. By assumption \npi \\ X = pi * \\ X so by Lemma 7 we conclude: pt+1 \\ X = pt* +1 \\ X and pt+1 = pt* +1. Thus C2R3C2 * holds \nfor i unchanged. (B) Suppose pt = p, then Ipt =(skip) and Ip* t =(X := E). By the semantics and the induction \nassumption pt+1 = pt + 1= p * t +1 = p * t+1. Also, C1RC1 * implies that pt \\ X = pt * \\ X. Since the \ninstructions only alter X, we can conclude that pt+1 \\ X = pt* +1 \\ X. So C2R3C2 * holds for i = t. \n(C) Otherwise pt = q, so Ipt =(X := E) and Ip* t = skip. By the semantics and the induction assumption \npt+1 = pt + 1= pt * +1= pt* +1. (i) Suppose CR1C * : By Lemma 1 the side condition  . - q |= A ((\u00acdef \n(x) . node(q)) . trans(e) W node(p)) in the rewrite rule implies that any maximal .nite backwards path \neventually ends up in a state pk where pk = p. This contra\u00addicts the assumption C1R1C1* . (ii) Suppose \nCR2C * : By assumption there exists an i such that pi = q. The side condition of q implies, by Lemma \n1: .g [g . t . pg |= node(p) . Vh (g<h . t =. ph |=(\u00acdef (X) . node(q)) . trans(E))] Since by assumption \npj = p for all i . j . t it follows that g<i. Thus for all j with i . j<t, pj |= \u00acdef (X) . node(q). \nNow the induction assumption says that pj = q for all j,i<j <t. This implies that pj |= \u00acdef (X) for \nall j with i<j<t. By Lemma 8 pt* (X)= pi* +1 (X). Further, pj |= trans(E) for all j with i . j<t, and \nby Lemma 9: [[E]]pt = [[E]]pi. Therefore pt* +1(X)= pt* (X) (semantics of Ip* t = skip) = i+1 (X) (argument \nabove) p * = pi+1 (X) (since CR2C * =. pi+1 = pi* +1) = [[E]]pi (semantics of Ipi =(X := E)) = [[E]]pt \n(argument above) = pt+1(X) (semantics of Ipt =(X := E)) Since Ipt only changes the variable X we can \nconclude that pt+1 = pt* +1. Therefore C2R2C2* . (iii) Otherwise there exists an i such that CR3C * holds: \nWe wish to show that C2RC2 * holds by case 2 of R for i = t, i.e. pt+1 = pt* +1. First note that since \nthe statements Ipt and Ip* t a.ect at most the variable X, we can see that pt+1 \\ X = t+1 \\ X. p * Next \nwe need to show that pt+1 (X)= pt* +1 (X). By assumption there exists an i such that pi = p. The side \ncondition of q implies, by Lemma 1: .a . t [pe |= node(p) . V( (a<( . t =. p! |=(\u00acdef (X) . node(q)) \n. trans(E)] Since by assumption pj = p for all i . j . t it follows that a . i. Thus pj |= \u00acdef (X) \n. node(q) for all i<j<t. Now the induction assumption says that pj = q for all i< j<t. This implies that \npj |= \u00acdef (X) for all i<j<t. By Lemma 8 pt* (X)= pi* +1(X). Also, pj |= trans(E) for all i<j<t. By Lemma \n9: [[E]]pt = [[E]]pi+1 . Using the above observations it can be shown that X maps to the same value in \npt+1 and pt* +1: pt+1(X) = [[E]]pt (semantics of Ipt =(X := E)) = [[E]]pi+1 (argument above) = [[E]]pi \n(semantics of Ipi = skip) = [[E]](pi \\ X) (since X E/vars(E)) = [[E]](pi * \\ X) (by CR3 C * ,pi \\ X = \npi * \\ X) = [[E]](pi* ) (since X E/vars(E)) = pi* +1 (X) (semantics of Ip* i =(X:=E)) = t(X) (argument \nabove) p * = pt* +1(X) (semantics of Ip* t = skip) Thus pt+1 = pt* +1 , so case 2 holds for i = t. By \ninduction we conclude that CRC * for any two compu\u00adtation pre.xes C ETpfx (a) and C * ETpfx (a * ) of \nthe same length on the same input v. Equivalence: Again, by R the program points of compu\u00adtation pre.xes \nof a and a * are the same and thus a termi\u00adnates if and only if a * terminates. Now suppose a and a * \nboth terminate on input v. Consider the store at the write statement: Imm =(write Y). Suppose CRC * for \nt = m = I * by case 3, then by the side condition there exists an i>m such that pi = q (since a terminates). \nBut by the seman\u00adtics pi = m = q for all i>t leading to a contradiction. Thus either case 1 or 2 must \nhold. In either case pt = t, p * p * transformation is semantics preserving: [[a]] = [[a * ]]. implying \npt(Y)= t(Y). It follows by Lemma 2 that the  6. DISCUSSION AND FUTURE WORK In this paper we have described \na framework in which tem\u00adporal logic plays a crucial role in the proofs of correctness of classical optimizing \ntransformations performed by a com\u00adpiler. In this framework transformations are speci.ed as rewrite rules \nwith side conditions that are written as tem\u00adporal logic formulas. To prove the correctness of the transformations \nwe had to show that if a transformation is applied it does not change the semantics of the program that \nis, [[a]] = [[a * ]]. The only creative part of the proof is .nding the relation R, but this relation \nis often closely related to the temporal logic side conditions of the transformation. The remainder of \nthe proof is straightforward. It is either routine, as when showing that the program states of computation \npre.xes of a and a * are the same before encountering the transformed program point. Otherwise it deals \ndirectly with the program transformation point. The proof of these cases is dramati\u00adcally simpli.ed since \nwe can assume that the temporal logic side condition holds (otherwise the transformation would not have \nhappened), and this assumption leads almost im\u00admediately to the proof of the case. While the proofs presented \nhere have been done by hand, the nature of the proofs seems well suited to (semi-) auto\u00admated theorem \nproving. The creative step in the proof is to create the relation we wish to prove inductively. The rest \nof the proofs tend to involve mechanically performing case splits and applying a small set of lemmas. \nHowever, even the creative step of providing the relation seems to be closely related with the temporal \nlogic side conditions. An interesting direction of further work would be to discover if the relation \ncould be mechanically created from the side conditions. The programming language on which these transformations \nhave been applied is admittedly very simple. There are very few types of statements and it does not include \nnecessary language features like exceptions and procedures. Limit\u00ading the number of types of statements \nreduces the number of cases in the proofs and this simpli.es their presentation, but adding additional \nstatements does not a.ect the appli\u00adcability of our method. Exceptions and procedures would however, \nrequire changes to the control .ow model and the transition systems used in the proof. The speci.cation \nof the transformations however does not dramatically change. A follow-up paper describing the required \nadjustments is in preparation. The language we have treated is rather like a traditional compiler s intermediate \nlanguage . We anticipate that our method could be used to validate a great many traditional optimizing \ncompiler transformations, e.g., many found in [2] and [20]. This work is part of a larger project to \nstudy declarative methods of specifying optimizations and means of automat\u00adically generating optimizers \nfrom these speci.cations. Here, the speci.cations of optimizing transformations are rewrite rules with \ntemporal logic side conditions that are atomi\u00adcally implemented by a graph rewriting system and model \nchecker [18]. Acknowledgements The Oxford authors would like to thank the Programming Tools Group in \nOxford and in particular Oege de Moor for fruitful discussions about this work and Microsoft Research \nfor its support of this research as part of the Intentional Programming project. 7. REFERENCES [1] S. \nAbramsky and C. Hankin. Abstract Interpretation of Declarative Languages. Ellis-Horwood, 1987. [2] A.V. \nAho and R. Sethi and J.D. Ullman. Compilers: Principles, Techniques, and Tools. Addison Wesley, 1986. \n[3] E.M. Clarke, E.A. Emerson, and A.P. Sistla. Automatic veri.cation of .nite-state concurrent systems \nusing temporal logic speci.cations. ACM Transactions on Programming Languages and Systems (TOPLAS), 8(2):244 \n263, 1986. [4] R. Cleaveland and D. Jackson. Proceedings of First ACM SIGPLAN Workshop on Automated Analysis \nof Software. Paris, France, January 1997. [5] P. Cousot and R. Cousot, Abstract interpretation: A uni.ed \nlattice model for static analysis of programs by construction or approximation of .x-points. In Fourth \nACM Symposium on Principles of Programming Languages, Los Angeles, California, January 1977, pp. 238 \n252, New York: ACM, 1977. [6] P. Cousot, Semantic foundations of program analysis, in S.S. Muchnick and \nN.D. Jones (eds.), Program Flow Analysis: Theory and Applications, chapter 10, pp. 303 342, Englewood \nCli.s, NJ: Prentice Hall, 1981. [7] C.C. Frederiksen. Correctness of classical compiler optimizations \nusing CTL. Unpublished TOPPS report, University of Copenhagen, 2001. http://www.diku.dk/research-groups/ \ntopps/bibliography/2001.html#D-443 [8] Th. Hafer and W. Thomas. Computation tree logic CTL* and path \nquanti.ers in the monadic theory of the binary tree. In Automata, Languages and Programming Proceedings, \nICALP 87, volume 267 of Lecture Notes in Computer Science, pages 267 279. Springer-Verlag, 1987. [9] \nK. Havelund. Stepwise Development of a Denotational Stack Semantics. M.Sc. thesis, University of Copenhagen, \n1984. [10] M. Hecht. Flow analysis of computer programs. North-Holland, 1977. [11] N.D. Jones (ed.), \nSemantics-Directed Compiler Generation. volume 94 of Lecture Notes in Computer Science, Springer-Verlag, \n1980. [12] N.D. Jones. Semantique: Semantic-Based Program Manipulation Techniques. In Bulletin European \nAssociation for Theoretical Computer Science 39:74-83, 1989. [13] N.D. Jones and F. Nielson. Abstract \nInterpretation: a Semantics-Based Tool for Program Analysis.. In Handbook of Logic in Computer Science, \nedited by S. Abramsky, D, Gabbay, T. Maibaum, pages 527 629, Oxford University Press, 1994. [14] J. Knoop \nand O. R\u00a8uthing and B. Ste.en. Optimal Code Motion: Theory and Practice. ACM Transactions on Programming \nLanguages and Systems (TOPLAS), 16(4):1117 1155, 1994. [15] D. Kozen, E.cient code certi.cation. Technical \nReport 98-1661, Computer Science Department, Cornell University, January, 1998. [16] D. Kozen and M. \nPatron. Certi.cation of compiler optimizations using Kleene algebra with tests. In J. Lloyd, V. Dahl, \nU. Furbach, M. Kerber, K.-K. Lau, C. Palamidessi, L. M. Pereira, Y. Sagiv, and P. J. Stuckey, eds, Proceedings \nof the 1st International Conference on Computational Logic (CL2000), Lecture Notes in Arti.cial Intelligence \nvolume 1861, Springer-Verlag, London, July 2000, pp. 568 582. [17] O. Kupferman and A. Pnueli. Once and \nfor all. In Proc. 10th IEEE Symposium on Logic in Computer Science, pages 25 35, San Diego, June 1995. \n[18] D. Lacey and O. de Moor. Imperative program transformation by rewriting. In Proc. 10th International \nConf. on Compiler Construction, volume 1113 of Lecture Notes in Computer Science, pages 52 68. Springer-Verlag, \n2001. [19] R. Milne and C. Strachey A Theory of Programming Language Semantics. Chapman and Hall, 1976. \n[20] S.S. Muchnick. Advanced Compiler Design and Implementation. Morgan Kaufmann, 1997. [21] S.S. Muchnick \nand N.D. Jones (eds.) Program Flow Analysis: Theory and Applications. Englewood Cli.s, NJ: Prentice Hall, \n1981. [22] G. Necula, Proof-carrying code. In 24th ACM Symposium on Principles of Programming Languages, \nParis, France, January 1997, pp. 106 119, New York: ACM, 1997. [23] F. Nielson. A Denotational Framework \nfor Data Flow Analysis. Acta Informatica, 18:265 287, 1982. [24] F. Nielson. Semantic Foundations of \nData Flow Analysis. M.Sc. thesis, Aarhus University, DAIMI PB-131, 1981. [25] F. Nielson, H.R. Nielson \nand C. Hankin. Principles of Program Analysis. Springer-Verlag, 1999. [26] S. S. Pinter and P. Wolper. \nA temporal logic for reasoning about partially ordered computations. In Proc. 3rd ACM Symposium on Principles \nof Distributed Computing, pages 28 37, 1984. [27] A. Podelski, B. Ste.en, M. Vardi. Schloss Ringberg \nSeminar: Model Checking and Program Analysis. Workshop, February 2000, Bavaria. [28] T. Rus and E. Van \nWyk. Using model checking in a parallelizing compiler. Parallel Processing Letters, 8(4):459-471, 1998. \n[29] D.A. Schmidt. Data-.ow analysis is model checking of abstract interpretations. In Proc. of 25th \nACM Symposium on Principles of Programming Languages, ACM, 1998. [30] D.A. Schmidt, B. Ste.en. Program \nanalysis as model checking of abstract interpretations. In Proc. of 5th Static Analysis Symposium, G. \nLevi. ed., Pisa, volume 1503 of Lecture Notes in Computer Science, Springer-Verlag, 1998. [31] Paul A. \nSteckler and Mitchell Wand. Lightweight Closure Conversion. ACM Transactions on Programming Languages \nand Systems, ACM, 19(1):48 86, January 1997. [32] B. Ste.en, A. Cla\u00dfen, M. Klein, J. Knoop, T. Margaria. \nThe Fixpoint Analysis Machine. In Proc. of the 6th International Conference on Concurrency Theory (CONCUR \n95), J. Lee, S. Smolka eds., Philadelphia, Pennsylvania (USA), volume 962 of Lecture Notes in Computer \nScience, Springer-Verlag, pp. 72 87, 1995. [33] G. Winskel. The Formal Semantics of Programming Languages. \nBoston, MA: the MIT Press, 1993. [34] P. Wolper. On the relation of programs and computations to models \nof temporal logic. In Proc. Temporal Logic in Speci.cation, volume 398 of Lecture Notes in Computer Science, \npages 75 123. Springer-Verlag, 1987.  \n\t\t\t", "proc_id": "503272", "abstract": "Many classical compiler optimizations can be elegantly expressed using rewrite rules of form: <i>I</i> &#8658; <i>I</i>&#8242; if <i>&phis;</i>, where <i>I</i>, <i>I</i>&#8242; are intermediate language instructions and <i>&phis;</i> is a property expressed in a temporal logic suitable for describing program data flow. Its reading: If the current program &pi; contains an instruction of form <i>I</i> at some control point <i>p</i>, and if flow condition <i>&phis;</i> is satisfied at <i>p</i>, then replace <i>I</i> by <i>I</i>&#8242;.The purpose of this paper is to show how such transformations may be proven correct. Our methodology is illustrated by three familiar optimizations, dead code elimination, constant folding and code motion. The meaning of correctness is that for any program &pi;, if <i>Rewrite</i>(&pi;, &pi;&#8242;, <i>p</i>,<i>I</i> &#8658; <i>I</i>&#8242; if &phis;) then [[&pi;]] = [[&pi;&#8242;]], i.e. &pi; and &pi;&#8242; have exactly the same semantics.", "authors": [{"name": "David Lacey", "author_profile_id": "81100466049", "affiliation": "University of Oxford", "person_id": "PP14162930", "email_address": "", "orcid_id": ""}, {"name": "Neil D. Jones", "author_profile_id": "81452616043", "affiliation": "University of Copenhagen", "person_id": "PP39048361", "email_address": "", "orcid_id": ""}, {"name": "Eric Van Wyk", "author_profile_id": "81319502946", "affiliation": "University of Oxford", "person_id": "PP40024450", "email_address": "", "orcid_id": ""}, {"name": "Carl Christian Frederiksen", "author_profile_id": "81545829756", "affiliation": "University of Copenhagen", "person_id": "P343125", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/503272.503299", "year": "2002", "article_id": "503299", "conference": "POPL", "title": "Proving correctness of compiler optimizations by temporal logic", "url": "http://dl.acm.org/citation.cfm?id=503299"}