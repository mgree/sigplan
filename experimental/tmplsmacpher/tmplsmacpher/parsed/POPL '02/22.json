{"article_publication_date": "01-01-2002", "fulltext": "\n Adaptive Functional Programming* Umut A. Acar Guy E. Blelloch Robert Harper Computer Science Department \nCarnegie Mellon University Pittsburgh, PA 15213 {umut,blelloch,rwh}@cs.cmu.edu Abstract An adaptive \ncomputation maintains the relationship be\u00adtween its input and output as the input changes. Although various \ntechniques for adaptive computing have been pro\u00adposed, they remain limited in their scope of applicability. \nWe propose a general mechanism for adaptive computing that enables one to make any purely-functional \nprogram adaptive. We show that the mechanism is practical by giving an e.cient implementation as a small \nML library. The library consists of three operations for making a program adaptive, plus two operations \nfor making changes to the input and adapting the output to these changes. We give a general bound on \nthe time it takes to adapt the output, and based on this, show that an adaptive Quicksort adapts its \noutput in logarithmic time when its input is extended by one key. To show the safety and correctness \nof the mechanism we give a formal de.nition of AFL, a call-by-value functional language extended with \nadaptivity primitives. The modal type system of AFL enforces correct usage of the adaptiv\u00adity mechanism, \nwhich can only be checked at run time in the ML library. Based on the AFL dynamic semantics, we formalize \nthe change-propagation algorithm and prove its correctness. Introduction An adaptive program responds \nto input changes by updat\u00ading its output while only re-evaluating those portions of the program a.ected \nby the change. Adaptive programming is useful in situations where input changes lead to relatively small \nchanges in the output. In limiting cases one cannot avoid a complete re-computation of the output, but \nin many cases the results of the previous computation may be re-used to obtain the updated output more \nquickly than a complete * This research was supported in part by NSF grants CCR-9706572, CCR-0085982, \nand CCR-0122581. Permission to make digital or hard copies of all or part of this work for personal or \nclassroom use is granted without fee provided that copies are not made or distributed for pro.t or commercial \nadvantage and that copies bear this notice and the full citation on the .rst page. To copy otherwise, \nto republish, to post on servers or to redistribute to lists, requires prior speci.c permission and/or \na fee. POPL 02, Jan. 16-18, 2002 Portland, OR USA 2002 ACM ISBN 1-58113-450-9/02/01...$5.00 re-evaluation. \nFor example, as we shall see below, an adap\u00adtive version of Quicksort takes expected logarithmic time \nto adapt its output when its input list is extended by one key. This is an improvement by a linear factor \nover simply re-evaluating the sort for the changed inputs. In this paper we propose a general mechanism \nfor adap\u00adtive programming. Our proposed mechanism extends call\u00adby-value functional languages with a small \nset of primitives to support adaptive programming. Apart from requiring that the host language be purely \nfunctional, we make no other restriction on its expressive power. In particular our mechanism is compatible \nwith the full range of e.ect-free constructs found in ML. Our proposed mechanism has these strengths: \n Generality: It applies to any purely functional pro\u00adgram. The programmer can build adaptivity into an \napplication in a natural and modular way.  Flexibility: It enables the programmer to control the amount \nof adaptivity. For example, a programmer can choose to make only one portion or aspect of a system adaptive, \nleaving the others to be implemented conven\u00adtionally.  Simplicity: It requires small changes to existing \ncode. For example, the adaptive version of Quicksort pre\u00adsented in the next section requires only minor \nchanges to the standard implementation.  E.ciency: The mechanism admits a simple imple\u00admentation and \nyields e.cient adaptivity. For example, the adaptive version of Quicksort updates the output in expected \nO(log n) time upon extension to the input.  Our adaptivity mechanism is based on the idea of a mod\u00adi.able \nreference (or modi.able, for short) and three opera\u00adtions for creating (mod), reading (read), and writing \n(write) modi.ables. A modi.able allows us to record the depen\u00addence of one computation on the value of \nanother. A modi\u00ad.able reference is essentially a write-once reference cell that records the value of \nan expression whose value may change as a (direct or indirect) result of changes to the inputs. Any expression \nwhose value can change must store its value in a modi.able reference; such an expression is said to be \nchange\u00adable. Expressions that are not changeable are said to be sta\u00adble; stable expressions are not associated \nwith modi.ables. Any expression that depends on the value of a changeable expression must express this \ndependence by explicitly read\u00ading the contents of the modi.able storing the value of that changeable \nexpression. This establishes a data dependency between the expression reading that modi.able, called \nthe reader, and the expression that determines the value of that modi.able, the writer. Since the value \nof the modi.able may change as a result of changes to the input, the reader must itself be deemed a changeable \nexpression. This means that a reader cannot be considered stable, but may only appear as part of a changeable \nexpression whose value is stored in some other modi.able. By choosing the extent to which modi.ables \nare used in a program, the programmer can control the extent to which it is able to adapt to change. \nFor example, a programmer may wish to make a list manipulation program adaptive to insertions into and \ndeletions from the list, but not under changes to the individual elements of the list. This can be represented \nin our framework by making only the tail elements of a list adaptive, leaving the head elements sta\u00adble. \nHowever, once certain aspects are made changeable, all parts of the program that depend on those aspects \nare, by implication, also changeable. The key to adapting the output to change of input is to record \nthe dependencies between readers and writers that arise during the initial evaluation. These dependencies \nmay be maintained as a graph in which each node represents a modi.able, and each edge represents a read \nwhose source is the modi.able being read and whose target is the modi.\u00adable being written. Also, each \nedge is tagged with the corre\u00adsponding reader. Whenever the source modi.able changes, the new value of \nthe target is determined by re-evaluating the associated reader. It is not enough, however, to maintain \nonly this depen\u00addency graph connecting readers to writers. It is also essen\u00adtial to maintain an ordering \non the edges and keep track of which edges (reads) are within the dynamic scope of which other edges. \nWe call this second relationship the contain\u00adment hierarchy. The ordering among the edges enables us \nto re-evaluate readers in the same order as they were evaluated in the initial evaluation. The containment \nhierarchy enables us to identify and remove edges that become obsolete. This occurs, for example, when \nthe result of a conditional inside a reader takes a di.erent branch than the initial evaluation. The \ndi.culty is maintaining the ordering and containment information during re-evaluation. We show how to \nmain\u00adtain this information e.ciently using time-stamps and an order-maintenance algorithm of Dietz and \nSleator [4]. Related Work Several researchers have studied approaches that are similar to what we call \nadaptive programming. The idea of using de\u00adpendency graphs for incremental updates was introduced by \nDemers, Reps and Teitelbaum [3] in the context of attribute grammars. Reps then showed an algorithm to \npropagate a change optimally [16], and Hoover generalized the approach outside the domain of attribute \ngrammars [9]. A crucial di.erence between this previous work and ours is that the previous work is based \non static dependency graphs. Al\u00adthough they allow the graph to be changed by the modify step, the propagate \nstep (i.e., the propagation algorithm) can only pass values through a static graph. This severely limits \nthe types of adaptive computations that the technique handles [14]. Another di.erence is that they don \nt have the notion of forming the initial graph/trace by running a com\u00adputation, but rather assume that \nit is given as input (often it naturally arises from the application). Yellin and Strom use the dependency \ngraph ideas within the INC language [18], and extend it by having incremental computations within each \nof its array primitives. Since INC does not have re\u00adcursion or looping, however, the dependency graphs \nremain static. Another approach to incremental/adaptive computa\u00adtions is function caching [14, 13]. In \nfunction caching, a computation reuses cached results from earlier evaluations whenever appropriate. \nThus, one must run the computa\u00adtion from scratch to identify the part of the computation that does not \nchange. In contrast, in our approach, an input change pinpoints the parts of the computation that need \nto be re-evaluated. Function caching therefore is bad at handling deep modi.cations. We conjecture, for \nexam\u00adple, that with function caching no algorithm can update a sorted linked-list in less than linear \nexpected time. This is because the inserted element is expected to end up half way down the list, and \nfunction caching will always recreate the part of the list ahead of the inserted element. There are two \nother problems with function caching. First it can be hard to e.ectively check for equality of arguments \nfor the pur\u00adpose of matching elements in the cache. This is particularly true if the inputs are functions \nthemselves, possibly with captured environments. Second, for e.ciency it is critical to evict elements \nfrom the cache. The suggested methods we have seen to decide when and what to evict seem ad-hoc, although \nLiu and Teitelbaum have made some progress us\u00ading automatic program transformation techniques to decide \nwhat to cache [11, 10]. In spite of these problems, function caching might have some advantages over \nour method for shallow modi.cations. We expect that these techniques can be integrated to further improve \nperformance in certain situations. Other approaches are based on various forms of partial evaluation \n[8, 17]. These approaches are arguably cleaner than the function caching approach (they don t have the \nis\u00adsues with equality of inputs or deciding when to evict from the cache), but are even more limited \nin the type of adap\u00adtivity they allow. Ramalingam and Reps wrote an excellent bibliography summarizing \nother work on incremental com\u00adputation [15]. 3 Overview of the Paper In Section 4 we illustrate the main \nideas of adaptive func\u00adtional programming in an algorithmic setting. We .rst de\u00adscribe how to implement \nan adaptive form of Quicksort in Standard ML based on the interface of a module implement\u00ading the basic \nadaptivity mechanisms. We then describe the change-propagation algorithm that lies at the heart of the \nmechanism and establish an upper bound for its running time. Using this bound, we then prove the expected \nO(log n) time bound for adaptive Quicksort to accommodate an ex\u00adtension to its input. We .nish by brie.y \ndescribing the implementation of the mechanism in terms of an abstract ordered list data structure. This \nimplementation requires less than 100 lines of Standard ML code. In Section 5 we de.ne an adaptive functional \nprogram\u00adming language, called AFL, which is an extension of a simple call-by-value functional language \nwith adaptivity primitives. The static semantics of AFL enforces properties that can only be enforced \nby run-time checks in our ML library. The dynamic semantics of AFL is given by an evaluation rela\u00ad signature \nADAPTIVE = sig type a mod type a dest type changeable val mod: ( a * a -> bool) -> ( a dest -> changeable) \n-> a mod val read: a mod * ( a -> changeable) -> changeable val write: a dest * a -> changeable val init: \nunit -> unit val change: a mod * a -> unit val propagate: unit -> unit end Figure 1: Signature of the \nadaptive library. tion that maintains a record of the adaptive aspects of the computation, called a trace, \nwhich is used by the change propagation algorithm. In Section 6 we present the change propagation algo\u00adrithm \nin the framework of the dynamic semantics of AFL. The change propagation algorithm interprets a trace \nto de\u00adtermine the correct order in which to propagate changes, and to determine which expressions need \nto be re-executed. The trace also records the containment structure of the com\u00adputation, which is updated \nduring change propagation. Us\u00ading this presentation we give a proof of correctness of the change propagation \nalgorithm stating that change propa\u00adgation yields essentially the same result as a complete re\u00adexecution \non the changed inputs. We note that we had originally thought that incorpo\u00adrating an adaptivity mechanism \nin ML would require the involvement of a compiler. Working out the semantics of AFL led to the particular \nmechanism we describe and its simple implementation as an ML library. 4 A Framework for Adaptive Computing \nWe give an overview of our adaptive framework based on our ML library and an adaptive version of Quicksort. \nThe ML library. The signature of our adaptive library for ML is given in Figure 1. The library provides \nfunctions to create (mod), to read from (read), and to write to (write) modi.ables, as well as meta-functions \nto initialize the li\u00adbrary (init), change input values (change) and propagate changes to the output (propagate). \nThe meta-functions are described later in this section. The library distinguishes be\u00adtween two handles \nto each modi.able: a source of type a mod for reading from, and a destination of type a dest for writing \nto. When a modi.able is created, correct usage requires that it only be accessed as a destination until \nit is written, and then only be accessed as a source.1 All change\u00adable expressions have type changeable, \nand are used in a destination passing style they do not return a value, but rather take a destination \nto which they write a value. Cor\u00adrect usage requires that a changeable expression ends with a write we \nde.ne ends with more precisely when we dis\u00adcuss time stamps. The destination written will be referred \n1 The library does not enforce this restriction statically, but can enforce it with run-time checks. \nIn the following discussion we will use the term correct usage to describe similar restrictions in which \nrun\u00adtime checks are needed to check correctness. The language described in Section 5 enforces all these \nrestrictions statically using a modal type system. to as the target destination. The type changeable \nhas no interpretable value. The mod takes two parameters, a conservative compari\u00adson function and an \ninitializer. A conservative comparison function returns false when the values are di.erent but may return \ntrue or false when the values are the same. This function is used by the change-propagation algorithm \nto avoid unnecessary propagation. The mod function creates a modi.able and applies the initializer to \nthe new modi.\u00adable s destination. The initializer is responsible for writing the modi.able. Its body \nis therefore a changeable expres\u00adsion, and correct usage requires that the body s target match the initializer \ns argument. When the initializer completes, mod returns the source handle of the modi.able it created. \nThe read takes the source of a modi.able and a reader, a function whose body is changeable. The read \naccesses the contents of the modi.able and applies the reader to it. Any application of read is itself \na changeable expression since the value being read could change. If a call Ra to read is within the dynamic \nscope of another call Rb to read, we say that Ra is contained within Rb. This relation de.nes a hierarchy \non the reads, which we will refer to as the containment hierarchy (of reads). Making an Application Adaptive. \nThe transformation of a non-adaptive program to an adaptive program involves two steps. First, the data \nstructures are made modi.able by placing desired elements in modi.ables. Second, the original program \nis updated by making the reads of modi.ables ex\u00adplicit and placing the results of each expression that \ndepends on a modi.able into another modi.able. This means that all values that directly or indirectly \ndepend on modi.able inputs are placed in modi.ables. As an example, consider the code for a standard \nQuick\u00adsort, qsort, and an adaptive Quicksort, qsort , as shown in Figure 2. To avoid linear-time concatenations, \nqsort uses an accumulator to store the sorted tail of the input list. The transformation is done in two \nsteps. First, we make the lists modi.able by placing the tail of each list ele\u00adment into a modi.able \nas shown in lines 1,2,3 in Figure 2. The resulting structure, a modi.able list, allows the user to insert \nand delete items to and from the list. Second, we change the program so that the values placed in modi.ables \nare accessed explicitly via a read. The adaptive Quicksort uses a read (line 21) to determine whether \nthe input list l is empty and writes the result to a destination d (line 23). This destination belongs \nto the modi.able that is created by a call to mod (through modl) in line 28 or 33. These mod\u00adi.ables \nform the output list, which now is a modi.able list. The function filter is similarly transformed into \nan adap\u00adtive one, filter (lines 6-18). The modl is de.ned to take an initializer and pass it to the mod \nwith a constant-time, conservative comparison function for lists. The comparison function returns true, \nif and only if both lists are NIL and returns false otherwise. This comparison function is suf\u00ad.ciently \npowerful to prove the O(log n) bound for adaptive Quicksort. Adaptivity. An adaptive computation allows \nthe program\u00admer to change input values and update the result. This process can be repeated as desired. \nThe library provides the meta-function change to change the value of a modi.\u00adable and the meta-function \npropagate to propagate these changes to the output. Figure 3 illustrates an example. The 1 datatype \na list = 1 datatype a list = 2 NIL 2 NIL 3 | CONS of ( a * a list) 3 | CONS of ( a * a list mod) 4 4 \nfun modl f = mod (fn (NIL,NIL) => true 5 5 | => false) f 6 fun filter f l = 6 fun filter f l = 7 let \n7 let 8 fun filt(l) = 8 fun filt(l,d) = read(l, fn l => 9 case l of 9 case l of 10 NIL => NIL 10 NIL \n=> write(d, NIL) 11 | CONS(h,r) => 11 | CONS(h,r) => 12 if f(h) then 12 if f(h) then write(d, 13 CONS(h, \nfilt(r)) 13 CONS(h, modl(fn d => filt(r,d)))) 14 else 14 else 15 filt(r) 15 filt(r, d)) 16 in 16 in 17 \nfilt(l) 17 modl(fn d => filt(l, d)) 18 end 18 end 19 fun qsort(l) = 19 fun qsort (l) = 20 let 20 let \n21 fun qs(l,rest) = 21 fun qs(l,rest,d) = read(l, fn l => 22 case l of 22 case l of 23 NIL => rest 23 \nNIL => write(d, rest) 24 | CONS(h,r) => 24 | CONS(h,r) => 25 let 25 let 26 val l = filter (fn x => x \n< h) r 26 val l = filter (fn x => x < h) r 27 val g = filter (fn x => x >= h) r 27 val g = filter (fn \nx => x >= h) r 28 val gs = qs(g,rest) 28 val gs = modl(fn d => qs(g,rest,d)) 29 in 29 in 30 qs(l,CONS(h,gs)) \n30 qs(l,CONS(h,gs),d) 31 end 31 end) 32 in 32 in 33 qs(l,NIL) 33 modl(fn d => qs(l,NIL,d)) 34 end 34 \nend Figure 2: The complete code for non-adaptive (left) and adaptive (right) versions of Quicksort. \n1 fun newElt(v) = modl(fn d => write(d,v)) 2 fun fromList(nil) = 3 let val m = newElt(NIL) 4 in (m,m) \n5 end 6 | fromList(h::r) = 7 let val (l,last) = fromList(r) 8 in (newElt(CONS(h,l)),last) 9 end 10 fun \ntest(lst,v) = 11 let 12 val = init() 13 val (l,last) = fromList(lst) 14 val r = qsort (l) 15 in 16 (change(last,CONS(v,newElt(NIL))); \n17 propagate(); 18 r) 19 end Figure 3: Example of changing input and change propaga\u00adtion for Quicksort. \nfromList function converts a list to a modi.able list, return\u00ading both the modi.able list and its last \nelement. The test function .rst performs an initial evaluation of the adaptive Quicksort by converting \nthe input list lst to a modi.able list l and sorting it into r. It then changes the input by adding a \nnew key v to the end of l. To update the output r, test calls propagate. The update will result in a \nlist identical to what would have been returned if v was added to the end of l before the call to qsort. \nIn general, any number of inputs could be changed before running propagate. Augmented Dependency Graphs. \nThe crucial issue is to support change propagation e.ciently. To do this, an adap\u00adtive program, as it \nevaluates, creates a record of the adaptive activity. It is helpful to visualize this record as a dependency \ngraph augmented with additional information regarding the containment hierarchy and the evaluation order \nof reads. In such a dependency graph, each node represents a modi.able and each edge represents a read. \nAn evaluation of mod adds a node, and an evaluation of read adds an edge to the graph. In a read, the \nnode being read becomes the source, and the target of the read (the modi.able that the reader .nished \nby writing to) becomes the target. We also tag the edges with the reader function. To operate correctly, \nthe change-propagation algorithm needs to know the containment hierarchy of reads. To main\u00adtain this \ninformation, we tag each edge and node with a time stamp, which are generated by the mod and read. All \nexpres\u00adsions are evaluated in a time range (ts,te) and time-stamps generated by the expression are allocated \nsequentially within that range, i.e., each generated time stamp is greater than the previous one, but \nless than the end of the time range. The time stamp of an edge is generated by the corresponding read, \nbefore the reader is evaluated, and the time stamp of a node is generated by the mod after the initializer \nis evaluated (the time stamp of a node corresponds to the time it was initialized). Correct usage of \nthe library requires that the order of time stamps is independent of whether the write or mod generate \nthe time stamp for the corresponding node. This is what we mean by saying that a changeable expression \nmust end with a write to its target. The time stamp of an edge is called its start time and the time \nstamp of the target of the edge is called the edge s stop time. The start and the stop time of the edge \nde.ne the time span of the edge. We note that the time span can be used to identify the containment relationship \nof reads. In particular, a read Ra is contained in a read Rb if and only Figure 4: The adg for an application \nof filter to the func\u00adtion fn x => x>2 and the input modi.able list 2::3::nil. The output is the modi.able \nlist 3::nil. (*) (*) (*) l0 l1 l2 li li (0.1) (0.2) (0.3) (. . .) l3 l4 (0.5) (0.4) if the start time \nof the edge associated with Ra is within the time span of the edge associated with Rb. For now, we will \nrepresent time stamps with real numbers, and as\u00adsume that top-level expressions are evaluated in the \nrange (0.0, 1.0). Subsequently, we will show how the Dietz-Sleator Order-Maintenance Algorithm can be \nused to maintain time stamps e.ciently [4]. We de.ne an augmented dependency graph (adg) as a DAG in \nwhich each edge has an associated reader and time stamp, and each node has an associated value and time \nstamp.2 We say that a node (and corresponding modi.able) is an input if it has no incoming edges. An \nexample should help make the ideas clear. Con\u00adsider the adaptive .lter function filter shown in Fig\u00adure \n2. The function takes another function f and a mod\u00adi.able list l as parameters and outputs a modi.able \nlist that contains the items of l satisfying f. Figure 4 shows the dependency graph for an evaluation \nof filter with the function (fn x => x > 2) and a modi.able input list of 2::3::nil. The output is the \nmodi.able list 3::nil. Al\u00adthough not shown in the .gure, each edge is also tagged with a reader. In this \nexample, all edges have an instance of reader (fn l => case l of ...) (lines 8-15 of qsort in Figure \n2). The time stamps for input nodes are not relevant, and are marked with an asterisk in Figure 4. Change \nPropagation. Given an augmented dependency graph and a set of changed input modi.ables, the change\u00adpropagation \nalgorithm updates the adg and the output by propagating changes in the adg. We say that an edge, or corresponding \nread, is invalidated if the source of the edge changes value. We say that an edge is obsolete if it is \ncon\u00adtained within an invalidated edge. Figure 5 de.nes the change-propagation algorithm. The algorithm \nmaintains a Priority Queue of invalidated edges. The queue is prioritized on the time stamp of each edge, \nand is initialized with the out-edges of the changed input values. Each iteration of the while loop processes \none inval\u00adidated edge, and we call the iteration an edge update. The update re-evaluates the associated \nreader. This makes any code that was within the reader s dynamic scope obsolete. A key aspect of the \nalgorithm is that when an edge is updated, all nodes and edges that are contained within that edge are \ndeleted from both the graph and queue. This prevents the reader of an obsolete edge from being re-evaluated. \nEval\u00aduating such a reader on a changed input may incorrectly 2 We do not formalize adgs more precisely \nhere since we view them as an implementation of a cleaner notion of traces, which we formalize in Section \n5. Propagate Changes I is the set of changed inputs (V, E)= G is an adg V 1 Q = v.I outEdges(v) 2 while \nQ is not empty 3 e = deleteMin(Q) 4 (Ts,Te)= timeSpan(e) 5 V = V -{v . V |Ts <T (v) <Te} 6 E' = {e' . \nE|Ts <T (e') <Te} 7 E = E - E' 8 Q = Q - E' 9 v' = apply(reader(e), val(src(e))) in time (Ts,Te) ' 10 \nif v = val(target(e)) then 11 val(target(e)) = v' 12 Q = Q + outEdges(target(e)) Figure 5: The change-propagation \nalgorithm. update a modi.able, incorrectly raise an exception, or even not terminate. After the reader \nis re-evaluated we check if the value of the target has changed (line 10) by using the conservative comparison \nfunction passed to mod. If it has changed, we add the out-edges of the target to the queue to propagate \nthat change. As an example, consider an initial evaluation of filter whose dependency graph is shown \nin Figure 4. Now, sup\u00adpose we change the modi.able input list from 2::3::nil to 2::4::7::nil by creating \nthe modi.able list 4::7::nil and changing the value of modi.able l1 to this list. The leftmost frame \nin Figure 6 shows the input change. Now, we run the change-propagation algorithm to update the output. \nFirst, we insert the sole outgoing edge of l1, namely (l1,l3), into the queue. Since this is the only \n(hence, the earliest) edge in the queue, we remove it from the queue and establish the cur\u00adrent time-span \nas (0.2)-(0.5). Next, we delete all the nodes and edges contained in this edge from the adg and from \nthe queue (which is empty) as shown by the middle frame in Figure 6. Then we redo the read by re-evaluating \nthe reader (fn l => case l of ...) (8-15 in Figure 2) in the cur\u00adrent time span (0.2)-(0.5). The reader \nwalks through the modi.able list 4::7::nil as it .lters the items and writes the head of the result list \nto l3, as shown in the rightmost frame in Figure 6. This creates two new edges, which are given the time \nstamps, (0.3), and (0.4). The targets of these edges, l7 and l8, are assigned the time stamps, (0.475), \nand (0.45), matching the order that they were initialized (these time stamps are otherwise chosen arbitrarily \nto .t in the range (0.4)-(0.5)). Implementing Change Propagation E.ciently. The change-propagation algorithm \ndescribed above can be implemented e.ciently using a standard representation of graphs, a standard priority-queue \nalgorithm, and an Order-Maintenance Algorithm for time stamps. The implementation of the adg needs to \nsupport deleting an edge, a node, and .nding the outgoing edges of a node. An adjacency list representation \nin which the edges of a node are maintained in a doubly-linked list implements these operations in constant \ntime. The algorithm also needs to identify all the edges between two time stamps so they can be deleted. \nThis can be implemented with a time-ordered, doubly-linked list of all edges. Inserting, deleting, and \n.nd-next all take constant time per edge.  Figure 6: Snapshots of the adg during change propagation. \nThe priority queue should support addition, deletion, and delete-minimum operations e.ciently. Standard \nbalanced-tree based priority-queue algorithms perform these operations in logarithmic time. This is su.cient \nfor our purposes and any of these algorithms can be used to implement priority queues. A more interesting \nquestion is how to implement time stamps e.ciently. To do this, we require e.cient support for four operations: \ncompare two time stamps, insert a new time stamp after a given time stamp, delete a time stamp, and retrieve \nthe next time stamp (used in deleting the time\u00adspan of an edge). Using real numbers is not an e.cient \nsolution, because, in change propagation, an arbitrary num\u00adber of new time stamps could be inserted between \ntwo .xed time stamps. This requires arbitrary precision real numbers, which are costly. A simple alternative \nto real numbers is to have all the time stamps ordered in a list. To insert or delete a time stamp, we \nsimply insert it into the list or delete it from the list. To compare two time stamps, we compare their \npositions in the list the time stamp closer to the be\u00adginning of the list is smaller. This comparison \noperation, however, can take linear time in the length of the list. A more e.cient approach is to assign \nan integer rank to each time stamp in the list such that nodes closer to the beginning of the list have \nsmaller ranks. This enables constant time comparisons by comparing the ranks. The insertion algo\u00adrithm \nthen may have to do some re-ranking to .nd space to insert an integer between two adjacent integers. \nDietz and Sleator give two e.cient algorithms for this problem, which is known as the Order-Maintenance \nProblem [4]. The .rst algorithm is a simple algorithm that performs all operations in amortized constant \ntime, the second more sophisticated algorithm achieves worst case constant time. Performance of Change \nPropagation. We show an upper bound on the running time of change propagation. As dis\u00adcussed above, we \nassume an adjacency list representation for augmented dependency graphs together with a time-ordered \nlist of edges, a priority queue that can support insertions, deletions, and remove-minimum operations \nin logarithmic time, and an order-maintenance structure that supports in\u00adsert, delete, compare, and .nd-next \noperations in constant time. We de.ne several performance measures for change prop\u00adagation. Consider \nrunning the change-propagation algo\u00adrithm, and let I denote the set of all invalidated edges. Of these \nedges, some of them participate in an edge update, whereas some become obsolete and are deleted before \npar\u00adticipating. We refer to the set of updated edges as Iu. For an updated edge e . Iu, let |e| denote \nthe re-evaluation time (complexity) of the reader associated with e assuming that mod, read, write, take \nconstant time, and let ||e|| denote the number of time stamps created during the initial evaluation of \ne. Let q be the maximum size of the priority queue at any time during the algorithm. Theorem 1 bounds \nthe time of a propagate step. Theorem 1 (Propagate) Change propagation takes time O e.Iu (|e| + ||e||) \n+ |I| log q . Proof: The time for propagate can be partitioned into 4 items: (1) re-evaluation of readers, \n(2) creation of time stamps, (3) deletion of time stamps and contained edges, and (4) insertion to and \ndeletions from the priority queue. s Re-evaluation of the readers takes |e| time. The num\u00ad e.Iu ber of \ntime stamps created during the re-evaluation of a reader is no greater than the time it takes to re-evaluate \nthe reader. Since creating one time stamp takes constant time, s creating time stamps takes O( |e|) time. \nDetermin\u00ad e.Iu ing each time stamp to delete, deleting the time stamp and the corresponding node or edge \nfrom the adg and the time\u00adordered doubly-linked edge list takes constant time. Thus s total time for \nthese deletions is ( ||e||). e.Iu Finally, each edge is added to the priority queue once and deleted \nfrom the queue once, thus the time for maintaining the priority queue is O(|I| log q). The total time \nis the sum of these terms. Performance of Adaptive Quicksort. We now analyze the propagate time for Quicksort \nwhen the input list is modi.ed by adding a new key at the end. The analysis is based on the bound given \nin Theorem 1. Theorem 2 Change propagation updates the output of adaptive Quick\u00adsort in O(log n) time \nafter the input list of length n is ex\u00adtended with a new key at the end. Proof: The proof is by induction \non the height h of a call tree representing just the calls to qs. When the input is extended, the value \nof the last element ln of the list is changed from NIL to CONS(v,ln+1), where the value of ln+1 is NIL \nand v is the new key. The induction hypothesis is that in change propagation on an input tree of height \nh, the number of invalidated reads is at most 2h (|I|= 2h and Iu = I), each reader takes constant time \nto re-evaluate (.e . I, |e| = O(1)), the time span of a reader contains no other time stamps (.e . I, \n||e|| = 0), and the maximum size of the priority queue is 4 (q = 4). In the base case, we have h = 1, \nand the call tree cor\u00adresponds to an evaluation of qs with an empty input list. The only read of ln is \nthe outer read in qs. The change propagation algorithm will add the corresponding edge to the priority \nqueue, and then update it. Now that the list has one element, the reader will make two calls to filter \nand two calls to qs both with empty input lists. This takes constant time and does not add any edges \nto the priority queue. There are no time stamps in the time span of the re-evaluated edge and the above \nbounds hold. For the inductive case assume that the hypothesis holds for trees up to height h - 1, and \nconsider a tree with height h> 1. Now, consider the change propagation starting with the root call to \nqs. The list has at least one element in it, therefore the initial read does not read the tail ln. The \nonly two functions that use the list are the two calls to .l\u00adter , and these will both read the tail \nin their last recursive call. Therefore, during change propagation these two reads (edges) are invalidated, \nwill be added to the queue, and then updated. Any other edges that these updates add to the queue will \nhave start times after the start times of these edges. Re-evaluating the reader of one of the two edges \nwill rewrite NIL and therefore not change its target. Re\u00adevaluating the other will change its target \nfrom NIL to the value CONS(v,ln+1), and therefore extend the correspond\u00ading list. Re-evaluating both \nreaders takes constant time and the update deletes no time stamps. Only one of the two recursive calls \nto qs has any changed data, and that one has its input extended with one element. Since the call tree \nof the qs has depth at most d - 1, the induction hypothesis applies. Thus, |e| = O(1) and ||e|| = 0 for \nall invalidated edges. Furthermore, the total number of invalidated edges is |I|= 2(d - 1)+2 = 2d and \nall edges are re-evaluated (Iu = I). To see that q = 4, note that the queue contains edges from at most \n2 di.erent qs calls and there are at most 2 edges invalidated from each call. It is known that the expected \nheight of the call tree is O(log n) (expectation is over all inputs). Thus we have: E[|I|]= O(log n), \nI = Iu, q = 4, and .e . I, |e| = O(1), ||e|| = 0. Thus by taking the expectation of the formula given \nin Theorem 1 and plugging in these values gives expected O(log n) time for propagate. The ML Implementation. \nWe present an implementation of our adaptive mechanism in ML. It uses a library for or\u00addered lists, which \nis an instance of the Order-Maintenance Problem, and a standard priority queue. In the ordered-list interface \n(shown in Figure 7), spliceOut deletes all time stamps between two given time stamps and isSplicedOut \nreturns true if the time stamp has been deleted and false otherwise. Figure 8 shows the code for the \nML implementation. The implementation di.ers somewhat from the algorithm described earlier, but the asymptotic \nperformance remains signature ORDERED LIST = sig type t val init : unit -> t (* Initialize *) val compare: \nt*t -> order (* Compare two nodes *) val insert : t ref -> t (* Insert a new node *) val spliceOut: t*t \n-> unit (* Splice interval out *) val isSplicedOut: t -> bool (* Is the node spliced? *) end Figure 7: \nThe signature of an ordered list. the same. The edge and node types correspond to edges and nodes in \nthe adg. The reader and time-span are represented explicitly in the edge type, but the source and destination \nare implicit in the reader. In particular the reader starts by reading the source, and ends by writing \nto the desti\u00adnation. The node consists of the corresponding modi.able s value (value), its out-edges \n(outEdges), and a write function (wrt) that implements writes or changes to the modi.able. A time stamp \nis not needed since edges keep both start and stop times. The currentTime is used to help generate the \nsequential time stamps, which are generated for the edge on line 37 and for the node on line 29 by the \nwrite operation. Some of the tasks assigned to the change-propagate loop in Figure 5 are performed by \nthe write operation in the ML code. This includes the functionality of lines 10 12 in Fig\u00adure 5, which \nare executed by lines 20 25 in the ML code. Another important di.erence is that the deletion of con\u00adtained \nedges is done lazily. Instead of deleting edges from the Queue and from the graph immediately, the time \nstamp of the edge is marked as invalid (by being removed from the ordered-list data structure), and is \ndeleted when it is next encountered. This can be seen in line 55. We note that the implementation given \ndoes not include su.cient run-time checks to verify correct usage . For ex\u00adample, the code does not verify \nthat an initializer writes its intended destination. The code, however, does check for a read before \nwrite. 5 An Adaptive Functional Language In the .rst part of the paper, we described an adaptivity mechanism \nin an informal setting. The purpose was to in\u00adtroduce the basic concepts of adaptivity and show that \nthe mechanism can be implemented e.ciently. We now turn to the question of whether the proposed mechanism \nis sound. To this end, we present a small, purely functional language with primitives for adaptive computation, \ncalled AFL. AFL ensures correct usage of the adaptivity mechanism statically by using a modal type system \nand employing implicit des\u00adtination passing. The adaptivity mechanisms of AFL are similar to those of \nthe adaptive library presented in Section 4. The chief di.er\u00adence is that the target of a changeable \nexpression is implicit in AFL. Because of this, AFL also includes two forms of func\u00adtion type, one for \nfunctions whose body is stable, and one for functions whose body is changeable. The former corre\u00adsponds \nto the standard function type found in any functional language. The latter is included to improve e.ciency \nby al\u00adlowing such functions to share their (implicit) target with the caller. This avoids the need to \nallocate a modi.able for the result of a procedure call, and is crucial to supporting the tail recursion \noptimization in changeable mode. AFL does not include analogues of the meta-operations 1 structure Adaptive \n:> ADAPTIVE = struct 2 type changeable = unit 3 exception unsetMod 4 type edge = {reader: (unit -> unit), \n5 timeSpan: (Time.t * Time.t)} 6 type a node = {value : (unit -> a) red, 7 wrt : ( a -> unit) ref, 8 \noutEdges : edge list ref} 9 type a mod = a node 10 type a dest = a node 11 val currentTime = ref(Time.init()) \n12 val PQ = ref(Q.empty) (* Priority queue *) 13 fun init() = (currentTime := Time.init(); PQ := Q.empty) \n 14 funmodcmpf=let 15 val value = ref(fn() => raise unsetMod) 16 val wrt = ref(fn(v) => raise unsetMod) \n17 val outEdges = ref(nil) 18 valm= {value=value, wrt=wrt, outEdges=outEdges} 19 funchangetv= 20 (if \ncmp(v,(!value)()) then () 21 else 22 (value := (fn() => v); 23 List.app (fn x => PQ := Q.insert(x,!PQ)) \n24 (!outEdges); 25 outEdges := nil); 26 currentTime := t) 27 fun write(v) = 28 (value := (fn() => v); \n29 Time.insert(currentTime); 30 wrt:= change(!currentTime)) 31 val = wrt := write 32 in 33 f(m); m 34 \nend 35 fun write({wrt, ...} : a dest, v) = (!wrt)(v) 36 fun read({value, outEdges, ...} : a mod, f) \n= let 37 val start = Time.insert(currentTime) 38 fun run() = 39 (f((!value)()); 40 outEdges := {reader=run, \n41 timeSpan=(start,(!currentTime))} 42 ::(!outEdges)) 43 in 44 run() 45 end 46 fun change(l: a mod, \nv) = write(l, v) 47 fun propagate () = 48 if (Q.isEmpty(!PQ)) then 49 () 50 else let 51 val (edge, pq) \n= Q.deleteMin(!PQ) 52 val =PQ:=pq 53 val {reader=f,timeSpan=(start,stop)} = edge 54 in 55 if (Time.isSplicedOut \nstart) then 56 propagate () (* Obsolete read, discard.*) 57 else 58 (Time.spliceOut(start,stop); (* Splice \nout *) 59 currentTime := start; 60 f(); (* Rerun the read *) 61 propagate ()) 62 end 63 fun propagate() \n= let 64 val ctime = !currentTime 65 in 66 (propagate (); 67 currentTime := ctime) 68 end 69 end Figure \n8: The implementation of the adaptive library. for making and propagating changes found in the ML li\u00adbrary. \nInstead, we give a direct presentation of the change\u00adpropagation algorithm in Section 6, which is de.ned \nin terms of the dynamic semantics of AFL given here. Just as with Types t : : = int | bool | t mod | \nt1 s. t2 | t1 c. t2 Values v : : = c | x | l | funs f(x : t1) : t2 is es end |func f(x : t1) : t2 is \nec end Op s o : : = not | + | \u00ad| = | < | . . . Const s c : : = n | true | false Exp s e : : = es | ec \nSt Exp s es : : = v | o(v1, . . . , vn) | applys(v1, v2) |let x be es in e ' s end | modt ec |if v then \nes else e ' s Ch Exp s ec : : = write(v) | applyc(v1, v2) |let x be es in ec end read v as x in e end \n|if v then ec else e ' c Figure 9: The abstract syntax of AFL. the ML implementation, the dynamic semantics \nmust keep a record of the adaptive aspects of the computation. How\u00adever, rather than use adg s, the semantics \nmaintains this information in the form of a trace, which guides the change propagation algorithm. By \ndoing so we are able to give a relatively straightforward proof of correctness of the change propagation \nalgorithm in Section 6. Abstract Syntax. The abstract syntax of AFL is given in Figure 9. We use the \nmeta-variables x, y, and z (and variants) to range over an unspeci.ed set of variables, and the meta-variable \nl (and variants) to range over a separate, unspeci.ed set of locations. The syntax of AFL is restricted \nto 2/3-cps , or named form , to streamline the presenta\u00adtion of the dynamic semantics. The types of AFL \ninclude the base types int and bool; s the stable function type, t1 . t2; the changeable function c type, \nt1 . t2; and the type t mod of modi.able references of type t. Extending AFL with product, sum, recursive, \nor polymorphic types presents no fundamental di.culties, but they are omitted here for the sake of brevity. \nExpressions are classi.ed into two categories, the stable and the changeable. The value of a stable expression \nis not sensitive to modi.cations to the inputs, whereas the value of a changeable expression may, directly \nor indirectly, be a.ected by them. The familiar mechanisms of functional programming are embedded in \nAFL as stable expressions. These include basic types such as integers and booleans, and a sequential \nlet construct for ordering evaluation. Or\u00addinary functions arise in AFL as stable functions. The body \nof a stable function must be a stable expression; the appli\u00adcation of a stable function is correspondingly \nstable. The stable expression modt ec allocates a new modi.able refer\u00adence whose value is determined \nby the changeable expression ec. Note that the modi.able itself is stable, even though its contents is \nsubject to change. Changeable expressions are written in destination\u00adpassing style, with an implicit \ntarget. The changeable ex\u00adpression write(v) writes the value v into the target. The changeable expression \nread v as x in ec end binds the contents of the modi.able v to the variable x, then con\u00ad Constants .; \nG .s n : int .; G .s true : bool .; G .s false : bool (.(l)= t) (G(x)= t) Locs, Vars .; G .s l : t mod \n.; G .s x : t s .; G,f : t1 . t2,x : t1 .s e : t2 Fun s.; G .s funs f(x : t1): t2 is e end :(t1 . t2) \nc .; G,f : t1 . t2,x : t1 .c e : t2 .; G .s func f(x : t1) : t2 is e end : (t1 c. t2) Prim .; G .s vi \n: ti (1 = i = n) .o o : (t1, . . . , tn) t .; G .s o(v1, . . . , vn) : t If .; G .s x : bool .; G .s \ne1 : t .; G .s e2 : t .; G .s if x then e1 else e2 : t Apply .; G .s v1 : (t1 s. t2) .; G .s v2 : t1 \n.; G .s applys (v1, v2) : t2 Let .; G .s e1 : t1 .; G, x : t1 .s e2 : t2 .; G .s let x be e1 in e2 end \n: t2 Mod .; G .c e : t .; G .s modt e : t mod Figure 10: Typing of stable expressions.  tinues evaluation \nof ec.A read is considered changeable because the contents of the modi.able on which it depends is subject \nto change. A changeable function itself is stable, but its body is changeable; correspondingly, the application \nof a changeable function is a changeable expression. The sequential let construct allows for the inclusion \nof stable sub-computations in changeable mode. Finally, condition\u00adals with changeable branches are themselves \nchangeable. Static Semantics. The AFL type system is inspired by the type theory of modal logic given \nby Pfenning and Davies [12]. We distinguish two modes, the stable and the changeable, corresponding to \nthe distinction between terms and expressions, respectively, in Pfenning and Davies work. However, they \nhave no analogue of our changeable function type, and do not give an operational interpretation of their \ntype system. The judgement .; G .s e : t states that e is a well-formed stable expression of type t, \nrelative to . and G. The judge\u00adment .; G .c e : t states that e is a well-formed changeable expression \nof type t , relative to . and G. Here . is a lo\u00adcation typing, a .nite function assigning types to locations, \nand G is a variable typing, a .nite function assigning types to variables. The rules for deriving these \njudgements are given in Figures 10 and 11. Dynamic Semantics. The evaluation judgements of AFL have one \nof two forms. The judgement s, es .s v, s ' , Ts states that evaluation of the stable expression es, \nrelative to the input store s, yields the value v, the trace Ts, and the Write .; G .s v : t .; G .c \nwrite(v) : t If .; G .s x : bool .; G .c e1 : t .; G .c e2 : t .; G .c if x then e1 else e2 : t Apply \n.; G .s v1 : (t1 c. t2) .; G .s v2 : t1 .; G .c applyc(v1, v2) : t2 Let .; G .s e1 : t1 .; G, x : t1 \n.c e2 : t2 .; G .c let x be e1 in e2 end : t2 Read .; G .s v1 : t1 mod .; G, x : t1 .c e2 : t2 .; G .c \nread v1 as x in e2 end : t2 Figure 11: Typing of changeable expressions.  .c s ' that evaluation of \nthe changeable expression ec, relative to the input store s, writes its value to the target l, and yields \nthe trace Tc and the updated store s ' . In the dynamic semantics, a store, s, is a .nite function mapping \neach location in its domain, dom(s), to either a value v or a hole D. The de.ned domain, def(s), of s \nconsists of those locations in dom(s) not mapped to . by s. When a location is created, it is assigned \nthe value . to reserve that location while its value is being determined. A trace is a .nite data structure \nrecording the adaptive aspects of evaluation. The abstract syntax of traces is given by the following \ngrammar: updated store s ' . The judgement s, l . ec , Tc states Trace T :: = Ts | Tc Stable Ts :: = \n. |(Tc)l:t | Ts ; Ts | Rx.e Changeable :: = ; Tc Tc Wtl (Tc) | Ts When writing traces, we adopt the convention \nthat ; is right-associative. A stable trace records the sequence of allocations of mod\u00adi.ables that arise \nduring the evaluation of a stable expres\u00adsion. The trace (Tc)records the allocation of the modi.\u00ad l:t \nable, l, its type, t , and the trace of the initialization code for l. The trace Ts ; Ts ' results from \nevaluation of a let expression in stable mode, the .rst trace resulting from the bound expression, the \nsecond from its body. A changeable trace has one of three forms. A write, Wt , records the storage of \na value of type t in the target. A sequence Ts ; Tc records the evaluation of a let expression in changeable \nmode, with Ts corresponding to the bound stable expression, and Tc corresponding to its body. A read \nRx.e(Tc) trace speci.es the location read, l, the context of l use of its value, x.e, and the trace, \nTc, of the remainder of evaluation with the scope of that read. This records the dependency of the target \non the value of the location read. The augmented dependency graphs described in Sec\u00adtion 4 may be seen \nas an e.cient representation of traces. Time stamps may be assigned to each read and write oper\u00adation \nin the trace in left-to-right order. These correspond to the time stamps in the adg representation. The \ncontain\u00adment hierarchy is directly represented by the structure of the trace. Speci.cally, the trace \nTc (and any read in Tc) is contained within the read trace Rlx.e(Tc). Stable Evaluation. The evaluation \nrules for stable expres\u00adsions are given in Figure 12. Most of the rules are stan\u00ad Value s, v .s v, s, \ne ' (v = app(o, (v1,...,vn))) Op s s, o(v1,...,vn) .s v ' , s, e s, e1 .s v, s ' , Ts If s, if true then \ne1 else e2 .s v, s ' , Ts s, e2 .s v, s ' , Ts s, if false then e1 else e2 .s v, s ' , Ts (v1 = funs \nf(x : t2): t is e end) s, [v1/f, v2/x] e .s v ' ,s ' , Ts Apply s, apply(v1,v2) .s v ' ,s ' , Ts s s, \ne1 .s v1,s ' , Ts s ' , [v1/x]e2 .s v1,s '' , T ' s Let s, let x be e1 in e2 end .s v2,s '' , (Ts ; T \n' ) s s[l . O],l . e .c s ' , Tc (l . dom(s)) Mod s, modt e .s l, s ' , (Tc)l:t Figure 12: Evaluation \nof stable expressions. Write s, l . write(v) .c s[l . v], Wt s, l . e1 .c s ' , Tc If s, l . if true \nthen e1 else e2 .c s ' , Tc s, l . e2 .c s ' , Tc s, l . if false then e1 else e2 .c s ' , Tc (v1 = func \nf(x : t1): t2 is e end) s, l . [v1/f, v2/x] e, Tc .c s ' Apply .c s ' s, l . apply(v1,v2) , Tc cs, e1 \n.s v1,s ' , Ts s '' s ' ,l . [v1/x]e2 .c , Tc Let s '' s, l . let x be e1 in e2 end .c , (Ts ; Tc) s, \nl ' . [s(l)/x] e .c s ' , Tc Read s, l ' . read l as x in e end .c s ' ,Rx.e(Tc) l Figure 13: Evaluation \nof changeable expressions. dard for a store-passing semantics. For example, the let rule sequences evaluation \nof its two expressions, and per\u00adforms binding by substitution. Less conventionally, it yields a trace \nconsisting of the sequential composition of the traces of its sub-expressions. The most interesting rule \nis the evaluation of modt e. Given a store s, a fresh location l is allocated and initial\u00adized to D prior \nto evaluation of e. The sub-expression e is evaluated in changeable mode, with l as the target. Pre\u00adallocating \nl ensures that the target of e is not accidentally re-used during evaluation; the static semantics ensures \nthat l cannot be read before its contents is set to some value v. Changeable Evaluation. The evaluation \nrules for change\u00adable expressions are given in Figure 13. The let rule is sim\u00adilar to the corresponding \nrule in stable mode, except that the bound expression, e1, is evaluated in stable mode, whereas the body, \ne2, is evaluated in changeable mode. The read ex\u00adpression substitutes the binding of location l in the \nstore s for the variable x in e, and continues evaluation in change\u00adable mode. The read is recorded in \nthe trace, along with the expression that employs the value read. The write rule simply assigns its argument \nto the target. Finally, applica\u00adtion of a changeable function passes the target of the caller to the \ncallee, avoiding the need to allocate a fresh target for the callee and a corresponding read to return \nits value to the caller. Type Safety. The static semantics of AFL ensures these four properties of its \ndynamic semantics: (1) each modi.able is written exactly once; (2) no modi.able is read before it is \nwritten; (3) dependencies are not lost, i.e. if a value depends on a modi.able, then its value is also \nplaced in a modi.able; (4) the store is acyclic. The proof of type safety for AFL hinges on a type preser\u00advation \ntheorem for the dynamic semantics. As may be ex\u00adpected, the preservation theorem ensures that the value \nof a well-typed stable expression is also well-typed (indeed, has the same type). In addition preservation \nensures that evaluation of a changeable expression preserves the type of the store. The typing relation \nfor stores ensures not only that the contents of locations are consistent with their type, but also that \nthere are no cyclic dependencies among them. Thus preservation for AFL ensures that no cycles can arise \nduring evaluation, which is consistent with pure functional programming. Space considerations preclude \na rigorous presentation of type safety for AFL. A complete proof is given in the com\u00adpanion technical \nreport [1]. 6 Change Propagation is Sound We formalize the notion of an input change and present a for\u00admal \nversion of the change-propagation algorithm. Using this formal framework, we prove that the change-propagation \nal\u00adgorithm is correct. Changing the Input. We represent an input change with a di.erence store. A di.erence \nstore is a .nite mapping assigning values to locations. Unlike a store, a di.erence store may contain \ndangling locations that are not de.ned within the store. The process of modifying a store with a di.erence \nstore is de.ned as follows. De.nition 3 (Store Modi.cation) Let s be a store and let d be a di.erence \nstore. The modi.\u00adcation of s by d is the store s ' = s . d given by the equation s . d = d .{ (l, s(l)) \n| l . dom(d) and l . dom(s) }. This store modi.cation yields an input change when it is applied to an \ninput store. Change Propagation Algorithm. We present a formal ver\u00adsion of the change-propagation algorithm, \nwhich is infor\u00admally described in Section 4. In the rest of this section, we will use the term change-propagation \nalgorithm to refer to this formal algorithm. p s, e, C .s, e, C s p s, l . Tc, C .s ' , T ' , C ' c c \nMod p s, (Tc)l:t , C .s ' , (T ' )l:t , C ' s cp , T '' s, Ts, C .s ' , C ' s s s ' p s '' , T ''' , \nC '' , T ' , C ' . ss s Let p s '' , (T '' ; T ''' ), C '' s, (Ts ; T ' ), C . ss ss p Write s, l . Wt \n, C .s, Wt , C s p s, l ' . Tc, C .s ' , T ' , C ' c c Read (l . C) p . Rx.e s ' ,Rx.e(T ' ), C ' s, \nl ' (Tc), C . l c lc s, l ' . [s(l)/x]e .c s ' , T ' c (l . C) p s, l ' . Rx.e(Tc), C .s ' ,Rx.e(T ' \n), C .{l ' } l c lcp s ' s, Ts, C ., T ' , C ' s s p s '' , C '' s ' ,l ' . Tc, C ' ., T ' c c Let p \ns '' ), C '' s, l ' . (Ts ; Tc), C ., (T ' ; T ' c sc Figure 14: Change propagation rules (stable and \nchange\u00adable). The change-propagation algorithm takes a modi.ed store, a trace obtained by evaluating \nan AFL program with respect to the original store, and a set of input locations that are changed by the \nstore modi.cation, called the changed set. The algorithm scans the trace as it seeks for reads of changed \nlocations. When such a read is found, the as\u00adsociated expression is re-evaluated with the new value to \nobtain a revised trace and store. Furthermore, the target of a re-evaluated read is added to the changed \nset, be\u00adcause re-evaluation may change its value. Thus, the or\u00adder in which the reads are re-evaluated \nis important. The change-propagation algorithm scans the trace in the order that it was originally generated. \nThis ensures that the trace is scanned only once and is done by establishing a correspon\u00addence between \nthe change-propagation rule that handles a trace and the AFL rule that generates that trace. Formally, \nthe change propagation algorithm is given by two judgements: p 1. Stable propagation: s, Ts, C .s ' , \nT ' s, C ' s p 2. Changeable propagation: s, l . Tc, C .s ' , T ' c, C ' c These judgement de.ne the \nchange-propagation for a stable and a changeable trace (Ts and Tc) with respect to a store (s) and a \nchanged set (C). In changeable propagation, a target (l) is maintained as in changeable evaluation mode \nof AFL. The rules de.ning the change-propagation judgements are given in Figure 14. Given a trace, change \npropagation mimics the evaluation rule of AFL that originally gener\u00adated that trace. To stress this correspondence, \neach change\u00adpropagation rule is marked with the name of the evaluation rule to which it corresponds. \nFor example, the propagation rule for the trace Ts ; T ' s mimics the let rule of the stable mode that \ngives rise to this trace. si initial eval si, e .s vi, s ' i, Ti s si . d s ' i s ' i . d ss sm ss, e \n.s vs, s ' s, Ts s subsequent eval propagate sm, Ti s, C .p s s ' m, Tm s , s ' s . s ' m Figure 15: \nChange propagation simulates a complete re\u00adevaluation.  The most interesting rule is the read rule. \nThis rule mimics a read operation, which evaluates an expression af\u00adter binding its speci.ed variable \nto the value of the location read. The read rule takes two di.erent actions depending on whether this \nlocation is in the changed set or not. If the location has changed (is in the changed set), then the \nex\u00adpression is re-evaluated with the new value of location. This re-evaluation yields a revised store \nand a new trace. The new trace repairs the original trace by replacing the trace of the read. Also, the \ntarget location is added to the changed set because it may now have a di.erent value. Finally, the repaired \ntrace, the revised store, and the revised changed set is yielded. If the read location has not been changed \n(is not in the changed set), then there is no need to re-evaluate this read and change-propagation continues \nby scanning the rest of the trace. This is because a re-evaluation would gen\u00aderate the same e.ects to \nthe store and to the trace as done by the initial evaluation. Since these e.ects are already present \nin the store and the trace, this read could safely be skipped. Note that the purely functional change-propagation \nal\u00adgorithm presented here scans the whole trace. Therefore, a direct implementation of this algorithm \nwill run in time lin\u00adear in the size of the trace. On the other hand, the change\u00adpropagation algorithm \nrevises the trace by only replacing the changeable trace of re-evaluated reads. Thus, if one is content \nwith updating the trace with side e.ects, then traces of re-evaluated reads can be replaced in place, \nwhile skipping all the rest of the trace. This is indeed how the ML implementation performs change propagation \nusing an augmented dependency graph as described in Section 4. Correctness of Change Propagation. Change \npropagation simulates a complete re-evaluation by only re-evaluating the a.ected sub-expressions of an \nAFL program. Here we show that change propagation yields the same output and the trace as a complete \nre-evaluation and thus is correct. Figure 15 illustrates this simulation process. First, we evaluate \na program e, which we assume to be a stable ex\u00adpression, with respect to an initial store si obtaining \na value vi, an extended store si' , and a trace Tis. This is called the initial evaluation. Then, we \nmodify the initial store with a di.erence store d as ss = si . d and re-evaluate the program with this \nstore in a subsequent evaluation. To simulate the subsequent evaluation via a change prop\u00adagation, we \n.rst apply the modi.cations d to si' , to ob\u00ad s ' change propagation with respect to sm, using the trace \nof the initial evaluation, and the set of changed locations C = dom(si' ) n dom(d). As a result, we obtain \na revised trace and store s ' and a revised trace Tm . For the change\u00adtain a new store sm as sm = i . \nd. We then perform ms propagation to work properly, we require that d changes only input locations, i.e., \ndom(si' ) n dom(d) . dom(si). To prove correctness, we compare the trace and store ob\u00adtained by the subsequent \nevaluation to those obtained by the change propagation. Since these two evaluations are inde\u00adpendent, \nwe do not expect the locations generated in these evaluations match. Thus, the two traces and the stores \ncan indeed contain di.erent locations. On the other hand, this is not a problem because locations themselves \nare transparent to the user. To capture this, we introduce an equivalence re\u00adlation for stores and traces \nthat disregards locations (names) via a partial bijection between locations. A partial bijection is a \none-to-one mapping from a set of locations D to a set of locations R that may not map all the locations \nin D. De.nition 4 (Partial Bijection) B is a partial bijection from set D to set R if it satis.es the \nfollowing: 1. B .{ (a, b) | a . D, b . R }, 2. if (a, b) . B and (a, b ' ) . B then b = b ' , 3. if \n(a, b) . B and (a ' ,b) . B then a = a ' .  A partial bijection, B can be applied to a trace T or a \nstore s, denoted B[T ] and B[s] by replacing each location l in T or s with its image B[l] whenever the \nimage is de.ned. The formal de.nitions for these are given in the compan\u00adion technical report [1]. The \ntheorem below states that the change-propagation algorithm is correct, the proof of the theorem is given \nin the companion technical report [1]. In the theorem, the reason that the store s ' is a super set of \nm ss ' is that s ' contains remnant locations from the initial m evaluation, whereas ss ' does not. Theorem \n5 (Correctness) Let si be an initial store, d be a di.erence store, ss = si . d, and sm = si ' . d as \nshown in Figure 15. If 1. si,e .s vi,s i' , Tis, (initial evaluation) 2. ss,e .s vs,s s' , Tss , (subsequent \nevaluation) 3. dom(si' ) n dom(d) . dom(si)  then the following holds: s ' 1. sm, Tis, (dom(si' ) n \ndom(d)) .p m, Tm , , s s 2. there is a partial bijection B such that (a) B[vi]= vs, (b) B[Tms ]= Tss, \n(c) B[s ' ] . s ' ms. Type Safety. The change-propagation algorithm also en\u00adjoys a type preservation \nproperty stating that if the initial state is well-formed, so is the result state. This ensures that \nthe results of change propagation can subsequently be used as further inputs. The proof requires that \nstore modi.cation operation respect the typing of the store being modi.ed and given in the companion \nreport [1]. 7 Discussion Variants. In the process of developing the mechanisms pre\u00adsented in this paper \nwe considered several variants. Here we mention a few of them. One variant is to replace the explicit \nwrite operation with an implicit one. In the ML library this requires making the target destination an \nargument to the read operation. In AFL it requires adding some implicit type subsumption rules. We decided \nto include the explicit write since we believe it is cleaner. We also considered a variant of our mechanism \nin which the mod, read, and write are combined into a single operation. This operation reads a modi.able, \nevaluates an expression with the value of the modi.able, and writes the result into a new modi.able. \nIn the ML library the operation can be de.ned as follows. function modrw(x : a mod, f : a -> b) : b = \nmod(fn d => read x (fn x => write(d, f(x )))) This operation, along with another that does two reads, \nwere su.cient to express many of the examples we were working with. The operations, however, are not \nexpres\u00adsive enough for many other examples, and in particular for Quicksort. In practice it would worthwhile \nincluding these two operations in a comprehensive adaptive library since im\u00adplementing them directly \nwould be more e.cient than the composition given above. Side E.ects. We require that the underlying language \nbe purely functional. The main reason for this is that each edge (read) stores a closure (code and environment) \nwhich might be re-evaluated. It is critical that this closure does not change. The key requirement, therefore, \nis not that there are no side-e.ects, but rather that all data is persistent (i.e., the closure s environment \ncannot be modi.ed). It is therefore likely that the adaptive mechanism could be made to work in an imperative \nsetting as long as relevant data structures are persistent. There has been signi.cant research on persistent \ndata-structures under an imperative setting [6, 5, 7]. We further note that certain benign side e.ects \nare not harmful. For example, side e.ects to objects that are not examined by the adaptive code itself \nare harmless. This includes print statements, or any changes to meta data structures that are somehow \nrecording the progress of the adaptive computation itself. For example, one way to de\u00adtermine which parts \nof the code are being re-evaluated is to sprinkle the code with print statements and see which ones print \nduring the change propagation. In fact, re-evaluations of a function can be counted by simply inserting \na counter at the start of the function. Also, the memoization of the kind done by lazy languages will \nnot a.ect the correctness of change-propagation, because the value remains the same whether it has been \ncalculated or not. We therefore expect that our approach can be applied to lazy languages, but we have \nnot explored this direction. Function Caching. As mentioned in the related work sec\u00adtion, it might be \nuseful to add function caching to our frame\u00adwork. We believe this is a promising extension, but should \nnote that it is not trivial to incorporate this feature. The problem is that function caching and modi.ables \ninteract in subtle ways function caching requires purely functional code, but our framework involves \nside-e.ects in its imple\u00admentation. Applications. The work in this paper was motivated by the desire \nto make it easier to de.ne kinetic data structures for problems in computational geometry [2]. Consider \nthe problem of maintaining some property of a set of objects in space as they move, such as the nearest \nneighbors or convex hull of a set of points. Kinetic data structures are designed to maintain such properties \nby re-evaluating parts of the code when certain conditions become violated (e.g.,a point moves from one \nside of a line to the other). Currently, however, every problem requires the design of its own kinetic \ndata structure. We believe that it is possible, instead, to use adaptive versions of non-kinetic algorithms. \nFull Adaptivity. It is not di.cult to modify the AFL se\u00admantics to interpret standard functional code \n(e.g. the call-by-value lambda-calculus) in a fully adaptive way (i.e., all values are stored in modi.ables, \nand all expressions are changeable). It is also not hard to describe a translator for converting functional \ncode into AFL, such that the result is fully adaptive. The only slightly tricky aspect is translat\u00ading \nrecursive functions. We in fact had originally considered de.ning a fully adaptive version of AFL but \ndecided against it since we felt it would be more useful to selectively choose what code is adaptive. \nMeta Language. We have not included a meta language for AFL that would allow a program to change input \nand run change-propagation. There are some subtle issues in de.ning such a language such as how to restrict \nchanges to inputs, and how to identify the safe parts of the code in which the program can make changes. \nWe worked on a system that includes an additional type mode, which we called meta-stable. Changes and \nchange-propagation could be performed only in this mode, and there was no way to get into this mode other \nthan from top-level. We felt, however, that this system did not add much to the main concepts covered \nin this paper. Conclusion We have presented a mechanism for adaptive computation based on the idea of \na modi.able reference. We expect that this mechanism can be incorporated into any purely func\u00adtional \ncall-by-value language. A key aspect of our mech\u00adanism is that it can dynamically create new computations \nand delete old computations. The main contributions of the paper are the particular set of primitives \nwe suggest, the change-propagation algorithm, and the semantics along with the proofs that it is sound. \nThe simplicity of the prim\u00aditives is achieved by using a destination passing style. The e.ciency of the \nchange-propagation is achieved by using an optimal order-maintenance algorithm. The soundness of the \nsemantics is aided by a modal type system. Acknowledgements We are grateful to Frank Pfenning for his \nadvice on modal type systems. We also would like to thank Mihai Budiu, Aleks Nanevski, and the anonymous \nreferees for their comments on the earlier drafts of this pa\u00adper. References [1] Umut A. Acar, Guy E. \nBlelloch, and Robert W. Harper. Adaptive functional programming. Technical Report CMU\u00adCS-01-161, Carnegie \nMellon University, Computer Science Department, November 2001. [2] Julien Basch, Leonidas J. Guibas, \nand John Hershberger. Data structures for mobile data. Journal of Algorithms, 31(1):1 28, 1999. [3] Alan \nDemers, Thomas Reps, and Tim Teitelbaum. Incre\u00admental evaluation of attribute grammars with application \nto syntax directed editors. In Conference Record of the 8th Annual ACM Symposium on POPL, pages 105 116, \nJanuary 1981. [4] P. F. Dietz and D. D. Sleator. Two algorithms for maintain\u00ading order in a list. In \nProceedings. 19th ACM Symposium. Theory of Computing, pages 365 372, 1987. [5] Paul F. Dietz. Fully persistent \narrays. In Workshop on Al\u00adgorithms and Data Structures, volume 382 of Lecture Notes in Computer Science, \npages 67 74. Springer-Verlag, August 1989. [6] James R. Driscoll, Neil Sarnak, Daniel D. Sleator, and \nRobert E. Tarjan. Making data structures persistent. Jour\u00adnal of Computer and System Sciences, 38(1):86 \n124, Febru\u00adary 1989. [7] James R. Driscoll, Daniel D. Sleator, and Robert E. Tarjan. Fully persistent \nlists with catenation. Journal of the ACM, 41(5):943 959, 1994. [8] J. Field and T. Teitelbaum. Incremental \nreduction in the lambda calculus. In Proceedings of the ACM 90 Conference on LISP and Functional Programming, \npages 307 322, June 1990. [9] Roger Hoover. Incremental Graph Evaluation. PhD thesis, Department of Computer \nScience, Cornell University, May 1987. [10] Yanhong A. Liu, Scott Stoller, and Tim Teitelbaum. Dis\u00adcovering \nauxiliary information for incremental computation. In Conference Record of the 23rd Annual ACM Symposium \non POPL, pages 157 170, January 1996. [11] Yanhong A. Liu and Tim Teitelbaum. Systematic derivation of \nincremental programs. Science of Computer Program\u00adming, 24(1):1 30, February 1995. [12] Frank Pfenning \nand Rowan Davies. A judgmental recon\u00adstruction of modal logic. Mathematical Structures in Com\u00adputer Science, \n11:511 540, 2001. Notes to an invited talk at the Workshop on Intuitionistic Modal Logics and Applica\u00adtions \n(IMLA 99), Trento, Italy, July 1999. [13] W. Pugh and T. Teitelbaum. Incremental computation via function \ncaching. In Conference Record of the 16th Annual Symposium on POPL, pages 315 328, January 1989. [14] \nWilliam Pugh. Incremental computation via function caching. PhD thesis, Department of Computer Science, \nCor\u00adnell University, August 1987. [15] G. Ramalingam and Thomas W. Reps. A categorized bibli\u00adography \non incremental computation. In Conference Record of the 20th Annual ACM Symposium on POPL, pages 502 \n510, January 1993. [16] Thomas Reps. Generating Language-Based Environments. PhD thesis, Department of \nComputer Science, Cornell Uni\u00adversity, August 1982. [17] R. S. Sundaresh and Paul Hudak. Incremental \ncompilation via partial evaluation. In Conference Record of the 18th Annual ACM Symposium on POPL, pages \n1 13, January 1991. [18] D. M. Yellin and R. E. Strom. Inc: A language for incre\u00admental computations. \nACM Transactions on Programming Languages and Systems, 13(2):211 236, April 1991. \n\t\t\t", "proc_id": "503272", "abstract": "An adaptive computation maintains the relationship between its input and output as the input changes. Although various techniques for adaptive computing have been proposed, they remain limited in their scope of applicability. We propose a general mechanism for adaptive computing that enables one to make any purely-functional program adaptive.We show that the mechanism is practical by giving an efficient implementation as a small ML library. The library consists of three operations for making a program adaptive, plus two operations for making changes to the input and adapting the output to these changes. We give a general bound on the time it takes to adapt the output, and based on this, show that an adaptive Quicksort adapts its output in logarithmic time when its input is extended by one key.To show the safety and correctness of the mechanism we give a formal definition of <b>AFL</b>, a call-by-value functional language extended with adaptivity primitives. The modal type system of <b>AFL</b> enforces correct usage of the adaptivity mechanism, which can only be checked at run time in the ML library. Based on the <b>AFL</b> dynamic semantics, we formalize the change-propagation algorithm and prove its correctness.", "authors": [{"name": "Umut A. Acar", "author_profile_id": "81100077236", "affiliation": "Carnegie Mellon University, Pittsburgh, PA", "person_id": "P286793", "email_address": "", "orcid_id": ""}, {"name": "Guy E. Blelloch", "author_profile_id": "81100282539", "affiliation": "Carnegie Mellon University, Pittsburgh, PA", "person_id": "P100820", "email_address": "", "orcid_id": ""}, {"name": "Robert Harper", "author_profile_id": "81100140064", "affiliation": "Carnegie Mellon University, Pittsburgh, PA", "person_id": "PP39029370", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/503272.503296", "year": "2002", "article_id": "503296", "conference": "POPL", "title": "Adaptive functional programming", "url": "http://dl.acm.org/citation.cfm?id=503296"}