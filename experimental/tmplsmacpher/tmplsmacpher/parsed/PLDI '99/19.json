{"article_publication_date": "05-01-1999", "fulltext": "\n   sizes other than 8. It should be noted that the result of consecutive-group packing is very close \nto the ideal case where the miss rate halves when cache line size doubles. As shown in the next section, \ndynamic packing, when combined with locality grouping, can reduce the miss rate to as low as 0.02%. We \nalso simulated 2K-sized caches and observed sim\u00adilar results. Consecutive packing reduces the miss rate \nof moldyn by 27% to a factor of 3.2. Consecutive-group packing improves mesh by 1% to 39%. 2.3 Combining \nComputation and Data Transformation When we combine locality grouping with data pack\u00ading on mesh (moldyn \nwas already in locality-grouped form), the improvement is far greater than when they are individually \napplied. Figure 4 shows miss rates of mesh after locality grouping. On a 4K cache, the miss rate on a \nunit-line cache is reduced from 64% to 0.37% after locality grouping. On longer cache-line sizes, data \npacking further reduces the miss rate by 15% to a factor of over 6. On the 16-molecule cache line case, \nthe com\u00adbined effect is a reduction from a miss rate of 4.52% (shown in Figure 3) to 0.02%, a factor \nof 226. On a 2K cache with 16-molecule cache lines, the combined transformations reduce miss rate from \n7.48% to 0.25%, a factor of 30. Although not shown in the graph, group and consecutive-group packing \ndo not perform as well as consecutive packing. In summary, the simulation results show that lo\u00adcality \ngrouping effectively extracts computation local\u00adity, and data packing significantly improves data lo\u00adcality. \nThe effect of data packing becomes even more pronounced in caches with longer cache lines. In both programs, \nsimple consecutive packing performs the best after locality grouping, and the combination of locality \ngrouping and consecutive packing yields the lowest miss rate. Compiler Support for Dynamic Data Packing \nRun-time data transformations, dynamic data packing in particular, involve redirecting memory accesses \nto each transformed data structure. Such run-time changes complicate program transformations and induce \nover\u00adhead during the execution. This section presents com\u00adpiler strategies to automate data transformations \nand minimize their run-time overhead. 3.1 Packing and Packing Optimizations The core mechanism for supporting \npacking is a run\u00adtime data map, which maps from the old location be\u00adfore data packing to the new location \nafter data pack\u00ading. Each access to a transformed array is augmented with the indirection of the corresponding \nrun-time map. Thus the correctness of packing is ensured regardless the location and the frequency of \npacking. Some exist\u00ading language features such as sequence and storage as\u00adsociation in Fortran prevent \na compiler from accurately detecting all accesses to a transformed array. However, this problem can be \nsafely solved in a combination of compile, link and run-time checks described in [7]. Although the compiler \nsupport can guarantee the correctness of packing, it needs additional information to decide on the profitability \nof packing. Our compiler currently relies on a one-line user directive to specify whether packing should \nbe applied, when and where packing should be carried out and which access sequence should be used to \ndirect packing. The packing directive provides users with full power of controlling data pack\u00ading, yet \nrelieves them from any program transformation work. At the end of this section, we will show how the \nprofitability analysis of packing can be automated with\u00adout relying on any user-supplied directive. The \nfollowing example illustrates our compiler sup\u00adport for data packing. The example has two compu\u00adtation \nloops: the first loop calculates cumulative forces on each object, and the second loop calculates the \nnew location of each object as a result of those forces. The packing directive specifies that packing \nis to be applied before the first loop. The straightforward (unoptimized) packing produces the following \ncode. The call to analyzes the interactions array, packs force array and generates the run-time data \nmap, After packing, indirec\u00adtions are added in both loops.    compiler does not do a good job of \noptimizing array ref\u00aderences. Since the excessive number of memory loads dominates execution time, the \ncompiler optimizations achieve a similar reduction (82%) in execution time. The number of loads is increased \nin magi after the op\u00adtimizations because array alignment transforms 19 more arrays than the base packing, \nand not all indirections to these arrays can be eliminated. Despite the increased number of memory loads, \nthe cache misses and TLB misses are reduced by 10% to 33%, and the overall speed is improved by 8%. For \n the compiler rec\u00adognizes that matrix entries are accessed in stride-one fashion and consequently, the \ncompiler replaces the in\u00addirection accesses with direct stride-one iteration of the reorganized data \narray. The transformed matrix-vector multiply kernel has the equally efficient data access as the original \nhand-coded version. As a result, the num\u00adber of loads and cache misses is reduced by 23% to 50%. The \nTLB working set fits in machine s TLB buffer after the optimizations, removing 97% of TLB misses. The \nexecution time is reduced by 60%, a speedup of 2.47. 6 Related Work To our knowledge, this work is the \nfirst study on the combination of run-time computation and data trans\u00adformation to improve cache performance \nof irregular and dynamic applications. It is also the first to pro\u00advide comprehensive compiler support \nfor run-time data transformation. Our work is close in spirit to the run-time paralleliza\u00adtion work on \ndynamic applications. The Chaos group, led by Saltz[10], partitions computation and reorganizes data \nat run time in order to balance the computational load across processors and reduce communication in \na parallel execution. Once computation is partitioned, the data accessed by each processor are grouped \nand placed in its local memory. However, the parallelization work did not include a general restructuring \nmethod to subsequently improve cache performance. The Chaos group is also the first to use run-time \n   \n\t\t\t", "proc_id": "301618", "abstract": "With the rapid improvement of processor speed, performance of the memory hierarchy has become the principal bottleneck for most applications. A number of compiler transformations have been developed to improve data reuse in cache and registers, thus reducing the total number of direct memory accesses in a program. Until now, however, most data reuse transformations have been <i>static</i>---applied only at compile time. As a result, these transformations cannot be used to optimize irregular and dynamic applications, in which the data layout and data access patterns remain unknown until run time and may even change during the computation.In this paper, we explore ways to achieve better data reuse in irregular and dynamic applications by building on the inspector-executor method used by Saltz for run-time parallelization. In particular, we present and evaluate a <i>dynamic</i> approach for improving both computation and data locality in irregular programs. Our results demonstrate that run-time program transformations can substantially improve computation and data locality and, despite the complexity and cost involved, a compiler can automate such transformations, eliminating much of the associated run-time overhead.", "authors": [{"name": "Chen Ding", "author_profile_id": "81309499457", "affiliation": "Computer Science Department, Rice University, Houston, TX", "person_id": "PP39075626", "email_address": "", "orcid_id": ""}, {"name": "Ken Kennedy", "author_profile_id": "81100453545", "affiliation": "Computer Science Department, Rice University, Houston, TX", "person_id": "PP40027435", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/301618.301670", "year": "1999", "article_id": "301670", "conference": "PLDI", "title": "Improving cache performance in dynamic applications through data and computation reorganization at run time", "url": "http://dl.acm.org/citation.cfm?id=301670"}