{"article_publication_date": "05-01-1999", "fulltext": "\n A semantics for imprecise exceptions Mic Simon Peyton Jones rosoft Research Ltd, Cambridge simonpj@microsoft.com \nAlastair Reid Yale University reid-alastairQcs.yale.edu Cambridge Tony Hoare University Computer carh@comlab.ox.ac.uk \nLaboratory Simon Marlow Microsoft Research Ltd, Cambridge t-simonm@microsoft.com Fergus Henderson The \nUniversity of Melbourne fjh@cs.mu.oz.au Abstract Some modern superscalar microprocessors provide only \nim-precise exceptions. That is, they do not guarantee to re-port the same exception that would be encountered \nby a straightforward sequential execution of the program. In ex-change, they offer increased performance \nor decreased chip area (which amount to much the same thing). This performance/precision tradeoff has \nnot so far been much explored at the programming language level. In this paper we propose a design for \nimprecise exceptions in the lazy functional programming language Haskell. We discuss several designs, \nand conclude that imprecision is essential if the language is still to enjoy its current rich algebra \nof trans- formations. We sketch a precise semantics for the language extended with exceptions. The paper \nshows how to extend Haskell with exceptions without crippling the language or its compilers. We do not \nyet have enough experience of using the new mechanism to know whether it strikes sn appropriate balance \nbetween ex-pressiveness and performance. 1 Introduction All current programming languages that support \nexceptions take it for granted that the language definition should spec-ify, for a given program, what \nexception, if any, is raised when the program is executed. That used to be the case in microprocessor \narchitecture too, but it is no longer so. Some processors, notably the Alpha, provide so-called im-precise \nexceptions. These CPUs execute many instructions in parallel, and perhaps out of order; it follows that \nthe first exception (divide-by-zero, say) that is encountered is =On study leave from the Oxford University \nComputing Laboratory PermiSSiOn to make digital or hard copies of all or part of this work for Personal \nor classroom use is granted without fee provided that COPieS are not made or distributed for profit or \ncommercial advan-tage and that copies bear this notice and the full citation on the first page. To COPY \notherwise. to republish, to post on servars or to redistribute to lists, requires prior specific permission \nand/or a fee. SIGNAN 99 5199 Atlanta, GA, USA 0 1999 ACM l-581 13-083-X/99/0004...$5.00 not necessarily \nthe first that would be encountered in sim- ple sequential execution. One approach is to provide lots \nof hardware to 80I-t~ the mess out, and maintain the program-mer s illusion of a simple sequential execution \nengine; this is what the Pentium does. Another, taken by the Alpha, is to give a less precise indication \nof the whereabouts of the exception. In this paper we explore this same idea at the level of the programming \nlanguage. The compiler, or the programmer, might want to improve performance by changing the pro-gram \ns evaluation order. But changing the evaluation order may change which exception is encountered first. \nOne so-lution is to ban such transformations, or to restrict them to evaluations that provably cannot \nraise exceptions [19]. The alternative we propose here is to trade precision for performance: permit \nricher transformations, and make the language semantics less precise with respect to which excep-tion \nis raised. Note that the use of imprecise exceptions at the program-ming language level is not due to \nthe use of imprecise ex- ceptions at the hardware level. (Indeed, the latter may well prove ephemeral.) \nRather, both of these arise from the same motivation: permitting better optimization. It s quite possible \nto have imprecise exceptions at the program- ming language level but not at the hardware level, or vice \nversa. However, the use of imprecise exceptions at the pro- gramming language level may make it much \neasier for im-plementations to generate efficient code on hardware that has imprecise exceptions. We \nmake all this concrete by considering a particular pro-gramming language, Haskell, that currently lacks \nexcep-tions. Our contributions are as follows: . We review and critique the folk-lore on exception-handling \nin a lazy language like Haskell (Section 2). Non-functional programmers may find the idea of exceptions-as-values, \nas opposed to exceptions-as-control-flow, interesting. l We present a new design, based on sets of exceptions, \nto model imprecision about which exceptions can occur (Section 3). l We sketch a semantics for the resulting \nlanguage, using two layers: a denotational semantics for pure expres-sions (including exception-raising \nones), and an oper- ational semantics on top that deals with exception handling, as well as input/output \n(Section 4). Informed by this semantics, we show that various ex-tensions of the basic idea, such as \nresource-exhaustion interrupts, can readily be accommodated; while oth-ers, such as a pure exception \nhandler, are more trou-blesome (Section 5). There has been a small flurry of recent proposals and papers \non exception-handling in Hsskell [3, 13, 121. The distinctive feature of this paper is its focus on the \nsemantics of the resulting language. The trick lies in getting the nice fea-tures of exceptions (efficiency, \nimplicit propagation, and the like) without throwing the baby out with the bath-water and crippling the \nlanguage design. Those less interested in functional programming per se may nevertheless find interesting \nour development of the (old) idea of exceptions-as-values, and the trade-off between pre- cision and \nperformance. 2 The status quo ante Haskell has managed without exceptions for a long time, so it is \nnatural to ask whether they are either necessary or appropriate. We briefly explore this question, as \na way of setting the scene for the rest of the paper. Before we begin, it is worth identifying three \ndiierent ways in which exceptions are typically used in languages that sup-port them: Disaster recovery \nuses an exception to signal a (hopefully rare) error condition, such as division by zero or an assertion \nfailure. In a language like ML or Haskell we may add pattern-match failure, when a function is ap- plied \nto a value for which it does not have a defining equation (e.g. head of the empty list). The program-mer \ncan usually also raise an exception, using a prim- itive such as raise. The exception handler typically \ncatches exceptions from a large chunk of code, and performs some kind of recovery action. Exception handling \nused in this way provides a degree of modularity: one part of a system can protect itself against failure \nin another part of the system. Alternative return. Exceptions are sometimes used as an alternative way \nto return a value from a function, where no error condition is necessarily implied. An example might \nbe looking up a key in a fmite map: it s not necessarily an error if the key isn t in the map, but in \nlanguages that support exceptions it s not un-usual to see them used in this way. The exception handler \ntypically catches exceptions from a relatively circumscribed chunk of code, and serves mainly as an alternative \ncontinuation for a call. Asynchronous events. In some languages, an asyn- chronous external event, such \nas the programmer typ- ing C or a timeout, are reflected into the program- mer s model as an exception. \nWe call such things asyn- chronous exceptions, to distinguish them from the two previous categories, \nwhich are both synchronous ex- ceptions. 2.1 Exceptions as values No lazy functional programming language \nhas so far sup-ported exceptions, for two apparently persuasive reasons. Firstly, lazy evaluation scrambles \ncontrol flow. Evaluation is demand-driven; that is, an expression is evaluated only when its value is \nrequired [14]. As a result, programs don t have a readily-predictable control flow; the only productive \nway to think about an expression is to consider the value it computes, not the way in which the value \nis computed. Since exceptions are typically explained in terms of changes in control flow, exceptions \nand lazy evaluation do not appear very compatible. Secondly, exceptions can be explicitly encoded in \nvalues, in the existing language, so perhaps exceptions are in any case unnecessary. For example, consider \na function, f, that takes an integer argument, and either returns an integer or raises an exception. \nWe can encode it in Haskell thus: data ExVal a = OK a I Bad Exception f :: Int -> ExVal Int fx= . ..defn \nof f... The data declaration says that a value of type ExVal t is . either of the form (Bad ex), where \nex has type Exception, or is of the form (OK val), where val has type t. The type signature of f declares \nthat f returns a result of type ExVal Int; that is, either an Int or an exception value. In short, the \nexception is encoded into the value retzlrned by f. Any consumer of f s result is forced, willy nilly, \nto first per-form a case analysis on it: case (f 3) of OK val -> . . .normal case.. . Bad ex -> . ..handle \nexception... There are good things about this approach: no extension to the language is necessary; the \ntype of a function makes it clear whether it ca4 raise an exception; and the type system makes it impossible \nto forget to handle an exception. The idea of exceptions as values is very old [lo, 181. Subse- quently \nit was realised that the exception type constructor, ExVal, forms a monad [6, 91. Rather than having \nlots of ad hoc pattern matches on OK and Bad, standard monadic ma-chinery such as Haskell s do notation, \ncan hide away much of the plumbing. 2.2 Inadequacies of exceptions as values Encoding exceptions explicitly \nin an un-modified language works beautifully for the alternative-return usage of excep- tions, but badly \nfor the disaster-recovery use, and not at all for asynchronous events. There are several distinct prob- \nlems: l Increased strictness. When adding exception handling to a lazy program, it is very easy to accidentally \nmake the program strict, by testing a function argument for errors when it is passed instead of when \nit is used. l Excessive clutter. The principal feature of an excep-tion mechanism is that exceptions \npropagate implic-itly, without requiring extra clutter in the code be-tween the place the exception is \nraised and where it is handled. In stark contrast, the explicit-encoding approach forces all the intermediate \ncode to deal ex-plicitly (or monadically) with exceptional values. The resulting clutter is absolutely \nintolerable for those sit- uations where exceptions are used to signal disaster, because in these cases \npropagation is almost always re-quired. For example, where we would originally have written: (f x1 + \n(g Jr) we are now forced to write : case (f x> of Bad ex -> Bad ex OK xv -> case (g y) of Bad ex -> \nBad ex OK yv -> OK (xv+yv) These strictures do not apply where exceptions are used as an alternative \nreturn mechanism. In this case, the approach works beautifully because propagation isn t nearly so important. \nl Built-in exceptions are un-cat&#38;able. In Haskell, all the causes of failure recognised by the language \nitself (such as divide by zero, and pattern-match failure) are treated semantically as bottom (I), and \nare treated in practice by bringing the program to a halt. Haskell al-lows the program to trigger a similar \nfailure by calling the standard function error, whose type is: error :: String -> a So, evaluating the \ncall (error YJrk ) halts execution, printing Urk on standard error. The language offers no way to catch \nand recover from any of these (syn-chronous) events. This is a serious problem when writ- ing programs \ncomposed out of large pieces over which one has little control; there is just no way to recover from \nfailure in any sub-component. l Loss of modularity and code re-use, especially for higher-order functions. \nFor example, a sorting func-tion that takes a comparison function as an argu-ment would need to be modified \nto be used with an exception-raising comparison function. The monadic version is nearly &#38;S bad. . \nPoor eficiency. Exceptions should cost very little if they don t actually occur. Alas, an explicit encoding \ninto Haskell values forces a test-and-propagate at every call site, with a substantial cost in code size \nand speed. . Loss of transformations. Programs written in a monadic style have many fewer transformations \nthan their pure counterparts. We elaborate on this problem in Section 3. . No asynchronous exceptions. \nAsynchronous excep-tions, by their nature, have nothing to do with the value of the unfortunate expression \nthat happens to be under evaluation when the external event occurs. Since they arise from external sources, \nthey clearly cannot be dealt with as an explicitly-encoded value. 2.3 Goals With these thoughts in mind, \nwe have the following goals: Haskell programs that don t invoke exceptions should have unchanged semantics \n(no clutter), and run with unchanged efficiency. All transformations that are valid for ordinary Haskell \nprograms should be valid for the language extended with exceptions. It turns out that we do not quite \nachieve this goal, for good reasons (Section 4.5). It should be possible to reason about which exceptions \na program might raise. For example, we might hope to be able to prove that non-recursive programs will \nter-minate, and programs that don t use arithmetic can t raise division by zero. In so far as non-determinism \narises, it should be possi- ble for,the programmer to confine the non-determinism to a clearly-delineated \npart of the program. These properties may seem obvious, but they are a little tricky to achieve. In existing \nlanguages that support excep-tions, such as ML or Ada, the need to maintain the exception semantics noticeably \nconstrains the valid set of transforma- tions and optimisations that a programmer or compiler can perform. \nCompilers often attempt to infer the set of possi- ble exceptions with a view to lifting these restrictions, \nbut their power of inference is limited; for example, they must be pessimistic across module boundaries \nin the presence of separate compilation. We claim that our design retains al-most all useful opportunities \nfor transformation, using only the monadic type system built into Haskell. No separate effect analysis \nis required. 3 A new design Adding exceptions to a lazy language, as opposed to encod- ing exceptions \nin the un-extended language, has received relatively little attention until recently. Dornan and Ham-mond \ndiscussed adding exceptions to the pure (non-I/O) part of a lazy language [2], and there has been a flurry \nof recent activity [3, 13, 121. Drawing on this work, we propose a programming interface for an exceptions \nmechanism. This sets the scene for the core of our paper, the semantics for the resulting language. 3.1 \nThe basic idea As discussed in Section 2.1, our first design decision is more or less forced by the fact \nthat Haskell is a lazy language: ez-ceptions are associated with data values, rather than with control \nflow. This differs fundamentally from the stan-dard approach to exceptions taken for imperative, or strict \nfunctional, languages, where exceptions are associated with control flow rather than with data flow. \nOne place that exceptions-as-values does show up in the imperative world is the NaNs (not-a-number) and \ninfinities of the IEEE float- ing point standard, where certain bit-patterns encode ex-ceptional values, \nwhich are propagated by the floating point operations [20]. We extend this exceptions-as-values idea \nuniformly to values of any type. A value (of any type) is either a normal value, or it is an exceptional \nvalue. An exceptional value contains an exception, and we must say what that is. The data type Exception \nis the type of exceptions. It is a new algebraic data type, supplied as part of the Haskell Prelude, \ndefined something like this: data Exception = DivideByZero I Overflow I UserError String . . . One could \nimagine a simpler type (e.g. encoding an ex- ception as an integer, or a string), or a richer type (e.g. \na user-extensible data type, such as is provided by ML), but this one is a useful compromise for this \npaper. Nothing we say depends on the exact choice of constructors in the data type; hence the . . . . \nFor each type a, the new, primitive function raise maps an Exception into an exceptional value of type \na: raise :: Exception -> a Here, immediately, we see a difference from the explicit-encoding approach. \nEvery type in the language contains exeptional values -previously only the type ExVal t had that possibility. \nWe can also see that the same Exception type serves to represent an exception, regardless of the type \ninto which the exception is embedded. The previously-primitive function error can now readily be defined \nusing raise: error :: String -> a error str = raise (UserError str) Next, we need to be able to catch \nexceptions. The new, primitive function getException takes a value4 and deter- mines whether or not it \nis an exceptional value : getException :: a -> ExVal a 2We will see later that there is a fundamental \nproblem with giv-ing getException this type, but we defer discussion of this point to Section 3.5. In \neffect, getException reifies the implicit presence or ab-sence of an exception in its argument to an \nexplicit dis-criminated union, represented by the new Prelude data type ExVal: data ExVal a = OK a I \nBad Exception Here is an example of how getException might be used: case getException (goop x) of OK \nval -> normal_case val Bad exn -> recovery-case exn Here, getException catches any exception raised \nwhile goop is evaluated, and presents the result as a value of type ExVal. The case expression scrutinises \nthat value and takes appro- priate action. 3.2 Propagation The whole point of exceptions is, of course, \nthat they propa- gate automatically. So integer addition, for example, should deliver an exceptional \nvalue if either of its arguments is an exceptional value. In a lazy language, however, we have to re-examine \nour notion of propagation. In particular, an exceptional value might lurk inside an unevaluated function \nargument or data structure. For example, consider the zipWith function: zipWith f [I Cl = Cl zipWith \nf (x:xs) (y:ys) = f x y : zipWith f xs ys zipWith f xs Ys = error Unequal lists A call to zipWith may \nreturn an exception value directly -for example, zipWith (+) [I Cl]. A call to zipWith may also return \na list with an exception value at the end -for example, zipWith (+I Cl1 Cl ,21. Finally, it may deliver \na list whose spine is fully defined, but some of whose elements are exceptional values -for example zipWith \n(/I Cl,21 Cl,Ol To repeat: it is values not calls that may be exceptional, and exceptional values may, \nfor example, hide inside lazy data structures. To be sure that a data structure contains no exceptional \nvalues one must force evaluation of all the elements of that structure (this can be done using Haskell \ns built-in seq function). 3.3 Implementation One advantage of the story so far is that it is readily, \nand cheaply, implementable. We certainly do not want the space and time cost of explicitly tagging every \nvalue with an in- dication of whether it is normal or exceptional . Fortu-nately, the standard exception-handling \nmechanisms from procedural languages work perfectly well: getException forces the evaluation of its argument \nto head normal form; before it begins this evaluation, it marks the evaluation stack in some way. raise \nex simply trims the stack to the top-most getException mark, and returns Bad ex as the result of getException. \nl If the evaluation of the argument to getException completes without provoking a call to raise, then \ngetException returns OK val, where val is the value of the argument. Actually, matters are not quite \nas simple as we suggest here. In particular, trimming the stack after a call (raise ex) we must be careful \nto overwrite each thunk that is under evalu-ation with (raise ax). That way, if the thunk is evaluated \nagain, the same exception will be raised again, which is as it should be3. The details are described \nby [12], and need not concern us here. The main point is that the efficiency of programs that do not \ninvoke exceptions is unaf%cted. Indeed, the efficiency of any function that does not invoke exceptions \nexplicitly is unatfected. Notice that an exceptional value behaves as a first class value, but it is \nnever explicitly represented as such. When an exception occurs, instead of building a value that represents \nit, we look for the exception handler right away. The semantic model (exceptional values) is quite different \nfrom the implementation (evaluation stacks and stack trim-ming). The situation is similar to that with \nlazy evaluation: a value may behave as an infinite list, but it is certainly never explicitly represented \nas such.  3.4 A problem and its solution There is a well-known difficulty with the approach we have \njust described: it invalidates many useful transformations. For example, integer addition should be commutative; \nthat is, ei+e2 = ez+ei. But what are we to make of this expres-sion? getException ( (l/O) + (error YJrk \n> 1 Does it deliver DivideByZero or UserError Urk ? Urk in-deed! There are two well known ways to address \nthis prob-lem, and one more cunning one which we shall adopt: l Fix the evaluation order, as part of \nthe language se-mantics. For example, the semantics could state that + evaluates its first argument first, \nso that if its first ar-gument is exceptional then that s the exception that is returned. This is the \nmost common approach, adopted by (among others) ML, FL, and some proposals for Haskell [2]. It gives \nrise to a simple semantics, but has the Very Bad Feature that it invalidates many useful transformations \n-in particular, ones that alter the order of evaluation. This loss of transformations is a serious weakness. \nWilliams, Aiken, and Wimmers give numerous exam-ples of how the presence of exceptions can seriously \nweaken the transformation algebra of the (strict) lan-guage FL [19]. For a lazy language, the loss of \ntrans- formations would be even more of a catastrophe. In particular, Haskell compilers perform strictness \nanal-ysis to to turn call-by-need into call-by-value. This 3Real implementations overwrite a thunk with \na black hole when its evaluation is begun to avoid a celebrated space leak [5]. That is why, when an \nexception causes their evaluation to be abandoned, they must be overwritten with something more informative. \ncrucial transformation changes the evaluation order, by evaluating a function argument when the function \nis called, rather than when the argument is demanded. Bather than remove such transformations altogether, \noptimising compilers often perform some variant of ef- fect analysis, to identify the common case where \nexcep- tions cannot occur (e.g. (81). They use this informa-tion to enable the otherwise-invalid transformations. \nWilliams, Aiken, and Wimmers describe a calculus for the language FL that expresses the absence of excep- \ntions as a special program annotation; they can then give a precise characterisation of the transformation \nalgebra of this augmented language [19]. What all these approaches have in common is that use-ful transformations \nare disabled if the sub-expressions are not provably exception-free. . Go non-deterministic. That is, \ndeclare that + makes a non-deterministic choice of which argument to eval-uate first. Then the compiler \nis free to make that choice however it likes. Alas, this approach exposes non-determinism in the source \nlanguage, which also invalidates useful laws. In particular, p reduction is not valid any more. For example, \nconsider: let x = (l/O) + (error Urk ) in getException x == getException x As it stands, the value of \nthis expression is presumably True. But if the two occurrences of x are each replaced by x s right hand \nside, then the non-deterministic + might (in principle) make a different choice at its two occurrences, \nso the expression could be False. We count this too high a price to pay. . The more cunning choice is \nto return both exceptions! That is, we redefine an exceptional value to contain a set of exceptions, \ninstead of just one; and + takes the union of the exception sets of its two arguments. Now (i/O) + (error \nUrk ) returns an exceptional value including both DivideByZero and UserError Urk , and (semantically) \nit will do so regardless of the order in which + evaluates its arguments. The beauty of this approach \nis that almost all trans-formations remain valid, even in the presence of excep- tions (Section 4.5 discusses \nthe almost ). No analysis required! 3.5 Fixing getException The allegedly cunning choice may have fixed \nthe commuta-tivity of +, but, now that an exceptional value can contain a set of exceptions, we must \nrevisit the question of what getException should do. There are two possibilities. One alternative is \nfor getException to return the complete set of exceptions (if any) in its argument value. This would \nbe an absolute disaster from an implementation point of view! It would mean that the implementation would \nreally have to maintain a set of exceptions; if the first argument to + failed, then the second would \nhave to be evaluated anyway so that any exceptions in it could be gathered up. The cunning choice is \nonly cunning because there is another alternative: getException can choose just one member of the set \nof exceptions to return. Of course, that simply exposes the non-determinism again, but we can employ \na now-standard trick [l]: put getException in the IO monad. Thus, we give getException the following \ntype: getException : : a -> IO (ExVal a) To make sense of this new definition, we digress briefly to \nin- troduce Haskell s IO monad. In Haskell, a value of type IO t is a computation that might perform \nsome input/output, be-fore eventually returning a value of type t. A value of type IO t is a first-class \nvalue -it can be passed as an argu-ments, stored in a data structure -and evaluating it has no side effects. \nOnly when it is performed does it have an effect. An entire Haskell program is a single value of type \nIO 0; to run the program is to perform the specified com-putation. For example, here is a complete Haskell \nprogram that gets one character from standard input and echoes it to standard output4: main : : IO 0 \nmain = getChar >>= (\\ch -> putChar ch >>= (\\O -> return 0 1) The types of the various functions involved \nare as follows: (>>=) :: IO a -> (a -> IO b) -> IO b return :: a -> IO a getChar :: IO Char putChar : \n: Char -> IO (1 The combinators >>= glues together two IO computations in sequence, passing the result \nfrom the first to the second. return does no input/output, simply returning its argu-ment. getChar gets \na character from the standard input and returns it; putChar does the reverse. When main is per- formed, \nit performs getchar, reading a character from stan-dard input, and then performs the computation obtained \nby applying the \\ch -> . . . abstraction to the character, in this case putChar ch. A more complete discussion \nof monadic I/O can be found in [17]. Now we return to the type of getException. By giving it an IO type \nwe allow getException to perform input/output. Hence, when choosing which of the exceptions in the set \nto choose, getException is free (although absolutely not re-quired) to consult some external oracle (the \nFT Share In-dex, say). Each call to getException can make a different choice; the same call to getException \nin different runs of the same program can make a diierent choice; and so on. Beta reduction remains valid. \nFor example the meaning of: let x = (l/O) + error Urk in getException x >>= (\\vl -> getException x >>= \n(\\v2 -> return (vl==v2))) is unaffected if both occurrences of x are replaced by x s right hand side, \nthus: 4The I\\ is Haskell s notation for X getException ((l/O) + error Urk ) >>= (\\vl -> getException \n((l/O) + error Urk ) >>= (\\v2 -> return (vl==v2))) Why? Because whether or not this substitution is made, \ngetException will be performed twice, making an inde-pendent non-deterministic choice each time. Like \nany IO computation, (getException e) can be shared, and even evaluated, without actually performing the \nnondeterminis-tic choice. That only happens when the computation is per- formed. The really nice thing \nabout this approach is that the stack- trimming implementation does not have to change. The set of exceptions \nassociated with an exceptional valve is repre- sented by a single member, namely the exception that hap-pens \nto be encountered first. getException works just as be- fore: mark the evaluation stack, and evaluate \nits argument. Successive runs of a program, using the same compiler opti-misation level, will in practice \ngive the same behaviour; but if the program is recompiled with diierent optimisation set-tings, then \nindeed the order of evaluation might change, so a different exception might be encountered first, and \nhence the exception returned by getException might change. The idea of using a single representative \nto stand for a set of values, from which a non-deterministic choice is made, is based on an old paper \nby Hughes and O Donnell [15]. Our contribution is to apply this idea in the setting of exception handling. \nThe key observation is that non-determinism in the exceptions can be kept separate from non-determinism \nin the normal values of a program. 4 Semantics So far we have reasoned informally. In this section we \ngive a precise semantics to (a fragment of) Haskell augmented with exceptions. Here are two difficulties. \nConsider loop + error Urk Here, loop is any expression whose evaluation di- verges. It might be declared \nlike this: loop = f True where f x = f (not x> So, does (loop + error Urk ) loop forever, or does it \nreturn an exceptional value? Answer: it all depends on the evaluation order of +. As is often the case, \nbottom muddies the waters. Is the following equation true? case x of (a,b) -> case y of (p,q) -> e = \ncase y of (p,q) -> case x of (a.b) -> e In Haskell the answer is yes ; since we are going to evaluate \nboth x and y, it doesn t matter which order we evaluate them in. Indeed, the whole point of strict- ness \nanalysis is to figure out which things are sure to be evaluated in the end, so that they can be evaluated \nin advance [7]. But if x and y are both bound to ex-ceptional values, then the order of the cases clearly \ndetermines which exception will be encountered. Un-like the + case, it is far from obvious how to combine \nthe exceptional value sets for x and y: in general the right hand side of a case alternative might depend \non the variables bound in the pattern, and it would be unpleasant for the semantics to depend on that. \nvariable constant application e 7 i  / ii; abstraction . . . e, constructors 1 caseeof C.. . pi-h-i \n; . . . 1 matching raise e raise exception el +e2 primitives j fix e ..- P ..-c x1 . . . xn pattern \nThe rest of this section gives a denotational semantics for Haskell extended with exceptions, that addresses \nboth of these problems. We solve the first by identifying I with the set of all possible exceptions; \nwe solve the latter by (semanti- cally) evaluating the case alternatives in exception-finding mode . \n4.1 Domains First we describe the domain [r] that is associated with each Haskell type 7. We use a rather \nstandard monadic translation, for a monad M, defined thus: M t = tl+P(E)I &#38; = {DivideByZero, Overflow, \nUserError,. . .} The I + in this equation is coalesced sum; that is, the bot- tom element of [r]* is \ncoalesced with the bottom element of P(C)*. The set E is the set of all the possible synchronous exceptions; \nto simplify the semantics we neglect the String argument to UserError. P(E) is the lattice of all subsets \nof E, under the ordering Sl g 92 I Sl 2 52 That is, the bottom element is the set E, and the top ele-ment \nis the empty set. This corresponds to the idea that the fewer exceptions that are in the exceptional \nvalue, the more information the value contains. The least informative value contains all exceptions. \nThis entire lattice is lifted, by adding an extra bottom element, which we also identify with a set of \nexceptions: I = &#38; U {NonTermination} At first we distinguished I from the set of all exceptions, \nbut that turns out not to work. Instead, we identify I with the set of all exceptions, adding one new \nconstructor, NonTermination, to the Exception type: data Exception = . . . --(as before) 1 NonTermination \n This construction of P(E)1 is a very standard semantic cod-ing trick; it is closely analogous to a \ncanonical representation of the Smyth powerdomain over a flat domain, given by [ll]. Here is an alternative, \nand perhaps more perspicuous, way to define M, in which we tag normal values with Ok, and Figure 1: Syntax \nof a tiny language exceptional values (including I) with Bad: Mt = {OkvIvct}u {BadsIsE&#38;}U {Bad \n(E U {NonTermination})) One might wonder what sort of a value Bad {} is: what is an exceptional value \ncontaining the empty set of exceptions? Indeed, such a value cannot be the denotation of any term, but \nwe will see shortly that it is nevertheless a very useful value for defining the semantics of case and \nfor reasoning about it (Section 4.3). Now that we have constructed the exception monad, we can translate \nHaskell types into domains in the usual way: [Int] = M 2 [n-)72] = M ([n] + [72]) [(71,T22)] = M (8~11 \nx [~2]) . . . etc . . . We refrain from giving the complete encoding for arbitrary recursive data types, \nwhich is complicated. The point is that we simply replace the normal Haskell monad, namely lifting, with \nour new monad M. 4.2 Combinators Next, we must give the denotation, or meaning, of each form of language \nexpression. Figure 1 gives the syntax of the small language we treat here. The denotation of an expres- \nsion e in an environment p is written [e]P. We start with +: 1.3 + e2]p =w @ v2 if Ok VI = [el]p and \nOk 212= [ez]p = Bad (S([ei]p) U S([es]p)) otherwise The first equation is used if both arguments are \nnormal val-ues. The second is used if either argument is an exceptional value, in which case the exceptions \nfrom the two arguments are unioned. We use the auxiliary function SO, which re-turns the empty set for \na normal value, and the set of ex- ceptions for an exceptional value: S(Ok v) = 0 S(Bad s) = s The auxiliary \nfunction @ simply does addition, checking for overflow: ul e3 v2 = Ok (Vl + v2) if -231 ( (vi + ~2) ( \n231 = Bad {Overflow} otherwise The definition of [+] is monotonic with respect to C, as it must be. The \nfact that + is strict in both arguments is a consequence of the fact that I is the set of all exceptions; \na moment s thought should convince you that if either argu-ment is I then so is the result. Next, we \ndeal with raise: [raise elp = Bad s if Bad s = [e]p = Bad {C} if Ok C= [e]p Thus equipped, we can now \nunderstand the semantics of the problematic expression given above: loop + error Urk Its meaning is the \nunion of the set of all exceptions (which is the value of loop), and the singleton set UserError IIrk \n, which is of course just I, the set of all exceptions. The rules for function abstraction and application \nare: Px.elp = Ok Gw~el~[~l~l) h e2lp = f (hip) if Ok f = [el]p = Bad(s U S([ea]p) if Bad s = [el]p A \nlambda abstraction is a normal value; that is Xz.1 # 1. The (more purist) identification of these two \nvalues is impossible to implement: how can getException distinguish Xz.1 from Xz.w, where w # I? Fortunately, \nin Haskell Az.1 and I are indeed distinct values. Applying a normal function to a value is straightforward, \nbut matters are more interesting if the function is an ex-ceptional value. In this case we must union \nits exception set with that of its argument, because under some circum-stances (notably if the function \nis strict) we might legiti-mately evaluate the argument first; if we neglected to union in the argument \ns exceptions, the semantics would not allow this standard optimisation. That is why we do not use the \nsimpler definition: We have traded transformations for precision. Notice, how-ever, that we must not \nunion in the argument s exceptions if the function is a normal value, or else we would lose /3 reduction; \nconsider (Xx.3)(1/0) The rules for constants and constructor applications are sim- ple; they both return \nnormal values. Constructors are non-strict, and hence do not propagate exceptions in their argu-ments. \nVariables and fixpoints are also easy. [k]p = Ok k [C el . ..en]p = Ok CC (BelIP). . . (k4d) I+ = P(X) \n[fix elp = k=O  4.3 case expressions Haskell contains case expressions, so we must give them a semantics. \nHere is the slightly surprising rule: [case e of {pi->ri)]p = m4~/Pil if Ok v = [e]P and v matches pi \n= Bad (3 U CUi W-MBa~ O/P~)) if Bad s = [e]P The first case is the usual one: if the case scrutinee evalu-ates \nto a normal value v, then select the appropriate case alternative. The notation is a little informal: \np[v/pi] means the environment p with the free variables of the pattern pi bound to the appropriate components \nof v. The second equation is the interesting one. If the scrutinee turns out to be a set of exceptions \n(which, recall, includes I), the obvious thing to do is to return just that set -but doing so would invalidate \nthe case-switching transfor-mation. Intuitively, the semantics must explore all the ways in which the \nimplementation might deliver an exception, so it must evaluate all the branches anyway, in Lexception-finding \nmode . We model this by taking the denotations of all the right hand sides, binding each of the pattern-bound \nvariables to the strange value Bad {}. Then we union to-gether all the exception sets that result, along \nwith the ex-ception set from the scrutinee. The idea is exactly the same as in the special case of +, \nand function application: if the first argument of + raises an exception we still union in the exceptions \nfrom the second argument. Here, if the case scru- tinee raises an exception, we still union in the exceptions \nfrom the alternatives. Remember that there is no implication that an implemen-tation will do anything \nother than return the first exception that happens to be encountered. The rather curious seman-tics is \nnecesary, though, to validate transformations that change the order of evaluation, such as that given \nat the beginning of Section 4.  4.4 Semantics of getException So far we have not mentioned getException. \nThe seman-tics of operations in the IO monad, such as getException, may involve input/output or non-determinism. \nThe most straightforward way of modelling these aspects is by giv- ing an operational semantics for the \nIO layer, in contrast to the denotational semantics we have given for the purely-functional layer. We \ngive the operational semantics as follows. From a seman- tic point of view we regard IO as an algebraic \ndata type with constructors return, >>=, putchar, getchar, getException. The behaviour of a program is \nthe set of traces obtained from the following labelled transition system, which acts on the denotation \nof the program. One advantage of this presen-tation is that it scales to other extensions, such as adding \nconcurrency to the language [16]. Here are the structural transition rules: Some transformations that \nare identities in Haskell become refinements in our new system. Consider: VI + 21.2 (VI >>= k) + (v2 \n>>=k)  lhs = (case e of < True -> f; False -> g )) ((return v) >>= k) + (Ic v) rhs = case e of < True \n-> (f x); False -> (g X) ) The first ensures that transitions can occur inside the first operand of the \n>>= constructor; the second explains that a return constructor just passes its value to the second argu- \nment of the enclosing >>=. The rules for input/output are now quite simple: getChar 3 return c putChar \nc 3 return () The ?c on top of the arrow indicates that the transition takes place by reading a character \nc from the environment; and inversely- for !c r Now we can get to the semantics of exceptions. The rules \nare: getException (Ok v) + return (OK (OIE VI> getException (Bad 8) + return (Bad z) if XE.9 getException \n(Bad s) + getException (Bad s) if NonTermination E s If getException scrutinises a normal value, it just \nreturns it, wrapped in an OK constructor. For exceptional values, there are two choices: either l pick \nan arbitrary member of the set of exceptions and return it, or l if NonTermination is in the set of exceptions, \nthen make a transition to the same state. The transition rules for getException are deliberately non-deterministic. \nIn particular, if the argument to getException is I, then getException may diverge, or it may return \nan arbitrary exception. To execute a Haskell program, one performs the computa-tion main, which has type \nIO 0. In the presence of excep- tions, the value returned might now be Bad x, rather than Ok 0. This \nsimply corresponds to an uncaught exception, which the implementation should report.  4.5 Transformations \nOur overall goal is to add exceptions to Haskell without losing useful transformations. Yet it cannot \nbe true that we lose no transformations. For example, in Haskell as it stands, the following equation \nholds: error This = error That Why? Because both are semantically equal to 1. In our semantics this equality \nno longer holds -and rightly not! So our semantics correctly distinguishes some expressions that Haskell \ncurrently identifies. Using e = raise E, x = raise X, and f = g = Xv.1, we get [lhs]p = Bad {E,X} but \n[rhs]p = Bad {E}. Hence, lhs _C rhs, but not lhs = rhs. We argue that it is legitimate to perform a transformation \nthat increases information -in this case, changing lhs to rhs reduces uncertainty about which exceptions \ncan be raised. We currently lack a systematic way to say which identities continue to hold, which turn \ninto refinements, and which no longer hold. We conjecture that the lost laws deserve to be lost, and \nthat optimising transformations are either identities or refinements. It would be interesting to try \nto formalise and prove this conjecture. 5 Variations on the theme 5.1 Asynchronous exceptions All the \nexceptions we have discussed so far are synchronous exceptions (Section 2). If the evaluation of an expression \nyields a set of synchronous exceptions, then another eval-uation of the same expression will yield the \nsame set. But what about asynchronous exceptions, such as interrupts and resource-related failures (e.g. \ntimeout, stack overflow, heap exhaustion)? They differ from synchronous exceptions in that they perhaps \nwill not recur (at all) if the same pro-gram is run again. It is obviously inappropriate to regard such \nexceptions as part of the denotation of an expression. Fortunately, they can fit in the same general \nframework. We have to enrich the Exception type with constructors indicating the cause of the exception. \nThen we simply add to getException s abilities. Since getException is in the IO monad, it can easily \nsay if the evaluation of my argument goes on for too long, I will terminate evaluation and return Bad \nTimeout , and similarly for interrupts and so on. We express this formally as follows: getException v \nf% return (Bad z) if x is an asynchronous exception The ix above the arrow indicates that the transition \nmay take place only when an asynchronous event x is received by the evaluator. Notice that v might not \nbe an exceptional value -it might be say, 42 -but if the event x is received, getException is nevertheless \nfree to discard v and return the asynchronous exception instead. In the case of a key- board interrupt, \nthe event ControlC is injected; in the case of timeout, some presumed external monitoring system in-jects \nthe event Timeout if evaluation takes too long; and so on. There is a fascinating wrinkle in the implementation \nof asyn- chronous exceptions: when trimming the stack, we must overwrite each thunk under evaluation \nwith a kind of re-sumable continuation , rather than a computation which raises the exception again. \nThe details are in [13].  5.2 Detectable bottoms There are some sorts of divergence that are detectable \nby a compiler or its runtime system. For example, suppose that black was declared like this: black = \nblack + 1 Here, black is readily detected as a so-called black hole by many graph reduction implementations. \nUnder these cir- cumstances, @Exception black is permitted, but not re-quired, to return Bad NonTermination \ninstead of going into a loop! Whether or not it does so is an implementation choice -perhaps implementations \nwill compete on the skill with which they detect such errors.  5.3 Fictitious exceptions There is actually \na continuum between our semantics and the Yixed evaluation order semantics, which fully deter-mines which \nexception is raised. As one moves along the spectrum towards our proposal, more compiler transforma-tions \nbecome valid -but there is a price to pay. That price is that the semantics becomes vaguer about which \nex-ceptions can be raised, and about when non-termination can occur. Our view is that we should optimise \nfor the no-exception case, accepting that if something does go wrong in the program, then the semantics \ndoes not guarantee very precisely what exception will show up. An extreme, and slightly troubling, case \nis this: getException loop Since loop has value I, g&#38;Exception is, according to our semantics, justified \nin returning Bad DivideByZero, or some other quite fictitious exception -and in principle a compiler \nrefinement might do the same. We sought a way to give loop the denotation Bad {NonTermination} rather \nthan (the less informative) I, but we know of no consistent way to do so. The modelling of non-termination \nto include all other behaviours is characteristic of the de-notational semantics of non-determinism. \nIt means that set inclusion gives a simple interpretation of program correct-ness, encompassing both \nsafety and liveness properties. It ensures that recursion can be defined as the weakest fixed point of \na monotonic function, and that this fixed point can be computed as the limit of a (set-wise) descending \nchain of approximations. But what is more important for our pur-poses is that it gives maximal freedom \nto the compiler, by assuming that non-termination is never what the program- mer intends . An operational \nsemantics would model more precisely what happens, and hence would not suffer from the problem of %deed, \nthere are a number of situations in which it is useful to be able to assume that a value is not 1. For \nexample, if v is not I, then the following law holds: CasevofITrue->a;Falsa->e) = e Our compiler has \na flag -fno-pedantic-bottoms that enables such transformations, in exchange for the programmer undertaking \nthe proof obligation that no sub-expression in the program has value 1. fictitious exceptions. Arguably, \nfor reasoning about diver- gent programs, the programmer should use an operational semantics anyway. \nBecause, in the end, it seems unlikely that a compiler will gratuitously report a fictitious excep- tion \nwhen the program gets into a loop, so this semantic technicality is unlikely to have practical consequences. \n  5.4 Pure functions on exceptional values Is it possible to do anything with an exceptional value other \nthan choose an exception from it with getException? Fol-lowing [15], one possibility suggests itself \nas a new primitive function (i.e. one not definable with the primitives so far described): mapException \n:: (Exception -> Exception) -> a -> a Semantically, mapException applies its functional argument to \neach member of the set of exceptions (if any) in its sec-ond argument; it does nothing to normal values. \nFrom an implementation point of view, it applies the function to the sole representative (if any) of \nthat set. Here s an example of using mapException to catch all exceptions in e and raise UserError YJrk \ninstead: mapException (\\x -> UserError Urk ) e Notice that mapException does not need to be in the IO \nmonad to preserve determinism. In short, mapException raises no new technical dificulties, although its \nusefulness and desirability might be debatable. mapException maps one kind of exception to another, but \nit doesn t let us get from exceptions back into normal values. Is it possible to go further? Is it possible, \nfor example, to ask is this an exceptional value ? isException :: a -> Boo1 (It would be easy to define \nisException with a monadic type a -> IO Bool; the question is whether it can have a pure, non-monadic, \ntype.) At first isException looks reasonable, because it hides just which exception is being raised -but \nit turns out to be rather problematic. What is the value of the following expression? isException ((l/O) \n+ loop) If the compiler evaluates the first argument of + first, the result will be True; but if the \ncompiler evaluates the sec-ond argument of + first, the computation will not terminate. Two diierent \nimplementations have delivered two different values! It is quite possible to give a perfectly respectable \ndenota-tional semantics for isException -in fact there are two different such semantics that we might \nuse, the optimistic one isException (Bad s) = True isException (Ok v) = False or the pessimistic one \nisException (Bad s) = 1 if NonTermination E s isException (Bad s) = True if NonTermination $?!s isException \n(Ok v) = False The trouble is that neither of these semantics are efficiently implementable, because \nthey require the implementation to detect nontermination. Consider our example isException ((l/O) + \nloop) An implementation that evaluates the arguments of + right- to-left would evaluate loop before l/O; \nhence, the call to isException would loop, i.e. evaluate to I, rather than returning True as the optimistic \nsemantics requires. But conversely, an implementation that evaluates the arguments of + left-to-right \nwould evaluate l/O before loop; hence, the call to isException would return True, rather than I as the \npessimistic semantics requires. Since we want implemen-tations to be able to evaluate arguments in any \norder, nei-ther the optimistic nor the pessimistic semantics will work. There are a number of possible \nthings we could say: 1. Because isException is unimplementable, it should be banned. 2. Programmers \nmay use isException, but when they do so they undertake a proof obligation that its argument is not 1. \nIf this can be assumed, the implementation is in no difficulty (c.f. Section 5.3). 3. The denotational \nsemantics for isException should be the pessimistic one; to make it implementable, the language semantics \nshould be changed so that result of the program is defined to be any value that is the same as or more \ndefined than the program s denotation. If the program yields I, then any value at all could be delivered. \n This alternative has the undesirable property that a program that goes into an infinite loop would be \njusti- fied in returning an IO computation that (say) deleted your entire f&#38;store. 4. The denotational \nsemantics for isException should be the optimistic one; to make it implementable, the language semantics \nshould be changed so that result of the program is defined to be any value that is the same as or less \ndefined than the program s denotation. I would always be a valid result.  This alternative has the undesirable \nproperty that an implementation could, in theory, abort with an error message or fail to terminate for \nany program at all, including programs that do not use isException. Still, in comparison to the previous \nalternative, at least the failure mode is much less severe: the semantics would only allow the implementation \nwould to loop or abort, not to perform arbitrary I/O operations. The latter two options would both require \na significant global change to Haskell s semantics, and even then, nei-ther of them really captures the \nintended behaviour with sufficient precision. It would be possible to refine these ap- proaches to give \nmore precision, but only at the cost of some additional semantic complexity. Therefore we prefer the \nsec-ond option, renaming isException to unsafeIsException to highlight the proof obligation. Other declarative \nlanguages, particularly logic programming languages such as GGdel and Mercury already make a dis- tinction \nbetween the declarative (i.e. denotational) seman-tics and the operational semantics similar to that \nmen-tioned in the fourth possibility above [4]. In Mercury, for example, the operational semantics allows \nnon-termination in some situations even though the declarative semantics specifies that the program should \nhave a result other than non-termination. So if our proposal for Haskell were to be adopted to other \nlanguages for which the operational seman-tics is already incomplete (in the above sense) with respect \nto the declarative semantics, then a refinement of the fourth alternative might well be the best approach. \n6 Other languages We have described a design for incorporating exceptions into Haskell. In this section \nwe briefly relate our design to that in other languages. First, it is clear that our design is somewhat \nless expressive than that in other languages; we will take ML as a typical example. In ML it is posssible \nto completely encapsulate a function that makes use of exceptions: one can declare an exception locally, \nraise it, and handle it, all without this implementation becoming visible to the function s caller. In \nour design, one cannot handle an exception without using the IO monad. Furthermore, the IO monad is (by \ndesign) like a trap door: you cannot encapsulate an I/O performing computation inside a pure function \n-and rightly not! Though we do not yet have much experience of using exceptions in Haskell, we speculate \nthat the fact that getException is in the IO monad will not prove awkward in practice, for several reasons: \nl Only exception handling, using getException, is af- fected. One can raise an exception without involving \nthe IO monad at all. l Most disaster-recovery exception handling is done near the top of the program, \nwhere all other input/output is in any case performed. l Much local exception handling can be done by \nencod- ing exceptions as explicit values (Section 2.1). No doubt there will remain situations where the \nlack of a pure getException will prove annoying. One altern* tive would be to provide an unsafeGetException \n(analogous to unsafeIsException; Section 5.4), with associated proof obligations for the programmer. \nSecond, the big payoff of our approach is that we lose no (useful) transformations compared to a guaranteed-exception-free \nprogram. Could the same technique be used in other languages, such as ML or Java? It is hard to see how \nit could apply directly; our approach depends crucially on distinguishing computations in the IO monad \n(whose transformations are restricted by the possibility of side ef- fects and non-determinism) from \npurely-functional expres-sions (whose transformations are unrestricted). Nevertheless, standard effect \nanalyses for ML and Java seek to find which portions of the program cannot raise an ex-ception, whereas \nin our system transformations are limited   \n\t\t\t", "proc_id": "301618", "abstract": "Some modern superscalar microprocessors provide only <i>imprecise exceptions</i>. That is, they do not guarantee to report the same exception that would be encountered by a straightforward sequential execution of the program. In exchange, they offer increased performance or decreased chip area (which amount to much the same thing).This performance/precision tradeoff has not so far been much explored at the programming language level. In this paper we propose a design for imprecise exceptions in the lazy functional programming language Haskell. We discuss several designs, and conclude that imprecision is essential if the language is still to enjoy its current rich algebra of transformations. We sketch a precise semantics for the language extended with exceptions.The paper shows how to extend Haskell with exceptions without crippling the language or its compilers. We do not yet have enough experience of using the new mechanism to know whether it strikes an appropriate balance between expressiveness and performance.", "authors": [{"name": "Simon Peyton Jones", "author_profile_id": "81100271851", "affiliation": "Microsoft Research Ltd., Cambridge", "person_id": "PP40033275", "email_address": "", "orcid_id": ""}, {"name": "Alastair Reid", "author_profile_id": "81406600118", "affiliation": "Yale University", "person_id": "PP77039764", "email_address": "", "orcid_id": ""}, {"name": "Fergus Henderson", "author_profile_id": "81100433365", "affiliation": "The University of Melbourne", "person_id": "P84172", "email_address": "", "orcid_id": ""}, {"name": "Tony Hoare", "author_profile_id": "81335491522", "affiliation": "Cambridge University Computer Laboratory", "person_id": "PP31079690", "email_address": "", "orcid_id": ""}, {"name": "Simon Marlow", "author_profile_id": "81100515135", "affiliation": "Microsoft Research Ltd., Cambridge", "person_id": "P265492", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/301618.301637", "year": "1999", "article_id": "301637", "conference": "PLDI", "title": "A semantics for imprecise exceptions", "url": "http://dl.acm.org/citation.cfm?id=301637"}