{"article_publication_date": "10-19-2008", "fulltext": "\n JavaPerformanceEvaluation through Rigorous Replay Compilation Andy Georges Lieven Eeckhout Dries Buytaert \nDepartment of Electronics and Information Systems, Ghent University, Belgium {ageorges,leeckhou}@elis.ugent.be, \ndries@buytaert.net Abstract A managed runtime environment, such as the Java virtual machine, is non-trivial \nto benchmark.Java performance is affected in various complex ways by the application and its input, as \nwell as by the virtual machine (JIT optimizer, garbage collector, thread scheduler, etc.). In addition, \nnon\u00addeterminism due to timer-based sampling for JIT optimiza\u00adtion, thread scheduling, and various system \neffects further complicate theJava performance benchmarkingprocess. Replay compilationisarecently introducedJava \nperfor\u00admance analysis methodology that aims at controlling non\u00addeterminism to improveexperimentalrepeatability. \nThekey idea of replay compilation is to control the compilation load during experimentation by inducing \na pre-recorded compi\u00adlation plan at replay time. Replay compilation also enables teasing apart performance \neffects of the application versus the virtual machine. This paper argues that in contrast to current \npractice which uses a single compilation plan at replay time, multi\u00adple compilationplansadd statisticalrigortothereplay \ncom\u00adpilation methodology. By doing so, replay compilation bet\u00adter accounts for the variability observed \nin compilation load across compilation plans. In addition, we propose matched\u00adpair comparison for statistical \ndata analysis. Matched-pair comparison considers the performance measurements per compilation plan before \nand after an innovation of interest as a pair, whichenables limiting the number of compilation plans \nneeded for accurate performance analysis compared to statistical analysis assuming unpaired measurements. \nCategories and Subject Descriptors D.2.8[Software En\u00adgineering]: Metrics Performance measures; D.3.4[Pro\u00adgramming \nLanguages]:Processors Run-time environments Permission to make digital or hard copies of all or part \nof this work for personal or classroom use is granted without fee provided that copies are not made or \ndistributed for pro.t or commercial advantage and that copies bear this notice and the full citation \non the .rst page.To copyotherwise, to republish, to post on servers or to redistribute to lists, requires \nprior speci.c permission and/or a fee. OOPSLA 08, October 19 23, 2008, Nashville,Tennessee, USA. Copyright \nc &#38;#169; 2008ACM 978-1-60558-215-3/08/10... $5.00 General Terms Experimentation, Measurement, Perfor\u00admance \nKeywords Java, virtual machine, performance evaluation, benchmarking, replay compilation, matched-pair \ncompari\u00adson 1. Introduction Managed runtime systems, such as Java virtual machines, are challengingto \nbenchmark because there arevariousfac\u00adtors affecting overall performance, such as the application and \nits input, the virtual machine (VM) and its con.gura\u00adtion (memory management strategy, heap size, dynamic \nop\u00adtimizer, thread scheduler, etc.), and also the hardware on which the benchmarkingexperimentis done.To \ncomplicate things even further, non-determinism leads to different per\u00adformance results when running \nthe same experiment multi\u00adple times. An important source of non-determinism comes from timer-based sampling \nfor selecting candidate methods for JIT compilation and optimization.Timer-based sampling may result \nin different samples being taken across multiple runs of the same experiment, which may lead to different \nmethods being optimized, and which in its turn results in execution time variability. Although some VMs \nuse method invocation counters to .nd optimization candidates [13, 28], mostVMs use timer-based sampling[1,2,11,25,34,36].In \nparticular, VMs that use multiple levels of optimization rely exclusively on sampling for identifying \noptimization candi\u00addates because of the high overhead invocation counters in\u00adtroduce in optimized code. \nResearchers are well aware of the dif.culty in under\u00adstanding and benchmarking managed runtime system \nperfor\u00admance.Thisis re.ectedinthegrowingbodyofworkonJava performance analysis, see for example [3, 16, \n18, 24, 35]. Some recentwork focusesonexperimental design[3,5,14], i.e., choosing an appropriate set \nof benchmarks and inputs, VMs, garbage collectors, heap sizes, and hardware plat\u00adforms; other recent \nwork focuses on data analysis [15], i.e., how to analyze and report the performance results in the presence \nof non-determinism. One particularly interesting and increasingly widely used experimental design methodology \nis replay compilation [19, 29]. Replay compilation .xes the compilation load through a so called compilation \nplan which is determined from a pro.le run. The compilation plan then forces the VM to compile each method \nto a predetermined optimization level in the replay run.Bydoing so, replay compilation eliminates the \nnon-determinism due to timer-based JIT optimization. Thisfacilitates performance analysis. Current practice \nin replay compilation uses a single com\u00adpilation plan during replay. In this paper, we argue that the \nperformance results obtained for one compilation plan may not be representative for other compilation \nplans, and may potentially lead to misleading results in practical research studies. We therefore advocate \nmultiple compilation plans in order to better represent average behavior. We propose a matched-pair comparison \nas the statistically rigorous data analysis method when comparing design alternatives un\u00adder replay compilation \nusing multiple compilation plans.A matched-pair comparison amortizes part of the overhead in\u00adtroduced \nby multiple compilation plans. In this paper, we make the following contributions: We show that different \ncompilation plans lead to statis\u00adtically signi.cant execution time variability. The reason is that different \ncompilation plans may lead to different methods being compiled to different optimization levels. And \nthis execution time variability may lead to incon\u00adsistent conclusions across compilation plans in practical \nresearch studies.  We advocate replay compilation using multiple compila\u00adtion plans in order to capture \nthe execution time variabil\u00adity across compilation plans. Multiple compilation plans result in a more \nrigorous replay compilation methodol\u00adogy compared to prior work which considers a single compilation \nplan during replay.  We propose matched-pair comparison for analyzing the performance numbers obtained \nfrom replay compilation using multiple compilation plans. Matched-pair compari\u00adson considers the performance \nnumbers foragiven com\u00adpilation plan before and after the innovation as a pair. In general, this yields \ntighter con.dence intervals than statistical analysis assuming unpaired measurements. Or, for the same \nlevel of accuracy, i.e., for the same con\u00ad.dence interval size, fewer compilation plans are to be considered \nunder matched-pair comparison.  We demonstrate that for a given experimentation time budget, it is bene.cial \nto consider more compilation plans rather than more runs per compilation plan.  Although the experimental \nresults in this paper are ob\u00adtained using JikesRVM [1], we believe that theoverall con\u00adclusions from \nthis paper generalize to other VMs that use timer-based sampling for driving just-in-time compilation \nand optimization. In addition, we believe that these conclu\u00adsions also generalize to VMs that use invocation \ncounters when running multithreaded benchmarks; different thread schedules may lead to different methods \nbeing optimized to different levels of optimization at different points in time. Multiple compilation \nplans will capture these differences. This paper is organized as follows. In Section 2, we .rst describe \nreplay compilation as an experimental design methodology for managed runtime systems. Section 3 ex\u00adplainstheexperimental \nsetup usedin this paper.In Section4 we evaluate the runtime variability across compilation plans at replay \ntime, and how this may affect conclusions in prac\u00adtical research studies. Section5presents matched-pair \ncom\u00adparison for the statistical analysis of performance numbers obtained from multiple compilation plans. \nIn Section6 we describe the overall framework for rigorous replay compila\u00adtion using multiple compilation \nplans.We .nally conclude in Section 7. 2. Replay Compilation Replay compilation [19, 29] is a recently \nintroduced ex\u00adperimental design methodology that .xes the compila\u00adtion/optimization load in a Java virtual \nmachine execution. As mentioned before, the motivation is to control non\u00addeterminism, andby doing so \ntofacilitate performance anal\u00adysis. 2.1 Replay compilation mechanism Replay compilation requires a pro.ler \nand a replayer. The pro.ler, see Figure 1, records the pro.ling information used to drive the compilation \ndecisions, e.g., edge counts, path and dynamic call graph information, etc., as well as the compilation \ndecisions, e.g., methodX was compiled at op\u00adtimization level Y. Typically, researchers run multiple ex\u00adperiments \nyielding multiple pro.les(p pro.les in Figure 1), and a single compilation plan is determined from these \npro\u00ad.les. The replayer then reads the compilation plan and upon the .rst invocation of a method, it induces \nthe compila\u00adtionplanby optimizingthe methodtothe optimizationlevel speci.ed in the compilation plan. \nBy doing so, the compi\u00adlation/optimization load is .xed which forms the basis of comparison when evaluating \nthe ef.cacyof an innovation of interest. During the replay phase, a benchmark is iterated twice within \na single VM invocation. The .rst iteration includes the compilation overhead according to the compilation \nplan this is referred to as the mix run. The second iteration then is the timing run with adaptive (re)compilation \nturned off this is called the stable run. In order to .x thegarbage col\u00adlection loadinthe stable run,a \nfull-heapgarbage collection is typically done between the mix and the stable runs. 2.2 Design options \nResearchers typically select a single compilation plan out of a number of pro.les, or,alternatively,combine \nthese pro.les into a single compilation plan. Some researchers pick the best pro.le as the compilation \nplan, i.e., the pro.le that Figure 1: The pro.ling phase for replay compilation. There are p VM invocations, \nand each VM invocation runs q benchmark iterations, yielding p pro.les. Current practice then selectsasingle \ncompilation plan from these pro.les: the optimal plan (yielding thefastestexecution time), or the ma\u00adjority \nplan (combining all pro.les through a majority vote). yields the best overall performance, see for example \n[3, 6, 7, 10] this is called the optimal plan. The motivation for using an optimal plan is to assess \nan innovation on a compilation plan that represents the best code quality. Others select the median optimization \nlevel observed across the pro.les for each method [37].Yet others select the methods that are optimized \nin the majority of the pro.les, and set the optimization level for the selected methods at the highest \noptimization levels observed in the majority of the pro.les, seeforexample[12,19,30,29] thisis calledthe \nmajority plan. And yet others select the methods observed in the intersection of multiple pro.les [32]. \nAnother design option relates to how many benchmark iterations (represented by q in Figure 1) to consider \nwhen collecting the pro.les. As the benchmark is iterated multiple times without restarting the VM, more \nand more methods will be selected for JIT optimization, i.e., the code quality will steadily improve \nas more and more methods get opti\u00admized to higher levels of optimization. The question then is when to \ncollect the pro.le across these iterations. One op\u00adtion could be to .nish the pro.le collection after \nthe .rst benchmark iteration, i.e., q =1. Another option would be to collect the pro.le across multiple \nbenchmark iterations, i.e., q> 1;this will result in a pro.le that represents better code quality. In \nthis paper, we consider both options, using 1-iteration (q =1)and10-iteration (q = 10)compilation plans. \nThe .nal design option is how to con.gure the system setup (virtual machine con.guration, garbage collection \nstrategy, heap size, etc.) when collecting the pro.les. 2.3 Discussion Asingle compilation plan. Current \npractice in replay com\u00adpilation considers a single compilation plan during replay. As we will show in \nthis paper, this can be misleading. The reason is that a single compilation plan does not account forthevariabilityobservedin \ncompilationloadacrossmulti\u00adple runs under non-deterministic VM executions. By conse\u00adquence, a single \ncompilation plan may not be representative for theaverage behavior seenby an end user.We therefore advocate \nusing multiple compilation plans at replay time. This is consistent with our prior work [15] on using \nstatis\u00adtical data analysis for coping with non-determinism, which advocates using an average performance \nnumber along with a con.dence interval computed from a number of bench\u00admarkingexperiments insteadof pickinga \nperformance num\u00adber from a single experiment. Non-determinism. Replay compilation, although it con\u00adtrols \nnon-determinism to a large extent, does not completely eliminate non-determinism. There are a number \nof remain\u00ading sources of non-determinism that replay compilation does not control, e.g., thread scheduling. \nDifferent thread schedul\u00ading decisions in time-shared and multi-threading environ\u00adments across different \nruns of the same experiment can af\u00adfect performance. For example, different thread schedules may lead \nto different points in time where garbage is be\u00ading collected leading to different data layouts which \nmay affect memory system performance as well as overall per\u00adformance. Also, various system effects, such \nas interrupts, introduce non-determinism when run on real hardware.To cope with the non-determinism, \nin our prior work [15], we recommended applying statistical analysis by comput\u00ading the average performance \nnumber as well as con.dence intervals across multiple experiments. By controlling non\u00addeterminism, replay \ncompilation reduces the required num\u00adberof measurementsto reach statisticallyvalid conclusions. Replay \ncompilation as experimental design. Replay com\u00adpilation is an experimental design setup that may be appro\u00adpriate \nfor someexperimentsbut inappropriate for others. It s up to the experimenter who has a good understanding \nof the system under measurement to determine whether replay compilation is an appropriate experimental \ndesign setup. Speci.cally, the implicit assumption for replay compilation is that the innovation under \nevaluation does not affect com\u00adpilation decisions, i.e., the compiler/optimizer is assumed to make the \nsame compilation/optimization decisions irrespec\u00adtiveoftheinnovation underevaluation.Thismayormaynot \nbe a valid assumption depending on the experiment at hand.  2.4 Use-case scenarios There are several \nuse-case scenarios for which replay com\u00adpilationisa usefulexperimental design setup.We enumerate a couple \nexamples here as they are in use today this enu\u00admeration illustrates the wide use of replay compilation \nas an experimental design setup for managed runtime systems. JIT innovation. JIT research, such as compiler/optimizer \ninnovation, may bene.t from replay compilation as anexper\u00adimental setup. Researchers evaluating the ef.cacy \nof a JIT innovation want to answer questions such as How does my innovation improve application code \nquality? What is the compilation time overhead that the innovation incurs? The problem at hand is that \nin a virtual execution environment with dynamic compilation, application code execution and compilation \noverhead are intermingled. The question then is how to tease apart the effect that the JIT innovation \nhas on code quality and compilation time? Replay compilationisamethodology that enables teasing apart \ncode quality and compile timeoverhead, see forexam\u00adple Cavazos and Moss [12], who study compiler scheduling \nheuristics. The mix run provides a way of quantifying the overhead the innovation has on compilation \ntime. The stable run provides a way of quantifying the effect of the innova\u00adtion on code quality. Innovation \nin pro.ling and JVM innovation A research topic that is related to JIT innovation is pro.ling, i.e., \nan im\u00adproved pro.ling mechanism provides a more accurate pic\u00adture for analysis and optimization.Forexample, \nBond and McKinley[9] use replay compilationtogaugetheoverhead of continuously maintainingaprobabilistic \ncalling contextin a Java virtual machine; the same research group uses similar setups in [8] and [10]. \nBecause the compilation load and the resulting code quality is .xed, the stable run can be used for performance \nand overhead measurement. Similarly, Schnei\u00adder et al. [31] optimize spatial locality by co-allocating \nob\u00adjectsina generationalgarbage collector based on measure\u00adments obtained from hardware performance monitors \nthat count cache misses. They use replay compilation to min\u00adimize the variability across multiple runs \nduring measure\u00adment. GC innovation. Garbage collection (GC) research also bene.ts from replay compilation. \nIn fact, many recent garbage collection research papers use replay compilation as their experimental \ndesign methodology [3, 6, 7, 17, 19, 29, 30, 32, 37]. The reason whyreplay compilation is useful forgarbage \ncollection research is that it .xes the compiler load, and by doing so, it controls non-determinism which \nfacilitates the comparisonofgarbage collection alternatives. GC research often uses the optimal plan \nunder replay compilation.ThemotivationfordoingsoisthatifaGC strat\u00adegy degrades mutator locality, this \nis likely to be exposed morebya compilation plan that represents higher code qual\u00adity. Although an optimal \nplan is an important experimental design choice for GC research for this reason, it may not accurately \nrepresent user-perceived performance. Other applications. Thereexista numberof other applica\u00adtions to \nreplay compilation. Krintz and Calder [21] for ex\u00adample annotate methods with analysis information collected \nof.ine, similar to a compilation plan. These annotations sig\u00adni.cantly reduce the time to perform dynamic \noptimizations. Ogata et al. [27] use replay compilation tofacilitate the de\u00adbugging of JIT compilers. \n3. Experimental Setup Before studying replay compilation in great detail, we .rst describe ourexperimental \nsetup.We discuss the virtual ma\u00adchine con.gurations, the benchmarks and the hardware plat\u00adforms considered \nin this paper. Finally, we also detail the replay compilation setup. 3.1 Virtual machine con.guration \nWeusetheJikes ResearchVirtual Machine(RVM)[1]which is an open source Java virtual machine written in \nJava. Jikes RVM employs baseline compilation to compile a method upon its .rst execution; hot methods \nare sampled by an OS\u00adtriggered sampling mechanism and subsequently scheduled for further optimization. \nThere are three optimization levels inJikesRVM:0,1and2.Weusethe February12,2007SVN versionof JikesRVMin \nallof ourexperiments. As Jikes RVM employs timer-based sampling to detect optimization candidates, researchers \nhave implemented re\u00adplay compilation in JikesRVM to control non-determinism using so called advice .les \n an advice .le is a compila\u00adtion plan in this paper s terminology. An advice .le speci.es (i) the optimization \nlevel for each method compiled, (ii) the dynamic call graph pro.le, and (iii) the edge pro.le. Advice \n.les are collected through a pro.le run: through command\u00adline arguments, JikesRVM can be instructed to \ngenerate an advice .le just before program execution terminates. Then, in the replay run, JikesRVM compiles \neach method in the advice .le to the speci.ed level upon a method s .rst in\u00advocation. If there is no \nadvice for a method, the method is compiled using JikesRVM s baseline compiler. In some of our experiments \nwe will be considering mul\u00adtiple garbage collection strategies across a range of heap sizes. This is \nsimilar in setup to whatgarbage collection re\u00adsearch papers are doing as mentioned before, many of thesegarbage \ncollection papers also employreplay compila\u00adtion as theirexperimental design.We consider.vegarbage collection \nstrategiesin total,allprovidedbytheJikes Mem\u00adory Management Toolkit (MMTk) [4]. The .ve garbage collection \nstrategies are: (i) CopyMS, (ii) GenCopy, (iii) GenMS, (iv) MarkSweep, and (v) SemiSpace; the gener\u00adational \ncollectors use a variable-size nursery. We did not include the GenRC, MarkCompact and RefCount collec\u00adtors \nfrom MMTk, because we were unable to successfully run Jikes with the GenRC and MarkCompact collectors \nfor benchmark description min heap size (MB) compress jess db javac mpegaudio mtrt jack .le compression \npuzzle solving database Java compiler MPEG decompression raytracing parsing 24 32 32 32 16 32 24 antlr \nbloat fop hsqldb jython luindex pmd parsing Java bytecode optimization PDF generation from XSL-FO database \nPython interpreter document indexing Java class analysis 32 56 56 176 72 32 64 Table 1: SPECjvm98 (top \nseven) and DaCapo (bottom seven) benchmarks considered in this paper. The rightmost column indicates \nthe minimum heap size, as a multiple of 8MB, for which all GC strategies run to completion. some of the \nbenchmarks; and RefCount did yield perfor\u00admance numbers that are statistically signi.cantlyworse than \nanyother GC strategy across all benchmarks. 3.2 Benchmarks Table 1 shows the benchmarks used in this \nstudy. We use the SPECjvm98 benchmarks [33] (.rst seven rows), as well as seven DaCapo benchmarks [5] \n(next seven rows). SPECjvm98isa client-sideJava benchmark suite consisting of seven benchmarks.We run \nall SPECjvm98 benchmarks with the largest input set(-s100). The DaCapo benchmark suite is a recently \nintroduced open-source benchmark suite; we use release version 2006-10-MR2. We use the seven benchmarks \nthat execute properly on the February 12, 2007 SVN version of Jikes RVM. We use the default (medium size) \ninput set for the DaCapo benchmarks. In all of our experiments, we consider a per-benchmark heap size \nrange, following Blackburn et al. [3]. We vary theheapsizefroma minimumheapsizeupto6timesthis minimum \nheap size, using increments of the minimum heap size. The per-benchmark minimum heap sizes are shown \nin Table 1.  3.3 Hardware platforms Following the advice by Blackburn et al. [5], we consider multiple \nhardware platforms in our performance evaluation methodology: a 2.1GHz AMD Athlon XP, and a 2.8GHz Intel \nPentium 4. Both machines have 2GB of main memory. These machines run the Linux operating system, version \n2.6.18. In all of our experiments we consider an otherwise idle and unloaded machine. 3.4 Replay compilation \nsetup  The compilation plans are computed by running a bench\u00admark on the JikesRVM using the GenMSgarbage \ncollector and a heap size that is 8 times the minimum heap size. In this paper, we compute pro.les after \n(i) a single iteration of the benchmark within a single VM invocation (yielding 1\u00aditeration plans), and \n(ii) after 10 iterations of the benchmark within a single VM invocation (yielding 10-iteration plans). \nWe compute separate compilation plans per hardware plat\u00adform.WeperformafullGC betweenthemixandstable \nruns. 4. The Casefor Multiple Compilation Plans Having detailed our experimental setup, we now study \nthe accuracy of selecting a single compilation plan for driving replay compilation. This study will make \nthe case for multi\u00adple compilation plans instead of a single compilation plan. This is done in four steps.We \n.rst demonstrate that dif\u00adferent compilation plans can lead to statistically signi.cantly different benchmark \nexecution times. This is the case for both GC time and mutator time. Second, we provide a rea\u00adson for \nthis difference in execution time, by comparing the methods that are compiled under different compilation \nplans. Third, we present a case study in which we compare various garbage collection strategies using \nreplay compilation as the experimental design setup. This case study demonstrates that the conclusions \ntaken from practical research studies may be subject to the chosen compilation plan. Finally, we demon\u00adstrate \nthata majority plan (which combines multiple pro.les into a single compilation plan) is no substitute \nfor multiple compilation plans. However, before doing so, we .rst need to explain the ANOVAstatistical \ndata analysis method which we will use throughout this section. 4.1 Statistical background: ANOVA Single-factor \nANOVA. AnalysisofVariance (ANOVA) [20, 23,26] separates the totalvariationina setof measurements into \na component due to random .uctuations in the mea\u00adsurements versus a component due to the actual differences \namong the alternatives. In other words, ANOVA separates the total variation observed in (i) the variation \nobserved be\u00adtween each alternative, and (ii) the variation within the alter\u00adnatives, which is assumed \nto be a result of random effects in the measurements. If the variation between the alternatives is larger \nthan the variation within each alternative, then we conclude that thereisa statistically signi.cantdifferencebe\u00adtween \nthe alternatives; if not, the variability across the mea\u00adsurements is attributed to random effects. Although \nwe will not describe the ANOVA mechanics in great detail we refer to a textbook on statistics for a \ndetailed description, see for example [20, 23, 26] we ratherexplain the outcomeof the ANOVA.A single-factor \nANOVA splits up the total variation, sum-of-squares total (SST), observed across all measurements into \na term, sum\u00adof-squares due to the alternatives (SSA), that quanti.es the variation between the alternatives, \nversus a term, sum-of\u00adsquares due to measurement errors (SSE), that quanti.es the variation within an \nalternative due to random errors. Mathematically, this means that SST = SSA + SSE. Thegoalnowisto quantify \nwhetherthevariationSSA across alternatives is larger in some statistical sense than the vari\u00adation SSE \nwithin each alternative.A simpleway for doing so is to compare the fractions SSA/SST and SSE/SST . Astatisticallymorerigorous \napproachisto applyastatistical test, called the F-test, whichis usedto test whethertwovari\u00adances are \nsigni.cantly different. The Fstatisticis computed as 2 s a F = , s2 e with SSA sa 2 = k - 1 and SSE se \n2 = k(n - 1) with k alternatives, and n measurements per alternative. If the F statisticislargerthanthe \ncriticalvalue F[1-a;(k-1),k(n-1)], which is to be obtained from a precomputed table, we can say that \nthe variation due to differences among the alterna\u00adtives is signi.cantly larger than the variation due \nto random measurement noise at the (1-a) con.dence level.A (1-a) con.dence interval, i.e.,a con.dence \ninterval witha (1 - a) con.dence level, means that there is a (1 - a) probability that the variation \ndue to differences between the alternatives islargerthanthevariationdueto random noise.Inthis paper we \nwill consider 95% con.dence intervals. ANOVA assumes that the variance in measurement error is the same \nfor all the alternatives. Also, ANOVA assumes that the errors in the measurements for the different alter\u00adnatives \nare independent and Gaussian distributed. However, ANOVA is fairly robust with respect to non-Gaussian \ndis\u00adtributed measurements, especiallyin case thereisa balanced number of measurements for each of the \nalternatives. This is the case in our experiments: we have the same number of measurements per alternative; \nwe can thus use ANOVAfor our purpose. Two-factor ANOVA. The ANOVA discussed above is a so called single-factor \nANOVA, i.e., a single input variable (factor)isvariedin theexperiment.A two-factorANOVA ontheotherhandallowsfor \nstudyingtwoinputvariablesand their mutual interaction.In practicular,atwo-factorANOVA splits up the total \nvariation between terms that quantify the variation for each of the twofactors, i.e., SSA and SSB, as \nwell as an interaction term between thefactors, i.e., SSAB, andaterm that quanti.es thevariation within \neach combina\u00adtion offactors due to random errors, i.e., SSE. Similarly to the above formulas, one can \nuse the F-test toverify the pres\u00adence of effects caused by either factor and the interaction between \nthefactors. Post-hoc test. After completing an ANOVA test, we may conclude that thereisa statistically \nsigni.cantdifferencebe\u00adtween the alternatives, however, the ANOVA test does not tell us between which \nalternatives there is a statistically sig\u00adni.cant difference. There exists a number of techniques to \n.nd out between which alternatives there is or there is not a statistically signi.cant difference. One \napproach, which we will be using in this paper, is called the Tukey HSD (Honestly Signi.cantly Different) \ntest. The advantage of the TukeyHSD test over simpler approaches, such as pairwise Student s t-tests, \nis that it limits the probability of making an incorrect conclusion in case there is no statistically \nsigni.\u00adcant difference between the alternatives and in case most of the alternatives are equalbutonlyafew \naredifferent.Fora more detailed discussion, we refer to the specialized litera\u00adture [20, 23, 26]. 4.2 \nExecution time variability We .rst study how benchmark execution time is affected by the compilationplanunderreplay \ncompilation.Todoso,we consider the followingexperiment.We collect10 compila\u00adtion plans per benchmark, \nand run each benchmark 10 times for each compilation plan. This yields 100 execution times in total per \nbenchmark. This measurement procedure is done for both 1-iteration and 10-iteration plans, for5GC strate\u00adgiesandfor6heap \nsizes.Thegoalofthisexperimentnowis to quantify whether the execution time variability observed across \nthese 100 measurements is determined more by the compilation plans thanby the runtimevariability per \ncompi\u00adlation plan. Example. Figure2illustrates thisexperiment fora typical benchmark, namely jython \nwe observed similar results for other benchmarks.Violin plots are displayed which show the GC time and \nmutator time variability within a compila\u00adtion plan (on the vertical axis). By comparing violin plots \nacross compilation plans (on the horizontal axis) we get in\u00adsight in how compilation plans affect execution \ntime. The middle point in a violin plot shows the median, and the shape of the violin plot represents \nthe distribution s proba\u00adbility density function: the wider the violin plot, the higher the density. \nThe top and bottom points show the maximum and minimum values. This .gure suggests that the variabil\u00adity \nwithin a compilation plan is muchsmaller than the vari\u00adability across compilation plans. Rigorous analysis. \nTo study the execution time variabil\u00adity across compilation plans in a more statistically rigorous (a) \nGC time  Figure 2: Violin plots illustrating the variability in (a) GC time and (b) mutator time within \nand across compilation plans for jython on the AMD Athlon, the GenMSgarbage collector, and a 144 MB heap \nsize; assuming a stable run and a 10-iteration compilation plan. Time is measured in milliseconds, and \nthe difference between the highest and lowest value for GC and mutator time is 3.6% and 6.2%, respectively. \nmanner, we now use a single-factor ANOVA in which the compilation plans are the alternatives. In other \nwords, the ANOVAwill .gure out whether the execution time variabil\u00adity across these 100 measurements \nis due to random effects rather than due to the compilation plans. Figure3showsthe percentageofthe30experimentsper \nbenchmark (there are5GC strategies and6heap sizes) for which the ANOVA reports there is a statistically \nsigni.\u00adcantdifferencein totalexecution timeatthe95% con.dence level between the various compilation plans \nfor a 1-iteration plan. The top graph in Figure3 shows the mix run results, whereas the bottom graph \nshows the stable run results; there are two bars per benchmark for the Intel Pentium 4 and AMD Athlon \nmachines, respectively.Forthe majorityofthe benchmarks, there is a statistically signi.cant difference \nin execution times across multiple compilation plans.For sev\u00aderal benchmarks, the score equals 100% which \nmeans that theexecution times are signi.cantly different across all com\u00adpilation plans. The difference \ntends to be higher for the mix runs than for the stable runs for most of the DaCapo bench\u00ad (a) mix run \nwith a 1-iteration compilation plan (b) stable run with a 1-iteration compilation plan   Figure 4: \nThe fraction of experiments for which there is a statistically signi.cant difference in GC, mutator and \ntotal time across compilation plans. These graphs assume 10\u00aditeration plans and stable runs on the AMD \nAthlon platform. (a) AMD Athlon  Figure 5:Average overlap across compilation plans on (a) the AMD Athlon \nplatform, and (b) the Intel Pentium4plat\u00adform, for the 1-iteration and 10-iteration compilation plans. \nmarks on the AMD platform. This suggests that performance seems to be more similar across compilation \nplans in the sta\u00adble run. Figure4 shows similar results for the 10-iteration com\u00adpilation plans,but now \nwe makea distinction betweenGC, mutator and total time, and we assume the stable run. (Al\u00adthough Figure \n4 only shows results for the AMD Athlon, we obtained similar results for the Intel Pentium 4.) We conclude \nthat even under 10-iteration compilation plans there still is a large fraction of experiments for which \nwe observe statistically signi.cant execution time differences across compilation plans. And this is \nthe case for GC, muta\u00adtor and total time. 4.3 Compilation loadvariability Nowthat we haveshown that different \ncompilation plans can result in statistically signi.cantly different execution times, this section quanti.es \nwhythis is the case. Our intuition tells usthatthevaryingexecutiontimesareduetodifferent meth\u00adods being \ncompiled at different levels of optimization across compilation plans. To support this hypothesis we \nquantify the relative difference in compilation plans. To do so, we determine the Method Optimization \nVec\u00adtor (MOV) per compilation plan. Each entry in the MOV represents an optimized method along with its \n(highest) op\u00adtimization level; the MOV does not include an entry for baseline compiled methods. For example, \nif in one com\u00adpilation plan, the method foo 1 gets optimized to level 1, method foo 2 gets optimized \nto level 0, and method foo 3 gets only baseline compiled, then the MOVlooks like [(foo 1,1);(foo 2,0)]. \nIn another compilation plan, method foo 1 gets optimized to optimization level 1 as well, whereas method \nfoo 2 gets baseline compiled, and foo 3 gets optimized to level 0, then the MOV looks like [(foo 1,1);(foo \n3,0)]. Comparing the two compilation plans can thenbe donebycomparing their respectiveMOVs. This is done \nby counting the number of (method, optimiza\u00adtionlevel)pairsthat appearinbothMOVs,dividedbytheto\u00adtal number \nof methods appearing in both compilation plans. In the above example, the overlap metric equals 1/3, \ni.e., there is one common (method, optimization level) pair that appears in both MOVs, namely (foo 1,1) \nand there are three methods optimized in at least one of the compilation plans. An overlap metric of \none thus represents a perfect match, and a zero overlap metric represents no match. Figure 5 quanti.es \nthe overlap metric per benchmark computed as an average across all (unique) pairs of 10 com\u00adpilation \nplans there are C2 = 45 unique pairs of compi\u00ad 10 lation plans over which the average overlap metric \nis com\u00adputed.Weobservethattheoverlapis rather limited, typically under 0.4 for most of the benchmarks. \nThere are a couple benchmarks with relatively higher overlap metrics, see for example compress and db. \nThese benchmarks have a small code footprintand therefore thereisa higher probability that the same methods \nwill get sampled across multiple pro.ling runs of the same benchmark.We conclude that the signi.\u00adcant \nperformance differences across compilation plans are due to compilation load differences. 4.4 Case study: \nComparing GC strategies We now study whether different compilation plans can lead to different conclusions \nin practical research studies. In order to do so, we consider a case study that compares GC strate\u00adgies \nusing replay compilation as the experimental design this re.ects a widely used methodology in GC research, \nsee for example [3, 6, 7, 17, 19, 29, 30, 32, 37]. GC poses a complex space-time trade-off, and it is \nunclear which GC strategy is the winner without detailed experimentation. We use the same data set as \nbefore. There are 14 bench\u00admarks (7 SPECjvm98 benchmarks and 7 DaCapo bench\u00admarks), and we consider 5 \nGC strategies and 6 heap sizes per benchmark.For each benchmark, GC strategy and heap size combination, \nwe have 10 measurements per compila\u00adtion plan for both the mix and stable runs; and we consider 1-iteration \nand 10-iteration plans.We then compute theav\u00aderage execution time along with its 95% con.dence inter\u00adval \nacross these 10 measurements, following the statistically rigorous methodology described in our prior \nwork [15] speci.cally, we use ANOVAin conjunction with theTukey HSD test to compute the simultaneous \n95% con.dence in\u00adtervals. This yields theaverageexecution time along with its con.dence interval per \nGC strategy and heap size, for each compilation plan i Hi 0is not rejected Hi 0is rejected, A > B Hi \n0is rejected, B > A is not rejected agree inconclusive inconclusive j 0 H compilation planj null hypothesis \nas Hi states thatGC strategiesAandBachieve the same meanex\u00adecution time under compilation plan pi. Hence, \nif the null j Hi and H 00 hypotheses for compilation plans pi and pj are 0 j is rejected, A>B inconclusive \nagree disagree H 0 j is rejected, B>A inconclusive disagree H agree 0 Table 2: Classifying pairwise \nGC comparisons when comparing compilation plans;A and B denote GC strategies, and A>B means A outperforms \nB. benchmark and compilation plan.Wethen compare theseav-(a) mix run eragesand con.dence intervalsbydoingapairwise \ncompar\u00adison across compilation plans. The goal of this comparison is to verify whether different compilation \nplans lead to con\u00adsistent conclusions about the best GC strategy for a given heap size and benchmark. \nWhen comparing two compilation plans, we compare the execution times per pair of GC strategies (per heap \nsize) and classify this comparison in one of the three categories: agree, disagree and inconclusive,see \nalsoTable2.Foragiven com\u00adpilation plan pi and GC strategies A and B, we de.ne the AB = \u00b5= \u00b5. The null \nhypothesis pi pi rejected, and if in both cases the same GC strategy outper\u00adforms the other,then the \ncomparison is classi.ed as an agree. This means that both compilation plans agree on thefact that GCstrategyAoutperformsGC \nstrategyB(or viceversa)in a statistically signi.cantway.In case both compilation plans yield the result \nthat both GC strategies are statistically indif\u00adferent,i.e.,for neither compilationplanthenullhypothesisis \nrejected, we also classify the GC comparison as an agree. If on the other hand both compilation plans \ndisagree on which GC strategy outperforms the other one, then we classify the comparison as disagree. \nIn case the null hypothesis is re\u00adjected for one compilation plan, but not for the other, we classify \nthe GC comparison as inconclusive, i.e., there is no basis for a statistically valid conclusion. 1-iteration \nplans. Figure 6 shows this classi.cation per benchmark for the total execution time under mix and sta\u00adble \nreplay for 1-iteration compilation plans. The disagree and inconclusive categories are shown as a percentage \n the agree categorythenisthe complementto 100%.Forsev\u00aderal benchmarks, the fraction disagree comparisons \nis higher than5%,andin some casesevenhigherthan10%.The mpe\u00adgaudio benchmarkisaspecialcasewithaveryhigh \ndisagree fraction although it has a very small live data footprint: the reason is that the various GC \nstrategies affect performance through their heap data layout see also later for a more rigorous analysis.For \nmanybenchmarks, the fraction incon\u00adclusive comparisons is larger than 10%, for both the mix and stable \nruns, and up to 20% and higher for several bench-Figure 6: Percentage inconclusive and disagreeing compar\u00adisons \non the AMD Athlon using 1-iteration compilations plans, under (a) mix replay and (b) stable replay. \nmarks. In other words, in a signi.cant amount of cases, dif\u00adferent compilation plans do not agree on \nwhichGC strategy performs best. 10-iteration plans. Figures 7 and 8 show the percentage of inconclusive \nand disagreeing comparisons for GC time and mutator time, respectively, assuming stable replay and 10-iteration \ncompilation plans. Although compilation plans mostly agree on the best GC strategy in terms of GC time \n(see Figure 7) for some benchmarks, such as jess, bloat, fop and mpegaudio, all compilation plans agree \n this is not the case for all benchmarks, see for example antlr and the 64MB to 96MB heap size range. \nIn contrast to the large fraction of agrees in terms of GC time, this is not the case for mutator time, \nsee Figure 8. For some benchmarks, the fraction disagrees and inconclusives can be as large as 13% (a) \ncompress (b) antlr (a) compress (b) antlr (c) jess (d) bloat (c) jess (d) bloat (e) db (f) fop (e) \ndb (f) fop (g) javac (h) hsqldb (g) javac (h) hsqldb   (i) mpegaudio (j) jython (i) mpegaudio (j) \njython (k) mtrt (l) luindex (k) mtrt (l) luindex (m) jack (n) pmd (m) jack (n) pmd   Figure 7: Percentage \ninconclusive and disagreeing compar-Figure 8: Percentage inconclusive and disagreeing compar\u00adisons for \nGC time under stable replay; heap size appears on isons for mutator time under stable replay; heap size \nappears the horizontal axis in each of the per-benchmark graphs. on the horizontal axis in each of the \nper-benchmark graphs. (a) plan1  Figure 9: Comparison between the mutator execution times for jython \nusing two different 10-iteration compilation plans asa functionof the heap size for.vegarbage collectors.We \nshow the mean of 10 measurements for each plan and the 95% con.dence intervals. (hsqldb)and 35%(fop), \nrespectively. (Again, mpegaudio is a special case for the same reason as above.) To further illustrate \nthe differences across compilation plans, we now compare the mutator execution times of jython for each \nof the .vegarbage collectors for two differ\u00adent compilations plans obtained after running the benchmark \nfor10 iterations. Figure9 shows that for the .rst plan, there is no clear winner given that there are \nonly small perfor\u00admance difference between CopyMS, GenCopyand SemiS\u00adpace. However, the second plan shows \na very different pic\u00adture, in which SemiSpace is the best collector by more than 3% for some heap sizes. \nAnalyzing mutator time. The high fraction disagrees and inconclusives for mutator time in the above experiment \nraises an important question: is this observation a result of the effect that the GC strategy has on \nthe data layout of the mutator, or in other words, is the GC strategy one of the main contributors to \nthe high fraction disagrees and in\u00adconclusives? Or, is this observation simply a result of the performance \nvariability observed across compilation plans and does the GC strategy not affect mutator time? To answer \nthis question we employa two-factor ANOVA withthetwofactorsbeingtheGC strategyandthe compila\u00adtion plan, \nrespectively. The two-factor ANOVAthen reports whetherthevariabilityin mutatortimeisduetotheGC strat\u00adegy, \nthe compilation plan, their mutual interaction, or ran\u00addom noise in the measurements. Figure 10 shows \nthe per\u00adcentage of the total variability in mutator time under stable replay thatis accountedforbythegarbage \ncollector (SSA), the compilation plan (SSB), their interaction (SSAB), and the residual variability (SSE) \ndue to random measurement noise.For almost all benchmarks, thegarbage collector hasa signi.cant impact \non mutator time. The same is true for both the compilation plans and the interaction between these two \nfactors. For all benchmarks, except for mtrt, garbage col\u00adlection accounts for over 15% of the observed \nvariability in these experiments, and for manybenchmarks, GC accounts for more than 60% of the total \nvariability. Remarkably, the GC strategy affects mutator time quite a lot for mpegaudio, accounting for \nover 50% of the observed variability, even though no time is spent in GC during the stable run. This \nexplains the earlier .nding for mpegaudio: its performance is very sensitive to the data layout. These \nresults show that bothfactors, theGC strategy and the compilation plan, as well as their mutual interaction, \nhave a signi.cant impact on the observed variability. Most importantly for this study, we conclude that \nthe GC strategy has a signi.cant impact on the mutator time variability in this experiment, and thus \nthe answer to the above question is that the large fraction of disagrees and inconclusives for mutator \ntime is in part due to GC, and is not just a result of the variability observed across compilation plans. \n4.5 Majority plan It follows from the above analyses that multiple compila\u00adtion plans should be used \nin replay compilation instead of a single compilation plan. These compilation plans are ob\u00adtained from \nmultiple pro.ling runs. Some researchers how\u00adever, have been using a majority compilation plan which \ncaptures information from multiple pro.les within a single compilation plan. A majority plan reduces \nthe number of experiments that need to be conducted compared to mul\u00adtiple compilation plans, while (presumably) \naccounting for the differences observed across compilation plans. We now evaluate whether a majority \nplan can be used as a substitute for multiple compilation plans.For doing so, we againuseasingle-factorANOVAsetupwiththeGC \nstrategy being thefactor. This yields usa mean anda con.dence in\u00adterval per GC strategy and per heap \nsize; this is done for the majority plan on the one hand, and for multiple compilation plans on the other \nhand.We subsequently perform pairwise GC comparisons between the majority plan against the mul\u00adtiple \ncompilation plans, and classify these comparisons in the agree, disagree and inconclusive categories. \nFigure 11 shows that the majority plan and the multiple plans may disagree under both mix and stable \nreplay. In ad\u00addition,the conclusionis inconclusiveinasubstantial fraction of the cases, i.e., one of \nthe approaches claims there is no difference between the alternatives, whereas the other does (a) SPECjvm98 \n (b) DaCapo .nd a signi.cant difference. We thus conclude that a ma\u00adjority plan cannot serve as a proxy \nfor multiple compilation plans. 4.6 Summary As a summary from this section, we conclude that (i) differ\u00adent \ncompilation plans can lead to execution time variability that is statistically signi.cant, and we observe \nthis variabil\u00adity in GC time, mutator time and total time, and, in addition, we observe this for compilation \nplans obtained from multi\u00aditeration pro.ling runs as well as from single-iteration pro\u00ad.ling runs; (ii) \nthe reason for this runtime variability is the often observed difference in the methods and their optimiza\u00adtion \nlevels appearing in the compilation plans; (iii) differ\u00adent compilation plans can lead to inconsistent \nconclusions in practical research studies; and (iv) a majority plan is not an accurate proxy for multiple \ncompilation plans.For these reasons we argue that, in order to yield more accurate perfor\u00admance results, \nreplay compilation should consider multiple compilation plans instead of a single one at replay time. \n5. Statistical Analysis Now that we have reached the conclusion that rigorous replay compilation should \nconsider multiple compilation plans, we need statistically rigorous data analysis for taking statistically \nvalid conclusions from these multiple compila\u00adtion plans. 5.1 Multiple measurements per compilation plan \nAs mentioned before, the performance measurements for a given compilation plan are still subject to non-determinism. \nTherefore, it is important to apply rigorous data analy\u00adsis when quantifying performance for a given \ncompilation plan [15]. Before analyzing the data in terms of whether an innovation improves performance, \nas will be explained in the following section, we .rst compute the average execu\u00adtion time per compilation \nplan. Assume we have k measure\u00adments xi, 1 = i = k, from a population with expected value \u00b5 and variance \ns2. The mean of these measurements x\u00afis computed as k i=1 xi x\u00af=. k (a) mix replay (b) stable replay \n The central limit theory states that, for large values of k (typically k = 30), x\u00afis approximately Gaussian \ndistributed v with expected value \u00b5 and standard deviation s/ k, pro\u00advided that the samples xi, 1 = i \n= k, are (i) independent and (ii) come from the same population with expected value \u00b5 and .nite standard \ndeviation s. In other words, irrespec\u00adtive of the underlying distribution population from which the measurements \nxi are taken, the average measurement mean x\u00afis approximately Gaussian distributed if the measurements \nare taken independently from each other.To reach indepen\u00addence in our measurements, we consider the approach \nde\u00adscribed in our prior work [15]: we discard the .rst measure\u00adment for a given compilation plan and \nretain the subsequent measurements this assumes that the libraries are loaded when doing the measurements \nand removes the dependence between subsequent measurements. 5.2 Matched-pair comparison Comparing design \nalternatives and their relative perfor\u00admance differences is of high importance to research and de\u00advelopment, \nmore so than quantifying absolute performance for a single alternative. When comparing two alternatives, \na distinction needs to be made between an experimental setup that involves corresponding measurements \nversus a setup that involves non-corresponding measurements. Under re\u00adplay compilation with multiple \ncompilation plans there is an obvious pairing for the measurements per compilation plan. In particular, \nwhen evaluating the ef.cacyof a given inno\u00advation, the performance is quanti.ed before the innovation \nas well as after the innovation, forming an obvious pair per compilation plan. This leads to a so called \nbefore-and-after or matched-pair comparison [20, 23]. To determine whether there is a statistically signi.cant \ndifference between the means before and after the innova\u00adtion, we must compute the con.dence interval \nfor the mean of the differences of the paired measurements. This is done as follows, assuming there are \nn compilation plans. Let \u00af bj, 1 = j = n, be the average execution time for compilation plan j before \nthe innovation; likewise, let a\u00afj , 1 = j = n, be the av\u00aderageexecution time for compilation plan j after \nthe innova\u00adtion.Wethenneedto computethe con.dence intervalforthe \u00af mean d of the n difference values \nd\u00af j =\u00afaj - \u00af bj , 1 = j = n. The con.dence interval for the mean of the differences [c1,c2] is de.ned \nsuch that the probability of the expected value d of the differences falls between c1 and c2 equals 1 \n- a; a is called the signi.cance level, and (1 - a) is called the con.dence level. Because the signi.cance \nlevel a is chosen a priori, we need to determine c1 and c2 such that Pr[c1 = d = c2]= 1 - a.Typically, \nc1 and c2 are chosen to form a symmetric interval around d, i.e., Pr[d<c1]= Pr[d>c2]= a/2. Applying the \ncentral-limit theorem as \u00af explained in the previous section, which states that a\u00af, b, and, byconsequence, \nd \u00afare Gaussian distributed, we thus .nd that s \u00af \u00af vd c1 = d - z1-a/2 n s \u00af \u00af vd c2 = d + z1-a/2 , n \nwith sd\u00afthe standard deviation of the difference values com\u00adputed as follows: n (d\u00af i - d\u00af)2 i=1 s =. \nd\u00af n - 1 The value z1-a/2 is de.ned such that a random variable Z that is Gaussian distributed with expected \nvalue \u00b5 = 0 and variance s2 =1 has a probability of 1 - a/2 to be smaller than or equal to z1-a/2. The \nvalue z1-a/2 is typically obtained from a precomputed table. The above assumes that the number of compilation \nplans is suf.ciently large, i.e., n = 30 [23]. In case there are less \u00af than 30 compilation plans, d \ncan no longer be assumed to be Gaussian distributed. Instead, it can be shown that the v distribution \nof the transformed value t =( d \u00af - d)/(sd\u00af/n) follows the Student s t-distribution with n - 1 degrees \nof freedom. The con.dence interval can then be computed as: s \u00af \u00af vd c1 = d - t1-a/2;n-1 n s \u00af \u00af vd c2 \n= d + t1-a/2;n-1 , n with the value t1-a/2;n-1 de.ned such that a random vari\u00adable T that follows the \nStudent s t distribution with n - 1 degreesof freedomhasaprobabilityof 1-a/2 to be smaller than or equal \nto t1-a/2;n-1. As with the z1-a/2 value from above, also the t1-a/2;n-1 value is typically obtained from \na precomputed table. It is interesting to note that as n in\u00adcreases, the Student s t-distribution approaches \nthe Gaussian distribution. Once the con.dence interval is computed, we then verify whether the con.dence \ninterval includes zero. If the con.\u00addence interval includes zero,we conclude,atagiven (1 -a) con.dence \nlevel, that the measured differences are not sta\u00adtistically signi.cant. If not, there is no statistical \nevidence to suggest that there is no statistically signi.cant difference. Note the careful wording here. \nThere is still a probability a that the observed differences are due to random effects in the measurements \nand not due to differences between the al\u00adternatives. In other words, we cannot assure with a 100% certainty \nthat there is an actual difference between the com\u00adpared alternatives. In some cases, taking such weak \ncon\u00adclusions may not be very satisfactory but it is the best we can do given the statistical nature of \nthe measurements. 6. Rigorous Replay Compilation Figure 12 illustrates the overall replay compilation \nmethod\u00adology that we advocate when comparing two alternatives. We startbycollectingn compilation plans.For \neachof these compilation plans we then collect k performance numbers for both the before and the after \nexperiments, and subse\u00adquently compute an average performance number per com\u00adpilation plan before the \ninnovation, \u00af bj, as well as after the innovation, a\u00afj. The differences between the alternatives per \n\u00af\u00af compilation plan, dj = bj - a\u00afj , then serve as input to the matched-pair comparison as explained \nin the previous sec\u00adtion. The replay methodology proposed in the previous sec\u00adtion is more rigorous than \ncurrent practice because it in\u00adcludes multiple compilation plans. The downside is that this methodology \nimplies that more experiments need to be run. We now need to collect performance numbers for multiple \ncompilation plans instead of a single compilation plan. This may be time-consuming. Fortunately, only \na limited number of compilation plans need to considered. The reason is that matched-pair compar\u00adison \nleverages the likely observation that the variability in relative performance difference between the \nalternatives is smaller than the variability observed across the compilation plans. More precisely, as \nwe observed in Section 4, the vari\u00adability in performance between different compilation plans can be \nlarge. However, the intuition is that the variability in relative performance across alternatives for \na given compi\u00adlation plan is notvery large.Acompilation plan leading to high performance for one alternative \nis likely to also yield high performance for the other alternative, even if the abso\u00adlute performance \nis different. In our experiments, we found thistobethe casein general,aswillbeshownlater.Wewill exploit \nthis property to limit the number of compilation plans that need to be considered while maintaining high \naccuracy and tight con.dence intervals. The underlying reason is that a matched-pair comparison exploits \nthe property of paired or so called corresponding measurements.To better understand this important property, \nwe .rst need to explain how to compare two alternatives in case of non-corresponding measurements. 6.1 \nNon-corresponding measurements Consider two alternatives and respective measurements x1j, 1 = j = n1 \nand x2j , 1 = j = n2; assume there is no correspondence or pairing.We now need to compute the con.dence \ninterval of the difference of the means.We .rst need to compute the averages x\u00af1 and x\u00af2 for the two \nalterna\u00adtives. The difference of the means equals x\u00af= x\u00af2 - x\u00af1. The standard deviation of the difference \nof the means equals 22 ss sx = 1 + 2 , n1 n2 with s1 and s2 the standard deviation for the two respective \nalternatives. We can now compute the con.dence interval[c1,c2] for the difference of the means, again \nbased on the central limit theory: c1 =\u00afx - z1-a/2 \u00b7 sx c2 =\u00afx + z1-a/2 \u00b7 sx. If the resulting con.dence \ninterval includes zero, we can conclude that, at the con.dence level chosen, there is no signi.cant difference \nbetween the two alternatives. (a) SPECjvm98 stable (b) DaCapo stable corresponding measurements statistics. \n6.2 Comparison between corresponding and non-corresponding measurements Let snowcompare the con.dence \ninterval computed for cor\u00adresponding measurements versus non-corresponding mea\u00adsurements. Assume n1 = \nn2 = n, and bi = x2i and ai = x1i. Recall the con.dence interval for corresponding measurements equals: \ns \u00af \u00af vd c1,2 = d \u00b1 z1-a/2 \u00b7 , n or n (d\u00af i - d\u00af)2 i=1 \u00af c1,2 = d \u00b1 z1-a/2 \u00b7 . n(n - 1) For non-corresponding \nmeasurements, the con.dence in\u00adterval for the difference of the means equals: nn (\u00afai - a\u00af)2 (\u00af bi - \n\u00af b)2 i=1i=1 \u00af c1,2 = d \u00b1 z1-a/2 \u00b7 + . n(n - 1) n(n - 1) Comparing both con.dence intervals boils down \nto com\u00adparing n n (d\u00af i - d\u00af)2 i=1 for the corresponding measurements, versus nn nn (\u00afai - a\u00af)2 + (\u00af \nbi - \u00af b)2 i=1 i=1 for the non-corresponding measurements. Writing d\u00af i as \u00af bi - a\u00afi, and d \u00af as \u00af b \n- a\u00af, enables expanding the expression for the corresponding measurements to: nnn nn n (\u00afai - a\u00af)2 + \n(\u00af bi - \u00af b)2 - 2 \u00b7 (\u00afai - a\u00af)(\u00af bi - \u00af b). i=1 i=1 i=1 By consequence, if the term n n (\u00afai - a\u00af)(\u00af \nbi - \u00af b) i=1 is positive, then the con.dence interval for the correspond\u00ading measurements is smaller \nthan the con.dence interval for the non-corresponding measurements.In otherwords, corre\u00adsponding measurements \nresult in tighter con.dence intervals if the performance variation is large across the compilation plans, \ni.e., a\u00afi - a\u00afand \u00af bi - \u00af b are large, and if the relative per\u00adformance variation is limited across \ncompilation plans when comparing two alternatives. To illustrate this .nding empirically through our \nGC case study, we compute the ratio R: n (d\u00af i - d\u00af)2 R = i=1, nn (\u00afai - a\u00af)2 + (\u00af bi - \u00af b)2 i=1i=1 \nacross all benchmarks and all heap sizes, for all pairwise GC strategy comparisons. If R is smaller than \none, this means that the con.dence interval computed through matched-pair comparison is smaller than \nthe con.dence interval com\u00adputed through statistics assuming non-corresponding mea\u00adsurements. Figure \n13 shows the cumulative distribution of R.Thevarious graphsshow thatinthe majorityofthe cases, matched-pair \ncomparison indeed results in smaller con.\u00addence intervals of the difference of the means. For exam\u00adple,for \nDaCapoand stable replay,forover85%ofthe cases, matched-pair comparison results in a smaller con.dence \nin\u00adterval. 6.3 Number of compilation plans The above analysis shows that matched-pair comparison for \nanalyzing the performance results from multiple compila\u00adtion plans is likely to result in tighter con.dence \nintervals than using non-corresponding measurements statistics. This (a) SPECjvm98 stable total execution \ntime (b) DaCapo stable total execution time (c) SPECjvm98 stable mutator execution time (d) DaCapo stable \nmutator execution time observation has an important implication. It means that for the same level of \naccuracy, i.e., for the same con.dence interval size, fewer compilation plans need to be consid\u00adered \nwhen using matched-pair comparison statistics instead of non-corresponding measurements statistics. Or, \nin other words, under matched-pair comparison, the number of com\u00adpilation plans that are needed to obtain \ntight con.dence in\u00adtervals is limited. Wenowleveragethis observationto.ndagood trade-off betweenthe numberof \ncompilation plansversusthe number of measurements per plan to obtain accurate performance numbers. For \nexploring this trade-off, we again consider our GC case study in which we consider 5 GC strategies and6 \nheap sizes.We now pairwise compare GC strategies per heap size through matched-pair comparison. Figure \n14 shows the fraction inconclusive and disagree conclusions (averaged across all SPECjvm98 and DaCapo \nbenchmarks) as a function of the number of compilation plans and the number of measurements per plan \nfor the stable run. We show results for both total execution time and mutator ex\u00adecution time. The reference \npoint is the setup for which we consider 10 compilation plans and 10 runs per compilation plan. In other \nwords, a point (x, y) in this graph shows the fraction inconclusive and disagree comparisons for x com\u00adpilation \nplans and y measurements per plan compared to 10 compilation plans and 10 measurements per plan. We observe \nthat the fraction inconclusive and disagree conclu\u00adsions quickly decreases with even a limited number \nof, say 4or 5, compilation plans. At the same time, the fraction in\u00adconclusive and disagree conclusions \nisfairly insensitive to the number of measurements per compilation plan. In other words, foragivenexperimentation \ntimebudget,itis bene.\u00adcial to consider multiple compilation plans rather than mul\u00adtiple measurements \nper compilation plan. 7. Summary Replay compilation is an increasingly widely used experi\u00admental design \nmethodologythat aims at controlling the non\u00addeterminism in managed runtime environments such as the Java \nvirtual machine. Replay compilation .xes the com\u00adpilation load by inducing a pre-recorded compilation \nplan at replay time. The compilation plan eliminates the non\u00addeterminism due to timer-based sampling \nfor JIT optimiza\u00adtion. Current practice typically considers a single compilation planatreplaytime, albeitaplanmayhavebeenderivedfrom \nmultiple pro.les. Thekeyobservation made in this paper is that a single compilation plan at replay time \ndoes not suf.\u00adciently account for the variability observed across different pro.les. The reason is that \ndifferent methods may be op\u00adtimized at different levels of optimization across different compilation \nplans. And this may lead to inconsistent con\u00adclusions across compilation plans in practical research \nstud\u00adies. In addition, we have shown that a majority compilation plan is no proxy for using multiple \ncompilation plans.We therefore advocate replay compilation using multiple com\u00adpilation plans so that \nthe performance number obtained from replay compilation is a better representative for average per\u00adformance. \nThe statistical data analysisthatwe advocate under replay compilation with multiple compilation plans \nis matched-pair comparison. Matched-pair comparison considers the before and after experiments for a \ngiven compilation plan as a pair, andbydoingso,achievestighter con.denceintervalsingen\u00aderal than assuming \nunpaired measurements. The reason is that replay compilation leverages the observation that the variability \nin the performance differences between two de\u00adsign alternatives is likely smaller than the variability \nacross compilation plans.Byconsequence, replay compilation with multiple compilation plans and matched-pair \ncomparison limits the number of compilation plans that need to be con\u00adsidered, and thuslimits theexperimentation \ntimeoverhead incurredby multiple compilation plans. Acknowledgments We would like to thank the anonymous \nreviewers for their valuable comments their detailed suggestions greatly helped us improving this paper. \nAndy Georges and Lieven Eeckhout are Postdoctoral Fellows of the Fund for Sci\u00adenti.c Research Flanders \n(Belgium) (FWO Vlaanderen). Dries Buytaertwas supportedbythe Institute for the Promo\u00adtion of Innovationby \nScience andTechnology in Flanders (IWT). References [1] M. Arnold, S. Fink, D. Grove, M. Hind, andP.F. \nSweeney. Adaptive optimization in the Jalape no JVM. In OOPSLA, pages 47 65, Oct. 2000. [2] BEA. BEA \nJRockit: Java for the enterprise. Technical white paper. http://www.bea.com, Jan. 2006. [3] S. Blackburn,P. \nCheng, and K. S. McKinley. Myths and reality: The performance impact ofgarbage collection. In SIGMETRICS, \npages 25 36, June 2004. [4] S. Blackburn,P. Cheng, andK.S. McKinley. Oil andwater? High performancegarbage \ncollectioninJava with JMTk. In ICSE, pages 137 146, May 2004. [5] S. M. Blackburn, R. Garner,C. Hoffmann, \nA. M. Khang, K. S. McKinley, R. Bentzur, A. Diwan, D. Feinberg, D. Frampton, S. Z. Guyer,M. Hirzel, A. \nHosking, M. Jump, H. Lee, J. E. B. Moss,A. Phansalkar,D.Stefanovic,T.VanDrunen,D.von Dincklage, and B.Wiedermann. \nThe DaCapo benchmarks: Java benchmarking development and analysis. In OOPSLA, pages 169 190, Oct. 2006. \n[6] S. M. Blackburn, M. Hertz, K. S. McKinley, J. E. B. Moss, andT.Yang. Pro.le-based pretenuring. ACM \nTrans. Program. Lang. Syst., 29(1):2, 2007. [7] S. M. Blackburn and A. L. Hosking. Barriers: Friend or \nfoe? In ISMM, pages 143 151, Oct. 2004. [8] M. D. Bond and K. S. McKinley. Continuous path and edge pro.ling. \nIn MICRO, pages 130 140, Dec. 2005. [9] M. D. Bond and K. S. McKinley. Probabilistic calling context. \nIn OOPSLA, pages 97 112, Oct. 2007. [10] M. D. Bond and K. S. McKinley. Bell: Bit-encoding online memory \nleak detection. In ASPLOS, pages 61 72, Oct. 2006. [11] D. Buytaert, A. Georges, M. Hind, M. Arnold, \nL. Eeckhout, and K. De Bosschere. Using HPM-sampling to drivedynamic compilation. In OOPSLA, pages 553 \n568, Oct. 2007. [12] J. Cavazos and J. E. B. Moss. Inducing heuristics to decide whether to schedule. \nIn PLDI, pages 183 194, June 2004. [13] M. Cierniak, M. Eng, N. Glew, B. Lewis, and J. Stichnoth. The \nopen runtime platform: A .exible high-performance managed runtime environment. IntelTechnologyJournal, \n7(1):5 18, 2003. [14] L. Eeckhout, A. Georges, and K. De Bosschere. How Java programs interact with virtual \nmachines at the microarchitec\u00adtural level. In OOPSLA, pages 169 186, Nov. 2003. [15] A. Georges, D. Buytaert, \nand L. Eeckhout. Statistically rigorous Java performance evaluation. In OOPSLA, pages 57 76, Oct. 2007. \n[16] D. Gu, C.Verbrugge, and E. M. Gagnon. Relativefactors in performance analysis of Java virtual machines. \nIn VEE, pages 111 121, June 2006. [17]S.Z.Guyer,K.S. McKinley,andD. Frampton. Free-me:A static analysis \nfor automatic individual object reclamation. In PLDI, pages 364 375, June 2006. [18] M. Hauswirth, P. \nF. Sweeney, A. Diwan, and M. Hind. Vertical pro.ling: Understanding the behavior of object\u00adoriented applications. \nIn OOPSLA, pages 251 269, Oct. 2004. [19] X. Huang, S. M. Blackburn, K. S. McKinley, J. E. B. Moss, Z.Wang, \nandP. Cheng. Thegarbage collection advantage: Improving program locality. In OOPSLA, pages 69 80, Oct. \n2004. [20] R. A. Johnson and D. W. Wichern. Applied Multivariate Statistical Analysis. Prentice Hall, \n2002. [21] C. Krintz and B. Calder. Using annotations to reduce dynamic optimization time. In PLDI, pages \n156 167, May 2001. [22] B. Lee, K. Resnick, M. D. Bond, and K. S. McKinley. Correcting the dynamic call \ngraph using control-.ow constraints. In CC, pages 80 95, March 2007. [23] D. J. Lilja. Measuring ComputerPerformance:A \nPracti\u00adtioner s Guide. Cambridge University Press, 2000. [24] J. Maebe, D. Buytaert, L. Eeckhout, and \nK. De Bosschere. Javana: A system for building customized Java program analysis tools. In OOPSLA, pages \n153 168, Oct. 2006. [25] V. Sundaresan, D. Maier, P. Ramarao, and M. Stoodley. Experiences with multithreading \nand dynamic class loading in a Java just-in-time compiler. In CGO, pages 87 97, Mar. 2006. [26]J. Neter,M.H.Kutner,W.Wasserman,andC.J. \nNachtsheim. Applied Linear Statistical Models. McGraw-Hill, 1996. [27] K. Ogata, T. Onodera, K. Kawachiya, \nH. Komatsu, and T. Nakatani. Replay compilation: Improving debuggability of a just-in-time compiler. \nIn OOPSLA, pages 241 252, Oct. 2006. [28] M.Paleczny,C.Vick, andC. Click. TheJava Hotspot server compiler. \nIn JVM, pages 1 12, Apr. 2001. [29] N. Sachindran, and J. E. B. Moss. Mark-copy:fast copying GC with \nless space overhead. In OOPSLA, pages 326 343, Nov. 2003. [30] N. Sachindran, J. E. B. Moss, and E. D. \nBerger. MC2: high\u00adperformance garbage collection for memory-constrained environments. In OOPSLA, pages \n81 98, Oct. 2004. [31] F. T. Schneider, M. Payer, and T. R. Gross. Online optimizations driven by hardware \nperformance monitoring. In PLDI, pages 373 382, June 2007. [32] S. Soman, C. Krintz, and D.F. Bacon. \nDynamic selection of application-speci.cgarbage collectors. In ISMM, pages 49 60, June 2004. [33] Standard \nPerformance Evaluation Corporation. SPECjvm98 Benchmarks. http://www.spec.org/jvm98. [34] T. Suganuma, \nT. Yasue, M. Kawahito, H. Komatsu, and T. Nakatani. Designandevaluationof dynamic optimizations for a \nJava just-in-time compiler. In TOPLAS, 27(4):732 785, July 2005. [35]P.F. Sweeney,M. Hauswirth,B. Cahoon,P. \nCheng,A.Diwan, D. Grove, and M. Hind. Using hardware performance monitors to understand the behavior \nof Java applications. In VM, pages 57 72, May 2004. [36]J.Whaley.Aportable sampling-based pro.lerforJavavirtual \nmachines. In Proceedings of theACM 2000 Conference on Java Grande, pages 78 87, June 2000. [37]T.Yang,M. \nHertz,E.D. Berger,S.F. Kaplan, andJ.E.B. Moss. Automatic heap sizing: taking real memory into account. \nIn ISMM, pages 61 72, June 2004.  \n\t\t\t", "proc_id": "1449764", "abstract": "<p>A managed runtime environment, such as the Java virtual machine, is non-trivial to benchmark. Java performance is affected in various complex ways by the application and its input, as well as by the virtual machine (JIT optimizer, garbage collector, thread scheduler, etc.). In addition, non-determinism due to timer-based sampling for JIT optimization, thread scheduling, and various system effects further complicate the Java performance benchmarking process.</p> <p>Replay compilation is a recently introduced Java performance analysis methodology that aims at controlling non-determinism to improve experimental repeatability. The key idea of replay compilation is to control the compilation load during experimentation by inducing a pre-recorded compilation plan at replay time. Replay compilation also enables teasing apart performance effects of the application versus the virtual machine.</p> <p>This paper argues that in contrast to current practice which uses a single compilation plan at replay time, multiple compilation plans add statistical rigor to the replay compilation methodology. By doing so, replay compilation better accounts for the variability observed in compilation load across compilation plans. In addition, we propose matched-pair comparison for statistical data analysis. Matched-pair comparison considers the performance measurements per compilation plan before and after an innovation of interest as a pair, which enables limiting the number of compilation plans needed for accurate performance analysis compared to statistical analysis assuming unpaired measurements.</p>", "authors": [{"name": "Andy Georges", "author_profile_id": "81100487568", "affiliation": "Ghent University, Gent, Belgium", "person_id": "P1223209", "email_address": "", "orcid_id": ""}, {"name": "Lieven Eeckhout", "author_profile_id": "81330490198", "affiliation": "Ghent University, Gent, Belgium", "person_id": "P1223210", "email_address": "", "orcid_id": ""}, {"name": "Dries Buytaert", "author_profile_id": "81100468396", "affiliation": "Ghent University, Gent, Belgium", "person_id": "P1223211", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1449764.1449794", "year": "2008", "article_id": "1449794", "conference": "OOPSLA", "title": "Java performance evaluation through rigorous replay compilation", "url": "http://dl.acm.org/citation.cfm?id=1449794"}