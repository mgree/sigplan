{"article_publication_date": "10-19-2008", "fulltext": "\n Towards Adaptive Programming Integrating Reinforcement Learning into a Programming Language Christopher \nSimpkins Sooraj Bhat Michael Mateas Charles Isbell, Jr. Computer Science Department College of Computing \nUniversity of California, Santa Cruz Georgia Institute of Technology michaelm@cs.ucsc.edu {simpkins,sooraj,isbell}@cc.gatech.edu \nAbstract Current programming languages and software engineer\u00ading paradigms are proving insuf.cient for \nbuilding intel\u00adligent multi-agent systems such as interactive games and narratives where developers are \ncalled upon to write in\u00adcreasingly complex behavior for agents in dynamic environ\u00adments. A promising \nsolution is to build adaptive systems; that is, to develop software written speci.cally to adapt to its \nenvironment by changing its behavior in response to what it observes in the world. In this paper we describe \na new programming language, An Adaptive Behavior Language (A2BL), that implements adaptive programming \nprimitives to support partial programming, a paradigm in which a pro\u00adgrammer need only specify the details \nof behavior known at code-writing time, leaving the run-time system to learn the rest. Partial programming \nenables programmers to more easily encode software agents that are dif.cult to write in ex\u00adisting languages \nthat do not offer language-level support for adaptivity. We motivate the use of partial programming with \nan example agent coded in a cutting-edge, but non-adaptive agent programming language (ABL), and show \nhow A2BL can encode the same agent much more naturally. Categories and Subject Descriptors D.3.3 [Programming \nLanguages]: Language Constructs and Features General Terms Algorithms, Languages, Design Keywords Adaptive \nProgramming, Reinforcement Learn\u00ading, Partial Programming, Object-Oriented Programming Permission to \nmake digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA 08, October \n19 23, 2008, Nashville, Tennessee, USA. Copyright c . 2008 ACM 978-1-60558-215-3/08/10. . . $5.00 1. \nIntroduction In this paper we present a language, A2BL, that is specif\u00adically designed for writing adaptive \nsoftware agents. By adaptive software we refer to the notion used in the ma\u00adchine learning community: \nsoftware that learns to adapt to its environment during run-time, not software that is written to be \neasily changed by modifying the source code and re\u00adcompiling. In particular, we use Peter Norvig s de.nition \nof adaptive software: Adaptive software uses available information about changes in its environment to \nimprove its behav\u00ad ior (Norvig and Cohn 1998). 1.1 The Need for Adaptivity in Agent Software We are particularly \ninterested in programming intelligent agents that operate in real environments, and in virtual en\u00advironments \nthat are designed to simulate real environments. Examples of these kinds of agents include robots, and \nnon\u00adplayer characters in interactive games and dramas. Unlike traditional programs, agents operate in \nenvironments that are often incompletely perceived and constantly changing. This incompleteness of perception \nand dynamism in the en\u00advironment creates a strong need for adaptivity. Programming this adaptivity by \nhand in a language that does not provide built-in support for adaptivity is very cumbersome. In this \npaper we will demonstrate and analyze the construction of an agent for a simple world, Predator-Food,in \nwhich the agent must simultaneously pursue food and avoid a preda\u00adtor. We will show the dif.culties of \nprogramming an adap\u00adtive agent for even this simple environment using ABL, an advanced agent programming \nlanguage. We will then show how A2BL, with its built-in adaptivity and support for par\u00adtial programming, \nmakes the construction of the same agent much easier. 1.2 How to Achieve Adaptive Software Norvig identi.es \nseveral requirements of adaptive soft\u00adware adaptive programming concerns, agent-oriented con\u00adcerns, and \nsoftware engineering concerns and .ve key technologies dynamic programming languages, agent tech\u00adnology, \ndecision theory, reinforcement learning, and prob\u00adabilistic networks needed to realize adaptive software. \nThese requirements and technologies are embodied in his model of adaptive programming given in Table \n1. Traditional Programming Adaptive Programming Function/Class Agent/Module Input/Output Perception/Action \nLogic-based Probability-based Goal-based Utility-based Sequential, single- Parallel, multi\u00ad Hand-programmed \nTrained (Learning) Fidelity to designer Perform well in environment Pass test suite Scienti.c method \n Table 1. Peter Norvig s model of adaptive programming (Norvig 1998). A2BL integrates two of Norvig s \nkey technologies: agent technology and reinforcement learning. We will explain how A2BL implements Norvig \ns adaptive programming model and argue that A2BL satis.es many of Norvig s require\u00adments, with the rest \nslated for future development. Before we proceed, we expand on Norvig s view of the role of ma\u00adchine \nlearning in general, and reinforcement learning (RL) in particular in the realization of adaptive programming, \nand discuss related work in integrating reinforcement learning into programming languages. 1.3 The Path \nto Adaptive Software: Integrating Machine Learning into a Programming Language One of the promises of \nmachine learning is that it allows designers to specify problems in broad strokes while allow\u00ading a machine \nto do further parameter .ne-tuning. Typically, one thinks of building a system or agent for some speci.c \ntask and then providing it some kind of feedback, allowing it to learn. In this case, the agent is the \npoint of the exercise. A2BL embeds this notion within a programming language itself by extending it with \nadaptive behaviors. The power of such a merger of machine learning and a programming language is that \nit allows for what has become known as partial programming; that is, it allows a designer to specify \nwhat he knows how to express exactly and leave the sys\u00adtem to learn how to do the rest. In the following \nsections we explain how this marriage of machine learning and program\u00adming languages supports the partial \nprogramming paradigm. 1.4 The Partial Programming Paradigm: Why Current Programming Models are Ill-Suited \nto Building Adaptive Software The model of computation, or control regime, supported by a language is \nthe fundamental semantics of language constructs that molds the way programmers think about programs. \nPROLOG provides a declarative semantics in which programmers express objects and constraints, and pose \nqueries for which PROLOG can .nd proofs. In C, programmers manipulate a complex state machine. Func\u00adtional \nlanguages such as ML and Haskell are based on Lambda Calculus. A2BL will be multi-paradigmatic, sup\u00adporting \ndeclarative semantics based on reactive planning, procedural semantics through its direct use of Java, \nand par\u00adtial programming semantics based on reinforcement learn\u00ading, in which the programmer de.nes the \nagent s actions and allows the learning system to select them based on states and rewards that come from \nthe environment. This point is important: partial programming represents a new paradigm which results \nin a new way of writing programs that is much better suited to certain classes of problems, namely adap\u00adtive \nagents, than other programming paradigms. A2BL fa\u00adcilitates adaptive agent programming in the same way \nthat PROLOG facilitates logic programming. While it is possible to write logic programs in a procedural \nlanguage, it is much more natural and ef.cient to write logic programs in PRO-LOG. The issue here is \nnot Turing-completeness, the issue is cognitive load on the programmer. In a Turing-complete language, \nwriting a program for any decidable problem is theoretically possible, but is often practically impossible \nfor certain classes of problems. If this were not true then the whole enterprise of language design would \nhave reached its end years ago. The essential characteristic of partial programming that makes it the \nright paradigm for adaptive software is that it enables the separation of the what of agent behavior \nfrom the how in those cases where the how is either unknown or simply too cumbersome or dif.cult to write \nexplicitly. Re\u00adturning to our PROLOG analogy, PROLOG programmers de.ne elements of logical arguments. \nThe PROLOG system handles uni.cation and backtracking search automatically, relieving the programmer \nfrom the need to think of such de\u00adtails. Similarly, in A2BL the programmer de.nes elements of behaviors \n states, actions, and rewards and leaves the language s runtime system to handle the details of how \npar\u00adticular combinations of these elements determine the agent s behavior in a given state. A2BL allows \nan agent programmer to think at a higher level of abstraction, ignoring details that are not relevant \nto de.ning an agent s behavior. When writ\u00adinganagent in A2BL the primary task of the programmer is to \nde.ne the actions that an agent can take, de.ne what\u00adever conditions are known to invoke certain behaviors, \nand de.ne other behaviors as adaptive, that is, to be learned by the A2BL runtime system. As we will \nsee in Sections 3 and 4, even compared to an advanced agent programming language, this ability to program \npartial behaviors relieves a great deal of burden from the programmer and greatly sim\u00adpli.es the task \nof writing adaptive agents. 1.5 Integrating Reinforcement Learning Into a Programming Language Among \nthe many different kinds of machine learning algo\u00adrithms, reinforcement learning is particularly well-suited \nto the task of learning agent behavior. The goal of a reinforce\u00adment learning algorithm is to learn a \npolicy a mapping from states to actions. In other words, for a given agent, a policy concretely answers \nthe question given the state the agent is in, what should it do? In Section 2 we will provide a broad \noverview of AI and machine learning and explain in more detail why reinforcement learning is well-suited \nto the task of constructing intelligent autonomous agents. There is already a body of work in integrating \nreinforce\u00adment learning into programming languages, mostly from Stuart Russell and his group at UC Berkeley \n(Andre and Russell 2001, 2002). Their work is based on hierarchical reinforcement learning (Parr and \nRussell 1998; Dietterich 1998), which enables the use of prior knowledge by con\u00adstraining the learning \nprocess with hierarchies of partially speci.ed machines. This formulation of reinforcement learn\u00ading \nallows a programmer to specify parts of an agent s be\u00adhavior that are known and understood already while \nallow\u00ading the learning system to learn the remaining parts in a way that is consistent with what the \nprogrammer speci.ed explic\u00aditly. The notion of programmable hierarchical abstract ma\u00adchines (PHAM) (Andre \nand Russell 2001) was integrated into a programming language in the form of a set of Lisp macros (ALisp) \n(Andre and Russell 2002). Andre and Rus\u00adsell provided provably convergent learning algorithms for partially \nspeci.ed learning problems and demonstrated the expressiveness of their languages, paving the way for \nthe development of RL-based adaptive programming. Our work builds on theirs but with a focus on practical \napplications. 1.6 The Path to Adaptive Software Engineering: Practical Languages for Large Agent-Based \nApplications We have chosen another language, ABL (which we shall de\u00adscribe in some detail later), as \nthe starting point for our adap\u00adtive programming language because ABL is designed for de\u00adveloping intelligent \nautonomous agents for signi.cant end\u00aduser applications, namely games and interactive narratives. A2BL \nserves two purposes. First, with a modular implemen\u00adtation of adaptive behaviors that enables the swapping \nof RL algorithms, A2BL provides a platform for RL research. Sec\u00adond, A2BL is the .rst step towards a \nlanguage that supports the needs of game designers and social science modelers writing practical, large \nscale agent systems. It is the second purpose, the practical purpose, that distinguishes our work from \nprevious work in RL-based adaptive programming.  2. Background In this section, we provide the reader \nwith some basic back\u00adground knowledge in a few key concepts from Arti.cial In\u00adtelligence (AI). While \nthe presentation here should suf.ce to understand the remainder of this paper, we provide pointers to \nmore detailed accounts in the literature for the interested reader. 2.1 AI Planning An intelligent agent \nmaximizes goal attainment given avail\u00adable information. In knowledge-based AI, a variety of tech\u00adniques \nare used to solve problems. Typical one-step problem\u00adsolving scenarios include board games, where an \nagent must decide on the best move given the current board state. Plan\u00adning algorithms are used in environments \nwhere an agent must .nd a sequence of actions in order to satisfy its goals. Like most Good Old-Fashioned \nAI (GOFAI), classical plan\u00adning algorithms rely on deterministic representations; that is, they are not \ndesigned to handle probabilistic settings where certain parts of the state space are hidden and some \nactions don t always result in exactly the same state change. As we will see in the next sections, machine \nlearning addresses such partially-observable, probabilistic environments di\u00adrectly. For a more detailed \ndiscussion of AI in general, and planning in particular, see (Russell and Norvig 2003). 2.2 Machine \nLearning Machine learning algorithms improve their performance on some task as they gain experience. \nLearning problems spec\u00adify a task, a performance metric, and a source of training ex\u00adperience. It is \nimportant that the training experience provide some feedback so that the learning algorithm can improve \nits performance. Sometimes the feedback is explicit, as in the case of supervised learning. In supervised \nlearning, an algorithm is presented with a set of examples of a target con\u00adcept, and the algorithm s \nperformance is judged by how well it judges new instances of the class. For example, a charac\u00adter recognition \nsystem can be trained by presenting it with a large number of examples of the letters of the alphabet, \nafter which it will be able to recognize new examples of al\u00adphabetic characters. Some commonly known \ntechniques for such tasks are neural networks, support vector machines, and k-nearest neighbor. Such \nlearning tasks are said to be batch-oriented or of\u00ad.ine because the training is separate from the performance. \nIn supervised learning, the learner such as a neural net\u00adwork is presented with examples of target \nconcepts and its performance task is to recognize new instances of the con\u00adcepts. A supervised learner \nlearns a mapping from instance features to classes by being presented with example map\u00adpings from instances \nto classes. In online virtual and real en\u00advironments, an agent does not have such training available. \nIt is not presented with example mappings of states to ac\u00adtions. Instead, it is presented with mappings \nfrom states to rewards, and it must learn a mapping from states to actions (which is precisely the task \nof a reinforcement learning al\u00adgorithm). Additionally, in online learning an agent must per\u00adform at the \nsame time it is learning, and the feedback here is obtained by exploration acting in the world and succeeding \nor failing. As we will see in the next section, reinforcement learning algorithms represent this type \nof algorithm and are particularly well-suited to the construction of intelligent au\u00adtonomous agents. \nFor a more detailed discussion of machine learning, see (Mitchell 1997). 2.3 Reinforcement Learning \nOne can think of reinforcement learning (RL) as a machine learning approach to planning. In RL, problems \nof decision\u00admaking by agents interacting with uncertain environments are usually modeled as Markov decision \nprocesses (MDPs). In the MDP framework, at each time step the agent senses the state of the environment, \nand chooses and executes an action from the set of actions available to it in that state. The agent s \naction (and perhaps other uncontrolled external events) cause a stochastic change in the state of the \nenvi\u00adronment. The agent receives a (possibly zero) scalar reward from the environment. The agents goal \nis to .nd a policy; that is, to choose actions so as to maximize the expected sum of rewards over some \ntime horizon. An optimal pol\u00adicy is a mapping from states to actions that maximizes the long-term expected \nreward. Many RL algorithms are guar\u00adanteed to converge to the optimal policy in the limit (as time increases), \nthough in practice it may be advantageous to employ suboptimal yet more ef.cient algorithms. Such algorithms \n.nd satis.cing policies that is, policies that are good enough similar to how real-world agents (like \nhu\u00admans) act in the world. Many RL algorithms have been developed for learning good approximations to \nan optimal policy from the agent s experience in its environment. At a high level, most algo\u00adrithms use \nthis experience to learn value functions (or Q\u00advalues) that map state-action pairs to the maximal expected \nsum of reward that can be achieved by starting from that state-action pair and then following the optimal \npolicy from that point on. The learned value function is used to choose actions. In addition, many RL \nalgorithms use some form of function approximation (parametric representations of com\u00adplex value functions) \nboth to map state-action features to their values and to map states to distributions over actions (i.e., \nthe policy). We direct the interested reader to any introductory text on reinforcement learning. There \nare several such texts, includ\u00ading (Sutton and Barto 1998; Kaelbling et al. 1996). 2.4 Modular Reinforcement \nLearning Real-world agents (and agents in interesting arti.cial worlds) must pursue multiple goals in \nparallel nearly all of the time. Thus, to make real-world partial programming feasible, we must be able \nto represent the multiple goals of realistic agents and have a learning system that handles them ac\u00adceptably \nwell in terms of computation time, optimality, and expressiveness. Typically, multiple-goal RL agents \nare mod\u00adeled as collections of RL sub-agents that share an action set. Some arbitration is performed \nto select the sub-agent action to be performed by the agent. In contrast to hierarchical rein\u00adforcement \nlearning, which decomposes an agent s subgoals temporally, we use a formulation of multiple-goal reinforce\u00adment \nlearning which decomposes the agent s subgoals con\u00adcurrently. This concurrent decompositional formulation \nof multiple-goal reinforcement learning, called modular rein\u00adforcement learning (MRL), is better suited \nto modeling the multiple concurrent goals that must be pursued by realistic agents. A more in-depth overview \nof modular reinforcement learning is available in (Sprague &#38; Ballard 2003).  3. A Behavior Language \n(ABL) ABL represents the cutting edge of implemented agent mod\u00adeling languages (Mateas and Stern 2004). \nABL is a reac\u00adtive planning language with Java-like syntax based on the Oz Project believable agent language \nHap (Loyall and Bates 1991). It has been used to build actual live interactive games and dramas, such \nas Facade (Mateas and Stern 2003). In Fa\u00adcade, developed by Andrew Stern and Michael Mateas, the player \nis asked to deal with a relationship between an argu\u00ading couple. It is a single act drama where the player \nmust negotiate her way through a mine.eld of personal interac\u00adtions with two characters who happen to \nbe celebrating their ten-year marriage. An ABL agent consists of a library of sequential and par\u00adallel \nbehaviors with reactive annotations. Each behavior con\u00adsists of a set of steps to be executed either \nsequentially or in parallel. There are four basic step types: acts, subgoals, men\u00adtal acts and waits. \nAct steps perform an action in the world; subgoal steps establish goals that must be accomplished in \norder to accomplish the enclosing behavior; mental acts per\u00adform bits of pure computation, such as mathematical \ncompu\u00adtations or modi.cations to working memory; and wait steps can be combined with continually-monitored \ntests to pro\u00adduce behaviors that wait for a speci.c condition to be true before continuing or completing. \nThe agent dynamically selects behaviors to accomplish speci.c goals and attempts to instantiate alternate \nbehaviors to accomplish a subgoal whenever a behavior fails. The cur\u00adrent execution state of the agent \nis captured by the active be\u00adhavior tree (ABT) and working memory. Working memory contains any information \nthe agent needs to monitor, orga\u00adnized as a collection of working memory elements (WMEs). There are several \none-shot and continually-monitored tests available for annotating a behavior speci.cation. For in\u00adstance, \npreconditions can be written to de.ne states of the world in which a behavior is applicable. These tests \nuse pattern matching semantics over working memory familiar from production rule languages; we will refer \nto them as WME tests. In the remainder of this paper, we will discuss the devel\u00adopment of agents in ABL, \npoint out the issues with writing agents in ABL, and show how A2BL addresses these issues. We will then \nimplement the same agent using A2BL to show the bene.ts to the programmer of integrating true adaptivity \ninto the programming language itself. We conclude with a discussion of the state of A2BL development \nand some re\u00adsearch issues to be addressed in its future development. 3.1 The Predator-Food World To provide \na concrete grounding for our discussion, we will analyze two different implementations of an agent for \nthe Predator-Food world. The Predator-Food world is a grid where there are two main activities: avoiding \nthe predator and .nding food. At every time step, the agent must pick a direction to move. Food appears \nrandomly at .xed locations, and there is a predator in the environment who moves towards the agent once \nevery other time step. 3.2 The Predator-Food Agent as a Reactive Planning Problem Recall from Section \n2 that a plan is a sequence of actions that accomplishes a goal. In the Predator-Food world, an agent \nhas two goals: .nding food and avoiding the predator. Accomplishing each of these goals requires a sequence \nof actions. In a reactive planning agent, the sequence of actions is determined in reaction to percepts \nfrom the environment. For example, if the food is sensed in a certain direction, the agent reacts by \nplanning movements in that direction. Note that there may be many plans that accomplish a goal, and in \na dynamic environment, constant replanning may be needed. The reactive planning approach naturally replans \nin response to such changes. In the next section we show how to code a reactive planning agent for the \nPredator-Food world in ABL.  3.3 A Predator-Food Agent in ABL Below we present ABL code for a reactive \nplanning agent that operates in the Predator-Food world. Lines 1 6 of Figure 1 de.ne an agent and its \nprincipal be\u00adhavior, LiveLongProsper. LiveLongProsper is de.ned as a parallel behavior to re.ect the \nfact that both of its subgoals must be pursued in parallel in order for the enclos\u00ading behavior to succeed. \nLines 9 14 de.ne the FindFood subgoal as a sequential behavior. Each of the subgoals MoveNorthForFood, \nMoveSouthForFood, MoveEastForFood,and MoveWest\u00adForFood must be performed in a particular sequence if \nthe agent is to succeed in .nding food. Note that, because some subgoals will not be selected for execution \nin any given time step, the subgoals must be annotated with ignore failure to prevent the enclosing behavior \nfrom failing. The agent will only move in one direction in each time step, so three of 1 2 3 4 5 6 7 \n8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 \n43 44 45 behaving_entity FurryCreature { parallel behavior LiveLongProsper() { subgoal FindFood (); subgoal \nAvoidPredator(); } // subgoal 1 sequential behavior FindFood () { with (ignore_failure) subgoal MoveNorthForFood(); \nwith (ignore_failure) subgoal MoveSouthForFood(); with (ignore_failure) subgoal MoveEastForFood(); with \n(ignore_failure) subgoal MoveWestForFood(); } // subgoal 2 sequential behavior AvoidPredator() { with \n(ignore_failure) subgoal MoveNorthAwayFromPredator(); with (ignore_failure) subgoal MoveSouthAwayFromPredator(); \nwith (ignore_failure) subgoal MoveEastAwayFromPredator(); with (ignore_failure) subgoal MoveWestAwayFromPredator(); \n} sequential behavior MoveNorthForFood() { precondition { (FoodWME x::foodX y::foodY) (SelfWME x::myX \ny::myY) ((foodY -myY) > 0) // The food is north of me } // Code for moving agent to the north elided \n} // ... sequential behavior MoveNorthAwayFromPredator() { precondition { (PredatorWME x::predX y::predY) \n(SelfWME x::myX y::myY) (moveNorthIsFarther(myX,myY,predX ,predY)) } // Code for moving agent to the \nnorth elided } } Figure 1. An ABL agent for the Predator-Food world. the subgoals will fail because their \npreconditions will not be satis.ed. Lines 24 32 de.ne MoveNorthForFood.The precondition block de.ned \nat the beginning of the behav\u00adior de.nes the circumstances under which ABL s run-time planning system \nmay select this behavior for execution, that is, the agent may react to this set of preconditions by \nse\u00adlecting this behavior. Line 26 assigns the x property of the FoodWME to the local variable foodX,and \nthe y property of the FoodWME to the local variable foodY. These local vari\u00adables are then used in the \nboolean condition ((foodY \u00admyY) > 0) to de.ne the precondition, which states that if the food is north \nof the agent s position, the agent should move north. A WME is a global variable de.ned by the envi\u00adronment \nwhich represents a thing that an agent can perceive. An agent perceives a particular aspect of the environment \nby inspecting its working memory for the appropriate WME. Thus, if an agent has sensed the food, it \nwill have a FoodWME that reports the position of the food. The precondition for MoveNorthForFood de.nes \nthe de\u00adsirability of moving north in search of food, but ignores the predator. We de.ne the behavior \nof moving north away from the predator in lines 36 44. As in the MoveNorthForFood behavior, the conditions \nunder which MoveNorthAway-FromPredator may be selected for execution are de.ned in a precondition block. \nNote that we have factored the code for computing whether the precondition has been met into a utility \nfunction, moveNorthIsFarther. Similar subgoal behavior would be de.ned for each direction of movement, \nand for each reason for such movement. The full code (with details elided) is given in Figure 1. While \nABL s reactive-planning paradigm and declara\u00adtive system make it possible to de.ne complex autonomous \nagents, there are several problems. First, each subgoal be\u00adhavior assumes that the position of both the \nfood and the predator are known. Second, if there is a con.ict between subgoals, the programmer must \nwrite code to resolve this con.ict. For example, what should the agent do if the Find-Food subgoal wants \nto move north to get to the food, but the AvoidPredator subgoal wants to move south to get away from \nthe predator? The biggest problem with this ABL agent is that low-level agent actions (movement) and \nthe reasons for selecting those actions are coupled. Because of this coupling, movement behaviors must \nbe duplicated for each possible reason the movement might be executed. Thus, moving north for food and \nmoving north to avoid the predator must be represented separately and the preconditions for each carefully \nspeci.ed. While the movement action itself could be factored into a separate function called by each \nbehavior, there is still a considerable cognitive burden on the programmer who must consider each combination \nof agent action and reason for action. Note that any programming language that does not provide a means \nfor separating the concerns of what must be done and how it is to be accomplished will impose a similar \ncognitive burden on agent programmers. Another problem with the ABL version of the Predator-Food agent \nis that the programmer must fully specify the agent s behavior. If there is a part of the agent s behavior \nthat the programmer does not know, he must implement his best guess. This becomes dif.cult in the typically \nill-speci.ed and dynamic environments where we would want to deploy intelligent agents, such as massively \nmulti-player games. As we will see in the next sections, integrating adaptivity into the programming \nlanguage not only reduces the amount of code required to implement an agent, but more impor\u00adtantly allows \nthe programmer to think about what the agent s goals are and leave the agent to .gure out how to achieve \nthem. This separation of concerns is enabled by partial pro\u00adgramming, in which the programmer need only \nspecify what he knows, leaving the run-time system to .gure out the rest.  4. An Adaptive Behavior \nLanguage (A2BL) Our solution to the problems described in the previous sec\u00adtion is to provide built-in \nlanguage support for adaptivity. In A2BL, adaptivity is achieved by integrating reinforcement learning \ndirectly into the language. In the following sections we show how to model a Predator-Food agent as a \nrein\u00adforcement learning problem, how this model maps to adap\u00adtive behaviors, and .nally how to implement \nan adaptive Predator-Food agent in A2BL. 4.1 The Predator-Food Agent as a Reinforcement Learning Problem \nIn reinforcement learning, agents and the worlds in which they operate are modeled by states, actions, \nand rewards. Goals are represented implicitly by rewards. Each state in the world provides an agent with \na scalar reward positive or negative that precisely speci.es the desirability of being in that state. \nIn the Predator-Food world, meeting the preda\u00adtor carries a large negative reward, .nding the food carries \na large positive reward, and other states carry zero reward. The job of a reinforcement learning agent \nis to maximize long-term reward by moving to states that carry higher re\u00adwards. In each state an agent \nhas a set of available actions that take the agent to another state. A reinforcement learning algorithm \nexplores the state space (.nding where the higher rewards lie) to learn a policy, that is, a function \nthat maps states to actions. The sequence of actions speci.ed by a pol\u00adicy is much like a plan, except \nthat the policy is learned au\u00adtomatically rather than deduced by analyzing the precondi\u00adtions and postconditions \nof the available actions. Specifying the rewards given by each state is far less cumbersome and error-prone \nthan specifying pre-and post-conditions for each action. 4.2 The Predator-Food Agent in A2BL: Mapping \na Reinforcement Learning Problem to Language Constructs A2BL provides language constructs to model reinforcement \nlearning agents without having to think about the details of reinforcement learning. When a behavior \nis marked as adaptive,A2BL employs a reinforcement algorithm un\u00adder the hood to determine how to select \nthe actions within the adaptive behavior. In a Predator-Food agent, for ex\u00adample, marking the FindFood \nbehavior as adaptive tells A2BL s runtime system to learn how to employ the actions speci.ed within the \nbehavior. No hand-coding of precondi\u00adtions is necessary. Within adaptive behaviors, reward and state \nconstructs provide the reinforcement learning algo\u00adrithm with the information it needs to perform its \nlearn\u00ading task. For example, the FindFood behavior would have a reward construct that de.nes a large \npositive reward for .nding food. A state construct within the behavior would specify how to map percepts \nfrom the environment (mod\u00adeled by WMEs) to objects that can be used in computa\u00adtions, such as grid coordinates. \nThese constructs will be ex\u00adplained in more detail in the next section, which presents a Predator-Food \nagent coded in A2BL. The value of adaptive behaviors is that it enables partial programming. An adaptive \nbehavior models part of the so\u00adlution to a problem, namely, the actions available to reach a particular \ngoal. The rest of the solution which of the actions to select and the order in which to select them \n are learned by the run-time reinforcement learning system. Note that the programmer speci.es a reinforcement \nlearn\u00ading problem using A2BL s adaptive language constructs, but does not deal directly with the reinforcement \nlearning algo\u00adrithms used internally by the A2BL run-time system.  4.3 The Predator-Food Agent In A2BL \nIn Section 3.3 we showed a Predator-Food agent coded in ABL. The ABL code for this agent had to deal \nwith many low-level issues of action selection, essentially hand-coding a policy. In this section we \nshow that, with adaptivity built into the language, it is possible for the programmer to think at a much \nhigher level, reducing the cognitive burden sig\u00adni.cantly. Using the state, reward, and action model \nof rein\u00adforcement learning, the programmer can simply say these are the agent s goals (in terms of rewards), \nand these are the actions available to achieve these goals. The reinforcement learning system learns \nthe states under which given actions should be selected. The full code (minus irrelevant details of movement \nim\u00adplementation) is given in Figure 2. The .rst difference be\u00adtween the ABL agent and the A2BL agent \nis that the prin\u00adcipal enclosing behavior, LiveLongProsper is de.ned as an adaptive collection behavior. \nThis tells the A2BL run-time system to treat the enclosed adaptive behaviors as sub-agents in the MRL \nframework. Each sub-agent behavior then de.nes a set of relevant actions (designated using the subgoal \nannotation inherited from ABL), and the action set of the agent as a whole is the union of all sub-agent \naction sets. Note that each sub-agent contains exactly the same ac\u00adtions. There is no need to de.ne different \naction subgoals and the conditions under which they are selected the learn\u00ading algorithms built into \nA2BL automatically handle these tasks. 4.3.1 The adaptive Keyword The most notable addition in A2BL is \nthe adaptive key\u00adword, used as a modi.er for behaviors. When modifying a sequential behavior, adaptive \nsigni.es that, instead of pur\u00adsuing the steps in sequential order, the behavior should learn a policy \nfor which step to pursue, as a function of the state of the world. Consider lines 9 22 of Figure 2; the \nadaptive modi.er on this behavior tells the A2BL run-time system to learn how to sequence the subgoals \nspeci.ed within the be\u00adhavior as it interacts in the environment. The programmer codes a partial speci.cation \nof the problem the subgoals and the system learns the rest, namely, how to sequence them 1 behaving_entityFurryCreature \n2 { 3 adaptivecollectionbehaviorLiveLongProsper(){ 4 subgoalFindFood(); 5 subgoalAvoidPredator(); 6 } \n7 8 // subgoal 1 9 adaptivesequentialbehaviorFindFood(){ 10 reward{ 11 100if{(FoodWME)} 12 } 13 state{ \n14 (FoodWMEx::foodXy::foodY) 15 (SelfWMEx::myXy::myY) 16 return(myX,myY,foodX,foodY); 17 } 18 subgoalMoveNorth(); \n19 subgoalMoveSouth(); 20 subgoalMoveEast(); 21 subgoalMoveWest(); 22 } 23 24 // subgoal 2 25 adaptivesequentialbehaviorAvoidPredator(){ \n26 reward{ 27 -10if{(PredatorWME)} 28 } 29 state{ 30 (PredatorWMEx::predXy::predY) 31 (SelfWMEx::myXy::myY) \n32 return(myX,myY,predX,predY); 33 } 34 subgoalMoveNorth(); 35 subgoalMoveSouth(); 36 subgoalMoveEast(); \n37 subgoalMoveWest(); 38 } 39 40 // ... 41 } Figure 2. An A2BL agent for the Predator-Food world. optimally \nin a dynamic environment. Note that an adaptive sequential behavior will be handled by A2BL with a sin\u00adgle \nreinforcement learning algorithm, whereas an adaptive collection behavior speci.es a set of behaviors, \neach of which is handled by a reinforcement learning algorithm (see Section 4.3.5) and whose outputs \nare combined by an arbi\u00adtrator function that ultimately decides the agent s action in a particular state. \nWe discuss arbitration functions in Sec\u00adtion 4.3.6. 4.3.2 The state Construct As there could be a large \namount of information in working memory (which is the agent s perception of the state of the world), \nwe have introduced a state construct to allow the programmer to specify which parts of working memory \nthe behavior should pay attention to in order to learn an effective policy. This allows for human-authored \nstate abstraction, a fundamental concept in reinforcement learning. In this example, we specify the state \nas: state{ (FoodWMEx::foodXy::foodY) (SelfWMEx::myXy::myY) return(myX,myY,foodX,foodY); } This tells \nthe A2BL runtime system what comprises the state to be used in its RL algorithms for this particular \nbe\u00adhavior or task. The policy learned for food-.nding will be predicated on this state. Note that the \nstate contains no ele\u00adments that are not needed for reasoning about .nding food. This is an essential \nfeature of modular behaviors, allowing them to be coded in a truly modular fashion. 4.3.3 The success \ncondition Condition In ABL, a behavior normally succeeds when all its steps succeed. Because it is unknown \nwhich steps the policy will ultimately execute, adaptive behaviors introduce a new continually-monitored \ncondition, the success condition, which indicates that the goal of the behavior has been met. When the \nsuccess condition becomes true, the behavior im\u00admediately succeeds. In our example agent, there is no \nsuch end-state goal. The agent must continually .nd food and avoid the predator. 4.3.4 The reward Construct \nTo learn a policy at all, the behavior needs a reinforcement signal. With the reward construct, authors \nspecify a func\u00adtion that maps world states to reinforcement signals. De.n\u00ading the reward that the environment \ngives to an agent in a given state is a straightforward way inject domain knowl\u00adedge into an agent. De.ning \nthe rewards in this manner re\u00adduces the need to de.ne complex preconditions in behav\u00adiors, which makes \nit possible for a domain expert who is not a programmer to participate directly in the construction of \nA2BL agents. In natural analogy to existing ABL constructs, these new constructs make use of WME tests \nfor reasoning and computing over working memory. Consider the follow\u00ading code: reward{ 100if{(FoodWME)} \n} The code above says that, if the agent .nds the food, it gets a large positive reward (recall that \nWMEs are the mech\u00adanism by which an agent senses the world in ABL and in A2BL). This reward is used by \nthe RL algorithms to learn an action selection policy that maximizes long-term reward. Note that the \nnumbers used for rewards only need to be inter\u00adnally consistent for a given task. For example, for the \nFind-Food task, the programmer only need specify the relative de\u00adsirability of .nding food compared to \nnot .nding food (here implicitly zero). We could have written this reward as 10 or 1000. What matters \nis that it is relatively better than not .nd\u00ading food. With modular reinforcement learning (MRL), the \nrewards for each task are de.ned completely separately, and the arbitration function combines the relative \npreferences of each sub-agent (e.g., FindFood and AvoidPredator) to deter\u00admine the agent s behavior. \nSo we could de.ne the rewards for FindFood on a 10 point scale and the rewards for Avoid-Predator on \na 100 point scale and the arbitrator would still do the right thing when determining the agent s behav\u00adior. \nThis modularity allows different behaviors to be devel\u00adoped independently and combined in agents in various \nways, greatly facilitating the engineering of large agent systems by multi-programmer teams.  4.3.5 \ncollection Behaviors An adaptive collection behavior is speci.cally designed for modeling the concurrency \nof MRL. This type of behav\u00adior contains within it several adaptive sequential behaviors, which correspond \nto the sub-agents in the MRL framework. Consider the following code: adaptivecollectionbehaviorLiveLongProsper(){ \nsubgoalFindFood(); subgoalAvoidPredator(); } This code de.nes the LiveLongProsper behavior as consisting \nof two concurrent subgoals FindFood and AvoidPredator.A2BL will attempt to pursue both of the goals \nconcurrently while the agent is running. 4.3.6 Arbitration: Resolving Con.icts Between Subgoals The \nexact manner in which arbitration functions will be de\u00ad.ned by the programmer is an active area of research, \nde\u00adpending partly on parallel work we are doing in modular reinforcement learning. Here we discuss some \nof the pos\u00adsibilities from the perspective of the agent programmer. Once we have de.ned the two adaptive \nsubgoals, we need to de.ne an arbitration function on the enclosing goal, LiveLongProsper. In previous \nwork, we showed that it is impossible to construct an ideal arbitration function auto\u00admatically (Bhat \net al. 2006), so we cannot employ the com\u00adpiler to generate an all-purpose arbitration rule.1 Instead, \nthe programmer must de.ne an arbitration function, either hand\u00adauthored or learned. A hand-authored arbitration \nfunction encodes the trade\u00adoffs the programmer believes to be true about the utilities of the subgoals. \nIn this example, we may decide that the bene\u00ad.t of .nding food equals the cost of running into a preda\u00adtor; \ngiven our reward signals, the arbitrator would select 1 the action maximizing Q1(s, a)+Q2(s, a)(recall \nfrom 10 Figure 2 that the reward for .nding food is 100 and the reward for meeting the predator is -10). \nAlternatively, the hand-authored arbitration function could be independent of the sub-agent Q-values; \nto simply avoid starvation, for in\u00adstance, one might consider round-robin scheduling. Finally, we could \ntry posing LiveLongProsper s arbi\u00adtration task as another reinforcement learning problem, with its own \nreward function encapsulating a notion of goodness 1 Brie.y, arbitration in MRL, as it has been typically \nde.ned, can be shown to be equivalent to .nding an optimal social choice function and thus falls prey \nto Arrow s Impossibility Result. One can avoid this impossibility by having the programmer explicitly \nde.ne the tradeoffs, essentially repealing the non-dictator property of a fair voting system. for living \nwell, as opposed to one that only makes sense for .nding food or avoiding a predator. For example, the \nreward function might provide positive feedback for having more offspring; this would be an evolutionary \nnotion of reward. The reader may wonder why FindFood and Avoid-Predator should have their own reward \nsignals if one is available for LiveLongProsper. The reasons should be fa\u00admiliar: modularity and speed \nof learning. The reward signal for FindFood, for instance, is speci.cally tailored for the task of .nding \nfood, so the learning should converge more quickly than learning via an indirect global reward signal. \nFurther, with the right state features, the behavior should be reusable in different contexts. Specifying \na reward sig\u00adnal for each behavior allows the reward signals to embody what each behavior truly cares \nabout: FindFood cares about .nding grid squares with food, AvoidPredator cares about avoiding the predator, \nand LiveLongProsper cares about ensuring the future of the species.  4.4 A2BL as a Model of Adaptive \nProgramming In the introduction, we listed the elements of Peter Norvig s model of adaptive programming \n(Norvig 1998). Here we discuss A2BL s implementation of this model. 4.4.1 Functions and Classes versus \nAgents and Modules A2BL inherits the agent-orientation of ABL. The funda\u00admental units of abstraction \nare agents and behaviors, where an agent is essentially a collection of behaviors. One could think of \nagents as analogous to classes/objects and behaviors as analogous to functions, but the analogy quickly \nbreaks down. First, agents cannot be composed of other agents the way objects can be composed of other \nobjects. Second, functions are called directly in a procedural fashion; behav\u00adiors are speci.ed declaratively \nand selected for execution by ABL s runtime planning system only if and when those be\u00adhaviors are needed \nto pursue some goal. ABL s declarative reactive planning paradigm, and A2BL s adaptive model pro\u00advide \nmuch better support for a style of programming that separates the what of agent behavior from the how. \n 4.4.2 Input/Output versus Perception/Action In traditional programming, even to a large extent in event\u00addriven \nobject-oriented programming, programs are written and reasoned about in terms of input/output behavior. \nA function is given some input and produces some output. A class is given responsibility for some part \nof the applica\u00adtion s data, responds to particular messages, and provides particular responses. In agent-oriented \nprogramming, on the other hand, the agent programmer thinks in terms of what an agent can perceive in \nthe world, and what actions the agent can execute to modify the state of the world. In ABL and A2BL, \nperception is modeled by WMEs that repre\u00adsent the agent s awareness of the world in which it is situ\u00adated. \nActions are procedural calls within behaviors that ef\u00adfect changes in whatever world the agent is operating \nin. The WMEs (perceptions) and actions constitute an API be\u00adtween agents and worlds, effectively decoupling \nagents from worlds. 4.4.3 Logic-based versus Probability-based In traditional programming, selection \nlogic (boolean tests and if/then constructs) is an important part of any non-trivial program. To a large \nextent, this is true even in ABL, where behaviors are selected based on logical preconditions. By integrating \nRL, A2BL incorporates probabilistic reasoning into the core of the language: RL algorithms build proba\u00adbilistic \nmodels of the world and of agent optimal behavior in that world. In this way, A2BL provides explicit \nsupport for probabilistic reasoning without the programmer having to think explicitly about stochasticity. \n 4.4.4 Goal-based versus Utility-based Goal attainment is a fundamental metaphor in ABL, and in agent \nprogramming in general. In A2BL, goal attainment is represented explicitly in terms of rewards, or utilities. \nEvery state in the world has an associated utility (often implicitly zero), and A2BL s adaptive features \nseek to maximize the agent s utility automatically. 4.4.5 Sequential, single-versus Parallel, multi- \nA2BL inherits ABL s parallelism and extends it to support concurrent modular reinforcement learning. \n 4.4.6 Hand-programmed versus Trained (Learning) With A2BL s support for partial programming, the program\u00admer \ncan ignore low-level behavior that is either too poorly speci.ed or too dynamic to encode explicitly \nand leave A2BL s run-time learning system to learn the details. 4.4.7 Fidelity to designer versus Perform \nwell in environment In traditional software engineering, a program is good if it conforms to its speci.cation. \nIn adaptive partial program\u00adming, a program is good if it performs well in whatever en\u00advironment it .nds \nitself in. With A2BL s explicit support for reward and state speci.cation, and its automatic learning \nof policies, A2BL agents are written to perform well in their environments even when design speci.cations \nare vague. 4.4.8 Pass test suite versus Scienti.c method Closely related to the previous point, test \nsuites are writ\u00adten to test a program s conformance to design speci.cations; however, a certain amount \nof experimentation is often neces\u00adsary to determine just what exactly is the right thing to do in given \nsituations. Yet there is always some imperative to act given whatever information you have at the moment. \nAs a technical matter, reinforcement learning makes explicit this tradeoff between the exploration of \nenvironments and the ex\u00adploitation of already gained knowledge. A2BL inherits this principled approach \nto the exploration/exploitation tradeoff by using RL to implement adaptivity. In a sense, RL algo\u00adrithms \nlearn by experimentation.  5. Research Issues and Future Directions Currently, we have implemented \nan ANTLR-based parser for A2BL, and we have tested several reinforcement learn\u00ading algorithms for use \nin A2BL agents. In particular, we have tested Q-Learning and Sarsa algorithms for single-goal agents \nand are working to design a general arbitration algo\u00adrithm, that is, to develop the theory of modular \nreinforcement learning. Current reinforcement learning algorithms work acceptably well on individual \ngoals, like FindFood or Avoid-Predator, but we have not yet successfully implemented an acceptable arbitration \nmechanism, which is a major focus of ongoing work. Aside from designing an arbitration algo\u00adrithm, the \nmajor remaining tasks in implementing A2BL and by far the major portion of the work are to integrate \nthe reinforcement learning algorithms with the A2BL run-time system and add to the code generation phase \nof the compiler the logic necessary to place calls to the run-time learning routines at the appropriate \nplaces in the generated code. Many challenging and important issues need to be ad\u00addressed to realize \nour vision for A2BL. These issues range from foundational RL theory to pragmatic software engi\u00adneering \nconsiderations. We discuss some of these below. 5.1 Adaptive Software Engineering Ultimately, an agent \nis a kind of computer program run\u00adning in a run-time environment. Whatever language features A2BL supports, \ncomputer programs will need to be written and debugged. Given the complexity of individual agents and \nour desire to support real world-scale multi-agent sys\u00adtem modeling, the task of writing A2BL agents \nand multi\u00adagent systems is likely to be a signi.cant effort, akin to that of a large software engineering \nproject. We will therefore need to address many of the same issues as traditional soft\u00adware engineering: \n Are there effective visual metaphors for agent behavior that would enable the effective use of a visual \nprogram\u00adming environment for A2BL?  What does it mean to debug an intelligent agent or multi-agent system? \n Can some of the mechanisms for structuring large soft\u00adware systems, such as objects and modules, be \ntrans\u00adferred effectively to an agent-authoring domain? What new kinds of structuring mechanisms need \nto be in\u00advented?  Can the A2BL language, compiler, and run-time environ\u00adment be designed in such a way \nthat the agent author need not be concerned with ef.ciency or optimization? If not, are we resigned to \nrequiring expert programmers to au\u00adthor intelligent agents?  5.2 OOPinA2BL ABL does not currently support \ninheritance. It seems nat\u00adural to model agents with an inheritance hierarchy similar to OO modeling in \nmodern software engineering; however, supporting inheritance in agents may not be as simple as borrowing \nthe body of existing theory from OOP. Agents are more than objects, and their behavior is stochastic. \nWhat would it mean for an agent to be a subtype of another agent? Would we call this an is-a relationship? \nWould we as\u00adcribe all of the semantics that OOP ascribes to is-a re\u00adlationships? In particular, how do \nwe model preconditions and postconditions in a stochastic agent? Because type in\u00adheritance, or some related \nform of reuse, seems useful for supporting large-scale, real-world agent programming, it is worthwhile \nto develop the theory necessary to implement an inheritance mechanism that (1) supports the design of \nlarge systems of agents and (2) supports reuse mechanisms for A2BL. 5.3 Usability Because behavior is \na part of the ABL acronym, one might believe that ABL is designed for experts in human behav\u00adior, such \nas psychologists or sociologists. While ABL can support the needs of such designers, ABL is a complex \nlan\u00adguage that exposes many technical details to agent authors, making it suitable mainly for programming \nexperts. So far, mainly senior undergraduate and graduate students in com\u00adputer science have been productive \nwith ABL. Given that we envision A2BL as a tool for non-programming experts, and A2BL is based on ABL, \nwe must consider several important questions: What kinds of abstractions and language features are required \nby behavior experts such as psychologists to effectively encode their domain knowledge in A2BL?  Can \nsuch non-programmer-oriented language features subsume the advanced features that lead to ABL s com\u00adplexity \nwithout losing the power they bring to ABL?  Noting Alan Perlis s epigram a programming lan\u00adguage is \nlow level when its programs require attention to the irrelevant what is irrelevant when modeling in\u00adtelligent \nagents?  Is it desirable to have both programmer-oriented, and domain expert-oriented language features \nin A2BL so that an agent author can choose to get down and dirty sometimes and maintain a higher level \nof abstraction at other times?  Is it realistic to expect psychologists or sociologists to adopt a form \nof computer programming as a basic part of their methodological tool kit? How should we go about making \nthat happen?   6. Conclusions In this paper we have presented A2BL, a language that inte\u00adgrates reinforcement \nlearning into a programming language. We have argued that it implements many of the features necessary \nfor partial programming while speci.cally using programming features that have proven useful for designing \nlarge adaptive software agents. We believe that while there is a great deal of work to do in proving \nconvergence and correctness of various machine learning algorithms in the challenging environments we \nen\u00advision, this is in some sense a straightforward exercise. The more dif.cult task is to understand \nhow one would build use\u00adful development and testing environments, and to understand the software engineering \nprinciples that apply for scalable partial programming. 7. Acknowledgments We are grateful for the generous \nsupport of DARPA under contract number HR0011-07-1-0028, and NSF under con\u00adtract numbers IIS-0644206 \nand IIS-0749316.  References David Andre and Stuart Russell. Programmable reinforce\u00adment learning agents. \nIn Advances in Neural Infor\u00admation Processing Systems, volume 13, 2001. URL citeseer.ist.psu.edu/article/andre00programmable.html. \nDavid Andre and Stuart Russell. State abstraction for pro\u00adgrammable reinforcement learning agents. In \nAAAI-02, Edmon\u00adton, Alberta, 2002. AAAI Press. Sooraj Bhat, Charles Isbell, and Michael Mateas. On the \ndif.\u00adculty of modular reinforcement learning for real-world partial programming. In Proceedings of the \nTwenty-First National Con\u00adference on Arti.cial Intelligence (AAAI-06), Boston, MA, USA, July 2006. Thomas \nG. Dietterich. The MAXQ method for hierar\u00adchical reinforcement learning. In Proc. 15th Inter\u00adnational \nConf. on Machine Learning, pages 118 126. Morgan Kaufmann, San Francisco, CA, 1998. URL citeseer.ist.psu.edu/dietterich98maxq.html. \nLeslie Pack Kaelbling, Michael L. Littman, and Andrew P. Moore. Reinforcement learning: A survey. Journal \nof Arti.cial Intelligence Research, 4:237 285, 1996. URL citeseer.ist.psu.edu/kaelbling96reinforcement.html. \nA. B. Loyall and J. Bates. Hap: A reactive adaptive architecture for agents. Technical Report CMU-CS-91-147, \n1991. URL citeseer.ist.psu.edu/loyall91hap.html. Michael Mateas and Andrew Stern. Facade: An experiment \nin building a fully-realized interactive drama. In Game Developers Conference: Game Design Track, San \nJose, CA, March 2003. Michael Mateas and Andrew Stern. Life-like Char\u00adacters. Tools, Affective Functions \nand Applica\u00adtions, chapter A Behavior Language: Joint Action and Behavioral Idioms. Springer, 2004. URL \n http://www.interactivestory.net/papers/MateasSternLifelikeBook04.pdf. Tom Mitchell. Machine Learning. \nMcGraw-Hill, 1997. Peter Norvig. Decision theory: The language of adap\u00ad tive agent software. Presentation, \nMarch 1998. URL http://www.norvig.com/adaptive/index.htm. Peter Norvig and David Cohn. Adaptive software, \n1998. URL http://norvig.com/adapaper-pcai.html. Ronald Parr and Stuart Russell. Reinforcement learning \nwith hier\u00adarchies of machines. In Michael I. Jordan, Michael J. Kearns, and Sara A. Solla, editors, Advances \nin Neural Information Processing Systems, volume 10. The MIT Press, 1998. URL citeseer.ist.psu.edu/parr97reinforcement.html. \n Stuart Russell and Peter Norvig. Arti.cial Intelligence: A Modern Approach. Prenticce Hall, Upper Saddle \nRiver, NJ, 2003. R.S. Sutton and A.G. Barto. Reinforcement Learning: An Introduction. MIT Press, Cambridge, \nMA, 1998. URL citeseer.ist.psu.edu/sutton98reinforcement.html. Sprague, N., and Ballard, D. 2003. Multiple-Goal \nReinforcement Learning with Modular Sarsa(0). In Proceedings of the Eigh\u00adteenth International Joint Conference \non Arti.cial Intelligence. Workshop paper.  \n\t\t\t", "proc_id": "1449764", "abstract": "<p>Current programming languages and software engineering paradigms are proving insufficient for building intelligent multi-agent systems--such as interactive games and narratives--where developers are called upon to write increasingly complex behavior for agents in dynamic environments. A promising solution is to build adaptive systems; that is, to develop software written specifically to adapt to its environment by changing its behavior in response to what it observes in the world. In this paper we describe a new programming language, An Adaptive Behavior Language (A2BL), that implements adaptive programming primitives to support partial programming, a paradigm in which a programmer need only specify the details of behavior known at code-writing time, leaving the run-time system to learn the rest. Partial programming enables programmers to more easily encode software agents that are difficult to write in existing languages that do not offer language-level support for adaptivity. We motivate the use of partial programming with an example agent coded in a cutting-edge, but non-adaptive agent programming language (ABL), and show how A2BL can encode the same agent much more naturally.</p>", "authors": [{"name": "Christopher Simpkins", "author_profile_id": "81381608482", "affiliation": "Georgia Institute of Technology, Atlanta, GA, USA", "person_id": "P1223255", "email_address": "", "orcid_id": ""}, {"name": "Sooraj Bhat", "author_profile_id": "81330488113", "affiliation": "Georgia Institute of Technology, Atlanta, GA, USA", "person_id": "P1223256", "email_address": "", "orcid_id": ""}, {"name": "Charles Isbell", "author_profile_id": "81100608549", "affiliation": "Georgia Institute of Technology, Atlanta, GA, USA", "person_id": "P1223257", "email_address": "", "orcid_id": ""}, {"name": "Michael Mateas", "author_profile_id": "81100130074", "affiliation": "University of California, Santa Cruz, Santa Cruz, CA, USA", "person_id": "P1223258", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1449764.1449811", "year": "2008", "article_id": "1449811", "conference": "OOPSLA", "title": "Towards adaptive programming: integrating reinforcement learning into a programming language", "url": "http://dl.acm.org/citation.cfm?id=1449811"}