{"article_publication_date": "10-19-2008", "fulltext": "\n Delegation-based Semantics for Modularizing Crosscutting Concerns Hans Schippers * Dirk Janssens Formal \nTechniques in Software Engineering University of Antwerp, Belgium {hans.schippers.dirk.janssens}@ua.ac.be \nAbstract We describe semantic mappings of four high-level program\u00adming languages to our delegation-based \nmachine model for aspect-oriented programming. One of the languages is a class-based object-oriented \none. The other three represent extensions thereof that support various approaches to mod\u00adularizing crosscutting \nconcerns. We explain informally that an operational semantics expressed in terms of the model s concepts \npreserves the behavior of a program written in one of the high-level languages. We hence argue our model \nto be semantically sound in that sense, as well as suf.ciently expressive in order to correctly support \nfeatures such as class-based object-oriented programming, the open-classes and pointcut-and-advice .avors \nof aspect-oriented program\u00adming, and dynamic layers. For the latter, being a core feature of context-oriented \nprogramming, we also provide a formal semantics. Categories and Subject Descriptors F.3.2 [Logics and \nMeanings of Programs]: Semantics of Programming Languages Operational Se\u00admantics General Terms Theory, \nLanguages Keywords Semantic Mappings, Aspect-oriented Seman\u00adtics, Context-oriented Programming, Modularization \n1. Introduction In previous work [12], we have introduced a machine model for aspect-oriented programming \n(AOP), which is centered * Ph.D. fellowship of the Research Foundation -Flanders (FWO) Permission to \nmake digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. Michael Haupt Robert \nHirschfeld Software Architecture Group Hasso-Plattner-Institut University of Potsdam, Germany {michael.haupt.hirschfeld}@hpi.uni-potsdam.de \non delegation and relies on the notion of join points as loci of late binding, upon whose occurrence \ndispatch operations de\u00adtermine the functionality to execute. The model was shown to elegantly support \na wide range of core mechanisms and features of both the object-and aspect-oriented program\u00adming paradigms, \nsuch as classes, inheritance, open classes [17] in the form of dynamic introductions and pointcut-and\u00adadvice \nbased AOP [15]. However, the model s ability to serve as the basis for complete aspect-oriented programming \nlanguage implementations was not shown. This paper demonstrates how the operational semantics of j, ij, \naj [21] and cj, four high-level programming lan\u00adguages, can be mapped onto the operational semantics \nof our machine model, as well as provides an informal argument suggesting their behavior is preserved \nin the process. Hence, whereas our previous work started from the model itself to demonstrate its capabilities, \nthe model now serves as a target platform for several existing languages and the modulariza\u00adtion techniques \nthey support. In other words, the model is shown to be suf.ciently expressive to meet the requirements \nof j, ij, aj and cj, which support different approaches to mod\u00adularization. j is, essentially, a Java \nsubset and hence supports basic object-oriented principles. The other languages are exten\u00adsions, adding \ndifferent modularity mechanisms for the im\u00adplementation of crosscutting concerns. More speci.cally, ij \nadds inter-type declarations, while aj adds aspects, point\u00adcuts and advice. Thus, ij represents an implementation \nof the open classes .avor of AOP, and aj, one of the pointcuts-and\u00adadvice .avor [15]. The languages are \nformally described by means of an operational semantics for each of them [21]. Al\u00adthough ij s semantics \nis described as a transformation into an equivalent j program, we will show our model to be capable of \ndirectly supporting ij s features. cj is our own contribution to the j language family. It does not include \nthe features of ij and aj, but instead adopts context-oriented programming (COP) [5, 13], a layer-based \napproach to the modularization of crosscutting concerns. OOPSLA 08, October 19 23, 2008, Nashville, \nTennessee, USA. Layers allow context-speci.c behavioral variations to be Copyright &#38;#169; 2008 ACM \n978-1-60558-215-3/08/10. . . $5.00 c composed based on the execution context. cj can be consid\u00adered a \nsubset of ContextJ [6]. The contributions of this paper are as follows: we show how the semantics of \nthe j, ij, and aj languages [21] can be directly expressed in terms of our model s semantics [12], \n we explain informally that these semantics are equivalent with their original counterparts,  we provide, \nfor the .rst time, a formal semantics for cj, and hence, for one .avor of context-oriented program\u00adming, \nand map it to our model s semantics like for the other three languages.  All of the aforementioned languages \nhave been implemented on top of a prototype implementation of the delegation-based model, and said implementations \nadhere to the semantics we present. A description of the implementations is out of the scope of this \npaper. This paper is organized as follows. The next section brie.y reviews our machine model along with \nits formal semantics. Sec. 3 describes the semantics mappings for all four languages, and additionally \nintroduces a formal seman\u00adtics for cj. Sec. 4 attends to related work. The paper is sum\u00admarized and future \nwork is discussed in Sec. 5. 2. A Machine Model for Aspect-Oriented Programming We restrict the presentation \nof the machine model we use in this work [12] to a brief summary of the model and its semantics to facilitate \nan easier understanding of the seman\u00adtic mappings described in Sec. 3. The following sections 2.1 and \n2.2 recapitulate, in condensed form, material that has been published in [12], while Sec. 2.3 describes \nsome mod\u00adi.cations to the original semantics that are required in the scope of this work. 2.1 The Model \nCore features of the model pertain to the representation of application entities and that of join points. \nThe latter are consistently regarded as loci of late binding, and hence of virtual functionality dispatch, \nwhere dispatch is organized along multiple dimensions. Each dimension is one possible way to choose a \nparticular binding of a piece of functionality to a join point, e. g., the current object, the target \nof a method call, the invoked method, the current thread, etc. Objects are, using a prototype-based object-oriented \nen\u00advironment, consistently represented as \"seas of fragments\" [18]: each object is visible to others \nonly in the form of a proxy. Messages sent to an object are received by its proxy and delegated to the \nactual object, as displayed in Fig. 1. Classes are represented likewise: each class is a pair of a proxy \nand an object representing the actual class. Each ob\u00adject references its class by delegating to the class \nproxy. Figure 1. An object is represented as a combination of a proxy and the actual object. The granularity \nof the supported join point model is that of message receptions. It is important to note that this gran\u00adularity \nexists only at the level of the execution model, where member .eld access is also mapped to messages. \nLanguage implementations on top of the model will map their own join point model to the one de.ned by \nthe machine model. A join point s nature as a locus of late binding is realized by means of inserting \nadditional proxy objects in between the proxy and the actual object, or in between the class object s \nproxy and the actual class-representing object. That way, a message passed on along the delegation chain \ncan be interpreted differently by various proxies understanding it, establishing late binding of said \nmessage to functionality. Weaving both static and dynamic is realized by allowing for the insertion and \nremoval of proxy objects into and from delegation chains. 2.2 Model Semantics The formal, operational \nsemantics of the machine model, provided in [12], is based on the d calculus [2]. More specif\u00adically, \nreduction rules are used in order to de.ne an opera\u00adtional semantics function which rewrites a combination \nof an expression and a store into an object address, represent\u00ading the result value, and a potentially \nmodi.ed store: .d : Exp \u00d7 Store . Address \u00d7 Store At the heart of the semantics are the Clone (1) and \nSelect (2) operations [12], which respectively handle object creation and message sending: a,s .d ., \ns\" .\" /. dom(s\") s\"\" = s\"[.\" . s\"(.); Del.\" . Del. ] .\"\" /. dom(s\"\") (1) s\"\"\" = s\"\"[.\"\" . []][Del.\"\" \n. .\"] clone(a),s .d .\"\",s\"\"\" a,s .d ., s\" Look(s\",.,m) = (b,.d ) s\"\" = s\"[this . .][msg . m][cur . .d \n] b,s\"\" .d .\",s\"\"\" (2) s\"\"\"\" = s\"\"\"[this . s(this)] [msg . s(msg)][cur . s(cur)] ,s\"\"\"\" a.m,s .d.\" where \ns denotes the store, which essentially maps addresses to objects. An object is represented as a list \n[m1: e1,...,mn : en] of messages it understands, along with their implementations. The store additionally \ncontains, for each address ., the Del. function, which determines the address of the delegate of the \nobject at .. The delegate is the object to which messages are delegated if they are not understood. In \ngeneral, an object s delegate may depend, for example, upon the currently ac\u00adtive thread. However, in \nthe context of this paper, Del. will always be a constant function. Updates to the store are expressed \nin square brackets, where the . symbol is used to either assign a new object to an address, or change \nthe value of a particular Del. function. \" For example, the store s\"[. . s(.)][Del.\" . Del. ] is \" identical \nto s\", except that address .now holds the same value as the one at address . (meaning a copy of the object \nat . has been stored at .\"), and the constant function Del.\" is now equal to the constant function Del. \n(meaning the object \" at .now has the same delegate as the object at .). Basically, Clone (1) creates \nobjects as a pair of objects: an empty proxy and the actual object, with the proxy sim\u00adply delegating \nall messages to the actual object. Select (2), on the other hand, looks up an implementation b of a mes\u00adsage \nm, sent to an object (expression) a, by means of the Look function, which recursively traverses a s delegate \nob\u00adjects until it encounters an implementation of m, and thus encapsulates the delegation mechanism inherent \nto the sys\u00adtem. Its de.nition is shown in Fig. 2. Next, b is evaluated while the store is extended to \nmap a number of symbols, representing special variables, to an appropriate value: this holds the address \nof the message re\u00adceiver object, msg holds the message name, and cur con\u00adtains the address of the speci.c \ndelegate object where the message implementation was eventually found. This infor\u00admation is necessary \nin order to support resending messages further along the delegation chain: Look(s,Dels(cur),s(msg)) = \n(b,.d ) s\" = s[cur . .d ] \" \"\" b,sd.,s(4) \"\"\" = s s\"\"[cur . s(cur)] \"\"\" resend,s d.,sNote that this is \nnot updated, and hence remains bound to the original receiver. This matches the common seman\u00adtics of \ndelegation-based object-oriented programming and is crucial for the proper functioning of the model [12]. \n 2.3 Minor Modi.cations In order to facilitate the description of the semantic map\u00adpings in Sec. 3, \nwe apply a few super.cial modi.cations, which do not, however, change the actual semantics. Store vs. \nStack/Heap The store variable s actually models the combination of a heap and a stack: Apart from storing \nall objects, it addition\u00adally takes care of stack variables such as this. However, this is a non-fundamental \ndesign choice [21, p. 66], and heap and stack may just as well be modeled separately by means of two \nvariables h and s. Both approaches are equivalent, pro\u00advided that, in case of a single store, care is \ntaken to rebind stack variables to their previous value after a method call, as opposed to passing a \nnewly constructed stack frame along with each method call. For example, the Select rule might just as \nwell be modeled as follows: a,h,s d.,h\" Look(h\" ,.,m)=(b,.d ) b,h\" ,{this . ., msg . m,cur . .d } d.\" \n,h\"\" (5) \" ,h\"\" a.m,h,s d. In the following, this style will be adopted for all rules. Message Parameters \nThe delegation-based machine model does not model a pa\u00adrameter passing mechanism. However, a strategy \nsupporting exactly one formal parameter which is always called x, sim\u00adilar to the one used in the original \nde.nitions of the j lan\u00adguage family [21], can be straightforwardly implemented. It comes down to simply \npassing one more variable on the stack (or store). Hence, where convenient, alternative de.ni\u00adtions of \nthe Select and Resend rules, as shown in Fig. 3, will be used. Note that, for reasons of simplicity, \nwe assume a message to be uniquely identi.ed by its name. Parameter overload\u00ading is disregarded. Multiple \nparameters can be simulated by regarding the formal parameter x as a container object en\u00adclosing the \nactual parameters. 3. Semantic Mappings This section will introduce the j, ij, aj [21] and cj languages, \nalong with their operational semantics. For each of these lan\u00adguages, we will express these semantics \nalternatively using the model semantics from Sec. 2. We will then explain infor\u00admally that these semantics \nresult in the same behavior, i. e., evaluating a certain expression results in the same value and the \nsame side-effects on the heap. A formal proof for this claim is considered future work. The original \noperational semantics of the languages is expressed by means of a rewriting function de.ned through a \nnumber of reduction rules [21]. This strategy is very similar to the one followed in Sec. 2, hence the \nuse of the same symbol , albeit without the d subscript as it is not based on the d calculus: P f exp,h, \ns .,h \" Note that, compared to the rewrite function in Sec. 2, an extra parameter P is added. It represents \nthe source code of the current program, and is used in some reduction rules in order to extract relevant \ninformation such as method decla\u00adrations or inheritance hierarchies.  (b,.) if s(.)= [...m : b...] Look(s,.,m)=(3) \nLook(s,Del. ,m) otherwise Figure 2. Look function. a1,h,s d.,h\" \" ,h\"\" a2,h\" ,s d.Look(h\"\" ,.,m)=(b,.d \n) (6) b,h\"\" ,{this . .,x . .\" ,msg . m,cur . .d } d.\"\" , h\"\"\" \"\" ,h\"\"\" a1.m(a2),h,s d. a,h,s d.,h\" Look(h\" \n,Dels(cur),s(msg)) = (b,.d ) b,h\" , {this . s(this),msg . s(msg),cur . .d ,x . .} d.\" ,h\"\" (7) \" , h\"\" \nresend(a),h,s d. Figure 3. Alternative de.nitions of Select and Resend, supporting a formal parameter \nx. 3.1 Class-Based Object-Oriented Programming in j This section handles j, which is a subset of Java \nand supports basic object-oriented principles such as object instantiation, inheritance and method invocation. \nSyntax An EBNF-style de.nition of the syntax of j [21] is given in Lst. 1. It is rather minimal, with \na PROGRAM consisting of a number of CLASS elements, which in turn encapsulate FIELD and METHOD constructs. \nMethod bodies are expres\u00adsions, where expressions may be recursively concatenated. Methods always accept \nexactly one parameter x, which can be referred to in a method body, along with the this variable. Finally, \nthree special values true, f alse and null are avail\u00adable, which we will assume to be prede.ned objects. \nHence, the value of an expression will always be an object. 1 P R O G R A M : : = C LA S S * 2 C LA \nS S : : = c l a s s C L S N A M E e x t C L S N A M E { D E C L * } 3 D E C L : : = F I E L D M E T \nH O D 4 F I E L D : : = T Y P E I D E N T 5 M E T H O D : : = T Y P E I D E N T ( T Y P E x ) { E X P \n} 6 E X P : : = S P E C IA L 7 V A R 8 n e w C L S N A M E 9 E X P I D E N T 10 E X P I D E N T \n: = E X P 11 E X P I D E N T ( E X P ) 12 E X P = = E X P 13 E X P ; E X P 14 i f ( E X P ) { E \nX P } e l s e { E X P } 15 S P E C IA L : : = t r u e f a l s e n u l l 16 V A R : : = t h i s x \nListing 1. j syntax. Objects, Classes and Inheritance In j, classes are static descriptions of the structure \nof their instances. Instances are created by analyzing that description to .nd out which .elds memory \nshould be allocated for. On the heap, an object is represented as a combination of a map of the names \nof these .elds to their values, and the name e of the class it belongs to. This is expressed by the newoperation \n[21], which essentially boils down to: C newe(P,C)= [ f1 : null... fn : null] (8)where AllFields(P,C)= \nf1 ... fn The AllFields function recursively .nds all .elds of a class C, de.ned in a program P, as well \nas its super classes. The reference to an object s class (in superscript in that object s representation) \nis needed to provide an entry point for method lookup. The reduction rule for class instantiation is \nas follows: . ./dom(h) newe(P,C) = Unde f ined (9) P f new C, h,s .,h[. . newe(P,C)] In an object-based \nsetting, the concept of a class does not exist, as the only entities available are objects. As the Clone \noperation (cf. Sec. 2, de.nition (1)) basically copies an ex\u00adisting object, it seems natural to provide \na so-called proto\u00adtype object for each class on the heap, which encapsulates messages corresponding to \nthe .elds of that class and all of its super classes. This prototype can then be cloned in order to create \ninstances. The implementations of said messages will hold the .eld values. Additionally, the class prototype \nshould have a delegate with all method implementations of that class, which in turn should have a delegate \nwith all im\u00adplementations of that class super class, and so on. This way, instances will understand the \ncorresponding messages via delegation. Finally, in order to be able to support class-wide manipulations \nof the delegation chain, a proxy object should be inserted for each class, as outlined in [12]. An example \nof such a con.guration is displayed in Fig. 4. hj = {C1 . .proxy,1,...,Cn . .proxy,n,ClPt(C1) . .1,...ClPt(Cn) \n. .n, P .1 . [ f1,1 : null ... f1,k1 : null],...,.n . [ fn,1 : null... fn,kn : null], .proxy,1 . [],...,.proxy,n \n. [], .meth,1 . [m1,1: b1,1,...,m1,p1: b1,p1 ],...,.meth,n . [mn,1: bn,1,...,mn,pn : bn,pn ], Del.1 = \n.proxy,1,...,Del.n = .proxy,n, Del.proxy,1 = .meth,1,...,Del.proxy,n = .meth,n, (10) Del.meth,1 = hPj \n(supere(P,C1)),...,Del.meth,n = hPj (supere(P,Cn))} where \" .i : Ci . Classes(P),AllFields(P,Ci)= fi,1 \n... fi,ki , . j : T mi, j(Tx){bi, j}. getMethods(Ci) ClPt : CLSNAME . Address .1 ....n,.proxy,1 ....proxy,n,.meth,1 \n....meth,n are unique  Figure 5. Prepared heap for j programs. Figure 4. Example of a prepared heap \nIn order to model this semantically, we assume that, in an object-based setting, a j program always executes \nin the context of a prepared heap, rather than an empty one, in a similar fashion as is done for aspects \nin the aj language [21, p. 120]. The heap is assumed to store, beside objects and the Del function, mappings \nof class names to addresses, in order to ensure that classes can be looked up by name, as well as the \nClPt function, which associates each class with its class prototype. The de.nition of the prepared heap \nis displayed in Fig. 5. Recall that the Del. function determines the delegate of the object at address \n., while AllFields returns the .elds of a class and all its super classes, and getMethods [21] results \nin a collection of the methods de.ned by one speci.c class. Finally, the super function [21] determines \nthe name of a class super class. If care is taken to always install a class super class on the heap prior \nto the class itself, the super class proxy can be looked up by means of this name, which is then installed \nas the delegate of the class method object. For completeness, a prede.ned Ob ject class could be Figure \n6. Result of class instantiation with delegation\u00adbased semantics. installed on the heap, serving as the \nsuper class of the .rst user-de.ned class. With this prepared heap in place at the start of program execution, \ninstance creation in the machine model works as follows: P f clone(ClPt(C)),h,s d.,h\" (11) P f new C,h, \ns d.,h\" The new operation will thus result in a proxy (created by clone), delegating all messages to \na copy of C s class pro\u00adtotype, the latter being available on the heap at ClPt(C). As the DelClPt(C) \nfunction is also stored on the heap, and hence copied as well, the copy ends up delegating to C s proxy, \nand hence eventually to the object holding C s methods. This is displayed in Fig. 6. Method Lookup Although \nit is now established how classes and objects are dealt with in an object-based setting, it should still \nbe ver\u00adi.ed that such strategy indeed results in the same program behavior. More speci.cally, a method \ncall (or message send) should, both in original j semantics as well as in delegation\u00adbased semantics, \nresult in the same method (or message) im\u00adplementation being found and executed. The two relevant reduction \nrules are the Select rule (cf. Sec. 2, de.nition (6)) on the one hand, and the j rule for method calls \non the other [21]: P f e0,h,s .,h\" .\" ,h\"\" P f e1,h\" ,s \" Me(P,type(h\"\" , .),m)= Tm(Tx){e} P f e,h\"\" \n,{this . ., x . .\"} .\"\" ,h\"\"\" \"\" ,h\"\"\" P f e0.m(e1),h,s . Me : PROGRAM \u00d7 CLSNAME \u00d7 Identi f ier . METHOD \nMe(P,C,m)= . . . . . Undefined if C = Object M if getMethods(C)/{m} = M (12) if getMethods(C)/{m} = \ne Me(P, super(P,C), m) Unde f ined otherwise Figure 7. Me function. where the type function [21] determines \nthe dynamic type of an object. The msg and cur variables in de.nition (6) can be ignored since they are \nonly ever used while resending mes\u00adsages, which never occurs here. Apart from that, the only difference \nbetween both rules is in the Me and Look (cf. Sec. 2, de.nition (3)) functions, which capture the respec\u00adtive \nlookup algorithms. Hence, the crucial question is now whether they traverse the space of method implementations \nin the same way. The Me function is de.ned as in Fig. 7 [21]. The getMethods helper function [21] in \nthe de.nition re\u00adsults in a collection of the methods de.ned by one speci.c class and the super function \n[21] determines the name of a class super class, while X/{q} [21, p. 50] is the domain re\u00adstriction function, \nwhich basically restricts a set of syntactic constructs X to the ones containing an identi.er q. Note \nthat words in smallcaps, such as METHOD, represent their corre\u00adsponding syntactic constructs as de.ned \nin Lst. 1. Me .rst checks whether a method named m is de.ned by the class passed as a parameter, which \nis the class to which the target object of the method call belongs. If not found there, the chain of \nsuper classes will be traversed recursively until m is encountered. Note that the checks are performed \nbased on the program text. The Look function, on the other hand, actually starts its search in the target \nobject (rather, on its proxy), and recur\u00adsively traverses that object s delegation chain until an object \nis encountered which has an implementation for m. Consid\u00adering a proxy by de.nition does not respond \nto any mes\u00adsages, and considering the setup of the prepared heap (10), the .rst candidate object is the \nactual object, i. e., the copy of the class prototype which was created during instantiation. This corresponds \nto actual_c in Fig. 6. However, as this ob\u00adject, by de.nition, contains only .elds, the search continues, \npassing the class proxy and ending up in the object holding the method implementations of the target \nobject s class. If a suitable method implementation is found here, it is returned; if not, the search \ncontinues, passing the class proxy of the super class, and ending up in the object holding the method \nimplementations of the super class, and so on. In conclusion, the lookup algorithm does indeed en\u00adcounter \nall candidate method implementations in the same order, and hence yields the same result. It is important \nto realize that the heart of the semantic mapping is in fact in prepared heap construction. It is the \nspeci.c con.guration of this heap which causes j method calling and message sending to deliver the same \nresults. Field Access Although it has now been argued that method calls will re\u00adsult in the same behavior \nin both versions of operational se\u00admantics, we still need to show that heap access semantics are preserved. \nWhile object creation, which has been cov\u00adered above, is obviously part of that, there is no assurance \nso far that .eld access works correctly. The j reduction rule for getting a .eld value is as follows: \nP f e,h, s .,h\" \" h\"(.)( f )= .(13) \" P f e. f ,h,s ., h\" This means that heap content is simply checked \nat the target object s address, which, considering the de.nition of newe above (8), indeed contains values \nfor all its .elds. In the object-based machine model, .elds, just like meth\u00adods, are mapped to messages, \nwhere the message implemen\u00adtations are simply the .eld values. Hence, the Select rule (cf. Sec. 2 (5)) \nshould be applied. By construction of the pre\u00adpared heap (10), we know that the receiver of the message \nis a proxy, hence the message implementation will be found in the actual object. By the de.nition of \nLook (cf. Sec. 2 (3)), this is indeed at h\"(.)( f ). As we know the implementation to already be an address, \nthe last line of Select, evaluating the implementation, can be ignored. Hence, the remainder is equivalent \nto (13). Field updates, which j handles in a straightforward way by modifying the appropriate location \non the heap, should obviously be handled by replacing the corresponding mes\u00adsage implementation. Other \nj Operations j supports other operations besides method calls and .eld access, such as testing for equality \nand an if-then-else con\u00adstruct. However, as the machine model only supports objects and messages, all \nthese should be implemented as message sends, in the spirit of purely object-oriented languages such \nas Smalltalk. In such a scenario, their semantics are covered by the previous discussion on method calls. \n Figure 8. Dynamic introduction of a .eld f and a message msg.  3.2 Inter-Type Declarations in ij ij \n[21] is an extension of j which adds inter-type declarations. More speci.cally, it allows classes to \ndeclare .elds and methods which actually belong to another class. To this end, it provides a simple extension \nto the j syntax, as listed in Lst. 2. 1 CLASS ::= class CLSNAME ext CLSNAME { DECL * ITD * } 2 ITD ::= \nCLSNAME < --DECL Listing 2. ij syntax. Semantics for this construct are de.ned in [21] in terms of a \nrather straightforward weaving approach: Initially, a syntax transformation is applied, turning each \nITD into a DECL in the proper class. This results in a valid j program, the semantics of which are known. \nIn our machine model, however, inter-type declarations are supported directly, and in fact implemented \nas dynamic introductions [12]. Each inter-type method declaration re\u00adsults in a new proxy object being \ncreated and inserted in the delegation chain of the target class. For .eld introductions, a class-wide \nmethod is added as well, execution of which re\u00adsults in another proxy, this time instance-local, to be \ndynam\u00adically inserted in the delegation chain of the instance object which received the message. All \nthis is displayed in Fig. 8, where in (b) actually just one class-wide proxy is inserted for two introductions. \nWhile this is indeed a possible opti\u00admization, we will assume here, for the sake of simplicity, that \neach inter-type declaration results in a new proxy. Method Introductions We now need to verify that, \ngiven an inter-type declara\u00adtion C<--TmeT' x) { b}, inserting a proxy with an implementation for m results \nin the same behavior as if m would have been declared in C s class de.nition. Formally, given a prepared \nheap hP such as (10) in Sec. 3.1, the effects of this inter-type declaration are the following: h \" P \n= hP[.ip . [m : b]][Del.ip . DelhP(C)][DelhP(C) . .ip] where .ip is an unused address in hP. In other \nwords, C s class proxy now delegates to the new proxy holding an implementation for m, which in turn \ndelegates to C s original method object. Recalling the Look function (cf. Sec. 2 (3)), which is at the \nheart of method call semantics in the delegation-based model, it now becomes clear that this semantics \nis indeed satisfactory: As Look recursively traverses the delegation chain through the Del function, \nit will eventually encounter m s implementation when m is called on an instance of C. Moreover, as the \nnew proxy delegates to C s original method object, implementations of other methods de.ned by C will \nstill be found as well. Note that these heap modi.cations do not take place at runtime. Rather, they \ncause the prepared heap to have a slightly different con.guration at program start, mimicking the static \ncharacter of inter-type declarations in ij. However, our machine model allows for similar delegation \nchain mod\u00adi.cations during program execution as well, which occurs for example in the context of .eld \nintroductions. Field Introductions As hinted above, .eld introductions require an additional step compared \nto method introductions. Consider an inter\u00adtype .eld declaration C<--Tf. Its effects on a prepared heap \nare similar at the .rst glance: h \" = hP[.ip . [ f : bip]][Del.ip . DelhP(C)][DelhP(C) . .ip] P Note, \nhowever, that bip, the implementation of the mes\u00adsage f , is this time not provided as part of the syntactic \nprogram. Instead, it has speci.c semantics, resulting in the installation of another proxy in the delegation \nchain of the target object, which is available on the stack through this. Formally, the semantics of \nits execution is as follows: .ip\"./dom(h) h\" . [ f : null]] = h[.ip\" . = s(this) h\"\" = h\"[Del.ip\" . Del. \n][Del. . .ip\" ] .. f ,h\"\" \" ,h\"\"\" ,s d. \" ,h\"\"\" b. p,h,s d. \" where we actually know that .= null and \nh\"\"\" = h\"\", because f being sent again will now encounter an implementation at .ip\" , which is null, \nand hence trivially evaluates to itself without modifying the heap. The important observation, however, \nis that the method implementation which is installed at .ip is only executed at .rst .eld access, and \neventually results in null being returned, exactly as if the .eld had been declared as part of C s class \nde.nition, which would have resulted in it being part of the prepared heap (cf. Sec. 3.1 (10)) and initialized \nto null there. Semantics of new (cf. Sec. 3.1 (11)) would then have copied it during cloning to become \npart of the instance, where it would have been available immediately at .rst access. For all subsequent \naccesses, a similar argument as the one for method introductions applies.  3.3 Before and After Advice \nin aj aj [21] is an extension of j which adds support for before and after advice at call, get and set \njoin points. Since in aj a call join point always coincides with an execution join point, call actually \nrepresents both. Syntactically, aj adds the ASPECT, ADVICE and POINTCUT constructs compared to j, as \ndisplayed in Lst. 3. 1 PROGRAM ::= < CLASS ASPECT 2 ASPECT ::= asp ect CLSNAME { 3 ADVICE ::= < before \nafter 4 POINTCUT ::= call ( TYPE TYPE >* DECL * ADVICE * } > POINTCUT { EXP } IDENT ( TYPE ) )  5 g \ne t ( T Y P E T Y P E I D E N T ) 6 s e t ( T Y P E T Y P E I D E N T ) 7 V A R : : = t h i s x s r v \n Listing 3. aj syntax. Additionally, it introduces the stack variables s for the caller object, r for \nthe target object and v for the new value of a .eld update. These variables must only occur in an advice \nbody. Note that the original aj supports a limited form of wildcards in pointcut speci.cations, but that \nfeature is left out for simplicity reasons. Semantics are expressed in terms of a number of syntax analyzing \nhelper functions, which determine whether some advice is applicable at .eld get, set or method call, \nand execute it accordingly. Our machine model, however, supports advice by insert\u00ading a proxy in the \ndelegation chain of the appropriate class, as displayed in Fig. 9. Actually, this technique is suf.ciently \npowerful in order to support around advice with proceed as well. Although this is not supported by aj, \nproceed is cov\u00adered later on in Sec. 3.4 as part of cj, with essentially identi\u00adcal semantics as would \nbe required here. As the model currently does not keep track of type in\u00adformation for .elds and methods, \nwe will disregard the type speci.ers in a POINTCUT and hence assume that the method or .eld name and \nits owning class uniquely determine a method or .eld. get Join Points Semantics for getting a .eld s \nvalue in aj are essentially given by the reduction rule shown in Fig. 10 [21]. First, the Fe function \nis applied, which is similar to Me (cf. Sec. 3.1 t (12)), except that it looks up a .eld s declaration \nin a class hierarchy, and returns the name of the class where it was declared along with it, as shown \nin Fig. 11. Recall that the type function [21] determines the dynamic type of an object. Next, the advicese \nfunction computes a number of or\u00addered sets of advice applicable at the join point in question. The fact \nthat there are multiple orderings to choose from is due to there being no way to express dominance of \none as\u00adpect over another. Hence the only requirement is that multi\u00adple advice applying to the same join \npoint and de.ned within the same aspect should be executed in order of syntactical appearance. Each advice \nis uniquely identi.ed by a symbol ai which is used by the owningAspect function to determine which as\u00adpect \nthat advice belongs to. This is important since the this variable should, during advice execution, be \nbound to a sin\u00adgleton object which has memory for this aspect s .elds. This implies a prepared heap, \nwhich is constructed as follows: ha j = {C1 . .1,...,Cn . .n, P .1 . newe(P,C1),...,.n . newe(P,Cn)} \nwhere .i : Ci . Aspects(P) .1 ....n are unique (15) Note that this prepared heap is still part of the \naj semantics as de.ned in [21], and the fact that newe (cf. Sec. 3.1 (8)) can be applied to an ASPECT \nis due to the syntactical similarity of ASPECT and CLASS. When the set of applicable advice has been \ndetermined, all before advice is executed. During advice execution, be\u00adsides this, the s and r variables \nare made available on the stack, and bound to the caller and target objects, respectively. After all \nbefore advice has been executed, the actual .eld value is fetched and .nally all after advice is executed. \nNote \" that it is the .eld value .which is indeed the result value of the complete expression. In our \nmachine model, in order to obtain equivalent semantics, a somewhat more extensive prepared heap is needed, \nwhich starts out with a setup holding the contents of (10) provided in Sec. 3.1 in order to support classes. \nIn addi\u00adtion, it requires a singleton object for each aspect, but since there is no reason for representing \nthese as a combination of a proxy and an actual object, aj semantics can be retained. If we assume .1 \n....n to be unused in the prepared heap of (10) in Sec. 3.1, and aspects to have globally unique names, \nthe required prepared heap hP, so far, is intuitively the union of (10) and (15).  However, this is \nnot yet suf.cient. In order to allow class\u00adwide advice for .elds, we have to deal with the fact that \n.eld messages are actually understood in instances, and hence never reach proxies installed in the delegation \nchain of the class. This problem can be solved by installing getter and setter messages in the class \nmethod object, and ensuring that .eld access is always performed via these accessor mes\u00adsages. Said messages \ncan easily access the .eld value by  Figure 9. Class-wide advice for bar applied. P f e,h,s .,h1 Ft \ne(P,type(h1,.), f )=(Tf ,C) A1 ...Ak . advicese(P,be f ore,get(C. f )) Ak+1 ...An . advicese(P,a fter,get(C. \nf )) .i : Ai = ai : a get(C. f ){ei}, owningAspecte(P, ai)= Oi a .{be f ore,a fter} P f e1,h1,{s . s(this),r \n. .,this . h1(O1)} .1,h2 (14) \u00b7\u00b7\u00b7 P f ek,hk,{s . s(this),r . .,this . hk(Ok)} .k,hk+1 \" hk+1(.)( f \n)= . P f ek+1,hk+1,{s . s(this),r . .,this . hk+1(Ok+1)} .k+1,hk+2 \u00b7\u00b7\u00b7 P f en,hn, {s . s(this),r . .,this \n. hn(On)} .n,hn+1 \" P f e. f ,h,s .,hn+1 Figure 10. Semantics for getting a .eld value in aj. Fe t \n:: PROGRAM \u00d7 CLSNAME \u00d7 Identi f ier . FIELD \u00d7 CLSNAME Ft e(P,C, f )= . . . . . Undefined if C = Object \n(F,C) if getFields(P,C)/{ f } = F if getFields(P,C)/{ f } = e Ft e(P,super(P,C), f ) Undefined otherwise \nFigure 11. Fe function, looking up a .eld declaration in a class hierarchy, as well as the name of the \nclass where the declaration t is found. sending the actual .eld message to this. For example, the getter \nmethod for a .eld f of a class C is now part of C s method object: .meth . [m1: b1,...,mn : bn,getF : \nthis. f ,...] Note that this does not require aj programs to provide such accessor methods, but that \nthe compiler should appro\u00adpriately translate .eld access into the corresponding message sends. Finally, \nwe can describe how advice applying to a join point geteT C.f) (where we disregard T) affects the pre\u00adpared \nheap. This is shown in Fig. 12. Basically, what happens is that each before advice is translated to a \nproxy which understands the getter message of the .eld in question. It provides an implementation which, \nafter execution of the actual advice body, resends the mes\u00adsage along the delegation chain until it reaches \nthe original getter implementation which returns the .eld value. Seman\u00adtics of resending a message are \nprovided by the Resend re\u00adduction rule (cf. Sec. 2 (4)). after advice is treated identi\u00adcally, except \nfor the fact that the resend now occurs prior to execution of the advice body. Furthermore, the heap \nalso contains a function Asp, which keeps track of the aspect each advice was de.ned in. This is important \nin order to be able to properly bind this during advice execution. It should now be clear that, similarly \nto Sec. 3.2, the existing semantics of the Select rule (cf. Sec. 2 (5)) and the lookup algorithm incorporated \nin the Look function (cf. Sec. 2 (3)) are essentially suf.cient for properly executing all before and \nafter advice as well as the actual .eld access. However, care should be taken to properly bind the stack \nvariables. Hence Select is slightly modi.ed, as shown in Fig. 13. The Select rule is responsible for \nmessage sending in general, and is oblivious as to whether it is executing advice or a normal method. \nThis is where the Asp function is useful, as, in case of a method belonging to an advice proxy, it h\" \n= hP[.ap,1 . [getF : e1;resend]][Del.ap,1 . DelhP(C)][DelhP(C) . .ap,1]P \u00b7\u00b7\u00b7 [.ap,k . [getF : ek;resend]][Del.ap,k \n. DelhP(C)][DelhP(C) . .ap,k] [.ap,k+1 . [getF : resend;ek+1]][Del.ap,k+1 . DelhP(C)][DelhP(C) . .ap,k+1] \n\u00b7\u00b7\u00b7 [.ap,n . [getF : resend;en]][Del.ap,n . DelhP(C)][DelhP(C) . .ap,n] [Asp(.ap,1) . O1]... [Asp(.ap,n) \n. On] (16) where .ap,1 ....ap,n are unique and .i : .ap,i ./dom(hP) Ak ... A1 . advicese(P,be f ore,get(C. \nf )) Ak+1 ...An . advicese(P,a fter,get(C. f )) .i : Ai = ai : a get(C. f ){ei}, Oi = owningAspecte(P,ai) \nAsp : Address . CLSNAME a .{be f ore,a fter} Figure 12. Updates to the prepared heap upon applying advice \nto a join point of the form geteT C.f) a,h,s d.,h\" Look(h\" ,.,m)=(b,.d ) {s . s(this),r . .,this . h\"(A),msg \n. m,cur . .d } if Asp(.d )= A s \" = {this . .,msg . m,cur . .d } if Asp(.d )= Unde fined \"\" ,h\"\" P f \nb,h\" ,s d. \" ,h\"\" P f a.m,h,s d. Figure 13. Slight modi.cation of Select, in order to correctly bind \nstack variables. returns the name of the aspect to which that advice belongs, and returns unde.ned otherwise. \nIn case of advice, this, s and r are bound to the owning aspect, sender and receiver objects respectively, \nwhereas in case of a normal method, this is bound to the receiver while s and r are not available. It \nis important to realize that the fundamentals of Select remain untouched, as the only changes are related \nto a pa\u00adrameter to the recursive invocation of the rewrite function d . Modi.cation of the Look function, \nfor example, would imply that the delegation mechanism is inadequate, in which case our model would be \nas well. Resend semantics need to be adapted in a similar super\u00ad.cial fashion as well, in order to correctly \nupdate this, and to deal with the case where the next object in the delegation chain which also understands \nthe message is not an advice proxy. Indeed, in that situation the special stack variables lose their \nmeaning, and moreover this should no longer be bound to an aspect singleton, but to the original receiver, \nwhich is stored in r. The modi.ed Resend rule is shown in Fig. 14. The only requirement left to verify \nis advice ordering. As explained before, it should be guaranteed that two advice which are de.ned within \nthe same aspect are executed in their order of syntactical appearance. For before advice, this means \ntheir corresponding proxies should appear in the delegation chain in the same order. For after advice, \nthey should appear in reverse order, as the proxy which resends .rst, will execute its advice last. As \ndeploy(proxy,target) inserts a proxy in the delegation chain immediately after the target object, this \nimplies the proxy of the last before advice should be deployed .rst and the proxy of the last after advice \nshould be deployed last. This is asserted in h\" P (16) by .rst deploying before advice proxies in reverse \norder (Ak ...A1 are the before advice) and then deploying after advice proxies in straight order (Ak+1 \n...An are the after advice). set Join Points set join points are handled analogously to get join points, \nexcept that setF should be used instead of getF, and hence the Select variant with parameter support \n(cf. Sec. 2 (6)) is relevant here. call Join Points In the high-level aj language, a call join point \nalways corre\u00adsponds to an execution join point. For that reason, only call is provided in the language. \nHowever, at the implementation level, we have to decide whether to affect the caller object or the receiver \nobject. As our delegation-based machine model supports execution join points in a natural way, the choice \nis simple here. However, in case of languages which do have a need to distinguish between call and execution \njoin points Look(h, Dels(cur),s(msg)) = (b,.d ) {this . s(r),msg . s(msg),cur . .d } if Asp(.d )= Unde \nf ined s \" = s[this . h(A)][cur . .d ] if Asp(.d )= A P f b,h,s \" d.,h\" P f resend,h,s d.,h\" Figure 14. \nModi.ed Resend rule. at the language level, support for call join points should be available as well. \nThis is considered future work. Support for execution join points is analogous to get join points, but \nslightly simpler. More speci.cally, the complica\u00adtion related to the extra getter methods is no longer \nrequired, as methods are stored at class level anyway.  3.4 Context-Oriented Programming in cj cj is \nour own contribution to the j language family. Unlike ij and aj, cj implements the concepts of context-oriented \npro\u00adgramming (COP) [5, 13], and is basically a subset of Con\u00adtextJ [6]. Context-oriented programming \nhelps developers to modularize context-dependent behavior. Behavioral vari\u00adations, or partial de.nitions \nof the underlying programming system, are organized in layers where each layer aggregates a context-dependent \npart of a system s property or concern. Layers can be activated and deactivated at runtime, based on \nthe system s context of use. All context-dependent compo\u00adsitions are scoped such that only dedicated \nsystem parts are affected, and only for a speci.c period of time, such as the dynamic extent of a method \nexecution. cj is de.ned as an extension to j. The syntactic differences are shown in Lst. 4. 1 PROGRAM \n::= < CLASS LAYER >* 2 CLASS ::= class CLSNAME ext CLSNAME { EXTDECL * } 3 LAYER ::= layer CLSNAME { \nDECL * } 4 EXTDECL ::= DECL LAYER 5 METHOD ::= TYPE CLSNAME IDENT ( TYPE x ) { EXP } 6 EXP ::= 7 withlayer \n( CLSNAME ) { EXP } 8 withoutlayer ( CLSNAME ) { EXP } 9 proceed ( EXP ) 10 VAR ::= thisLayer this x \ns Listing 4. cj syntax. Layers can be partially de.ned at top level as well as class level. A top level \nlayer de.nition contains method de.nitions that directly pertain to certain classes, hence the METHOD \nrule is slightly modi.ed to include the name of said class. Class-level layers implicitly affect methods \nof the surround\u00ading class, hence the class name in the METHOD rule is redun\u00addant in this case. However, \nfor simplicity and uniformity rea\u00adsons, it is more convenient to use one unique rule for method de.nitions. \nThe EXP rule has been extended with withlayer and withoutlayer expressions for activating and deactivating \na given layer, while proceed allows for proceeding execution in the next layer or invoking the original \nmethod implemen\u00adtation. While s denotes the caller object just like in aj, an additional thisLayer keyword \nis needed here. In code sur\u00adrounded by withlayer, it can be used to access layer .elds, while this is \nused to access .elds belonging to the class the code occurs in. Note that, for the semantics, we can \neven go as far as to assume a program contains only global layers. This is be\u00adcause a class layer, provided \nits methods are correctly quali\u00ad.ed with the name of the enclosing class, may be moved out of the class \nwhile the program retains equivalent meaning. Similarly, cj normally allows for layers to be composed \nof different fragments. More precisely, it is syntactically pos\u00adsible to have several layer de.nitions \nwith the same name but containing different methods. Semantically, however, that layer is equivalent \nto the union of all fragments, which means the feature is just syntactic sugar for the case where each \nlayer is de.ned only once, as a whole. Therefore, this facet can once more be ignored in semantics speci.cation. \nFor illustration purposes, a cj code sample showing an implementation of the well known observer pattern \n[9] is listed in Lst. 5. 1 class Subj ect { 2 Integer attr 3 4 void Subj ect setAttr(Integer x) { 5 \nthis attr := x 6 } 7 8 void Subj ect changed(Obj ect x) { 9 out println ( this attr ) 10 } 11 } 12 13 \nlay er Observer { 14 bool changed 15 16 void Subj ect setAttr(Integer x) { 17 thisLayer changed := x \nneq(this attr); 18 proceed (x); 19 if( thisLay er changed ) { 20 this changed (null) 21 } else { 22 null \n23 } 24 } 25 } 26 27 class Main { 28 Subj ect s 29 30 void Main main (Main x) { 31 x s := new Subj ect; \n32 x s setAttr (1 2 ); 33 withla y er ( Observer ) { 34 x s setAttr (2 5) 35 } 36 } 37 } Listing 5. cj \ncode sample of the observer pattern. Its execution results in output of the number 25, but not 12. This \nis because the assignment of 12 happens outside a withlayer block, and hence the setAttr method in the \nObserver layer is not executed. Upon assignment of 25, however, the Observer layer is active, hence the \nchanged method is eventually called, resulting in output. We provide a formal semantics for cj as an \nextension of the semantics of j [21]. Syntax Functions For a layer L of the form L = layer C {D1 ...Dn}, \nwe de.ne its identi.er as follows: id : LAYER . CLSNAME id(L)= C while its method de.nitions can be obtained \nthrough getMethods [21], which works as well for a class Z = class C ext C\"{D1 ...Dn}: * getMethods : \nPROGRAM \u00d7 CLSNAME . METHOD getMethods(P,C)= Dj1 ...Djk where P/{C} = O{D1 ...Dn} O .{layer C,class C \next C\"} 1 = j1 < jk = n and .i : ji < ji+1 .i : Dji . METHOD Recall that X/{q} is the domain restriction \nfunction [21, p. 50], which restricts a set of syntactic constructs X to the ones containing an identi.er \nq. Layers is the set of names of all layers in a program: * Layers : PROGRAM . CLSNAME Layers(e)= f Layers(L \n:: P)= id(L) :: Layers(P) Layers(Z :: P)= Layers(P) where P . PROGRAM, L . LAYER and Z . CLASS. Dealing \nWith Layers When calling a method m on a certain object, lookup in\u00advolves more than just investigating \nthe inheritance hierarchy of that object s dynamic type. Indeed, the method imple\u00admentation found using \nthe standard object-oriented lookup strategy may be superseded by an identically named method implementation \nin an active layer. Note that, for simplicity reasons, we do not take the possibility of overloaded meth\u00adods \ninto account. The layerMethod function, given a PROGRAM, a method name, the name of its de.ning class \nand an ordered set of (globally unique) names of active layers, looks for the .rst applicable method \nimplementation in one of these layers, and returns it together with the name of the layer where it was \nencountered, as shown in Fig. 15. Operational Semantics In the same style as j semantics, operational \nsemantics for cj are expressed by means of reduction rules, which manipulate con.gurations consisting \nof a cj expression, a set of active layers, a heap and a stack frame. More precisely, semantics are de.ned \nby means of a function which calculates the value of an expression in the context of a certain program \nand a set of currently active layers. Additionally, the function returns a potentially modi.ed heap, \nwhich models side effects: : PROGRAM . EXP \u00d7 Heap \u00d7 Stack \u00d7P(CLSNAME) . Address \u00d7 Heap The heap stores \nobjects, which means it maps addresses to an array of .eld values, while the stack holds values for this, \nx, s and thisLayer. Layer activation is done by means of the withlayer ex\u00adpression, for which the semantics \nare given below: P f e,h,s,{L1 ...Ln,L} .,h\" P f withlayer(L){e},h,s, {L1 ...Ln} .,h\" It simply appends \nthe speci.ed layer name L to the ordered set of currently active layers {L1 ...Ln}, and evaluates the \nbody of the withlayer expression. Analogously, the withoutlayer expression allows evalu\u00adation of its \nbody while temporarily deactivating a currently active layer: P f e,h,s,L \\{lyr . L |lyr = L} .,h\" P \nf withoutlayer(L){e},h,s,L .,h\" Activating and/or deactivating multiple layers can be achieved by nesting \nthe appropriate expressions. Note that withlayer semantics allows the same layer to be activated multiple \ntimes, in which case withoutlayer semantics deac\u00adtivates them all. thisLayer During execution of a layer \nmethod, it is possible for that method to access the singleton instance of its de.ning layer by means \nof the thisLayer variable. It can be used to access that layer s .elds. In order to model this semantically, \nwe assume a cj program always executes in the context of a pre\u00adpared heap. More speci.cally, as a layer \nclosely resembles a class syntactically, class instantiation semantics (cf. the new expression in Sec. \n3.1 (9)) can be applied to a layer in order to obtain an object stored on the heap which maps the names \nof the layer .elds to values. Moreover, the de.nition of the Heap function is slightly extended in order \nto allow these singleton layer instances to be looked up by name: Heap : (Address . Ob ject) . (CLSNAME \n. Address) layerMethod : PROGRAM \u00d7 Identi fier \u00d7 CLSNAME \u00d7P(CLSNAME) . METHOD \u00d7 CLSNAME layerMethod(P,m,C,e)= \nUnde fined layerMethod(P,m,C,{L1 ...Lk})= (17) . . (M, L1) if L1 . Layers(P) and .!i : getMethods(P,L1)/{m} \n= \" M1 ...Mi ...Mn and Mi = TC.m(Tx){e} . layerMethod(P,m,C,{L2 ...Lk}) otherwise Figure 15. layerMethod \nfunction, which looks for a method matching a given method name in an active layer. The prepared heap \nfor a cj program is therefore de.ned as follows: hc j = {L1 . .1,...,Ln . .n, P .1 . newe(P,L1),...,.n \n. newe(P, Ln)} (18)where .i : Li . Layers(P) .1 ....n are unique Note that this requires layer names \nto be globally unique. Method Call Semantics Determining which layers are currently active, and in which \norder their respective functionalities are to be applied, is ob\u00adviously a very important task during \nmethod lookup. This is incorporated in the reduction rule shown in Fig. 16, express\u00ading method call semantics. \nAs this rule is fundamental in order to understand cj semantics, we will explain it in more detail. The \n.rst line applies the semantics function recursively in order to obtain the address of the target object \nfor the call. Note that L is the set of currently active layers, while e0 can be any valid EXP, as far \nas its evaluation results in an address. In a similar way, the second line calculates the value of the \nactual parameter e1. Note that the calculations so far might have caused side effects on the heap (e. \ng., if a subexpression of e0 or e1 involves .eld assignment), which is re.ected by returning differently \nnamed heap variables. The third line performs traditional object-oriented lookup, .nding the method implementation \nto be executed in a con\u00adtext where no layers are active. The Me function is a slight t variation of Me \n[21] which returns, apart from the relevant method implementation, also the name of the class where it \nwas found. It is shown in Fig. 17. The type and super functions are as in [21] and respec\u00adtively determine \nthe name of the dynamic type of an object and name of the super class of a class. The fourth line of \n(19) uses the layerMethod function from Sec. 3.4 in order to determine whether there is a method implementation \nin a layer superseding the one found by Mt e . If there is no such method, M2 will be unde.ned. The next \nline invokes another auxiliary function called actualMethod to select which method should ultimately \nbe executed. Basically, this is M2, unless that was unde.ned, in which case M1 is selected: actualMethod \n: METHOD \u00d7 METHOD . METHOD M1 if M2 = Unde f ined actualMethod(M1,M2)= M2 otherwise In case M1 is unde.ned \nas well, it is still selected, but execu\u00adtion will never happen since the result of actualMethod will \n\" not have the required form TC.m(Tx){e}. The last two lines, .nally, are responsible for executing the \nbody of the selected method. First, the appropriate stack variables are bound, including msg, which stores \nthe current message in order to deal with proceed semantics later on. thisLayer and s, on the other hand, \nonly have meaning in layer methods. Ultimately, execution takes place by recursively apply\u00ading the semantics \nfunction, passing the newly created stack frame. The result of this execution is returned as the result \nvalue of the complete reduction rule. Proceed Semantics If method lookup results in selecting a layer \nmethod for execution, the proceed expression may appear in its body, which should result in executing \nthe next applicable layer method or (if there are none left) the method implementation selected by standard \nobject-oriented lookup (M1 in (19)). A different argument may be passed along. Its semantics are speci.ed \nby the reduction rule in Fig. 18. It looks very similar to the method call reduction rule, except that \nthe set of active layers is not considered in its entirety, but merely those elements which appear after \nthisLayer, which denotes the method currently being executed. Further\u00admore, note that this, s and msg \nremain unchanged, while x and thisLayer are updated appropriately. Mapping to the Machine Model The main \ndifference between j and cj lies in the latter s sup\u00adport for layers, which can be dynamically activated \nand de\u00ad P f e0, h,s,L ., h\" P f e1,h\" ,s,L .\" ,h\"\" Me t (P,type(h\"\" ,.),m) = (M1,C) layerMethod(P,m,C,L \n) = (M2,L) s \" = actualMethod(M1,M2) = T C.m(T \" x){e}{this . .,msg . m,x . .\" }{s . s(this),thisLayer \n. h\"\"(L),this . .,msg . m,x . .\" }P f e,h\"\" ,s \" , L .\"\" ,h\"\"\" i f M2 = Unde f ined otherwise (19) P \nf e0.m(e1),h,s,L .\"\" ,h\"\"\" Figure 16. Reduction rule expressing method call semantics.  Me t : PROGRAM \n\u00d7 CLSNAME \u00d7 Identi fier . METHOD \u00d7 CLSNAME Mt e(P,C,m)= . . . . . Undefined if C = Object (M,C) i f \ngetMethods(P,C)/{m} = M i f getMethods(P,C)/{m} = e Mt e(P,super(P,C),m) Undefined otherwise Figure 17. \nMe function. t P f e0,h,s,{L1 ...Ln} .,h\" .i : type(h\" ,s(thisLayer)) = Li Mt e(P,type(h\" , s(this)),s(msg)) \n= (M1,C) layerMethod(P,s(msg),C,{Li+1 ...Ln})=(M2,L) \" actualMethod(M1,M2)= TC.m(Tx){e} (20) {this \n. s(this),msg . s(msg),x . .} if M2 = Unde f ined \" s = s[thisLayer . h\"(L)][x . .] otherwise P f e,h\" \n,s \" ,{L1 ...Ln} .\" ,h\"\" P f proceed(e0), h,s,{L1 ...Ln} .\" ,h\"\" Figure 18. Reduction rule expressing \nproceed semantics. activated. Actually, code outside a withlayer block is exe\u00adcuted strictly according \nto the j semantics. withlayer, how\u00adever, activates a certain layer, causing the methods speci.ed in that \nlayer to shadow the corresponding class methods. cj semantics handle this by utilizing a number of helper \nfunc\u00adtions analyzing the syntax and determining the applicable method, based on an ordered set of active \nlayers. Our machine model, on the other hand, once more takes advantage of the built-in delegation principle \nin order to es\u00adtablish the desired behavior. This requires a correct arrange\u00adment of delegation chains \non the heap upon layer activation and deactivation. At program start, no layers are active yet, so the \nheap should initially be prepared in the same way as (10) for j in Sec. 3.1. However, similarly to aspects \nin Sec. 3.3, layers support .elds that may be accessed by layer methods. Hence, a singleton object for \neach layer is stored on the prepared heap as well, as shown in (18). In summary, the required prepared \nheap is intuitively the union of (10) and (18). We assume there to be no overlaps of their respective \naddress spaces. Upon activation of a certain layer via withlayer,a layer proxy is created for each layer \nmethod. The proxy contains that method s implementation, and is inserted in the delega\u00adtion chain of \nthe targeted class. An example of the resulting situation is displayed in Fig. 19. After executing the \nlast in\u00adstruction of the withlayer block, layer proxies are removed again. Hence, the delegation-based \nsemantics of withlayer are as shown in Fig. 20, where Lyr : Address . CLSNAME is a function which is \nstored on the heap, and keeps track of which layer the proxies are associated with. This is neces\u00adsary \nin order to be able to deactivate a certain layer, which should happen as a result of the withoutlayer \nexpression, which may appear in a withlayer block. Its semantics are getMethods(P,L)= M1 ...Mn \" .i \n: Mi = Ti Ci.mi(Ti x){ei}.k . [1,n] : .lp,k ./dom(h) h\" = h[.lp,1 . [m1: e1]][Del.lp,1 . Delh(C1)][Delh(C1) \n. .lp,1] \u00b7\u00b7\u00b7 [.lp,n . [mn : en]][Del.lp,n . Delh(Cn)][Delh(Cn) . .lp,n] [Lyr(.lp,1) . L]...[Lyr(.lp,n) \n. L] d., h\"\" P f e,h\" ,s h\"\"\" = h\"\"[Delh(C1) . Del.lp,1 ]...[Delh(Cn) . Del.lp,n ] P f withlayer(L){e},h,s \nd., h\"\"\" Figure 19. A layer which shadows method m1 in Class1 and m5 in Class2 has been activated. Class3 \nis not affected. expressed by the following reduction rule: {.1,...,.n} = {.\". dom(h)|Lyr(.\")= L} .i \n: .!.Ci : Del.Ci = .i h\" = h[Del.C1 . Del.1 ]...[Del.Cn . Del.n ] d., h\"\" P f e,h\" ,s h\"\"\" = h\"\"[Del.C1 \n. .1]...[Del.Cn . .n] P f withoutlayer(L){e},h,s d., h\"\"\" Essentially, all proxies belonging to the speci.ed \nlayer are temporarily removed from the delegation chain of the class they apply to, and reinserted after \nexecution of the withoutlayer block is .nished. By construction, the existing Select semantics (cf. Sec. \n2 (6)) once more suf.ce to select the correct method imple\u00admentation: If a method is shadowed by an active \nlayer, the corresponding proxy will be in place, and it will intercept the message before it reaches \nits target. Nevertheless, sim\u00adilarly to Sec. 3.3, Select semantics do need a slight adapta\u00adtion in order \nto correctly bind the stack variables s, which denotes the caller object, and thisLayer, which may be \nused in a layer method to access layer .elds. The modi.ed Select rule is shown in Fig. 21. Note that, \nunlike before and after advice in Sec. 3.3, messages are not resent by default. In concordance with context-oriented \nprogramming principles, they completely shadow the corresponding class method, unless the layer method \nexplicitly includes the proceed expression in its body. cj semantics for proceed, as de.ned by (20), \nuse the thisLayer stack variable to determine which layer the current layer method pertains to. Next, \nan attempt is made to .nd a layer that also contains an applicable layer method, and that was activated \nlater than the current layer. If successful, said method is executed. If not, the originally shadowed \nmethod is executed. In the delegation-based model, however, this is, by con\u00adstruction, exactly the order \nin which the Resend rule (cf. Sec. 2 (7)) executes methods. Hence, proceed simply maps to resend: P f \nresend(e),h,s d.,h\" P f proceed(e),h,s d.,h\" where the Resend rule has been slightly modi.ed in order \nto correctly update thisLayer, as shown in Fig. 22. Note that, although the aj language does not support \naround advice (cf. Sec. 3.3), it could be added straightfor\u00adwardly using a similar proceed instruction, \nwith essentially identical semantics. 4. Related Work A substantial amount of other work deals with the \nformal\u00adization of aspect-oriented programming. Aspectual CAML [16] is an aspect-oriented extension to \nOCAML, with join points for typical functional features such as curried func\u00adtion calls and variant construction, \nas well as support for inter-type declarations. A compiler translates an Aspectual CAML program into \nan OCAML one, where function bod\u00adies are modi.ed to call advice. This is similar to the original ij semantics \nby Skipper [21], where an ij program is trans\u00adlated into a j program. The Aspect Sand Box [15] is an \nexperimental workbench for different styles of aspect-oriented programming, cen\u00adtered on a small object-oriented \nbase language, called BASE, e0, h,s d .,h\" e1,h\" ,s d .\" ,h\"\" Look(h\"\" ,.,m) = (b,.d ) s \" = {s . s(this),thisLayer \n. h\"\"(L),this . .,msg . m,x . .\" ,cur . .d }{this . .,msg . m,x . .\" ,cur . .d }P f b,h\"\" ,s \" d .\"\" \n,h\"\"\" i f Lyr(.d ) = L i f Lyr(.d ) = Unde f ined P f e0.m(e1),h,s d .\"\" ,h\"\"\" Figure 21. Modi.ed Select \nrule, ensuring correct binding of stack variables.  e,h,s d.,h\" Look(h\" ,Dels(cur),s(msg)) = (b,.d ) \ns[thisLayer . h\"(L)][x . .][cur . .d ] i f Lyr(.d )= L \" s = s[x . .][cur . .d ] otherwise \" ,h\"\" P \nf b,h\" ,s \" d. \" ,h\"\" P f resend(e),h, s d. Figure 22. Modi.ed Resend rule. which is extended with aspect-oriented \nfeatures. Wand et al. [24] consider a procedural subset of BASE in order to inves\u00adtigate the formal semantics \nof a number of aspect-oriented features, including dynamic join points, pointcut designators and before, \nafter and around advice. A monadic semantics is used, which explicitly models a weaving approach: At \neach procedure call, the weaver is invoked, taking a list of ad\u00advice and a join point. The weaver determines \nwhich advice are applicable, and results in a new procedure, wrapping the original procedure in all of \nthe applicable advice. The Common Aspect Semantics Base [8] takes a similar approach, but applies a two-stage \nfunction at every join point, .rst checking whether the current instruction should be advised, and then \nverifying whether the present dynamic state calls for advice application. Clifton and Leavens [4] introduce \nMiniMAO0, a small object-oriented language for which an operational seman\u00adtics is provided. The aspect-oriented \nMiniMAO1 extension mimicks AspectJ s semantics of around advice on call and execution join points. Advice \ninvocation occurs in several steps, essentially modeling a weaver: join points are created explicitly \nfor each call, a list of applicable advice is searched for and .nally evaluated. Interestingly, the authors \nstate that they regard advice binding to be a primitive operation, simi\u00adlar to the object-oriented virtual \ndispatch mechanism. In our delegation-based semantics, however, advice invocation ac\u00adtually becomes part \nof virtual dispatch, every message send being a join point implicitly. In [1], Aldrich introduces TinyAspect, \na language with pointcuts and around advice. In the formal semantics, func\u00adtion calls are replaced by \ncalls to applicable advice, where occurences of proceed are replaced by calls to the original function. \nThese explicit weaving approaches, whether they stati\u00adcally generate an aspect-free program, or dynamically \nin\u00advoke advice lookup strategies, have the disadvantage that aspect-oriented features are implemented \nas add-ons to a base which cannot deal with them directly. In fact, a weav\u00ading approach is encountered \nas well in the semantics of aj as de.ned originally by Skipper [21], as can be derived from Fig. 10 in \nSec. 3.3, which also performs explicit ad\u00advice lookup. Our work, however, shows that such helper con\u00adstructs \ncan be avoided by mapping mechanisms such as ad\u00advice lookup onto a machine model, which solely relies \non delegation and message sending, and reuses the existing dis\u00adpatch strategy. Call join points, however, \nare currently not supported by our model. Walker et al. [23] also use an intermediate model with inherent \nsupport for aspects. More speci.cally, their aspect\u00adoriented core calculus is an extension of lambda \ncalculus, which adds labeled join points and advice at those join points. Advice invocation is handled \nonce more explicitly, as a list of advice is checked at each named join point and a composed function \nof all applicable advice is created and called. A language MinAML, which distinguishes between before, \nafter and around advice, is then introduced, in or\u00adder to show that these features can be translated \nto the core calculus. To this end, labeled join points are explicitly intro\u00adduced at function entry and \nexit, where before advice applies to the former, and after advice to the latter. around advice is handled \nby means of a goto mechanism which allows jump\u00ading directly to a labeled join point. Ultimately, all \nthis means that the explicit advice invocation mechanism is still present in the MinAML semantics. In \n[7], the AspectML language is translated to the core calculus in a similar way. In contrast to our work, \nthe core calculus of Walker et al. does include a type system, which is considered future work here. \nAd\u00additionally, complex pointcut speci.cations, including c.ow, are supported, while we did not yet cover \na language exhibit\u00ading those features, although our machine model itself has the ability to handle c.ow \n[12]. L\u00e4mmel [14] observes that method call interception (MCI), which can be regarded as a more explicit \nform of delegation, is a powerful construct which can capture several aspect-oriented features. However, \nhe proposes to introduce this as a language construct, at the same level as (virtual) method invocation. \nThis is different from our work, where delegation is part of the machine model, but not necessarily of \nhigh-level aspect-oriented languages which are translated to it. Composition .lters [3] is another approach \nwhich ex\u00adploits the language level, rather than the machine level. Here, outgoing as well as incoming \nmessages pass through a num\u00adber of .lters, which may for example reroute dispatch. Del\u00adegation is mentioned \nas a possible application. In summary, our approach is unique in the sense that se\u00admantics of aspect-oriented \nfeatures are expressed in terms of a machine model, which seamlessly incorporates aspect\u00adorientation \nas opposed to explicitly modeling advice invo\u00adcation. While this is clearly bene.cial to understandability, \nanother advantage of our approach is in the implementation of an aspect-oriented virtual machine, which \nmay be sim\u00adpli.ed as well as explicit helper mechanisms such as advice lookup can be avoided. Implementation-wise, \na number of projects are concerned with virtual machine support for aspect-orientation [19, 20, 11, 10, \n22]. Among these, PROSE 2 [20] is the only one to adopt, like our machine model, a view on the running \nappli\u00adcation that explicitly regards all points in the execution as potential join points by default. \nThis is realized by instru\u00admenting the implementation to call an AOP infrastructure at each potential \njoin point, and having the infrastructure explicitly check for the applicability of advice. Our model, \nconversely, is based on implicitly executing advice by means of message interception and dynamically \nupdating delega\u00adtion chains. 5. Summary and Future Work We have presented semantic mappings from four \nhigh-level programming languages, j, ij, aj and cj to our previously in\u00adtroduced machine model. Additionally, \nwe explained infor\u00admally that our model correctly supports the various mecha\u00adnisms which are included \nin these languages in order to ad\u00address crosscutting concerns. Hence, the model is shown to be suf.ciently \nexpressive to meet the requirements of differ\u00adent languages with different approaches to modularization. \nMoreover, as the semantics used in this paper are opera\u00adtional, the mappings suggest how an implementation \nof these languages based on our model can be derived. A prototypi\u00adcal implementation of all four languages \non top of an imple\u00admentation of the model has indeed been realized as well. The paper has introduced \na formal semantics for cj, a language supporting context-oriented programming, and essentially a subset \nof ContextJ. There are several areas of future work. The semantic map\u00adpings presented here need to be \nextended with features such as parameter overloading and type soundness. To that end, the formal semantics \nof our model needs to be extended with type information. Additionally, a formal proof will be pro\u00advided \nfor the claim that the semantic mappings are behav\u00adior preserving. A proof-of-concept implementation \nof our model is already in place, and the languages for which se\u00admantics have been presented above have \nbeen implemented using this implementation. The focus in devising these lan\u00adguage implementations has \nbeen on faithfully adopting their semantics instead of providing high performance. Whereas this work \nshows that the language mappings can be done correctly, we intend to demonstrate that this can be done \nin an ef.cient manner as well. Dedicated caching mechanisms to deal with message lookup, and garbage \ncollection strate\u00adgies to deal with the large amount of small (proxy) objects, are a worthwhile path \nto investigate. The model itself is be\u00ading investigated further, especially regarding the support of \nadditional constructs such as call join points. References [1] Jonathan Aldrich. Open Modules: A Proposal \nfor Modular Reasoning in Aspect-oriented Programming. In Foundations of Aspect Languages, 2004. [2] Christopher \nAnderson and Sophia Drossopoulou. d An Imperative Object-based Calculus with Delegation. In Proc. USE \n02, Malaga, 2002. [3] Lodewijk Bergmans and Mehmet Aksit. Composing Crosscutting Concerns Using Composition \nFilters. Commun. ACM, 44(10):51 57, 2001. [4] Curtis Clifton and Gary T. Leavens. MiniMAO1 An Imperative \nCore Language for Studying Aspect-oriented Reasonings. Sci. Comput. Program., 63(3):321 374, 2006. [5] \nPascal Costanza and Robert Hirschfeld. Language Con\u00adstructs for Context-oriented Programming: An Overview \nof ContextL. In Dynamic Languages Symposium (DLS) 05, co-organised with OOPSLA 05. ACM Press, 2005. [6] \nPascal Costanza, Robert Hirschfeld, and Wolfgang De Meuter. Ef.cient Layer Activation for Switching Context\u00addependent \nBehavior. In JMLC, pages 84 103, 2006. [7] Daniel S. Dantas, David Walker, Geoffrey Washburn, and Stephanie \nWeirich. AspectML: A polymorphic aspect\u00adoriented functional programming language. ACM Trans. Program. \nLang. Syst., 30(3):1 60, 2008. [8] Simplice Djoko Djoko, Remi Douence, Pascal Fradet, and Didier Le Botlan. \nCASB: Common Aspect Semantics Base. Technical Report AOSD-Europe Deliverable D41, AOSD-Europe-INRIA-7, \nINRIA, France, 10 February 2006. [9] Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides. Design \nPatterns Elements of Reusable Object\u00adoriented Software. Addison-Wesley, 1994. [10] Michael Haupt. Virtual \nMachine Support for Aspect-oriented Programming Languages. PhD thesis, Software Technology Group, Darmstadt \nUniversity of Technology, 2006. [11] Michael Haupt, Mira Mezini, Christoph Bockisch, Tom Dinkelaker, \nMichael Eichberg, and Michael Krebs. An Ex\u00adecution Layer for Aspect-oriented Programming Languages. In \nProc. VEE 2005. ACM Press, June 2005. [12] Michael Haupt and Hans Schippers. A Machine Model for Aspect-oriented \nProgramming. In ECOOP 2007 -Object\u00adoriented Programming, 21st European Conference, Berlin, Germany, July \n30 -August 3, 2007, Proceedings, volume 4609 of Lecture Notes in Computer Science, pages 501 524. Springer, \n2007. [13] Robert Hirschfeld, Pascal Costanza, and Oscar Nierstrasz. Context-oriented Programming. Journal \nof Object Technol\u00adogy (JOT), 7(3):125 151, March-April 2008. [14] Ralf L\u00e4mmel. A Semantical Approach \nto Method-call Interception. In Proc. AOSD 02, pages 41 55. ACM Press, 2002. [15] Hidehiko Masuhara and \nGregor Kiczales. Modeling Cross\u00adcutting in Aspect-oriented Mechanisms. In Luca Cardelli, editor, ECOOP, \nvolume 2743 of Lecture Notes in Computer Science, pages 2 28. Springer, 2003. [16] Hidehiko Masuhara, \nHideaki Tatsuzawa, and Akinori Yonezawa. Aspectual Caml An Aspect-oriented Func\u00adtional Language. In \nICFP 05: Proceedings of the tenth ACM SIGPLAN international conference on Functional program\u00adming, pages \n320 330, New York, NY, USA, 2005. ACM. [17] Todd D. Millstein and Craig Chambers. Modular Statically \nTyped Multimethods. In ECOOP 99: Proceedings of the 13th European Conference on Object-oriented Programming, \npages 279 303, London, UK, 1999. Springer-Verlag. [18] Harold Ossher. A Direction for Research on Virtual \nMachine Support for Concern Composition. In Proc. Workshop VMIL 07. ACM Press, 2007. [19] Andrei Popovici, \nThomas Gross, and Gustavo Alonso. Dynamic Weaving for Aspect-oriented Programming. In Gregor Kiczales, \neditor, Proc. AOSD 2002. ACM Press, 2002. [20] Andrei Popovici, Thomas Gross, and Gustavo Alonso. Just\u00adin-Time \nAspects. In Proc. AOSD 2003. ACM Press, 2003. [21] Mark C. Skipper. Formal Models for Aspect-oriented \nSoftware Development. PhD thesis, Imperial College, London, 2004. [22] Eric Tanter and Jacques Noy\u00e9. \nA Versatile Kernel for Multi-Language AOP. In Proc. GPCE 05. Springer, 2005. [23] David Walker, Steve \nZdancewic, and Jay Ligatti. A Theory of Aspects. In ICFP 03: Proceedings of the eighth ACM SIGPLAN international \nconference on Functional programming, pages 127 139, New York, NY, USA, 2003. ACM. [24] Mitchell Wand, \nGregor Kiczales, and Christopher Dutchyn. A Semantics for Advice and Dynamic Join Points in Aspect\u00adoriented \nProgramming. ACM Trans. Program. Lang. Syst., 26(5):890 910, 2004.  \n\t\t\t", "proc_id": "1449764", "abstract": "<p>We describe semantic mappings of four high-level programming languages to our delegation-based machine model for aspect-oriented programming. One of the languages is a class-based object-oriented one. The other three represent extensions thereof that support various approaches to modularizing crosscutting concerns. We explain informally that an operational semantics expressed in terms of the model's concepts preserves the behavior of a program written in one of the high-level languages. We hence argue our model to be semantically sound in that sense, as well as sufficiently expressive in order to correctly support features such as class-based object-oriented programming, the open-classes and pointcut-and-advice flavors of aspect-oriented programming, and dynamic layers. For the latter, being a core feature of context-oriented programming, we also provide a formal semantics.</p>", "authors": [{"name": "Hans Schippers", "author_profile_id": "81367592776", "affiliation": "University of Antwerp, Antwerp, Belgium", "person_id": "P1223237", "email_address": "", "orcid_id": ""}, {"name": "Dirk Janssens", "author_profile_id": "81100205341", "affiliation": "University of Antwerp, Antwerp, Belgium", "person_id": "P1223238", "email_address": "", "orcid_id": ""}, {"name": "Michael Haupt", "author_profile_id": "81100288845", "affiliation": "Hasso-Plattner-Institut, University of Potsdam, Potsdam, Germany", "person_id": "P1223239", "email_address": "", "orcid_id": ""}, {"name": "Robert Hirschfeld", "author_profile_id": "81329489343", "affiliation": "Hasso-Plattner-Institut, University of Potsdam, Potsdam, Germany", "person_id": "P1223240", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1449764.1449806", "year": "2008", "article_id": "1449806", "conference": "OOPSLA", "title": "Delegation-based semantics for modularizing crosscutting concerns", "url": "http://dl.acm.org/citation.cfm?id=1449806"}