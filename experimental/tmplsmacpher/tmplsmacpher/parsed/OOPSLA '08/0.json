{"article_publication_date": "10-19-2008", "fulltext": "\n Caching and Incrementalisation in the Java Query Language Darren Willis, David J. Pearce and James Noble \nComputer Science, Victoria University of Wellington, NZ {darren,djp,kjx}@mcs.vuw.ac.nz Abstract Many \ncontemporary object-oriented programming languages support .rst-class queries or comprehensions. These \nlan\u00adguage extensions make it easier for programmers to write queries, but are generally implemented no \nmore ef.ciently than the code using collections, iterators, and loops that they replace. Crucially, whenever \na query is re-executed, it is recomputed from scratch. We describe a general ap\u00adproach to optimising \nqueries over mutable objects: query results are cached, and those caches are incrementally main\u00adtained \nwhenever the collections and objects underlying those queries are updated. We hope that the performance \nbene.ts of our optimisations may encourage more general adoption of .rst-class queries by object-oriented \nprogrammers. Categories and Subject Descriptors D.3.3 [Programming Languages]: Language Constructs and \nFeatures; D.1.5 [Programming Techniques]: Objected-Oriented Program\u00adming General Terms Languages, Performance, \nDesign Keywords Querying, Incrementalization, Java 1. Introduction First-class constructs for querying \nlists, sets, collections, and databases are making their way from research systems into practical object-oriented \nlanguages. Language-level queries have their roots in the set comprehensions and list compre\u00adhensions \noriginating in the 1960s in SETL [36], and named in the 1970s in NPL [5]. Meanwhile, object-oriented \nlan\u00adguages have made do with external iterators (in Java/C++) and internal iterators (Smalltalk) to query \ncollection li\u00adbraries [7]. More recent languages such as Python, LINQ for Cq [28] and C. [2] have included \nqueries as .rst-class language constructs. Permission to make digital or hard copies of all or part of \nthis work for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. OOPSLA 08, October 19 23, 2008, Nashville, Tennessee, USA. Copyright \nc &#38;#169; 2008 ACM 978-1-60558-215-3/08/10. . . $5.00 Explicit, .rst-class query constructs have several \nadvan\u00adtages over expressing queries implicitly using collection APIs and external/internal iterators. \nExplicit queries can be more compact and readable than open-coding queries using other APIs. Queries \n(especially in LINQ) can be made poly\u00admorphic across different collection implementations, using the \nsame query syntax for XML and relational data as well as objects. By making programmers intentions to \nquery collections explicit in their code, .rst-class query constructs enable numerous automatic optimisations, \noften drawing on database techniques [41]. The problem addressed in this paper is that existing language-based \nquery constructs don t really take mutable state into account. List and set comprehensions were intro\u00adduced \nin functional languages, where neither the collections being queried nor the data contained in those \ncollections were mutable; thus, the questions of how queries should best perform when their underlying \ndata was updated does not arise. Relational databases support updates to their un\u00adderlying tables, but \ndon t support object identity and gener\u00adally assume relatively few updates to small parts of large, external \n.le structures. In contrast, collections of objects are routinely mutated in object-oriented programming \nlan\u00adguages: objects are added to and removed from collections between queries as part of the general \nrun of a program. Objects themselves are also mutable: an object s .elds can be assigned to new values \nby any code with permission to write to those .elds. Changing the collections underlying a query, or \naltering the objects within those collections can certainly change the set of objects that result from \nthe query. This means that exactly the same query code, executed at two different instants during a program \ns run, may produce very different results. In this paper, we present a general technique for caching \nand incrementalising queries in object-oriented languages in the presence of mutable state. In previous \nwork [41] we have presented JQL, the Java Query Language, which pro\u00advides explicit queries for Java and \nuses a range of techniques primarily join ordering to optimise queries. Unfor\u00adtunately, the previous \nversion of JQL has no memory for the queries it calculates: every query is re-evaluated from scratch. \nIn this paper we detail an extension to JQL that optimises results across multiple queries in two important \nways. First, we cache query results: if the same query is resubmitted, the cached result is reused. Second, \nwe incre\u00admentally update these caches to take account of changes in objects and collections between queries. \nThis is crucial be\u00adcause Java is an imperative, object-oriented language: the objects and collections \nused in queries can be updated by the rest of the program in between query executions. In\u00adcremental updating \nmeans that JQL can rely on its caches, and can avoid re-executing queries from scratch, even when their \nunderlying objects have been updated. Of course, such caching costs time (the incremental updates) and \nspace (to store): so our extended caching JQL includes mechanisms to decide when and where to cache queries. \n  This paper makes the following contributions: We present a general approach to optimising repeated \n.rst-class queries in object-oriented programs, by caching results and updating the caches as the programs \nrun.  We detail an implementation of our approach for the Java Query Language, using AspectJ to maintain \nand incrementalise cached query results.  We present experimental data demonstrating that our im\u00adplementation \nperforms ef.ciently.  We describe an inspection study of the use of loops in programs which suggests \nthat many loops could be con\u00adverted to queries and would then bene.t from our ap\u00adproach.  While our \ndesign and implementation are based on the Java Query Language (JQL), we expect our techniques will apply \nto queries and set comprehensions in any program\u00adming language. 2. Queries, Caching and Incrementalisation \nTo illustrate our approach, we present an example based on a real-world application called Robocode [31]. \nThis simple Java game pits user-created, simulated robots against each other in a 2D arena. The game \nhas a serious side as it has been used to develop and teach ideas from Arti.cial Intel\u00adligence [13, 16, \n32]. A Robocode Battle object maintains a private list of robots, with an accessor method that returns \nall robots in the battle: class Battle { private List<Robot> robots; ... public List<Robot> getRobots() \n{ return robots; } } During each turn of the game, robots scan their .eld-of-view within the battle \narena to locate other Robots which they can attack: class Robot { public int state = STATE ACTIVE; public \nboolean isDead() {return state == STATE DEAD;} public void die() { state = STATE DEAD; } ... private \nvoid scan() { // Scan .eld-of-view to .nd robots for(Robot r : battle.getRobots()) {if(r!=null &#38;&#38; \nr!=this &#38;&#38; !r.isDead() &#38;&#38; r.intersects(...)){.... }}}} In JQL (and similarly for other \nlanguages with .rst-class queries, such as LINQ), we would rewrite the above into a for-loop query: for(Robot \nr : battle.getRobots() | r!=null &#38;&#38; r!=this &#38;&#38; !r.isDead() &#38;&#38; r.intersects(...)) \n{ ... } This extended for-loop statement executes its body for each element matching the given conditions \n(i.e. those after the | ) in the collection returned by battle.getRobots().This sub-collection is called \nthe query s result set. No order of it\u00aderation is implied by the for-loop query; furthermore, while side-effecting \nstatements can (currently) be used within query conditions, their use has unde.ned behaviour. In par\u00adticular, \nthe programmer is responsible for ensuring method calls used inside query conditions are free from side-effects. \nThe advantage of the for-loop query is that the conditions are made explicit and, hence, it is more declarative \nin nature. Our previous work with JQL demonstrated how database op\u00adtimisations can be applied to such \nqueries to give greatly im\u00adproved performance, especially for those involving multiple collections [41]. \nWriting a JQL or LINQ query, or a set or list comprehension, is also generally simpler than writing equiv\u00adalent \ncode using explicit loops and branches this is why languages are now adopting constructs to support \nqueries and comprehensions directly. The existing JQL optimisation techniques detailed in [41] only go \nso far with methods like scan(),however. The main problem with scan() is that it is called in the inner \nloop of Robocode, run once for every Robot at every simulation step. While we can optimise each individual \nquery, existing language-based query systems like JQL or LINQ treat every query execution as a separate \nevent: they do not use the pre\u00advious execution of a query to assist in subsequent executions of the query. \n 2.1 Query Caching To optimise programs like Robocode, programmers focus on methods like scan() that \nare called repeatedly. A com\u00admon and effective approach is to cache intermediate results which, in this \ncase, are the sub-collection(s) being frequently traversed. For example, the programmer might know that, \non average, there are a large number of dead robots. To avoid repetitively and needlessly iterating over \nmany dead robots in scan(), the programmer might maintain a cache a list of just the alive robots as \nfollows: class Battle { private List<Robot> robots; // master list of all robots private List<Robot> \naliveRobots; // cached list of alive robots ... public List<Robot> getRobots() { return robots; } public \nList<Robot> getAliveRobots() { return aliveRobots; }} Then, each robot can scan the list of alive robots, \nwithout needing to check whether each is alive or dead: class Robot { ... private void scan() { for(Robot \nr : battle.getAliveRobots()) { if(r!=null &#38;&#38; r!=this &#38;&#38; r.intersects(...)) { ... } }}} \n Here, aliveRobots is a sub-collection of robots containing only those where !isDead() holds. Thus, the \nfor-loop in scan() no longer needlessly iterates over dead robots. Since (after the game has been running \na while) more robots are typically dead than alive, this reduces the time taken for the loop at the cost \nof extra memory (the cache). Explicitly maintaining extra collections has several draw\u00adbacks. Caches \ncan be dif.cult to introduce when the inter\u00adface of the providing object (i.e. Battle) is .xed (e.g. \nit s part of a third-party library, and/or the source is not avail\u00adable, etc). Furthermore, the optimisation \nreduces readability and maintainability as the source becomes more cluttered. Maintaining cached collections \nis also rather tedious, since they need to be updated whenever the underlying collection or the objects \nin those collections are updated whenever a new robot spawns into the game, or whenever an alive robot \ndies. Finally, code to maintain these optimised collec\u00adtions must be written anew for each collection. \nFor example, Robocode s Battle class also maintains a list of Bullets and employs a loop similar to scan() \nfor collision detection. Pro\u00adgrammers can introduce a sub-collection to cache live bul\u00adlets, but only \nby duplicating much of the code necessary for the sub-collection of live robots. To address these issues, \nwe have developed an extension to JQL that automatically caches the result set of a query when this is \nbene.cial. Speci.cally, our system caches a query s result set so it can be quickly recalled when that \nquery is evaluated again. Our system employs heuristics to decide when it is bene.cial to cache a query \ns result set. Thus, the advantages of caching important sub-collections are obtained without the disadvantages \nof performing such optimisations by hand. The pragmatic effect is that program\u00admers can write code using \nqueries directly over underlying collections, but the program performs as if specialised sub\u00adcollections \nwere hand-coded to store just the information re\u00adquired for particular queries. 2.2 Cache Incrementalisation \nWhen the source collection(s) of a query are updated (e.g. by adding or removing elements), or an element \nof a source col\u00adlection is itself updated, any cached result sets may become invalidated. Traditionally, \nencapsulation is used to prevent this situation from arising, by requiring all updates are made via a \ncontrolled interface. Thus, updates to a collection can be intercepted to ensure any cached result sets \nare updated appropriately. To illustrate, consider a simple addRobot() method for adding a new robot \nto the arena, where a cache is being maintained explicitly: class Battle { private List<Robot> robots, \naliveRobots; ... public List<Robot> getRobots() { return robots; } public List<Robot> getAliveRobots() \n{ return aliveRobots; } public void addRobot(Robot r) { robots.add(r); if(!r.isDead()) { aliveRobots.add(r); \n} } public void robotDied(Robot r) { aliveRobots.remove(r); }} Here we see that when a robot is added \nvia addRobot(),the aliveRobots list is incrementally updated to ensure it remains consistent with the \nrobots collection. Likewise, when a robot dies, robotDied() is called to ensure aliveRobots is updated \naccordingly. To deal with object updates that may invalidate a cached result set for some query, our \nsystem incrementally updates that result set. To do this, we intercept operations which may alter the \nresult set of a given query that is, where evaluating the same query again, with the same input collec\u00adtions, \nwould yield different results. There are essentially two kinds of operations in programs we must consider: \nthose that add or remove objects to the underlying collections (like the addRobot() method) or those \nthat change the state of objects in those collections (e.g. the die() method on class Robot,or any other \nassignments to the dead .eld). An important issue in this respect is the query/update ratio for a particular \nquery. This arises because there is a cost associated with incrementally maintaining a cached result \nset: when the number of updates affecting a result set is high, compared with how often it is actually \nused, it becomes uneconomical to cache that result set. To deal with this, our system monitors the query/update \nratio and dynamically determines when to begin, and when to stop caching the result set. Queries that \nare not repeated, or that occur infrequently, are not cached; neither are queries where the underlying \ndata changes often between queries. Where there is only a relatively small change to the data between \neach query over that data, then our system can effectively cache and optimise those queries. 2.3 Discussion \nWe believe our approach frees the programmer from tedious and repetitive optimisations; from the burden \nof working around .xed interfaces; from the possibility of bugs caused by out-of-synch result sets (e.g. \nif someone forgets to call robotDied() above); and, .nally, that it opens up the door for more sophisticated \noptimisation strategies. For example, we couldemploya cache replacement policy that saves memory by discarding \ninfrequently used result sets. One might argue, however, that our approach will further limit the ability \nof programmers to make important per\u00adformance tweaks . Such arguments have been made before about similar \nlosses of control (e.g. assembly versus high\u00adlevel languages, garbage collection, etc); and yet, in the \nlong run, these have proven successful. Furthermore, JQL must retain the original looping constructs \nof Java and, hence, the programmer may retake control if absolutely necessary, by writing loops and .lters \nexplicitly. The central tenet of this paper then, is that we can provide an extremely .exible interface \n(.rst-class queries), whilst at the same time yielding the performance (or close to) of hand\u00adcoded implementations. \nThe key to this is a mechanism for caching and incrementalising our queries. We choose JQL as a test-bed \nfor exploring this, but the ideas should apply more generally. For example, Python s list comprehensions, \nor Cq s LINQ would be excellent candidates for our approach.  3. Incrementalised Caching for JQL The \nJava Query Language (JQL) is a prototype extension to Java which introduces .rst-class queries [41]. \nQuerying is provided through the extended for-loop syntax (as shown in previous sections), and also the \nlist-comprehension syntax. For example, the following query uses list comprehension syntax to select \nelements from two collections: List<StringBuffer> words= new ArrayList<StringBuffer>(); List<Integer> \nlengths= new ArrayList<Integer>(); words.add(new StringBuffer( Hello )); words.add(new StringBuffer( \nWorlds )); words.add(new StringBuffer( blah )); lengths.add(4); lengths.add(5); List<Object[]> r = [StringBuffer \nx:words, Integer y:lengths |x.length() == y &#38;&#38; x.length() > 1] This returns all pairs of words \nand lengths matching the condition1. So, for the above example, the query returns {[ Hello ,5], [ Blah \n,4]}. JQL employs optimisations from the database literature which can signi.cantly improve upon the \nobvious nested\u00adloop implementation of a query. To process a query, the 1 The JQL notation has changed \nsince our earlier work [41] to bring it closer to Haskell and Python. JQL evaluator pushes tuples through \na staged pipeline. Each stage, known as join in the language of databases, corre\u00adsponds to a condition \nin the query. Figure 1 shows the query pipeline for the above query. The ordering of joins in the pipeline, \nas well as the join type, can greatly affect per\u00adformance. JQL uses heuristics to select good join orders, \nand supports several join types, including hash-join, sort\u00adjoin and nested-loop join. In fact, many more \nstrategies have been proposed which could further improve performance (e.g. [29, 38, 12, 37]). 3.1 Overview \nWe have extended JQL with an incrementalised caching sys\u00adtem. The cache stores evaluation statistics \nfor each query evaluated in the program, and uses a caching policy to de\u00adtermine when to cache queries. \nThe cache is incrementally updated to ensure results stored in the cache are consistent with the program \nstate. The incrementalised caching scheme is controlled by the cache manager, which keeps statistics \nfor all queries, and de\u00adtermines which queries to cache. Whenever a cached query is evaluated, the cache \nmanager intercepts the evaluation call and supplies its cached results back. When non-cached queries \nare evaluated, the cache manager may decide to be\u00adgin caching that query, in which case it builds a cache \nfrom its results. The decision of when to cache a query is deter\u00admined by a programmer-speci.ed caching \npolicy. The cache manager needs to match submitted queries with their cached results. Static program \nposition, for exam\u00adple, is insuf.cient. For example, consider: List<Student> xs = [ Student s : students \n| s == jim ]; The result set of this query depends upon the actual values of students and jim at the \ntime of execution. Cached results generated from a prior evaluation of this query can only be reused \nin an evaluation where the values for these variables match. Therefore, the cache manager maintains a \ncache map from concrete queries (i.e. those whose variables are substi\u00adtuted for their actual values) \nto result sets. Whenever a query is submitted, the cache manager searches this map to check for any matching \nresults. If results are available they are re\u00adturned as the result of the query. Here, incremental updat\u00ading \nensures they are correct even if the underlying objects or collections have changed. If the results aren \nt available, the query is executed and the results returned. During this, the cache manager consults \nthe caching policy to determine whether to cache these results. If so, the results are entered into the \ncache map and scheduled for incremental updating.  3.2 Incremental Cache Maintenance Once cached results \nare in place, it is essential that they ac\u00adcurately re.ect the program state. That is, they must be iden\u00adtical \nto those that would be returned by a non-cached evalua\u00adtion of the query. Updates to heap objects may \nrender cached results inconsistent, and we must incrementally update them lengths words  Figure 1. \nA query pipeline by adding/removing tuples as necessary. To capture this, we introduce the concept of \nquery dependencies. The intuition is that a (concrete) query is dependent upon some object if changes \nto the state of that object can affect the results of the query. For example, consider: xs = [ Student \ns : students | s.status().code == ENROLLED ]; Here, the collection object students,the Student objects \nit contains and, for each, the objects returned by status() are all dependencies of this query. This \nis because adding/removing a student from students, or changing the status code of a student in students \ncan affect its results. When an object in the system changes state, we must de\u00adtermine which (if any) \ncached queries depend upon it. For each, we check whether any tuples must be added/removed from its result \nset. For additions/removals from source col\u00adlections, this is easy enough, whilst for other updates it \ns more challenging. Object Addition/Removal. The results returned from a query may change if an object \nis added to one of its source collections. While this may increase the size of the result set, it cannot \ndecrease it because the evaluation of each tuple is independent of others. Updating a cached result set \nafter an object addition re\u00adquires generating the new tuples (if any), and adding them to the result \nset. Generating these tuples is simple, and uses the existing query pipeline. The cache manager evaluates \na sim\u00adpli.ed version of the query, with the new object substituted for its query variable. This requires \ntime proportional to the product of the sizes of the source collections, excluding that which was updated. \nFor example, in Figure 1, adding a new word to words means .rst checking it s length is > 1,and then \ncomparing this against all elements of lengths. The strategy for dealing with object removal from a source \ncollection is similar, but not identical the result set may decrease in size, but cannot increase. To \nremove tu\u00adples which are now invalid, we search the cached result set deleting any involving the removed \nobject. Object Updates. Cache consistency can also be affected when an object changes state, since a \nquery may now fail a condition it previously passed, and vice versa. The strategy is, again, to run affected \nobjects through a simpli.ed query pipeline to avoid re-evaluating entirely from scratch. The issue is \nthat we must determine which objects in the source collections are actually affected. When the updated \nobject is actually a member of one or more source collections, the process is similar to before: we .rst \nremove all tuples from the result set involving the object, and then evaluate a simpli.ed query pipeline \nwith it substituted for the relevant query variable(s). Any tuples produced are then added back to the \nresult set. For exam\u00adple, updating the length of a word from words in Figure 1 can potentially invalidate \nany resulting tuple involving that word; at the same time, it can also result in more tuples being added \nto the results. Our strategy ensures the right tuples are present at the end by essentially starting \nover with respect to this object. When the updated object is only indirectly accessible through a member \nof a source collection(s), things are more complex. We must decide which objects in the source col\u00adlections \nreach the updated object, and then start over with respect to them using a simpli.ed query pipeline. \nTracking the necessary dependency information to determine this set is non-trivial and, in Section 4.1, \nwe detail how our proto\u00adtype implementation conservatively approximates this.  3.3 Caching Policy Cached \nresults need incremental updating whenever opera\u00adtions affecting them occur. Such operations incur non-trivial \noverheads when they cause cached results to be incremen\u00adtally updated. The more frequently a cached result \nset is incrementally updated, the greater the cost of keeping it. Likewise, the more frequently its originating \nquery is eval\u00aduated, the greater the bene.t from keeping it. However, queries whose result sets aren \nt cached don t incur these costs. Hence, re-evaluating a query from scratch each time may be cheaper \nwhen the query/update ratio is low. Caching policy dictates when a query warrants caching, and when it \ndoesn t. An intelligent caching policy is crit\u00adical to obtaining the best performance possible. In the \nab\u00adsence of updates which can invalidate the cache, there s no overhead from incrementalisation, and \nso caching is always worthwhile2. In the presence of updates, however, we must balance the cost of incrementalising \ncached results versus the bene.t from keeping them. Obtaining a perfect caching policy is impossible \nsince it requires future knowledge of the query/update ratio. There\u00adfore, any solution must employ heuristics \nto determine when it is bene.cial to cache results. We have currently two simple policies as part of \nour incremental caching scheme. These are: Always On. This policy begins caching a query on the .rst \nevaluation and never stops (unless the cached result set is forceably evicted due to memory constraints). \n Query/Update Ratio. This policy records how often a query is evaluated and also the number of update \noper\u00adations that could affect the query. Caching the result set of a query begins when the query/update \nratio reaches a certain threshold (we use a default of 0.25 for this) and ceases when it drops below. \n A subtle aspect of the Query/Update Ratio policy is that we must maintain execution information on \nqueries even when their result sets aren t being cached.Otherwise, we cannot tell when the query/update \nratio for a query crosses our threshold and becomes bene.cial to cache. Therefore, we maintain a record \nfor each concrete query encountered. This incurs additional overhead, especially as the number of concrete \nqueries is, in principle, unbounded. To deal with this, the cache manager must garbage collect records \nfor queries which become inactive.  4. Evaluation Query caching can yield a large performance bene.t \nin pro\u00adgrams. However, caching introduces overhead the cache must be built and kept consistent with \nchanges in program state. We have developed a prototype implementation of our incrementalised caching \nsystem for JQL and we now dis\u00adcuss this, and present the results of two experiments investi\u00adgating performance: \nthe .rst examines the performance of Robocode with and without incrementalised caching; the second investigates \nthe trade-off of incrementally maintain\u00ading a cached result set, versus always recomputing it from scratch. \nFinally, we report on a study into the number of loops which can be turned into cachable queries across \na small corpus of Java applications. 2 Subject to the constraints of .nite memory resources. For example, \nif caching a query exhausts physical memory and causes paging, performance may be adversely affected. \nIn principle, a predetermined cache size much like the JVM heap limit could be used to control the \ncache size. In all experiments which follow, the experimental ma\u00adchine was an Intel Pentium IV 3.2GHz, \nwith 1.5GB RAM running NetBSD v3.99.11, Sun s Java 1.5.0 Runtime Envi\u00adronment and Aspect/J version 1.5.4. \nTiming was performed using the standard System.currentTimeMillis() method, which provides millisecond \nresolution (on NetBSD). The source code for JQL and the incrementalised caching prototype can be obtained \nfrom http://www.mcs.vuw.ac.nz/ djp/JQL/. 4.1 Prototype Implementation We have developed a prototype implementation \nof our in\u00adcrementalised caching system for JQL built using Aspect/J. While this supports the main features \nof our system, it is not optimal for several reasons. In particular, the use of Aspect/J imposes large \nmemory overheads, whilst its ability to track dependency information is somewhat limited (indeed, to \ndo this effectively may require VM support). The choice to use AspectJ here is purely for convenience, \nsince it eliminates the need to write a custom bytecode manipulation mecha\u00adnism; however, replacing it \nwith such a mechanism would likely yield performance improvements. Nevertheless, we believe the results \nobtained using our prototype demonstrate the potential that incrementalised query caching can offer. \nWe now discuss its salient features: Cache Map. For ef.ciency, the cache manager implements the cache \nmap (recall this maps concrete queries to their results) using a hash map. For this to work, a hash code \nmust be computed for a concrete query which is not affected by state changes in objects referred to by \nquery variables. For example, in an evaluation of the following query: List<Student> xs = [ Student s \n: students | s == jim ]; the variables students and jim refer to particular objects. If the query is \nre-evaluated with the same values for those variables, it should map to the same result set. Furthermore, \nif, in the meantime, say the object referred to by jim is updated, the (concrete) query must still map \nto the same result set (which must be incrementally updated to ensure consistency). Source Collections. \nTo determine when an addition/re\u00admoval from a source collection occurs, we use AspectJ to instrument \ncollection operations like Collection.add(Object). When such operations occur, the cache manager .rst \nchecks to ensure the addition/removal proceeded correctly (i.e., that the call returned true) and, if \nso, updates the cache using a simpli.ed query pipeline. We assume user-supplied collec\u00adtions adhere to \nthe Collection interface. For example, a class implementing Collection could break our incrementalisation \nwith an add() method that returned true, but actually did nothing. Implementations that adhere to the \nCollection inter\u00adface, such as the standard Collection implementations, will function correctly. Dependency \nTracking. To determine when an object up\u00addate occurs, we use Aspect/J to intercept .eld writes. Ide\u00adally \nwe would only intercept assignments to .elds of ob\u00adjects upon which cached queries depend. In practice, \nwe must intercept all .eld writes and then check whether any cached queries depend upon them. Since this \nwould add a prohibitive overhead, we require the programmer to annotate .elds with @Cachable to indicate \nwhich .elds to monitor. For safety, queries involving .elds which are not annotated @Cachable cannot \nbe cached, as we can t guarantee cache consistency. If a programmer knows a particular .eld is used in \na time-sensitive query, we assume they will annotate it ac\u00adcordingly. Determining whether an updated \nobject may invalidate a cached query s result set requires knowing the dependencies of that query. This \nis non-trivial and necessitates maintain\u00ading a dependency tree for that query. For simplicity, we only \nconsider updates to objects held directly in source collec\u00adtions. Thus, for safety, queries involving \nmulti-level indirec\u00adtions cannot be cached. The use of method calls in a query represents a similar problem \nin determining when an object update may inval\u00adidate cached results. For example, if a get() method call \nis used to access a .eld within a query, rather than a direct ac\u00adcess, the cache manager needs to know \nwhich .eld updates may invalidate the cache. Rather than developing a full im\u00adplementation for this, \nwe instead support just the most com\u00admon case namely, accessors. We again require the pro\u00adgrammer to \nannotate accessors with @Cachable and, within the annotation, to list those .elds which are read. This \nen\u00adsures our system knows which .elds to monitor. The ac\u00adcessor cannot, however, further dereference \n.elds of its ob\u00adject (since this results in the multi-level indirection issue from before) and cannot \ncall other methods which have side\u00adeffects or depend on .elds not listed. For safety, our system will \nnot cache queries containing other kinds of method call. A program analysis could be used to statically \ndetermine such dependencies for our accessors; likewise, full depen\u00addency tracking would allow general \nmethod calls to be used (although we would still require a notion of purity to prevent side-effects). \nWhile we are investigating static and dynamic techniques for resolving these issues, the contribution \nof this paper is in the design and evaluation of the incrementalised caching system, not object-update \ntracking.  4.2 Study 1: Robocode This study was an investigation into the bene.t obtainable from incrementalised \nquery caching on a real-world bench\u00admark, namely Robocode [31]. Here, software robots .ght each other \nin an arena. The robots can move around, swivel their turret to adjust their .eld-of-view and .re bullets. \nThere are several tunable parameters to a game, including the num\u00adber of robots and size of the arena. \nWe pro.led every loop in the Robocode application and found only six were heavily executed. Of these, \nfour could be easily converted into JQL extended for-loop queries. They all had the following form: for(Robot \nr : battle.getRobots()) {if(r!=null &#38;&#38; r!=this &#38;&#38; !r.isDead() &#38;&#38; r.intersects(...)) \n{ .... }} We translated the four loops into an extended for-loop, such as: for(Robot r : battle.getRobots() \n| r!=null &#38;&#38; !r.isDead()) {if(r!=this &#38;&#38; r.intersects(...)) {.... }} An important observation \nhere is that we explicitly chose not to include the condition r!=this in the query. To under\u00adstand why, \nit s important to consider that each robot executes this query during a turn of the game. Thus, including \nr!=this means its result set differs by exactly one element for each robot, with this robot omitted in \neach case. This causes many near identical result sets to be cached, with each requiring an incremental \nupdate for each change to the robots collection. In contrast, omitting the r!=this comparison ensures \nthe re\u00adsult set is the same for each robot and, hence, that only one cached result set is required, substantially \nlowering the cost. Experimental Setup. For this experiment, we measured the time for a one round game \nto complete, whilst varying the number of robots and the arena size. A single ramp up run was used, followed \nby .ve proper runs from which the average time was taken. These parameters were suf.cient to generate \ndata with a variant coef.cient = 0.2 indicating low variance between runs. Discussion. Figure 2 presents \nthe results of the Robocode experiments. In the .gure, no caching corresponds to the base JQL system \nwith caching disabled; observe that, in this situation, the Aspect/J weaver is not used to weave any \nclasses weaving only occurs when caching is enabled. The main observation from Figure 2 is that, on \nthe two larger arenas, the effectiveness of using incrementalised query caching becomes apparent as the \nnumber of robots increases. In the largest arena (size 4096x4096), with a large number of robots, we \n.nd a speedup of around one third. This is quite a signi.cant result, especially as this speedup includes \nthe cost of detecting changes, incrementally up\u00addating caches, and the underlying AspectJ dynamic weaver. \nThe reason for this is that the length of the game and, most importantly, the amount of time a robot \nwill be in the dead state increases with the number of robots. Thus, the caching scheme becomes effective \nwhen there are more robots, since there will be many dead robots that it avoids scanning, unlike the \nuncached implementation. For a small arena (size 1024x1024) or few robots, little advantage is seen from \nincrementalised query caching. The reason for this is that, in a smaller arena containing the same number \nof robots, those robots will be pushed more closely Arena Size = 1024x1024 Arena Size = 2048x2048 70 \n90 80 60  Average Runtime (s) Average Runtime (s) Average Runtime (s) Average Runtime (s) 70 50 60 \n50 40 40 30 3020 20 10 10 Arena Size = 4096x4096 Arena Size = 8192x8192 160 350  140 120 100 80 60 \n40 20 300 250 200 150 100 50 0 0 0 20 40 60 80 100 120 140 160 0 20 40 60 80 100 120 140 160 # Robots \n# Robots Figure 2. Experimental results comparing the performance of the Robocode application with and \nwithout our incrementalised caching scheme. and the arena size is small, the non-caching implementa- \nArena Size = 1024x1024 tion slightly wins out overall. We would expect that, if the Average Memory Usage \n(MB) 60 50 40 30 20 10 0# Robots arena size decreased further, this difference would become more pronounced \nat the lower end. Likewise, we would ex\u00adpect that, as the arena size increased further, the advantages \nof incrementalised query caching would become more pro\u00adnounced. Another observation from Figure 2 is \nthat using the always-caching heuristic is never really worse than with caching disabled. While this \nmay seem surprising, it simply re.ects the fact that, for this application (i.e. Robocode), the query/update \nratio never falls low enough for caching to be detrimental. Finally, we investigated the memory consumption \nof our Figure 3. Experimental results evaluating the memory con\u00adsumption of the Robocode application \nwith and without our incrementalised caching scheme. Observe that weave only is used to identify the \nmemory usage caused by the AspectJ load-time weaver. In this situation no caching is taking place, but \nthe AspectJ load-time weaver is still weaving classes as though it were. This contrasts with no caching \n, where the the AspectJ load-time weaver is not weaving any classes. together. Thus, they destroy each \nother at a much higher rate, which lowers the query/update ratio and decreases the length of the game. \nWe can see that, when there are few robots incrementalised caching scheme. The results are reported in \nFigure 3. From this we can see that, while the memory over\u00adheads are high, this is almost entirely caused \nby the AspectJ load-time weaver (see the points marked weave only ). We also con.rmed this independently \nusing the HProf pro\u00ad.ler [39]. We would expect these overheads could be sig\u00adni.cantly improved using \na bytecode weaver customised for the JQL system. Indeed, we would not expect large mem\u00adory overheads \nfor caching in this application since there are only four result sets (arising from the four queries) \nbeing cached, each with no more than 150 robots. Finally, while not shown, the overheads remain very \nmuch the same irre\u00adspective of Arena size. Name Details One Source [ Attends a:attends | a.course == \nCOMP101 ] This query has a single pipeline stage, and a single domain variable (COMP101 is a constant \nvalue for a Course object). Removing, inserting or updating an object in attends requires the system \nto simply check the affected object s course .eld against COMP101. Two Sources [ Attends a:attends, Student \ns:students | a.course == COMP101 &#38;&#38; a.student == s ] This query requires two pipeline stages \nand has two domain variables. Removing, inserting or updating an object in attends requires the system \nto check the affected object s course against COMP101, and to compare its student .eld against all student \nobjects in students. Three Sources [ Attends a:attends, Student s:students, Course c:courses | a.course \n== COMP101 &#38;&#38; a.student == s &#38;&#38; a.course = c ] This query requires three pipeline stages, \nand has three domain variables. Removing, inserting or updating an object in attends requires the system \nto check the affected object s course against COMP101, to compare its student .eld against all student \nobjects in students, and then to compare those matching with all courses. Table 1. Details of the three \nbenchmark queries 4.3 Study 2: Cache Incrementalisation Caching query result sets can greatly improve \nperformance. Once a query s result set has been cached, it must then be incrementally updated when changes \noccur which may af\u00adfect the cached results. Therefore, we have conducted ex\u00adperiments exploring the trade-off \nof querying performance against the overhead of incremental updates and we now re\u00adport on these. There \nare two ways that query results can be altered be\u00adtween evaluations: either objects can be added to or \nremoved from one of the source Collections for a query; or, the value of an object .eld used in the query \ncan be changed. In either case, the process for dealing with the change is the same: the affected object \nis passed through the existing query pipeline to ascertain whether it should be added or removed from \nthe cached result set. We have constructed a benchmark which varies the ratio of query evaluations to \nupdates to explore the trade-offs involved. The benchmark constructs three initial collections containing \nnStudents, nCoursesand nAttends objects respectively. The benchmark performs 5000 opera\u00adtions consisting \nof either a JQL query or the random addition and removal of an Attends object from its collection (which \nis a source for the JQL query). The size of the Attends collec\u00adtion is kept constant as this would otherwise \ninterfere with the experiment, since the cost of a query evaluation depends upon the size of the Attends \ncollection. Finally, we consid\u00adered (in isolation) three queries of differing length to explore the effect \nof query size on performance. The three queries are detailed in Table 1. Experimental Setup. For each \nquery in Table 1, we mea\u00adsured the time to perform 5000 operations whilst varying the ratio of evaluations \nto updates (in steps of 0.02 between 0 ...0.2, and 0.1 between 0.2 ...1). As discussed above, each operation \nwas either: a random addition and random removal from the Attends collection; or an evaluation of the \nquery being considered. This was repeated 50 times with the average being taken. These parameters were \nsuf.cient to generate data with a variation coef.cient of = 0.15 indicating low variance between runs. \nDiscussion. Figure 4 presents the data for each of the queries in Table 1. There are several observations \nwhich can be made from these plots. Firstly, incrementalised caching is not optimal when the ratio of \nqueries to updates is low. This re.ects the fact that, when the number of evaluations is low, the pay \noff from caching a result set is small compared with the cost of maintaining it. As the ratio increases, \nhowever, the advantages of caching quickly become apparent. For the ratio caching policy, we can clearly \nsee the point at which it begins to cache the query result sets. The plots highlight both the advantages \nand disadvantages of this heuristic: when the query/update ratio is low, it can outperform the always-on \npolicy by not caching result sets; unfortunately, however, the point at which it decides to cache result \nsets is .xed and, hence, it does not necessarily obtain the best performance on offer. Another interesting \nobservation from the plots is that the complexity of the query affects the point at which it becomes \nfavourable to cache result sets. This is because the cost of an update operation is directly affected \nby the number of source collections in the query. If there is just one source collection, then each update \ncorresponds to simply passing the affected object through the pipeline; however, if there is more than \none source collection then, roughly speaking, we One Source Benchmark, #Ops=5000, Collection Size=1000 \n1.2  Average Runtime (s) 1 0.8 0.6 0.4 0.2 0 # Query Evaluations / # Update Operations Two Source Benchmark, \n#Ops=5000, Collection Size=1000 Three Source Benchmark, #Ops=5000, Collection Size=1000 1.8 8 1.67 \n Average Runtime (s) 1.4 1.2 1 0.8 0.6 0.4 6 5 4 3 2 10.2 0 0# Query Evaluations / # Update Operations \n# Query Evaluations / # Update Operations Figure 4. Experimental results comparing the evaluation time \nwithout caching, with always-on caching and with ratio caching. must iterate through the product of those \nnot containing the affected object. Clearly, this quickly becomes expensive and can easily outweigh the \ngains from incrementally maintain\u00ading cached result sets.  4.4 Study 3: Understanding Program Loops \nQueries represent a general mechanism for replacing loops over collections. This raises the question \nof how many loops found in typical programs can, in fact, be replaced by queries. To understand this, \nwe inspected every loop in the Java programs listed in Table 2. We found that the majority of loops can \nbe classi.ed into a small number of categories, most of which can be replaced by queries. For the purposes \nof this paper, we are also interested in which loops can po\u00adtentially bene.t from caching and incrementalisation. \n4.4.1 Taxonomy of Loop Categories. Our inspection used the following categories of loops in pro\u00adgrams: \nUn.ltered, Level-1 Filters, Level-2+ Filters, Reduce, and Other for loops which did not .t in to one \nof the other categories. Un.ltered. Loops in this category take a collection of ob\u00adjects and apply some \nfunction to every element. This roughly corresponds to the map function found in languages such as Haskell \nand Python and is the simplest category of loops that can be expressed as queries. An example from Robocode \nis the following: for(Robot r : robots) { r.out.close(); } These loops do not stand to gain from caching \nand incremen\u00adtalisation since they always operate over whole collections thus, there is nothing to cache. \nLevel-1 Filters. These loops iterate over a collection and apply some function to a subset of its elements. \nWe ve dis\u00adcussed how the Robocode game frequently operates on every alive robot, by selecting from the \nmain robot list: for(Robot r : battle.getRobots()) { if(r!=null &#38;&#38; r!=this &#38;&#38; !r.isDead() \n&#38;&#38; r.intersects(...)) { ... } } We have already seen how this loop can be turned into a JQL \nextended for-loop. The key point is that, in our system, the list of all robots which are not dead can \nbe cached and incrementally maintained. This prevents us from scanning every robot every time the query \nis evaluated (as is done in the original code). Hence, this class of loops stand to bene.t from caching \nand incrementalisation. Level-2+ Filters. This category is a logical continuation from the previous, \nexcept that it identi.es nested loops, rather than simple loops, which operate on a subset of the product \nof the source collections. A simple example is the following code for operating on students who are also \nteach\u00aders: for(Student s : students) { for(Teacher t : teachers) { if(s.name.equals(t.name)) {... } }} \n This would be translated into the following JQL query: for(Student s:students, Teacher t:teachers |s.name.equals(t.name)) \n{ ... } The greater the level of nesting, the greater the potential cost of such a loop is. But, at \nthe same time, the greater the potential gain from the optimising query evaluator used by JQL. Intelligent \njoin ordering strategies and incrementalised query caching can all greatly speed up such nested loop \noperations. In our categorisation, we distinguish level-1 from level\u00ad2+ .lters for several reasons: .rstly, \nlevel-1 .lters are by far the most common in our benchmark programs; secondly, they stand to gain from \nthe incrementalised caching tech\u00adnique presented in this paper, but not from the join ordering strategies \noutlined in our earlier work [41]. Reduce. The Reduce category consists of operations which reduce a \ncollection(s) to either a single value or a very small set of values. Summing the elements of a collection \nis per\u00adhaps the most common example of this. Concatenating a collection into one large string is another. \nThese operations cannot be expressed as queries in our query language and, hence, do not stand to bene.t \nfrom incrementalised query caching. With a suf.ciently expressive query language it is possible to maintain \nthem incrementally, although this re\u00adquires more complex techniques than we are considering here [24]. \nOther. The majority of loops classi.ed under Other are re\u00adlated to I/O (e.g. reading until the end of \na .le, etc). The remainder are mostly loops on collections which either: de\u00adpend on or change the position \nof elements in the collec\u00adtion; or operate on more than one element of the collection at a time. Collections.sort(), \nfor example, cannot easily be expressed as a query since it relies upon the ordering of ele\u00adments. Likewise, \nCollections.reverse() is also not expressible as a query. 4.4.2 Results and Discussion. Using this taxonomy \nwe examined, by hand, the set of open source Java programs listed in Table 2. The results of our analysis \nare presented in Figure 5. Roughly two-thirds of the loops we encountered were expressible as JQL queries \nand, of these, roughly half would stand to bene.t from our incre\u00admentalised caching approach. Un.ltered \nis the most com- Robocode (Game) 1.2.1A 23K RSSOwl (RSS Reader) 1.2.3 46K ZK (AJAX Framework) 2.2.0 \n45K Chungles (File Transfer) 0.3 4K Freemind (Diagram Tool) 0.8 70K SoapUI (WebService Testing) 1.7b2 \n68K Table 2. Java Programs Inspected for Study 3 monly occurring category of loops, which seems unfortu\u00adnate \nas these cannot gain from incrementalised caching. An interesting observation, however, is that many \nof the Un.l\u00adtered loops may, in fact, already be operating on manually maintained query results. To understand \nthis, consider the Battle.getAliveRobots() method from \u00a72.1 and corresponding loop in Robot.scan(). Since \nthis searches the entire collec\u00adtion returned by Battle.getAliveRobots(), it would be classi\u00ad.ed as Un.ltered \n. However, this loop would not even exist in a system making use of cached, incrementalised queries! \nA deeper analysis of the structure of programs is needed to prove this hypothesis, however. Another observation \nfrom Figure 5 is that there are rel\u00adatively few level-2+ Filter loops. Such operations are, al\u00admost certainly, \nactively avoided by programmers since they represent fairly expensive operations. The high proportion \nof other loops found in the Freemind benchmark may also seem somewhat surprising. Upon closer examination \nof the code it became apparent that the majority of these came from an automatically generated XML parser. \nFinally, it is important to realise that, although our anal\u00adysis of these programs indicates many of \ntheir loops could be transformed into queries which could bene.t from incre\u00admentalised query caching, \nthis does not mean they necessar\u00adily will. In many cases, for example, the loops in question may operate \nover collections which are small and, hence, the potential gain from incremental caching would be lim\u00adited. \nNevertheless we argue that, even if performance is not improved, the readability and understandability \nof the code will be.  5. Discussion 5.1 Designing for Querying Optimised, .rst-class query support \nin programming lan\u00adguages should change the way programs are designed. To canvas these issues, consider \nthe following interface for a Graph: interface Graph { boolean addEdge(Edge e); boolean removeEdge(Edge \ne); Set<Edge> edges(Object n); } Operation Expressible as Query? Bene.t from C/I? Robocode RSSOwl ZK \nChungles Freemind SoapUI Un.ltered Yes L1 Filter Yes L2+ Filter Yes Reduce No Other No No Yes Yes No \nNo 38 92 8 21 66 117 109 2 34 67 140 124 4 14 62 24 18 0 6 31 211 160 2 30 696 372 154 1 39 126 Total \n225 329 344 79 1099 692   Figure 5. Categorisations of loop operations in Java programs. Note, C/I \nstands for Caching and Incrementalisation . Hence, L(evel-)1 Filter and L(evel-)2+ Filter are the two \nloop classes which can bene.t from the incrementalised query cache approach outlined in this paper. \nThis provides a .xed set of common operations for manip\u00adulating graphs: addEdge(e), removeEdge(e) and \nedges(n) (this returns the set of edges involving some object n). The add/remove operations are updates, \nwhilst the edges() acces\u00adsoris a query. But, what does it query over? In an abstract sense, a graph is \nsimply a set of pairs E;the edges(n) oper\u00adation is then a query over this set and can be formulated as \na set comprehension: edges(n)={(x, y). E | x =n . y =n} Thus, it becomes apparent that our Graph ADT \n.xes the set of possible queries when, in fact, many more are possible. For example, we might like to \nobtain the set of edges involv\u00ading a node n, whilst excluding loops (i.e. edges to/from the same node): \nno loops(n)={(x, y). E | (x =n . y =n). x =.y} Since this operation is not part of our Graph ADT, a user \nwanting this functionality must obtain it manually. This is not hard to do; one simply iterates over \nedges(n) and uses a conditional to narrow it down appropriately. However, this is cumbersome and, we \nargue, most programmers expend considerable time writing such loops needlessly. First-class queries, \non the other hand, provide a much cleaner and more general interface. The challenge is making them competi\u00adtive \nwith manual ADT implementations, which are typically optimised for the queries they support. We believe \nour incre\u00admentalised query caching scheme represents an important step in this direction. We hope it \nmay allow programmers to write less optimised ADT implementations, with more general APIs, and rely on \nincrementalised query caching to boost performance. To see how incrementalised query caching can affect \nprogram design, compare two implemen\u00adtations of the Graph ADT. First, an adjacency list design pro\u00advides \nan ef.cient edges(n) operation (iteration is linear in |edges(n)|), storing a set of edges for each node: \nclass AdjacencyList implements Graph { HashMap<Object,HashSet<Edge>>edges = ...; boolean addEdge(Edge \ne) { edges(e.head()).add(e); return edges(e.tail()).add(e); } boolean removeEdge(Edge e) { ... } Set<Edge>edges(Object \nn) { HashSet<Edge>rs = edges.get(n); if(rs == null) { rs = new HashSet<Edge>(); edges.put(n,rs); }  \nreturn rs; } } A simpler, somewhat na\u00a8ive implementation can maintain edges as a single set: class CompactGraph \nimplements Graph { HashSet<Edge>edges = ...; boolean addEdge(Edge e) { edges.add(e); } boolean removeEdge(Edge \ne) { ... } Set<Edge>edges(Object n) { HashSet<Edge>rs = new HashSet<Edge>(); for(Edge e : edges) { if(e.to() \n== n || e.from() == n) { rs.add(e); } }  return rs; } } In the simpler design, answering the edges(n) \nquery requires traversing the entire edge set whilst building up the result set. The advantage is a reduced \nmemory footprint and faster ad\u00add/remove operations (which avoid both HashMap lookups). Such trade-offs \nin ADT implementations are, of course, well understood. The advantage of incremental querying incorporated \ninto a programming language is that suppliers of ADTs can choose not to make these design tradeoffs: \nthey can produce a straightforward design, and rely on incremental caches to improve the performance \nof those queries that are actually executed by particular client programs. If, for example, a graph is \nupdated often but edges(n) is called relatively infre\u00adquently, then the time and space costs to maintain \nthe hash map, not to mention the additional programmer effort re\u00adquired to implement the more sophisticated \nimplementation, are all unnecessary. So we can view the HashMap in AdjacencyList as pre\u00adcisely the kind \nof incremental cache that we discuss in this paper. If the ADT queries are cachable, even our limited \npro\u00adtotype will cache the result of edges(n) in CompactGraph, and prevent its recomputation if edges(n) \nis called again for the same n. Of course, if the graph changes due to an up\u00addate operation, JQL s cache \nwill be incrementally updated to re.ect the new graph. So, JQL s caches will correspond almost exactly \nto the AdjacencyList implementation ex\u00adcept that the work is done automatically within the caching system, \nrather than manually by the programmer. Over time, we expect that programmers will be able to design \nsimpler and more straightforward programs, and rely upon incre\u00admentalised caching to provide acceptable \nperformance.  5.2 Usage Patterns An important feature of our caching implementation is that it can \nreact to changing patterns of usage at runtime (assuming the query/update ratio policy is used). Manu\u00adally \nimplemented caching schemes do not typically do this and, instead, assume .xed usage patterns. For example, \nthe AdjacencyList implementation above makes a .xed assump\u00adtion about the ratio of calls to edges(n) \nversus addEdge() (i.e. that it always favours caching edges(n)). A programmer is forced to choose either \nno caching (i.e. CompactGraph) or always caching (i.e. AdjacencyList) there s no inbe\u00adtween. Of course, \none can in theory construct such a hybrid Graph implementation but a typical programmer proba\u00adbly wouldn \nt (because of the additional complexity). Indeed, even the Java collections library doesn t provide such \nhy\u00adbrid collections (e.g. of ArrayList and LinkedList) despite the potential bene.ts. While our query/update \nratio policy is certainly bene.cial in this regard, it has some limitations. For example, while the heuristic \ncan react to changing program usage, it may be slow to do this since it must wait until enough opera\u00adtions \nhave occurred to push the ratio above (or below) the threshold. Whilst this is .ne for most situations, \nit will be problematic in some. For example, when operations occur in short bursts, the heuristic may \nnot react until near the end of the current burst, or not at all! Developing heuristics to deal with \nthis kind of activity seems like challenging, but interesting future work. 5.3 Concurrency Our prototype \nimplementation of JQL deals with concur\u00adrency in a relatively simple manner. The cache of all result \nsets is implemented using a ConcurrentHashMap which sup\u00adports concurrent access via non-blocking synchronisation. \nIndividual result sets within the cache are only locked for the duration of a single update. This mechanism \nprevents ConcurrentModi.cationExceptions from being thrown by the JQL system. The desired semantics of \nJQL in a concurrent setting remains to be properly considered, however. We also believe there is scope \nto exploit concurrency to optimise query execution further, although this is not cur\u00adrently supported \nby our prototype. Allowing query execu\u00adtion to take advantage of data parallelism would seem an obvious, \nand potentially valuable, strategy.  6. Related Work Language queries and set comprehensions generally \nwithout caching or incrementalisation have been pro\u00advided in many languages from SETL [36] and NPL [5] \nthrough Haskell and Python up to C. [2], and LINQ in Cq and VB [28]. Regarding optimisation of queries, \nan important work is that of Liu et al. [24] who regard all programs as a series of queries and updates. \nThey developed an automatic system for transforming programs in an object-oriented language extended \nwith set comprehensions; this operates at com\u00adpile time, adding code to explicitly cache and incrementalise \nset comprehensions. Thus, their incrementalised caches are hard-coded into the program, which contrasts \nwith our more dynamic approach. They demonstrate, for several Python list comprehensions, that the code \nproduced by their sys\u00adtem is signi.cantly faster than the base implementation. This approach seems interesting \nsince it takes the programmer closer to the goal of specifying complex operations, rather than implementing \nthem laboriously by hand. In other work, Liu et al. consider ef.ciently evaluating Datalog rules us\u00ading \nincrementally maintained sets [23] and, elsewhere, have demonstrated the value of this in the context \nof type infer\u00adence [17]. They have also considered incrementalisation of more general computations, including \narray aggregation (es\u00adsentially multi-dimensional reduce) [25] and recursive func\u00adtions [26]. More recently, \nAcar et al. have designed a general incrementalisation framework as an extension to ML, and proved that \ntheir incremental computations have the same result as non-incremental computations with the same in\u00adputs \n[1]. The problem of incrementally evaluating database queries, known as the view maintenance problem, \nhas received some considerable attention in the past (e.g. [3, 10, 9, 30, 18]). This problem differs \nsomewhat from ours in several ways: .rstly, it is usually assumed that the choice to incremen\u00adtally maintain \na table is made by the database administrator; secondly, certain operations (in particular, reduce) are \nnot relevant in this setting. Nevertheless, it is useful to con\u00adsider what has been done here. Gupta \nand Mumick exam\u00adined the view maintenance problem in a traditional database setting [10]. They discuss \na number of optimisations and algorithms found in the literature. For example, some al\u00adgorithms operate \nwhen the source tables are only partially available and this limits the situations where incrementali\u00adsation \ncan be safely performed; others (e.g. [11]) use some\u00adthing akin to reference counting to make delete \noperations more ef.cient. By counting the number of ways a tuple can enter the result set (known as derivations), \nthese systems can avoid re-examining the whole source domain when a tuple is deleted. Another interesting \nwork is that of Nakamura, who considered the incremental view problem in the context of object-oriented \ndatabases [30]. This setting is considered more challenging than for traditional databases as OODBs must \nhandle more complex data structures and, presumably, queries. Another relevant work is that of Lencevicius \net al.,who developed a series of Query-Based Debuggers [21, 19] to address the cause-effect gap [6]. \nThe effect of a bug (erro\u00adneous output, crash, etc) often occurs some time after the statement causing \nit was executed, making it hard to identify the real culprit. Lencevicius et al. observed that typical \nde\u00adbuggers provide only limited support for this in the form of breakpoints that trigger when simple \ninvariants are broken. They extended this by allowing queries on the object graph to trigger breakpoints \n thereby providing a mechanism for identifying when complex invariants are broken. They also considered \nthe problem of incrementally maintaining cached query result sets [20, 22]. Their system always chose \nto in\u00adcrementalise queries, rather than trying to be selective about this as we are. Nevertheless, they \nobserved speed ups of sev\u00aderal orders of magnitude when caching and incrementalisa\u00adtion were used. Several \nother systems have used querying to aid debug\u00adging and, although none of these support caching or in\u00adcrementalisation, \nit seems likely they could bene.t from it. The Fox [33, 34] operates on program heap dumps to check certain \nownership constraints are properly maintained. The Program Trace Query Language (PTQL) permits re\u00adlational \nqueries over program traces with a speci.c focus on the relationship between program events [8]. The \nPro\u00adgram Query Language (PQL) is a similar system which al\u00adlows the programmer to express queries capturing \nerroneous behaviour over the program trace [27]. Hobatr and Mal\u00adloy [14, 15] present a query-based debugger \nfor C++ that uses the OpenC++ Meta-Object Protocol [4] and the Object Constraint Language (OCL) [40]. \nThis system consists of a frontend for compiling OCL queries to C++, and a back\u00adend that uses OpenC++ \nto generate the instrumentation code necessary for evaluating the queries. Our JQL system was originally \ninspired by these debugging systems, but was ex\u00adtended to support a range of join optimisations over \nsingle queries [41]. This paper describes how we extended JQL to cache results between queries, and then \nincrementally to up\u00addate those caches to account for changes in the program as it executes between queries. \nFinally, Ramalingam and Reps have produced a cate\u00adgorised bibliography of incrementally computation, \nwhich covers the diverse ways in which incrementalisation has been applied in computer science [35]. \n 7. Conclusion In this paper we have presented the design and implementa\u00adtion of a system for caching \nand incrementalisation in the Java Query Language. This improves the performance of object queries which \nare frequently executed, by caching queries results and then updating their caches as the pro\u00adgram runs. \nThis means that subsequent queries can bene.t from the work performed by earlier queries, even while \nthe objects and collections underlying the queries are updated between each query execution. An important \naspect of our design is the choice of when to incrementalise a query. This is a challenge because incre\u00admentalisation \nis not for free: instrumentation is required to track updates to objects and to determine how these affect \nthe cached result sets, and memory is required to store the caches. We have detailed an experimental \nstudy looking at different ratios of queries to updates in an effort to under\u00adstand the trade-offs here. \nFurthermore, although we consid\u00adered only relatively simple caching policies in this paper, it seems \nlikely that many interesting heuristics could be de\u00adveloped to address this problem. We have also presented \na study inspecting loops in Java programs, which indicates that many loops could be rewritten with queries \nand would stand to bene.t from caching and incrementalisation. The complete source for our prototype \nimplementation is available for download from http://www.mcs.vuw. ac.nz/ djp/JQL/. We hope that it will \nmotivate further study of object querying as a .rst-class language construct.  Acknowledgements Thanks \nto all the anonymous reviewers who have read this paper. This work is supported by the University Research \nFund of Victoria University of Wellington, and the Royal Society of New Zealand Marsden Fund. References \n[1] U. Acar, A. Ahmed, and M. Blume. Imperative self-adjusting computation. In Proceedings of the ACM \nConference on Principles of Programming Languages (POPL), 2008. [2] G. Bierman, E. Meijer, and W. Schulte. \nThe essence of data access in c..In Proceedings of the European Conference on Object-Oriented Programming \n(ECOOP), volume 3586 of Lecture Notes in Computer Science, pages 287 311. Springer-Verlag, 2005. [3] \nS. Ceri and J. Widom. Deriving production rules for incremental view maintenance. In Proceedings of the \nInternational Conference on Very Large Data Bases (VLDB), pages 577 589. Morgan Kaufmann Publishers Inc., \n1991. [4] S. Chiba. A metaobject protocol for C++. In Proceedings of the ACM conference on Object-Oriented \nProgramming, Systems, Languages and Applications (OOPSLA), pages 285 299. ACM Press, 1995. [5] J. Darlington. \nProgram transformation and synthesis: Present capabilities. Technical Report Res. Report 77/43, Dept. \nof Computing and Control, Imperial College of Science and Technology, London, 1977. [6] M. Eisenstadt. \nMy hairiest bug war stories. Communications of the ACM, 40(4):30 37, 1997. [7] E. Gamma, R. Helm, R. \nE. Johnson, and J. Vlissides. Design Patterns: Elements of Reusable Object-Oriented Software. Addison-Wesley, \n1994. [8] S. Goldsmith, R. O Callahan, and A. Aiken. Relational queries over program traces. In Proceedings \nof the ACM Conference on Object-Oriented Programming, Systems, Languages and Applications (OOPSLA), pages \n385 402. ACM Press, 2005. [9] T. Grif.n and L. Libkin. Incremental maintenance of views with duplicates. \nIn Proceedings of the international conference on Management of Data, pages 328 339. ACM Press, 1995. \n[10] A. Gupta and I. S. Mumick. Maintenance of materialized views: Problems, techniques and applications. \nIEEE Quar\u00adterly Bulletin on Data Engineering; Special Issue on Materi\u00adalized Views and Data Warehousing, \n18(2):3 18, 1995. [11] A. Gupta, I. S. Mumick, and V. S. Subrahmanian. Maintain\u00ading views incrementally. \nIn Proceedings of the international conference on Management of Data, pages 157 166. ACM Press, 1993. \n[12] P. J. Haas, J. F. Naughton, and A. N. Swami. On the relative cost of sampling for join selectivity \nestimation. In Proceedings of the thirteenth ACM symposium on Principles of Database Systems (PODS), \npages 14 24. ACM Press, 1994. [13] K. Hartness. Robocode: using games to teach arti.cial intelligence. \nJournal of Computing Sciences in Colleges, 19(4):287 291, 2004. [14] C. Hobatr and B. A. Malloy. The \ndesign of an OCL query\u00adbased debugger for C++. In Proceedings of the ACM Symposium on Applied Computing \n(SAC), pages 658 662. ACM Press, 2001. [15] C. Hobatr and B. A. Malloy. Using OCL-queries for debugging \nC++. In Proceedings of the IEEE International Conference on Software Engineering (ICSE), pages 839 840. \nIEEE Computer Society Press, 2001. [16] J.-H. Hong and S.-B. Cho. Evolution of emergent behaviors for \nshooting game characters in robocode. In Proceedings of the 2004 IEEE Congress on Evolutionary Computation, \npages 634 638. IEEE Press, 2004. [17] K. Hristova, T. Rothamel, Y. A. Liu, and S. D. Stoller. Ef.cient \ntype inference for secure information .ow. In Proceedings on Programming Languages and Analysis for Security, \npages 85 94. ACM Press, 2006. [18] K. Y. Lee, J. H. Son, and M. H. Kim. Ef.cient incremental view maintenance \nin data warehouses. In Proceedings of the conference on Information and knowledge management, pages 349 \n356. ACM Press, 2001. [19] R. Lencevicius. Query-Based Debugging. PhD thesis, University of California, \nSanta Barbara, 1999. TR-1999\u00ad 27. [20] R. Lencevicius. On-the-.y query-based debugging with examples. \nIn Proceedings of the Workshop on Automated and Algorithmic Debugging (AADEBUG), 2000. [21] R. Lencevicius, \nU. H\u00a8olzle, and A. K. Singh. Query-based debugging of object-oriented programs. In Proceedings of the \nACM conference on Object-Oriented Programming, Systems, Languages and Applications (OOPSLA), pages 304 \n317. ACM Press, 1997. [22] R. Lencevicius, U. H\u00a8olzle, and A. K. Singh. Dynamic query-based debugging. \nIn Proceedings of the European Conference on Object-Oriented Programming (ECOOP), volume 1628 of Lecture \nNotes in Computer Science, pages 135 160. Springer-Verlag, 1999. [23] Y. A. Liu and S. D. Stoller. From \ndatalog rules to ef.cient programs with time and space guarantees. In Proceedings of the ACM Conference \non Principles and Practice of Declarative Programming, pages 172 183. ACM Press, 2003. [24] Y. A. Liu, \nS. D. Stoller, M. Gorbovitski, T. Rothamel, and Y. E. Liu. Incrementalization across object abstraction. \nIn Proceedings of the ACM conference on Object-Oriented Pro\u00adgramming, Systems, Languages and Applications \n(OOPSLA), pages 473 486. ACM Press, 2005. [25] Y. A. Liu, S. D. Stoller, N. Li, and T. Rothamel. Optimizing \naggregate array computations in loops. ACM Transactions on Programming Languages and Systems, 27(1):91 \n125, 2005. [26] Y. A. Liu, S. D. Stoller, and T. Teitelbaum. Static caching for incremental computation. \nACM Transactions on Program\u00adming Languages and Systems, 20(3):546 585, 1998. [27] M. Martin, B. Livshits, \nand M. S. Lam. Finding application errors and security .aws using PQL: a program query language. In Proceedings \nof the ACM conference on Object-Oriented Programming, Systems, Languages and Applications (OOPSLA), pages \n365 383. ACM Press, 2005. [28] E. Meijer, B. Beckman, and G. M. Bierman. LINQ: recon\u00adciling object, relations \nand XML in the .NET framework. In Proceedings of the ACM Symposium on Principles Database Systems, 2006. \n[29] P. Mishra and M. H. Eich. Join processing in relational databases. ACM Computing Surveys, 24(1):63 \n113, 1992. [30] H. Nakamura. Incremental computation of complex object queries. In Proceedings of the \nACM conference on Object-Oriented Programming, Systems, Languages and Applica\u00adtions (OOPSLA), pages 156 \n165. ACM Press, 2001. [31] M. Nelson. Robocode, http://robocode.sourceforge.net, 2007. [32] J. O Kelly \nand J. P. Gibson. Robocode &#38; problem\u00adbased learning: a non-prescriptive approach to teaching programming. \nIn Proceedings of the ACM conference on Innovation and technology in computer science education, pages \n217 221, 2006. ACM Press. [33] A. Potanin, J. Noble, and R. Biddle. Checking ownership and con.nement. \nConcurrency and Computation: Practice and Experience, 16(7):671 687, 2004. [34] A. Potanin, J. Noble, \nand R. Biddle. Snapshot query-based debugging. In Proceedings of the IEEE Australian Software Engineering \nConference (ASWEC), pages 251 261. IEEE Computer Society Press, 2004. [35] G. Ramalingam and T. Reps. \nA categorized bibliography on incremental computation. In Proceedings of the Conference on the Principles \nof Programming Languages, pages 502 510. ACM Press, 1993. [36] J. Schwartz, R. Dewar, E. Dubinsky, and \nE. Schonberg. Programming with Sets: An Introduction to SETL. Springer-Verlag, 1986. [37] M. Steinbrunn, \nG. Moerkotte, and A. Kemper. Heuristic and randomized optimization for the join ordering problem. The \nVLDB Journal, 6(3):191 208, 1997. [38] A. N. Swami and B. R. Iyer. A polynomial time algorithm for optimizing \njoin queries. In Proceedings of the Inter\u00adnational Conference on Data Engineering, pages 345 354, Washington, \nDC, USA, 1993. IEEE Computer Society. [39] D. Viswanathan and S. Liang. Java virtual machine pro.ler \ninterface. IBM Systems Journal, 39(1):82 95, 2000. [40] J. Warmer and A. Kleppe. The Object Constraint \nLanguage: precise modeling with UML. Addison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, 1999. \n[41] D. Willis, D. J. Pearce, and J. Noble. Ef.cient object querying for Java. In Proceedings of the \nEuropean Conference on Object-Oriented Programming (ECOOP), volume 4067 of Lecture Notes in Computer \nScience, pages 28 49. Springer-Verlag, 2006.  \n\t\t\t", "proc_id": "1449764", "abstract": "<p>Many contemporary object-oriented programming languages support first-class queries or comprehensions. These language extensions make it easier for programmers to write queries, but are generally implemented no more efficiently than the code using collections, iterators, and loops that they replace. Crucially, whenever a query is re-executed, it is recomputed from scratch. We describe a general approach to optimising queries over mutable objects: query results are cached, and those caches are incrementally maintained whenever the collections and objects underlying those queries are updated. We hope that the performance benefits of our optimisations may encourage more general adoption of first-class queries by object-oriented programmers.</p>", "authors": [{"name": "Darren Willis", "author_profile_id": "81381593782", "affiliation": "Victoria University of Wellington, Wellington, New Zealand", "person_id": "P1223142", "email_address": "", "orcid_id": ""}, {"name": "David J. Pearce", "author_profile_id": "81100242905", "affiliation": "Victoria University of Wellington, Wellington, New Zealand", "person_id": "P1223143", "email_address": "", "orcid_id": ""}, {"name": "James Noble", "author_profile_id": "81100588708", "affiliation": "Victoria University of Wellington, Wellington, New Zealand", "person_id": "P1223144", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1449764.1449766", "year": "2008", "article_id": "1449766", "conference": "OOPSLA", "title": "Caching and incrementalisation in the java query language", "url": "http://dl.acm.org/citation.cfm?id=1449766"}