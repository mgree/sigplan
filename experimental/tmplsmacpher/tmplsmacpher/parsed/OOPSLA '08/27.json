{"article_publication_date": "10-19-2008", "fulltext": "\n Ef.cient Local Type Inference Ben Bellamy Pavel Avgustinov Oege de Moor Damien Sereni Programming Tools \nGroup, University of Oxford, UK benjamin.bellamy@magd.ox.ac.uk, {pavel,oege,damien}@comlab.ox.ac.uk \nAbstract Inference of static types for local variables in Java bytecode is the .rst step of any serious \ntool that manipulates bytecode, be it for decompilation, transformation or analysis. It is important, \ntherefore, to perform that step as accurately and ef.ciently as possible. Previous work has sought to \ngive solutions with good worst-case complexity. We present a novel algorithm, which is optimised for \nthe common case rather than worst-case performance. It works by .rst .nding a set of minimal typings \nthat are valid for all assignments, and then checking whether these minimal typings satisfy all uses. \nUnlike previous algorithms, it does not explicitly build a data structure of type constraints, and it \nis easy to implement ef.ciently. We prove that the algorithm produces a typing that is both sound (obeying \nthe rules of the language) and as tight as possible. We then go on to present extensive experiments, \ncompar\u00ading the results of the new algorithm against the previously best known method. The experiments \ninclude bytecode that is generated in other ways than compilation of Java source. The new algorithm is \nalways faster, typically by a factor 6, but on some real benchmarks the gain is as high as a factor of \n92. Furthermore, whereas that previous method is sometimes suboptimal, our algorithm always returns a \ntightest possible type. We also discuss in detail how we handle primitive types, which is a dif.cult \nissue due to the discrepancy in their treatment between Java bytecode and Java source. For the application \nto decompilation, however, it is very important to handle this correctly. Categories and Subject Descriptors \nD.3.4 [Programming Languages]: Processors Compilers General Terms Experimentation, Languages, Performance \nKeywords type inference, program analysis Permission to make digital or hard copies of all or part of \nthis work for personal or classroom use is granted without fee provided that copies are not made or distributed \nfor pro.t or commercial advantage and that copies bear this notice and the full citation on the .rst \npage. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior \nspeci.c permission and/or a fee. OOPSLA 08, October 19 23, 2008, Nashville, Tennessee, USA. Copyright \nc &#38;#169; 2008 ACM 978-1-60558-215-3/08/10. . . $5.00 1. INTRODUCTION We discuss local type inference: \nthe problem of inferring static types for local variables in an object-oriented lan\u00adguage. We assume \nthe types of method signatures and .elds are given but type information for local variables is unavail\u00adable; \nthis is precisely the case with Java bytecode, in which method calls are fully resolved and .elds are \ntyped but lo\u00adcal variables have been compiled away into stack code. We then wish to compute types for \nlocal variables that are as tight as possible, in the sense that they are as low in the inheritance hierarchy \nas the typing rules allow. Motivation The motivating application is the conversion of Java bytecode to \na typed 3-address intermediate representa\u00adtion for analysis, transformation and decompilation [8, 15]. \nAt .rst it might seem trivial to infer types for locals from bytecode, but this is not so because in \nbytecode, stack loca\u00adtions are given types depending on the control .ow. By con\u00adtrast, we wish to infer \nstatic types that are not .ow-sensitive. Gagnon et al. [8] investigated this problem in depth, and pre\u00adsented \nan algorithm that has good worst-case complexity, which is at the heart of the popular Soot framework \n[26]. In certain cases, however, that algorithm performs quite badly. For example, when processing the \nabc compiler [1] with it\u00adself, 98% of the time is spent inferring types. Another application is the use \nof this algorithm in general type inference for object-oriented languages. This is a harder problem than \nthe one we are concerned with here, as the aim is to infer all types, including those of methods. There \nexists a vast literature on the subject, going back at least to Suzuki s paper on type inference for \nSmalltalk [25]. The key advance was the framework of Palsberg and Schwartzbach [20], on which most later \nworks are based. That framework uses in\u00adtraprocedural type inference, the problem considered here, as \na subroutine. Consequently, an improvement to that sim\u00adpler problem will also bene.t more general type \ninference. A third application is in language design. Popular lan\u00adguages like Visual Basic 9 allow a \nvery limited form of type inference for local variables, but only by inferring the type of the initialising \nexpression. A truly ef.cient algorithm for the problem addressed here would make it possible to relax \nthat restriction, giving the tightest possible type if one exists, and clear error messages when the \ntype is ambiguous. Contributions We shall present a novel algorithm for local type inference, which is \nbased on the following observation. Write t1 =t2 to indicate that t1 is a subtype of t2. State\u00adments \ninduce constraints on the type of local variables. In particular, an assignment v =E induces the constraint \n[e] =[v] where [e] is the type of the expression e and [v] is the type of the local variable v. In words, \nassignments induce lowerbounds on the types of variables. All other uses induce upperbounds of the form \n[v] =t for local variable v and some .xed type t. Therefore, to .nd minimal types for variables, it suf.ces \nto .rst process only assignments, and to .nd a minimal solution for those. Then, in a second stage, the \nalgorithm checks whether the minimal solution satis.es all the other constraints. Note that if a valid \ntyping exists, the minimal solution found in the .rst stage is such a typing. The above observation opens \nthe door towards a much simpler algorithm than those that have been considered be\u00adfore. Apart from being \nsimpler to implement, it is also vastly more ef.cient, dealing very well with common cases. For example, \nwhen we substitute our new algorithm for the one of [8], we see a 92-fold improvement in execution time \nof abc processing its own bytecode. On other benchmarks the gain is even greater, up to a factor of 575. \nNot only is the new algorithm faster in practice, it also guarantees a tightest possible result, whereas \nthe algorithm of [8] does not. The contributions of this paper are: a novel, fast algorithm for local \ntype inference;  a proof of its soundness and optimality;  a careful discussion of implementation decisions; \n extensive experiments demonstrating its performance.  Overview The structure of this paper is as follows. \nFirst, in Section 2, we discuss the algorithm in abstract form, and we prove its correctness. The proof \nthat the least .xpoint is a sound solution of the constraints induced by assignment statements is of \nparticular interest. Next, in Section 3, we discuss a number of implementation decisions, and we report \nperformance experiments for type inference in Section 4, us\u00ading the type hierarchy employed in Java bytecode. \nThat hier\u00adarchy is different from the Java source type hierarchy in the way primitive types are treated, \nand this issue is investigated in Section 5. We then proceed to present a further experi\u00admental evaluation \nof such source type inference in Section 6. As we have already mentioned, there exists a vast body of \nliterature on type inference and its variations, and we review the most pertinent previous works in Section \n7. We conclude in Section 8, and we point out opportunities for further work.  2. TYPE INFERENCE ALGORITHM \nThe key idea of the inference algorithm is to proceed in two phases. In the .rst phase, we only consider \nassignments where the left-hand side is a local variable, and we compute a minimal type for each local \nvariable by a simple .xpoint iteration. The second phase then only consists of checking the solution. \nWe .rst present the algorithm making the assumption that types form a lattice. That assumption is not \nsatis.ed for types in Java, so we show how to take the partial order of Java type conversions and construct \na lattice of typings. That construction in terms of so-called upwards-closed sets (which is standard) \nshows the algorithm is correct, but it would be expensive to implement in practice. We go on, therefore, \nto consider the representation of upwards-closed sets by small sets of representative elements. 2.1 Lattice \nalgorithm Let (T, =)be the lattice of types. For now we shall not de.ne the notion of types further, \nleaving a more detailed discus\u00adsion until we consider Java types. A sample type lattice is shown below: \n Figure 1. A type lattice A typing s : V .T is a .nite map from variables to types. The set of all typings \nis itself a lattice, with the pointwise order, given by s1 =s2 =.v :s1(v)=s2(v) The type evaluation \nmapping eval :((V .T)\u00d7E).T evaluates an expression with a given typing, to yield the type of the whole \nexpression. We require that the type system of the programming language is such that eval is monotonic: \ns1 =s2 implies eval(s1, e)=eval(s2, e) (1) Again, we do not specify eval further at this point, but \nwe shall discuss it in more detail in the next subsection, when we relate it to the Java type system. \nA typing s is said to be valid for an assignment instruc\u00adtion a of the form v := e whenever eval(s, e) \n= s(v) A typing s is said to be assignment-valid if it is valid for all assignment instructions a in \nthe program. A use of a variable v is a pair (v, t) which models the situation where v is used in a position \nwhere a variable of type t is expected. A typing s is said to be valid for a use (v, t) whenever s(v) \n= t A typing s is said to be use-valid if it is valid for all uses in the program. A typing s is valid \nif it is both assignment-valid and use-valid. Our aim is to construct a smallest valid typing. We shall \ndo that by constructing a smallest assignment-valid typing p, and then checking that that p is also use-valid. \nIf it is, then p is the smallest valid typing. Conversely, suppose that p' is a smallest valid typing. \nThen p' =p and (because p is the smallest assignment-valid typing) p =p'. Least .xpoint To compute the \nsmallest assignment-valid typing, de.ne f (s)(v)= {eval(s, e) |(v := e) .P } (2) In words, we take the \nleast upperbound of eval(s, e) over all assignments v := e in the program. We claim that s is a pre.x \npoint of f if and only if s is assignment valid. The proof is a simple calculation: f (s) =s ={pointwise \norder on typings (1)} .v : f (s)(v) =s(v) ={de.nition of f (2)} .v : {eval(s, e) |(v := e) .P }=s(v) \n={least upperbound} .v : .(v := e) .P : eval(s, e) =s(v) ={since every variable is assigned} .(v := e) \n.P : eval(s, e) =s(v) The smallest assignment-valid typing exists by virtue of the fact that f is monotonic, \nand so it has a least .xpoint by the Knaster-Tarski theorem [14]. In conclusion, we can compute the smallest \nassignment\u00advalid typing by computing a least .xpoint of f . In doing so, it would obviously be bene.cial \nto track dependencies be\u00adtween variables, and keep a worklist of assignments that may need to be revisited \nupon each iteration. We shall discuss those and related issues later in Section 3. It is worthwhile to \nnote, however, how at this abstract level our type algorithm is disarmingly simple.  2.2 Completing \nthe partial order of typings Write t1 <: t2 to indicate that a Java type t1 can be converted to Java \ntype t2. A full speci.cation of this partial order can be found in the Java Language Speci.cation [9]. \nThe above algorithm is cute, but at .rst sight it may appear useless in the context of Java, because \nthe partial order <: does not have least upperbounds. One reason for the absence of least upperbounds \nis multiple inheritance via interfaces in Java: two classes A and B may implement the same two interfaces \nI and J. Figure 2. Partial type hierarchy with interfaces Because there are no least upperbounds for \nJava types, that are no least upperbounds for typings either. Now one might think a suitable solution \nis to work with sets of types (as in e.g. [20]) but that would defeat the purpose of our inference algorithm: \nwe want to .nd a typing that is correct according to the rules of Java. To illustrate, consider the situation \nabove with class A and B that may implement the same two interfaces I and J, and take the program fragment \nx = new A(); y = new B(); x = y; y = x; Working with sets, the conclusion would be that both x and y \nare assigned type {I, J}. In terms of Java typings, we would however have to choose either type I for \nboth x and y, or type J,but I for x and J for y is not allowed. We need to track such dependencies during \ntype inference. For that reason we shall work with sets of typings, or more precisely upward-closed sets \nof typings. A set of typ\u00adings S is said to be upward-closed when s .S .s =s' implies s' .S Upward-closed \nsets of typings are ordered by S =S' = S' .S (3) The minimal elements of a set are those that have no \npredecessors. Formally, we have s .mnl(S) when for all s', the following equivalence holds: s' .S .s' \n=s = s' = s (4) It is easy to check that for upwards-closed S and S',we have S .S' = mnl(S) .S' (5) \n In words, to check that S is included in S', we only need to consider the minimal elements of S, as \nby virtue of upward\u00adclosure, all the other elements of S arethenalsoin S'. Our aim is now to de.ne a \ngeneralisation of the mapping f in the previous section (2), which we used to compute the minimal assignment-valid \ntyping. Instead, we shall be computing a least set of minimal assignment-valid typings. That set will \nbe least in the = order, so it is greatest in the . order, and therefore all assignment-valid typings \nwill be represented in the result. For brevity, de.ne the following predicate on pairs (s, s ') of typings: \nstep(s, s ')= .(v := e) .P : eval(s, e) =s '(v) It is easy to see that step(s, s) is a restatement of \nassignment\u00advalidity of s. Now de.ne a mapping F on upward-closed sets of typings as follows: F(S) = {s \n' |.s .S: step(s, s ') .s =s ' } (6) Note that the result is indeed upward-closed. It is worthwhile to \ncompare this de.nition of F to that of the mapping f in the previous section: it is a natural generalisation \nfor the situation where least upperbounds need not exist. We now claim that to compute the least (in \n=)set of minimal assignment-valid typings, all we need to do is to take the least .xpoint of F; the minimal \nelements of that least .xpoint are the desired typings. To prove that, we reason as follows: F(S) =S \n={de.nition of =(3)} S .F(S) ={inclusion of upward-closed sets (5)} mnl(S) .F(S) ={de.nition of F (6)} \n.s ' .mnl(S) : .s .S: step(s, s ') .s =s ' ={de.nition of minimal (4)} .s ' .mnl(S) : step(s ' ,s ') \nWe conclude that a simple generalisation of our original algorithm suf.ces to .nd the set of all assignment-valid \ntypings.  2.3 Representing upwards-closed sets In practice, representing each upwards-closed set of \ntypings explicitly is prohibitively expensive consider the fact that the computation of a least .xpoint \nwill start with the bottom upwards-closed set of typings, which by de.nition contains all possible typings. \nA seemingly obvious solution is to represent each upwards\u00adclosed set only by its minimal elements. While \nthat is cer\u00adtainly an improvement over keeping all elements, we have found that the requirement that \nthe results be minimal at every step imposes an unduly large penalty in terms of com\u00adparisons. We want \nto keep the sets small throughout the al\u00adgorithm execution, certainly, but there is no harm in having \na few non-minimal elements. In other words, we need to show how our algorithm can be implemented on \nsets of representative types, while giving the same answer (up to the operation of picking the minimal \nelements of a set), and without necessarily using minimal sets at each step. To make this intuition precise, \nwe de.ne a new preorder (a re.exive and transitive relation) on arbitrary (not necessarily upwards-closed) \nsets of typings: S jS' = up(S) =up(S') (7) where up(.) = {s ' |.s ..: s =s ' }.Thatis, jmimics our partial \norder =on upwards-closed sets of typings, by just working with sets of representative elements. We can \nin fact implement the test for jwithout the expensive computation of upwards-closed sets, for we have \nS jS' =.s ' .S' : .s .S: s =s ' (8) In words, for every typing in S', there exists a smaller repre\u00adsentative \nin S (cf. Figure 3). This preorder is very common in programming language semantics and program analysis, \nin particular since it is the order in the Smyth powerdo\u00admain [23]. Figure 3. Order on sets of typings: \nS jS' Now say that two sets S and S' are equivalent if each is at least as large as the other: S S' \n= S jS' .S' jS (9) Equivalent sets represent the same upwards-closed set (by equivalence (7)), so via \n(8) we now have an effective test for equality of upwards-closed sets, still just working with representative \nelements. As a special case, note that any set is equivalent to its minimal elements: S mnl(S) (10) \nOur aim, therefore, is to implement the above abstract al\u00adgorithm by keeping a set of typings S' that \nis equivalent to the set S the abstract algorithm would have computed in the same step. First we note \nthat by monotonicity of F on =,we have that F is monotonic on j also, and therefore F preserves equivalence \nof sets of typings. Furthermore, F(S) = {de.nition of F (6)}  {s ' |step(s, s ') . s =s ' } s.S {union \npreserves equivalence, (10)} mnl({s ' |step(s, s ') . s =s ' }) s.S In words, this shows that we can \nimplement F by selecting minimal typings after doing one pass over all assignment statements with a given \ntyping, making any updates to the typing as necessary. It remains to show how one could im\u00adplement the \noperation next(s)= mnl({s ' |step(s, s ') . s =s ' }) Clearly s ' should map each variable to a minimal \ntype satis\u00adfying the indicated predicate. Therefore, de.ne a new func\u00adtion lca on sets of Java types, \nsuch that lca(S) contains pre\u00adcisely the least common ancestors (i.e. supertypes) of the types in S. \nTo illustrate, with the hierarchy displayed in Fig\u00adure 1, we have lca({C, D})= {I, J}. With the de.nition \nof lca in hand, obviously we have next(s)= {s ' |.v : s '(v) .lca({s(e) |(v := e) .P})} In summary, we \nhave shown that when SS' , F(S) F '(S') (11) where F'(S')= {s ' |.s . S' : s ' . next(s) }. Writing lfp(.,f) \nfor the operator that returns a least .xpoint of f in preorder ., we conclude that mnl(lfp(=, F)) = mnl(lfp(j, \nF ')) (12) This shows how to implement our abstract algorithm on sets of representative elements, avoiding \nboth the expensive construction of upwards-closed sets of typings, and also avoiding the need to reduce \nto minimal elements every time the union operator is applied.  2.4 Second Phase We have now seen how \nto infer a minimal set of assignment\u00advalid typings S for a method. However, we are interested in inferring \nvalid types, i.e. they should be both assignment\u00advalid and use-valid. Suppose the method is typable (i.e. \nthere exists some valid typing), and let p be a minimal valid typing. By de.\u00adnition p is assignment-valid, \nand so s =p for some s . S, since all minimal assignment-valid typings are in S.But since variable uses \ninduce upper bounds on types, if p is use-valid then any smaller typing is also use-valid, and so s is \nvalid. By minimality of p, p = s which implies p = s, and we have shown that all minimal valid typings \nare con\u00adtained in S. Therefore, the second phase of our algorithm goes through S, discarding each element \nthat is not also use-valid.If after this pruning S contains only one type assignment, then this is the \noptimal valid typing. If S has several elements, then all of them are minimal, and we pick one non-deterministically. \nIf S is empty, then there exists no valid typing, and the algo\u00adrithm fails.  3. IMPLEMENTATION The \nprevious section presented our type inference algorithm at a high level of abstraction, and provided \nproofs of both soundness and optimality that is, we know that any in\u00adferred types follow the typing \nrules and, moreover, are as tight as possible. In practice, some care needs to be taken to ensure an \nim\u00adplementation remains ef.cient. In particular, as is usual in such cases, our .xpoint iteration makes \nuse of a worklist, so that iterations only revisit those statements that may in.u\u00adence the result. 3.1 \nFixpoint Iteration with a Worklist As in Section 2, for simplicity we will .rst assume that the type \nhierarchy is a lattice (that is, we do not account for multiple inheritance), and later generalise this \nto the full language. As shown in Section 2.1, in the simpler case we need only consider a single typing \nthat is repeatedly re.ned, rather than sets of typings. The data structure we use for the worklist is \na queued set it contains no duplicates, and elements can be taken out in the order in which they were \nput in. The al\u00adgorithm would be equally correct with other representations of the worklist, like sets \n(with non-deterministic order of re\u00adtrieval) or lists (which may contain duplicates), but a queued set \nleads to less work overall, as variables tend to be assigned textually before they are used (there may \nbe exceptions, due to jumps). We will also need information about dependencies be\u00adtween variables. Informally, \nthe type of a variable v1 depends (or, rather, may depend) on the type of v2 if v2 occurs on the right-hand \nside of some assignment to v1. We construct a map depends such that depends(v) is a set containing all \nassignments to some local with v on the right-hand side. Given that, our implementation proceeds as shown \nin Algorithm 1. In words, we start off with the bottom typing (Lines 1 and 2). Next, we iterate over \nthe assignments to local variables. For each assignment v := e, the typing is updated (on Line 7) to \nthe least common ancestor (in lattice terms, the least upperbound) of the type s(v) and the type of the \nright\u00adhand side under s,thatis eval(s, e). Should this induce a change in s (Line 8), all the assignments \nthat depend on v are queued for consideration in a later iteration (Lines 9 and 10). It is evident that \nthe above computes the same result as the abstract algorithm in Section 2.1, as taking the least common \nsupertype of a set of types is the same as taking the pairwise least common supertype of elements of \nthat set. Algorithm 1: Type inference algorithm for a type lattice 1 for every local variable v do \n2 s(v) ..; 3 worklist .set of all assignments to local variables; 4 while worklist is not empty do 5 \n(v := e) .head(worklist); 6 worklist .tail(worklist); 7 t .lca(s(v), eval(s, e)); 8 if t = s(v) then \n9 s(v) .t; 10 worklist .worklist + depends(v); 11 return s; To extend this algorithm to the general \ncase, we need to consider sets of typings, each with an associated worklist, and the .xpoint we are computing \nwill be such a set of typings. Write worklist (s) for the worklist of the typing s. Note that we will \nrepresent upwards-closed sets of typings by their minimal elements, as shown in Section 2.3. Algorithm \n2: General type inference algorithm 1 for every local variable v do 2 s(v) ..; 3 S .{s}; 4 worklist(s) \n.set of all assignments to local variables; 5 while for some s .S, worklist(s) = \u00d8do 6 Pick s .S where \nworklist(s) = \u00d8; 7 S .S \\{s}; 8 (v := e) .head(worklist(s)); 9 worklist(s) .tail(worklist(s)); 10 for \neach t in lca(s(v), eval(s, e)) do 11 if t = s(v) then 12 S .S .{s}; 13 else 14 s ' .s[v .t]; 15 worklist(s \n') . worklist(s)++ depends(v); 16 S .S .{s ' }; 17 return S; We proceed as shown in Algorithm 2, which \nit is worth\u00adwhile to examine in some detail. Again, we start with the bottom typing (Lines 1 and 2), \nand put that in our set of candidate typings (Line 3). The worklist for S initially con\u00adsists of all \nassignment statements (Line 4). The iteration now continues as long as there exists some non-empty worklist. \nLet us now look at the way iteration steps are performed a bit more closely. We pick a typing s that \nhas a non-empty 1 class CA { void f() {} } 2 class CB { void g() {} } 3 ... 4 void method () { 5 <untyped \n> x; 6 if (... ) { 7 x= new CA(); x.f(); 8 } else { 9 x= new CB(); x.g(); 10 } 11 x. toString () 12 \n} Figure 4. A method with no valid type for x worklist (Lines 6 and 7), and an assignment v := e from \nthat worklist (Lines 8 and 9). Then for each type t in the set of least common ancestors of s(v) and \neval(s, e), we check whether s needs to be updated (Line 11). If not, we just add s to our current set \nof candidate typings (Line 12). On the other hand, if an update is required, we create a new version \ns ' of s that maps v to t (Line 14), and expand the worklist of s ' accordingly (Line 15). Finally, we \nadd s ' to the set of candidate typings (Line 16). From this description, it becomes evident that our \nalgo\u00adrithm has a potential source of severe inef.ciency, namely the iteration in Lines 10 to 16, which \ncould multiply the size of the set of candidates each time it is executed. As we shall see shortly through \na series of experiments, that does not happen in practice because it is very rare for lca to return non-singleton \nresults. 3.2 Arrays So far we have only considered assignment statements of the form v := e where v \nis a local variable. There is also another case that must be handled in order to generate assignment\u00advalid \ntypings for Java: assignments to array references. In Jimple all array references take the form v[i] \nwhere v is alo\u00adcal variable, so we need to consider assignments statements of the form v[i]:= e. The \nrequired modi.cations to Algo\u00adrithms 1 and 2 are minor: in the case of such assignments we use t .lca(s(v), \neval(s, e)[]). Notice the [] notation, indicating that we take the lca with the array type whose el\u00adements \nare of the type of the expression e.If e has an array type already then we are taking the lca with a \nmultidimen\u00adsional array type. 3.3 Type Inference for Arbitrary Bytecode The above algorithm infers a \nset of minimal typings, which (by the proof we presented earlier in Section 2) are assignment\u00advalid. \nThere is no guarantee, however, that the checking phase (Section 2.4) is then going to succeed, even \nfor Java bytecode that is veri.able. Let us consider the reasons why no valid typing might exist. The \nproblem stems from the fact that the bytecode veri.er does a simple .ow analysis to estimate the type \nof stack locations at each program point; in particular, each stack location can have different types \nat different points, while we are concerned with inferring a single type for each variable that is valid \nthroughout the method. Consider the code snippet in Figure 4 (this example is due to Gagnon et al. [8]). \nAs far as the bytecode veri.er is concerned, on line 7 x has type CA, and on line 9 it has type CB, while \noutside the if statement it has type Object. However, none of these types work throughout the whole method. \nTo deal with such a case, we transform the method body into an equivalent form for which a valid typing \nexists. Of course such transformations are undesirable, and so we only apply them if the type inference \nalgorithm .nds no valid typing. Following [8], we have two transformation stages after the .rst type \ninference stage. The .rst is a partic\u00adular variable-splitting transformation at object creation sites \nthat allows inferring types in some previously problematic cases at the cost of introducing more Jimple \nvariables. In\u00addeed, our experiments con.rm the conjecture .rst voiced by Gagnon et al. stating that in \nthe vast majority of practical cases this stage is suf.cient to infer types; more details are given inSection6. \nStill, there are certain cases (e.g. Figure 4) which remain untypable. The third stage, therefore, starts \nwith assignment\u00advalid typings and introduces casts to make them use-valid. In the presence of several \npossibilities, the one that requires fewest casts is deemed preferable. Now, if it is the case that an \nassignment-valid typing exists, then the above two additional transformation steps are guaranteed to \n.nd some validly typed solution at least validly typed according to the type conversions allowed by \nJava bytecode.  4. EXPERIMENTAL EVALUATION Algorithm 2 for local type inference was implemented as \npart of the bytecode to Jimple pass of the Soot optimisation and decompilation framework [26]. For this \n.rst set of ex\u00adperiments we used the augmented value set hierarchy intro\u00adduced in Section 5, as opposed \nto the Java source type hierar\u00adchy. In particular implicit conversions are allowed between all integer \ntypes (boolean, int, byte, short and char). This is in fact not the case in the original Soot implementa\u00adtion \nof [8], and in Section 5 we show how to .x that. For the experiments, we chose a wide variety of bench\u00admarks, \ntotalling over 295K methods, from many different .elds. A description of each benchmark follows. rt is \nthe Sun Java 1.5 runtime library. The main interest of this benchmark is its size (108K methods), and \nthe fact that it exercises many features of the Java language. tools is the Sun JDK 1.5 tools libray \nincluding javac. Again we chose this benchmark because it is an interest\u00ad ing piece of Java, albeit of \nmodest size (14K methods.) abc-complete is version 1.2.1 of the AspectBench Com\u00adpiler for the AspectJ \nprogramming language including Soot and Polyglot. This is interesting as a benchmark be\u00adcause it contains \nmany large, generated methods. jython is version 2.2.1 of Jython, a Python implementation written in \nJava. This is chosen as a mid-sized example of typical Java code. groovy is version 1.5.4 of the compiler \nfor the Groovy pro\u00adgramming language. Again this is written in Java and pro\u00advides another benchmark containing \ntypical Java code. gant is version 1.1.1 of the Gant build system, similar to Ant but compiled to bytecode \nby Groovy instead of javac. This is an important experi ment because the algorithm is designed to handle \nall valid bytecode, not just bytecode generated from Java source. kawa is version 1.9.1 of the Kawa \ncompiler for the Scheme programming language. Here part of the jar is boot\u00adstrapped, again giving bytecode \nsequences that would not normally occur as the output of javac. scala is version 2.7.0 of the Scala compiler \nand runtime library, both of which are written in Scala, and compiled by the Scala compiler (again, instead \nof javac). cso is a concurrency library, loosely inspired by the CSP calculus, written by Bernard Sufrin \nin Scala. This bench\u00admark is also compiled by Scala instead of javac. jigsaw is version 2.2.6 of the \nW3C s Jigsaw web server implementation, and this is included as a typical web application written in \nJava. jedit is version 1.4pre13 of the jEdit text editor, an exam\u00adple of an interactive application written \nin Java. bluej is version 2.2.1 of the BlueJ IDE for the Java pro\u00adgramming language, again chosen as \nan interactive ap\u00adplication. java3d is version 1.5.1 of the Java 3D API. As we shall see later in this \npaper, (numerical) primitive types can pose a challenge for type inference algorithms on Java bytecode, \nand this is a potential example of that phenomenon. jgf is version 2.0 of the Java Grande Forum Sequential \nBenchmark Suite, again chosen for its use of primitive type operations. havoc is a contrived example \nof Java bytecode which takes unusually long for the JVM to verify [6], in fact it was designed to be \na denial-of-service attack on the Java bytecode veri.er. For each experiment, the bytecode to Jimple \nfeature of Soot was executed on each bytecode class .le in the bench\u00admark. Table 1 presents the total \ntime spent inferring types under our implementation of Algorithm 2 and, for compar\u00adison, under the original \nSoot algorithm of Gagnon et al. The separate integer typing stage of the original Soot al\u00adgorithm was \ncarefully disabled. Our implementation is, in fact, doing slightly more work (in .nding an assignment\u00advalid \ninteger typing under the augmented value set hierar\u00adchy discussed later in Section 5) than necessary \nto provide a precise comparison. Each experiment was run on the same quad-core Intel Xeon 3.2GHz machine \nwith 4GB RAM run\u00adning Linux 2.6.8 SMP. Each benchmark was tested 5 times independently, and the middle \n3 results (selected indepen\u00addently between the two algorithms compared) were aver\u00adaged. From these two \ntimes we determine the relative im\u00adprovement. Also recorded are the number of methods tested in each \nbenchmark and the number of methods for which Algorithm 2 .nds a tighter typing (it never gives a weaker \ntyping). For each benchmark we present the mean number of minimal candidate typings generated by Algorithm \n2, which is representative of the extent of multiple-inheritance. Fi\u00adnally we show the the number of \nmethods typed at each of the three stages of code transformations: Stage 1 A valid typing exists for \nthe original method so no transformation is required. Stage 2 A variable-splitting transformation is \nused at object creation sites. Stage 3 The least number of safe casts are inserted where required. Note \nthat the original Soot algorithm also uses the three\u00adstage technique, and the same stage is always used \nin both algorithms. Algorithm 2 is typically around 6 times faster in the benchmarks tested, however \ntwo cases stand out where a dramatic improvement is found. On closer examination we notice that both \nabc-complete.jar and havoc.jar con\u00adtain several huge (>9000 Jimple statements) methods. It is interesting, \ntherefore, to measure the performance of the re\u00adspective algorithms as a function of the size of the \nmethod bodies being processed. These results are plotted as Figure 5. Note that the vertical axis has \na cube-root scale as noted in [8], the asymptotic complexity of Soot s type inference algorithm is cubic, \nand the plot shows that this is indeed at\u00adtained in practice. For clarity, the data points corresponding \nto Algorithm 2 are shown on their own in Figure 6; the scale here is linear. It is easy to see that the \noverwhelming trend in the common case is linear; there are a few outliers (which are still low compared \nto the other algorithm), and they usually corre\u00adspond to cases where multiple inheritance induced multiple \ncandidate typings. Table 1 validates our earlier remark that multiple inheri\u00adtance is extremely rare \nin real-world programs: the seventh column lists the mean number of minimal typings for each method, \nand in the vast majority of cases this is 1. Note that typically it is non-javac sources (kawa, scala, \ncso)that show an abnormally high value, and indeed this seems cor\u00adrelated with a comparatively lower \nrelative improvement. The numbers also show that almost all methods can be typed by applying only stage \n1 of the algorithm, and the ma\u00adjority of methods requiring stage 2 stem from scala.In to\u00adtal, in our \nbenchmarks (which were chosen to present chal\u00adlenges to a type inference algorithm) only 0.1% of methods \nrequired the second stage, and 0.02% the third. 5. INFERRING JAVA SOURCE TYPES So far, we have operated \non the assumption that the type system used is that of Java bytecode. If the purpose is just to have \na typed intermediate representation of bytecode, with the aim of analysis or optimisation, that is just \nthe right choice, and the algorithm presented so far would be suf.\u00adcient. However, the framework in which \nwe are conducting our experiments (Soot) has another purpose, namely decom\u00adpilation of bytecode to Java \nsource. For that application, it is desirable that the inferred types are correct with respect to the \ntype system of the Java source language. In respect to integers, the legal widening conversions be\u00adtween \nprimitive types in Java source (which are identical to those in Jimple source) are rather different to \nJava bytecode (cf. Figure 7). The narrowing conversions allowed in Jimple source are identical to Java \nsource, with one essential excep\u00adtion: Jimple allows casts between boolean and any integer type, while \nJava does not allow boolean to be cast to or from any other type. In bytecode, all integer types of less \nthan 4 bytes are represented as int, and the only difference is the range of values they can be used \ninterchangeably. The larger primitive types are separate, but there are instructions for converting between \nthem (which at the Jimple and at the Java level level look like casts). The reason this causes problems \nis that it is possible to have veri.able bytecode without an assignment-valid typing with respect to \nthe source type hierarchy. Consider the fol\u00adlowing sequence of statements, which is perfectly valid in \nbytecode: x=(a < b); x = 2;. The .rst statement assigns a boolean to x, the second a value that cannot \nbe a boolean,and since neither is a supertype of the other there is no typing that makes both assignments \nwork. Integer type inference is further complicated by the fact that it is not clear how to de.ne the \neval function with this hierarchy, as in x=0;, the right-hand side could have the types boolean, byte \nor char (or, of course, their supertypes). Thus, the fact that Jimple uses the source type hierarchy \n(for the sake of decompilation) works against us here it would be much more convenient if we could simply \ninfer int whenever such an integer type is expected! To address this issue, we consider an augmented \ntype hi\u00aderarchy (called the value set hierarchy) for the integer types, as shown in Figure 8. Essentially, \nwe introduce three vir\u00adtual types (called value set types) based on the value ranges of integer constants: \n[0..1], [0..127] and [0..32676]. These don t correspond to real source types, but allow us to defer the \ndecision of whether, for example, the integer con\u00ad Benchmark # Methods Old Time (s) New Time (s) Improvement \n# Tighter Mean # Cndts. # Stg. 1 # Stg. 2 # Stg. 3 rt 107792 84.48 10.77 7.84x 39 1.00032 107681 77 34 \ntools 14180 13.07 2.37 5.52x 5 1.00014 14160 17 3 abc-complete 33866 480.28 4.37 109.88x 52 1.00027 33865 \n1 0 jython 9192 6.67 1.25 5.35x 0 1.00000 9187 5 0 groovy 13799 10.12 1.92 5.27x 7 1.00087 13778 4 17 \ngant 707 1.75 0.44 4.01x 0 1.00000 702 3 2 kawa 9226 7.70 1.58 4.88x 25 1.00618 9195 31 0 scala 65161 \n37.66 5.36 7.03x 117 1.00453 64865 296 0 cso 2395 2.20 0.51 4.34x 6 1.00167 2392 3 0 jigsaw 13577 11.50 \n1.84 6.24x 1 1.00007 13571 6 0 jedit 5980 5.74 1.09 5.26x 12 1.00318 5969 0 11 bluej 5690 5.37 0.98 5.48x \n0 1.00053 5690 0 0 java3d 13453 17.86 2.46 7.26x 5 1.00037 13453 0 0 jgf 557 3.61 0.48 7.59x 0 1.00000 \n557 0 0 havoc 23 198.53 0.46 428.17x 0 1.00000 23 0 0 Total 295598 886.53 35.87 24.72x 269 295088 443 \n67 Table 1. Performance comparison between both algorithms under the augmented value set hierarchy 91125Gagnon \net al. Algorithm 2 64000 42875 27000 15625 8000 3375 1000 125 0 0 2000 4000 6000 8000 10000 12000 14000 \n16000 Method Length (Stmt Count) Figure 5. Comparison of the two algorithms: runtime against method size; \ncube-root plot. stant 0 is a boolean, byte or char temporarily. We also make Thus, we can now give a \nprocedure for inferring integer the observation that the value set type [0..1] actually co-types: Generate \nthe least typings that are assignment-valid incides with boolean, so we combine the two, ensuring that \nunder the value set hierarchy (such typings need not be booleans are assignable to other integer types. \nWith the value assignment-valid in Jimple without inserting casts, but any set hierarchy, any value that \ncan be assigned safely to a vari-such casts are guaranteed to preserve semantics since they able of type \nt can also always be assigned to any ancestor move up in the value set hierarchy). Resultant typings \nmay type of t. contain value set types which we must promote to concrete Duration (ms) 350 Algorithm \n2 Duration (ms) 300 250 200 150 100 50 0 0 2000 4000 6000 8000 10000 12000 14000 16000 Method Length \n(Stmt Count) Figure 6. Our algorithm: runtime against method size. Figure 7. The type hierarchy in Java \nbytecode and Java source Figure 8. The augmented value set hierarchy types and, again, insert casts \nwhere required under the source type hierarchy. This gives a valid Jimple typing. Algorithm 3: Type promotion \nalgorithm Input: A typing s possibly containing value set types 1 for each variable use (v, t) where \ns(v) = int do 2 if s(v) = t then no valid typing exists; fail; 3 if s(v) is a value set type then 4 s(v) \n. the least t ' = s(v) such that all ancestors of t ' are comparable to t; 5 for each local v where \ns(v) is a value set type do 6 switch s(v) do 7 case [0..1] 8 s(t) . boolean 9 case [0..127] 10 s(v) \n. byte 11 case [0..32767] 12 s(v) . char 13 return s; Concretely, the .rst stage of type inference \nruns the algo\u00adrithm described in Section 2 while lumping all small integer types together into int. As \nobserved above, we could stop at this point if the purpose of type inference is optimisation, and hence \nbytecode types suf.ce. The second stage revis\u00adits all variables typed as int, and applies the main algorithm \nin conjunction with the value set hierarchy (treating non-int types as .xed). We then proceed to type \npromotion,as shown in Algorithm 3. We consider in turn each use of an integer-typed variable (lines 1 \n4); if it is incompatible with the current typing, then no valid typing can exist and the algorithm fails. \nIf the variable has a value set type, it is promoted to the least type such that all ancestors of that \ntype are comparable to the use (and so can be converted, if necessary). For example, suppose we have \na variable v typed as [0..1].If we .nd a use (v, boolean) then we type v as boolean.If we .nd a use (v, \nint) then we type v as [0..127],and if we .nd a use (v, byte) then we type v as byte. We can note from \nthe augmented hierarchy chosen that this step will never omit a potential concrete type from consideration. \n Some variables may still have value set types, so we pro\u00admote those to a suitable concrete type (lines \n5 12) and thus obtain a valid typing. By suitable we mean it can be veri\u00ad.ed that this step will never \nintroduce invalid assignments or uses, whatever the program or typing. If the type promotion fails, then \nwe roll back to the initial assignment-valid typing with value set types and proceed to the second stage \nof integer typing (cf. Algorithm 4). The idea is that each variable currently typed with a value set \ntype actually needs to be typed with a concrete Java type, and so before checking uses we generate a \nset of candidates consisting of every valid combination of least concrete types. Applying use constraints \nmay introduce casts, so we simply choose the candidate that requires fewest casts. A subtlety lies in \nthe fact that each time we promote a value set type to a concrete type, we must re-apply Algorithm 2 \nto propagate assignment constraints (although we can restrict it to only integer-typed variables, and \nset the initial worklist to the dependencies of the promoted variable). It is clear that the set of candidates \ncan grow exponen\u00adtially in the number of integer-typed locals. Luckily, the type promotion algorithm \n(Algorithm 3) suf.ces in almost all cases. If we were to drop the requirement for inserting the fewest \ncasts, which is the approach taken by Gagnon et al. [8] in the second stage of their integer typing algo\u00adrithm, \nthen this exponential-time algorithm could be reduced to simply applying a single pre-de.ned promotion \nto all value set types. A suitable such promotion would be the one used at the end of the type promotion \nalgorithm: [0..1] . boolean, [0..127] . byte and [0..32767] . short. Let us examine Algorithm 4 in some \ndetail. We initialise the set of candidates to contain our assignment-valid typ\u00ading (line 1), and then \niterate until each candidate only uses concrete types. While there exists some typing s giving a value \nset type to v, we remove it from the set of candidates, and create new typings for each least concrete \nsupertype of v (lines 3 11). Algorithm 2 is run on each typing contained in this way, potentially raising \nthe types of other variables, and the results are added to the candidate set (lines 12-13). As mentioned \nabove, the resulting set of candidates is then checked against uses, introducing casts as necessary, \nand the typing with fewest casts is returned. Note that such casts are usually acceptable, since they \neither move up the value set hierarchy (thus preserving values), or, if the casts are narrowing, then \nthe semantics of the underlying byte\u00adcode must have been dubious to begin with. As a concrete example, \nconsider the following code fragment: Algorithm 4: Second stage integer typing Input: A typing s possibly \ncontaining value set types 1 candidates .{s}; 2 while Some s .candidates uses value set types do same \nnumbers for the algorithm of Gagnon et al..The two next columns show the improvement of our integer type \ninference method over the one in Soot, and the improve\u00adment for the complete type inference process (including \ninteger typing Section 4 only considered type infer\u00adence for the bytecode type hierarchy). The .nal \ncolumn lists for how many methods the new algorithm found a tighter typing (of course, it can never .nd \na less tight typ\u00ading, as it is optimal). The bottom part of Table 2 gives an indication of the use of \ndifferent stages of the two type inference frameworks. Both algorithms use two stages to infer integer \ntypes, but these two stages are not trivially related. Let us now examine these numbers in some detail. \nFirst, in both the Soot algorithm and the new algorithm, the cost of dealing with integer types is considerable, \naccounting for 30.5% and 39.9% of the total time spent in type inference. In fact, for some of our benchmarks, \nthe percentage is as high as 47%, so almost half the time of type inference is spent just on getting \nthe integer types right. This underlines the importance of only using the source hierarchy for primitive \ntypes when necessary. The Integer Improvement column demonstrates that the new method of dealing with \ninteger types performs better than the one in Soot in all cases, de\u00adspite the fact that it .nds a tightest \npossible typing, whereas Soot s algorithm does not. The latter point is illustrated by the .nal column, \nwhich shows that a small but signi.cant fraction of the methods gets a suboptimal typing in Soot. In \nterms of performance, our integer typing stage is 16x faster than the Soot version, but this is mostly \ndue to the (contrived) havoc benchmark. The total improvement hov\u00aders between 5 and 6 times, with two \nextremely high ratios that push the overall runtime down 21-fold. The least im\u00adproved benchmark is gant, \nwhere we only gain a factor of 2.82. Moving to the bottom table, it is clear that type promotion (Algorithm \n3) is almost always effective in .nding the types required, and in fact only the Kawa and Scala benchmarks \nrequire the use of Algorithm 4. Similarly the second stage of Soot s integer typing is invoked only for \nKawa, albeit less often. It is noteworthy that this concerns bytecode that is not generated by a Java \ncompiler, but instead directly from Scheme. The reader may wonder whether it would not be possi\u00adble to \nforego the type promotion step, and instead always directly use 4. Indeed, in theory that will yield \ncorrect re\u00adsults, but in practice that can give an exponential blowup in the number of typings that need \nto be considered. In fact, we conducted that experiment, and found that only two very small benchmarks \ncso and jgf run to completion if type promotion is omitted. We conclude, therefore, that type pro\u00admotion \nis the key to ef.cient yet optimal handling of integer types under the Java source type hierarchy. As \nmentioned in Section 5, the problem of exponential blowup could be alle\u00ad 3 4 5 6 7 8 9 10 11 12 13 Remove \ns .candidates using a value set type; Pick v where s(v) is a value set type; switch s(v) do case [0..1] \nnew .{s[v .boolean],s[v .byte], s[v .char]}; case [0..127] new .{s[v .byte],s[v .char]}; case [0..32767] \n new .{s[v .char],s[v .short]}; new .the result of Algorithm 2 on new; candidates .candidates .new; \n 14 return candidates; 1 boolean f() { <untyped > x = 200; g(x ); }2 void g( byte y) { ... } The algorithm \nwill transform the call g(x) into g((byte)x), equivalent to the i2b bytecode instruction that must have \nbeen present. It is interesting to note the distinct similarities between this second stage algorithm \nand the handling of multiple inheritance in Algorithm 2. The reason we chose to separate it out was simply \nthe dramatic increase of performance given in almost all cases by the type promotion algorithm. While \nmultiple inheritance in reference types is relatively rare in practice, [0..1]-valued integer constants \nare very common in bytecode indeed, every conditional jump uses such a constant. The next section shows \nthe performance cost of inferring precise integer types.   6. EXPERIMENTS WITH SOURCE TYPES We repeated \nthe experiments of Section 4 to determine the effect of integer type inference (under the Java source \nhier\u00adarchy) on the performance of the algorithm. Once again, this part of the algorithm is not strictly \nnecessary if all we want to do is, say, class hierarchy analysis. On the other hand, it is a crucial \ncomponent of a decompiler. It is interesting, therefore, to determine the exact cost of this additional \nfunc\u00adtionality, so it can be judiciously invoked. The results of our experiments are displayed in the \ntwo parts of Table 2: The top part in Table 2 presents the average times spent inferring integer types \nin our algorithm with type pro\u00admotion, and the percentage that represents of the total time for type \nassignments. The next two columns give the  Soot Soot New New Integer Total Benchmark Time (s) %total \nTime (s) %total Imprv. Imprv. # Tighter rt 22.88 21.31 8.98 45.46 2.55x 5.44x 1061 tools 5.51 29.66 1.04 \n30.50 5.30x 5.45x 165 abc-complete 89.98 15.78 2.90 39.88 31.03x 78.43x 177 jython 3.59 35.02 1.13 47.50 \n3.19x 4.32x 45 groovy 3.84 27.50 1.39 41.94 2.77x 4.22x 386 gant 0.59 25.06 0.39 47.30 1.49x 2.82x 70 \nkawa 3.42 30.74 0.78 33.01 4.40x 4.72x 198 scala 6.28 14.30 2.56 32.30 2.46x 5.55x 190 cso 0.43 16.23 \n0.31 38.02 1.37x 3.21x 6 jigsaw 3.20 21.77 1.07 36.62 3.00x 5.05x 131 jedit 2.17 27.43 0.56 34.09 3.84x \n4.78x 170 bluej 1.12 17.28 0.46 31.88 2.44x 4.51x 78 java3d 5.60 23.89 1.58 39.15 3.54x 5.80x 264 jgf \n0.81 18.36 0.24 33.90 3.33x 6.14x 16 havoc 239.55 54.68 0.41 47.03 581.90x 500.47x 0 Total 388.98 30.50 \n23.79 39.88 16.35x 21.38x 2957  Benchmark # Type Prom. # Our Stg. 2 # Soot Stg. 1 # Soot Stg. 2 rt \n107792 0 107792 0 tools 14180 0 14180 0 abc-complete 33866 0 33866 0 jython 9192 0 9192 0 groovy 13799 \n0 13799 0 gant 707 0 707 0 kawa 9202 24 9223 3 scala 65160 1 65161 0 cso 2395 0 2395 0 jigsaw 13577 0 \n13577 0 jedit 5980 0 5980 0 bluej 5690 0 5690 0 java3d 13453 0 13453 0 jgf 557 0 557 0 havoc 23 0 23 \n0 Total 295573 25 295595 3 Table 2. Performance comparison for integer typing under the Java source \nhierarchy hierarchy viated by relaxing the requirement that we insert as few casts as possible in the \ncase of integers (this is the approach taken by Gagnon et al.). At the beginning of this paper we stressed \nthat the new algorithm is designed to be ef.cient for the common case.It may now appear suspicious that \naccording to the numbers in Table 2, it is always better. This is however not the case, it is just that \nall these benchmarks are whole jars, each consisting of many methods. There are individual methods where \nour method performs worse than the one in Soot, but that effect is drowned out by the better performance \non most other methods. All these benchmarks, and scripts for reproducing our experiments, can be downloaded \nfrom [5].  7. RELATED WORK There exists a rich literature on the topic of type inference in object-oriented \nlanguages, [2 4, 7, 10 13, 15, 16, 18 21, 24, 25] to name but a few. Almost all of these take the notion \nof type constraints as their starting point. What sets the present paper apart is the idea to .rst consider \nonly the constraints that induce a lowerbound on typing, and .nd a minimal solution for that restricted \nset of constraints. When that minimal solution is found, it remains to do a check of compatibility with \nthe other constraints. We now make a more detailed comparison between the results of this paper and \nthree previous works, namely the algorithm of Gagnon, Hendren and Marceau, the framework of Knoblock \nand Rehof, and that of Agesen, Palsberg and Schwartzbach. Gagnon, Hendren and Marceau The original motivation \nfor this work was to try to improve on the performance of the algorithm proposed by Gagnon et al. [8]. \nGagnon s work was a milestone in object-oriented type inference, providing the inference algorithm at \nthe foundation of the widely used Soot framework. Our entire understanding of the problem was shaped \nby [8], and indeed we have adopted its frame\u00adwork of applying transformations to deal with bytecode that \nis veri.able yet cannot be typed statically. However, our ap\u00adproach differs signi.cantly. The type inference \nphase of [8] works by constructing a graph of type constraints. This graph has two kinds of node, namely \nhard ones (which represent explicit types) and soft ones (representing type variables). An edge a . b \nmeans a = b in our terms. The construction of this graph is, in itself, quite expen\u00adsive. It contains \nall type constraints induced by Jimple in\u00adstructions. An important difference with our algorithm is that \nin .rst instance we only consider constraints that derive from assignments; and even those are not explicitly \nrepresented by a datastructure. Solving the constraints means transforming the graph by merging soft \nnodes with hard nodes. Such a merging step is equivalent to inferring a type for a local variable. Merging \ncan be done in the following three ways merge all elements of a connected component in the graph: a = \nb . b = a . a = b merge primitive types; if t is a primitive type then a = t . a = t  merge soft nodes \nthat have only a single incoming edge (say from p) with p.  In addition to these merging steps, during \nthe solution pro\u00adcess one may remove transitive edges that are implied by others: if we have edges a \n. b . c, an edge a . c is redundant. The above solution process is sound: when it succeeds, the result \nis a valid typing. It is however not optimal in the sense that there may exist a strictly smaller typing \nthat is also valid. Some heuristics are applied, in particular in the choice of single-parent constraints \nto merge, to improve pre\u00adcision. These heuristics contrast sharply with the algorithm presented here, \nwhere there is a guarantee of optimality. In Section 6, we demonstrated that while it is rare for the \nalgo\u00adrithm of [8] to return suboptimal results, it does happen in practice. It is sometimes suboptimal \nfor reference types, but more often for primitive types. There is a price to pay for that optimality \nguarantee in our algorithm, however, and that is in the worst-case complexity. The algorithm of Gagnon \net al. is clearly polynomial: the number of constraints is polynomial, and each step of the solution \nprocess is polynomial. By contrast, as we have indicated in Section 2, our algorithm can take exponential \ntime. Indeed, in [8], it is argued that the problem of .nding an optimal typing is NP-hard. However, \nas is argued there, for the type hierarchies found in practice, the exponential behaviour does not occur. \nThe experiments in Section 6 con.rm that observation. Overall, our experiments in Section 6 also showed \nthat the new algorithm presented here outperforms that of [8]. Furthermore, it is apparent from Figure \n6 that the running time of the algorithm of Gagnon et al. is in practice cubic in the length of the method, \nwhereas ours is linear. From the above discussion, the reasons for that performance dif\u00adference are clear: \nour algorithm ellides the construction of a constraint graph. While the solution process of [8] is al\u00adways \nquite costly, requiring the identi.cation of strongly connected components, in our algorithm the most \ncommon case is two iterations of the .xpoint computation in the .rst stage. Knoblock and Rehof A very \nthorough study of the prob\u00adlem of reconstructing types for local variables in Java byte\u00adcode was conducted \nby Knoblock and Rehof [15]. Like our\u00adselves, they start with the observation that the problem is easily \nsolvable if types form a lattice. They then go on to ob\u00adserve that there exists a smallest lattice in \nwhich the original type order is embedded, namely the Dedekind-MacNeille completion. Where the type inference \nalgorithm .nds a type that is not represented in the original program, a new type de.nition is generated. \nThis is quite different from the framework of Gagnon [8] where the code is sometimes transformed to \nmake it typable, in the last resort by introducing casts. In [8] and in our setting, the type hierarchy \nitself is never modi.ed. We believe that introducing new types is too drastic a structural change to \nthe program to be allowed by an analysis and transformation framework, and that is de.nitely the case \nwhen used in decompilation. The type elaboration algorithm has some similarities with that of [8] as \nwell as ours, and it consists of the following steps: 1. First, all type constraints are collected. As \nemphasised earlier, we avoid the explicit representation of con\u00adstraints, instead choosing to generate \nthem on-the-.y from instructions. 2. Next, the set of type constraints is closed to take account of \narray types, again similar to [8]. In our setting, array types are treated in virtually the same way \nas other ref\u00aderence types (we have commented on our array-speci.c considerations in Section 3.) 3. The \nstrongly connected components in the constraint set are collapsed, as in [8]. 4. The new elements of \nthe type hierarchy are introduced. This has no direct equivalent in our algorithm or that of [8], although \nat the level of primitive types, we do introduce a few .ctitious types in the augmented value\u00adset hierarchy \nof Figure 8. 5. The lattice algorithm is run to .nd a minimal typing. This is similar to Algorithm 1, \nthe simple algorithm we started out with. 6. The solution is applied , possibly introducing unsafe \nnarrowing conversions between primitive types. Obvi\u00adously such unsafe narrowing conversions are undesirable, \nand should be avoided. While this step is dismissed as  an afterthought in [15], it is a non-trivial \ncontribution of the present paper to solve it carefully, as detailed in Sec\u00adtion 5. A small unpleasant \nissue is that sometimes introducing new types is not permissible according to the Java type system, because \nmultiple inheritance between classes is not allowed. In the rare cases where the algorithm encounters \nsuch problems, it resorts to inferring the type Object and inserting casts. Knoblock and Rehof report \ngood experimental results for their algorithm, showing linear time growth of execution time against sizes \nof method bodies; in view of Figure 6 that compares very well against the algorithm of Gagnon et al.. \nThe experiments in [15] are however rather less comprehen\u00adsive than those reported here (and those in \n[8]), as they tested only 22,300 methods, against the 295,598 that we have ex\u00adperimented with. They do \nnot report on tests that process bytecode that was not generated from Java source, which in our experience \n(and that of [8]) is crucial to test correctness, especially for the handling of primitive types. Furthermore \nthere is no comparison in [15] of the quality of the typings against another algorithm, as we have done \nwith [8] there are many subtle issues that an implementation must handle, and it is very hard to get \nall the details right without at least one other algorithm to compare against. Unfortunately there is \nno publicly available implementation of [15], to compare its performance against the algorithm of [8], \nand the one pre\u00adsented here. In view of the complexity of the datastructures involved, it seems very \nunlikely, however, that it would out\u00adperform the very simple methods considered here. Further\u00admore, in \n[15], runtimes of up to 18s per method are reported for a method of 200 KB that is the result of a parser \ngen\u00aderator. Our algorithm processes methods of similar size and origin in 0.16s, which is much faster \neven allowing for a 10\u00adfold speed increase in processors. We remarked earlier in Section 2.2 that the \nuse of sets of types, while predominant in the literature on object-oriented type inference, are inherently \nimprecise for certain examples where two types have multiple minimal common ancestors. In the worst case, \nthis worsens the time complexity of our algorithm, but as we have shown, in practice the price for precision \nis not prohibitive. Palsberg and Schwartzbach In a seminal paper [20], Pals\u00adberg and Schwartzbach laid \nthe foundation for most subse\u00adquent work on type inference for object-oriented programs. A year later, \nthey followed it up with various improvements (in particular to deal with collection types), and an ef.cient \nimplementation [18]. The problem considered by Palsberg and Schwartzbach is more general and harder than \nthe one considered here: given a program with no type annotations, infer the types of local variables \nand method signatures. Nevertheless, it is possible to make some observations about the connections between \ntheir work and the present paper. The notion of types proposed in [20] is just a set of classes. However, \nprior to the type inference process, the inheritance hierarchy is expanded by augmenting each class with \nall the members it inherits from its supertypes, and making corresponding changes to statements in the \ncode. As noted in [20], this .attening can result in a quadratic increase in the program size. When presenting \nour algorithm, we already observed that using sets of types is not adequate because our aim is to ob\u00adtain \ntypings that are valid according to the Java type rules. Recall, however, that we did use upward-closed \nsets of typ\u00adings. In fact, .attening the class hierarchy as Palsberg and Schwartzbach do corresponds \nto using upwards-closed sets of types: S =T = up(S).up(T) where up(X)={t |.s .X :s =t }. In our implementa\u00adtion, \nwe used small sets of representative elements to com\u00adpute with upwards-closed sets. As a consequence \nwe do not pay the cost associated with the expansion of the hierarchy in [20], which was again employed \nin the implementation paper [18]. Palsberg and Schwartzbach construct a graph representa\u00adtion of type \nconstraints, named the trace graph. The nodes of the trace graph are methods, and the edges represent \npo\u00adtential method invocations. Each node is decorated with a set of local constraints, which are precisely \nthe constraints considered in the present paper. Additional constraints are attached on edges: these \nare different in nature from the sim\u00adple type inequalities found as local constraints, instead being \nHorn clauses, relating assumptions about method arguments to method results. Viewed in this light, it \nbecomes clear that the algorithm we have presented here could be employed as a subroutine in a more general \ntype constraint solver, doing the intrapro\u00adcedural analysis required to solve local constraints. Indeed, \nas described in [18], one could construct the trace graph on demand, and whenever a new node is visited, \nwe sim\u00adply solve its local constraints with the new algorithm pre\u00adsented here. In [4], Agesen, Palsberg \nand Schwartzbach ex\u00adtend their approach to deal with dynamic and multiple in\u00adheritance. While the present \npaper has addressed multiple inheritance, we have not considered dynamic inheritance. The algorithm of \nGagnon et al. [8] is in fact more simi\u00adlar to that of Agesen, Palsberg and Schwartzbach than to our own, \nas it also operates on an explicitly constructed graph of constraints. The worst-case time complexity \nof our al\u00adgorithm is worse than that of the constraints-based type in\u00adference algorithms. The complexity \nblow-up occurs because we maintain sets of typings, where a typing maps each vari\u00adable to one type. Consequently, \nwhen (due to multiple inher\u00aditance) a variable x is given m types, and y is given n types, there are \nm \u00d7n typings maintained in our algorithm. This is not a problem in practice because while multiple inheritance \nis allowed in Java, its use is relatively rare. It is fair, there\u00adfore, to classify our approach to type \ninference as optimising for the common case rather than the worst case. It is natural to wonder whether \nthe principal idea under\u00adlying the present paper (process lowerbounds .rst to ob\u00adtain minimal solution, \nthen check upperbounds) can be ap\u00adplied to other constraint-based program analyses. While this seems \nlikely, we have not yet investigated that question in any depth.  8. CONCLUSIONS AND FUTURE WORK We \nhave presented an algorithm for local type inference, and measured its performance in inferring types \nfor local vari\u00adables in Java bytecode. Our algorithm outperforms the previ\u00adously best available solution \nfor that problem (due to Gagnon et al. [8]), especially on methods that contain many state\u00adments. Not \nonly does it exhibit better runtime ef.ciency, its results are also guaranteed to be optimal, in the \nsense that a tightest possible typing is returned. The key design principle we have used is to optimise \nfor the common case, rather than for the worst case. In particular, while our algorithm handles multiple \ninheritance, it exploits the observation that in prac\u00adtice, multiple inheritance is used relatively infrequently. \nAt a more technical level, one key idea is to .rst pro\u00adcess type constraints that impose lowerbounds \n(i.e. con\u00adstraints from assignments), and .nd minimal solutions to those. Next, in a second phase, we \ncheck use constraints (which all impose upperbounds) and prune out minimal so\u00adlutions that do not satisfy \nthose additional constraints. This turns out to be very effective if the goal is to infer types ac\u00adcording \nto the type rules of Java bytecode. For use in a typed intermediate language with the aim of optimisation, \nthis is the type inference algorithm to choose. However, as pointed out by a number of previous works \n[8, 15, 17, 22], when the purpose is decompilation, the re\u00adquirements on type inference are somewhat \ndifferent. Here we must consider the type hierarchy not as it is dictated by bytecode, but as it is given \nby the Java source language. The discrepancy lies in the way primitive types are treated: for example \nboolean, byte and char are incomparable in source, but all represented by integers at bytecode level. \nWe have shown how to handle that problem by augmenting the check\u00ading phase of our algorithm to make appropriate \nadjustments. The cost of having to do this is signi.cant, however, some\u00adtimes taking up to 47% longer. \nIt is therefore recommended that when the purpose is optimisation and not decompilation, the bytecode \ntype hierarchy is used instead. The main item of future work is to examine the impact of this local type \ninference algorithm in the context of inter\u00adprocedural type inference, in particular for the ef.cient \ncon\u00adstruction of call graphs. Another is to examine other applica\u00adtions of constraint-based program analysis, \nand whether the method given here can be generalised to such other analysis problems.  ACKNOWLEDGEMENTS \nWe would like to thank the members of the Programming Tools Group at Oxford for many inspiring discussions \non the topic of this paper. Laurie Hendren and Etienne Gagnon patiently explained the .ner points of \ntheir type inference algorithm. Eric Bodden suggested the Havoc benchmark [6] as an interesting challenge. \n References [1] abc. The AspectBench Compiler. Home page with downloads, FAQ, documentation, support \nmailing lists, and bug database. http://aspectbench.org. [2] Ole Agesen. The Cartesian Product Algorithm: \nSimple and precise type inference of parametric polymorphism. In Walter G. Olthoff, editor, European \nConference on Object-Oriented Programming (ECOOP), volume 952 of Lecture Notes in Computer Science, pages \n2 26. Springer, 1995. [3] Ole Agesen. Concrete Type Inference: Delivering Object-Oriented Applications. \nPhD thesis, Stanford University, 1996. Sun Microsystems, Technical report TR-96-52. [4] Ole Agesen, Jens \nPalsberg, and Michael I. Schwartzbach. Type inference of SELF: Analysis of objects with dynamic and multiple \ninheritance. Software Practice and Experience, 25(9):975 995, 1995. [5] Ben Bellamy, Pavel Avgustinov, \nOege de Moor, and Damien Sereni. Implementation of our local type inference algorithm (including experiments). \nhttp://musketeer.comlab.ox. ac.uk/typeinference/, 2008. [6] Eric Bodden. A denial-of-service attack on \nthe java bytecode veri.er. http://www.bodden.de/research/javados/, 2008. [7] Alan Donovan, Adam Kiezun, \nMatthew S. Tschantz, and Michael D. Ernst. Converting java programs to use generic libraries. In Object-Oriented \nProgramming, Systems and Languages, pages 15 34, 2004. [8] Etienne Gagnon, Laurie J. Hendren, and Guillaume \nMarceau. Ef.cient inference of static types for Java bytecode. In Static Analysis Symposium, volume 1824 \nof Lecture Notes in Computer Science, pages 199 219, 2000. [9] James Gosling, Bill Joy, Guy Steele, and \nGilad Bracha. The Java Language Speci.cation Second Edition. Addison-Wesley, 2000. [10] Justin Owen Graver. \nType-Checking and Type Inference for Object-Oriented Programming Languages. PhD thesis, University of \nIllinois at Urbana-Champaign, 1989. [11] Justin Owen Graver and Ralph E. Johnson. A type system for Smalltalk. \nIn Symposium on Principles of Programming Languages (POPL), pages 136 150. ACM Press, 1990. [12] Andreas \nV. Hense. Polymorphic Type Inference for Object-Oriented Programming Languages. PhD thesis, Universit\u00a8at \ndes Saarlandes, 1994. [13] Eric J. Holstege. Type Inference in a Declarationless, Object-Oriented Language. \nPhD thesis, California Institute of Technology, 1982. Technical report 5035. [14] Bronislav Knaster. \nUn th\u00b4eor`eme sur les fonctions d ensembles. Annales de la Societ\u00b4e Polonaise de Mathematique, 6:133 \n134, 1928. [15] Todd B. Knoblock and Jakob Rehof. Type elaboration and subtype completion for java bytecode. \nACM Transactions on Programming Languages and Systems (TOPLAS), 23(2):243 272, 2001. [16] Dexter Kozen, \nJens Palsberg, and Michael I. Schwartzbach. Ef.cient inference of partial types. Journal of Computer \nand System Sciences, 49(2):306 324, 1994. [17] J. Miecnikowski and L. J. Hendren. Decompiling java bytecode: \nproblems, traps and pitfalls. In R. N. Horspool, editor, Compiler Construction, volume 2304 of Lecture \nNotes in Computer Science, pages 111 127. Springer Verlag, 2002. [18] Nicholas Oxh\u00f8j, Jens Palsberg, \nand Michael I. Schwartzbach. Making type inference practical. In Ole Lehrmann Madsen, editor, European \nConference on Object-Oriented Program\u00adming (ECOOP), volume 615 of Lecture Notes in Computer Science, \npages 329 349, 1992. [19] Jens Palsberg. Ef.cient inference of object types. Information and Computation, \n123(2):198 209, 1995. [20] Jens Palsberg and Michael I. Schwartzbach. Object-oriented type inference. \nIn Andreas Paepcke, editor, Conference on Object-Oriented Programming Systems, Languages and Applications \n(OOPSLA), pages 146 161. ACM Press, 1991. [21] John Plevyak and Andrew A. Chien. Precise concrete type \ninference for object-oriented languages. In Conference on Object-Oriented Programming Systems, Languages, \nand Applications (OOPSLA), pages 324 340. ACM Press, 1994. [22] Todd A. Proebsting and Scott A. Watterson. \nKrakatoa: Decompilation in java (does bytecode reveal source?). In Conference on Object-Oriented Technologies \n(COOTS), pages 185 198, 1997. [23] Michael B. Smyth. Power domains. Journal of Computer and System Sciences, \n16:23 96, 1978. [24] Steven Alexander Spoon. Demand-driven type inference with subgoal pruning. PhD thesis, \nGeorgia Institute of Technology, 2005. [25] Norihisa Suzuki. Inferring types in smalltalk. In Symposium \non Principles of Programming Languages, pages 187 199. ACM Press, 1981. [26] Raja Vall\u00b4ee-Rai, Etienne \nGagnon, Laurie J. Hendren, Patrick Lam, Patrice Pominville, and Vijay Sundaresan. Optimizing Java bytecode \nusing the Soot framework: Is it feasible? In Compiler Construction, 9th International Conference (CC \n2000), pages 18 34, 2000.  \n\t\t\t", "proc_id": "1449764", "abstract": "<p>Inference of static types for local variables in Java bytecode is the first step of any serious tool that manipulates bytecode, be it for decompilation, transformation or analysis. It is important, therefore, to perform that step as accurately and efficiently as possible. Previous work has sought to give solutions with good worst-case complexity.</p> <p>We present a novel algorithm, which is optimised for the common case rather than worst-case performance. It works by first finding a set of minimal typings that are valid for all assignments, and then checking whether these minimal typings satisfy all uses. Unlike previous algorithms, it does not explicitly build a data structure of type constraints, and it is easy to implement efficiently. We prove that the algorithm produces a typing that is both sound (obeying the rules of the language) and as tight as possible.</p> <p>We then go on to present extensive experiments, comparing the results of the new algorithm against the previously best known method. The experiments include bytecode that is generated in other ways than compilation of Java source. The new algorithm is always faster, typically by a factor 6, but on some real benchmarks the gain is as high as a factor of 92. Furthermore, whereas that previous method is sometimes suboptimal, our algorithm always returns a tightest possible type.</p> <p>We also discuss in detail how we handle primitive types, which is a difficult issue due to the discrepancy in their treatment between Java bytecode and Java source. For the application to decompilation, however, it is very important to handle this correctly.</p>", "authors": [{"name": "Ben Bellamy", "author_profile_id": "81381599182", "affiliation": "University of Oxford, Oxford, United Kingdom", "person_id": "P1223227", "email_address": "", "orcid_id": ""}, {"name": "Pavel Avgustinov", "author_profile_id": "81100580373", "affiliation": "University of Oxford, Oxford, United Kingdom", "person_id": "P1223228", "email_address": "", "orcid_id": ""}, {"name": "Oege de Moor", "author_profile_id": "81100198102", "affiliation": "University of Oxford, Oxford, United Kingdom", "person_id": "P1223229", "email_address": "", "orcid_id": ""}, {"name": "Damien Sereni", "author_profile_id": "81100584039", "affiliation": "University of Oxford, Oxford, United Kingdom", "person_id": "P1223230", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1449764.1449802", "year": "2008", "article_id": "1449802", "conference": "OOPSLA", "title": "Efficient local type inference", "url": "http://dl.acm.org/citation.cfm?id=1449802"}