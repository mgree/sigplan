{"article_publication_date": "10-19-2008", "fulltext": "\n JOLT: Lightweight Dynamic Analysis and Removal of Object Churn Ajeet Shankar Matthew Arnold Rastislav \nBod\u00b4ik University of California, Berkeley IBM T.J. Watson Research Center University of California, Berkeley \naj@cs.berkeley.edu marnold@us.ibm.com bodik@cs.berkeley.edu Abstract It has been observed that component-based \napplications ex\u00adhibit object churn, the excessive creation of short-lived ob\u00adjects, often caused by trading \nperformance for modularity. Because churned objects are short-lived, they appear to be good candidates \nfor stack allocation. Unfortunately, most churned objects escape their allocating function, making es\u00adcape \nanalysis ineffective. We reduce object churn with three contributions. First, we formalize two measures \nof churn, capture and control (15). Second, we develop lightweight dynamic analyses for mea\u00adsuring both \ncapture and control. Third, we develop an algo\u00adrithm that uses capture and control to inline portions \nof the call graph to make churned objects non-escaping, enabling churn optimization via escape analysis. \nJOLT is a lightweight dynamic churn optimizer that uses our algorithms. We embedded JOLT in the JIT compiler \nof the IBM J9 commercial JVM, and evaluated JOLT on large application frameworks, including Eclipse and \nJBoss. We found that JOLT eliminates over 4 times as many allocations as a state-of-the-art escape analysis \nalone. Categories and Subject Descriptors D.3.4 Processors [Pro\u00adgramming Languages]: Optimization General \nTerms Algorithms, Performance Keywords Churn, allocation optimization, Java, virtual machine, selective \noptimization, escape analysis, inlining 1. Introduction Large-scale applications are often built on \napplication frame\u00adworks, such a Sun s J2EE and IBM s Eclipse. These frame\u00adworks employ many reusable \ncomponents and third-party libraries. To allow composition with other components in an application, components \ntypically provide a simple, gen\u00aderal interface, which often necessitates construction of many Permission \nto make digital or hard copies of all or part of this work for personal or classroom use is granted without \nfee provided that copies are not made or distributed for pro.t or commercial advantage and that copies \nbear this notice and the full citation on the .rst page. To copy otherwise, to republish, to post on \nservers or to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA 08, October \n19 23, 2008, Nashville, Tennessee, USA. Copyright c &#38;#169; 2008 ACM 978-1-60558-215-3/08/10. . . \n$5.00 intermediate objects that are quickly discarded. This phe\u00adnomenon is known as object churn (15; \n23). Object churn is harmful for several reasons. First, it puts pressure on the garbage collector. Second, \nif temporary ob\u00adjects appear to require synchronization, it inhibits paral\u00adlelization. Third, construction \nof temporary objects may re\u00adquire unjusti.ed computational overhead, as .elds in these overly general \nobjects might be used only once or not at all. Churn-inducing code can be optimized manually, for ex\u00adample \nwith code refactoring. Dufour et al. (15) developed a hybrid static/dynamic analyzer that uses notions \nof object capture and control to guide programmers to problematic program fragments. To eliminate the \nidenti.ed churn, a pro\u00adgrammer may need to change the data structure representa\u00adtion. Because churn is \npervasive (23), such refactoring may involve several components, which may not be economical. Churn may \nalso involve libraries for which source code is not available, preventing refactoring. This paper explores \nautomatic reduction of object churn as might be performed in a JIT compiler. On .rst sight, the problem \nof object churn removal appears to be solved: escape analysis (10; 12; 29) identi.es objects that can \nbe stack allocated, or even promoted to registers via scalar replacement, and subsequent dead value analysis \nmay be able to remove useless object .elds; existing JIT compilers contain advanced implementations of \nthese techniques. However, we observed that object allocation optimiza\u00adtions based on escape analysis \ndo not perform well on component-based applications, largely because many short\u00adlived objects escape \ntheir allocating functions. In a typical scenario, objects are passed up the stack via object facto\u00adries \nor functions that compute intermediate values. While the escape analyses reported in literature usually \neliminate over 20% of allocations on test programs (10; 12; 29) , our measurements on large component-based \napplications indi\u00adcate that even a sophisticated escape analysis performed on function boundaries in \na high-performing commercial JVM generally eliminates fewer than 10% of allocations (see Sec\u00adtion 6). \nAnother factor limiting the effectiveness of escape analy\u00adsis is the stringent budget available to the \nJIT compiler. The VM must select methods for compilation and choose which optimizations to apply to each \nmethod. A common heuris\u00adtic used by pro.le-guided compilers is to select optimiza\u00adtions based on execution \nfrequency (or hotness ), but such a heuristic may overlook many of the tens of thousands of functions \nin a component-based applications that exhibit ob\u00adject churn but are not very hot. In addition, the hottest \nmeth\u00adods are not necessarily involved in object churn. Excessive inlining, which improves churn elimination, \ncan negatively impact performance (8), particularly if applied unjudiciously in large framework-based \napplications. Figure 1 illustrates these concerns with an example of a standard library function used \nin many business applica\u00adtions. The code exhibits typical object churn: objects cre\u00adated in functions \ncalled by divide are immediately used in divide and subsequently discarded. Since these functions are \nnot often among the hottest functions in a program, es\u00adcape analysis may not be performed on them. Even \nif it was, the analysis would .nd that the objects escape their allocat\u00ading functions. This problem can \nbe alleviated with inlining, but we observed that several of the called functions are over\u00adlooked by \na standard inliner, because it considers them too large and is unaware of the potentially large stack \nallocation bene.t that could result. In summary, there are two main obstacles to the elimi\u00adnation of \nobject churn from component-based applications using a JIT compiler: 1. The JIT lacks the budget to perform \nallocation optimiza\u00adtions on all functions and, conversely, suf.cient informa\u00adtion to determine which \nsubset of functions would bene.t from allocation optimizations. 2. Short-lived objects often escape \ntheir allocating func\u00adtions, defeating traditional escape analysis.  JOLT, the system described in this \npaper, deals with these two issues by (a) using a lightweight dynamic analysis to identify churn scopes, \nwhich are subgraphs of the call graph with signi.cant churn; and (b) performing selective function inlining \nto make allocation sites in churn scopes amenable to traditional allocation optimizations. Our solution \nfor short-lived escaping objects is moti\u00advated by the empirical observation of Dufour et al. (15) that \nmany escaping objects are eventually contained. Con\u00adsistently across large programs, over 50% of objects \ndid not escape from their allocating method or one of its three an\u00adcestors in the call chain at the time \nof object allocation. The churn scopes computed by JOLT encapsulate live ranges of short-lived objects. \nThe JIT compiler s inliner is instructed to inline methods in the churn scope, making them visible to \nescape analysis. With this pro.le guidance, the JIT s opti\u00admizer can better select and optimize targets \nfrom among the thousands moderately hot functions that typically exist in a large-scale application. \nJOLT optimizes the code in Figure 1 in three steps. First, the JOLT dynamic analysis detects that divide() \ncaptures many objects and hence makes divide() the start of a churn scope, meaning that some subgraph \n(of the call graph) rooted at divide() may be worth optimizing. Second, JOLT identi.es what subgraph \nthat is. Although divide() makes many calls, JOLT s inliner, still observing an inlining budget, will \ninline into divide() those calls that aid in eliminating churn. These inlined calls become part of the \nchurn scope. Finally, JOLT invokes the escape analysis on this scope, even if the divide function were \nnot hot enough to justify this level of optimization using traditional JIT heuristics. This paper makes \nthe following contributions: We formalized two measures of churn locality, capture and control (15). \nTogether, they form the basis for identi\u00adfying areas of optimizeable object churn (Section 2).  We developed \nef.cient algorithms for measuring capture and control at run time. The algorithms are designed for a \nVM with contiguous memory allocations and thread\u00adlocal heaps (Section 3), a common con.guration in mod\u00adern \nVMs.  We developed an alternate algorithm for approximating these measures in any VM with a garbage \ncollector (Ap\u00adpendix A).  We developed an inlining algorithm that uses the results of these churn analyses \nto expose pro.table allocation contexts, called churn scopes, to standard JIT compiler allocation optimizations \n(Section 4).  We implemented JOLT for Java in a development version IBM s J9 JVM (Section 5). Our evaluation \nof this system on several large-scale applications (Section 6) shows that our transformation increases \n4-fold the rate of object stack allocation.   2. Capture and Control Analysis The ultimate goal of \nour dynamic analysis is to compute churn scopes, subgraphs of the static call graph that encapsu\u00adlate \nlifetimes of many objects; the functions comprising each churn scope are inlined and handed over to escape \nanalysis. Clearly, the program as a whole represents a churn scope that maximizes contained object lifetimes; \nit is, however, usually too large too large to inline and optimize. Therefore, in Sec\u00adtion 4 we give \nheuristics that seek to balance the amount of encapsulated lifetimes and the scope size. In this section \nwe take the .rst step towards computing churn scopes. We formalize capture and control, two pro\u00adgram \nproperties that we use to compute churn scopes in Sec\u00adtion 4. We motivate capture and control with how \none would compute churn scopes. In this section we assume that churn scopes are rooted at some call graph \nnode and contain all nodes reachable from this root; this restriction is relaxed in Section 4. This section \n.rst de.nes capture and control for a dynamic call tree, and then does so for the static call graph. \n2.1 De.nitions Consider an execution trace E of a program P composed of a set of functions F . The trace \nE is a sequence of events corresponding to the start and end points of function invo\u00adFigure 1. An example \nof object churn from the the GNU Classpath implementation of the standard Java Class Library method BigDecimal.divide(). \nThe calls shown here each return either BigDecimal or BigInteger objects, which are immediately used \nand then discarded. The number of churned object allocations is shown next to each call. public BigDecimal \ndivide(BigDecimal val, int newScale, int roundingMode) throws ArithmeticException, IllegalArgumentException \n{ ... valIntVal = valIntVal.multiply(BigInteger.valueOf(10).pow(-power)); // 3 objects ... BigInteger \ndividend = intVal.multiply(BigInteger.valueOf(10).pow(power)); // 3 objects BigInteger parts[] = dividend.divideAndRemainder \n(valIntVal); // 2 objects ... BigInteger posRemainder = parts[1].signum() < 0 ? parts[1].negate() : parts[1]; \n// 1 object valIntVal = valIntVal.signum() < 0 ? valIntVal.negate () : valIntVal; // 1 object int half \n= posRemainder.shiftLeft(1).compareTo(valIntVal); // 1 object ... // (valueOf most likely using constant \nhere) unrounded = unrounded.add (BigInteger.valueOf (sign >0?1: -1)); // 1 object ... return new BigDecimal \n(unrounded, newScale); } cations and object lifetimes. The events are identi.ed with unique timestamps. \nFor each function f . F ,let fi denote the ith invocation of f in E.Let start(fi)denote the begin\u00adning \nof the ith invocation of fi and end(fi)the time when the function returns. For an object o allocated \nin E, start(o) gives the time of o s allocation and end(o)marks the end of o s lifetime. Our implementation \nconservatively de.nes the end of the lifetime as the time at which o becomes unreach\u00adable. A function \ninvocation cj is a child of invocation fi if fi directly invokes cj.The set children(fi)gives all children \nof fi.Wehave cj . children(fi). start(fi)< start(cj )< end(cj )< end(fi) Child invocations cannot overlap, \nso we have cj ,ck . children(fi). j . =k . end(cj )< start(ck). end(ck)< start(cj ) The children relation \non invocations de.nes the dynamic call tree. The static counterpart, de.ned over functions in F , de.nes \nthe static call graph: a function c is a child of f . F if there are i, j such that fi invokes cj . We \nde.ne alloc(fi)as the set of all objects allocated by fi and its descendents. We de.ne escape(fi)as the \nsubset of alloc(fi)composed of objects that escape fi: alloc(fi):={o | start(fi)<start(o)<end(fi)} escape(fi):= \n{o | start(fi)<start(o)<end(fi)<end(o)} A dynamic churn scope of the function invocation fi is a subtree \nof the dynamic call tree rooted at the node fi;the churn scope contains all nodes reachable from node \nfi.A static churn scope of a function f is a subgraph of the call graph rooted at the node f that contains \nall nodes reachable from the node f. 2.2 Capture Our .rst property uses the notion of capture introduced \nby Dufour, et al. (15). We de.ne the capture of a function invocation fi as the set of all objects allocated \nby fi and its descendents whose lifetimes end before fi returns. capture(fi):=alloc(fi)\\ escape(fi) \nThe capture of a function invocation fi is an indicator of object churn: if fi or its descendents create \nmany short-lived objects, these objects are likely captured by fi. In the context of the optimization \nthat we have in mind, capture(fi)gives an upper bound on the number of object allocations that can be \neliminated if the optimization selects fi as the root of a churn scope. Though fi s scope may have signi.cant \noptimizeable ob\u00adject churn, we want to balance optimization bene.t with its cost, in.uenced primarily \nby the size of the scope. First, we are interested in whether choosing an ancestor of fi might expose \neven more churn. Second, we want to determine whether there is a smaller scope with comparable optimiza\u00adtion \nbene.t. Such a scope may be rooted at one of fi s chil\u00addren. The second question is answered in the following \nsubsec\u00adtion. To answer the .rst question, we use the capture rate, %capture, which normalizes capture \nto the number of object Figure 2. Object lifetime behavior of a function invocation fi and one of its \nchild calls. The circles denote allocation The set of objects controlled by fi is de.ned as follows. \n control(fi):=capture(fi)\\ capture(cj ) cj .children(fi) We also de.ne the control rate, denoted %control: \n|control(fi)|%control(fi):=|alloc(fi)| The control rate can be used to identify suitable churn scopes. \nImagine that we are looking for a suitable churn scope by moving the churn scope root down the call graph. \nIn this case, the control rate %control acts as a sentinel: a high value of %control(fi) suggests that \nshrinking the scope from fi to one of fi s children would deprive a signif\u00adicant number of objects controlled \nby fi of the context that bounds their lifetimes. The control rate thus complements %capture, which acts \nas a sentinel when moving the root upwards (we may want to place the root high enough to cap\u00adture most \nof the allocated objects). For a diagram of control and capture, see Figure 2. 2.4 Aggregating Dynamic \nInformation We have de.ned capture and control for dynamic function invocations, whose children relation \nde.nes the dynamic call tree. Since JOLT needs to select scopes on the static call graph, we also de.ne \ncapture and control for static func\u00ad tions by aggregating the dynamic values. Note that capture and control \nfor function invocations range over sets of ob\u00ad jects. To enable statistical aggregation, we de.ne their \nstatic events. The objects marked E, P ,and N were are allocated by fi or its child. The objects marked \nE remain reachable past the exit of fi and are considered escaped. The object marked P is unreachable \nat fi s exit and is considered cap\u00ad tured. The objects marked N are also captured but have an additional \nproperty: they are live during fi s execution, not just during its descendants execution, and are thus \nconsid\u00ad ered controlled by fi. (The object marked A was live before fi s execution but is unreachable \nafter it; it is considered ab\u00ad sorbed and is relevant to Section A.3.) counterparts,capture(f)and control(f), \nas the mean over the cardinalities of these sets. (Instead of comptuing set car\u00addinalities, we sometimes \ncompute the aggregate memory allocations: footprint of these objects.) Given a function f with invo\u00ad|capture(fi)| \ncations f1, ..., fn,we de.ne capture(f)and %capture(f) %capture(fi):= as |alloc(fi)| n 1 capture(f):= \n|capture(fi)| The higher the value of %capture(fi), the less attractive it n i=0 is to grow the scope \nto the parent of fi, because most objects n 1 are already captured by fi. %capture(f): %capture(fi) \n= n i=0  2.3 Control The static values control(f)and %control are de.ned anal- To determine whether \nit may be more pro.table to root the ogously, as the mean of the dynamic values control(fi)and scope \nat a child of fi, rather than at fi, we formalize control, %control(fi), respectively. which indicates \nthe level of object encapsulation provided n by fi.Wede.ne control(fi)as those objects allocated by \n1 control(f):= |control(fi)| fi and its descendents whose lifetimes end in fi but not in n i=0 any of \nits children. The objects controlled by fi include in n 1 particular objects that escape the immediate \nchildren of fi %control(f): %control(fi) = but do not escape fi. The invocation fi controls these objects \nn i=0 in the sense that it uses them in its computation (including It may seem that we could save work \nby computing the static passing these objects among its children), but they do not control values from \nthe static capture values, as follows: survive past its return. Control is an indicator of churn: If \nfi   controls few of the objects that it captures, it may not be a control(f):=capture(f)\\ capture(c) \npro.table root of a churn scope. c.children(f) where the children relation is given by the static call \ngraph. This formula would avoid computing the dynamic values control(fi) during pro.ling. The price, \nhowever, is the loss of context sensitivity present in the dynamic values control(fi). Because the dynamic \nvalues are computed on the dynamic call tree, each invocation is analyzed in its call\u00ading context. Since \ncontrol(f) is computed from the dynamic control values, it re.ects only the behavior of children of f \nwhen they are called from f, ignoring them when they are called from other functions.  3. Computing \nCapture and Control JOLT computes capture and an approximation of control with low-level mechanisms typically \navailable in modern virtual machines. Speci.cally, our dynamic analysis exploits the fact that (i) virtual \nmachines contain a tracing garbage collector; (ii) memory managers often allocate objects con\u00adtiguously, \nas in a copying collector; and (iii) heaps are typi\u00adcally thread-local, i.e., they service allocation \nrequests from a single thread. For JVMs that do not meet the assumption of thread-local heaps (TLHs) \nand a contiguous allocator, we have devised an alternate set of algorithms. These analyses, given in \nAppendix A, approximate capture and control with any garbage collector. In this section, we .rst show \nhow to compute alloc(fi) and escape(fi) with a few simple measurements; as de\u00adscribed in Section 2, these \nvalues can be used to compute capture(fi), %capture(fi), capture(f),and %capture(f). We then describe \na dif.culty with computing control(fi) with a garbage collector, and present an approximation of control(f) \nthat circumvents this dif.culty. 3.1 Computing alloc and escape In a JVM with contiguous allocations \nand thread-local heaps it is straightforward to compute alloc(fi), the total size of objects allocated \nby fi. Each thread has a pointer to the beginning of the free space in its TLH. When a function allocates \nan object, the free space pointer is incremented by the size of the object, and the intervening space \nis returned to the function to be used by that object. We use the function tlhp(fi,t) to denote the value \nof the free space pointer at time tin the thread executing fi. To compute alloc(fi), we determine by \nhow much the free space pointer has moved between start(fi) and end(fi). Since allocations are contiguous \nand thread-local, this value gives the total size of objects allocated by fi. The range of addresses \nof these objects is tlhp(fi,start(fi)),tlhp(fi,end(fi)) This range is valid if no garbage collection \noccurred during fi. If it did, our sampling pro.ler discards the measurement. To compute escape,JOLT \ninvokes the tracing garbage collector at the end of fi and passes it the allocation range computed above. \nAs the GC traverses live objects on the tlhp(fi, start(fi)) tlhp(c1, start(c1)) alloc(c1) tlhp(c1, end(c1)) \ntlhp(c2, start(c2)) alloc(fi) alloc(c2) tlhp(c2, end(c2)) tlhp(fi, end(fi)) Figure 3. A view of the \nportion of a thread-local heap rel\u00adevant to a function invocation fi, illustrating the informa\u00adtion that \na thread-local heap allocation pointer provides in computing churn analyses. By tracking the position \nof this pointer (shown at the left of the heap) as the program exe\u00adcution progresses, JOLT computes alloc \nfor the function and each of its children. Furthermore, by running a garbage col\u00adlection at the end of \nfi,JOLT identi.es the set of objects that are still live, escape(fi) (shown here in gray). The remaining \nobjects, in white, constitute capture(fi). Finally, the gray objects in each child allocation region \nalloc(cj ) provide a lower bound on escape(cj ). heap, it checks whether the object s address falls within \nthe allocation range of fi. Objects in alloc(fi) that are still alive at fi s exit comprise escape(fi). \nFigure 3 illustrates this computation. 3.2 Computing control Recall that to compute control n 1 . control(f):= \ncontrol(fi) n i=0 we require a value for control(fi) for each invocation fi, whichinturnreliesonthe \nvalue of escape(cj ) for each child cj of fi. Unfortunately, we cannot compute escape(cj ) for each child \ncj of fi using the method described in the pre\u00adceding subsection. Since the collector is copying, invoking \nit at the end of cj will rearrange the heap, invalidating mea\u00adsurements for fi and its other children. \nThus, to compute escape(fi), we can afford only one garbage collection dur\u00ading the execution of fi,atthe \nend of fi. To circumvent this problem, JOLT samples the value escape(cj ,fi),which is the value of escape(cj \n) when cj executed as a child of fi. In more detail, JOLT computes control(f) not from the dynamic values \ncontrol(fi), but from static information that preserves context-sensitivity whenever possible. The approximation \nuses static call graph edge frequencies freq to weight the data from each of its static child calls: \nfunction mark_object(object o) { ... control*(f)=capture(f)- . function f = currently_profiled_function; \n* freq(f, c)\u00b7 cap (c) if (f != null &#38;&#38; o >= tlhp(f, start(f)) &#38;&#38; f c.children(f) o < \ntlhp(f, end(f))) { The value freq(f, c)\u00b7 cap *(c)approximates the total objects f captured by c when \nc was called from f.The value cap *(c) f is computed in one of three ways, in order of preference: Content-sensitive \nsampling of c in f. This approach measures the behavior of c in the context of f.After f has been suf.ciently \nsampled to establish a stable value for capture(f)(in practice, we use six samples), JOLT alters its \nbehavior the next time f is scheduled for sampling. Rather than sampling capture(fi),JOLT randomly selects \na child call cj from fi s execution. It then pro.les that child call, computing capture(cj )by invoking \nthe garbage collector at the end of cj and storing this information as the calling\u00adcontext-sensitive \nvalue capture(cj ,fi). During the summa\u00adrization of f, if such a context-sensitive sample of c is avail\u00adable, \nthe speci.c value capture(cj ,fi)is summarized to * capturef (c),which cap then uses to compute control(f) \nf as per the formula above. Context-insensitive value for c. If no such context\u00adsensitive value is available \nfor a given child c,JOLT attempts to use the context-insensitive value capture(c)that is com\u00adputed as \nthe mean of all available samples of capture(cj ). This approach is less accurate, because it does not \nconsider the context-sensitive behavior of c when called by f. Upper-bound value of capture(cj ). Finally, \nin the rare case that no samples of capture(cj )have been taken, JOLT falls back to computing an upper \nbound for capture(cj ). This is done by computing a lower bound on escape(cj ). During the initial pro.les \nof f, when the garbage collector is being invoked at the end of fi,JOLT passes the collector the allocation \nrange not only for fi but also for each of its children (see Figure 3). Then, during the collector s \nmark\u00ading phase, if a live object happens to fall within the alloca\u00adtion range of fi, the GC checks further \nto see whether the object also lies within the allocation range of cj (see Fig\u00adure 4 for pseudocode). \nIf so, the object escapes cj since it escapes its caller fi. Because alloc(cj )is measured pre\u00adcisely, \nthis lower bound on escape(cj )yields an upper bound on capture(cj ). Though this upper bound is not \nvery accu\u00adrate (speci.cally, it cannot determine how many objects es\u00adcaping cj are controlled by fi), \nit is inexpensive to obtain and serves to bound the error when no other information about c is available. \nAs with the context-sensitive sampling, this upper bound on capture(cj )is summarized to an upper * bound \non capturef (c),which cap uses in the computation f of control(f).  4. Selecting Churn Scopes For Optimization \nThis section describes the heuristics used to select and opti\u00admize churn scopes, the sets of functions \nencompassing ob\u00ad add_to_escaped(f, o); foreach(c in f.child_calls) { if (o >= tlhp(c, start(c)) &#38;&#38; \n o < tlhp(c, end(c)) { add_to_escaped_with_context(c, f, o); } } ...  } Figure 4. Pseudocode for JOLT \ns escape algorithm during the GC s live object marking step. ject churn that are transformed into a single \nfunction via inlining and then handed to the optimizer. In the .rst step, JOLT uses the dynamic analyses \ndescribed in the previous sections to .nd the root of a scope. In the second step, JOLT inlines certain \ndescendents of the root, depending on their bene.t and cost to the optimization, in order to expose allo\u00adcation \nsites to a traditional escape analysis. This algorithm is based on several approximations, but it has \nthe bene.t of be\u00ading very simple to implement, and has good results in prac\u00adtice; see Section 6. 4.1 \nStep 1: Finding the Root of the Scope A churn scope is a subgraph of the static call graph that has a \nsingle root from which all nodes in the scope are reachable. This step .nds a suitable root of the scope. \nJOLT uses three of the analyses from previous sections in conjunction to select scopes that are likely \nto result in bene.cial optimization. JOLT selects scope roots that have a high capture value, which \nensures that there are many churned objects in the scope, justifying the expense of optimizing it.  \nhaveahigh %capture value, which indicates that the scope would not gain many newly captured objected \nwere it to be grown to include a caller of the root function: most of the allocated objects are being \ncaptured already.  have a high %control value, which indicates that many of the captured objects are \nlive at the scope s root. Thus, the scope would lose these captured objects were it to shrink to a child \nof the root, since the controlled objects would escape that scope.  When a function f has accumulated \na set number of sampled pro.les, a summary is generated using the tech\u00adniques described in Section 3. \nJOLT evaluates the churn scope rooted at f by summing the rankings of capture(f ), %capture(f ),and %control(f \n)against all currently sam\u00adpled functions. (No functions are ranked until 100 functions have been summarized.) \nIf f s summed ranking is in the top 1% of all functions, then it is selected as an churn scope root, \nand its scope is scheduled for churn optimization. 4.2 Step 2: Inlining Descendents Ideally, we would \ninline into the churn scope root f all func\u00adtions reachable from it. The resulting single function would \nencapsulate all of its captured objects lifetimes, which would allow standard escape analysis to optimize \nthem. Un\u00adfortunately, excessive inlining causes well-known problems, and JOLT therefore selects descendents \nof the root such that an inlining budget is not exceeded. To aid in explaning this selection process, \nwe view the static call graph rooted at f as a call tree, by conceptually removing any cycles, and duplicating \nany vertices in the resulting DAG that have mul\u00adtiple in-edges. JOLT s strategy for selection is based \non two observations. Our .rst observation is that for the escape analysis to identify and remove the \nchurn, we need to inline into the root f only those functions that allocate objects captured by f; this \nwill make them optimizable by the escape analysis. Functions without captured allocations can be given \na lower priority for inlining into f. Our second observation helps us deal with the loss of precision \nthat occurs when we aggregate capture information from the dynamic call tree onto the static call graph. \nIdeally, to decide whether a function g should be inlined into the root f, we want to know how many objects \nallocated by g are captured by f. This seems to require maintaining information for all pairs of reachable \ncall graph nodes, which we do not track for reasons of ef.ciency. To avoid this overhead, we observe \nthan f was selected as the root because of its high %capture value, which means that most objects allocated \nin f s scope are captured by f. Thus, we simplify the problem with the assumption that any allocation \nsite is worth inlining, since it most likely allocates captured objects. Since inlining every function \nwith an allocation site of\u00adten exceeds the maximum permitted size bound, we need to select the most pro.table \nfunctions. We prefer to inline functions that allocate the most objects; this policy gives the largest \nnumber of allocations the chance of optimization. We phrase our .nal problem as follows: Given as input \na call tree rooted at f, with the bene.t of each function in the tree equal to the number of allocations \nit contains, what is the subtree rooted at f with the greatest bene.t that respects an inlining size \nbound k? We show by reduction from the classic Knapsack problem that this problem is NP-Hard. The inputs \nto Knapsack are a set of items, each with a cost and a bene.t, and the output is the subset of these \nitems with the maximum bene.t that does not exceed a given cost bound. We represent each of the input \nitems to Knapsack by a function whose size is the item s cost and which contains totalCost := 0 S := \nset of initial candidate inlining decisions while (totalCost < LIMIT &#38;&#38; S != emptyset) choose \ninlining decision D from S with the largest benefit/cost ratio if (totalCost + cost(D) < LIMIT) inline \nD totalCost += cost(D) add children of D in tree to S S=S\\{D} Figure 5. The inlining algorithm, based \non an approxima\u00adtion of Knapsack, used by JOLT. as many allocations as the item s bene.t. Each of these \nfunctions is added as a child of a root function f to form a call tree. The size bound is set to the \nKnapsack input s cost bound. The solution to our problem yields a subtree containing those items that \nmaximize the total bene.t while respecting the bound, which solves the Knapsack problem. We observe, \nbased on this reduction, that our problem is the Knapsack problem with the added restriction that for \nany item to be selected, its ancestors in a provided tree must be selected as well. Thus, we use an ef.cient \napproximation of Knapsack, discussed in the context of inlining in Arnold, et al. (5), and presented \nin a modi.ed version in Figure 5, to solve it. The initial inputs to this approximation are f s children. \nThe algorithm chooses the input with the greatest bene\u00ad.t/cost ratio to inline .rst, and subsequently \nadds that input s children as new inputs, repeating this process until the cost limit is reached. Since \nit inlines from the root node f down the tree, the restriction of requiring ancestor nodes to be in\u00adlined \nis implicitly satis.ed. However, the algorithm with its stated inputs is ill-suited for maximizing the \nnumber of inlined allocations from the whole tree, since it greedily favors local maxima. Consider the \nchain of function calls f . g . h.If g has no allocations, it has no bene.t, and the greedy algorithm \nwould not inline along the f . g edge, even if h has many allocations. To address this problem, JOLT \nuses the alloc value from its dynamic analyses to provide the Knapsack approximation with more holistc \ncost and bene.t values. We change the bene.t of each input n to be alloc(n) the number of allocations \nthat occur in the entire subtree starting at n and the cost to be the total size of the functions in \nthe subtree. In the above example of f . g . h, the bene.t of g, then, is not just the number of allocations \nit contains, but the number of allocations its subtree (including h) contains. Thus, even at f we can \nchoose to inline g, knowing that further along its subtree are functions with many allocations. This \nuse of alloc is imprecise in the sense that it assumes that an entire subtree s cost and bene.t will \nbe acquired Figure 6. A schematic of the JOLT implementation. The pro.ler gathers data for the analyses \n(described in Section 2) much like a normal VM pro.ler does, via sampling. It passes this information \non to a selector, which rates each function. If a function s aggregate rank is in the top 1% of all pro.led \nfunctions, it is selected for optimization. The JOLT inliner (Section 4) then uses smart inlining (aided \nby the standard JIT pro.ler) to expose as many allocations as possible. The standard escape analysis \nis also prodded to run on the ex\u00adpanded function, even if it might not have otherwise. when the subtree \ns root is selected for inlining, when in fact only part of the subtree may be inlined, depending on the \ninlining budget and what other inlining candidates are available. Once the churn scope has been selected \nand a subset of its functions is inlined into the root function f, traditional escape analysis is performed \non the expanded f, eliminating the captured allocations therein.  5. Implementation in J9 In this section \nwe describe implementation details. JOLT is implemented in a development version IBM s J9 JVM. See Figure \n6 for a diagram of the JOLT architecture. Analysis implementation. To reduce overhead, full\u00adduplication \nsampling (7) is used to gather runtime pro.les. Thus, JOLT s pro.ler only runs during a function s slow \npath. JOLT optimizes slow-path overhead in two ways. First, it does not instrument leaf calls (since \nthey are served well by normal JIT optimizations), and does not instrument around child calls to functions \nwhich themsleves have no calls and no allocations (and thus no objects to capture). Note that even though \nthese functions and calls may not be pro.led directly, their behavior is still captured by the statistics \ntaken by the calling function. Second, JOLT only instruments func\u00adtions when they are compiled at the \nintermediate optimiza\u00adtion level. Thus, the hottest functions accrue some samples, and then have no instrumentation \nat all when they are re\u00adcompiled by the JIT at a higher optimization level. After 10 samples, a function \nis considered for churn optimization. Naturally, error is introduced by sampling data rather than recording \nstatistics from all function invocations. This tradeoff is well documented (7): a lower sampling interval \nenables quicker convergence toward unsampled data at the cost of higher overhead. A judicious choice \nof interval can affect the error as desired. Note that this error does not affect any individual sample \non an invocation fi; it only affects the aggregate statistics for function f because not all of its invocations \nare tracked. Though the analysis summaries are de.ned in terms of numbers of objects, JOLT s implementation \nactually com\u00adputes the size of those objects in bytes. This decision makes implementation easier, since \nit requires only computing the size of heap ranges (a simple subtraction), rather than walk\u00ading the heap \nto count objects. Inlining implementation. JOLT s inliner is implemented alongside the standard VM inliner. \nEdge and basic block frequencies used in dynamic allocation counts are gathered from the VM s built-in \npro.ler. In the implementation, the call graph explored starting from a function f is bounded at depth \n6. Though we describe the inlining algorithm in Section 4.2 as operating on a call tree, due to the greedy \nnature of the algorithm JOLT does not need to explicitly convert the call graph to a tree. Instead, it \nretains the call graph and only lazily duplicates nodes as necessary when their parents are chosen for \ninlining. 6. Evaluation We evaluated JOLT on a number of large and popular framework-based benchmarks. \nThese are: an implemen\u00adtation of TPC-W (2) running atop the JBoss application server (16); the Eclipse \nbenchmark found in the DaCapo benchmark suite (9); the JPetStore e-commerce applica\u00adtion running atop \nthe Spring application framework (1) (the most popular framework listed on sourceforge.net); and SPECjbb2005, \na three-tiered client/server benchmark. We also evaluated it against the DaCapo benchmark suite. 6.1 \nMethodology Measurements were made with a developmental version of IBM s J9, a leading commercial VM, \non a dual Xeon 2.8 Ghz machine with 1 GB of RAM running Red Hat Linux with a 2.6.9 kernel. Baseline numbers \nwere measured by running the VM in its default con.guration. Each result was obtained by taking the median \nsteady\u00adstate performance of 5 program runs. The DaCapo bench\u00admarks, including Eclipse, were version 2006-10 \nand were run with the parameters -n 10 -s large; the time of the tenth run was taken as the steady-state. \nUnfortunately, the developmental version of our VM was unable to execute the jython benchmark from the \nDaCapo suite. Load for the Petstore application was generated by the petload pro\u00adgram (3), running on \nanother server. Two 100-second execu\u00adtions of petload were performed, with the second taken to be steady-state. \nThe TPC-W workload was generated with the implementation s accompanying remote browser pro\u00adgram; it was \nexecuted on another server with the relevant parameters -TT 0.0 -RU 100 -MI 100 -GETIM false Table 1. \nSteady-state performance numbers of JOLT on several benchmarks. Note that compilation time was measured \nover the entire run and thus may exceed the single steady-state benchmark time. Performance numbers with \na * indicate higher is better. Number precision is discarded only for display; full precision was used \nin computing percentages and ratios. Benchmark Comp. Time Runtime/ Throughput % Speedup #Objs Eliminated \n%Objs Eliminated Improvement Over Base Eclipse Baseline Inlining Selection JOLT 53s 75s 58s 62s 94.7s \n98.0s 93.2s 90.4s -3.5% 1.6% 4.8% 5.3m/1.2b 9.6m/1.2b 20.5m/1.2b 23.6m/1.2b 0.4% 0.7% 1.7% 1.9% 2.0x \n3.8x 4.6x JPetstore on Spring Baseline Inlining Selection JOLT 61s 66s 64s 77s 810 * 790 * 816 * 828 \n* -2.5% 0.7% 2.2% 1.2m/169m 1.6m/170m 2.2m/166m 4.1m/166m 0.7% 1.0% 1.3% 2.5% 1.3x 1.9x 3.5x TPCW on \nJBoss Baseline Inlining Selection JOLT 34s 44s 36s 43s 98.9 * 100.7 * 97.2 * 101.5 * 1.8% -1.7% 2.6% \n4.8k/198m 896k/193m 5.7m/191m 8.0m/186m 0.0% 0.5% 3.0% 4.3% 192x 1.3x103 1.7x103 SPECjbb2005 Baseline \nInlining Selection JOLT 11s 18s 18s 20s 20173 * 20196 * 22233 * 22828 * 0.1% 10.2% 13.2% 25.5m/267m 24.0m/241m \n43.1m/238m 81.2m/253m 9.6% 9.9% 18.1% 32.1% 1.0x 1.9x 3.4x DaCapo antlr Baseline antlr JOLT 15s 21s 6.6s \n6.7s -1.5% 39.2k/85.2m 1.7m/83.2m 0.0% 2.0% 44.4x bloat Baseline bloat JOLT 19s 28s 81.4s 70.9s 14.8% \n82.7m/1.1b 375m/1.1b 7.3% 33.8% 4.6x chart Baseline chart JOLT 16s 21s 21.3s 20.1s 6.0% 20.7m/896m 47.0m/905m \n2.3% 5.2% 2.3x fop Baseline fop JOLT 5s 8s 4.2s 4.1s 2.4% 120k/10.5m 1.0m/9.6m 1.1% 10.6% 9.3x hsqldb \nBaseline hsqldb JOLT 22s 29s 11.4s 11.4s 0.0% 0/97.2m 4.7m/103m 0.0% 4.6% 8 luindex Baseline luindex \nJOLT 14s 16s 9.2s 8.6s 7.0% 2.1m/114m 4.9m/111m 1.8% 4.4% 2.4x lusearch Baseline lusearch JOLT 47s 49s \n11.6s 10.6s 9.4% 9.1m/395m 173m/405m 2.3% 42.7% 18.6x pmd Baseline pmd JOLT 19s 28s 20.3s 20.5s -1.0% \n163m/1.1b 164m/1.1b 15.5% 15.5% 1.0x xalan Baseline xalan JOLT 41s 37s 39.1s 38.3s 2.1% 955k/732m 9.8m/734m \n0.1% 1.3% 10.2x -CUST 144000 -ITEM 10000. The SpecJBB2005 bench\u00admark was con.gured to run from 1 to \n4 warehouses, with a ramp up period of 60 seconds and a measurement period of 60 seconds. The throughput \nwith four warehouses was taken to be the steady-state performance.  6.2 Results and Analysis The results \nare presented in Table 1. For each benchmark, Baseline shows the steady-state performance of the orig\u00adinal, \nunmodi.ed VM, while JOLT shows the steady-state performance of the full JOLT optimizer, including pro.ling \noverhead. The Inlining and Selection con.gurations are dis\u00adcussed later in this section. % Speedup is \ncomputed from the ratio of a con.gura\u00adtion s runtime to the Baseline runtime. # Objs Eliminated displays \ntwo numbers, X/Y, where X is the number of dy\u00adnamic object allocations eliminated of Y total dynamic \nob\u00adject allocations during an entire program run; k signi.es thousands, m millions, and b billions. The \nImprovement Over Base column shows the ratio of the number of objects elim\u00adinated by a particular con.guration \nto the number of objects eliminated by the Baseline con.guration. In every benchmark, JOLT was able to \neliminate more al\u00adlocations than escape analysis alone. The increase ranged from just over 1x to 8 (in \nthe case where escape analy\u00adsis was unable to remove any objects at all), with a me\u00addian of 4.6x. Note \nthat due to our measurement method\u00adology, the numbers measured for the evaluation represent eliminated \nobject allocations, whereas JOLT s optimizer at\u00adtempts to eliminate the maximum number of bytes allocated. \nThus the percentage of allocated bytes eliminated by JOLT is likely to be greater. Performance also improved \nunder JOLT, with an av\u00aderage speedup of 4.8% for all applications, 5.7% for the component-based applications, \nand a max speedup of 14.8% for bloat. The compilation overhead ranged from -10% with xalan1 to 82% with \nSPECjbb2005, with an average of 32%. We feel that this increase in the relatively constant-factor overhead \nof compilation is acceptable in the context of long-running programs that may execute from tens of minutes \nto days. A surprising result is how few allocations escape analysis is able to remove on these large-scale \nprograms. Though JOLT is able to improve upon these numbers, there remains a signi.cant opportunity for \nfurther optimization that we are continuing to investigate. JOLT is composed of two primary mechanisms, \na set of dynamic analyses and an inliner. It is possible that JOLT s empirical results are more due to \none or the other of these; for instance, it might be possible to do just as well with only a dynamic \nanalyses-driven scope selector and then a stan\u00addard inliner. We evaluated this hypothesis on the component\u00ad \n1 For reasons we could not fully diagnose, the JOLT run performed less compilation than the baseline \nrun. Benchmark Pro.ling Overhead Eclipse 0.8% JPetstore/Spring 1.1% TPCW/JBoss 1.7% SPECjbb2005 1.2% \nantlr 2.3% bloat -0.2% chart -1.6% fop 2.6% hsqldb 2.7% luindex 0.5% lusearch -0.4% pmd 1.5% xalan 0.3% \n Table 2. Pro.ling overheads for computing the capture and control analyses. based benchmarks as follows. \nIn addition to comparing the default VM eliminations against the JOLT eliminations, we also measured \ntwo other con.gurations: (a) the analyses were used to select churn scopes, which were then fed to the \ndefault VM inliner, rather than the JOLT inliner, before escape analysis ( Selection ) (b) no churn scopes \nwere se\u00adlected, and instead all hot functions were optimized using the JOLT inliner ( Inlining ). The \nresults are shown in Ta\u00adble 1. The Inlining con.guration tended to slow the program down; however, it \ndid allow for more allocation elimination, possibly because every hot method fed to the escape analysis \nhad far more allocation sites present. The Selection con.gu\u00adration generally performed right at the baseline. \nNeither ex\u00adceeded the combined JOLT con.guration on any of the four benchmarks, which seems to indicate \nthat both are contribut\u00ading to its performance. We report the overhead of the capture and control anal\u00adysis \npro.ler in Table 2. The overhead numbers measure the total overhead over the full long-running execution. \nThe pro\u00ad.ler sampled one function invocation of every 100,000. See Section 5 for details on how functions \nwere pro.led. The speedups for several benchmarks are possibly due to the pro\u00ad.ler s behavior of recompiling \na function once it has taken enough measurements, to remove the sampling code. The cost of this approach \nis re.ected in compilation time (Ta\u00adble 1), but the aggressive recompilation of frequently exe\u00adcuted \nmethods may be improving performance.  7. Related Work Mitchell, et al. (23) identi.ed the object churn \nand excessive computation pervasive in component-based software. Du\u00adfour, et al. (15) informally explored \nthe notions of capture and control, and used a hybrid analysis to aid a programmer in .nding areas of \nobject churn. Our work strives to address these same problems by automatically identifying and opti\u00admizing \nchurn in a virtual machine. JOLT is prototyped in a leading production VM that per\u00adforms pro.ling and \nadaptive recompilation similar to other virtual machines (4; 6; 18; 19; 25; 28). Methods begin ex\u00adecuting \nin an interpreter, and hot methods are pro.led and promoted to higher levels of optimization using a \nJIT com\u00adpiler. For the aggressive dynamic analysis, JOLT employs the Arnold-Ryder sampling framework \n(7) to keep overhead low. Escape analysis (14; 26) (as pertains to Java (10; 12; 29)) is a critical component \nin JOLT s optimization scheme. Whaley and Rinard (29), Gay and Steensgaard (17), and Blanchet (10) have \ndescribed extensions to their escape anal\u00adyses that can detect object capture 1 and arbitrarily many \nfunctions up the call stack, respectively. However, these ex\u00adtensions either are not benchmarked or do \nnot perform well in practice (10), possibly due to the large amount of dupli\u00adcated context necessary \nto eliminate each captured object. Since then, advances have been made in the sophistica\u00adtion and aggressiveness \nof escape analysis in JIT compil\u00aders (21) and the transformations that eliminate object alloca\u00adtion, \nsuch as lazily reallocating eliminated allocations if nec\u00adessary (22). Escape analyses can be used directly \nby JOLT in its optimization procedure as they become available in state-of-the-art VMs. Although these \nescape analyses use in\u00adterprocedural analysis to identify objects that do not escape from non-inlined \ncallees, unlike JOLT they do not use inter\u00adprocedural analysis to identify callees containing key alloca\u00adtions, \nwhich thus must be inlined for the escape analysis to be effective. Many papers have used pro.ling information \nto guide inlining (5; 8; 11; 20). Schei.er (27) .rst reduced a size\u00adbounded inlining problem to Knapsack. \nLike JOLT, several works have used inlining not as an end in itself (as a call overhead reduction) but \nas a means to enable other optimiza\u00adtions. EDO (24) inlined hot exception paths so that thrown exceptions \ndo not have to walk the call stack. Dean and Chambers (13) decided between multiple inlining options \nbased on the bene.ts accrued by optimizations applied to the inlined method s body.  8. Conclusion In \nthis paper, we have presented JOLT, a fully-automatic online churn optimizer. It selects scopes to optimize \nvia a novel lightweight dynamic analysis based on the notions of control and capture. Churn elimination \nis achieved by using combined dynamic and static analysis to guide inlining deci\u00adsions, making the resulting \ncompilation unit more amenable to escape analysis. The resulting code is fed to a state of the art JIT \noptimizer that performs escape analysis along with other standard optimizations.  Acknowledgements We \nthank Nick Mitchell for several early discussions on ob\u00adject churn, Gary Sevitsky for his feedback on \na draft of this paper, and Bill McCloskey for help with his petload tool. We are grateful to the anonymous \nreferees for their help\u00adful comments. This work was supported in part by the Na\u00adtional Science Foundation \nwith grants CCF-0085949, CNS\u00ad0326577, and CNS-0524815, an NSF Graduate Fellowship, a generous gift from \nIBM Corporation, the IBM Open Col\u00adlaborative Research project, the AF-TRUST project, and the University \nof California MICRO program . References [1] Spring framework. http://www.springframework.org/. [2] \nTPC-W NYU. http://cs.nyu.edu/~totok/ professional/software/tpcw/tpcw.html. [3] Personal communication \nwith William McCloskey, October 2007. [4] Ali-Reza Adl-Tabatabai, Jay Bharadwaj, Dong-Yuan Chen, Anwar \nGhuloum, Vijay Menon, Brian Murphy, Mauricio Ser\u00adrano, and Tatiana Shpeisman. The StarJIT compiler: A \ndy\u00adnamic compiler for managed runtime environments. Intel Technology Journal, 7(1):19 31, February 2003. \n[5] Matthew Arnold, Stephen Fink, Vivek Sarkar, and Peter F. Sweeney. A comparative study of static and \npro.le-based heuristics for inlining. In DYNAMO 00: Proceedings of the ACM SIGPLAN workshop on Dynamic \nand adaptive compi\u00adlation and optimization, pages 52 64, New York, NY, USA, 2000. ACM. [6] Matthew Arnold, \nMichael Hind, and Barbara G. Ryder. On\u00adline feedback-directed optimization of java. In Proceedings of \nthe 17th ACM SIGPLAN conference on Object-oriented pro\u00adgramming, systems, languages, and applications, \npages 111 129. ACM Press, 2002. [7] Matthew Arnold and Barbara G. Ryder. A framework for reducing the \ncost of instrumented code. In Proceedings of the 2001 ACM SIGPLAN Conference on Prgramming Lan\u00adguage \nDesign and Implementation (PLDI), pages 168 179, June 2001. [8] Andrew Ayers, Richard Schooler, and Robert \nGottlieb. Ag\u00adgressive inlining. SIGPLAN Not., 32(5):134 145, 1997. [9] Stephen M. Blackburn, Robin Garner, \nChris Hoffmann, As\u00adjad M. Khang, Kathryn S. McKinley, Rotem Bentzur, Amer Diwan, Daniel Feinberg, Daniel \nFrampton, Samuel Z. Guyer, Martin Hirzel, Antony Hosking, Maria Jump, Han Lee, J. Eliot B. Moss, B. Moss, \nAashish Phansalkar, Darko Ste\u00adfanovi\u00b4c, Thomas VanDrunen, Daniel von Dincklage, and Ben Wiedermann. The \ndacapo benchmarks: java benchmarking development and analysis. In OOPSLA 06: Proceedings of the 21st \nannual ACM SIGPLAN conference on Object\u00adoriented programming systems, languages, and applications, pages \n169 190, New York, NY, USA, 2006. ACM. [10] Bruno Blanchet. Escape analysis for object-oriented lan\u00adguages: \napplication to java. In OOPSLA 99: Proceedings of the 14th ACM SIGPLAN conference on Object-oriented \npro\u00ad gramming, systems, languages, and applications, pages 20 34, New York, NY, USA, 1999. ACM. [11] \nPohua P. Chang, Scott A. Mahlke, William Y. Chen, and Wen-Mei W. Hwu. Pro.le-guided automatic inline \nexpansion for C programs. Software Practice and Experience, 22(5):349 369, May 1992. [12] Jong-Deok Choi, \nManish Gupta, Mauricio Serrano, Vu\u00adgranam C. Sreedhar, and Sam Midkiff. Escape analysis for java. In \nOOPSLA 99: Proceedings of the 14th ACM SIG-PLAN conference on Object-oriented programming, systems, languages, \nand applications, pages 1 19, New York, NY, USA, 1999. ACM. [13] Jeffrey Dean and Craig Chambers. Towards \nbetter inlining decisions using inlining trials. In LFP 94: Proceedings of the 1994 ACM conference on \nLISP and functional programming, pages 273 282, New York, NY, USA, 1994. ACM. [14] Alan Deutsch. On determining \nlifetime and aliasing of dy\u00adnamically allocated data in higher-order functional speci.ca\u00adtions. In POPL \n90: Proceedings of the 17th ACM SIGPLAN-SIGACT symposium on Principles of programming languages, pages \n157 168, New York, NY, USA, 1990. ACM. [15] Bruno Dufour, Barbara G. Ryder, and Gary Sevitsky. Blended \nanalysis for performance understanding of framework-based applications. In ISSTA 07: Proceedings of the \n2007 inter\u00adnational symposium on Software testing and analysis, pages 118 128, New York, NY, USA, 2007. \nACM. [16] Marc Fleury and Francisco Reverbel. The JBoss extensi\u00adble server. In Markus Endler and Douglas \nSchmidt, editors, Middleware 2003 ACM/IFIP/USENIX International Mid\u00addleware Conference, volume 2672 \nof LNCS, pages 344 373. Springer-Verlag, 2003. [17] David Gay and Bjarne Steensgaard. Fast escape analysis \nand stack allocation for object-based programs. In th Interna\u00adtional Conference on Compiler Construction \n(CC 2000),vol\u00adume 1781. Springer-Verlag, 2000. [18] Nikola Grcevski, Allan Kilstra, Kevin Stoodley, Mark \nStood\u00adley, and Vijay Sundaresan. Java just-in-time compiler and virtual machine improvements for server \nand middleware ap\u00adplications. In 3rd Virtual Machine Research and Technology Symposium (VM), May 2004. \n[19] Kazuaki Ishizaki, Mikio Takeuchi, Kiyokuni Kawachiya, Toshio Suganuma, Osamu Gohda, Tatsushi Inagaki, \nAkira Koseki, Kazunori Ogata, Motohiro Kawahito, Toshiaki Yasue, Takeshi Ogasawara, Tamiya Onodera, Hideaki \nKomatsu, and Toshio Nakatani. Effectiveness of cross-platform optimiza\u00adtions for a Java just-in-time \ncompiler. ACM SIGPLAN Notices, 38(11):187 204, November 2003. [20] Owen Kaser and C. R. Ramakrishnan. \nEvaluating inlining techniques. Computer Languages, 24(2):55 72, 1998. [21] Thomas Kotzmann and Hanspeter \nM\u00a8ossenb\u00a8ock. Escape anal\u00adysis in the context of dynamic compilation and deoptimiza\u00adtion. In VEE 05: \nProceedings of the 1st ACM/USENIX inter\u00adnational conference on Virtual execution environments, pages \n111 120, New York, NY, USA, 2005. ACM. [22] Thomas Kotzmann and Hanspeter Mossenbock. Run-time support \nfor optimizations based on escape analysis. In CGO 07: Proceedings of the International Symposium on \nCode Generation and Optimization, pages 49 60, Washington, DC, USA, 2007. IEEE Computer Society. [23] \nNick Mitchell, Gary Sevitsky, and Harini Srinivasan. Model\u00ading runtime behavior in framework-based applications. \nIn Eu\u00adropean Conference on Object-Oriented Computing (ECOOP) 2006, 2006. [24] Takeshi Ogasawara, Hideaki \nKomatsu, and Toshio Nakatani. Edo: Exception-directed optimization in java. ACM Trans. Program. Lang. \nSyst., 28(1):70 105, 2006. [25] Michael Paleczny, Christopher Vick, and Cliff Click. The Java Hotspot \nserver compiler. In Java Virtual Machine Research and Technology Symposium (JVM), pages 1 12, April 2001. \n[26] Young Gil Park and Benjamin Goldberg. Escape analysis on lists. In PLDI 92: Proceedings of the ACM \nSIGPLAN 1992 conference on Programming language design and implemen\u00adtation, pages 116 127, New York, \nNY, USA, 1992. ACM. [27] Robert W. Schei.er. An analysis of inline substitution for a structured programming \nlanguage. Commun. ACM, 20(9):647 654, 1977. [28] Toshio Suganuma, Toshiaki Yasue, Motohiro Kawahito, \nHideaki Komatsu, and Toshio Nakatani. A dynamic optimiza\u00adtion framework for a Java just-in-time compiler. \nACM SIG-PLAN Notices, 36(11):180 195, November 2001. In Confer\u00adence on Object-Oriented Programming, Systems, \nLanguages and Applications (OOPSLA). [29] John Whaley and Martin Rinard. Compositional pointer and escape \nanalysis for java programs. In OOPSLA 99: Pro\u00adceedings of the 14th ACM SIGPLAN conference on Object\u00adoriented \nprogramming, systems, languages, and applications, pages 187 206, New York, NY, USA, 1999. ACM. APPENDIX \nA. Algorithms for Approximating Capture and Control in Any VM The algorithms for computing capture and \ncontrol presented in this paper rely on two virtual machine features: a contigu\u00adous object allocator \nand thread-local heaps. Though most production VMs support these features, not all VMs do, and in this \nsection we present an alternate set of algorithms that can approximate capture and control on just about \nany VM. The only requirement is that the VM have (a) a garbage collector that can report the number of \nlive objects it has found after a collection and (b) an allocator that can count the number of allocations \nit has made. Naturally, with these two simple primitives, we aim to approximate capture and control rather \nthan to compute them exactly. We do this by reducing the problem from tracking the reachability of individual \nobjects as they are allocated to simply tracking differences in the number of reachable objects present \nin the program as the execution progresses. Below, we describe the two major steps of this approximation, \nand then discuss the error the approximation introduces. A.1 Aggregation We begin with the assumption \nthat we can approximate the two simple notions de.ned in Section 2.1, alloc and escape, from the two \nVM primitives described above; the actual approximation is described in Section A.2. Our .rst step toward \nan approximation is to sacri.ce object-level precision in favor of aggregation. In other words, we forego \nthe knowledge of, say, exactly which objects are captured by a particular function in favor of knowing \nhow many such objects there are. Recall the de.nitions of capture and control in terms of alloc and escape. \nHere, we show that these de.nitions hold if we replace sets with their cardinalities. LEMMA 1. |capture(fi)|=|alloc(fi)|-|escape(fi)| \n|control(fi)|=|capture(fi)|- |capture(c)| c.children(fi) This computation of cardinality from the primitives \nalloc and escape is precise, even in the presence of set subtraction. In other words, we prove that we \nonly lose the knowledge of what objects are in each set; we still know the exact size of the sets. Proof. \nConsider some sets t, s1,s2, \u00b7\u00b7\u00b7 and a set sub\u00adtraction t \\.isi. We would like to show that |t \\.isi| \n= |t|-|si|. i For each si, then, we must show that (a) s . t (or else subtracting the cardinality will \nundercount the result) and (b) .j.i=j si nsj =\u00d8(or else the sum of the cardinalities of the sis will \nbe overcounted, and the result undercounted). For capture(fi), we must only show that escape(fi). alloc(fi). \nConsider an object o. o . escape(fi)=. start(fi)<start(o)<end(fi)=. o .alloc(fi). We expand control(fi)to \ncontrol(fi)=(alloc(fi)\\escape(fi))\\  capture(c) c.children(fi) Consider an object o and a child c of \nfi. By the def\u00adinitions of capture, child,and alloc, we know that o . capture(c)=. start(c)<start(o)<end(c)=. \nstart(fi)<start(o)<end(fi)=. o .alloc(fi). Thus, we have escape(fi) . alloc(fi) (as above) .c.children(fi)capture(c) \n. alloc(fi) To prove null intersection, we .rst compare the escape term to each of the capture terms, \nand then compare the capture terms to each other. Again, consider an object o and an arbitrary child \ncall c. By the de.nitions of escape, capture,and child, o . escape(fi)=. end(fi)<end(o)=. end(c)< end(o)=. \no ..capture(c). Now consider two children, c1 and c2. By the de.ni\u00adtion of child, w.l.o.g. assume end(c1)<start(c2).Then \no . capture(c1)=. end(o) <end(c1)=. end(o)<start(c2)=. o ..capture(c2),and o . capture(c2)=. start(c2)<start(o)=. \nend(c1)< start(o)=. o ..capture(c1). This yields .c.children(fi)capture(c)nescape(fi)= \u00d8 =dcapture(c)ncapture(d)= \n\u00d8 .c,d.children(fi).c This completes the proof. . A.2 Runtime Values It remains to approximate alloc \nand escape for each function in a program at runtime given the two memory management primitives described \nat the beginning of the Appendix, which we call objcount and reachcount. objcount(t): the number of objects \nallocated from time 0 to time t reachcount(t): the number of reachable objects at time t Objcount is \nobtained by keeping a running counter of object allocations, and gives us the exact cardinality of alloc. \n|alloc(fi)|=objcount(end(fi))-objcount(start(fi)) Reachcount is obtained by running the garbage collector \nand counting the number of live objects it .nds, and serves as an approximation for escape. |~|escape(fi)=reachcount(end(fi))-reachcount(start(fi)) \nThe error introduced by this approximation is discussed in Section A.3. For a particular function invocation \nfi, we compute obj\u00adcount and reachcount at its entry and exit. These data points are suf.cient to compute \ncapture(fi)and control(fi).A full-duplication sampling pro.ler is used to gather samples without running \nan excessive number of garbage collections. fi is also instrumented to compute objcount and reach\u00adcount \nadditionally at the beginning and end of any child calls it makes, yielding |alloc(c)|and |escape(c)|for \neach child call. This additional information yields control(fi),as per the de.ntion in Section 2.3. A \nsummary of the runtime approximations of the analy\u00adses described in Section 2 is shown in Figure 7. Unlike \nthe algorithm presented in the body of this paper, this approx\u00adimation generates control values for individual \nfunction in\u00advocations fi. Thus, the formal de.nition of control can be used to compute function averages. \n A.3 Error The approximation of capture and control detailed above introduces error in two ways, discussed \nhere. Escape approximation. In Section A.2, we approxi\u00admated escape by measuring the difference in the \nnumber |capture *(fi)| =(objcount(end(fi)) -objcount(start(fi))) -(reachcount(end(fi)) -reachcount(start(fi))) \n|capture *(fi)| |%capture *(fi)| = objcount(end(fi)) -objcount(start(fi)) |control(fi)* | = |capture \n*(fi)|- |capture *(c)| c.children(fi) |control *(fi)||%control *(fi)| = objcount(end(fi)) -objcount(start(fi)) \n * Figure 7. A summary of the analysis approximations used by the generalized system described in the \nAppendix. We use x to denote the approximate version of x. To compute values for a static function f, \nwe compute the median values over all available dynamic datapoints fi. of reachable objects between function \nbeginning and func\u00adtion end. However, this difference actually accounts for two phenomena: locally allocated \nobjects that escape increase the difference, but objects allocated before the function start that are \nno longer reachable at function exit decrease the difference. We call these latter objects absorbed by \nfi.Let .reachcount(x, y)= reachcount(y) -reachcount(x). absorb(fi):= {o|start(o) <start(fi) <end(o) <end(fi)} \n .reachcount(start(fi),end(fi)) = |escape(fi)|-|absorb(fi)| Thus the error introduced by this approximation \nis exactly equal to the number of absorbed objects. From this we can compute the error for the analyses. \nWe use x * to denote the approximate version of x. |capture *(fi)|= |capture(fi)|+ |absorb(fi)| |absorb(fi)||%capture \n*(fi)|= |%capture(fi)|+ |alloc(fi)| |control *(fi)| = |control(fi)|+ |absorb(fi)|- |absorb(c)| c.children(fi) \n|%control *(fi)| = |%control(fi)|+ |absorb(fi)|-|absorb(c)||alloc(fi)| To mitigate some of this error, \nan implementation of JOLT using these approximation algorithms can make use of a sim\u00adple observation. \nConsider the case in which .reachcount(start(fi),end(fi)) = x. If every object al\u00adlocated in fi s churn \nscope were captured, x =0 (no more objects are reachable at the end of fi than at the beginning). On \nthe other hand, if x< 0, at least x objects must have been absorbed by fi, to account for the fact that \nthe num\u00adber of reachable objects has decreased. JOLT can thus put a lower bound on the number of absorbed \nobjects for any fi whose .reachcount is negative, and use this lower bound, for instance, by subtracting \nit directly from the approxima\u00adtion of capture, reducing the error. This situation may arise infrequently, \nthough, since ab\u00adsorbed can be masked by escaping objects. Now imagine that reachcount were computed \nafter every instruction m1..k in fi. Then, as in with the end of fi,iffor any n = k, .reachcount(start(fi),mn)= \nx where x< 0, at least x objects must have been absorbed between the start of fi and that instruction. \nEach instruction, then, provides an op\u00adportunity to place a lower bound on absorbed. Of course, JOLT \ncannot calculate reachcount after every instruction in fi, since that would require a very large num\u00adber \nof GCs. However, it does have some existing GC data points already being gathered, around the child calls \nthat fi makes, as described in Section A.2. Thus, JOLT can com\u00adpute .reachcount between the start of \nfi and each other pro.led point. If any of these deltas is negative, that places a lower bound on absorbed(fi), \nwhich is then used to reduce the error. We measured absorbed objects with runtime instrumen\u00adtation on \nthe SPECjbb2005 benchmark. For the functions selected by JOLT for optimization, the ratio of absorbed \nob\u00adjects to allocated objects was 1:8, indicating that the error is likely to be low in practice for \nfunctions amenable to churn optimization. Multithreading. The black-box approach that leads to the error \nin the escape approximation also introduces error in another way. Consider the computation of alloc(fi).A \nglobal allocation counter is kept, and the difference is taken between this counter s value at the end \nof fi and its value at the beginning of fi. In a single-threaded program, this result is exact, since \nany intervening allocations must have occurred within fi and its children. However, if the program execution \nhas multiple threads, the global counter will be incremented by any allocations that occur in other threads \nthat might have been run between the start and end of fi. Similarly, liveness counts may be erroneously \nin.ated or de\u00ad.ated by behavior in other threads. Of course, the magnitude of this error is largely dependent \non the speci.c behavior of these threads. One way to overcome this error is to discard obvious outliers \nin the pro.le data. These outliers generally indicate extra-thread behavior rather than anomalous function \nbehav\u00adior. (This intuition is supported by the very quick conver\u00adgence of pro.le data in the thread-local \nalgorithms.) Once outliers are discarded and the deviation in pro.le values is low, it becomes possible \nto gauge the true behavior of sam\u00adpled functions, even in the presence of multiple threads.   \n\t\t\t", "proc_id": "1449764", "abstract": "<p>It has been observed that component-based applications exhibit <i>object churn</i>, the excessive creation of short-lived objects, often caused by trading performance for modularity. Because churned objects are short-lived, they appear to be good candidates for stack allocation. Unfortunately, most churned objects escape their allocating function, making escape analysis ineffective.</p> <p>We reduce object churn with three contributions. First, we formalize two measures of churn, <i>capture</i> and <i>control</i> (15). Second, we develop lightweight dynamic analyses for measuring both <i>capture</i> and <i>control</i>. Third, we develop an algorithm that uses <i>capture</i> and <i>control</i> to inline portions of the call graph to make churned objects non-escaping, enabling churn optimization via escape analysis.</p> <p>JOLT is a lightweight dynamic churn optimizer that uses our algorithms. We embedded JOLT in the JIT compiler of the IBM J9 commercial JVM, and evaluated JOLT on large application frameworks, including Eclipse and JBoss. We found that JOLT eliminates over 4 times as many allocations as a state-of-the-art escape analysis alone.</p>", "authors": [{"name": "Ajeet Shankar", "author_profile_id": "81100643801", "affiliation": "UC Berkeley, Berkeley, CA, USA", "person_id": "P1223161", "email_address": "", "orcid_id": ""}, {"name": "Matthew Arnold", "author_profile_id": "81100021720", "affiliation": "IBM T.J. Watson Research Center, Hawthorne, NY, USA", "person_id": "P1223162", "email_address": "", "orcid_id": ""}, {"name": "Rastislav Bodik", "author_profile_id": "81100033082", "affiliation": "UC Berkeley, Berkeley, CA, USA", "person_id": "P1223163", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1449764.1449775", "year": "2008", "article_id": "1449775", "conference": "OOPSLA", "title": "Jolt: lightweight dynamic analysis and removal of object churn", "url": "http://dl.acm.org/citation.cfm?id=1449775"}