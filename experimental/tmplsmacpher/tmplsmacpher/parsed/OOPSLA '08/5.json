{"article_publication_date": "10-19-2008", "fulltext": "\n Mixing Source and Bytecode A Case for Compilation by Normalization Lennart C. L. Kats Martin Bravenboer \nEelco Visser Department of Software Technology Department of Computer and Department of Software Technology \nDelft University of Technology, Information Science Delft University of Technology, The Netherlands University \nof Oregon, USA The Netherlands l.c.l.kats@tudelft.nl martin.bravenboer@acm.org visser@acm.org Abstract \nLanguage extensions increase programmer productivity by providing concise, often domain-speci.c syntax, \nand sup\u00adport for static veri.cation of correctness, security, and style constraints. Language extensions \ncan often be realized through translation to the base language, supported by pre\u00adprocessors and extensible \ncompilers. However, various kinds of extensions require further adaptation of a base compiler s internal \nstages and components, for example to support sep\u00adarate compilation or to make use of low-level primitives \nof the platform (e.g., jump instructions or unbalanced synchro\u00adnization). To allow for a more loosely \ncoupled approach, we propose an open compiler model based on normalization steps from a high-level language \nto a subset of it, the core language. We developed such a compiler for a mixed Java and (core) bytecode \nlanguage, and evaluate its effective\u00adness for composition mechanisms such as traits, as well as statement-level \nand expression-level language extensions. Categories and Subject Descriptors D.1.5 [Programming Techniques]: \nObject-oriented Programming; D.2.3 [Soft\u00adware Engineering]: Coding Tools and Techniques; D.3.3 [Programming \nLanguages]: Language Constructs and Fea\u00adtures; D.3.4 [Programming Languages]: Processors General Terms \nLanguages, Design Keywords Dryad Compiler, Stratego, Java, Bytecode, SDF, Compilers, Meta Programming, \nEmbedded Languages, Lan\u00adguage Extensions, Domain-Speci.c Languages, Source Trac\u00ading, Traits, Iterators \nPermission to make digital or hard copies of all or part of this work for personal or classroom use is \ngranted without fee provided that copies are not made or distributed for pro.t or commercial advantage \nand that copies bear this notice and the full citation on the .rst page. To copy otherwise, to republish, \nto post on servers or to redistribute to lists, requires prior speci.c permission and/or a fee. OOPSLA \n08, October 19 23, 2008, Nashville, Tennessee, USA. Copyright c &#38;#169; 2008 ACM 978-1-60558-215-3/08/10. \n. . $5.00 1. Introduction Programming languages should be designed for growth in order to evolve according \nto the needs of the user [34]. General-purpose programming languages offer numerous features that make \nthem applicable to a wide range of appli\u00adcation domains. However, such languages lack the high-level \nabstractions required to adequately cope with the increasing complexity of software. Through the introduction \nof lan\u00adguage extensions, it is possible to increase the expressivity of a language, by turning programming \nidioms into linguis\u00adtic constructs. Language extensions allow for static veri.ca\u00adtion of correctness, \nsecurity, and style constraints. Language extensions may be domain-speci.c in nature, such as embed\u00added \nSQL, or may be general purpose in nature, such as traits or the enhanced for loop, enumerations, and \nother features added in Java 5.0. The mechanisms available for realizing language exten\u00adsions determine \nthe quality of extension implementations and the effort needed for their construction. If language ex\u00adtensions \ncan be realized with relative ease, then building new abstraction mechanisms can be used in the software \ndevelop\u00adment process to replace programming idioms and boilerplate code. The quality of a language extension \ncomprises robust\u00adness (are error messages reported on the extended language? is code generation complete?), \ncomposability (does the ex\u00adtension combine with other extensions?), and the quality of the generated \ncode. Finally, separate compilation, i.e. binary distribution of compiled, statically veri.ed (library) \ncompo\u00adnents, is an important feature to reduce compilation time. The creation of a language extension \nimplies the reuse of an existing implementation of the base language. Generally, such reuse can be categorized \nas either black box reuse of a compiler in the form of a preprocessor, or white box reuse by means of \na deep integration with the compiler implementation. Language Extension by Preprocessing A preprocessor \ntransforms a program in an extended language into a pro\u00adgram in the base language. Examples of preprocessors \nin\u00adclude annotation processors such as XDoclet [37] and Java s APT [35], SQLJ [24] an embedding of SQL \nin Java, and StringBorg [5], a generic approach for embedding languages such as SQL. As they are employed \nas a separate tool, rather than requiring integration into a compiler, preprocessors are highly portable. \nBy avoiding direct compiler extension, their implementation only requires basic knowledge of a lan\u00adguage \ns structure and semantics, not of its compiler. Thus, the compiler is considered as a black box; only \nits published interface the syntax of the programming language is used, and the internal architecture \nof the compiler is hid\u00adden. This separation of concerns makes preprocessors well suited for rapid implementation \nor prototyping of language extensions. While preprocessors are an attractive, lightweight solu\u00adtion to \nlanguage extensibility, they are not considered a ma\u00adture solution for production implementation of languages. \nProduction of a parser that is exactly compatible with the base language is not always a trivial undertaking. \nThe lack of a (complete) source level semantic analyzer results in error messages by the base compiler \nabout code fragments gener\u00adated by the preprocessor, rather than the source code written by the programmer. \nSeparate compilation is only possible if the compilation units of the extended language align well with \nthose of the base language. This fails in the case of new modularity abstractions. In other words, considering \nthe base compiler as a black box condemns the preprocessor imple\u00admenter to reimplement the front-end \nof the base compiler. Reusing an Existing Compiler To avoid reimplementation efforts, language extensions \nare often implemented by exten\u00adsion of the front-end of a compiler. This level of integration ensures \nthat existing compiler components, such as a parser and semantic analysis, can be reused in the extended \ncom\u00adpiler. By generating code in the front-end language, it can then be further compiled using the base \ncompiler. Traditional monolithic compilers are typically not designed for extensi\u00adbility, and adding \na new feature may require extensive refac\u00adtoring of the implementation. Since such refactorings are not \nincorporated upstream, this effort needs to be repeated with each release of the compiler. Extensible \ncompilers, such as Polyglot [28], ableJ [42], and the JastAdd extensible Java Compiler [13], are designed \nfor extensibility with the prin\u00ad ciple that the implementation effort should be proportional to the size \nof the language extension. This front-end exten\u00adsion pattern is illustrated in Figure 1. However, even \nthese systems do rely on white box extension, by exposing their internal structure. Extending a compiler \npurely by transformation to the base language is sometimes inadequate. Compilers typically consist of \nmultiple stages, parsing and gathering semantic in\u00adformation in the early stages (the front-end), and \ngenerating and optimizing code in later stages (the back-end) [1]. This strict separation of stages is \nimposed to ensure straightfor\u00adward modularization and reuse of compiler components. As such, some compilers \nfor different languages share a com-Figure 1. The front-end extension pattern, applied by many conventional \nextensible compilers. mon intermediate language and associated back-end. Only the .nal stage or stages \nof a compiler back-end actually out\u00adput the target-machine code. Integration into the back-end makes \nit possible to also manipulate compiled code. This can be necessary to output speci.c instruction sequences \nor clauses not otherwise gen\u00aderated by the base compiler. For example, for Java, the front\u00adend does not \nexpose a goto operation, unbalanced synchro\u00adnization primitives, or means of including debugging infor\u00admation. \nThe back-end of a compiler also forms an essen\u00adtial participant in composition of source and compiled \ncode. Consider language extensions aimed at modularity, such as traits [12] and aspects [20]. To support \nseparate compilation, such extensions require integration into a compiler s back\u00adend. Separate compilation \nenables distribution of modules in compiled form, or weaving of code into compiled classes. Using a classic, \nmulti-staged compiler, implementing such extensions is a demanding task that requires in-depth under\u00adstanding \nof the compiler. Extensions that span across mul\u00adtiple compilation stages get tangled throughout the \ndifferent components of a compiler, and create a large dependency on its implementation. Mixing Source \nand Bytecode Summarizing, the decision to extend a compiler using a simple, front-end based ap\u00adproach, \nor a more deeply integrated approach, comes down to a choice between black box or white box reuse of \na base compiler, each with their own drawbacks. Rather than dis\u00admissing preprocessors, we want to embrace \ntheir simplic\u00adity and modularity and propose a compiler architecture that deals with their .aws. In this \npaper, we propose an open compiler architecture based on mixing source and bytecode in order to enable \ncompilation by normalization. That is, the base language Figure 2. Modular extension of a normalizing \ncompiler. is a combination of the high-level source language (Java) and the low-level bytecode core language. \nThis means that it is possible to use bytecode primitives, such as the goto instruction, directly from \nJava, as in the statement if (condition) `goto label; where the backtick (`) operator is used to distinguish \nbe\u00adtween the syntax of the two constituent languages. Similarly, source code expressions and statements \ncan be embedded in bytecode, nested to arbitrary depth. The compiler for this mixed base language normalizes \nan input program in the combined language to a program in the bytecode core lan\u00adguage. Thus, the language \nproduced as output of the com\u00adpiler is a subset of the input language. This architecture combines the \nlight weight construction of extensions using a preprocessor, with the access to the compiler back-end \nof a compiler extension. Without know\u00ading the internals of the compiler, a preprocessor can gen\u00aderate \ncode in the core language where needed, as well as the extended input language where possible (see Figure \n2). In other words, an extension preprocessor extends the base compiler by adding transformations from \nthe extended lan\u00adguage into a mix of low-level and high-level code, as is con\u00advenient for the de.nition \nof the particular extension. The idea of transformation-based compilers is not new. In par\u00adticular, Peyton \nJones and Santos [31] applied the idea in the implementation of the Glasgow Haskell Compiler GHC in which \ncompiler optimizations are formulated as transfor\u00admations on core language constructs. The front-end \nof the compiler consists of type analysis and simple desugarings. However, in GHC the core language is \nnot a subset of the compiler s input language. As a result it is not possible to feed the output of the \ncompiler back into the compiler. In our approach the complete compiler can be used as a nor\u00admalization \ntool, which allows the construction of pipelines of preprocessors, and the implementation of separate \ncom\u00adpilation for new abstraction mechanisms. By keeping track of origin information when generating code, \nerror messages produced later in the chain can refer to the original source input. To evaluate the notion \nof compilation by normalization, we have created a prototype implementation of a Java com\u00adpiler based \non this principle. This prototype, the Dryad Com\u00adpiler1, uni.es the Java language with the underlying \nbyte\u00ad code instruction language. Extending a regular Java type\u00adchecker, we provide full typechecking \nfor this combined lan\u00adguage. Using the collected type information, we introduce overloaded instructions \nto the bytecode language that can be normalized to regular instructions, and facilitate code gener\u00adation. \nOutline We proceed as follows. In Section 2 we discuss the design of a number of language extensions, \nevaluating how they bene.t from compilation by normalization when implemented as an extension of the \nDryad Compiler. We de\u00adscribe the syntax and semantics of the mixed Java/bytecode language and the architecture \nof its compiler in Section 3. In Section 4 we discuss how normalization rules incrementally transform \nthe Java/bytecode language and its extensions to the core language. In Section 5 we offer a discussion \nof the architecture of the Dryad compiler, and compilation by nor\u00admalization in general. We present related \nand future work in Section 6, and .nally conclude in Section 7. 2. Extending the Dryad Compiler In this \nsection, we discuss a number of compiler extensions for the Dryad Compiler. We .rst discuss extension \nat the class level, with partial and open classes in Section 2.1 and traits in Section 2.2. In Section \n2.3 we discuss how the principle of compilation by normalization can be applied at the statement level \nfor iterators, and in Section 2.4 we show how it can bene.t the implementation of expression-level extensions. \n2.1 Extension with Partial and Open Classes In Java, classes are de.ned in a single source .le. Partial \nclasses enable the distribution of class members over mul\u00adtiple source .les. Partial classes can be used \nfor separation of concerns, for example dividing GUI-related and event\u00adhandling code, or as a way of \nmodifying existing classes included in a third-party application or library. Another ap\u00adplication of \npartial classes is merging generated code frag\u00adments, such as code generated from partial models [44] \nor from modular transformations generating code for speci.c aspects [16]. Partial classes can be implemented \nrelatively easily as an extension of an existing compiler, by merging them to regular classes. Open classes \nextend the notion of partial classes by al\u00adlowing the extension of compiled classes, rather than just \nclasses in source code form. One implementation of open 1 http://strategoxt.org/Stratego/TheDryadCompiler \n Figure 3. Using open classes for incremental compilation. classes for Java is provided by MultiJava \n[8, 9]. MultiJava is derived from the Kopi Java Compiler, a fairly small open\u00adsource implementation written \nin Java itself. This makes it a relatively accessible candidate for such an extension. Multi-Java uses \ndifferent stages that add to the existing stages of the Kopi compiler (i.e., parsing, importing de.nitions, \ndiffer\u00adent typechecking stages, grouping of multi-methods, byte\u00adcode generation). This required a signi.cant \npart of the de\u00advelopment effort to be spent in understanding the base com\u00adpiler. Clifton notes that this \nwas in fact the case, attributing this to a lack of documentation on Kopi [8]. Existing implementations \nof partial and open classes only allow the de.nition of classes that augment others in source form, merging \nthem into either source or compiled classes. A third scenario, merging together multiple classes in com\u00adpiled \nform, is not supported. This scenario can be applied to use open classes for supporting separate compilation \nin compilers that target the Java platform, a technique for ex\u00adample applied in [44] and [16]. Consider \nFigure 3, where open classes facilitate merging of newly and previously com\u00adpiled class fragments. In \nthis architecture, a compiler for a domain-speci.c language (DSL), for example producing GUI-related \ncode, is implemented as an extension of the Dryad Compiler. The code it produces is processed by an\u00adother \nextension, which merges the open classes for .nal compilation. The mixed source/bytecode language allows \nus to think of source and bytecode classes as instances of a single lan\u00adguage; there is no fundamental \ndifference in the merge pro\u00adcess required for them. Figure 4 shows the architecture of Figure 4. Extending \nthe Dryad Compiler to support open classes. class Calculator { // From Calculator_Operations.java public \nvoid add() { operation = new Add(getDisplay()); . . . } // From Calculator_Gui.class `private setDisplay(int \nnumber : void) [ iload number; . . . ] . . . } Figure 5. Open classes merged to a single Java/bytecode \nde.nition. the open classes extension. It merges together members of class fragments, either in source \nor bytecode form. The re\u00adsulting Java/bytecode class (see Figure 5) is passed to the Dryad Compiler, \nwhich provides the support for compilation of these composed fragments. This design allows a straight\u00adforward \nimplementation of the extension, no longer requir\u00ading implementation-level knowledge of the open compiler \nit builds on. Compilation of the merged classes is handled by the base compiler; the extension only provides \nthe com\u00adposition mechanism. Using the technique of source tracing (on which we elaborate in Section 3.4), \nit maintains location information relative to the original source .les for compile\u00adtime errors and debugging \ninformation. 2.2 Extension with Traits Traits are primitive units of reusable code that de.ne a set \nof methods that can be imported into regular classes. Using class Shape with TDrawing { Vertices getVertices() \n{ ... } . . . } trait TDrawing { void draw() { ... } require Vertices getVertices(); } Figure 6. Example \nof a class Shape importing a trait TDrawing. a set of operators, traits can be composed and manipulated. \nFor instance, the with operator composes two or more traits, and is also used to extend a class with \nthe methods de.ned in a trait (see Figure 6). Traits were originally introduced by Sch\u00a8 arli et al. in \nthe context of Smalltalk [12]. They have since been ported to statically typed languages, such as Java, \nC#, and Scala. To support traits in a statically typed context, they must explicitly specify their required \nmethods, i.e. all methods that are referenced but not provided by a trait. To the best of our knowledge, \nonly Scala which sup\u00adports the feature natively rather than in the form of an ex\u00adtension supports separate \ncompilation of traits [29]. This allows for statically veri.ed, binary distribution of traits in libraries, \nbut requires a signi.cantly different compilation model than the source-to-source transformation commonly \napplied by implementations of traits. To enable separate compilation to binary class .les in our traits \nextension, we designed a compilation scheme trans\u00adlating traits to regular, abstract classes. This way, \nrequired methods can be mapped to abstract methods. Although we have no intention for the Java Virtual \nMachine (JVM) to load these classes at run-time traits are merely compile-time en\u00adtities this mapping \nenables us to use the base compiler s veri.cation and compilation facilities for abstract classes. After \ntraits are compiled, the resulting class .les can be composed according to the composition operators. \nFor the with operator this means that a trait s methods are added to a client class. Similarly, the minus \noperator removes methods from a trait. The rename operator renames a method declara\u00adtion and all occurrences \nof it in the core language invocation constructs. Unlike in Java, the names used in these constructs \nare fully quali.ed and unambiguous, making this a straight\u00adforward operation. The composition operations \nare followed by a basic consistency check that con.rms that all required methods are de.ned and that \nthere are no duplicate de.ni\u00adtions. More extensive typechecking is performed by the base compiler. Consider \nFigure 7, which illustrates the architec\u00ad ture of this extension. By leveraging the Java/bytecode lan\u00adguage \nfor inclusion of compiled code, the extension remains loosely coupled from the base compiler. Our implementation \nFigure 7. Separate compilation of a trait (left) and a class importing it (right). for (String s : getShortWords(\"foo \nbar\")) { System.out.println(s); } Iterator<String> it = getShortWords(\"foo bar\").iterator(); while (it.hasNext()) \n{ String s = it.next(); System.out.println(s); } Figure 8. The enhanced for loop (top) and its desugared \nform (bottom). of traits currently spans 104 lines of code2, making it a rel\u00ad atively lightweight implementation. \nStill, it includes support for separate compilation, consistency checking, and source tracing. As it \ndoes not require implementation-level knowl\u00adedge of the base compiler, but only of its input language, \nthe focus shifts to these issues rather than on direct adaptation of the existing compilation process. \n 2.3 Extension with Iterator Generators Java 5.0 introduced the enhanced for loop, a language fea\u00adture \nthat allows programmers to iterate over collections. The Java compiler treats this as a form of syntactic \nsugar, and translates it to a regular while loop that uses the java.lang.Iterable and java.util.Iterator \ninter\u00adfaces (see Figure 8). As such, the enhanced for loop can be used to conveniently iterate over any \ntype that implements the Iterable interface. Implementing the iterator interfaces is somewhat in\u00advolved \nand follows a common pattern with considerable 2 Not included is the syntax de.nition. Implemented as \na stand-alone pro\u00adgram, this .gure does include 47 lines of I/O code, imports, and comments, all written \nin the Stratego program transformation language [6]. boilerplate code. A complementary language extension \nthat deals with this problem is that of iterator generators,a feature supported by other languages such \nas Python and Sather [27]. Like the enhanced for loop, this feature ab\u00ad stracts away from the underlying \ninterfaces. Consider Fig\u00adure 9, which uses it to de.ne an iterator method that splits a string at every \nspace character. It loops over the results of a call to String.split(), and uses the yield statement \nto return all substrings with less than four characters as ele\u00adments of the resulting iterator. The iterator \nmethod operates as a coroutine: control is yielded back to the client of the iterator at every yield \nstatement. When the client requests the next element, the iterator method is resumed after the last yield \nstatement. In earlier work, we implemented the yield statement as a source-to-source transformation, \nabstracting over regular Java control .ow statements [19]. The yield statement in\u00adtroduces a form of \nunstructured control .ow: the method can be entered at any arbitrary point it is used. To express this \nin a legal Java program, the desired control-.ow graph must be transformed to make use of Java s structured \ncontrol .ow statements, a non-trivial problem, also faced when decom\u00adpiling bytecode [26]. We accommodated \nfor this by using a switch statement and case labels at every point in the con\u00adtrol .ow graph. Since \nthe Java switch statement can only be used in a structured fashion (i.e., it disallows case labels inside \nblocks of code nested in it), all existing control .ow statements in the method must be rewritten to \nbecome part of the switch. This turned out to require signi.cant effort, es\u00adsentially re-implementing \na considerable part of the existing Java language. The infomancers-collections library [11] aims to avoid \nthe complications of source-to-source transformation. It ef\u00adfectively hides the language extension from \nthe Java com\u00adpiler, by using a dummy yield() method that can be in\u00advoked from anonymous classes that \nimplement the iterator interfaces. A regular Java compiler can then be used to com\u00adpile the code, unaware \nof the special semantics of these in\u00advocations. The resulting bytecode is then altered by the li\u00adbrary, \nreplacing the dummy invocations with an actual im\u00adplementation, modifying the (unstructured) control \n.ow of the method. This is a rather intricate process that requires the use of a bytecode manipulation \nlibrary to generate a new class. Since the Java compiler is oblivious to the special se\u00admantics of the \ndummy invocations, it is unable to perform proper control .ow analysis during the initial compilation, \nwhich may lead to unexpected results. In particular, com\u00adpilers may optimize or otherwise generate code \nthat vio\u00adlates the stack-neutrality property of statements (see Sec\u00adtion 3.3), which can result in invalid \nprograms after inserting new jump instructions. In our approach, we treat the language extension as a \nform of syntactic sugar that can be projected to the base language (i.e., Java/bytecode), just like the \nenhanced for public Iterable<String> getShortWords(String t) { String[] parts = t.split(\" \"); for (int \ni = 0; i < parts.length; i++) { if (parts[i].length() < 4) { yield parts[i]; } } } Figure 9. Iterator \nde.nition with the yield statement class ShortWords implements Iterator<String> { int _state = 0; String \n_value; boolean _valueReady; String[] _parts; int _i; private void prepareNext() { if (_valueReady || \n_state == 2) return; if (_state == 1) `goto afterYield; _parts = t.split(\" \"); for (_i = 0; _i < _parts.length; \n_i++) { if (_parts[_i].length() < 4) { _state = 1; _valueReady = true; _value = _parts[_i]; return; \n// yield value afterYield: } } _state = 2; } public String next() { prepareNext(); if (!_valueReady) \n throw new NoSuchElementException(); _valueReady = false; return _value; } public boolean hasNext() { \nif (!_valueReady) prepareNext(); return _valueReady; } public void remove() { throw new UnsupportedOperationException(); \n} } Figure 10. Iterator de.nition, generated from Figure 9. loop. For this, the yield statement is rewritten \nto a return statement to exit the iterator method, in conjunction with a small, bytecode-based jump table \nat the beginning of the method for re-entry. Based on a .nite state machine model of the method, a .eld \n_state is maintained to indicate the next method entry point. Consider Figure 10, which shows the class \nthat will be generated local to the orig\u00adinal getShortWords() method. It highlights all lines of code \nthat directly correspond to the original method. The prepareNext() method is central to this implementation, \nand includes an adapted version of the original method body. Its control .ow is adapted by the addition \nof unstructured control .ow primitives, in the form of bytecode instructions. In this example, we implement \nthis using a jump table and goto instructions, embedded in Java using the backtick (`) operator. Alternatively, \na tableswitch instruction could be used to form an unstructured switch statement. It is not possible \nto statically determine if the iterator will return a value or not for a given state, and we can\u00adnot \nmake assumptions on the order of invocation of the Iterator.next() and Iterator.hasNext() interface methods. \nTherefore, a call to prepareNext() is used in both these methods, caching the value as necessary. To \nen\u00adsure persistence of local variables during the lifetime of the iterator, all local variables must \nbe replaced or shadowed by .elds. The amount of code required to de.ne a custom itera\u00adtor by implementing \nthe iterator interfaces (Figure 10) illus\u00ad trates the increase in productivity that can be gained by \nab\u00adstracting over this using iterator generators (Figure 9). The projection to Java/bytecode that realizes \nthis abstraction is relatively straightforward, as it maintains the structure of the original method, \nunlike in our earlier source-to-source ap\u00adproach. On the other hand, this approach also avoids the complexity \nof the low-level, purely bytecode-oriented ap\u00adproach, eliminating the need for special libraries and \nus\u00ading the convenience and familiarity of the Java language for most part of the implementation.  2.4 \nAssimilating Expression-Level Extensions Embedded domain-speci.c languages add to the expres\u00adsivity of \ngeneral-purpose languages, combining domain\u00adspeci.c constructs with the general-purpose expressivity \nof the host language. Examples include embedded database queries, or integrated regular expressions. \nWe have explored DSL embedding with Java as a base language and described MetaBorg [7], a general approach \nfor DSL embeddings. In that context, we have coined the term assimilation for the transformation that \nmelds the embedding with its host code. Assimilation preserves the semantics of language extension constructs, \nwhile making it possible to compile them using the base language compiler. Small, statement-or expression-level \nlanguage exten\u00adsions are especially well-suited for assimilation. They can often be assimilated locally \nto an implementation in the host System.out.println(e1 ?? e2); (a) The ?? operator, with operands of \ntype T. System.out.println( {| T lifted = e1; // evaluate e1 only once if (lifted == null) lifted = e2; \n| lifted |}); (b) In-line assimilation to an expression block. T lifted = e1; if (lifted == null) lifted \n= e2; System.out.println(lifted); (c) Lifted to pure Java. Figure 11. Assimilation using statement-lifting. \nlanguage (often API-calls), without disturbing the surround\u00ading code. However, for speci.c kinds of language \nextensions this is not possible. One class of such extensions is that of language extensions that take \nthe form of expressions but re\u00adquire assimilation to statements in the host language (e.g., to use control \n.ow constructs or declare local variables, which is not possible in expressions). In-place assimilation \nto an expression does not suf.ce in these cases, because Java and similar languages do not allow nesting \nof statements in ex\u00adpressions. One technique to overcome this problem is using an intermediate syntax \nin the form of expression blocks [7]. These enable the use of statements in expressions, facilitat\u00ading \nin-line expression assimilation. Expression blocks take the form {| statements | expression |} where \nthe statements are executed before the evaluation of the expression, and the value of the expression \nblock is the value of the embedded expression. A separate, generally applicable normalization step in \nthe form of statement-lifting can be used to lift the statement component of the expression to the statement \nlevel [7]. Consider for example the C# coalescing operator: e1 ?? e2 This operator returns the value \nof expression e1 if it is non-null, or otherwise the value of e2. It could be used in conjunction with \nJava s nullable (or boxed ) types and its support for auto-unboxing, providing a default value when converting \nto a non-nullable (i.e., primitive) type. Consider Figure 11, which shows how the coalescing operator \ncan be assimilated to regular Java statements. In Figure 11(a), the operator is used in a method call. \nUsing an expression block, it is straightforward to translate it in-line to regular Java statements, \nas seen in Figure 11(b). Finally, in Figure 11(c), the expression block is lifted to the statement level. \nIterator<Integer> it = ...; for (int i=i0; it.hasNext(); i = it.next() ?? 0) { ... } (a) The coalescing \noperator in a loop. Iterator<Integer> it = ...; Integer lifted = next(); if (lifted == null) lifted = \n0; for (int i = i0; it.hasNext(); i = lifted) { ... } (b) Incorrect program after statement-lifting. \nFigure 12. Statement-lifting in a for loop. Unfortunately, proper statement-lifting is not trivial to \nimplement: it requires syntactic knowledge of every lan\u00adguage construct that an expression may appear \nin, includ\u00ading various statements, other expressions, and possibly other language extensions. Furthermore, \nit requires seman\u00adtic knowledge of the language, as it is not always suf.cient to simply move statements \nto the syntactic statement level. For instance, the simple lifting pattern from [7] cannot be applied \nif the expression is the operand of short-circuiting operators (e.g., || in Java) or the conditional \nof a for loop (see Figure 12). In these cases, simply lifting the translated statements to the statement \nlevel changes the semantics of the program. In the bytecode core language, there is no statement level \nor expression level. This distinction only exists in Java source code, and is simply a consequence of \nthe Java syn\u00adtax. Thus, we can overcome this limitation by assimilating the operator directly to the \nbytecode core language, using instructions in place of the original operator (see Figure 13). Given that \nthe core language is enriched with constructs of the more convenient Java language, we can also apply \nthe complete, mixed language to synergistic effect and assim\u00adilate the operator in a more elegant fashion. \nConsider Fig\u00adure 14, which assimilates the coalescing operator to Java statements, embedded in a bytecode \nblock. In addition to these statements, we use a push pseudo-instruction to place the lifted variable \non the stack (we elaborate on the role of the stack and the push instruction in Section 3.1). This value \nforms the result of the expression, and is used as the argument of the call to System.out.println. Like \nexpres\u00adsion blocks, the bytecode fragment can contain any num\u00adber of statements and a single resulting \nexpression, making normalization of the expression block extension to a byte\u00adcode block trivial. As such, \nthis pattern effectively does away with the complications associated with statement-lifting, and thereby \nsimpli.es the implementation of expression-level language extensions using statements. System.out.println(`[ \npush `e1; astore lifted; ifnull else; push `e2; goto endif; else: aload lifted; end: ]); Figure 13. Assimilation \nof the ?? operator to bytecode. System.out.println(`[ `T lifted = e1; `if (lifted == null) lifted = e2; \npush `lifted; ]); Figure 14. Assimilation to bytecode-embedded Java. 3. Realization of the Base Compiler \nThe Dryad Compiler operates by normalization of the mixed Java/bytecode language to its core (bytecode) \nlanguage. We implemented a parser for the language as an extension of the regular Java language, using \nthe modular syntax de.nition formalism SDF [4]. The language is normalized through normalization rules \nexpressed in the Stratego [6] program transformation language. In the remainder of this section, we give \nan overview of the design of the language and its compiler. 3.1 Language Design Key to compilation by \nnormalization is the notion of a core language that is often incrementally extended with new abstractions \nto form a rich, high-level language. For the Dryad compiler, we build upon the existing bytecode lan\u00adguage, \nan assembly-like core language, and mix it with the standard Java language [14]. The two syntax forms \nare inte\u00ad grated using the backtick operator `, which toggles the syn\u00adtax form of a fragment of code. \nFigure 15 gives an overview of the syntax of the language. In this .gure we use italics to refer to other \nsymbols in the syntax, and an overline to indicate lists of symbols (e.g., Tx indicates a list of type/i\u00addenti.er \npairs). For brevity, we left out some of the more advanced Java language constructs. For mixing of class, \nmethod, and .eld declarations, the ` notation is optional, and was also left out from this overview. \nThe bytecode assembly language we use shares similar\u00adities with existing languages such as Jasmin [25] \nand its derivatives. It provides a somewhat abstracted representation of the underlying binary class \nformat. For instance, it allows the use of symbolic names rather than relative and absolute offsets for \nlocals and jump locations. Like Jasmin, our rep\u00ad General p ::= cd Program (start symbol) T ::= C | void \n| int | int | long | double | .oat | boolean | char | byte Types Java cd ::= class C extends C { fd \nmd cd } Class declaration md ::= C (Tx) { super (e); s } | Tm (Tx) { s } Method/constructor declaration \nfd ::= Tf ; Field declaration s ::= { s } Statement block | e; Expression statement | Cx = e; Local variable \ndeclaration | if (e) s else s | while (e) s | for (e; e; e) s | return; | return e; Control .ow | throw \ne; Throw exception | synchronized (e) { s } Synchronization | try { s } catch(Cx) { s } Exception handling \n| l: Label e ::= x = e | x Local variables | (T) e Cast | e + e | e -e Basic operators | e.m (e) Method \ninvocation | new C (e) Object creation Bytecode cd ::= class.le C extends C .elds f methods md Class \ndeclaration md ::= C (Tx : T)[ I ] | <init> (Tx : C)[ I ] Method/constructor declaration fd ::= f : T \nField declaration I ::= catch (l : C)[ I ] Exception handling | (see Figure 16) Bytecode instruction \nMixing s ::= `I | `[I] Bytecode statement e ::= `I | `[I] Bytecode expression I ::= `s | `[s] | push \n`e Embedded Java Tracing s ::= trace (o)[ s ] Statement trace e ::= trace (o)[ e ] Expression trace \nI ::= trace (o)[ I ] Instruction trace md ::= trace (o)[ md ] Method trace o ::= s @ loc | e @ loc | \nI @ loc | md @ loc Trace originating code loc ::= path:i:i Location speci.cation Figure 15. Syntax for \nthe mixed Java/bytecode language. Arithmetic Stack Arrays Control .ow Comparison Conversions/truncations \nadd ldc c aload ifeq l lt x2i div ldc2 w c astore ifne l gt x2l mul new C arraylength goto l eq x2d neg \npop newarray T l: le x2f rem pop2 multianewarray T n athrow ge i2b sub dup return ge i2s shl dup x1 Fields \nxreturn i2c shr dup x2 getstatic C.f : T tableswitch n to n: l default: l Miscelleneous checkcast C ushr \ndup2 putstatic C.f : T lookupswitch n : l default: l instanceof C xor dup2 x1 get.eld C.f : T monitorenter \nInvocations and dup2 x2 put.eld C.f : T monitorexit invokevirtual C.m(T : T) or swap nop invokestatic \nC.m(T : T) inc breakpoint invokeinterface C.m(T : T) dec invokespecial C.m(T : T) Figure 16. The (reduced) \nbytecode instruction set.  resentation allows constants to be used in-line, eliminating the need to \nmaintain a separate constant pool with all con\u00adstants and types used in a class. Still, our representation \nre\u00admains close to the underlying binary bytecode instructions. Most notably, it preserves the stack machine-based \nnature of the instruction set. More abstract representations are for ex\u00adample provided by Soot [39], \nbut this does not match our design goal of exposing the complete core language func\u00adtionality. Instead, \nwe embrace the operand stack to provide low-level operations, and use it for interoperability between \nthe two languages. Figure 15 shows the basic elements of the bytecode language, while Figure 16 gives \nan overview of the instruction set. Note that this .gure shows a reduced bytecode instruction set, using \noverloaded instructions, on which we elaborate in Section 4.2. For compatibility, we also support the \ncomplete, standard set of instructions, which can be mapped to the reduced set. Interoperability between \nJava and Bytecode Java and bytecode have many of the same basic units of code, e.g. classes, methods, \nand .elds. In our mixed language, the Java and bytecode representations of these units can be combined \narbitrarily. How to combine the two languages at a .ner level of granularity, i.e. inside method bodies, \nis less obvious. In Java, a method body is a tree-like structure of statements that may contain leafs \nof expression trees. Bytecode meth\u00adods consist of .at, scopeless lists of instructions. Perhaps the most \nelementary form of mixing at this level is to allow bytecode fragments in place of statements, forming \nbytecode statements, and vice versa. This among other things al\u00adlows statement-level separate compilation \nand basic inser\u00adtion of source code into compiled methods. At the statement level, state is maintained \nthrough local variables and .elds. These can be shared between Java and bytecode statements, as the language \nuses a common, symbolic representation for both .elds and local variables. Expressions can be used as \nthe operands of statements or other expressions, passing a value to the enclosing con\u00adstruct. Bytecode \nexpressions are fragments of bytecode that can be mixed into expressions. They are conceptually sim\u00adilar \nto bytecode statements and share the same syntax for embedding. However, as expressions, they must produce \na resulting value. At the bytecode level, such values are ex\u00adchanged using the operand stack. For example, \nthe load con\u00adstant instruction (ldc) pushes a value onto the stack, and the add instruction consumes \ntwo values and pushes the addi\u00adtion onto the stack. Such instructions can be used to form a legal bytecode \nexpression: int i = `[ ldc 1; ldc 2; add ]; Vice versa, the push pseudo-instruction places the value \nof a Java expression onto the stack: push `\"Java\" + \"expression\"; void locals() { { // Java blocks introduce \na new scope `[ ldc 1; store var ]; System.out.println(var); } // var is out of scope and can be redefined \nintvar= 2; } Figure 17. Local variable scoping.  3.2 Name Management and Hygiene Local variables shared \nbetween Java and bytecode follow the standard Java scoping rules. Bytecode has no explicit no\u00adtion of \ndeclaration of variables, only of assignment (using the store instruction). In the mixed language, the \n.rst as\u00adsignment of a local variable is treated as its declaration, and determines its scope (see Figure \n17). In regular bytecode there exists no notion of local variable scoping; all scopes are lost in the \nnormalization process. To ensure proper hy\u00adgiene during normalization, this means that all local vari\u00adable \nidenti.ers both intermediate and user-de.ned need to be substituted by a name that is unique in the \nscope of the entire method. For the example in Figure 17, two unique variables can be identi.ed in separate \nscopes. After normal\u00adization, these variables get a different name. 3.3 Typechecking and Veri.cation \nTypechecking is an essential part of the compilation of a statically typed language. The Java Language \nSpeci.cation speci.es exactly how to perform name analysis and type\u00adchecking of the complete Java language \n[14]. The analy\u00ad ses provide type information required for the compilation (e.g., for overloading resolution) \nand give feedback to the programmer in case of errors. We employ a Stratego-based typechecker for Java, \nwhich is implemented as a tree traver\u00adsal that adds semantic information to the abstract syntax tree. \nThe typechecker for the mixture of Java source and byte\u00adcode is a modular extension of a typechecker \nfor Java source code. The source code typechecker is designed to handle language extensions by accepting \na function parameter that is invoked for extensions of the Java source language. The extension can inspect \nor modify the environment of the typechecker, recursively apply the typechecker, or com\u00adpletely take \nover the traversal of the abstract syntax tree. For the mixed Java/bytecode language, the extended lan\u00adguage \nconstructs are the mixing constructs of Figure 15, where bytecode is embedded in Java. If any of these \ncon\u00adstructs are encountered, the extension of the typechecker takes over the traversal by switching to \nthe bytecode ver\u00adi.er. The current typechecker environment is passed to the veri.er, ensuring all variables \nand other identi.ers are shared with the surrounding code. The veri.er in turn re\u00adturns the resulting \noperand stack of the bytecode fragment. For bytecode expressions, these must consist of a single type, \nwhich is passed back to the Java typechecker. Vice versa, the bytecode veri.er invokes the source code \ntype\u00adchecker for any embedded Java constructs, using it to re\u00adsolve the types of embedded Java expressions \nand variables declared in Java statements. Stack-Consistency of Mixing Constructs The bytecode veri.er \nensures correct stack behavior and type safety of a program. In the mixture of Java and bytecode, we \nimpose speci.c rules on the stack behavior to ensure safe interoper\u00adability and natural composability \nbetween Java and bytecode fragments. These are veri.ed by the bytecode veri.er. One restriction we impose \non bytecode expressions is that they must leave a single value on the stack, just like Java expressions3. \nLeaving no value on the stack is considered an error. Likewise, leaving more than one value on the stack \nis considered illegal as this can lead to a stack over.ow when it is not properly cleaned up. Unlike \nJava expressions, statements do not leave a value on the stack; they compile to a sequence of stack-neutral \nbytecode instructions. That is, they may use the stack for intermediate values during evaluation, but \nmust restore it to its original height afterwards. This ensures that all Java statements can safely be \nplaced in a loop construct, with\u00adout the risk of a stack over.ow or under.ow. Even more so, the JVM actually \nrequires that methods have a .xed max\u00adimum stack height, disallowing loops with a variable stack height \n[22]. For compound statements (e.g., for, while), stack-neutrality extends to the contained statements: \nthe in\u00adput stack of all nested statements must be the same as the stack outside the containing statement. \nThis restriction goes hand in hand with the JVM restriction that at any point in a method, the types \nof the values on the stack must statically be known, and must be the same for all incoming edges of the \ncontrol .ow graph. Any other jumps to a location are considered illegal. The restriction can always be \nsatis.ed on the statement level based on the property of stack-neutrality. A jump from one statement \nto another is therefore always legal. To preserve this property in the mixed language, we place the same \nrestriction of stack-neutrality on bytecode statements: only bytecode sequences that restore the stack \nheight are legal bytecode statements. This ensures that frag\u00adments of Java and bytecode can be composed \nnaturally with\u00adout risk of stack inconsistencies, and ensures support for ar\u00adbitrary control .ow between \nJava and bytecode statements. Veri.er Implementation The JVM speci.cation [22] in\u00ad cludes a description \nof how a bytecode veri.er should oper\u00adate. It describes the process as a .x-point iteration, where a \nmethod s instructions are iterated over a number of times un\u00adtil all possible execution paths have been \nanalyzed. Because of restrictions on the form of a method and its instructions, 3 Actually, void method \ninvocations are an exception to this, but these cannot be nested into other expressions. this is a straightforward \nprocess. One restriction is that for all instructions the effect on the stack can be statically de\u00adtermined. \nFor instance, for a method invocation instruction, this means that it must specify the arguments it takes \nfrom the stack and what the return type is. This also means that the veri.cation can be done in an intraprocedural \nsetting, us\u00ading a static, global environment. This allows it to be tightly integrated into the Java source \ncode typechecker, as it can be used to verify individual fragments at a time. We implement our analysis \nusing a monotone frame\u00adwork [18, 1]. This representation allows a generic formu\u00ad lation of such analyses \nusing a speci.c set of operators and constants for each framework instance. Bytecode veri.ca\u00adtion is \na forward data-.ow analysis, and assumes an empty stack at the beginning of a method. The operators that \nde\u00adtermine the types on the stack through .x-point iteration are de.ned as follows: The transfer function \ndetermines the resulting stack of an instruction, given the input stack. For instance, for an ldc instruction, \na single value is loaded onto the stack.  The join operation merges the stack at branch targets (i.e., \nlabels and exception handlers), unifying the types of the stack states if they are compatible (e.g., \nString and Integer unify to the Object type).  3.4 Source Tracing During compilation, code often undergoes \nmultiple normal\u00adization steps. If there are any errors in any of these steps, they should re.ect the \noriginating source code, and not the intermediate code. To maintain this information, we intro\u00adduce source \ntracing information that indicates the source of a fragment of code, in the form of a path and location \nof the originating .le and an abstract syntax tree reference. Source tracing is explicitly available \nas a language feature, using the trace keyword (see Figure 15). This ensures maximal, source-level interoperability \nwith other tools that may exist in the compilation chain. Consider Figure 18, which shows a class .le \ncompiled using the traits extension. In addition to language-level support, we provide an API to transparently \ninclude this information in the result of normalization rules in Stratego. Using this facility, extensions \nof the compiler can fall back on the error reporting and checking mecha\u00adnisms already provided by the \nbase compiler, and may catch these errors or to improve the user experience. Source tracing information \nis also used to generate de\u00adbugging information in the produced Java class .le. This takes the form of \na table that maps instruction offsets to orig\u00adinal source .le locations, and enables stepping through \nthe code using a debugger, as well as accurate position informa\u00adtion in run-time exceptions. 3.5 Data-Flow \nAnalysis on the Core Language Leveraging the bytecode veri.er and the source tracing fa\u00adcility, we implemented \nanalyses such as unreachable code classfile Shape methods trace (void draw() {...} @ TDrawing.java:2:2) \n[ public draw(void) [ ... ] ] . . . trace (... getVertices() {...} @ Shape.java:9:2) [ public getVertices(void) \n[ ... ] ] Figure 18. Compiled methods with source tracing. analysis and checking for de.nite assignment \nat the byte\u00adcode level. By performing these analyses on the core lan\u00adguage, we can formulate them as \na monotone framework in\u00adstance, or make use of the information already provided by the veri.er. Furthermore, \ndealing with a normalized form of the language, these analyses have fewer constructs to deal with, reducing \ntheir complexity. Reachability of statements can be determined by use of the regular bytecode veri.er: \nit returns a list of stack states, leaving the stack states of all unreachable instructions unini\u00adtialized. \nUsing source tracing information, the Java state\u00adments that generated such code (which is illegal in \nJava) can be reported as unreachable. For testing de.nite assignment, we formulated another monotone \nframework instance that maintains a list of all assigned variables, matching against the load and store \ninstructions. Through iteration it deter\u00admines whether or not variables are assigned before use, and \nif variables marked final are not assigned more than once. While the core language has no direct notion \nof final vari\u00adables, this information can be retrieved using the method s source tracing information. \nIn addition to veri.cation, we applied these analysis tech\u00adniques for optimizations on the core language, \nincluding dead code elimination and peephole optimizations. Simi\u00adlarly, other optimizations such as constant \npropagation can be implemented at this level. 4. Normalization Rules for Code Generation Using normalization \nrules, high-level language constructs can be rewritten to lower-level equivalents, e.g. from con\u00adstructs \nof language extensions to the Java/bytecode lan\u00adguage, and then to the core bytecode language. We express \nthese rules as Stratego rewrite rules, which take the form L: p1 . p2 where s where L is the name of \nthe rule, p1 is the left-hand side pattern to be matched against, and p2 is the result of the rewrite \nrule. The where clause s may specify additional conditions for the application of the rule, or may declare \nvariables to be used in the result. Using the technique of concrete syntax embedding, Stratego rewrite \nrules may use the concrete syntax of the language being transformed as normalize-finally : |[ synchronized \n(e){ bstm* } ]| . |[ Object locked = e; try { `[ push ` locked; monitorenter ]; bstm* } finally { `[ \npush ` locked; monitorexit ]; } ]| Figure 19. Normalization of the synchronized statement. patterns [43, \n6]. These concrete syntax fragments are parsed at compile-time and converted to equivalent match or build \noperations using the abstract syntax tree. These patterns are typically enclosed in semantic braces : \nnormalize-if : |[ if (e) s ]| . |[ if (e) s else ; ]| this rule normalizes the basic, single-armed if \nstatement to the more general two-armed if statement, with the empty statement in the else clause. This \nnormalization ensures that other rules only have to deal with the latter form. 4.1 Mixed Language Normalization \nRather than directly normalizing from high-level language constructs to bytecode, this is often done \nthrough a series of small normalization steps. Often, these are rules that produce a mixture of Java \nand bytecode, which is further normalized in later steps. Iterative rule application and leveraging the \nprimitives made available in the mixed Java/bytecode lan\u00adguage make it possible to normalize the complete \nlanguage using relatively small steps that focus on a single aspect. Consider Figure 19, which demonstrates \na normalization rule for the standard Java synchronized statement. It is rewritten to a mix of more low-level \nJava statements and the monitorenter and monitorexit unbalanced synchro\u00adnization instructions. The resulting \nfragments can in turn be normalized to core language constructs themselves. We apply normalization rules \nin both the core compiler as well as in the language extensions. For example, in Sec\u00adtion 2.2 we discussed \nthe extension of Java with traits, map\u00ad ping trait classes to Java abstract classes for separate com\u00adpilation. \nConsider Figure 20, which illustrates this map\u00ad ping by means of a normalization rule. This rule makes \nuse of a where clause, and depends on two helper functions: trait-name, which determines and registers \na new name for the resulting abstract class, and trait-methods, which determines the set of methods to \nbe included. Given the map\u00adping of traits to regular Java classes, the core normalization rules can proceed \nto normalize the result further down to bytecode. normalize-trait : |[ trait x trait* { method1* } ]| \n. |[ abstract class y { method2 * } ]| where y := <trait-name > x method2 * := < trait-methods > (method1 \n*, trait*) Figure 20. Normalization of traits to abstract classes. 4.2 Pseudo-Instruction Normalization \nThe JVM supports over two hundred instructions, ranging from very low-level operations, such as manipulation \nof the stack, to high-level operations, such as support for syn\u00adchronization and arrays. Many of these \ninstructions are spe\u00adcialized for speci.c types, such as the iadd and fadd in\u00adstructions that respectively \nadd two integers or two .oats. Rather than requiring the code generator to select the proper specialization \nof <T> add, we introduce overloaded pseudo\u00adinstructions to defer this to the .nal step of the compilation. \nThis simpli.es the implementations of speci.c language ex\u00adtensions, as they do not have to reimplement \nthis specializa\u00adtion process. The pseudo-instructions form a reduced set of only 67 es\u00adsential instructions \n(see Figure 16). These are normalized to regular bytecode instructions, based on the type information \nprovided by the veri.er. For this, the veri.er is extended with a transformation operator, which uses \nthe type information provided by the transfer function of the veri.er to replace all overloaded instructions \nwith type-speci.c, standard byte\u00adcode instructions. Consider Figure 21, which illustrates how very different \nbytecode instructions must be generated for identical Java expressions if they operate on different types. \nInstructions such as iload and iadd have a type pre.x, and the dup or dup2 instruction depend on the \nnumber of words that a value occupies (i.e., two for long values). In the reduced set, identical instructions \ncan be used for many such patterns, thus simplifying normalization rules from Java (or another language) \nto the instruction set, and reducing their total number. 5. Discussion Compilation by Normalization in \nPractice From an exter\u00adnal view, the Dryad Compiler has no discernible stages, and simply normalizes \nthe input code to resulting code that can be normalized again. At each normalization step, the trans\u00adformed \ncode forms a valid Java/bytecode program. This de\u00adsign enables extensions as well as built-in features \n to make use of a wider range of language constructs, and pre\u00advents scattering their implementation across \ndifferent compi\u00adlation stages. Furthermore, it allows for different (separate) compilation scenarios \nthan possible with conventional open compilers. Java Reduced instruction set Regular bytecode intVar++ \nload intVar iload 1 dup dup inc iconst 1 iadd store intVar istore 1 longVar++ load longVar lload 1 dup \ndup2 inc lconst 1 ladd store longVar lstore 1 Figure 21. Instructions generated for identical Java expres\u00adsions \nof type int and long.  Still, the internal architecture of the Dryad Compiler does employ separate, \ndiscernible components. For instance, it employs a (global) semantic analysis phase, based on the Dryad \ntypechecker component. As such, it does not con\u00adform to what may be an idealized image of how a normal\u00adizing \ncompiler should work: by pure, iterative application of declaratively de.ned normalization rules. As \nit is, the Dryad Compiler uses strategies to maintain a degree of control over the application order \nof normalization rules. To simplify their implementation, the rules are formulated without spe\u00adcial logic \nfor preserving the semantic information collected in the analysis. This means that in some cases, the \nanalyzer must be reapplied. While this may not be ideal, this architec\u00adture does not hinder the applications \npresented here: for the extensions, the Java/bytecode language acts as the interface of the compiler. \nThe internal implementation of the com\u00adpiler, how this language is normalized, and how this design may \n(and likely will) change in the future, is of no concern to the developer of an extension. Core Language-Based \nAnalysis We perform a number of data-.ow analyses and optimizations on programs after they have been \nnormalized to the core language. By doing these at this level they can be applied independently of the \nsource language. The analyses have fewer constructs to deal with than for source code, reducing their \ncomplexity. Still, reduc\u00ading a program to the bytecode form can also mean a loss of precision, due to \nthe missing high-level structure and the re\u00adduced size of the code window used by transfer functions. \nLogozzo and F\u00a8ahndrich [23] offer a discussion comparing the relative completeness of either approach. \nComposition of Language Extensions Modular de.nition of language extensions aids in the composability \nwith other extensions, possibly developed by different parties. Ideally, these can be imported into a \nlanguage without requiring any implementation level knowledge of the extensions. At the syntax level, \nthis can be achieved using a modular syn\u00adtax de.nition formalism such as SDF [4]. On the seman\u00ad tic level, \nthe primary determinant of the compositionality of extensions is the type of analysis and transformations \nthat are performed (global or local). For global transformations, an ordering of application must be \ndetermined, which re\u00adquires implementation-level knowledge. Thus, composable language extensions should \nuse primarily local transforma\u00adtions; composition of global transformations is an orthogo\u00adnal issue. \nUsing small normalization steps, facilitated by ex\u00adpression blocks, pseudo-instructions, and the increased \nex\u00adpressivity of the general Java/bytecode language, many ex\u00adtensions can be expressed using local transformations. \n6. Related and Future Work Compiler Extension Extensible compilers, such as the JastAdd Extensible Java \ncompiler [13], ableJ [42], Poly\u00ad glot [28], and OpenJava [36] provide a foundation for com\u00ad pilation \nof language extensions. Polyglot, ableJ, and Open-Java follow the front-end extension approach; they \noffer an extensible semantic analysis stage and aid in projection to the base language. JastAdd on the \nother hand provides its own, modular back-end. Our approach does not preclude the use of these tools, \nbut can add to them by offering at once the Java language for projection and direct use of the un\u00adderlying \nbytecode primitives or inclusion of compiled class fragments. Forwarding, as supported by ableJ, avoids \nthe need to im\u00adplement full semantic analysis for a language extension [42, 41]. Instead, forwarding \nallows the metaprogrammer to de\u00ad .ne the projection of a language construct to the base lan\u00adguage, and \nby default applies semantic analysis over the for\u00adwarded program fragment. Our work is related to forward\u00ading, \nas we similarly de.ne a mapping from a source lan\u00adguage to a base language, using normalization rules. \nSeman\u00adtic analysis can be performed on the projected code, and any errors can be reported in reference \nto the user code using source tracing. Unlike in ableJ, we introduce core language constructs into the \nsource language, to increase its expres\u00adsivity and to facilitate normalization to this core language \nform. Macro-based systems, such as JSE [3] and the Jakarta Tool Suite [33] implement direct projection \nto a base lan\u00ad guage, forgoing semantic analysis. This can lead to confus\u00ading error messages in references \nto locations and constructs in the generated code. We support semantic analysis on the source language \nusing an extensible typechecker and veri\u00ad.er, and provide source tracing facilities to ensure any errors \non code resulting from transformations can be traced back to user code. Source tracing is a technique \nrelated to origin track\u00ading [40], which maintains the origins (i.e., source terms and positional information) \nof terms during rewriting. We extend this notion by de.ning language constructs to main\u00adtain and share \nthis information at the source level, to help interoperability between tools and simplify the internal \nrep\u00adresentation. Language Composition A similarity can be drawn be\u00adtween the mixed Java/bytecode language \npresented here and existing languages such as C++ and Ada that allow inline assembly code languages. \nThese too can be used for op\u00adtimizations or obfuscation and provide access to low-level operations. Assembly \ncode, however, is a representation of instructions for a speci.c CPU architecture, and therefore is much \nmore low-level and less safe than bytecode. This makes it more dif.cult to use it for composition of \nsource and compiled code. The Java/bytecode language also shows similarities with the Jeannie language \n[17], which integrates Java with C code, compiling to code that makes use of the Java Native Interface \n(JNI) [21], the JVM s standard foreign function in\u00ad terface. The C language can be used for accessing \nplatform functionality or (legacy) libraries, but does not actually form the core language of the Java \nplatform. Similar to the Dryad Compiler, Jeannie performs semantic analysis on the com\u00adbined language, \nand introduces a number of bridging con\u00adstructs for conversions and interoperability between the two \nconstituent languages. Different Platforms We have applied the compilation by normalization architecture \nfor the Java platform. Java pro\u00advides a safe, mature, and ever-evolving platform for develop\u00ading applications, \nand has been frequently used to host other languages and language extensions [15, 24, 32]. A similar \narchitecture can be realized for other platforms based on a bytecode language. For instance, .NET may \nbe an interest\u00ading case given it was designed from the ground up to host multiple languages, and includes \nsupport for unsafe code, which allows direct manipulation of object pointers. Other platforms provide \na more high-level intermediate language, such as the Glasgow Haskell Compiler CORE language [31]. CORE \nis not a strict subset of the Haskell language and can\u00adnot be directly compiled [38], but this and similar \nlanguages make good candidates for use as a core language that is grown to a more extensive, (not necessarily \nexisting) high\u00adlevel language, by introducing new abstractions, while pre\u00adserving the core functionality. \nOther Language Extensions We presented a number of compiler extensions, demonstrating how the Dryad Com\u00adpiler \ncan facilitate their implementation. Future work could include other extensions, such as adding full \nsupport for aspect-oriented programming (AOP) [20], building upon the implementation of open classes \n(i.e., intertype declarations in AOP) and composition of code at the statement and ex\u00adpression level. \nFull aspect weaving can be performed by composition of compiled classes with aspect source code, or vice \nversa. A related design is used for the AspectBench Compiler (abc) [2], which applies aspect weaving \non the Jimple language, a three-address (stackless) representation of bytecode. The abc compiler uses \nSoot [39] to (de)compile Java and bytecode to Jimple. Thereby they avoid some of the complexities associated \nwith bytecode-level weaving. Using the Java/bytecode language instead would enable the direct insertion \nof regular Java code into class .les for both ad\u00advice and any dynamic checks or other wrapping code that \naccompanies it. By providing typechecking and veri.cation for the combined language, as well as the guarantee \nof stack\u00adneutrality of inserted statements, we provide the same level of safety as is possible with weaving \nusing Jimple. Similar techniques can be used in the implementation of annotation processing tools (such \nas Java APT [35]), which typically operate purely based on source code. There is a growing interest in \nrunning dynamic pro\u00adgramming languages on the JVM. These compilers, such as Jython [30] for the Python \nlanguage, typically compile to Java source code. Using the Dryad Compiler as a basis, such compilers \ncan make use of speci.c bytecode features and optimizations, such as the newly considered dynamic in\u00advocation \ninstruction.4 Similarly, other Java code generators could be retro.tted to generate selected bytecode \nfeatures. The F2J Fortran compiler, for instance, used to generate Java source code, before it was re-engineered \nto directly generate bytecode instead, to support goto instructions and speci.c optimizations [32]. JCilk \nalso generates Java code, to assim\u00ad ilate an extension with fork-join primitives and exception handling \nfor multithreaded computations [10]. Similar to the problems with implementing the yield statement as \na source-to-source transformation (Section 2.3), this signif\u00ad icantly complicates the control .ow semantics. \nHowever, rather than directly generating bytecode for the complete JCilk language, the current implementation \ndepends on a modi.ed version of the GNU Compiler for Java as its back\u00adend. It would be straightforward \nto use the Dryad Compiler instead, making use of the mixed language and introducing support for source \ntracing. Java s source model of single inheritance with interfaces does not always match that of a given \nlanguage that is in\u00adtended to target the JVM. For instance, a restriction of Java interfaces is that \nonly one method with a given signature may exist. This renders interfaces incompatible if they de.ne \nmethods with the same signature but with a different return type. For generated code, such as interfaces \ngenerated from annotations, this can be a restricting or incalculable factor. At the bytecode level, \nthis restriction does not exist. Addi\u00adtionally the synthetic modi.er, used for instance on byte\u00adcode \nmethods of inner classes, can be used to mark methods inaccessible from user code (JVM Spec. [14], \u00a74.7.6). \nIt can be used to hide multiple methods with the same signature, and can enable friend class semantics \nfor generated code. 7. Conclusions To increase programmer productivity, language extensions, both domain-speci.c \nand general-purpose, have been and continue to be developed. These may generate source code or bytecode; \neither approach has its advantages. Mixing 4 JSR 292: http://www.jcp.org/en/jsr/detail?id=292. source \ncode and bytecode, a new language can be formed that has a synergistic effect, resulting in a language \nthat at once provides the low-level expressivity of bytecode and the convenience and familiarity of Java. \nThe combined language allows rapid development of language extensions through normalization steps that \ncan remain loosely coupled from the base compiler. Using intermediate forms such as ex\u00adpression blocks \nand pseudo-instructions, these steps remain relatively small and maintainable. Mixing source and bytecode \nopens the doors for new, more .ne-grained forms of separate compilation, providing a foundation for composition \nof source and bytecode classes up to instruction-and expression-level precision. By use of source tracing, \nthese composed fragments of code can be traced back to their original source .les, enabling accurate \nlocation information for error messages and debugging. Acknowledgements This research was supported by \nNWO/JACQUARD projects 612.063.512, TFA: Transforma\u00adtions for Abstractions, and 638.001.610, MoDSE: Model-Driven \nSoftware Evolution. References [1] A. V. Aho, M. S. Lam, R. Sethi, and J. D. Ullman. Compilers: Principles, \nTechniques, and Tools (2nd Edition). Addison Wesley, August 2006. [2] P. Avgustinov, A. S. Christensen, \nL. Hendren, S. Kuzins, J. Lhot\u00b4ak, O. de Moor, D. Sereni, G. Sittampalam, ak, O. Lhot\u00b4and J. Tibble. \nabc: an extensible AspectJ compiler. In Aspect-oriented software development (AOSD 05), pages 87 98, \nNew York, NY, USA, 2005. ACM. [3] J. Bachrach and K. Playford. The Java syntactic extender (JSE). In \nProceedings of the 16th ACM SIGPLAN conference on Object oriented programming, systems, languages, and \napplications (OOPSLA 01), volume 36 of ACM SIGPLAN Notices, pages 31 42, New York, NY, USA, 2001. ACM. \n[4] M. G. J. van den Brand, J. Scheerder, J. Vinju, and E. Visser. Disambiguation .lters for scannerless \ngeneralized LR parsers. In N. Horspool, editor, Compiler Construction (CC 2002), volume 2304 of Lecture \nNotes in Computer Science, pages 143 158, Grenoble, France, April 2002. Springer-Verlag. [5] M. Bravenboer, \nE. Dolstra, and E. Visser. Preventing injection attacks with syntax embeddings. A host and guest language \nindependent approach. In J. Lawall, editor, Generative Programming and Component Engineering (GPCE 2007), \npages 3 12, New York, NY, USA, October 2007. ACM. [6] M. Bravenboer, K. T. Kalleberg, R. Vermaas, and \nE. Visser. Stratego/XT 0.17. A language and toolset for program transformation. Science of Computer Programming, \n72(1\u00ad2):52 70, June 2008. Special issue on experimental software and toolkits. [7] M. Bravenboer and \nE. Visser. Concrete syntax for objects. Domain-speci.c language embedding and assimilation with\u00adout restrictions. \nIn D. C. Schmidt, editor, Proceedings of the 19th ACM SIGPLAN Conference on Object-Oriented Programing, \nSystems, Languages, and Applications (OOP\u00adSLA 04), pages 365 383, Vancouver, Canada, October 2004. ACM \nPress. [8] C. Clifton. MultiJava: Design, implementation, and eval\u00aduation of a Java-compatible language \nsupporting modular open classes and symmetric multiple dispatch. Technical Report 01-10, Department of \nComputer Science, Iowa State University, Nov. 2001. [9] C. Clifton, T. Millstein, G. T. Leavens, and \nC. Chambers. Multijava: Design rationale, compiler implementation, and applications. ACM Transactions \non Programming Languages and Systems (TOPLAS), 28(3):517 575, 2006. [10] J. Danaher, I. Angelina Lee, \nand C. Leiserson. Programming with exceptions in JCilk. Science of Computer Programming, 63(2):147 171, \n2006. [11] A. B. Dov. infomancers-collections. http://code.google.com/p/infomancers-collections/. [12] \nS. Ducasse, O. Nierstrasz, N. Sch\u00a8arli, R. Wuyts, and A. Black. Traits: A mechanism for .ne-grained reuse. \nACM Transactions on Programming Languages and Systems (TOPLAS), 28(2):331 388, 2006. [13] T. Ekman and \nG. Hedin. The JastAdd extensible Java compiler. In Proceedings of the 22nd ACM SIGPLAN conference on \nObject-oriented programming systems and applications (OOPSLA 07), pages 1 18, New York, NY, USA, 2007. \nACM. [14] J. Gosling, B. Joy, G. Steele, and G. Bracha. The Java Language Speci.cation. Prentice Hall \nPTR, Boston, Mass., third edition, 2005. [15] J. C. Hardwick and J. Sipelstein. Java as an intermediate \nlanguage. Technical Report CMU-CS-96-161, School of Computer Science, Carnegie Mellon University, August \n1996. [16] Z. Hemel, L. C. L. Kats, and E. Visser. Code generation by model transformation. A case study \nin transformation modularity. In J. Gray, A. Pierantonio, and A. Vallecillo, editors, Proceedings of \nthe International Conference on Model Transformation (ICMT 2008), volume 5063 of Lecture Notes in Computer \nScience, pages 183 198. Springer, June 2008. [17] M. Hirzel and R. Grimm. Jeannie: granting Java Native \nInterface developers their wishes. In R. P. Gabriel, D. F. Bacon, C. V. Lopes, and G. L. S. Jr., editors, \nObject-Oriented Programming, Systems, Languages, and Applications, (OOP\u00adSLA 07), pages 19 38. ACM, 2007. \n[18] J. B. Kam and J. D. Ullman. Monotone data .ow analysis frameworks. Acta Inf., 7:305 317, 1977. [19] \nL. C. L. Kats. java-csharp: C#-inspired language extensions for Java. http://strategoxt.org/Stratego/JavaCSharp/. \n[20] G. Kiczales, J. Lamping, A. Menhdhekar, C. Maeda, C. Lopes, J.-M. Loingtier, and J. Irwin. Aspect-oriented \nprogramming. In M. Aks\u00b8it and S. Matsuoka, editors, Proceedings of the European Conference on Object-Oriented \nProgramming (ECOOP 07), volume 1241 of Lecture Notes in Computer Science, pages 220 242. Springer, 1997. \n[21] S. Liang. Java Native Interface: Programmer s Guide and Reference. Addison-Wesley Longman Publishing \nCo., Inc., Boston, MA, USA, 1999. [22] T. Lindholm and F. Yellin. The Java Virtual Machine Speci.cation. \nAddison-Wesley Longman Publishing Co., Inc., Boston, MA, USA, second edition, 1999. [23] F. Logozzo and \nM. F\u00a8ahndrich. On the relative completeness of bytecode analysis versus source code analysis. In L. Hendren, \neditor, Compiler Construction (CC 08), volume 4959 of Lecture Notes in Computer Science, pages 192 212. \nSpringer, 2008. [24] J. Melton and A. Eisenberg. Understanding SQL and Java Together: A Guide to SQLJ, \nJDBC, and Related Technologies. Morgan Kaufmann, 2000. [25] J. Meyer and T. Downing. Java Virtual Machine. \nO Reilly &#38; Associates, Inc., Sebastopol, CA, USA, 1997. [26] J. Miecznikowski and L. Hendren. Decompiling \nJava using staged encapsulation. Workshop on Decompilation Tech\u00adniques, appeared in Proceedings of the \nWorking Conference on Reverse Engineering (WCRE 01), pages 368 374, 2001. [27] S. Murer, S. Omohundro, \nD. Stoutamire, and C. Szyperski. Iteration abstraction in Sather. ACM Transactions on Programming Languages \nand Systems (TOPLAS), 18(1):1 15, 1996. [28] N. Nystrom, M. Clarkson, and A. Myers. Polyglot: An Extensible \nCompiler Framework for Java. Compiler Construction (CC 03), 2622:138 152, Apr. 2003. [29] M. Odersky \nand al. An overview of the Scala programming language. Technical Report IC/2004/64, EPFL Lausanne, Switzerland, \n2004. [30] S. Pedroni and N. Rappin. Jython Essentials. O Reilly Media, Inc., 2002. [31] S. L. Peyton \nJones and A. L. M. Santos. A transformation\u00adbased optimiser for Haskell. Science of Computer Program\u00adming, \n32(1 3):3 47, September 1998. [32] K. Seymour and J. Dongarra. Automatic translation of Fortran to JVM \nbytecode. In Joint ACM Java Grande ISCOPE 2001 Conference, Stanford University, California, June 2 4, \n2001, New York, NY 10036, USA, 2001. ACM. [33] Y. Smaragdakis and D. Batory. Mixin layers: an object\u00adoriented \nimplementation technique for re.nements and collaboration-based designs. ACM Transactions on Software \nEngineering and Methodology (TOSEM), 11(2):215 255, 2002. [34] G. Steele. Growing a language. Higher-Order \nand Symbolic Compututation, 12(3):221 236, October 1999. [35] Sun Microsystems. The annotation processing \ntool (apt). http://java.sun.com/j2se/1.5.0/docs/guide/apt. [36] M. Tatsubori, S. Chiba, K. Itano, and \nM.-O. Killijian. OpenJava: A class-based macro system for Java. In W. Cazzola, R. J. Stroud, and F. Tisato, \neditors, First OOPSLA Workshop on Re.ection and Software Engineering (OORaSE 99), volume 1826 of Lecture \nNotes in Computer Science, pages 117 133. Springer, Nov. 1999. [37] The XDoclet team. XDoclet: attribute-oriented \nprogramming. http://xdoclet.sourceforge.net/. [38] A. Tolmach. An external representation for the GHC \ncore language. http://haskell.org/ghc/docs/papers/core.ps.gz, September 2001. [39] R. Vall\u00b4ee-Rai, P. \nCo, E. Gagnon, L. Hendren, P. Lam, and V. Sundaresan. Soot -a Java bytecode optimization frame\u00adwork. \nIn CASCON 99: Proceedings of the 1999 conference of the Centre for Advanced Studies on Collaborative \nresearch, page 13. IBM, 1999. [40] A. van Deursen, P. Klint, and F. Tip. Origin tracking. Journal of \nSymbolic Computation, 15(5/6):523 545, 1993. [41] E. Van Wyk, O. de Moor, K. Backhouse, and P. Kwiatkowski. \nForwarding in attribute grammars for modular language design. In R. N. Horspool, editor, Proceedings \nof the 11th International Conference on Compiler Construction (CC 02), volume 2304 of Lecture Notes on \nComputer Science, pages 128 142, London, UK, 2002. Springer-Verlag. [42] E. Van Wyk, L. Krishnan, A. \nSchwerdfeger, and D. Bodin. Attribute grammar-based language extensions for Java. In E. Ernst, editor, \nEuropean Conference on Object Oriented Programming (ECOOP 07), volume 4609 of Lecture Notes on Computer \nScience, pages 575 599. Springer Verslag, July 2007. [43] E. Visser. Meta-programming with concrete object \nsyntax. In D. Batory, C. Consel, and W. Taha, editors, Generative Programming and Component Engineering \n(GPCE 2002), volume 2487 of Lecture Notes in Computer Science, pages 299 315, Pittsburgh, PA, USA, October \n2002. Springer-Verlag. [44] J. Warmer and A. Kleppe. Building a .exible software factory using partial \ndomain speci.c models. In J. Gray, J.-P. Tolvanen, and J. Sprinkle, editors, Proceedings of the 6th OOPSLA \nWorkshop on Domain-Speci.c Modeling (DSM 2006), volume TR-37 of Computer Science and Information System \nReports, pages 15 22, Finland, October 2006. University of Jyv\u00a8a. askyl\u00a8   \n\t\t\t", "proc_id": "1449764", "abstract": "<p>Language extensions increase programmer productivity by providing concise, often domain-specific syntax, and support for static verification of correctness, security, and style constraints. Language extensions can often be realized through translation to the base language, supported by preprocessors and extensible compilers. However, various kinds of extensions require further adaptation of a base compiler's internal stages and components, for example to support separate compilation or to make use of low-level primitives of the platform (e.g., jump instructions or unbalanced synchronization). To allow for a more loosely coupled approach, we propose an open compiler model based on normalization steps from a high-level language to a subset of it, the core language. We developed such a compiler for a mixed Java and (core) bytecode language, and evaluate its effectiveness for composition mechanisms such as traits, as well as statement-level and expression-level language extensions.</p>", "authors": [{"name": "Lennart C.L. Kats", "author_profile_id": "81381609357", "affiliation": "Delft University of Technology, Delft, Netherlands", "person_id": "P1223156", "email_address": "", "orcid_id": ""}, {"name": "Martin Bravenboer", "author_profile_id": "81100378172", "affiliation": "University of Oregon, Eugene, OR, USA", "person_id": "P1223157", "email_address": "", "orcid_id": ""}, {"name": "Eelco Visser", "author_profile_id": "81100561215", "affiliation": "Delft University of Technology, Delft, Netherlands", "person_id": "P1223158", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1449764.1449772", "year": "2008", "article_id": "1449772", "conference": "OOPSLA", "title": "Mixing source and bytecode: a case for compilation by normalization", "url": "http://dl.acm.org/citation.cfm?id=1449772"}