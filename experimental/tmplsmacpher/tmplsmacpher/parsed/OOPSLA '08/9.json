{"article_publication_date": "10-19-2008", "fulltext": "\n Contention-AwareScheduler:UnlockingExecutionParallelismin MultithreadedJavaPrograms FengXian,WitawasSrisa-an, \nandHongJiang DepartmentofComputerScience&#38;Engineering UniversityofNebraska-Lincoln Lincoln,NE68588-0115 \n {fxian,witty,jiang}@cse.unl.edu Abstract In multithreaded programming, locks are frequently used as \na mechanismforsynchronization.Becausetoday soper\u00adating systems do not consider lock usage as a scheduling \ncriterion,schedulingdecisionscanbe unfavorableto multi\u00adthreadedapplications,leadingtoperformanceissues \nsuch as convoying andheavylock contentionin systems with multi\u00adpleprocessors.Previous efforts to address \ntheseissues(e.g., transactional memory, lock-free data structure) often treat scheduling decisions as \na fact of life, and therefore these solutions try to cope with the consequences of undesirable schedulinginstead \nofdealing with theproblemdirectly. In this paper, we introduce Contention-Aware Scheduler (CA-Scheduler), \nwhich is designed to support ef.cient ex\u00adecution of large multithreaded Java applications in mul\u00adtiprocessor \nsystems. Our proposed scheduler employs a scheduling policy that reduces lock contention. As will be \nshown in this paper, our prototype implementation of the CA-Scheduler in Linux and Sun HotSpot virtual \nmachine only incurs 3.5% runtime overhead, while the overall per\u00adformance differences, when compared \nwith a system with no contention awareness, range from a degradation of 3% in a small multithreaded benchmark \nto an improvement of 15%in alargeJava application serverbenchmark. Categories and Subject Descriptors \nD.3.4[Programming Language]: Processors Run-time Environments; D.4.1 [Operating System]: Process Management \nConcurrency, Scheduling,Synchronization,Threads General Terms Experimentation,Languages,Performance Permission \nto make digital or hard copies of all or part of this work for personal or classroomuseisgranted withoutfeeprovided \nthat copiesarenot madeordistributed forpro.tor commercial advantage andthat copiesbearthis notice andthefull \ncitation onthe .rstpage.Tocopy otherwise,torepublish,topostonserversortoredistribute tolists, requiresprior \nspeci.cpermission and/or afee. OOPSLA 08, October19 23,2008,Nashville,Tennessee,USA. Copyright c &#38;#169; \n2008ACM978-1-60558-215-3/08/10. . .$5.00 1. Introduction To support portability, better security, and \nease of resource management, modern object-oriented languages such as JavaandC# utilize virtual machinetechnologiestoprovide \nexecutionenvironmentsbyemulatingsimpleinstruction sets such as Java bytecodes or the .NET intermediate \nlanguage. Because these virtual machines often provide complete ex\u00adecution environments, they also generate \nrich runtime in\u00adformationduring execution.Past studieshave shownthat a virtual machine often exploits \nsuch information to further optimize itself in subsequent runs or continuously during execution, especiallyinlong-runningprograms. \nFor example, method invocation information has been usedbyVMsto selectthe complexity ofdynamiccompila\u00adtion \noptimizations[18].Runtimebehaviorshavebeen used to select the optimalgarbage collection techniquefor \nan ap\u00adplication or an executionphase within an application[28]. Moreover,loggingofruntimeinformationhasbeen \nvaluable inhelpingprogrammersdetect andisolate errors as well as identifyperformancebottlenecks. Yet, \nsuchinformationhas rarelybeen exploitedbythe un\u00adderlyingoperating systems to create more ef.cient environ\u00admentsforhigh-levellanguage \nexecutions.Todate,the usage of this information has been limited to garbage collection tuning through \nvirtual memory managers[12,37].We see a greatopportunitytoleverage thisinformation tofurtherim\u00adprovethe \nef.ciency of thread-levelparallelism, aprogram\u00adming paradigm widely adopted today due to the emerging \npopularityof chip multiprocessor systems. In multithreaded applications, locks are frequently used as \na mechanismtopreventmultiplethreadsfromhavingcon\u00adcurrentaccessto shared resources andto synchronizethe \nex\u00adecution order of threads.For example, alock canbe used to protecta sharedcodesection, commonlyreferredto \nas acrit\u00adicalsection.When athreadwantsto accessthis codesection, itmust .rst obtainthelock.Ifitissuccessful,it \ncanperform memoryaccess.Ifitis notsuccessful(i.e.,lockcontention1), it eitherkeepstrying to obtainthelock(spin-locking) \noris suspendeduntilthelockbecomes available.Becausetoday s operatingsystemsdo notconsiderlockusage as \na scheduling criterion,they can: schedule threads that contend for currently locked re\u00adsources. In this \nscenario, multiple threads that try to ac\u00adcess the same critical section can be scheduled to run at the \nsame time ondifferentprocessors.The one that suc\u00adcessfully obtainsthelock continuesto execute, while \nthe ones thatfail to obtainthelock are suspended.  preempt an executing thread in the middle of a critical \nsection. In this scenario, a threadholding thelock to the critical section is suspended due to time quantum \nexpi\u00adration.This means thatif the operating system schedules otherthreadsthat needto accessthe same critical \nsection, they will alsobe suspended(i.e., convoyed).  While thereis alargebodyof work that addresses \ntheis\u00adsue oflock contentionby eliminating or minimizing the use oflocks(e.g.,transactionalmemoryandlock-freedatastruc\u00adtures[16,24]),thefocus \nof our workis entirelydifferent. Speci.cally, our research goal is not to avoid using locks, butinsteadtoproactivelyavoidlock \ncontentionby supplying the necessary runtimeinformation to the underlying operat\u00ading system so that it \ncan make better scheduling decisions. Our rationalefor takingthis approachis outlinedbelow: 1. It is \nconceivable that more informed scheduling deci\u00adsions can reduce the number of lock contention, lead\u00ading \nto greater execution parallelism and improved over\u00adall performance. However, it is unclear whether such \nimprovement outweighs the additional scheduling com\u00adplexity that may result in longer and non-deterministic \ndecision times. Over the past few years, we have seen that in certain types of applications, it is worthwhile \nto trade somebottom-lineperformanceforhigherprogram\u00admingproductivity(e.g.,the adoption ofgarbage collec\u00adtion \nand transactional memory), better security and cor\u00adrectness, andgreaterportability(e.g.,the adoption \nof vir\u00adtual machine monitors and high-level language virtual machines).Based on a similar argument, our \nworkinves\u00adtigates whetheritis worthwhileto trade the simplicity of modern schedulersforhigher executionparallelism. \n 2. Itis commonforhardware components supportinglarge\u00adenterpriseJava applicationstobe very specialized(e.g., \nalargenumberofprocessing cores,large storagespace, and wide network bandwidth). However, these systems \nfrequentlyutilizegeneric commercial operating systems. Itisdebatablewhetherthe stringentperformancerequire\u00adments \n(e.g., high throughput and short response time) of these server applications warrant customized operat\u00ad \n 1A lock contention occurs whenever one process or thread attempts to acquire alockheldby anotherprocessorthread[26]. \ning systems and virtual machines. As will be shown in thispaper, ourimplementation effort to extend a \nwidely\u00adadopted operating system (OS) and a commercial Java VirtualMachine(JVM) to achieve our researchgoalis \nquite modest, whileyieldingsigni.cantperformanceim\u00adprovements.As a result,theintegrationof ourproposed \nsolutioninto commercialoperatingsystemsis worth con\u00adtemplating. 3.TherearemanydeployedJavaand.NETbasedprograms \nthat already uselocksto manage concurrency.Ourpro\u00adposed solution will allow these applications to immedi\u00adately \ntake advantage of these bene.ts without having to rewrite the application code. Thispaper.Weintroduce \na Contention-AwareScheduler or CA-Scheduler.Ourproposedscheduler receivesinformation directly from the \nJVM to proactively reduce the possibility oflock contention by 1. dynamically clustering threads that \nshare similar lock\u00adprotected resources, and then serializing each cluster to reducethelikelihood oflock \ncontention, and 2. givinglonger executionquanta andhigher executionpri\u00adorityto threadsin the middle \nof critical sections.  We implemented aprototype of CA-Scheduler aspart of a multiprocessor version \nof the Linux scheduler with nec\u00adessary supportfromSunHotSpotJVM(presentedinSec\u00adtion2).Our analysis of \nthe complexity of theCA-Scheduler showed that it incurred only 2 to 3.5% runtime overhead (presentedinSection3).We \nthen evaluateditsperformance using seven multithreaded Java benchmarks in a computer system with16processors.Ourresults \nshowed thatthedif\u00adferencesin the overallperformance rangedfrom adegrada\u00adtion of3%in adesktopapplication \nwithlowlock contention to an improvement of 15% in a large application server benchmarkwithhighlock contention.The \nreductionsin the number oflock contention are38% and45%intwoJava ap\u00adplicationsserverbenchmarks,ECPerf \nand jAppServer2004, respectively(presentedinSection4). 2. Introducing CA-Scheduler In this section, we \nbrie.y describe an overview of the pro\u00adposed CA-Scheduler, discuss its design goals, and provide theimplementationdetails. \n2.1 Overview Recently, we have seen signi.cant advancements of virtual machinetechnologies,rangingfromhigh-levellanguagevir\u00adtual \nmachinessuch asJVM and .NETCLR tosystemvirtual machinessuch asMicrosoftVirtualPC[20] andTransmeta CodeMorphing[10].One \ncommon characteristic of these virtual machinesis that they are verydynamic.Variouspro\u00ad.ling and optimizationtechniquesare \nused toimproveper\u00adformance. For example, most JVMs often monitor garbage collectionperformanceand allocation \nratestodecideif the heapsize shouldbe adjusted.Moreover,modernJVMs mon\u00aditor lock-contention behavior \nto adaptively apply different locking mechanisms to maintaingoodperformance andim\u00adproveCPU utilization[4,11].Unfortunately, \nsuch runtime informationis usuallydiscardedatthe end of execution oris used only within virtual machines[3,27,28]. \n Figure1. AnOverview oftheproposedCA-Scheduler Webelievethatthis runtimeinformationisalsovaluable to \noperatingsystems.However,therehavebeen veryfew ef\u00adforts that allow operating systems to exploit such \ninforma\u00adtion. Two notable examples are efforts by Yang et al. [37] andGrzegorczyk et al. [12]to exploit \nvirtual memoryinfor\u00admation to adaptivelyidentify optimalheap sizes toimprove garbage collectionperformance.Another \nexampleis an ef\u00adfortby Xian et al.to exploitobjectallocationbehavioras a scheduling criterion to reduce \ngarbage collection overhead [36]. Based on the results of these efforts, we see a great opportunity to \nmake operating systems exploit runtime in\u00adformationgeneratedby virtual machinesin order to control thread \nexecution orders to avoidlock contention. Manymodernschedulersmakedecisionsaccordingtothe amount oftime \nathreadis spent ontheCPU andthe amount of time athreadis spentondoingI/O accesses[1].To sup\u00adport our \nproposed technique, we created three additional components,twointheHotSpotJVM and oneintheLinux kernel, \nto assist with making contention-aware scheduling decisions(seeFigure1): a Synchronization Monitor, which \nis implemented in the JVM to record synchronization ac\u00adtivities; a Contention Analysis Engine,whichis \nalsoimple\u00admented in the JVM to analyze the lock usage information and createCPU mappingplansthat willbe \nusedby our mod\u00adi.ed kernel; and a Thread Migration system call, which is implemented in the kernel to \nmigrate threads to the corre\u00adspondingCPUsbased ontheJVMgeneratedplan. We also modi.ed a typical scheduler \nto consider lock usage as a scheduling criterion(i.e.,theContention-Aware Scheduler in Figure 1). The \nmodi.ed JVM passes infor\u00admation to the kernel through two variables that contain the Thread-to-CPU Mapping \nPlan and the number of monitors heldbythecurrentlyexecutingthread(i.e.,lockCountinFig\u00adure 1). The next \ntwo subsections provide detailed informa\u00adtion about the design and implementation of each of these components. \n 2.2 DesignGoals The proposed CA-Scheduler is designed to reduce the oc\u00adcurrences oflock contentioninlarge \nmultithreadedJava ap\u00adplicationsbyallowing our modi.edJavaVirtualMachineto push pertinentruntimeinformationto \nourmodi.ed oper\u00adating system so thatit can accomplishtwo schedulingtasks: Task1 Preventingcontentiondue \nto concurrent execution of multiple threads on multiple processors. When threads thatshare alock-protectedresourceare \nexecutinginparallel, they may contend on that resource, causing threads that cannotobtainthelocktobe \nsuspended.(We refertothistype of contention as inter-processor contention.) One possible solutionis to \nnot schedule threadsthat may contend.Todo so,theschedulermustknowexactlywhenathreadisholding thelock \nandprecisely whichthreads share that samelock. In typical multiprocessor schedulers such as the one em\u00adployedinLinux, \na threadcanbe scheduled ondifferentpro\u00adcessors throughout its lifetime to achieve load balancing. Thus,dynamicallymaintainingthe \nnecessaryinformationto achieve our goal is likely to incur signi.cant runtime over\u00adhead.A studyhas shown \nthatpropagating suchinformation to everyprocessor canbequite expensive.Moreover,propa\u00adgationdelays can \nalso resultin unstable system states.[35]. We modi.ed Sun HotSpot JVM to cluster threads based onlockusage.In \nourapproach,eachcluster contains agroup of threads that are likely to contend with each others. The clustering \nand thread-to-CPU mapping information is then made available to the operating system kernel for the ac\u00adtualCPUassignment(moreinformationaboutthisprocessis \nprovidedinSections2.3.1 and2.3.2).By executing threads that share lock-protected resources (from now \non, referred to as shared resources)serially, we eliminate thepossibility that multiple threads executing \non different processors ac\u00adcess a shared resource simultaneously. Note that our study also revealed that \nthere areinstancesin which threads share resources across clusters(e.g., some resources are sharedby \nmost threads or all threads). In such a scenario, contention maybe unavoidable. One possible shortcoming \nof this approach is underuti\u00adlization of processing cores. The execution of threads may be serialized \neven though there are enoughprocessing units to supportgreaterparallelism.As willbe showninSection 4.2, \nwefoundthatin a server application utilizing the same number of threads as the number of processors, \nit is pos\u00adsible that the performance bene.t of reducing contention is greater than the performance bene.t \nof higher parallelism. In addition,inlargeapplicationserverapplications,the num\u00adber of threads tends \ntobe signi.cantlylarger than the num\u00adber ofprocessingcores; thus,proactively serializing threads still \nallowsprocessorstobe well utilized.Lastly, our system alsohas aload-balancingmechanismto equallydivide \nsome special clustersto multipleCPUs(moreinformation about theloadbalancing mechanismisprovidedinSection2.3.1). \nTask 2 Avoiding thread preemption while locks are be\u00ading held. With clustering, we can reduce lock contention \ndue to concurrently executing threads. However, in most schedulers, a thread can be preempted while it \nis holding locks, leading to contention when other threads sharing the same resources are scheduled.(We \nrefer to this type of con\u00adtention as intra-processor contention.)To reducethistype of contention, we \nadopted a quantum-renewing approach that giveslongerexecutionquantatothreadsin critical sections [34, \n2, 19]. Our scheduler checks ifan executing thread is holdingalock.Ifitis, the schedulerignores the timerinter\u00adrupt \nand schedulesthethreadfor one morequantum.If the renewedquantum expiresbefore the thread relinquishes \nthe lock,the scheduler can make morequantum renewals. Topreventotherthreadsfrom starving, our scheduler \ncan only make at most a prede.ned number of consecutive re\u00adnewal requests for an executing thread. We \ndiscovered that inJava, a thread alreadyholding alock sometimes acquires several morelocks.This meansthatthetimetakento \nrelease the .rstlock canbeverylong.Moreover,suspendedthreads that hold multiple locks can cause other \nthreads to convoy. Thus,itis morebene.cial to schedulethesethreads .rst so that locks are relinquished \nquickly. We propose a dynamic prioritization strategycalled Critical-Section-First (CSF)to schedule suspended \nthreads that are holding multiple locks at higher priority. With this approach, we can signi.cantly reduce \noccurrences of convoying. Moreover, our approach also shortens the periods that threads must stay in \nthe sus\u00adpended state waiting for locks to be released. In the next subsections, we detail the necessary \nmodi.cations made to theJVM and theOS to support theproposed tasks.We also provide information related \nto tuning the proposed system and analyzeits overhead.  2.3 ImplementationDetails We implemented the \nCA-Scheduler in the HotSpot JVM, whichis released aspart ofSun sOpenJDKproject(version 7)2 and the Linux \nmultiprocessor kernel version 2.6.20. In the rest of this section, we describe modi.cations made to HotSpot(from \nnow on, referred to as JVM)and the Linux kernel(from now on, referredto as kernel) to support the proposedCA-Scheduler. \n 2.3.1 JavaVirtualMachineModi.cations Toprovidethe necessaryinformationtothekernel, we mod\u00ad i.edtheJVM \ntoperformthefollowingfunctions: Record synchronization events. We modi.ed HotSpot to periodicallyrecordsynchronizationeventsforbothinterpre\u00adtation \nand dynamic compilation modes. We captured these eventsby augmentingthefunctionsinsideHotSpotthathan\u00addle \nmonitors and manipulate locks. Our recording mech\u00adanism is activated once the number of created threads \nis greater than a prede.ned value. In our experiment, we set this number to 10 because our smallest benchmark \nspawns 10 threads. Once the number of created threads has not changed for a long period of time, the \nmechanism is deac\u00adtivated.The mechanism canbe reactivatedif the number of threads changes again. When \nathread entersamonitor,theJVMinsertsthe ob\u00adject associated with the monitor into the synchronization\u00adevent \nvectors (seVectors).Eachthreadhas an associatedvec\u00adtor.Toprovide .exibilityinchoosing theamount of storage \noverhead,the size(referredto as s)of seVector canbetuned. We willdiscussthe empiricalprocesstoidentify \nageneral\u00adizedvalue of s inSection3.2. Form clusters of related threads. There are two steps in the clusteringprocess.Once \nthe number of synchronization events of a threadhas reached s (i.e., one ofthe seV ectors hasthe size \nofs),theJVMperformsthe .rststepbyprocess\u00ading each seVector to generate a corresponding contention vector \n(conVector).Eachentry of a conVector indicates the number of times that a thread has attempted to access \na sharedobject(i.e.,the numberofsynchronizationeventsper\u00adformed on the object). An example of converting \nseVectors to conVectors isprovidedinFigure2. S = 8 Figure2. ConvertingseVectors to conVectors.Inthis \nexam\u00adple, we assumethats =8.When aseV ector isfullyutilized (thatofT1in this example), all seV ectors \nareprocessed to generateconV ectors. 2SeeOpenJDKproject athttp://openjdk.java.net. The second step is \nclustering threads. There are many existing clustering algorithms such as K-means Fuzzy C\u00admeans andhierarchical \nclustering[5,15,6].However,these algorithms are too expensivetobedeployed on-lineinlarge multithreaded \nsystems. Another option is to employ of.ine pro.ling, whichapplies clustering algorithms to traceinfor\u00admationgeneratedduring \ntheprevious runs[9].This cluster\u00ading information is then used to perform various optimiza\u00adtionsinthe \nsubsequentruns.Whilepro.lingcanbe accurate, assuming that the runtime behavior does not change from one \nrunto another,it requiresan overhead of executing the pro.le run.Ifthe runtimebehavior changes, thepro.ledin\u00adformation \nmaynolongerbe useful. Toperformclusteringaccuratelyandcheaply,wepropose an on-line heuristic clustering \nalgorithm. The algorithm is based on an assumption that clusters canbeformedin such a waythatthreadsbelongingtotwodifferentclusters \nseldom contend, except for some objects that are shared by a large number of threads(referto asglobally-sharedobjects).This \nassumption is based on the result of our previous work on thread clustering[9].Our algorithm considers \nan objectto be globally shared if more than half of the total number of threads has tried to acquire \nthe monitor associated with this object. When forming clusters, these globally-shared objects are not \nconsidered as clustering criteria. Typically, the number of globally-shared objects is very small (see \nTable1). Our clustering mechanism works as follows. First, the JVMrandomlychooses athreadas a representativeofa \nclus\u00adter.It then calculates the similarity of each of the remaining threads to this thread.The similarity \nof two threads, T1 and T2,is a normalized vectorproduct that canbe calculated us\u00adingthefollowingformula: \nv 1 similarity(T1,T2)= T1[i] \u00d7 T2[i], 2 s i=1 where Tx[i] istheith entryofthe conVector ofthread Tx and \nv is the size ofthe conVector. Asde.nedearlier,s isthe size oftheseVector.Thus,the sum of a conVector \nis equal to s, resulting in similarity values ranging from 0 to 1. Also note that the JVM disables the \nrecording mechanism when it tries to form clusters. This is necessaryto maintain consistencyin all seVectors. \nThe rationalefor choosingtheproposedsimilarity metric istwo-fold.First,itonly considersthe same object \nentriesin both vectorswith non-zerovalues.A non-zerovaluemeans that these two threads(T1 and T2)access \nthe same objects and therefore can possibly contend on the same object.Sec\u00adond, the degree of similarity \nbetween T1 and T2 also indi\u00adcatesthelikelihoodthat T1 and T2 will contendon an object. Thatis, ahighersimilarity \nvalue meansthatthelikelihoodof contentionis alsohigher. If the similarity between the representative \nthread and a thread is greater than simThreshold, then this thread is placedin the same cluster as the \nrepresentativethread.(The tuning of simThreshold will be discussed in Section 3.2.) Theprocessis repeateduntileverythreadhasbeencompared \nwith therepresentativethread.Atthispoint,the .rstcluster is formed. Our JVM then repeats the same process \non the remaining threads that are not part of any cluster, until no more clusters canbeformed. The computation \ncomplexity of the clustering algorithm isO(Nt \u00d7Nc), where Nt isthetotalnumber ofJavathreads and Nc is \nthe total number of clusters. Generally, Nc is much smaller than Nt. Also note that it is possible that \nse\u00adlecting adifferent representativethread canyield adifferent clustering result. As will be shown in \nSection 3.2, the pro\u00adposed clustering scheme provides a good balance between accuracy(over75%in allapplicationswhencomparedto \nmanually-formedclusters) andlow clustering overhead(less than2% of overall executiontimein all applications).This \nhighdegree ofaccuracyindicates that thedifferencesdue to randomnessin selectingthe representativethreadmay \nnotbe signi.cant. Whileformingclusters, ourJVM also calculatesthe con\u00adtention intensity of each cluster \nby averaging all the simi\u00adlarity values within that cluster. This value is then used to classify whethera \nclusteris strongly contended(referred to as strong-contention) or weakly contended(referred to as weak-contention).This \nclassi.cationis usedtohelpwiththe load balancing process, in which our JVM generates infor\u00admationto map \nclusters toCPUs. Generate CPU mapping plan. Once clusters are formed, ourJVMcreatesa mappingplanthatachievesloadbalancing \nand minimizes contention across multipleprocessors.There are three steps in this process. In the .rst \nstep, our JVM segregates clusters into two groups: 1) a strong-contention group that comprises strong-contention \nclusters, and 2) a weak-contention group that comprises weak-contention clusters. Speci.cally, if the \ncontention intensity of a clus\u00adter is higher than the ciThreshold, the cluster is placed in the strong-contention \ngroup. If the intensity is less than ciThreshold, the cluster is placed in the weak-contention group(more \ninformation about tuning ciThreshold is given inSection3.2). Next, the JVM calculates the number of threads \nthat should be assigned to each processor in an ideal case. As stated earlier, Nc represents the number \nof clusters and Nt representsthe number of threads.The number ofprocessors isdenotedasNp.Ideally,there \nshouldbe Tavg = Nt threads Np assignedto eachprocessor.Supposethatprocessors are rep\u00adresented as P1, \nP2, ..., PNp andtheir workloads(i.e.,the number of threads) are TP1, TP2, ..., TPNp . We denote a sorted \nlist of underutilized processors as U. A processor is underutilized if its workload is less than Tavg \n. Since the initial workload of eachprocessor with respect to our appli\u00adcationis0, U isinitiallyequalto \nP1, P2, ..., PNp .Notethat U willdynamically changethroughoutthisprocess. Inthesecond step,theJVM mapsclustersinthestrong\u00adcontentiongrouptotheprocessors.Supposethatthere \nare m strong-contention clusters. Our JVM assigns these clusters toprocessorsinthefollowingmanner.First,thesem \nclusters are sortedfromthelargestsizetothe smallest size, whichare denotedas SC1, SC2, ..., SCm, respectively.The \nnumbers of threads in these clusters are T SC1, T SC2, ..., T SCm. Then, the JVM assigns the largest \ncluster SC1 to P1 and updates the workload of P1 (i.e., TP1 . T SC1). At the sametime,it also updates \nU byremoving anyprocessorthat has a workload equal to or greater than Tavg . Once these processors are \nremoved,U is rearrangedinthe orderof the lightestworkloadto theheaviest workload. Inthethird step,theJVMperformsloadbalancing \nwith weak-contention clusters.Assume that the current size of U is n; i.e., there are n processors that \nare underutilized. We denote these processors as PU1, PU2, ..., PUn in an in\u00adcreasing order of workload, \nwhich are TU1, TU2, ..., TUn, respectively. Note that at this point, the remaining proces\u00adsors(Np -n)shouldbefully \nutilized with strong-contention clusters and are not consideredas availableprocessorsinthe current step. \nSuppose that there are k clusters in the weak-contention group. They are denoted as WC1, WC2, ..., WCk \nfrom thelargest size to the smallest size.The numbers of threads in these clusters are TWC1, TWC2, ..., \nTWCk, respec\u00adtively. To achieve load balancing, there should be Tavg2 = nk TUi + TWCj i=1 i=1 threads \nassigned to each of the re\u00ad n maining processors. The load balancing process simply merges or splits \neach of the weak-contention clusters un\u00adtil the workload of each underutilizedprocessoris equal to Tavg2. \nIf some threads become inactive over time, it may become necessary to regenerate a new mappingplanto \nre\u00adbalancetheload. In terms of time complexity, the .rst step (segregating clusters into strong-and weak-contention \ngroups) has the complexity of O(Nc). The second step (strong-contention mapping) has two major tasks: \nsorting cluster (O(m \u00d7 log m))and updating U (O(m \u00d7 log Np)). Thus, the com\u00adplexityof the second stepis \nO(m \u00d7 log m + m \u00d7 log Np). For the last step, the time complexity is O(k \u00d7 log m + k \u00d7 log Np). As a \nresult, the total time complexity of clus\u00adtering and making the thread-to-processor mapping plan is O(Nc \n\u00d7 log Nc + Nc \u00d7 log Np) because (m + k)= Nc, (m \u00d7log m + k \u00d7log k) < (m + k) \u00d7log (m + k) or Nc \u00d7 log \nNc. Mark when threads arein critical sections. For eachJava thread, our JVM maintains a .eld called lockCount, \nwhich indicatesthe number ofuniquelocksheldby athread.When a thread has successfully entered a monitor, \nour JVM in\u00adcrements its lockCount value. When a thread exits a mon\u00aditor, our JVM correspondingly decrements \nits lockCount value.Therefore,we can usethe value of lockCountto check whether a threadis holdinglocks.If \nthe valueis 0, then the threadis notholdinganylock.Ifitis1,thethreadisholding alock.Ifitisgreater than1, \nthen the threadisholding mul\u00adtiple locks. This information will be used by our proposed Critical-Section-First \n(CSF) scheduler to dynamically ad\u00adjust the execution priority of a thread. More information about the \nCSF scheduler will be given in the next subsec\u00adtion.  2.3.2 KernelModi.cations Inthis section, wedescribe \nmodi.cations madetothekernel to (i) segregate Java threads from other types of threads; (ii) physically \nassignthreadstoCPUs; and(iii) makethe scheduler contention-aware. Create a system call to register Java \nthreads. The pro\u00adposedCA-Schedulerisbuilt on the top of thedefaultLinux scheduler.Thus, these two schedulingpolicies \ncan co-exist, and the CA-Scheduler can selectively apply its schedul\u00ading policy to the corresponding \nJava threads. However, in open systems such asLinux, there are also non-Java threads (e.g., essential \nservices) concurrently running with Java threads. To distinguish between Java and non-Java threads, we \ncreated a new system call, register thread, to allowJava threadstoset a .eld, isJava in the threaddata \nstructure(i.e., task struct inLinux). When aJava threadis executing,timerinterrupts still oc\u00adcur asinthedefault \nscheduler;however,theCA-Scheduler can choose to ignore the interrupts and continue to renew moretime \nslices as needed.Itis alsolikelythata system may have multiple Java applications running at the same \ntime. TheseJava applications mayrequiredifferentcon.gurations of the CA-Scheduler(e.g.,differentparameter \nvalues).The proposed CA-Scheduler is designed to be customizable so that eachJava applicationcancon.gureits \nown scheduling parameters.Toachievethis .exibility,wemodi.edHotSpot to create a metadata structurefor \neachVMinstantiation.The structure is instantiated through a new system call, regis\u00adter vm, during initialization \nand is referenced by the CA-Scheduler through a pointer .eld, jvm info in task struct to enforcethe correspondingpolicywhen \nathreadbelongingto aJava applicationis executed. Create a system call to map threads to CPUs. Because \nour mechanism to map thread to CPU is an on-line one, our JVM does not initially have the mapping information \nas it is incrementally generated during runtime. Therefore, at the beginning of execution, the default \nthread scheduler is used. Once the JVM has generated the mapping plan, threads may have to be migrated \nto different CPUs based on this plan. To accomplish this task, we created a system called migrate cluster \nto retrievethe mappingplanfromthe JVM(discussedinSection2.3.1) andtophysically assign threads toCPUs.Note \nthat eachJava threadispinned to the assignedprocessor untilthe next mapping callto ensurethat the default \nload-balancing mechanism used in Linux does not interfere with our mapping. Note that our system can \ncreate multiple mapping plans throughout execution of an application. Modify thread scheduler. After \nthreadshavebeen mapped to CPUs, the kernel independently schedules threads as\u00adsignedto eachprocessor.To \nreducethe occurrences ofintra\u00adprocessor contention, we propose a Critical-Section-First (CSF)schedulingstrategythatmakesthreadsrelinquishheld \nlocks as soon aspossible.This minimizesthe waitingtime of otherthreadsto obtaintheseheldlocks.There aretwo \nmajor components of the proposed Critical-Section-First schedul\u00adingstrategy: Dynamic prioritization mechanism. \nTo schedule threads in critical sections .rst, we need to prioritize the threads that havelargerlockCountvalues.In \nourimplementation, we ex\u00adtended the Linux scheduler todynamically adjust theprior\u00adity of each thread \naccording to its lockCount. The default Linux scheduler uses a priority-based round-robin schedul\u00ading \npolicy and adjusts thread priority based on the average sleeping time.We adoptedthe existing mechanisminLinux \ntoperformpriorityadjustment.Thatis, eachthread canget a bonustoincreaseitspriority.The calculationsof \nthebonus andpriority adjustment are asfollows: MAXbonus bonus =(p . lockCount) \u00d7 MAXlockCount prio = \np . static prio -bonus This formula indicates that the bonus is proportional to the lockCount value. \nIt maps a thread s lockCount to the range of0 to MAXBONUS .If a threadis notin any critical section(i.e., \nlockCount=0),thenitsprioritydoes not change. Otherwise, its static priority is subtracted, meaning that \nits priority rises. Note that bonus is between -5 and +5. By prioritizingthreadsin criticalsections, \nwe also avoidpriority inversion, in which a suspended thread with low priority holds alockthatis neededby \nahigherprioritythread[26]. Quantum renewal mechanism. We adopted a quantum re\u00adnewing mechanism that allows \na thread in a critical sec\u00adtion to continue execution until it exits the critical section [2, 19, 34]. \nOur scheduler checks the executing thread s lockCount when a timer interrupt occurs. If its lockCount \nis greater than 0, our scheduler ignores the interrupts and reschedulesthethreadfor anotherquantum.To \navoid starva\u00adtion and live-lock, we set the maximal number of renewals (maxRenewal)to .ve.Thatis,athreadwillbepreemptedaf\u00adterthe \nschedulerhas made .ve consecutiverenewalrequests. We selected.vebasedon our empirical studythat showsthat \na thread rarely needs more than .ve quanta to acquire and relinquish alock.   2.4 Summary TheproposedCA-Schedulerisdesignedto \nreducelockcon\u00adtentionin multithreadedJava applicationsthrougha collabo\u00adrationbetween our modi.edHotSpotJVM \nand our modi.ed Linux kernel. We extended HotSpot to include the follow\u00ading services:(i) monitoring synchronization \nevents;(ii)pro\u00adcessing the recordedinformation andperforminganalysis to clusterthreadsthat arelikelyto \ncontend; and(iii)generating thread-to-CPUmappingplansthatachievecontentionreduc\u00adtions andload-balancing.Wethen \nextendedtheLinuxkernel toincludethefollowingfeatures:(i) being able to receive thread-to-CPU mappingplansfrom \ntheJVM;(ii) executing the mappingplans and enforcing the contention-awarepol\u00adicy;(iii) being abletofallbacktothedefaultpolicy \nwhen native threads are scheduled or when the contention-aware policyis notapplicable; and(iv)having \nescape mechanisms to avoid starvation and livelock. The proposed system also contains several parameters \nthat users can tune to yield ef\u00adfective contention reductions. 3. Tuning andOverheadAnalysis The multi-core \nsystem used in our experiment has eight Dual-Core AMD Opteron processors (16 processors in total). The \nsystem has 32GB of physical memory. We chose four client-side benchmarks: eclipse, hsqldb, luse\u00adarch, \nand xalan from the DaCapo suite [7]3. We did not include any single-threaded benchmark because our pro\u00adposedcontention-awarepolicyis \nnotactivatedwhen running single-threadedJavaprograms.We also chosethree server\u00adside benchmarks: jbb2005 \n(from SPEC), jAppServer2004 (from SPEC), and ECPerf (from Sun Microsystems). It is worth noting that \nwe initially set the size of Java heap to 64MBand allowedit tofreelygrow upto2GB as neededby the applications. \nWeexecutedthe .vestand-alonebenchmarks(eclipse, hsqldb, lusearch, xalan, and jbb2005)on the above system \nwith unmodi.edHotSpot and unmodi.edkernel(from now on, referred to as default) and the CA-Scheduler-enabled \n(from now on, referredto as CASe). As mentionedearlier, HotSpot is released as part of the OpenJDK 7 \nproject and theLinuxkernelisVersion2.6.20. For jAppServer2004and ECPerf, we set up a real-world Java \napplicationserverenvironment.The environment con\u00adsists of three machines: application server, database \nserver, and client. These machines are connected through an inter\u00adnal private network to minimize latencies \ndue to network congestion.The application serveris the above system con\u00ad.gured to run default and CASe.The \nserver machineis also con.gured with no othermajorservices(e.g.,DNS,mail, database) to create a real-world \nscenario where the applica\u00adtion serveris not likely to be used for other major services. The database \nserver is a Sun Blade with dual 2GHz AMD 3The version ofDaCapobenchmarks that we usedisdacapo-2006-10. \nBenchmark Workload size Number of threads Shared objects Number of globally contended objects Contention \nper object average min max eclipse (DaCapo) large 10 58 2 3 2 12 hsqldb (DaCapo) large 400 532 1 79 2 \n1020 lusearch (DaCapo) large 64 248 2 13 1 121 xalan (DaCapo) large 16 404 3 12 1 513 jbb2005 (SPEC) \n16 warehouses 16 2371 0 121 0 413 jServer2004 (SPEC) Tx=20 2848 11130 0 301 0 421 ECPerf Tx=20 1368 10456 \n0 271 0 357 Table 1. Contention characteristic of each benchmark. Note that the column Contention per \nobject reports the actual occurrences of contention observedinHotSpot. Opteron processors with 2GB of \nmemory; it runs Fedora Core 2 Linux. The client machine is a single-processor 1.6 GHzAthlon with1GB of \nmemory. We executed each application on default .ve times and on CASe .ve times. We then compared the \nbest run from CASe to the best run from default. It is worth noting that when the best run of CASe is \ncompared to the worst run of CASe and thebest run of default to the worst run of default, the performance \ndifferences range from 0.1% to 12.9% in allbenchmarks.Thebiggestdifference(12.9%)occurs when xalan is \nexecutedusingtwoprocessorsinCASe.Thebiggest difference in default (11.1%)occurs when lusearch is exe\u00adcuted \nwith twoprocessors.The averageperformancediffer\u00adence when1,2,4,8, and16processors are utilizedis5.1%. \nInterestingly, as the number of processors becomes larger than two(i.e., utilizing4,8, and16processors),the \naver\u00adageperformancedifference reducesto3.3%.In addition, we found thedifferencesinthroughputperformanceof \nserver\u00adside benchmarks to be less than 5%. This is because these benchmarks runfor along time(tens of \nminutesto afew hours), resultingin more consistentoverallperformances. 3.1 BenchmarkCharacteristics \nTable1describeseachbenchmark and thepossible sources of contention obtained through source codeinspection.The \nnumbers of threads generated by these benchmarks vary signi.cantly.In addition, the numbers of shared \nobjects and contended objectsalsovary signi.cantly.Also notethat we report the number of objects that \nare accessibleby multiple threads(shared objectsin thefourth column).However, not all of these objects \nexperience contention; therefore, we see that there are shared objects experiencing no contention in \njbb2005, jAppServer2004, and ECPerf. eclipse (DaCapo) is a simpli.ed integrated development environment(IDE) \nwithoutdisplayingGUI.Itperforms a subsetof .vetasksinIDE(setting up workspace,creating projects, andtypinghierarchytests,ASTtests, \nsearch tests). In our experiment, we con.guredeclipse to execute thefull set of tasks. Since these tasks \nare independent from each other, they only have a few occurrences of resource con\u00adtention[7]. hsqldb \n(DaCapo)is anSQLrelationaldatabase engine writ\u00adten in Java. Dacapo provides a modi.ed benchmark pro\u00adgramJDBCBenchto \ntest an hsqldbdatabase thatemulates a transactionprocessing system(TPS)[7].TheTPS database has 4 tables: \nbranch, teller and account and history. In our experiments, we ran 400 client threads. Each thread per\u00adforms \n50 transactions. Each transaction randomly chooses anaccount,tellerorbranch and updatesitsbalance.We \nex\u00adpected that threads, which tend to update the same table, wouldhave more contentionthan threads updatingdifferent \ntables. One major shortcoming of hsqldb is that while the database serveris multithreaded,the coredatabase \nengineis not.Thus, requests are serviced sequentially[17]. lusearch (DaCapo)is a tool that performs a \ntext search of keywords over theShakespeareCorpus(42documents)[7]. It creates multiple threads to search \nthesedocumentsinpar\u00adallel. It also maintains a hit queue, which ranks the docu\u00adments by the order of \nkeyword hits. Each thread updates the hit queue during search. Contention occurs when mul\u00adtiplethreads \nupdatethehitqueue concurrently.In our exper\u00adiments, we ran64threads. xalan (DaCapo) is an XSLT processor \nfor transforming XMLdocuments[7].Itpushessomepre-selected .lestoa workqueueandgeneratesseveralthreadstoparsethese \n.les independently.Once a threadhas completed transformation of a XML .le, it removes the .le from the \nwork queue. Contention occurs when threads update the work queue si\u00admultaneously.In our experiments,we \nmodi.ed the number ofpre-selected.lesto64 andthenumberof workthreadsto 16. jbb2005 (SPEC) is a Java-based \nbenchmark that models a wholesale company, with warehouses that serve a num\u00adber of districts. Each warehouse \nis implemented as a sim\u00adpledatabaseby usingHashMaps orTreeMaps.Each thread simulates a customer which \naccesses a .xed warehouse for thelifetime of the experiment[30].Giventhe nature of the benchmark, threads \naccessing the same warehouse should have more contentionthanthreads accessingdifferentware\u00ad houses.Inourexperiments,we \nused16 warehouses,result\u00adingin16activethreads. jAppServer2004 (SPEC) is a standardized benchmark for \ntesting theperformanceofJavaApplicationServers.It em\u00adulates an automobile manufacturing company and its \nasso\u00adciateddealerships[29].It complies with theJ2EE1.3 speci\u00ad.cation.Weranit onJBoss4 and usedMySQL5 \nasdatabase server.Thelevel of workload canbe con.guredby transac\u00adtion rate(Tx).This workload stresses \nthe ability of theWeb and EJB containers to handle the complexities of memory management, connection \npooling, passivation/activation, caching,etc.Thethroughput of thebenchmarkis measured inJOPS(joboperationsper \nsecond).We setTxto20, which generates 2848 threads. These threads will intensively con\u00adtend on sockets, \ncachepools, anddatabases. ECPerf was introduced by Sun Microsystems in 2001. It consists of a speci.cation \nand a benchmarking kit, which isdesignedspeci.callytotest and measureperformanceand scalability ofJ2EE \napplication servers.ECPerfdoes notpro\u00advide a workload related to web-tier and Java Message Ser\u00advice(JMS). \nSimilar to jAppServer2004, the level of work\u00adload canbe con.guredbytransactionrate(Tx)[31]. The throughput \nof the benchmark is measured in JOPS. We set Tx to 20, which generates 1368 threads. Notice that the \nnumberof threadscreatedby ECPerfis roughlyhalf of the number of threads createdbyjAppServer2004using \na simi\u00adlar workloadsetting.These threads also contend on sockets, cachepools, anddatabases.  3.2 PerformanceTuning \nThe proposed system introduces several parameters that must be tuned to achieve a balance between good \nperfor\u00admance andlow overhead.Inthis section, we studythe effects oftheseparameters onperformanceand clustering \naccuracy. Similarity Threshold: As a reminder, similarity between two threads is used as a metric to \ndetermine if two threads arelikelyto contend.Its valueis a normalizedproductoftwo conVectors, each of \nwhichis a counting vectorthatindicates the number of monitor entriesperformby aparticularthread on each \nshared object.Because theJVMderives conVectors fromprocessingseVectors, the size of seVector (s)canhave \na profound effect on the clustering accuracy and runtime overhead. First, weinvestigatedthe effects ofchangingthe \nvalues of s and simThreshold on the overhead to record synchroniza\u00adtion events,formclusters, andgenerateathread-to-processor \nmappingplan.We conductedour study onthreebenchmarks representingthreedistinct workloads and contentionbehav\u00adiors: \neclipse (representative of desktopapplication with 10 threads and very few occurrences of contention), \njbb2005 (representative of small server application with 16 threads and moderate contention), and jAppServer2004(representa\u00ad \n4See www.jboss.orgfor moreinformation. 5See www.mysql.comfor moreinformation.  tive of large server \napplications with thousands of threads andheavy contention). Figure3presents the execution overheadinside \ntheJVM (i.e. overhead for clusteringand generating thread-to-CPU mapping plan) over a wide range of \nseVector size(s) and simThreshold.Notethatthe overalloverhead(i.e.,overheads ofJVMandkernel)ispresentedinSection3.3.We \nonlyused foursimThreshold values(i.e.,0.1,0.2,0.3, and0.4)because our experimental result reveals that \nthe similarity between two threads rarely exceeds0.45. The graph shows an expected trend of increasing \nover\u00adhead withlarger s.Our experiment shows that the overhead isgenerallyless than2%of the overall executiontime \nwhen s isless than or equalto2048.Notethatineclipse,the over\u00adhead no longer increases when s is greater \nthan 2048. This isbecausethetotal numberof monitor entry eventsof each thread in eclipse is less than \n2048. Also, it is worth notic\u00ading that varying simThreshold has very little effect on the overhead in \neclipse, but has signi.cant effects on jbb2005 and jAppServer2004.In subsequent experiments, we set s \nto 2048basedon the results of this study. Another observation is that smaller simThreshold yields less \ncomputational overhead under a .xed value of s. With smaller simThreshold, threads are more likely to \nbe clus\u00adtered. Therefore, fewer clusters will be formed. Since the number of clustersdeterminesthe overhead \noftheprocessor assignment,fewer clusters resultin smaller overhead. Next, weinvestigatedthe effects of \nvarying the values of s and simThreshold onclustering accuracy.To conductthis investigation, we created \nan oracle for each benchmark by manually forming clusters using a combination of a brute\u00adforce approach \nand a source code investigation to establish thecontentioncharacteristics.Byinspectingthesourcecode, \nwe discovered that we can cluster threads based on task in eclipse. Therefore, threads that are created \nto perform a certain task are assigned to the same cluster. For jbb2005, we observed that threads participating \nin a transaction also share resources. Thus, we clustered threads based on the frequency with which they \naccessed each warehouse. For jAppServer2004and ECPerf, we observed that threads with the same functionality \nshare a large number of resources; thus, they are clustered together. Next, webuilt a relationship matrix, \nM of the manually\u00adformedclusters. M is a matrix of size n where n is number of threads. M is generated \nas follows: M[i][j] is 1(1. i,j . n) if threadi andj areinthe same cluster,indicating an inclusive relationship. \nOtherwise, M[i][j] is 0, denoting an exclusive relationship. In a similar fashion, we constructed another \nrelationship matrix, M1, thatisbased on the clustering result of ourpro\u00adposed algorithm. Once the two \nmatrices were constructed, we calculatedthe accuracy of our clustering algorithm using thefollowingformula: \nExecution overhead of Clustering (%) Execution overhead of Clustering (%)Execution overhead of Clustering \n(%) (a)eclipse seVector size (s) (b)jbb2005 seVector size (s) Figure 3. Analysis of runtime overhead \nof CA-Scheduler inside theJVM nn LL 1 accuracy = Aij , C2 n i=1 j=i+1 1, when M[i][j]= M1[i][j]where \nAij =0, when M[i][j] M1[i][j] = Aij indicates whetherthe relationship(i.e.,inclusive or exclusive)between \nthreadi andjis the samein M and M1. C2 refers to the total number of threadpairs, whichis equal n (n-1) \n to n \u00d7 2 . Therefore, accuracy indicates whether M1 is similar to M. In this scheme, the maximum accuracy \nis 1, meaningthat thetwo matrices,M and M1 are exactlythe same.Figure4 showsthe accuracyover wide-rangingvalues \nof s and simT hreshold. ciThreshold: Inthis experiment,thevaluesofs and simThresh\u00adold are set to 2048 \nand 0.3, respectively. As a reminder, ciThreshold is used to determine whether a cluster should be classi.edas \nstrong-contention or weak-contention.Thus, it canhaveaprofoundeffect onoursystem s ability toper\u00adformloadbalancing.Ifwe \nset ciThreshold to1.0, all clusters are classi.ed as weak-contention, meaning that every clus\u00adter can \nbe split to achieve a good balance. However, such action can resultin a muchhigher number ofinter-processor \ncontention. On the other hand, if we set ciThreshold to 0, all clusters are categorized as strong-contention, \nresultingin less effective load balancing. Figure 6 depicts the effect of varying ciThreshold values \nonthe overallperformanceand contention. 3.3 Discussion The results of our study indicate that a good \nbalance be\u00adtween high clustering accuracy (70% to 80% in all three benchmarks) andlow clustering overhead(lessthan2%in \nallthree applications)canbe achievedifwe set s to2048and simThreshold to 0.3. Our study of ciThreshold \nalso shows thatcarefullyselectingthe value of ciThreshold canimprove the overallperformanceand reduce \ncontention.The results also reveal that the optimal values of ciThreshold can vary across applications. \nFor example, the value of ciThreshold that allows the CA-Scheduler to yield its highest perfor\u00admance \nand reduce the most contentionis0.4for eclipse and jAppServer2004and0.2for jbb2005.However,the value \nof 0.4 is a good compromise, as it still results in a very good performanceimprovementfor jbb2005,buta \nslightlylower contention reductionthan the optimal value. We then conducted an analysis of the overall \nexecution overhead. We set the values of s to 2048, simThreshold to 0.3, and ciThreshold to0.4.As showninFigure5,theCA-Scheduleronlyincurreda \nmaximumof3.5%executionover\u00adhead(lusearch).Theinvestigation also reveals thatrecoding synchronization \nevents andbuilding conVectors accountfor 83%to92% of the entire overhead.  Percentage of similarity \nbetween M and M1 Percentage of similarity between M and M1 seVector size (s) (a)eclipse seVector size \n(s) (b)jbb2005 seVector size (s) Figure4. Accuracyanalysis ofthe clusteringalgorithmem\u00adployedinCA-Scheduler \nFigure5. Overallruntime overhead ofCA-Scheduler 4. PerformanceEvaluation Inthis section, weinvestigatethe \neffectoftheCA-Scheduler on reducingthe number of contention andimprovingoverall performance.We also comparetheCPU-scalabilityof \nCASe with that of default.We set s to 2048, simThreshold to 0.3, and ciThreshold to0.4. 4.1 ContentionReduction \nIn this study, we compare the number of contention be\u00adtween CASe and default. Furthermore, we create \ntwo ver\u00adsions ofCASe:Version1includes clusteringto address only interprocessor contention, andVersion2includes \nclustering andCriticalSectionFirst(CSF) scheduling to addressboth interprocessor-andintraprocessor-contention. \nCASe-V1 can reasonably reduce the number of con\u00adtention in four out of seven applications. We can achieve \nthese reductions because our approach assigns threads, which contend heavily, to the same processor. \nThis as\u00adsignment reduces the number of inter-processor contention (ranging from 10% to 29%). With the \nCSF scheduling strategy (CASe-V2), our approach can also reduce intra\u00adprocessor contention. As a result, \nthe overall reductions range from 15% to 45%. This result indicates that intra\u00adprocessor contentionis \na signi.cantfactor whenthe number of threads exceeds the number ofprocessors(400 threadsin hsqldbto nearly3000threadsin \njAppServer2004). Fortheremainingthreeapplications,ourapproachachieves a very small reductionbecause contention \namongthreadsin these applicationsis rare.For example, xalan only uses two synchronized methodsto updatetheworkqueue(adding \nor removing anXML transformationtask).The executiontime of these methods is also very short. Therefore, \ncontention rarely occurs. As a result, our approach only reduces the number of contentionby3%. (a)eclipse \n (b)jbb2005 Figure6. Investigatingthe effects ofciThreshold onperfor\u00admance eclipse hsqldb lusearch \nxalan SPEC SPEC ECPerf jbb2005 jAppServer2004 Figure7. Contention reductions overdefault  4.2 OverallPerformanceImprovement \nIn eclipse, lusearch, hsqldb and xalan,performanceis mea\u00adsured by execution time. In jbb2005, jAppServer2004 \nand ECPerf,performanceis measuredinjobsper second(JOPS). Figure8showstheperformanceimprovements ofCASe-V1 \nandCASe-V2 over default. As shown in Figure 8, eclipse suffers a 3% degradation in performance. This \nis because our approach can achieve a very small contentionreductionin eclipse. Therefore, the degradationof3%is \nmainlycausedbythe overheadincurred by the CA-Scheduler. In lusearch and xalan, we do not achieve any \nnoticeable performance improvement. Again, this is due to small numbers of contention in these two benchmarks. \nFor the remaining four benchmarks, performance im\u00adprovementsofCASe-V2 rangefrom7% to15%.The mag\u00adnitude \nofperformancegain re.ectsits ability to reducethe number of contention. For example, in jAppServer2004, \nCASe-V2is abletoimproveperformanceby15%by elim\u00adinating45% of the contention.In addition,CSF scheduling \ndecreases the time that a thread must spend waiting to enter critical sections. So far, we have measured \nexecution times and through\u00adput performances using all 16 processors. Next, we inves\u00adtigate the CA-Schedulers \nability to perform with a vary\u00ading numberofprocessors.We reportperformanceimprove\u00adment in terms of speedup \nwith respect to the performance of default with one core [22]. For example, the speedup of the CA-Scheduler \nwith 16-core in jAppServer2004 is throughputCA-16 , where throughputCA-16 represents throughputdef ault-1 \nthe throughputperformance of jAppServer2004running on CASe-V2with16processorsandthroughputdef ault-1 \nrep\u00adresents the throughputperformance of jAppServer2004run\u00adning on default with oneprocessor core. Figure8. \nPerformanceimprovements overdefault We maintained the same workload while increasing the number of CPU \ncores. After we added a CPU, we mea\u00adsuredtheperformance of each application.As showninFig\u00adure 9, CASe-V2 \ncan perform worse than default when the numberof coresis small.Thisisbecausea smallernumber of cores \nreduces contention. Although CASe-V2 performs worsewith onecore,itsperformanceimprovesquickly and surpasses \nthat of default, as the number of cores increases in .veoutof sevenapplications.Twoexceptionsare eclipse \nand xalan. In these two benchmarks, the CA-Scheduler is ineffectivedue to a small number of contention. \nInjbb2005,themaximumthroughputof defaultis achieved (58839 JOPS) when all 16 cores are used. CASe-V2 \ncan achieve a similar throughput by using only 12 cores. Be\u00adcause this application has only 16 active \nthreads, there is a one-to-onemappingbetweenthreads andCPUs,yielding maximumparallelism.This result shows \nthatCASe-V2 uti\u00adlizes CPUs much more ef.ciently than default in jbb2005. Thispreliminaryresultindicatesthatitispossibleto \nachieve betterCPUutilizationbyfavoring contention reduction over increasingCPU cores.  4.3 CPUScalability \nInthis section,we comparetheCPU-scalability of theCA\u00adScheduler(usingCASe-V2) with default. Note that \nCPU\u00adscalability refersto anincreaseinperformance(i.e., anin\u00adcrease of throughput or a decrease in execution \ntime) with respect to anincreasein the number ofCPU cores. As showninFigure9, the speedups of mostbenchmarks \nbegin to saturate after the number of CPU cores reaches a certain number. For example, in eclipse, peak \nperformance is achieved at .ve or more CPUs. This is because there are .veindependenttasks(10 threadsin \ntotal), which seldom contend.Therefore,thereis no signi.cantperformancegain after .ve CPU cores. In jbb2005, \nthe throughput begins to saturate at 12 cores. Between 12 cores to 16 cores, the throughputimprovementislessdramatic. \n As can be seen in Figure 9, the CA-Scheduler improves CPU scalability in .ve out of seven benchmarks. \nWe also discovered an interesting scalability behavior in hsqldb, jAppServer2004, and ECPerf when default \nis used.In these three applications, performance actually degrades when more CPUs are added to the system. \nIn hsqldb, a smaller number of CPUs limits the number of requests the system cantake atonetime.As we \nadd moreCPUs,theadditional computation power is used mainly to take more requests. This does not help \nwith servicing queries because the core database engineis single-threaded[17] and must sharethe processors \nwiththe restofthethreads.Thus, we see reduced performancein the default system when the number ofpro\u00adcessorsis12 \nor more. On the otherhand,CASe-V2 assigns ahigherpriority to threadsin critical sections.Because the \ncore enginehandles requestsin a serializedfashion,CASe-V2 schedulesthe core engine more frequently than \nother threads. This means that in a single core system, threads taking requests are less frequentlyscheduled.As \nwe add moreCPUs to the system, one processor is mostly used by the core engine, and the remainingprocessors \nare used to accept concurrent requests andqueuethem up forthe core engine.As a result,CASe-V2 scales \nwell with more processor cores and eventually outperforms the default system once the number of CPUs \nis14or more. In jAppServer2004and ECPerf,CASe-V2 shows modest improvementsin the scalability over default, \nwhich slightly degrades performance with more CPU cores. Because the numbersof threadsinthesetwo applicationsare \nverylarge (severalhundredconcurrent threads with thousands created throughout execution),increasing the \nnumber ofprocessors from 1 to 16 modestly improves parallelism in these two applications. However, it \nappears that reducing contention, as in the case of CASe-V2, allows the system to scale in spite of a \nmodestincreaseinparallelism. 5. OtherConsiderations Security:In our currentimplementation, wehave notincor\u00adporated \nany security measures to proactively prevent other threadsfrom registeringwiththekernel.Therefore,itispos\u00adsible \nfor native threads, as an example, to register with the kernel and to be scheduled based on the contention-aware \npolicy. However, it is not possible for unregistered threads to simplyinvoke anykind of system call to \nrenew execution quanta,asthe renewalprocessisdoneautomaticallyby the kernel.Onepossible security measureisto \nmakethe regis\u00adtrationprocess more secure(e.g., encryptedpolicy con.gu\u00adration .les,authenticationof allprocessestrying \ntoregister, andtighter controlfrom system administrators). (a)eclipse (b)hsqldb (c)lusearch (d)xalan \n(e)jbb2005 (f)jAppServer2004 Figure9. ComparingPerformance andCPU-scalability ofCASe-V2 with default \nIncremental Migration: The current implementation of the thread migration mechanism assigns thread-to-CPU \nin a stop-the-worldfashion, meaningthat allJava threads must stop executing during this process. Because \nmigration sel\u00addom occurs, its overhead appears to be small when it is averaged over the entire execution \nof a program. However, we found that each migration causes the application server benchmarks to exhibit \nnoticeable pauses of about 500 mil\u00adliseconds to 1 second. As part of future work, we plan to makethe \nmigrationprocessincrementalto reducethelength of each pause. We expect that such an approach will make \npausesless noticeable.However,it will also resultinhigher migration overhead due to additional bookkeeping \nactivi\u00adties and longer delays before the approach can reap its full bene.ts. Portability: It is also \npossible to implement our scheduler as a user-level thread management package. One approach is to map \na cluster to a kernel thread and then rely on the user-level thread manager to multiplex threads belonging \nto a cluster. However, it is likely that such an approach would resultin alongerthread waitingtime.For \nexample,if three actual timequanta are neededfor a thread to complete executing in a critical section, \nin our current approach, the thread can get through this critical section in one turn on the CPU (2 renewals). \nIn the user-level thread approach, the thread may be suspended by the kernel before it can get throughthe \ncritical section, resultingin more convoying and longer waiting time. On the other hand, the user-level \nthreadapproachis moreportablethanthekernel-levelthread approach. SupportingNativeThreads: Currently, \nourprototype only works withJava.However,integratingtheproposed cluster\u00ading and planning mechanism to \nlanguages such as C and C++ should be quite straightforward. For example, similar clustering and thread-to-CPU \nmapping mechanisms can be includedin threadinglibraries suchaspthread. 6. RelatedWork The goal of this \nwork is very similar to many other works that attemptto makethread-levelparallelismmore ef.cient in multiprocessorsystems.Our \napproachis uniquebecause itachievesthe samegoalthrough operating system augmen\u00adtationinstead of(i) relying \nonprogrammerstoprovidelock\u00adfree code(e.g.,lock-freedata structures[24]);(ii)employing runtime systemstoprovidelock-free \nexecution(e.g.,hard\u00adware and software transactional memory[16,14]); and(iii) utilizing hardware to assist \nwith lock-free execution (e.g. speculativelock elision[23]).One common characteristic of thesetechniquesisthattheytreat \nschedulingdecisions made by the kernels as a fact of life. Therefore, these three ap\u00adproachesaredesignedto \ncope with contentionthat results from these schedulingdecisions.Our approachdiffersfrom thesethreetechniquesinthatit \n.rstattemptstoproactively improve the scheduling, so that the execution sequence ex\u00adperiencesfew occurrences \noflockcontention.Any unavoid\u00adable contentionisthenmanaged using existing approaches. Thus, ourproposed \nworkis orthogonalbut complementary to these threetechniques. In addition, recent emergence of transactional \nmemory alsopromisesless effortbyprogrammersto coordinatepar\u00adallelism[13].Currently, mosttransactional \nmemory systems are tuned according to the conventional wisdom that con\u00ad.icts rarely occur, and thus aborting \nmemorytransactions, a typically more expensive operation, should rarelytakeplace [16,21].However,this \nwisdomhas notbeen veri.edinlarge multithreaded applications such as Java application servers withhundreds \nto thousands of concurrently running threads manipulatingunstructureddata(e.g.lists,trees,graphs,hash \ntables). It is very likely that applications wanting to utilize transactionalmemoryhavetobepartially \nrewritten, or atthe very least, recompiled[8].Thus, such restrictionslimitthe applicability of transactional \nmemoryin currentlydeployed software.As shownin thispaper, ourproposed solution can work well withlargeapplicationservers.Moreover,ourso\u00adlution \nallows existing applications to immediately take ad\u00advantageofitsbene.tswithoutrewritingtheapplicationcode. \nThere have been several research efforts that attempt to reduce contentionthrough morefavorable scheduling.Work \nbyTucker andGupta suggests that one way toprevent early preemption of processes in critical sections \nis to make the number of runnable processes the same as the number of processors.This effectivelyeliminates \nthe needforpreemp\u00adtion [33]. Because they only evaluated their work using highly parallel applications, \nit is unclear how such an ap\u00adproach would perform in large multithreaded applications with ahighfrequency \nof contention. WorkbyAnderson et al.[2] proposed scheduler activa\u00adtionsto addresstheissuethat a user-level \nthreadisblocked or preempted when it is in critical section. They adopted a solutionbased on recovery.Thatis,if \na threadhasbeenpre\u00adempted in a critical section, the thread is allowed to tem\u00adporarily continue until \nit exits the critical section. Marsh et al.[19] proposed a set ofkernel mechanisms and conven\u00adtionstogrant \n.rst-classtouser-level threads.Theirscheme providedcoordinationbetween schedulingand synchroniza\u00adtion \nto reduce waiting time causedbylock contention.Sim\u00adilarly, our work employs a time-slice renewing mechanism \nto reduce contention. However, our approach also employs clustering anddynamicprioritizationtofurther \nreduce wait\u00adingtime. Workby Tucker et al.[34] introduces a scheduling con\u00adtrolmechanismtoprovide limited \ncontrol overtheschedul\u00ading of a lightweight process in Solaris operating system. Functionschedctl start \nis usedtoprovide ahinttothekernel thatpreemption of alightweightprocess shouldbe avoided. Functionschedctl \nstop is usedto removethehint.One com\u00admonuseofthismechanismisto blockpreemption whilea threadisholdingalock.Because \nweimplementedour mech\u00adanisminLinux, suchfeatureis not availableto us.However, we can see that the availability \nof such feature can support theimplementationof theproposedCSF schedulingpolicy byutilizingblockingpreemptioninsteadof \nutilizingour cur\u00adrent technique ofincreasing executionpriority. Rossbach et al.[25] presented a variant \nofLinux called txLinux to supporthardwaretransactional memory.TxLinux provideda transaction-aware schedulerthat \ntakes advantage of processes transaction states to mitigate the effects of high contention.The schedulermakes \nschedulingdecisions or assigns priority based on the current transaction state. Similarly, our approach \nprioritizes threads based on their lockusage. Tam et al.[32] proposed a clustered scheduling scheme to \nreducehigh-latencydue to cross-chip sharing.They used a similar approachto cluster threads thatheavily \nsharedata. Their workleveragedinformation availablein thehardware performancemonitoringtoguidethreadclusteringto \nreduce the cost ofinter-chip cache snoopingin chip-multiprocessor systems.Onthe otherhand, our work exploits \nruntimeinfor\u00admation from high-level language virtual machines to guide clustering and reducelock contention.With \ntheCSF sched\u00aduler, our work also reduces the number of intra-processor contention.  7. Conclusions Lock \ncontention is a major bottleneck that not only af\u00adfects performance, but can also affect scalability \nof mul\u00adtithreaded Java applications. To reduce the occurrences of lock contention, we introduced a Contention-Aware \nsched\u00aduler (CA-Scheduler) that maps threads sharing the same lock-protectedresources to the sameprocessor. \nIn addition, weintroduceda critical-section-.rstschedul\u00ading strategy, which considers lock usage as a \nscheduling criterion to further reduce thread waiting time due to lock contention. Our experimental results \nshow that the CA-Scheduler can achieve signi.cantperformanceimprovement in applicationswithheavylock-contention(upto15%)with\u00adoutincurring \nsigni.cant runtime overhead(3.5% of overall executiontime). 8. Acknowledgments This work was supported \nin part by the National Science Foundation underAwardNumbersCNS-0411043andCNS\u00ad0720757.Anyopinions, .ndings, \nand conclusions or recom\u00admendations expressed in this material are those of the au\u00adthors anddo not necessarily \nre.ectthe views oftheNational ScienceFoundation.We thankthe anonymous reviewersfor providingvaluable \ncomments andinsights that signi.cantly improvethe .nal versionof thispaper.Thiswork wascom\u00adpleted utilizing \nthe Research Computing Facility with the associatedUSCMSTier-2 site at theUniversity ofNebraska Lincoln. \nReferences [1] J. Aas. Understanding the Linux 2.6.8.1 Scheduler. On-line article,2006. http://josh.trancesoftware.com/linux/ \nlinux cpu scheduler.pdf. [2] T. E. Anderson, B. N. Bershad, E. D. Lazowska, and H. M. Levy. SchedulerActivations:EffectiveKernelSupportforthe \nUser-Level Management of Parallelism. In Proceedings of ACMSymposium onOperatingSystemsPrinciples(SOSP), \npages95 109, NewYork,NY,1991. [3] M. Arnold, A. Welc, and V. T. Rajan. Improving Virtual MachinePerformanceUsing \naCross-RunPro.leRepository. In Proceedings of theACMSIGPLANConference onObject OrientedProgrammingSystems \nandApplications(OOPSLA), pages297 311, SanDiego,CA,2005. [4] D. F. Bacon, R. Konuru, C. Murthy, and M. \nSerrano. Thin Locks:Featherweight Synchronization forJava. In Proceed\u00adings of the ACM SIGPLAN Conference \non Programming LanguageDesign andImplementation(PLDI), pages 258 268,Montreal,Quebec,Canada,June1998. \n[5] J. C. Bezdek, R. Ehrlich, and W. Full. FCM: The fuzzy C-Means Clustering Algorithm. Computers &#38; \nGeosciences, 10(2-3):191 203, 1984. [6] C. M. Bishop. Neural Networks for Pattern Recognition. OxfordUniversityPress,November1995. \n[7] S. M. Blackburn, R. Garner, C. Hoffmann, A. M. Khang, K. S. McKinley, R. Bentzur, A. Diwan, D. Feinberg, \nD.Frampton, S.Z.Guyer, M.Hirzel,A.Hosking, M.Jump, H. Lee, J. Eliot, B. Moss, A. Phansalkar, D. Stefanovi\u00b4c, \n T. VanDrunen, D. von Dincklage, and B. Wiedermann. The DaCapo Benchmarks: Java Benchmarking Development \nand Analysis. In Proceedings of theACMSIGPLANConference on Object-Oriented Programming Systems, Languages, \nand Applications(OOPSLA),pages169 190,Portland,OR,2006.  [8] B. D. Carlstrom, J. Chung, H. Cha., A. \nMcDonald, C. Cao Minh, L. Hammond, C. Kozyrakis, and K. and Olukotun. TransactionalExecution ofJavaPrograms. \nInOOPSLA2005 Workshop on Synchronization and Concurrency in Object\u00adOrientedLanguages(SCOOL).Oct2005. \n [9] M. Cohen, S. B. Kooi, and W. Srisa-an. Clustering the Heap inMulti-Threaded Applications forImproved \nGarbage Collection. In Proceedings of the Conference on Genetic andEvolutionaryComputation(GECCO),pages1901 \n1908, Seattle,WA,2006.  [10] J. C. Dehnert, B. K. Grant, J. P. Banning, R. Johnson, T. Kistler, A. Klaiber, \nand J. Mattson. The Transmeta Code Morphing Software: Using Speculation, Recovery, andAdaptiveRetranslation \ntoAddressReal-LifeChallenges. In Proceedings of the International Symposium on Code Generation and Optimization \n(CGO), pages 15 24, San Francisco,CA,2003. [11] R. Dimpsey, R. Arora, and K. Kuiper. Java Server Perfor\u00admance: \nA Case Study of Building Ef.cient, Scalable JVMs. IBMSystemsJournal,39(1):151 174, 2000. [12] C. Grzegorczyk, \nS. Soman, C. Krintz, and R. Wolski. Isla Vista Heap Sizing: Using Feedback to Avoid Paging. In Proceedings \nof the International Symposium on Code Generation andOptimization(CGO), pages 325 340, San Jose,CA,March2007. \n[13] T. Harris, A. Cristal, O. Unsal, E. Ayguade, F. Gagliardi, B. Smith, and M. Valero. Transactional \nMemory: An Overview. IEEEMicro,27(3):8 29,May-June 2007. [14] T.Harris,M.Plesko, A.Shinnar, and D.Tarditi. \nOptimizing MemoryTransactions. InProceedings of theACMSIGPLAN Conference on Programming Language Design \nand Imple\u00admentation(PLDI), pages 14 25. Ottawa, Ontario, Canada, Jun2006. [15] J. A. Hartigan and M. \nA. Wong. A K-Means Clustering Algorithm. AppliedStatistics,28:100 108, 1979. [16] M. Herlihy and J. E. \nB. Moss. Transactional Memory: Architectural Support for Lock-Free Data Structures. In Proceedings of \nthe International Symposium on Computer Architecture(ISCA),pages289 300. May1993. [17] HSQL Database \nEngine. hsqldb. On-Line Documentation, Lastvisited:December2007.http://hsqldb.org/web/hsqlFAQ.html. [18] \nIBM. JikesRVM. http://jikesrvm.sourceforge.net. [19] B.D.Marsh, M.L.Scott,T.J.LeBlanc, and E.P.Markatos. \nFirst-ClassUser-LevelThreads. In Proceedings of the ACM Symposium onOperatingSystemsPrinciples(SOSP),pages \n110 121, NewYork,NY,1991. [20] Microsoft Corp. Using Microsoft Virtual PC 2007 for Application Compatibility. \nWhite Paper, August 2006. http://www.microsoft.com/windows/products/winfamily/ virtualpc/appcompat.mspx. \n[21] K. E. Moore, J. Bobba, M. J. Moravan, M. D. Hill, and D. A. Wood. LogTM: Log-Based Transactional \nMemory. In Proceedings of the International Symposium on High-Performance Computer Architecture(HPCA), \npages 254 265.Feb2006. [22] D.A.Patterson and J.L.Hennessy. Computer Organization and Design (3rd ed.): \nthe Hardware/Software Interface. MorganKaufmannPublishersInc.,SanFrancisco,CA,2004. [23] R. Rajwar and \nJ. R. Goodman. Speculaive Lock Elision: Enabling Highly Concurrent Multithreaded Execution. In Proceedings \nof the International Symposium on Microarchi\u00adtecture(MICRO),pages294 305, Austin,TX,2001. [24] R. Rajwar \nand J. R. Goodman. Transactional Lock-Free Execution of Lock-Based Programs. In Proceedings of the International \nConference on Architectural Support for Programming Languages andOperatingSystems(ASPLOS), pages5 17,SanJose,CA,2002. \n[25] C.J.Rossbach,O.S.Hofmann,D.E.Porter,H.E.Ramadan, B. Aditya, and E. Witchel. TxLinux: Using and Managing \nHardware Transactional Memory in an Operating System. In Proceedings of ACM Symposium on Operating Systems \nPrinciples(SOSP),pages87 102, NewYork,NY,2007. [26] Silberschatz and Galvin and Gagne. Operating System \nConcepts, 7thEdition. AddisonWesley,2007. [27] J. Singer, G. Brown, I. Watson, and J. Cavazos. Intelligent \nSelection of Application-Speci.c Garbage Collectors. In Proceedings of the International Symposium on \nMemory Management (ISMM),pages91 102, Montr\u00b4eal,Quebec, Canada,2007. [28] S. Soman, C. Krintz, and D. \nF. Bacon. Dynamic Selection of Application-Speci.c Garbage Collectors. In Proceedings of the International \nSymposium on Memory Management (ISMM),pages49 60,Vancouver,BC,Canada,2004. [29] Standard Performance \nEvaluation Corporation. SPEC\u00adjAppServer2004user sguide. http://www.spec.org. [30] StandardPerformanceEvaluationCorporation.SPECjbb2005. \nOn-LineDocumentation,Lastvisited:July2007. http://www.spec.org/jbb2005. [31] SunMicrosystems. ECPERF. \nhttp://java.sun.com/developer/ earlyAccess/j2ee/ecperf/download.html. [32] D. Tam, R. Azimi, and M. Stumm. \nThread Clustering: Sharing-Aware Scheduling on SMP-CMP-SMT Multipro\u00adcessors. SIGOPS Operating System \nReview, 41(3):47 58, 2007. [33] A.Tucker andA.Gupta. ProcessControl andScheduling Is-suesforMultiprogrammedShared-MemoryMultiprocessors. \nInProceedings oftheACMSymposium onOperatingSystems Principles(SOSP),pages159 166, NewYork,NY,1989. [34] \nA. Tucker, B. Smaalders, D. Singleton, and N. Kosche. US patent 5,937,187: Method and Apparatus for Execution \nand PreemptionControl ofComputerProcessEntities,1999. [35] V. Uhlig. The Mechanics of In-Kernel Synchronization \nfor a Scalable Microkernel. SIGOPS Operating System Review, 41(4):49 58, 2007. [36] F.Xian,W.Srisa-an,andH.Jiang. \nAllocation-PhaseAware Thread Scheduling Policies to Improve Garbage Collection Performance. In Proceedings \nof the ACM SIGPLAN InternationalSymposium onMemoryManagement(ISMM), pages79 90,Montr\u00b4eal,Quebec,Canada,October2007. \n[37] T. Yang, E. D. Berger, S. F. Kaplan, and J. E. B. Moss. CRAMM: Virtual Memory Support for Garbage-Collected \nApplications. In Proceedings of the USENIX Conference on Operating SystemDesign and Implementation(OSDI),pages \n103 116, Seattle,WA,November2006.  \n\t\t\t", "proc_id": "1449764", "abstract": "<p>In multithreaded programming, locks are frequently used as a mechanism for synchronization. Because today's operating systems do not consider lock usage as a scheduling criterion, scheduling decisions can be unfavorable to multithreaded applications, leading to performance issues such as convoying and heavy lock contention in systems with multiple processors. Previous efforts to address these issues (e.g., transactional memory, lock-free data structure) often treat scheduling decisions as \"a fact of life,\" and therefore these solutions try to cope with the consequences of undesirable scheduling instead of dealing with the problem directly.</p> <p>In this paper, we introduce <i>Contention-Aware Scheduler (CA-Scheduler)</i>, which is designed to support efficient execution of large multithreaded Java applications in multiprocessor systems. Our proposed scheduler employs a scheduling policy that reduces lock contention. As will be shown in this paper, our prototype implementation of the CA-Scheduler in Linux and Sun HotSpot virtual machine only incurs 3.5% runtime overhead, while the overall performance differences, when compared with a system with no contention awareness, range from a degradation of 3% in a small multithreaded benchmark to an improvement of 15% in a large Java application server benchmark.</p>", "authors": [{"name": "Feng Xian", "author_profile_id": "81100606421", "affiliation": "University of Nebraska-Lincoln, Lincoln, NE, USA", "person_id": "P1223167", "email_address": "", "orcid_id": ""}, {"name": "Witawas Srisa-an", "author_profile_id": "81100018125", "affiliation": "University of Nebraska-Lincoln, Lincoln, NE, USA", "person_id": "P1223168", "email_address": "", "orcid_id": ""}, {"name": "Hong Jiang", "author_profile_id": "81361606829", "affiliation": "University of Nebraska-Lincoln, Lincoln, NE, USA", "person_id": "P1223169", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1449764.1449778", "year": "2008", "article_id": "1449778", "conference": "OOPSLA", "title": "Contention-aware scheduler: unlocking execution parallelism in multithreaded java programs", "url": "http://dl.acm.org/citation.cfm?id=1449778"}