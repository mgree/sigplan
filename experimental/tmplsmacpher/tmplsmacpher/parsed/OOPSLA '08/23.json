{"article_publication_date": "10-19-2008", "fulltext": "\n Analyzing the Performance of Code-copying Virtual Machines Gregory B. Prokopski Clark Verbrugge School \nof Computer Science, McGill University {gproko,clump}@sable.mcgill.ca Abstract Many popular programming \nlanguages use interpreter-based execution for portability, supporting dynamic or re.ective properties, \nand ease of implementation. Code-copying is an optimization technique for interpreters that reduces the \nper\u00adformance gap between interpretation and JIT compilation, offering signi.cant speedups over direct-threading \ninterpre\u00adtation. Due to varying language features and virtual ma\u00adchine design, however, not all languages \nbene.t from code\u00adcopying to the same extent. We consider here properties of interpreted languages, and \nin particular bytecode and vir\u00adtual machine construction that enhance or reduce the impact of code-copying. \nWe implemented code-copying and com\u00adpared performance with the original direct-threading virtual machines \nfor three languages, Java (SableVM), OCaml, and Ruby (Yarv), examining performance on three different \nar\u00adchitectures, ia32 (Pentium 4), x86_64 (AMD64) and Pow\u00aderPC (G5). Best speedups are achieved on ia32 \nby OCaml (maximum 4.88 times, 2.81 times on average), where a small and simple bytecode design facilitates \nimprovements to branch prediction brought by code-copying. Yarv only slightly improves over direct-threading; \nlarge working sizes of bytecodes, and a relatively small fraction of time spent in the actual interpreter \nloop both limit the application of code\u00adcopying and its overall net effect. We are able to show that \nsimple ahead of time analysis of VM and execution proper\u00adties can help determine the suitability of code-copying \nfor a particular VM before an implementation of code-copying is even attempted. Categories and Subject \nDescriptors D.3.4 [Programming Languages]: Processors Interpreters; Optimization; C.4 [Performance of \nSystems]: Design Studies General Terms Performance, Design, Languages Permission to make digital or hard \ncopies of all or part of this work for personal or classroom use is granted without fee provided that \ncopies are not made or distributed for pro.t or commercial advantage and that copies bear this notice \nand the full citation on the .rst page. To copy otherwise, to republish, to post on servers or to redistribute \nto lists, requires prior speci.c permission and/or a fee. OOPSLA 08, October 19 23, 2008, Nashville, \nTennessee, USA. Copyright c &#38;#169; 2008 ACM 978-1-60558-215-3/08/10. . . $5.00 Keywords virtual machines, \ncompiler optimization, code\u00adcopying, branch prediction, performance, dynamic analysis 1. Introduction \nDeveloping a suite of ahead of time compilers, or even a Just-in-Time (JIT) compiler environment for \nmany lan\u00adguages is expensive, and interpreter solutions remain state of the art. Typically a virtual \nmachine interpreter executes an interpreter loop, switching between internal implemen\u00adtations of the \ninput bytecode. Optimized implementations such as code-copying improve performance by reducing the cost \nof branching and branch mispredictions that tends to dominate execution overhead in such a design. While \ncode\u00adcopying is expected to help any virtual machine, the suitabil\u00adity of this approach for different \nvirtual machines with vary\u00ading implementation and bytecode characteristics is unclear. For purposes of \nvirtual machine design and direction of im\u00adplementation resources it is important to know which fac\u00adtors \nare both in.uenced by and in.uence the runtime bene\u00ad.ts of virtual machine optimization techniques such \nas code\u00adcopying. We investigate the impact of code-copying applied to three virtual machines, SableVM \n(Java) [12], OCaml [17], and Yarv (Ruby) [28]. Using newly proposed enhancements to gcc [24], code-copying \nversions of the original direct\u00adthreaded interpreters were developed and performance im\u00adprovements analyzed \non three architectures, ia32 (Pentium 4), x86_64 (AMD64), and PowerPC (G5). Low-level pro.l\u00ading allows \nfor relatively easy identi.cation of the effects of code-copying, and by comparing VM behaviours the \nrelation between code-copying and VM design is more evident. Dif\u00adferent bytecode features dramatically \nalter how well code\u00adcopying works in practice: while OCaml achieves impres\u00adsive speedups, up to 4.88, \nYarv improves only marginally. Although the quality of hardware branch prediction affects performance \nas well, factors such as bytecode size, imple\u00admentation complexity, and interpreter loop design dominate, \nsuggesting a few general design principles for virtual ma\u00adchines and bytecode if code-copying is to be \neffective. Much of this variation in improvement can in fact also be pre\u00addicted through simple static \nand dynamic measurements of an initial (or prototype) interpreter. This allows the expected bene.t of \ncode-copying to be considered early, and weighed against any considerations of implementation complexity \nor other optimization goals. Speci.c contributions of this work include: Building from a newly-proposed \nresearch technique for fa\u00adcilitating code-copying (controlled modi.cations to gcc), we demonstrate implementations \nof code-copying for three distinct virtual machines (languages), supported on three machine architectures. \n Using static and dynamic software metrics and hardware performance counters we provide a detailed analysis \nof code-copying under all combinations of our investigated languages and environments. This includes \ncomparisons between direct-threaded and code-copying approaches.  Not all virtual machines or architectures \nbene.t equally from code-copying. Our analysis further includes guidance on the expected performance \nof code-copying, prior to ac\u00adtual implementation. Ahead-of-time examination of inter\u00adpreter and bytecode \ncharacteristics can allow a VM devel\u00adoper to make appropriate design decisions for improving the performance \nof an interpreter solution, or at least un\u00adderstand the practical bene.t from code-copying that may be \nrealized given a speci.c design.  In the next section we describe related work on interpreter VM implementation \nand optimization. Section 3 gives basic background on VM design and the code-copying technique. This \nis followed in Section 4 with an initial exploration of the potential value of applying code-copying \nto each virtual ma\u00adchine. Section 5 gives actual experimental data from code\u00adcopying implementations, \nproviding insight into the perfor\u00admance bene.t and validating our ahead of timer observa\u00adtions. Finally \nin Section 6 we conclude and describe future work. 2. Related Work The most basic (and slowest) bytecode \ninterpreter design is known as switch-threaded, basically consisting of a while loop containing a large \nswitch statement where each byte\u00adcode is processed by branching to the appropriate inter\u00adnal functionality. \nWhile simple in concept and implemen\u00adtation such interpreters suffer from an extremely costly dis\u00adpatch \noverhead, and it is much better to use the more effec\u00adtive direct-threaded technique. These approaches \nhave been compared and analyzed by Ertl and Gregg [7]. Code-copying is, in turn, an improvement over \ndirect\u00adthreading. Code-copying was .rst described by Rossi and Sivalingam [27] and followed by a better \nknown work on se\u00adlective inlining by Piumarta and Riccardi [21]. Gagnon [12] was the .rst to use the \ncode-copying technique in a Java in\u00adterpreter. His implementation solved some important issues speci.c \nto the interpretation of Java bytecode by analyzing and splitting certain operations into multiple, VM-speci.c \nbytecodes. The ef.ciency of this design proved to be so effective that, for example, a(n admittedly non-optimizing) \nportable JIT compiler for SableVM (SableJIT by B\u00e9langer [1]) was only barely able to achieve speeds comparable \nto that of the code-copying engine. Our experience in analyz\u00ading more virtual machines here bears out \nthe importance of simpli.ed bytecode behaviour. Application of the code-copying technique to GForth has \nbeen analyzed in a few studies [8, 11] showing that the majority of performance improvement is due to \nimprove\u00adments in branch prediction. This work also compared code\u00adcopying (called dynamic superinstructions \nby Ertl et al.) to other techniques like dynamic and static instruction replica\u00adtion and static creation \nof superinstructions, both of which can also bring signi.cant performance improvements. These results \ndemonstrated that speedup due to branch prediction improvements should outweigh other negative effects, \nsuch as an increase in instruction-cache misses. Peng et al. [20] described an interpreter using a stack \ncaching technique which exploits the fact that in stack\u00adbased virtual machines elements on top of the \nstack are the ones accessed most often. By forcing the build compiler to use registers for these elements \nand creating multiple ver\u00adsions of certain instructions (for accommodating different arrangements of \nstack elements in the cache) while maxi\u00admizing code reuse this technique achieves modest speedups, of \nabout 13%. A solution similar to a code-copying engine is a JIT technique based on using code generated \nahead of time by a C compiler, as developed by Ertl and Gregg [9]. In this solution parts of interpreter \nexecutable are used as template code segments, copied, concatenated and modi.ed (patched) on the .y, \nso as to contain appropriate immediate values and remove the need for the instruction counter. Due to \nthe patching architecture-speci.c code is necessary. Berndl et al. [2], Zaleski et al. [35] introduced \na new technique called context threading which leverages the ex\u00adisting hardware prediction mechanisms \nfor call and return assembly instructions to remove about 95% of mispredic\u00adtions. They show it is possible \nto improve the speed of their technique by selective use of code-copying for very small bytecode instructions. \nVitale and Zaleski [34] analyzed ap\u00adplicability of this technique to a Tcl interpreter character\u00adized \nby large bytecode bodies and, for most applications and benchmarks, little time spent in interpreter \ndispatch. On a selected set of dispatch-intensive benchmarks they achieved an almost 10% speedup. Most \nof the implementations of code-copying (with the exception of, for example, Ertl and Gregg [9]) and similar \ntechniques did not explicitly consider safety issues. Many virtual machines are implemented in C, and \nproblems arise from the fact that there is nothing in the C standard nor in most C compiler capabilities \nthat guarantees that part of bi\u00adnary code copied from one place in memory to another will be functionally \nequivalent to its original image. Code op\u00adtimization can rearrange instructions outside of expected, \nsource-based limits, and without speci.c optimization guar\u00adantees it is dif.cult to ensure the correctness \nof the .nal implementation. These issues were addressed by Prokop\u00adski et al. [25] by .rst attempting \nto use specialized, VM\u00adspeci.c test suites and eventually by a GNU C Compiler extension explicitly supporting \ncode-copiyng [23, 24]. Our work makes use of this basic compiler support. Pro.le-based tuning has also \nbeen used to improve byte\u00adcode execution. Stephenson and Holst [30] show another optimization based on \nexploiting frequently occurring byte\u00adcode sequences under the name of multicode substitution.In this \nwork hot sequences of bytecodes are discovered off-line and new code is created (either by a JIT or ahead \nof time in an interpreter) with aggregated, compact instructions rep\u00adresenting larger execution sequences. \nThey showed that to limit the total number of instructions (including those cre\u00adated by the optimization \nitself) such an approach must be combined with careful selection of sequences based on how well a sequence \nof bytecodes can be optimized. A more on-line solution is presented by a system such as DyC [14]. Here \nprograms are dynamically recompiled dur\u00ading execution to bene.t from runtime values, permitting op\u00adtimizations \nbased on partial evaluation. A similar solution was presented by Consel et al. [5] in a C program special\u00adization \nsystem. It included run-time specialization based on source code templates compiled ahead of time with \na static compiler. The specialization technique was applied to Java by Masuhara and Yonezawa [16] and \nJava and Ocaml by Thibault et al. [32] with good results. Moving toward a full, optimizing JIT represents \nof course a signi.cant resource commitment, out of the reach of many scripting or experi\u00admental languages. \nPortable JITs like GNU Lightning exist, but these often come with support for limited number of plat\u00adforms. \nSpecialized interpreters are another route to optimized performance. Ertl et al. [10] in Vmgen showed \nhow the VM system can be trained on a set of programs to detect the most often occurring, small sequences \nof bytecodes and then how to modify the source of the interpreter to combine these se\u00adquences into superinstructions \nfor optimized execution once the interpreter is recompiled. Such a generalized solution can be used for \nrapid development of systems that normally in\u00adclude an interpreter as done by Palacz et al. [19]. Somewhat \nsimilar in spirit is a proposal by Varma and Bhattacharyya [33] for a system where Java bytecode is translated \ninto cus\u00adtom generated C code, including Java-speci.c optimizations, and then compiled using a standard \nC compiler. While the speed bene.ts of these solution are indisputable, they are still basically static \nsolutions, requiring ahead of time pro\u00ad.ling and training, and interpreter (re)compilation. Hybrid and \nother approaches to bytecode execution are of course also possible, as demonstrated by Bothner [4] (in \nGCJ) and Lattner and Adve [15] (in LLVM). GCJ is a GCC-based ahead of time compiler, including also a \ndirect-Figure 1. A comparison of code execution in a traditional, direct-threaded (left) vs. a code-copying \n(right) code inter\u00adpreter. Each arrow is a jump; the total number is reduced by code-copying and the \nexisting jumps are more predictable, mostly due to multiple copies of the same instruction. threaded \ninterpreter for dynamically loaded code. GCJ takes as its input either Java source or Java bytecode (class \n.les) and compiles to an architecture-speci.c executable; classes not compiled can still be executed \nthrough the runtime inter\u00adpreter. Many commercial (and research) JITs employ similar mixed-mode execution \ndesigns [31]. LLVM is a compi\u00adlation framework created for lifelong program analysis that features its \nown code representation, own compiler and other tools that make it very extendable and reusable while \nallow\u00ading close integration with the existing GCC framework. 3. Background Interpreters have the advantage \nof simplicity, although im\u00adproved performance is possible with different design ap\u00adproaches. Switch-threaded, \ndirect-threaded, and code-copy\u00ading interpreters provide a spectrum of interpreter imple\u00admentation strategies \nwith increasing complexity and ex\u00adpected performance. For code-copying certain implementa\u00adtion safety \nguarantees are required to ensure correct execu\u00adtion. Below we describe basic interpreter designs and \nimple\u00admentation concerns, the nature of safety considerations for code-copying, and brie.y describe how \nwe applied code\u00adcopying to the three virtual machines, SableVM, OCaml, and Yarv. 3 Kinds of Interpreters \nA switch-threaded interpreter simulates a basic fetch, de\u00adcode, execute cycle, reading the next bytecode \nto execute and using a large switch-case statement to branch to the ac\u00adtual VM code appropriate for that \nbytecode. This process is straightforward but if, such as in Java, bytecodes often encode only small \noperations the overhead of fetching and decoding an instruction is proportionally high, making the overall \ndesign quite inef.cient. A direct-threaded interpreter is a more advanced inter\u00adpreter that minimizes \ndecoding overhead. This kind of in\u00adterpreter requires an extension offered by some compilers known as \nlabels-as-values in order to be able to reference runtime code addresses. In C, for example, a goto instruction \ncan normally only target a statically speci.ed label. With the labels-as-values extension it is possible \nto take the address of a label and store it in a pointer type variable. This variable can then be used \nas the argument to a goto instruction, al\u00adlowing indirect control transfer. A direct-threaded interpreter \nmakes use of this capability by replacing a stream of byte\u00adcodes with a stream of labels targeting the \ncorresponding bytecode implementations. With this mechanism the inter\u00adpreter can immediately execute \na direct goto to reach the correct bytecode implementation. Optimization is implied by reducing the repeated \ndecoding of instructions, trading repeated test-and-branch sequences for a one-time prepara\u00adtory action \nwhere a stream of bytecodes is translated into a stream of addresses. Code-copying is a further optimization \nto interpreter de\u00adsign, albeit one which makes relatively strong assumptions about compiler code generation. \nThe basic idea behind code\u00adcopying is to make use of the compiler applied to the VM to generate binary \ncode for matching bytecodes. The chunks of VM code used to implement the behaviour of each bytecode are \nidenti.ed in the source code using the same labels-as\u00advalues extension as direct-threading. At runtime, \nthe inter\u00adpreter copies the binary chunks corresponding to an input stream of bytecodes and concatenates \nthem into a new place in memory, as shown in Figure 1. Such a set of concatenated instructions is called \na superinstruction, and by eliminating branches between component instructions it can execute at a much \ngreater speed than code using either of the previously described techniques. In most implementations \na stream of bytecodes is split into superinstructions by .nding two kinds of split points: after VM branches \nand before branch tar\u00adgets. If all bytecodes were copyable it would equate su\u00adperinstructions to basic \nblocks but sincethere exist certain large or for other reasons non-copyable bytecodes superin\u00adstuctions \nare usually smaller. More detailed descriptions of selection of bytecodes for code-copying and creation \nof su\u00adperinstructions can be found in [11, 12, 21, 27]. Depending on the application and other factors \nthe code-copying tech\u00adnique can give substantial performance gain over a direct\u00adthreading technique (e.g. \nGagnon [12] showed 1.2 to 3 times speedups). Safety Considerations An inherent dif.culty with code-copying \nis ensuring the in\u00adtegrity of the copied code copied code must not have de\u00adpendencies on its initial \ncode location, or copied execution will differ from the original. Unfortunately, the C standard does \nnot contain any semantics that would allow us to ex\u00adpress and impose the necessary restrictions on selected \nparts of code. In particular, bracketing labels placed before and after source code chunks and used to \naddress them do not guarantee contiguity of the resulting binary code chunks, nor do they place restrictions \non the use of PC1-relative address\u00ading. General compiler optimizations, essential to good VM performance, \nmay relocate basic blocks within a chunk out\u00adside of the bracketing labels, and ef.cient code-generation \nmakes best use of short, relative addressing instructions. At VM runtime this can result in incomplete \ncopies of such code chunks being copied and executed, producing unde\u00ad.ned behaviour or segmentation faults. \nThese safety issues have prevented code-copying from becoming a mainstream technique despite the large \nperformance advantages. Previous implementations of code-copying have disabled optimization and/or used \nmanual identi.cation to discover problems with code-copying [25]. Our implementations here are based \non a recently proposed compiler extension to gcc that enforces the hard barrier requirements of code\u00adcopying \n[24]. This extension allows a programmer to use asimpleC pragma to mark chunks of VM source code that \nwill be used at runtime for code-copying. The compiler then ensures that the code chunks generated from \nthese parts of the source code are contiguous in memory and can be safely copied at runtime, while also \nensuring as many code opti\u00admizations as possible are still applied. This approach makes code-copying \npractically usable, and was essential in allow\u00ading us to scale our investigation to three very different \nvirtual machines and hardware architectures. Application to 3 Different VMs Our experimentation is based \non examining code-copying in several environments. SableVM already supported code\u00adcopying (in fact all \nthree interpreter designs [12]). Our mod\u00adi.cations to SableVM were thus mainly to modify the VM to use \nthe gcc enhancements described above, obviating the existing system for verifying correctness of copied \ncode. OCaml and Yarv (Ruby)2 use only direct-threading, and so changes were more invasive. For both OCaml \nand Yarv VMs we used the same general scheme for the code-copying implementations. At a high level this \ninvolved a) adding a small set of functions and data structures sup\u00adporting the management of superinstructions, \nb) modifying the interpreter loop, either through C macros (OCaml) or changes to the C code generator \n(Yarv) to use the special pragmas to identify the copyable code regions implementing individual bytecodes, \nand c) modifying the function responsible for bytecode prepara\u00adtion to construct and use copied code, \nstoring the revised code in a modi.ed code array. 1 program counter 2 A more popular Ruby interpreter \nexists, but uses a different interpretation technique, not readily compatible with code-copying. It is \nalso the case that Yarv contains analyses that can potentially simplify bytecode and improve opportunities \nfor code-copying. To identify copyable sequences several language-speci.c bytecode analyses are required, \nmaking changes to bytecode preparation the most complicated step. For example, before attempting the \ncreation of copied code all bytecodes chang\u00ading control .ow must be found and their jump targets iden\u00adti.ed. \nIn the case of SableVM other analyses are necessary for detecting potential class loading, identifying \nGC points, and other language-speci.c concerns.  4. Execution Properties Basic properties essential \nto good code-copying perfor\u00admance can be gathered ahead of time, through simple dy\u00adnamic metrics and \nexamination of the virtual machine byte\u00adcode design. This allows an assessment of the suitability of \na virtual machine for code-copying prior to implementation; our results here can also serve as a heuristic \nguide during bytecode design for maximizing the performance of code\u00adcopying. Performance and behaviour \nof three different virtual ma\u00adchines, on three hardware platforms is examined. These cases illustrate \na spectrum of implementations and poten\u00adtial performance. Each of a Java virtual machine, an OCaml interpreter, \nand a Ruby interpreter are examined running on 32-bit Intel, 64-bit AMD64, and 64-bit PowerPC hardware. \nBelow we give details on the precise architectures, virtual machines, and benchmarks. We follow this \nwith a detailed analysis of runtime behaviour with respect to relatively eas\u00adily discovered properties \nof virtual machine execution and implementation. These properties serve as strong predictors for the \nperformance of code-copying, as we will show in the next section. 4.1 Architectures and Virtual Machines \nPerformance of any program is obviously closely tied to the machine architecture on which it runs. Variability \ndue to dif\u00adferent hardware designs can be magni.ed when investigat\u00ading the performance of a code-copying \nvirtual machine the bene.t of code-copying is largely produced by better enabling branch prediction, \nimproving code locality, etc.. These are factors which can vary signi.cantly on differ\u00adent computer hardware. \nOur analysis thus compares perfor\u00admance on three popular architectures: A 32-bit Intel machine; Pentium \n4 3GHz with hyper\u00adthreading. L2 cache size of 1MB, L1 data cache size of 16kB, L1 instruction cache (trace \ncache) 12K ops. This machine had 1GB RAM installed and ran Debian GNU Linux 4.0 (\"Etch\") using Linux \nkernel version 2.6.18 with SMP support, optimized for i686 machines.  A 64-bit x86_64 machine; AMD64 \nDual Core 3800+ 2GHz. L2 cache size was 512kB per CPU (1GB total), L1 data cache 64kB per CPU and L1 \ninstruction cache 64kB per CPU. This machine had 4GB RAM installed and ran Ubuntu 7.10 (codename \"Gutsy\") \nusing Linux kernel version 2.6.22 with SMP support.  A 64-bit PowerPC machine; Power Macintosh G5 with \ntwo CPUs 970 running at 1.8GHz. L2 cache size was 512kB per CPU (1GB total), L1 data cache 32kB per CPU \nand L1 instruction cache 64kB per CPU. This ma\u00adchine had 1.5GB RAM installed and was running Mac OS X \nServer version 10.4.11. External performance in.uences were eliminated or re\u00adduced as much as possible. \nOn all machines all used software was installed on a local hard drive, and unnecessary pro\u00adcesses and \nnetwork activity were stopped. To reduce noise due to multiprocessing on the Pentium 4 and AMD64 we en\u00adsured \nthrough the cpu af.nity functionality in glibc and the Linux kernel that all VM threads execute on a \nsingle CPU only. Mac OS X, however, (intentionally) does not provide such functionality, leaving the \ndecision of CPU assignment to the OS. For each virtual machine (on each architecture) we ex\u00adamine performance \nfor a variety of language-speci.c bench\u00admarks. Java. We use the SableVM Java virtual machine inter\u00adpreter \n(v1.13) [12]. Benchmarks include the standard SPECJvm98 benchmark suite [29], two large in-house benchmarks \nSableCC (a parser generator) [13] and Soot (analysis and optimization framework for Java) [22], and several \nbenchmarks (bloat, fop, luindex and pmd)3 from the DaCapo suite (v2006-10-MR2) [3].  OCaml. We use the \nOCaml interpreter (v3.10.0) [17]. Benchmarks come from the interpreter itself (in the test subdirectory \nof the sources), with further additions from the Debian Shootout suite [6].  Ruby. We use the YARV (Yet \nAnother Ruby VM) inter\u00adpreter (v0.4.1) [28]. As with OCaml we use several of the benchmarks that come \nwith the source code (in the bench\u00admark subdirectory) and several benchmarks from Debian Shootout. We \nfurther include YARV s supplied self-test program, executed by running make test-all in its source directory. \n All of these virtual machines are (primarily) implemented in C, and so we use earlier work on GCC extensions \nfor code-copying to evaluate that performance [24]. To measure the performance we timed 7 runs of each \nbenchmark and averaged the results. We computed standard deviation; for almost all cases it was less \nthan 0.01 of the average measured value, and never greater than 0.04. 4.2 VM and Language Characteristics \nThe different bytecode sets offered by different virtual ma\u00adchines can have a large impact on code-copying \nperfor\u00admance. More complex bytecode instructions perform more work per bytecode, while smaller bytecodes \nwill tend to 3 Not all DaCapo benchmarks are able to execute on the version of SableVM we used. have \nmore relative branching overhead, and thus greater op\u00adportunity for improvement through code-copying. \nThe ex\u00adtent to which copyable sequences can be exploited is also driven by characteristics of VM workload \nprograms which exercise copyable sequences more often will naturally see greater improvement. This is \nin relation to non-copyable se\u00adquences, but in order to account for whole program improve\u00adment also in \nrelation to the fraction of whole program time spent in the interpreter loop. If execution depends on \nexpen\u00adsive library or runtime services then again the relative im\u00adprovement provided by code-copying \nwill be reduced. Be\u00adlow we discuss these concerns and present dynamic data from our three virtual machines. \nThe relative complexity of operations executed by a byte\u00adcode is a measure of how much work (in terms \nof CPU time) each bytecode does, including work done by functions called from within a bytecode. A reasonable \nexpectation is that the more work an average bytecode does the less the positive ef\u00adfect of using a code-copying \ntechnique. As previously men\u00adtioned, much of the improvement shown by code-copying is due to reduced \noverhead in jumping between bytecodes, re\u00admoving jumps and simplifying branch prediction. Complex bytecodes \nmean this optimization operates on a smaller run\u00adtime overhead per bytecode, and so has less overall \nimpact. The presence of consecutive groups of simple or small bytecodes (doing little work ), not containing \ncontrol .ow branches or targets (from the bytecode point of view, not the internal implementation) is \nalso an important factor. Such consecutive groups are candidates to become copied, su\u00adperinstructions \nduring runtime, and so a greater percentage of execution spent in such groupings implies greater bene\u00ad.t \nfrom code-copying. Arithmetic operations tend to be ex\u00adpressed with small bytecodes, and so programs \nincluding signi.cant amounts of direct computation, not too densely interleaved with control .ow changes \nwill show a greater bene.t. Note that in our implementation we currently require that copied code contains \nat most one control-.ow changing bytecode, and it must be the last one in a superinstruction. This is \nnot, however, an inherent limitation of code-copying. If all bytecode instructions were copyable it would \nequate a superinstruction span to a basic block but since not all byte\u00adcode instructions are copyable \nsuperinstructions are usually smaller. Static measurements for our three VMs are illustrated are shown \nin Table 1. Here we show the overall size of the ac\u00adtual interpreter loop, the number of distinct bytecodes, \nthe size of instructions, and the relative percentage of copyable bytecode instructions. Copyable instructions \nwere manually identi.ed for this data. Lines-of-code is of course a very approximate measure, not taking \ninto account usage of C macros or inlined functions. SableVM for example does not use C macros, but does \ngenerate C code using a preproces\u00adsor; the number of lines is thus higher than in other inter\u00adpreters \neven for similar bytecodes. Coupled with the com\u00adplexity of any internal functions called by bytecode \nimple\u00admentations this, as we will show later in comparison to dy\u00adnamic results makes average static size \nof bytecodes a fairly poor indicator of runtime complexity. Relative number of copyable bytecodes is \nmore useful; while it is also a static indication of performance, suggesting the proportion of ex\u00adecution \ntime that may bene.t from code-copying, it re.ects runtime performance quite well. Dynamic results are \nshown in Figure 2. Actual execution time is measured, and the relative amount of runtime spent in the \ninterpreter loop is shown, along with the relative number of bytecodes loaded at runtime that were statically \nidenti.ed as copyable. This data is from the x86_64 architecture, but other machines demonstrate comparable \nresults. It is impor\u00adtant to note that these statistics can be easily gathered early in the VM design \nprocess, and importantly before any effort is invested in implementing code-copying.  4.3 Analysis Analysis \nof these simple measurements already provides strong indicators for the potential performance for code\u00adcopying. \nHere we separately consider .rst Java, then OCaml, and .nally Ruby. Java is an example of a good environment \nfor code-copying; OCaml serves as an example where code\u00adcopying excels, and Ruby of an environment in \nwhich code\u00adcopying will not tend to provide signi.cant improvements. Analyzing SableVM s execution behaviour \nis made more complex due to the use of some internal code optimiza\u00adtions. Java, for instance, requires \ndynamic loading and link\u00ading of classes at .rst runtime reference, and so many oth\u00aderwise simple bytecodes \ninclude more complex implemen\u00adtation behaviour to handle this special case. SableVM (and other Java VMs) \noptimizes this by splitting such instructions into two versions, a slower version to handle initialization, \nand a faster version for repeated execution. For example, ex\u00adecution of a PUTFIELD bytecode might trigger \nclass load\u00ading and static initialization, and minimally needs to calcu\u00adlate the relative address of the \ntarget .eld within a class of objects. Once a speci.c PUTFIELD is executed, however, class loading will \nnot be required, and the relative address can be cached and used by subsequent execution. Other optimizations \ninclude using specialized bytecodes in place of instructions that operate on generic classes of primitives \n(short, byte, int, etc). For a full explanation of the design see Gagnon [12]. Figure 2 shows that SableVM \nspends almost 90% of execution time executing code within the interpreter loop (not counting code executed \nin functions called from the loop), and that almost 80% of dynamically loaded bytecodes are potentially \ncopyable. With respect of overall execution time, there is certainly signi.cant room for code-copying \nto achieve very good performance. From Table 1, however, the average size of an instruction is relatively \nlarge (47 lines). As mentioned, SableVM code, unlike the other two VMs has expanded macros, and moreover \nmany very complex opera\u00ad interpreter instructions LOC per copyable instructions loop kLOC total instruction \n# % SableVM (15.0) 3.4 319 (57) 25 194 61% OCaml 1.0 133 8 96 72% Yarv 1.2 121 10 (71) 61 (59%) 50% \n Table 1. Average size of an instruction (in lines of code) and fraction of copyable instructions in \nthe 3 considered VMs. For SableVM the number in parenthesis is the number of lines after m4 macros expansion. \nAlso, for SabelVM interpreter loop LOC count we didn t include instructions initialization code which, \nin this particular VM, happens to be interleaved with instructions code. For Yarv the best performance \nwas obtained when the maximum size of copyable instructions was limited to 100-150 bytes, lowering the \nnumber of copyable instructions from 71 to 61. Figure 2. Percentage of time spent in the interpreter \nloop of the direct-threaded interpreters for x86_64, and of bytecodes loaded that were potentially copyable, \nfor a) SableVM, b) OCaml, and c) Yarv. tions related to class loading are implemented directly in the \ninterpreter loop source. The impact of the apparent complex\u00adity of bytecodes here can be further discounted \nby compar\u00ading static and dynamic results. 61% of bytecode instructions were statically found to be copyable \nin the interpreter loop, whereas almost 80% of bytecodes loaded at runtime were copyable. Larger, non-copyable \nbytecodes seem to be found at execution time less often than the shorter, copyable byte\u00adcodes. These \ncharacteristics suggest a great positive impact of code-copying application and as previous works have \nal\u00adready noted [12] Java bytecode is in fact a great candidate for code-copying, even in the presence \nof many bytecodes executing complex operations. OCaml bytecode in many ways resembles that of Java, without \nmuch of the complexity brought by Java s Ob\u00adject model, VM and class library interface, and other fac\u00adtors. \nFrom Table 1 a quick comparison of the source code of SableVM and OCaml reveals that implementations \nof OCaml instructions are several times smaller than Java in\u00adstructions (in SableVM), only about 8 lines \nof code. The fraction of bytecodes de.ned in interpreter loop that are copyable is actually higher than \nSableVM (72% vs. 61%) while the copyable bytecodes constitute a similar ratio, al\u00admost 80% of the loaded \nbytecodes (Figure 2). In this case bytecodes are more evenly sized, and so dynamic measure\u00adment better \nmatches static. The OCaml interpreter spends about 85% of execution time in the interpreter loop, also \ngiving it a vast space for improvement. With similar overall runtime characteristics, and less complex \ncode size (even accounting for lack of macro expansion) these initial mea\u00adsurements suggest that the \nOCaml interpreter may be an even better candidate for code-copying than SableVM. The size of the Ruby \ninterpreter loop and the average size of an instruction in terms of lines of code are comparable to that \nof OCaml (Table 1). Still, overall Yarv has the over\u00adall weakest positive indicators. Although 59% of \nbytecodes are statically copyable, later implementation results showed that applying code-copying to \nlarger bytecodes (more than 100 150 bytes; values in this range produced similar results) reduced performance, \nand a threshold of 50% of bytecodes provided the best performance (this lower setting is used for actual \ncode-copying experiments later in the paper). Unlike SableVM or OCaml the time spent in the interpreter \nloop of Yarv varied greatly between different benchmarks; Fig\u00adure 2 shows an average of slightly less \nthan 70%, but rang\u00ading between 7% and 96%. The number of potentially copy\u00adable bytecodes loaded was also \nmuch lower, only about 60%, compared to almost 80% in case of SableVM and OCaml. An inspection of the \nsource code for bytecode instructions further reveals that many of the bytecodes that are small in terms \nof code size use internal VM functions calls, making the bytecodes fairly heavy at runtime, and heuristically \nlimiting the impact of code-copying. These properties sug\u00adgest that code-copying may not bring nearly \nas much general performance improvement to Yarv as to the other VMs. Although coarse, for each of the \nthree VMs these obser\u00advations are suf.cient to at least give a relative ranking of expected performance \nbene.t. OCaml has almost all posi\u00adtive attributes, while SableVM suffers from more complex instructions, \nand Yarv much less runtime opportunity for its even smaller set of copyable codes. This gives three widely \nspread data points for examining code-copying performance. By considering VMs serving as both positive \nand negative examples we hope to validate our ahead of time analysis, and to allow future researchers \nand practitioners to understand where opportunities and pitfalls lie in terms of implement\u00ading or optimizing \na code-copying VM. In the next section we examine actual performance of code-copying implemen\u00adtations \nof all of these VMs on different architectures.  5. Code-Copying Results Deeper analysis of code-copying \nis performed by making detailed measurements of code-copying versions of all three VMs on our different \narchitectures. These implementations all make use of the basic GCC support for code-copying described \nin [24]. Results from analyzing these implemen\u00adtations demonstrate the value of code-copying in terms \nof speedup, as well as providing good evidence of how and why code-copying achieves the performance it \ndoes, given different VM and architectural characteristics. Below we .rst discuss our low-level pro.ling \nstrategy, along with some of its attendant complexities. We have made use of hardware counter information \nfor detailed and low\u00adoverhead pro.ling, although this necessarily exposes ma\u00adchine differences and pro.ling \nlimitations. Based on our pro\u00ad.les we .rst present the most general overall performance results. From \nthis we are able to show that the basic ahead of time analysis predictions are validated, and that several \npo\u00adtential contributors to code-copying performance do indeed re.ect runtime performance. This is followed \nby a closer ex\u00adamination of the best and worst performing benchmark for each VM; precise identi.cation \nof hardware and VM fea\u00adtures that impact performance helps understand performance and orient future optimization \nand design. 5.1 Pro.ling To assess the interaction of the benchmarked VMs with hardware architectures \nwe used hardware performance coun\u00adters to do online pro.ling. On the Pentium 4 and AMD64 machines we \nused OPro.le [18] versions 1.9.2 and 1.9.3 respectively. On PowerPC 970 (G5) we used the standard OS \nX tool Shark 4.5. Acquiring counter data is straightforward in concept, but poses a number of technical \nchallenges. For instance, be\u00adcause of hardware limitations it is not possible to gather all counter data \nin a single program execution; only a limited set (and certain combinations) of counters can be used \nat any one time. We therefore ran each benchmark once as a warm-up with hardware counters disabled, then \nran the same benchmark several times with different con.guration of hardware counters until all desired \ndata was collected. On the Pentium 4 and AMD64 we collected 7 sets of re\u00adsults for each benchmark. PowerPC \nresults are more limited. Available tools permit the measurement of only one hard\u00adware counter at a time \n(there can only be one counter set as Trigger in Shark), and data must be manually processed for the \naccurate event counts (rather than relative percentages) necessary to evaluate code-copying behaviour. \nOn PowerPC 970 (G5) we therefore limited the number of collected re\u00adsults to 3 sets per benchmark for \n4 benchmarks per virtual machine (with the exception of Yarv where we measured 2 benchmarks only). Variability \nin hardware counter behaviour was also as\u00adsessed. For each VM we chose 2 benchmarks (the slowest and \nfastest) and collected data for each counter 7 times (with the exception of PowerPC where due to the \nlabour-intensive process of gathering counter data we limited the number of trials to 3 per counter). \nWith the exception of Yarv, to further ensure any trends visible in the results are actually related \nto the observed performance we selected 2 more benchmarks, the second slowest and second fastest. For \nthese benchmarks we collected data 3 times (once on PowerPC) for each hard\u00adware counter. Primary and \nsecondary tests here correlate well, indicating reasonably stable behaviour. To decide on the level of \ntrust for each result we used the standard devi\u00adation divided by the average of the values on which it \nwas computed. This statistic is low for the majority of counter measurements. On Pentium 4 and PowerPC \nour deviation metric had values below 0.1 and on AMD64 below 0.2 in most cases. Generally higher values \nexisted, as expected for the more noisy cache-related counters. In all cases our ob\u00adservations and relative \njudgements are based on data changes dramatically larger than variance. Full data is available in the \nraw data results [26], not included here for space reasons. On the Pentium 4 we collected hardware counter \nresults in two runs with the following settings: First run L2 cache misses: BSQ_CACHE_REFERENCE events \nwith mask 0x700, trigger count 6000.  Branches: BRANCH_RETIRED events with mask 0xc, trigger count \n6000.  Mispredicted branches: RETIRED_MISPRED_BRANCH\u00ad_TYPE events with mask 0x1f, trigger count 6000. \n  Second run  CPU time: GLOBAL_POWER_EVENTS events, trigger count 100000.  On AMD64 we collected \nhardware counter results in two runs with the following settings:  First run  L1 instruction cache \nmisses: INSTRUCTION_CACHE\u00ad_MISSES, trigger count 600.  L1 data cache misses: DATA_CACHE_MISSES, trigger \ncount 600.   Total no. of branches: RETIRED_BRANCH_INSTRUCTIONS, trigger count 600.  Mispredicted \nbranches: RETIRED_MISPREDICTED_BRANCH\u00ad_INSTRUCTIONS, trigger count 600.  Second run  L2 cache misses: \nL2_CACHE_MISS events with mask 0x7, trigger count 600.  Instruction fetch stalls: INSTRUCTION_FETCH_STALL \nevents, trigger count 600.  Dispatch stall because of branch abort: DISPATCH_STALL\u00ad_FOR_BRANCH_ABORT, \ntrigger count 600.  CPU time: CPU_CLK_UNHALTED, trigger count 6000.  On PowerPC 970 (G5) we collected \neach hardware counter result in a separate run:  Flush caused by branch misprediction (presented as \nbranches mispredicted in the next section): trigger count 6000.  Total number of branches: trigger count \n60000.  CPU cycles: trigger count 600000.  L2 data cache misses: trigger count 600.  L2 instruction \ncache misses: trigger count 600.  Several other hardware counter results were also col\u00adlected on PowerPC, \nbut did not provide signi.cant or further insights. Again, our raw data contains full details. On all \narchitectures to ensure best possible view of hard\u00adware behaviour we used higher trigger counts for events \nthat occur more frequently (like CPU cycles) and lower trigger counts for rare events (like cache misses). \nThis is also sub\u00adject to software and hardware constraints; e.g., we were not able to use trigger counters \nbelow 6000 on the Pentium 4. To obtain more comparable results for counters collected at different rates \nwe rescaled the .nal values as if all hardware counters were set to trigger events at the same counter \nvalue, 10000. Code-copying is primarily an optimization of the inter\u00adpreter loop, and so in most data \ngathering we focus on the ex\u00adecution of the loop (and copied) code. Our measurements are narrowed to \nonly those events that were registered while ex\u00adecuting inside of the interpreter function (static code \nspace) or in the copied code. Events registered in functions called from the interpreter function were \nnot taken into account. In particular, in the current section unless otherwise speci\u00ad.ed total time values \nare only measured for the time spent in the interpreter loop, be it of a direct-threaded interpreter \nor code-copying one. Time in copied code values tell how much of the total time spent in interpreter \nloop was spent in copied code. Other hardware counter values related to branches and cache behavior were \nalso analyzed only for the code exe\u00adcuted inside of interpreter loop.  5.2 Dynamic behavior of copied \ncode We have gathered both overall and detailed performance and other pro.ling data from code-copying \nimplementations of all three VMs. Overall speedup is summarized in Table 2, showing the ratio of direct-threaded \nexecution time to code\u00adcopying time. More detailed examination is done through a variety of metrics, \npresented in Table 3. The speedups in Table 2 show that across architec\u00adtures code-copying works best \non ia32, with high average speedups on our selected benchmarks of 1.44, 2.81, and 1.14 for SableVM, OCaml, \nand Yarv respectively. On x86_64 code-copying also achieves high speedups of 1.32, 1.80 on SableVM and \nOCaml, and more marginal improvement with Yarv. The PowerPC implementation of code-copying is able to \nreach a high speedup of 1.52 only by the OCaml inter\u00adpreter, with only small improvement for SableVM \nand Yarv. As our ahead of time analysis predicted, across virtual ma\u00adchines the best performance was \nobserved on OCaml (reach\u00ading a maximum speedup of 4.88 on the ia32 execution of the quicksort.fast benchmark), \nwith good but less-improved per\u00adformance on SableVM, and minimal overall improvement to Yarv. SableVM \nOCaml Yarv ia32 1.44 2.81 1.14 x86_64 1.32 1.80 1.06 ppc 1.05 1.52 1.03 Table 2. Average speedups of \ntotal execution time of code\u00adcopying over direct-threading measured across virtual ma\u00adchines and architectures \nrunning all used benchmarks. Note this table presents the overall VM performance, as opposed to other \nresults that measure VM behavior only within the interpreter function. To gain a better insight into \nactual dynamic usage of copied code we gathered further pro.ling data pertaining to bytecode and superinstruction \ncreation and execution and translated them into several dynamic metrics. Table 3 summarizes the following \nmeasurements for our three vir\u00adtual machines. Note that we only present these metrics on x86_64 architecture, \nas they are similar between architec\u00adtures with appropriate scaling. Dynamic average superinstruction \nlength in bytecodes. Calculated as the number of executed bytecodes divided by the number of dispatches \n(jumps) this is the length of executed superinstructions in bytecodes weighted by rela\u00adtive number of \nexecutions of each sequence. More directly this metric allows us to see how many dispatches are re\u00admoved \nby code-copying and thus the length of an average executed instruction.  Copied code memory usage. This \nis the amount of mem\u00adory allocated and used to store copied code (superinstruc\u00adtions). Excessive memory \nusage can contribute to reduced performance, and so it is important to know the extent of code-copying \nmemory requirements.  Unique superinstructions. The number of superinstructions created during a VM \nexecution measures the variety of execution, giving indications of memory requirements and potential \noverhead.  Copied code memory divided by number of unique su\u00adperinstructions. This is the average amount \nof memory used by a superinstruction; another indicator of size and resource requirements.  Memory occupied \nby 90% of used copied code instruc\u00adtions. Heuristically, the majority of instructions will come from \na small set of potential instructions. We counted the total number of executions of both simple and superin\u00adstructions, \nthen found the subset of instructions that make up 90% of executed instructions. Out of those 90% we \nsummed the memory used only by superinstructions (cre\u00adated using code-copying).  Memory occupied by \n90% of used copied code instruc\u00adtions divided by copied code memory usage. This is the percentage of \ntotal memory used for copied code that ac\u00adcounts for 90% of executed code.  More detailed overall performance \ndata is further pre\u00adsented in Figure 3. Branch prediction behaviour is known to be critical to code-copying \nperformance Ertl and Gregg [8], Ertl et al. [11], and so we present data in Table 4. Another important \nproperty for both understanding perfor\u00admance and for considering future, further optimizations is the \nlength of superinstructions; this is shown in Figure 4. For the best and worst performing benchmark in \neach VM we give further hardware counter data, including cache be\u00adhaviour. This is shown for SableVM \nin Figures 5 and 6, for OCaml in Figures 7 and 8, and for Yarv in Figures 9 and 10. Dynamic avg. sup.ins. \nlen.[bc] Copied code memory[kB] Unique superinstructions Copied c. mem./ unique sup.[B] 90% copied code \nmem.[kB] 90% copied c.mem./c.c.mem. [%] SableCC 2.13 505 1972 262 16 3.16 Soot 2.04 975 3698 270 17 1.69 \ncompress 4.70 294 1232 244 10 3.48 db 2.66 290 1307 227 4 1.21 jack 2.10 358 1562 235 17 4.76 javac 2.40 \n590 2607 232 18 2.94 jess 1.99 398 1744 233 8 2.08 mpeg. 7.96 750 1557 493 23 2.99 mtrt 1.74 359 1591 \n231 5 1.25 bloat 2.20 1227 4065 309 4 0.32 fop 2.35 1891 4050 478 16 0.84 luindex 2.98 961 3366 292 22 \n2.27 pmd 1.86 1195 3862 3175 10 0.80 Average 2.84 719 2421 288 13 2.30 SableVM (Java) almabench 7.68 \n114 675 172 23 20.07 almabench fast 7.68 111 675 169 21 18.56 bdd 3.24 31 242 131 3 10.69 fft 17.34 32 \n171 189 9 27.02 fft.fast 17.34 32 171 189 9 27.02 kb 2.03 41 425 98 1 3.00 nucleic 2.14 113 734 157 2 \n2.06 quicksort 3.00 19 152 131 1 4.90 quicksort fast 2.88 18 159 113 1 5.15 ray 2.34 93 710 134 4 4.51 \nsorts 3.30 214 1478 148 5 2.30 norm 5.12 82 635 132 1 1.54 Average 6.17 75 519 145 7 9.70 OCaml .b 1.37 \n0.42 16 27 0.2 46.15 pentomino 1.22 4.98 72 71 0.4 8.39 tak 1.44 0.62 16 40 0.1 15.65 meteor-contest \n1.21 14.33 145 101 0.3 2.11 nsieve 1.53 2.18 41 54 0.3 15.41 Average 1.36 4.51 58 59 0.3 17.54 test-all \n1.29 436.29 2328 192 2.4 0.53 Average 1.34 76.47 436 81 0.6 14.71 Yarv (Ruby) Table 3. Dynamic bytecode \nexecution metrics for SableVM, OCaml, and Yarv on x86_64. To make observations of benchmark behaviour \nbased on hardware counter data easier in Figures 5 to 10 under each bar chart of hardware counter results \nan additional mark\u00ading showing the expected impact of the measured hardware event on the overall performance \nis added. The ++ sym\u00ad a) b) c) Figure 4. Average lengths (in bytecodes) of executed su\u00adperinstructions \nfor a) SableVM, b) OCaml, c) Yarv. bol indicates a very substantial improvement from using code-copying \nover direct-threaded, + indicates a useful improvement, . means a lack of meaningful change, and  an \nexpected degradation of performance.  General Findings Overall branch prediction is, unsurprisingly, \none of the main reasons for performance improvement. Reductions in the rate of branch mispredictions \n(or pipeline .ushes due to branch misprediction) closely follows performance. Reduc\u00adtions in the total \nnumber of branches as measured through average superinstruction length also mirrors performance. Both \nare, however, constrained by the relative amount of time spent in copied code. Branch misprediction (or \npipeline .ush due to branch misprediction) rates are found in Table 4. A comparison be\u00adtween the rates \nfor direct-threaded and code-copying VMs reveals a correlation between these results and performance. \nIn each of the analyzed cases a signi.cant drop in branch mispredictions due to code-copying results \nin a signi.cant performance improvement, and smaller branch mispredic\u00adtion drops resulted in more moderate \nperformance improve\u00adments. The relative impact can largely be explained by consid\u00adering the branch prediction \ncapabilities of the different ar\u00adchitectures. The Pentium 4 (Prescott core) has a 31 stage pipeline, \nalong with 4k entries in the front-end BTB (Branch Target Buffer) table, and 2k entries in the back-end \nBTB. A specialized predictor borrowed from the Pentium M series is used to improve the prediction of \nindirect branches. Unfortu\u00adnately, this predictor has serious performance problems with consecutive indirect \nbranches, and is designed to work best when indirect branching is interleaved with direct branches, a \nproperty which is generally not true of direct-threaded code execution. Limitations such as this, the \nrelatively small size of BTB tables and a very long pipeline mean the impact of complex branching can \nbe large on the Pentium 4, and we conclude these as the reasons for the very high branch mis\u00adprediction \nrates in the direct-threaded engines. The AMD64 X2 Dual CPU (Hammer core) has a 12 stage pipeline and \nuses three branch prediction structures. The local branch history table has 2k entries, the global history \ntable has 16k entries, and a branch selection table is used to decide which of these two predictors is \nexpected to give a more accurate prediction. Additionally it also has a specialized unit called the branch \ntarget address calculator which diminishes the penalty caused by a wrong prediction. A short pipeline, \nadvanced, hybrid prediction strategy, and more abundant resources allow this architecture to greatly \nreduce misprediction-rates over ia32. Our PowerPC G5 (970FX) has a 25 stage pipeline; similar to the \nAMD64 it employs a three-part branch misprediction strategy, although with tables allowing 16k entries \neach. Up to 2 branches can be predicted per cycle and up to 16 predicted branches can be in .ight. Despite \nthe larger tables, on a directed-threaded engine this has a roughly comparable branch-prediction behaviour \nto AMD64. Improvements brought by code-copying mostly correlate well with branch prediction behaviour \nthe AMD64 ma\u00adchine is able to better predict branch targets for SableVM and OCaml than the Pentium 4 \n(ia32 architecture), and so the drop in mispredictions due to using code-copying is cor\u00adrespondingly \nsmaller on x86_64 than ia32. PowerPC shows even less impact from branch prediction improvements, and \nthis hierarchy is re.ected in the overall performance of the three architectures. The code-copying implementation \nof the compress benchmark, for example, reduces mispredictions on ia32 from 0.584 to 0.038 (just 7% of \ndirect-threading), on x86_64 from 0.373 to 0.056 (down to 15%), and on PowerPC pipeline .ushes due to \nbranch mispredictions drop from 0.343 to 0.173 (32%), and similarly (Figure 3) ia32 per\u00adformance is greatly \nincreased (speedup 2.66), x86_64 per\u00adformance is nicely improved (speedup 1.92), and PowerPC performance \nimproved somewhat (speedup 1.45).  Figure 3. Performance (speedup) results for a) SableVM (Java), b) \nOCaml, and c) Yarv (Ruby). Note in this .gure we measured the overall VM performance, as opposed to other \nresults that measure VM behaviour only within the interpreter function. The number of branches is obviously \nan important per-of fewer instructions. The average, dynamic length of su\u00adformance factor affected by \ncode-copying, both through perinstructions (in bytecodes) is shown in Figure 4. this reduced branch mispredictions, \nand with the elision of behaviour can be directly related to the average VM per\u00adbranches within a superinstruction, \nthrough the execution formance shown in summary Table 2. The longest superin\u00ad VM benchmark ia32 compress \n58.4% SableVM jack 40.7% a) quicksort.fast 53.2% Ocaml almabench 38.9% nsieve 12.8% Yarv pentomino 9.6% \n Direct x86_64 37.3% 33.0% 20.9% 27.1% 15.2% 14.2% ppc 34.3% 29.5% 28.7% 37.0% 15.5% 12.8% ia32 3.8% \n20.2% 4.5% 3.8% 0.2% 6.2% Copying x86_64 5.6% 15.0% 4.2% 6.5% 2.5% 6.7% ppc 17.3% 22.2% b) 11.3% 26.1% \n3.8% 10.8% VM Dir ia32 Cpy ia32 SableVM 37% 11% OCaml 44% 5% Yarv 12% 8%  Table 4. a) Direct-threaded \nand code-copying implementation branch misprediction percentages across architectures, for best and worst \nperforming benchmark on each VM. For ia32 and x86_64 the numbers represent the ratio of mispredicted \nbranches to total branches, and for PowerPC the numbers represent the ratio of instruction pipeline .ushes \ndue to branch misprediction to total branches. b) Average branch misprediction percentages across benchmarks \nfor ia32 on each VM, direct-threaded and code-copying. structions (6.17 bytecode on average) are found \nin OCaml and it also delivers the best performance out of all 3 VMs. Somewhat shorter (2.84 bytecode \non average) superinstruc\u00adtions are used by SableVM, allowing it to deliver very good performance, but \nnot as much improved as OCaml. Yarv execution results in very short (1.36 bytecode on average) superinstructions, \nand comparatively poor overall perfor\u00admance. Unfortunately, although Yarv still has about half of its \nbytecodes copyable these do not tend to form large con\u00adtiguous sequences at runtime. The overall impact \nof bene.ts to branch prediction and code execution are reduced if they are not applied reason\u00adably ubiquitously. \nData on proportion of time spent in ac\u00adtual copied code as opposed to non-copyable bytecode exe\u00adcutions \nis shown for the individual case studies in Figures 5 through 10. In the case of OCaml benchmarks in \nFigures 7 and 8, for example, we see that the fraction of interpreter loop time spent in copied code \nranges from about 70% to about 90%. The other extreme is again represented by Yarv, where the time spent \nin copied code ranges from only about 15% to about 25%. Modulo cache and other non-local ef\u00adfects these \nratios provide an upper limit on the potential per\u00adformance improvements.  Overhead Overhead in a code-copying \nsystem comes from several sources. Actual copying of code increases code-preparation time, and memory \nand superinstruction management add additional costs, although these are one-time costs amor\u00adtized over \nthe lifetime of execution. Ongoing overhead is mainly due to changes in code-generation from the com\u00adpiler \nenhancements that support code-copying. The addition of code-motion barriers can be expected to inhibit \nor alter application of some optimizations. To measure the overall overhead we compared perfor\u00admance \nof direct-threaded VMs with the performance of code-copying VMs, where the copied code is created, but \nnot actually used at runtime. Figure 11 summarizes results; SableVM data is not gathered due to dif.culties \nin separat\u00ad a) b) c) OCaml Yarv ia32 -2.78% 10.57% x86_64 1.36% 3.19% ppc -3.09% 5.93% Figure 11. Overhead \nof VMs with pragmas inserted around bytecode instructions used by code-copying and with copied code prepared \nbut not executed, over standard direct\u00adthreaded VMs. Part a) shows OCaml, b) Yarv, and c) av\u00aderages. \n ing the code-copying engine from direct-threaded execution. Overhead varies considerably, changing \nperformance on av\u00aderage between -5% and 11%, and overall between -20% and nearly 25% for the two VMs. \nNegative overhead is possible due to instruction cache changes, and perhaps poor heuristic Figure 5. \nHardware counter results for SableVM JVM running the SPEC compress benchmark. Sub.gures a) and b) present \nbranch-related results. Sub.gure c) shows the number of i-cache and d-cache misses per 1M (1000000) CPU \ncycles.  Dir Cpy ia32 Cpy x86_64 Cpy ppc total time (interp. func.) 1.0 0.31 0.31 0.65 time in copied \ncode 0.0 0.74 0.51 0.79 branches total 1.0 0.58 0.46 0.64 branches mispred. / total 1.0 0.07 0.15 branches \nmispred. .ush 1.0  0.32 branch abort stalls 1.0  0.40 instruction fetch stalls 1.0  0.45 ia32 Dir \nCpy x86_64 Dir Cpy ppc Dir Cpy L1 d  3461 4349 L1 i  6 2052 L2 d    53 153 L2 i    3 8 L2 i+d \n2 4 122 385  Dir Cpy ia32 Cpy x86_64 Cpy ppc total time (interp. func.) 1.0 0.83 0.63 1.52 time in \ncopied code 0.0 0.45 0.31 0.62 branches total 1.0 0.79 0.70 0.80 branches mispred. / total 1.0 0.50 0.45 \nbranches mispred. .ush 1.0  0.60 branch abort stalls 1.0  0.67 instruction fetch stalls 1.0  0.82 \n ia32 Dir Cpy x86_64 Dir Cpy ppc Dir Cpy L1 d  9167 8721 L1 i  1450 13233 L2 d    25 212 L2 i \n   11 4041 L2 i+d 3 16 76 311 b) c) Figure 6. Hardware counter results for SableVM JVM running the \nSPEC jack benchmark. Sub.gures a) and b) present branch\u00adrelated results. Sub.gure c) shows the number \nof i-cache and d-cache misses per 1M (1000000) CPU cycles. Dir Cpy ia32 Cpy x86_64 Cpy ppc total time \n(interp. func.) 1.0 0.15 0.48 0.57 time in copied code 0.0 0.76 0.68 0.77 branches total 1.0 0.50 0.55 \n0.48 branches mispred. / total 1.0 0.08 0.20 branches mispred. .ush 1.0  0.19 branch abort stalls 1.0 \n 0.71 instruction fetch stalls 1.0  0.54 ia32 Dir Cpy x86_64 Dir Cpy ppc Dir Cpy L1 d  522 472 L1 \ni  1 2 L2 d    2 2 L2 i    0 1 L2 i+d 0 1 5 10  Figure 7. Hardware counter results for OCaml \nrunning the quicksort.fast benchmark. Sub.gures a) and b) present branch\u00adrelated results. Sub.gure c) \nshows the number of i-cache and d-cache misses per 1M (1000000) CPU cycles. + ++++++ + +  Dir Cpy \nia32 Cpy x86_64 Cpy ppc total time (interp. func.) 1.0 0.38 0.69 0.61 time in copied code 0.0 0.94 0.92 \n0.85 branches total 1.0 0.43 0.71 0.55 branches mispred. / total 1.0 0.10 0.24 branches mispred. .ush \n1.0  0.39 branch abort stalls 1.0  0.31 instruction fetch stalls 1.0  0.90 ia32 Dir Cpy x86_64 Dir \nCpy ppc Dir Cpy L1 d  10749 8979 L1 i  401 6491 L2 d    2 12 L2 i    2 9 L2 i+d 0 1 47 185 \n b) c) Figure 8. Hardware counter results for OCaml running the almabench benchmark. Sub.gures a) and \nb) present branch-related results. Sub.gure c) shows the number of i-cache and d-cache misses per 1M \n(1000000) CPU cycles. a)  ++ ++ + Dir Cpy ia32 Cpy x86_64 Cpy ppc total time (interp. func.) 1.0 0.73 \n1.05 0.81 time in copied code 0.0 0.20 0.22 0.38 branches total 1.0 0.84 1.85 0.90 branch mispred. / \ntotal 1.0 0.02 0.17 branches mispred. .ush 1.0  0.22 branch abort stalls 1.0  1.15 instruction fetch \nstalls 1.0  1.32 ia32 Dir Cpy x86_64 Dir Cpy ppc Dir Cpy L1 d  2108 1914 L1 i  2 2 L2 d    6 \n5 L2 i    2 3 L2 i+d 428 598 522 633 b) c) Figure 9. Hardware counter results for Yarv Ruby VM running \nthe nsieve benchmark. Sub.gures a) and b) present branch\u00adrelated results. Sub.gure c) shows the number \nof i-cache and d-cache misses per 1M (1000000) CPU cycles. Dir Cpy ia32 Cpy x86_64 Cpy ppc total time \n(interp. func.) 1.0 1.18 1.18 1.21 time in copied code 0.0 0.11 0.12 0.17 branches total 1.0 1.01 1.12 \n0.91 branches mispred. / total 1.0 0.64 0.47 branches mispred. .ush   0.77 branch abort stalls 1.0 \n 1.13 instruction fetch stalls 1.0  1.13 Figure 10. Hardware counter results for Yarv Ruby VM running \nthe pentomino benchmark. Sub.gures a) and b) present branch-related results. Sub.gure c) shows the number \nof i-cache and d-cache misses per 1M (1000000) CPU cycles.  ia32 Dir Cpy x86_64 Dir Cpy ppc Dir Cpy \nL1 d  2340 3585 L1 i  4235 2484 L2 d    7 5 L2 i    7 6 L2 i+d 16 26 133 120 b) c) performance \nof basic block rearrangement or other opti\u00admizations that are (locally) prevented by the code-copying \nenhancement. We note, however, that for OCaml overhead is fairly low and often negative; for Yarv overhead \nis almost uniformly positive and sometimes large. This also partially accounts for and re.ects the lower \nimprovement experienced by Yarv due to code-copying.  Further Performance Factors While branch instruction \neffects are primary in a general sense, individual benchmarks naturally vary, and raise a number of issues \nnot obvious or clear from a simple con\u00adsideration of the branch-reducing nature of code-copying or its \nbasic overhead. Instruction cache behaviour, for in\u00adstance, is also potentially negatively affected by \nthe size and number of superinstructions. Other, benchmark-speci.c and compiler-driven factors can further \nintrude. Below we dis\u00adcuss these issues in relation to our experimental data. Instruction Cache Impact \nUsing any dynamic code creation technique carries the dan\u00adger of creating too much code and lowering \nthe performance (increasing miss rates) of the CPU instruction cache. Our results show that excessive \ncode size is not a signi.cant con\u00adcern for our VMs and tests. From Table 3 copied code usu\u00adally occupies \nat most a few hundred bytes per superinstruc\u00adtion, with total size averaging less than 1MB in Java and \nwell less than 100kB in OCaml and Ruby. Despite this large size difference, a consideration of important \ncode size shows the VMs are both similar and not overly affected by too many distinct superinstructions. \nWhen looking at code within the 90% of most actively executed bytecode instructions mem\u00adory requirements \nare much reduced, 13kB on average for Java, 7kB on average for OCaml, and never more than 23kB in any \ntested case. Code size is a rough predictor of I-cache performance; better information is obtained by \nmeasuring actual cache miss behaviour. Part c) of Figures 5, 6, 7, 8, 9, 10 shows runtime instruction \nand data cache miss rates for the best and worst performing benchmarks. While there is a general, and \nsometimes marked (Figure 5) increase in L1 I-cache misses the effect is not uniform, and does not correlate \nwell with performance. Of course L1 misses are not necessarily that expensive if caught by the L2 cache. \nAn increase in I-cache pressure can cause more spillover into the L2, and thus greater chance of having \nto undergo the costly step of retrieving data from main memory. This effect is evident in the SPEC jack \nbenchmark on PowerPC; in part c) of Table 6 there is a large increase in the L2 I-cache miss rate due \nto code-copying. We believe that this higher L2 I-cache miss rate is part of the reason for this benchmark \ns lower performance on PowerPC. Improvements to instruction cache miss rates may be pos\u00adsible by exploiting \nthe heavy concentration of execution in a relatively small number of superinstructions We expect we would \nbe able to achieve most of the performance bene.ts of code copying by only creating a small fraction \nof the code we create currently, although this requires pro.ling or adap\u00adtive techniques to discover \nthe 90% of most actively used superinstructions. VM Overhead Although the test-all benchmark on Yarv \nrepresents one of the larger Ruby benchmarks we used its behaviour and char\u00adacteristics are very different \nfrom other benchmarks. This benchmark performs surprisingly poorly, but for other rea\u00adsons than originally \nexpected. As can be seen in the dynamic metrics of Table 3 it creates a large amount of copied code, \nover 3200 superinstructions using over 430kB of memory, an exceptionally large number of superinstructions \nin com\u00adparison with other Ruby benchmarks. As a code sanity test, the bulk of code in this program consists \nof small tests ex\u00adecuted once, or only a few times. This results in only about 0.5% of the code created \naccounting for 90% of execution. Code-copying itself has little positive impact in this situa\u00adtion; interestingly, \nas seen in the actual performance of test\u00adall (Figure 3) the larger overhead of copied code does not \nhave as large a negative impact as these relative results for Yarv may suggest. Performance degradation \n(on Intel-like architectures) is in fact dominated by code preparation costs, in which the VM initializes \ncode for execution, adding an overhead which is not in this case amortized over multiple executions of \nthe created code: only about 7% of execution time is spent in the interpreter loop. Since the behaviour \nof this benchmark is unusual we present averages in Table 3 with and without the test-all benchmark included. \n Compiler Optimizations In early experiments with the code-copying version of Yarv an anomaly was found \nwhen running the nsieve benchmark on the x86_64 architecture. Paradoxically, the number of branches executed \nin the interpreter loop grew by a factor of 1.85 in the code-copying engine, and moreover disabling runtime \nuse of code-copying and re-measuring performance produced the same result. We attribute this to the interac\u00adtion \nof code-copying and compiler optimization, an over\u00adhead consideration in general. Non-localized disabling \nof branch or basic block reordering optimizations, in combina\u00adtion with benchmark-speci.c behaviour would \naccount for the increased number of branches in the interpreted code, irrespective of whether it was \nactually code-copying or exe\u00adcuting code in the normal, direct-threaded fashion. This be\u00adhaviour did \nnot occur with other benchmarks or on other platforms, and while it may have reduced performance it did \nnot result in a slowdown the overall overhead for nsieve is still small on x86_64 (Figure 11). Summary \nDifferent languages and VM (bytecode) designs have a strong impact on the performance bene.t provided \nby code\u00adcopying. At one end of the scale languages and bytecode instruction sets like Ruby (and the particular \nVM we used Yarv) introduce several problems when it comes to the appli\u00adcation of code-copying. Outwardly \nsmall bytecodes in fact perform signi.cant amounts of runtime work, making nu\u00admerous calls to helper \nfunctions and often changing control .ow of the program. This constrains code-copying, mak\u00ading long superinstructions \nunlikely. With little time overall spent in the actual interpreter loop, the main positive effect of \ncode-copying in reducing branch costs is greatly lessened. Dramatically better effects are shown on a \nlanguage like OCaml, that has functionally small, .ne-grained bytecodes containing mostly simple operations. \nFull implementations are also typically contained within the bytecode itself, with no need for functions \ncalled externally. Code-copying ap\u00adplies very well to this kind of VM design since it is possible to \ncreate many superinstructions (improving branch predic\u00adtion, even for short superinstructions) and superinstructions \nthat are longer (removing many branches). Java bytecode has qualities of both Ruby and OCaml; performance \nfortu\u00adnately falls closer to the OCaml side of the spectrum. Further simpli.cation of bytecode implementations, \nhowever, would likely increase the performance improvement. All behaviour is affected by the quality \nof hardware branch prediction. In general PowerPC shows the least gains and ia32 the most, largely re.ecting \nthe relative branch pre\u00addiction capabilities of the different architectures. The effect of hardware is \nstill not as pronounced as aspects of vir\u00adtual machine design, and hardware resources are scarce, but \nclearly improved hardware branch prediction would be another source of optimization especially applicable \nto interpreter-based virtual machines.  6. Conclusions An examination of the behaviour of different \nlanguages, vir\u00adtual machines is useful for any cross-context optimization. Code-copying has been prototyped \nin single environments before, but our work represents the .rst multi-language, multi-platform examination \nof performance, based on a safe implementation model for code-copying. There is clearly a spectrum in \nthe performance impact of code-copying, with behaviour depending on virtual machine implementation, bytecode \ndesign, and hardware considerations. Although various factors can in.uence results, bytecode properties \ndominate. Simple bytecodes can be easily col\u00adlected into superinstructions, and imply more time in the \nactual interpreter loop, raising the upper bound on poten\u00adtial impact. Virtual machine designs that stress \nsimple as opposed to complex bytecode behaviour represent a trade\u00adoff between simplicity of code generation/execution, \nand smaller code size. Code-copying can be applied in both sce\u00adnarios, but practical limitations mean \nperformance improve\u00adment is best for simple designs, where overhead is heavily concentrated in interpreter \nloop dispatch costs. Determining the potential impact of code-copying ahead of time bene.ts virtual machine \ndesigners as well as imple\u00admenters contemplating using code-copying. Based on our experiments we are \nable to tie certain characteristics of byte\u00adcode and interpreters using direct-threading to the perfor\u00admance \nimprovement provided by code-copying. Our experi\u00adence suggests a short, step-by-step procedure for estimating \nthe degree to which a virtual machine currently using direct\u00adthreading may improve after application \nof code copying. 1. Determine the bytecode instructions in the source code that can be used in code-copying. \n 2. Calculate the relative number of bytecodes in the inter\u00adpreter source code that are potentially copyable. \nCheck if applying an upper limit to the bytecode size, e.g. 150-200 bytes, makes a difference. Refer \nto Table 1 for evaluation. The more (and more small) bytecodes that are copyable the better for code-copying. \n 3. On a set of benchmarks measure how many loaded byte\u00adcodes are potentially copyable. Refer to Figure \n2 for eval\u00aduation. The more loaded bytecodes that are copyable the better. 4. On a set of benchmarks \nmeasure how much time is spent in the interpreter loop (excluding functions called from within the loop). \nThe more consistent the results and the more time is spent in the loop the more generally useful and \neffective code-copying will be. 5. On a set of benchmarks and using a given architecture measure branch \nprediction miss-rates within the inter\u00adpreter loop. Refer to Table 2 for interpretation. The higher the \nmiss-rate the more room for improvement by using code-copying.  Based on these simple measurements (or \nas many as can be acquired) it should be possible to gain a meaningful in\u00adsight into whether an investment \nof time and effort into a code-copying implementation for a particular language and virtual machine is \nworthwhile. Note that an unpromising evaluation outcome does not necessarily mean code-copying will never \nbe applicable to a language or a virtual machine. For example, some forms of bytecode optimization or \nre\u00adstructuring, as was originally done for SableVM, may im\u00adprove the end result. It is expected that \napplication of code-copying can pro\u00advide an improvement in startup time for languages where it matters \n(e.g. Java VM extensive bootstrap), where a direct\u00adthreaded interpreter is normally used for freshly \nloaded code before its hot sections are detected and sent to a JIT. Be\u00adcause our test system (SableVM) \ndoes not contain an actual optimizing JIT we were unable to perform measurements of such impact. Our \nimplementations of code-copying form an initial step in analyzing and optimizing performance through \nthis tech\u00adnique. The typically small size of the core runtime bytecode working-set (top 90% of executed \nbytecodes) for all our vir\u00adtual machines suggests signi.cant improvements to runtime overhead are possible \nby not copying all copyable sequences executed, either through adaptive selection of the sequences to \ncopy (e.g., using hotness counts) or by using ahead of time pro.les to guide choices. This would further \nimprove performance, and make the technique more generally attrac\u00adtive even when branching is not known \nto be a major perfor\u00admance bottleneck.  Acknowledgments This research was supported by the Natural Science \nand Engineering Research Council of Canada. References [1] David B\u00e9langer. SableJIT: A retargetable \njust-in-time compiler. Master s thesis, McGill University, August 2004. [2] Marc Berndl, Benjamin Vitale, \nMathew Zaleski, and Angela Demke Brown. Context threading: A .exible and ef.cient dispatch technique \nfor virtual machine interpreters. In Proceedings of CGO-4, 2005. [3] S. M. Blackburn, R. Garner, C. Hoffman, \nA. M. Khan, K. S. McKinley, R. Bentzur, A. Diwan, D. Feinberg, D. Frampton, S. Z. Guyer, M. Hirzel, A. \nHosking, M. Jump, H. Lee, J. E. B. Moss, A. Phansalkar, D. Ste\u00adfanovi\u00b4 c, T. VanDrunen, D. von Dincklage, \nand B. Wie\u00addermann. The DaCapo benchmarks: Java benchmark\u00ading development and analysis. In OOPSLA 06: \nPro\u00adceedings of the 21st annual ACM SIGPLAN conference on Object-Oriented Programing, Systems, Languages, \nand Applications, New York, NY, USA, October 2006. ACM Press. [4] Per Bothner. Compiling Java with GCJ. \nLinux J., 2003 (105):4, 2003. ISSN 1075-3583. [5] C. Consel, J.L. Lawall, and A.-F. Le Meur. A tour of \nTempo: A program specializer for the C language. Science of Computer Programming, 2004. [6] Debian Shootout. \nhttp://shootout.alioth. debian.org/. [7] M. Anton Ertl and David Gregg. The behavior of ef\u00ad.cient virtual \nmachine interpreters on modern archi\u00adtectures. In Euro-Par 01: Proceedings of the 7th In\u00adternational \nEuro-Par Conference Manchester on Par\u00adallel Processing, pages 403 412, London, UK, 2001. Springer-Verlag. \nISBN 3-540-42495-4. [8] M. Anton Ertl and David Gregg. Optimizing indirect branch prediction accuracy \nin virtual machine inter\u00adpreters. In SIGPLAN 03 Conference on Programming Language Design and Implementation, \n2003. [9] M. Anton Ertl and David Gregg. Retargeting JIT com\u00adpilers by using C-compiler generated executable \ncode. In Parallel Architecture and Compilation Techniques (PACT 04), pages 41 50, 2004. [10] M. Anton \nErtl, David Gregg, Andreas Krall, and Bernd Paysan. Vmgen: a generator of ef.cient virtual machine interpreters. \nSoftw. Pract. Exper., 32(3):265 294, 2002. ISSN 0038-0644. doi: http://dx.doi.org/10.1002/spe. 434. [11] \nM. Anton Ertl, Christian Thalinger, and Andreas Krall. Superinstructions and replication in the Cacao \nJVM interpreter. Journal of .NET Technologies, 4:25 32, 2006. ISSN 1801-2108. [12] Etienne M. Gagnon. \nA Portable Research Framework for the Execution of Java Bytecode. PhD thesis, McGill University, 2002. \n[13] Etienne M. Gagnon and Laurie J. Hendren. SableCC, an object-oriented compiler framework. In TOOLS \n98: Proceedings of the Technology of Object-Oriented Languages and Systems, page 140, Washington, DC, \nUSA, 1998. IEEE Computer Society. ISBN 0-8186\u00ad8482-8. [14] Brian Grant, Matthai Philipose, Markus Mock, \nCraig Chambers, and Susan J. Eggers. A retrospective on: an evaluation of staged run-time optimizations \nin DyC . SIGPLAN Not., 39(4):656 669, 2004. ISSN 0362\u00ad1340. doi: http://doi.acm.org/10.1145/989393.989458. \n[15] Chris Lattner and Vikram Adve. LLVM: A com\u00adpilation framework for lifelong program analysis &#38; \ntransformation. In CGO 04: Proceedings of the in\u00adternational symposium on Code generation and opti\u00admization, \npage 75, Washington, DC, USA, 2004. IEEE Computer Society. ISBN 0-7695-2102-9. [16] Hidehiko Masuhara \nand Akinori Yonezawa. Run-time bytecode specialization. Lecture Notes in Computer Science, 2053, 2001. \n[17] OCaml. http://caml.inria.fr. [18] OPro.le. http://oprofile.sf.net/. [19] K. Palacz, J. Baker, C. \nFlack, C. Grothoff, H. Ya\u00admauchi, and J. Vitek. Engineering a customizable intermediate representation. \nIn IVME 03: Proceed\u00adings of the 2003 workshop on Interpreters, virtual ma\u00adchines and emulators, pages \n67 76, New York, NY, USA, 2003. ACM. ISBN 1-58113-655-2. doi: http: //doi.acm.org/10.1145/858570.858578. \n[20] Jinzhan Peng, Gansha Wu, and Guei-Yuan Lueh. Code sharing among states for stack-caching interpreter. \nIn IVME 04: Proceedings of the 2004 workshop on Inter\u00adpreters, virtual machines and emulators, pages \n15 22, New York, NY, USA, 2004. ACM. ISBN 1-58113-909\u00ad 8. doi: http://doi.acm.org/10.1145/1059579.1059584. \n[21] Ian Piumarta and Fabio Riccardi. Optimizing direct threaded code by selective inlining. In PLDI \n98: Proceedings of the ACM SIGPLAN 1998 conference on Programming language design and implementation, \npages 291 300, New York, NY, USA, 1998. ACM Press. ISBN 0-89791-987-4. doi: http://doi.acm.org/ 10.1145/277650.277743. \n[22] Patrice Pominville, Feng Qian, Raja Vall\u00e9e-Rai, Laurie Hendren, and Clark Verbrugge. A framework \nfor op\u00adtimizing Java using attributes. In Reinhard Wilhelm, editor, Proceedings of the 10th International \nConfer\u00adence on Compiler Construction (CC 01), volume 2027 of Lecture Notes in Computer Science (LNCS), \npages 334 354, April 2001. [23] Gregory B. Prokopski and Clark Verbrugge. Towards GCC as a compiler for \nmultiple VMs. In GCC Devel\u00adopers Summit, 2007. [24] Gregory B. Prokopski and Clark Verbrugge. Compiler\u00adguaranteed \nsafety in code-copying virtual machines. In Compiler Construction, 17th International Conference, LNCS. \nSpringer, 2008. to appear. [25] Gregory B. Prokopski, Etienne M. Gagnon, and Chris\u00adtian Arcand. Bytecode \ntesting framework for SableVM code-copying engine. Technical Report SABLE\u00adTR-2007-9, Sable Research Group, \nSchool of Com\u00adputer Science, McGill University, Montr\u00e9al, Qu\u00e9bec, Canada, September 2007. [26] Raw results \nused for this publication. http: //www.sable.mcgill.ca/~gproko/gcc/ multi-08-raw-results.pdf. [27] Markku \nRossi and Kengatharan Sivalingam. A survey of instruction dispatch techniques for byte-code inter\u00adpreters. \nTechnical Report TKO-C79, Faculty of Infor\u00admation Technology, Helsinki Univeristy of Technology, May \n1996. [28] Koichi Sasada. YARV: yet another RubyVM: inno\u00advating the Ruby interpreter. In OOPSLA 05: Com\u00adpanion \nto the 20th annual ACM SIGPLAN conference on Object-oriented programming, systems, languages, and applications, \npages 158 159, New York, NY, USA, 2005. ACM. ISBN 1-59593-193-7. doi: http://doi.acm. org/10.1145/1094855.1094912. \n[29] Standard Performance Evaluation Corporation. SPEC JVM98 Benchmarks. http://www.spec.org/ jvm98. \n[30] Ben Stephenson and Wade Holst. Multicodes: op\u00adtimizing virtual machines using bytecode sequences. \nIn OOPSLA 03: Companion of the 18th annual ACM SIGPLAN conference on Object-oriented pro\u00adgramming, systems, \nlanguages, and applications, pages 328 329, New York, NY, USA, 2003. ACM Press. ISBN 1-58113-751-6. doi: \nhttp://doi.acm.org/10.1145/ 949344.949436. [31] T. Suganuma, T. Ogasawara, M. Takeuchi, T. Ya\u00adsue, M. \nKawahito, K. Ishizaki, H. Komatsu, and T. Nakatani. Overview of the IBM Java Just-in-Time compiler. IBM \nSystems Journal, 39(1):175 193, 2000. [32] S. Thibault, C. Consel, J. Lawall, R. Marlet, and G. Muller. \nStatic and dynamic program compilation by interpreter specialization. Higher-Order and Symbolic Computation, \n13(3):161 178, September 2000. [33] Ankush Varma and Shuvra S. Bhattacharyya. Java\u00adthrough-C compilation: \nAn enabling technology for Java in embedded systems. In DATE 04: Proceedings of the conference on Design, \nautomation and test in Eu\u00adrope, page 30161, Washington, DC, USA, 2004. IEEE Computer Society. ISBN 0-7695-2085-5-3. \n[34] Benjamin Vitale and Mathew Zaleski. Alternative dis\u00adpatch techniques for the tcl vm interpreter. \nIn Proceed\u00adings of 12th Annual Tcl/Tk Conference, October 2005. [35] Mathew Zaleski, Marc Berndl, and \nAngela Demke Brown. Mixed mode execution with context threading. In CASCON 05: Proceedings of the 2005 \nconference of the Centre for Advanced Studies on Collaborative research, pages 305 319. IBM Press, 2005. \n \n\t\t\t", "proc_id": "1449764", "abstract": "<p>Many popular programming languages use interpreter-based execution for portability, supporting dynamic or reflective properties, and ease of implementation. <i>Code-copying</i> is an optimization technique for interpreters that reduces the performance gap between interpretation and <i>JIT</i> compilation, offering significant speedups over direct-threading interpretation. Due to varying language features and virtual machine design, however, not all languages benefit from codecopying to the same extent. We consider here properties of interpreted languages, and in particular bytecode and virtual machine construction that enhance or reduce the impact of code-copying. We implemented code-copying and compared performance with the original direct-threading virtual machines for three languages, Java (SableVM), OCaml, and Ruby (Yarv), examining performance on three different architectures, ia32 (Pentium 4), x86_64 (AMD64) and PowerPC (G5). Best speedups are achieved on ia32 by OCaml (maximum 4.88 times, 2.81 times on average), where a small and simple bytecode design facilitates improvements to branch prediction brought by code-copying. Yarv only slightly improves over direct-threading; large working sizes of bytecodes, and a relatively small fraction of time spent in the actual interpreter loop both limit the application of codecopying and its overall net effect. We are able to show that simple ahead of time analysis of VM and execution properties can help determine the suitability of code-copying for a particular VM before an implementation of code-copying is even attempted.</p>", "authors": [{"name": "Gregory B. Prokopski", "author_profile_id": "81381605328", "affiliation": "McGill University, Sable Research Group, Montreal, PQ, Canada", "person_id": "P1223215", "email_address": "", "orcid_id": ""}, {"name": "Clark Verbrugge", "author_profile_id": "81100085529", "affiliation": "McGill University, Sable Research Group, Montreal, PQ, Canada", "person_id": "P1223216", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/1449764.1449796", "year": "2008", "article_id": "1449796", "conference": "OOPSLA", "title": "Analyzing the performance of code-copying virtual machines", "url": "http://dl.acm.org/citation.cfm?id=1449796"}