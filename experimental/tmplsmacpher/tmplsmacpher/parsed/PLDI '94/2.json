{"article_publication_date": "06-01-1994", "fulltext": "\n Lazy Functional State Threads John Launchbury and Simon L Peyton Jones University of Glasgow Email: \n{simonpj, jl}@dcs .glasgow. ac. uk. Phone: +44-41-330-4500 Abstract Some algorithms make critical internal \nuse of updat\u00ad able state, even though their external specification is purely functional. Based on earlier \nwork on monads, we present a way of securely encapsulating stateful compu\u00ad tations that manipulate multiple, \nnamed, mutable ob\u00adjects, in the context of a non-strict, purely-functional language. The security of \nthe encapsulation is assured by the type system, using parametricity. Intriguingly, this parametricity \nrequires the provision of a (single) con\u00ad stant with a rank-2 polymorphic type. 1 Introduction Purely \nfunctional programming languages allow many algorithms to be expressed very concisely, but there are \na few algorithms in which in-place updatable state seems to play a crucial role. For these algo\u00adrithms, \npurely-functional languages, which lack updat\u00adable state, appear to be inherently inefficient (Ponder, \nMcGeer &#38; Ng [1988]). Take, for example, algorithms based on the use of incrementally-modified hash \ntables, where lookups are interleaved with the insertion of new items. Similarly, the union/find algorithm \nrelies for its efficiency on the set representations being simplified each time the struc\u00adture is examined. \nLikewise, many graph algorithms re\u00adquire a dynamically changing structure in which shar\u00ading is explicit, \nso that changes are visible non-locally. There is, furthermore, one absolutely unavoidable use of state \nin every functional program: input/output. The plain fact of the matter is that the whole pur- Permission \nto copy without fee all or part of this material is granted provided that the copies are not made or \ndistributed for direct commercial advantage, the ACM copyright notice and the title of the publication \nand its date appear, and notice is given that copying is by permission of the Association of Computing \nMachinery. To copy otherwise, or to republish, requires a fee and/or specific permission. SIGPLAN 94-6/94 \nOrlando, Florida USA @ 1994 ACM 0-89791 -662-x19410006 ..$3.5O pose of running a program, functional \nor otherwise, is to make some side effect on the world an update\u00adin-place, if you please. In many programs \nthese 1/0 effects are rather complex, involving interleaved reads from and writes to the world state. \nWe use the term stateful to describe computations or algorithms in which the programmer really does want \nto manipulate (updatable) state. What has been lacking until now is a clean way of describing such al\u00adgorithms \nin a functional language especially a non\u00adstrict one without throwing away the main virtues of functional \nlanguages: independence of order of eval\u00aduation (the Church-Rosser property), referential trans\u00adparency, \nnon-strict semantics, and so on. In this paper we describe a way to express stateful algorithms in non-strict, \npurely-functional languages. The approach is a development of our earlier work on monadic 1/0 and state \nencapsulation (Launchbury [1993]; Peyton Jones&#38; Wadler [1993]), but with an im\u00adportant technical \ninnovation: we use parametric poly\u00admorphism to achieve safe encapsulation of state. It turns out that \nthis allows mutable objects to be named without losing safety, and it also allows input/output to be \nsmoothly integrated with other state mainpula\u00adtion. The other important feature of this paper is that \nit describes a complete system, and one that is im\u00adplemented in the Glasgow Haskell compiler and freely \navailable. The system has the following properties: Complete referential transparency is maintained. \nAt first it is not clear what this statement means: how can a stateful computation be said to be refer\u00adentially \ntransparent? To be more precise, a state\u00adful computation is a state transformer, that is, a function \nfrom an initial state to a final state. It is like a script , detailing the actions to be per\u00adformed \non its input state. Like any other function, it is quite possible to apply a single stateful com\u00adputation \nto more than one input state. So, a state transformer is a pure function. But, because we guarantee that \nthe state is used in a single-threaded way, the final state can be con\u00adstructed by modifying the input \nstate in-place. This efficient implementation respects the purely\u00adfunctional semantics of the state-transformer \nfunc\u00adtion, so all the usual techniques for reasoning about functional programs continue to work. Sim\u00adilarly, \nstateful programs can be exposed to the full range of program transformations applied by a compiler, \nwith no special cases or side conditions.  The programmer has complete control over where in-place updates \nare used and where they are not. For example, there is no complex analysis to de\u00adtermine when an array \nis used in a single-threaded way. Since the viability of the entire program may be predicated on the \nuse of in-place updates, the programmer must be confident in, and be able to reason about, the outcome. \n Mutable objects can be named. This ability sounds innocuous enough, but once an object can be named \nits use cannot be controlled as readily. Yet naming is important. For example, it gives us the ability \nto manipulate multiple mutable objects simultaneously.  . Input/output takes its place as a specialised \nform of stateful computation. Indeed, the type of I/O\u00adperforming computations is an instance of the (more \npolymorphic) type of stateful computations. Along with 1/0 comes the ability to call impera\u00adtive procedures \nwritten in other languages. . It is possible to encapsulate stateful computations so that they appear \nto the rest of the program as pure (stateless) functions which are guaranteed by the type system to have \nno interactions what\u00adever with other computations, whether stateful or otherwise (except via the values \nof arguments and results, of course). Complete safety is maintained by this encapsula\u00adtion. A program \nmay contain an arbitrary number of stateful sub-computations, each simultaneously active, without concern \nthat a mutable object from one might be mutated by another. Stateful computations can even be performed \nlazily without losing safety. For example, suppose that stateful depth-first search of a graph returns \na list of vertices in depth-first order. If the consumer of this list only evaluates the first few elementfi \nof the list, then only enough of the stateful compu\u00adtation is executed to produce those elements.  2 \nOverview This section introduces the key ideas of our ap\u00ad proach to stateful computation. We begin with \nthe programmer s-eye-view. 2.1 State transformers A value of type (ST s a) is a computation which trans\u00adforms \na state indexed by type s, and delivers a value of type a. You can think of it as a box, like this: Result \nI if Ir > State in State out Notice that this is a purely-functional account of state. The ST stands \nfor a state transformer , which we take to be synonymous with a stateful computation : the computation \nis seen as transforming one state into another. (Of course, it is our intention that the new state will \nactually be constructed by modifying the old one in place, a matter to which we return in Section 6.) \nA state transformer is a first-class value: it can be passed to a function, returned as a result, stored \nin a data structure, duplicated freely, and so on. A state transformer can have other inputs besides \nthe state; if so, it will have a functional type. It can also have many results, by returning them in \na tuple. For example, a state transformer with two inputs of type Int, and two results of type Int and \nBoo1, would have the type: Int -> Int -> ST s (Int ,Bool) Its picture might look like this: Inputs Results \nD State in State out The simplest state transformer, returnST, simply de\u00adlivers a value without affecting \nthe state at all: returnST :: a -> ST s a The picture for returnST is like this: II D State in State \nout 2.2 References the first. Indeed, we often refer to a state transformer as a thread, invoking the \npicture of a series of primi- What, then, is a state ? Part of every state is a finite tive stateful \noperations threaded together by a state mapping from references to values. (A state may also passed from \none to the next. have other components, as we will see in Section 4.) Putting together what we have so \nfar, here is a pro- A reference can be thought of as the name of (or ad\u00adcedure which swaps the contents \nof two variables: dress of) a variable, an updatable location in the state capable of holding a value. \nThe following primitive op\u00adswap :: MutVar sa-> MutVar sa-> STs ()erations are provided: swap v w = readVar \nv thenST (\\a -> readVar w thenST (\\b ->newVar :: a > ST s (MutVar s a) writeVar v b thenST( (\\-->readVar \n:: MutVar s a -> ST s a writeVar w a) ) ) writeVar :: MutVar s a -> a -> ST s () The syntax needs a \nlittle explanation. The form The function newVar takes an initial value, of (\\a->e is Haskell s syntax \nfor a lambda abstraction. type a, say, and delivers a state transformer of type ST s (MutVar s a), When \nthis is applied to a state, The body of the lambda abstraction, e, extends as far to the right as possible. \nSo in the code for swap, the it allocates a fresh reference that is, one currently not used in the state. \nIt augments the state with a second argument of the first thenST extends all the way from the \\a to the \nend of the function. That s just as mapping from this reference to the supplied value, and you would \nexpect: the second argument of a thenST returns the reference along with the modified state. The type \nMutVar s a is the type of references allo\u00ad is meant to be a function. The _ in the second-last line is \na wild-card pattern, which matches any value. cated from a store of type s, containing a value of type \na. Notice that, unlike SML S Ref types, for example, We use it here because the writeVar does not return \na value of interest. MutVars are parameterised over the type of the state as The parentheses can be omitted, \nsince infix oper\u00ad well as over the type of the value to which the reference ations bind less tightly \nthan the lambda abstractionis mapped by the state. (We use the name MutVar for operator. Furthermore, \nwe provide a special form of the type of references, rather than Ref, specifically to thenST, called \nthenST_, with the following type signa\u00adavoid confusion with SML. ) ture: Given a reference v, readVar \nv is a state transformer which leaves the state unchanged, but uses the state to thenST_ :: STso->STsb->STsb \nmap the reference to its value. Unlike thenST its second argument is not a function, The function writevar \ntransforms the state so that so the lambda isn t required. So we can rewite swap as it maps the given \nreference to a new value. Notice follows: that the reference itself does not change; it is the state \nwhich is modified. writeVar delivers a result of the swap :: MutVar s a -> MutVar s a -> ST s () unit \ntype (), a type which only has one value (apart swap v w = readVar v thenST \\a -~ from bottom), also \nwritten (). A state transformer of readVar w thenST \\b -> writeVar v b thenST_ type ST s ( ) is useful \nonly for its effect on the state. writeVar w a 2.3 Composing state transformers When swap v w is executed \nin a state thread (that is, when given a state), v is dereferenced, returning a State transformers can \nbe composed in sequence, to value which is bound to a. Similarly the value of w form a larger state transformer, \nusing thenST, which is bound to b. New values are then written into the has type state at these locations, \nthese values being b and a thenST :: STsa->(a -> STsb)->ST sb respectively. In addition to thenST and \nreturnST, we have found The picture for (s i thenST s2) is like thisl: it useful to introduce one other \nplumbing combina\u00adtor, fixST, It has the type fixST:: (a >STsa) > STsa   stateT-lm=mL and the usual \nknot-tying semantics, which we depict thus: Notice that the two computations must manipulate state indexed \nby the same type, s. Notice also that , tI thenST is inherently sequential, because the state con\u00adsumed \nby the second computation is that produced by statF 4EmA% 1Backquotes are Haskell s notation for an infix \noperator. This is the only point that relies on laziness. Every\u00ad thing else in the paper is directly \napplicable to strict languages. 2.4 Encapsulation So far we have been able to combine state transform\u00ad \ners to make larger state transformers, but how can we make a state transformer part of a larger program \nwhich does not manipulate state at all? What we need is a function, runST, with a type something like \nthe following: runST :: STsa->a The idea is that runST takes a state transformer as its argument, conjures \nup an initial empty state, ap\u00adplies the state transformer to it, and returns the result while discarding \nthe final state. The initial state is empty in the sense that no references have been al\u00adlocated in it \nby newvar; it is the empty mapping. But there seems to be a terrible flaw: what is to pre\u00advent a reference \nfrom one thread being used in another? For example: let v = runST (newVar True) in runST (readVar v) \n Here, the reference allocated in the first runST s thread is used inside the second runST. Doing so \nwould be a great mistake, because reads in one thread are not sequenced with respect to writes in the \nother, and hence the result of the program would depend on the evaluation order used to execute it. It \nseems at first that a runtime check might be required to ensure that references are only dereferenced \nin the thread which al\u00adlocated them. Unfortunately this would be expensive. Even worse, our experience \nsuggests that it is surpris\u00adingly tricky to implement such a check the obvious ideas fail as it then \nbecomes possible to test the identity of a thread so losing referential transparency and we still do \nnot know a straightforward way to do so. This problem brings us to the main technical contri\u00adbution of \nthe paper: the difficulties with runST can all -..be solved by giving it a more specific type. The type \ngiven for runST above is implicitly universally quantif\u00adied over both .s and a. If we put in the quantification \nexplicitly, the type might be written: runST :: Vs,a. (ST sa-> a) Now, what we really want to say is \nthat runST should only be applied to a state transformer which uses newvar to create any references which \nare used in that thread. To put it another way, the argument of runST should not make any assumptions \nabout what has al\u00ad ready been allocated in the initial state. That is, runST should work regardless of \nwhat a nttzal state zt is given. So the type of xunST should be: runST :: ~a. (~s. ST s a) -> a This \nis not a Hindley-Milner type, because the quan\u00adtifiers are not all at the top level; it is an example \nof rank-2 polymorphism (McCracken [1984]). Why does this type prevent the capture of ref\u00aderences from \none thread into another? Consider our example again let v = runST (newVar True) in runST (readVar v) \n In the last line a reference v is used in a stateful thread (readVar v), even though the latter is suppos\u00adedly \nencapsulated by runST. This is where the type checker comes into its own. During typechecking, the type \nof readVar v will depend on the type of v so, for example, the type derivation will contain a judgement \nof the form: {.. ., v : MutVar s Bool} l--readVar v : ST s Bool Now in order to apply runST we have \nto be able to generalise the type of readVar v with respect to s, but we cannot as s is free in the type \nenvironment: readVar v simply does not have type Vs. ST s Bool. What about the other way round? Let s \ncheck that the type of runST prevents the escape of references from a thread. Consider the definition \nof v above: v = runST (newVar True) Here, v is a reference that is allocated within the thread, but \nthen released to the outside world. Again, consider what happens during typecheck\u00ading. The expression \n(newVar True) has type ST s (MutVar s Bool ), which will generalise nicely to Vs .ST s (MutVar s Bool). \nHowever, this still does not match the type of runST. To see this, con\u00adsider the instance of runST with \na instantiated to MutVar s Bool: runST :: ( ds . ST s (MutVar s Bool)) -> MutVar s Bool We have had \nto rename the bound variable s in the type of runST to avoid it erroneously capturing the s in the type \nMutVar s Bool. The argument type now doesn t match v s type. Indeed there is no instance of runST which \ncan be applied to v. Just to demonstrate that the type of runST does al\u00adlow some nice examples here is \none that is fine: f :: MutVar s a -> MutVar s a f v = runST (newVar v thenST \\w > readVar w)  where \nv is a reference from some arbitrary state thread. 3.1 Haskell Arrays Because v is not accessed, its \nstate type does not af\u00adfect the local state type of the short thread (which is in fact totally polymorphic \nin v). Thus it is fine for an encapsulated state thread to manipulate references from other threads so \nlong as no attempt is made to dereference them. In short, by the expedient of giving runST a rank-2 polymorphic \ntype we can enforce the safe encapsulation of state transformers. More details on this are given in Section \n5.2, where we show that runST s type can be accommodated with only a minor enhancement to the type checker. \n Array references So far we have introduced the idea of references (Sec\u00adtion 2.2), which can be thought \nof as a single mutable box . Sometimes, though we want to update an array which should be thought of \nas many boxes , each in\u00addependently mutable. For that we provide primitives to allocate, read and write \nelements of arrays. They have the following typesz: newArr :: Ix i => (i, i) -> elt -> ST s (NutArr s \ni elt) readArr :: Ix i => MutArr s i elt -> i > STselt wrlteArr :: Ix i => MutArr s 1 elt -> i -> elt \n-> STS () freezeArr :: Ix i => MutArr s 1 elt -> ST s (Array i elt) Like references, newArr allocates \nanew array whose bounds are given by its first argument. The second ar\u00adgument is a value to which each \nlocation is initialised. The state transformer returns a reference to the ar\u00adray, which we call an array \nreference. The functions readArr and writ eArr do what their names suggest. The result is undefined if \nthe index is out of bounds. The interesting function is freezeArr which turns a llutArr into a standard \nHaskell array. The latter is an immutable value, which can certainly be returned from a stateful thread, \nand hence lacks the parameterisation on the state s. Operationally speaking, f reezeArr takes the name \nof an array as its argument, looks it up in the state, and returns a copy of what it finds, along with \nthe unaltered state. The copy is required in case a subsequent writeArr changes the value of the array \nin the state, but it is sometimes possible to avoid the overhead of making the copy (see Section 6.2.3). \n2The IX i => part of the type is just Haskell s way of saying that the type a must be an index type; \nthat is, there must be a mapping of a value of type a to an offset in a linear array. Integers, characters \nand tuples are automatically in the Ix class, but array indexing is not restricted to these. Any type \nfor which a mapping to Int is provided (via an instance declaration for the class Ix at that type) will \ndo. Using mutable arrays, we shall define the Haskell primitive accumArray, a high level array operation \nwith the type3: accumArray : : Ix i => (a->b->a) -> a -> (i, i) > [(i, b)l -> Array 1 a The result of \na call ( accumArray f x bnds ivs ) is an array whose size is determined by bnds, and whose values are \ndefined by separating all the values in the list ivs according to their index, and then performing a \nleft-fold operation, using f, on each collection, starting with the value x. Typical uses of accumArray \nmight be a histogram, for example: hist :: Ix i => (i, i) -> [i] -> Array i Int hist bnds is = accumArray \n(+) O bnds [(i,l)li<-is, inRange bnds iI which counts the occurrences of each element of the list is \nthat falls within the range given by the bounds bnds. Another example is bin sort: blnSort :: Ix i => \n(i, i) -> (a->i) -> [a] ->Array ia binSort bnds key vs = accuntArray (flip (: ) ) [1 bnds [(key V,V) \nI V< VSI where the value in vs are placed in bins according to their key value as defined by the function \nkey (whose results are assumed to lie in the range specified by the bounds bnds). Each bin that is, \neach element of the array will contain a list of the values with the same key value. The lists start \nempty, and new elements are added using a version of cons in which the order of arguments is reversed. \nIn both examples, the array is built by a single pass along the input list. The implementation of accumArray \nis as follows. accumArray bnds f z ivs = runST (newArr bnds z thenST \\a -> fill a f ivs thenST_ freezeArr \na) fill a f [] = returnST () fill a f ((i, v) :ivs) = readArr a i <thenST \\x -> writeArr a i (f x v) \n thenST_ fill a f ivs) @ evaluating a call to accumArray, a new state thread is generated. Within this \nthread an array is al\u00adlocated, each element of which is initialised to z. The reference to the array \nis named a. This is passed to the fill procedure, together with the accumulator func\u00adtion f, and the \nlist of index/value pairs. When this list is exhausted, fill simply returns. If there is at least one \nelement in the list, it will be a pair 3Technically the (i ,b) should be Assoc i b (1, v). The array \na is accessed at location i, the value obtained being bound to x, and a new value, namely (f x v), is \nwritten into the array, again at location i. Then fill is called recursively on the rest of the list. \nOnce fill has finished, the array is frozen into an immutable Haskell array which is returned from the \nthread. Using mutable-array operations has enabled us to describe a complex array primitive in terms \nof much simpler operations. Not only does this make the compiler-writer s job easier, but it also allows \nprogram\u00ad mers to define their own variants for, say, the cases when accumArray does not match their application \nprecisely. The example is also interesting because of its use of encapsulated state. The tmplementatton \n(or internal details) of accumArray is imperative, but its external behaznour is purely functional. Even \nthe presence of the state cannot be detected from outside the definition of accumArray, 3.1.1 Combining \nState Transformers Because state transformers are first class values, we can use the power of the functional \nlanguage to define new combining forms. One that would be useful in the example above is for sequencing \na list of procedures : seqST :: [ST S ()] -> STS() seqST = foldr thenST-(returnST ()) Using this the \nexample above can be rewritten: accumArray bnds f z ivs = runST (neWArr bnds z thenST< \\a -> seqST (map \n(update a f ) ivs) thenST-( freezeArr a) update a f (i ,v) = readArr a i thenST ( \\x-> writeArr a i \n(f x v) The local function update takes an index/value pair and evaluates to a state transformer which \nupdates the array referenced by a. Mapping this function down the list of index/value pairs ivs produces \na list of stat e transformers, and these are sequenced together by seqST.  4 Input/output Now that \nwe have the state-transformer framew\u00ad ork in place, we can give a new account of in\u00ad put /output. An \nI/O-performing computation is of type ST RealWorld a; that is, it is a state transformer transforming \na state of type RealWorld, and delivering a value of type a. The only thing which makes it special is \nthe type of the state it transforms, an abstract type whose values represent the real world. It is convenient \nto use a type synonym to express this specialisation: type IO a = ST RealWorld a since IO a is an instance \nof ST s a, it follows that all the state-transformer primitives concerning refer\u00adences and arrays work \nequally well when mixed with 1/0 operations. More than that, the same plumbing combinators, thenST, returnST \nand so on, work for 1/0 as for other state transformers. In addition, how\u00adever, we provide a variety \nof 1/0 operations that work only on the IO instance of state (that is, they are not polymorphic in the \nstate), such as: put Char :: Char -> IO () .getChar :: IO Char It is easy to build more sophisticated \n1/0 operations on top of these. For example: putString :: [Char] -> IO () put String [1 = returnST ( \n) put String (c: CS) = putChar c thenST- putString cs or, equivalent ly, putString cs = seqST (map \nputChar CS) There is no way for a caller to tell whether putstring is primitive or programmed . Indeed, \nputChar and getChar are not primitive either. There is actually only one primitive 1/0 operation, called \nccall, which allows the Haskell programmer to call any C procedure. For example, putChar is defined like \nthis: put Char :: Char -> IO () put Char c = ccall putchar c thenST \\_ -> returnST ( ) That is, the \nstate transformer (put Char c) trans\u00adforms the real world by calling the C function put char, passing \nit the character c. The value returned by the call is ignored, as indicated by the _ wild card. Sim\u00adilarly, \nget Char is implemented like this: getChar :: IO Char getChar = ccall getchar ccall is actually implemented \nas a new language construct, rather than as an ordinary function, because we want it to work regardless \nof the number and type of its arguments. The restrictions placed on its use are: All the arguments, and \nthe result, must be types which C understands: Int, Float, Double, Bool, or Array. There is no automatic \nconversion of more complex structured types, such as lists or trees. The first argument of ccall, which \nis the name of the C function to be called, must appear liter\u00adally. It is really part of the construct. \n 4,1 Running IO The IO type is a particular instance of state transform\u00aders so, in particular, 1/0 operations \nare not polymor\u00adphic in the state. An immediate consequence of this is that IO operat~ons cannot be encapsulated \nustng runST. Why not? Again, because of runsST s type. It de\u00admands that its state transformer argument \nbe univer\u00adsally quantified over the state, but that is exactly what IO is not! Fortunately, this is exactly \nwhat we want. If IO op\u00aderations could be encapsulated then it would be possi\u00adble to write apparently \npure functions, but whose be\u00adhaviour depended on external factors, the contents of a file, user input, \na shared C variable etc. The language would no longer exhibit referential transparency. However, this \ndoes leave us with a problem: how are IO operations executed? The answer is to provide a top level identifier, \nmainIO : : IO () and to define the meaning of a program in terms of it. When a program is executed, mai.nIO \nis applied to the true external world state, and the meaning of the program is given by the final world \nstate returned by the program (including, of course, all the incremental changes en route). By this means \nit is possible to give a full definition of Haskell s standard input/output behaviour (involving lists \nof requests and responses) as well as much more. Indeed, the Glasgow implementation of the Haskell 1/0 \nsystem is itself now written entirely in Haskell, using ccall to invoke Unix 1/0 primitives directly. \nThe same techniques have been &#38;ed to write libraries of routines for calling X, etc. 5 Type Rules \nHaving given the programmer s eye view, it is time now to be more formal, In this paper we simply present \nthe necessary typing judgments to achieve our goal. In the full version of the paper we present a denotational \nsemantics and an outlined proof of safety for the en\u00adcapsulation (Launchbury &#38; Peyton Jones [1994]). \nUp to now, we have presented state transformers in the context of the full-sized programming language \nHaskell, since that is where we have implemented the ideas. Here, however, it is convenient to restrict \nour\u00adselves to the essentials. 5.1 A Language We focus on lambda calculus extended with the state transformer \noperations. The syntax of the language is given by: e .._.. %Iklelezlk.el let x = el in ez I runST e \nI ccall z el .em k ::== . . . I thenST I returnST I fixST I newVar I readVar I writeVar I newArr I readArr \nI wri.teArr I freezeArr  5.2 Types Most of the type rules are the usual Hindley-Milner rules. The most \ninteresting addition is the typing judgement for runST. Treating it as a language con\u00adstruct avoids the \nneed to go beyond Hindley-Milner types. So rather than actually give runST the type runST :: Va. (b s. \nST s a) -> a as suggested in the introduction, we ensure that its typing judgment has the same effect. \nSo because it is consistent with the rank-2 type, our previous intuition still applies. As usual, we \ntalk both of types and type schemes (that is, types possibly with universal quantifiers on the outside). \nWe use T for types, S for type schemes, and K for type constants such as Int and Bool. In addition we \nuse C to range over the subset of 1< that correspond to the (C-types described in Section 4. T ::= t11{lTl+~21STTl~21 \nMutVar T1 T2 I MutArr T1 T2 S ::= T 1 Vt.S Note that the MutArr type constructor has only two arguments \nhere. The missing one is the index type. For the purposes of the semantics we shall assume that arrays \nare always indexed by naturals, starting at O. The type rules are given in Figure 1. r ranges over type \nenvironments (that is, partial functions from references to types), and we write FV(T) for the free variables \nof type T and likewise for type environments.  6 Implementation The whole point of expressing stateful \ncomputations in the framework that we have described is that opera\u00adtions which modify the state can update \nthe state zn place. The implementation is therefore crucial to the whole enterprise, rather than being \na peripheral issue. We have in mind the following implementation framework: The state of each encapsulated \nstate thread is represented by a collection of objects in heap\u00adallocated storage. APP LAM LET VAR SPEC \nGEN CCALL RUN I t-el:Tl+T2 I 1-e2:Tl I l-(el e2):T2 r,x:Tlke:T2 I 1-Ax.e:T1+T2 I l-el:S I , x: Ste2:T \nI \\(letz=elinez):T  r,x:sbx:s 17Fe: Qt.S t $2 F V(T) r E e : S[T/t] I te:S t @Fv(r) 17t-e:W.S l?t-el:C \nl .I t-en:Cn I F(ccall xel. ..en):C l?l--e:W.s rt T t(j/ W(T) r t-(runST e) : T Figure 1: Type rules \n. A reference is represented by the address of an object in heap-allocated store. . A read operation \nreturns the current contents of the object whose reference is given.  A write operation overwrites the \ncontents of the specified object or, in the case of mutable arrays, part of the contents,  . The 1/0 \nthread is a little different because its state also includes the actual state of the real world. 1/0 \noperations are carried out directly on the real world (updating it in place, as it were). As the previous \nsection outlined, the correctness of this implement ation relies totally on the type system. such a reliance \nis quite familiar: for example, the implemen\u00adtation of addition makes no attempt to check that its arguments \nare indeed integers, because the type sys\u00adtem ensures it. In the same way, the implementation of state \ntransformers makes no attempt to ensure, for example, that references are only used in the same state \nthread in which they were created; the type system en\u00adsures that this is so. 6.1 Update in place The \nmost critical correctness issue concerns the update-in-place behaviour of write operations. Why is update-in-place \nsafe? It is safe because all the com\u00adbinators (thenST, returnST, f ixST) use the state only in a single-threaded \nmanner (Schmidt [1985]); that is, they each use the incoming state exactly once, and none duplicates \nit. Furthermore, all the primitive operations on the state are strict in it. A write operation can mod\u00adify \nthe state in place, because (a) it has the only copy of the incoming state, and (b) since it is strict \nin the in\u00adcoming state, there can be no as-yet-unevaluated read operations pending on that state. Can \nthe programmer somehow duplicate the state? No: since the ST type is opaque, the only way the pro\u00adgrammer \ncan manipulate the state is via the combina\u00adtors thenST, returnST and f ixST, On the other hand, the \nprogrammer certainly does have access to named references into the state. However, it is perfectly OK \nfor these to be duplicated, stored in data structures and so on. Variables are zmmutable; it is only \nthe state to which they refer that is altered by a write operation. We find these arguments convincing, \nbut they are cert airily not formal. A formal proof would necessarily involve some operational semantics, \nand a proof that no evaluation order could change the behaviour of the program. We have not yet undertaken \nsuch a proof. 6.2 Efficiency considerations Itwould be possible to implement state transformers by providing \nthe combinators (thenST, returnST, etc) and primitive operations (readVar, writeVar etc) as library functions. \nBut this would impose a very heavy overhead on each operation and (worse still) on com\u00adposition. For \nexample, a use of thenST would entail the construction of two function-valued arguments, fol\u00adlowed by \na procedure call to thenST. This compares very poorly with simple juxtaposition of code, which is how \nsequential composition is implemented in conven\u00adtional languages! A better way would be to treat state-transformer \noperations specially in the code generator. But that risks complicating an already complex part of the \ncom\u00adpiler. Instead we implement state transformers in a way which is both direct and efficient: we simply \ngive Haskell definitions for the combinators. type ST s a = State s -> (a, State s) returnST x s = (x, \ns) thenSTmk s = k x s where (x, s ) =111 s fixST ks= (r,s ) where (r,s ) =kr s runST m = r where (r ,s) \n= m current State Rather than provide ST as a built-in type, opaque to the compiler, we give its representation \nwith an ex\u00adplicit Haskell type definition. (The representation of ST is not, of course, exposed to the \nprogrammer, lest he or she write functions which duplicate or discard the state. ) It is then easy to \ngive Haskell definitions for the combinators. The implementation of runST is intriguing. Since its argument, \nm, works regardless of what state is passed to it, we simply pass a value representing the current state \nof the heap. As we will see shortly (Section 6.2.2), this value is never actually looked at, so a constant \nvalue will do. The code generator must, of course, remain respon\u00adsible for producing the appropriate \ncode for each prim\u00aditive operation, such as readVar, ccall, and so on. In our implementation we actually \nprovide a Haskell wrapper for each primitive which makes explicit the evaluation of their arguments, \nusing so-called unboxed values . Both the motivation for and the implementa\u00adtion of our approach to unboxed \nvalues is detailed in Peyton Jones &#38; Launchbury [1991], and we do not re\u00adhearse it here. 6.2.1 Transformation \nThe beauty of this approach is that all the combina\u00adtors can then be inlined at their call sites, thus \nlargely removing the plumbing costs. For example, the ex\u00adpression ml thenST \\vl -> m2 thenST < \\v2 -> \nreturnST e becomes, after inlining thenST and returnST, \\s -> let (vi, sl) =ml s (v2, s2) = m2 SI in \n(e, s3)  Furthermore, the resulting code is now exposed to the full range of analyses and program transformations \nimplemented by the compiler, For example, if the com\u00adpiler can spot that the above code will be used \nin a con\u00adtext which is strict in either component of the result tuple, it will be transformed to \\s -> \ncase ml sof (vi, s2) -> case m2 S1 of (v2, s2) -> (e, s2)  In the let version, heap-allocated thunks \nare created for ml s and m2 s 1; the case version avoids this cost. These sorts of optimisations could \nnot be performed if the ST type and its combinators were opaque to the compiler. 6.2.2 Passing the state \naround The implementation of the ST type, given above, passes around an explicit state. Yet, we said \nearlier that state\u00admanipulating operations are implemented by perform\u00ading side effects on the common, \nglobal heap. What, then, is the role of the explicit state values which are passed around by the above \ncode? It plays two impor\u00adtant roles. Firstly, the compiler shakes the code around quite considerably: \nis it possible that it might somehow end up changing the order in which the primitive opera\u00adtions are \nperformed? No, it is not. The input state of each primitive operation is produced by the preceding operation, \nso the ordering between them is maintained by simple data dependencies of the explicit state, which are \ncertainly preserved by every correct program trans\u00adformation. Secondly, the explicit state allows us \nto express to the compiler the strictness of the primitive operations ir. e state. The State type is \ndefined like this: aaca State s = MkState (State# s) That is, a state is represented by a single-constructor \nalgebraic data type, whose only contents is a value of type State# s, the (finally!) primitive type of \nstates. The lifting implied by the MkState constructor corre\u00adsponds exactly to the lifting in the semantics. \nUsing this definition of State we can now define newVar, for example, like this: newVar init (MkState \ns#) = case newVar# init s# of (v, t#) -> (v, MkState t#)  This definition makes absolutely explicit \nthe evalua\u00adtion of the strictness of newvar in its state argument, finally calling the truly primitive \nnewVar# to perform the allocation. We think of a primitive state that is, a value of type Stat e# s, \nfor some type s as a token which stands for the state of the heap and (in the case of the 1/0 thread) \nthe real world. The implementation never actually inspects a primitive state value, but it is faithfully \npassed to, and returned from every primitive state-transformer operation, By the time the program reaches \nthe code generator, the role of these state values is over, and the code generator arranges to generate \nno code at all to move around values of type State# (assuming an underlying RAM architecture of course). \n 6.2.3 Arrays The implementation of arrays is straightforward. The only complication lies with freezeArray, \nwhich takes a mutable array and returns a frozen, immutable copy. Often, though, we want to construct \nan array incremen\u00adtally, and then freeze it, performing no further muta\u00adtion on the mutable array. In \nthis case it seems rather a waste to copy the entire array, only to discard the mutable version immediately \nthereafter. The right solution is to do a good enough job in the compiler to spot this special case. \nWhat we ac\u00adtually do at the moment is to provide a highly dan\u00adgerous operation dangerousFreezeArray, \nwhose type is the same as freezeArray, but which works with\u00adout copying the mutable array. Frankly this \nis a hack, but since we only expect to use it in one or two crit\u00adical pieces of the standard library, \nwe couldn t work up enough steam to do the job properly just to handle these few occasions. We do not \nprovide general access to dangerousFreezeArray, 6.2.4 More efficient 1/0 The 1/0 state transformer is \na little special, because of the following observation: the final state of the 1/0 thread wdl certainly \nbe demanded. Why? Because the whole point in running the program in the first place is to cause some \nside effect on the real world! We can exploit this property to gain a little extra efficiency. Since \nthe final state of the 1/0 thread will be demanded, so will every intermediate thread. So we can safely \nuse a strict, and hence more efficient, version of thenST: thenIO :: IO a -> (a->10 b) -> IO b thenIO \nmk s=case nsof (r, s ) ->krs  By using case instead of the let which appears in thenST, we avoid the \nconstruction of a heap-allocated thunk for m s.  7 Other useful combinators We have found it useful \nto expand the range of combi\u00ad nators and primitives beyond the minimal set presented so far, This section \npresents the ones we have found most useful. 7.1 Equality The references we have correspond very closely \nto pointers to variables . One useful additional opera\u00adtion on references is to determine whether two \nrefer\u00adences are aliases for the same variable (so writes to the one will affect reads from the other). \nIt turns out to be quite straightforward to add an additional constant, eqMutVar :: MutVar s a -> MutVar \ns a -> Bool eqMutArr :: Ix i => MutArr s i a-> MutArr s i a-> Bool Notice that the result does not depend \non the state it is simply a boolean. Notice also that we only pro\u00advide a test on references which exist \nin the same state thread. References from different state threads cannot be aliases for one another. \n 7.2 Interleaved and parallel operations The state-transformer composition combinator defined so far, \nthenST, is strictly sequential: the state is passed from the first state transformer on to the second. \nBut sometimes that is not what is wanted. Consider, for example, the operation of reading a file. We \nmay not want to specify the precise relative ordering of the in\u00addividual ;haracter-by-character reads \nfrom the file and other 1/0 operations. Rather, we may want the file to be read lazily, as its contents \nis demanded. We can provide this ability with a new combinator, int erleaveST: interleaves :: STsa->STsa \n Unlike every other state transformer so far, lnterleaveST actually duplicates the state! The plumbing \ndiagram for (interleaves s ) is like this: Result II sL-J State in ~ State out More precisely, lnterleaveST \nsplits the state into two parts, which should be disjoint. In the lazy-file-read example, the state of \nthe file is passed into one branch, and the rest of the state of the world is passed into the other. \nSince these states are disjoint, an arbitrary interleaving of operations in each branch of the fork is \nlegitimate. To make all this concrete, here is an implementation of lazy file read: readFile : : String \n-> IO [Char] readFile filename = openFile filename thenST \\f -> readCts f readCts : : FileDescriptor \n-> ID [char] readCts f = interleaves (readChf thenST \\c -> if c == eofChar then returnST [1 else readCts \nf thenST \\cs -> returnST (c:cs)) A parallel version of interleaveST, which startsup a concurrent task \nto perform the forked 1/0 thread, seems as though it would be useful in building respon\u00adsive graphical \nuser interfaces. The idea is that forkIO wouldbeused tocreate anew widget, orwindow, which wouldbe capable \nof independent 1/0 through its part of the screen. The only unsatisfactory feature of all this is that \nwe see absolutely no way to guarantee that the side effects performed in the two branches of the fork \nare indeed independent. That has to be left as a proof obliga\u00adtionfor the programmer; the only consolation \nis that at least the location of these proof obligations is ex\u00adplicit. We fear that there may be no absolutely \nsecure system which is also expressive enough to describe the programs which real programmers want to \nwrite.  8 Related work Several other languages from the functional stable pro\u00advide some kind of state. \nForexample, Standard ML provides reference types, which maybe updated (Paulson [1991]). The resulting \nsystem hasserious shortcomings, though. The meaning of programs which use references depends on a com\u00adplete \nspecification of the order of evaluation of the pro\u00adgram. Since SML is strict this is an acceptable price \nto pay, but it would become unworkable in a non-strict language where the exact order of evaluation is \nhard to figure out, What is worse, however, is that referen\u00adtial transparency is lost. Because an arbitrary \nfunction may rely on state accesses, its result need not depend purely on the values of its arguments. \nThis has ad\u00additional implications for polymorphism, leading to a weakened form in order to maintain type \nsafety (Tofte [1990]). We have none of these problems here, The dataflow language Id provides I-structures \nand M-structures as mutable datatypes (Nikhil [1988]). Within a stateful program referential transparency \nis lost . For I-structures, the result is independent of evaluation order, provided that all sub-expressions \nare eventually evaluated (in case they side-effect an I\u00adstructure). For M-structures, the result of a \nprogram can depend on evaluation order. Compared with I\u00adstructures and M-structures, our approach permits \nlazy evaluation (where values are evaluated on demand, and may never be evaluated if they are not required), \nand supports a much stronger notion of encapsulation. The big advantage of I-structures and M-structures \nis that they are better suited to parallel programming than is our method. The Clean language takes a \ndifferent approach (Barendsen &#38; Smetsers [1993]). The Clean type system supports a form of linear \ntypes, called unique types . A value whose type is unique can safely be updated in place, because the \ntype system ensures that the up\u00addating operation has the sole reference to the value, The contrast with \nour work is interesting. We separate references from the state to which they refer, and do not permit \nexplicit manipulation of the state. Clean identifies the two, and in consequence requires state to be \nmanipulated explicitly. We allow references to be duplicated, stored in data structures and so on, while \nClean does not. Clean requires a new type system to be explained to the programmer, while our system \ndoes not. On the other hand, the separation between refer\u00adences and state is sometimes tiresome. For \nexample, while both systems can express the idea of a mutable list, Clean does so more neatly because \nthere is less explicit de-referencing, The tradeoff between implicit and explicit state in purely-functional \nlanguages is far from clear. There are significant similarities with Gifford and Lucassen s effect system \nwhich uses types to record side effects performed by a program (Gifford &#38; Lucassen [1986]), However, \nthe effects system is designed to de\u00adlimit the effect of side effects which may occur as a result of \nevaluation. Thus the semantic setting is still one which relies on a predictable order of evaluation. \nOur work also has strong similarities with Odersky, Rabin and Hudak s Aoar (Odersky, Rabin &#38; Hudak \n[1993]), which itself was influenced by the Imperative Lambda Calculus (ILC) of Swarup, Reddy &#38; Ireland \n[1991]. ILC imposed a rigid stratification of applica\u00adtive, state reading, and imperative operations. \nThe type of runST makes this stratification unnecessary: state operations can be encapsulated and appear \npurely functional. This was also true of A.ar but there it was achieved only through run-time checking \nwhich, as a direct consequence, precludes the style of lazy state given here. In two earlier papers, \nwe describe an approach to these issues based on monads, in the context of non\u00adstrict, purely-functional \nlanguages. The first, Pey\u00adton Jones &#38; Wadler [1993], focusses mainly on in\u00adput/output, while the \nsecond, Launchbury [1993], deals with stateful computation within a program. The ap\u00adpreach taken by these \npapers has two major shortcom\u00adings: State and input/output existed in separate frame\u00ad works. The same \ngeneral approach can handle both but, for example, different combinators were required to compose stateful \ncomputations from those required for I/O-performing computation. State could only safely be handled if \nit was anony\u00admous. Consequently, it was difficult to write pro\u00adgrams which manipulate more than one piece \nof state at once. Hence, programs became rather brittle : an apparently innocuous change (adding an extra \nupdatable array) became difficult or im\u00ad possible. Separate state threads required expensive run\u00ad tirne \nchecks to keep them apart. Without this, there was the possibility that a reference might be created \nin one stateful thread, and used asyn\u00ad chronously in another, which would destroy the Church- Rosser \nproperty.  Acknowledgements The idea of adding an extra type variable to state threads arose in discussion \nwith John Hughes, and was presented briefly at the 1993 Copenhagen workshop on State in Programming Languages, \nthough at that time we suggested using an existential quantification in the type of runST. In addition, \nall these ideas have ben\u00ad efited from discussions amongst the Functional Pro\u00ad gramming Group at Glasgow. \n References E Barendsen &#38; JEW Smetsers [Dee 1993], Conven\u00adtional and uniqueness typing in graph rewrite \nsystemsl in Proc 13th Conference on the Foundations of Software Technology and The\u00adoretical Computer \nScience, Springer Velrlag LNCS. DK Gifford &#38; JM Lucassen [Aug 1986], Integrating functional and imperative \nprogramming, in ACM Conference on Lisp and Functional Pro\u00ad gramming, MIT, ACM, 28-38. J Launchbury \n[June 1993], Lazy imperative pro\u00adgramming, in Proc ACM Sigplan Work\u00adshop on State in Programming Languages, \nCopenhagen (available as YALEU/DCS/RR\u00ad968, Yale University), PP46-56. J Launchbury &#38; SL Peyton Jones \n[Feb 1994], Lazy functional state threads, Technical report FP\u00ad94-05, Department of Computing Science, \nUni\u00adversit y of Glasgow (FTP:ftp. dcs .glasgow. ac .uk: pub[glasgow-f p/tech-reports/ FP-94-05: state. \nps. Z). NJ McCracken [June 1984], The typechecking of pro\u00adgrams with implicit type structure, in Seman\u00adtics \nof data types, Springer Verlag LNCS 173, 301-315. JC Mitchell &#38; AR Meyer [1985], (Second-order logical \nrelations, in Logics of Programs, R Parikh, cd., Springer Verlag LNCS 193. Rishiyur Nikhil [March 1988], \nId Reference Manual, Lab for Computer Sci, MIT. M Odersky, D Rabin &#38; P Hudak [Jan 1993], Call by \nname, assignment, and the lambda calculus, in 20th ACM Symposium on Principles of Pro\u00adgramming Languages, \nCharleston, ACM, 43\u00ad 56. LC Paulson [1991], ML for the working programmer, Cambridge University Press. \nSL Peyton Jones &#38; J Launchbury [Sept 1991], Un\u00adboxed values as first class citizens, in Func\u00adtional \nProgramming Languages and Computer Architecture, Boston, Hughes, ed., LNCS 523, Springer Verlag, 636-666. \nSL Peyton Jones &#38; PL Wadler [Jan 1993], Imperative functional programming, in 20th ACM Sym\u00adposium \non Principles of Programming Lan\u00adguages, Charleston, ACM, 71-84. CG Ponder, PC McGeer &#38; A P-C Ng \n[June 1988], Are applicative languages inefficient ? , SIGPLAiV Notices 23, 135-139. DA Schmidt [Apr \n1985], Detecting global variables in denotational specifications, TOPLAS 7, 299 310. V Swarup, US Reddy \n&#38; E Ireland [Sept 1991], As\u00adsignments for applicative languages, in Func\u00adtional Programming Languages \nand Computer Architecture, Boston, Hughes, ed., LNCS 523, Springer Verlag, 192-214. M Tofte [Nov 1990], \nType inference for polymorphic references, Information and Computation 89.  \n\t\t\t", "proc_id": "178243", "abstract": "<p>Some algorithms make critical internal use of updatable state, even though their external specification is purely functional. Based on earlier work on monads, we present a way of securely encapsulating stateful computations that manipulate multiple, named, mutable objects, in the context of a non-strict, purely-functional language.</p><p>The security of the encapsulation is assured by the type system, using parametricity. Intriguingly, this parametricity requires the provision of a (single) constant with a rank-2 polymorphic type.</p>", "authors": [{"name": "John Launchbury", "author_profile_id": "81100462557", "affiliation": "University of Glasgow", "person_id": "PP39043890", "email_address": "", "orcid_id": ""}, {"name": "Simon L. Peyton Jones", "author_profile_id": "81100271851", "affiliation": "University of Glasgow", "person_id": "PP14102537", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/178243.178246", "year": "1994", "article_id": "178246", "conference": "PLDI", "title": "Lazy functional state threads", "url": "http://dl.acm.org/citation.cfm?id=178246"}