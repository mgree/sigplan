{"article_publication_date": "06-01-1994", "fulltext": "\n Effective Partial Redundancy Elimination Preston Briggs Keith D. Cooper Department of Computer Science \nRice University Houston, Texas 77251-1892 Abstract Partial redundancy elimination is a code optimization \nwith a long history of literature and implementation. In practice, its effectiveness depends on issues \nof naming and code shape. This paper shows that a combination of global reassociation and global value \nnumbering can increase the effectiveness of partial redundancy elimina\u00adtion. By imposing a discipline \non the choice of names and the shape of expressions, we are able to expose more redundancies, As part \nof the work, we introduce a new algorithm for global reassociation of expressions. It uses global in\u00adformation \nto reorder expressions, creating opportunities for other optimization. The new algorithm generalizes \nearlier work that ordered FORTRAN array address ex\u00ad pressions to improve optimization ~25]. 1 Introduction \nPartial redundancy elimination is a powerful optimiza\u00adtion that has been discussed in the literature \nfor many years (e.g., [21, 8, 14, 12, 18]). Unfortunately, partial redundancy elimination has two serious \nlimitations. It can only recognize lexically-iclentical expressions; this makes effectiveness a function \nof the choice of names in the front end. It cannot rearrange sub expressions; this makes effectiveness \na function of the shape of the code generated by the front end. The net result is that de\u00adcisions made \nin the design of the front end dictate the effectiveness of partial redundancy elimination. This paper \nshows how an optimizer can use g~obal reassociation (see Section 3.1 ) and a form of partition\u00adbased \nglobal value numbering ~2] to improve the effec\u00adtiveness of partial redundancy elimination. We consider \nthese to be enabling transformations. They do not im- This work has been supported by ARPA through ONR \ngrant NOOO14-91-.J-1989. Permission to co without fee all or part of this material is granted provide${ \nt at the copies are not made or distributed for direct commercial acfvanta e, the ACM copyiight notice \nand the titleofthepublkation and!tsdate appear, andnotice is given that copying is by permission of the \nAssociation of Computing Machinery. To copy otherwfse, or to republish, requires a fee and/or specific \npermission, SIGPLAN 84-6/84 Orlando, Florida USA @ 1994 ACM O-89791-662-X184JOO06..$3.5O prove the code \ndirectly; instead, they rearrange the code to make other transformations more effective. The com\u00adbination \nof these transformations with partial redun\u00addancy elimination results in removing more redundant expressions, \nhoisting more loop-invariant expressions (and sometimes hoisting them farther), and removing some partially-dead \nexpressions. By using global reas\u00adsociation and partition-based global value numbering to generate the \ncode shape and name space automatically, the optimizer can isolate partial redundancy elimination from \nthe vagaries of the front end. This lets the opti\u00admizer obtain good results on code generated by sources \nother than a careful front end for example, on code re\u00adsulting from other optimization passes or from \nrestruct\u00aduring front ends. The primary contributions of this paper are: (1) the use of reassociation \nto achieve a canonical code shape for expressions, (2) the use of partition-based global value numbering \nto achieve a canonical naming, and (3) a new technique for global reassociation of expressions. Addi\u00adtionally, \nwe present experimental evidence that demon\u00adstrates the effectiveness of partial redundancy elimina\u00adtion, \nwith and without our transformations. 2 Partial Redundancy Elimination Partial redundancy elimination \n(PRE) is a global op\u00adtimization introduced by Morel and Renvoise ~21]. It combines and extends two other \ntechniques. common su bexpression elimination An expression is re\u00addundant at some point p if and only \nif it is comp\u00aduted along every path leading to p and none of its constituent subexpressions has been \nredefined. If e is redundant at p, the evaluation of e at p can be replaced with a reference. loop-invariant \ncode motion An expression is loop in\u00advariant if it is computed inside a loop and its value is identical \nin all the iterations of the loop. If e is invariant in a loop, it can be computed before the loop and \nreferenced, rather than evaluated, inside the loop. PRE combines and extends these two techniques. ,S \n0urce code Low-1evel, three-addres: 1i Code Tree A Xyz Figure 1: Alternate An expression is partially \nredundant at point p if it is redundant along some, but not all, paths that reach p. PRE converts partially-redundant \nexpressions into redundant expressions. The basic idea is simple. First, it uses data-flow analysis to \ndiscover where expressions are partially redundant. Next, it solves a data-flow problem that shows where \ninserting copies of a com\u00adputation would convert a partial redundancy into a full redundancy. Finally, \nit inserts the appropriate code and deletes the redundant copy of the expression. A key feature of PRE \nis that it never lengthens an execution path. To see this more clearly, consider the example below. In \nthe fragment on the left, the second computation of ~ + y is partially redundant; it is only available \nalong one path from the if. Inserting an eval\u00aduation of C+ y on the other path makes the computation \nredundant and allows it to be eliminated, as shown in the right-hand fragment. Note that the left path \nstays the same length while the right path has been shortened. if p branch ~ if p branch ~ +1 x+-... \nx+-... +Z+y +- Y+-...  :;+y +Z+y G +.+-y M partially redundant redundant Loop-invariant expressions \nare also partially redundant, as shown in the example below. On the left, z + y is partially redundant \nsince it is available from one pre\u00addecessor (along the back edge of the loop), but not the other. Inserting \nan evaluation of x + y before the loop allows it to be eliminated from the loop body. x+\u00adx+ tz+y ++ \n+Z+y w ... 33 if p branch if p branch partiaJly redundant redundant Code Shapes 2.1 Code Shape The optimizer \nin our compiler uses a low-level interme\u00addiate language. Most operations have three addresses: two source \noperands and a target. Translating a source expression to three-address code can introduce artificial \nordering constraints. Figure 1 shows the different pos\u00adsibilities for the source expression z + y + z. \nConsider the case where rz = 3, rz = 2, and TV is a variable. Only the middle shape will allow constant \npropagation to transform the expression into y +5. Al\u00adternatively, if rv and r, are both loop invariant, \nonly the rightmost shape will allow PRE to hoist the loop\u00adinvariant sub expression. This case is quite \nimportant, since it arises routinely in multi-dimensional array ad\u00addressing computations. The choice \nof expression ordering occurs with associa\u00adtive operations such as add, multiply, and, or, rein, and \nmax. In general, there are a combinatorial number of or\u00adderings for an associative expression having \n71 operands. Source language specifications sometimes restrict possi\u00adble reordering, especially in the \ncase of floating-point, arithmetic where numerical precision may be affected. The large number of possible \norderings makes an ex\u00adhaustive search for optimal solutions impractical.  2.2 Naming Another important \nissue is the selection of names. Our implementation of PRE distinguishes between variable names and expression \nnames. This distinction was in\u00adtroduced by Morel and Renvoise [21, page 97]. A vari\u00adaMe name is the target \nof a copy instruction; conceptw ally, these correspond to source-level assignments. An expression name \nis the target of a computation in practice, an instruction other than a branch or copy. This gives every \nexpression (and suhexpression) a name. Thus, the statement i = i + 1 might be represented as: ?-1 -1 \nTz-T, +TI T,~ T2 where ri is the name of the variable i, r-l is the name of the expression 1 , and \nr2 is the name of expression %i + rl . Within a single routine, lexically-icientical expressions always \nreceive the same name. Therefore, whenever we see the expression rt + rl, we would expect to see it named \nrz. This naming discipline can be implemented in the compiler s front end by maintaining a hash table \nof ex\u00ad pressions and creating a new name whenever a new ex\u00ad pression is discovered [3]. Unfortunately, \nrelying on the front end limits the applicability of PRE. It is difficult to maintain the naming rules \nacross other optimization; thus, PRE must be run first and only once. Further\u00ad more, the ability of PRE \nto recognize identities is limited by the programmer s choice of variable names. Consider the following \nsource sequence and its corresponding in\u00ad termediate representation: rl-Tr/+rz X=y+z TZ -TI a=y ra+-Tv \nb=a+z 7_2GTa+Tz T~-T2 Obviously, T1 and r2 receive the same value (that is, the expression named by \nrz is redundant). PRE can\u00adnot discover this fact even though value numbering can eliminate this redundancy \n[1 O]. Of course, this is a sim\u00adple example, but its very simplicity should suggest the large number \nof opportunities missed by PRE when con\u00adsidering an entire routine. 3 Effective PRE To address the limitations \nof PRE, we propose a set of techniques that reorder and rename expressions. Global reassociation uses \ninformation about global code shape to rearrange individual expressions. Global value num\u00adbering uses \nknowledge about run-time equivalence of values to rename expressions. In combination, they transform \nthe program in a way that exposes more op\u00adportunities to PRE. 3.1 Global Reassociation To address the \ncode shape problems, we use a technique called global reassociation. It uses algebraic properties of \narithmetic to rearrange the code. In broad terms, it uses commutativity, assoc.nativity, and distributivity \nto expose common subexpressions and loop-invariant ex\u00adpressions. The effects can be substantial; Cocke \nand Markstein note that as much as 509%0of the code in some inner loops can be eliminated as a result \nof reassocia\u00adtion [9, page 225]. Our approach has three steps: 1.Compute a rank for every expression. \n2. Propagate expressions forward to their uses. FUNCTION foo(y, Z) ~=o X=y+z DO1 =X, 100 S=l+s+x ENDDO \nRETURN S END foo Figure 2: Source Code The next three sections discuss these s~eps and intro\u00adduce several \nimportant refinements. To help clarify the process, we provide a running example. Figure 2 shows the \nsource code and Figure 3 shows a transla\u00adtion into a simple intermediate form. This translation does \nnot conform to the naming discipline discussed in Section 2.2. Computing Ranks To guide reassociation, \nthe opti\u00ad mizer assigns a rank to each expression and subexpres\u00adsion. Intuitively, we want loop-invariant \nexpressions to have lower ranks than loop-variant expressions, In a deeply nested loop, we would like \nthe rank of an ex\u00adpression that is invariant in the inner two loops to be lower than the rank of an expression \nthat is invariant only in the innermost loop. In practice, we compute ranks on the SSA form of the routine \nduring a reverse\u00adpostorder traversal of the control-flow graph; therefore, our first step is to build \nthe pruned SSA form of the rou\u00adtine [11, 7]. During the renaming step [11, Figure 12], we remove all \ncopies, effectively folding them into #-nodes. This approach simplifies the intermediate code by re\u00admoving \nour dependence on the programmer s choice of variable names (recall Section 2.2). Given the SSA form, \nwe traverse the control-flow graph in reverse postorder, assigning ranks. Each block is given a rank \nas it is visited, where the first block vis\u00adited is given rank 1, the second block is given rank 2, enter(rY, \nTZ) Ts+o TX+ TY+TZ T,+ r= if T, > 100 branch ~ I ~ ret.rn T. ,.3Reassociate expressions, sorting their \noperands by Figure 3: Intermediate Form ranks. I 1 ,, + cj(,,,.,) return r9 Figure4: Pruned SSA Form \nand so forth. Each expression in a block is ranked using three rules:  1.A constant receives rank zero. \n2. The result of a qLnode receives the rank of the block, as do any variables modified by procedure calls. \nThis includes the result of a load instruction. 3. An expression receives a rank equal to its highest\u00adranked \noperand. Since the code is in SSA form, each operand will have one definition point and will have been \nranked before it is referenced.  Figure 4 shows the result of rewriting into pruned SSA form (minimal \nSSA would have required many more @ nodes). Notice that the copy r-t + Tc has been folded into the first \n@-node. The rank of r2 is O, the rank of ro, rl, and r-3 is 1, the rank of rq,rs,. . . . ?% is 2, and \nthe rank of rg is 3. These ranks have the intuitive proper\u00adties described above loop-invariant expressions \nare of lower rank than loop-variant expressions and the rank of a loop-variant expression corresponds \nto the nesting depth of the loop in which it changes. Forward Propagation After ranks have been com\u00adputed, \nwe copy expressions forward to their uses. For\u00adward propagation is important for several reasons. It \nbuilds large expressions from small expressions, allow\u00ading more scope for reassociation. Additionally, \nwithout forward propagation into loops, the compiler would have to cycle between reassociation and PRE \nto ensure best results with deeply-nested loops. Finally, forward prop\u00adagation avoids a subtle problem \nin PRE that arises from the distinction between variable names and expression names (see Section 5. 1). \nAs a matter of correctness, the last reason seems to require forward propagation. We propagate each expression \nand subexpression as far forward as possible, effectively building expression trees for ~-node inputs, \nvalues used to control program flow, parameters passed to other routines, and values returned from the \ncurrent routine. In practice, we first remove each d-node z -~(g, z) by inserting the copies ~ -y and \nz + z at the end of the appropriate pre\u00ad decessor blocks, then trace from each copy back along the SSA \ngraph to construct the expression trees. (If necessary, the entering edges are split and appropriate \npredecessor blocks are created.) Continuing our example, Figure 5 shows how q-nodes are eliminated by \ninserting copies. New blocks were required to hold the copies. Figure 6 shows the effect of forward propagation. \nIt is interesting to note that forward propagation eliminates partially-clead expressions [ 15, 19]. \nAn ex\u00adpression is live at its definition point if its result is used on some path to an exit. Alternatively, \nan expression is dead if its result will never be used on any path. l?Jy copying expressions to their \nuse points, forward propa\u00adgation trivially ensures that every expression is used on every path to an \nexit. Subsequent application of PRE will preserve this invariant, since PRE will never place an expression \non a path where it is partially dead. On the other hand, forward propagation is not really an optimization. \nSince it duplicates code, it can expand the size of the routine (see Section 4.3). Furthermore, it can \nmove code into loops, substantially increasing path lengths. However, recall that our plan is to transform \nthe code so that later application of PRE will achieve greater optimization. We expect that PRE will \nbe able to reverse the negative effects of forward propagation and achieve significantly improved code \nas a result of the opportunities afforded by forward propagation. enter(ro, r_I ) T2+0 r~t?-o+? ~ if \nrs > 100 branch --\u00ad + TT+T~+Ts T~+T~+] if r8 < 100 branch J 1 Tg+ TT Tg i--TZ ~  ret,lrnrg Figure 5: \nAfter Inserting Copies enter(ro, rI) T3+? ()+T1 f 3 >100 branch ~ II ~T,+,,+] -Jif rs < 100 branch \n+ Figure6: After Forward Propagation Sorting Expressions Given ranks and expression trees, we are almost \nready to reassociate expressions. First, though, we rewrite certain operations to expose more opportunities \nfor reassociation. As suggested by Frailey [17], we rewrite expressions of the form x y+ .z as z + ( \ny) + z, since addition is associative and sub\u00adtraction is not. We also perform similar transformations \nfor Boolean operations. On the other hand, we avoid rewriting x/y as x x I/y to avoid introducing precision \nproblems. We rely on a later pass, a form of global peep\u00adhole optimization, to reconstruct the original \noperations . when profitable. To reassociate, we traverse each expression, sorting the operands of each \nassociative operation by rank so that the low-ranked operands are placed together. This allows PRE to \nhoist the maximum number of subex\u00adpressions the maximum dist ante. Furthermore, since constants are given \nrank O, all the constant operands in a sum will be sorted together. For example, the expres\u00adsion 1 + \nr~ + 2 becomes 1 + 2 + rc. Constant propagation cannot improve the original form; it can easily turn \nthe reordered expression into 3 + T-m. Figure 7 shows the result of reassociation. Notice that the low-ranked \nexpressions, 1, r-o, and rl, have been sorted to the beginning of the sums. After sorting expressions, \nwe look for opportunities to distribute multiplication over addition; that is, we rewrite expressions \nof the form w x (x + y + z) as w x x+ w x y+ w x z. This distribution is not al\u00adways profitable, so we \nagain use ranks as a guide. In our current implementation, we distribute a low-ranked multiplier over \na higher-ranked sum. For example, if we have an expression a + b x ((c+ d) + e)), where a, b, c, and \nd have rank 1 and e has rank 2, we would distribute partially, giving a+b x (c+d)+b x e. This allows \nPRE to hoist a+ b x (c+ d) even if b x e cannot be hoisted. Note that a complete distribution would result \nin extra multi\u00adplications without allowing any additional code motion. It is important to re-sort sums \nafter distribution. 3.2 Global Renaming To address the naming problems, we use a global re\u00adnaming scheme \nbased on Alpern, Wegman, and Zadeck s algorithm for determining when two variables have the same value \n[2]. We refer to their technique as partition\u00adbased global value numbering . Instead of building up complex \nequality relationships from simpler ones, as in traditional value numbering, their technique works from \nthe optimistic assumption that all variables are equiv\u00adalent and uses the individual statements in the \ncode to disprove equivalences. We use a straightforward version of their algorithm to discover when two \nnames have the same value and then rename all values to reflect these equivalences. Re\u00adnaming encodes \nthe value equivalences into the name space; this exposes new opportunities to PRE. It also constructs \nthe name space required by PRE (recall Sec\u00adtion 2.2). Each lexically-identical expression will have the \nsame name; copies inserted during reassociation will only target variable names. Of course, the variables \nnamed at this point do not necessarily correspond to source variables; instead, they correspond to the \n@ nodes introduced during conversion to SSA form, The names are the only things changed during this phase; \nno instructions are added, deleted, or moved. enter(ro, ) rI rs+ro+r~ if T3 > 100 branch \\ LT8!T4+I if \nT8 < 100 branch J Tc +--To+] T~i--Tc +7 ] l 7+ T~+T5 rz+o T~+ r~ T9 +- T2 ~ re~/rnT9 Figure 7: After \nReassociation  I ~~,+r,+l I if rg <100 branch J I T r~+r7+r5 7-2+0 T]o - T8 TIO+ rz Figure8: After \nValue Numbering Figure 8 shows a naming that might be discovered by global value numbering. In this \ncase, none of the ex\u00adposed redundancies are particularly surprising, since we created them during forward \npropagation. However, it is important to note that the code now conforms with the naming requirements \nstated in Section 2.2. Expres\u00adsions are named uniquely by r., rl ~P2, r3, ? G,T7, rs, and rg. The remaining \nnames, r~, r5, and rl o, are defined exclusively by copies and serve as variable names. Finishing the \nExample Applying partial redun\u00addancy elimination to the code in Figure 8 produces the code in Figure \n9. The invariant expressions ? Gand r7 have been hoisted from the loop and the redundant com\u00adputations \nof r3, rfj, and ? T have been removed. Finally, the coalescing phase of a Chaitin-style global register \nal\u00adlocator will remove unnecessary copy instructions [6]. In this example, coalescing is able to remove \nall the copies (as shown in Figure 10), though this will not always be possible. Taken together, the \nsequence of transformations re\u00adduced the length of the loop by 1 operation without increasing the length \nof any path through the routine. However, it is worth noting that the final code is not optimal. If the \nexpressions rfj and r7 had been arranged differently, we would have been able to take advantage of the \nfact that r. + rl had already been computed. As noted in Section 2.2, finding the optimal solution would \nrequire examination of a combinatorial number of cases. We use a fast heuristic that produces good, though \nnot optimal, results. enter(ro, 9_I ) T~tTo+T] if r >100 branch + T2+0 r~+ r~ r~ * T2 r8+rj. +r5 r~4--.T0+l \nT4 -T9 r~+r6+rl T5 t T8 7T  L-,,:,,+l -Jif rg < 100 branch + r8+-r7+r5 r2+0 TIo+ ra TIot TZ Figure \n9: After Partial Redundancy Elimination enter(ro, rl ) r~tr~+r] f >100 branch ~ I I r~+r6+rl r~+r~+T.5 \n L-T4!r4+l7 if T4 < 100 branch 1 TIO + T?+ T5 rlo +-O Figure 10: After Coalescing 4 Experimental Study \nTo test the effectiveness of our techniques, we have im\u00adplemented versions of global reassociation, global \nvalue numbering, and partial redundancy elimination in the context of an experimental FORTRAN compiler. \nThe compiler is structured as a front end that consumes FORTRAN and produces lLOC (our intermediate lan\u00adguage), \nan optimizer that consumes and produces lLOC, and a back end that consumes ILOC and produces C. The generated \nC code is instrumented to accumulate dynamic counts of ILOC operations. Thus, we are able to compile \nindividual FORTRAN routines, perhaps se\u00adlected from a large program, and test the effectiveness of different \noptimizations on the routine in the context of its complete program. The optimizer is structured as a \nsequence of passes, where each pass is a Unix filter that consumes and pro\u00adduces ILOC. Each pass performs \na single optimiza\u00adtion, including all the required control-flow and data\u00adflow analyses. While this approach \nis not suitable for production compilers, its flexibility makes it ideal for experimentation. Our implementation \nof PRE uses a variation described by Drechsler and Stadel [14]. Their formulation sup\u00adports edge placement \nfor enhanced optimization and simplifies the data-flow equations that must be solved, avoiding the bidirectional \nequations typical of some other approaches [13]. Our implementation of global value numbering uses the \nsimplest variation described by Alpern, Wegman, and Zadeck, possibly missing some opportunities discovered \nby their more powerful ap\u00adproaches [2, Sections 3 and 4]. 4.1 Results We ran several versions of the \noptimizer on a suite of test routines. Each version adds new passes to the pre\u00advious one. Our test suite \nconsists of 50 routines, drawn from the Spec benchmark suite and from Forsythe, Mal\u00adcolm, and Moler s \nbook on numerical methods [16]. The results are given in Table 1. We report results for four different \nlevels of optimization: baseline This column provides the dynamic operation count, including branches, \nfor each routine when optimized using a sequence of global constant prop\u00adagation [26], global peephole \noptimization, global dead code elimination [11, Section 7.1], coalescing, and a final pass to eliminate \nempty basic blocks. 1 partial The left column gives the operation counts for routines optimized with \nPRE, followed by the se\u00adquence of optimizations used to establish the base\u00adline. The right column gives \nthe percentage im\u00adprovement over the baseline. 1The sizes of the test cases for mat rix300 and tomcat \nv have been reduced to ease testing. reassociation The left column provides the operation counts for \nroutines optimized using global reasso\u00adciation ( witfiout distribution of multiplication over addition) \nand global value numbering before PRE and the other optimizations. The right column gives the percentage \nimprovements over partial. distribution The left column gives the operation counts for routines optimized \nusing global reassociation (including distribution of multiplication over addi\u00adtion) and global value \nnumbering before PRE and the other optimizations. The right column gives the percentage improvements \nover reassociation. The total column gives the percentage improvements achieved over the baseline by \nthe entire set of optimiza\u00adtion, while the new column gives the improvement over partial contributed \nby the combination of reassociation and distribution with global value numbering. Empty entries indicate \nno improvement, whereas en\u00adtries of O?ZOand 070 indicate very small improvements and degradations. Limitations \nof the Optimizer Our optimizer is not complete. In particular, we are currently missing passes for strength \nreduction and hash-based value numbering, Nevertheless, we believe our results are still valid indi\u00adcations \nof the importance of reassociation. Indeed, it may be that our results understate the eventual ben\u00adefits \n-strength reduction should reduce non-essential overhead and hash-based value numbering should also benefit \nfrom reassociation. 4.2 Code Degradation The results in Table 1 reveal several cases where our improvements \nslowed down the code. Since we are us\u00ading heuristic approaches to difficult problems, we should not be \nsurprised by occasional losses, annoying as they are. Examination of the code revealed three sources \nof difficulty; each is discussed in the sections below. Reassociation Sometimes reassociation can disguise \ncommon sub expressions. Recall our example from Fig\u00adures 2 though 10. The final arrangement of the code, \nr~+r~+r~ and ~fj-ro+l r~t~G+?_]  hid the fact that r. + rl was being recomputed. We found that this \nsort of problem occurred quite often in the routines of our test suite. Fortunately, the effect is usually \ndominated by the improved motion of loop invariants. ro u tine baseline partial reassociation distribution \nnew total fmin 4,817 3,807 20% 1,908 49% 1,908 49% 60% gamgen 462,285 180,260 61yo 143,065 20% 107,031 \n25~o 40% 76% fmtset 705 538 23% 460 14% 397 13~o 26% 43% rkf 45 62 62 58 6~0 46 20% 25% 25% sgemv 1,496 \n1,341 10% 1,241 7% 1,003 19% 25% 32% saxp y 867 667 23% 667 525 21?70 21% 39% iniset 75,289 56,912 24% \n56,766 o% 47,426 16yo 16% 37% spline 1,659 961 42% 885 7% 802 9% 16% 51% t omcatv 858,364,988 250,343,458 \n70% 251,509,201 o% 213,985,244 14% 14~o 75% debico 6,645 3,234 51% 2,946 8% 2,802 4% 13% 57% seval 105 \n98 6% 87 11% 86 170 12% 18% sgemm 1,393 1,095 21% 1,096 070 954 12% 12% 31% cardeb 1,716 989 42% 999 \n1% 889 11% 10% 48% hmoy 47 28 40% 27 3% 25 7% 10% 46% orgpar 188 135 28% 135 121 10% 10% 35% repvid 4,270 \n3,042 28% 3,038 070 2,762 9% 9% 35% drepvi 409 321 21% 303 5% 294 2% 8% 28% heat 229 201 12% 190 5% 184 \n3% 8% 19% svd 6,834 X21V21 403 258 I 35% 258 239 7~o 7% 40yo inideb 1,733 888 48% 954 7% 829 13% 6% 52% \npast em 6,353 4,070 35% 3,941 3% 3,821 3% 6% 39% si 206 176 14% 177 o% 165 6% 6% 1970 deseco 33,873 14,430 \n57% 13,864 3% 13,707 1% 5% 59% fmtgen 236 207 1270 202 2% 195 3% 5% 17% f pppp 7,767 5,838 24% 5,514 \n5% 5,514 5% 29% yeh 160 139 13% 132 5% 132 5% 17% paroi 7,489 3,724 50% 3,677 1% 3,571 2% 4% 52% tuldrv \n122,220,766 90,895,146 25% 86,945,328 4% 87,122,050 o% 4% 28% debflu 8,066 5,170 35% 5,156 o% 4,965 3% \n3% 38% colbur 152 126 17% 121 3% 123 1% 2% 19% de c omp 876 635 27% 641 o% 617 370 2% 29% inithx 5,918 \n3.086 47% 3.067 0% 3.018 1% 2% 49% II II 111 ! !1 III11 coeray 117 105 I 10% 104 o% 104 o% 11% rkfs 456 \n298 I 34% 297 o% 297 0% 34% ! int egr 5,803 2,424 58% 2,436 o% 2,447 o% o% 57% subb 704 632 10% 636 \no% 636 o% 9% Supp 906 813 10% 814 o% 814 o% 10% urand 221 220 o% 221 o% 222 o% o% o% zeroin 1,020 739 \n27% 743 o% 743 OYO 27yo f ehl 785 510 35% 510 517 1% 1% 34% ihbt r 513 453 11% 452 o% 458 170 1% 10% \nsaxurr 322 318 1% 323 1% 323 1% -o% !1 II solve 223 169 24% 168 o% 172 2% 1% 22% R97 9C!Z. R.KA _2v.-,. \n!.?AK 1 w. ,. CIT. 9KO% ,0 ----.. -=. -------- III III II 39?0 9 % 160 12% 165 3% 16,5 I . .. Ii .,. \n3,355 67% 3,447 2% 3,509 1% 4~o 65% h II drigl 161 113 29% 126 11% 125 o% lo% 22% 0 prophy 3,904 74% \n4,016 2% 4)351 8!70 1170 72% efill 205 9% 230 12% 230 12% 1% = baseline partial reassociation distribution \nnew totalE Table 1: Experimental Results Distribution Similarly, distribution of multiplication over \naddition can cause problems in some cases. Con\u00adsiderthe following pair of expressions arising from a \npair ofarray accesses, onetoasingle-precision array and the other to a double-precision array: 4X( T, \n1) 8x(r, 1) Distribution of the multiplies would yield: 4xrt 4xl 8X? , -8X1  and eventually, via constant \nfolding: lxr, -4 8X T, 8 llnfortunately, this version is slightly worse than the original code since \nthe original allowed commoning of the subexpression ri 1. Despite disappointments of this sort, it isclear \nfrom theresults in Table 1 that dis\u00adtribution is quite important. We believe that some of the problems \nof distribution can be avoided by employ\u00ading a slightly more sophisticated approach, though this is a \ntopic for further study. Forward Propagation Earlier, we mentioned that forward propagation eliminates \npartially-dead expres\u00adsions. However, forward propagation can also result in code degradation if expressions \nare moved into loops where they will be invariant but PRE will be unable to hoist them. For an example, \nconsider the (simplified) code below, where the left and right fragments show the same code before and \nafter forward propagation: lbtj+k 2 +-0 i+i+l if i = m branch ifi= mbranch ~ J + n+-j+k iti+n ita+n \nJ - J / iti+l i-i+l if z< 100 branch if i < 100 branch In this case, the computation of n ~ j + k has \nbeen pushed into the loop, ~otentially shortening some paths through the program. However, since we expect \nthe loop to execute many times, the code on the right is potentially much slower (of course, the actual \ntradeoff is undecidable, as it depends on the values of j, k, and m). Recalling from Section 2 that PRE \nwill never lengthen a path through the code, we realize that PRE will not be able to hoist the evaluation \nof j + k out of the loop without lengthing the path around the use of n. 4,3 Code Expansion The speed \nand space requirements of our approach are primarily dependent on the amount of code expansion introduced \nby forward propagation. In the worst case, this expansion can be exponential in the size of the rou\u00adtine. \nTo see how bad the expansion is likely to be in practice, we measured the the effect of forward propa\u00adgation \non the routines in our test suite. Table 2 shows the results of these tests. The entries in the before \nand after columns represent static counts of the number of ILOC operations-in each routine. The column \nlabeled expansion indicates the code growth factor due to for\u00ad ward propagation. 5 Discussion In implementing \nthese techniques, we encountered sev\u00aderal issues that merit further attention. 5.1 Forward Propagation \nand Correctness If an expression name is live across a basic. block bound\u00adary, PRE will sometimes hoist \nan expression past a use of its name. Consider the example below: rIO + sqrt(rg) rIO +-sqrt(rg) if p \nbranch ~ if p branch ~ 7-9 + ?-1000 + L 1 7 20 + ?-10 before PRE after PRE The problem is that the fragment \non the left violates a requirement for correct behavior of PRE; namely, an expression defined in one \nbasic block may not be ref\u00aderenced in another basic block. z Forward propaga\u00adtion satisfies this rule \nby moving the computation of rl o * sqrt (rg) directly before its use, relying on the re\u00adnaming introduced \nby SSA to preserve the correct ver\u00adsion of rg. We note that Chow also mentions using for\u00adward propagation \n[8]; we conjecture that it helped him avoid the same difficulty with PRE. An alternative approach to \nensuring that no expres\u00adsion name is live across a basic block boundary is to in\u00adsert copies to newly \ncreated variable names and rewrite later references so that they refer to the variable name rather than \nthe expression name. While it is possible that this approach could be used to avoid some of the negative \neffects of forward propagation, it may detract from the effectiveness of reassociation. This remains \na topic for future research. zwe nave ~lever ~~ell this reqtirelnent stated in tbe literature and believe \nit to be a source of confusion in the community. 5.2 Il~teraction with Other Optimization ro u tin e \nbefore aft er ?xpansion bilan 2,000 2,357 1.179 cardeb 916 1,024 1.118 coeray 280 397 1.418 colbur 659 \n1,155 1.753 dcoera 422 1,050 2.488 ddef lu 4,040 7,089 1.755 debflu 3,767 4,822 1.280 debico 2,728 2,984 \n1.094 decomD 941 1.144 1.216 . ,I deseco 11,545 13:537 1.173 ,I I drepvi 1:750 2:262 1.293 dri~l 565 \n667 1.181 t efill 1,257 1,996 1.588 f ehl 552 581 1.053 fmin 372 661 1.777 fmtgen 588 696 1.184 f mt \nset 551 600 1.089 f pppp 20,147 27,358 1.358 gamgen 842 1,070 1.271 heat 925 1<817 1.964 hmo y 153 168 \n1.098 ihbt r 772 790 1.023 inideb 1,064 1,200 1.128 iniset 6,566 6,747 1.028 lnithx 2,378 2,539 1.068 \nint egr 812 967 1.191 orgpar 1,352 1,641 1.214 paroi 4,300 4,921 1.144 pastern 2,567 2,794 1.088 prophy \n2,695 3,473 1.289 repvid 1,584 1,922 1.213 rkf45 164 228 1.390 rkfs 881 1,180 1.339 saturr 1.524 2.131 \n1.398 1I I s axpy 95 ] 102 1.074 seval 167 190 1.138 sgemm 677 976 1.442 sgemv 293 340 1.160 si 178 202 \n1.135 solve 319 375 1.176 sDline 1.173 1.220 1.040 subb 1,199 1,199 1.0001 Supp 1,589 2,075 1.306 svd \n3,984 1.554 2,563 t omcatv 2,645 3,610 1.365 tvldrv 13,405 15,870 1.184 urand 189 212 1.122 x21y2i 70 \n75 1.071 yeh 929 1,628 1.752 zeroin 276 400 1.449 totals 107,475 136,377 1.269 1 Table 2: Code Expansion \nfrom Forward Propagation Some optimization interact poorly with our technique. For example, many compilers \nreplace an integer mu\u00adtiply with one constant argument by a series of shifts, adds, and subtracts [4]. \nSince shifts are not associative, this optimization should not, be performed until after global reassociation. \nFor example, if ((z x y) x 2) x z is prematurely converted into ((x x y) << 1)x z, we lose the opportunity \nto group z with either z or y. This effect is measurable; indeed, we have accidentally measured it more \nthan once. We expect that strength reduction will improve the code beyond the results shown in this paper. \nReassoci\u00adation should let strength reduction introduce fewer dis\u00adtinct induction variables, particularly \nin code with corrl\u00adplex subscripts like that produced by cache and register blocking [5, 27]. Of course, \nsome particularly sophisti\u00adcated approaches to strength reduction include a form of reassociation [20]; \nwe believe that a separate pass of reassociation will significantly simplify the implementat\u00adion of strength \nreduction. Additionally, implementing global reassociation as a separate pass provides benefits to other \noptimization, even in loop-free code. 5.3 CommoII Subexpression Elin~i~~aticm The experiments described \nin Section 4 show that PRE is a powerful component of an optimizing compiler. A natural question is: \nHow does it compare to other ap\u00adproaches? To answer this, we will consider three differ\u00adent approaches. \nAssume for each that we have used the techniques described in Sections 3.1 and 3.2 to encode value equivalence \ninto the name space, 1. Alpern, Wegman, and Zadeck suggest the following scheme: If a value z is computed \nat two points, p and q, and p dominates q, then the computation at, q is redundant and may be deleted \n~2, page 2]. 2. The classic approach to global common subexpres\u00adsion elimination is to calculate the \nset of expressions available at each point in a routine. If z is available on every path reaching p, \nthen any computation of x at p is redundant and may be deleted. 3. Partial redundancy elimination, as \ndescribed in Section 2.  These methods form a hierarchy. The first method re\u00ad moves only a subset of \nthe redundancies in the code. For instance, it cannot remove the redundancy shown in the first example \nof Section 2 where z + y occurs in each clause of an if-then-else and again in the block that follows. \nThe second method, based on available expres\u00ad sions, will handle this case; it removes all redundancies. \nPRE is stronger yet it removes all redundancies and many partial redundancies as well. 168 6 Related \nWork While there have been many papers discussing par\u00adtial redundancy elimination (e.g., ~21, 14, 12, \n18]), none mention the deficiencies discussed in Sections 2.2 and 2.3. Rosen etal. recognize the naming \nproblem and propose a complex alternative to PRE; however, they do not consider reordering complex expressions \n[X]. The idea of exploiting associativity and distributivity to rearrange expressions is well known [17, \n1]; however, early work concentrated on simplifying individual ex\u00adpressions. We know of two prior approaches \nto reasso\u00adciation with the goal of exposing loop-invariant expres\u00adsions, both discovered within IBM and \npublished the same year. Scarborough and Kolsky describe a front\u00adend discipline for generating an array \naddress expression as a sum of products and associating the sum to expose the loop-invariant parts [25]. \nCocke and Markstein also mention the idea of reassociation, this time within the optimizer instead of \nthe front end [9]. In a chapter for an upcoming book, Markstein et al. describe a sophisticated algorithm \nfor strength reduc\u00adtion that includes a form of reassociation ~20]. Their algorithm attacks the problem \non a loop-by-loop basis, working from inner to outer loops. In each loop, they perform some forward propagation \nand sort srrbexpres\u00adsions into loop-variant and loop-invariant parts, hoist\u00ading the invariant parts. \nWe presume their approach is a development of earlier work within IBM. Other work by O Brien et al. and \nSanthanam briefly describe what are apparently further development,s of the Cocke and Markstein approach \n[22, 24]. It is difficult to compare our approach directly to these earlier methods. We were motivated \nby a desire to separate concerns. We already had solutions to hoist\u00ading loop invariants and strength \nreduction; therefore, we looked for a way to reassociate expressions. We also prefer our global approach \nto loop-by-loop alternatives since it can make improvements in loop-free code and may admit simpler implementation. \nRecent work by Feigen et al. and by Knoop et al. describe alternative approaches to the problem of elim\u00adinating \npartially-dead expressions [15, 19]. While an adequate comparison of the alternatives would require trial \nimplementations and empirical measurements, it is clear that they solve a similar class of problems in \nradically different ways. In our case, the elimination of some partially-dead expressions is an unexpected \nbene\u00adfit of forward propagation. 7 Summary In this paper, we show how to use global reassociation and \nglobal value numbering to reshape code in a way that improves the effectiveness and applicability of \npar\u00adtial redundancy elimination. The effect of these trans\u00ad formations is to expose new opportunities \nfor optimizat\u00adion. In particular, more expressions are shown to be redundant or loop-invariant; partial \nredundancy elimi\u00adnation optimizes these newly exposed cases. Addition\u00adally, some partially-dead expressions \nare eliminated. We showed experimental results that demonstrate the effectiveness of partial redundancy \nelimination. The data also shows that applying our transformations be\u00adfore partial redundancy elimination \ncan produce signif\u00ad icant further improvements. We introduced an algorithm for global reassociation. \nIt efficiently reorders the operands of associative opera\u00adtions to expose loop-invariant expressions. \nIts simplicity should make it easy to add to an existing compiler. Acknowledgements We owe a debt of \ngratitude to our colleagues on the compiler project: Tim Harvey, Rob Shillingsburg, Tay\u00adlor Simpson, \nLisa Thomas, and Linda Torczon. With\u00adout their support and implementation efforts, this work would have \nbeen impossible. We also appreciate the thorough reviews provided by the program committee; they significantly \nimproved both the form and content of this paper. References [1] Alfred V. Aho, Ravi Sethi, and Jeffrey \nD. Unman. Compilers, Principles, Techniques and Tools. Addison-Wesley, Reading, MA, 1986. [2] Bowen Alpern, \nMark N. Wegman, and F. Kenneth Zadeck. Detecting equality of variables in programs. In Conference Record \nof the Ftfteenth Annual ACM Synl\u00adposwrn on Principles of Programming Languages, pages 1-11, San Diego, \nCalifornia, January 1988. [3] Marc A. Auslander and Martin E. Hopkins. An overview of the PL.8 compiler. \nSIGPLA N Notices, 17(6):22-31, June 1982. Proceedings of the ACM SI(7-PLAN 82 Symposium on Contpiler \nConstruction. [4] Robert L. Bernstein. Multiplication by integer con\u00adstants. Software Practice and Experience, \n16(7) :641 652, July 1986. [5] Steve Carr and Ken Kennedy. Blocking linear algebra codes for memory hierarchies. \nIn Jack Dongarra, Paul Messina, Danny C. Sorensen, and Robert C;. Voight, editors, Proceedings of the \nFourth SIAM Conference on Parallel Processing for Scientific Computing, pages 400-405, 1990. [6] Gregory \nJ. Chaitin, Marc A. Auslander, Ashok K. Chandra, John Cocke, Martin E. Hopkins, and Pe\u00adter W. Markstein. \nRegister allocation via coloring. Conlputer Languages, 6:47-57, January 1981. [7] Jong-Deok Choi, Ron \nCytron, and Jeanne Ferrante. Automatic construction of sparse data flow evaluation graphs. In Conference \nRecord of the Eighteenth Annual ACM Syrnposiurn on Principles oj Programming Lan\u00adguages, pages 55 66, \nOrlando, Florida, January 1991. [8] Fred C. Chow. A Portable Mczchme-Independent Global Opttmazer -Design \nand Measurements. PhD thesis, Stanford University, December 1983. [9] John Cocke and Peter W. Markstein. \nMeasurement of program improvement algorithms. In Proceedings oj In~ormatzon Processing 80. North Holland \nPublishing Company, 1980. [10] John Cocke and Jacob T. Schwartz. Programming lan\u00adguages and their compilers: \nPreliminary notes. Techni\u00adcal report, Courant Institute of Mathematical Sciences, New York University, \n1970. [11] Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. Efficiently \ncomput\u00ading static single assignment form and the control de\u00adpendence graph. ACM i ransactions on Programming \nLanguages and Systems, 13(4):451-490, October 1991. [12] Dhananjay M. Dhamdhere. Practical adaptation \nof the global optimization algorithm of Morel and Ren\u00advoise. ACM Transactions on Programming Languages \nand Sgstents, 13(2):291 294, April 1991. [13] Dhananjay M. Dhamdhere and Uday P. Khedker. Comp\u00adlexity \nof bidirectional data flow analysis. In Confer\u00adence Record of the Twentieth Annual ACM SIGPLAN-SIGA CT \nSymposium on Principles of Prograrnmzng Languages, pages 397 408, Charleston, South Carolinaj January \n1993. [14] Karl-Heinz Drechsler and Manfred P. Stadel. A solution to a problem with Morel and Renvoise \ns Global opti\u00admization by suppression of partial redundancies . ACM Transactions on Programming Languages \nand Systems, 10(4):635-640, October 1988. [15] Lawrence Feigen, David Klappholz, Robert Casazza, and \nXing Xue. The revival transformation. In Con\u00adference Record of POPL 9.4: 21st ACM SIGPLAN-SIGA CT Sympostum \non Principles of Programrntng Languages, pages 421-434, Portland, Oregon, January 1994. [16] George E. \nForsythe, Michael A. Malcolm, and Cleve B. Moler. C omputer Methods for Mathematical Compu\u00adtations. Prentice-Hall, \nEnglewood Cliffs, New Jersey, 1977. [17] Dennis J. Frailey. Expression optimization using unary complement \noperators. SIGPLA N Notices, 5(7):67-85, July 1970. Proceedings of a Symposium on Comp$ier Optimization. \n[18] Jens Knoop, Oliver Ruthing, and Bernhard Steffen. Lazy code motion. SIGPLAN Nott.e., 27(7):224-234, \nJuly 1992. Proceedings of the ACM SIGPLA N 92 Con\u00ad ference on Programming Language Design and Imple\u00admentation. \n [19] Jens Knoop, Oliver Riithing, and Bernhard Steffen. Partial dead code elimination. SIGPLA N Notices, \n29(6), June 1994. Proceedings of the ACM SIGPLAN 94 Conference on Programming Language Design and ]mp~ementation. \n [20] Peter W. Markstein, Victoria Markstein, and F. Ken\u00adneth Zadeck. Reassociation and strength reduction. \nIn Optimization in Compilers. ACM Press, to appear. [21] Etienne Morel and Claude Renvoise. Global optin~iza\u00adtion \nby suppression of partial redundancies. C; omn~unt\u00adcations of the ACM, 22(2):96 103, February 1979. [22] \nKevin O Brien, Bill Hay, Joanne Minish, Hart mann Schaffer, Bob Schloss, Arvin Shepherd, and Matthew \nZaJeski. Advanced compiler technology for the RISC; System/6000 architecture. In IBM RISC Sgstem/6 000 \n? ethnology. IBM Corporation, Armonk, New York, 1990. [23] Barry K. Rosen, Mark N. Wegrnan, and F. Kenneth \nZadeck. Global value numbers and redundant compu\u00adtations. In Conference Record of the Fifteenth Annual \nACM Symposium on Principles of Programmmg Lari\u00adguages, pages 12 27, San Diego, California, Jannary 1988. \n[24] Vatsa Santhanam. Register reassociation in PA-RISC: compilers. Hewlett-Packard Journal, pages 33 \n38, June 1992. [25] Randolph G. Scarborough and Harwood G. Kolsky. Improved optimization of FORTRAN object \nprograms. IBM Journal of Research and Development, pages 660\u00ad676, November 1980. [26] Mark N. Wegman \nand F. Kenneth Zadeck. (.;on\u00ad stant propagation with conditional branches. ACM Transactions on Programming \nLanguages and Systems, 13(2):181-210, April 1991. [27] Michael E. Wolf and Monica S. Lam. A data local\u00adity \noptimizing algorithm. SIGPLA N Notices, 26(6):30 44, June 1991. Proceedings of the ACM SIGPLAN 91 Conference \non Programming Language Design and Im\u00adplementation.   \n\t\t\t", "proc_id": "178243", "abstract": "<p>Partial redundancy elimination is a code optimization with a long history of literature and implementation. In practice, its effectiveness depends on issues of naming and code shape. This paper shows that a combination of <italic>global reassociation</italic> and <italic>global value numbering</italic> can increase the effectiveness of partial redundancy elimination. By imposing a discipline on the choice of names and the shape of expressions, we are able to expose more redundancies.</p><p>As part of the work, we introduce a new algorithm for global reassociation of expressions. It uses global information to reorder expressions, creating opportunities for other optimizations. The new algorithm generalizes earlier work that ordered FORTRAN array address expressions to improve  otpimization [25].</p>", "authors": [{"name": "Preston Briggs", "author_profile_id": "81100277551", "affiliation": "Department of Computer Science, Rice University, Houston, Texas", "person_id": "PP14104037", "email_address": "", "orcid_id": ""}, {"name": "Keith D. Cooper", "author_profile_id": "81100286556", "affiliation": "Department of Computer Science, Rice University, Houston, Texas", "person_id": "P158954", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/178243.178257", "year": "1994", "article_id": "178257", "conference": "PLDI", "title": "Effective partial redundancy elimination", "url": "http://dl.acm.org/citation.cfm?id=178257"}