{"article_publication_date": "06-01-1994", "fulltext": "\n Separate Compilation for Standard ML Andrew W. Appel* Princeton University appel@princeton .edu Abstract \nLanguages that support abstraction and modular structure, such as Standard ML, Modula, Ada, and (more \nor less) C++, may have deeply nested depen\u00addency hierarchies among source files. In ML the prob\u00adlem is \nparticularly severe because ML s powerful pa\u00ad rametrized module (functor) facility ent ails depend en\u00ad \ncies among implementation modules, not just among interfaces. To efficiently compile individual modules \nin such lan\u00adguages, it is useful (in ML, necessary) to infer, digest, and cache the static environment \nresulting from the compilation of each module. Our system provides a simple model of compilation and \nlinkage that supports incremental recompilation (a restricted form of separate compilation) with type-safe \nlinkage. This model is made available to user programs in the form of a set of internal compiler modules, \na fea\u00adture that we call the visible compiler. The chief client of this interface is the IRM incremental \nrecompilation manager from CMU. 1 Interfaces and Implementa\u00adt ions Modular languages, such as Ada and \nModula, distin\u00ad guish between interface modules and implementation modules. In a weaker sense even C \nand C++ support such a distinction by organizing source code in .h and .C files. Compilers for these \nlanguages support separate compilation in which the compilation of each imple\u00admentation module requires \nthe examination of those in\u00ad *Supp.rted in part by NSF grant &#38;XL9200790. Permission to co y without \nfee all or part of this material is granted provrd Jthat the copies are not made or distributed for direct \ncommercial advantage, the ACM copyright notice and the title of the publication and its date appear, \nand notice is given that copying is by permission of the Association of Computing Machinery. To copy \notherwise, or to republish, requires a fee ancf/or specific permission. David B. MacQueen AT&#38;T Bell \nLaboratories macqueen@research. att .com terface modules that it imports or exports, but rarely, if ever, \nthe examination of another implementation mod\u00ad ule. When the dependency graph of interfaces is shallow, \nit s reasonable to re-parse all the interfaces imported by each compilation unit when any imported interface \nis modified; this is what C compilers do with Unix make[7]. There are still some problems, however: Q \nThe dependent y DAG of just the interface mod\u00adules can be quite deep: some files in the 01 library[5], \na well-known C++ library for building X-based user interfaces, bring in about 20,000 lines of header \nfiles [13], which can be time-consuming to process. Sometimes many files depend on a particular mod\u00adule, \nso we want to avoid unnecessary recompila\u00adtion when that module is changed.  Languages may, for various \nreasons, allow depen\u00ad  dencies between implementation modules. Having one implementation module dependent \non other implementations leads to a deeper dependency DAG, causing timestamp-based make algorithms to \ndo large amounts of unnecessary recompilation. Standard ML is an example of a modular language that \nallows inter-implement ation dependencies in this case, to support a powerful functor (parameterized \nmodule) facility (see section 2). Ada [4] and Modula\u00ad3 [20, 9] have much more limited forms of parame\u00adtrized \nmodules, called generics. As we will explain in section 2, Ada generics also have nontrivial inter\u00adimplement \nation dependencies. Modula-3 generics have restrictions that avoid this problem, but yield a less powerful \nand less convenient module system. The func\u00adt or facility of Standard ML is so powerful and conve\u00adnient \nthat it is heavily used; so a heavy-duty approach to its separate compilation is justified. Another source \nof inter-implementation dependen\u00adcies is in-line expansion of functions across module SIGPIAN 94-6/94 \nOrlando, Florida USA @ 1994 ACM 0-89791 -662-xKWO006..$3.5O boundaries whether performed in response \nto an in\u00adline pragma (as in Ada), or automatically by an op\u00adtimizing compiler for any language. The system \nwe describe here does not deal with dependencies of this sort, though it could in principle. Some of \nthe separate compilation issues enumer\u00adated above have been addressed before: Adams et al. [1] review \n(and measure) several replacements for timestamp-based recompilation: an interface can be hashed into \na stamp, so that trivial modifications to source code don t necessarily change the hash-and un\u00adnecessary \nrecompilation are avoided. Some Modula compilers [8, 12] create digested ver\u00adsions of interface files \nto speed the importing process. In ML, an export interface is derived by semantic anal\u00adysis of both interface \nand implementation modules, making it especially necessary to use digested inter\u00adface descriptions, and \nnot just re-elaborate the source code of imported files. The implementation modules are just too big \nto re-elaborate for each client that im\u00adports them. In this paper we give a compiler s-eye view of the \nseparate compilation mechanism as a set of primitives to be used by a make system, and we describe the \nim\u00adplement ation of those primitives. In section 8 we give an overview of a major client of these primitives, \nan in\u00adcrement al recompilation manager, source-dependence analyzer, and library manager called IRM. A \ncompan\u00adion paper[15] gives a full description of this utility. Our new separate compilation mechanism \nfor Stan\u00addard ML, implemented in the Standard ML of New Jersey compiler, allows incremental recompilation \nin a language wit h inter-implement at ion dependencies; provides a uniform compilation and linkage envi\u00adronment \nso that interactive compile-and-execute commands are compatible with separately com\u00adpiled modules; uses \nstatic environments (analogous to symbol ta\u00adbles) and dynamic environments (playing a role similar to \nlinkage tables) that are first-class values and can be composed in an elegant, side-effect-free manner; \ninfers the exact export interface of an implemen\u00adtation module, and hashes this into an intrinsic stamp \nfor precise recompilation (see section 5); enforces type-safe dynamic linkage without requir\u00ading link-time \ntype checking; provides an architecture for type-safe usage of in\u00adternal components of an interactive \nsystem by its own clients (see section 7); can be generalized to support cross-module opti\u00admization, \nwhich (in any language) leads to inter\u00adimplement ation dependencies. To infer full interface descriptions \nin a language with separately compiled higher order modules is not at all a trivial thing, and we believe \nwe are the first to do this. (In existing Ada or Modula-3 systems, the gener\u00adics cannot be fully compiled \nseparately from their ar\u00adguments.) Cutoff Recompilation What we have achieved, however, is better termed \nan incremental recompilation system. It is not always pos\u00adsible, in our system, to compile a module in \nvacuo, without seeing all of the modules it imports. But if any module is recompiled without changing \nits export inter\u00adface, we avoid recompilation of the dependent modules. Given a set of interfaces 111,..., \nlln and corre\u00adsponding implementations Ml, . . . . Mn, classical sepa\u00adrat e compilation allows the compilation \nof any mod\u00adule M~ with the examination of some subset {Hj } of the interfaces, but without examining \nthe text of the other implementations. Classical separate compilation is not possible for ML, or any \nlanguage with inter\u00adimplement ation dependence. 1 Having once compiled a set of implementations Mj, recompilation \nallows a module Mi (dependent on the implement ations Mj ) to be compiled without (in the same session \n) compiling the Mj again. If one of the modules Mj is altered in a way that does not affect its exported \ninterface and recompiled, cutofl recompilation [21] (we use Adams s terminology [1]) allows the previously \ncompiled M; (dependent on Mj ) tobe used without recompilation. Finally, smartest recompilation[23] allows \nMi to be compiled even without the examination of Mj (or any of the Hj !); the linker must then be made \nsmarter so that it can verify hypotheses about Hj and Mj made during the compilation of Mi. But smartest \nrecompila\u00adtion has not yet been demonstrated feasible for higher\u00adorder funct ors. In this paper we will \ndescribe a cutofl recompilation system, not a classical or smartest system.  2 Standard ML modules The \nmodule system of ML, like those of Modula and Ada, distinguishes between interfaces (called signa\u00ad tures \nin ML) and implementations (called structures 1In ML the user can always decouple implementations from \neach other using functors. This is helpful to the user, when true separate compilation is desire~ for \nexample, when a client mod\u00adule must be compiled independently of the modules it imports. and ~unctors). \nHowever, in ML the interface of a struc\u00adture is often only partially described by an explicit signature2. \nsignature PARTIAL. ORDER = s ig type elem val less : elem * elem -> bool end signature SORT = s ig type \nt val sort : t list -> t list end functor TopSort(P : PARTIAL_OmER) : SORT . struct type t = P.elem \nfun sort(1) = . . . P.less(x,y) . . . ~ end structure Factors : PARTIAL-ORDER = Struct type elem = int \nfun less(i,j) = (j mod i = O) end structure FSort : SORT = TopSort(Factors) Figure 1: Transparent signatures \nA signature in ML specifies what type elements of a structure will be visible to client structures, but \nun\u00adlike aModula interface, it does not render these ele\u00admerits opaque. ML s transparent signature matching \nis designed to support parameterized structures (func\u00adtors), whose parameter signature is a partial interface \ndescription that will be instantiatedby actual param\u00adeter structures containing different types, which \nmust be propagated through the functor. Forexample, the structure FSort of figure 1 will be of use only \nifit is known that FSort.tisthesame asint. However, thisis not specified in the signature SORT nor shouldit \nbe, for thenit would not bepossible to apply TopSortto partial orders over other types.3 Consequently, \nto compile a client of FSotionelmust first inspect the implementation module structure FSort : SORT = \nTopSort(Factors ) and not merely the (partial) interface, SORT. z~fact, the~iaatweis optional, and can \nbeinferredifitis not specified. 30nec~exP=d theresdt constraint SORT of Top%rt and include asharingconstraint \nequatingt to P.elem; but this breaks down with higher-order functom. Seealso section 10.2. More generally, \nto compile one implementation mod\u00ad ule it maybe necessary to inspect the implementations of all the modules \nit imports (directly or indirectly) todetermine thedefinitions ofexported types that are not fully described \nin signatures. This contrasts with the situation in Modula 2 and Modula 3, where it is necessary to inspect \nonly (the transitive closure of) the inter face specifications of the imported modules. The generic interfaces \nand modules of Modula3 do not change this situation much, because they can only be inst antiated at the \ntop level. So an im\u00ad plementation module A that is an instance of a generic module G is an example of \nan inter-implementation dependency; but no other implementation module can depend on the implementation \nof A (only on its in\u00ad terface), so there is never a nontrivial chain of these dependencies. However, \nthis restriction does limit the utility and convenience of Modula-3 generics. In Ada, an instance of \na generic package can occur as a submodule in a package body, and this causes a de\u00adpendency between that \npackage body and the generic package that it instantiates. Importers of a package that exports inline \nprocedures also depend on that package s implementation. So Ada also is subject to inter-implementation \ndependencies, though they arise from different mechanisms from those applying in Stan\u00addard ML. The solution \nused in Ada and Modula-3 is to view generic instantiation as a kind of source-language ex\u00adpansion. This \nis not really separate compilation of generic modules themselves, since only their instances are truly \ncompiled. In ML, a functor is type-checked and compiled into machine code independently of its applications. \nThough this is a very significant advantage of the ML module system, it does come at the cost of inter\u00adimplementation \ndependence. In section 10 we discuss alternative module systems and programming styles for ML that avoid \nthis dependence. These alternatives can significantly weaken the original Standard ML module system, \nso our separate compilation system deals di\u00adrectly with inter-implementation dependence.  3 ML compilation \nmodel The original idea of an ML compilation unit waa a single closed signature, structure, or functor \ndefini\u00adtion (having no free references to external structures). For various reasons this approach proved \ninadequate (though it can be patched, as we describe in section 10), so for many years there was been \nno agreed-upon no\u00adtion of separate compilation. The first ML compilers were interactive systems, in which \nsuccessive declarations were evaluated relative to a top level environment , augmenting that environ\u00adment \nwith new bindings, Even the ML module system (signatures, structures, and functors) has been used in \nthis way. Hence, separate compilation was not a critical requirement for the design of the module system, \nwhich mainly addressed issues of parameterization, program composition, and name spaces. How, then, \nare ML programs to be broken up into compilation units? We have taken the liberal ap\u00adproach that a compilation \nunit is a single source file containing any sequence of ML declarations: signa\u00adtures, structures, functors, \nvalues, types, etc.4 There is no special syntax that delineates compilation units: we use the same module \nlanguage described in The Defi\u00adnition of Standard ML[19]. Thus, all existing ML programs are automatically \nupward compatible with the new separate compila\u00adtion system. Evaluation interface ML source code is evaluated \nin an environment map\u00adping names to types and values, and produces new bindings of names to types and \nvalues a new envi\u00adronment: env : name * type x ualue evaluate : source X env --+ env To distinguish between \ncompilation and execution, we factor evaluate into compile and execute, and at the same time we factor \nenv into static and dynamic parts. We use persistent identifiers, or pids as a form of inter\u00admediate \nname. env = statenv x dgnenv statenv name + type x pid dynenv pid --+ value compile source x statenv \n--+ statenv x code Unit ezecute code Unit x dynenv + dynenv Thus evaluate is the appropriate composition \nof com\u00adpile and execute. But note that having decoupled these phases, we can perform a chain of compiles \nat one time, and the corresponding chain of executes can be done much later. The values exported from \nML modules are dynamic run-time values, such as function closures and run-time construct ed dat a n ot \nmerely static linkage addresses. It is necessary to execute an ML module even to pro\u00adduce its export \nfields. In a more conventional model of linking (in Ada, Modula, C), all imports and exports exist as \nstatic addresses even prior to the execution of any module, The conventional model requires that 4We \nrecommend (and the IRM compilation manager requires) that separately compiled units contain structures, \nfnnctors, and signaturca, but not top-level values and types. all variables be initialized to constant \nvalues (though they may subsequently be re-initialized by the start-UP code of a module). This cannot \nwork in ML, where many kinds of values (such as function closures created from the application of higher-order \nfunctions) have no reasonable stat ic default init ializat ion. Furthermore, variables in ML do not really \nvary, so they cannot be re-initialized. The dynenv maps internal names (pids) to runtime addresses. Thus, \nit serves the same purpose as the link-editor table of the conventional model. But it is produced only \nas the modules execute: the exported fields of a module are produced by, and do not exist prior to, the \nexecution of the module s machine code. (Note that this precludes cyclic dependencies among modules in \nML.) The compile function returns a pair of a statenv and a code Unit, which together we call a compiled \nUntt. The code Unit breaks down into three components: code, the machine code for the unit; imports, \na list of pids des\u00adignating the imported values; and exports, another list of pids designating the exported \nvalues. compiled Unit = statenv x code Unit code Unit = code x imports x exports With further processing \nof external references in the siatenv (as described in section 4), the compiled-Unit can be written (by \nthe Incremental Recompilation Manager) to a binary file. Linkage and execution: The code of a compila\u00adtion \nunit implements a (fully closed) function mapping a vector of import values to a vector of export val\u00adues. \nIt has no external references that require link\u00adediting; all external references are obtained from the \nimports, which are passed as arguments to the ma\u00adchine code when it is executed. Individual functions \nexport ed by the code will generally be ciosures con\u00adstructed during the execution phase, and these run\u00adtime-created \nclosures hold the imported external refer\u00adences. This has always been the case in Standard ML of New \nJersey[3], though our current use of pids is more sophist i cat ed than our previous use of lvar-numbers \n,) which were not persist ent (lvar-numbers did not re\u00adtain their meaning from one ML process to another, \nand thus were useless for separate compilation). The interactive read-eval-print loop maintains a global \nenvironment ( env) comprised of a statenv and a dynenv. After compiling a compilation unit yielding a \ncompiled Unit the interactive loop looks up each of the imports pids in the dynenv, producing a vector \nof values. Then it applies the code to this vector, pro duc\u00ading a vector of export values. Then the export \nvalues are bound to the corresponding elements of the exports pid list to form an new dynenv matching \nthe exported statenv. This coordinated pair of static and dynamic environments constitutes the exported \nenvironment of the compilation unit. This increment al exported envi\u00ad ronment is then layered onto the \nglobal environment. In a separate compilation system, we should not be so eager to execute a unit right \nafter compiling it,. In\u00ad stead, we will save the compiled Unit in a bin fine for later use, retaining \njust the statenv as input for further compilations. But since the interactive loop uses the {same compiled \nUnit interface aa the separate compilation sys\u00ad tem, separately compiled modules can be loaded into an \ninteractive system. The following simple example illustrates the com\u00ad pilation and linkage process, using \nvalue declarations instead of module declarations: Source: val a=x+y val b = x+2*z; Compilation environment: \n(SC, dc) static : SC= { z w (int, pidr), y + (int, pidv), z I+ (int, pidz) } dynamic : dc = { pidc +3, \npidy ~ 4, pidz *5} Compilation results: statenv = {a++ (int, pida), b # (int, pid,)} code = A(z, y,z).(z+Y, \n~+2 x ~) imports = [pid~, pidy, pi%] exports = ~ida, pid~] Execution: dynenv = {pida w v=, pid~ * v~} \nwhere (va, v,)== code((dc(pid$)) dc(pidg), dc(pidz)))  4 Making binary files For each compilation unit, \nour compiler produces a compiled Unit: a tuple of code, pids (import, export), and (increment al) static \nenvironment. TO do sepa\u00adrate compilation (or recompilation) these parts must be written to a binary file \nfor later use in another com\u00adpilation. It is easy to write machine code to a binary file; each machine \nhas a well defined representation of machine instructions as sequences of bits [25]. For pids we use \n128-bit integers which are also easy to write to files. But our static environments are complex pointer \ndata structures with no easy representation aa bit strings. Our representation of static environments \n[17] (for structures, functors, signatures, functor signatures, types, datatypes, values, fixities, symbol \ntables, etc.) uses 36 different dat atypes, many of which have sev\u00aderal variants there are a total of \n115 variants of these types, with a total of 193 record fields. It is not a triv\u00adial matter to write \na linked structure over these types into a file. Therefore we use a pickier, a program integrated with \nthe garbage collector of our system that writes a pickled version of an arbitrary data structure to a \nfile, and can read it back in again [20, page 80]. Even so, we found that our internal representation \nof static environments was not suitable for pickling with\u00adout some modifications: . Static environments \nshould be self-contained and portable. This means that they should not have pointers into global data \nstructures, nor should they contain function closures because this would complicate cross-compilation \nfor a different archi\u00adtecture. . Export environments should not be affected by in\u00adternal, unexported \ntypes and structures we de\u00adscribe the problem and its solution in section 5. The static environment of \na compilation unit may contain pointers into the static environment of other units on which it depends. \nManaging sharing The exported static environment for a unit points right into the middle of the static \nenvironments of other units. The pickier would happily traverse such point\u00aders, but then the binary file \nwill contain a private copy of a shared entity. Later, when the static environment is read from the binary \nfile, it will include a new copy of the (previously shared) entity. In the worst case, the DAG of references \nbetween environment components will be converted into a tree, leading to exponential blowup both of binary \nfiles and of core memory. To solve this problem, we dehydrate the exported environment before writing \nit to a file: that is, we iden\u00adtify external pointers and replace them by stubs. Later, after reading \nthe binary file, we will rehydrate the environment by turning the stubs back into pointers. This is the \nstatic linkage process. There are two problems to solve in designing the de\u00adhydration algorithm: 1. How \ncan the dehydrate tell which things are ex\u00adternal pointers into structures shared with other bin files, \nsince all pointers in core look the same? 2. Given a stub, how can the rehydrater find the real in-core \npointer with which to replace it? Every significant object in a static environment (module (structure \nor functor), signature, or type con\u00adstructor) has its own stamp, composed of a persistent identifier \n(pid) and a small integer. All objects created in one compilation unit will have the same pid but dif\u00adferent \nintegers. The dehydrate traverses the static en\u00advironment exported by current compilation unit; when\u00adever \na stamped object is found whose pid belongs to another compilation unit, this object is external and \ncan be replaced by a stub containing only the stamp. This is the solution to problem 1. But later, after \nthe dehydrated environment has been read from the bin file, how can the rehydrater replace the stub with \nthe real pointer? Each source file is compiled with some (context en\u00advironment made from the combination \nof the export\u00adenvironments of all the imported modules. Any ex\u00adternal object must live somewhere within \nthe context environment. But the environments are not indexed by stamp ; they are mappings from source-language \nsymbols to objects cent aining stamps. An exhaustive search of the context environment would find the \nobject containing any given stamp, so in principle the context environment is sufficient to rehydrat \ne. We make this more efficient by constructing, for each environment, the index mapping from stamps to \nobjects. Each environment now comprises two map\u00adpings: from symbols to objects, and from stamps to ob\u00adjects. \nStatic environment is an abstract type within the compiler, with operations for construction, layering, \nlookup, and so on; Indexed static environment matches the same specification but also supports lookups \nby stamp. It is possible to coerce either kind of environ\u00ad ment into the other, either by constructing \nthe index or by throwing it away. Within the elaboration of a single compilation unit, non-indexed environments \nare used (for efficiency) because lookups by stamp are not needed; IRM uses only indexed environments \nand ap\u00adplies coercions at the appropriate e points. For the rehydrater to work, it must be given a con\u00adtext \nenvironment cent aining all the external stamps that might be found in the current compilation unit. \nFortunately, this context is simply the indexed version of the static environment originally used in \ncompila\u00adtion of this unit. For compilation, IRM constructs cent ext environments by layering the exported \nstat envs of imported units; for dehydration, it just reproduces an equivalent layering. When indexed \nenvironments are layered, both the symbol mappings and the stamp mappings are combined separately; this \nis more effi\u00adcient than layering the symbol mappings and then re\u00adindexing. The dehydration and dehydration \nprocesses are ex\u00adtremely similar: both traverse a static environment, replacing certain leaf-like nodes \n(external-pointers be\u00adcome stubs, or vice versa). A single program, with a dehydrate/rehydrat e parameter, \naccomplishes both tasks. Dehydration/rehy dration is done in a nondestruc\u00adtive, functional style: the \nenvironment is copied. Copy\u00ading is quite efficient in SML/NJ (because of fast allo\u00adcation and fast garbage \ncollection) but one problem is that DAG structures can turn into tree structures (with a big blowup in \nmemory use) unless one is care\u00adful. This is not trivial in ML, which has no notion of pointer identity; \nnor, for that matter, in any language with copying garbage collection, where pointers can\u00adnot be used \nas hash-table indices. But the stamps of internal objects (at least for those objects significant enough \nto have st amps) can serve as indices into tempo\u00adrary tables to ensure that sharing of pointer structure \nis preserved. Ha.sh-consing would work even better, but we don t have that. The most troublesome and \ntime-consuming part of the implementation was the detailed work on identify\u00ading external sharing (with \nstubs) and preserving inter\u00adnal sharing (DA G structure). This is in part because there are so many cases \nto handle: a set of mutually re\u00adcursive functions had to traverse, and preserve sharing in, all the 115 \nvariants in 36 data types. Summary: to compile one source file, a context environment is built, and the \ncompiler produces the compiled Unit described earlier. The static-environment (statenv) part of the compiled \nUnit is saved for use in further compilations now, and a dehydrated copy is written to the binary file \n(along with the code Unit) for use in other compilations tomorrow. On another day, an equivalent context \nenvironment is built; the binary file is read back in; and the statenv is dehydrated using the context \nenvironment. The dehydrated environment will then be combined with the exported environments of other \nmodules to form contexts for loading other binary files, or compil\u00ading other source files. 5 Type-safe \nlinkage Once the modules have been compiled (or recompiled) they must be linked together. A classical \nlinker re\u00adsolves names that refer to machine code addresses and global data addresses (with link-time \nstatic allocation). A classical linker does not maintain any type informa\u00adtion, so it is up to some other \npart of the programming environment (typically the programmer) to ensure that the modules linked together \nare compatible. For example, a C programmer may neglect to include some .h/ .C file dependent y in the \nmakefile. Then when a .h file is altered to change the parameters of some procedure ~, a client module \ncalling ~ with the old arguments might not be recompiled. This error will not be detected by the linker. \nA type-safe linker, on the other hand, ensures that the compiled implementations it is linking together \nhave a consistent view of their interfaces[14]. Natu\u00adrally, we have implemented a type-safe linker. Rolllins \nhas implemented an automatic dependency analyzer whose output is used to control order of evaluation, \npreventing makefile bugs [15]. If the dependency an\u00adalyzer is not used, or if it should fail, our linker \nwill detect any inconsistency. Recall from section 3 that the linkage of dynamic values is not by names, \nbut by pids which label iim\u00adported/exported values. An exported pid could be con\u00adstructed in several \nways: 1.The pid could be a simple timestamp (aug\u00admented with host and process identifiers to ensure uniqueness). \nThis will lead to unnecessary recom\u00adpilation; when a module is edited and recompiled without changing \nits interface, a new timest almp (pid) will be generated, and the dependent mod\u00adules must then be recompiled. \n2 The pid could be made by hashing source code (ex\u00adclusive of comments) of the module together with the \npids of the imported interfaces. This approach was taken in the Cedar/Mesa system[14]. Trivial changes \nto the source code (editing the comments, for example) will not cause the pid to change, and then the \ndependent modules will not need recom\u00adpilation. 3. The pid can be a hash of the exported static in\u00adterface \n[22, 12]. Then any modification of the im\u00adplementation module that does not affect its ex\u00adport interface \nwill yield the same pid. Bugs can be fixed, and algorithms can be changed, without causing a cascade \nof recompilation. Either of the latter two kinds of pid can be called intrinsic because they are independent \nof (time, place) context. The last alternative is obviously preferable. We use intrinsic pids, though \nour system would ac\u00ad commodate timest amps. A system using intrinsic pids will naturally use cutofl recompilation \navoiding many unnecessary recompila\u00ad tion. Cutoff recompilation can be implemented ~21] by comparing \nthe new compiler output (describing the export interface) with the previous one; but with the use of \nintrinsic pids, it is unnecessary to save (or compare) the previously compiled version because we compare \nhash values and not full environments. There is always the possibility of collision: two clif\u00adferent \nstatenvs could hash to the same value. Be\u00adcause we use a good hash function (a CRC of 128 bits) this \nshould happen only once for every 212s pairs of statenvs. A very large software system would have perhaps \n213 pids, or 226 pairs, so the probability of collision is 2-102. But if this is not good enough, IRM \ncan use intrinsic (hash) pids for software development, and switch to timestamp-bssed pids for pre-production \nverification. Intrinsic pids The more precisely the intrinsic pids express only the exported interface, \nand the less they are dependent on other aspects of the implementation modules, the fewer unnecessary \nrecompilation will be done. The intrinsic pids are computed by hashing the ex\u00adported static environment. \nWe do a prefix-order traver\u00adsal of the stat env. For each node, of some datatype with n variants, we \nemit an integer between 1 and n de\u00adpending on the variant. We take appropriate care to avoid doing a \ntree traversal of what is actually a DAG. When we come to a pid (either internal or imported from somewhere \nelse and here being re-exported) we emit the pid as an integer. This produces a sequence of integers, \nwhich we then hash by a CRC algorithm to produce the export static pid for this compilation unit. The \nexport pids of the k dynamic objects produced by this unit are derived from the static pid by adding \n1 through k respectively to the export static pid. Note, however, that the static environment contains \nall the entities to be exported; these entities cent ain their own pids; these pids must be derived from \nthe result of the hash; and yet these entities are traversed in order to compute the hash. This paradox \nis resolved by having the elaboration (semantic analysis) phase of the compiler give all enti\u00adties provisional \npids. These are grist for the hasherl and they are replaced by real pids after the CRC is computed. In \nthe course of a compilation, many substructures and types are created with provisional pids, but only \nsome of these are exported. The hasher should not hash the provisional pids directly, for this will make \nthe hash value dependent on the number of hidden internal structures and types used in the implementation \nof the module. Instead, it must alpha-convert them: the hasher keeps a record of which provisional pids \nhave been seen, and uses the hssh value of n for the nth distinct pid seen thus far. Hashing,andreplacement \nof provisional pids, are done before the dehydration phase described in sec\u00adtion 4. Performance Look \nhow many passes we are taking over the export environments! e Hash the environment to compute intrinsic \npids; Dehydrate: replace external references with stubs;  Later, rehydrate: replace stubs with external \nref\u00aderences.  We measured the time taken by these phases in a 32-minute compilation of 65,000 lines.5 \nThe mea\u00adsured time for hashing was O.0 seconds (modulo clock-tick resolution 0.01 second) and for dehydra\u00adtion/rehydration \nwas 20 seconds (i.e., lyo overhead). We believe this 20 seconds could be very substantially reduced by \nreplacing some of the linear searches we use to identify previously seen nodes (to preserve DAG structure). \nStatic pids The imports pid list of a unit expresses the dynamic dependence of the unit: what imported \nvalues are re\u00adquired for execution. It would be nice if this list com\u00adpletely completely summarized the \ndependence of the unit on its environment. But some dependence have no dynamic component: type and signatures \nhave only a static effect. Thus, to implement proper cutoff re\u00ad compilation, IRM needs more information. \nTo characterize a unit s exported static information, it is sufficient to hash the exported statenv into \na single st atic-pid. In the binary file for each unit Al, the IRM system writes the st atic-pids of \nall imported units and the exported static-pid. When the static exports of a compilation unit change, \nits new static-pid will be dif\u00adferent. Then, when examining a dependent binary file for currency, IRM \nwill be able to detect that it imports some st atic-pid that is no longer exported anywhere. It is IRM \ns responsibility to manage static-pids. IRM, not the compiler, calculates the list of imported units \nand assembles the corresponding list of static\u00adpids. To make the compiler keep track of imported units \nwould greatly complicate the lookup functions for types and values throughout the compiler. In contrast, \nthe compiler does produce the dynamic imports list. This turns out to be easy to do: the lambda-expression \n(intermediate representation) pro\u00adduced by the front end of the compiler explicitly con\u00adt ains these \npids as free variables. The dynamic pids suffice for type-safe linkage. Any inconsistency of static imports/exports \nthat has no ef\u00adfect on dynamic imports/exports cannot cause execu\u00adtion to go wrong. 6 5on a DECstation \n5000/240 6 For ~xmple, ~M shotid recompile unit 2, below, if tit 1 changes to type t=real. But even if \nunit 2 were not recompile@ execution would not go wrong. 1. type t =int 2. local type u=t in val i=5 \nend  6 -Compilation interface Previous make systems for SML/NJ accessed the compiler through the use \nprocedure, which takes a file name, compiles the file, and side-effects the global envi\u00adronment by extending \nit with static/dynamic bindings. The primitives described in this paper are intended to be used with \na separate compilation manager that understands source files, binary files, dates, directo\u00adries, file-dependency \nDAGs, version, makefiles, and li\u00adbraries. IRM [10], summarized in section 8, is such a system. The SML/NJ \ncompiler makes available an interface with one component for each phase of compilation: parsing, elaboration \n(semantic analysis) and hashing, code generation, and code execution. Also provided are several static-environment \nmanipulation primitives: layering, index mapping, dehydration/rehy dration, and so on. The compiler itself \ndoes not know the format of a bi\u00adnary file. Instead, IRM takes the compiled Unit, dehy\u00addrates the statenv, \nand writes/reads the statenv, code, imports, exports, import-static-pids, export-static-pid to/from binary \nfiles in its own format. The interactive top-level Before the development of IRM and the mechanisms described \nin this paper, the interface to the SML/NJ compiler was an interactive read-eval-print loop with the \nprovision for compiling some units from files (the use command). Of course, this provided no separate \ncompilation whatsoever. We have re-engineered the read-eval-print loop (and the use command) to call \nupon the compilation prim\u00aditives described in this paper. Thus, there is only one interface into the \ncompiler proper. Consequently, units compiled under IRM are compatible with units com\u00adpiled in the top-level \nloop. (Of course, IRM cannot analyze the static dependence of source code typed in interactively.) And \nit should be easy (though we have not yet implemented it) for the interactive loop to load binary files \nwithout dragging in the full IRM compila\u00adtion management system.  7 The Visible Compiler The Lisp community \nhas long enjoyed access to some internal functions of an interactive system (e.g. eval) using the big \nball of mud paradigm. We show how to do this cleanly in a strongly typed, modular system, based on the \nmodel of compilation, linkage, and en\u00advironments described here. We can selectively expose internal modules \nof the compiler simply by adding their static interfaces to the user-level environment. Standard ML \nof New Jersey compiles ML code into machine code segments. The ML process can then ex\u00adecute these segments \nby putting them in memory (in the heap) and jumping to them. Linkage is achieved, as explained above, \nby looking up imported values in a dynamic environment and passing these values as ar\u00adguments to the \nmachine code function. In the interactive system, the user s program exe\u00adcutes in the same process as \nthe compiler itself. This is the usual way of building an interactive compilation environment (and would \nnot be true of stand-alone ML applications). Our compiler is, of course, compiled using the sepa\u00adrate \ncompilation system described in this paper. The compiled compiler consists of a set of binary files, \neach with machine code, static environment, pids, etc. To link the compiler, the runtime system has a \nprimitive bootstrap loader (implemented in C) that under\u00adstands only the machine code and import/export \nlpids of the binary files. The bootstrap loader never reads the static environment component of the files. \nFor each compilation unit in succession, the loader looks up val\u00ad ues in a simple dynamic environment \nand applies ma\u00ad chine code functions to them. Once the compiler is running, it wants to make shared \nlibrary modules (such as List, String, etc.), and perhaps its own internal modules, accessible to the \nuser program in a type-safe way. To accomplish this, the ML function that initializes the environment \nfor the interactive top-level loop re-reads the binary files, this time loading only the static environments \nand ignoring the machine code. These incremental static environ\u00adments are dehydrated and composed to \nbuild a static environment matching the dynamic environment con\u00adstructed by the bootstrap loader, which \nis passed as an argument to the ML startup function. The result\u00ading combined environment contains all \nof the internal modules of the compiler, but it is normally filtered to provide only a limited selection \nof modules to be shabred by the compiler and user programs. Most of these shared modules are libraries \nproviding built-in types or operating system interfaces, but others provide access to compiler control \nflags and to internal compiler types and operations, such as syntax trees, environments, and the compilation \nphases. This window into the compiler has many applica\u00ad tions:  It supports shared fundamental libraries: \nI,ist, String, 1/0, etc.  The separate compilation manager runs as an or\u00addinary user program, but calls \nupon certain phases of the compiler. For example, it uses the parser to implement its own file dependent \ny analysis.  It s easy to implement a compiler hacker s testbed: one can add replacement modules to \na running interactive compiler in which every module of the compiler is visible and usable. It provides \na convenient basis for various forms of metaprogramming , including the synthesis and evaluation of ML \ncode by user programs. IRM Gene Rollins of Carnegie-Mellon University has imple\u00ad mented an Incremental \nRecompilation Manager (IRM) [15], which is a client of the low-level tools described in this paper. IRM \nruns aa an ordinary application pro\u00ad gram in the SML/NJ interactive system, calling upon the compiler \nthrough the Visible Compiler interface. We will only briefly summarize the capabilities of the IRM system. \nA user of IRM specifies groups of source files. A group may refer to other groups which it uses as libraries. \nThe source files may be ML files or programs in other languages (Lex, Yacc, etc.) which are processed \nby compilers other than SML/NJ. IRM automatically analyzes the dependencies be\u00adtween compilation units. \nFor ML source files, it calls upon the parser of the SML/NJ compiler to scan files for free variables \n(imported names). For each file foo.sml, IRM keeps a cached version of the dependency information in \n. d. foo. sml, and uses timestamps to de\u00adtermine that the dependent y information is up to date. However \nintrinsic pidsnot timestamps are used to ensure which files must be recompiled. IRM performs cutoff recompilation, \nnot cascading recompilation. IRM provides interfaces at several levels. The simplest highest level interface \nis a simple make\u00adfile listing the source files (by file name) and the names of imported libraries (by \nmentioning the name of the li\u00adbrary s make file). Because dependencies are automat\u00adically computed by \nIRM, the makefile contains only an unordered list of file names. This system is simple and convenient. \nA lower-level interface is via ML structures and sig\u00adnatures of components of the IRM system. Users with \nspecial needs (for example, a theorem prover whose sources are not kept in files; or a different style \nof li\u00adbrary manager) can customize the IRM system, which has been implemented in a pleasingly modular \nstyle specifically to support such users. 9 Cross-compilation The Visible Compiler requires the compiled \ncode of the compiler to use the same representation for dehydrated environments as the compiler that \nproduced it. The IRM system calls upon the same functions to compile 71 -. top-level interactive commands \n(telling IRM to perform make, for example) as for generating binary files. These restrictions mean that \ncross-compilation (production of a new version of the compiler using an old version, or running the compiler \non architecture A to generate code for architecture l?) should be impossible. A compiler unable to do \nthese forms of cross\u00adcompilation is at an evolutionary dead end. We do sup\u00adport cross compilation, in \nthe following ways. In gen\u00aderating code for a different target architecture, or for the same architecture \nwith different parameter-passing conventions or garbage-collector interface (etc.) we do not use the \nVisible Compiler paradox. The old compiler loads all the binary files of the new compiler as a user program; \nonly fundamental libraries such as buffered 1/0 are shared. The old compiler s phases are accessible \nto IRM as a standard structure Compiler with signature COMPILER. The new compiler is loaded as a structure \nNew-Compiler matching the same signature. The IRM sys\u00adtem takes the compiler aa a functor parameter; \nso it can be applied to NewCompiler. Now commands to IRM are compiled using the old compiler; but IRM \nit\u00adself calls upon New Compiler to generate binary files. (Of course, the old interactive system should \nnot at\u00ad tempt to execute these files!) The many constraints between old and new compilers make cross \ncompilation difficult to under\u00adstand, but a simple axiomatization makes things much more manageable [2]. \n10 Alternatives Dependencies bet ween implementations arise from one implementation module mentioning \nanother directly. If the interfaces were fully expressive (as they are in Modula 2) this would not matter; \nbut in Standard ML, information about the identity of types can leak through the signatures in other \nwords, the signa\u00adtures do not enforce abstraction. This is necessary, as explained in the introductory \nsections, to support the definition of higher-order modules with a single notion of signature mat thing. \nMany of the difficulties which we have had to address in our work stem from these dependencies between \nim\u00adplement ation modules. Here we describe some alterna\u00adtive solutions that avoid these dependencies. \n10.1 The fully-functorized style The first solution is to adopt a programming style called the fully \nfunctorized style in which all exter\u00adnal references are made opaque by making the external modules into \nfunct or parameters. This can be done in the existing Standard ML language; but in order to ab\u00adstract \nover functors as well as structures, it s necessary to use higher-order functors, as described by Tofte \nand MacQueen [24, 18]. The problem with this approach is that it is very cumbersome for the programmer: \nthe funct or parame\u00adters must become complex enough to express the deeply nested dependency DAGs that \nshould be handled au\u00adtomatically anyway. The DAGs must be explicitly and redundantly expressed in each \nimplementation module. 10.2 Eliminate transparent signature mat thing Another approach is to make all \nsignature matching opaque, so that no information about the identity of types leaks through the interface \n(export ed types are abstract). To make this practical, there must be a way to override this abstraction \nwhen we want to export a concrete t ype but the overriding should be explicit in the signature, so that \nall dependencies are on inter\u00adfaces. Versions of the Standard ML module system with opaque signature \nmatching have been proposed [11, 16]. But it appears that they are not true higher-order module systems, \nbecause dependencies that can be ex\u00adpressed at first order cannot be expressed at higher orders. The \nML community is currently debating this issue. (See also CardeHi s proposed framework[6].) A similar \neffect can be achieved in Standard ML by adding type abbreviations to signatures, and using ab\u00adstractions \n(with opaque interfaces) instead of struc\u00ad tures (with partially transparent interfaces). But this is \neven less powerful than the proposals mentioned in the previous\u00ad paragraph.  11 Conclusion The separate \ncompilation mechanism described in this paper, with the IRM system implemented by Rollins ~15]: are regularly \nused by students in an un\u00addergraduate compiler course at Princeton. The com\u00adbined IRM/Visible-Compiler \nsystem is robust enough that we rely on it to compile the SML/NJ compiler itself a large system comprising \nover 200 compilation units. The interfaces that express this compilation/linkage architecture are remarkably \nsimple and easy to use. This is due to the functional style of environment ma\u00adnipulation, and the fact \nthat executable-code linkage is expressed by function application. The ML module language has long been \nrecognized as powerful ~but this work shows that it is also practical for organizing software systems. \nThe fact that ML can support only incremental recompilation instead of true separate compilation is unfortunate, \nbut we would not [16] Xavier Leroy. Manifest types, modules, and separate compilation. In Twenty First \nAnnual ACM Symp. on Principles of Prog. Languages, pages 109 122, New York, Jan 1994. ACM Press. [17] \nDavid B. MacQueen. The implementation of Standard ML modules. In ACM Conf. on Lisp and Functional Programming, \npages 212-23, New York, 1988. ACM Press. [18] David B. MacQueen and Mads Tofte. A semantics for higher-order \nfunctors. In Proc. European Symposium on Programming (ESOP 94), page (to appear), April 1994. [19] Robin \nMilner, Mads Tofte, and Robert Harper. The Definition of Standard ML. MIT Press, Cambridge, MA, 1990. \n[20] Greg Nelson, editor. Systems Programming with Modula-3. Prentice Hall, Englewood Cliffs, NJ, 1991. \n[21] Mark Rain. Avoiding trickle-down recompilation in the Mary2 implementation. Soflware-Practice and \nExperience, 14(12):1149-1157, 1984. [22] Ed Satterthwaite. e-mail, October 1993. Personal com\u00admunication \ndescribing work done on the Cedar system at Xerox PARC in 1979-83 by Butler Lampson, Eric Schmidt, and \nSatterthwaite, but never published. [23] Zhong Shao and Andrew W. Appel. Smartest recom\u00adpilation. In \nProc. Twentieth Annual A CM SIGPLAN-SIGA CT Symp. on Principles of Programming Lan\u00adguages, pages 439 \n45o. ACM Press, 1993. [24] Mads Tofte. Principal signatures for higher-order pro\u00adgram modules. In Nineteenth \nAnnual A CM Symp. on Principles of Prog. Languagq pages 189 199, New York, Jan 1992. ACM Press. [25] \nJohn von Neumann. First draft of a report on the EDVAC. In William Aspray and Arthur Burks, edi\u00adtors, \nPapers of John won Neumann on Computing and Computer Theory, pages 17 82. The MIT Press, Cam\u00adbridge, \nMassachusetts, 1945. want to solve this problem by limiting power of pa\u00adrametrized modules, as is done \nin Modula-3, Instead, perhaps the research summarized in section 10.2 will eventually provide a better \nsolution.   References Rolf Adams, Walter Tichy, and Annette Weiner. The cost of selective recompilation \nand environment pro\u00adcessing. ACM TOSEM, 1994. to appear. Andrew W. Appel. Axiomatic bootstrapping: A \nguide for the compiler hacker. Technical Report CS-TR-451\u00ad94, Princeton University, March 1994. Andrew \nW. Appel and David B. MacQueen. A $Han\u00addard ML compiler. In Gilles Kahn, editor, Functional Programming \nLanguages and Computer Architecture (LNCS 274), pages 301-24, New York, 1987. Springer-Verlag. J. G. \nP. Barnes. Programming in Ada plus Language Reference Manual. Addison-Wesley, New York, 1991. Amber Benson \nand Gary Aitken. 01 Programmer s Guide, June 1993. Luca Cardelli. Program fragments, linking, and lmod\u00adularization, \nApril 1993. S. 1. Feldman. Make a program for maintaining com\u00adputer programs, In Unit Programmer s Manual, \nSev\u00adenth Edition, Volume 2A. Bell Laboratories, 1979. Jurg Gutknecht. Separate compilation in moduJa-2: \nAn approach to efficient symbol files. IEEE Software, pages 29-38, November 1986. Samuel P. Harbison. \nModula-3. Prentice Hall, Engle\u00ad wood Cliffs, NJ, 1992. Robert Harper, Peter Lee, Frank Pfenning, and \nEu\u00adgene Rollins. Incremental recompilation for standard ml of new jersey. Technical Report CMU-CS-94-1 \n16, Department of Computer Science, Carnegie-Mellon University, February 1994. Robert Harper and Mark \nLillibridge. A type-theoretic approach to higher-order modules with sharing. In Twenty First Annual ACM \nSymp. on Principles of Prog. Languages, pages 123-137, New York, Jan 1994. ACM Press. Bill Kalsow and \nEric Muller. SRC Modula-3 Version 1.6 manual, February 1991. Andrew Koenig. Personal communication, \n1993. Butler W. Lampson and Eric E. Schmidt. Organiz\u00ading soft ware in a distributed environment. In ACM \nSIGPLAN 83 Symposium on Programming Language Issues in Software Systems, pages 1-13, June 1983. Peter \nLee, Gene Rollins, and Robert Harper. In\u00adcremental recompilation for Standard ML. Technical Report (in \npreparation), Carnegie-Mellon Univexsit y, 1993. [1] [2] [3] [4] [5] [6] [7] [8] [9] [10] [11] [12] [13] \n[14] [15]  \n\t\t\t", "proc_id": "178243", "abstract": "<p>Languages that support abstraction and modular structure, such as Standard ML, Modula, Ada, and (more or less) C++, may have deeply nested dependency hierarchies among source files. In ML the problem is particularly severe because ML's powerful parameterized module (functor) facility entails dependencies among implementation modules, not just among interfaces.</p><p>To efficiently compile individual modules in such languages, it is useful (in ML, necessary) to infer, digest, and cache the static environment resulting from the compilation of each module.</p><p>Our system provides a simple model of compilation and linkage that supports incremental recompilation (a restricted form of separate compilation) with type-safe linkage. This model is made available to user programs in the form of a set of internal compiler modules, a feature that we call the &#8220;visible compiler&#8221;. The chief client of this interface is the IRM incremental recompilation manager from CMU.</p>", "authors": [{"name": "Andrew W. Appel", "author_profile_id": "81100498630", "affiliation": "Princeton University", "person_id": "PP14174176", "email_address": "", "orcid_id": ""}, {"name": "David B. MacQueen", "author_profile_id": "81100334808", "affiliation": "AT&T Bell Laboratories", "person_id": "P62710", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/178243.178245", "year": "1994", "article_id": "178245", "conference": "PLDI", "title": "Separate compilation for Standard ML", "url": "http://dl.acm.org/citation.cfm?id=178245"}