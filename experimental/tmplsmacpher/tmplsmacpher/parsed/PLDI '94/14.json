{"article_publication_date": "06-01-1994", "fulltext": "\n The Program Structure Tree: Computing Control Regions in Linear Time Richard Johnson David Pearson Keshav \nPingali rjohnsont@cs.comell.edtt pearson@cs.cornell.edtt pirtgaM@cs.comell.edu Department of Computer \nScience Cornell UniversiQ, Ithaca, NY 14853 Abstract In this paper, we describe the program structure \ntree (PST), a hierarchical representation of program structure based on single entry single exit (SESE) \nregions of the control flow graph. We give a linear-time algorithm for finding S13SE regions and for \nbuilding the PST of arbitrary control flow graphs (including irreducible ones). Next, we establish a \nconnection between SESE regions and control dependence equivalence classes, and show how to use the algorithm \nto find control regions in linear time. Finally, we discuss some applications of the PST. Many control \nflow algorithms, such as construction of Static Single Assignment form, can be speeded up by applying \nthe rdgorithms in a divide-and\u00adconquer style to each SESE region on its own. The PST is also used to \nspeed up data flow analysis by exploiting sparsity . Experimental results from the Perfect Club and SPEC89 \nbenchmarks conftrrn that the PST approach finds and exploits program structure. Introduction The contributions \nof this paper are the following. In Section 2, we introduce the program structure tree (PSq which is \na hierarchical representation of the control structure of a program. Nodes in this tree represent single \nentry single em-t(SESE) regions of the program, while edges represent nesting of regions. The PST is \ndefined for all control flow graphs, including irreducible graphs. In Section 3, we give an O(E) algorithm \nfor finding SESE regions. This algorithm works by reducing the problem to 1~i~ research was supported \nby an NSFPresidentiat hung hvestigator award CCR-8958543, NSF grant CCR-9008526, ONR grant NOOO14-93\u00ad1-0103, \nand a grantfrom Hewlett-Packard Corporation. David Pearson is supported by a Fannie and John Hertz Fellowship. \nPermission to copy without fee all or part of this material is granted provided that the oopies are not \nmade or distributed for direct commercial advantage, the ACM copyright notice and the title of the publication \nand its date appear, and notice is given that copying is by permission of the Association of Computing \nMachinery. To copy otherwise, or to republish, requiras a fae ancf/or specific permission. that of determining \na simple graph property that we catl cy\u00adcle equivalence: two edges are cycle equivalent in a strongly \nconnected component iff for all cycles C, C contains either both edges or neither edge. We give a fast, \nlinear-time algo\u00adrithm based on depth-tirst search for solving the cycle equiv\u00adalence problem, thereby \nfinding SESE regions in linear time. This algorithm runs very fast in practice for example, our empirical \nresults show that it mns faster than Lengauer and 12wjan s atgorithm for finding dominators &#38;T79]. \nWe use this algorithm to build the PST for arbitrary flow graphs in O(E) time. In Section 4, we give \nexperimental results that characterize the structure of the PST in standard bench\u00admarks such as Perfect \nClub, SPEC, and Linpack programs. As one would expect, the PST is usually broad and shallow roughly 97% \nof all SESE regions have a nesting depth of 6 or less. In Section 5, we apply the cycle equivalence algorithm \nto finding control regions in O(E) time, ho nodes are said to be in the same control region if they have \nthe same set of control dependence lJ?OW87]. Previous al\u00adgorithms for this problem are either restricted \nto reducible flow graphs [Ba192] or have O(EIV) complexity [CFS90]. Control region information is useful \nfor problems such as in\u00ad struction scheduling for pipelined machines [GS87]; there\u00ad fore, our linear-time \nalgorithm for region determination is of wide interest. The PST is a tool which can enhance the performance \nof many program analysis algorithms. Each SESE region is a control flow graph in its own right, so any \nprogram analysis algorithm can be applied directty to it. The partial results from each SESE region can \nbe combined using the PST to give the result for the entire procedure. Provided that the combining is \nnot overly expensive, this divide\u00adand-conquer style of applying analysis algorithms can be advantageous \nsince the PST is a natural data structure for exploiting global structure (nesting), local structure \n(of each SESE region), and sparsity. We make these points in Sec\u00adtion 6 by showing how the PST cart be \nused in three prob\u00adlems: conversion to SSA form, data flow analysis and dom\u00adinator computation. We also \ndiscuss possible applications of the PST to parallel and incremental program analysis. SIGPIAN 94-6/94 \nOrlando, Florida USA @ 1994 ACM 0-89791 -662-x/9410006..~.5O 2 Single entry single exit regions and the \nprogram structure tree In the literature, the term single entry single exit region is not used consistently \n there appear to be several re\u00adlated constructs aliased to this term [Kas75, Va178, TV80, GPS90]. Therefore, \nwe begin this section with a formal definition of single entry single exit regions as used in this paper. \nThis definition is motivated in part by considerations of control dependence, as will be made precise \nin Section 5. We then show that single entry single exit regions can be organized into a tree called \ntheprogram structure tree (PST). Figure l(a) shows a control flow graph with its single entry single \nexit regions marked. Note that each SESE region is enclosed by a pair of control flow graph edges called \nthe entry and exit edges respectively. SESE regions are either nested, sequentially composed, or disjoint. \nWhen regions are sequentially composed, the exit edge of one region is also the entry edge of the following \nregion. Figure l(b) shows the PST of the control flow graph of Figure l(a). The PST captures the nesting \nrelationship of SESE regions; chains of sequentially composed SESE regions, such as regions c, d and \ne, are grouped in the PST. 2.1 Defining single entry single exit regions First, we recall a few standard \ndefinitions. Definition 1 A control flow graph G is a graph with dis\u00adtinguished nodes start and end such \nthat every node occurs on some pathfrom start to end. start has no predecessors and end has no successors. \nDefinition 2 A node x is said to dominate node y in a directed graph if every pathfrom start toy includes \nx. A node z is said to postdominate a node y if every path from y to end includes x. By convention, a \nnode dominates and postdominates it\u00adself. The notions of dominance and postdominance can be extended \nto edges in the obvious way. Single entry single exit regions can now be defined as follows. Definition \n3 A SESE region in a graph G is an ordered edge pair (a,b) of distinct controlj?ow edges a and b where \n1. a dominates b, 2. b postdominates a, and 3. every cycle containing a also contains b and vice versa. \n We refer to a as the entry edge and b as the exit edge of the SESE region. The first condition ensures \nthat every path from start into the region passes through the region s entry edge, a. The second condition \nensures that every path from inside the region to end passes through the region s exit edge, b. The first \ntwo conditions are necessary but not sufficient to characterize SESE regions: since backedges do not \nalter the dominance or postdominance relationships, the first two conditions alone do not prohibit backedges \nentering or exiting the region. The third condition encodes two constraints: every path from inside the \nregion to a point above a passes through b, and every path from a point below b to a point inside the \nregion passes through a. For future reference, we define the notion of cycle equiv\u00adalence. Definition \n4 Edges a and b are said to be edge cycle equiv\u00adalent l~eve~ cycle containing a contains b, and vice \nversa. Similarly, two nodes are said to be node cycle equivalent iff every cycle containing one of the \nnodes also contains the other. If (a, b) is a SESE region and (b, c) is a SESE region, then (a, c) is \na SESE region as well. Therefore, a graph with E edges can have O(E2) SESE regions for example, every \nedge pair in a linear sequence of nodes encloses a SESE region. However, we have never found any use \nfor complete enumeration of all SESE regions of a graph. Instead, for each edge e in the graph, we want \nto find the smallest SESE regions, if they exist, for which e is an entry edge or an exit edge. We will \ncall these the canonical SESE regions associated with e. We express this more formally as follows. Definition \n5 A SESE region (a, b) is canonical provided b dominates b!for any SESE region (a, b ), and  a postdominates \nat for any SESE region (a , b).  In straightline code, the region between any two points is single entry \nsingle exit we will ignore these trivial regions and focus on SESE regions in the block-level CFG, in \nwhich straightline code sequences have been coalesced into basic blocks. Every edge in the block level \nCFG is either between a control operator (switch or merge) and a basic block, or between two control \noperators.  2.2 The program structure tree We now consider the nesting structure of canonical SESE regions \nand their organization into the program structure tree. Definition 6 A node n in a graph G is contained \nwithin the SESE region (a, b) ifa dominates n and b postdominates n. Intuitively, node n is between a \nand b in the graph. This definition can beextendedin the obvious way to containment of SESE regions. \nIkorem 1 describes how canonical SESE regions in a graph are related. Theorem 1 If RI and R2 are two \ncanonical SESE regions of a graph, one of the following statements applies. 1. RI and R2 are node disjoint. \n 2. R1 is contained within Rz or vice versa.  start }  ,,----------------------------------------------------- \na ~;b. ..~>F . . ..~ .................... :: . ..................: ::,-------\u00ad   ,,,?1 ;; ~ >J. \n..-j. . ...=. : j :---j; ;::ge /\\ :: :[h !  ::: v;; : ,---\u00ad,,,,,.-.; ,.,,.:---,, /\\ :;;~to : :J *,, \n:::....,..,,-., :: v \\] j; : ,--------\u00ad---..., Hi 1: ,:::::::::j~; L:::::: #. ;; ::6? : .. ----------, \nt, ~----------,: ~ -------. . . . . . -------\u00ad ,-$, J ij end (a) control flow graph with SESE regions \n Figure 1: A control flow graph In other words, canonical SESE regions cannot have any partial overlap \n if two regions have any nodes in common, they are either nested or in tandem? This is obvious in the \ncase of structured programs. For general control flow graphs, the required result may be proved as follows. \nProofi Suppose distinct canonical SESE regions (u, b) and (s, t) both contain a node n. Since a and s \nboth dominate n, they are ordered by dominance. Without loss of generality, assume a dominates s. SitniIarly, \nb and t both postdominate n, so they axe ordered by postdominance. If b postdominates t, then (s, t) \nis contained within (a, b). Otherwise, t postdominates b. There are now three cases to conside~ in each \ncase we derive a contradiction. 1. b ands are the same edge. Note that an edge cannot both dominate and \npostdominate a node. Since b postdomi\u00adnates n ands dominates n, this case cannot happen. 2. b ands me \ndistinct ands dominates b. Since a dominates s ands dominates b, there is a b-free path from start to \na to s. Therefore, every path from s to end must contain bsince otherwise we would have a b-free path \nfrom a tos to end which contradicts the fact that bpostdominates a. Therefore, b postdominatess. Similarly, \ns postdominates a; otherwise there is a s-free path from start to a to b to end, which contradicts the \nfact thats dominates b.  Every cycle through b passes through a and therefore contains a path from a \nto b; this path must contains since a dominatess which dominates b. Therefore, every cycle through b \npasses through s. A similar argument shows that every cycle through s must contain t and therefore b. \nTherefore, (s, b) is a SESE region; sinces postdominates a, it follows that (a, b) is not canonical which \nis a contra\u00addiction. ZNotice hat his property may not be true for SESE regions that am not canonical. \n(b) pm-structure tree and its program structure tree 3. b ands are distinct ands does not dominate b. \nThen there is a s-free path from start to b. This means thats must postdominate ~ otherwise, we have \na path from start to bto end which passes through t (since tpostdominates b) but nots, violating the \nassumption thats dominates t. Since b postdominates n, s atso postdominates n. Buts dominates n. Since \nan edge cannot both dominate and postdominate a node, this is a contradi ction. o In Figure 1, regions \nb and c are disjoint, regions a and b are nested, and regions ~ and g are sequentially composed. It follows \nfrom Theorem 1 that SESE regions can be or\u00adganized as a tree. Each node in this tree represents a SESE \nregion. The parent of a region is the closest containing re\u00adgion, and children of a region are all the \nregions immediately contained within it. We call this the program structure tree (PST). We now show how \nthe PST can be determined in O(E) time. 3 Building the PST in linear time The atgorithm has two steps \n first, find SESE regions and second, organize canonical SESE regions into the PST. 3.1 Cycle equivalence \nis adequate To find SESE regions, it is convenient to reduce the three conditions for SESE regions to \nthe single property of cycle equivalence in a related graph. Theorem 2 In a controlflow graph G, edges \na and b enclose a single entry single em t region lf and only lJ a and b are cycle equivalent in the \ngraph formed from G by adding an edgefrom end to start. Proofi [ + ] Suppose a and 11enclose a SESE region \nin control flow graph G. By definition, a and ZIare cycle equivalent in G; we must show they are cycle \nequivalent in S, the strongly connected graph formed by adding edge end + start to G.  rn - ee Consider \nany cycle in S not in G. Such a cycle is formed by a path from start to end together with the backedge \nend + start. E this cycle contains a, then it atso contains b since b postdominates a. Similarly, if \nthis cycle contains b, then it atso contains a since a dominates b. Therefore, a and b are cycle equivalent \nin S. [ + ] Suppose a and b are cycle equivalent in S. Then a and b are cycle equivalent in G since every \ncycle in G is also a cycle in S. Now consider any path P from start to end containing both a and b; such \na path exists since every edge occurs on some path from start to end, and since a and b me cycle equivalent \nin S. Without loss of generality, assume a occurs first on this path. There can be no b-bee path from \na to end, since this would yield a cycle in S containing a but not b (using the portion of P from start \nto a, the b-free path to end, and the backedgefiom end to start). Therefore, bpostdominates a, and the \nportion of P from the last occurrence of b to end is a-free. There can be no a-tlee path horn start to \nb, since this would yield a cycle in S containing b but not a. Therefore a dominates b. a 3.2 From directed \nto undirected graphs Further simplification is possible because of the rather sur\u00adprising result that \ncycle equivalence in a strongly connected graph remains the same when edge directions are removed. This \nresult allows us to find cycle equivalence classes in the undirected Multigraph corresponding to a strongly \ncon\u00adnected graph. The advantage of working with undirected graphs is that algorithms based on depth-lint \nsearch are sim\u00adplified in undirected graphs since cross edges and forward edges are eliminated. Theorem \n3 Let S be a strongly connected component, and let U be the undirected multigraphformed from S by remov\u00ading \nedge directions. Edges a and b are cycle equivalent in S ~and only #the corresponding undirected edges \na and b are cycle equivalent in U. Proofi [ + ] We show that if edges a and b are not cycle equivalent \nin U, then corresponding edges a and b are not cycle equivalent in S. Without loss of generality, assume \nthere is at least one cycle in U containing a but not b . Each edge on such acycle hasan associateddirection \nin S. Adjacent edgesin the cycle either have the ssme direction or opposing directions; if adjacent \nedges have opposing directions, we say there is a direcfion change at the node between these edges. Choose \nC to be a cycle in U containing a but not b such that this cycle has a minimum number of direction changes. \nIf C has no direction changes, then the corresponding edges in S form a duected cycle containing a but \nnot k Otherwise, C has some minimum, non-zero number of direction changes. Traversing C from a along \nthe direction of a, let x and y be the nodes on C where edge direction tirst changes and then changes \nback. Since S is strongly connected, there exists a directed path in S from x to y; let E be the corresponding \n Figure 2: Undirected cycle C and path E , with edge di\u00adrections shown undirected path in U (Figure 2). \nSuppose neither a nor b occur on E , and consider the cycle obtained by replacing the portion of C between \nz and y with path E . The resulting cycle contains a but not b and has fewer dmection changes than C \n, contradicting the assumption that C has a minimum number of direction changes. Otherwise, a and b may \noccur (perhaps severat times) on II . If the tirst occurrence on E is a , then the path from a to x atong \nC together with the path along E from z to the first occurrence of a corresponds to a directed b-tke \ncycle through a. Simikdy, if the last occurrence of either a or b on E is a , the path iiiong E horn \nthe last occurrence of a to y, together with the path from y to a along C , forms a b -free cycle through \na having fewer direction changes than C . Otherwise, the tirst and the last occurrence of either a or \nb on E are both bl. The path from b to y along E , y to z along C , and then z to b along E corresponds \nto a directed a-free cycle through b. [ ~ ] Suppose a and b are not cycle equivalent in S. Without loss \nof generality, there is a directed cycle in S containing a but not b. The corresponding undirected cycle \nin U contains a but not b , so a and b are not cycle equivalent in U. o 3.3 A slow algorithm for cycle \nequivalence Given a strongly connected graph S, let U be the undirected Multigraph formed by removing \nedge directions. Since U is connected, a depth-lirst traversal will yield a depth-first spanning tree, \nand the edges of U are divided into a set of tree edges and a set of backedges. Notice that any cycle \nin U must contain at least one backedge. We use this observation to recast the problem of cycle equivalence \nin terms of sets of backedges rather than sets of cycles. Definition 7 In any depth-jirst traversal of \nU, a bracket of a tree edge t is a backedge connecting a descendant of t to an ancestor of t. Now consider \nwhether two edges in U are cycle equiva\u00adlent. ho backedges cannot be cycle equivalent since the cycle \nformed from a backedge together with the tree path connecting its endpoints contains no other backedges. \nOn the other hand, a tree edge and a backedge or two tree edges may be cycle equivalent. The following \ntheorems establish conditions for detecting these equivalences. Theorem 4 A backedge b and a tree edge \nt are cycle equiv\u00adalent if and only if b is the only bracket of t. Proofi [ +-] Suppose b and t are cycle \nequivalent in U. Since b together with the tree path connecting its endpoints forms a cycle, b must be \na bracket of t. No other backedge can be a bracket oft, since such a backedge together with the tree \npath connecting its endpoints would form a cycle containing t but not b. [ -+=] Suppose b is the only \nbracket of t. Then b is the only backedge connecting a descendant oft toartancestor of t,and since every \ncycle must contain a backedge, any cycle through t must contain b. Any cycle through b is comprised of \nb together with a b-free path connecting b s endpoints. Any such path must contain t, since every Mee \npath to a descendant of t must pass through t. D The following lemma is needed to prove when two tree \nedges are cycle eqttivalenq the condition for equivalence and proof follow. Lemma 1 In a depth-jirst \nspanning tree of U, l~tree edges s and t have any bracket in common then they are ordered by the ancestor \nrelation in the tree. ProoE (by contradiction) Supposes and t are not ordered by the ancestor relation. \nThen no descendant ofs is a descendant oft and vice versa. Any bracket ofs connects a descendant of s \n(say node z) to art ancestor ofs; since z is not a descendant oft, this cannot be a bracket oft. o Theorem \n5 Tree edges s and t are cycle equivalent in U if and only if they have the same set of brackets in any \ndepth-first spanning tree of U. Proofi [ + ] We show that if hvo tree edges do not have the same set \nof brackets in a depth-tint spanning tree of U, they are not cycle equivalent. Suppose edge b is a bracket \nofs but is not a bracket of t.By the definition of brackets, s must occur on the tree path connecting \nthe endpoints of b, but tdoes no~ this tree path together with b forms a cycle containing s but not t. \n[-4=]Ifs and t have the same set of brackets, Lemma 1 asserts that they are ordered by the ancestor relation \nin the depth-tirst spanning tree. Without loss of generality, assumes is an ancestor oft. Any cycle through \ns must contain at least one backedge connecting a descendant ofs to an ancestor ofs. Let b be the first \nsuch backedge afters on the cycl~ note that alt nodes in the cycle betweens and b are descendants ofs. \nSince b is a bracket ofs it is also a bracket of t,and so tison the tree path between s and the lowe# \nendpoint of b, If the cycle path from .s to b does not contsin t, there must be some edge (p, q) on the \npath which bypasses t, i.e. p is an ancestor of tand g is a descendant 3~roughout thi5 section, we use \nvariations of high ~d l~w to ~fer m relative positions in the depth-first search tree. Higher locations \nare closer to root and have smatler DIN numbers. oft. However, both p and q are descendants of s. So \n(p, g) is a bracket of tbut is not a bracket ofs, a contradiction. The proof that every cycle containing \nt contains s is similar and is omitted. 1 During an undirected depth-first traversal, we can com\u00adpute \nthe set of brackets for each tree edge. When retreating out of a node, we form the union of bracket sets \nfrom the node s children, together with the set of backedges from the node to an ancestor, minus the \nset of backedges from a de\u00adscendant to the nod~ the result is the bracket set for the tree edge into \nthe current node. Intuitively, the set of brackets of a tree edge is a name for the edge s cycle equivalence \nclass; by comparing these sets, we find cycle equivalent edges. However, building and comparing sets \nis expensive, so the algorithm is inefficient. In the next section, we describe a compact naming scheme \nfor bracket sets that allows us to avoid building and comparing entire sets. 3.4 Compact names for sets \nof brackets Consider the graph shown in Figure 3(a) in which the depth\u00adtirst spanning tree is a simple \nchain and backedges corre\u00adspond to structured loops that are are either disjoint or nested within each \nother. For such graphs, it is easy to see that the set of brackets of an edge is uniquely named by the \ninnermost bracket of that edge, so the entire bracket set at each tree edge is not needed. Instead, we \ncan simply visit nodes in reverse depth-first order and maintain a stack of brackets. At each node, we \ndelete brackets that connect a descendant to the current node, and we add any brackets connecting the \nnode to an ancestor. Since the backedges are disjoint or properly nested, the deletions and insertions \nall occur at the top of the bracket stack. When retreating out of a node, the tree edge from its parent \nis labeled with the name of the topmost bracket in the bracket stack, after this traversal, tree edges \nwith the same bracket label belong to the same equivalence class. In Figure 3(a), each tree edge is labeled \nwith the topmost element of the bracket stack, and cycle equivalent edges have the same label. Now consider \nthe slightty more general case of linear spanning trees in which the backedges are not properly neste@ \nan example is shown in Figure 3(b). The diffi\u00adculty here is that in the reverse depth-lirst traversal, \nbrackets are not deleted in stack order. Moreover, note that edges a and b do not have the same set of \nbrackets even though the topmost element of the bracket stack of both edges is z. To allow arbitrary \ndeletion, we implement the bracket stack with a doubly-linked list. Brackets are always added to the \ntop of the stack, but they maybe deleted from any po\u00adsition within the stack. In this way, the most recently \nadded bracket (the bracket whose lower endpoint is highest in the tree) will beat the top of the stack. \nIn addition, we will keep track of the size of the bracket stack. It is easy to see that the pair < topmost \nbracket, set size > uniquely labels each equivalence class for example, in Figure 3(b), edges a *> . \n. / , <x> <X.1> / / . / / ,. . . ,. . <z> . - b <Z ,2> / , , * / , , , ,. ,, w: <w> , ,8 a <2,3> \n, , , , ZI t , , . t Z:,~ .A~ \\ , . . x; :{w: , <z> <W ,4>x, .. , t . t 1, 1, Y\\ . ,\\ ..c \\ <x> \\ \n<Z ,3> \\ \\ , .. , ,, \\ . , . \\ . . . Y: <y> . <y, z> . ., . . . . . . <x> <X,1> . . .. w (a) structured \nloops (b) unstructured loops (c) general tree node Figure 3: Compact names for bracket sets and b will \nbe placed in different equivalence classes, while edges a and c are placed in the same equivalence class. \nFinally, we must handle general depth-tirst spanning trees; an example is shown in Figure 3(c). When \nwe encounter a node that has more than one child, the bracket sets of the children must be merged. Unfortunately, \nthe notion of innermost bracket is no longer well-defined. For example, at node e in Figure 3(c), it \nis not clear whether the most\u00adrecently added backedge should be edge (~, d) or edge (h, c). The resolution \nof this difficulty rests on the observation that only one of the subtrees below node e can contain any \nedges cycle equivalent to an ancestor of e. This is because an edge in a subtree of e can only have brackets \noriginating in the same subtree; therefore, any ancestor of e having brackets from multiple subtrees \nof e cannot be cycle equivalent with any descendant of e. For example, edges between e and b cannot be \ncycle equivalent to any edge below e. However, edges between b and a can be cycle equivalent to edges \nbetween h and i. The solution therefore is to add an additional capping backedge whenever we need to \nmerge two or more bracket sets. This backedge becomes the topmost bracket in the set, and the children \ns bracket sets are then concatenated in arbitrary order. The new bracket originates from the node whose \nchildren are being merged, and extends up to the highest node whose brackets come from more than one \nof the branches. To add this new backedge requires keeping track (at each node in the tree) of the highest \nnode reached by any backedge below this point. The destination of the new backedge from a node is the \nsecond-highest of the node s children s highest backcdges. This could be found by examination of the \nbracket sets, but the highest-ending backedge is not necessarily related to the first bracket in each \nset (the highest-originating), so a full search of the bracket set would be necessary. Fortunately, we \ncan simply compute this information independently in constant time for each node. In Figure 3(c), we \nwould add anew backedge from e to b, as shown by the dotted edge. We must show that once this backedge \nis added, the pair < topmost bracket, set size > identifies the equivalence class as before. Lemma 2 \nThe capping backedges added by the algorithm do not alter the cycle equivalence relation for tree edges. \nProofi By Theorem 5, two Geeedgesarecycle equivalent if and only if they have the same set of brackets. \nConsider Eee edgess and t. If they have the same set of brackets after adding capping backedges, then \nthey have the same set of brackets without adding capping backedges. We must show that they wilt share \nthesamesetof newbracketswhencappingbackedgesareadded. We witt use the example in Figure 3(c) for illustration. \nSup\u00adpose edge s is bracketed by a capping backedge (e, b). The origin of that backedge, e, is a node \nwith at least two children: the highest-reaching branch has a backedge to a point at least as high in \nthe tree as b, and the second-highest-reaching branch has a backedge to b. Now consider where edge t(which \nhas the same original set of brackets as s) can occur. Edge t must be within the bracket (g, b) from \nthe second-highest-reaching sub\u00adtree of e, so t must be somewhere on the tree path from g to &#38; t \nmust also be within the bracket (i, a) from the highest-reaching subtree of e, so t must occur on the \ntree path from i to a. The intersection of these two parhs is the tree path horn e to b. Thus, the new \nbracket (e, b) is a bracket of t. o Theorem 6 The compact bracket set names uniquely iden\u00adtlfi bracket \nsets. ProoR We need to prove that two edges will have the same compact name if and only if they are cycle \nequivalent. One direction is reasonably easy: if two edges are cycle equivalent, they witl receive the \nsame compact name. By Theorem 5 two cycle. equivalent edges will have the same bracket sets. By Lemma \n2 the backedges added during the depth-first traversal will not affect the cycle equivalence relation. \nTherefore the bracket sets, as computed by the algorithm, wilt have the same size and the same top bracket. \nThe cycle equivalent edges witt therefore receive the same compact name. To complete the proof, we need \nto establish that if two edges are not cycle equivalen~ then they witt not receive the same compact name. \nLet a and ZIbe two edges that are not cycle equivalent. By Theorem 5 they must have different bracket \nsets, including (by Lemma 2) the new backedges added by the algorithm. If these sets are different size, \nthe atgorithm clearly gives them different compact names, so let us suppose the bracket sets are the \nsame size. By Lemma 1, if a and b are not ordered by the ancestor relation, then they have no brackets \nirt common and therefore receive different compact names. Otherwise, assume without loss of generality \nthat a is an ancestor of b. Since the sets are the same size, but not identical, a must have a bracket \n(p, q) not shared by b, and b must have a bracket (r,s) not shared by a. The node p is a descendant of \na if it is also an ancestor of b then the edge (p, q) must be linked on the bracket list sheadof b s \ntop bracket. Either (p, q) will be the top bracket or there wilt be another still higher. This bracket \ncannot include b, so b will have a different top bracket and will receive a different compact name than \na. Now assume that p is not an ancestor of b. In that case the paths from a top and from a to b diverge \nat some point. Catt this node d. Since d has multiple children, a backedge was added from d to a point \nat which backedges from only cme of the branches were stilt present. If that point is above a, then the \nadded backedge will bracket a, and either it or a higher backedge will be the top bracket for a, while \nit could not be the top bracket for b. If the point is below a, then all backedges from the branch b \nis on must have ended below a, so the top bracket for b, whatever it is, must have ended also and so \ncannot be a bracket of a. In either case, a and b will have different top brackets, and so they wilt \nhave different compact names. o 3.5 A fast algorithm for cycle equivalence We can put these observations \ntogether into a fast algorithm which makes use of art abstract data type called BracketList to maintain \nlists of brackets. The following operations are defined on this data type. create () : BracketList make \nan empty BracketList structure. size (6/ : Bracket List) : integer number of elements in BracketList \nstructure. push (bl : BracketList, e : bracket) : BracketList push eon top of bl. top (b/ : BracketList) \n: bracket topmost bracket in b!. delete (b/ : BracketList, e : bracket) : BracketList delete e from \nbl. concat (b/l, blz : BracketList) : BracketList concatenate bil and blz. This abstract data type \ncan be implemented as a record consisting of a doubly-linked list of brackets, a pointer to the last \ncell of the list, and an integer representing the size of the list. The doubly-linked list permits deletions \nanywhere in the list. The pointer to the last cell of the llSt permits fast concatenation of lists by \nin-place update to the cell. We leave it to the reader to verify that each of the operations of the abstract \ndata type can be implemented in constant time using this concrete representation. The only subtlety is \nin delete. When an edge is pushed onto a bracket list, the edge data structure is updated so it has a \npointer to the bracket list cell containing that edg~ this permits constant time deletion of an edge \nhorn a bracket list. We use integers to identify cycle equivalence classes. procedure new-class () returns \na new integer each time it is called. This can be implemented using a static variable initialized to \nzero that is incremented and returned each time the procedure is called. We assume each node structure \nhas the following fields: n.dfsnum depth-tirst search number of node.  n.blist pointer to node s \nbracketlist.  n.hi dfsnum of destination node closest to root of any edge originating from a descendant \nof node n.  The edge data structure saves the equivalence class num\u00adber and the size of the bracket \nlist when the edge was most recently the topmost bracket of a bracket list. For example, in Figure 3(b), \nedge z is the topmost bracket for edges c, a and finally b. a is given the same equivalence class number \nas c because the size of the bracket list at a is the same as it was when z was previously the topmost \nbracket (at edge c). In contrast, a and b are given different equivalence class numbers. To access the \nvalues saved on brackets, each edge structure has the following fields: . e.class index of edge s cycle \nequivalence class. Q e.recentSize size of bracket set when e was most recently the topmost edge in a \nbracket set. e.recentClass equivalence class number of tree edge for which e was most recentty the topmost \nbracket. The edge and node data types can be implemented using records in the obvious way. Figure 4 \ngives the pseudocode for computing edge cycle equivalence classes efficiently. It is easy to see that \nduring the depth-tirst traversal of the undirected graph, the amount of work performed at each node is \nsome constant amount together with work proportional to the number of edges incident at the node. Thus, \nthe algorithm requires O(E) time, where E is the number of edges in the control flow graph. 3.6 Building \nthe program structure tree Since cycle equivalent edges are totally ordered in the con\u00adtrol flow graph \nby dominance and postdominance, each ad\u00adjacent pair of edges in this order encloses a canonical SESE \nregion. To find canonical regions, we tirst compute cycle equivalence classes for edges in O(E) time \nusing the algo\u00adrithm in Figure 4. Any depth-fist traversal of the original control flow graph will visit \nedges in a given cycle equiv\u00adalence class in orde~ during this traversat, entry rmd exit edges of canonical \nSESE regions are identified. Canonical regions can be organized into aprogram struc\u00adture tree such that \na region s parentis the closest containing region and its children are all the regions immediately con- \nProcedttre CycleEquiv (G) { 1: perform an undirected depth-fwst search 2: for each node n in reverse \ndepth-lirst order do 3: I* compute n.hi *I; 4: hio := mitt {t.dfsnum ] (n, t) is a backedge } $ 5: hil \n:= mitt {c.hi I cis achildof n } ; 6: n.hi := min {hio, hil} ; 7: hichild := any child c of n having \nc.hi = hil ; 8: hiz := mitt {c.hi I c is a child of n other than hichild } ; 9: 10: P compute bracketlist \n*/ 11: n.blist := create () ; 12: for each child c of n do 13: n. blist := concat (c.ldist, n.blist) \n; 14: ettdfor 15: for each capping backedge d from a descendant of n to n do 16: delete (n. blist, d) \n; 17: endfor 18: for each backedge b from a descendant of n to n do 19: delete (n. blist, b) ; 20: if \nb.class undefined then 21: b.class := new-class () ; 22: endif 23: endfor 24: for each backedge e from \nn to an ancestor of n do 25: push (n. blist, e) ; 26: end for 27: if hiz < hi. then 28: J* create capping \nbackedge */ 29: d:= (n, node[hi~]) ; 30: push (n. blist, d) ; 31: endif 32: 33: F determine class for \nedge from parent(n) ton*/ 34: if n is not the root of dfs tree then 35: let e be the tree edge from parent(n) \nto n; 36: b := top (n. blist) ; 37: if b.recentSize # size (n. blist) then 38: b.recentSize := size (n. \nblist) ; 39: b.re.entClass := new-class () ; 40: endtf 41: e.class := b.receniClass ; 42: 43: J* check \nfore, b equivalence */ 44: If b.recentSize = 1 then 45: b.class := e.class; 46; endif 47: endif 48: endfor \n} Figure 4: The cycle equivalence algorithm5 4min returns intlnity (i.e. N + 1) whenever set is empty. \n 5The code in C is roughly 200 lines long and maybe obtained from the authors. tained within the region. \nWe discover the nesting relation\u00adship during the same depth-first traversal that determines eanonicat \nregions. The depth-first search keeps track of the most recently entered region (i.e. the current region). \nWhen a region is first entered, we set its parent to the current re\u00adgion and then update the current \nregion to be the region just entered. When a region is exited, the current region is set to be the exited \nregion s parent. From Theorem 1, it follows that the pushing artd popping follows a stack discipline. \nThe topmost SESE region on this stack when DFS reaches the entry node of a SESE region RI is the name \nof the smallest SESE region containing RI. Once the depth-first traversal is complete, the program structure \ntree has been built. 4 Empirical properties of the PST We now present empirical evidence to characterize \nthe properties of the PST. We gathered data from 254 proce\u00addures taken from the Perfect Club benchmmk \nsuite and the SPEC89 benchmark suite, using Dennis Gannon s Sigma FORTRAN front-end (modified extensively \nby Mayan Moudgill at Cornell), and a back-end of our own design. The programs are listed below. suite \nprogram lines procedures Perfect APS 6105 97 LGS 2389 34 TFs 1986 27 TIs 485 7 SPEC89 dnasa7 1105 17 \ndoduc 5334 41 fPPPP 2718 14 matrix300 439 5 tomcatv 195 1 linpack 793 11 total 21549 254 Figure 5(a) \npresents the distribution of region depth. In the 254 PS 13 there are 8609 regions. The maximum depth \nis 13, and the average depth is 2.68. This agrees with conventional wisdom that typical programs do not \ncontain deeply nested control structures. Figure 5(b) shows the cumulative number of regions at or below \neach level; from this we see that about 97 percent of all regions have a nesting level of 6 or less. \nIn Figure 6, we show that as procedures grow larger, the PST also grows in size, but it becomes broader \nrather than deeper.6 Figure 6(a) plots each PST s size in number of regions versus proedure size, and \nwe see that the number of regions does grow with procedure size. This indicates that larger procedures \nhave larger opportunities for exploit\u00ading structure, as desired. Figure 6(b) shows that the nesting 6~e \n6 la%est prmdures are omitted from Figures 6 and 9 to avoid compressing the horizontal axis. Their PSTS \nfotlow the general trend in each figure. . N= 8609 average depth = 2.68 1 L L L. 46 81012 depth in progmm \nstructure tree  (a) number of regions at each depth in PS13 (b) cumulative regions at each depth in \nPSTS Figure 5: Analysis of PST depth o ,. . . ......... .... . .. ........... .. .............. .... \n.....  ~ 00 N=248 0 N=248 I 0 5Kldo maximum depth = 13 ~ t c1 ~ 000 0 00 000 0 0 0 .s *O 0  000 \n.0 0 100 200 300 400 ! ) o 100 200 3eo 400 500 procedure size pmcedum size (a) PST size versus procedure \nsize (b) average PST depth versus procedure size Figure 6: PST size and depth with procedure size 23.2% \nblock orhe.r2.0% Figure 7: Weighted proportion of regions by kind depth of structures is independent \nof procedure size, as ex\u00adpected. Once SESE regions have been detected, we can further identify the kind \nof structure present in each region. Using a simple pattern-matching pass, we identify each region as \nbeing a basic block, a case construct, a loop, a dag, or a cyclic unstructured region. Figure 7 shows \nthe proportion of each kind of region, where each region is weighted by the number of nested maximal \nSESE regions. For example, an if-then-else has a weight of two since it contains two nested maximal regions. \nThis weighting gives a measure of region sizq blocks have unit weight. It is interesting that even this \nsimple heuristic finds considerable structure. In fact, 182 of the 254 procedures are completely structured, \nand we find considerable structure for the remaining 72 procedures. These empirical results from standard \nbenchmarks show that real programs contain an abundance of SESE structure that can be exposed quickly \nby our algorithm. ~ical PSTS are flat and broad, not narrow and deep. We now show how the algorithms \nin this paper and the PST in particular can be used to solve a variety of compilation problems. 5 Control \nregions in linear time The first application of our results is to the computation of control regions. \nThis application does not use the PST, rather, it is a reworking of the cycle equivalence algorithm that \nalso motivates the particular definition of single entry single exit regions we have used. The notion \nof control dependence plays an important role in optimization and parallelization. Intuitively, a node \nn is control dependent on a node c if c determines whether n is executed. Control dependence is defined \nformally as follows. Definition 8 A node n is control dependent [FOW87] on node c with direction 1 t~ \nthere is a path P from c to n beginning with edge 1such that 1. n postdominates all nodes other than \nc on P, and 2. if n and c are distinct, n does not postdominate c.  Control dependence for an edge \ncan be defined analo\u00adgously. Nodes or edges having the same control depen\u00addence are in the same control \ndependence equivalence class, or control region. Ferrante, Ottenstein, and Warren tirst posed the problem \nof partitioning control flow graph nodes into control regions CFOW87]. Their algorithm used hashing to \ncompute control regions in O(N) expected time, 0( IV2E) worst-case time and 0( IV12) space. These results \nwere improved by Cytron, Ferrante, and Sarkar [CFS90] who gave an O(EN) time, O(E + N) space algorithm \nfor finding control regions. Briefly, their algorithm works by placing all nodes in a single equivalence \ncla.% and then repeatedly refining the equivalence relation by considering the effect of each control \ndependence on the existing par\u00adtition. In the worst-case, the algorithm performs O(N) work for each of \nO(E) control dependence. The prob\u00adlem with this approach is that control dependence equiva\u00adlence is defined \nin terms of the control dependence relation, which has O(EN) size in the worst case. Ball [Ba1921 has \nrecognized the need to characterize control dependence equivalences without using control dependence \nand has de\u00adveloped a linear-time algorithm for computing control de\u00adpendence equivalences. However, his \nalgorithm works only for reducible graphs and requires computation of both dom\u00adinators and postdominators. \nPodgurski has given a linear\u00adtime algorithm forforward control dependence equivalence, which is a special \ncase of general control dependence equiv\u00adalence lJ%d93]. Using the results of Section 3, we can design \nan O(E) at\u00ad gorithm to determine control regions of arbitrary flow graphs and which runs faster than \njust dominator computation, the first step in all previous algorithms for this problem! The key technical \nresult in this section is that control dependence equivalence can be reduced to cycle equivalence. Theorem \n7 Let S be the strongly connected component constructed by adding the edge end + stark to a control Jow \ngraph G . Nodes a and bin G have the same set of control dependence iff a and b are cycle equivalent \nin S. We leave it to the reader to verify this theorem for the example shown in Figure 1(a). The proof \nof this theorem is straightforward, if tedious, and can be found in [JPP93]. Un\u00adlike the edge cycle equivalence \nrelation, node cycle equiv\u00adalence is not preserved when edge directions are removed from a graph. Fortunately, \na simple construction lets us reduce the problem of finding node cycle equivalence in di\u00adrected graphs \nto the problem of edge cycle equivalence in a related directed graph. Definition 9 Given a directed graph \nG, we dejine a node\u00ad expanding transformation T. For each node n in G, there is a pair of nodes ni and \nno in T(G), connected with the edge ni ~ nO; we call this edge the representative edge for n, denoted \nas n . For each edge n 4 m in G, there is a corresponding edge no ~ mi in T(G). v the divide-and-conquer \napproach is approximately (N/k)2 \\/ per region, or N2/k overall, and the atgorithm is speeded up Figure \n8: Node expansion Figure 8 shows the node expansion step pictorially. The following theorem, together \nwith Theorem 7, establishes the reduction of control dependence equivalence to edge cycle equivalence. \nThe proof is obvious and is omitted. Theorem 8 Two nodes a and b in a strongly connected component S \nare node cycle equivalent if and only if their representative edges a and b are edge cycle equivalent \nin the node-expanded graph, T(S). Therefore, we can use our algorithm for edge cycle equiv\u00adalence to \ndetermine control regions in O(E) time. Our al\u00adgorithm is asymptotically optimal; in addition, the constant \nfactor is small and the algorithm runs fast in practice. One detail of our implementation is worth noting \nwe avoid ex\u00adplicitly expanding nodes and undirecting edges. Instead, we use doubly-linked control flow \nedges (so that de@h\u00adfirst search can traverse edges in either direction), and we maintain a tuple of \ninformation at each control flow node, corresponding to the information that would be stored on the expanded \nnodes. The resulting code is slightly more com\u00adplex, but the savings in space and time over working with \nthe explicitly transformed graph are significant. In a related technical report, we have shown that this \nalgorithm runs faster than dominator computation, which is just the tirst step in atl previous algorithms \nfor this problem [JPF93]. Applications of the PST The Program Structure Tree is a tool for enhancing \nthe performance of program analysis algorithms by providing a simple framework for exploiting global \nstructure, local structure, and sparsity. The intuitive idea is the following. Global structure: The \nPST is a tree of SESE regions in which nesting structure is made explicit. Moreover, each SESE region \nis a control flow graph in its own right. Therefore, any global analysis algorithm can be applied un\u00adchanged \nto each SESE region, and the partial results can be combined using the PST to give the global result. \nThis lets us apply analysis algorithms in a divide-and-conquer fash\u00adion to the program, which can be \na win if the combining of partial results is not overly expensive, For example, suppose we have an 0(N2) \natgorithm and suppose there are k SESE regions of roughly equal size in the PST of the control flow graph. \nProvided combining can be done quickly, the cost of by a factor of k. As a concrete example, the static \nsingle as\u00adsignment (SSA) form is usually computed using dominance frontiers which can be 0(N2) in size \n[CFR+91]. We show that using the PST, SSA computation can be performed sep\u00adarately in each SESE region. \nSince the size of a SESE region is roughly independent of program size (Figure 9) and there is no combining \nof partial results to be done in this problem, PST-based exploitation of nesting structure is a win. \nLocal structure: The PST lets us tailor anatysis algo\u00adrithms to the structure of each SESE region. Figure \n7 shows that in practice, most SESE regions are basic blocks, con\u00additionals, DAGs and loops; therefore, \nfast algorithms can be used for these regions even if other regions in the PST are unstructured or even \nirreducible. One way to view this is that the PST lets us localize the effect of lack of struc\u00adture into \nSESE regions which do not affect analysis of other regions. Sparsity: In many analysis problems, the \nsolution is determined bya small subset of the SESE regions in thePSZ the other regions do not contribute \nto the solution and need not be analyzed. For example, in converting a program to SSA form, we show that \n#-function placement for a variable z can be solved completely by analyzing only those regions that contain \nan assignment to z. This lets us ignore the vast majority of SESE regions, as we show experimentally. \nWe illustrate these points by discussing how the PST can be used to speed up algorithms for two problems \ncomputing the static single assignment form and performing data flow analysis. In particular, our experimental \nresults highlight the importance of exploiting sparsity. 6.1 Using the PST in conversion to SSA form \nTranslation into SSA form requires the introduction of @ functions at some merge points in the control \nflow graph. Cytron et al [CFR+91] showed that a d-function is needed at a merge if it is the tirst point \nin common on two paths from distinct definitions of a variable v to a use of v. They char\u00adacterized this \nset of merges in terms of a property called the dominancefrontier. Briefly, a merge m is in the dominance \nfrontier of a node n, DF(n), if n dominates a predeces\u00adsor of m but does not dominate m. Extending dominance \nfrontiers to sets, DF(S) = U,E.S DJ (s). For a variable v defined at nodes in the set V, Cytron et al \nshowed that the set of merges needing @functions for v is exactly the iter\u00adated dominance frontier, DF+ \n( V), which is the limit of the sequence DF~+l = DF (VU.DF~), where DF1 = DF(V). The computation of DF+ \n(V) is performed with a worklist algorithm. The size of the dominance frontier of a node is 0(lV2) in \nthe worst case. Our algorithm uses the nesting structure in the PST to avoid computing the entire dominance \nfrontier for each node. The key theorem is the following one. Theorem9 ?fa merge node needsa $-function \nforvari\u00adable v, then it is in the iterated dominance frontier of some assignment to v in the same SESE \nregion as the merge node. We omit the proof and describe only the intuition. First, consider dominance \nfrontiers. If the merge is the first node in common on two paths from distinct definitions of v, then \nboth definitions cannot be outside the region containing the merge, since then the two paths must join \nprior to entering the merge s region. Likewise, both definitions cannot be in the same region nested \nwithin the merge s region, since the two paths would join prior to exiting this nested region. Therefore, \na merge that is in the dominance frontier of two assignments to v must be in the same SESE region as \none of them. By induction on the definition of iteraled dominance frontiers, the result is proved for \niterated dominance frontiers in general. Note that this implies that any region containing no definitions \nof v needs no ~-functions. We use this result to exploit both global structure and spar\u00ad sity in the \nPST. Instead of computing dominance frontiers for an entire procedure, we compute dominance frontiers \nfor each SESE region separately. This can be advanta\u00adgeous for example, the size of dominance frontiers \nfor nested repeat-until loops reaches the worst-case bound of 0(N2) [CFR+9 1]. When we exploit nesting \nstructure us\u00ading the PST, each loop is a SESE region whose dominance frontiers are computed independently, \nthereby avoiding the quadratic blowup. This is an example that illustrates the exploitation of global \nstructure using the PST. To exploit sparsity, we note that SESE regions that do not contain an assignment \nto the variable can be omitted from the analysis. Putting these observations together gives us the following \nalgorithm for enhancing the performance of SSA algorithms. Algorithm for ~-placement: Build the program \nstmtcture tree. For each variable v, do the following. 1. In the PST, mark every region containing an \nassignment to v. 2. For each region, collapse immediately nested regions into single statements as follows: \nif that region contains a def\u00adinition of the variable, treat the region as a definition of the variable; \notherwise, treat the region as a NO-OP. From Theorem 9, it follows that collapsing nested regions as \nde\u00adscribed maintains the path properties that determine where @functions are needed. 3. Apply any algorithm \nfor finding the SSA form to each marked  region, treating the entry point of the region as a definition \nand the exit as a use of the variable. By maintaining a list of definitions for each variable, we can \nperform the marking step in time proportional to the number of regions marked. Figure 10 shows the fraction \nof SESE regions examined when placing ~-functions for 5072 variables. We see that for most variables, \nonly a small fraction of SESE regions are examined. Seventy percent of variables required examining less \nthan one-fifth of the regions. In Step 3, it is possible to exploit local structure and use different \nSSA algorithms in each region if that is 301 N = 248 0 0 i0 10-0%:0 00 0\u00b0 00 2 :0\u00b0 0 0 05 -Om%moo \nm 0 JDammooooo 000 00 r-Cmooooooow o o~ 0 0 100 200 300 400 500 procedure size Figure 9: Maximum region \nsize versus procedure size 35 11 N=5072 1 Figure 10: Percentage of regions examined while placing ~-functions \ndesired. For example, it is trivial to convert if-then-else and loop structures into SSA form. Figure7 \nsuggests it might be worth doing this type of algorithm specialization. Moreover, the PST can even be \ndistributed across the local memories of a parallel machine, and computations in SESE regions can be \nperformed in parallel. Given the overheads of parallel computation on current machines, this approach \nis unlikely to yield much speed-up, but the principle is clear the PST can be used to exploit parallelism \nin compilation since it tells us how to divide the work and how to combine partial results. The divide-and-conquer \nstrategy works particularly well in this problem because no combining of individual region solutions \nis needed to generate the solution for the entire procedure. Next, we discuss dataflow analysis, a problem \nin which region solutions must be combined to yield the solution for the entire procedure. 6.2 Using \nthe PST in data flow analysis Solution techniques for monotone data flow analysis prob\u00adlems are classified \ninto iterative methods and elimination methods [Ken81, RP86]. We show discuss how the PST can be used \nwith either class of methods. Exploiting global and local structure: Elimination methods exploit nested \nprogram structure to solve data flow equations efficiently. Given some hierarchical decompo\u00adsition of \nprogram structure, anatysis is performed in two phases. In the first phase, local information is computed \nfor increasingly larger regions of the program; at each stage only the information from nested regions \nis taken into considera\u00adtion. In the second phase, global information is propagated to increasingly smaller \nregions. The classic approach to elimination algorithms uses an intervat decomposition of the program \n[AC76]. The PST can be used as the hierarchical decomposition for solving data flow systems via the elimination \nmethod in the first phase, local information is computed for each SESE region in bottom-up order in the \nPST, and then global information is propagated from larger to smaller SESE re\u00adgions during atop-down \ntraversal of thePST. In both phases, we need some algorithm to collect or propagate information within \na SESE region. As discussed in Section 4, most re\u00adgions are simple constructs such as blocks, if-then \nor loop constructs; these regions may be processed quickly using structure-based methods [Ken8 1]. What \nabout the remain\u00ading unstructured regions? An important aspect of the PST is that it is compatible with \nmethods based on intervals. In particular, we have the following theorem whose proof is straightforward. \nTheorem 10 If a controlfiow graph G is reducible, then all SESE regions of G are reducible. Therefore, \nif the original graph is reducible, the (few, small) unstructured SESE regions in the PST can be an\u00adalyzed \nusing interval methods. Finally, for irreducible regions, we can fall back on a general iterative method, \nwhich is similar in spirit to so-catled hybrid algo\u00adrithms [Zad84, HDT87, MR90]. It is interesting to \nnote that Graham and Wegman exploited single-exit intervals to speed up elimination-based data flow analysis \n[GW76]. Exploiting sparsity: Recent work on speeding up data flow analysis has focused on solving individual \ninstances of data flow problems, such as finding the availability of z + y, as opposed to analyzing a \nproperty for all variables or expressions simultaneously as is done in the traditional bit-vector approach. \nIn this case, much of the control flow graph does not contribute (i.e. modify or use) to the solution. \nSparse me[hods of data flow analysis attempt to avoid prop\u00adagating information through regions of the \nprogram where the data flow vatues are not modified. Our approach to exploiting sparsity using the PST \nis to bypass SESE regions having only identity transfer functions. It is easy to show that bypassing \nsuch transparent regions does not effect the global data flow solution. Given an data flow problem instance, \nwe build a quickpropagatimt graph (QPG), which is much smaller than the control flow graph, and then \nsolve the data flow system using this graph. The solution in the QPG can then be projected back into \nthe control flow graph. The nodes in a QPG are a subset of the control flow graph nodes, and each edge \nin a QPG is denoted by a pair of control flow edges (el, ez) such that either el and e2 are the same \nedge, or (el, e2) encloses a SESE region. Therefore, the QPG edge connects the source of el to the destination \nof ez. QPGs are constructed so that each edge bypasses a maximat SESE region having only identity transfer \nfunctions. (Optimization to the QPG that allow additional forms of bypassing and special treatment of \nconstant transfer functions are discussed in Johnson s dissertation.) Once the quick propagation graph \nis built, the data flow system is solved using this graph, thereby avoiding transpar\u00adent regions altogether. \nSince bypassing is performed on the basis of SESE regions, and since these regions are also the basis \nfor exploiting structure using an elimination method, use of the PST allows structure and sparsity to \nbe exploited simultaneously. Of course, nothing precludes the use of an iterative method for the entire \nQPG. Once the solution in the QPG is obtained, it is a simple matter to transfer this solution to the \nCFG as explained below. Atgorithm for PST-based data flow analysis: 1. Mark SESE regions containing a \nnon-identity transfer func\u00adtion. This is done by starting at the leaf nodes (i.e. basic blocks) having \nstatements with non-identity ~ansfer func\u00adtions and then marking all ancestors in the PST. 2. Construct \nthe QPG by traversing the CFG, bypassing any unmarked SESE regions as explained above. 3. In the QPG, \nsolve the data flow system using any solution method. 4. Transfer the solution from the QPG to the CFG \nas follows. Every edge in the CFG is either present in the QPG or it is part of a transparent SESE region \n(el, ez) bypassed in the construction of the QPG. In the tirst case, the data flow solution on the corresponding \nQPG edge is transferred to the CFG edge. In the second case, the data flow solution on edge el (or ez) \nin the QPG is transfemed to the CFG edge.  Note that the marking step can be done in time propor\u00adtional \nto the number of marked regions if we know the location of the non-identity transfer functions. For com\u00admon \noptimization, the non-identity transfer functions can be found by maintaining a list of definitions and \nuses for each variable. The totat time required to build a QPG is pro\u00adportional to the size of the QPG \nplus the number of marked PST regions. In the worst case, atl PST regions are marked, no regions are \nbypassed, and the QPG is simply the original CFG. As we have shown in Section 4, PSTS tend to be broad \nand shallow. Therefore, if the number of leaf nodes contain\u00ad ing non-identity transfer functions is smatl, \nthen the total number of regions which cannot be bypassed will be small. Preliminary studies show that \nthe QPG is usually quite small compared to the original CFG, averaging less that 10% the size of the \n(statement-level) CFG. Since QPGs are often so small relative to the size of the CFG, it is a significant \nsavings that our algorithm does not examine transparent regions. In a previous paper, we discussed a \nrepresentation of depen\u00ad dence called the dependencejhv graph(DFG) [JP931. In\u00ad tuitively, the DFG is \na set of basis graphs from which we can construct the QPG for a given data flow problem. For lack of \nspace, we postpone discussion of this connection. In principle, the PST (or QPG) can be used to perform \ndata flow analysis in parallel as is standard with divide and conquer algorithms, we work on leaf regions \nin parallel, and work on an interior node of the PST (or QPG) when all its children have been processed. \nWe refer the interested reader to related work by Gupta, Pollack and Soffa [GPS90] who use the SESE decomposition \nof programs in a struc\u00adtured programming language to perform data flow analysis in parallel. Note that \nour definition of SESE regions is stronger than theirs since we require unique entry and exit edges, \nwhereas they allow multiple edges to the entry node from outside the region, as well as out of the exit \nnode. As in the case of SSA computation, parallel data flow analy\u00adsis is likely to be a whimsical idea \nunless communication latencies on parallel machines are reduced significantly. 6.3 Discussion The PST \ncan be used to design divide-and-conquer style al\u00adgorithms for a surprising variety of problems. For \nexample, it is not difficult to design such an algorithm for computing the dominator tree of a control \nflow graph Iirst, build the dominator tree of each SESE region, and then piece together the locat trees \nusing global structure (nesting) information In the PST. Such an approach might lead to fast incremental \nalgorithms for anaIysis problems since the PST can be used to isolate regions of the graph where information \nmust be recomputed. The PST is also useful in generating code for dataflow machines from programs in \na language like FOR-TRAN or C since it exposes SESE regions which dataftow edges can potentially bypass \n[BJP91, BM0901. There is an enormous body of work on elimination and iteration algorithms, and we refer \nthe reader to surveys by Ryder and Paull @lP86], and by Kennedy [Ken81]. Tarjan and Valdes use a hierarchical \nrepresentation of SESE regions of a different kind to do elimination [Va178, TV80]. Sparsity was highlighted \nby Choi, Citron, and Ferrartte [CCF91], and by Dhamdhere, Rosen, and Zadeck [DRZ92]. Choi et al ex\u00adtend \nthe SSA form to build sparse evaluation graphs (SEGS); these graphs ako bypass uninteresting regions \nof the control flow graph and in general will be smaller than our quick propagation graphs. However, \nthey are more costly to build and it is unclear how to exploit both sparsity and structure us\u00ading SEGS, \nsince their edges cross interval (or SESE region) boundaries in an ad hoc manner. Recently, Cytron and \nFer\u00adrante [CF93] have improved the time for placing @functions (needed to build SSA form and SEGS) to \nO(Ea(E)) tim~ Sreedhar and Gao [SG94] have a linear-time algorithm for @function placement. It would \nbe interesting to compare the performance of these algorithms to the performance of a PST based algorithm \nthat used the dominance frontier algo\u00adrithm [CFR+91] selectively in the few, small unstructured SESE \nregions in the PSTS of typical programs.  7 Conclusions The program structure tree (PST) is a hierarchical \nrepresen\u00adtation of program structure in which nodes represent single entry single exit (SESE) regions \nand edges represent region nesting. The PST is defined for arbitrary flow graphs, even irreducible ones. \nWe showed that finding SESE regions is equivalent to solving the naturally stated graph problem of cycle \nequivalence edges are equivalent iff each cycle in the graph contains all or none of the edges in art \nequivalence class. In this paper, we discussed an O(E) algorithm for the cycle equivalence problem and \nused it to compute the PST of a control flow graph in O(E) time. We presented experimental evidence that \nreal programs contain abundant SESE regions organized into broad, shal\u00ad low PSTS; even the worst unstructured \nportions of proce\u00ad dures contain nested structure and comprise only a small t%action of the total procedure \nsize. Intuitively, the PST en\u00ad ables us to isolate the effect of lack of structure into small SESE regions, \nthereby letting us exploit structure globally. Our results have many applications. We showed that the \nproblem of determining control regions, which is needed in global code scheduling for example, can be \nsolved in O(E) time using the cycle equivalence algorithm.7 The recursive structure of the PST makes \nit possible to design divide\u00ad and-conquer style algorithms for control flow and data flow problems, exploiting \nglobal structure, local structure, and sparsity. We conclude that single entry single exit regions and \ntheir nesting relationship provide a simple, intuitive, and inex\u00ad pensive approach to representing and \nexploiting hierarchical program structure based on control dependence equivalence. Acknowledgments: We \nwould like to thank Bob Tarjan for his extensive comments on art earlier version of this pa\u00ad per, and \nfor bringing his work with Jacobo Valdes to our attention. Thanks also to Mayan Moudgill who wrote and \nmaintained some of the software used to gather the exper\u00ad imental results. Discussions with Dexter Kozen \nand Eva Tardos helped us simplify the presentation of the results in this paper. Finally, Micah Beck, \nWei Li, and Paul Stodghill gave us extensive feedback throughout this research. 7 rhe psT -~ ~ed tO give \na linear time and space factorhtion of control dependence rhat usuatly returns control dependence sets \nin time proportionat to their size. The problem of providing such a factorization that always returns \ncontrol dependence sets in proportional time remains open. References [GW76] S. Graham and M. Wegman. \nA fast and usually linear [AC76] @a192] mJP91] &#38;lM090] [CCF91] [CF93] [CFR+91] [CFS90] [DRZ92] [FOW87] \n[GPS90] [GS87] F. E. Allen and J. Cocke. A program data flow analysis procedure. Communications of the \nACM, 19(3]137 147, March 1976. Thomas Ball. what s in a region? -or-computing con\u00adtrol dependence regions \nin linear time and space. T*h\u00adnical Report 1108, University of Wisconsin -Madison, Computer Sciences \nDepartment September 1992. To appear in LOPLAS. Micah Beck, Richard Johnson, and Keshav Pingali. From \ncontrol flow to dataflow. Journal ofParaI1eland Distributed Computing, 12118-129,1991. Robert A. Ballance, \nArthurB. Maccabe, and KarlJ. Ot\u00adtenstein. The Program Dependence Web: A represen\u00adtation supporting control-, \ndata-, and demand-driven interpretation of imperative languages. In Proceedings of the SIGPLAN 90 Con#erenceonProgrammin \ngLan\u00adguage Design and Implementation, pages 257-271, White Plains, New York, June 20-22,1990. Jong-Deok \nChoi, Ron Cytron, and Jeanne Ferrante. Automatic consbuction of sparse data flow evaluation graphs. In \nConferenceRecordof the 18th Annua1ACM Symposium on Principles ofProgrammingLanguages, pa8es 55-66, Orlando, \nFlorid% January 21-23,1991. Ron Cytron and Jeanne Ferrante. Efficiently comput\u00ading ~-nodes on-the-fly. \nIn Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, pages 461-476, \nAugust 1993. Published as Lecture Notes in Computer Science, number 768. R. Cytron, J. Ferrante, B. K. \nRosen, M. N. Wegman, and F. K. Zadeck. Efficiently computing static single assignment form and the control \ndependence graph. ACM Transactions on Progr amming Lunguages and Systems, 13(4):451-490, October 1991. \nRon Cytron, Jeanne Ferrante, and Vivek Sarkar. Com\u00adpact representations for control dependence. In Pro\u00adceedings \nof the SIGPLAN 90 Conference on Program\u00adming Language Design and Implementation, pages 337-351, White \nPlains, New York June 20-22,1990. Dhananjay M. Dharndhere, Barry K. Rosen, and F. Kenneth Zadeck. How \nto analyze large programs efficiently and informatively. In Proceedings of the SIGPLAN 92 Conference \non ProgramrningLanguage Design andhnplementation,p ages 212-223, San Fran\u00adcisco, California, June 17-19, \n1992. J. Ferrante, K. J. Ottenstein, and J. D. Warren. The program dependency graph and its uses in optimiza\u00adtion. \nACM Transactwns on Programming Languages and Systems, 9(3):319-349, June 1987. Rajiv Gupta, Lori Pollock, \nand Mary Lou Soffa. Par\u00adallelizing data flow analysis. In Proceedings of the Workshop on ParaUel Compilation, \nKingston, Ontario, May 6-8, 1990. Queen s University. Rajiv Gupta and Mary Lou Soffa. Region scheduling. \nIn 2nd International Conference on Supercotnputing, pages 141 148, 1987. [HDT87] [JP93] [JPP93] [Kas75] \n[Ken81] [LT79] [MR90] [Pod93] [RP86] [SG94] [Va178] [Zad84] algorithm for global flow analysis. JournaloftheACh4, \n23(1~172 202, January 1976. S. Horwitz, A. Demers, andT. Teitelbaum. An efficient general iterative algorithm \nfor data-flow analysis. Acts Ir$ormatica, 24(6):679494, 1987. Richard Johnson and Keshav Pingali. Dependence\u00adbased \nprogram analysis. In Proceedings of the SIG-PLAN 93 Conference on ProgrammingLanguage De\u00adsign and Implementatwn, \npages 78-89, Albuquerque, New Mexico, June 23 25, 1993. Richard Johnson, David Pearson, and Keshav Pingali. \nFinding regions fas~ Single entry single exit and con\u00adtrol regions in linear time. Technical Report 93-1365, \nDepartment of Computer Science, Cornell University, July 1993. V. N. Kas jsnov. Distinguishing hammocks \nin a di\u00adrected graph. Soviet Math. Doklady, 16(5~448-450, 1975. Ken Kennedy. A survey of data flow analysis \ntech\u00adniques. In Steven S. Muchnick and Neil D. Jones, edhors, Program Flow Analysis: Theory and Applica\u00adtion, \nchapter 1, pages 5-54. Prentice-Hall, Englewood Chffs, NJ, 1981. Thomas Lengauer and Robert En&#38;e \nTarjan. A fast algorithm for finding dominators in a flowgraph. ACM Transactwns on Programming Languages \nand Sys\u00adtems, 1(1):121-141, July 1979. Thomas J. Marlowe and Barbara G. Ryder. An efficient hybrid algorithm \nfor incremental data flow analysis. Jn Conference Record of the 17th Annual ACM Sympo\u00adsium on Principles \nof ProgrammingLanguages, pages 184-196, San Francisco, California, January 1990. Andy Podgurski. Reordering-transformations \nthat pre\u00adserve control dependence. Technical Report CES-93\u00ad16, Case Western Reserve University, July \n1993. B. G. Ryder and M. C. PaulI. Elimination algorithms for data flow analysis. ACM Computing Surveys, \n18(3):277-316, September 1986. Vugranam C. Sreedhar and Guang R. Gao. Computing ~-nodes in linear time \nusing DJ-graphs. Technical Re\u00adport ACAPS Technical Memo 75, McGill University School of Computer Science, \nJanuary 1994. Robert E. Tarjan and Jacobo Valdes. Prime subpro\u00adgram parsing of a program. In Conference \nRecord of the 7th Annual ACM Symposium on Principles of Programming Lunguages, pages 95-105, Las Vegas, \nNevad&#38; January 28-30,1980. Jacobo Valdes, Parsing Flowcharts and Series-Parallel Graphs. PhD thesis, \nStanford University, De\u00adcember 1978. Report STAN-CS-78-682. F. Kenneth Zadeck. Incremental data flow \nanalysis in a structured program editor. In Proceedings of rhe 1984 SIGPL4NSymposium on Compiler Comtruction, \npages 132-143, Montreal, Canad&#38; June 17-22,1984.  \n\t\t\t", "proc_id": "178243", "abstract": "<p>In this paper, we describe the program structure tree (PST), a hierarchical representation of program structure based on single entry single exit (SESE) regions of the control flow graph. We give a linear-time algorithm for finding SESE regions and for building the PST of arbitrary control flow graphs (including irreducible ones). Next, we establish a connection between SESE regions and control dependence equivalence classes, and show how to use the algorithm to find control regions in linear time. Finally, we discuss some applications of the PST. Many control flow algorithms, such as construction of Static Single Assignment form, can be speeded up by applying the algorithms in a divide-and-conquer style to each SESE region on its own. The PST is also used to speed up data flow  analysis by exploiting &#8220;sparsity&#8221;. Experimental results from the Perfect Club and SPEC89 benchmarks confirm that the PST approach finds and exploits program structure.</p>", "authors": [{"name": "Richard Johnson", "author_profile_id": "81406600453", "affiliation": "Department of Computer Science, Cornell University, Ithaca, NY", "person_id": "PP31081507", "email_address": "", "orcid_id": ""}, {"name": "David Pearson", "author_profile_id": "81100662754", "affiliation": "Department of Computer Science, Cornell University, Ithaca, NY", "person_id": "PP14227548", "email_address": "", "orcid_id": ""}, {"name": "Keshav Pingali", "author_profile_id": "81100554731", "affiliation": "Department of Computer Science, Cornell University, Ithaca, NY", "person_id": "PP39048331", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/178243.178258", "year": "1994", "article_id": "178258", "conference": "PLDI", "title": "The program structure tree: computing control regions in linear time", "url": "http://dl.acm.org/citation.cfm?id=178258"}