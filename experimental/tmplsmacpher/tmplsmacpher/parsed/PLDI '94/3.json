{"article_publication_date": "06-01-1994", "fulltext": "\n VLIW Compilation Techniques in a Superscalar Environment Kemal Ebcioglu*, Randy D, Groves**, Ki-Chang \nKim*, Gabriel M. Silberman* and Isaac Ziv* * IBM T.J. Watson Research Center P.O. Box 218, Yorktown Heights, \nNY 10598 ** IBM Rise S ystern/6000 Division Abstract We describe techniques for converting code representation \nof a given program, modern compiler, to another representation the same run-time results, but can superscalar \nmachine. The algorithms, parallelization techniques for Very Long (VLIW) architectures, find and independently \nexecutable operations that 11400 Bumet Road, the intermediate as generated by a which produces run faster \non a based on novel Instruction Word place together may be far apart in the original code, i.e., they \nmay be separated by many conditional branches or belong to loop. As a result, the functional are presented \nwith more work that thus achieving higher performance using hardware instruction dispatch different iterations \nof a units in the superscalar can proceed in parallel, than the approach of techniques alone. While general \nscheduling techniques improve performance by removing idle pipeline cycles, to further improve performance \non a superscalar with only a few functional units requires a reduction in the pathlength. We have designed \na set of new algorithms for reducing pathlength and removing stalls due to branches, namely speculative \nload-store motion out of loops, unspeculation, limited combining, basic block expansion, and prolog tailoring. \nThese algorithms were implemented in a prototype version of the IBM RS/6000 XIC compiler and have shown \nsignificant improvement in SPEC integer benchmarks on the IBM POWER machines. Permission to copy without \nfee all or part of this material is granted provided that the copies are not made or distributed for \ndirect commercial advantage, the ACM copyright notice and the title of the publication and its date appear, \nand notice is given that copying is by permission of the Association of Computing Machinery. To copy \notherwise, or to republish, requires a fee and/or specific permission. SIGPLAN 94-6/94 Orlando, Florida \nUSA 0 1994 ACM 0-89791 -662-x/94/0006,,$3,50 Austin, TX 78758 Also, we describe a new technique to obtain \nprofiling information with low overhead, and some applications of profiling directed feedback, including \nscheduling heuristics, code reordering and branch reversal. Keywords: Global scheduling, software pipelining, \nVLIW, superscalars, compiler optimizations, profiling directed feedback INTRODUCTION A great amount of \nattention is presently being paid to improving the performance of RISC superscalar processors, i.e., \nuniprocessors that can achieve a peak execution rate of more than one instruction per cycle. Such architectures \nexecute a standard sequential instruction set, such as one normally executed by RISC uniprocessors, but \nare able to fetch and dispatch to their multiple functional units a peXk of two or more instructions \nin each cycle. Most speedup measurements on superscalar machines, tested on code generated by existing \ncompilers, have been disappointing. For example, Smith et al. [23] indicate that practical speedups over \nan existing RISC processor would be limited to a factor of about 2, even with aggressive superscalar \nconfigurations. The purpose of this paper is to describe some new global compiler optimization techniques \nto help overcome the obstacles to speedup on superscalar architectures. One reason for poor performance \nof superscalar uniprocessors on existing code, is the small Iookahead window of the hardware, which limits \nthe parallelism that can be extracted. Another reason is the unpredictability of branches, and the expense \nand difficulty of maintaining the execution state on all possible paths in hardware, in case one tried \nto avoid branch prediction, and execute operations on all paths instead. A third reason is the difficulty \nof maintaining a sustained execution rate of several conditional branches per cycle, to achieve a high \ndegree of performance in commercial applications or system code; branches are very frequent in these \ntypes of codes, so high branch throughput seems mandato~. The scheduling techniques described here should \nhelp to solve the first problem and part of the second problem. To solve all three problems, and increase \nthe issue rate without complicating the hardware, requires (in our view) a VLIW hardware implementation. \nThe techniques presented in this paper are inspired by the new compilation techniques and architectural \nfeatures that have been incorporated in the compiler and architecture for the IBM VLIW machine project \nat the IBM T.J. Watson Research Center [7,9,20,10,22,18,21]. This project consists of the design of a \nparallelizing compiler and an architecture (an 8-ALU hardware prototype is currently operational) for \nextracting parallelism from extremely sequential, non-numerical code. Our compiler techniques can bring \ntogether, into the same VLIW instruction, independently executable operations and tests that may be separated \nby many conditional branches in the original code, or that may belong to different iterations of a loop. \n(There have been some publications that have approached inter-basic block scheduling for superscalars \nas a new problem, e.g. [16, 14]. However, these authors dc~ not appear to have done a thorough literature \nsearch on previously published VLIW scheduling techniques.) The resulting code can execute operations \non all paths as soon as their operands are ready if resources permit; register renaming to maintain execution \non multiple paths is managed at compile time. Resources are conserved by stopping execution of the remaining \noperations on a path, as soon as it is known that the path will not be taken. Also, the compiler merges \nredundant computations on multiple paths into a single computation, to further conserve resources. Advanced \nmemory disambiguation techniques (enhancements of those used in the Bulldog compiler [11 ]) are used \nfor determining if two memory references can access the same location. The same scheduling techniques \ncan be applied to superscalars as well, by imagining a VLIW with the same resources as the superscalar, \nscheduling for that VLIW, but leaving the resulting code in superscalar format, rather than in VLIW format. \nThe program dependence graph (PDG) [13] has been considered in scheduling superscalar code for extracting \nfine-grain parallelism. A PDG for a procedure has two parts, a control dependence graph and a data dependence \ngraph. The control dependence graph portion explicitly indicates which tests in the program matter in \nwhether a given statement in the program is executed or not, and what values these tests should have \n(true of false) to cause this statement to be executed (these are the tests on which the statement is \ncontrol dependent). As a typical application of the program dependence graph described in [13], in the \ncontext of standard optimization, statements (including complex ones, such as loops or diamonds) that \nare control dependent on an identical set of test results can be re-ordered, if data dependence permit. \nIn the context of scheduling, this re-ordering corresponds to code motion of non-speculative operations \nthat do not require duplication (non-speculative, because if one statement among a set of statements \nwith identical control dependence is executed, then all will be executed). Bernstein and Rodeh [4] have \ndescribed an application of the program dependence graph to superscalar scheduling, that includes re-ordering \nof non-branch instructions with identical control dependence conditions within a given loop body, as \nwell as a limited speculative code motion technique that allows an instruction to be moved above one \nconditional branch. Gupta and Soffa [15] have applied the program dependence graph for somewhat more \naggressive code motions. Their technique allows code motion of operations above join points with duplication, \nmotion of a conditional branch above an operation, speculative motion of operations above conditional \nbranches, and loop peeling and unrolling (but no software pipelining). However, in the Gupta-Soffa approach, \nthe unit of code duplication is an entire region; thus, moving conditional branches above join points \nrequires more duplication than necessary. Also, the technique is applied only as an intermediate step \nfor increasing parallelism of basic blocks and large regions containing sub-regions, rather than for \nthe actual generation of machine code. Thus, it is difficult to define it as a true finite-resource scheduling \ntechnique. The parallel regions executed dynamically in a single cycle by superscalars, and VLIWS with \nmulti-way branching, tend to cross basic block and structured region boundaries in rather arbitrary ways. \nScheduling per se improves performance of a superscalar by removing idle slots in the pipeline, or by \nkeeping the functional units busier. For superscalars with a small number of resources, further performance \ngains can be achieved by using compiler techniques beyond classical optimization, aimed at reducing patlzlength. \nWe have developed a number of original pathlength reduction techniques, including speculative load-store \nmotion out of loops, unspeculation, limited combining, expansion of basic blocks, and prolog tailoring, \nwhich result in good performance improvements, which we present in this paper. In the following we present \nsome of the VLIW-inspired scheduling and optimization techniques (we shall refer to these collectively \nas VLIW scheduling techniques ) and their application to code generation for superscalars. Examples are \ngiven which illustrate their use. Some of these techniques have been incorporated into an experimental \nversion of the RS/6000 XIC compiler, contributing to significant performance enhancements on RS/6000. \nThe same compiler is used to generate code for the PowerPC 601 and Power2 processors, with similar performance \ngains. Experimental results for the SPEC integer benchmarks, measured on RS/6000 hardware, are presented. \nOur VLIW scheduling techniques do not depend on branch probabilities to generate efficient code, as opposed \nto trace scheduling and its derivatives ([11 ,6]). Nevertheless, profiling information should be used \nwhere it is available, since it allows approaching traditional compiler optimizations in a totally new \nlight, making further performance improvements possible. We describe our current work on profiling directed \nfeedback (PDF), including a new technique for obtaining profiling information with low overhead, and \nsome applications of PDF, such as scheduling heuristics, basic block re-ordering, and branch reversal. \nVLIW SCHEDULING TECHNIQUES In the following we outline some of the novel components of VLIW scheduling, \nwhich fit within the back-end of a modern compiler, usually after classical optimizations have been applied, \nbut before register allocation is performed. We ignore here the compiler front-end portion (which parses \nthe high level language input program and generates intermediate code), since VLIW scheduling does not \naffect its function. Each component by itself contributes a small portion of the overall performance \nimprovement. But, the synergy among them results in significant gains, as measured on several benchmarks, \nincluding SPECint92. We include here only those components which are original work, These techniques \ninclude speculative load/store motion out of loops, unspeculation, scheduling, limited combining, and \nbasic block expansion. Also mentioned is the prolog tailoring technique. Other techniques, which are \nadaptations of previously published work are not presented in this paper. Speculative Load/Store Motion \nOut of Loops This technique is aimed at avoiding memory accesses from within loops, specially loads from \nmemory. These instructions tend to introduce delays into the operation of the processor, specially if \nthe requested data does not reside in the cache. In essence, this is a generalization of the idea to \nmove loop-invariant instructions out of loops [1], with the additional capability of moving loads and \nstores which are conditionally executed (i.e., part of if statements), if they are considered safe. Although \nnon-memory operations have ordinarily been moved speculatively out of loops by previous techniques, the \nmotion of memory operations has been very conservative in the past. The code motion is done as follows: \nA group of load and/or stores is a candidate to move out of a loop if all the following conditions hold: \n1. Each load and/or store in the group: (a) uses the same base register, (b) has the same displacement \nfrom this base, and (c) identical operand length and data type. 2. The base register in question is \nnot written in the loop. 3. The location accessed is not declared volatile (shared variables or memory-mapped \nI/0) in the source language. 4. There is no possibility for the operands in this group of loads and/or \nstores to overlap with any other memory references (loads, stores or calls) within the same loop, or \ninside any inner loops contained in this loop. 5. On every path to the loop entry point coming from \n outside the loop (i.e., not counting any back edges from inside the loop), there is either (a) a load \nof the address constant of an external variable of sufficient size, into the base register (the Table \nof Contents (TOC.) in RS16000 software conventions is an area that holds such address constants), or \n(b) a load or store to the same location (base+displacement). This last set of conditions ensures that \nthis load/store group is always safe to perform, i.e., it will never cause an exception, even if it is \nonly sometimes executed in the original loop. A simple example of load/store motion out of loops follows. \nThe original code shows: L r4=. a(r2,0) [[ load from TOC (address of // a), sizeof(a)>=16 ... CL.0: I \nI beginning of lOOP ... BT CL.1 / / conditional inside 100P L r3=a(r4,12) [ I candidate ~or motion AI \nr3=r3, 1 ST a(r4,12)=r3 I I candidate for motion CL.1: ... BCT CL. O II end of looP which becomes: Notice \nthat in the above example, both LR operations inside the loop will eventually be eliminated by a later \ncoalescing or limited combining stage, leaving only an AI rlo.rlo,l instruction in the loop. Clearly, \nthe new loop has fewer instructions, resulting in higher performance. But, sometimes parts of an inner \nloop containing loads and stores are infrequently executed, and thus moving these loads and stores out \nof the loop may potentially slow down an outer loop. Therefore, execution projiles may be very helpful \nin deciding when this type of optimization should be applied (see PDF below). L r4=.a(r2,0) . . . L r10=a(r4,12) \n// r10 is a register\u00ad // cached COPY of a(r4,12) CL.0: . . . BT CL.1 LR r3.r10 II replaces L r3=a(r4,12) \nAI r3=r3,1 LR r10=r3 II replacea ST a(r4,1z)=r3 CL.1: . . . BCT CL.O ST a(r4,12)=r10 // needed at all \nexits II from the loop . . . As a special case of load/store motion out of loops, for certain I/O library \nprocedures with known properties (e.g., storage modifications confined to parameters), loads and stores \nare moved out of the loop even if there are calls to such subroutines in the loop (this is an exception \nto point 4 above). Before each of these I/O procedure calls, those memory locations for which loads and/or \nstores were moved out of the loop (which the called procedure may use) are updated from their register-cached \ncopies before the call, and reloaded into registers after the call. This seems justified because I/Oprocedures \nare either executed infrequentlyor doalot ofwork anyway, sothat thestonng and reloading overhead will \nnot cause much degradation. This strategy can be extended to general procedures, using an inter-procedural \nanalysis tool (that has access to library routines as well) to extract the relevant information about \naccesses to memory locations. Unspeculation Speculative operations (operations whose results do not always \ncontribute to the program s final result) can appear in the intermediate code due to various code hoisting \ntechniques that have been applied, or due to the fact that such operations were present in the original \nprogram. A sequence of operations becomes speculative if it is moved above a conditional branch which \ndetermines whether they should be executed to achieve the desired result of the program. Thus, inthose \ncases when theconditional branch leads away from the path where the instructions were originally located, \ntheir execution may cause performance degradation. The objective of unspeculation is to discover and \nreduce the number of speculative operations, thus avoiding the potential performance degradation, by \npushing down (groups of) speculative operations onto one of the two targets of a conditional branch, \nmaking them non-speculative there. (Groups of instructions refer to possibly a number of basic blocks \nwith a single entry and exit. Single exit loops and nested if-then-else-endif statements are examples \nof such groups.) To perform unspeculation on a speculative (group of,) instruction(s) 1, in the context \n1 other instructions or groups of instructions a conditional branch the following conditions must be \nmet: 1. The destination registers of I are all dead in one of the targets of the conditional branch, \nbut not on the other branch. 2. Any of the instructions between I and the conditional branch must not \n a. set any source or destination registers fOr 1, b. use any destination registers for 1, or c. contain \ninstructions with side effects on any memory location(s) loaded from in 1.  3. None of the instructions \nin I has side effects (e.g., stores to memory, access to volatile variables).  If all of the above conditions \nare met, then I can be deleted from its original place and moved to the branch target edge where its \ndestinations are live. The instruction scheduler can later choose to make single operations speculative \nagain, when it can determine that there is an otherwise idle resource to execute such operations. This \nis in contrast to unspeculation, which moves groups of instructions to avoid unnecessary execution of \nspeculative instructions. Unspeculation can also remove inefficiencies the programmer may have introduced \nfor simplifying coding. Here is a common example, using a C program: flag=l; I* result not always used \n1 if (cond) {. . . . flag= o;} becomes the following after unspeculation: if (cond) {. . . . fla9=O; \n} else {flag= l;} The unspeculation algorithm proceeds with the following steps: 1. The basic blocks \nare physically re-ordered using a reverse post-order enumeration of the flow graph nodes. When two basic \nblocks were consecutive in the original ordering, but are not consecutive in the new ordering, a label \nis introduced at the beginning of the second basic block if needed, and an unconditional branch to this \nlabel is introduced at the end of the first basic block, to retain the original program semantics. The \nreverse post-order enumeration places all basic blocks of a group of instructions (e.g., a nested if-then-else-endtf \nconstruct) consecutively in the program, so the construct can be easily moved as a whole. 2. The hierarchy \nof single-ent~-single-exit groups of instructions is identified in each loop and in the entire procedure. \nLocal data flow information (the registers which are used and set in this group, etc.) is collected for \neach group. 3. For ~ach conditional branch in the program (following their order of occurrence in the \nprogram), each instruction or group of instructions that precedes it is examined in reverse program order, \ndeciding if the instruction or group of instructions stays in place, goes to the left target edge of \nthe conditional branch, or goes to the right target edge of the conditional branch, according to the \nconditions described above. The backward traversal stops when a join point or another conditional branch \nis encountered at the same level in the group hierarchy. A speculative (group of) instruction(s) can \nthus be pushed repeatedly under successive conditional branches. Live variables and other data flow information \nare incrementally updated, as new instructions are pushed under conditional jumps. Code is never pushed \ninto loops from the outside. Speculative code inside a loop can, however, be pushed out of exits.  A \nsimilar technique, called revival transformation, was independently discovered and reported in [12]. \nIn contrast with the revival transformation, our version of unspeculation handles loops, and pushing \nspeculative code (including complex constructs) out of loops. Also, our algorithm is applied to the control \nflow graph, rather than the control dependence graph, thus legal serial code is guaranteed after each \ncode motion step. Furthermore, we move all movable constructs down at once; pushing down one speculative \nconstruct may enable further speculative constructs above it to be pushed down as well. Unrolling, Renaming, \nGlobal Scheduling, Software Pipelining The regions of the program are compacted through the combination \nof global scheduling [10] and enhanced pipeline scheduling [7], starting from the innermost regions (loops) \nand ending with the outermost region (the whole procedure). Global scheduling consists of choosing the \nbest operation from the set of operations that can move to a point, moving all instances of the operation \nto that point, making bookkeeping copies for edges that join the paths of code motion but are not on \nthem, and updating the data flow attributes of basic blocks only on the paths that were traversed by \nthe instances of the moved operations. To compact the loops, not just within an iteration, but also across \niterations, enhanced pipeline scheduling puts a fence at the current scheduling point, and lets global \nscheduling search for the best operation on all paths which can possibly cross the loop back edges, but \nnot the fence. The loops are unrolled prior to scheduling and live range renaming is performed, to increase \nscheduling opportunities. The VLIW compiler techniques developed by our group are beyond the scope of \nthis paper and have been described in detail in [10,18,9,21]. They are different from other VLIW and \nsuperscalar scheduling techniques, in features that include the following: 1. They operate on an entire \nprogram with arbitrary control structure, rather than on a single most probable path, as in trace scheduling. \nThe code motion technique is very general: whenever there is a path from a point A to a point B in the \nprogram, where there are no true data dependence for an operation currently at B, that operation can \nbe moved from B to A on that path, even if there are other paths from A to B where there are dependence. \nMultiple instances of the same operation on different paths are hoisted as one operation. 2. Conditional \nbranches can be moved up above other operations when dependence permit such an action. While conditional \nbranches are rarely on the critical path, executing them early reduces the guesswork for determining \nwhich speculative operations to execute. 3. Our software pipelining technique (enhanced pipeline scheduling \n[9]) applies to loops with arbitrary flow control, and allows generating schedules with a variable iteration \nissue rate, depending on which path is followed at run time (unlike Lam s hierarchical reduction technique \nand its derivatives, that respect the worst case data dependence cycle across all paths in the loop [17]). \n The particular flavor of our VLIW scheduling techniques used in the context of the XIC compiler will \nbe described in detail in [8]. We try to provide the gist of the techniques with an example here (SPEC \nli xlygetvalue subroutine). In the context of the RS/6000 code below, the objective is to put one independent \ninstruction between a loop, a non-coalesceable register copy operation LR r=r load and the use of its \ndestination register, and three is inserted at that exit edge before live range renaming. independent \ninstructions between a compare and a This technique increases scheduling opportunities. dependent conditional \nbranch. This is conceptually the same as scheduling for a VLIW that can simultaneously loop : accommodate \na maximum of one integer operation and one L r4 =(r8,4) branch per VLIW instruction, while respecting \npipeline L r5 =(r4 ,4) latencies for compare and load operations. c crO =r5 ,r3 BT found ,crO .eq L r8 \n=(r8,8) r8.address of first list element c crl =r8 ,0 r3. item to match against BT endofchain,crl .eq \nL rl=(r8 ,4) extracts the car field of list L r4=(r8 ,4) element pointed to by r8 L r5=(r4,4) L r2=(r8,8) \nextracts the cdr field of list c crO=r5,r3 element pointed to by r8 BT found,crO.eq No. of cycles are \nshown next to each instr. L r8=(r8 ,8) c crl=r8,0 loop : BF loop,crl.eq L r4=(r8,4) 1 (BB #l) endofchain: \nL r5=(r4,4) 2 ... c crO=r5,r3 2 found : #non-coalesceable BT found,crO.eq O if (car(car(r8))==r3) LR \nr4=r4 #coPY operation for detaching goto found #live ranges of r4 and r4 L r8=(r8,8) 1 r8=cdr(r8) (BB \n#2) #(can be copy-propagated later) c crl=r8,0 2 found: # r4 is live here BF loop,crl.eq 3 if (r8!=NULL) \ngoto 100P endofchain: ... In what follows we can see aversion of the loop which found: # r4 is live \nhere executes at 14 cycles for 2 iterations, obtained by code SPEC li benchmark inner loop. Executes \nat motion within a loop body (global scheduling with no 11 cycles per iteration. software pipelining). \nWhen we combine global scheduling with software pipelining, the loop executesat 10 cycles for 2iterations, \nas shown inthe next page. Hereis the loop after unrolling and live range renaming. For each register \nr that is live at an edge that leaves the (unrolled original looP) loop : loop : L r4 =(r8,4) 1 L r4 \n=(r8,4) L r8 =(r8,8) 1 (moved)<-----------------I L r5 =(r4 ,4) 1 IL r5 =(r4 ,4) c crl =r8 ,0 1 (moved)<---------------I \nI c crO =r5 ,r3 1 Ilc crO =r5 ,r3 L r4=(r8 ,4) 1 (moved)<-------------l I I L r8=(r8 ,8) 1 (moved)<-----------l \nI I I L r5=(r4,4) 1 (moved)<---------l I I I I BT found ,crO .eq o I I I I I BT found ,crO .eq I I I \nI -<L r8 =(r8,8) Iill--<c crl =r8 ,0 c crl=r8,0 1 (moved)<-------l I I I BT endofchain,crl .eq O 1111 \nBT endofchain,crl .eq I I I 1----< L r4=(r8 ,4) I 1-1------< L r5=(r4,4) c crO=r5,r3 1 II c crO=r5,r3 \nBT found,crO.eq 3 II BT found,crO.eq I 1------< L r8=(r8 ,8) 1----------< c crl=r8,0 BF loop,crl.eq 1 \nBF loop,crl.eq endofchain: ... found : LR r4=r4 found: # r4 is live here 41 (loop from the previous \nfigure) loop : loop : L r4 =(r8,4) 1-------< L r4 =(r8,4) L r8 =(r8,8) 1-1-------< L r8 =(r8,8) L r5 \n=(r4 ,4) !-l-l-------< L r5 =(r4 ,4) c crl =r8 ,0 1-1-1-1-------< c crl =r8 ,0 loopl : (new loop starts \nhere) 1111 c crO =r5 ,r3 1 1111 c crO =r5 ,r3 L r4=(r8 ,4) 1 1111 L r4=(r8 ,4) L r8=(r8 ,8) 1 1111 L \nr8=(r8 ,8) L r5=(r4,4) 1 1111 L r5=(r4,4) BT found ,crO .eq o 1111 BT found ,crO .eq c crl=r8,0 1 1111 \nc crl=r8,0 BT endofchain,crl .eq O 1111 BT endofchain,crl .eq c crO=r5,r3 1 1111 c crO=r5,r3 L r4 =(r8,4) \n1 <---l-l-l-1 L r8 =(r8,8) 1 <---l-l-1 L r5 =(r4 ,4) 1 <---l-1 BT found,crO.eq o I BT found,crO.eq c \ncrl =r8 ,0 1 <---1 BF loopl,crl.eq o BF loop,crl.eq endofchain: ... found : LR r4.r41 found: # r4 is \nlive here Note that we have assumed that the first few bytes of page thestarting instruction in this \nsequence arereplaced by the zero contains zeros in this example, as suggested in [5]. source (literal \nor register) for the starting instruction, and This allows operations such as taking car(car(NIL)) to \nan unconditional branch to the instruction following the proceed without causing an exception. last use \nis added. Any unreachable code left from this transformation may be deleted by common unreachable Limited \nCombining code elimination techniques. Combining as a dynamic technique used on-the fly during scheduling \nis further discussed in [20]. This technique scans the code looking for opportunities to reduce path \nlength by combining collapsible operations suchas LR r4=r5 ; A r6=r4,r7 where rA isused for the For example: \nlast time by the second operation, into A r6=r5,r7. This is normally achieved by the value numbering \noptimization LR r5. r4 [1], when both operations are inside the same basic block. . . . // instruction \ngroup (1) The limited combining technique can span a number of B L3 basic blocks looking for collapsible \noperations, possibly ... including join points, through code duplication. L3 : // other paths meY join \nhere This transformation will search asequence of instructions, . . . // instruction group (2) L r3=4(r5) \nstarting from a load immediate to register or register copy . . . // instruction group (3)instruction \n( <register means fixed point, floating point, B L4 or condition code register), through unconditional \nbranches, until a last use of the destination register for L4 : . . . // instruction group (4) thestarting \ninstruction is found. Neither thesource nor the L r7=8(r5) // this is the last use of r5 destination \nregisters for this instruction should be set by . . . // operation following last use any instruction \non the way to the last use. If the search succeeds, the whole sequence of instructions (beginning from \nthe instruction following the starting one, and ending where neither rs nor rA are set in instruction \ngroups (l), at the last use instruction) replaces the original starting (2), (3) or (4), becomes: instruction. \nAll occurrences of the destination register for // LR r5=r4 deleted!! . . . // instruction group (1) \n. . . // instruction group (2) L r3=4(r4) /1 uses r4 instead of r5 . . . // instruction group (3) . . \n. // instruction group (4) L r7=8(r4) // uses r4 instead of r5 B L1O // new branch closee duplicate // \nsequence L3 : // original sequence kept for // other paths joining here . . . // instruction group (2) \nL r3=4(r5) . . . // instruction group (3) B L4 L4 : . . . // instruction group (4) L r7=8(r5) // end \nof original eequence L1O : // join from duplicated code . . . // operation following last // useofr5 \n Basic Block Expansion Some of the optirnizations, such as limited combining above, create small basic \nblocks that end with an unconditional branch. Moreover, the original program itself may also naturally \ncontain tiny basic blocks that end with an unconditional branch. These branches are not considered as \nresources in a VLIW environment, but actually consume resources (and possibly cycles) in a superscalar \nmachine. For example, the RS/6000 can be significantly slowed down ifan untaken conditional branch is \nfollowed immediately by a(taken) unconditional branch. Since conditional branches cannot be avoided, \nthe basic block expansion technique triesto minimize the occurrence of unconditional branches on the \nexecution trace, by copying code from the target of abranch. For a given unconditional branch, basic \nblock expansion determines how many cycles worth of non-branch instructions we need to copy, by examining \nthe code immediately preceding the branch, using machine specific rules. For example the RS/6000 requires \n4-5 non-branch instructions (cycles) between an integer compare, a dependent (untaken) conditional branch, \nand an unconditional branch, in order to avoid a stall. Then the code at the target of the unconditional \nbranch is examined, to determine the stopping point for the copying process (i.e., how many instructions \nto copy). The search fora stopping point can go past conditional branches, CALLS, or unconditional branches. \nFor the latter case, searching continues with the target of the unconditional branch. Labels encountered \non the way are not copied. As the search passes a conditional branch or CALL, the objective number of \nconsecutive non-branch instructions to copy, is re-calculated. The search stops when enough consecutive \nnon-branch instructions have been gathered, or when an instruction (inside a loop) is revisited. If a \nreturn from a procedure is encountered (or a branch on count in the RS16000, which ends inner loops), \nthe search stops as well. Obviously, there is a limit to the number of instructions scanned in this process \n(the window size ), and exceeding this number aborts the search. In this case, the stopping point that \ngives the minimal machine stall, among the ones encountered so far, is chosen. Good candidates for stopping \npoints are instructions immediately preceding (hopefully untaken) conditional branches. Once a stopping \npoint is chosen, code is copied, starting from the target of the unconditional branch until the stopping \npoint. The original unconditional branch is deleted and anew unconditional branch is inserted after the \ncopied code, branching to the instruction immediately following the stopping point. The net effect of \nbasic block expansion, when successful, is to remove an unconditional branch altogether from the execution \ntrace, push it outside an inner loop, or put sufficiently many non-branch operations before it, so that \nit can be executed concurrently with other operations on . the RS16000, Power2 or PowerPC hardware. Basic \nblock expansion on the following code: c crO=. . . // set condition register O BF Ll, crO // branch conditional \non crO Op 1 B L2 // need 4 instruction here ... L2 : // other paths may join here c crl=. . . // set \ncondition register 1 BF L3, crl // reset objective to 5 inst. op2 Op 3 // potential stonping point BF \nL5 Op 5 Op 6 BCT loop // stopping point L3 : // code continues here produces: Note that the original \ncode starting at L2 will be deleted as unreachable code if no other instructions branch to it. c crO=. \n. . II set condition register O BF Ll,crO // branch conditional on crO Op 1 c crl=. . . // set condition \nregister 1 BF L3, crl // Op 2 Op 3 // potential stopping point BF L5 Op 5 Op 6 BCT loop // etopping point \nB L3 // rejoin code Amilder formof basic block expansion was describedin [14], where the process of \nadding instructions would stop when any branch was encountered. A more aggressive experimental code replication \ntechnique appears in [19], which replicates all operations on the shortest path from each unconditional \nbranch to the end of the procedure, in order to eliminate unconditional branches almost totally. This \ntechnique entails a high degree of code expansion, and can have an uncertain effect on the machine stalls \nof superscalars like the RS/6000, due to the conditional branches that are reversed to make the replicated \npath a straight line. Our technique uses a window size constraint to keep the code expansion reasonable, \nand makes intelligent decisions on where to stop copying to minimize the machine stall; and thus it seems \nmore appropriate for a practical commercial environment. Prolog Tailoring In the RS/6000 linkage conventions, \na register belonging to a particular subset of the machine registers must be saved upon entry and restored \nupon exit in a procedure, if that register is killed (overwritten) inside the procedure. A prolog is \nthe portion of code at the beginning of a procedure which stores the values of registers that are killed \nin the procedure, whereas the epilog restores these registers at the end of the procedure. Our prolog \ntailoring technique delays the execution of store instructions for killed-registers as late as possible \ninto the procedure, so that each execution path therein contains a reduced number of such store instructions. \nHowever, register save operations are never pushed inside loops. Some high level languages, like those \nin the PLA family, require correctly unwinding the stack titer an interrupt, in order to return to an \nexception handler established by a procedure high up in the call chain. For example, assume procedure \nA establishes an exception handler; then A calls B, which saves some registers on the stack and calls \nC, which saves some registers on the stack and then causes a trap. The trap handler code must correctly \nunwind the stack by first restoring the registers saved by C, then those saved by B. Control must finally \nbe transfemed to the user s exception handler in A. In the existing software conventions, the set of \nregisters to restore is found by the exception handler in a traceback table at the end of a procedure, \nand is fixed for a given procedure. If prolog tailoring is applied, the set of registers to restore is \nno longer constant, varying depending on the point of execution in the procedure. This requires special \nhandling when a program interrupt occurs, to ensure proper exception behavior of the program. In order \nto ensure correct unwinding of the stack after program interrupts, we enforce the rule that at any point \nin the procedure, all paths reaching this point from the start of the procedure have the same set of \nsaved registers. Thus, the registers to be restored on an exception can be found by back-tracing any \npath from the point where the exception occurred to the start of the procedure (a flow graph must first \nbe constructed from the binary code by the exception handler for this purpose [22]), without having to \nknow what particular path was followed to reach the point of exception. In the following page we show \nan example of the application of prolog tailoring. The prolog tailoring algorithm has the following stages: \n1. Identify outermost loops first and collapse them to single nodes. Make a copy of each basic block \nthat ends with a return, for each edge that goes to it. Straighten the flow graph, and do this once more, \nif code size constraints allow. Then identify hi-connected components in the undirected version of the \nflow graph using Tarjan s algorithm [2]. For example, an outermost ij then else endif statement constitutes \na hi-connected component in the undirected version of the flow graph. Create a tree from these hi-connected \ncomponents where the root is the component containing the special procedure start node, and its children \nare the components that share a node with the root. The children of a non-root component c are the components \nother than the parent of c, that share a node with c. For the purpose of computing the registers killed \nin a hi-connected component in the next step, the basic block shared between a parent component and a \nchild component in the tree is considered to be inside the parent and not the child. 2. Determine a \ndata flow attribute at each node of the tree, called the MustKill registers. These are the registers \nthat will be definitely killed (written into) starting at this tree node, regardless of which path is \n WITHOUT TAILORED PROLOG WITH TAILORED PROLOG (saves all registers that are killed anywhere in the procedure) \n PROC sub PROC sub save r28,r29,r30,r31 . . . . . . (restore nothing on exception) BTL1 BTL1 save r29,r31 \nL r29= L r29= (restore r29,r31 on L r31. L r31. exception) ... ... restore r29,r31 restore r29,r31 return \nreturn L1: L1: save r28 L r28= L r28. . . . . . . (restore r28 on exception) BT L2 BT L2 save r30 L \nr30= L r30. (restore r28,r30 on exception) L r28= L r28= restore r30 L2: L2: (r30 not used or set after \nhere) (restore r28 on exception) ... ... restore r28,r30 restore r28 return return takenin the tree. \nThe A4ustKill attribute ofatree node Experimental Results n is computed as: The performance of code \ngenerated by an experimental MustKill(n) = [, ~cc(~)MustKill(s)] U KilledBy(n) version of the RS/6000 \nxlc compiler, using VLIW scheduling, has been extensively tested on Power where succ(n) is the set of \nthe children of n, and (Rs/6ooo), Power2 and PowerPC 601 hardware. KilledBy(n) is the set of registers \nthat are killed by Followingis asnapshot ofthese measurements, using the any basic block inside n. This \ndata flow attribute can 6 SPEC 92 integer benchmarks. (These do not represent be determined in one backward \npass over the tree, in any official measurement by IBM.) In this case the reverse topological order. \nresulting improvement is about 13Y0. Compared tothe-O 3. For each node following a forward topological \norder option of XIC, there was an average compile time increase of the tree, if a register will definitely \nbe killed of 36Y0, and an average code size increase of 8% using starting at this tree node, but has \nnot yet been saved static binding. The most time consuming transformation on the path from the root, \ngenerate code to save that is VLIW scheduling. Aggressive compiler techniques such register on every \nactual flow graph edge that connects as the ones described here are thus appropriate for the-03 this \nnode and its parent in the tree (multiple actual option of the XIC compiler, which is intended for flow \ngraph edges may connect two nodes in the tree). optimization that require longer compile time. Consider \nmaximal sequences entered only at the top, In the following table we present performance consisting of \nnon-branch instructions, z~-then-else-etzdif measurements done on an RS/6000 model 580 machine, constructs \nand single entry, single exit loops (a branchless using XIC version 1.2 and the KAP C Preprocessor frombasic \nblock is such a sequence). If a register is set for the Kuck &#38; Associates. The columns labeled xIc \nshow the first time since the beginning of the program in such a baseline measurement, with VLIWoptimizations \ndisabled,sequence, and is never used or set after the sequence, that while the VLIW columns show performance \nwith theregister can be saved on entry to the sequence, restored on new scheduling and optimization techniques, \nincluding the exit from the sequence, and then the entire sequence can effects of some AIX library routines \nthat were rewrittenbe treated as if it did not kill this register. This analysis by the authors for better \nperformance. The VLIWand insertion of the save and restore code for the register should be done before \nstage 2 of the algorithm above. measurements were done on an RS/6000 model 980, whose performance on \nSPECint92 is equivalent to that of a model 580. SPECint92 Measurements Benchmark xl c xl c VLIW VLIW \nTime SPECmark Time SPECmark espresso 41.70 54.44 38.30 59.27 li 99.00 62.66 81.90 75.82 eqnt ot t 13.60 \n80.88 10.70 102.80 compress 53.90 51.39 48.10 57.59 Sc 69.20 65.46 62.40 72.60 gcc 91.40 59.61 90.20 \n60.53 SPECint92 61.73 69.93  PROFILING DIRECTED FEEDBACK Profiling Directed Feedback {PDF) has been \nused in the past for VLIW approaches based on trace scheduling and its derivatives [11,6]. Whereas PDF \nis a fundamental requirement in these techniques, our VLIW scheduling techniques do not depend on the \navailability of branch probabilities, and already generate good schedules when there is no profiling \ndirected feedback. Nevertheless, profiling directed feedback provides the compiler with new information, \nallowing traditional optimization to be approached in a fashion heretofore considered too risky, because \nofpotential performance degradation, violation of program semantics, or both. In the following, redescribe \nour current work for using profiling directed feedback in superscalar optimization. The optimizations \ndescribed below (consisting of scheduling heuristics, basic block re-ordenng, and branch reversal) have \nbeen implemented and result in a 4-5% additional improvement on SPECint92 (using the short SPEC inputs \nfor generating profiling data, where available). Low Overhead Profiling Directed Feedback Regardless \nof what profiling directed feedback is used for, the collection of the profile information, in the form \nof execution counts in the flow graph, should have low overhead in order to make profiling more usable \nin a commercial compiler environment. Therefore, we gather exact counts for basic block execution, and \nderive from them the edge counts which are used in the optimization. Furthermore, we have devised techniques \nto reduce the number of basic block counts required, to further decrease the execution overhead of profiling. \nFor generating and using profiling code, we compile a program twice, and run each version of it separately. \nDuring the first pass of PDF, the compiler inserts run-time counting code in a subset of the basic blocks. \nWhen the program thus compiled is executed, it creates a file that indicates the number of times each \nbasic block that contained counting code was executed. Counts from multiple runs of the same program \ncan be accumulated. During the second pass of PDF, the compiler reads back from this file the execution \ncounts, at the same place in the compiler where counting code was inserted in the jirst pass, computes \nthe complete set of edge counts and basic block counts from the partial set of basic block counts that \nwere read back, and then uses these edge counts for aggressive optimization. The flow graph edge counts \nare maintained as compiler transformations occur, so subsequent stages of the compiler can also use the \nprofiling feedback. Note that it is sometimes necessary to create new ( dummy ) basic blocks during PDF, \nso that the edge counts can be uniquely determined from the basic block counts. In this case, the flow \ngraph is modified in the same way on both passes, so the state of the program at the point the counting \ncode is inserted in the first pass, and at the point the counts are read back during the second pass, \nis identical. As we observed above, not all basic blocks need to have runtime counting code for uniquely \ncomputing all the edge counts. It is also possible to reduce the amount of counting instructions in inner \nloops, and thus reduce the profiling overhead, by moving out invariant loads and stores from the loop, \nin a manner similar to our speculative load-store motion described above. Following is an example on \nthe inner loop of the SPEC eqntott benchmark (the original version, without the KAP/C preprocessor). \nOnly the basic blocks BB1, BB2, and BB4 inside the loop, and BB7 and BB8 outside the loop, have been \naugmented by (profiling) counting code. The reader can verify that all the remaining basic block and \nedge counts for the flow graph of this loop can be uniquely determined from the basic block counts of \nthe selected subset. Outside the inner loop (e.g., in BB8), the counting code overhead is three instructions \nper block (load the previous count for this basic block, add one, and store back the updated count). \nThe load and store instructions related to counting code have been moved out of the inner loop, since \ntheir operand addresses are loop invariant, and thus the counting code overhead is one instruction per \nbasic block inside the loop. The profiling code is inserted before scheduling, register allocation, and \nother optimization, so further code improvements are possible. These are not shown in the example, to \npreserve its structure. We use a constraint-propagation algorithm, inspired by Artificial Intelligence \ntechniques applied to electrical networks (e.g., [24]), for finding (and possibly creating) the basic \nblocks for counting code insertion. The idea is to have just enough counts, so that all the remaining \nedge and basic block counts in the flow graph can be uniquely determined from the gathered counts. The \ndetails of this algorithm are beyond the scope of this paper. /* =31= initialized tO addre~~ Of global \nbasic block counts table / I* moved loop invariant loads to preheader */ L rll.bbtable (r31,4004) counting \ncode L r12=bbtable(r31,4008) . L r13=bbtable(r31,4012) . (BB 1) CL.0: AI rll=rll,l counting for BB \n1 LHAU r4,r3=a(r3,2) load hlfwrd / incr r3 LHAU r6,r5=b(r5,2) c crO=r4,2 BP CL.l,crO.eq (BB 2) AI r12=r12,1 \ncounting for BB 2 LI r4=0 (BB 3) CL.1: c crl=r6,2 BF CL.2,crl.eq (BB 4) AI r13=r13,1 counting for BB \n4 LI r6.O (BB 5) CL.2: c crO.r4,r6 ET CL.10,crO.eq (BB 6) BCT CL.O (BB 7) I* invariant storee Dushed \nto loop exit */ ST bbtable(r31,4004).rll counting code ST bbtable(r31, 4008)=r12 ST bbtable(r31,4012) \n=r13 . L rO.bbtable(r31,4016) counting code AI rO=rO, 1 for ST bbtable(r31,4016) =r0 BB 7 (other code \nfor BB 7) ... (BB 8) CL.1O: /* invariant etores pushed to loop exit I ST bbtable(r31,4004) =rll counting \ncode ST bbtable(r31,4008)=r12 . ST bbtable(r31,4012)=r13 . L rO=bbtable(r31,4020) counting code AI rO=rO, \n1 for ST bbtable (r31,4020)=r0 BB 8 (other code for BB 8) Ball and Larus [3] have used a profiling technique \nthat inserts counting codeon asubset of the edges in the flow graph. They apply their technique to a \nfinal executable program, whereas in our case profiling code is insertedin the compiler backend, allowing \nmany sophisticated optimizations, such as motion of profiling overhead code out ofloops and scheduling, \nto be applied thereafter. Also, the backends infrastructure provides us with better static prediction \nknowledge, so that counting code can be placed in less frequently executed locations in the program. \nFurther, we prefer placing counting codein existing basic blocks where possible, rather than generating \nnew unconditional branches and extra tiny basic blocks that result from placing counting code on edges. \nThis facilitates the application of subsequent optimization phases. Scheduling Heuristics, Basic Block \nRe-ordering, Branch Reversal The Profile directed feedback feature is currently used for supplying branch \nprobabilities to the VLIW scheduler. This alters the view of the scheduler in terms of which operations \nare speculative and which others are non-speculative (if an operation is present only on a less frequently \nexecuted path it is considered speculative). Non-speculative operations are preferred over speculative \nones as usual, and there are no major modifications to the scheduling otherwise. The true (not from profiling) \nspeculativeness of operations is still computed for determining the safeness of loads. Similarly, scheduling \nwith duplication can be used on the most frequent path only, but now without fear of slowing down the \nother paths. Also, just before final code generation, the basic blocks are physically reordered following \na depth-first enumeration of the flow graph, using the basic block ordering procedure that is already \navailable in the context of unspeculation, described above. Then standard code straightening optimizations \nof the XIC compiler are applied to eliminate any awkward branching that may have resulted from the re-ordering. \nDuring the depth-first enumeration, the flow graph edges that are executed most frequently are followed \nfirst, unless the target of the edge is already visited. That is, the enumeration algorithm performs \nthe following steps: First it assigns the next number to the current node (basic block), and marks it \nvisited. Next it recursively visits the most probable successor of the current node, if it is not already \nvisited. Lastly, it recursively visits each remaining successor of the current node, if it has not been \nvisited. This causes themost frequently executed path tooccurfirst in the enumeration, and thereforebe \narranged in astraight line, where almost all branches fall through. By itself, this re-ordering technique \nwill not cause all conditional branches to fall through most of the time. For example, the basic block \nthat ends a critical loop may occur last in the enumeration, and therefore the code may end with a conditional \nbranch that is taken most of the time. Nevertheless, most Power and PowerPC hardware work better when \nconditional branches fall through most of the time, and additional performance benefits may be obtained \nif unconditional branches are eliminated. To achieve the latter goals, any conditional branches that \nare taken most of the time are reversed, so they are not taken most of the time, as follows: BT CL.l, \ncrl. eq taken most of the time becomes BF CL.2, crl. eq untaken most of the time B CL.1 CL.2: a new label \nand then code is copied from the basic block(s) at the target label CL.1, using the basic block expansion \ntechnique described above. Profiling directed feedback seems to offer many more optimization for superscalars \nbeyond the ones described above, either in the form of new transformations, or traditional optimization \nre-cast in a new light. This fertile area is the current focus of our work in progress. Acknowledgements \nWe are grateful to Bob Blainey, Sohel Saiyed, and Marty Hopkins for many helpful discussions. Bibliography \n[1] A.V. Aho, R. Sethi, and J.D. Unman, Compilers: Principles, Techniques and Tools, Addison-Wesley, \n1986. [2] A.V. Aho, J.D. Unman, and J.E. Hopcroft, The Design and Analysis of Computer Algorithms, Addison-Wesley, \n1974. [3] T. Ball and J.R. Larus, Optimally Profiling and Tracing Programs, Technical Report no. 1031, \nComputer Sciences Dept., U. of Wisconsin-Madison, 1991. Also in POPL 92. [4] D. Bernstein and M. Rodeh, \nGlobal Instruction Scheduling for Superscalar Machines, Proc. SIGPLAN 91 Symp. on Programming Lunguage \nDesign and Implementation (published as SIGPLAN Notices, Vol. 26, No. 6), pp. 241-255, June 1991. [5] \nD. Bernstein, M. Rodeh, and S. Sagiv, Proving Safety of Speculative Load Instructions at Compile Time, \nLecture Notes in Computer Science (Proc. 4th European Symposium on Programming), No. 582, B. Krieg-Brueckner \n(Ed.), Springer-Verlag, February 1992. [6] P.P. Chang, S.A. Mahlke, W.Y. Chen, N.J. Warter, and W.W. \nHwu, IMPACT: An Architectural Framework for Multiple-Instruction Issue Processors, Proc. 18th International \nSymposium on Computer Architecture, pp. 266-275, May 1991. [7] K. Ebcioglu, A Compilation Technique for \nSoftware Pipelining of Loops with Conditional Jumps, Proc. of the 20th Annual Workshop on Microprogramming, \npp. 69-79, ACM Press, 1987. [8] K. Ebcioglu, Global Scheduling, section of a book on compiler optimization, \nF. Allen, K. Zadeck, B.K. Rosen (eds.), to appear. [9] K. Ebcioglu and T. Nakatani, A New Compilation \nTechnique for Parallelizing Loops with Unpredictable Branches on a VLIW Architecture, in Lang. and Comp. \nfor Parallel Comp., D. Gelernter, A. Nicolau, and D. Padua (eds.), Research Monographs in Parallel and \nDistributed Computing, pp. 213-229, MIT Press, 1990. [10] K. Ebcioglu and A. Nicolau, A Global Resource \nConstrained Parallelization Technique, Proc. of 1989 International Conference on Supercomputing, pp. \n154-163, Crete, Greece, 1989. [11] J. Ellis, Bulldog: A Compiler for VLIW Architectures, MIT Press, 1986. \n[12] L. Feigen, D. Klappholz, R. Casazza, and X. Xue, The Revival Transformation, Proc. POPL 94, pp. \n421-434, ACM Press, January 1994. [13] J. Ferrante, K.J. and Ottenstein, and J.D. Warren, The Program \nDependence Graph and its Use in Optimization, ACM Trans. on Programming Languages and Systems, Vol. 9, \nNo. 3, pp. 319-349, July 1987. [14] M.C. Golumbic, and V. Rainish, Instruction Scheduling Beyond Basic \nBlocks, ZBM J. Research and Development, 34, 1, pp. 93-97, 1990. [15] R. Gupta and M. Soffa, Region Scheduling: \nAn Approach for Detecting and Redistributing Parallelism, IEEE Trans. on Software Engineering, Vol. 16, \nNo. 4, pp. 421-431, 1990. [16] S. Jain, Circular Scheduling: A new technique to perform software pipelining, \nProc. ACM SIGPLAN 91 Conference on Programming Language Design and Implementation, 1991. [17] M. Lam, \nSoftware Pipelining: An Effective Scheduling Technique for VLIW Machines, Proc. ACM SIGPLAN 88 Conference \non Programming Language Design and Implementation, pp. 318-327, June 1988. [18] S.-M. Moon and K. Ebcioglu, \nAn Efficient Resource-Constrained Global Scheduling Technique for Superscalar and VLIW Processors, Proc. \n25th Annual International Symposium on Microarchitecture, pp. 55-71, ACM, 1992. [19] F. Mueller and D.B. \nWhalley, Avoiding Unconditional Jumps by Code Replication, Proc. of PLDI 92, pp. 322-330, ACM, 1992. \n[20] T. Nakatani and K. Ebcioglu, Combining as a Compilation Technique for VLIW Architectures, in Proc. \n22nd Workshop on Microprogramming and Microarchitecture, Dublin, ACM, pp. 43-57, 1989. [21] T. Nakatani \nand K. Ebcioglu, Making Compaction Based Parallelization Affordable, IEEE Transactions on Parallel and \nDistributed Systems, Vol. 4, No. 9, pp. 1014-1029, September 1993. [22] G.M. Silberman and K. Ebcioglu, \nAn Architectural Framework for Migration from CISC to Higher Performance Platforms, Proc. 1992 International \nConference on Supercomputing, pp. 198-215, ACM Press, 1992. [23] D. Smith, M. Johnson, and M. Horowitz, \nLimits on Multiple Instruction Issue, Proc. of the Third International Conference on Architectural Support \nfor Programming Lunguages and Operating Systems (ASPLOS-111), pp. 290-302, ACM and IEEE, Boston Massachusetts, \n1989. [24] G.J. Sussman and G.L. Steele, Constraints--A Language for Expressing Almost Hierarchical Descriptions, \nArt.ijicial Intelligence, Vol. 14, pp. 1-39, 1980.\n\t\t\t", "proc_id": "178243", "abstract": "<p>We describe techniques for converting the intermediate code representation of a given program, as generated by a modern compiler, to another representation which produces the same run-time results, but can run faster on a superscalar machine. The algorithms, based on novel parallelization techniques for <italic>Very Long Instruction Word (VLIW)</italic> architectures, find and place together independently executable operations that may be far apart in the original code. i.e., they may be separated by many conditional branches or belong to different iterations of a loop. As a result, the functional units in the superscalar are presented with more work that can proceed in parallel, thus achieving higher performance than the approach of using hardware instruction dispatch techniques alone.</p><p>While general scheduling techniques improve performance by removing idle pipeline cycles, to further improve performance on a superscalar with only a few functional units requires a reduction in the pathlength. We have designed a set of new algorithms for reducing pathlength and removing stalls due to branches, namely speculative load-store motion out of loops, unspeculation, limited combining, basic block expansion, and prolog tailoring. These algorithms were implemented in a prototype version of the IBM RS/6000 xlc compiler and have shown significant improvement in SPEC integer benchmarks on the IBM POWER machines.</p><p>Also, we describe a new technique to obtain profiling information with low overhead, and some applications of profiling directed feedback, including scheduling heuristics, code reordering and branch reversal.</p>", "authors": [{"name": "Kemal Ebcioglu", "author_profile_id": "81100654810", "affiliation": "IBM T.J. Watson Research Center, P.O. Box 218, Yorktown Heights, NY", "person_id": "P159343", "email_address": "", "orcid_id": ""}, {"name": "Randy D. Groves", "author_profile_id": "81332502396", "affiliation": "IBM Rise Systern/6000 Division, 11400 Bumet Road, Austin, TX", "person_id": "P239202", "email_address": "", "orcid_id": ""}, {"name": "Ki-Chang Kim", "author_profile_id": "81539060856", "affiliation": "IBM T.J. Watson Research Center, P.O. Box 218, Yorktown Heights, NY", "person_id": "PP310950000", "email_address": "", "orcid_id": ""}, {"name": "Gabriel M. Silberman", "author_profile_id": "81100357845", "affiliation": "IBM T.J. Watson Research Center, P.O. Box 218, Yorktown Heights, NY", "person_id": "PP39083493", "email_address": "", "orcid_id": ""}, {"name": "Isaac Ziv", "author_profile_id": "81100424681", "affiliation": "IBM T.J. Watson Research Center, P.O. Box 218, Yorktown Heights, NY", "person_id": "P117010", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/178243.178247", "year": "1994", "article_id": "178247", "conference": "PLDI", "title": "VLIW compilation techniques in a superscalar environment", "url": "http://dl.acm.org/citation.cfm?id=178247"}