{"article_publication_date": "06-01-1994", "fulltext": "\n # Accurate Static Estimators for Program Optimization* Tim A. Wagner! Vance Maverick, Susan L. Graham \nand Michael A. Harrison Computer Science Division University of California, Berkeley Abstract Determining \nthe relative execution frequency of program regions is essential for many important op\u00adtimization techniques, \nincluding register allocation, function inlining, and instruction scheduling. Es\u00adtimates derived from \nprofiling with sample inputs are generally regarded as the most accurate source of this information; \nstatic (compile-time) estimates are considered to be distinctly inferior. If static estimates were shown \nto be competitive, however, their convenience would outweigh minor gains from profiling, and they would \nprovide a sound basis for optimization when profiling is impossible. We use quantitative metrics to compare \nestimates from static analysis to those derived from profiles. For C programs, simple techniques for \npredicting branches and loop counts sufhce to estimate intra\u00ad procedural frequency patterns with high \naccuracy. To determine inter-procedural estimates success\u00ad fully, we combine function-level information \nwith a Markov model of control flow over the call graph to produce arc and basic block frequency estimates \nfor the entire program. *This research has been sponsored in part by the Ad\u00advanced Research Projects \nAgency (ARPA) under Grant MDA972-92-J-1028, and supported in part by NSF grant CDA-8722788. The content \nof this paper does not neces\u00adsarily reflect the position or the policy of the Government. tFor correspondence: \nt wagner@cs. berkeley. edu or Tim Wagner, Computer Science Division, 571 Evans Hall, University of California, \nBerkeley CA 94720 Permission to ca y w thout fee all or patt of this material is althat the copies are \nnot made or distributed for granted prowd direct commercial advantage, the ACM copyright notica and the \ntitle of the publication and its date appear, and notice is given that @pYin is by permission of the \nAssociation of Computing Machinery.7 o copy otherwise, or to republish, requires a fee ancf/orspecific \npermission. SIGPLAN 84-6/64 Orlando, Florida USA @ 1994 ACM O-89791=662-X16410006..$35O For a suite of \n14 programs, including the C pro\u00adgrams from the SPEC92 benchmark suite, we demonstrate that static estimates \nare competitive with those derived from profiles. Using simple heuristics, we can determine the most \nfrequently executed blocks in each function with 8170 accu\u00adracy. With the Markov model, we identify 80% \nof the frequently called functions. Combining the two techniques, we identify 76% of the most frequently \nexecuted call sites. 1 Introduction Many compiler optimizations require information about relative execution \nfrequencies to allocate scarce resources or to reduce execution times. Intra-procedural frequency information \nis useful for per-function register allocation [4], code motion [3], and code layout for instruction \ncache packing [8]. Tail duplication, which requires trace scheduling, has been shown to improve the performance \nbenefit of classic optimizations [6]. More aggressive opti\u00admization require inter-procedural analyses, \nsuch as estimates of function invocation or call site frequen\u00adcies. Selective function inlining [5], \ninter-procedural register allocation [10], and inter-procedural mem\u00adory allocation tuning [2] would all \nbenefit from ac\u00adcurate estimates of this information. Execution frequencies may be estimated from pro\u00adfiles \nor by static techniques. Profiles are derived from running the program on inputs that are (usu\u00adally) \ndifferent from the one eventually used. Static estimates are derived by examining the structure of the \nprogram loop nests, conditional branch ex\u00adpressions, the static call graph and using this in\u00adformation \nto predict elements of the dynamic be\u00ad havior, and compare estimates. Since static estimation does not \nrequire a separate profiling compilation, selection of sample inputs, or feedback of run-time information \nto the compiler, it is more convenient to use than profiling. Also, since the collection of profiles \nor the selection of representative inputs might be difficult, static esti\u00admates may sometimes be the \nbest option available. Thus, an assessment of the accuracy of static esti\u00admates as input to compiler \noptimizations, and an idea of which estimation techniques are successful, are import ant for compiler \nwriters. Several researchers have explored the estimation of the running time of programs or subprograms, \nwhich is a related but somewhat different problem. Ramamoorthy [9] used a Markov model of control flow \nfor this purpose. Sarkar [11] used a combina\u00adtion of profiling and program analysis to estimate execution \ntimes and their variances. His approach used profiling because then-current static branch prediction \nwas inadequate for the task. Fisher and Freudenberger [7] demonstrated that branches in programs behave \nconsistently enough that static branch prediction is feasible. Ball and Larus [1] developed techniques \nthat make accurate branch predictions by identifying idioms in ex\u00adecut able code. We have employed a \nsimilar tech\u00adnique within the compiler, operating at the level of the abstract syntax and the C type \nsystem. Wall [12] addressed essentially the same problem as this paper, constructing estimated profiles \nand com\u00adparing them to real profiles. He found estimated profiles inferior. We extend his techniques \nby in\u00adcorporating branch prediction, by modelling control flow more accurately, and by separating the \nintra\u00adand inter-procedural analyses. We present first a set of simple heuristics, and then explore Markov \ntechniques for both levels of analysis. At the intra\u00adprocedural level, the simple AST-based heuristics \nperform well, but at the inter-procedural level, the greater sophistication of the Markov model yields \nappreciably more accurate estimates. Experimental Framework Our experiments were carried out with two \ntools: a C compiler modified to record analyses of each function and to instrument the code for profiling, \nand an off-line program that read both profile and analysis information and combined them to create The \ncompiler we used for instrumentation and anal\u00adysis was the GNU C compiler, gcc.1 We augmented its internal \nrepresentation with an explicit abstract syntax tree (AST) and control-flow graph (CFG) for each function. \nFor each source file, the com\u00adpiler saved a simplified representation of the ASTS and CFGS, along with \nadditional data needed to do static estimation. This information and the pro\u00adfile output for all sample \ninput files were then read by a single off-line program. Performing estimator construction and evaluation \nthis way simplified the task of changing estimators, since no re-compilation or re-running of our program \nsuite was required. By working with the AST, we see the structure of the program before some transformations \nhave been applied. It is well known that some compiler analyses, (e.g., constant propagation plus dead \ncode elimination) can eliminate basic blocks or otherwise change the number of branches. Also, computing \nbranch prediction miss rates in the presence of con\u00adstant test expressions would make the rate appear \nlower [7]. Our software corrects for this by predict\u00ading , but not counting towards the score, branches \nwhose conditional expressions could be determined via constant foIding. We limited our analysis methods \nto those whose running time was comparable to conventional se\u00adquential compiler optimizations. Our (untuned) \nim\u00adplementation required analysis time similar to that of gee s standard optimization option. We used \nall the C programs in the SPEC92 bench\u00admark suite and added several others. Table 1 lists the programs \nused in this study, along with the number of source lines they contain and a brief de\u00adscription. 3 A \nMetric for Evaluating Estimates For most optimizations, the benefit of correctly identifying a block, \nor the cost of misidentification, depends on its frequency of execution. To mea\u00adsure the utility of an \nestimate, we use the weight\u00admatching metric introduced by Wall [12]. This met\u00adric is designed to compare \nestimates and actual measurements for a set of program entities, for ex\u00ad lFree Software Foundation distribution \nfor Sun-OS 4.x, version 2.4.5. From-am II Lines alvinn 272 compress 1,503 ear 5,239 eqntott 3,482 espresso \n14,863 gcc 88,045 Sc 8,570 xlisp 7,741 awk 17,962 bison 10,887 cholesky 1,898 gs 78,228 mpeg 13,761 water \n1,490 Table 1: Programs used in this study. The /* Find first occurrence of a character in a string. \n*I char *strchr (char *str, int c) while (*str) { if (*str == c) return stx; str++; } return NULL; } \n Figure 1: Our running example: a simple imple\u00ad mentation of strchr. ample execution frequencies for \nthe basic blocks in a function. It measures how well the estimate iden\u00adtifies the top n blocks. We always \ndefine n as a percentage of the number of blocks, rather than as a fixed count. We select this quantile \naccording to the estimate and according to the measurement; the weight-matching score is the sum of the \nactual frequencies for the blocks in the estimated quan\u00adtile, divided by the sum of the actual frequencies \nfor the actual quantile. If the items in the estimated and actual quantiles are identical, this value \nwill be 100~0. When the estimate places different items in the quantile, the metric will be lower (or \npossibly equal the cut-off point may come between actual items that have the same value). Figure 1 and \nTable 2 show the application of this metric to a simple example of intra-procedural fre\u00adquencies. The \nfunction is an implementation of the C string library routine strchr. To generate the actual counts in \nthe table, the function was called once with arguments ( abc , a ) and once with I Description . Back-propagation \non neural Unix compression utility Simulate sound processing Translate boolean functions Minimize boolean \nfunctions GNU C compiler Unix spreadsheet Lisp interpreter net in the ear to truth table Unix pattern-matching \nutility LALR(l) parser generator Cholesky factorize a sparse matrix PostScript previewer Play MPEG video \nfiles Simulate first eight { system of water molecules are the C programs from the SPEC92 benchmark \nsuite. arguments ( abc , b ). Estimated counts were generated by the techniques described in Section \n4. The actual and estimated counts are ranked in in\u00adcreasing order. We examine cut-off points of 20% \n(1 block) and 60% (3 blocks). At 20%, the esti\u00admate correctly identifies the single busiest block, for \na score of 100%. At a cutoff of 60%, it iden\u00adtifies the first and second, but not the third; the score \nis 8870.2 Often scores are higher for wider cutoffs, but this is by no means universal. For modeling \nintra-procedural analysis, we permit the basic blocks of a function to compete only against others within \nthe same function. In mea\u00adsuring the success of function invocation estimates, we compare whole functions. \nOnly in ranking call sites in the entire program do we compare basic blocks from different functions. \nWe ran each program on several inputs (four or more in almost all cases). Weight-matching scores for \nan estimate were computed by comparing it sep\u00adarately to each profile, then averaging these scores. Scores \nfor profiling were computed by matching each profile to the aggregate of all the other profiles and averaging \nthe results. To aggregate profiles, we normalized them to have the same total basic block counts, then \nsummed each block s counts. 2When the percentage does not exactly divide the num\u00ad ber of blocks, we round \nup, and weight the extra block fractionally.  Actual Estimate Points Points Block Actual Estimate Ranks \nRanks at 2070 at 60y0 whale 3 5 11 3 if 3422 3 returnl 2 0.8 3 5 zncr 1443 1 returrqo 1 5 4 100% 88% \nSCORE: (3/3) (7/8) Table 2: Intra-procedural weight-matching scores for the strchr example (Figure 1). \nThe actual counts were generated by searching the string abc for the characters a and b . The estimated \ncounts were derived by the smart heuristic (Section 4). At the 20% cutoff, the estimate picks the most \nfrequent basic block; at the 60% cutoff it misses the third block.  A Simple Techniques for were found. \nStatic Prediction Ball and Larus [1] describe a branch predictor that identifies programming idioms in \nexecutable code. We attempted to design a smart branch predictor, This section explores simple methods \nfor predict\u00adwhich used similar heuristics to turn AST structure, ing frequencies. At the intra-procedural \nlevel an type information, and dataflow information in the AST-based model is used to form basic block \nfre\u00adcompiler into branch predictions.3 quency estimates. At the inter-procedural level, simple means \nare used to combine the call graph We used several common programming idioms to and a set of intra-procedural \nestimates into esti-predict the likely run-time direction of branches, in\u00admates for function frequencies. \nThe more sophis-cluding: ticated Markov analysis (including the CFG-based e Pointers are unlikely to \nbe NULL intra-procedural model), is discussed in Section 5. e Errors (calling abort or exit) are unlikely \nWhen one arm of a conditional construct writes 4.1 Branch and Loop Predictions to variables read elsewhere, \nthat arm is more likely Both the simple AST-based models and the Markov Multiple logical ANDs make a \ncondition less models depend upon branch and loop predictions in likely order to estimate local control \nflow. We adopted a very simple loop model, predicting that all loops Figure 2 shows the results of our \nbranch predic\u00aditerate five times. The reason for this choice was tion heuristics, and compares them to \nthe results that the programs in our benchmark set fall roughly from using other profiles (aggregated \nas described into two categories: numerical programs with sim\u00ad in Section 3) and to the perfect static \npredictor.4 ple control flow, and others with complex loop be- Overall, our branch predictor has a miss \nrate about havior. In the numerical category, it is often possi\u00ad twice that for profiling, which is similar \nto the re\u00adble to estimate the iteration counts of loops accu\u00ad sults in [1]. (The rates are not directly \ncomparable, rately. Since the control flow of these programs is since our predictions were made at the \nlevel of the generally simple, though, the standard loop count AST, rather than from the executable code,) \nwas quite sufficient for ordering basic blocks within 3we tried two techniques for handling switches: \nguessing functions and functions within programs. In the each arm to be equally likely, and weighting \narms by the non-scientific programs, the distribution of loop number of case labels on them. The latter \nperformed slightly iteration counts was highly skewed, with a peak at better, although switches did \nnot represent a large enough the low end and a long right tail. For this sub-fraction of dynamic branches \nfor the choice in handling them to have much effect. set of programs, the standard count of 5 (near the \n(&#38; defined in [I], this uses a single profile to predict average of the observed values) was a reasonable \nits own result; it thus represents the upper bound on the choice; none of the structural properties we \ninvesti\u00ad performance of static branch prediction, since the remaining gated were good predictors of iteration \ndistribution misses are intrinsic to any software-based scheme. n i (n CD Figure 2: Miss rates (percentages \nof dynamic branches mispredicted) for the predictor we used, profiling with alternate inputs, and for \nthe perfect static predictor (PSP). We omit branches whose condition is constant and all switch statements \n(the latter account for less for alvinn are uniformly low (0.23%), because its only 4.2 Predicting Basic \nBlock Frequencies within Functions To predict the ordering of blocks within functions, three methods \nfor producing relative frequency in\u00adformation were considered. Loop measures the con\u00adtribution due to \nlocating loops and assuming they execute five times whenever they are encountered. Each branch direction \nis considered equally likely (a 50/50 split for an if test, for example). Smart extends loop with the \nbranch prediction heuristics. Ivlarkov is discussed in Section 5: it uses a CFG\u00adbased model to more accurately \ndetermine the con\u00adtrol flow within a function. Loop and smart, the AST-based methods, require converting \nindividual branch predictions and loop iteration guesses into a relative ordering for the ba\u00adsic blocks \nwithin each function. To derive basic block frequencies, which are normalized to a single entry of the \nfunction, each branch prediction was converted into a probability.5 Then we traversed the AST top-down, \ncomputing the execution fre\u00adquency of each basic block based on the structures containing it. The AST-based \nmodel ignores break, cent inue, got o, and return statements. Figure 3 5We chose 0.8 for the predicted \narm of a binary branch. The exact value chosen did not have a significant effect. Predictor  Profiling \n PSP   than 3% of dynamic branches on average). The values branches are for loops that iterate many \ntimes. shows the ple. Since frequency times the estimated the basic Given the abstract syntax tree for \nour strchr exam\u00ad the if testis predicted false, the estimated of the statement return str; is 0.2 estimated \nfrequency of its parent. The frequencies from the AST are mapped to blocks in the CFG. relative ordering \nfrom the estimated basic block counts, we used the weight-matching heuristic to compare them to the actual \nbasic block counts derived from profiling. Each function was analyzed independently, and the resulting \nper-function scores were then averaged, weighted by the dynamic zhvo\u00ad cation count of the function in \nquestion. (As with all the analyses, this comparison was done for each available profile; only the mean \nresult is reported.) The results are shown in Figure 4, for a 5% cut\u00adoff margin, and are compared to \nthe results using an aggregate of other profiles for prediction. Es\u00ad sentially all the benefit to using \nloop iteration alone; branch prediction and the cise control flow analysis do improvement. Furthermore, \nbe derived comes from the addition of smarter refinement of more pre\u00ad not provide a significant the difference \nbetween static estimation and profile-based estimation at the intra-procedural level is small: in almost \nall cases, intra-procedural optimizations can successfully rely on a simple, compiler-provided analysis \ninstead of   100 90 80 ? 0 60 50 40 30 20 10 0 (n m Figure 4: Weight-matching scores for estimates \nof intra-procedural basic block frequency. For each program, we show the score for the loop heuristic, \nthe smart heuristic, the Markov technique (Section 5), and profiling with alternate inputs. The final \ncolumn is the average across all programs. resorting to profile inputs. 1.0 4.3 Predicting Function \nInvocation Counts \u00ad \\ Inlining, aggressive register allocation, and other optimizations require inter-procedural \ninformation in addition to intra-procedural knowledge. Since it  50.\u00ad is used to determine other inter-procedural \ninfor\u00admation, we first examine the prediction of function invocation counts. Having used some technique \nto assign per-function basic block frequencies, we can then combine those estimates with information \nabout the program s call graph to arrive at estimates for the dynamic entry ,8 ,*, count for each function. \nSeveral simple models were explored for this conversion. call site assigns an es-Figure 3: The AST for \nour running example, timated invocation count to each function which is strchr. The if statement is predicted \nfalse in this the sum of the basic block counts of its call sites. example; the while loop is assumed \nto execute five direct uses the same approach, but multiplies the times, so items in its body execute \nfour times. A final invocation count of any directly-recursive func\u00adsingle top-down tree walk computes \nan estimated tion by five. all rec multiplies the invocation count count (shown to the left of each node) \nfor each ba\u00ad of functions involved in any recursion by five, Fi\u00adsic block. For this function, the mapping \nfrom the nally, all rec2 uses the function invocation counts AST to basic blocks is simple. of all rec \nto scale up the execution counts of ba\u00adsic blocks, then reapplies the algorithm to compute new function \ncounts. For all four of these predictors, the indirect call 90 100 90 80 70 60 50 40 30 20 10 0 ti \n(a). Simple heuristic scores. From left to right; call-site, direct, all-ret, all-ret 2, and profiling. \nAll are weight-matching with a 2570 cutoff. 100 90 80 . 70 60 50 . 40 30 20 10 0 1 (b). direct, Markov, \nand profiling, at (c). direct, Markov, and profiling, at the 10% cutoff. the 2570 cutoff. Figure 5: Weight-matching \nscores for function invocations by estimates and profiling. All estimates are built on the smart intra-procedural \nestimator. At the 2570 cutoff, the Markov function-invocation estimator scores 8170 on average. site \ncounts are summed and divided among the functions whose address is taken, weighted by the W 0 (static) \nnumber of address-of operations done on that function name. Figure 5a gives the results of function invocation \nprediction for the simple predictors, using smart to generate the initial basic block counts, and for \npro\u00adfiling. all rec2 does slightly better at the 25% cut\u00adoff direct performs nearly as well, and is slightly \nmore stable across a range of cut-offs (for this rea\u00adson, we will use it hereafter as the best choice \namong this set of predictors). As we will see in the next section, function invoca\u00ad tion prediction can \nbe distinctly improved by using a more accurate model of control flow. 5 Markov Modeling of Control \nFlow Control flow within a function maybe modeled as a Markov process. The state of the model is the \nblock currently being executed; the transitions between states are the arcs between blocks. The relative \nex\u00adecution frequencies of the blocks may be computed from the probabilities associated with the arcs. \nWe first illustrate this concept on a single function, then apply it to inter-procedural analysis. This \nmodel was introduced by Ramamoorthy as a method of estimating program execution time [9]. We apply it \ninstead to estimating the relative exe\u00adcution frequency of program regions. We also han\u00addle certain difficulties \nin the model that he did not discuss. 5.1 Estimating Block Frequencies If all the branch probabilities \nin a function are known, including those for the controlling expres\u00adsions of loops, the relative execution \nfrequencies of all the blocks in the function may be computed ex\u00ad actly. Consider the example in Figures \n6 and 7. The execution frequency of every block in the function s control-flow graph may be expressed \nas a linear function of the other frequencies, using the branch probabilities associated with the arcs \nas multipliers. The frequency of the entry block is set arbitrarily at 1. The graph is thus translated \ninto a system of n equations in n unknowns, where n is the number of nodes in the graph. These equations \nare solved us- Figure 6: Control-flow graph for strchr, anno\u00adtated with entry value and branch probabilities. \nUnmarked arcs have probability 1. ing ordinary methods for linear systems. The structural heuristics \nfor estimating basic block frequencies take no account of break, cent inue, got o, or return statements. \nFrequencies inside a loop are multiplied uniformly by a constant. The Markov model, however, reflects \nthese explicit transfers of control. In Figure 6, the probabilities assigned to the while statement are \n0.8 true and 0.2 false, corresponding to a test count of 5. The solu\u00adtion to the equations yields a test \ncount of only 2.78, because the return within the loop reduces the flow back to the top. For static estimation, \nwe apply this technique with the same estimated probabilities used for the smart intra-procedural heuristic. \nAs we see in Figure 4, this model shows no improvement over the simple heuristics. In the next section, \nthough, we will see that the Markov model performs well at the inter\u00adprocedural level. It is an open \nquestion whether static branch pre\u00addiction can be accurate enough to make good use of the intra-procedural \nMarkov model (for example, by using a static predictor that generates probabil\u00adities directly, rather \nthan a true/false guess). 5.2 Estimating Function Invocation Frequencies Computing execution frequencies \non the call graph is amenable to the same Markov representation as control-flow graphs within functions. \nFrom intra\u00adprocedural analysis, whatever the technique, we have an estimate of execution frequency for \nevery entry = 1 while = entry + incr if = 0.8 whale returnl = 0.2 if incr = 0.8 if returnz = 0.2 while \n(a). Linear equations for the relative exe\u00adcution frequencies of nodes in the control flow graph of strchr \n(Figure 6). 100000 1 1 0 .8 0 0 0 0 0.2 o 1 .2 1 .8 0 0 o 0 0 1 0 o 1 0 0 0 0 o 1 entry while if returnl \nincr returnz 1 o 0 0 0 0 Mentry while if returnl = incr returnz 1 2.78 2.22 0.44 1.78 0.56 (b). Matrix \nsolution. Figure 7: Deriving block execution frequencies from branch probabilities. call site, relative \nto the frequency with which the containing function is called. In inter-procedural analysis, functions \nare nodes, and these relative fre\u00adquencies become the multipliers associated with the arcs. Arcs between \nthe same pair of functions are merged. The function main is assigned an execution count of 1. Solving \nthis system of equations yields an estimate of the execution frequency of each func\u00adtion in the program. \nIn Figure 5, we compare the best simple predic\u00adtor of function invocations, direct, to the Markov model; \nMarkov improves noticeably on the simple estimate, scoring 10% higher at both the 10% and 25 % cutoffs. \nBefore applying this technique to other inter\u00adprocedural questions, we must discuss two problems that \narise in constructing the Markov model. 5.2.1 Problem: Function pointers Function pointers make the call \ngraph inexact. We cannot determine statically which functions will be reached by a call through a pointer, \nand so are un\u00adable to assign correct weights to the potential arcs from that call site. We approximate \nthe weights by creating a special node, the pointer node , that acts as the target of all calls through \npointers. We draw an arc from this node to every function that has its address taken, weighting it according \nto the number of (static) address-of operations done on that function name. This approximation is a risk \nonly with codes that use function pointers very heavily. The Markov model cannot assign a high estimated \nfrequency to an indirectly called function, because it is forced to make all of them nearly equiprobable. \nOne program in this category is xlisp: all the 173 built-in Lisp functions are called by pointer. In \npractice, though, regardless of the input program, the Lisp interpreter spends most of its time in the \nread/eval/print loop and in garbage collection. The Markov model cor\u00adrectly identifies these functions \nas among the busi\u00adest. The only one of the programs in which a complex system of function pointers is \nused heavily enough for this analysis to fail is gs, in which some 650 functions (about half the functions \nin the program) are referenced indirectly. Here both the Markov and the simple heuristics do badly. Indirect \ncontrol flow can also arise in the intra\u00adprocedural case: switch statements are one exam\u00adple, as are \npointers to labels (a non-ANSI extension supported by gee).  5.2.2 Problem: Recursion Recursion can \nmake the call graph numerically inf\u00adormed. Because call frequencies are only esti\u00admates, we may assign \ncall-graph arc weights that are, strictly speaking, impossible. Specifically, we may give a weight greater \nthan 1 to an arc from a function to itself. If correct, this would mean that /* Count the number of \nnodes in a binary tree */ int count.nodes(struct tree.node *node) { if (node == NULL) return O; else \nreturn (count_nodes(node->left) + count_nodes(node >right) + 1) ; Figure 8: Incorrect branch prediction \ncan lead to invalid estimates of recursion frequency. for every time the function is called, it calls \nitself again more than once that is, it never returns. In solving the linear system, this invalid weight \nwill lead to anegative value for the function s execution frequency. Figure 8 shows how this problem \ncan arise from an incorrect branch prediction. The conditional statement in countnodes branches on a \ntest ofa pointer against NULL. This is one of the patterns our branch predictor recognizes: it predictsthat \nthe else branch willbe taken with greater probability. (In reality, the pointer will be NULL more often \nthan not, because a binary tree, in this implemen\u00adtation, will always havemore empty child slots than \nfilled ones.) Because the branch we predict contains two recursive calls, the first formulation of the \ncall graph will give a multiplier of 1.6 to the arc from countnodes to itself. In the case of direct \nrecursion, this problem can be corrected easily: recursive arcs with aprobabil\u00adity greater than 1 are \nchanged to a standard value of 0.8. With more complex recursion, it is difficult to detect by inspection. \nAccordingly, we first at\u00adtempt a solution for the entire program; then, if any estimates are negative, \nwe separate the graph into strongly-connected components (SCCS). We con\u00adsider each of these in isolation, \nconstructing a sub\u00adgraph with anartificial main node, which calls the various nodes in the SCC in proportion \nas they are called from other nodes inthe real graph. We apply a somewhat stricter criterion to these \nsubproblems than to the graph as a whole: their solution may not include negative values, or values larger \nthan some ceiling.e If the solution of this subproblem fails, we scale down all the arc probabilities \nin the SCC by 6After some experimentation, we chose a ceiling of 5. The arc from the artificial main \nnode of the subproblem to each of the nodes in the SCC received a flow of m/n, where m is the number \nof calls to the target from outside the SCC, and n the total number of calls into the SCC from outside. \na constant, repeating until the solution succeeds. Once each of the SCCS in the graph has passed this \ntest, the entire graph may be solved without error. In principle, a loop created by a got o could cause \na similar problem at the intra-procedural level, if control were transferred out of the loop only by \nlongjmp or exit. There were no such cases in any of the programs examined.  5.3 Estimating Call Site \nFrequencies In function inlining, the crucial information derived from a profile is the frequency of \nexecution of spe\u00adcific call sites [5]. Combining our intra-and inter\u00adprocedural heuristics, we can estimate \nthe global ranking of call sites by frequency. It is difficult or impossible to inline calls through \npointers, so we omit them from these scores. Figure 9 shows the accuracy of call site estimates. We identify \nthe bus\u00adiest 1/4 of the call sites in our programs with 76% accuracy.  6 Example: Using Estimates to \nGuide Optimization One simple practical use of function invocation es\u00adtimates is selective optimization: \noptimizing only those functions that are expected to be called fre\u00adquently. We measured the performance \nof selective optimization with compress,7 using function rank\u00adings from our heuristics and from two forms \nof pro\u00adfiling. We timed compress on a sample load; first without optimization, then with 1, 2, 3, 4, \n5, and 6 functions optimized (chosen according to each of the three rankings), and then with all 16. \nWe used gee s -02 switch to optimize the candidate func\u00adtions. The ordering was chosen in three different \nways: by the static Markov estimate of function invocations, by the results of the first profile used \non compress in the earlier sections, and by the aggregated (nor\u00admalized and summed) results of the remaining \nthree profiles. For each ordering, we added one function at a time to the optimized set, and ran this \nver\u00adsion of the binary on an input set different from the one used for profiling. Figure 10 shows the \nre\u00adsults. The performance increases monotonically as 7This was the only program that was small enough \nto make this experiment feasible. 1Uu 90  direct 80 Markov 70 y 60 profile 50 40 30 20 10 0  Figure \n9: The combination of intra-and inter-procedural heuristics provides estimates of call site frequency. \nThese scores were computed at the 25% cutoff. L estimate//.  F /: profile - /j / -.. -,. aggregate/~ \n~~ /~ /~ I1 /# / -------.. -. .-. . -.. < 0123456 16 Number of Functions Optimized Figure 10: Speedup \nachieved by selectively optimizing functions in compress. The run time of the program is dominated by \n4 of its 16 functions. The Markov estimate, which scores 100% at this 25% cutoff, identifies these functions \ncorrectly. functions are added, but the specific ordering deter\u00admines how the performance improves with \neach ad\u00additional function. None of the estimates select the best ordering for this input (in fact, the \naggregate is particularly unlucky). However, at the 25% cut\u00adoff point, the static estimate has identified \nthe top four functions correctly. Optimizing the remain\u00ad ing 12 functions does not measurably improve \nthe performance.   Conclusion The goal of this work was to provide optimizing compilers with accurate \nestimates of the relative execution counts of various program entities: basic blocks, functions, and \ncall sites. We have presented a series of estimation techniques and measured their accuracy by comparing \nthem to profiling. For intra-procedural frequencies, a simple loop\u00adbased analysis is sufficient, and \nrivals the ability of profiles to predict the frequently executed block(s) within each function. To predict \nfunction invocation counts, relative call site use, or other inter-procedural quantities, simple prediction \ntechniques do not perform as well; their performance may be improved by better modeling of control flow \nusing a Markov model. The time to perform this analysis is comparable to that of other optimizations, \nand may eliminate the expense and complexity of profiling. All the techniques discussed in this paper \nmay be easily implemented in a pro\u00adduction compiler.  Acknowledgments David Wall and Stefan Freudenberger \nassisted us in collecting profiling inputs. Jim Larus provided fur\u00adther information about the techniques \nused in [1]. Andrew Gelman and Chris Genovese provided sta\u00adtistical advice. The paper benefited from \nthe com\u00adments of many colleagues, including Steven Lucco, Ethan Munson, Robert Wahbe, and the referees. \nFinally, we are grateful to the Free Software Foun\u00addation for providing the GNU C compiler, without which \nthis work could not have been accomplished. [1] Thomas Ball and James R. Larus. Branch prediction for \nfree. Proceedings of the ACM SIGPLAN 93 CorL\u00adference on Programming Language Design and Imple\u00admentation, \npages 300 313, 1993. [2] David A. Barrett and Benjamin G. Zorn. Using life\u00adtime predictors to improve \nmemory allocation perfor\u00admance. In Proceedings of the ACM SIGPLA N 93 Con\u00adference on Programming Language \nDesign and Imple\u00admentation, pages 187 196, Albuquerque, New Mexico, 1993. [3] Preston Briggs and Keith \nD. Cooper. Effective par\u00adtial redundancy elimination. Proceedings of the ACM SIGPLAN 94 Conference on \nProgramming Language Design and Implementation, 1994. [4] David Callahan and Brian Koblenz. Register \nallocation via hierarchical graph coloring. Proceedings of the ACM SIGPLAN 91 Conference on Programming \nLanguage Design and Implementation, pages 192 202, 1991. [5] Pohua P. Chang, Scott A. Mahlke, William \nY. Chen, and Wen-mei W. Hwu. Profile-guided automatic inline expansion for C programs. Software-Practice \nand Ex\u00adperience, 22(5):349 369, May 1992. [6] Pohua P. Chang, Scott A. Mahlke, and Wen-mei W. Hwu. Using \nprofile information to assist classic code optimizations. Software-Practice and Experience, 21(12):1301 \n1321, December 1991. [7] Joseph A. Fisher and Stefan M. Freudenberger. Predict\u00ading conditional branch \ndirections from previous runs of a program. Fifth International Conference on Architec\u00adtural Support \nfor Programming Languages and Operat\u00adang Systems, pages 85 95, 1992. [8] Scott McFarling. Program optimization \nfor instruction caches. Second International Conference on Architec\u00adtural Support for Programming Languages \nand Operat\u00ading Systems, pages 183 191, 1989. [9] C. V. Ramamoorthy. Discrete Markov analysis of com\u00adputer \nprograms. In ACM 20th Natzonal Conference, pages 386 391, Cleveland, Ohio, 1965. [10] Vasta Santhanam \nand Daryl Odnert. Register alloca\u00adtion across procedure and module boundaries. Proceed\u00adings of the ACM \nSIGPLAN 91 Confer-ence on Pro\u00ad gramming Language Design and Implementation, pages 28 39, 1991 [11] Vivek \nSarkar. Determining average program execution times and their variance. In Proceedings of the ACM SIGPLAN \n89 Conference on Programming Language Design and Implementation, pages 298 312, Portland, Oregon, 1989. \n[12] David W. Wall. Predicting program behavior using real or estimated profiles. Pr-oceedings of the \nACM SIG-PLAN 91 Conference on Programming Language De\u00adsign and Implementation, pages 59 7o, 1991. 96 \n \n\t\t\t", "proc_id": "178243", "abstract": "<p>Determining the relative execution frequency of program regions is essential for many important optimization techniques, including register allocation, function inlining, and instruction scheduling. Estimates derived from profiling with sample inputs are generally regarded as the most accurate source of this information; static (compile-time) estimates are considered to be distinctly inferior. If static estimates were shown to be competitive, however, their convenience would outweigh minor gains from profiling, and they would provide a sound basis for optimization when profiling is impossible.</p><p>We use quantitative metrics to compare estimates from static analysis to those derived from profiles. For C programs, simple techniques for predicting branches and loop counts suffice  to estimate intraprocedural frequency patterns with high accuracy. To determine inter-procedural estimates successfully, we combine function-level information with a Markov model of control flow over the call graph to produce arc and basic block frequency estimates for the entire program.</p><p>For a suite of 14 programs, including the C programs from the SPEC92 benchmark suite, we demonstrate that static estimates are competitive with those derived from profiles. Using simple heuristics, we can determine the most frequently executed blocks in each function with 81% accuracy. With the Markov model, we identify 80% of the frequently called functions. Combining the two techniques, we identify 76% of the most frequently executed call sites.</p>", "authors": [{"name": "Tim A. Wagner", "author_profile_id": "81100167382", "affiliation": "Computer Science Division, University of California, Berkeley", "person_id": "P282437", "email_address": "", "orcid_id": ""}, {"name": "Vance Maverick", "author_profile_id": "81100468092", "affiliation": "Computer Science Division, University of California, Berkeley", "person_id": "P290080", "email_address": "", "orcid_id": ""}, {"name": "Susan L. Graham", "author_profile_id": "81452606376", "affiliation": "Computer Science Division, University of California, Berkeley", "person_id": "PP14173434", "email_address": "", "orcid_id": ""}, {"name": "Michael A. Harrison", "author_profile_id": "81100011116", "affiliation": "Computer Science Division, University of California, Berkeley", "person_id": "PP14016859", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/178243.178251", "year": "1994", "article_id": "178251", "conference": "PLDI", "title": "Accurate static estimators for program optimization", "url": "http://dl.acm.org/citation.cfm?id=178251"}