{"article_publication_date": "06-01-1994", "fulltext": "\n Memory Access Coalescing: A Technique for Eliminating Redundant Memory Accesses JACK W. DAVIDSON and \nSANJAY JINTURKAR { jwd, sj3e}@virgini.a. edu Department of Computer Science, Thornton Hall University \nof Virginia Charlottesville, VA 22903 U. S. A. ABSTRACT As microprocessor speeds increase, memory bandwidth \nis increasing y the performance bottleneck for microprocessors. This has occurred because innovation \nand technological improvements in processor design have outpaced advances in memory design. Most attempts \nat addressing this problem have involved hardware solutions. Unfortunately, these solutions do little \nto help the situation with respect to current microprocessors. In previous work, we developed, implemented, \nand evaluated an algorithm that exploited the ability of newer machines with wide-buses to load/ store \nmultiple floating-point operands in a single memory reference. This paper describes a general code improvement \nrdgorithm that transforms code to better exploit the available memory bandwidth on existing microprocessors \nas well as wide\u00ad bus machines. Where possible and advantageous, the algorithm coalesces narrow memory \nreferences into wide ones. An interesting characteristic of the algorithm is that some decisions about \nthe applicability of the transformation are made at run time. This dynamic analysis significant y increases \nthe probability of the transformation being applied. The code improvement transformation was implemented \nand added to the repertoire of code improvements of an existing retargetable optimizing back end. Using \nthree current architectures as evaluation platforms, the effectiveness of the transformation was measured \non a set of compute-and memory-intensive programs. Interestingly, the effectiveness of the transformation \nvaried significantly with respect to the instruction-set architecture of the tested platform. For one \nof the tested architectures, improvements in execution speed ranging from 5 to 40 percent were observed. \nFor another, the improvements in execution speed ranged from 5 to 20 percent, while for yet another, \nthe transformation resulted in slower code for all programs. 1 INTRODUCTION Processor speeds are increasing \nmuch faster than memory speeds. For example, microprocessor performance has increased by 50 to 100 percent \nin the last decade, while memory performance has increased by only 10 to 15 percent. Additional hardware \nsupport such as larger, faster caches [Joup90], software-assi steal caches [Cal191], speculative loads \n[Roge92], stream memory controllers Permission to copy without fee all or part of this material is granted \nprovided that the copies are not made or distributed for direot commercial advantage, the ACM copyright \nnotice and the title of the publication and its date appear, and notice is given that copying is by permission \nof the Association of Computing Machinery. To copy otherwise, or to republish, requires a fee and/or \nspecific permission. SIGPLAN 94-6/94 Orlando, florida USA 0 1994 ACM 0-89791 -662-xt94/0006..@.5O [McKe94], \nand machines with wider memory buses, helps, but the problem is serious enough that performance gains \nby any approach, including software, are worth pursuing. Furthermore, even with additional hardware, \nprocessors often do not obtain anywhere near their peak performance with respect to their memory systems. \nThis paper describes a code improvement transformation that attempts to utilize a processor s memory \nsystem more effectively by coalescing narrow loads and stores of width N bits into more efficient wide \nloads and stores of width N x c where c is a multiple of two and the processor can fetch N x c bits efficiently. \nThe terms narrow and wide are relative to the target architecture. On a 16-bit architecture, for example, \ntwo narrow loads of bytes (8\u00adbits) that are in consecutive memory locations might be coalesced into a \nsingle wide load of 16 bits. Similarly, on a 64-bit architecture, four narrow stores of words (16-bits) \nthat are in consecutive memory locations and proper] y aligned might be coalesced into a single wide \nstore of 64-bits. As the paper shows, the analysis to perform such transformations is difficult but doable, \nand in many cases well worth the effort. The two questions that the analysis must answer are: Is the \ntransformation safe and is the transformation profitable? Safety analysis determines whether the transformation \ncan be done without changing the semantics of the program. The two key components of the safety analysis \naddress aliasing and data alignment issues. Alias analysis, in particular, is extremely difficult when \nthe source language contains unrestricted pointers [Land92, Land93]. The problem is further compounded \nbecause for many codes where this transformation would be beneficial, the code is structured so that \naliasing and data alignment hazards cannot precisely be determined via interprocedural, compile-time \nanalysis. The paper describes a new technique, called ron-time alias and alignment analysis, that neatly \nsolves this problem. Profitability analysis determines whether the transformation wi 11result in code \nthat runs faster, This is perhaps the most difficult part of the analysis because memory coalescing interacts \nwith other code improvements. For example, to expose more narrow, consecutive memory references for possible \ncoalescing, loops are sometimes unrolled by the optimizer. However, naive loop unrolling may cause the \nsize of a loop to grow larger than the instruction cache, and any gains in performance by memory coalescing \nmay be more than offset by degraded cache performance. Similarly, memory coalescing collects memory accesses \nthat are distributed throughout the loop into a single reference. This gathering of dependencies into \na single instruction can adversely affect instruction scheduling. The paper discusses how these and other \nissues are resolved so that the memory coalescing yields code that runs faster, not slower. The following \nsection briefly discusses work related to reducing memory bandwidth requirements of programs. Section \n2 describes the algorithm with emphasis on the analyses required to apply the transformation safely and \nprofitably. Section 3 describes the implementation of the algorithm in an existing retargetable back \nend called vpo [Beni89, Beni94]. Using a C front end and vpo, the effectiveness of the transformation \nwas evaluated on three processors: DEC S Alpha [Digi92], Motorola s 88100 [Moto91], and Motorola s 68030 \n[Moto85]. Section 4 contains a summary. 1.1 RELATED WORK Software approaches to the memory bandwidth \nproblem focus on reducing the memory bandwidth requirements of programs. For example, there is a plethora \nof research describing algorithms for register allocation, a fundamental transformation for reducing \na program s memory bandwidth requirements. Register allocation identifies variables that can be held \nin registers. By allocating the variable to a register, memory loads and stores previously necessary \nto access the variable are eliminated. An evaluation of the register coloring approach to register allocation \nshowed that up to 75 percent of the scalar memory references can be removed using these techniques [Chow90]. \nCache blocking and register blocking are code transformations that also reduce aprogram s memory bandwidth \nrequirement. These transformations can profitably be applied to codes that process large sets of data \nheld in arrays. For example, consider the multiplication oftwo large arrays. By large, we mean that the \narrays are much larger than the size of the machine s data cache. Because the arrays are larger than \nthe cache, processing the entire array results in data being read from memory to the cache many times. \nCache blocking, however, transforms thecode so that ablockof the array that will fit in the cache is \nread in once, used many times, andtben replaced bythenext block. The performance benefits from this transformation \ncan be quite good. Lam, Rothberg, and Wolf[Lam91] show that for multiplication of large, floating-point \narrays, cache blocking can easily triple the performance of a cache-based system. Register blocking is \nsimilar in concept to cache blocking, but instead of transforming code to reduce the number of redundant \nloads of array elements into the cache, it transforms code so that unnecessary loads of array elements \nare eliminated. Register blocking can be considered a specific application of scalar replacement of subscripted \nvariables [CaH90, Dues93] and loop unrolling. Scalar replacement identifies reuse of subscripted variables \nand replaces them by references to temporary scalar variables. Unrolling the loop exposes ablockof these \nsubscripted variables to which scalar replacement can be applied. Register blocking and cache blocking \ncan be used in combination to reduce the number of memory references and cache misses. Another program \ntrattsformation that reduces aprogram s memory bandwidth requirements is called recurrence detection \nand optimization [Beni91]. Knuthdefines arecurrence relation asande that defines each element ofasequence \ninterrns of the preceding elements [Knut73]. Rectrrrence relations appear in the sohttionsto many compute-and \nmemory-intensive problems. Interestingly, codes containing recurrences often cannot bevectorized. Consider \nthe following C code: for (i =2; i<n; i++) x[i] = z[ i]* (y [i] -x[i-l l); This is the fifth Livermore \nloop, which is a tri-diagonal elimination below the diagonal. It contains a recurrence since x [ i ] \nis defined in terms of x [i-II . By detecting the fact that a recurrence is being evaluated, code can \nbe generated so that the x [ i ] computed on one iteration of loop is held in a register and is obtained \nfrom that register on the next iteration of the loop. For this loop, the transformation yields code that \nsaves one memory reference per loop iteration. For machines with wide-buses (the size of the bus is greater \nthan the size of a single-precision floating-point value), it is possible to compact some number of floating-point \nloads into a single reference [Alex93]. Indeed, the work reported here is a generalization and extension \nof this technique applied to data of any size. We call this technique memory access coalescing. This \ntechnique can be used with the techniques mentioned previously. 2 MEMORY ACCESS COALESCING 2.1 MOTIVATION \nTo describe memory access coalescing and highlight the potential hazards that must be handled by an optimizer, \nconsider the C code in Figure 1a. The code computes the dot product of two vectors containing 16-bit \nintegers. The code is taken from a signal processing application, and 16-bits was sufficient to represent \nthe dynamic range of the sampled signal. Figure 1b contains the DEC Alpha machine code in register transfer \nlists (RTLs) generated by our compiler. Because the DEC Alpha is a relatively new architecture, and because \nit has some interesting characteristics that affect code generation, a few relevant details of the architecture \nare described. There are 3264\u00adbit fixed point registers, and all operations are performed on 64-bit registers. \nThe load and store instructions can move 32-bit (longWord) or 64-bit (quadword) quantities from and to \nmemory. Memory addresses must be naturally aligned. Data that is 2N bytes in size is naturally aligned \nif it is stored at an address that is a multiple of 2N. To accommodate loading and storing of data that \nis unaligned, the architecture contains unaligned loads and stores of 64-bits. These instructions fetch \nthe aligned quadword that contains the unaligned data. There is a full complement of arithmetic and logical \ninstructions that manipulate 64-bit values. In addition, there are three instructions, add, subtract, \nand multiply, that operate on 32-bit data. In a departure from other RISC architectures, the Alpha does \nnot include instructions for loading and storing bytes or shortwords (16-bits). Instead, architectural \nsupport is provided for extracting 8-bit (byte) and 16-bit (shortword) quantities from 64-bit registers. \nFor example, there are instructions for efficiently extracting and inserting bytes or shortwords frornho \na register. The rationales for these design decisions are outlined in the Alpha architecture handbook \n[Dec92]. With this information in mind, the code in Figure lb can be explained. In the RTL code, q [ \nn I and r [ n ] refer to fixed-point registers. r [n I is used when the operation is 32-bit. In the code, \nQ [ addrl refers to quadword memory. The unaligned load instruction at line 12 fetches the aligned quadword \nthat contains a [ i ] It is necessary to use an unaligned load because the base addresses of a and b \nare not guaranteed to be aligned on a quadword boundary, but they are guaranteed to be aligned on a shortword \nboundary. The instructions at lines 14 through 16 extract the shortword from the quadword. Line 14 computes \nthe offset of the shortword within the register. int dotproduct(short a[l, short b[l, int n) { int c, \ni; ~=o; for (i = O; c += a[i] return c; 1 i < * n; b[i]; i++) Figure 1a. Dot-product loop. 1. r[4] =O; \n 2. // test n for zero-trip loop 3. r[O] = r[311 -r[181; 4. Pc =r[O] >= o-> L15; 5. // compute address \nof a+n*2 6. q[61 = r[18] << 32; 7. q[6] = q[6] >> 32; 8. q[6] = q[61 << 1; 9. q[61 = q[161 + r[61; \n 10. L17 11. // load quad containing a[i] 12. q[21 = Q[(q[161 )&#38;-71; 13. // extract and sign extend \na[il 14. q[8] = q[161 + 2; 15. q[l] = EQH[q[2],q[8]]; 16. r[l] = q[ll >> 48; 17. // load quad containing \nb[il 18. q[31 = Q[(q[171 )&#38;-71; 19. // extract and sign extend b[i] 20. q[8] = q[171 + 2; 21. \nq[2] = EQH[q[3],q[8]]; 22. r[2] = q[21 >> 48; 23. // compute product and accumulate 24. r[l] = r[l] \n* r[21; 25. r[4] = r[4] + r[ll; 26. / advance to next array elements 27. q[17] = q[17] + 2; 28. q[16] \n= q[16] + 2; 29. // test for loop termination 30. q[o] = q[161 -q[61; 31. PC=q[O] <0-> L17; 32. L15 \n 33. r[O]=r[41;  Figure lb. Original code for loop. 1. r[4] =O; 2. // test for zero-trip loop 3. \nr[O] = r[31] -r[18]; 4. PC = r[O] >= O-> L15; 5. // compute loop termination 6. q[6] = r[18] << 32; \n 7. q[6] = q[6] >> 32; 8. q[6] = q[6] << 1; 9. q[61 = q[16] + q[6]; 10. L17 11. // load quad containing \na[i] 12. CI[211 = Q[ [1611; 13. // extract a[il (two bytes) 14. q[8] = q[16] + 2; 15. q[l] = EQH[q[211,q[8]]; \n 16. r[l] = q[l] >> 48; 17. // load quad containing b[i] 18. q[201 = Q[q[1711; 19. // extract b[i] \n(two bytes) 20. q[8] = q[17] + 2; 21. q[21 = EQH[q[201,q[811; 22. r[2] = q[2] >> 48; 23. // compute \ndot product 24. r[l] = r[l] * r[2]; 25. r[4] = r[4] + r[l]; 26. // extract a[i+l] 27. q[8] = q[16] \n+ 4; 28. q[l] = EQH[q[21],q[8]]; 29. r[l] = q[l] >> 48; 30. // extract b[i+ll 31. q[8] = q[17] + \n4; 32. q[2] = EQH[q[201,q[8]]; 33. r[2] = q[2J >> 48; 34. // compute dot product 35. r[l] = r[l] \n* r[2]; 36. r[4] = r[4] + r[l]; 37. . 38. .. 39. // adv to next a[il &#38; b[il 40. q[16] = q[16] \n+ 8; 41. q[17] = q[171 + 8; 42. // test for loop termination 43. q[o] = q[161 -q[61; 44. pc =. < \n-> L17; uIOI 0  45. L15 46. r[O]=r[4];  Figure lc. Unrolled loop with coalesced memory references. \nFigure 1: DEC Alpha code for dot-product. 1 //Main routine to coalesce memory accesses 2 proc CoalesceMemoryAccesses(CtrrrFunction) \nis 3 II Consider each loop in the current function. 4 VLOOP e CurrFunction.Loop do 5 LOOP. InductionVars \n+--FindInductionVars(LOOP) 6 // Unroll the loop. Ifit fits in the cache, use it, else use the rolled \nloop. 7 CurrFunction.Loop +\u00ad {UnRollLoopIfProfitable(LOOP)} u CurrFunction.Loop 8 //Classifies memory \nreferences into different partitions ifa unique identljier is found to distinguish 9 //a set of such \nreferences. lkus, all references to an array A passed as a parameter will have a loop 10 //invariant \nregister (most probably the register containing the start address of A) as their partition 11 // identtfrez \n12 ClassifyMemoryReferencesIntoPartitions(LOOP) 13 // Calculate relative offsets of the memory references \nbelonging to same partition from the induction variable. 14 //ffa constant offset is not found, it is \nnot safe to do memory coalescing. Sort the offsets. 15 16 17 18 19 CalculateRelativeOffsets(LOOP) EliminateInductionVariables(LOOP) \n//Attempt Wide reference optimization WideRefOptimization(LOOP, CurrFunction) enddo 20 endproc Figure \n2: Memory access coalescing algorithm main loop. The RTL q[l] = EQH[q[2], q[8]] shifts register q[2] \nleft bythenumber of bytes specifiedbythelow three bits of q [ 8 ], inserts zeros into the vacated bit \npositions, and then extracts 8 bytes into register q[l]. Line 16sign extends the shortword. Lines 18 \nthrough 22 perform a similar operation for b[i]. The code in Figure 1b, as it stands, is fair] y tight. \nHowever, the loop fetches the same quadwords for the respective arrays every four iterations. Thus, for \nevery four iterations six redundant loads are executed. It is these redundant memory accesses that memory \ncoalescing eliminates. By unrolling the loop four times, and applying the memory coalescing algorithm, \nthe optimizer produces the code in Figure 1c, Notice that there are still two loads in the loop (lines \n12 and 18), but the modified loop iterates one-fourth as many times as the loop of Figure lb. Thus, the \noriginal loop performs 2xn memory references, while the coalesced loop performs ~ memory references forasavingsof75 \npercent. At this point, the transformation may seem rather straightforward. However, there are subtle \ndetails that must be addressed. First, the code in Figure lC assumes that the starting address of the \nvectors a and b are aligned on a quadword boundary. This may, or may not betroe. If it is not trtte, \nthe first memory reference to an unaligned address will trap. Second, the code also assumes that the \nlength of the vectors indivisible by four, This too may, or may not be true. If it is not true, the loop \nwill fetch data outside the arrays and possibly fault (we d be lucky if it did), but is more likely that \nsilently an incorrect result will be computed. Third, the loop body has gotten larger, and the assumption \nis that any potential negative effects due to increasing the size of the loop will be offset by the gains \nresulting from reducing the number of memory references, This may or may not be a reasonable assumption. \n 2.2 MEMORY ACCESS COALESCING ALGORITHM These safety and profitability issues mentioned above, and others, \nmust be handled by the memory access coalescing algorithm. The Ccode in Figure lahighlights thedifficulty \nof this analysis. For this routine, standard intraprocedural analysis cannot gather the necessary information \nto safely coalesce memory references. The vectors and n, the number of elements in the arrays, are parameters. \nInterProcedural analysis would help, but often the routines of interest are part of a library and are \nnot accessible until link time. One could limit the applicability of the algorithm to routines where \nstatic, compile-time analysis is sufficient, but our experience shows that this would eliminate most \nopportunities for applying the algorithm. Theapproach taken here attempts to clothe analysis at compile \ntime, if possible, but if it is not possible code is generated to check the safety issues at run time, \nOur evaluations show that it is generally possible to do this in a way that the impact of the extra code \nfor checking is negligible. Figure 2contains thehigh-level portion of the algorithm. Due to space limitations, \nthe entire algorithm cannot be presented. The focus here is the profitability and safety analysis. Line \n7 determines if it is profitable to unroll the loop. Our heuristic is that if the original loop will \nfit in the instruction cache, then the algorithm must ensure that the unrolled loop will fit as well. \nIn addition, this routine, if necessary, produces code to execute the loop body enough times so that \nthe number of iterations of the main loop is a multiple of the unrolling factor. Line 12 analyzes the \nmemory reference of the loop andpartitions them into disjoint sets for later analysis [Beni9 1]. The \nmemory access coalescing is done by WideRefOptimization. After identifying candidate memory references \nfor coalescing, DoProfitability AnalysisAndModify is called. Thealgorithm is in Figure 3. The algorithm \nmakes a copy of the loop and performs memory coalescing on it. This involves not only coalescing the \nmemory accesses, but inserting code to extract the required information from the coalesced memory reference. \nAfter doing this, it calls the scheduler to schedule both loops and does a comparison. Ifit appears advantageous \nto use the coalesced loop, then vasious 1 //Do Cost/Benefit analysis before doing memory coalescing \n2 proc DoProfitability Anal ysisAndModify(LOOP, S, T, WideSize, CurrFunction) is 3 Inst &#38; 0 4 /! \nDo data hazard analysis for possible aligned and unaligned wide references AlignedWideType +--DoHazardAnal \nysis(LOO~ S, ALIGNED, LOOP. PossibleAliases, Inst, WideSize AlignedWideReferencePosition, AlignedWideReferenceAddress) \n6 UnAlignedWideType + DoHazardAnalysis(LOOP, S, -ALIGNED, LOOP. PossibleAliases, Inst, WideSize, UnalignedWideReferencePosition, \nUnAlignedWideReferenceAddress) 7 // Chack if a valid Wide memory reference which can replace the narrow \nreferences is found 8 if IsValidType(AlignedWideType) v IsValidType(UnAlignedWideType) then 9 //Make \na copy of the loop. Schedule the instructions in the original loop and find the number of cycles necessary, \n// Then insert appropriate wide references in ~he copy of the loop and schedule it 100. If the latter \nrequires 11 //less cycles, then go ahead. 12 LCOPY + DoReplication(LOOP) 13 // Calculate the cycles required \nby the original loop by static scheduling 14 CyclesforOriginalLoop +\u00adSchedule(LOOP) if IsValidType(AlignedWideType) \nthen 16 InsertWideReferences( LCOPY, S, AlignedWideReferencePosition, AlignedWideReferenceAddress) 17 \nendif 18 if IsValidType(UnAlignedWideType) then 19 InsertWideReferences( LCOPY, T, UnAlignedWideReferencePosition, \nUnA1ignedWideReferenceAddress) endif 21 [I Calculate the cycles required by the loop after replacing \nthe narrow references by wide ones. 22 CyclesforCopiedLoop + Schedule(LCOPY) 23 if CyclesforCopiedLoop \n< CyclesforOriginalLoop then 24 I/If the alignment checking for the aligned wide address under consideration \nis not there, //then insert a check that will allow the execution of the LCOPY if the address is actually \naligned 26 //at runtime 27 if _AlignmentCheckExists( LOOP, WideType, WideReferenceAddress) then 28 InsertAlignmentCheckInPreheader(LOOP.Preheader, \nLOOP. Label, LCOPY.Label WideReferenceAddress, WideType) 29 InsertAliasingChecksInPreheader(LOOP.Preheader, \nLOOP. Label, LCOPY.Label, Inst) else 31 1/Else just use the LCOPY instead of the original one, since \nthis is better 32 Target + FindTargetOfUnalignedAddress(LOOP.Preheader) 33 InsertAliasingChecksInPreheader(LOOP.Preheader, \nLCOPY.Label, 34 ChangeATargetOfAlignmentCheck(LOOP.Preheader, LOOP, Label, CurrFunction.Loop ~ CurrFunction.Loop \n-{LOOP} 36 endif 37 CurrFunction.Loop .$-CurrFunction.Loop 38 endif 39 if LOOP e Curr.Function then VZc \nSuTdo 41 Z. Modify +--TRUE 42 enddo 43 endif 44 return TRUE endif 46 return FALSE 47 endproc Figure 3: \nProfitability analysis algorithm. u {LCOPY} Target. Label, Inst) LCOPY,Label) 1 //Check if there are \nany data hazards 2 proc IsHazard(S, WideReferencePosition, ReferenceType, C, Inst) is 3 vMEsdo 4 //A \nwide load is inserted before the dominating loads of all the narrow loads. So the narrow load reference \nis //called the BottomInst. A wide store is inserted after the dominated store of all the narrow stores. \nSo the 6 //narrow store is called the TopInst. 7 if ReferenceType = LOAD then 8 BottomInst &#38; M 9 \nTopInst -WideReferencePosition else 11 TopInst e M 12 BottomInst ~ WideReferencePosition 13 endif 14 \n1/The narrow and the wide reference have to lie in the same basic block if BottomInst,BasicBlock # TopInst.BasicBIock \nthen 16 Return(TRUE) 17 endif 18 CurrInst + BottomInst 19 II Check all the instructions between the Bottomlnst \nand TopInst while CurrInst + CurrInst.PrevInst A CurrInst # TopInst do 21 //We cannot allow a load between \ntwo stores, all belonging to same partition. If they do not lie in same 22 //partition, there is a possibility \nof aliasing, which can probably be detected only at run time. 23 if IsStore(M.Reference) A IsLoad(CurrInst. \nReference) then 24 if (M. Paritition = CurrInst. Partition) then if ReferenceSameLocation(M.Reference, \nCurrInst.Reference) A -JsNeededTodoNarrowStoreOnly(M.Reference, CurrInst.Reference) then 26 Return(TRUE) \n27 endif 28 else 29 Inst + Inst u DoAliasDetection(CurrInst.Partition.Load, M. Partition. Store, C) endif \n31 //We cannot allow a store between two load or store references. 32 elseif Is Store(CurrInst, Reference) \nthen 33 if (M. Paritition = CurrInst.Partition) then 34 if ReferenceSameLocation(M. Reference, CurrInst.Reference) \n. Return(TRUE) 36 endif 37 elseif IsLoad(M,Reference) then 38 Inst + Inst u DoAliasDeteetion(CurrInst.Partition,Store, \nM. Partition. Load, C) 39 else Inst + Inst u DoAliasDetection(CurrInst.Partition.Store, M. Partition. \nStore, C) 41 endif 42 endif 43 FindBaseAndDisplacementOfAddress(CurrInst.Reference, Base, Displacement) \n44 llIf the base register has been modified, then the coalescing may not be safe. if IsModifiedBase(CurrInst. \nReference, Base) then 46 Return(TRUE) 47 endif 48 endwhile 49 end for IINo data hazards were found 51 \nReturn(FALSE) 52 endproc Figure 4: Hazard analysis algorithm, Figure 5: Flow graph showing alignment \nand alias checks. Potential Original Loop Body Hazard iterate n mod unrollfactor times + itcrate n / \nunrollfactor times checks are done to see if it is necessary to put alignment checks in the preheader \nof the loop. Additionally, if it is not possible to do static alias detection (for example, do the memory \nreferences overlap), then code is inserted in the preheader to do the checks at run time. A second key \nalgorithm is IsHazard that does the safety analysis. l ltis routine is contained in Figure 4. Most of \nthe analysis here is straightforward, The routine assures that coalesced memory references are in the \nsame basic block and that sequential consistency is preserved. In addition if aliasing cannot be resolved \nstatically, the routine DoAliasDetection is called which generates code that will be inserted in the \nloop preheader to check for potential aliasing problems (e.g., two arrays overlap in memory). If at run \ntime, an alias condition is detected, the original safe loop is executed. The result is code represented \nby the flow graph in Figure 5. For the example in Figure 1, this results in additional code being added \nto the loop preheader for each possible alias pair <a,b>. In particular, the following instructions are \nadded: // q[161: address of a; // q[171: address of b; q[18]: n q[l] = q[171 + q[181; q[o] = q[161 < \nq[l]; Pc =q[o] <= o-> L16; q[ll = q[161 + q[181; q[o] = q[171 < q[ll; PC=q[OJ >0-> L13; L16 q[O] = q[18] \n% 4; PC = q[O] != O-> L13; q[Ol = q[161 &#38; 7; PC = q[Ol != O-> L13; q[ol = q[171 &#38; 7; Pc = q[o] \n!= (J > L13; // do memory coalesced unrolled loop ... ... L13 // do original safe loop The code appearing \nbefore L16 checks to make sure the arrays do not overlap, while the code after L 16 checks for the ability \nto unroll and that the arrays are properly aligned. 3 IMPLEMENTATION AND RESULTS A prototype implementation \nof the memory access coalescing algorithm has been implemented in an existing retargetable compiler, \nand tested on platforms containing the following processors: DEC Alpha, Motorola 88100, and Motorola \n68020. Usingaset ofcompute-andmemory-intensive kemelloops listed in Table I, the effectiveness of the \nalgorithm was evaluated. These benchmarks were chosen because they represent realistic code, and they \ncontain loops that are memory-intensive and contain memory references that are candidates for memory \naccess coalescing. Memory access coalescing, unlike register allocation, code motion, induction variable \nelimination, etc., is not a code improvement that applies to code in general. However, like cache blocking, \nregister blocking, iteration space tiling, software pipelining, and recurrence detection and optimization, \nit does apply to a small set of important codes, and as the results show, it provides high payoff when \nit does apply. The results for the DEC Alpha are presented in Table 11.All timings were gathered by running \neach program ten times on a single-user machine. The two highest execution times and the two lowest were \ndiscarded, and an average of the remaining six times wastaken. Column 2(labe1ed cc-0) is the execution \ntimetakenby code produced by the native compiler with the loop unrolled. Column 3 is the execution time \ntaken by the code produced by our compiler again with the loop unrolled. The loops were unrolled so that \nthe effect of memory access coalescing could be isolated and observed. Column 4 contains the average \nexecution time for the benchmark when loads were coalesced, and column 5 contains the average execution \ntime when both loads and stores were coalesced Column 6 contains the percentage speedup. The first thing \nto notice is that the optimizing compiler in which the memory access coalescing algorithm is embedded \nis comparable to the native compiler. This indicates that the speedups in column 6 are not artifacts \nof embedding the algorithm in a poor compiler. The second thing to notice is that, in general, the percentage \nspeedup is quite good Table III contains similar timing information for the Motorola 88100 -based platform. \nIt is interesting to note that the code with both loads and stores coalesced runs slower than the code \nwith just loads coalesced. The reason is that the Motorola 88100 has efficient instructions for extracting \nbytes and words from a 32-bit Vpcclvpo -o Program Convolution Image Add Image xor Translate Eqntott \nMirror Description Gradient Directional Edge Convolution of a 500 by 500 black and white Image [Lind91] \nImage addition of two 500 by 500 black and white frames Image addition of two 500 by 500 black and white \nframes Translate 500 by 500 black and white image image to a new position Part of the SPEC 89 benchmark \nsuite Generate mirror image of 500 by 500 black and white image Lines of Code 154 48 48 48 146 50 Table \nI: Compute\u00ad and memory-intensive benchmarks. Percent Savings vpcc/vpo -o Program cc -o vpcc/vpiJ -o \n(coalesce loads (C013 -cl+) and stores) x 100co12 (coalesce loads) Convolution 16.67 I 17.76 I 15.62 \nI 15.76 I 11.26 Image add 17.41 17.71 11.48 10.44 41.05 Image add (16-12.03 12.02 8.97 8.13 32.36 bit) \n IImage xor 17.43 I 17.49 1 11.48 10.48 40.08 I I I Translate 11.46 10.52 8.45 7.04 33.11  II I I \nI Eqntott 19.17 21.55 I 20.72 20.72 3.86 I II I Mirror 15,62 I 14.49 I 12.63 I 9.84 I 32.09 Table H: \nDEC Alpha execution times (in seconds) and percent improvement. register, but there are no instructions \nfor inserting bytes and words expensive than simply loading the bytes and words directly. This into a \nregister without affecting the other bytes or words in the again highlights how most optimizations are \nmachine dependent. register, Thus, code must be generated using logical instructions to 4 SUMMARY place \nthe word into the proper position in the register. These sequences outweigh the gains of coalescing stores. \nHowever, We have described an algorithm for coalescing redundant coalescing loads was profitable exhibiting \nspeedups of up to 25 memory accesses in loops. If possible, static analysis is used to percent. resolve \nsafety and profitability issues. However, in most interesting We also implemented the algorithm in a \ncompiler for the cases, it is necesswy to rely on rtmtime tests to handle aliasing and Motorola 68030. \nUnfortunately, in all cases the code ran slower, alignment issues. Such code is relatively easy to generate. \nInspection of the code revealed that while the Motorola 68030 has Typically, 10 to 15 instructions must \nbe added in the loop preheader instructions for extracting bytes and words, these are much more to check \nfor possible hazards. The results on two machines show that the technique can result in substantial speedups. \nFor the DEC Percent Savings vpcc/vpo -o Vpcc(vpo -o Program cc -o Vpcclvpo -o (coalesce loads (C013-C014) \n~ ~m (coalesce loads) and stores) C012 Convolution Image add Image xor Translate Eqntott Mirror 22.86 \n15.33 15.34 16.32 130.3 20.52 22.82 25.74 15.34 17.52 145.0 19.23 18.87 12,97 12.94 13.49 143.2 16.03 \n22.64 13.45 13.7 16.91 143.2 16.89 17.3 15.39 15.64 24.46 1,3 16.64 Table III: Motorola 88100 execution \ntimes (in seconds) and percent improvement. Alpha, we observed speed ups ranging from 3 percent up to \n40 Systems, Santa Clara, CA, April 1991, pp. 132 percent. For the Motorola 88100, we observed speed ups \nof a few 141. percent up to 25 percent, technique resulted in slower while code, for the Motorola 68030 \nthe [Beni89] Benitez, Global M. E., and Davidson J, W., A Portable Optimizer and Linker , Proceedings \nof ACKNOWLEDGEMENTS SIGPLAN 88 Conference on Programming This work was supported in part by National \nScience Language Design and Implementation, June, 1988, pp.329 338. Atlanta, GA, Foundation grant CCR-9214904. \nREFERENCES [cal191] Callahan, Software D., Kennedy, Prefetching , K., and Porterfield, A,, Proceedings \nof the Fourth [Alex93] Alexander, M. J., Bailey, M. W., Childers, B. R., Davidson, J. W., Jinturkar, \nS., Memory Bandwidth Optimizations for Wide-Bus Machines , International Conference on Architectural \nSupport for Programming Languages and Operating Systems, Santa Clara, CA, April 1991, pp. 40-52. Proceedings \nInternational HI, January of the 26th Annual Hawaii Conference on System Sciences, Maui, 1993, pp. 466475. \n[Cal190] Callahan, Improving Variables. D. and Carr, S. and Kennedy, K, Register Allocation for Subscripted \nProceedings of the ACM SIGPLAN 91 [Aho86] Aho, A. V., Sethi, R., and Unman, J. D., Compilers Principles, \nTechniques and Tools, Addison-Wesley, Conference on Programming Language Design and Implementation, White \nPlains, NY, June, 1990, pp Reading, MA, 1986. 53 65. [Beni94] Benitez, M, Advantages E., and Davidson \nJ. of Machine-Dependent W., The Global [Chow90] Chow, F. C. and Hennessy, J. L. The Priority-Based Coloring \nApproach to Register Allocation. ACM Optimization , Conference Proceedings of on Programming the International \nLanguages and Transactions on Programming Systems 12(4):501 536 October Languages 1990. and System Architectures, \nSpringer Verlag Lecture Notes in Computer Science, Zurich, Switzerland, March, 1994, pp. 105 124. [Digi92] \nAlpha Architecture Corporation, 1992. Handbook, Digital Equipment [Beni91] Benitez, M, E., and Davidson, \nJ. W., Code Generation for Streaming: an Access/Execute Mechanism , Proceedings of the Fourth International \nSymposium on Architectural Support for Programming Languages and Operating [Dues93] Duesterwald, E., \nGupta, R., and Soffa, M. L., A Practical Data Flow Framework for Array Reference Analysis and its Use \nin Optimizations , Proceedings of the ACM SIGPLAN 93 Conference on Programming Language Design and Implementation, \nAlbuquerque, NM, June 1993, pp. 68 77. [Joup90] [Knut73] [Lam91] [Land93] [Land92] [Lind91] [McFa91] \n[McKe94] [Moto91] [Moto85] [Roge92] [Spec89] Jouppi, N., Improving Direct-Mapped Cache Performance by \nthe Addition of a Small Fully Associative Cache and Prefetch Buffers , Proceedings of the 17th Annual \nInternational Symposium on Computer Architecture, Seattle, WA, May 1990, pp. 364 373. Knuth, D. E., \nVolume 1: Fundamental Algorithms. Addison-Wesley, Reading, MA, 1973. Lam, M. and Rothberg, E. E. and \nWolf, M. E., The Cache Performance and Optimizations of Blocked Algorithms , Proceedings of the Fourth \nInternational Conference on Architectural Support for Programming Languages and Operating Systems, Santa \nClara, CA, April, 1991, pp 63 74. Landi, W., Ryder, B. G., and Zhang, S., InterProcedural Modification \nSide Effect Analysis with Pointer Aliasing , Proceedings of the ACM SIGPLAN 93 Conference on Programming \nLanguage Design and Implementation, Albuquerque, NM, June 1993, pp. 56-67. Landi, W., and Ryder, B. G., \nA Safe Approximation Algorithm for Interprocedural Pointer Aliasing , Proceedings of the ACM SIGPLAN \n92 Conference on Programming Language Design and Implementation, San Francisco, CA, June 1992, pp. 235 \n248. Lindley, C. A., Practical Image Processing in C, John Wiley and Sons, Inc., New York, NY, 1991. \nMcFarling, S., Procedure Merging with Instruction Caches , Proceedings of the ACM SIGPL4N 91 Conference \non Programming Language Design and Implementation, Toronto, Ontario, June 1991, pp. 71 79. McKee, S. \nA., Klenke, R. H., Schwab, A, J,, Wulf, W. A., Moyer, S. A., and Aylor, J. H., Experimental Implementation \nof Dynamic Access Ordering , Proceedings of the 27th Annual Hawaii International Conference on System \nSciences, Maui, HI, January 1994. MC88I1O: Second Generation RISC Microprocessor User s Manual. Motorola, \nInc., Phoenix, AZ, 1991. MC68020 32-bit Microprocessor User s Manual, Prentice-Hall, Inc. Englewood Cliffs, \nN. J. 07632. Rogers, A., and Li, K., Software Support for Speculative Loads , Proceedings of the Fifth \nInternational Conference on Architectural Support for Programming Languages and Operating Systems, Boston, \nMA, October 1992, pp. 38 50. Systems Performance Evaluation Cooperative, c/o Waterside Associates, Fremont, \nCA, 1989. \n\t\t\t", "proc_id": "178243", "abstract": "<p>As microprocessor speeds increase, memory bandwidth is increasingly the performance bottleneck for microprocessors. This has occurred because innovation and technological improvements in processor design have outpaced advances in memory design. Most attempts at addressing this problem have involved hardware solutions. Unfortunately, these solutions do little to help the situation with respect to current microprocessors. In previous work, we developed, implemented, and evaluated an algorithm that exploited the ability of newer machines with wide-buses to load/store multiple floating-point operands in a single memory reference. This paper describes a general code improvement algorithm that transforms code to better exploit the available memory bandwidth on existing microprocessors as  well as wide-bus machines. Where possible and advantageous, the algorithm coalesces narrow memory references into wide ones. An interesting characteristic of the algorithm is that some decisions about the applicability of the transformation are made at run time. This dynamic analysis significantly increases the probability of the transformation being applied. The code improvement transformation was implemented and added to the repertoire of code improvements of an existing retargetable optimizing back end. Using three current architectures as evaluation platforms, the effectiveness of the transformation was measured on a set of compute- and memory-intensive programs. Interestingly, the effectiveness of the transformation varied significantly with respect to the instruction-set   architecture of the tested platform. For one of the tested architectures, improvements in execution speed ranging from 5 to 40 percent were observed. For another, the improvements in execution speed ranged from 5 to 20 percent, while for yet another, the transformation resulted in slower code for all programs.</p>", "authors": [{"name": "Jack W. Davidson", "author_profile_id": "81100099215", "affiliation": "Department of Computer Science, Thomton Hall, University of Virginia, Charlottesville, VA, U.S.A.", "person_id": "PP14044617", "email_address": "", "orcid_id": ""}, {"name": "Sanjay Jinturkar", "author_profile_id": "81100255539", "affiliation": "Department of Computer Science, Thomton Hall, University of Virginia, Charlottesville, VA, U.S.A.", "person_id": "P259843", "email_address": "", "orcid_id": ""}], "doi_number": "10.1145/178243.178259", "year": "1994", "article_id": "178259", "conference": "PLDI", "title": "Memory access coalescing: a technique for eliminating redundant memory accesses", "url": "http://dl.acm.org/citation.cfm?id=178259"}